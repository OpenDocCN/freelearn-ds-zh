- en: Chapter 1. Classification in Data Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the last decade, we saw a huge growth in social networking and e-commerce
    sites. I am sure that you must have got information about this book on Facebook,
    Twitter, or some other site. Chances are also high that you are reading an e-copy
    of this book after ordering it on your phone or tablet.
  prefs: []
  type: TYPE_NORMAL
- en: This must give you an idea of how much data we are generating over the Internet
    every single day. Now, in order to obtain all necessary information from the data,
    we not only create data but also store this data. This data is extremely useful
    to get some important insights into the business. The analysis of this data can
    increase the customer base and create profits for the organization. Take the example
    of an e-commerce site. You visit the site to buy some book. You get information
    about books on related topics or the same topic, publisher, or writer, and this
    helps you to take better decisions, which also helps the site to know more about
    its customers. This will eventually lead to an increase in sales.
  prefs: []
  type: TYPE_NORMAL
- en: Finding related items or suggesting a new item to the user is all part of the
    data science in which we analyze the data and try to get useful patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Data analysis is the process of inspecting historical data and creating models
    to get useful information that is required to help in decision making. It is helpful
    in many industries, such as e-commerce, banking, finance, healthcare, telecommunications,
    retail, oceanography, and many more.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take the example of a weather forecasting system. It is a system that
    can predict the state of the atmosphere at a particular location. In this process,
    scientists collect historical data of the atmosphere of that location and try
    to create a model based on it to predict how the atmosphere will evolve over a
    period of time.
  prefs: []
  type: TYPE_NORMAL
- en: 'In machine learning, classification is the automation of the decision-making
    process that learns from examples of the past and emulates those decisions automatically.
    Emulating the decisions automatically is a core concept in predictive analytics.
    In this chapter, we will look at the following points:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working of classification systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model evaluation methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing the classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The word classification always reminds us of our biology class, where we learned
    about the classification of animals. We learned about different categories of
    animals, such as mammals, reptiles, birds, amphibians, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: If you remember how these categories are defined, you will realize that there
    were certain properties that scientists found in existing animals, and based on
    these properties, they categorized a new animal.
  prefs: []
  type: TYPE_NORMAL
- en: Other real-life examples of classification could be, for instance, when you
    visit the doctor. He/she asks you certain questions, and based on your answers,
    he/she is able to identify whether you have a certain disease or not.
  prefs: []
  type: TYPE_NORMAL
- en: Classification is the categorization of potential answers, and in machine learning,
    we want to automate this process. Biological classification is an example of **multiclass**
    classification and finding the disease is an example of **binary** classification.
  prefs: []
  type: TYPE_NORMAL
- en: In data analysis, we want to use machine learning concepts. To analyze the data,
    we want to build a system that can help us to find out which class an individual
    item belongs to. Usually, these classes are mutually exclusive. A related problem
    in this area is finding out the probability that an individual belongs to a certain
    class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Classification is a supervised learning technique. In this technique, machines—based
    on historical data—learn and gain the capabilities to predict the unknown. In
    machine learning, another popular technique is unsupervised learning. In supervised
    learning, we already know the output categories, but in unsupervised learning,
    we know nothing about the output. Let''s understand this with a quick example:
    suppose we have a fruit basket, and we want to classify fruits. When we say classify,
    it means that in the training data, we already have output variables, such as
    size and color, and we know whether the color is red and the size is from 2.3"
    to 3.7". We will classify that fruit as an apple. Opposite to this, in unsupervised
    learning, we want to separate different fruits, and we do not have any output
    information in the training dataset, so the learning algorithm will separate different
    fruits based on different features present in the dataset, but it will not be
    able to label them. In other words, it will not be able to tell which one is an
    apple and which one is a banana, although it will be able to separate them.'
  prefs: []
  type: TYPE_NORMAL
- en: Application of the classification system
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Classification is used for prediction. In the case of e-mail categorization,
    it is used to classify e-mail as spam or not spam. Nowadays, Gmail is classifying
    e-mails as primary, social, and promotional as well. Classification is useful
    in predicting credit card frauds, to categorize customers for eligibility of loans,
    and so on. It is also used to predict customer churn in the insurance and telecom
    industries. It is useful in the healthcare industry as well. Based on historical
    data, it is useful in classifying particular symptoms of a disease to predict
    the disease in advance. Classification can be used to classify tropical cyclones.
    So, it is useful across all industries.
  prefs: []
  type: TYPE_NORMAL
- en: Working of the classification system
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's understand the classification process in more detail. In the process of
    classification, with the dataset given to us, we try to find out informative variables
    using which we can reduce the uncertainty and categorize something. These informative
    variables are called **explanatory variables** or features.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final categories that we are interested are called target variables or
    labels. Explanatory variables can be any of the following forms:'
  prefs: []
  type: TYPE_NORMAL
- en: Continuous (numeric types)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Categorical
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Word-like
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text-like
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If numeric types are not useful for any mathematical functions, those will be
    counted as categorical (zip codes, street numbers, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: 'So, for example, we have a dataset of customer''s'' loan applications, and
    we want to build a classifier to find out whether a new customer is eligible for
    a loan or not. In this dataset, we can have the following fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Customer Age**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Customer Income (PA)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Customer Account Balance**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Loan Granted**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'From these fields, **Customer Age**, **Customer Income (PA)** and **Customer
    Account Balance** will work as explanatory variables and **Loan Granted** will
    be the target variable, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Working of the classification system](img/4959OS_01_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'To understand the creation of the classifier, we need to understand a few terms,
    as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Working of the classification system](img/4959OS_01_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Training dataset**: From the given dataset, a portion of the data is used
    to create the training dataset (it could be 70 percent of the given data). This
    dataset is used to build the classifier. All the feature sets are used in this
    dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Test dataset**: The dataset that is left after the training dataset is used
    to test the created model. With this data, only the feature set is used and the
    model is used to predict the target variables or labels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model**: This is used to understand the algorithm used to generate the target
    variables.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'While building a classifier, we follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Collecting historical data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cleaning data (a lot of activities are involved here, such as space removal,
    and so on)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining target variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining explanatory variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selecting an algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training the model (using the training dataset)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running test data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adjusting explanatory variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rerunning the test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While preparing the model, one should take care of outlier detection. **Outlier
    detection** is a method to find out items that do not conform to an expected pattern
    in a dataset. Outliers in an input dataset can mislead the training process of
    an algorithm. This can affect the model accuracy. There are algorithms to find
    out these outliers in the datasets. Distance-based techniques and fuzzy-logic-based
    methods are mostly used to find out outliers in the dataset. Let's talk about
    one example to understand the outliers.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have a set of numbers, and we want to find out the mean of these numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: 10, 75, 10, 15, 20, 85, 25, 30, 25
  prefs: []
  type: TYPE_NORMAL
- en: 'Just plot these numbers and the result will be as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Working of the classification system](img/4959OS_01_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Clearly, the numbers 75 and 85 are outliers (far away in the plot from the other
    numbers).
  prefs: []
  type: TYPE_NORMAL
- en: Mean = sum of values/number of values = 32.78
  prefs: []
  type: TYPE_NORMAL
- en: 'Mean without the outliers: = 19.29'
  prefs: []
  type: TYPE_NORMAL
- en: So, now you can understand how outliers can affect the results.
  prefs: []
  type: TYPE_NORMAL
- en: While creating the model, we can encounter two majorly occurring problems—**Overfitting**
    and **Underfitting**.
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting occurs when the algorithm captures the noise of the data, and the
    algorithm fits the data too well. Generally, it occurs if we use all the given
    data to build the model using pure memorization. Instead of finding out the generalizing
    pattern, the model just memorizes the pattern. Usually, in the case of overfitting,
    the model gets more complex, and it is allowed to pick up spurious correlations.
    These correlations are specific to training datasets and do not represent characteristics
    of the whole dataset in general.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram is an example of overfitting. An outlier is present,
    and the algorithm considers that and creates a model that perfectly classifies
    the training set, but because of this, the test data is wrongly classified (both
    the rectangles are classified as stars in the test data):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Working of the classification system](img/4959OS_01_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: There is no single method to avoid overfitting; however, we have some approaches,
    such as a reduction in the number of features and the regularization of a few
    of the features. Another way is to train the model with some dataset and test
    with the remaining dataset. A common method called cross-validation is used to
    generate multiple performance measures. In this way, a single dataset is split
    and used for the creation of performance measures.
  prefs: []
  type: TYPE_NORMAL
- en: Underfitting occurs when the algorithm cannot capture the patterns in the data,
    and the data does not fit well. Underfitting is also known as high bias. It means
    your algorithm has such a strong bias towards its hypothesis that it does not
    fit the data well. For an underfitting error, more data will not help. It can
    increase the training error. More explanatory variables can help to deal with
    the underfitting problem. More explanatory fields will expand the hypothesis space
    and will be useful to overcome this problem.
  prefs: []
  type: TYPE_NORMAL
- en: Both overfitting and underfitting provide poor results with new datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Classification algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will now discuss the following algorithms that are supported by Apache Mahout
    in this book:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Logistic regression / Stochastic Gradient Descent (SGD)**: We usually read
    regression along with classification, but actually, there is a difference between
    the two. Classification involves a categorical target variable, while regression
    involves a numeric target variable. Classification predicts whether something
    will happen, and regression predicts how much of something will happen. We will
    cover this algorithm in [Chapter 3](ch03.html "Chapter 3. Learning Logistic Regression
    / SGD Using Mahout"), *Learning Logistic Regression / SGD Using Mahout*. Mahout
    supports logistic regression trained via Stochastic Gradient Descent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Naïve Bayes classification**: This is a very popular algorithm for text classification.
    Naïve Bayes uses the concept of probability to classify new items. It is based
    on the Bayes theorem. We will discuss this algorithm in [Chapter 4](ch04.html
    "Chapter 4. Learning the Naïve Bayes Classification Using Mahout"), *Learning
    the Naïve Bayes Classification Using Mahout*. In this chapter, we will see how
    Mahout is useful in classifying text, which is required in the data analysis field.
    We will discuss vectorization, bag of words, n-grams, and other terms used in
    text classification.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hidden Markov Model (HMM)**: This is used in various fields, such as speech
    recognition, parts-of-speech tagging, gene prediction, time-series analysis, and
    so on. In HMM, we observe a sequence of emissions but do not have a sequence of
    states which a model uses to generate the emission. In [Chapter 5](ch05.html "Chapter 5. Learning
    the Hidden Markov Model Using Mahout"), *Learning the Hidden Markov Model Using
    Mahout*, we will take one more algorithm supported by Mahout Hidden Markov Model.
    We will discuss HMM in detail and see how Mahout supports this algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Random Forest**: This is the most widely used algorithm in classification.
    Random Forest consists of a collection of simple tree predictors, each capable
    of producing a response when presented with a set of explanatory variables. In
    [Chapter 6](ch06.html "Chapter 6. Learning Random Forest Using Mahout"), *Learning
    Random Forest Using Mahout*, we will discuss this algorithm in detail and also
    talk about how to use Mahout to implement this algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-layer Perceptron (MLP)**: In [Chapter 7](ch07.html "Chapter 7. Learning
    Multilayer Perceptron Using Mahout"), *Learning Multilayer Perceptron Using Mahout*,
    we will discuss this newly implemented algorithm in Mahout. An MLP consists of
    multiple layers of nodes in a directed graph, with each layer fully connected
    to the next one. It is a base for the implementation of neural networks. We will
    discuss neural networks a little but only after a detailed discussion on MLP in
    Mahout.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will discuss all the classification algorithms supported by Apache Mahout
    in this book, and we will also check the model evaluation techniques provided
    by Apache Mahout.
  prefs: []
  type: TYPE_NORMAL
- en: Model evaluation techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We cannot have a single evaluation metric that can fit all the classifier models,
    but we can find out some common issues in evaluation, and we have techniques to
    deal with them. We will discuss the following techniques that are used in Mahout:'
  prefs: []
  type: TYPE_NORMAL
- en: Confusion matrix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ROC graph
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AUC
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Entropy matrix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The confusion matrix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The confusion matrix provides us with the number of correct and incorrect predictions
    made by the model compared with the actual outcomes (target values) in the data.
    A confusion matrix is a N*N matrix, where N is the number of labels (classes).
    Each column is an instance in the predicted class, and each row is an instance
    in the actual class. Using this matrix, we can find out how one class is confused
    with another. Let''s assume that we have a classifier that classifies three fruits:
    strawberries, cherries, and grapes. Assuming that we have a sample of 24 fruits:
    7 strawberries, 8 cherries, and 9 grapes, the resulting confusion matrix will
    be as shown in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Predicted classes by model |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Actual class** |   | **Strawberries** | **Cherries** | **Grapes** |'
  prefs: []
  type: TYPE_TB
- en: '| **Strawberries** | 4 | 3 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| **Cherries** | 2 | 5 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| **Grapes** | 0 | 1 | 8 |'
  prefs: []
  type: TYPE_TB
- en: So, in this model, from the 8 strawberries, 3 were classified as cherries. From
    the 8 cherries, 2 were classified as strawberries, and 1 is classified as a grape.
    From the 9 grapes, 1 is classified as a cherry. From this matrix, we will create
    the table of confusion. The table of confusion has two rows and two columns that
    report about true positive, true negative, false positive, and false negative.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, if we build this table for a particular class, let''s say for strawberries,
    it would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **True Positive**4 (actual strawberries classified correctly) (a) | **False
    Positive**2 (cherries that were classified as strawberries)(b) |'
  prefs: []
  type: TYPE_TB
- en: '| **False Negative**3 (strawberries wrongly classified as cherries) (c) | **True
    Negative**15 (all other fruits correctly not classified as strawberries) (d) |'
  prefs: []
  type: TYPE_TB
- en: 'Using this table of confusion, we can find out the following terms:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Accuracy**: This is the proportion of the total number of predictions that
    were correctly classified. It is calculated as (True Positive + True Negative)
    / Positive + Negative. Therefore, *accuracy = (a+d)/(a+b+c+d)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Precision or positive predictive value**: This is the proportion of positive
    cases that were correctly classified. It is calculated as (True Positive)/(True
    Positive + False Positive). Therefore, *precision = a/(a+b)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Negative predictive value**: This is the proportion of negative cases that
    were classified correctly. It is calculated as True Negative/(True Negative +
    False Negative). Therefore, *negative predictive value = d/(c+d)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sensitivity / true positive rate / recall**: This is the proportion of the
    actual positive cases that were correctly identified. It is calculated as True
    Positive/(True Positive + False Negative). Therefore, *sensitivity = a/(a+c)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Specificity**: This is the proportion of the actual negative cases. It is
    calculated as *True Negative/(False Positive + True Negative)*. Therefore, *specificity
    =d /(b+d)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**F1 score**: This is the measure of a test''s accuracy, and it is calculated
    as follows: *F1 = 2.((Positive predictive value (precision) * sensitivity (recall))/(Positive
    predictive* *value (precision) +sensitivity (recall)))*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Receiver Operating Characteristics (ROC) graph
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'ROC is a two-dimensional plot of a classifier with false positive rate on the
    *x* axis and true positive rate on the *y* axis. The lower point (0,0) in the
    figure represents never issuing a positive classification. Point (0,1) represents
    perfect classification. The diagonal from (0,0) to (1,1) divides the ROC space.
    Points above the diagonal represent good classification results, and points below
    the line represent poor results, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Receiver Operating Characteristics (ROC) graph](img/4959OS_01_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Area under the ROC curve
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is the area under the ROC curve and is also known as AUC. It is used to
    measure the quality of the classification model. In practice, most of the classification
    models have an AUC between 0.5 and 1\. The closer the value is to 1, the greater
    is your classifier.
  prefs: []
  type: TYPE_NORMAL
- en: The entropy matrix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before going into the details of the entropy matrix, first we need to understand
    **entropy**. The concept of entropy in information theory was developed by Shannon.
  prefs: []
  type: TYPE_NORMAL
- en: 'Entropy is a measure of disorder that can be applied to a set. It is defined
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Entropy = -p1log(p1) – p2log(p2)- …….*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Each p is the probability of a particular property within the set. Let''s revisit
    our customer loan application dataset. For example, assuming we have a set of
    10 customers from which 6 are eligible for a loan and 4 are not. Here, we have
    two properties (classes): eligible or not eligible.'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(eligible) = 6/10 = 0.6*'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(not eligible) = 4/10 = 0.4*'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, entropy of the dataset will be:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Entropy = -[0.6*log2(0.6)+0.4*log2(0.4)]*'
  prefs: []
  type: TYPE_NORMAL
- en: '*= -[0.6*-0.74 +0.4*-1.32]*'
  prefs: []
  type: TYPE_NORMAL
- en: '*= 0.972*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Entropy is useful in acquiring knowledge of information gain. Information gain
    measures the change in entropy due to any new information being added in model
    creation. So, if entropy decreases from new information, it indicates that the
    model is performing well now. Information gain is calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '*IG (classes , subclasses) = entropy(class) –(p(subclass1)*entropy(subclass1)+
    p(subclass2)*entropy(subclass2) + …)*'
  prefs: []
  type: TYPE_NORMAL
- en: Entropy matrix is basically the same as the confusion matrix defined earlier;
    the only difference is that the elements in the matrix are the averages of the
    log of the probability score for each true or estimated category combination.
    A good model will have small negative numbers along the diagonal and will have
    large negative numbers in the off-diagonal position.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have discussed classification and its applications and also what algorithm
    and classifier evaluation techniques are supported by Mahout. We discussed techniques
    like confusion matrix, ROC graph, AUC, and entropy matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will move to the next chapter and set up Apache Mahout and the developer
    environment. We will also discuss the architecture of Apache Mahout and find out
    why Mahout is a good choice for classification.
  prefs: []
  type: TYPE_NORMAL
