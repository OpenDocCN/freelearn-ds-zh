["```py\n> skim(iris)\n── Data Summary ────────────────────────\n                           Values\nName                       iris\nNumber of rows             150\nNumber of columns          5\n_______________________\nColumn type frequency:\n  factor                   1\n  numeric                  4\n________________________\nGroup variables            None\n── Variable type: factor ─────────────────────────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique top_counts\n1 Species               0             1 FALSE          3 set: 50, ver: 50, vir: 50\n── Variable type: numeric ────────────────────────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate mean    sd  p0 p25  p50 p75 p100 hist\n1 Sepal.Length          0             1 5.84  0.828  4.3  5.1  5.8  6.4  7.9 ▆▇▇▅▂\n2 Sepal.Width           0             1 3.06  0.436  2   2.8  3    3.3  4.4 ▁▆▇▂▁\n3 Petal.Length          0             1 3.76  1.77  1   1.6  4.35  5.1  6.9  ▇▁▆▇▂\n4 Petal.Width           0             1 1.20  0.762 0.1 0.3  1.3  1.8  2.5  ▇▁▇▅▃\n```", "```py\nif(!require(GGally)){install.packages(\"GGally\")}\nIf(!require(TidyDensity)){install.packages(\"TidyDensity\")}\nlibrary(GGally)\nlibrary(TidyDensity)\ntidy_normal(.n = 200) |>\n  ggpairs(columns = c(\"y\",\"p\",\"q\",\"dx\",\"dy\"))\n```", "```py\ninstall.packages(\"DataExplorer\")\nlibrary(DataExplorer)\nlibrary(TidyDensity)\nlibrary(dplyr)\ndf <- tidy_normal(.n = 200)\ndf |>\n  introduce() |>\n  glimpse()\n```", "```py\n> df |>\n+   introduce() |>\n+   glimpse()\nRows: 1\nColumns: 9\n$ rows                 <int> 200\n$ columns              <int> 7\n$ discrete_columns     <int> 1\n$ continuous_columns   <int> 6\n$ all_missing_columns  <int> 0\n$ total_missing_values <int> 0\n$ complete_rows        <int> 200\n$ total_observations   <int> 1400\n$ memory_usage         <dbl> 12344\n```", "```py\ndf[c(\"q\",\"y\")] |>\n  plot_qq()\n```", "```py\nimport pandas as pd\nimport numpy as np\n# Create a DataFrame with missing data, duplicates, and mixed data types\ndata = {\n    'ID': [1, 2, 3, 4, 5, 6],\n    'Name': ['Alice', 'Bob', 'Charlie', 'Alice', 'Eva', 'Eva'],\n    'Age': [25, np.nan, 30, 28, 22, 23],\n    'Salary': ['$50,000', '$60,000', 'Missing', '$65,000', '$55,000',\n    '$75,000']\n}\ndf = pd.DataFrame(data)\n# Introduce some missing data\ndf.loc[1, 'Age'] = np.nan\ndf.loc[3, 'Salary'] = np.nan\n# Introduce duplicates\ndf = pd.concat([df, df.iloc[1:3]], ignore_index=True)\n# Save the sample data in present working directory\ndf.to_excel('dirty_data.xlsx')\n```", "```py\nimport pandas as pd\nimport numpy as np\n# Load Excel data into a pandas DataFrame\ndf = pd.read_excel('dirty_data.xlsx', index_col=0)\n# Handling Missing Data\n# Identify missing values\nmissing_values = df.isnull().sum()\n# Replace missing values with the mean (for numeric columns)\ndf['Age'].fillna(df['Age'].mean(), inplace=True)\n# Replace missing values with the mode (for categorical columns)\ndf['Salary'].fillna(df['Salary'].mode()[0], inplace=True)\n# Forward-fill or backward-fill missing values\n# This line is a placeholder to show you what's possible\n# df['ColumnWithMissingValues'].fillna(method='ffill', inplace=True)\n# Interpolate missing values based on trends\n# This line is a placeholder to show you what's possible\n# df['NumericColumn'].interpolate(method='linear', inplace=True)\n# Remove rows or columns with missing data\ndf.dropna(axis=0, inplace=True)  # Remove rows with missing data\ndf.dropna(axis=1, inplace=True)  # Remove columns with missing data\ndf.to_excel('cleaned_data.xlsx')\n```", "```py\n# Handling Duplicates\n# Detect and display duplicate rows\nduplicate_rows = df[df.duplicated()]\nprint(\"Duplicate Rows:\")\nprint(duplicate_rows)\n# Remove duplicate rows\ndf.drop_duplicates(inplace=True)\n```", "```py\n# Handling Data Type Conversion\n# Check data types\nprint(df.dtypes)\n# Convert a column to a different data type (e.g., float)\ndf.loc[df['Salary']=='Missing', 'Salary'] = np.NaN\ndf.loc[:, 'Salary'] = df['Salary'].str.replace(\"$\", \"\")\ndf.loc[:, 'Salary'] = df['Salary'].str.replace(\",\", \"\")\ndf['Salary'] = df['Salary'].astype(float)\nprint(df)\n# Now that Salary is a numeric column, we can fill the missing values with mean\ndf['Salary'].fillna(df['Salary'].mean(), inplace=True)\n```", "```py\nimport pandas as pd\nimport random\n# Create a sample DataFrame\ndata = {\n    'Age': [random.randint(18, 60) for _ in range(100)],\n    'Gender': ['Male', 'Female'] * 50,\n    'Income': [random.randint(20000, 100000) for _ in range(100)],\n    'Region': ['North', 'South', 'East', 'West'] * 25\n}\ndf = pd.DataFrame(data)\n# Calculate summary statistics for numerical features\nnumerical_summary = df.describe()\n# Calculate frequency counts and percentages for categorical features\ncategorical_summary = df['Gender'].value_counts(normalize=True)\nprint(\"Summary Statistics for Numerical Features:\")\nprint(numerical_summary)\nprint(\"\\nFrequency Counts and Percentages for Categorical Features (Gender):\")\nprint(categorical_summary)\n```", "```py\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nimport statsmodels.api as sm\n# Generate sample data from a lognormal distribution\nnp.random.seed(0)\ndata = np.random.lognormal(mean=0, sigma=1, size=1000)\n# Create a Pandas DataFrame\ndf = pd.DataFrame({'Data': data})\n# Next, we can plot a histogram of the data to get visual insights into the distribution:\n# Plot a histogram of the data\nplt.hist(data, bins=30, color='skyblue', edgecolor='black')\nplt.title('Histogram of Data')\nplt.xlabel('Value')\nplt.ylabel('Frequency')\nplt.show()\n```", "```py\n# Perform the Shapiro-Wilk test for normality\nshapiro_stat, shapiro_p = stats.shapiro(data)\nis_normal = shapiro_p > 0.05  # Check if data is normally distributed\nprint(f'Shapiro-Wilk p-value: {shapiro_p}')\nprint(f'Is data normally distributed? {is_normal}')\n# Create Q-Q plot with a Normal distribution\nsm.qqplot(data, line='s', color='skyblue')\nplt.title('Q-Q Plot (Normal)')\nplt.xlabel('Theoretical Quantiles')\nplt.ylabel('Sample Quantiles')\nplt.show()\n# Create Q-Q plot with a lognormal distribution\nlog_data = np.log(data)\nsm.qqplot(log_data, line='s', color='skyblue')\nplt.title('Q-Q Plot (Lognormal)')\nplt.xlabel('Theoretical Quantiles')\nplt.ylabel('Sample Quantiles')\nplt.show()\n```", "```py\n# Calculate skewness and kurtosis\nskewness = stats.skew(data)\nkurtosis = stats.kurtosis(data)\nprint(f'Skewness: {skewness}')\nprint(f'Kurtosis: {kurtosis}')\n```", "```py\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport ppscore as pps\n# Generate test data with three variables\nnp.random.seed(0)\ndata = {\n    'Feature1': np.random.randn(100),\n    'Feature2': np.random.randn(100) * 2,\n}\n# Create a linear Target variable based on Feature1 and a non-linear function of Feature2\ndata['Target'] = data['Feature1'] * 2 + np.sin(data['Feature2']) + np.random.randn(100) * 0.5\n# Create a DataFrame\ndf = pd.DataFrame(data)\n```", "```py\n# Calculate and plot the correlation heatmap\ncorr_matrix = df.corr()\nplt.figure(figsize=(8, 6))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\",\n    linewidths=.5)\nplt.title('Correlation Heatmap')\nplt.show()\n```", "```py\n# Calculate the Predictive Power Score (PPS)\nplt.figure(figsize=(8, 6))\nmatrix_df = pps.matrix(df)[['x', 'y', 'ppscore']].pivot(columns='x',\n    index='y', values='ppscore')\nsns.heatmap(matrix_df, vmin=0, vmax=1, cmap=\"Blues\", linewidths=0.5,\n    annot=True)\nplt.title(\"Predictive Power Score (PPS) Heatmap\")\nplt.show()\n# Additional insights\ncorrelation_target = df['Feature1'].corr(df['Target'])\npps_target = pps.score(df, 'Feature1', 'Target') ['ppscore']\nprint(f'Correlation between Feature1 and Target: {correlation_target:.2f}')\nprint(f'Predictive Power Score (PPS) between Feature1 and Target: {pps_target:.2f}')\n```"]