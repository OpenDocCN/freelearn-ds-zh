<html><head></head><body>
        <section>

                            <header class="header-title chapter-title">
                    Bringing It All Together
                </header>
            
            <article>
                
<p>Welcome to the capstone chapter, where we'll walk through some examples to show you how the skills learned you've throughout this book can be applied<span><span>. In this chapter, we will learn about open source real-world datasets, some tips on how to report results, and a capstone project that blends, transforms, and visualizes data from multiple sources. </span></span>Data analysis is a craft and a journey rewarded by the fact that you never stop learning new ways to work with data, provide insights, and answer questions. The <strong>data literacy</strong> skills of reading, working with, analyzing, and arguing with data is agnostic to any technology, but in my experience, nothing replaces the experience of using a specific technology head down and hands-on. Our tool of choice in this book was Jupyter Notebook, along with the extendable libraries available when using the Python ecosystem, such as pandas, NumPy, and NTLK. As you continue to practice and apply these skills, you will become a fungible asset who can solve problems using data personally and professionally. </p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Discovering real-world datasets</li>
<li>Reporting results</li>
<li>Capstone project</li>
</ul>
<p>Let's get started!</p>
<h1 id="uuid-c42811fa-fdaa-4380-9bbf-88da31ad5473">Technical requirements</h1>
<p>The GitHub repository for this book can be found at <a href="https://github.com/PacktPublishing/Practical-Data-Analysis-using-Jupyter-Notebook/tree/master/Chapter12">https://github.com/PacktPublishing/Practical-Data-Analysis-using-Jupyter-Notebook/tree/master/Chapter12</a>.</p>
<p>Furthermore, you can download and install the required software for this chapter from: <a href="https://www.anaconda.com/products/individual" target="_blank">https://www.anaconda.com/products/individual</a>.</p>
<h1 id="uuid-cef418b2-e72a-43d5-8604-c5efe5560c9b">Discovering real-world datasets</h1>
<p>Throughout this book, I have emphasized that the power of analytics comes from blending data together from multiple sources. An individual data source alone rarely includes all the fields required to answer key questions. For example, if you have a timestamp field but not a <span>geographic field about a user</span>, you can't answer any questions about the data related to where an event took place. </p>
<p>As a good data analyst, always offer up creative solutions that have filled data gaps or offer a different perspective by including an external data source. Finding new data sources is much easier today than ever before. Let's go over a few examples.</p>
<h2 id="uuid-778db457-93db-409b-a298-58d388be0957">Data.gov</h2>
<p><strong>Data.gov</strong> is managed by the United States General Services Administration, which offers hundreds of thousands of datasets regarding various topics at the State and Federal levels. Most are curated from specific agencies and posted for public use. They are open source with limited restrictions. What I like about using data.gov is its catalog, which allows you to search across all the topics, tags, formats, and organizations. The site was created using open source technologies, including CKAN.org, which stands for Comprehensive Knowledge Archive Network. This is a platform explicitly built as an open data portal for organizations to host datasets and make them transparent. This process democratizes datasets and creates standards for publishers to follow, such as exposing data formats (CSV, API, and XML, for example) of the source and providing details about how often the data is refreshed.</p>
<p>The following is a screenshot from the data.gov website where you can search for open source datasets from the United States government:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/fef37123-cbaa-4aad-aa70-628a21f5ed24.png" style="width:47.50em;height:42.83em;"/></p>
<p>The DATA.GOV (<a href="http://data.gov">data.gov</a>) site is a good starting point but it can be overwhelming to find specific data elements. I find the next example easier and faster to OVind and use datasets.</p>
<h2 id="uuid-4d2b9872-3e12-45d7-9596-c82d886e95e5">The Humanitarian Data Exchange</h2>
<p><strong>The Humanitarian Data Exchange</strong> (<strong>HDX</strong>) has become topical due to the COVID-19 pandemic but has been sponsoring open source datasets for years. These datasets contain health-specific statistics from around the world with a focus on helping humanity. This is a true example of what is commonly known as <strong>Data for Good</strong> because the site provides transparency on the impact it has on people for free. What I like about this site is how it integrates the Creative Commons License into the data catalog so that you can understand any limitations around reusing or distributing the data from the source. Part of their terms of service is to restrict the use of any <strong>Personally Identifiable Information</strong> (<strong>PII</strong>) so that the data already adheres to regulations that support protecting individuals from being directly identified.</p>
<p><span>The following is a screenshot from The Humanitarian Data Exchange website (<a href="http://data.humdata.org">data.humdata.org</a>), where you can search for humanitarian datasets about locations all around the world:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c2e615e5-4ea4-4874-8ed7-2986ce574145.png" style="width:55.33em;height:34.75em;"/></p>
<p>If you are searching for financial data elements categorized by global statistical measures, You can begin your search at <strong>The World Bank</strong> data portal.</p>
<h2 id="uuid-29aff4d6-cac1-48fc-bc06-8d1e01552f23">The World Bank</h2>
<p><strong>The World Bank</strong> has an open data repository that includes thousands of datasets categorized and conformed by country with metrics classified as indicators. The site allows you to compare your data to global baseline metrics such as <strong>Gross Domestic Product</strong> (<strong>GDP</strong>), which creates thresholds and performance metrics for your analysis. I find that the website is easy to navigate and is quick to identify datasets that can easily be joined to other datasets because it includes defined data type values such as ISO country codes.</p>
<p><span>The following </span>is a screenshot from the World Bank Open Data<span> website</span> (<a href="http://data.worldbank.org">data.worldbank.org</a>), where you can search for financially focused datasets as they impact countries around the world:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/7edc9087-995c-4cc9-adf1-b4eb4978ce27.png" style="width:48.67em;height:34.58em;"/></p>
<p>The World Bank data portal has many rich examples that include data visualizations for quick analysis and preview before you start using them. The next site we will look at, <strong>Our World in Data</strong>, has similar usability features.</p>
<h2 id="uuid-328f0a35-6008-4271-8125-daec93d17436">Our World in Data</h2>
<p>The <strong>Our World in Data</strong> site started with research data from Oxford University but has evolved into an online publication based on scientific studies focused on helping the world solve problems using data. I enjoy the site because you can uncover historical trends that provide context regarding how humanity is improving in many cases, but not at the same pace when you look at different countries. I also find their data visualizations easy to use and navigate; for example, you can filter and compare results between different countries or regions. Their data and site have become invaluable during the COVID-19 pandemic as you can track cases and compare progress between different countries and within the United States.</p>
<p><span>The following is a screenshot from the Our World in Data website (<a href="https://ourworldindata.org/">ourworldindata.org</a>), where you can explore thousands of charts and open source data focused on helping solve global problems:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b09a8c73-26d4-426d-b9ae-7e5eae3579fa.png" style="width:40.08em;height:28.58em;"/></p>
<p>With only the few examples I showcased here, you can see that you have access to thousands of datasets available to use for research, blending, or learning. Be mindful of the open source licenses that may restrict distribution or limit how often you can extract the data. This becomes important when building any automated data pipelines or using APIs to pull data on demand. You will probably have to find alternative paid data companies in those situations. In either case, even when the source data is conformed and structured, it may not answer all the questions required for your analysis. Another point to note is that the aggregation level of the data <span>might not be very high. For example, if data is aggregated by country, you can't join the data by city. </span>In those situations, being transparent in terms of how you are using the data from external sources is important, along with quoting the source and providing a disclaimer stating that external data is being used.</p>
<p>Next, we will cover some best practices that can be used to report the results from your analysis.</p>
<h1 id="uuid-db295dab-97f1-4cf6-9551-39ed78befe92">Reporting results</h1>
<p>How to present your analysis results will vary by the audience, the time available, and the level of detail required to tell a story about the data. Your data may have an inherent bias, be incomplete, or require more attributes in order to create a complete picture, so don't be afraid to include this information in your analysis. For example, if you have done some research on climate change, which is a very broad topic, presenting the consumers of your analysis with a narrow scope of assumptions specific to your dataset is important. How and where you include this information is not as important as ensuring it is available for peer review.</p>
<h2 id="uuid-1c53987c-e57b-4f39-b31b-13d60c8d46ac">Storytelling</h2>
<p>Storytelling with data requires some practice and you need time to sell your message to the audience. Like any good story, presenting the data results in a cadence with a beginning, middle, and end will help with the flow of the analysis being consumed. I also find using analogies to compare your findings will offer some connectivity between the data and the intended consumers. Knowing who you are presenting the results of your analysis to is just as important as understanding the data itself. </p>
<p>For example, in poker, knowing the probability of your hand and your bankroll are not the only factors for success. Who you are playing against is a contributing factor to how much you will win or lose. So, <span>understanding the reactions of the players at the table will help you make decisions to fold, call, or raise during the game. </span></p>
<p>So, when presenting results, think like a poker player and be mindful of who you are presenting your data to in order to convince the audience about your conclusions. For example, if you are presenting to senior management, time is limited, so being direct, brief, and skipping to the summary results is suggested. If the audience is your peers, then walking through the journey of how you came to your conclusions will resonate because it builds credibility and trust.</p>
<p>Regardless of the audience, if you present a chart with trends becoming higher over time, be prepared to offer proof of the underlining source, along with how the metric was calculated. Without that information, doubt about the ability to recreate your findings will lead to your analysis being dismissed.</p>
<p>A good data analyst will be able to <strong>read the room</strong> and understand how much detail is required when presenting results. There have been plenty of times when I have presented findings where I had to cut my message short because when I looked up to see how engaged the people were, I realized they were lost. Rather than continuing, I stopped and offered up questions so I could provide more clarity. Just changing the format to offer time for questions in the middle of the presentation helped both the audience and myself refocus our attention on the conclusions.</p>
<p>So, be authentic and provide transparency in your analysis. If you make a mistake or misinterpret the data, the audience will be forgiving as long as you continue to improve and avoid repeating a missed step. I find having peers, mentors, and managers who can provide honest and constructive feedback before you present yourself, helps improve your messaging and presentation of the data artifacts.</p>
<p>From a <strong>data literacy</strong> perspective, focus less on the technology used and more on the insights gained in your analysis. Realize that the people interpreting your results will come from a diverse set of perspectives. A CEO can understand a chart of the balance sheet of company financial data but probably does not care which NumPy library was used for your analysis. In our final exercise in this book, we will create a capstone project with a focus on answering a real-world question using data from multiple sources.</p>
<h1 id="uuid-a5ee000a-4b56-408a-9a21-d1c12cfa1a16">The Capstone project</h1>
<p>For our real-world dataset example, we are going to use two different sources and blend them together using the techniques we've learned throughout this book. Since <strong>Know Your Data</strong> (<strong>KYD</strong>) still applies, let's walk through the sources.</p>
<h2 id="uuid-11455f72-6a8e-4c65-a0e0-ccc34555ad68">KYD sources</h2>
<p>The first source is from the World Bank and is a list of green bonds, which are used to fund the reduction of carbon emissions and climate-related projects. It was downloaded from the website, so it's a snapshot based on a point in time stored as a CSV file with 115 rows and 10 columns, including a header. </p>
<p>A visual preview of the data in Microsoft Excel can be seen in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/44f8155a-c7d3-4463-90d7-7705eeba8666.png" style="width:57.50em;height:19.42em;"/></p>
<p>The source data has some insights that we can mine through <em>as is</em>, such as the following:</p>
<ul>
<li>How many <strong>bonds</strong> are issued by <strong>Currency?</strong></li>
<li>What is the total distribution of the bonds by <strong>Currency</strong>?</li>
<li>Which <strong>bonds</strong> are maturing in the next 3, 5, 7, or 10 years?</li>
</ul>
<p>However, we have a data gap for the questions related to the local currency for the country of issuance. Since currency exchange rates fluctuate daily, there are more questions we could answer if we had that information available to join by the <span class="packt_screen">Currency</span> field. So, our second source of data that we want to work with is from the <strong>Humanitarian Data Exchange</strong> (<strong>HDX</strong>) site. This includes the <strong>Foreign Exchange</strong> (<strong>FX</strong>) rate by country designated by the currency as it relates to the <strong>United States Dollar</strong> (<strong>USD</strong>) by date from <kbd>1/4/1999</kbd> to <kbd>5/7/2020</kbd> in the date format of <span class="packt_screen">M/D/YYYY</span>. This is another CSV file that can be downloaded with 5,465 rows and 34 columns on a specific date. There is a header row, and the first record of data includes metadata tags prefixed with a hash sign, <span class="packt_screen">#,</span> which is used by the HDX site for metadata management and cataloging. </p>
<p>A visual preview of the data in Microsoft Excel can be seen in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/5fa20245-f3b0-49a3-b7d2-ed686d00b10e.png" style="width:44.83em;height:22.75em;"/></p>
<div class="packt_tip">Previewing data in Microsoft Excel or any spreadsheet tool is a best practice if you wish to visually understand the structure of the data before working with it in Jupyter Notebook. If the data volumes are large, break off a sample so it can be loaded on your workstation.</div>
<p>So, our bonds data with currencies lists the values by rows and our FX rate data is listing the values for each currency by column with a value assigned for each specific date. There are a few ways to solve this but for our example, we are interested in blending the latest FX rate by currency to our bond data so that we can convert the <strong>USD Equivalent</strong> value into the <strong>Local CCY Equivalent</strong>. That way, we can perform analysis of the data in either USD or the respective country's currency and report the findings.</p>
<h2 id="uuid-0a7a5cd7-2ad6-4e6b-b0b1-4befc7d78a39">Exercise</h2>
<p>Let's open a new Jupyter Notebook session and get started:</p>
<ol>
<li>We are going to import the libraries required to work with and analyze the results by including the following commands:</li>
</ol>
<pre style="padding-left: 60px">In[]:import pandas as pd<br/>import numpy as np<br/>%matplotlib inline</pre>
<ol start="2">
<li>We will read in the first <kbd>.csv</kbd> file using the <kbd>pandas</kbd> library and assign the result to a variable named <kbd>df_greenbonds</kbd>:</li>
</ol>
<pre style="padding-left: 60px">In[]:df_greenbonds = pd.read_csv('Green_Bonds_since_2008.csv')</pre>
<div class="packt_tip">Be sure to upload the source CSV file in the correct file location so that you can reference it in your Jupyter Notebook.</div>
<ol start="3">
<li>To validate all the records have loaded successfully, we need to run a <kbd>shape()</kbd> function against the DataFrame:</li>
</ol>
<pre style="padding-left: 60px">In[]:df_greenbonds.shape</pre>
<p style="padding-left: 60px">The output will look as follows, where the values <kbd>115</kbd> and <kbd>10</kbd> will be displayed. These match the number of rows and columns in the source CSV file:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/bada3bff-2665-4c0b-907d-548e9c7b298f.png" style="width:14.75em;height:3.50em;"/></p>
<ol start="4">
<li>To preview the DataFrame, we can run the following <kbd>head()</kbd> function:</li>
</ol>
<pre style="padding-left: 60px">In[]:df_greenbonds.head()</pre>
<p style="padding-left: 60px">The output will look as follows, where the DataFrame results will be displayed in the Notebook:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/7f52c4c7-6f01-4706-b86f-ef1e63ddb8ed.png" style="width:53.25em;height:22.33em;"/></p>
<ol start="5">
<li>We will read in the second CSV file using the <kbd>pandas</kbd> library and assign the result to a variable named <kbd>df_fx_rates</kbd>:</li>
</ol>
<pre style="padding-left: 60px">In[]:df_fx_rates = pd.read_csv('ECB_FX_USD-quote.csv')</pre>
<div class="packt_tip">Be sure to upload the source CSV file to the correct file location so that you can reference it in your Jupyter Notebook.</div>
<ol start="6">
<li>To validate all the records have loaded successfully, run a <kbd>shape()</kbd> function against the DataFrame:</li>
</ol>
<pre style="padding-left: 60px">In[]:df_fx_rates.shape</pre>
<p style="padding-left: 60px">The output will look as follows,<span> </span>where the values <kbd>5464</kbd> and <kbd>34</kbd> will be displayed in parentheses. These match the number of rows and columns in the source CSV file:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/eb736af9-b7b4-4988-98bc-b649b6a4d695.png" style="width:14.17em;height:3.92em;"/></p>
<ol start="7">
<li>To preview the DataFrame, we can run the following <kbd>head()</kbd> function:</li>
</ol>
<pre style="padding-left: 60px">In[]:df_fx_rates.head()</pre>
<p style="padding-left: 60px">The output will look as follows,<span> </span>where the DataFrame results will be displayed in the Notebook:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0157ac3b-a660-434d-a8ee-3a5f90472b42.png" style="width:52.92em;height:17.67em;"/></p>
<ol start="8">
<li>Since we know from the prior chapters that data is inherently messy and requires some cleanup, we will delete the first row because it contains the HDX hashtag metadata values, which are not required for our analysis:</li>
</ol>
<pre style="padding-left: 60px">In[]:df_fx_rates = df_fx_rates.drop(0)<br/>df_fx_rates.head()</pre>
<div class="packt_tip">It is recommended that you clean up as you go through the analysis step by step in case you need to troubleshoot and recreate any prior steps in the data analysis workflow.</div>
<p style="padding-left: 60px">The output will look as follows,<span> </span>where the DataFrame results will be displayed in the Notebook:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/207cab68-a260-4634-9d3b-17becd4eb3e3.png" style="width:44.75em;height:16.67em;"/></p>
<ol start="9">
<li>For our analysis, we want to focus on the latest FX rate available in the file. You could take the first row available in the DataFrame, but a more robust method would be to use the <kbd>max()</kbd> function so that how the data is sorted becomes irrelevant. To verify that the correct value will work before we filter the DataFrame, use the following command:</li>
</ol>
<pre style="padding-left: 60px">In[]:df_fx_rates['Date'].max()</pre>
<p style="padding-left: 60px">The output will look as follows,<span> </span>where the results from the command will be displayed in the Notebook. In this dataset, the max date at the time of download is <kbd>2020-05-07</kbd>, with the date format as <kbd>YYYY-MM-DD</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/ad385d86-1983-4768-ba44-59ed153e846d.png" style="width:14.33em;height:3.58em;"/></p>
<ol start="10">
<li>From the prior step, we are confident that our filter will use the correct <kbd>Date</kbd> value, so we will create a new DataFrame with only one specific date value so that we can join the results in later steps. The new DataFrame is named <kbd>df_fx_rates_max_date</kbd> and is a result of filtering the original DataFrame, named <kbd>df_fx_rates</kbd>, by the <kbd>Date</kbd> field, where only the calculated max <kbd>Date</kbd> value will be returned. We will add the following <kbd>head()</kbd> function to validate the results in the Notebook:</li>
</ol>
<pre style="padding-left: 60px">In[]:df_fx_rates_max_date = df_fx_rates[df_fx_rates.Date==df_fx_rates['Date'].max()]<br/>df_fx_rates_max_date.head()</pre>
<p style="padding-left: 60px">The output will look as follows, where the results from the command will be displayed in the Notebook. The new DataFrame, named <kbd>df_fx_rates_max_date</kbd>, will only have one record with a header containing 34 columns. Each column will represent the latest available currency value using the three-letter country's designation, such as <kbd>EUR</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/2b1883d9-90d4-407a-b20d-7fb90fcc302c.png" style="width:55.75em;height:10.75em;"/></p>
<ol start="11">
<li>We still have more work to do in order to join this data to our original bond DataFrame. We need to transform it using the <kbd>transpose()</kbd> function, which will change all the columns into rows. In other technologies, this concept is called a pivot, crosstab, or crosstable; this was covered in more detail in <a href="c6eceec8-e006-404b-9be1-bc96de29991a.xhtml">Chapter 4</a>,<q> </q><em>Creating Your First pandas DataFrame</em>. The results are stored in a new DataFrame called <kbd>df_rates_transposed</kbd>. We rename the columns to make it easier to work with them. We also need to run the following <kbd>head()</kbd> command to preview the results:</li>
</ol>
<pre style="padding-left: 60px">In[]:df_rates_transposed = df_fx_rates_max_date.transpose()<br/>df_rates_transposed.columns.name = 'Currency'<br/>df_rates_transposed.columns = ['Currency_Value']<br/>df_rates_transposed.head(10)</pre>
<p style="padding-left: 60px">The output will look as follows,<span> </span>where the new DataFrame, named <kbd>df_rates_transposed</kbd>, is displayed. Here, all the columns have been converted into rows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/4767865b-6765-4e5d-9f19-c50afe379b74.png" style="width:28.58em;height:23.67em;"/></p>
<ol start="12">
<li>The goal is for our reference table to have all the FX rates by currency values in the same format. However, notice that, on the first row in <span>the following diagram, </span>the <kbd>Date</kbd> value is mixed with <kbd>Currency_values</kbd>, which need to have the same data type. The need to have conformed and consistent data values represented in structured data has been reinforced throughout this book, so we will clean up the DataFrame by dropping the <kbd>Date</kbd> record. We will also use the <kbd>reindex()</kbd> function to make it easier to join in the next step and then run the following <kbd>head()</kbd> command to verify the results:</li>
</ol>
<pre style="padding-left: 60px">In[]:df_rates_transposed = df_rates_transposed.drop('Date')<br/>df_rates_transposed = df_rates_transposed.reindex()<br/>df_rates_transposed.head()</pre>
<p style="padding-left: 60px">The output will look as follows, where the new DataFrame, named <kbd>df_rates_transposed</kbd>, is displayed as before, except now, the <kbd>Date</kbd><span class="packt_screen"> </span>record has been deleted. This means the first row will be <kbd>EUR</kbd> with a <kbd>Currency_Value</kbd> of <kbd>1.0783</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/909df028-ecd0-4dae-8ed1-a2fa8a8e42dc.png" style="width:31.50em;height:16.08em;"/></p>
<ol start="13">
<li>We are now ready to join the transformed and cleaned FX rates to our original bonds source using the common <kbd>Currency</kbd> join key field. Because we want all the records from the <kbd>df_greenbonds</kbd> source and only the matching values from <kbd>df_rates_transposed</kbd>, we will use a left join. To display and verify the results, we use the following <kbd>head()</kbd> command:</li>
</ol>
<pre style="padding-left: 60px">In[]:df_greenbonds_revised = df_greenbonds.merge(df_rates_transposed, how='left', left_on='Currency', right_index=True)<br/>df_greenbonds_revised.head()</pre>
<p style="padding-left: 60px">The output will look as follows,<span> </span>where the results of the left join are stored in the <kbd>df_greenbonds_revised</kbd> DataFrame. The following screenshot shows a table with 5 rows and 11 columns. It includes a header row and index values that are not labeled:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d3fb925a-3455-4a43-a0a6-f3b4a28f4e38.png" style="width:59.75em;height:26.50em;"/></p>
<div class="packt_tip"><span>Like in the preceding diagram, be sure to scroll to the right to see that a new column called </span><kbd>Currency_Value</kbd><span> is included.</span></div>
<ol start="14">
<li>An advantage of constantly running the <kbd>head()</kbd> command to validate results in each step is that you can make observations about the data as you prepare and clean it for further analysis. In <span>the preceding screenshot </span>, we can see a <kbd>null()</kbd> value in <kbd>Currency_Value</kbd>, which is displayed as <kbd>NaN</kbd>. We covered working with NaN values in <a href="bea8a62c-4469-47e3-a668-10fbb91815ea.xhtml">Chapter 5</a>,<q> </q><em>Gathering and Loading Data in Python</em>. This is a result of the left join and is expected because there was no value of <kbd>USD</kbd> in the FX rates source data. This makes sense because you don't need to convert the currency of USD. However, this will have an impact when we attempt to create calculations from the values in this column. The solution, in this case, is to just convert all the <kbd>NaN</kbd> values to <kbd>1</kbd> because the FX rate conversion for USD is 1. There will be no output after running this command:</li>
</ol>
<pre style="padding-left: 60px">In[]:df_greenbonds_revised["Currency_Value"].fillna(1, inplace=True)</pre>
<ol start="15">
<li><span>Since our</span> CSV file sources do not include a data dictionary, the defined data type for each field is unknown. We can solve any inconsistencies within the data values by applying the <kbd>astype()</kbd> function. We will focus on the two columns we used for calculating the local currency rate by converting them into a <kbd>dtype</kbd> of the <kbd>float</kbd> type. There will be no output after running this command:</li>
</ol>
<pre style="padding-left: 60px">In[]:df_greenbonds_revised["Currency_Value"] = df_greenbonds_revised.Currency_Value.astype(float)<br/>df_greenbonds_revised["USD Equivalent"] = df_greenbonds_revised["USD Equivalent"].astype(float)</pre>
<ol start="16">
<li>We are now ready to create a new calculated column in our existing DataFrame that will divide the <kbd>USD Equivalent</kbd> column by <kbd>Currency_Value</kbd>. The result will be stored in a new column named <kbd>Local CCY</kbd> in the same DataFrame. There will be no output after running this command:</li>
</ol>
<pre style="padding-left: 60px">In[]:df_greenbonds_revised['Local CCY'] = df_greenbonds_revised['USD Equivalent']/df_greenbonds_revised['Currency_Value']</pre>
<ol start="17">
<li>Now, we can convert the data types of the specific columns back into integers and focus our attention on the key columns by explicitly identifying them. We can do this by using the following commands:</li>
</ol>
<pre style="padding-left: 60px">In[]:df_greenbonds_revised['Local CCY'] = df_greenbonds_revised['Local CCY'].astype(int)<br/>df_greenbonds_revised['USD Equivalent'] = df_greenbonds_revised['USD Equivalent'].astype(int)<br/>df_greenbonds_revised[['ISIN', 'Currency', 'USD Equivalent', 'Currency_Value', 'Local CCY']]</pre>
<p style="padding-left: 60px">The output will look as follows,<span> </span>where the results from the preceding commands will be displayed in the Notebook. The original column, <kbd>USD Equivalent</kbd>, is displayed as an integer and we now have the <kbd>Local CCY</kbd> column available to the right of <kbd>Currency_Value</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/61e98320-646e-48fa-9bff-8a19b9877049.png" style="width:44.58em;height:23.33em;"/></p>
<ol start="18">
<li>To analyze the results, let's group the data by <kbd>Currency</kbd> and only sum the values for both the <kbd>USD Equivalent</kbd> and <kbd>Local CCY</kbd> fields using the following commands:</li>
</ol>
<pre style="padding-left: 60px">In[]:df_greenbonds_revised[['Currency', 'USD Equivalent', 'Local CCY']].groupby(['Currency']).sum().astype(int)</pre>
<p style="padding-left: 60px">The output will look as follows, where the data is now aggregated by <kbd>Currency</kbd>, with the total sum of <kbd>USD Equivalent</kbd> and <kbd>Local CCY</kbd> also being displayed:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/547ec7ea-af5e-466d-af9e-725a732d4f33.png" style="width:52.92em;height:14.08em;"/></p>
<ol start="19">
<li>Another type of analysis would be to see how the data is distributed by <kbd>Currency</kbd> visually by creating a horizontal bar chart using the <span><kbd>matplotlib</kbd> </span>library's <kbd>plot()</kbd> function:</li>
</ol>
<pre style="padding-left: 60px">In[]:df_greenbonds_revised[['Currency', 'USD Equivalent']].groupby(['Currency']).size().plot(kind='barh');</pre>
<p style="padding-left: 60px">The output will look as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6b65612c-0f3b-435a-9b33-2c82427cfd0e.png" style="width:53.92em;height:18.17em;"/></p>
<p>As you can see, an overwhelming number of green bonds have been issued in USD because it has the largest bar by a large margin compared to the other currencies. The reason why this is the case is not evident by looking exclusively at this data, so additional analysis will need to be done. When you present your findings to others within the organization, this is often the reality, where blending data together offers more insights but then leads to more unanswered questions. This, in turn, leads to the need for more data to be added to your existing sources. Finding a balance between when to stop blending more and more data together is challenging, so adding incremental milestones to present your findings is encouraged.</p>
<h1 id="uuid-39c768ab-00f3-4beb-bb9f-3194ea4efbd5">Summary</h1>
<p>With that, we have walked through many of the different concepts we covered throughout this book in one comprehensive exercise. In this chapter, we learned more about real-world data sources that can be used for analysis. We also created a repeatable workflow that can be summarized as a workflow that collects external data sources, joins them together, and then analyzes the results.<span><span> </span></span>Since we know the reality of working with data is never that straightforward, we walked through some inherent challenges of working with it. We have broken down the steps of collecting multiple sources, transforming them, and cleansing, joining, grouping, and visualizing the results. The more you work hands-on with data, the easier it is to apply these concepts to any dataset with the foundation remaining constant. As you increase your data literacy skills when it comes to working with data, you will notice the syntax and tools will change but that the challenges and opportunities to solve problems remain the same. I encourage you to continue investing in yourself by continuously learning more about data. I hope you find it as fulfilling a journey as I do!</p>
<h1 id="uuid-101833b1-4010-4b0c-b389-944bf012bd30">Further reading</h1>
<ul>
<li>The Humanitarian Data Exchange site: <a href="https://data.humdata.org/" target="_blank">https://data.humdata.org/</a></li>
<li>Data.gov <span>– </span>the US government's open data: <a href="https://www.data.gov/" target="_blank">https://www.data.gov/</a></li>
<li>The Creative Commons site: <a href="https://creativecommons.org/">https://creativecommons.org/</a></li>
<li>The Open Data Commons site: <a href="https://opendatacommons.org/" target="_blank">https://opendatacommons.org/</a></li>
<li>The Our World in Data site: <a href="https://ourworldindata.org/" target="_blank">https://ourworldindata.org/</a></li>
<li>The World Bank Open Data site: <a href="https://data.worldbank.org/" target="_blank">https://data.worldbank.org/</a></li>
</ul>


            </article>

            
        </section>
    </body></html>