- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Establishing a Foundation with Snowpark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, you learned the basics of Snowpark, its benefits, and
    how it allows developers to develop complex data applications using Python. This
    chapter will focus on establishing a solid foundation with Snowpark. Here, you
    will learn how to configure and operate Snowpark, select a coding style and structure,
    and explore Snowpark’s fundamentals in depth. This will help you acquire practical
    knowledge and skills to work efficiently with Snowpark, including setting up the
    environment, structuring the code, and utilizing it for different workloads.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the Snowpark development environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Operating with Snowpark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Establishing a project structure for Snowpark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this chapter, you’ll require an active Snowflake account and Python installed
    with Anaconda configured locally. You can refer to the following documentation
    for installation instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: You can sign up for a Snowflake Trial account at [https://signup.snowflake.com/](https://signup.snowflake.com/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To configure Anaconda, follow the guide at [https://conda.io/projects/conda/en/latest/user-guide/getting-started.html](https://code.visualstudio.com/docs/python/python-tutorial).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, to install and set up Python for VS Code, follow the guide at [https://code.visualstudio.com/docs/python/python-tutorial](https://code.visualstudio.com/docs/python/python-tutorial)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To learn how to operate a Jupyter Notebook in VS Code, go to [https://code.visualstudio.com/docs/datascience/jupyter-notebooks](https://code.visualstudio.com/docs/datascience/jupyter-notebooks)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The supporting materials for this chapter are available in this book’s GitHub
    repository at [https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark](https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark).
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the Snowpark development environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first step in developing Snowpark is to set up the Snowpark development
    environment. Developers have much flexibility regarding which **integrated development
    environments** (**IDEs**) they can use to get started with Snowpark; the only
    thing they need to do is install the Snowpark client **application programmable
    interface** (**API**) and connect to their Snowflake account. The development
    environment can be a local Python environment that contains your favorite IDE
    or the new Snowflake Python worksheets in Snowsight. This section will cover setting
    up a Snowpark Python worksheet and the local development environment.
  prefs: []
  type: TYPE_NORMAL
- en: Snowpark Python worksheet
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Snowflake released *Snowflake Worksheets for Python*, a new type of worksheet
    in Snowsight for developing a Python-based Snowpark environment from within Snowflake.
    This game-changing feature allows developers to easily leverage the power of Snowpark
    Python within Snowsight to perform data processing and create data pipelines,
    **machine learning** (**ML**) models, and applications by integrating Snowpark
    Python directly into the browser without setting up Python environments or installing
    open source libraries on the client. Instead, developers can easily use pre-existing
    packages from Anaconda or import their own Python files from stages into the Worksheet.
    In addition, they can quickly deploy the Python worksheets as a stored procedure.
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisites for using Python worksheets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To enable and use Snowflake Python worksheets, you must first acknowledge the
    Anaconda terms of service in Snowsight. The following steps are to be performed
    by the organization’s administrator:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you must sign into your Snowflake account. In the **Switch Role** section
    on the left panel, switch to the **ORGADMIN** role in the user context:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.1 – The ORGADMIN role in Snowflake](img/B19923_02_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 – The ORGADMIN role in Snowflake
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, go to **Admin** | **Billing &** **Terms**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.2 – Admin | Billing & Terms](img/B19923_02_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 – Admin | Billing & Terms
  prefs: []
  type: TYPE_NORMAL
- en: 'Click **Enable** next to **Anaconda** **Python packages**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.3 – Enabling Anaconda Python packages](img/B19923_02_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 – Enabling Anaconda Python packages
  prefs: []
  type: TYPE_NORMAL
- en: 'You’ll get a popup, as shown in the following screenshot. Click **Acknowledge
    & Continue** to enable the packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.4 – Anaconda’s Terms and Services](img/B19923_02_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.4 – Anaconda’s Terms and Services
  prefs: []
  type: TYPE_NORMAL
- en: You’ll need to enable the Anaconda packages every time you create a new Snowflake
    environment. Now that we’ve enabled the packages, let’s see how we can work with
    Python worksheets.
  prefs: []
  type: TYPE_NORMAL
- en: Database and schema creation in Snowflake Snowsight
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To create a database called `SNOWPARK_DEFINITIVE_GUIDE` and a schema called
    `MY_SCHEMA` using Snowflake’s Snowsight UI interface, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to the Snowsight web interface, log in using your Snowflake credentials,
    and navigate to the **Data** | **Databases** section. This is typically located
    in the left-hand navigation menu:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.5 – The Databases section](img/B19923_02_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.5 – The Databases section
  prefs: []
  type: TYPE_NORMAL
- en: 'Look for a button or link that says **+ Database** in the top-right corner
    and click it. In the dialogue box that appears, enter **SNOWPARK_DEFINITIVE_GUIDE**
    as the name for the new database. Optionally, you can specify other settings,
    such as **Comment**, if needed, and click **Create**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.6 – The New Database dialogue](img/B19923_02_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.6 – The New Database dialogue
  prefs: []
  type: TYPE_NORMAL
- en: 'After creating the database, click on the **SNOWPARK_DEFINITIVE_GUIDE** database.
    This will take you to the following page:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.7 – The SNOWPARK_DEFINTIVE_GUIDE page](img/B19923_02_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.7 – The SNOWPARK_DEFINTIVE_GUIDE page
  prefs: []
  type: TYPE_NORMAL
- en: 'On this page, look for a button or link that says **+ Schema** and click on
    it. In the dialogue box that appears, enter **MY_SCHEMA** as the name for the
    new schema. Optionally, you can specify other settings, such as **Comment** and
    **Managed access**. Click **Create** to create the schema:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.8 – The New Schema dialogue](img/B19923_02_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.8 – The New Schema dialogue
  prefs: []
  type: TYPE_NORMAL
- en: We’ll consistently utilize the same database and schema throughout this book
    unless explicitly instructed otherwise in specific chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Working with Python worksheets in Snowflake
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Python worksheets come with a sample code template that can be used as a starter.
    The whole experience is embedded in Snowflake based on the Snowsight UI. Perform
    the following steps to create and work with Python worksheets in Snowflake:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to the **Worksheets** section from the menu in the Snowsight UI:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.9 – The Worksheets menu option](img/B19923_02_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.9 – The Worksheets menu option
  prefs: []
  type: TYPE_NORMAL
- en: 'On the **Worksheets** pane, click the **+** icon on the right and select **Python
    Worksheet** to create a Python worksheet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.10 – Creating a Python worksheet](img/B19923_02_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.10 – Creating a Python worksheet
  prefs: []
  type: TYPE_NORMAL
- en: 'Select a database and schema so that you can work in the worksheet context.
    The scope of the Python code will operate based on these details:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.11 – Worksheet context](img/B19923_02_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.11 – Worksheet context
  prefs: []
  type: TYPE_NORMAL
- en: 'The worksheet has a handler function that’s invoked when the worksheet is executed.
    The **Settings** menu allows you to configure your worksheets. The default handler
    function is **main()**, and the return type can be specified as **Table()**, **Variant**,
    or **String**. The result will be shown in the format you choose:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.12 – Worksheet settings](img/B19923_02_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.12 – Worksheet settings
  prefs: []
  type: TYPE_NORMAL
- en: Worksheet preferences such as linting and line wrapping can also be customized
    here.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Python code can be executed by clicking on the **RUN** button at the top
    right of the worksheet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.13 – Executing the worksheet](img/B19923_02_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.13 – Executing the worksheet
  prefs: []
  type: TYPE_NORMAL
- en: You can start developing and executing the Python code within the worksheet
    to see the results.
  prefs: []
  type: TYPE_NORMAL
- en: Managing Anaconda packages in Python worksheets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Python worksheets come with an integrated Anaconda environment that supports
    importing the most common Anaconda libraries without having to worry about managing
    dependencies. It also supports importing custom packages from the internal stage.
    In this section, we will look at managing Anaconda packages in Python. To do this,
    perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to the **Packages** tab from the menu and select **Anaconda Packages**.
    This will show a list of pre-installed packages and their versions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.14 – Anaconda Packages](img/B19923_02_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.14 – Anaconda Packages
  prefs: []
  type: TYPE_NORMAL
- en: 'You can search for and install the required packages by using the search bar:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.15 – Searching for packages](img/B19923_02_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.15 – Searching for packages
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also modify the version of the packages by selecting the available
    versions from the respective dropdown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.16 – Package versioning](img/B19923_02_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.16 – Package versioning
  prefs: []
  type: TYPE_NORMAL
- en: 'You can execute the following query to check the available Python packages
    in Snowflake using SQL:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This query obtains the results from the packages view in the information schema:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.17 – Anaconda packages query](img/B19923_02_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.17 – Anaconda packages query
  prefs: []
  type: TYPE_NORMAL
- en: 'If you need to check the version information of specific packages, you can
    filter the query by the package’s name:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.18 – Anaconda Python package query](img/B19923_02_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.18 – Anaconda Python package query
  prefs: []
  type: TYPE_NORMAL
- en: This query shows the list of Anaconda packages that are available in my account.
    In the next section, we’ll learn how to manage custom Python packages.
  prefs: []
  type: TYPE_NORMAL
- en: Managing custom packages in Python worksheets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Python worksheets also support the ability to import custom Python packages
    that can be used in Python code. However, the package must be uploaded into the
    internal stage, which is the storage part of the Snowflake account, and imported
    from it. In Snowsight, you can load files into a named internal stage area, allowing
    you to conveniently view and utilize them within your Python worksheets or load
    the data into a table using SQL. However, it’s important to note that Snowsight
    does not support loading files into user or table stages.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a named internal stage, ensure that the role you are using has the
    **USAGE** privilege on the relevant database and schema and the **CREATE STAGE**
    privilege on the schema. Let’s begin:'
  prefs: []
  type: TYPE_NORMAL
- en: Sign in to Snowsight.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Access the **Data** section and navigate to **Databases**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose the desired database and schema where you want to create the stage and
    load files.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click **Create**, select **Stage**, and click **Snowflake Managed**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.19 – Creating a stage](img/B19923_02_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.19 – Creating a stage
  prefs: []
  type: TYPE_NORMAL
- en: 'Provide a name for the stage and opt-in to enable a directory table for the
    stage, allowing you to visualize the files. Once you’re done, click **Create**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.20 – Creating an internal stage](img/B19923_02_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.20 – Creating an internal stage
  prefs: []
  type: TYPE_NORMAL
- en: 'To load files into a Snowflake-managed named internal stage using Snowsight,
    follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Access the **Data** section and select **Databases**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose the database schema where you created the stage and select the stage
    itself.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **+ Files** to load the desired files into the stage.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the **Upload Your Files** dialogue that appears, select the files you want
    to upload (multiple files can be selected simultaneously):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.21 – Uploading a custom package](img/B19923_02_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.21 – Uploading a custom package
  prefs: []
  type: TYPE_NORMAL
- en: Optionally, specify or create a path within the stage where you want to store
    the files.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **Upload**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the files have been successfully loaded into the stage, you can perform
    various actions, depending on your requirements.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With that, the package has been uploaded into the stage and is ready to be imported.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The maximum permissible file size is 50 MB. You need a role with the **USAGE**
    privilege on the database and schema and the **WRITE** privilege on the stage.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the stage has been created and the package has been uploaded, you can
    import the module so that you can use it in the program. To import a package,
    follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'From the **Packages** menu, select **Stage Packages**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.22 – Stage Packages](img/B19923_02_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.22 – Stage Packages
  prefs: []
  type: TYPE_NORMAL
- en: 'Enter the path to the package. You can refer to the stage in the same database
    and schema using **@Stage/path/to/package.py**. If the stage is in a different
    database and schema, you can use **@Database.Schema.Stage/path/to/package.py**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.23 – Importing a stage package](img/B19923_02_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.23 – Importing a stage package
  prefs: []
  type: TYPE_NORMAL
- en: 'Click **Import** to install the package. The module will now be visible under
    **Installed Packages**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.24 – Installing a stage package](img/B19923_02_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.24 – Installing a stage package
  prefs: []
  type: TYPE_NORMAL
- en: You can import the package into your code by using **import <****package name>**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the next section, we will cover how to deploy a Python stored procedure using
    the UI.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a Python stored procedure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Python script in our worksheet can be seamlessly deployed as a stored procedure.
    This can then be used in a regular SQL context or scheduled to execute as a task.
    To deploy a stored procedure, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Click the **Deploy** button at the top right of the worksheet.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Give the stored procedure a name; an optional comment can also be specified
    that provides information about it. If the stored procedure already exists, then
    check the **Replace if exists** box to replace the existing stored procedure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.25 – Deploying a stored procedure](img/B19923_02_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.25 – Deploying a stored procedure
  prefs: []
  type: TYPE_NORMAL
- en: Click **Deploy** to deploy the stored procedure. It will be deployed under the
    database and schema you provided in the worksheet context.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The stored procedure can now be executed in your worksheets using the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the next section, we will cover the various features of Python worksheets.
  prefs: []
  type: TYPE_NORMAL
- en: Features of Python worksheets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Python worksheets consist of some features that are developer-friendly and
    support productivity. Here are some of the distinct characteristics of Python
    worksheets:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Interactive Python environment**: Worksheets support the Python language
    and support Snowpark **user-defined functions** (**UDFs**) and stored procedures.
    Features such as syntax highlighting, type-sensitive autocomplete for keywords,
    and handy diagnostics such as undeclared variables or invalid method usage help
    increase developers’ productivity:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 2.26 – Interactive Python environment](img/B19923_02_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.26 – Interactive Python environment
  prefs: []
  type: TYPE_NORMAL
- en: '**Python libraries support**: Python worksheets come with an integrated Anaconda
    environment that supports importing the most common Anaconda libraries without
    the need to worry about managing dependencies. It also supports importing custom
    packages from the internal stage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Snowpark debugging**: Python worksheets can display the results from a DataFrame
    inside Snowsight using the **show()** or **print()** function. The preview of
    the DataFrame can also be returned to display the output in tabular format, which
    is very useful when it comes to debugging Python programs:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 2.27 –  Snowpark debugging](img/B19923_02_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.27 – Snowpark debugging
  prefs: []
  type: TYPE_NORMAL
- en: '**Collaboration**: Snowpark Python worksheets can be shared with developers.
    This makes collaboration much easier as multiple developers can access and work
    on the worksheet at the same time:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 2.28 – Worksheet collaboration](img/B19923_02_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.28 – Worksheet collaboration
  prefs: []
  type: TYPE_NORMAL
- en: '**Charts and data exploration**: Python worksheets provide a convenient way
    to visualize data and DataFrames as charts, which helps with data exploration
    and provides an easy way to analyze the data quickly:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 2.29 – Charts and data exploration](img/B19923_02_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.29 – Charts and data exploration
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will cover the limitations of Python worksheets.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations of Python worksheets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Python worksheets are relatively new in Snowsight and have some limitations
    in terms of functionality. Let’s take a closer look:'
  prefs: []
  type: TYPE_NORMAL
- en: Logging levels lower than **WARN** do not appear in the **Results** area by
    default.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python worksheets do not support breakpoints or selective execution of the portions
    of code. Instead, the entire code is run in the worksheet.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Python worksheet cannot display images or other artifacts of the Python
    code it generates. It can only show results that are returned through the DataFrame.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It only supports Python 3.8 and libraries that use Python 3.8.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Snowflake constantly updates features for Python worksheets, improving them
    for developers. As a result, Python worksheets make it easier and faster for developers
    to code in Python from within the Snowflake environment.
  prefs: []
  type: TYPE_NORMAL
- en: Snowpark development in a local environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Snowpark can also be developed conveniently from a local environment with your
    favorite IDE. The key benefit of Snowflake’s partnership with Anaconda is Anaconda’s
    Snowflake Snowpark for Python channel, which contains the necessary Python packages
    to run Snowpark. To use this channel, Anaconda or Miniconda should be installed
    and set up on the machine. In this section, we will walk you through how to set
    up Snowpark in your local development environment using Anaconda.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: This book will utilize Anaconda for Snowpark development as it is the recommended
    approach and utilizes the benefits of Anaconda’s package manager to easily set
    up and manage the Snowpark development environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Snowpark API requires Python 3.8 to be installed. Snowflake recommends
    that you use Anaconda for easy package management. You can check out the Python
    version you have by running the following command in the **command-line** **interface**
    (**CLI**):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Your output should look similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.30 – Python CLI version](img/B19923_02_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.30 – Python CLI version
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also check the Python version from within the Python code by running
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This will print an output similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.31 – Python version](img/B19923_02_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.31 – Python version
  prefs: []
  type: TYPE_NORMAL
- en: Once Python has been installed, the virtual environment needs to be created.
    So, let’s create one.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a virtual environment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It’s recommended that you create a Python virtual environment to ensure a seamless
    developer experience when working with Snowpark; it isolates the Snowpark API
    and allows you to manage all dependencies that are required for development. To
    create a virtual environment using Anaconda, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This command creates a new Python virtual environment named `def_gui_3.8_env`
    with Python 3.8 and installs the necessary packages, such as `numpy` and `pandas`,
    from Anaconda’s Snowflake channel.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the Snowpark Python package
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before installing the package, let''s activate our Python virtual environment
    using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The Snowpark package can be installed from Anaconda’s Snowflake channel using
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Next, let’s install some additional packages.
  prefs: []
  type: TYPE_NORMAL
- en: Installing additional Python packages
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To install additional packages that are necessary for development, such as
    `pandas` and `numpy`, you can use the same Anaconda Snowflake channel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The virtual environment is now ready for development and is connected to your
    favorite IDE, Jupyter Notebook, or VS Code development. Similarly, we can install
    an IPython notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Jupyter Notebook
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Jupyter Notebook is one of the most popular IDEs for developers. In this section,
    we will cover how to configure the Jupyter IDE for Snowpark since the examples
    in this book use Jupyter. Jupyter Notebook needs to be installed in your local
    environment. The Jupyter environment comes installed alongside Anaconda. So, let’s
    open Jupyter Notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The **def_gui_3.8_env** virtual environment must be activated for development
    if it is not activated in the previous section. To activate the virtual environment,
    run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Launch Jupyter Notebook by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: On the Jupyter Notebook web page, click the **New** button in the top-right
    corner. From the drop-down menu, select **Python 3** under the **Notebooks** section.
    This will open a new notebook with an empty cell that’s ready for code execution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: This book will utilize Jupyter Notebook for all the examples.
  prefs: []
  type: TYPE_NORMAL
- en: Importing Snowpark modules
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Python classes for the Snowpark API are part of the `snowflake.snowpark`
    module. You can import particular classes from the module by specifying their
    names. For example, to import the `average` function, you can use the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Now that the development environment has been set up, let’s learn how to operate
    with Snowpark.
  prefs: []
  type: TYPE_NORMAL
- en: Operating with Snowpark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Snowpark for Python consists of client APIs, UDFs, and stored procedures that
    execute directly on the Python engine. The following screenshot shows the various
    Snowpark objects that you can choose from:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.32 – Snowpark Python objects](img/B19923_02_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.32 – Snowpark Python objects
  prefs: []
  type: TYPE_NORMAL
- en: 'Snowpark uses DataFrame objects to query and process data. The guiding principle
    in operating with Snowpark is to keep the data in Snowflake and process it right
    within Snowflake using the various Snowflake objects. The following figure shows
    Snowpark’s architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.33 – Snowpark Python architecture](img/B19923_02_33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.33 – Snowpark Python architecture
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will cover the Python Engine.
  prefs: []
  type: TYPE_NORMAL
- en: The Python Engine
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Python Engine is an Anaconda-powered secure sandboxed Python environment
    that’s executed on top of Snowflake’s virtual warehouse and hosted on Snowflake’s
    compute infrastructure. This lets you process data using Python without the need
    to extract the data outside the environment. The Python Engine consists of the
    UDF engine and the stored procedure engine. The UDF engine is a restricted engine
    that cannot read or write data outside of Snowflake, whereas the stored procedure
    engine is more permissive and consists of a session object for interacting with
    the Snowflake database.
  prefs: []
  type: TYPE_NORMAL
- en: Client APIs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The client API is the `snowflake-snowpark-python` library and it can be installed
    in any Python environment. It provides a session and the DataFrame APIs as methods
    to support queries being pushed down in Snowflake’s Python Engine. The client
    APIs consist of notable objects, including sessions, DataFrames, and more. Let’s
    look at these in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Working with sessions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Snowpark session is part of the Snowpark API and connects to Snowflake
    to interact with it and perform operations using Snowpark objects. The `Session`
    function in Snowpark API is responsible for operating with the session. To import
    the `Session` function for Snowpark, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The Snowpark session consists of a Python dictionary containing the parameter
    values that are required to establish a connection to Snowflake:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The connection consists of the following mandatory parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Account**: The Snowflake account identifier is **<orgname>-<account_name>**.
    There is no need to specify the **snowflakecomputing.com** suffix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**User**: The username of the Snowflake user.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Password**: The password to authenticate to Snowflake.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following are the optional parameters that can be passed to the connection:'
  prefs: []
  type: TYPE_NORMAL
- en: '**role**: The role to be used in the Snowpark session. If left blank, the user’s
    default role will be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**warehouse**: The warehouse that’s used to execute the process. If left blank,
    the user’s default warehouse will be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**database**: The database for the context of execution. If left blank, the
    user’s default database will be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**schema**: The schema for the context of execution. If left blank, the user’s
    default schema will be used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can create the `session` object by passing this dictionary to the session
    builder, which is then used to establish the session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the session has been created, the `session` object acts as a handle to
    interact with Snowflake through various methods to perform operations such as
    reading and manipulating data. Some of the session methods that can be used are
    shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, you can close the session by using the `close()` method. This terminates
    the session and the ongoing queries associated with it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Snowflake recommends that you close the session after the process has been completed.
    The `Session` method can be used to establish a session and interact with the
    `Session` objects.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced authentication
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Snowpark sessions also support advanced authentication, such as key pair authentication,
    if it’s been configured for the user connecting to Snowflake. The private key
    must be serialized and then passed to the connection object of the session builder.
    We will be using the hazmat crypto package to serialize the private key. This
    private key is provided as a variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We encode the private key using the passphrase and then serialize it to assign
    it to a variable that’s passed into the Snowflake connection object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The connection parameters will have the private key passed in serialized form,
    and the session can be established using the session builder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Next, let’s discuss Snowpark DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: Snowpark DataFrames
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Snowpark for Python consists of a client API that provides a DataFrame-based
    approach that queries and processes data with a DataFrame object. The following
    diagram explains the code blocks that are utilized:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.34 – Snowpark DataFrame API](img/B19923_02_34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.34 – Snowpark DataFrame API
  prefs: []
  type: TYPE_NORMAL
- en: 'The Snowpark DataFrame API yields more efficiency with less effort required
    by the developer as it has a concise syntax that is easy to understand and debug.
    The following figure compares a DataFrame and a SQL query:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.35 – Snowpark DataFrame versus a query](img/B19923_02_35.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.35 – Snowpark DataFrame versus a query
  prefs: []
  type: TYPE_NORMAL
- en: In addition, Snowpark DataFrames can directly read data from tables, views,
    and `SELECT` statements that support pushdown so that they can be executed in
    Snowflake. The Snowpark client APIs also support converting pandas DataFrames
    into Snowflake DataFrames so that data can be written back into Snowflake. Finally,
    Snowpark DataFrames support lazy evaluation as data computation is performed once
    an action is invoked.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Lazy evaluation in Snowpark means that data processing operations are not executed
    immediately when they are defined. Instead, Snowpark builds a sequence of transformations
    without executing them until you explicitly request the result. This approach
    optimizes performance and resource usage, allowing you to construct complex data
    workflows efficiently and interactively. Lazy evaluation is a key feature for
    handling large datasets and optimizing data processing tasks in Snowpark.
  prefs: []
  type: TYPE_NORMAL
- en: Working with DataFrames
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: DataFrames are the dataset objects in Snowpark in which data is queried and
    processed. They represent relational datasets that provide lazy evaluation. The
    DataFrame executes SQL in a push-down manner and can perform operations such as
    creating objects and reading, writing, and working with data from the Python code.
    Various methods in the `Session` object are used to work with DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create an employee data table called `SAMPLE_EMPLOYEE_DATA`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will create a table with the required fields for employee
    data. Let’s insert some data into the table for operational purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will populate the table with the data that we can query.
    To query the data, we can directly pass the SQL statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will return the results after it’s been executed in Snowflake.
    We can also store these results in a DataFrame so that we can operate on it in
    Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code will save the results in a `df_subset_row` DataFrame that
    can be displayed using `show()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.36 – DataFrame data](img/B19923_02_36.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.36 – DataFrame data
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will look at Snowpark UDFs and stored procedures.
  prefs: []
  type: TYPE_NORMAL
- en: A note on code snippets
  prefs: []
  type: TYPE_NORMAL
- en: The examples presented in the following section have been simplified intentionally.
    Our main objective is to grasp and distinguish the concepts, rather than delving
    into complex scenarios. However, we’ll delve into more sophisticated examples
    in the upcoming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: UDFs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Snowpark for Python supports UDFs that allow developers to write reusable custom
    lambdas and functions to process the data through DataFrames. Like built-in functions,
    UDFs can be called from SQL, which enhances SQL with functionality that it doesn’t
    have or doesn’t do well. UDFs also provide a way to encapsulate functionality
    so that you can call it repeatedly from multiple places in your code. For example,
    you can write a UDF that returns a single value called a *scalar* function, also
    known as a UDF, or a group of values called a *tabular* function, also known as
    a **user data table function** (**UDTF**). These UDFs can be developed from within
    Python worksheets or by using Snowpark from your local development environment.
  prefs: []
  type: TYPE_NORMAL
- en: Scalar UDFs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Scalar UDFs are invoked once per row and return one output row for each input
    row. These UDFs are called just like a standard SQL function, with columns or
    expressions as arguments. It produces a row consisting of a single column/value
    as output. The data gets processed in parallel across each node within a multi-node
    virtual warehouse.
  prefs: []
  type: TYPE_NORMAL
- en: Working with UDFs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Once a Snowpark session has been created, the UDF can be turned into a standard
    function that can be registered in Snowflake:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The preceding template delineates the steps in creating a UDF in Snowflake using
    Python. It involves defining the primary Python function that will be used by
    the UDF and registering it in Snowflake. Next, the function and UDF names are
    specified, along with the Snowflake stage, where the UDF files will be uploaded.
    Finally, the required Snowpark `DataType` object for the UDF’s return value is
    imported, and its specific object is determined. This is not the only template
    we can follow – we can also leverage decorators to perform this. But for beginners,
    this can be very helpful to templatize and organize UDFS, UDTFs, and stored procedures.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, any necessary Snowpark `DataType` objects for the UDF’s input
    arguments are imported and determined. The template also allows you to include
    extra packages and imports in the UDF. We can also specify whether the UDF should
    be temporary and whether an existing UDF with the same name should be overwritten:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This simple function gets the input name and splits it so that it returns the
    last name. The UDF is registered to the internal `My_Stage` stage and is deployed
    into Snowflake. The UDF can be invoked directly in SQL as a function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.37 – UDF Snowpark execution](img/B19923_02_37.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.37 – UDF Snowpark execution
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we invoked the `LAST_NAME_FINDER` function with the `Name`
    column, which returned the last name by splitting it. The function can also be
    called within the `DataFrame` function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.38 – UDF DataFrame execution](img/B19923_02_38.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.38 – UDF DataFrame execution
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s look into UDTFs.
  prefs: []
  type: TYPE_NORMAL
- en: UDTF
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Tabular UDFs, also known as UDTFs, require stateful operations to be performed
    on data batches and are invoked once per row, just like scalar UDFs, but they
    can return multiple rows as output for each input row. The UDTF handler method
    consists of an additional optional parameter that helps initialize the handler
    once for each partition and finalize processing for each section. A UDTF is a
    type of UDF that executes similarly to a UDF but with tabular output. Therefore,
    they can be developed in Python worksheets and Snowpark development environments.
  prefs: []
  type: TYPE_NORMAL
- en: Working with UDTFs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Creating a UDTF in Snowpark is similar to creating a UDF in that after a Snowpark
    session is created, the UDTF can be made directly in a standard command that can
    be registered in Snowflake.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following Snowpark UDTF template provides a basic outline for creating
    a UDTF in Snowpark using Python. The following code shows the key elements in
    this template:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: This template creates a UDTF in Snowflake. First, a main Python handler class
    is defined for the UDTF, which can utilize other functions from the script or
    be imported from external sources. It is important to note that only one main
    Python handler class can be assigned to the UDTF.
  prefs: []
  type: TYPE_NORMAL
- en: In this template, you are expected to replace `<name of main Python class>`
    with a meaningful name for your UDTF class. This is the main class where you will
    define the logic for processing data within your UDTF.
  prefs: []
  type: TYPE_NORMAL
- en: The `__init__` method is marked as optional, meaning you may or may not include
    it in your UDTF implementation. If you choose to include it, this method will
    execute once per partition before breaking out into individual rows.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use the `__init__` method to perform any partition-level setup or initialization
    specific to your UDTF. For example, you might use it to initialize variables and
    open connections or set up data structures that will be used throughout the UDTF’s
    execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: This method is responsible for processing each input row within a partition
    and generating tabular data as tuples. Inside the `process` method, you can write
    custom Python code that executes for every input row in the partition. The key
    part of this method is the usage of `yield` statements or a `return` statement
    to produce tuples as output.
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of `yield` statements, you can output one or more tuples for each
    input row, allowing for flexibility in generating tabular results. Alternatively,
    you can use a `return` statement with a list of tuples to achieve the same result.
    In essence, the `process` method serves as the core logic for your UDTF, where
    you manipulate and transform data from each input row into tabular format, making
    it suitable for further processing or analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: This method is used to execute logic that’s specific to a partition after processing
    all the input rows in that partition. Inside the `end_partition` method, you can
    write custom Python code that performs calculations or generates results based
    on the data that’s processed within that partition. This method can also be used
    to yield or return tabular data as tuples, similar to the `process` method, but
    this data typically represents aggregated or summarized information for the entire
    partition.
  prefs: []
  type: TYPE_NORMAL
- en: You have the option to use `yield` statements to output one or more tuples,
    or you can use a `return` statement with a list of tuples to provide the partition-level
    result. This allows you to perform partition-specific operations and return the
    results in a structured format.
  prefs: []
  type: TYPE_NORMAL
- en: The `end_partition` method in a Snowpark UDTF template is used for executing
    partition-level logic and returning tabular data or results specific to that partition
    after processing all input rows within it. It’s especially useful for tasks such
    as aggregations or calculations, which require data from the entire partition.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code template provides details on how to register a UDTF in Snowflake
    and the corresponding options to define the UDTF:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The `udtf()` method of the Snowflake Snowpark `Session` object is used to create
    the UDTF in Snowflake. The process involves several steps: determining the Python
    class that the UDTF will use, specifying the name of the UDTF within Snowflake
    (it can be a fully qualified name or created in the same namespace as the Snowpark
    `Session` object), and providing the name of the Snowflake stage where the UDTF
    files will be uploaded.'
  prefs: []
  type: TYPE_NORMAL
- en: Specific Snowpark `DataType` objects are imported to define the structure of
    the UDTF. This includes importing objects for defining tabular structures, such
    as table schemas (using `StructType`) and fields within a table (using `StructField`).
    Furthermore, a specific Snowpark `DataType` object is imported for the values
    that are passed into and returned by the UDTF. The output schema of the UDTF is
    defined using the imported Snowpark `DataType` objects, embedding them into `StructField`
    and `StructType` objects. Additionally, a list of specific Snowpark `DataType`
    objects is defined for the input arguments of the UDTF. It is crucial to include
    all these `DataType` objects in the import and ensure they match the expected
    arguments that are passed to the `process` method within the `handler` class.
  prefs: []
  type: TYPE_NORMAL
- en: The template allows a temporary UDTF to be created that only exists within the
    specific Snowflake Snowpark `Session` object. Additionally, an option exists to
    overwrite an existing UDTF with the same name; an error will be returned if it’s
    set to `False` and a UDTF already exists. Lastly, the template briefly mentions
    adding additional packages and imports to the UDTF, which is optional and can
    be done using the provided rows.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example illustrates how to use a Snowpark UDTF to calculate the
    averages of numeric data within Snowflake tables. This showcases the practical
    application of UDTFs in Snowpark for custom data processing tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The `CalculateAverage` Snowpark UDTF is designed to compute the average of a
    numeric column within a Snowflake table. It does this by accumulating the input
    values for each partition of the data and then calculating the average when the
    partition ends.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `process` method collects input values one by one and stores them in a
    list. When the partition ends (in the `end_partition` method), it calculates the
    average by summing up all the collected values and dividing by the count of values.
    Finally, it yields the calculated average as the UDTF’s output. This UDTF simplifies
    the process of computing averages in Snowflake SQL queries, especially when dealing
    with large datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we’re creating a UDTF function called `Average_Age` that calculates
    the average age by getting the age as input. The function is uploaded into `MY_STAGE`
    and registered in Snowflake.
  prefs: []
  type: TYPE_NORMAL
- en: 'The function can be executed to get the average age per country from the sample
    employee data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'This will display the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.39 – UDTF Snowpark execution](img/B19923_02_39.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.39 – UDTF Snowpark execution
  prefs: []
  type: TYPE_NORMAL
- en: The output shows the execution output of the UDTF. In the next section, we will
    cover vectorized UDFs.
  prefs: []
  type: TYPE_NORMAL
- en: Vectorized UDFs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Vectorized UDFs operate similarly to scalar UDFs in that they let you define
    Python functions that receive batches of input rows as pandas DataFrames and return
    collections of results as pandas arrays or series.
  prefs: []
  type: TYPE_NORMAL
- en: Vectorized UDFs parallelize the operation on batches of data and provide significant
    performance advantages on sets of rows compared to serial row processing. In addition,
    they reduce the complexity of using libraries that operate on pandas DataFrames
    and arrays.
  prefs: []
  type: TYPE_NORMAL
- en: Working with vectorized UDFs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The same example we looked at previously can be executed in the Snowpark environment
    after establishing the session by passing a vectorized DataFrame as the input
    to the standard UDF:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The example UDF returns the city and country in the `CITY_COUNTRY` column for
    each row in the `Sample` `Employee` table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.40 – Vectorized UDF in Snowpark](img/B19923_02_40.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.40 – Vectorized UDF in Snowpark
  prefs: []
  type: TYPE_NORMAL
- en: The output shows the execution output of the vectorized UDF. In the next section,
    we will cover stored procedures.
  prefs: []
  type: TYPE_NORMAL
- en: Stored procedures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A Python stored procedure is a series of code statements you can parameterize
    and execute on demand. They run in a less restricted environment than UDFs and
    support interacting with Snowflake objects, as well as performing DDL and DML
    operations on tables.
  prefs: []
  type: TYPE_NORMAL
- en: Stored procedures in Snowpark are utilized for executing tasks and streams within
    Snowflake’s data processing framework. These stored procedures encapsulate specific
    logic or functionality, allowing users to perform various operations on data seamlessly.
    Tasks, which are often associated with batch processing, involve executing predefined
    actions or workflows on datasets at scheduled intervals. Stored procedures enable
    users to automate these tasks, ensuring consistent and efficient data processing.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, streams are continuous data pipelines that capture changes
    in real-time from a data source. Stored procedures play a vital role in managing
    and processing streams by defining how incoming data should be processed and integrated
    into the target destination. With Snowpark, users can create stored procedures
    to handle these streaming data scenarios, including data transformation, filtering,
    and loading data into Snowflake tables.
  prefs: []
  type: TYPE_NORMAL
- en: Working with stored procedures
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A stored procedure can be created in Snowpark with the following template:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Here, we define the main Python function that will be used in the stored procedure
    and an additional argument called `snowpark_session`, which allows us to interact
    with Snowflake objects. Next, we use the `sproc.register()` method to create the
    stored procedure, specifying the Python function, stored procedure’s name, and
    Snowflake stage for file uploads. Finally, we import specific Snowpark `DataType`
    objects for the stored procedure’s return value and input arguments.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `snowflake_session` argument is implicitly understood and not included
    in the input arguments. Optional rows allow for additional packages and imports.
    Here, we can determine whether the stored procedure will be temporary. We can
    also decide whether to overwrite an existing one with the same name and specify
    whether it will execute as the caller or the owner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The stored procedure returns the column name and the age from the `Employee
    Data` table. It’s registered as `SPROC_SUBSET_TABLE` and uploaded through `My_Stage`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.41 – Stored procedure execution](img/B19923_02_41.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.41 – Stored procedure execution
  prefs: []
  type: TYPE_NORMAL
- en: The stored procedure can be executed by running the `CALL` command.
  prefs: []
  type: TYPE_NORMAL
- en: The difference between UDFs and stored procedures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'UDFs and stored procedures have significant differences in terms of functionality
    and usage. The following figure shows the basic differences between UDFs and stored
    procedures and what they’re used for:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.42 – UDFs versus stored procedures](img/B19923_02_42.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.42 – UDFs versus stored procedures
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table shows the differences and similarities between UDFs and
    stored procedures based on their properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **UDF****s** | **Stored Procedure****s** |'
  prefs: []
  type: TYPE_TB
- en: '| Purpose | Perform calculations and return the results. UDFs require a value
    to be returned. | Perform complex operations by executing SQL statements. They
    do not require an explicit value to be returned. |'
  prefs: []
  type: TYPE_TB
- en: '| Usage | UDFs can be used when logic needs to be called as part of SQL statements
    that return a value. | When database operations or administrative tasks need to
    be performed. |'
  prefs: []
  type: TYPE_TB
- en: '| Output | UDFs always need to return a result. | Stored procedures don’t need
    to return a result. |'
  prefs: []
  type: TYPE_TB
- en: '| Context | UDF return values are directly accessible in the SQL context. |
    Stored procedure return values are not accessible in the SQL context. |'
  prefs: []
  type: TYPE_TB
- en: '| Execution | UDFs can be called in the context of another SQL statement. In
    addition, multiple UDFs can be invoked in a single SQL statement. | Stored procedures
    are called independently. Therefore, only a single stored procedure is invoked
    in a SQL statement. |'
  prefs: []
  type: TYPE_TB
- en: '| Security | UDFs cannot access the database or perform operations directly
    on it. | Stored procedures can access the database and perform data operations
    on it. |'
  prefs: []
  type: TYPE_TB
- en: Table 2.1 – Comparison of UDFs and stored procedures
  prefs: []
  type: TYPE_NORMAL
- en: Both stored procedures and UDFs can be used together to expand the capabilities
    of Python execution in Snowflake.
  prefs: []
  type: TYPE_NORMAL
- en: Establishing a project structure for Snowpark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To assist with the development of Snowpark in Python and to make it easy to
    create a Snowpark project, Snowflake has released Snowpark project templates for
    Python. These contain everything you’ll need for developing, testing, and deploying
    with Snowpark – they provide all the boilerplate required to develop UDFs and
    stored procedures, along with unit tests and even GitHub Actions workflow files
    for CI/CD.
  prefs: []
  type: TYPE_NORMAL
- en: 'The project template has been released as open source on GitHub, making it
    easy for developers to clone and use the project. To clone the project, follow
    these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the files or clone the repository from [https://github.com/Snowflake-Labs/snowpark-python-template](https://github.com/Snowflake-Labs/snowpark-python-template).
    A new GitHub repository can be created from the template by using the GitHub CLI,
    like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The new repository’s name needs to be specified. The repository will be similar
    to the Snowpark project template.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Set up the following environment variables so that you can configure the necessary
    Snowflake details:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: These environment variables are required to connect to the Snowflake environment
    and for Snowpark to establish a session.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create an Anaconda virtual environment and install the dependencies from the
    **environment.yml** file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can test the connection and check if the environment has been set up by
    executing the **app.py** stored procedure. Navigate to the project folder and
    run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This produces an output called **Hello World** that establishes a connection
    to Snowflake.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The Snowpark project supports functions and stored procedures. The project structure
    consists of the `procs` directory for stored procedures, the `udf` directory for
    UDFs, and the `util` directory for the utilities methods and classes that are
    shared between UDFs and stored procedures.
  prefs: []
  type: TYPE_NORMAL
- en: The `test` folder consists of the test cases that can be tested via `pytest`.
    There’s also a GitHub workflow that can be deployed via GitHub Actions. We will
    cover this in detail in the following chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Snowpark is very versatile and supports complex development patterns. In this
    chapter, we learned how to configure the Snowpark development environment and
    different Snowpark objects, such as sessions, UDFs, and stored procedures, and
    how to use them. We also learned how to set up a Snowpark development project
    locally and inside a Python worksheet before looking at some sample code that
    we could use to start developing.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will cover how to perform data processing with Snowpark,
    as well as how to ingest, prepare, and analyze data.
  prefs: []
  type: TYPE_NORMAL
