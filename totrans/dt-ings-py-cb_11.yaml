- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Automating Your Data Ingestion Pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data sources are frequently updated, and this requires us to update our data
    lake. However, with multiple sources or projects, it becomes impossible to trigger
    data pipelines manually. Data pipeline automation makes ingesting and processing
    data mechanical, obviating the human actions to trigger it. The importance of
    automation configuration lies in the ability to streamline data flow and improve
    data quality, reducing errors and inconsistency.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will cover how to automate the data ingestion pipelines
    in Airflow, along with two essential topics in data engineering, data replication
    and historical data ingestion, as well as best practices.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Scheduling daily ingestions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scheduling historical data ingestion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scheduling data replication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up the `schedule_interval` parameter
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving scheduling errors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can find the code from this chapter in the GitHub repository at [https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_11](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_11).
  prefs: []
  type: TYPE_NORMAL
- en: Installing and running Airflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter requires that Airflow is installed on your local machine. You can
    install it directly on your **Operating System** (**OS**) or use a Docker image.
    For more information, refer to the *Configuring Docker for Airflow* recipe in
    [*Chapter 1*](B19453_01.xhtml#_idTextAnchor022).
  prefs: []
  type: TYPE_NORMAL
- en: After following the steps described in [*Chapter 1*](B19453_01.xhtml#_idTextAnchor022),
    ensure your Airflow instance runs correctly. You can do that by checking the Airflow
    UI at `http://localhost:8080`.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are using a Docker container (as I am) to host your Airflow application,
    you can check its status in the terminal with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the status of the container:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.1 –  Airflow containers running](img/Figure_11.01_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1 – Airflow containers running
  prefs: []
  type: TYPE_NORMAL
- en: 'Or you can check the container status on **Docker Desktop**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.2 – Docker Desktop showing Airflow running containers](img/Figure_11.02_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2 – Docker Desktop showing Airflow running containers
  prefs: []
  type: TYPE_NORMAL
- en: Scheduling daily ingestions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data constantly changes in our dynamic world, with new information being added
    every day and even every second. Therefore, it is crucial to regularly update
    our data lake to reflect the latest scenarios and information.
  prefs: []
  type: TYPE_NORMAL
- en: Managing multiple projects or pipelines concurrently and manually triggering
    them while integrating new data from various sources can be daunting. To alleviate
    this issue, we can rely on schedulers, and Airflow provides a straightforward
    solution for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will create a simple **Directed Acyclic Graph** (**DAG**)
    in Airflow and explore how to use its parameters to schedule a pipeline to run
    daily.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Please refer to the *Technical requirements* section for this recipe since we
    will handle it with the same technology mentioned here.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this exercise, we will create a simple DAG. The structure of your Airflow
    folder should look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.3 – daily_ingestion_dag DAG folder structure](img/Figure_11.03_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.3 – daily_ingestion_dag DAG folder structure
  prefs: []
  type: TYPE_NORMAL
- en: All code in this recipe will be placed inside `daily_ingestion_dag.py`. Ensure
    you have created the file by following the folder structure in *Figure 11**.3*.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'These are the steps for this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by importing the required libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we will define `default_args` for our DAG. For the `start_date` parameter,
    insert today’s date or a few days before you are doing this exercise. For `end_date`,
    insert a date a few days ahead of today’s date. In the end, it should look like
    the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we will define our DAG and the tasks inside it. Since we want to focus
    on how to schedule daily ingestion, our tasks will each be a `BashOperator` since
    they can execute Bash commands with simplicity, as you can see here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With the DAG written, let’s enable it on the Airflow UI, and the DAG should
    run immediately. After running, the DAG will have a `SUCCESS` status, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.4 –  daily_ingestion_dag DAG in the Airflow UI](img/Figure_11.04_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.4 – daily_ingestion_dag DAG in the Airflow UI
  prefs: []
  type: TYPE_NORMAL
- en: 'If we check the logs, it will show the `echo` command output, similar to the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we need to ensure the DAG will run daily. To confirm this, select the
    **Calendar** option on your DAG page. You will see something similar to this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.5 – DAG’s Calendar visualization in the Airflow UI](img/Figure_11.05_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.5 – DAG’s Calendar visualization in the Airflow UI
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the execution is depicted in the shaded region to the left,
    indicating a successful outcome (`end_date`, are marked with a dot inside, indicating
    the job will run every day for the next few days.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 11**.5* shows some days when the job was executed successfully. This
    is shown to users how the same calendar behaves on previous executions.'
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Airflow’s scheduler is mainly defined by three parameters: `start_date`, `end_date`,
    and `schedule_interval`. These three parameters define the beginning and end of
    the job and the interval between executions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at `default_args`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Since I am writing this exercise on April 12, 2023, I set my `start_date` parameter
    to the same day. This will make the job retrieve information relating to April
    12, and if I put it a few days before the current date, Airflow will retrieve
    the earlier date. Don’t worry about it now; we will cover more about this in the
    *Scheduling historical data* *ingestion recipe*.
  prefs: []
  type: TYPE_NORMAL
- en: The key here is the `schedule_interval` parameter. As the name suggests, this
    parameter will define the periodicity or the interval of each execution, and,
    as you can observe, it was simply set using the `@``daily` value.
  prefs: []
  type: TYPE_NORMAL
- en: The **Calendar** option on the DAG UI page is an excellent feature of Airflow
    2.2 onward. This functionality allows the developers to see the next execution
    days for the DAG, preventing some confusion.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The DAG parameters are not limited to the ones we have seen in this recipe.
    Many others are available to make the data pipeline even more automated and intelligent.
    Let’s take a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'There are three additional parameters here: `queue`, `pool`, and `priority_weight`.
    As we saw in [*Chapter 9*](B19453_09.xhtml#_idTextAnchor319) and [*Chapter 10*](B19453_10.xhtml#_idTextAnchor364),
    the Airflow architecture includes a queue (usually executed by `pool` parameter
    limits the number of simultaneous jobs. Finally, `priority_weight`, as the name
    suggests, defines the priority of a DAG over other DAGs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can read more about these parameters in the Airflow official documentation
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://airflow.apache.org/docs/apache-airflow/1.10.2/tutorial.xhtml](https://airflow.apache.org/docs/apache-airflow/1.10.2/tutorial.xhtml)'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can read more about scheduling with crontab also at [https://crontab.guru/](https://crontab.guru/).
  prefs: []
  type: TYPE_NORMAL
- en: Scheduling historical data ingestion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Historical data is vital for data-driven decisions, providing valuable insights
    and supporting decision-making processes. It can also refer to data that has been
    accumulated over a period of time. For example, a sales company can use historical
    data from previous marketing campaigns to see how they have influenced the sales
    of a specific product over the years.
  prefs: []
  type: TYPE_NORMAL
- en: This exercise will show how to create a scheduler in Airflow to ingest historical
    data using the best practices and common concerns related to this process.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Please refer to the *Technical requirements* section for this recipe since we
    will handle it with the same technology mentioned here.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this exercise, we will create a simple DAG inside our DAGs folder. The structure
    of your Airflow folder should look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.6 – historical_data_dag folder structure in your local Airflow
    directory](img/Figure_11.06_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.6 – historical_data_dag folder structure in your local Airflow directory
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are the steps for this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by importing our libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s define `default_args`. Since we wish to process old data, I will
    set `datetime` for `start_date` before the current day, and `end_date` will be
    near the current day. See the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we will create a simple function to print the date Airflow used to execute
    the pipeline. You can see it here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we will declare our DAG parameters and a `PythonOperator` task to
    execute it, as you can see here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Heading to the Airflow UI, let’s proceed with the usual steps to enable the
    DAG and see its execution. On the `historical_data_dag` page, you should see something
    similar to the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.7 – historical_data_dag DAG in the Airflow UI](img/Figure_11.07_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.7 – historical_data_dag DAG in the Airflow UI
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the task ran with success.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s check our `logs` folder. If we select the folder with the same name
    as the DAG we created (`historical_data_dag`), we will observe `run_id` instances
    on different days, beginning on April 2 and finishing on April 10:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.8 – Airflow logs folder showing retroactive ingestion](img/Figure_11.08_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.8 – Airflow logs folder showing retroactive ingestion
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s open the first `run_id` folder to explore the log for that run:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.9 – DAG log for April 2, 2023](img/Figure_11.09_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.9 – DAG log for April 2, 2023
  prefs: []
  type: TYPE_NORMAL
- en: The log tells us the `execution_date` parameter, which is the same as the `start_date`
    parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a closer look at the logs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We will observe the same pattern for the `run_id` for April 3:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.10 – DAG log for April 3, 2023](img/Figure_11.10_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.10 – DAG log for April 3, 2023
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a closer look at the log output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The `execution_date` also refers to April 3.
  prefs: []
  type: TYPE_NORMAL
- en: This shows us that Airflow has used the interval declared on `start_date` and
    `end_date` to run the task!
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s proceed to understand how the scheduler works.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we saw, scheduling and retrieving historical data with Airflow is straightforward,
    and the key parameters were `start_date`, `end_date`, and `schedule_interval`.
    Let’s discuss them in a little more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: The `start_date` parameter defines the first date Airflow will look at when
    the pipeline is triggered. In our case, it was April 2.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next is `end_date`. Usually, this is not a mandatory parameter, even for recurrent
    ingests. However, the purpose of using it was to show that we can set a date as
    a limit to stop the ingestion.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, `schedule_interval` dictates the interval between two dates. We used
    a daily interval in this exercise, but we could also use `crontab` if we needed
    more granular historical ingestion. We will explore this in more detail in the
    *Setting up the schedule_interval* *parameter* recipe.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With this information, it is easier to understand the logs we got from Airflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.11 – Airflow logs folder showing historic ingestion](img/Figure_11.11_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.11 – Airflow logs folder showing historic ingestion
  prefs: []
  type: TYPE_NORMAL
- en: Each folder represents one historical ingestion that occurred at a daily interval.
    Since we did not define a more granular date-time specification, the folder name
    uses the time that the job was triggered. This information is not included in
    the logs.
  prefs: []
  type: TYPE_NORMAL
- en: 'To show what date Airflow was using behind the scenes, we created a simple
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The only purpose of the function is to show the execution date of the task.
    The `execution_date` parameter is an internal parameter that displays when a task
    is executed and can be used by operators or other functions to execute something
    based on a date.
  prefs: []
  type: TYPE_NORMAL
- en: For example, let’s say we need to retrieve historical data stored as a partition.
    We can use `execution_date` to pass the date-time information to a Spark function,
    which will read and retrieve data from that partition with the same date information.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, retrieving historical data/information in Airflow requires a
    few configurations. A good practice is to have a separate and dedicated DAG for
    historical data processing so that current data ingestion is not impaired. Also,
    if it is necessary to reprocess data, we can do it with a few parameter changes.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Inside the technique of ingesting historical data using Airflow are two important
    concepts: *catchup* and *backfill*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Scheduling and running DAGs for past periods that may have been missed for
    various reasons is commonly known as `schedule_interval`. By default, this feature
    is enabled in Airflow. Therefore, if a paused or uncreated DAG’s `start_date`
    lies in the past, it will be automatically scheduled and executed for missed time
    intervals. The following diagram illustrates this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.12 – Airflow catchup timeline. Source: https://medium.com/nerd-for-tech/airflow-catchup-backfill-demystified-355def1b6f92](img/Figure_11.12_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.12 – Airflow catchup timeline. Source: https://medium.com/nerd-for-tech/airflow-catchup-backfill-demystified-355def1b6f92'
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, Airflow’s *backfill* functionality allows you to execute
    DAGs retroactively, along with their associated tasks for past periods that may
    have been missed due to the DAG being paused, not yet created, or for any other
    reason. Backfilling in Airflow is a powerful feature that helps you to fill the
    gaps and catch up on data processing or workflow execution that may have been
    missed in the past.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can read more about it on *Amit Singh Rathore’s* blog page here: [https://medium.com/nerd-for-tech/airflow-catchup-backfill-demystified-355def1b6f92](https://medium.com/nerd-for-tech/airflow-catchup-backfill-demystified-355def1b6f92).'
  prefs: []
  type: TYPE_NORMAL
- en: Scheduling data replication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the first chapter of this book, we covered what data replication is and why
    it’s important. We saw how vital this process is in the prevention of data loss
    and in promoting recovery from disasters.
  prefs: []
  type: TYPE_NORMAL
- en: Now, it is time to learn how to create an optimized schedule window to make
    data replication happen. In this recipe, we will create a diagram to help us decide
    the best moment to replicate our data.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This exercise does not require technical preparation. However, to make it closer
    to a real scenario, let’s imagine we need to decide the best way to ensure the
    data from a hospital is being adequately replicated.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will have two pipelines: one holding patient information and another with
    financial information. The first pipeline collects information from a patient
    database and synthesizes it into readable reports used by the medical team. The
    second will feed an internal dashboard used by the hospital executives.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Due to infrastructure limitations, the operations team has only one requirement:
    only one pipeline can have its data replicated quickly.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are the steps for this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Identify the targets to replicate**: As described in the *Getting ready*
    section, we have identified the target data, which are the pipeline holding patient
    information, and the pipeline with financial data to feed a dashboard.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: However, suppose this information is not coming promptly from stakeholders or
    other relevant people. In that case, we must always start by identifying our project’s
    most critical tables or databases.
  prefs: []
  type: TYPE_NORMAL
- en: '**Replication periodicity**: We must define the replication schedule based
    on our data’s criticality or relevance. Let’s take a look at the following diagram:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.13 – Periodicity of data replication](img/Figure_11.13_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.13 – Periodicity of data replication
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the more critical the data is, the more frequently the replication
    is recommended. In our scenario, the patient reports would fit better with 30
    minutes to 3 hours after the ingestion, while the financial data can be replicated
    until 24 hours have passed.
  prefs: []
  type: TYPE_NORMAL
- en: '**Set a schedule window for replication**: Now, we need to create a schedule
    window to replicate the data. This replication decision needs to take into consideration
    two important factors, as you can see in the following diagram:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.14 – Replication window](img/Figure_11.14_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.14 – Replication window
  prefs: []
  type: TYPE_NORMAL
- en: Based on the two pipelines (and remembering we need to prioritize one), the
    suggestion would be to replicate the financial data every day after business working
    hours, while the patient data can be done at the same time as new information
    arrives.
  prefs: []
  type: TYPE_NORMAL
- en: Don’t worry if this seems a bit confusing. Let’s explore the details in the
    *How it* *works* section.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data replication is a vital process that ensures data availability and disaster
    recovery. Its concept is older than the current ETL process and has been used
    for many years in on-premises databases. Our advantage today is that we can carry
    out this process at any moment. In contrast, replication had a strict schedule
    window a few years ago due to hardware limitations.
  prefs: []
  type: TYPE_NORMAL
- en: In our example, we handled two pipelines that had distinct severity levels.
    The idea behind this is to teach attentive eyes to decide when doing each replication.
  prefs: []
  type: TYPE_NORMAL
- en: The first pipeline, which is the patient reports pipeline, handles sensitive
    data such as personal information and medical history. It also may be helpful
    for doctors and other health workers to help a patient.
  prefs: []
  type: TYPE_NORMAL
- en: Based on this, the best approach is to replicate this data within a few minutes
    or hours of it being processed, allowing high availability and redundancy.
  prefs: []
  type: TYPE_NORMAL
- en: At first look, the financial data seems to be very critical and demands fast
    replication; we need to remember this pipeline feeds data to a dashboard, and
    therefore, an analyst can use the raw data to generate reports.
  prefs: []
  type: TYPE_NORMAL
- en: The decision to schedule data replication must consider other factors besides
    the data involved. It is also essential to understand who is interested in or
    needs to access the data and how it impacts the project, area, or business when
    it becomes unavailable.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This recipe covered a simple example of setting a scheduling agenda for data
    replication. We also covered in *step 3* the two main points to have in mind when
    doing so. Nevertheless, many other factors can influence the scheduler’s performance
    and execution. A few examples are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Where Airflow (or a similar platform) is hosted on a server
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The CPU capacity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of schedulers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Networking throughput
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you want to know more about it, you can find a complete list of these factors
    in the Airflow documentation: [https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/scheduler.xhtml#what-impacts-scheduler-s-performance](https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/scheduler.xhtml#what-impacts-scheduler-s-performance).'
  prefs: []
  type: TYPE_NORMAL
- en: The great thing about this documentation is that many points also apply to other
    data pipeline processors and can serve as a guide.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the schedule_interval parameter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most widely used parameters in Airflow DAG scheduler configuration
    is `schedule_interval`. Together with `start_date`, it creates a dynamic and continuous
    trigger for the pipeline. However, there are some small details we need to pay
    attention to when setting `schedule_interval`.
  prefs: []
  type: TYPE_NORMAL
- en: This recipe will cover different forms to set up the `schedule_interval` parameter.
    We will also explore a practical example to see how the scheduling window works
    in Airflow, making it more straightforward to manage pipeline executions.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While this exercise does not require any technical preparation, it is recommended
    to take notes about when the pipeline is supposed to start and the interval between
    each trigger.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here, we will show only the `default_args` dictionary to avoid code redundancy.
    However, you can always check out the complete code in the GitHub repository:
    [https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_11/settingup_schedule_interval](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_11/settingup_schedule_interval).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how we can declare `schedule_interval`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`schedule_interval` value is by using accessible names such as `@daily`, `@hourly`,
    or `@weekly`. See what it looks like in the following code:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`schedule_interval` using crontab notation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this case, we set the scheduler to start every weekday, from Monday to Friday,
    at 10 pm (or 22:00 hours).
  prefs: []
  type: TYPE_NORMAL
- en: '`schedule_interval` is by using the `timedelta` method. In the following code,
    we can set the pipeline to trigger with an interval of one day between each execution:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `schedule_interval` parameter is an essential aspect of scheduling DAGs
    in Airflow and provides a flexible way to define how frequently your workflows
    should be executed. We can think of it as the core of the Airflow scheduler.
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal of this recipe was to show the different ways to set `schedule_interval`
    and when to use each of them. Let’s explore them in more depth:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Friendly names:** As the name suggests, this notation uses user-friendly
    labels or aliases. It provides an easy and convenient way to specify the exact
    time and date for scheduled tasks to run. It can be an easy and simple solution
    if you don’t have a specific date-time to run the scheduler.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Crontab notation:** Crontabs have long been widely used across applications
    and systems. Crontab notation consists of five fields, representing the minute,
    hour, day of the month, month, and day of the week. It is a great choice when
    handling complex schedules, for example, executing the trigger at 1 p.m. on Mondays
    and Fridays, or other combinations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timedelta(minutes=5)`). It is also a user-friendly notation but with more
    granular power.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Although we have seen three ways to set the `schedule_interval` here, remember
    Airflow is not a streaming solution, and having multiple DAGs running with a small
    interval between them can overload the server. Consider using a streaming tool
    if you or your team needs to schedule ingestions every 10-30 minutes, or less.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*TowardsDataScience* has a fantastic blog post about how `schedule_interval`
    works behind the scenes. You can find it here: [https://towardsdatascience.com/airflow-schedule-interval-101-bbdda31cc463](https://towardsdatascience.com/airflow-schedule-interval-101-bbdda31cc463).'
  prefs: []
  type: TYPE_NORMAL
- en: Solving scheduling errors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point, you may have already experienced some issues with scheduling
    pipelines not being triggered as expected. If not, don’t worry; it will happen
    sometime and is totally normal. With several pipelines running in parallel, in
    different windows, or attached to different timezones, it is expected to be entangled
    with one or another.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid this entanglement, in this exercise, we will create a diagram to assist
    in the debugging process, identify the possible causes of a scheduler not working
    correctly in Airflow, and see how to solve it.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe does not require any technical preparation. Nevertheless, taking
    notes and writing down the steps we will follow here can be helpful. Writing when
    learning something new can help to fix the knowledge in our minds, making it easier
    to remember later.
  prefs: []
  type: TYPE_NORMAL
- en: 'Back to our exercise; scheduler errors in Airflow typically give the DAG status
    `None`, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.15 – DAG in the Airflow UI with an error in the scheduler](img/Figure_11.15_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.15 – DAG in the Airflow UI with an error in the scheduler
  prefs: []
  type: TYPE_NORMAL
- en: We will now find out how to fix the error and make the job run again.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s try to identify what could be the cause of the error in our scheduler.
    Don’t worry about understanding why we used the approaches that we have. We will
    cover them in detail in *How* *it works*:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can first check whether `start_date` has been set to `datetime.now()`. If
    this is the case, the best approach here is to change this parameter value to
    a specific date, as you can see here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.16 – Error caused by start_date parameter](img/Figure_11.16_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.16 – Error caused by start_date parameter
  prefs: []
  type: TYPE_NORMAL
- en: 'The code will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can verify whether `schedule_interval` is aligned with the `start_date`
    parameter. In the following diagram, you can see three possibilities to fix the
    issue:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.17 – Error caused by the start_date and schedule_interval parameters](img/Figure_11.17_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.17 – Error caused by the start_date and schedule_interval parameters
  prefs: []
  type: TYPE_NORMAL
- en: 'You can prevent this error by using crontab notation in `schedule_interval`,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'If you are facing problems with the timezone, you can define which timezone
    Airflow will trigger the job in by using the `pendulum` library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, another standard error scenario is when `schedule_interval` changes
    after the DAG has been running for some time. In this case, the solution usually
    is to recreate the DAG:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 11.18 – Error caused by a change in schedule_interval](img/Figure_11.18_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.18 – Error caused by a change in schedule_interval
  prefs: []
  type: TYPE_NORMAL
- en: 'At the end of these steps, we will end up with a debug diagram similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.19 – Diagram to assist in identifying an error caused in the Airflow
    scheduler](img/Figure_11.19_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.19 – Diagram to assist in identifying an error caused in the Airflow
    scheduler
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you can see, the goal of this recipe was to show three different scenarios
    in which it is common to observe errors related to the scheduler. Errors in the
    scheduler normally lead to a DAG status as `None`, as we saw in the *Getting ready*
    section. However, having a trigger that does not behave as expected is also considered
    an error. Now, let’s explore the three addressed scenarios and their solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first scenario usually occurs when we want to use the current date for
    `start_date`. Although it seems like a good idea to use the `datetime.now()` function
    to represent the current date-time, Airflow will not interpret it as we do. The
    `datetime.now()` function will create what we call a *dynamic scheduler*, and
    the trigger will never be executed. It happens because the execution schedule
    uses `start_date` and `schedule_interval` to know when to execute the trigger,
    as you can see here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.20 – Airflow execution scheduler equation](img/Figure_11.20_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.20 – Airflow execution scheduler equation
  prefs: []
  type: TYPE_NORMAL
- en: If we use `datetime.now()`, it moves along with time and will never be triggered.
    We recommend using a static schedule definition, as we saw in *step 1*.
  prefs: []
  type: TYPE_NORMAL
- en: A typical error is when `start_date` and `schedule_interval` are not aligned.
    Based on the explanation of *Figure 11**.20*, we can already imagine why aligning
    these two parameters and preventing overlapping are so important. As addressed
    in *step 2*, a good way to prevent this is by using crontab notation to set `schedule_interval`.
  prefs: []
  type: TYPE_NORMAL
- en: 'A vital topic is the timezones involved in the process. If you look closely
    at the top of the Airflow UI, you will see a clock and its associated timezone,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.21 – Airflow UI clock with the timezone displayed](img/Figure_11.21_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.21 – Airflow UI clock with the timezone displayed
  prefs: []
  type: TYPE_NORMAL
- en: 'This indicates that the Airflow server is running in the UTC timezone, and
    all DAGs and tasks will be executed using the same logic. If you are working in
    a different timezone and want to ensure it will run according to your timezone,
    you can use the `pendulum` library, as you can see here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '`pendulum` is a third-party Python library that provides easy date-time manipulations
    using the built-in `datetime` Python package. You can find out more about it in
    the `pendulum` official documentation: [https://pendulum.eustace.io/](https://pendulum.eustace.io/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the last scenario has a straightforward solution: recreate the DAG
    if `schedule_interval` changes after some executions. Although this error may
    not always occur, it is a good practice to recreate the DAG to prevent further
    problems.'
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have provided in this recipe some examples of what you can check if the
    scheduler is not working, but other common errors in Airflow can happen. You can
    find out more about this on *Astronomer’s* blog page here: [https://www.astronomer.io/blog/7-common-errors-to-check-when-debugging-airflow-dag/](https://www.astronomer.io/blog/7-common-errors-to-check-when-debugging-airflow-dag/).'
  prefs: []
  type: TYPE_NORMAL
- en: In the blog, you can find other scenarios where Airflow throws a silent error
    (or an error without an explicit error message) and how to solve them.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[https://airflow.apache.org/docs/apache-airflow/stable/faq.xhtml#what-s-the-deal-with-start-date](https://airflow.apache.org/docs/apache-airflow/stable/faq.xhtml#what-s-the-deal-with-start-date)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://se.devoteam.com/expert-view/why-my-scheduled-dag-does-not-runapache-airflow-dynamic-start-date-for-equally-unequally-spaced-interval/](https://se.devoteam.com/expert-view/why-my-scheduled-dag-does-not-runapache-airflow-dynamic-start-date-for-equally-unequally-spaced-interval/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://stackoverflow.com/questions/66098050/airflow-dag-not-triggered-at-schedule-time](https://stackoverflow.com/questions/66098050/airflow-dag-not-triggered-at-schedule-time)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
