- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Automating Your Data Ingestion Pipelines
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动化您的数据摄取管道
- en: Data sources are frequently updated, and this requires us to update our data
    lake. However, with multiple sources or projects, it becomes impossible to trigger
    data pipelines manually. Data pipeline automation makes ingesting and processing
    data mechanical, obviating the human actions to trigger it. The importance of
    automation configuration lies in the ability to streamline data flow and improve
    data quality, reducing errors and inconsistency.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 数据源经常更新，这需要我们更新我们的数据湖。然而，随着多个来源或项目的增加，手动触发数据管道变得不可能。数据管道自动化使摄取和处理数据变得机械化，消除了触发它的手动操作。自动化配置的重要性在于能够简化数据流并提高数据质量，减少错误和不一致性。
- en: In this chapter, we will cover how to automate the data ingestion pipelines
    in Airflow, along with two essential topics in data engineering, data replication
    and historical data ingestion, as well as best practices.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍如何在 Airflow 中自动化数据摄取管道，以及数据工程中的两个重要主题：数据复制和历史数据摄取，以及最佳实践。
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍以下菜谱：
- en: Scheduling daily ingestions
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安排每日摄取
- en: Scheduling historical data ingestion
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安排历史数据摄取
- en: Scheduling data replication
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安排数据复制
- en: Setting up the `schedule_interval` parameter
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置 `schedule_interval` 参数
- en: Solving scheduling errors
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决调度错误
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You can find the code from this chapter in the GitHub repository at [https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_11](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_11).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从本章的 GitHub 仓库中找到代码，网址为 [https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_11](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_11)。
- en: Installing and running Airflow
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装和运行 Airflow
- en: This chapter requires that Airflow is installed on your local machine. You can
    install it directly on your **Operating System** (**OS**) or use a Docker image.
    For more information, refer to the *Configuring Docker for Airflow* recipe in
    [*Chapter 1*](B19453_01.xhtml#_idTextAnchor022).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章要求在您的本地机器上安装 Airflow。您可以直接在 **操作系统**（**OS**）上安装它，或使用 Docker 镜像。有关更多信息，请参阅
    [*第 1 章*](B19453_01.xhtml#_idTextAnchor022) 中的 *配置 Docker 以用于 Airflow* 菜谱。
- en: After following the steps described in [*Chapter 1*](B19453_01.xhtml#_idTextAnchor022),
    ensure your Airflow instance runs correctly. You can do that by checking the Airflow
    UI at `http://localhost:8080`.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在遵循 [*第 1 章*](B19453_01.xhtml#_idTextAnchor022) 中描述的步骤后，请确保您的 Airflow 实例运行正确。您可以通过检查
    `http://localhost:8080` 上的 Airflow UI 来做到这一点。
- en: 'If you are using a Docker container (as I am) to host your Airflow application,
    you can check its status in the terminal with the following command:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您像我一样使用 Docker 容器来托管您的 Airflow 应用程序，您可以使用以下命令在终端中检查其状态：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here is the status of the container:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这是容器状态：
- en: '![Figure 11.1 –  Airflow containers running](img/Figure_11.01_B19453.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.1 – 运行的 Airflow 容器](img/Figure_11.01_B19453.jpg)'
- en: Figure 11.1 – Airflow containers running
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.1 – 运行的 Airflow 容器
- en: 'Or you can check the container status on **Docker Desktop**:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以在 **Docker Desktop** 上检查容器状态：
- en: '![Figure 11.2 – Docker Desktop showing Airflow running containers](img/Figure_11.02_B19453.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.2 – Docker Desktop 显示运行中的 Airflow 容器](img/Figure_11.02_B19453.jpg)'
- en: Figure 11.2 – Docker Desktop showing Airflow running containers
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.2 – Docker Desktop 显示运行中的 Airflow 容器
- en: Scheduling daily ingestions
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安排每日摄取
- en: Data constantly changes in our dynamic world, with new information being added
    every day and even every second. Therefore, it is crucial to regularly update
    our data lake to reflect the latest scenarios and information.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们动态的世界中，数据不断变化，每天都有新的信息被添加，甚至每秒都有。因此，定期更新我们的数据湖以反映最新的场景和信息至关重要。
- en: Managing multiple projects or pipelines concurrently and manually triggering
    them while integrating new data from various sources can be daunting. To alleviate
    this issue, we can rely on schedulers, and Airflow provides a straightforward
    solution for this purpose.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在集成来自各种来源的新数据的同时，管理多个项目或管道并手动触发它们可能会很困难。为了解决这个问题，我们可以依赖调度器，Airflow 提供了一个简单直接的解决方案。
- en: In this recipe, we will create a simple **Directed Acyclic Graph** (**DAG**)
    in Airflow and explore how to use its parameters to schedule a pipeline to run
    daily.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将创建一个简单的 **有向无环图**（**DAG**）在 Airflow 中，并探讨如何使用其参数来安排管道每天运行。
- en: Getting ready
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Please refer to the *Technical requirements* section for this recipe since we
    will handle it with the same technology mentioned here.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考此菜谱的*技术要求*部分，因为我们将以这里提到的相同技术来处理它。
- en: 'In this exercise, we will create a simple DAG. The structure of your Airflow
    folder should look like the following:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将创建一个简单的DAG。你的Airflow文件夹的结构应该如下所示：
- en: '![Figure 11.3 – daily_ingestion_dag DAG folder structure](img/Figure_11.03_B19453.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图11.3 – daily_ingestion_dag DAG文件夹结构](img/Figure_11.03_B19453.jpg)'
- en: Figure 11.3 – daily_ingestion_dag DAG folder structure
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.3 – daily_ingestion_dag DAG文件夹结构
- en: All code in this recipe will be placed inside `daily_ingestion_dag.py`. Ensure
    you have created the file by following the folder structure in *Figure 11**.3*.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 此菜谱中的所有代码都将放置在`daily_ingestion_dag.py`文件中。确保你已经按照*图11.3*中的文件夹结构创建了该文件。
- en: How to do it…
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'These are the steps for this recipe:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这是此菜谱的步骤：
- en: 'Let’s start by importing the required libraries:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们先导入所需的库：
- en: '[PRE1]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, we will define `default_args` for our DAG. For the `start_date` parameter,
    insert today’s date or a few days before you are doing this exercise. For `end_date`,
    insert a date a few days ahead of today’s date. In the end, it should look like
    the following:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将为我们的DAG定义`default_args`。对于`start_date`参数，插入今天的日期或你做这个练习前几天。对于`end_date`，插入今天日期几天后的日期。最后，它应该看起来像以下这样：
- en: '[PRE2]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then, we will define our DAG and the tasks inside it. Since we want to focus
    on how to schedule daily ingestion, our tasks will each be a `BashOperator` since
    they can execute Bash commands with simplicity, as you can see here:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将定义我们的DAG及其内部的任务。由于我们想专注于如何安排每日摄取，我们的每个任务都将是一个`BashOperator`，因为它们可以简单地执行Bash命令，正如你在这里看到的：
- en: '[PRE3]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'With the DAG written, let’s enable it on the Airflow UI, and the DAG should
    run immediately. After running, the DAG will have a `SUCCESS` status, as follows:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DAG编写完成后，让我们在Airflow UI上启用它，DAG应该立即运行。运行后，DAG将有一个`SUCCESS`状态，如下所示：
- en: '![Figure 11.4 –  daily_ingestion_dag DAG in the Airflow UI](img/Figure_11.04_B19453.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图11.4 – Airflow UI中的daily_ingestion_dag DAG](img/Figure_11.04_B19453.jpg)'
- en: Figure 11.4 – daily_ingestion_dag DAG in the Airflow UI
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.4 – Airflow UI中的daily_ingestion_dag DAG
- en: 'If we check the logs, it will show the `echo` command output, similar to the
    following:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们检查日志，它将显示类似于以下内容的`echo`命令输出：
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, we need to ensure the DAG will run daily. To confirm this, select the
    **Calendar** option on your DAG page. You will see something similar to this:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要确保DAG将每天运行。为了确认这一点，在你的DAG页面上选择**日历**选项。你会看到类似以下的内容：
- en: '![Figure 11.5 – DAG’s Calendar visualization in the Airflow UI](img/Figure_11.05_B19453.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图11.5 – Airflow UI中DAG的日历可视化](img/Figure_11.05_B19453.jpg)'
- en: Figure 11.5 – DAG’s Calendar visualization in the Airflow UI
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.5 – Airflow UI中DAG的日历可视化
- en: As you can see, the execution is depicted in the shaded region to the left,
    indicating a successful outcome (`end_date`, are marked with a dot inside, indicating
    the job will run every day for the next few days.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，执行过程被描绘在左侧的阴影区域，表示成功的结果（`end_date`，用点标记，表示工作将在接下来的几天内每天运行）。
- en: Note
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '*Figure 11**.5* shows some days when the job was executed successfully. This
    is shown to users how the same calendar behaves on previous executions.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*图11.5*显示了工作成功执行的一些日子。这是向用户展示相同日历在先前执行中的行为。'
- en: How it works…
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'Airflow’s scheduler is mainly defined by three parameters: `start_date`, `end_date`,
    and `schedule_interval`. These three parameters define the beginning and end of
    the job and the interval between executions.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Airflow的调度器主要由三个参数定义：`start_date`、`end_date`和`schedule_interval`。这三个参数定义了作业的开始和结束以及执行之间的间隔。
- en: 'Let’s take a look at `default_args`:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看`default_args`：
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Since I am writing this exercise on April 12, 2023, I set my `start_date` parameter
    to the same day. This will make the job retrieve information relating to April
    12, and if I put it a few days before the current date, Airflow will retrieve
    the earlier date. Don’t worry about it now; we will cover more about this in the
    *Scheduling historical data* *ingestion recipe*.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我是在2023年4月12日写这个练习的，我将我的`start_date`参数设置为同一天。这将使工作检索与4月12日相关的信息，如果我将它放在当前日期几天前，Airflow将检索更早的日期。现在不用担心这个问题；我们将在*调度历史数据*摄取菜谱中更详细地介绍这一点。
- en: The key here is the `schedule_interval` parameter. As the name suggests, this
    parameter will define the periodicity or the interval of each execution, and,
    as you can observe, it was simply set using the `@``daily` value.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 关键在于`schedule_interval`参数。正如其名称所暗示的，该参数将定义每次执行的周期性或间隔，并且如您所观察到的，它简单地使用`@daily`值进行了设置。
- en: The **Calendar** option on the DAG UI page is an excellent feature of Airflow
    2.2 onward. This functionality allows the developers to see the next execution
    days for the DAG, preventing some confusion.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: DAG UI页面上的**日历**选项是Airflow 2.2及以后版本的一个优秀功能。此功能允许开发者查看DAG的下一个执行日期，从而避免一些混淆。
- en: There's more…
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容…
- en: 'The DAG parameters are not limited to the ones we have seen in this recipe.
    Many others are available to make the data pipeline even more automated and intelligent.
    Let’s take a look at the following code:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: DAG参数不仅限于我们在本食谱中看到的那些。还有许多其他参数可供选择，可以使数据处理管道更加自动化和智能化。让我们看一下以下代码：
- en: '[PRE6]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'There are three additional parameters here: `queue`, `pool`, and `priority_weight`.
    As we saw in [*Chapter 9*](B19453_09.xhtml#_idTextAnchor319) and [*Chapter 10*](B19453_10.xhtml#_idTextAnchor364),
    the Airflow architecture includes a queue (usually executed by `pool` parameter
    limits the number of simultaneous jobs. Finally, `priority_weight`, as the name
    suggests, defines the priority of a DAG over other DAGs.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这里还有三个额外的参数：`queue`、`pool`和`priority_weight`。正如我们在[*第9章*](B19453_09.xhtml#_idTextAnchor319)和[*第10章*](B19453_10.xhtml#_idTextAnchor364)中看到的，Airflow架构包括一个队列（通常由`pool`参数限制同时作业的数量。最后，`priority_weight`，正如其名称所暗示的，定义了DAG相对于其他DAG的优先级。
- en: 'You can read more about these parameters in the Airflow official documentation
    here:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在Airflow官方文档中了解更多关于这些参数的信息：
- en: '[https://airflow.apache.org/docs/apache-airflow/1.10.2/tutorial.xhtml](https://airflow.apache.org/docs/apache-airflow/1.10.2/tutorial.xhtml)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://airflow.apache.org/docs/apache-airflow/1.10.2/tutorial.xhtml](https://airflow.apache.org/docs/apache-airflow/1.10.2/tutorial.xhtml)'
- en: See also
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考信息
- en: You can read more about scheduling with crontab also at [https://crontab.guru/](https://crontab.guru/).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以在[https://crontab.guru/](https://crontab.guru/)上了解更多关于使用crontab进行调度的信息。
- en: Scheduling historical data ingestion
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调度历史数据摄取
- en: Historical data is vital for data-driven decisions, providing valuable insights
    and supporting decision-making processes. It can also refer to data that has been
    accumulated over a period of time. For example, a sales company can use historical
    data from previous marketing campaigns to see how they have influenced the sales
    of a specific product over the years.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 历史数据对于数据驱动决策至关重要，它提供了有价值的见解并支持决策过程。它也可以指代在一段时间内积累的数据。例如，一家销售公司可以使用以前营销活动的历史数据来查看它们如何影响特定产品多年的销售。
- en: This exercise will show how to create a scheduler in Airflow to ingest historical
    data using the best practices and common concerns related to this process.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 本练习将展示如何在Airflow中创建一个调度器，以使用最佳实践和与此过程相关的常见问题来摄取历史数据。
- en: Getting ready
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Please refer to the *Technical requirements* section for this recipe since we
    will handle it with the same technology mentioned here.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考本食谱的**技术要求**部分，因为我们将会使用这里提到的相同技术来处理它。
- en: 'In this exercise, we will create a simple DAG inside our DAGs folder. The structure
    of your Airflow folder should look like the following:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将在我们的DAGs文件夹内创建一个简单的DAG。您的Airflow文件夹结构应该如下所示：
- en: '![Figure 11.6 – historical_data_dag folder structure in your local Airflow
    directory](img/Figure_11.06_B19453.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图11.6 – 本地Airflow目录中historical_data_dag文件夹结构](img/Figure_11.06_B19453.jpg)'
- en: Figure 11.6 – historical_data_dag folder structure in your local Airflow directory
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.6 – 本地Airflow目录中historical_data_dag文件夹结构
- en: How to do it…
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作…
- en: 'Here are the steps for this recipe:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是这个食谱的步骤：
- en: 'Let’s start by importing our libraries:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从导入我们的库开始：
- en: '[PRE7]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, let’s define `default_args`. Since we wish to process old data, I will
    set `datetime` for `start_date` before the current day, and `end_date` will be
    near the current day. See the following code:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们定义`default_args`。由于我们希望处理旧数据，我将设置`start_date`的`datetime`在当前日期之前，而`end_date`将接近当前日期。以下代码展示了这一点：
- en: '[PRE8]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then, we will create a simple function to print the date Airflow used to execute
    the pipeline. You can see it here:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将创建一个简单的函数来打印Airflow执行管道所使用的日期。您可以在以下位置看到它：
- en: '[PRE9]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Finally, we will declare our DAG parameters and a `PythonOperator` task to
    execute it, as you can see here:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将声明我们的DAG参数和一个`PythonOperator`任务来执行它，正如您所看到的：
- en: '[PRE10]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Heading to the Airflow UI, let’s proceed with the usual steps to enable the
    DAG and see its execution. On the `historical_data_dag` page, you should see something
    similar to the following screenshot:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转向Airflow UI，让我们按照常规步骤启用DAG并查看其执行情况。在`historical_data_dag`页面上，你应该看到以下截图类似的内容：
- en: '![Figure 11.7 – historical_data_dag DAG in the Airflow UI](img/Figure_11.07_B19453.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图11.7 – Airflow UI中的historical_data_dag DAG](img/Figure_11.07_B19453.jpg)'
- en: Figure 11.7 – historical_data_dag DAG in the Airflow UI
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.7 – Airflow UI中的historical_data_dag DAG
- en: As you can see, the task ran with success.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，任务成功运行。
- en: 'Now, let’s check our `logs` folder. If we select the folder with the same name
    as the DAG we created (`historical_data_dag`), we will observe `run_id` instances
    on different days, beginning on April 2 and finishing on April 10:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们检查我们的`logs`文件夹。如果我们选择与创建的DAG同名（`historical_data_dag`）的文件夹，我们将观察到不同日期的`run_id`实例，从4月2日开始，到4月10日结束：
- en: '![Figure 11.8 – Airflow logs folder showing retroactive ingestion](img/Figure_11.08_B19453.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图11.8 – 显示回溯摄入的Airflow日志文件夹](img/Figure_11.08_B19453.jpg)'
- en: Figure 11.8 – Airflow logs folder showing retroactive ingestion
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.8 – 显示回溯摄入的Airflow日志文件夹
- en: 'Let’s open the first `run_id` folder to explore the log for that run:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们打开第一个`run_id`文件夹，以探索该次运行的日志：
- en: '![Figure 11.9 – DAG log for April 2, 2023](img/Figure_11.09_B19453.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图11.9 – 2023年4月2日的DAG日志](img/Figure_11.09_B19453.jpg)'
- en: Figure 11.9 – DAG log for April 2, 2023
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.9 – 2023年4月2日的DAG日志
- en: The log tells us the `execution_date` parameter, which is the same as the `start_date`
    parameter.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 日志告诉我们`execution_date`参数，它与`start_date`参数相同。
- en: 'Here is a closer look at the logs:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是日志的更详细查看：
- en: '[PRE11]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We will observe the same pattern for the `run_id` for April 3:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将观察到4月3日`run_id`相同的模式：
- en: '![Figure 11.10 – DAG log for April 3, 2023](img/Figure_11.10_B19453.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图11.10 – 2023年4月3日的DAG日志](img/Figure_11.10_B19453.jpg)'
- en: Figure 11.10 – DAG log for April 3, 2023
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.10 – 2023年4月3日的DAG日志
- en: 'Here is a closer look at the log output:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是日志输出的更详细查看：
- en: '[PRE12]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The `execution_date` also refers to April 3.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`execution_date`也指的是4月3日。'
- en: This shows us that Airflow has used the interval declared on `start_date` and
    `end_date` to run the task!
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明Airflow已经使用了在`start_date`和`end_date`上声明的间隔来运行任务！
- en: Now, let’s proceed to understand how the scheduler works.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续了解调度器是如何工作的。
- en: How it works…
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'As we saw, scheduling and retrieving historical data with Airflow is straightforward,
    and the key parameters were `start_date`, `end_date`, and `schedule_interval`.
    Let’s discuss them in a little more detail:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，使用Airflow安排和检索历史数据很简单，关键参数是`start_date`、`end_date`和`schedule_interval`。让我们更详细地讨论它们：
- en: The `start_date` parameter defines the first date Airflow will look at when
    the pipeline is triggered. In our case, it was April 2.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start_date`参数定义了当管道被触发时Airflow将查看的第一个日期。在我们的例子中，它是4月2日。'
- en: Next is `end_date`. Usually, this is not a mandatory parameter, even for recurrent
    ingests. However, the purpose of using it was to show that we can set a date as
    a limit to stop the ingestion.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来是`end_date`。通常，即使对于周期性摄入，这也不是一个强制参数。然而，使用它的目的是为了展示我们可以设置一个日期作为停止摄入的限制。
- en: Finally, `schedule_interval` dictates the interval between two dates. We used
    a daily interval in this exercise, but we could also use `crontab` if we needed
    more granular historical ingestion. We will explore this in more detail in the
    *Setting up the schedule_interval* *parameter* recipe.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，`schedule_interval`决定了两个日期之间的间隔。在这个练习中，我们使用了日间隔，但如果我们需要更细粒度的历史摄入，我们也可以使用`crontab`。我们将在*设置schedule_interval参数*菜谱中更详细地探讨这一点。
- en: 'With this information, it is easier to understand the logs we got from Airflow:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个信息，理解我们从Airflow获得的日志就更容易了：
- en: '![Figure 11.11 – Airflow logs folder showing historic ingestion](img/Figure_11.11_B19453.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图11.11 – 显示历史摄入的Airflow日志文件夹](img/Figure_11.11_B19453.jpg)'
- en: Figure 11.11 – Airflow logs folder showing historic ingestion
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.11 – 显示历史摄入的Airflow日志文件夹
- en: Each folder represents one historical ingestion that occurred at a daily interval.
    Since we did not define a more granular date-time specification, the folder name
    uses the time that the job was triggered. This information is not included in
    the logs.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 每个文件夹代表每天发生一次的历史摄入。由于我们没有定义更细粒度的日期时间规范，文件夹名称使用的是作业被触发的时间。这些信息不包括在日志中。
- en: 'To show what date Airflow was using behind the scenes, we created a simple
    function:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示Airflow在幕后使用的是哪个日期，我们创建了一个简单的函数：
- en: '[PRE13]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The only purpose of the function is to show the execution date of the task.
    The `execution_date` parameter is an internal parameter that displays when a task
    is executed and can be used by operators or other functions to execute something
    based on a date.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 函数的唯一目的是显示任务的执行日期。`execution_date` 参数是一个内部参数，显示任务何时执行，并且可以被操作员或其他函数用于根据日期执行某些操作。
- en: For example, let’s say we need to retrieve historical data stored as a partition.
    We can use `execution_date` to pass the date-time information to a Spark function,
    which will read and retrieve data from that partition with the same date information.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们需要检索存储为分区的历史数据。我们可以使用 `execution_date` 将日期时间信息传递给 Spark 函数，该函数将读取并检索具有相同日期信息的分区中的数据。
- en: As you can see, retrieving historical data/information in Airflow requires a
    few configurations. A good practice is to have a separate and dedicated DAG for
    historical data processing so that current data ingestion is not impaired. Also,
    if it is necessary to reprocess data, we can do it with a few parameter changes.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，在 Airflow 中检索历史数据/信息需要一些配置。一个好的做法是为历史数据处理保留一个单独且专门的 DAG，这样就不会影响当前的数据摄取。此外，如果需要重新处理数据，我们可以通过一些参数更改来完成。
- en: There's more…
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Inside the technique of ingesting historical data using Airflow are two important
    concepts: *catchup* and *backfill*.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 Airflow 摄取历史数据的技巧中，有两个重要的概念：*catchup* 和 *backfill*。
- en: 'Scheduling and running DAGs for past periods that may have been missed for
    various reasons is commonly known as `schedule_interval`. By default, this feature
    is enabled in Airflow. Therefore, if a paused or uncreated DAG’s `start_date`
    lies in the past, it will be automatically scheduled and executed for missed time
    intervals. The following diagram illustrates this:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 调度和运行可能因各种原因而遗漏的过去时期的 DAG，通常被称为 `schedule_interval`。默认情况下，此功能在 Airflow 中已启用。因此，如果暂停或未创建的
    DAG 的 `start_date` 在过去，它将自动为遗漏的时间间隔进行调度和执行。以下图表说明了这一点：
- en: '![Figure 11.12 – Airflow catchup timeline. Source: https://medium.com/nerd-for-tech/airflow-catchup-backfill-demystified-355def1b6f92](img/Figure_11.12_B19453.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.12 – Airflow 补充时间线。来源：https://medium.com/nerd-for-tech/airflow-catchup-backfill-demystified-355def1b6f92](img/Figure_11.12_B19453.jpg)'
- en: 'Figure 11.12 – Airflow catchup timeline. Source: https://medium.com/nerd-for-tech/airflow-catchup-backfill-demystified-355def1b6f92'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.12 – Airflow 补充时间线。来源：https://medium.com/nerd-for-tech/airflow-catchup-backfill-demystified-355def1b6f92
- en: On the other hand, Airflow’s *backfill* functionality allows you to execute
    DAGs retroactively, along with their associated tasks for past periods that may
    have been missed due to the DAG being paused, not yet created, or for any other
    reason. Backfilling in Airflow is a powerful feature that helps you to fill the
    gaps and catch up on data processing or workflow execution that may have been
    missed in the past.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，Airflow 的 *backfill* 功能允许您对过去可能因 DAG 暂停、尚未创建或其他任何原因而遗漏的 DAG 执行回溯性执行，以及它们相关的任务。在
    Airflow 中，回填是一个强大的功能，可以帮助您填补空白并赶上过去可能遗漏的数据处理或工作流程执行。
- en: 'You can read more about it on *Amit Singh Rathore’s* blog page here: [https://medium.com/nerd-for-tech/airflow-catchup-backfill-demystified-355def1b6f92](https://medium.com/nerd-for-tech/airflow-catchup-backfill-demystified-355def1b6f92).'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在以下链接的 Amit Singh Rathore 的博客页面上了解更多信息：[https://medium.com/nerd-for-tech/airflow-catchup-backfill-demystified-355def1b6f92](https://medium.com/nerd-for-tech/airflow-catchup-backfill-demystified-355def1b6f92).
- en: Scheduling data replication
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调度数据复制
- en: In the first chapter of this book, we covered what data replication is and why
    it’s important. We saw how vital this process is in the prevention of data loss
    and in promoting recovery from disasters.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的第一章中，我们介绍了数据复制是什么以及为什么它很重要。我们看到了这个过程在防止数据丢失和促进灾难恢复中的重要性。
- en: Now, it is time to learn how to create an optimized schedule window to make
    data replication happen. In this recipe, we will create a diagram to help us decide
    the best moment to replicate our data.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候学习如何创建一个优化的调度窗口，以便数据复制能够发生。在这个菜谱中，我们将创建一个图表，帮助我们决定复制数据的最佳时机。
- en: Getting ready
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: This exercise does not require technical preparation. However, to make it closer
    to a real scenario, let’s imagine we need to decide the best way to ensure the
    data from a hospital is being adequately replicated.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这个练习不需要技术准备。然而，为了使其更接近真实场景，让我们假设我们需要决定最佳方式来确保医院的数据得到充分复制。
- en: 'We will have two pipelines: one holding patient information and another with
    financial information. The first pipeline collects information from a patient
    database and synthesizes it into readable reports used by the medical team. The
    second will feed an internal dashboard used by the hospital executives.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将有两个管道：一个包含患者信息，另一个包含财务信息。第一个管道从患者数据库收集信息，并将其综合成医疗团队使用的可读报告。第二个管道将为医院管理层使用的内部仪表板提供数据。
- en: 'Due to infrastructure limitations, the operations team has only one requirement:
    only one pipeline can have its data replicated quickly.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 由于基础设施限制，运维团队只有一个要求：只能快速复制一个管道的数据。
- en: How to do it…
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Here are the steps for this recipe:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是这个菜谱的步骤：
- en: '**Identify the targets to replicate**: As described in the *Getting ready*
    section, we have identified the target data, which are the pipeline holding patient
    information, and the pipeline with financial data to feed a dashboard.'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**确定要复制的目标**：如“准备就绪”部分所述，我们已经确定了目标数据，即包含患者信息的管道和用于仪表板的财务数据管道。'
- en: However, suppose this information is not coming promptly from stakeholders or
    other relevant people. In that case, we must always start by identifying our project’s
    most critical tables or databases.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果这个信息不是从利益相关者或其他相关人士那里及时获得的，那么我们必须始终从识别我们项目中最关键的表或数据库开始。
- en: '**Replication periodicity**: We must define the replication schedule based
    on our data’s criticality or relevance. Let’s take a look at the following diagram:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**复制周期性**：我们必须根据我们数据的紧迫性或相关性来定义复制计划。让我们看看下面的图表：'
- en: '![Figure 11.13 – Periodicity of data replication](img/Figure_11.13_B19453.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图11.13 – 数据复制的周期性](img/Figure_11.13_B19453.jpg)'
- en: Figure 11.13 – Periodicity of data replication
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.13 – 数据复制的周期性
- en: As we can see, the more critical the data is, the more frequently the replication
    is recommended. In our scenario, the patient reports would fit better with 30
    minutes to 3 hours after the ingestion, while the financial data can be replicated
    until 24 hours have passed.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，数据越重要，建议的复制频率就越高。在我们的场景中，患者报告更适合在摄入后30分钟到3小时内进行，而财务数据可以复制到24小时后。
- en: '**Set a schedule window for replication**: Now, we need to create a schedule
    window to replicate the data. This replication decision needs to take into consideration
    two important factors, as you can see in the following diagram:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**设置复制的时间窗口**：现在，我们需要创建一个复制数据的时间窗口。这个复制决策需要考虑两个重要因素，如下面的图表所示：'
- en: '![Figure 11.14 – Replication window](img/Figure_11.14_B19453.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图11.14 – 复制窗口](img/Figure_11.14_B19453.jpg)'
- en: Figure 11.14 – Replication window
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.14 – 复制窗口
- en: Based on the two pipelines (and remembering we need to prioritize one), the
    suggestion would be to replicate the financial data every day after business working
    hours, while the patient data can be done at the same time as new information
    arrives.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 基于两条管道（并记住我们需要优先考虑其中一条），建议在业务工作结束后每天复制一次财务数据，而患者数据可以在新信息到达时同时处理。
- en: Don’t worry if this seems a bit confusing. Let’s explore the details in the
    *How it* *works* section.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这看起来有点混乱，请不要担心。让我们在“如何它工作”部分中探讨细节。
- en: How it works…
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: Data replication is a vital process that ensures data availability and disaster
    recovery. Its concept is older than the current ETL process and has been used
    for many years in on-premises databases. Our advantage today is that we can carry
    out this process at any moment. In contrast, replication had a strict schedule
    window a few years ago due to hardware limitations.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 数据复制是一个确保数据可用性和灾难恢复的重要过程。其概念比当前的ETL过程更早，并且在本地数据库中已经使用了多年。我们今天的优势在于我们可以在任何时刻执行此过程。相比之下，由于硬件限制，复制在几年前有一个严格的调度窗口。
- en: In our example, we handled two pipelines that had distinct severity levels.
    The idea behind this is to teach attentive eyes to decide when doing each replication.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们处理了两个具有不同严重程度的管道。背后的想法是教会有警觉性的眼睛决定何时进行每次复制。
- en: The first pipeline, which is the patient reports pipeline, handles sensitive
    data such as personal information and medical history. It also may be helpful
    for doctors and other health workers to help a patient.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个管道，即患者报告管道，处理敏感数据，如个人信息和医疗历史。它还可能有助于医生和其他卫生工作者帮助患者。
- en: Based on this, the best approach is to replicate this data within a few minutes
    or hours of it being processed, allowing high availability and redundancy.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 基于此，最佳做法是在数据处理后的几分钟或几小时内复制这些数据，以实现高可用性和冗余。
- en: At first look, the financial data seems to be very critical and demands fast
    replication; we need to remember this pipeline feeds data to a dashboard, and
    therefore, an analyst can use the raw data to generate reports.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 初看之下，财务数据似乎非常关键，需要快速复制；我们需要记住这个管道为仪表板提供数据，因此，分析师可以使用原始数据生成报告。
- en: The decision to schedule data replication must consider other factors besides
    the data involved. It is also essential to understand who is interested in or
    needs to access the data and how it impacts the project, area, or business when
    it becomes unavailable.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 决定调度数据复制时，除了涉及的数据之外，还必须考虑其他因素。了解谁对数据感兴趣或需要访问数据，以及当数据不可用时它如何影响项目、区域或业务，也是至关重要的。
- en: There's more…
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'This recipe covered a simple example of setting a scheduling agenda for data
    replication. We also covered in *step 3* the two main points to have in mind when
    doing so. Nevertheless, many other factors can influence the scheduler’s performance
    and execution. A few examples are as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱涵盖了设置数据复制调度议程的简单示例。我们还在*步骤3*中讨论了进行此类操作时需要考虑的两个主要点。尽管如此，许多其他因素都可能影响调度器的性能和执行。以下是一些例子：
- en: Where Airflow (or a similar platform) is hosted on a server
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Airflow（或类似平台）托管在服务器上的位置
- en: The CPU capacity
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPU容量
- en: The number of schedulers
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调度器的数量
- en: Networking throughput
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络吞吐量
- en: 'If you want to know more about it, you can find a complete list of these factors
    in the Airflow documentation: [https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/scheduler.xhtml#what-impacts-scheduler-s-performance](https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/scheduler.xhtml#what-impacts-scheduler-s-performance).'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多信息，你可以在Airflow文档中找到这些因素的完整列表：[https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/scheduler.xhtml#what-impacts-scheduler-s-performance](https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/scheduler.xhtml#what-impacts-scheduler-s-performance)。
- en: The great thing about this documentation is that many points also apply to other
    data pipeline processors and can serve as a guide.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这份文档的伟大之处在于，其中许多点也适用于其他数据处理程序，并可以作为指南。
- en: Setting up the schedule_interval parameter
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置`schedule_interval`参数
- en: One of the most widely used parameters in Airflow DAG scheduler configuration
    is `schedule_interval`. Together with `start_date`, it creates a dynamic and continuous
    trigger for the pipeline. However, there are some small details we need to pay
    attention to when setting `schedule_interval`.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在Airflow DAG调度器配置中最广泛使用的参数之一是`schedule_interval`。与`start_date`一起，它为管道创建了一个动态和连续的触发器。然而，在设置`schedule_interval`时，我们仍需注意一些小细节。
- en: This recipe will cover different forms to set up the `schedule_interval` parameter.
    We will also explore a practical example to see how the scheduling window works
    in Airflow, making it more straightforward to manage pipeline executions.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱将涵盖设置`schedule_interval`参数的不同形式。我们还将探讨一个实际示例，以了解Airflow中的调度窗口是如何工作的，从而使管理管道执行更加简单。
- en: Getting ready
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: While this exercise does not require any technical preparation, it is recommended
    to take notes about when the pipeline is supposed to start and the interval between
    each trigger.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这项练习不需要任何技术准备，但建议记录下管道应该开始的时间和每个触发器之间的间隔。
- en: How to do it…
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'Here, we will show only the `default_args` dictionary to avoid code redundancy.
    However, you can always check out the complete code in the GitHub repository:
    [https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_11/settingup_schedule_interval](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_11/settingup_schedule_interval).'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将仅展示`default_args`字典以避免代码冗余。然而，你始终可以在GitHub仓库中查看完整的代码：[https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_11/settingup_schedule_interval](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_11/settingup_schedule_interval)。
- en: 'Let’s see how we can declare `schedule_interval`:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何声明`schedule_interval`：
- en: '`schedule_interval` value is by using accessible names such as `@daily`, `@hourly`,
    or `@weekly`. See what it looks like in the following code:'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`schedule_interval`的值可以使用诸如`@daily`、`@hourly`或`@weekly`这样的可访问名称。以下代码展示了它的样子：'
- en: '[PRE14]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '`schedule_interval` using crontab notation:'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 crontab 语法表示的 `schedule_interval`：
- en: '[PRE15]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In this case, we set the scheduler to start every weekday, from Monday to Friday,
    at 10 pm (or 22:00 hours).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们将调度器设置为每周工作日（周一至周五）晚上10点（或22:00小时）启动。
- en: '`schedule_interval` is by using the `timedelta` method. In the following code,
    we can set the pipeline to trigger with an interval of one day between each execution:'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`schedule_interval` 是通过使用 `timedelta` 方法实现的。在下面的代码中，我们可以设置管道以一天为间隔触发：'
- en: '[PRE16]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: How it works…
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: The `schedule_interval` parameter is an essential aspect of scheduling DAGs
    in Airflow and provides a flexible way to define how frequently your workflows
    should be executed. We can think of it as the core of the Airflow scheduler.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '`schedule_interval` 参数是 Airflow 中调度 DAG 的一个重要方面，它提供了一种灵活的方式来定义你的工作流程应该多久执行一次。我们可以将其视为
    Airflow 调度器的核心。'
- en: 'The goal of this recipe was to show the different ways to set `schedule_interval`
    and when to use each of them. Let’s explore them in more depth:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这个菜谱的目标是展示设置 `schedule_interval` 的不同方法以及何时使用它们。让我们更深入地探讨它们：
- en: '**Friendly names:** As the name suggests, this notation uses user-friendly
    labels or aliases. It provides an easy and convenient way to specify the exact
    time and date for scheduled tasks to run. It can be an easy and simple solution
    if you don’t have a specific date-time to run the scheduler.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**友好名称**：正如其名所示，这种表示法使用用户友好的标签或别名。它提供了一种简单方便的方式来指定计划任务运行的精确时间和日期。如果你没有特定的日期和时间来运行调度器，这可能是一个简单且直接的解决方案。'
- en: '**Crontab notation:** Crontabs have long been widely used across applications
    and systems. Crontab notation consists of five fields, representing the minute,
    hour, day of the month, month, and day of the week. It is a great choice when
    handling complex schedules, for example, executing the trigger at 1 p.m. on Mondays
    and Fridays, or other combinations.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Crontab 语法**：Crontab 在应用程序和系统中已经广泛使用很长时间了。Crontab 语法由五个字段组成，分别代表分钟、小时、月份中的日期、月份和星期几。当处理复杂的调度时，这是一个很好的选择，例如，在星期一和星期五下午1点执行触发器，或其他组合。'
- en: '`timedelta(minutes=5)`). It is also a user-friendly notation but with more
    granular power.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`timedelta(minutes=5)`）。它也是一个用户友好的表示法，但具有更细粒度的功能。'
- en: Note
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Although we have seen three ways to set the `schedule_interval` here, remember
    Airflow is not a streaming solution, and having multiple DAGs running with a small
    interval between them can overload the server. Consider using a streaming tool
    if you or your team needs to schedule ingestions every 10-30 minutes, or less.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们已经在这里看到了三种设置 `schedule_interval` 的方法，但请记住，Airflow 不是一个流式解决方案，并且当有多个 DAG
    以小间隔运行时，可能会过载服务器。如果你或你的团队需要每10-30分钟或更短的时间进行调度，请考虑使用流式工具。
- en: See also
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '*TowardsDataScience* has a fantastic blog post about how `schedule_interval`
    works behind the scenes. You can find it here: [https://towardsdatascience.com/airflow-schedule-interval-101-bbdda31cc463](https://towardsdatascience.com/airflow-schedule-interval-101-bbdda31cc463).'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '*TowardsDataScience* 有一个关于 `schedule_interval` 在幕后如何工作的精彩博客文章。你可以在这里找到它：[https://towardsdatascience.com/airflow-schedule-interval-101-bbdda31cc463](https://towardsdatascience.com/airflow-schedule-interval-101-bbdda31cc463)。'
- en: Solving scheduling errors
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解决调度错误
- en: At this point, you may have already experienced some issues with scheduling
    pipelines not being triggered as expected. If not, don’t worry; it will happen
    sometime and is totally normal. With several pipelines running in parallel, in
    different windows, or attached to different timezones, it is expected to be entangled
    with one or another.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，你可能已经遇到了一些问题，即管道调度没有按预期触发。如果没有，请不要担心；这迟早会发生，而且是完全正常的。当有多个管道并行运行，在不同的窗口或连接到不同的时区时，出现一些纠缠是预料之中的。
- en: To avoid this entanglement, in this exercise, we will create a diagram to assist
    in the debugging process, identify the possible causes of a scheduler not working
    correctly in Airflow, and see how to solve it.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这种纠缠，在这个练习中，我们将创建一个图表来辅助调试过程，识别 Airflow 中调度器不正确工作的可能原因，并了解如何解决它。
- en: Getting ready
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: This recipe does not require any technical preparation. Nevertheless, taking
    notes and writing down the steps we will follow here can be helpful. Writing when
    learning something new can help to fix the knowledge in our minds, making it easier
    to remember later.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这个菜谱不需要任何技术准备。尽管如此，记录下我们将遵循的步骤可能会有所帮助。在学习新事物时记录下来可以帮助我们在脑海中固定知识，使其更容易在以后记住。
- en: 'Back to our exercise; scheduler errors in Airflow typically give the DAG status
    `None`, as shown here:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的练习；Airflow 中的调度器错误通常会使 DAG 状态显示为 `None`，如下所示：
- en: '![Figure 11.15 – DAG in the Airflow UI with an error in the scheduler](img/Figure_11.15_B19453.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.15 – Airflow UI 中的 DAG，调度器存在错误](img/Figure_11.15_B19453.jpg)'
- en: Figure 11.15 – DAG in the Airflow UI with an error in the scheduler
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.15 – Airflow UI 中的 DAG，调度器存在错误
- en: We will now find out how to fix the error and make the job run again.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将找出如何修复错误并再次运行作业。
- en: How to do it…
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'Let’s try to identify what could be the cause of the error in our scheduler.
    Don’t worry about understanding why we used the approaches that we have. We will
    cover them in detail in *How* *it works*:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试确定我们的调度器中错误可能的原因。不用担心理解我们为什么使用这些方法。我们将在 *如何工作* 中详细说明：
- en: 'We can first check whether `start_date` has been set to `datetime.now()`. If
    this is the case, the best approach here is to change this parameter value to
    a specific date, as you can see here:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先检查 `start_date` 是否已设置为 `datetime.now()`。如果是这种情况，这里最好的方法是将此参数值更改为一个特定日期，如下所示：
- en: '![Figure 11.16 – Error caused by start_date parameter](img/Figure_11.16_B19453.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.16 – 由 start_date 参数引起的错误](img/Figure_11.16_B19453.jpg)'
- en: Figure 11.16 – Error caused by start_date parameter
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.16 – 由 start_date 参数引起的错误
- en: 'The code will look like this:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 代码将看起来像这样：
- en: '[PRE17]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now we can verify whether `schedule_interval` is aligned with the `start_date`
    parameter. In the following diagram, you can see three possibilities to fix the
    issue:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以验证 `schedule_interval` 是否与 `start_date` 参数对齐。在下面的图中，你可以看到三种修复问题的可能性：
- en: '![Figure 11.17 – Error caused by the start_date and schedule_interval parameters](img/Figure_11.17_B19453.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.17 – 由 start_date 和 schedule_interval 参数引起的错误](img/Figure_11.17_B19453.jpg)'
- en: Figure 11.17 – Error caused by the start_date and schedule_interval parameters
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.17 – 由 start_date 和 schedule_interval 参数引起的错误
- en: 'You can prevent this error by using crontab notation in `schedule_interval`,
    as follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过在 `schedule_interval` 中使用 crontab 语法来防止此错误，如下所示：
- en: '[PRE18]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'If you are facing problems with the timezone, you can define which timezone
    Airflow will trigger the job in by using the `pendulum` library:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你遇到时区问题，你可以通过使用 `pendulum` 库来定义 Airflow 将在哪个时区触发作业：
- en: '[PRE19]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Finally, another standard error scenario is when `schedule_interval` changes
    after the DAG has been running for some time. In this case, the solution usually
    is to recreate the DAG:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，另一个标准的错误场景是 DAG 运行一段时间后 `schedule_interval` 发生变化。在这种情况下，通常的解决方案是重新创建 DAG：
- en: '![Figure 11.18 – Error caused by a change in schedule_interval](img/Figure_11.18_B19453.jpg)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.18 – 由 schedule_interval 变化引起的错误](img/Figure_11.18_B19453.jpg)'
- en: Figure 11.18 – Error caused by a change in schedule_interval
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.18 – 由 schedule_interval 变化引起的错误
- en: 'At the end of these steps, we will end up with a debug diagram similar to this:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些步骤结束时，我们将得到一个类似的调试图：
- en: '![Figure 11.19 – Diagram to assist in identifying an error caused in the Airflow
    scheduler](img/Figure_11.19_B19453.jpg)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.19 – 帮助识别 Airflow 调度器中错误引起的图](img/Figure_11.19_B19453.jpg)'
- en: Figure 11.19 – Diagram to assist in identifying an error caused in the Airflow
    scheduler
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.19 – 帮助识别 Airflow 调度器中错误引起的图
- en: How it works…
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: As you can see, the goal of this recipe was to show three different scenarios
    in which it is common to observe errors related to the scheduler. Errors in the
    scheduler normally lead to a DAG status as `None`, as we saw in the *Getting ready*
    section. However, having a trigger that does not behave as expected is also considered
    an error. Now, let’s explore the three addressed scenarios and their solutions.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这个菜谱的目标是展示三种常见的与调度器相关的错误场景。调度器中的错误通常会导致 DAG 状态显示为 `None`，正如我们在 *准备就绪* 部分所看到的。然而，一个不符合预期行为的触发器也被认为是错误。现在，让我们探讨这三种解决场景及其解决方案。
- en: 'The first scenario usually occurs when we want to use the current date for
    `start_date`. Although it seems like a good idea to use the `datetime.now()` function
    to represent the current date-time, Airflow will not interpret it as we do. The
    `datetime.now()` function will create what we call a *dynamic scheduler*, and
    the trigger will never be executed. It happens because the execution schedule
    uses `start_date` and `schedule_interval` to know when to execute the trigger,
    as you can see here:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个场景通常发生在我们想要为`start_date`使用当前日期时。虽然使用`datetime.now()`函数来表示当前日期时间似乎是个好主意，但Airflow不会像我们那样解释它。`datetime.now()`函数将创建我们所说的*动态调度器*，触发器永远不会被执行。这是因为执行调度使用`start_date`和`schedule_interval`来确定何时执行触发器，正如您在这里看到的：
- en: '![Figure 11.20 – Airflow execution scheduler equation](img/Figure_11.20_B19453.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![图11.20 – Airflow执行调度方程](img/Figure_11.20_B19453.jpg)'
- en: Figure 11.20 – Airflow execution scheduler equation
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.20 – Airflow执行调度方程
- en: If we use `datetime.now()`, it moves along with time and will never be triggered.
    We recommend using a static schedule definition, as we saw in *step 1*.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用`datetime.now()`，它将随时间移动，永远不会被触发。我们建议使用静态调度定义，正如我们在*步骤1*中看到的。
- en: A typical error is when `start_date` and `schedule_interval` are not aligned.
    Based on the explanation of *Figure 11**.20*, we can already imagine why aligning
    these two parameters and preventing overlapping are so important. As addressed
    in *step 2*, a good way to prevent this is by using crontab notation to set `schedule_interval`.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的错误是当`start_date`和`schedule_interval`不匹配时。根据*图11.20*的解释，我们已能想象到为什么使这两个参数对齐并防止重叠如此重要。正如*步骤2*中提到的，防止这种情况的一个好方法是通过使用crontab记法来设置`schedule_interval`。
- en: 'A vital topic is the timezones involved in the process. If you look closely
    at the top of the Airflow UI, you will see a clock and its associated timezone,
    as follows:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的话题是涉及过程中的时区。如果您仔细查看Airflow UI的顶部，您将看到一个时钟及其关联的时区，如下所示：
- en: '![Figure 11.21 – Airflow UI clock with the timezone displayed](img/Figure_11.21_B19453.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![图11.21 – 显示时区的Airflow UI时钟](img/Figure_11.21_B19453.jpg)'
- en: Figure 11.21 – Airflow UI clock with the timezone displayed
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.21 – 显示时区的Airflow UI时钟
- en: 'This indicates that the Airflow server is running in the UTC timezone, and
    all DAGs and tasks will be executed using the same logic. If you are working in
    a different timezone and want to ensure it will run according to your timezone,
    you can use the `pendulum` library, as you can see here:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明Airflow服务器正在UTC时区运行，所有DAG和任务都将使用相同的逻辑执行。如果您在不同的时区工作并希望确保它将根据您的时区运行，您可以使用`pendulum`库，如下所示：
- en: '[PRE20]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '`pendulum` is a third-party Python library that provides easy date-time manipulations
    using the built-in `datetime` Python package. You can find out more about it in
    the `pendulum` official documentation: [https://pendulum.eustace.io/](https://pendulum.eustace.io/).'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '`pendulum`是一个第三方Python库，它使用内置的`datetime`Python包提供简单的日期时间操作。您可以在`pendulum`官方文档中了解更多信息：[https://pendulum.eustace.io/](https://pendulum.eustace.io/)。'
- en: 'Finally, the last scenario has a straightforward solution: recreate the DAG
    if `schedule_interval` changes after some executions. Although this error may
    not always occur, it is a good practice to recreate the DAG to prevent further
    problems.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，最后一个场景有一个简单的解决方案：如果`schedule_interval`在执行一些操作后发生变化，则重新创建DAG。尽管这种错误可能并不总是发生，但重新创建DAG以防止进一步的问题是良好的实践。
- en: There’s more…
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'We have provided in this recipe some examples of what you can check if the
    scheduler is not working, but other common errors in Airflow can happen. You can
    find out more about this on *Astronomer’s* blog page here: [https://www.astronomer.io/blog/7-common-errors-to-check-when-debugging-airflow-dag/](https://www.astronomer.io/blog/7-common-errors-to-check-when-debugging-airflow-dag/).'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这个菜谱中提供了一些示例，说明如果调度器不工作，您可以检查的内容，但Airflow中可能发生的其他常见错误。您可以在以下*Astronomer*博客页面上了解更多信息：[https://www.astronomer.io/blog/7-common-errors-to-check-when-debugging-airflow-dag/](https://www.astronomer.io/blog/7-common-errors-to-check-when-debugging-airflow-dag/)。
- en: In the blog, you can find other scenarios where Airflow throws a silent error
    (or an error without an explicit error message) and how to solve them.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在博客中，您可以找到其他一些场景，其中Airflow会抛出静默错误（或没有明确错误消息的错误）以及如何解决它们。
- en: Further reading
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '[https://airflow.apache.org/docs/apache-airflow/stable/faq.xhtml#what-s-the-deal-with-start-date](https://airflow.apache.org/docs/apache-airflow/stable/faq.xhtml#what-s-the-deal-with-start-date)'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[关于Airflow的FAQ：关于开始日期的处理](https://airflow.apache.org/docs/apache-airflow/stable/faq.xhtml#what-s-the-deal-with-start-date)'
- en: '[https://se.devoteam.com/expert-view/why-my-scheduled-dag-does-not-runapache-airflow-dynamic-start-date-for-equally-unequally-spaced-interval/](https://se.devoteam.com/expert-view/why-my-scheduled-dag-does-not-runapache-airflow-dynamic-start-date-for-equally-unequally-spaced-interval/)'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[为什么我的预定DAG没有运行？Apache Airflow的动态开始日期用于等距或不等距间隔](https://se.devoteam.com/expert-view/why-my-scheduled-dag-does-not-runapache-airflow-dynamic-start-date-for-equally-unequally-spaced-interval/)'
- en: '[https://stackoverflow.com/questions/66098050/airflow-dag-not-triggered-at-schedule-time](https://stackoverflow.com/questions/66098050/airflow-dag-not-triggered-at-schedule-time)'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Airflow DAG在预定时间未触发的解决方案](https://stackoverflow.com/questions/66098050/airflow-dag-not-triggered-at-schedule-time)'
