["```py\n    tar -xvzf spark-3.0.0-bin-hadoop2.7.tgz\n    mv spark-3.0.0-bin-hadoop2.7 ~/spark3\n    ```", "```py\n    cp -r spark3/ spark-node\n    ```", "```py\n    cd ~/spark3/sbin\n    cp start-master.sh start-head.sh\n    cd ~/spark-node/sbin\n    cp start-slave.sh start-node.sh\n    ```", "```py\n    ./start-head.sh\n    ./start-node.sh spark://pop-os.localdomain:7077 -p 9911\n    ```", "```py\nexport SPARK_HOME=/home/paulcrickard/spark3\nexport PATH=$SPARK_HOME/bin:$PATH\nexport PYSPARK_PYTHON=python3 \n```", "```py\nsource ~/.bashrc\n```", "```py\n    export PYSPARK_DRIVER_PYTHON=jupyter\n    export PYSPARK_DRIVER_PYTHON_OPTS='notebook'\n    ```", "```py\n    findspark method, add the following two lines to your notebook and run it:\n\n    ```", "```py\n\n    If the preceding lines ran without error, then the code was able to find Spark. \n    ```", "```py\n    import findspark\n    findspark.init()\n    ```", "```py\n    import pyspark\n    from pyspark.sql import SparkSession\n    spark=SparkSession.builder.master('spark://pop-os.localdomain:7077').appName('Pi-Estimation').getOrCreate()\n    ```", "```py\n    import random\n    NUM_SAMPLES=1\n    def inside(p):\n        x, y = random.random(), random.random()\n        return x*x + y*y < 1\n    count = spark.sparkContext.parallelize(range(0,\n                         NUM_SAMPLES)).filter(inside).count()\n    print('Pi is roughly {}'.format(4.0 * count / \n                                    NUM_SAMPLES))\n    ```", "```py\n    spark.stop()\n    ```", "```py\nimport findspark\nfindspark.init()\nimport pyspark\nfrom pyspark.sql import SparkSession\nimport os\nos.chdir('/home/paulcrickard')\nspark=SparkSession.builder.master('spark://pop-os.localdomain:7077').appName('DataFrame-Kafka').getOrCreate()\n```", "```py\ndf = spark.read.csv('data.csv')\ndf.show(5)\ndf.printSchema()\n```", "```py\ndf = spark.read.csv('data.csv',header=True,inferSchema=True)\ndf.show(5)\n```", "```py\ndf.select('name').show()\n```", "```py\ndf.select(df['age']<40).show()\ndf.filter(df['age']<40).show()\ndf.filter('age<40').select(['name','age','state']).show()\n```", "```py\nu40=df.filter('age<40').collect()\nu40\n```", "```py\nu40[0]\nu40[0].asDict()\nu40[0].asDict()['name']\n```", "```py\nRow(name='Patrick Hendrix', age=23, street='5755 Jonathan Ranch', city='New Sheriland', state='Wisconsin', zip=60519, lng=103.914462, lat=-59.0094375)\n{'name': 'Patrick Hendrix', 'age': 23, 'street': '5755 Jonathan Ranch', 'city': 'New Sheriland', 'state': 'Wisconsin', 'zip': 60519, 'lng': 103.914462, 'lat': -59.0094375}\n'Patrick Hendrix'\n```", "```py\nfor x in u40:\n    print(x.asDict())\n```", "```py\ndf.createOrReplaceTempView('people')\ndf_over40=spark.sql('select * from people where age > 40')\ndf_over40.show()\n```", "```py\ndf_over40.describe('age').show()\n```", "```py\ndf.groupBy('state').count().show()\n```", "```py\ndf.agg({'age':'mean'}).show()\n```", "```py\nimport pyspark.sql.functions as f\ndf.select(f.collect_set(df['state'])).collect()\n# Returns a Row of unique states which will be all 50.\ndf.select(f.countDistinct('state').alias('states')).show()\n#returns a single column named states with a single value of 50.\ndf.select(f.md5('street').alias('hash')).collect()\n#Returns an md5 hash of the street value for each row\n# Row(hash='81576976c4903b063c46ed9fdd140d62'),\ndf.select(f.reverse(df.state).alias('state-reverse')).collect()\n# returns each rows street value reversed\n# Row(state-reverse='nisnocsiW')\nselect(f.soundex(df.name).alias('soundex')).collect()\n# returns a soundex of the name field for each row\n# Row(soundex='P362')\n```", "```py\nspark.stop()\n```"]