- en: Chapter 9. From Big to Small Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have some cleansed data ready for analysis, let''s first see how
    we can find our way around the high number of variables in our dataset. This chapter
    will introduce some statistical techniques to reduce the number of variables by
    dimension reduction and feature extraction, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Principal** **Component Analysis** (**PCA**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Factor** **Analysis** (**FA**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multidimensional Scaling** (**MDS**) and a few other techniques'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Most dimension reduction methods require that two or more numeric variables
    in the dataset are highly associated or correlated, so the columns in our matrix
    are not totally independent of each other. In such a situation, the goal of dimension
    reduction is to decrease the number of columns in the dataset to the actual matrix
    rank; or, in other words, the number of variables can be decreased whilst most
    of the information content can be retained. In linear algebra, the matrix rank
    refers to the dimensions of the vector space generated by the matrix—or, in simpler
    terms, the number of independent columns and rows in a quadratic matrix. Probably
    it''s easier to understand rank by a quick example: imagine a dataset on students
    where we know the gender, the age, and the date of birth of respondents. This
    data is redundant as the age can be computed (via a linear transformation) from
    the date of birth. Similarly, the year variable is static (without any variability)
    in the `hflights` dataset, and the elapsed time can be also computed by the departure
    and arrival times.'
  prefs: []
  type: TYPE_NORMAL
- en: These transformations basically concentrate on the common variance identified
    among the variables and exclude the remaining total (unique) variance. This results
    in a dataset with fewer columns, which is probably easier to maintain and process,
    but at the cost of some information loss and the creation of artificial variables,
    which are usually harder to comprehend compared to the original columns.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of perfect dependence, all but one of the perfectly correlated variables
    can be omitted, as the rest provide no additional information about the dataset.
    Although it does not happen often, in most cases it's still totally acceptable
    to keep only one or a few components extracted from a set of questions, for example
    in a survey for further analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Adequacy tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first thing you want to do, when thinking about reducing the number of dimensions
    or looking for latent variables in the dataset with multivariate statistical analysis,
    is to check whether the variables are correlated and the data is normally distributed.
  prefs: []
  type: TYPE_NORMAL
- en: Normality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The latter is often not a strict requirement. For example, the results of a
    PCA can be still valid and interpreted if we do not have multivariate normality;
    on the other hand, maximum likelihood factor analysis does have this strong assumption.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You should always use the appropriate methods to achieve your data analysis
    goals, based on the characteristics of your data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Anyway, you can use (for example) `qqplot` to do a pair-wise comparison of
    variables, and `qqnorm` to do univariate normality tests of your variables. First,
    let''s demonstrate this with a subset of `hflights`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'So we filter our dataset to only those flights heading to the John F. Kennedy
    International Airport and we are interested in only two variables describing how
    long the taxiing in and out times were in minutes. The preceding command with
    the traditional `[` indexing can be refactored with `subset` for much more readable
    source code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Please note that now there''s no need to quote variable names or refer to the
    `data.frame` name inside the `subset` call. For more details on this, please see
    [Chapter 3](ch03.html "Chapter 3. Filtering and Summarizing Data"), *Filtering
    and Summarizing Data*. And now let''s see how the values of these two columns
    are distributed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![Normality](img/2028OS_09_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: To render the preceding plot, we created a new graphical device (with `par`
    to hold two plots in a row), then called `qqnorm`, to show the quantiles of the
    empirical variables against the normal distribution, and also added a line for
    the latter with `qqline` for easier comparison. If the data was scaled previously,
    `qqline` would render a 45-degree line.
  prefs: []
  type: TYPE_NORMAL
- en: 'Checking the QQ-plots suggest that the data does not fit the normal distribution
    very well, which can be also verified by an analytical test such as the Shapiro-Wilk
    normality test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The `p-value` is really small, so the null hypothesis (stating that the data
    is normally distributed) is rejected. But how can we test normality for a bunch
    of variables without and beyond separate statistical tests?
  prefs: []
  type: TYPE_NORMAL
- en: Multivariate normality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar statistical tests exist for multiple variables as well; these methods
    provide different ways to check if the data fits the multivariate normal distribution.
    To this end, we will use the `MVN` package, but similar methods can be also found
    in the `mvnormtest` package. The latter includes the multivariate version of the
    previously discussed Shapiro-Wilk test as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'But Mardia''s test is more often used to check multivariate normality and,
    even better, it does not limit the sample size to below 5,000\. After loading
    the `MVN` package, calling the appropriate R function is pretty straightforward
    with a very intuitive interpretation—after getting rid of the missing values in
    our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For more details on handling and filtering missing values, please see [Chapter
    8](ch08.html "Chapter 8. Polishing Data"), *Polishing Data*.
  prefs: []
  type: TYPE_NORMAL
- en: Out of the three p values, the third one refers to cases when the sample size
    is extremely small (<20), so now we only concentrate on the first two values,
    both below 0.05\. This means that the data does not seem to be multivariate normal.
    Unfortunately, Mardia's test fails to perform well in some cases, so more robust
    methods might be more appropriate to use.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `MVN` package can run the Henze-Zirkler''s and Royston''s Multivariate
    Normality Test as well. Both return user-friendly and easy to interpret results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'A more visual method to test multivariate normality is to render similar QQ
    plots to those we used before. But, instead of comparing only one variable with
    the theoretical normal distribution, let''s first compute the squared Mahalanobis
    distance between our variables, which should follow a chi-square distribution
    with the degrees of freedom being the number of our variables. The `MVN` package
    can automatically compute all the required values and render those with any of
    the preceding normality test R functions; just set the `qqplot` argument to be
    `TRUE`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![Multivariate normality](img/2028OS_09_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'If the dataset was normally distributed, the points shown in the preceding
    graphs should fit the straight line. Other alternative graphical methods can produce
    more visual and user-friendly plots with the previously created `mvt` R object.
    The `MVN` package ships the `mvnPlot` function, which can render perspective and
    contour plots for two variables and thus provides a nice way to test bivariate
    normality:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![Multivariate normality](img/2028OS_09_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'On the right plot, you can see the empirical distribution of the two variables
    on a perspective plot, where most cases can be found in the bottom-left corner.
    This means that most flights had only relatively short **TaxiIn** and **TaxiOut**
    times, which suggests a rather heavy-tailed distribution. The left plot shows
    a similar image, but from a bird''s eye view: the contour lines represent a cross-section
    of the right-hand side 3D graph. Multivariate normal distribution looks more central,
    something like a 2-dimensional bell curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![Multivariate normality](img/2028OS_09_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: See [Chapter 13](ch13.html "Chapter 13. Data Around Us"), *Data Around Us* on
    how to create similar contour maps on spatial data.
  prefs: []
  type: TYPE_NORMAL
- en: Dependence of variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Besides normality, relatively high correlation coefficients are desired when
    applying dimension reduction methods. The reason is that, if there is no statistical
    relationship between the variables, for example, PCA will return the exact same
    values without much transformation.
  prefs: []
  type: TYPE_NORMAL
- en: 'To this end, let''s see how the numerical variables of the `hflights` dataset
    are correlated (the output, being a large matrix, is suppressed this time):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding example, we have created a new R object to hold only the numeric
    columns of the original `hflights` data frame, leaving out five character vectors.
    Then, we run `cor` with pair-wise deletion of missing values, which returns a
    matrix with 16 columns and 16 rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The number of missing values in the resulting correlation matrix seems to be
    very high. This is because `Year` was 2011 in all cases, thus resulting in a standard
    variation of zero. It''s wise to exclude `Year` along with the non-numeric variables
    from the dataset—by not only filtering for numeric values, but also checking the
    variance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now the number of missing values is a lot lower:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Can you guess why we still have some missing values here despite the pair-wise
    deletion of missing values? Well, running the preceding command results in a rather
    informative warning, but we will get back to this question later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now proceed with analyzing the actual numbers in the 15x15 correlation
    matrix, which would be way too large to print in this book. To this end, we did
    not show the result of the original `cor` command shown previously, but instead,
    let''s rather visualize those 225 numbers with the graphical capabilities of the
    `ellipse` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![Dependence of variables](img/2028OS_09_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we see the values of the correlation matrix represented by ellipses, where:'
  prefs: []
  type: TYPE_NORMAL
- en: A perfect circle stands for the correlation coefficient of zero
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ellipses with a smaller area reflect the relatively large distance of the correlation
    coefficient from zero
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The tangent represents the negative/positive sign of the coefficient
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To help you with analyzing the preceding results, let''s render a similar plot
    with a few artificially generated numbers that are easier to interpret:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![Dependence of variables](img/2028OS_09_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Similar plots on the correlation matrix can be created with the `corrgram` package.
  prefs: []
  type: TYPE_NORMAL
- en: 'But let''s get back to the `hflights` dataset! On the previous diagram, some
    narrow ellipses are rendered for the time-related variables, which show a relatively
    high correlation coefficient, and even the `Month` variable seems to be slightly
    associated with the `FlightNum` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: On the other hand, the plot shows perfect circles in most cases, which stand
    for a correlation coefficient around zero. This suggests that most variables are
    not correlated at all, so computing the principal components of the original dataset
    would not be very helpful due to the low proportion of common variance.
  prefs: []
  type: TYPE_NORMAL
- en: KMO and Barlett's test
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can verify this assumption on low communalities by a number of statistical
    tests; for example, the SAS and SPSS folks tend to use KMO or Bartlett''s test
    to see if the data is suitable for PCA. Both algorithms are available in R as
    well via, for example, via the `psych` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Unfortunately, the `Overall MSA` (*Measure of Sampling Adequacy*, representing
    the average correlations between the variables) is not available in the preceding
    output due to the previously identified missing values of the correlation matrix.
    Let''s pick a pair of variables where the correlation coefficient was `NA` for
    further analysis! Such a pair can be easily identified from the previous plot;
    no circle or ellipse was drawn for missing values, for example, for `Cancelled`
    and `AirTime`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This can be explained by the fact, that if a flight is cancelled, then the
    time spent in the air does not vary much; furthermore, this data is not available:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'So we get missing values when calling `cor` due to these `NA`; similarly, we
    also get `NA` when calling `cor` with pair-wise deletion, as only the non-cancelled
    flights remain in the dataset, resulting in zero variance for the `Cancelled`
    variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This suggests removing the `Cancelled` variable from the dataset before we
    run the previously discussed assumption tests, as the information stored in that
    variable is redundantly available in other columns of the dataset as well. Or,
    in other words, the `Cancelled` column can be computed by a linear transformation
    of the other columns, which can be left out from further analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'And let''s see if we still have any missing values in the correlation matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'It seems that the `Diverted` column is responsible for a similar situation,
    and the other three variables were not available when the flight was diverted.
    After another subset, we are now ready to call KMO on a full correlation matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Overall MSA`, or the so called **Kaiser-Meyer-Olkin** (**KMO**) index,
    is a number between 0 and 1; this value suggests whether the partial correlations
    of the variables are small enough to continue with data reduction methods. A general
    rating system or rule of a thumb for KMO can be found in the following table,
    as suggested by Kaiser:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Value | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| KMO < 0.5 | Unacceptable |'
  prefs: []
  type: TYPE_TB
- en: '| 0.5 < KMO < 0.6 | Miserable |'
  prefs: []
  type: TYPE_TB
- en: '| 0.6 < KMO < 0.7 | Mediocre |'
  prefs: []
  type: TYPE_TB
- en: '| 0.7 < KMO < 0.8 | Middling |'
  prefs: []
  type: TYPE_TB
- en: '| 0.8 < KMO < 0.9 | Meritorious |'
  prefs: []
  type: TYPE_TB
- en: '| KMO > 0.9 | Marvelous |'
  prefs: []
  type: TYPE_TB
- en: The KMO index being below 0.5 is considered unacceptable, which basically means
    that the partial correlation computed from the correlation matrix suggests that
    the variables are not correlated enough for a meaningful dimension reduction or
    latent variable model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although leaving out some variables with the lowest MSA would improve the `Overall
    MSA`, and we could build some appropriate models in the following pages, for instructional
    purposes we won''t spend any more time on data transformation for the time being,
    and we will use the `mtcars` dataset, which was introduced in [Chapter 3](ch03.html
    "Chapter 3. Filtering and Summarizing Data"), *Filtering and Summarizing Data*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'It seems that the `mtcars` database is a great choice for multivariate statistical
    analysis. This can be also verified by the so-called Bartlett test, which suggests
    whether the correlation matrix is similar to an identity matrix. Or, in other
    words, if there is a statistical relationship between the variables. On the other
    hand, if the correlation matrix has only zeros except for the diagonal, then the
    variables are independent from each other; thus it would not make much sense to
    think of multivariate methods. The `psych` package provides an easy-to-use function
    to compute Bartlett''s test as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The very low `p-value` suggests that we reject the null-hypothesis of the Bartlett
    test. This means that the correlation matrix differs from the identity matrix,
    so the correlation coeffiecients between the variables seem to be closer to 1
    than 0\. This is in sync with the high KMO value.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before focusing on the actual statistical methods, please be advised that, although
    the preceding assumptions make sense in most cases and should be followed as a
    rule of a thumb, KMO and Bartlett's tests are not always required. High communality
    is important for factor analysis and other latent models, while for example PCA
    is a mathematical transformation that will work with even low KMO values.
  prefs: []
  type: TYPE_NORMAL
- en: Principal Component Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finding the really important fields in databases with a huge number of variables
    may prove to be a challenging task for the data scientist. This is where **Principal
    Component Analysis** (**PCA**) comes into the picture: to find the core components
    of data. It was invented more than 100 years ago by Karl Pearson, and it has been
    widely used in diverse fields since then.'
  prefs: []
  type: TYPE_NORMAL
- en: The objective of PCA is to interpret the data in a more meaningful structure
    with the help of orthogonal transformations. This linear transformation is intended
    to reveal the internal structure of the dataset with an arbitrarily designed new
    basis in the vector space, which best explains the variance of the data. In plain
    English, this simply means that we compute new variables from the original data,
    where these new variables include the variance of the original variables in decreasing
    order.
  prefs: []
  type: TYPE_NORMAL
- en: This can be either done by eigendecomposition of the covariance, correlation
    matrix (the so-called R-mode PCA), or singular value decomposition (the so-called
    Q-mode PCA) of the dataset. Each method has great advantages, such as computation
    performance, memory requirements, or simply avoiding the prior standardization
    of the data before passing it to PCA when using a correlation matrix in eigendecomposition.
  prefs: []
  type: TYPE_NORMAL
- en: Either way, PCA can successfully ship a lower-dimensional image of the data,
    where the uncorrelated principal components are the linear combinations of the
    original variables. And this informative overview can be a great help to the analyst
    when identifying the underlying structure of the variables; thus the technique
    is very often used for exploratory data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: PCA results in the exact same number of extracted components as the original
    variables. The first component includes most of the common variance, so it has
    the highest importance in describing the original dataset, while the last component
    often only includes some unique information from only one original variable. Based
    on this, we would usually only keep the first few components of PCA for further
    analysis, but we will also see some use cases where we will concentrate on the
    extracted unique variance.
  prefs: []
  type: TYPE_NORMAL
- en: PCA algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: R provides a variety of functions to run PCA. Although it's possible to compute
    the components manually by `eigen` or `svd` as R-mode or Q-mode PCA, we will focus
    on the higher level functions for the sake of simplicity. Relying on my stats-teacher
    background, I think that sometimes it's more efficient to concentrate on how to
    run an analysis and interpreting the results rather than spending way too much
    time with the linear algebra background—especially with given time/page limits.
  prefs: []
  type: TYPE_NORMAL
- en: 'R-mode PCA can be conducted by `princomp` or `principal` from the `psych` package,
    while the more preferred Q-mode PCA can be called by `prcomp`. Now let''s focus
    on the latter and see what the components of `mtcars` look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Please note that we have called `prcomp` with `scale` set to `TRUE`, which
    is `FALSE` by default due to being backward-compatible with the S language. But
    in general, scaling is highly recommended. Using the scaling option is equivalent
    to running PCA on a dataset after scaling it previously, such as: `prcomp(scale(mtcars))`,
    which results in data with unit variance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, `prcomp` returned the standard deviations of the principal components,
    which shows how much information was preserved by the 11 components. The standard
    deviation of the first component is a lot larger than any other subsequent value,
    which explains more than 60 percent of the variance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Besides the first component, only the second one has a higher standard deviation
    than 1, which means that only the first two components include at least as much
    information as the original variables did. Or, in other words: only the first
    two variables have a higher eigenvalue than one. The eigenvalue can be computed
    by the square of the standard deviation of the principal components, summing up
    to the number of original variables as expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Determining the number of components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PCA algorithms always compute the same number of principal components as the
    number of variables in the original dataset. The importance of the component decreases
    from the first one to the last one.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a rule of a thumb, we can simply keep all those components with higher standard
    deviation than 1\. This means that we keep those components, which explains at
    least as much variance as the original variables do:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'So the preceding summary suggests keeping only two components out of the 11,
    which explains almost 85 percent of the variance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'An alternative and great visualization tool to help us determine the optimal
    number of component is scree plot. Fortunately, there are at least two great functions
    in the `psych` package we can use here: the `scree` and the `VSS.scree` functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '![Determining the number of components](img/2028OS_09_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '![Determining the number of components](img/2028OS_09_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The only difference between the preceding two plots is that `scree` also shows
    the eigenvalues of a factor analysis besides PCA. Read more about this in the
    next section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: As can be seen, `VSS.scree` provides a visual overview on the eigenvalues of
    the principal components, and it also highlights the critical value at 1 by a
    horizontal line. This is usually referred to as the Kaiser criterion.
  prefs: []
  type: TYPE_NORMAL
- en: Besides this rule of a thumb, as discussed previously one can also rely on the
    so-called Elbow-rule, which simply suggests that the line-plot represents an arm
    and the optimal number of components is the point where this arm's elbow can be
    found. So we have to look for the point from where the curve becomes less steep.
    This sharp break is probably at 3 in this case instead of 2, as we have found
    with the Kaiser criterion.
  prefs: []
  type: TYPE_NORMAL
- en: 'And besides Cattell''s original scree test, we can also compare the previously
    described `scree` of the components with a bit of a randomized data to identify
    the optimal number of components to keep:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '![Determining the number of components](img/2028OS_09_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Now we have verified the optimal number of principal components to keep for
    further analysis with a variety of statistical tools, and we can work with only
    two variables instead of 11 after all, which is great! But what do these artificially
    created variables actually mean?
  prefs: []
  type: TYPE_NORMAL
- en: Interpreting components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The only problem with reducing the dimension of our data is that it can be
    very frustrating to find out what our newly created, highly compressed, and transformed
    data actually is. Now we have `PC1` and `PC2` for our 32 cars:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'These values were computed by multiplying the original dataset with the identified
    weights, so-called loadings (`rotation`) or the component matrix. This is a standard
    linear transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Both variables are scaled with the mean being zero and the standard deviation
    as described previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'All scores computed by PCA are scaled, because it always returns the values
    transformed to a new coordinate system with an orthogonal basis, which means that
    the components are not correlated and scaled:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'To see what the principal components actually mean, it''s really helpful to
    check the loadings matrix, as we have seen before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Probably this analytical table might be more meaningful in some visual way,
    for example as a `biplot`, which shows not only the original variables but also
    the observations (black labels) on the same plot with the new coordinate system
    based on the principal components (red labels):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '![Interpreting components](img/2028OS_09_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We can conclude that `PC1` includes information mostly from the number of cylinders
    (`cyl`), displacement (`disp`), weight (`wt`), and gas consumption (`mpg`), although
    the latter looks likely to decrease the value of `PC1`. This was found by checking
    the highest and lowest values on the `PC1` axis. Similarly, we find that `PC2`
    is constructed by speed-up (`qsec`), number of gears (`gear`), carburetors (`carb`),
    and the transmission type (`am`).
  prefs: []
  type: TYPE_NORMAL
- en: 'To verify this, we can easily compute the correlation coefficient between the
    original variables and the principal components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Does this make sense? How would you name `PC1` and `PC2`? The number of cylinders
    and displacement seem like engine parameters, while the weight is probably rather
    influenced by the body of the car. Gas consumption should be affected by both
    specs. The other component's variables deal with suspension, but we also have
    speed there, not to mention the bunch of mediocre correlation coefficients in
    the preceding matrix. Now what?
  prefs: []
  type: TYPE_NORMAL
- en: Rotation methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Based on the fact that rotation methods are done in a subspace, rotation is
    always suboptimal compared to the previously discussed PCA. This means that the
    new axes after rotation will explain less variance than the original components.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, rotation simplifies the structure of the components and thus
    makes it a lot easier to understand and interpret the results; thus, these methods
    are often used in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Rotation methods can be (and are) usually applied to both PCA and FA (more on
    this later). Orthogonal methods are preferred.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two main types of rotation:'
  prefs: []
  type: TYPE_NORMAL
- en: Orthogonal, where the new axes are orthogonal to each other. There is no correlation
    between the components/factors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oblique, where the new axes are not necessarily orthogonal to each other; thus
    there might be some correlation between the variables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Varimax rotation is one of the most popular rotation methods. It was developed
    by Kaiser in 1958 and has been popular ever since. It is often used because the
    method maximizes the variance of the loadings matrix, resulting in more interpretable
    scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Now the first component seems to be mostly affected (negatively dominated)
    by the transmission type, number of gears, and rear axle ratio, while the second
    one is affected by speed-up, horsepower, and the number of carburetors. This suggests
    naming `PC2` as `power`, while `PC1` instead refers to `transmission`. Let''s
    see those 32 automobiles in this new coordinate system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '![Rotation methods](img/2028OS_09_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Based on the preceding plot, every data scientist should pick a car from the
    upper left quarter to go with the top rated models, right? Those cars have great
    power based on the *y* axis and good transmission systems, as shown on the *x*
    axis—do not forget about the transmission being negatively correlated with the
    original variables. But let's see some other rotation methods and the advantages
    of those as well!
  prefs: []
  type: TYPE_NORMAL
- en: Quartimax rotation is an orthogonal method, as well, and minimizes the number
    of components needed to explain each variable. This often results in a general
    component and additional smaller components. When a compromise between Varimax
    and Quartimax rotation methods is needed, you might opt for Equimax rotation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Oblique rotation methods include Oblimin and Promax, which are not available
    in the base stats or even the highly used `psych` package. Instead, we can load
    the `GPArotation` package, which provides a wide range of rotation methods for
    PCA and FA as well. For demonstration purposes, let''s see how Promax rotation
    works, which is a lot faster compared to, for example, Oblimin:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: The result of the last command supports the view that oblique rotation methods
    generate scores that might be correlated, unlike when running an orthogonal rotation.
  prefs: []
  type: TYPE_NORMAL
- en: Outlier-detection with PCA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'PCA can be used for a variety of goals besides exploratory data analysis. For
    example, we can use PCA to generate eigenfaces, compress images, classify observations,
    or to detect outliers in a multidimensional space via image filtering. Now, we
    will construct a simplified model discussed in a related research post published
    on R-bloggers in 2012: [http://www.r-bloggers.com/finding-a-pin-in-a-haystack-pca-image-filtering](http://www.r-bloggers.com/finding-a-pin-in-a-haystack-pca-image-filtering).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The challenge described in the post was to detect a foreign metal object in
    the sand photographed by the Curiosity Rover on the Mars. The image can be found
    at the official NASA website at [http://www.nasa.gov/images/content/694811main_pia16225-43_full.jpg](http://www.nasa.gov/images/content/694811main_pia16225-43_full.jpg),
    for which I''ve created a shortened URL for future use: [http://bit.ly/nasa-img](http://bit.ly/nasa-img).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following image, you can see a strange metal object highlighted in the
    sand in a black circle, just to make sure you know what we are looking for. The
    image found at the preceding URL does not have this highlight:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Outlier-detection with PCA](img/2028OS_09_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'And now let''s use some statistical methods to identify that object without
    (much) human intervention! First, we need to download the image from the Internet
    and load it into R. The `jpeg` package will be really helpful here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: The `readJPEG` function returns the RGB values of every pixel in the picture,
    resulting in a three dimensional array where the first dimension is the row, the
    second is the column, and the third dimension includes the three color values.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: RGB is an additive color model that can reproduce a wide variety of colors by
    mixing red, green, and blue by given intensities and optional transparency. This
    color model is highly used in computer science.
  prefs: []
  type: TYPE_NORMAL
- en: 'As PCA requires a matrix as an input, we have to convert this 3-dimensional
    array to a 2-dimensional dataset. To this end, let''s not bother with the order
    of pixels for the time being, as we can reconstruct that later, but let''s simply
    list the RGB values of all pixels, one after the other:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'In a nutshell, we saved the original height of the image (in pixels) in variable
    `h`, saved the width in `w`, and then converted the 3D array to a matrix with
    1,357,105 rows. And, after four lines of data loading and three lines of data
    transformation, we can call the actual, rather simplified statistical method at
    last:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: As we've seen before, data scientists do indeed deal with data preparation most
    of the time, while the actual data analysis can be done easily, right?
  prefs: []
  type: TYPE_NORMAL
- en: 'The extracted components seems to perform pretty well; the first component
    explains more than 96 percent of the variance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Previously, interpreting RGB values was pretty straightforward, but what do
    these components mean?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'It seems that the first component is rather mixed with all three colors, the
    second component misses the green color, while the third component includes almost
    only green. Why not visualize that instead of trying to imagine how these artificial
    values look? To this end, let''s extract the color intensities from the preceding
    component/loading matrix by the following quick helper function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Calling this on the absolute values of the component matrix results in the
    hex-color codes that describe the principal components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'These color codes can be easily rendered—for example, on a pie chart, where
    the area of the pies represents the explained variance of the principal components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '![Outlier-detection with PCA](img/2028OS_09_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now we no longer have red, green, or blue intensities or actual colors in the
    computed scores stored in `pca$x`; rather, the principal components describe each
    pixel with the visualized colors shown previously. And, as previously discussed,
    the third component stands for a greenish color, the second one misses green (resulting
    in a purple color), while the first component includes a rather high value from
    all RGB colors resulting in a tawny color, which is not surprising at all knowing
    that the photo was taken in the desert of Mars.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can render the original image with monochrome colors to show the intensity
    of the principal components. The following few lines of code produce two modified
    photos of the Curiosity Rover and its environment based on `PC1` and `PC2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '![Outlier-detection with PCA](img/2028OS_09_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Although the image was rotated by 90 degrees in some of the linear transformations,
    it's pretty clear that the first image was not really helpful in finding the foreign
    metal object in the sand. As a matter of fact, this image represents the noise
    in the desert area, as `PC1` included sand-like color intensities, so this component
    is useful for describing the variety of tawny colors.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the second component highlights the metal object in the sand
    very well! All surrounding pixels are dim, due to the low ratio of purple color
    in normal sand, while the anomalous object is rather dark.
  prefs: []
  type: TYPE_NORMAL
- en: 'I really like this piece of R code and the simplified example: although they''re
    still basic enough to follow, they also demonstrate the power of R and how standard
    data analytic methods can be used to harvest information from raw data.'
  prefs: []
  type: TYPE_NORMAL
- en: Factor analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although the literature on confirmatory **factor analysis** (**FA**) is really
    impressive and is being highly used in, for example, social sciences, we will
    only focus on exploratory FA, where our goal is to identify some unknown, not
    observed variables based on other empirical data.
  prefs: []
  type: TYPE_NORMAL
- en: The latent variable model of FA was first introduced in 1904 by Spearman for
    one factor, and then Thurstone generalized the model for more than one factor
    in 1947\. This statistical model assumes that the manifest variables available
    in the dataset are the results of latent variables that were not observed but
    can be tracked based on the observed data.
  prefs: []
  type: TYPE_NORMAL
- en: FA can deal with continuous (numeric) variables, and the model states that each
    observed variable is the sum of some unknown, latent factors.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Please note the that normality, KMO, and Bartlett's tests are a lot more important
    to check before doing FA compared to PCA; the latter is a rather descriptive method
    while, in FA, we are actually building a model.
  prefs: []
  type: TYPE_NORMAL
- en: The most used exploratory FA method is maximum-likelihood FA, which is also
    available in the `factanal` function in the already installed `stats` package.
    Other factoring methods are made available by the `fa` functions in the `psych`
    package—for example, **ordinary least squares** (**OLS**), **weighted least squares**
    (**WLS**), **generalized weighted least squares** (**GLS**), or principal factor
    solution. These functions take raw data or the covariance matrix as input.
  prefs: []
  type: TYPE_NORMAL
- en: 'For demonstration purposes, let''s see how the default factoring method performs
    on a subset of `mtcars`. Let''s extract all performance-related variables except
    for displacement, which is probably accountable for all the other relevant metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Now simply call and save the results of `fa` on the preceding `data.frame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Well, this is a rather impressive amount of information with a bunch of details!
    `MR1` stands for the first extracted factor named after the default factoring
    method (Minimal Residuals or OLS). Since there is only one factor included in
    the model, rotation of factors is not an option. There is a test or hypothesis
    to check whether the numbers of factors are sufficient, and some coefficients
    represent a really great model fit.
  prefs: []
  type: TYPE_NORMAL
- en: 'The results can be summarized on the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '![Factor analysis](img/2028OS_09_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here we see the high correlation coefficients between the latent and the observed
    variables, and the direction of the arrows suggests that the factor has an effect
    on the values found in our empirical dataset. Guess the relationship between this
    factor and the displacement of the car engines!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Well, this seems like a good match.
  prefs: []
  type: TYPE_NORMAL
- en: Principal Component Analysis versus Factor Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unfortunately, principal components are often confused with factors, and the
    two terms and related methods are sometimes used as synonyms, although the mathematical
    background and goals of the two methods are really different.
  prefs: []
  type: TYPE_NORMAL
- en: 'PCA is used to reduce the number of variables by creating principal components
    that then can be used in further projects instead of the original variables. This
    means that we try to extract the essence of the dataset in the means of artificially
    created variables, which best describe the variance of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Principal Component Analysis versus Factor Analysis](img/2028OS_09_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'FA is the other way around, as it tries to identify unknown, latent variables
    to explain the original data. In plain English, we use the manifest variables
    from our empirical dataset to guess the internal structure of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Principal Component Analysis versus Factor Analysis](img/2028OS_09_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Multidimensional Scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Multidimensional Scaling** (**MDS**) is a multivariate technique that was
    first used in geography. The main goal of MDS is to plot multivariate data points
    in two dimensions, thus revealing the structure of the dataset by visualizing
    the relative distance of the observations. MDA is used in diverse fields such
    as attitude study in psychology, sociology, and market research.'
  prefs: []
  type: TYPE_NORMAL
- en: While the `MASS` package provides non-metric MDS via the `isoMDS` function,
    we will concentrate on the classical metric MDS, which is available in the `cmdscale`
    function offered by the `stats` package. Both types of MDS take a distance matrix
    as the main argument and can be created from any numeric tabular data by the `dist`
    function.
  prefs: []
  type: TYPE_NORMAL
- en: 'But before we explore more complex examples, let''s see what MDS can offer
    us while working with an already existing distance matrix, such as the built-in
    `eurodist` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding values represents the travel distance between 21 European cities
    in kilometers, although only the first 5-5 values were shown. Running classical
    MDS is fairly easy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: These scores are very similar to two principal components, such as running `prcomp(eurodist)$x[,
    1:2]`. As a matter of fact, PCA can be considered as the most basic MDS solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Anyway, we have just transformed the 21-dimensional space into 2 dimensions,
    which can be plotted very easily (unlike the previous matrix with 21 rows and
    21 columns):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '![Multidimensional Scaling](img/2028OS_09_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Does this ring a bell? If not, please feel free to see the following image,
    where the following two lines of code also show the city names instead of the
    anonymous points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '![Multidimensional Scaling](img/2028OS_09_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Although the *y* axis is flipped, which you can fix by multiplying the second
    argument of text by -1, we have just rendered a European map of cities from the
    distance matrix—without any further geographical data. I find this rather impressive.
  prefs: []
  type: TYPE_NORMAL
- en: Please find more data visualization tricks and methods in [Chapter 13](ch13.html
    "Chapter 13. Data Around Us"), *Data Around Us*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s see how to apply MDS on non-geographic data that was not prepared
    with a view to its being a distance matrix. Let''s get back to the `mtcars` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '![Multidimensional Scaling](img/2028OS_09_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The plot shows the 32 cars of the original dataset scattered in a two-dimensional
    space. The distance between the elements was computed by MDS, which took into
    account all the 11 original variables, and it's very easy to identify the similar
    and very different car types. We will cover these topics in more details in the
    next chapter, [Chapter 10](ch10.html "Chapter 10. Classification and Clustering"),
    *Classification and Clustering*.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered a number of ways to deal with multivariate data
    to reduce the number of available dimensions in the means of artificially computed
    continuous variables and to identify underlying, latent, and similarly numeric
    variables. On the other hand, sometimes it's rather difficult to describe reality
    with numbers and we should rather think in categories.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will introduce new methods to define data types (clusters)
    and will also demonstrate how to classify elements with the help of available
    training data.
  prefs: []
  type: TYPE_NORMAL
