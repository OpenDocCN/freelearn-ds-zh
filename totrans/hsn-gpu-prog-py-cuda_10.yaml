- en: Working with Compiled GPU Code
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与编译后的GPU代码一起工作
- en: Throughout the course of this book, we have generally been reliant on the PyCUDA
    library to interface our inline CUDA-C code for us automatically, using just-in-time
    compilation and linking with our Python code. We might recall, however, that sometimes
    the compilation process can take a while. In [Chapter 3](6ab0cd69-e439-4cfb-bf1a-4247ec58c94e.xhtml),
    *Getting Started With PyCUDA*, we even saw in detail how the compilation process
    can contribute to slowdown, and how it can even be somewhat arbitrary as to when
    inline code will be compiled and retained. In some cases, this may be inconvenient
    and cumbersome given the application, or even unacceptable in the case of a real-time
    system.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的整个过程中，我们通常依赖于PyCUDA库来自动为我们接口我们的内联CUDA-C代码，仅使用即时编译和与我们的Python代码链接。然而，我们可能会记得，有时编译过程可能需要一段时间。在[第3章](6ab0cd69-e439-4cfb-bf1a-4247ec58c94e.xhtml)“开始使用PyCUDA”中，我们甚至详细看到了编译过程如何导致减速，以及内联代码何时编译和保留可能具有一定的任意性。在某些情况下，考虑到应用程序，这可能会不方便且繁琐，甚至在实时系统中可能是不被接受的。
- en: To this end, we will finally see how to use pre-compiled GPU code from Python.
    In particular, we will look at three distinct ways to do this. First, we will
    look at how we can do this by writing a host-side CUDA-C function that can indirectly
    launch a CUDA kernel. This method will involve invoking the host-side function
    with the standard Python Ctypes library. Second, we will compile our kernel into
    what is known as a PTX module, which is effectively a DLL file containing compiled
    binary GPU. We can then load this file with PyCUDA and launch our kernel directly.
    Finally, we will end this chapter by looking at how to write our own full-on Ctypes
    interface to the CUDA Driver API. We can then use the appropriate functions from
    the Driver API to load our PTX file and launch a kernel.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 为了这个目的，我们最终将看到如何从Python中使用预编译的GPU代码。特别是，我们将探讨三种不同的方法来实现这一点。首先，我们将探讨如何通过编写一个主机端CUDA-C函数来间接启动CUDA内核。这种方法将涉及使用标准的Python
    Ctypes库调用主机端函数。其次，我们将编译我们的内核到一个称为PTX模块的东西，这实际上是一个包含编译后的GPU二进制的DLL文件。然后我们可以使用PyCUDA加载这个文件并直接启动我们的内核。最后，我们将通过探讨如何编写我们自己的完整的Ctypes接口到CUDA驱动程序API来结束这一章。然后我们可以使用驱动程序API中的适当函数来加载我们的PTX文件并启动内核。
- en: 'The learning outcomes for this chapter are as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的学习成果如下：
- en: Launching compiled (host-side) code with the Ctypes module
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Ctypes模块启动编译后的（主机端）代码
- en: Using host-side CUDA C wrappers with Ctypes to launch a kernel from Python
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Ctypes在Python中启动内核的主机端CUDA C包装器
- en: How to compile a CUDA C module into a PTX file
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何将CUDA C模块编译成PTX文件
- en: How to load a PTX module into PyCUDA to launch pre-compiled kernels
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何将PTX模块加载到PyCUDA中以启动预编译的内核
- en: How to write your own custom Python interface to the CUDA Driver API
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何编写自己的自定义Python接口到CUDA驱动程序API
- en: Launching compiled code with Ctypes
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Ctypes启动编译后的代码
- en: We will now give a brief overview of the Ctypes module from the Python Standard
    Library. Ctypes is used for calling functions from the Linux `.so` (shared object)
    or Windows. DLL (Dynamically Linked Library) pre-compiled binaries. This will
    allow us to break out of the world of pure Python and interface with libraries
    and code that have been written in compiled languages, notably C and C++—it just
    so happens that Nvidia only provides such pre-compiled binaries for interfacing
    with our CUDA device, so if we want to sidestep PyCUDA, we will have to use Ctypes.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将简要概述Python标准库中的Ctypes模块。Ctypes用于调用Linux `.so`（共享对象）或Windows DLL（动态链接库）预编译的二进制文件中的函数。这将使我们能够跳出纯Python的世界，与用编译语言（尤其是C和C++）编写的库和代码进行接口，碰巧的是，Nvidia只为与我们的CUDA设备接口提供这样的预编译二进制文件，因此如果我们想绕过PyCUDA，我们就必须使用Ctypes。
- en: 'Let''s start with a very basic example: we will show you how to call `printf`
    directly from Ctypes. Open up an instance of IPython and type `import ctypes`.
    We are now going to look at how to call the standard `printf` function from Ctypes.
    First, we will have to import the appropriate library: in Linux, load the LibC
    library by typing `libc = ctypes.CDLL(''libc.so.6'')` (in Windows, replace `''libc.so.6''`
    with `''msvcrt.dll''`). We can now directly call `printf` from the IPython prompt
    by typing `libc.printf("Hello from ctypes!\n")`. Try it for yourself!'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从一个非常基础的例子开始：我们将向您展示如何直接从 Ctypes 调用 `printf`。打开一个 IPython 实例并输入 `import ctypes`。现在我们将查看如何从
    Ctypes 调用标准的 `printf` 函数。首先，我们需要导入适当的库：在 Linux 上，通过输入 `libc = ctypes.CDLL('libc.so.6')`
    来加载 LibC 库（在 Windows 上，将 `'libc.so.6'` 替换为 `'msvcrt.dll'`）。现在我们可以直接在 IPython 提示符中通过输入
    `libc.printf("Hello from ctypes!\n")` 来调用 `printf`。自己试试吧！
- en: 'Now let''s try something else: type `libc.printf("Pi is approximately %f.\n",
    3.14)` from IPython; you should get an error. This is because the `3.14` was not
    appropriately typecast from a Python float variable to a C double variable—we
    can do this with Ctypes like so:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们尝试另一件事：从 IPython 中输入 `libc.printf("Pi is approximately %f.\n", 3.14)`；你应该会得到一个错误。这是因为
    `3.14` 没有适当地从 Python 浮点变量转换为 C 双精度浮点变量——我们可以使用 Ctypes 如此完成：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The output should be as expected. As in the case of launching a CUDA kernel
    from PyCUDA, we have to be equally careful to typecast inputs into functions with
    Ctypes.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该是预期的。就像从 PyCUDA 启动 CUDA 内核的情况一样，我们必须同样小心地将输入类型转换为 Ctypes 中的函数，这些函数具有适当的
    C 数据类型。
- en: 'Always be sure to appropriately typecast inputs into any function that you
    call with Ctypes from Python to the appropriate C datatypes (in Ctypes, these
    are preceded by c_: `c_float`, `c_double`, `c_char`, `c_int`, and so on).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 总是要确保将输入适当地类型转换为 Python 中使用 Ctypes 调用的任何函数的适当 C 数据类型（在 Ctypes 中，这些数据类型前面带有 c_：`c_float`、`c_double`、`c_char`、`c_int`
    等）。
- en: The Mandelbrot set revisited (again)
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 再次回顾曼德布罗特集（再次）
- en: Let's revisit the Mandelbrot set that we looked at in [Chapter 1](f9c54d0e-6a18-49fc-b04c-d44a95e011a2.xhtml),
    *Why GPU Programming?*, and [Chapter 3](6ab0cd69-e439-4cfb-bf1a-4247ec58c94e.xhtml),
    *Getting Started with PyCUDA*. First, we will write a full-on CUDA kernel that
    will compute the Mandelbrot set, given a particular set of parameters, along with
    an appropriate host-side wrapper function that we may interface to from Ctypes
    later. We will first be writing these functions into a single CUDA-C `.cu` source
    file and then compile this into a DLL or `.so` binary with the NVCC compiler.
    Finally, we will write some Python code so that we can run our binary code and
    display the Mandelbrot set.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下我们在 [第 1 章](f9c54d0e-6a18-49fc-b04c-d44a95e011a2.xhtml) “为什么进行 GPU 编程？”和
    [第 3 章](6ab0cd69-e439-4cfb-bf1a-4247ec58c94e.xhtml) “使用 PyCUDA 入门”中看到的曼德布罗特集。首先，我们将编写一个完整的
    CUDA 内核，该内核将根据一组特定的参数计算曼德布罗特集，以及一个适当的主机端包装函数，我们可以在以后通过 Ctypes 接口。我们首先将这些函数写入一个单独的
    CUDA-C `.cu` 源文件，然后使用 NVCC 编译器将此编译成 DLL 或 `.so` 二进制文件。最后，我们将编写一些 Python 代码，以便我们可以运行我们的二进制代码并显示曼德布罗特集。
- en: We will now apply our knowledge of Ctypes to launch a pre-compiled CUDA kernel
    from Python without any assistance from PyCUDA. This will require us to write
    a host-side *k**ernel launcher* wrapper function in CUDA-C that we may call directly,
    which itself has been compiled into a dynamic library binary with any necessary
    GPU code—that is, a Dynamically Linked Library (DLL) binary on Windows, or a shared-object
    (so) binary on Linux.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将应用我们对 Ctypes 的了解，从 Python 中启动预编译的 CUDA 内核，而不需要 PyCUDA 的任何帮助。这要求我们编写一个主机端
    *内核启动器* 包装函数，我们可以直接调用它，该函数本身已被编译成动态库二进制文件，其中包含必要的 GPU 代码——即在 Windows 上的动态链接库 (DLL)
    二进制文件，或在 Linux 上的共享对象 (so) 二进制文件。
- en: 'We will start, of course, by writing our CUDA-C code, so open up your favorite
    text editor and follow along. We will begin with the standard `include` statements:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们首先将编写我们的 CUDA-C 代码，所以请打开您喜欢的文本编辑器并跟随操作。我们将从标准的 `include` 语句开始：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We''ll now jump directly into writing our kernel. Notice `extern "C"` in the
    code, which will allow us to link to this function externally:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将直接编写我们的内核。注意代码中的 `extern "C"`，这将允许我们从外部链接到这个函数：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let''s think for a minute about how this will work: we will use a single one-dimensional
    array for both the real and imaginary components called `lattice`, which is of
    length `lattice_size`. We will use this to compute a two-dimensional Mandelbrot
    graph of the shape (`lattice_size`, `lattice_size`) into the pre-allocated array,
    `mandelbrot_graph`. We will specify the number of iterations to check for divergence
    at each point with `max_iters`, specifying the maximum upper bound as before by
    providing its squared value with `upper_bound_squared`. (We''ll look at the motivation
    for using the square in a second.)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们思考一下这将如何工作：我们将使用一个名为`lattice`的单维数组来存储实部和虚部，其长度为`lattice_size`。我们将使用这个数组来计算一个形状为(`lattice_size`,
    `lattice_size`)的二维曼德布罗特图，并将其写入预分配的数组`mandelbrot_graph`中。我们将使用`max_iters`指定每个点检查发散的迭代次数，通过提供其平方值`upper_bound_squared`来指定最大上限。（我们将在下一部分讨论使用平方的原因。）
- en: 'We will launch this kernel over a one-dimensional grid/block structure, with
    each thread corresponding to a single point in the graph image of the Mandelbrot
    set. We can then determine the real/imaginary lattice values for the corresponding
    point, like so:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在一个一维网格/块结构上启动这个内核，每个线程对应于曼德布罗特集图形图像中的单个点。然后我们可以确定对应点的实/虚晶格值，如下所示：
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Let's talk about this for a minute. First, remember that we may have to use
    slightly more threads than necessary, so it's important that we check that the
    thread ID will correspond to some point in the output image with the `if` statement.
    Let's also remember that the output array, `mandelbrot_graph`, will be stored
    as a one-dimensional array that represents a two-dimensional image stored in a
    row-wise format, and that we will be using `tid` as the index to write in this
    array. We will use `i` and `j`, as well as the `x` and `y` coordinates of the
    graph on the complex plane. Since lattice is a series of real values sorted from
    small to large, we will have to reverse their order to get the appropriate imaginary
    values. Also, notice that we will be using plain floats here, rather than some
    structure or object to represent a complex value. Since there are real and imaginary
    components in every complex number, we will have to use two floats here to store
    the complex number corresponding to this thread's lattice point (`c_re` and `c_im`).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们花一分钟来讨论这个问题。首先，记住我们可能需要使用比必要的稍微多一点线程，因此检查线程ID是否与输出图像中的某个点相对应是很重要的，我们可以使用`if`语句来完成这个检查。同时，我们也应该记住输出数组`mandelbrot_graph`将作为一个一维数组存储，它代表了一个以行格式存储的两维图像，我们将使用`tid`作为索引来写入这个数组。我们还将使用`i`和`j`，以及复平面上图形的`x`和`y`坐标。由于晶格是一系列从小到大排序的实数值，我们将需要反转它们的顺序以获得适当的虚数。此外，请注意，我们将在这里使用普通的浮点数，而不是某种结构或对象来表示复数值。由于每个复数都有实部和虚部，我们在这里将需要使用两个浮点数来存储对应于这个线程晶格点的复数（`c_re`和`c_im`）。
- en: 'We will set up two more variables to handle the divergence check, `z_re` and
    `z_im`, and set the initial value of this thread''s point on the graph to `1`
    before we check for divergence:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将设置两个额外的变量来处理发散检查，`z_re`和`z_im`，并在检查发散之前将这个线程在图上的初始值设置为`1`：
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now we will do our check for divergence; if it does diverge after `max_iters`
    iterations, we set the point to `0`. Otherwise, it is left at 1:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将进行发散检查；如果在`max_iters`次迭代后发散，我们将点设置为`0`。否则，它将保持在`1`：
- en: '[PRE5]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Let's talk about this chunk of code for a minute before we continue. Let's remember
    that each iteration of a Mandelbrot set is computed with complex multiplication
    and addition for example, `z_new = z*z + c`. Since we are not working with a class
    that will handle complex values for us, the preceding operation is exactly what
    we need to do to compute the new real and imaginary values of `z`. We also need
    to compute the absolute value and see if it exceeds a particular value—remember
    that the absolute value of a complex number, *c = x + iy*, is computed with *√(x²+y²)*.
    It will actually save us some time here to compute the square of the upper bound
    and then plug that into the kernel, since it will save us the time of computing
    the square root of `z_re*z_re + z_im*z_im` for each iteration here.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，让我们花一点时间来讨论这段代码。让我们记住，曼德布罗特集的每一次迭代都是通过复数乘法和加法来计算的，例如，`z_new = z*z +
    c`。由于我们不是在处理一个会为我们处理复数值的类，所以前面的操作正是我们用来计算`z`的新实部和虚部的操作。我们还需要计算绝对值并查看它是否超过某个特定值——记住，复数*c
    = x + iy*的绝对值是通过*√(x²+y²)*计算的。实际上，在这里计算上限的平方并将其放入内核中可以节省我们一些时间，因为它将节省我们每次迭代计算`z_re*z_re
    + z_im*z_im`平方根的时间。
- en: 'We''re now pretty much done with this kernel—we just need to close off the
    `if` statement and return from the kernel, and we''re done:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们基本上完成了这个内核——我们只需要关闭`if`语句并从内核返回，我们就完成了：
- en: '[PRE6]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'However, we are not completely finished just yet. We need to write a host-side
    wrapper function with only `extern "C"` in the case of Linux, and `extern "C"
    __declspec(dllexport)` in the case of Windows. (In contrast to a compiled CUDA
    kernel, this extra word is necessary if we want to be able to access a host-side
    function from Ctypes in Windows.) The parameters that we put into this function
    will correspond directly to those that go into the kernel, except these will be
    stored on the host:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们还没有完全完成。我们需要为Linux编写一个仅包含`extern "C"`的主机端包装函数，而在Windows的情况下，则需要`extern
    "C" __declspec(dllexport)`。（与编译后的CUDA内核相比，如果我们想在Windows中从Ctypes访问主机端函数，这个额外的关键字是必要的。）我们放入这个函数的参数将直接对应于进入内核的参数，除了这些将存储在主机上：
- en: '[PRE7]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, the first task we will have to do is allocate sufficient memory to store
    the lattice and output on the GPU with `cudaMalloc`, and then copy the lattice
    to the GPU with `cudaMemcpy`:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们首先要做的是使用`cudaMalloc`为存储晶格和输出在GPU上分配足够的内存，然后使用`cudaMemcpy`将晶格复制到GPU上：
- en: '[PRE8]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Like many of our other kernels, we will launch this over one-dimensional blocks
    of size 32 over a one-dimensional grid. We will take the ceiling value of the
    number of output points to compute, divided by 32, to determine the grid size,
    like so:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们其他许多内核一样，我们将在这个一维网格上以大小为32的一维块来启动。我们将计算输出点的数量的天花板值除以32来确定网格大小，如下所示：
- en: '[PRE9]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now we are ready to launch our kernel by using the traditional CUDA-C triple-triangle
    brackets to specify grid and block size. Notice how we square the upper bound
    beforehand here:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备使用传统的CUDA-C三重三角形括号来指定网格和块大小来启动我们的内核。注意我们在这里先平方了上限：
- en: '[PRE10]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now we just need to copy the output to the host after this is done, and then
    call `cudaFree` on the appropriate arrays. Then we can return from this function:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们只需要在完成之后将输出复制到主机，然后对适当的数组调用`cudaFree`。然后我们可以从这个函数返回：
- en: '[PRE11]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: And with that, we are done with all of the CUDA-C code that we will need. Save
    this to a file named `mandelbrot.cu`, and let's continue to the next step.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们就完成了所有需要的CUDA-C代码。将其保存为名为`mandelbrot.cu`的文件，然后继续下一步。
- en: You can also download this file from [https://github.com/btuomanen/handsongpuprogramming/blob/master/10/mandelbrot.cu](https://github.com/btuomanen/handsongpuprogramming/blob/master/10/mandelbrot.cu).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以从[https://github.com/btuomanen/handsongpuprogramming/blob/master/10/mandelbrot.cu](https://github.com/btuomanen/handsongpuprogramming/blob/master/10/mandelbrot.cu)下载此文件。
- en: Compiling the code and interfacing with Ctypes
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编译代码和与Ctypes接口
- en: 'Now let''s compile the code we just wrote into a DLL or `.so` binary. This
    is actually fairly painless: if you are a Linux user, type the following into
    the command line to compile this file into `mandelbrot.so`:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将我们刚刚编写的代码编译成DLL或`.so`二进制文件。这实际上相当简单：如果您是Linux用户，请在命令行中输入以下内容以将此文件编译成`mandelbrot.so`：
- en: '[PRE12]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'If you are a Windows user, type the following into the command line to compile
    the file into `mandelbrot.dll`:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您是Windows用户，请在命令行中输入以下内容以将文件编译成`mandelbrot.dll`：
- en: '[PRE13]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now we can write our Python interface. We will start with the appropriate import
    statements, excluding PyCUDA completely and using just Ctypes. For ease of use,
    we''ll just import all of the classes and functions from Ctypes directly into
    the default Python namespace, like so:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以编写我们的 Python 接口。我们将从适当的导入语句开始，完全排除 PyCUDA，只使用 Ctypes。为了方便使用，我们将直接将 Ctypes
    中的所有类和函数导入到默认的 Python 命名空间中，如下所示：
- en: '[PRE14]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let''s set up an interface for the `launch_mandelbrot` host-side function using
    Ctypes. First, we will have to load our compiled DLL or `.so` file as such (Linux
    users will, of course, have to change the file name to `mandelbrot.so`):'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 Ctypes 为 `launch_mandelbrot` 主机端函数设置一个接口。首先，我们必须以这种方式加载我们的编译好的 DLL 或 `.so`
    文件（Linux 用户当然需要将文件名更改为 `mandelbrot.so`）：
- en: '[PRE15]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now we can get a reference to `launch_mandelbrot` from the library, like so;
    we''ll call it `mandel_c` for short:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以从库中获取 `launch_mandelbrot` 的引用，如下所示；我们将简称为 `mandel_c`：
- en: '[PRE16]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now before we call a function with Ctypes, we will have to make Ctypes aware
    of what the input types are. Let''s remember that for `launch_mandelbrot`, the
    inputs were `float-pointer`, `float-pointer`, `integer`, `float`, and `integer`.
    We set this up with the `argtypes` parameter, using the appropriate Ctypes datatypes
    (`c_float`, `c_int`), as well as the Ctypes `POINTER` class:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在调用 Ctypes 函数之前，我们必须让 Ctypes 了解输入的类型。让我们记住，对于 `launch_mandelbrot`，输入是 `float-pointer`、`float-pointer`、`integer`、`float`
    和 `integer`。我们通过使用适当的 Ctypes 数据类型（`c_float`、`c_int`）以及 Ctypes 的 `POINTER` 类，通过
    `argtypes` 参数来设置这些：
- en: '[PRE17]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now let''s write a Python function that will run this for us. We will specify
    the width and height of the square output image with `breadth`, and the minimum
    and maximum values in the complex lattice for both the real and imaginary components.
    We will also specify the maximum number of iterations, as well as the upper bound:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们编写一个 Python 函数来自动执行这个过程。我们将使用 `breadth` 指定正方形输出图像的宽度和高度，以及复数晶格中实部和虚部的最小值和最大值。我们还将指定最大迭代次数以及上限：
- en: '[PRE18]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now, we will create our lattice array with NumPy''s `linspace` function, like
    so:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用 NumPy 的 `linspace` 函数创建我们的晶格数组，如下所示：
- en: '[PRE19]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Let''s remember that we will have to pass a pre-allocated float array to `launch_mandelbrot`
    to get the output in the form of an output graph. We can do this by calling NumPy''s
    `empty` command to set up an array of the appropriate shape and size, which will
    act as a C `malloc` call here:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们记住，我们必须传递一个预先分配的浮点数组给 `launch_mandelbrot`，以获取以输出图形式的结果。我们可以通过调用 NumPy 的 `empty`
    命令来设置适当形状和大小的数组，这在这里将充当 C 的 `malloc` 调用：
- en: '[PRE20]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now, we are ready to compute the Mandelbrot graph. Notice that we can pass
    the NumPy arrays to C by using their `ctypes.data_as` method with the appropriate
    corresponding types. After we have done this, we can return the output; that is,
    the Mandelbrot graph in the form of a two-dimensional NumPy array:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好计算曼德布罗特图了。请注意，我们可以通过使用它们的 `ctypes.data_as` 方法以及相应的类型将 NumPy 数组传递给
    C。完成此操作后，我们可以返回输出；即，以二维 NumPy 数组形式的曼德布罗特图：
- en: '[PRE21]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, let''s write our main function to compute, time, and view the Mandelbrot
    graph with Matplotlib:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们编写我们的主函数来计算、计时并使用 Matplotlib 查看 Mandelbrot 图。
- en: '[PRE22]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We will now try running this. You should get an output that looks exactly like
    the Mandelbrot graph from [Chapter 1](f9c54d0e-6a18-49fc-b04c-d44a95e011a2.xhtml),
    *Why GPU Programming?* and [Chapter 3,](6ab0cd69-e439-4cfb-bf1a-4247ec58c94e.xhtml)
    *Getting Started with PyCUDA*:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在尝试运行这个程序。你应该得到一个输出，其外观与[第 1 章](f9c54d0e-6a18-49fc-b04c-d44a95e011a2.xhtml)“为什么进行
    GPU 编程？”和[第 3 章](6ab0cd69-e439-4cfb-bf1a-4247ec58c94e.xhtml)“使用 PyCUDA 入门”中的曼德布罗特图完全一样：
- en: '![](img/0620985f-949b-4f6b-bba4-1be7b0bf7eff.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0620985f-949b-4f6b-bba4-1be7b0bf7eff.png)'
- en: The code for this Python example is also available as the file `mandelbrot_ctypes.py`
    in the GitHub repository.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 Python 示例的代码也作为 GitHub 仓库中的文件 `mandelbrot_ctypes.py` 提供。
- en: Compiling and launching pure PTX code
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编译和启动纯 PTX 代码
- en: We have just seen how to call a pure-C function from Ctypes. In some ways, this
    may seem a little inelegant, as our binary file must contain both host code as
    well as the compiled GPU code, which may seem cumbersome. Can we just use pure,
    compiled GPU code and then launch it appropriately onto the GPU without writing
    a C wrapper each and every time? Fortunately, we can.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚看到了如何从 Ctypes 调用一个纯 C 函数。在某种程度上，这可能看起来有点不太优雅，因为我们的二进制文件必须包含主机代码以及编译后的 GPU
    代码，这可能会显得有些繁琐。我们能否只使用纯编译后的 GPU 代码，然后适当地将其加载到 GPU 上，而不必每次都编写 C 包装器？幸运的是，我们可以。
- en: The NVCC compiler compiles CUDA-C into **PTX** (**Parallel Thread Execution**),
    which is an interpreted pseudo-assembly language that is compatible across NVIDIA
    's various GPU architectures. Whenever you compile a program that uses a CUDA
    kernel with NVCC into an executable EXE, DLL, `.so`, or ELF file, there will be
    PTX code for that kernel contained within the file. We can also directly compile
    a file with the extension PTX, which will contain only the compiled GPU kernels
    from a compiled CUDA .cu file. Luckily for us, PyCUDA includes an interface to
    load a CUDA kernel directly from a PTX, freeing us from the shackles of just-in-time
    compilation while still allowing us to use all of the other nice features from
    PyCUDA.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: NVCC 编译器将 CUDA-C 编译成 **PTX**（**并行线程执行**），这是一种与 NVIDIA 各类 GPU 架构兼容的解释型伪汇编语言。每当您使用
    NVCC 编译包含 CUDA 内核的程序为可执行文件 EXE、DLL、`.so` 或 ELF 文件时，该文件中都会包含该内核的 PTX 代码。我们还可以直接编译扩展名为
    PTX 的文件，该文件将只包含从编译的 CUDA .cu 文件编译出的 GPU 内核。幸运的是，PyCUDA 包含一个接口，可以直接从 PTX 加载 CUDA
    内核，从而摆脱即时编译的束缚，同时仍然允许我们使用 PyCUDA 的所有其他优秀功能。
- en: 'Now let''s compile the Mandelbrot code we just wrote into a PTX file; we don''t
    need to make any changes to it. Just type the following into the command line
    in either Linux or Windows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将把刚才编写的 Mandelbrot 代码编译成一个 PTX 文件；我们不需要对其做任何修改。只需在 Linux 或 Windows 的命令行中输入以下内容：
- en: '[PRE23]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now let''s modify the Python program from the last section to use PTX code
    instead. We will remove `ctypes` from the imports and add the appropriate PyCUDA
    imports:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将修改上一节中的 Python 程序，使其使用 PTX 代码。我们将从导入中移除 `ctypes` 并添加适当的 PyCUDA 导入：
- en: '[PRE24]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now let''s load the PTX file using PyCUDA''s `module_from_file` function, like
    so:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用 PyCUDA 的 `module_from_file` 函数加载 PTX 文件，如下所示：
- en: '[PRE25]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now we can get a reference to our kernel with `get_function`, just like did
    with PyCUDA''s `SourceModule`:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用 `get_function` 获取我们的内核引用，就像使用 PyCUDA 的 `SourceModule` 一样：
- en: '[PRE26]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We can now rewrite the Mandelbrot function to handle using this kernel with
    the appropriate `gpuarray` objects and `typecast` inputs. (We won''t go over this
    one line-by-line since its functionality should be obvious at this point.):'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以重写 Mandelbrot 函数，使其能够使用适当的 `gpuarray` 对象和 `typecast` 输入来处理这个内核。（我们不会逐行解释这一行，因为在此阶段其功能应该是显而易见的。）：
- en: '[PRE27]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The `main` function will be exactly the same as in the last section:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`main` 函数将与上一节中的完全相同：'
- en: '[PRE28]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Now, try running this to ensure that the output is correct. You may also notice
    some speed improvements over the Ctypes version.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，尝试运行它以确保输出正确。您也可能注意到与 Ctypes 版本相比，速度有所提升。
- en: This code is also available in the `mandelbrot_ptx.py` file under the "10" directory
    in this book's GitHub repository.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码也包含在此书的 GitHub 仓库中 "10" 目录下的 `mandelbrot_ptx.py` 文件中。
- en: Writing wrappers for the CUDA Driver API
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为 CUDA 驱动 API 编写包装器
- en: We will now look at how we can write our very own wrappers for some pre-packaged
    binary CUDA library functions using Ctypes. In particular, we will be writing
    wrappers for the CUDA Driver API, which will allow us to perform all of the necessary
    operations needed for basic GPU usage—including GPU initialization, memory allocation/transfers/deallocation,
    kernel launching, and context creation/synchronization/destruction. This is a
    very powerful piece of knowledge; it will allow us to use our GPU without going
    through PyCUDA, and also without writing any cumbersome host-side C-function wrappers.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将探讨如何使用 Ctypes 为一些预包装的二进制 CUDA 库函数编写自己的包装器。特别是，我们将编写 CUDA 驱动 API 的包装器，这将使我们能够执行所有必要的操作，以实现基本的
    GPU 使用——包括 GPU 初始化、内存分配/传输/释放、内核启动以及上下文创建/同步/销毁。这是一项非常强大的知识；它将使我们能够使用 GPU，而无需通过
    PyCUDA，也无需编写任何繁琐的主机端 C 函数包装器。
- en: We will now write a small module that will act as a wrapper library for the
    **CUDA Driver API**. Let's talk about what this means for a minute. The Driver
    API is slightly different and a little more technical than the **CUDA Runtime
    API**, the latter being what we have been working within this text from CUDA-C.
    The Driver API is designed to be used with a regular C/C++ compiler rather than
    with NVCC, with some different conventions like using the `cuLaunchKernel` function
    to launch a kernel rather than using the `<<< gridsize, blocksize >>>` bracket
    notation. This will allow us to directly access the necessary functions that we
    need to launch a kernel from a PTX file with Ctypes.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将编写一个小的模块，它将作为**CUDA驱动API**的包装库。让我们先谈谈这意味着什么。驱动API与**CUDA运行时API**略有不同，后者是我们在这篇CUDA-C文本中一直在使用的，后者稍微技术性更强。驱动API旨在与常规的C/C++编译器一起使用，而不是与NVCC一起使用，有一些不同的约定，例如使用`cuLaunchKernel`函数来启动内核，而不是使用`<<<
    gridsize, blocksize >>>`括号表示法。这将使我们能够直接访问从PTX文件启动内核所需的函数，使用Ctypes。
- en: 'Let''s start writing this module by importing all of the Ctypes into the module''s
    namespace, and then importing the sys module. We will make our module usable from
    both Windows and Linux by loading the proper library file (either `nvcuda.dll`
    or `libcuda.so`) by checking the system''s OS with `sys.platform`, like so:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从将所有Ctypes导入模块命名空间开始编写这个模块，然后导入sys模块。我们将通过检查系统的OS（`sys.platform`）来加载适当的库文件（`nvcuda.dll`或`libcuda.so`），使我们的模块在Windows和Linux上均可使用：
- en: '[PRE29]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: We have successfully loaded the CUDA Driver API, and we can now begin writing
    wrappers for the necessary functions for basic GPU usage. We will look at the
    prototypes of each Driver API function as we go along, which is generally necessary
    to do when you are writing Ctypes wrappers.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已成功加载CUDA驱动API，现在我们可以开始编写用于基本GPU使用的必要函数的包装器。我们将随着每个驱动API函数的原型一起查看，这在编写Ctypes包装器时通常是必要的。
- en: 'The reader is encouraged to look up all of the functions we will be using in
    this section in the official Nvidia CUDA Driver API Documentation, which is available
    here: [https://docs.nvidia.com/cuda/cuda-driver-api/](https://docs.nvidia.com/cuda/cuda-driver-api/).'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 鼓励读者查阅本节中我们将使用的所有函数的官方Nvidia CUDA驱动API文档，该文档可在以下链接找到：[https://docs.nvidia.com/cuda/cuda-driver-api/](https://docs.nvidia.com/cuda/cuda-driver-api/)。
- en: 'Let''s start with the most fundamental function from the Driver API, `cuInit`,
    which will initialize the Driver API. This takes an unsigned integer used for
    flags as an input parameter and returns a value of type CUresult, which is actually
    just an integer value. We can write our wrapper like so:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从驱动API中最基本的函数`cuInit`开始，该函数将初始化驱动API。它接受一个用作标志的无符号整数作为输入参数，并返回一个类型为CUresult的值，实际上只是一个整数值。我们可以这样编写我们的包装器：
- en: '[PRE30]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now let''s start on the next function, `cuDeviceCount`, which will tell us
    how many NVIDIA GPUs we have installed on our computer. This takes in an integer
    pointer as its single input, which is actually a single integer output value that
    is returned by reference. The return value is another CUresult integer—all of
    the functions will use CUresult, which is a standardization of the error values
    for all of the Driver API functions. For instance, if any function we see returns
    a `0`, this means the result is `CUDA_SUCCESS`, while non-zero results will always
    mean an error or warning:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们开始编写下一个函数，`cuDeviceCount`，它将告诉我们我们在计算机上安装了多少NVIDIA GPU。它接受一个整数指针作为其单一输入，实际上是一个返回的单一整数输出值，通过引用返回。返回值是另一个CUresult整数——所有函数都将使用CUresult，这是所有驱动API函数错误值的标准化。例如，如果我们看到的任何函数返回`0`，这意味着结果是`CUDA_SUCCESS`，而非零结果将始终表示错误或警告：
- en: '[PRE31]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now let''s write a wrapper for `cuDeviceGet`, which will return a device handle
    by reference in the first input. This will correspond to the ordinal GPU given
    in the second input. The first parameter is of the type `CUdevice *`, which is
    actually just an integer pointer:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们编写一个`cuDeviceGet`的包装器，它将通过第一个输入返回一个设备句柄。这将对应于第二个输入中给出的序号GPU。第一个参数是`CUdevice
    *`类型，实际上只是一个整数指针：
- en: '[PRE32]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Let's remember that every CUDA session will require at least one CUDA Context,
    which can be thought of as analogous to a process running on the CPU. Since this
    is handled automatically with the Runtime API, here we will have to create a context
    manually on a device (using a device handle) before we can use it, and we will
    have to destroy this context when our CUDA session is over.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们记住，每个CUDA会话至少需要一个CUDA上下文，这可以被认为是与CPU上运行的进程类似。由于这是通过运行时API自动处理的，因此在这里我们将在使用它之前，必须手动在设备上（使用设备句柄）创建一个上下文，并且当我们的CUDA会话结束时，我们必须销毁这个上下文。
- en: 'We can create a CUDA context with the `cuCtxCreate` function, which will, of
    course, create a context. Let''s look at the prototype listed in the documentation:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`cuCtxCreate`函数创建一个CUDA上下文，它当然会创建一个上下文。让我们看看文档中列出的原型：
- en: '[PRE33]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Of course, the return value is `CUresult`. The first input is a pointer to
    a type called `CUcontext`, which is actually itself a pointer to a particular
    C structure used internally by CUDA. Since our only interaction with `CUcontext`
    from Python will be to hold onto its value to pass between other functions, we
    can just store `CUcontext` as a C `void *` type, which is used to store a generic
    pointer address for any type. Since this is actually a pointer to a CU context
    (again, which is itself a pointer to an internal data structure—this is another
    pass-by-reference return value), we can set the type to be just a plain `void
    *`, which is a `c_void_p` type in Ctypes. The second value is an unsigned integer,
    while the final value is the device handle on which to create the new context—let''s
    remember that this is itself just an integer. We are now prepared to create our
    wrapper for `cuCtxCreate`:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，返回值是`CUresult`。第一个输入是一个指向名为`CUcontext`的类型的指针，实际上它本身是一个指向CUDA内部使用的特定C结构的指针。由于我们从Python与`CUcontext`的唯一交互将是保留其值以在函数之间传递，因此我们可以将`CUcontext`存储为C的`void
    *`类型，它用于存储任何类型的通用指针地址。由于这实际上是一个指向CU上下文的指针（再次，它本身是一个指向内部数据结构的指针——这是另一个通过引用返回的值），我们可以将类型设置为纯`void
    *`，这在Ctypes中是`c_void_p`类型。第二个值是一个无符号整数，而最后一个值是在其上创建新上下文的设备句柄——让我们记住这本身只是一个整数。我们现在已经准备好为`cuCtxCreate`创建包装器：
- en: '[PRE34]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: You can always use the `void *` type in C/C++ (`c_void_p` in Ctypes) to point
    to any arbitrary data or variable—even structures and objects whose definition
    may not be available.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '在C/C++中，您始终可以使用`void *`类型（在Ctypes中为`c_void_p`）来指向任何任意的数据或变量——甚至定义可能不可用的结构和对象。 '
- en: 'The next function is `cuModuleLoad`, which will load a PTX module file for
    us. The first argument is a CUmodule by reference (again, we can just use a `c_void_p`
    here), and the second is the file name, which will be a typical null-terminated
    C-string—this is a `char *`, or `c_char_p` in Ctypes:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个函数是`cuModuleLoad`，它将为我们加载一个PTX模块文件。第一个参数是一个通过引用的CUmodule（再次，我们在这里可以使用`c_void_p`），第二个参数是文件名，它将是一个典型的以null结尾的C字符串——这是一个`char
    *`，或者在Ctypes中为`c_char_p`：
- en: '[PRE35]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The next function is for synchronizing all launched operations over the current
    CUDA context, and is called `cuCtxSynchronize` (this takes no arguments):'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个函数用于同步当前CUDA上下文中所有启动的操作，它被称为`cuCtxSynchronize`（此函数不接受任何参数）：
- en: '[PRE36]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The next function is used for retrieving a kernel function handle from a loaded
    module so that we may launch it onto the GPU, which corresponds exactly to PyCUDA''s
    `get_function` method, which we''ve seen many times at this point. The documentation
    tells us that the prototype is `CUresult cuModuleGetFunction ( CUfunction* hfunc,
    CUmodule hmod, const char* name )`. We can now write the wrapper:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个函数用于从加载的模块中检索内核函数句柄，以便我们可以在GPU上启动它，这正好对应于PyCUDA的`get_function`方法，我们在此处已经多次见过。文档告诉我们原型是`CUresult
    cuModuleGetFunction ( CUfunction* hfunc, CUmodule hmod, const char* name )`。我们现在可以编写包装器：
- en: '[PRE37]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now let''s write the wrappers for the standard dynamic memory operations; these
    will be necessary since we won''t have the vanity of using PyCUDA gpuarray objects.
    These are practically the same as the CUDA runtime operations that we have worked
    with before; that is, `cudaMalloc`, `cudaMemcpy`, and `cudaFree`:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来编写标准动态内存操作的包装器；这些将是有必要的，因为我们不会使用PyCUDA的gpuarray对象来炫耀。这些实际上与我们在之前工作中使用的CUDA运行时操作相同；也就是说，`cudaMalloc`、`cudaMemcpy`和`cudaFree`：
- en: '[PRE38]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Now, we will write a wrapper for the `cuLaunchKernel` function. Of course,
    this is what we will use to launch a CUDA kernel onto the GPU, provided that we
    have already initialized the CUDA Driver API, set up a context, loaded a module,
    allocated memory and configured inputs, and have extracted the kernel function
    handle from the loaded module. This one is a little more complex than the other
    functions, so we will look at the prototype:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将为`cuLaunchKernel`函数编写一个包装器。当然，这是我们用来在GPU上启动CUDA内核的方法，前提是我们已经初始化了CUDA驱动API，设置了一个上下文，加载了一个模块，分配了内存并配置了输入，并从加载的模块中提取了内核函数句柄。这个比其他函数要复杂一些，所以我们将看看原型：
- en: '[PRE39]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The first parameter is a handle to the kernel function we want to launch, which
    we can represent as `c_void_p`. The six `gridDim` and `blockDim` parameters are
    used to indicate the grid and block dimensions. The unsigned integer, `sharedMemBytes`,
    is used to indicate how many bytes of shared memory will be allocated for each
    block upon kernel launch. `CUstream hStream` is an optional parameter that we
    can use to set up a custom stream, or set to NULL (0) if we wish to use the default
    stream, which we can represent as `c_void_p` in Ctypes. Finally, the `kernelParams`
    and `extra` parameters are used to set the inputs to a kernel; these are a little
    involved, so for now just know that we can also represent these as `c_void_p`:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个参数是我们想要启动的内核函数的句柄，我们可以将其表示为`c_void_p`。六个`gridDim`和`blockDim`参数用于指示网格和块维度。无符号整数`sharedMemBytes`用于指示在内核启动时为每个块分配多少字节的共享内存。`CUstream
    hStream`是一个可选参数，我们可以用它来设置一个自定义流，或者将其设置为NULL（0），如果我们希望使用默认流，在Ctypes中我们可以将其表示为`c_void_p`。最后，`kernelParams`和`extra`参数用于设置内核的输入；这些有点复杂，所以现在只需知道我们也可以将这些表示为`c_void_p`：
- en: '[PRE40]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Now we have one last function to write a wrapper for, `cuCtxDestroy`. We use
    this at the end of a CUDA session to destroy a context on the GPU. The only input
    is a `CUcontext` object, which is represented by `c_void_p`:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们还有一个函数需要编写包装器，即`cuCtxDestroy`。我们在CUDA会话结束时使用这个函数来在GPU上销毁一个上下文。唯一的输入是一个`CUcontext`对象，它由`c_void_p`表示：
- en: '[PRE41]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Let's save this into the `cuda_driver.py` file. We have now completed our Driver
    API wrapper module! Next, we will look at how to load a PTX module and launch
    a kernel using only our module and our Mandelbrot PTX.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这个保存到`cuda_driver.py`文件中。我们现在已经完成了我们的驱动API包装模块！接下来，我们将看看如何仅使用我们的模块和我们的Mandelbrot
    PTX来加载一个PTX模块并启动一个内核。
- en: This example is also available as the `cuda_driver.py` file in this book's GitHub
    repository.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例也作为本书GitHub仓库中的`cuda_driver.py`文件提供。
- en: Using the CUDA Driver API
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用CUDA驱动API
- en: 'We will now translate our little Mandelbrot generation program so that we can
    use our wrapper library. Let''s start with the appropriate import statements;
    notice how we load all of our wrappers into the current namespace:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将翻译我们的Mandelbrot生成程序，以便我们可以使用我们的包装库。让我们从适当的导入语句开始；注意我们如何将所有的包装器加载到当前命名空间中：
- en: '[PRE42]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Let''s put all of our GPU code into the `mandelbrot` function, as we did previously.
    We will start by initializing the CUDA Driver API with `cuInit` and then checking
    if there is at least one GPU installed on the system, raising an exception otherwise:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把所有的GPU代码都放到`mandelbrot`函数中，就像我们之前做的那样。我们将从使用`cuInit`初始化CUDA驱动API开始，然后检查系统上是否至少安装了一个GPU，如果没有，则抛出异常：
- en: '[PRE43]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Notice the `byref` here: this is the Ctypes equivalent of the reference operator
    (`&`) from C programming. We''ll now apply this idea again, remembering that the
    device handle and CUDA context can be represented as `c_int` and `c_void_p` with
    Ctypes:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这里的`byref`：这是C编程中引用操作符（`&`）的Ctypes等价物。我们现在将再次应用这个想法，记住设备句柄和CUDA上下文可以用Ctypes表示为`c_int`和`c_void_p`：
- en: '[PRE44]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We will now load our PTX module, remembering to typecast the filename to a
    C string with `c_char_p`:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将加载我们的PTX模块，记住使用`c_char_p`将文件名转换为C字符串：
- en: '[PRE45]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Now we will set up the lattice on the host side, as well as a NumPy array of
    zeros called `graph` that will be used to store the output on the host side. We
    will also allocate memory on the GPU for both the lattice and the graph output,
    and then copy the lattice to the GPU with `cuMemcpyHtoD`:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将在主机端设置晶格，以及一个名为`graph`的零值NumPy数组，它将用于在主机端存储输出。我们还将为晶格和图形输出在GPU上分配内存，然后使用`cuMemcpyHtoD`将晶格复制到GPU：
- en: '[PRE46]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Now we will get a handle to the Mandelbrot kernel with `cuModuleGetFunction`
    and set up some of the inputs:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用`cuModuleGetFunction`获取Mandelbrot内核的句柄并设置一些输入：
- en: '[PRE47]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: The next step is a little complex to understand. Before we continue, we have
    to understand how the parameters are passed into a CUDA kernel with `cuLaunchKernel`.
    Let's see how this works in CUDA-C first.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步理解起来稍微复杂一些。在我们继续之前，我们必须了解如何使用 `cuLaunchKernel` 将参数传递给 CUDA 内核。让我们先看看在 CUDA-C
    中它是如何工作的。
- en: 'We express the input parameters in `kernelParams` as an array of `void *` values,
    which are, themselves, pointers to the inputs we desire to plug into our kernel.
    In the case of our Mandelbrot kernel, it would look like this:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将输入参数以 `void *` 值的数组形式表示在 `kernelParams` 中，这些值本身是指向我们想要插入内核的输入的指针。在我们的 Mandelbrot
    内核的情况下，它看起来像这样：
- en: '[PRE48]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Now let''s see how we can express this in Ctypes, which isn''t immediately
    obvious. First, let''s put all of our inputs into a Python list, in the proper
    order:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看我们如何在 Ctypes 中表达这一点，这并不立即明显。首先，让我们将所有输入放入一个 Python 列表中，按照正确的顺序：
- en: '[PRE49]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Now we need pointers to each of these values, typecast to the `void *` type.
    Let''s use the Ctypes function `addressof` to get the address of each Ctypes variable
    here (which is similar to `byref`, only not bound to a particular type), and then
    typecast it to `c_void_p`. We''ll store these values in another list:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要每个这些值的指针，将其转换为 `void *` 类型。让我们使用 Ctypes 函数 `addressof` 来获取每个 Ctypes 变量的地址（类似于
    `byref`，但未绑定到特定类型），然后将其转换为 `c_void_p`。我们将这些值存储在另一个列表中：
- en: '[PRE50]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Now let''s use Ctypes to convert this Python list to an array of `void *` pointers,
    like so:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用 Ctypes 将这个 Python 列表转换为 `void *` 指针的数组，如下所示：
- en: '[PRE51]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'We can now set up our grid''s size, as we did previously, and launch our kernel
    with this set of parameters using `cuLaunchKernel`. We then synchronize the context
    afterward:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以设置我们网格的大小，就像之前做的那样，并使用 `cuLaunchKernel` 这组参数启动我们的内核。然后我们同步上下文：
- en: '[PRE52]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'We will now copy the data from the GPU into our NumPy array using `cuMemcpyDtoH`
    with the NumPy `array.ctypes.data` member, which is a C pointer that will allow
    us to directly access the array from C as a chunk of heap memory. We will typecast
    this to `c_void_p` using the Ctypes typecast function `cast`:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将使用 `cuMemcpyDtoH` 将数据从 GPU 复制到我们的 NumPy 数组中，使用 NumPy 的 `array.ctypes.data`
    成员，这是一个 C 指针，它将允许我们直接将数组从 C 作为堆内存块访问。我们将使用 Ctypes 类型转换函数 `cast` 将其转换为 `c_void_p`：
- en: '[PRE53]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We are now done! Let''s free the arrays we allocated on the GPU and end our
    GPU session by destroying the current context. We will then return the graph NumPy
    array to the calling function:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在完成了！让我们释放我们在 GPU 上分配的数组，并通过销毁当前上下文来结束我们的 GPU 会话。然后我们将 NumPy 图形数组返回给调用函数：
- en: '[PRE54]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Now we can set up our `main` function exactly as before:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以像以前一样设置我们的 `main` 函数：
- en: '[PRE55]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Now try running this function to ensure that it yields the same output as the
    other Mandelbrot programs we just wrote.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试运行这个函数，以确保它产生与我们所写的其他 Mandelbrot 程序相同的输出。
- en: Congratulations—you've just written a direct interface to the low-level CUDA
    Driver API and successfully launched a kernel with it!
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你——你刚刚编写了一个直接访问低级 CUDA 驱动 API 的接口，并成功使用它启动了一个内核！
- en: This program is also available as the `mandelbrot_driver.py` file under the
    directory in this book's GitHub repository.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 此程序也作为 `mandelbrot_driver.py` 文件包含在此书 GitHub 仓库的目录下。
- en: Summary
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: We started this chapter with a brief overview of the Python Ctypes library,
    which is used to interface directly with compiled binary code, and particularly
    dynamic libraries written in C/C++. We then looked at how to write a C-based wrapper
    with CUDA-C that launches a CUDA kernel, and then used this to indirectly launch
    our CUDA kernel from Python by writing an interface to this function with Ctypes.
    We then learned how to compile a CUDA kernel into a PTX module binary, which can
    be thought of as a DLL but with CUDA kernel functions, and saw how to load a PTX
    file and launch pre-compiled kernels with PyCUDA. Finally, we wrote a collection
    of Ctypes wrappers for the CUDA Driver API and saw how we can use these to perform
    basic GPU operations, including launching a pre-compiled kernel from a PTX file
    onto the GPU.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章开始时简要概述了 Python Ctypes 库，该库用于直接与编译的二进制代码接口，特别是用 C/C++ 编写的动态库。然后我们探讨了如何使用
    CUDA-C 编写一个包装器来启动 CUDA 内核，然后使用它通过编写 Ctypes 函数的接口间接从 Python 启动 CUDA 内核。然后我们学习了如何将
    CUDA 内核编译成 PTX 模块二进制文件，这可以被视为 DLL，但包含 CUDA 内核函数，并看到了如何使用 PyCUDA 加载 PTX 文件并启动预编译的内核。最后，我们编写了一组
    CUDA 驱动 API 的 Ctypes 包装器，并看到了如何使用这些包装器执行基本的 GPU 操作，包括从 PTX 文件启动预编译的内核到 GPU 上。
- en: 'We will now proceed to what will arguably be the most technical chapter of
    this book: Chapter 11, *Performance Optimization in CUDA*. In this chapter, we
    will learn about some of the technical ins and outs of NVIDIA GPUs that will assist
    us in increasing performance levels in our applications.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将进入本书可能最具技术性的章节：第11章，*CUDA中的性能优化*。在这一章中，我们将了解一些关于NVIDIA GPU的技术细节，这将帮助我们提高应用程序的性能水平。
- en: Questions
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Suppose that you use `nvcc` to compile a single `.cu` file containing both host
    and kernel code into an EXE file, and also into a PTX file. Which file will contain
    the host functions, and which file will contain the GPU code?
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设你使用`nvcc`编译一个包含主机和内核代码的单一`.cu`文件，将其编译成EXE文件，同时也编译成PTX文件。哪个文件将包含主机函数，哪个文件将包含GPU代码？
- en: Why do we have to destroy a context if we are using the CUDA Driver API?
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么在使用CUDA驱动程序API时，我们必须销毁上下文？
- en: At the beginning of this chapter when we first saw how to use Ctypes, notice
    that we had to typecast the floating point value 3.14 to a Ctypes `c_double` object
    in a call to `printf` before it would work. Yet we can see many working cases
    of not typecasting to Ctypes in this chapter. Why do you think `printf` is an
    exception here?
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在本章的开头，当我们第一次看到如何使用Ctypes时，请注意，在调用`printf`之前，我们必须将浮点值3.14强制转换为Ctypes的`c_double`对象，这样它才能正常工作。然而，在本章中我们可以看到许多不需要强制转换为Ctypes就能正常工作的案例。你认为为什么`printf`在这里是个例外？
- en: Suppose you want to add functionality to our Python CUDA Driver interface module
    to support CUDA streams. How would you represent a single stream object in Ctypes?
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设你想向我们的Python CUDA驱动程序接口模块添加功能以支持CUDA流。你将如何用Ctypes表示单个流对象？
- en: Why do we use `extern "C"` for functions in `mandelbrot.cu`?
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么我们在`mandelbrot.cu`中的函数使用`extern "C"`？
- en: Look at `mandelbrot_driver.py` again. Why do we *not* use the `cuCtxSynchronize`
    function after GPU memory allocations and host/GPU memory transfers, and only
    after the single kernel invocation?
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再看看`mandelbrot_driver.py`。为什么我们在GPU内存分配和主机/GPU内存传输之后，以及仅在单个内核调用之后才使用`cuCtxSynchronize`函数？
