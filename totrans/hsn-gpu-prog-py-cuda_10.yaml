- en: Working with Compiled GPU Code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout the course of this book, we have generally been reliant on the PyCUDA
    library to interface our inline CUDA-C code for us automatically, using just-in-time
    compilation and linking with our Python code. We might recall, however, that sometimes
    the compilation process can take a while. In [Chapter 3](6ab0cd69-e439-4cfb-bf1a-4247ec58c94e.xhtml),
    *Getting Started With PyCUDA*, we even saw in detail how the compilation process
    can contribute to slowdown, and how it can even be somewhat arbitrary as to when
    inline code will be compiled and retained. In some cases, this may be inconvenient
    and cumbersome given the application, or even unacceptable in the case of a real-time
    system.
  prefs: []
  type: TYPE_NORMAL
- en: To this end, we will finally see how to use pre-compiled GPU code from Python.
    In particular, we will look at three distinct ways to do this. First, we will
    look at how we can do this by writing a host-side CUDA-C function that can indirectly
    launch a CUDA kernel. This method will involve invoking the host-side function
    with the standard Python Ctypes library. Second, we will compile our kernel into
    what is known as a PTX module, which is effectively a DLL file containing compiled
    binary GPU. We can then load this file with PyCUDA and launch our kernel directly.
    Finally, we will end this chapter by looking at how to write our own full-on Ctypes
    interface to the CUDA Driver API. We can then use the appropriate functions from
    the Driver API to load our PTX file and launch a kernel.
  prefs: []
  type: TYPE_NORMAL
- en: 'The learning outcomes for this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Launching compiled (host-side) code with the Ctypes module
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using host-side CUDA C wrappers with Ctypes to launch a kernel from Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to compile a CUDA C module into a PTX file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to load a PTX module into PyCUDA to launch pre-compiled kernels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to write your own custom Python interface to the CUDA Driver API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Launching compiled code with Ctypes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now give a brief overview of the Ctypes module from the Python Standard
    Library. Ctypes is used for calling functions from the Linux `.so` (shared object)
    or Windows. DLL (Dynamically Linked Library) pre-compiled binaries. This will
    allow us to break out of the world of pure Python and interface with libraries
    and code that have been written in compiled languages, notably C and C++—it just
    so happens that Nvidia only provides such pre-compiled binaries for interfacing
    with our CUDA device, so if we want to sidestep PyCUDA, we will have to use Ctypes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with a very basic example: we will show you how to call `printf`
    directly from Ctypes. Open up an instance of IPython and type `import ctypes`.
    We are now going to look at how to call the standard `printf` function from Ctypes.
    First, we will have to import the appropriate library: in Linux, load the LibC
    library by typing `libc = ctypes.CDLL(''libc.so.6'')` (in Windows, replace `''libc.so.6''`
    with `''msvcrt.dll''`). We can now directly call `printf` from the IPython prompt
    by typing `libc.printf("Hello from ctypes!\n")`. Try it for yourself!'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s try something else: type `libc.printf("Pi is approximately %f.\n",
    3.14)` from IPython; you should get an error. This is because the `3.14` was not
    appropriately typecast from a Python float variable to a C double variable—we
    can do this with Ctypes like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The output should be as expected. As in the case of launching a CUDA kernel
    from PyCUDA, we have to be equally careful to typecast inputs into functions with
    Ctypes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Always be sure to appropriately typecast inputs into any function that you
    call with Ctypes from Python to the appropriate C datatypes (in Ctypes, these
    are preceded by c_: `c_float`, `c_double`, `c_char`, `c_int`, and so on).'
  prefs: []
  type: TYPE_NORMAL
- en: The Mandelbrot set revisited (again)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's revisit the Mandelbrot set that we looked at in [Chapter 1](f9c54d0e-6a18-49fc-b04c-d44a95e011a2.xhtml),
    *Why GPU Programming?*, and [Chapter 3](6ab0cd69-e439-4cfb-bf1a-4247ec58c94e.xhtml),
    *Getting Started with PyCUDA*. First, we will write a full-on CUDA kernel that
    will compute the Mandelbrot set, given a particular set of parameters, along with
    an appropriate host-side wrapper function that we may interface to from Ctypes
    later. We will first be writing these functions into a single CUDA-C `.cu` source
    file and then compile this into a DLL or `.so` binary with the NVCC compiler.
    Finally, we will write some Python code so that we can run our binary code and
    display the Mandelbrot set.
  prefs: []
  type: TYPE_NORMAL
- en: We will now apply our knowledge of Ctypes to launch a pre-compiled CUDA kernel
    from Python without any assistance from PyCUDA. This will require us to write
    a host-side *k**ernel launcher* wrapper function in CUDA-C that we may call directly,
    which itself has been compiled into a dynamic library binary with any necessary
    GPU code—that is, a Dynamically Linked Library (DLL) binary on Windows, or a shared-object
    (so) binary on Linux.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start, of course, by writing our CUDA-C code, so open up your favorite
    text editor and follow along. We will begin with the standard `include` statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll now jump directly into writing our kernel. Notice `extern "C"` in the
    code, which will allow us to link to this function externally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s think for a minute about how this will work: we will use a single one-dimensional
    array for both the real and imaginary components called `lattice`, which is of
    length `lattice_size`. We will use this to compute a two-dimensional Mandelbrot
    graph of the shape (`lattice_size`, `lattice_size`) into the pre-allocated array,
    `mandelbrot_graph`. We will specify the number of iterations to check for divergence
    at each point with `max_iters`, specifying the maximum upper bound as before by
    providing its squared value with `upper_bound_squared`. (We''ll look at the motivation
    for using the square in a second.)'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will launch this kernel over a one-dimensional grid/block structure, with
    each thread corresponding to a single point in the graph image of the Mandelbrot
    set. We can then determine the real/imaginary lattice values for the corresponding
    point, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Let's talk about this for a minute. First, remember that we may have to use
    slightly more threads than necessary, so it's important that we check that the
    thread ID will correspond to some point in the output image with the `if` statement.
    Let's also remember that the output array, `mandelbrot_graph`, will be stored
    as a one-dimensional array that represents a two-dimensional image stored in a
    row-wise format, and that we will be using `tid` as the index to write in this
    array. We will use `i` and `j`, as well as the `x` and `y` coordinates of the
    graph on the complex plane. Since lattice is a series of real values sorted from
    small to large, we will have to reverse their order to get the appropriate imaginary
    values. Also, notice that we will be using plain floats here, rather than some
    structure or object to represent a complex value. Since there are real and imaginary
    components in every complex number, we will have to use two floats here to store
    the complex number corresponding to this thread's lattice point (`c_re` and `c_im`).
  prefs: []
  type: TYPE_NORMAL
- en: 'We will set up two more variables to handle the divergence check, `z_re` and
    `z_im`, and set the initial value of this thread''s point on the graph to `1`
    before we check for divergence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will do our check for divergence; if it does diverge after `max_iters`
    iterations, we set the point to `0`. Otherwise, it is left at 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Let's talk about this chunk of code for a minute before we continue. Let's remember
    that each iteration of a Mandelbrot set is computed with complex multiplication
    and addition for example, `z_new = z*z + c`. Since we are not working with a class
    that will handle complex values for us, the preceding operation is exactly what
    we need to do to compute the new real and imaginary values of `z`. We also need
    to compute the absolute value and see if it exceeds a particular value—remember
    that the absolute value of a complex number, *c = x + iy*, is computed with *√(x²+y²)*.
    It will actually save us some time here to compute the square of the upper bound
    and then plug that into the kernel, since it will save us the time of computing
    the square root of `z_re*z_re + z_im*z_im` for each iteration here.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''re now pretty much done with this kernel—we just need to close off the
    `if` statement and return from the kernel, and we''re done:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'However, we are not completely finished just yet. We need to write a host-side
    wrapper function with only `extern "C"` in the case of Linux, and `extern "C"
    __declspec(dllexport)` in the case of Windows. (In contrast to a compiled CUDA
    kernel, this extra word is necessary if we want to be able to access a host-side
    function from Ctypes in Windows.) The parameters that we put into this function
    will correspond directly to those that go into the kernel, except these will be
    stored on the host:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the first task we will have to do is allocate sufficient memory to store
    the lattice and output on the GPU with `cudaMalloc`, and then copy the lattice
    to the GPU with `cudaMemcpy`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Like many of our other kernels, we will launch this over one-dimensional blocks
    of size 32 over a one-dimensional grid. We will take the ceiling value of the
    number of output points to compute, divided by 32, to determine the grid size,
    like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we are ready to launch our kernel by using the traditional CUDA-C triple-triangle
    brackets to specify grid and block size. Notice how we square the upper bound
    beforehand here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we just need to copy the output to the host after this is done, and then
    call `cudaFree` on the appropriate arrays. Then we can return from this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: And with that, we are done with all of the CUDA-C code that we will need. Save
    this to a file named `mandelbrot.cu`, and let's continue to the next step.
  prefs: []
  type: TYPE_NORMAL
- en: You can also download this file from [https://github.com/btuomanen/handsongpuprogramming/blob/master/10/mandelbrot.cu](https://github.com/btuomanen/handsongpuprogramming/blob/master/10/mandelbrot.cu).
  prefs: []
  type: TYPE_NORMAL
- en: Compiling the code and interfacing with Ctypes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now let''s compile the code we just wrote into a DLL or `.so` binary. This
    is actually fairly painless: if you are a Linux user, type the following into
    the command line to compile this file into `mandelbrot.so`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'If you are a Windows user, type the following into the command line to compile
    the file into `mandelbrot.dll`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can write our Python interface. We will start with the appropriate import
    statements, excluding PyCUDA completely and using just Ctypes. For ease of use,
    we''ll just import all of the classes and functions from Ctypes directly into
    the default Python namespace, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s set up an interface for the `launch_mandelbrot` host-side function using
    Ctypes. First, we will have to load our compiled DLL or `.so` file as such (Linux
    users will, of course, have to change the file name to `mandelbrot.so`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can get a reference to `launch_mandelbrot` from the library, like so;
    we''ll call it `mandel_c` for short:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now before we call a function with Ctypes, we will have to make Ctypes aware
    of what the input types are. Let''s remember that for `launch_mandelbrot`, the
    inputs were `float-pointer`, `float-pointer`, `integer`, `float`, and `integer`.
    We set this up with the `argtypes` parameter, using the appropriate Ctypes datatypes
    (`c_float`, `c_int`), as well as the Ctypes `POINTER` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s write a Python function that will run this for us. We will specify
    the width and height of the square output image with `breadth`, and the minimum
    and maximum values in the complex lattice for both the real and imaginary components.
    We will also specify the maximum number of iterations, as well as the upper bound:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will create our lattice array with NumPy''s `linspace` function, like
    so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s remember that we will have to pass a pre-allocated float array to `launch_mandelbrot`
    to get the output in the form of an output graph. We can do this by calling NumPy''s
    `empty` command to set up an array of the appropriate shape and size, which will
    act as a C `malloc` call here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we are ready to compute the Mandelbrot graph. Notice that we can pass
    the NumPy arrays to C by using their `ctypes.data_as` method with the appropriate
    corresponding types. After we have done this, we can return the output; that is,
    the Mandelbrot graph in the form of a two-dimensional NumPy array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s write our main function to compute, time, and view the Mandelbrot
    graph with Matplotlib:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now try running this. You should get an output that looks exactly like
    the Mandelbrot graph from [Chapter 1](f9c54d0e-6a18-49fc-b04c-d44a95e011a2.xhtml),
    *Why GPU Programming?* and [Chapter 3,](6ab0cd69-e439-4cfb-bf1a-4247ec58c94e.xhtml)
    *Getting Started with PyCUDA*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0620985f-949b-4f6b-bba4-1be7b0bf7eff.png)'
  prefs: []
  type: TYPE_IMG
- en: The code for this Python example is also available as the file `mandelbrot_ctypes.py`
    in the GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: Compiling and launching pure PTX code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have just seen how to call a pure-C function from Ctypes. In some ways, this
    may seem a little inelegant, as our binary file must contain both host code as
    well as the compiled GPU code, which may seem cumbersome. Can we just use pure,
    compiled GPU code and then launch it appropriately onto the GPU without writing
    a C wrapper each and every time? Fortunately, we can.
  prefs: []
  type: TYPE_NORMAL
- en: The NVCC compiler compiles CUDA-C into **PTX** (**Parallel Thread Execution**),
    which is an interpreted pseudo-assembly language that is compatible across NVIDIA
    's various GPU architectures. Whenever you compile a program that uses a CUDA
    kernel with NVCC into an executable EXE, DLL, `.so`, or ELF file, there will be
    PTX code for that kernel contained within the file. We can also directly compile
    a file with the extension PTX, which will contain only the compiled GPU kernels
    from a compiled CUDA .cu file. Luckily for us, PyCUDA includes an interface to
    load a CUDA kernel directly from a PTX, freeing us from the shackles of just-in-time
    compilation while still allowing us to use all of the other nice features from
    PyCUDA.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s compile the Mandelbrot code we just wrote into a PTX file; we don''t
    need to make any changes to it. Just type the following into the command line
    in either Linux or Windows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s modify the Python program from the last section to use PTX code
    instead. We will remove `ctypes` from the imports and add the appropriate PyCUDA
    imports:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s load the PTX file using PyCUDA''s `module_from_file` function, like
    so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can get a reference to our kernel with `get_function`, just like did
    with PyCUDA''s `SourceModule`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now rewrite the Mandelbrot function to handle using this kernel with
    the appropriate `gpuarray` objects and `typecast` inputs. (We won''t go over this
    one line-by-line since its functionality should be obvious at this point.):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The `main` function will be exactly the same as in the last section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Now, try running this to ensure that the output is correct. You may also notice
    some speed improvements over the Ctypes version.
  prefs: []
  type: TYPE_NORMAL
- en: This code is also available in the `mandelbrot_ptx.py` file under the "10" directory
    in this book's GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: Writing wrappers for the CUDA Driver API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now look at how we can write our very own wrappers for some pre-packaged
    binary CUDA library functions using Ctypes. In particular, we will be writing
    wrappers for the CUDA Driver API, which will allow us to perform all of the necessary
    operations needed for basic GPU usage—including GPU initialization, memory allocation/transfers/deallocation,
    kernel launching, and context creation/synchronization/destruction. This is a
    very powerful piece of knowledge; it will allow us to use our GPU without going
    through PyCUDA, and also without writing any cumbersome host-side C-function wrappers.
  prefs: []
  type: TYPE_NORMAL
- en: We will now write a small module that will act as a wrapper library for the
    **CUDA Driver API**. Let's talk about what this means for a minute. The Driver
    API is slightly different and a little more technical than the **CUDA Runtime
    API**, the latter being what we have been working within this text from CUDA-C.
    The Driver API is designed to be used with a regular C/C++ compiler rather than
    with NVCC, with some different conventions like using the `cuLaunchKernel` function
    to launch a kernel rather than using the `<<< gridsize, blocksize >>>` bracket
    notation. This will allow us to directly access the necessary functions that we
    need to launch a kernel from a PTX file with Ctypes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start writing this module by importing all of the Ctypes into the module''s
    namespace, and then importing the sys module. We will make our module usable from
    both Windows and Linux by loading the proper library file (either `nvcuda.dll`
    or `libcuda.so`) by checking the system''s OS with `sys.platform`, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: We have successfully loaded the CUDA Driver API, and we can now begin writing
    wrappers for the necessary functions for basic GPU usage. We will look at the
    prototypes of each Driver API function as we go along, which is generally necessary
    to do when you are writing Ctypes wrappers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reader is encouraged to look up all of the functions we will be using in
    this section in the official Nvidia CUDA Driver API Documentation, which is available
    here: [https://docs.nvidia.com/cuda/cuda-driver-api/](https://docs.nvidia.com/cuda/cuda-driver-api/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with the most fundamental function from the Driver API, `cuInit`,
    which will initialize the Driver API. This takes an unsigned integer used for
    flags as an input parameter and returns a value of type CUresult, which is actually
    just an integer value. We can write our wrapper like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s start on the next function, `cuDeviceCount`, which will tell us
    how many NVIDIA GPUs we have installed on our computer. This takes in an integer
    pointer as its single input, which is actually a single integer output value that
    is returned by reference. The return value is another CUresult integer—all of
    the functions will use CUresult, which is a standardization of the error values
    for all of the Driver API functions. For instance, if any function we see returns
    a `0`, this means the result is `CUDA_SUCCESS`, while non-zero results will always
    mean an error or warning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s write a wrapper for `cuDeviceGet`, which will return a device handle
    by reference in the first input. This will correspond to the ordinal GPU given
    in the second input. The first parameter is of the type `CUdevice *`, which is
    actually just an integer pointer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Let's remember that every CUDA session will require at least one CUDA Context,
    which can be thought of as analogous to a process running on the CPU. Since this
    is handled automatically with the Runtime API, here we will have to create a context
    manually on a device (using a device handle) before we can use it, and we will
    have to destroy this context when our CUDA session is over.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create a CUDA context with the `cuCtxCreate` function, which will, of
    course, create a context. Let''s look at the prototype listed in the documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Of course, the return value is `CUresult`. The first input is a pointer to
    a type called `CUcontext`, which is actually itself a pointer to a particular
    C structure used internally by CUDA. Since our only interaction with `CUcontext`
    from Python will be to hold onto its value to pass between other functions, we
    can just store `CUcontext` as a C `void *` type, which is used to store a generic
    pointer address for any type. Since this is actually a pointer to a CU context
    (again, which is itself a pointer to an internal data structure—this is another
    pass-by-reference return value), we can set the type to be just a plain `void
    *`, which is a `c_void_p` type in Ctypes. The second value is an unsigned integer,
    while the final value is the device handle on which to create the new context—let''s
    remember that this is itself just an integer. We are now prepared to create our
    wrapper for `cuCtxCreate`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: You can always use the `void *` type in C/C++ (`c_void_p` in Ctypes) to point
    to any arbitrary data or variable—even structures and objects whose definition
    may not be available.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next function is `cuModuleLoad`, which will load a PTX module file for
    us. The first argument is a CUmodule by reference (again, we can just use a `c_void_p`
    here), and the second is the file name, which will be a typical null-terminated
    C-string—this is a `char *`, or `c_char_p` in Ctypes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The next function is for synchronizing all launched operations over the current
    CUDA context, and is called `cuCtxSynchronize` (this takes no arguments):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The next function is used for retrieving a kernel function handle from a loaded
    module so that we may launch it onto the GPU, which corresponds exactly to PyCUDA''s
    `get_function` method, which we''ve seen many times at this point. The documentation
    tells us that the prototype is `CUresult cuModuleGetFunction ( CUfunction* hfunc,
    CUmodule hmod, const char* name )`. We can now write the wrapper:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s write the wrappers for the standard dynamic memory operations; these
    will be necessary since we won''t have the vanity of using PyCUDA gpuarray objects.
    These are practically the same as the CUDA runtime operations that we have worked
    with before; that is, `cudaMalloc`, `cudaMemcpy`, and `cudaFree`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will write a wrapper for the `cuLaunchKernel` function. Of course,
    this is what we will use to launch a CUDA kernel onto the GPU, provided that we
    have already initialized the CUDA Driver API, set up a context, loaded a module,
    allocated memory and configured inputs, and have extracted the kernel function
    handle from the loaded module. This one is a little more complex than the other
    functions, so we will look at the prototype:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The first parameter is a handle to the kernel function we want to launch, which
    we can represent as `c_void_p`. The six `gridDim` and `blockDim` parameters are
    used to indicate the grid and block dimensions. The unsigned integer, `sharedMemBytes`,
    is used to indicate how many bytes of shared memory will be allocated for each
    block upon kernel launch. `CUstream hStream` is an optional parameter that we
    can use to set up a custom stream, or set to NULL (0) if we wish to use the default
    stream, which we can represent as `c_void_p` in Ctypes. Finally, the `kernelParams`
    and `extra` parameters are used to set the inputs to a kernel; these are a little
    involved, so for now just know that we can also represent these as `c_void_p`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we have one last function to write a wrapper for, `cuCtxDestroy`. We use
    this at the end of a CUDA session to destroy a context on the GPU. The only input
    is a `CUcontext` object, which is represented by `c_void_p`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Let's save this into the `cuda_driver.py` file. We have now completed our Driver
    API wrapper module! Next, we will look at how to load a PTX module and launch
    a kernel using only our module and our Mandelbrot PTX.
  prefs: []
  type: TYPE_NORMAL
- en: This example is also available as the `cuda_driver.py` file in this book's GitHub
    repository.
  prefs: []
  type: TYPE_NORMAL
- en: Using the CUDA Driver API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will now translate our little Mandelbrot generation program so that we can
    use our wrapper library. Let''s start with the appropriate import statements;
    notice how we load all of our wrappers into the current namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s put all of our GPU code into the `mandelbrot` function, as we did previously.
    We will start by initializing the CUDA Driver API with `cuInit` and then checking
    if there is at least one GPU installed on the system, raising an exception otherwise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice the `byref` here: this is the Ctypes equivalent of the reference operator
    (`&`) from C programming. We''ll now apply this idea again, remembering that the
    device handle and CUDA context can be represented as `c_int` and `c_void_p` with
    Ctypes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now load our PTX module, remembering to typecast the filename to a
    C string with `c_char_p`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will set up the lattice on the host side, as well as a NumPy array of
    zeros called `graph` that will be used to store the output on the host side. We
    will also allocate memory on the GPU for both the lattice and the graph output,
    and then copy the lattice to the GPU with `cuMemcpyHtoD`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will get a handle to the Mandelbrot kernel with `cuModuleGetFunction`
    and set up some of the inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: The next step is a little complex to understand. Before we continue, we have
    to understand how the parameters are passed into a CUDA kernel with `cuLaunchKernel`.
    Let's see how this works in CUDA-C first.
  prefs: []
  type: TYPE_NORMAL
- en: 'We express the input parameters in `kernelParams` as an array of `void *` values,
    which are, themselves, pointers to the inputs we desire to plug into our kernel.
    In the case of our Mandelbrot kernel, it would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s see how we can express this in Ctypes, which isn''t immediately
    obvious. First, let''s put all of our inputs into a Python list, in the proper
    order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we need pointers to each of these values, typecast to the `void *` type.
    Let''s use the Ctypes function `addressof` to get the address of each Ctypes variable
    here (which is similar to `byref`, only not bound to a particular type), and then
    typecast it to `c_void_p`. We''ll store these values in another list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s use Ctypes to convert this Python list to an array of `void *` pointers,
    like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now set up our grid''s size, as we did previously, and launch our kernel
    with this set of parameters using `cuLaunchKernel`. We then synchronize the context
    afterward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now copy the data from the GPU into our NumPy array using `cuMemcpyDtoH`
    with the NumPy `array.ctypes.data` member, which is a C pointer that will allow
    us to directly access the array from C as a chunk of heap memory. We will typecast
    this to `c_void_p` using the Ctypes typecast function `cast`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now done! Let''s free the arrays we allocated on the GPU and end our
    GPU session by destroying the current context. We will then return the graph NumPy
    array to the calling function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can set up our `main` function exactly as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Now try running this function to ensure that it yields the same output as the
    other Mandelbrot programs we just wrote.
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations—you've just written a direct interface to the low-level CUDA
    Driver API and successfully launched a kernel with it!
  prefs: []
  type: TYPE_NORMAL
- en: This program is also available as the `mandelbrot_driver.py` file under the
    directory in this book's GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started this chapter with a brief overview of the Python Ctypes library,
    which is used to interface directly with compiled binary code, and particularly
    dynamic libraries written in C/C++. We then looked at how to write a C-based wrapper
    with CUDA-C that launches a CUDA kernel, and then used this to indirectly launch
    our CUDA kernel from Python by writing an interface to this function with Ctypes.
    We then learned how to compile a CUDA kernel into a PTX module binary, which can
    be thought of as a DLL but with CUDA kernel functions, and saw how to load a PTX
    file and launch pre-compiled kernels with PyCUDA. Finally, we wrote a collection
    of Ctypes wrappers for the CUDA Driver API and saw how we can use these to perform
    basic GPU operations, including launching a pre-compiled kernel from a PTX file
    onto the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now proceed to what will arguably be the most technical chapter of
    this book: Chapter 11, *Performance Optimization in CUDA*. In this chapter, we
    will learn about some of the technical ins and outs of NVIDIA GPUs that will assist
    us in increasing performance levels in our applications.'
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Suppose that you use `nvcc` to compile a single `.cu` file containing both host
    and kernel code into an EXE file, and also into a PTX file. Which file will contain
    the host functions, and which file will contain the GPU code?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we have to destroy a context if we are using the CUDA Driver API?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At the beginning of this chapter when we first saw how to use Ctypes, notice
    that we had to typecast the floating point value 3.14 to a Ctypes `c_double` object
    in a call to `printf` before it would work. Yet we can see many working cases
    of not typecasting to Ctypes in this chapter. Why do you think `printf` is an
    exception here?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Suppose you want to add functionality to our Python CUDA Driver interface module
    to support CUDA streams. How would you represent a single stream object in Ctypes?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we use `extern "C"` for functions in `mandelbrot.cu`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Look at `mandelbrot_driver.py` again. Why do we *not* use the `cuCtxSynchronize`
    function after GPU memory allocations and host/GPU memory transfers, and only
    after the single kernel invocation?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
