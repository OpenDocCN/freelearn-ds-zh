<html><head></head><body><div class="chapter" title="Chapter&#xA0;6.&#xA0;Beyond the Linear Trend Line (authored by Renata Nemeth and Gergely Toth)"><div class="titlepage"><div><div><h1 class="title"><a id="ch06"/>Chapter 6. Beyond the Linear Trend Line (authored by Renata Nemeth and Gergely Toth)</h1></div></div></div><p>Linear regression models, which <a class="indexterm" id="id439"/>we covered in the previous chapter, can handle continuous responses that have a linear association with the predictors. In this chapter, we will extend these models to allow the response variable to differ in distribution. But, before getting our hands dirty with the generalized linear models, we need to stop for a while and discuss regression models in general.</p><div class="section" title="The modeling workflow"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec41"/>The modeling workflow</h1></div></div></div><p>First, some<a class="indexterm" id="id440"/> words about the terminology. Statisticians call the <span class="emphasis"><em>Y</em></span> variable the response, the <a class="indexterm" id="id441"/>outcome, or the dependent variable. The <span class="emphasis"><em>X</em></span> variables are often called the predictors, the <a class="indexterm" id="id442"/>explanatory variables, or the<a class="indexterm" id="id443"/> independent variables. Some of the predictors are of our main interest, other predictors are added just because they are potential confounders. Continuous predictors are sometimes called <a class="indexterm" id="id444"/>covariates.</p><p>The GLM<a class="indexterm" id="id445"/> is a generalization of linear regression. GLM (also referred to as <code class="literal">glm</code> in R, from the <code class="literal">stats</code> package) allows the predictors to be related to the response variable via a link function, and by allowing the magnitude of the variance of each measurement to be a function of its predicted value.</p><p>Whatever regression model you use, the main question is, "in what form can we add continuous <a class="indexterm" id="id446"/>predictors to the model?" If the relationship between the response and the predictor does not meet the model assumptions, you can transform the variable in some way. For example, a logarithmic or quadratic transformation in a linear regression model is a very common way to solve the problem of non-linear relationships between the independent and <a class="indexterm" id="id447"/>dependent variables via linear formulas.</p><p>Or, you can transform the continuous predictor into a discrete one by subdividing its range in a proper way. When choosing the classes, one of the best options is to follow some convention, like choosing 18 as a cut-point in the case of age. Or you can follow a more technical way, for example, by categorizing the predictor into quantiles. An advanced way to go about this process would be to use some classification or regression trees, on which you will be able to read more in <a class="link" href="ch10.html" title="Chapter 10. Classification and Clustering">Chapter 10</a>, <span class="emphasis"><em>Classification and Clustering</em></span>.</p><p>Discrete predictors <a class="indexterm" id="id448"/>can be added to the model as dummy variables using reference category coding, as we have seen in the previous chapter for linear regression models.</p><p>But how do we actually build a model? We have compiled a general workflow to answer this question:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">First, fit the<a class="indexterm" id="id449"/> model with the main predictors <a class="indexterm" id="id450"/>and all the relevant confounders, and then reduce the number of confounders by dropping out the non-significant ones. There are some automatic procedures (such as backward elimination) for this.<div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note50"/>Note</h3><p>The given sample size limits the number of predictors. A rule of thumb for the required sample size is that you should have at least 20 observations per predictor.</p></div></div></li><li class="listitem">Decide whether to use the continuous variables in their original or categorized form.</li><li class="listitem">Try to achieve a better fit by testing for non-linear relationships, if they are pragmatically relevant.</li><li class="listitem">Finally, check the model assumptions.</li></ol></div><p>And how do we find the best model? Is it as simple as the better the fit, the better the model? Unfortunately not. Our aim is to find the best fitting model, but with as few predictors as possible. A good model fit and a low number of independent variables are contradictory to each other.</p><p>As we have seen earlier, entering newer predictors into a linear regression model always increases the value of R-squared, and it may result in an over-fitted model. Overfitting means that the model describes the sample with its random noise, instead of the underlying data-generating process. Overfitting occurs, for example, when we have too many predictors in the model for its sample size to accommodate.</p><p>Consequently, the best model gives the desired level of fit with as few predictors as possible. AIC is one of those proper measures that takes into account both fit and parsimony. We highly recommend using it when comparing different models, which is very easy via the <code class="literal">AIC</code> function from<a class="indexterm" id="id451"/> the <code class="literal">stats</code> package.</p></div></div>
<div class="section" title="Logistic regression"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec42"/>Logistic regression</h1></div></div></div><p>So far, we have discussed linear <a class="indexterm" id="id452"/>regression models, an appropriate method to model continuous response variables. However, non-continuous, binary responses (such as being ill or healthy, being faithful or deciding to switch to a new job, mobile supplier or partner) are also very common. The main difference compared to the continuous case is that now we should rather model probability instead of the expected value of the response variable.</p><p>The naive solution would be to use the probability as<a class="indexterm" id="id453"/> outcome in a linear model. But the problem with this solution is that the probability should be always between 0 and 1, and this bounded range is not guaranteed at all when using a linear model. A better solution is to fit a logistic regression model, which models not only the probability but also the natural logarithm of the odds, called the <a class="indexterm" id="id454"/>
<span class="strong"><strong>logit</strong></span>. The logit can be any (positive or negative) number, so the problem of limited range is eliminated.</p><p>Let's have a simple example of predicting the probability of the death penalty, using some information on the race of the defendant. This model relates to the much more complicated issue of racism in the infliction of the death penalty, a question with a long history in the USA. We will use the <code class="literal">deathpenalty</code> dataset from the<a class="indexterm" id="id455"/> <code class="literal">catdata</code> package about the judgment of defendants in cases of multiple murders in Florida between 1976 and 1987. The cases are classified with respect to the death penalty (where 0 refers to no, 1 to yes), the race of the defendant, and the race of the victim (black is referred as 0, white is 1).</p><p>First, we expand the frequency table into case form via the <code class="literal">expand.dtf</code> function from the<a class="indexterm" id="id456"/> <code class="literal">vcdExtra</code> package, then we fit our first generalized model in the dataset:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; library(catdata)</strong></span>
<span class="strong"><strong>&gt; data(deathpenalty)</strong></span>
<span class="strong"><strong>&gt; library(vcdExtra)</strong></span>
<span class="strong"><strong>&gt; deathpenalty.expand &lt;- expand.dft(deathpenalty)</strong></span>
<span class="strong"><strong>&gt; binom.model.0 &lt;- glm(DeathPenalty ~ DefendantRace,</strong></span>
<span class="strong"><strong>+   data = deathpenalty.expand, family = binomial)</strong></span>
<span class="strong"><strong>&gt; summary(binom.model.0)</strong></span>

<span class="strong"><strong>Deviance Residuals: </strong></span>
<span class="strong"><strong>    Min       1Q   Median       3Q      Max  </strong></span>
<span class="strong"><strong>-0.4821  -0.4821  -0.4821  -0.4044   2.2558  </strong></span>

<span class="strong"><strong>Coefficients:</strong></span>
<span class="strong"><strong>              Estimate Std. Error z value Pr(&gt;|z|)    </strong></span>
<span class="strong"><strong>(Intercept)    -2.4624     0.2690  -9.155   &lt;2e-16 ***</strong></span>
<span class="strong"><strong>DefendantRace   0.3689     0.3058   1.206    0.228    </strong></span>
<span class="strong"><strong>---</strong></span>
<span class="strong"><strong>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</strong></span>

<span class="strong"><strong>(Dispersion parameter for binomial family taken to be 1)</strong></span>

<span class="strong"><strong>    Null deviance: 440.84  on 673  degrees of freedom</strong></span>
<span class="strong"><strong>Residual deviance: 439.31  on 672  degrees of freedom</strong></span>
<span class="strong"><strong>AIC: 443.31</strong></span>

<span class="strong"><strong>Number of Fisher Scoring iterations: 5</strong></span>
</pre></div><p>The regression coefficient is <a class="indexterm" id="id457"/>statistically not significant, so at first sight, we can't see a racial bias in the data. Anyway, for didactic purposes, let's interpret the regression coefficient. It's <code class="literal">0.37</code>, which means that the natural logarithm of the odds of getting a death penalty increases by 0.37 when moving from the black category to the white one. This difference is easily interpretable if you take its exponent, which is the ratio of the odds:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; exp(cbind(OR = coef(binom.model.0), confint(binom.model.0)))</strong></span>
<span class="strong"><strong>                      OR      2.5 %    97.5 %</strong></span>
<span class="strong"><strong>(Intercept)   0.08522727 0.04818273 0.1393442</strong></span>
<span class="strong"><strong>DefendantRace 1.44620155 0.81342472 2.7198224</strong></span>
</pre></div><p>The odds ratio pertaining to the race of the defendant is <code class="literal">1.45</code>, which means that white defendants have 45 percent larger odds of getting the death penalty than black defendants.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note51"/>Note</h3><p>Although R produces this, the odds ratio for the intercept is generally not interpreted.</p></div></div><p>We can say something more general. We have seen that in linear regression models, the regression coefficient, <span class="emphasis"><em>b</em></span>, can be interpreted as a one unit increase in <span class="emphasis"><em>X</em></span> increases <span class="emphasis"><em>Y</em></span> by <span class="emphasis"><em>b</em></span>. But, in <a class="indexterm" id="id458"/>logistic regression models, a one unit increase in <span class="emphasis"><em>X</em></span> multiplies the odds of <span class="emphasis"><em>Y</em></span> by <code class="literal">exp(b)</code>.</p><p>Please note that the preceding predictor was a discrete one, with values of 0 (black) and 1 (white), so it's basically a dummy variable for white, and black is the reference category. We have seen the same solution for entering discrete variables in the case of linear regression models. If you have more than two racial categories, you should define a second dummy for the third race and enter it into the model as well. The exponent of each dummy variables' coefficients equal to the odds ratio, which compares the given category to the reference. If you have a continuous predictor, the exponent of the coefficient equals to the odds ratio pertaining to a one unit increase in the predictor.</p><p>Now, let's enter the race of<a class="indexterm" id="id459"/> the victim into the examination, since it's a plausible confounder. Let's control for it, and fit the logistic regression model with both the <code class="literal">DefendantRace</code> and <code class="literal">VictimRace</code> as predictors:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; binom.model.1 &lt;- update(binom.model.0, . ~ . + VictimRace)</strong></span>
<span class="strong"><strong>&gt; summary(binom.model.1)</strong></span>

<span class="strong"><strong>Deviance Residuals: </strong></span>
<span class="strong"><strong>    Min       1Q   Median       3Q      Max  </strong></span>
<span class="strong"><strong>-0.7283  -0.4899  -0.4899  -0.2326   2.6919  </strong></span>

<span class="strong"><strong>Coefficients:</strong></span>
<span class="strong"><strong>              Estimate Std. Error z value Pr(&gt;|z|)    </strong></span>
<span class="strong"><strong>(Intercept)    -3.5961     0.5069  -7.094 1.30e-12 ***</strong></span>
<span class="strong"><strong>DefendantRace  -0.8678     0.3671  -2.364   0.0181 *  </strong></span>
<span class="strong"><strong>VictimRace      2.4044     0.6006   4.003 6.25e-05 ***</strong></span>
<span class="strong"><strong>---</strong></span>
<span class="strong"><strong>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</strong></span>

<span class="strong"><strong>(Dispersion parameter for binomial family taken to be 1)</strong></span>

<span class="strong"><strong>    Null deviance: 440.84  on 673  degrees of freedom</strong></span>
<span class="strong"><strong>Residual deviance: 418.96  on 671  degrees of freedom</strong></span>
<span class="strong"><strong>AIC: 424.96</strong></span>

<span class="strong"><strong>Number of Fisher Scoring iterations: 6</strong></span>

<span class="strong"><strong>&gt; exp(cbind(OR = coef(binom.model.1), confint(binom.model.1)))</strong></span>
<span class="strong"><strong>                       OR       2.5 %      97.5 %</strong></span>
<span class="strong"><strong>(Intercept)    0.02743038 0.008433309  0.06489753</strong></span>
<span class="strong"><strong>DefendantRace  0.41987565 0.209436976  0.89221877</strong></span>
<span class="strong"><strong>VictimRace    11.07226549 3.694532608 41.16558028</strong></span>
</pre></div><p>When controlling for <code class="literal">VictimRace</code>, the effect of <code class="literal">DefendantRace</code> becomes significant! The odds ratio is <code class="literal">0.42</code>, which means that white defendants' odds of getting the death penalty are only 42 percent of the odds of black defendants, holding the race of the victim fixed. Also, the odds ratio of <code class="literal">VictimRace</code> (11.07) shows an extremely strong effect: killers of white victims are 11 times more likely to get a death penalty than killers of black victims.</p><p>So, the effect of <code class="literal">DefendantRace</code> is exactly the opposite of what we have got in the one-predictor model. The reversed association may seem to be paradoxical, but it can be explained. Let's have a look at the following output:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; prop.table(table(factor(deathpenalty.expand$VictimRace,</strong></span>
<span class="strong"><strong>+              labels = c("VictimRace=0", "VictimRace=1")),</strong></span>
<span class="strong"><strong>+            factor(deathpenalty.expand$DefendantRace, </strong></span>
<span class="strong"><strong>+              labels = c("DefendantRace=0", "DefendantRace=1"))), 1)</strong></span>
<span class="strong"><strong>           </strong></span>
<span class="strong"><strong>               DefendantRace=0 DefendantRace=1</strong></span>
<span class="strong"><strong>  VictimRace=0      0.89937107      0.10062893</strong></span>
<span class="strong"><strong>  VictimRace=1      0.09320388      0.90679612</strong></span>
</pre></div><p>The data seems to be <a class="indexterm" id="id460"/>homogeneous in some sense: black defendants are more likely to have black victims, and vice versa. If you put these pieces of information together, you start to see that black defendants yield a smaller proportion of death sentences just because they are more likely to have black victims, and those who have black victims are less likely to get a death penalty. The paradox disappears: the crude death penalty and <code class="literal">DefendantRace</code> association was confounded by <code class="literal">VictimRace</code>.</p><p>To sum it up, it seems that taking the available information into account, you can come to the following conclusions:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Black defendants are more likely to get the death penalty</li><li class="listitem" style="list-style-type: disc">Killing a white person is considered to be a more serious crime than killing a black person</li></ul></div><p>Of course, you should draw such conclusions extremely carefully, as the question of racial bias needs a very thorough analysis using all the relevant information regarding the circumstances of the crime, and much more.</p><div class="section" title="Data considerations"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec40"/>Data considerations</h2></div></div></div><p>Logistic regression <a class="indexterm" id="id461"/>models work on the assumption that the observations are totally independent from each other. This assumption is violated, for example, if your observations are consecutive years. The deviance residuals and other diagnostic statistics can help validate the model and detect problems such as the misspecification<a class="indexterm" id="id462"/> of the link function. For further reference, see the <code class="literal">LogisticDx</code> package.</p><p>As a general rule of thumb, logistic regression models require at least 10 events per predictors, where an event denotes the observations belonging to the less frequent category in the response. In our death penalty example, death is the less frequent category in the response, and we have 68 death sentences in the database. So, the rule suggests that a maximum of 6-7 predictors are allowed.</p><p>The regression coefficients are estimated using the<a class="indexterm" id="id463"/> maximum likelihood method. Since there is no closed mathematical form to get these ML estimations, R uses an optimization algorithm instead. In some cases, you may get an error message that the algorithm doesn't reach convergence. In such cases, it is unable to find an appropriate solution. This may occur for a number of reasons, such as having too many predictors, too few events, and so on.</p></div><div class="section" title="Goodness of model fit"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec41"/>Goodness of model fit</h2></div></div></div><p>One measure of <a class="indexterm" id="id464"/>model fit, to evaluate the performance of the model, is the significance of the overall model. The corresponding likelihood ratio tests whether the given model fits significantly better than a model with just an intercept, which we call the<a class="indexterm" id="id465"/> null model.</p><p>To obtain the test results, you have to look at the residual deviance in the output. It measures the disagreement between the maxima of the observed and the fitted log likelihood functions.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note52"/>Note</h3><p>Since logistic regression follows the maximal likelihood principle, the goal is to minimize the sum of the deviance residuals. Therefore, this residual is parallel to the raw residual in linear regression, where the goal is to minimize the sum of squared residuals.</p></div></div><p>The null deviance represents how well the response is predicted by a model with nothing but an intercept. To judge the model, you have to compare the residual deviance to the null deviance; the difference follows a chi-square distribution. The corresponding test is available in the <a class="indexterm" id="id466"/>
<code class="literal">lmtest</code> package:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; library(lmtest)</strong></span>
<span class="strong"><strong>&gt; lrtest(binom.model.1)</strong></span>
<span class="strong"><strong>Likelihood ratio test</strong></span>

<span class="strong"><strong>Model 1: DeathPenalty ~ DefendantRace + VictimRace</strong></span>
<span class="strong"><strong>Model 2: DeathPenalty ~ 1</strong></span>
<span class="strong"><strong>  #Df  LogLik Df  Chisq Pr(&gt;Chisq)    </strong></span>
<span class="strong"><strong>1   3 -209.48                         </strong></span>
<span class="strong"><strong>2   1 -220.42 -2 21.886  1.768e-05 ***</strong></span>
<span class="strong"><strong>---</strong></span>
<span class="strong"><strong>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</strong></span>
</pre></div><p>The <span class="emphasis"><em>p</em></span> value indicates a highly significant decrease in deviance. This means that the model is significant, and the predictors have a significant effect on the response probability.</p><p>You can think of the<a class="indexterm" id="id467"/> likelihood ratio as the F-test in the <a class="indexterm" id="id468"/>linear regression models. It reveals if the model is significant, but it doesn't tell anything about the<a class="indexterm" id="id469"/> goodness-of-fit, which was described by the adjusted R-squared measure in the linear case.</p><p>An equivalent statistic for logistic regression models does not exist, but several pseudo R-squared have been developed. These usually range from 0 to 1 with higher values indicating a better fit. We will use the <code class="literal">PseudoR2</code> function from <a class="indexterm" id="id470"/>the <code class="literal">BaylorEdPsych</code> package to compute this value:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; library(BaylorEdPsych)</strong></span>
<span class="strong"><strong>&gt; PseudoR2(binom.model.1)</strong></span>
<span class="strong"><strong>        McFadden     Adj.McFadden        Cox.Snell       Nagelkerke </strong></span>
<span class="strong"><strong>      0.04964600       0.03149893       0.03195036       0.06655297</strong></span>
<span class="strong"><strong>McKelvey.Zavoina           Effron            Count        Adj.Count </strong></span>
<span class="strong"><strong>      0.15176608       0.02918095               NA               NA </strong></span>
<span class="strong"><strong>             AIC    Corrected.AIC </strong></span>
<span class="strong"><strong>    424.95652677     424.99234766  </strong></span>
</pre></div><p>But be careful, the pseudo R-squared cannot be interpreted as an OLS R-squared, and there are some documented problems with them as well, but they give us a rough picture. In our case, they say that the explanative power of the model is rather low, which is not surprising if we consider the fact that only two predictors were used in the modeling of such a complex process as judging a crime.</p></div><div class="section" title="Model comparison"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec42"/>Model comparison</h2></div></div></div><p>As we have seen in the <a class="indexterm" id="id471"/>previous chapter, the adjusted R-squared provides a good base for model comparison when dealing with nested linear regression models. For nested logistic regression models, you can use the likelihood ratio test (such as the <code class="literal">lrtest</code> function from the<a class="indexterm" id="id472"/> <code class="literal">lmtest</code> library), which compares the difference between the residual deviances.</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; lrtest(binom.model.0, binom.model.1)</strong></span>
<span class="strong"><strong>Likelihood ratio test</strong></span>

<span class="strong"><strong>Model 1: DeathPenalty ~ DefendantRace</strong></span>
<span class="strong"><strong>Model 2: DeathPenalty ~ DefendantRace + VictimRace</strong></span>
<span class="strong"><strong>  #Df  LogLik Df Chisq Pr(&gt;Chisq)    </strong></span>
<span class="strong"><strong>1   2 -219.65                        </strong></span>
<span class="strong"><strong>2   3 -209.48  1 20.35   6.45e-06 ***</strong></span>
<span class="strong"><strong>---</strong></span>
<span class="strong"><strong>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</strong></span>
</pre></div><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note53"/>Note</h3><p>
<code class="literal">LogLiK</code>, in the preceding output denotes the log-likelihood of the model; you got the residual deviance by multiplying it by 2.</p></div></div><p>For un-nested models, you can use AIC, just like we did in the case of linear regression models, but in logistic regression models, AIC is part of the standard output, so there is no need to call the AIC function separately. Here, the <code class="literal">binom.model.1</code> has a lower AIC than <code class="literal">binom.model.0</code>, and the difference is not negligible since it is greater than 2.</p></div></div>
<div class="section" title="Models for count data"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec43"/>Models for count data</h1></div></div></div><p>Logistic regression can<a class="indexterm" id="id473"/> handle only binary responses. If you have count data, such as the number of deaths or failures in a given period of time, or in a given geographical area, you can use<a class="indexterm" id="id474"/> Poisson or negative binomial regression. These data types are particularly common when working with aggregated data, which is provided as a number of events classified in different categories.</p><div class="section" title="Poisson regression"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec43"/>Poisson regression</h2></div></div></div><p>Poisson regression models<a class="indexterm" id="id475"/> are <a class="indexterm" id="id476"/>generalized linear models with the logarithm as the link function, and they assume that the response has a <a class="indexterm" id="id477"/>
<span class="strong"><strong>Poisson distribution</strong></span>. The Poisson distribution takes only integer values. It is appropriate for count data, such as events occurring over a fixed period of time, that is, if the events are rather rare, such as a number of hard drive failures per day.</p><p>In the following example, we will use the <a class="indexterm" id="id478"/>Hard Drive Data Sets for the year of 2013. The dataset was downloaded from <a class="ulink" href="https://docs.backblaze.com/public/hard-drive-data/2013_data.zip">https://docs.backblaze.com/public/hard-drive-data/2013_data.zip</a>, but we polished and simplified it a bit. Each record in the original database corresponds to a daily snapshot of one drive. The failure variable, our main point of interest, can be either zero (if the drive is OK), or one (on the last day of the hard drive before failing).</p><p>Let's try to determine<a class="indexterm" id="id479"/> which factors affect the appearance of a failure. The<a class="indexterm" id="id480"/> potential predictive factors are the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">model</code>: The manufacturer-assigned model number of the drive</li><li class="listitem" style="list-style-type: disc"><code class="literal">capacity_bytes</code>: The drive capacity in bytes</li><li class="listitem" style="list-style-type: disc"><code class="literal">age_month</code>: The drive age in the average month</li><li class="listitem" style="list-style-type: disc"><code class="literal">temperature</code>: The hard disk drive temperature</li><li class="listitem" style="list-style-type: disc"><code class="literal">PendingSector</code>: A logical value indicating the occurrence of unstable sectors (waiting for remapping on the given hard drive, on the given day)</li></ul></div><p>We aggregated the original dataset by these variables, where the <code class="literal">freq</code> variable denotes the number of records in the given category. It's time to load this final, cleansed, and aggregated dataset:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; dfa &lt;- readRDS('SMART_2013.RData')</strong></span>
</pre></div><p>Take a quick look at the number of failures by model:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; (ct &lt;- xtabs(~model+failure, data=dfa))</strong></span>
<span class="strong"><strong>              failure</strong></span>
<span class="strong"><strong>model             0    1    2    3    4    5    8</strong></span>
<span class="strong"><strong>  HGST          136    1    0    0    0    0    0</strong></span>
<span class="strong"><strong>  Hitachi      2772   72    6    0    0    0    0</strong></span>
<span class="strong"><strong>  SAMSUNG       125    0    0    0    0    0    0</strong></span>
<span class="strong"><strong>  ST1500DL001    38    0    0    0    0    0    0</strong></span>
<span class="strong"><strong>  ST1500DL003   213   39    6    0    0    0    0</strong></span>
<span class="strong"><strong>  ST1500DM003    84    0    0    0    0    0    0</strong></span>
<span class="strong"><strong>  ST2000DL001    51    4    0    0    0    0    0</strong></span>
<span class="strong"><strong>  ST2000DL003    40    7    0    0    0    0    0</strong></span>
<span class="strong"><strong>  ST2000DM001    98    0    0    0    0    0    0</strong></span>
<span class="strong"><strong>  ST2000VN000    40    0    0    0    0    0    0</strong></span>
<span class="strong"><strong>  ST3000DM001   771  122   34   14    4    2    1</strong></span>
<span class="strong"><strong>  ST31500341AS 1058   75    8    0    0    0    0</strong></span>
<span class="strong"><strong>  ST31500541AS 1010  106    7    1    0    0    0</strong></span>
<span class="strong"><strong>  ST32000542AS  803   12    1    0    0    0    0</strong></span>
<span class="strong"><strong>  ST320005XXXX  209    1    0    0    0    0    0</strong></span>
<span class="strong"><strong>  ST33000651AS  323   12    0    0    0    0    0</strong></span>
<span class="strong"><strong>  ST4000DM000   242   22   10    2    0    0    0</strong></span>
<span class="strong"><strong>  ST4000DX000   197    1    0    0    0    0    0</strong></span>
<span class="strong"><strong>  TOSHIBA       126    2    0    0    0    0    0</strong></span>
<span class="strong"><strong>  WDC          1874   27    1    2    0    0    0</strong></span>
</pre></div><p>Now, let's get rid of <a class="indexterm" id="id481"/>those hard-drive models that didn't have any failure, by <a class="indexterm" id="id482"/>removing all rows from the preceding table where there are only zeros beside the first column:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; dfa &lt;- dfa[dfa$model %in% names(which(rowSums(ct) - ct[, 1] &gt; 0)),]</strong></span>
</pre></div><p>To get a quick overview on the number of failures, let's plot a histogram on a log scale by model numbers, with the help of the<a class="indexterm" id="id483"/> <code class="literal">ggplot2</code> package:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; library(ggplot2)</strong></span>
<span class="strong"><strong>&gt; ggplot(rbind(dfa, data.frame(model='All', dfa[, -1] )), </strong></span>
<span class="strong"><strong>+   aes(failure)) + ylab("log(count)") + </strong></span>
<span class="strong"><strong>+   geom_histogram(binwidth = 1, drop=TRUE, origin = -0.5)  + </strong></span>
<span class="strong"><strong>+   scale_y_log10() + scale_x_continuous(breaks=c(0:10)) + </strong></span>
<span class="strong"><strong>+   facet_wrap( ~ model, ncol = 3) +</strong></span>
<span class="strong"><strong>+   ggtitle("Histograms by manufacturer") + theme_bw()</strong></span>
</pre></div><div class="mediaobject"><img alt="Poisson regression" src="graphics/2028OS_05_01.jpg"/></div><p>Now, it's time to fit a <a class="indexterm" id="id484"/>Poisson regression model to the data, using the <code class="literal">model</code> <a class="indexterm" id="id485"/>number as the predictor. The model can be fitted using the <code class="literal">glm</code> function with the option, <code class="literal">family=poisson</code>. By default, the expected log count is modeled, so we use the <code class="literal">log</code> link.</p><p>In the database, each observation corresponds to a group with a varying number of hard drives. As we need to handle the different group sizes, we will use the <code class="literal">offset</code> function:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; poiss.base &lt;- glm(failure ~ model, offset(log(freq)),</strong></span>
<span class="strong"><strong>+   family = 'poisson', data = dfa)</strong></span>
<span class="strong"><strong>&gt; summary(poiss.base)</strong></span>

<span class="strong"><strong>Deviance Residuals: </strong></span>
<span class="strong"><strong>    Min       1Q   Median       3Q      Max  </strong></span>
<span class="strong"><strong>-2.7337  -0.8052  -0.5160  -0.3291  16.3495  </strong></span>

<span class="strong"><strong>Coefficients:</strong></span>
<span class="strong"><strong>                  Estimate Std. Error z value Pr(&gt;|z|)    </strong></span>
<span class="strong"><strong>(Intercept)        -5.0594     0.5422  -9.331  &lt; 2e-16 ***</strong></span>
<span class="strong"><strong>modelHitachi        1.7666     0.5442   3.246  0.00117 ** </strong></span>
<span class="strong"><strong>modelST1500DL003    3.6563     0.5464   6.692 2.20e-11 ***</strong></span>
<span class="strong"><strong>modelST2000DL001    2.5592     0.6371   4.017 5.90e-05 ***</strong></span>
<span class="strong"><strong>modelST2000DL003    3.1390     0.6056   5.183 2.18e-07 ***</strong></span>
<span class="strong"><strong>modelST3000DM001    4.1550     0.5427   7.656 1.92e-14 ***</strong></span>
<span class="strong"><strong>modelST31500341AS   2.7445     0.5445   5.040 4.65e-07 ***</strong></span>
<span class="strong"><strong>modelST31500541AS   3.0934     0.5436   5.690 1.27e-08 ***</strong></span>
<span class="strong"><strong>modelST32000542AS   1.2749     0.5570   2.289  0.02208 *  </strong></span>
<span class="strong"><strong>modelST320005XXXX  -0.4437     0.8988  -0.494  0.62156    </strong></span>
<span class="strong"><strong>modelST33000651AS   1.9533     0.5585   3.497  0.00047 ***</strong></span>
<span class="strong"><strong>modelST4000DM000    3.8219     0.5448   7.016 2.29e-12 ***</strong></span>
<span class="strong"><strong>modelST4000DX000  -12.2432   117.6007  -0.104  0.91708    </strong></span>
<span class="strong"><strong>modelTOSHIBA        0.2304     0.7633   0.302  0.76279    </strong></span>
<span class="strong"><strong>modelWDC            1.3096     0.5480   2.390  0.01686 *  </strong></span>
<span class="strong"><strong>---</strong></span>
<span class="strong"><strong>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</strong></span>

<span class="strong"><strong>(Dispersion parameter for poisson family taken to be 1)</strong></span>

<span class="strong"><strong>    Null deviance: 22397  on 9858  degrees of freedom</strong></span>
<span class="strong"><strong>Residual deviance: 17622  on 9844  degrees of freedom</strong></span>
<span class="strong"><strong>AIC: 24717</strong></span>

<span class="strong"><strong>Number of Fisher Scoring iterations: 15</strong></span>
</pre></div><p>First, let's interpret the coefficients. The model number is a discrete predictor, so we entered a number of dummy<a class="indexterm" id="id486"/> variables to represent it is as a predictor. The reference category is not present in <a class="indexterm" id="id487"/>the output by default, but we can query it at any time:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; contrasts(dfa$model, sparse = TRUE)</strong></span>
<span class="strong"><strong>HGST         . . . . . . . . . . . . . .</strong></span>
<span class="strong"><strong>Hitachi      1 . . . . . . . . . . . . .</strong></span>
<span class="strong"><strong>ST1500DL003  . 1 . . . . . . . . . . . .</strong></span>
<span class="strong"><strong>ST2000DL001  . . 1 . . . . . . . . . . .</strong></span>
<span class="strong"><strong>ST2000DL003  . . . 1 . . . . . . . . . .</strong></span>
<span class="strong"><strong>ST3000DM001  . . . . 1 . . . . . . . . .</strong></span>
<span class="strong"><strong>ST31500341AS . . . . . 1 . . . . . . . .</strong></span>
<span class="strong"><strong>ST31500541AS . . . . . . 1 . . . . . . .</strong></span>
<span class="strong"><strong>ST32000542AS . . . . . . . 1 . . . . . .</strong></span>
<span class="strong"><strong>ST320005XXXX . . . . . . . . 1 . . . . .</strong></span>
<span class="strong"><strong>ST33000651AS . . . . . . . . . 1 . . . .</strong></span>
<span class="strong"><strong>ST4000DM000  . . . . . . . . . . 1 . . .</strong></span>
<span class="strong"><strong>ST4000DX000  . . . . . . . . . . . 1 . .</strong></span>
<span class="strong"><strong>TOSHIBA      . . . . . . . . . . . . 1 .</strong></span>
<span class="strong"><strong>WDC          . . . . . . . . . . . . . 1</strong></span>
</pre></div><p>So, it turns out that the reference category is <code class="literal">HGST</code>, and the dummy variables compare each model with the <code class="literal">HGST</code> hard drive. For example, the coefficient of <code class="literal">Hitachi</code> is <code class="literal">1.77</code>, so the expected log-count for <code class="literal">Hitachi</code> drives is about 1.77 greater than those for <code class="literal">HGST</code> drives. Or, you can compute its exponent when speaking about ratios instead of differences:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; exp(1.7666)</strong></span>
<span class="strong"><strong>[1] 5.850926</strong></span>
</pre></div><p>So, the expected number of failures for <code class="literal">Hitachi</code> drives is 5.85 times greater than for <code class="literal">HGST</code> drives. In general, the interpretation goes as: a one unit increase in <span class="emphasis"><em>X</em></span> multiplies <span class="emphasis"><em>Y</em></span> by <code class="literal">exp(b)</code>.</p><p>Similar to logistic regression, let's determine the significance of the model. To do this, we compare the present model to the null model without any predictors, so the difference between the residual deviance and the null deviance can be identified. We expect the difference to be large enough, and the corresponding chi-squared test to be significant:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; lrtest(poiss.base)</strong></span>
<span class="strong"><strong>Likelihood ratio test</strong></span>

<span class="strong"><strong>Model 1: failure ~ model</strong></span>
<span class="strong"><strong>Model 2: failure ~ 1</strong></span>
<span class="strong"><strong>  #Df LogLik  Df  Chisq Pr(&gt;Chisq)    </strong></span>
<span class="strong"><strong>1  15 -12344                          </strong></span>
<span class="strong"><strong>2   1 -14732 -14 4775.8  &lt; 2.2e-16 ***</strong></span>
<span class="strong"><strong>---</strong></span>
<span class="strong"><strong>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</strong></span>
</pre></div><p>And it seems that the <a class="indexterm" id="id488"/>model is significant, but we should also try to determine<a class="indexterm" id="id489"/> whether any of the model assumptions might fail.</p><p>Just like we did with the linear and logistic regression models, we have an independence assumption, where Poisson regression assumes the events to be independent. This means that the occurrence of one failure will not make another more or less likely. In the case of drive failures, this assumption holds. Another important assumption comes from the fact that the response has a Poisson distribution with an equal mean and variance. Our model assumes that the variance and the mean, conditioned on the predictor variables, will be approximately equal.</p><p>To decide whether the assumption holds, we can compare the residual deviance to its degree of freedom. For a well-fitting model, their ratio should be close to one. Unfortunately, the reported residual deviance is <code class="literal">17622</code> on <code class="literal">9844</code> degrees of freedom, so their ratio is much above<a class="indexterm" id="id490"/> one, which suggests that the variance is much greater than the mean. This phenomenon is called<a class="indexterm" id="id491"/> <span class="strong"><strong>overdispersion</strong></span>.</p></div><div class="section" title="Negative binomial regression"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec44"/>Negative binomial regression</h2></div></div></div><p>In such a case, a negative binomial distribution<a class="indexterm" id="id492"/> can be used to model an <a class="indexterm" id="id493"/>over-dispersed count response, which is a generalization of the Poisson regression since it has an extra parameter to model the over-dispersion. In other words, Poisson and the negative binomial models are nested models; the former is a subset of the latter one.</p><p>In the following output, we use the <code class="literal">glm.nb</code> function from the<a class="indexterm" id="id494"/> <code class="literal">MASS</code> package to fit a negative binomial regression to our drive failure data:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; library(MASS)</strong></span>
<span class="strong"><strong>&gt; model.negbin.0 &lt;- glm.nb(failure ~ model,</strong></span>
<span class="strong"><strong>+  offset(log(freq)), data = dfa)</strong></span>
</pre></div><p>To compare this model's performance to the Poisson model, we can use the likelihood ratio test, since the two models are nested. The negative binomial model shows a significantly better fit:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; lrtest(poiss.base,model.negbin.0)</strong></span>
<span class="strong"><strong>Likelihood ratio test</strong></span>

<span class="strong"><strong>Model 1: failure ~ model</strong></span>
<span class="strong"><strong>Model 2: failure ~ model</strong></span>
<span class="strong"><strong>  #Df LogLik Df Chisq Pr(&gt;Chisq)    </strong></span>
<span class="strong"><strong>1  15 -12344                        </strong></span>
<span class="strong"><strong>2  16 -11950  1 787.8  &lt; 2.2e-16 ***</strong></span>
<span class="strong"><strong>---</strong></span>
<span class="strong"><strong>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</strong></span>
</pre></div><p>This result clearly suggests choosing the negative binomial model.</p></div><div class="section" title="Multivariate non-linear models"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec45"/>Multivariate non-linear models</h2></div></div></div><p>So far, the only predictor in our<a class="indexterm" id="id495"/> model was the model name, but we have<a class="indexterm" id="id496"/> other potentially important information about the drives as well, such as capacity, age, and temperature. Now let's add these to the model, and determine whether the new model is better than the original one.</p><p>Furthermore, let's check the importance of <code class="literal">PendingSector</code> as well. In short, we define a two-step model building procedure with the nested models; hence we can use likelihood ratio statistics to test whether the model fit has significantly increased in both steps:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; model.negbin.1 &lt;- update(model.negbin.0, . ~ . + capacity_bytes + </strong></span>
<span class="strong"><strong>+   age_month + temperature)</strong></span>
<span class="strong"><strong>&gt; model.negbin.2 &lt;- update(model.negbin.1, . ~ . + PendingSector)</strong></span>
<span class="strong"><strong>&gt; lrtest(model.negbin.0, model.negbin.1, model.negbin.2)</strong></span>
<span class="strong"><strong>Likelihood ratio test</strong></span>

<span class="strong"><strong>Model 1: failure ~ model</strong></span>
<span class="strong"><strong>Model 2: failure ~ model + capacity_bytes + age_month + temperature</strong></span>
<span class="strong"><strong>Model 3: failure ~ model + capacity_bytes + age_month + temperature + </strong></span>
<span class="strong"><strong>    PendingSector</strong></span>
<span class="strong"><strong>  #Df LogLik Df  Chisq Pr(&gt;Chisq)    </strong></span>
<span class="strong"><strong>1  16 -11950                         </strong></span>
<span class="strong"><strong>2  19 -11510  3 878.91  &lt; 2.2e-16 ***</strong></span>
<span class="strong"><strong>3  20 -11497  1  26.84  2.211e-07 ***</strong></span>
<span class="strong"><strong>---</strong></span>
<span class="strong"><strong>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</strong></span>
</pre></div><p>Both of these steps are significant, so it was worth adding each predictor to the model. Now, let's interpret the best model:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; summary(model.negbin.2)</strong></span>

<span class="strong"><strong>Deviance Residuals: </strong></span>
<span class="strong"><strong>    Min       1Q   Median       3Q      Max  </strong></span>
<span class="strong"><strong>-2.7147  -0.7580  -0.4519  -0.2187   9.4018  </strong></span>

<span class="strong"><strong>Coefficients:</strong></span>
<span class="strong"><strong>                    Estimate Std. Error z value Pr(&gt;|z|)    </strong></span>
<span class="strong"><strong>(Intercept)       -8.209e+00  6.064e-01 -13.537  &lt; 2e-16 ***</strong></span>
<span class="strong"><strong>modelHitachi       2.372e+00  5.480e-01   4.328 1.50e-05 ***</strong></span>
<span class="strong"><strong>modelST1500DL003   6.132e+00  5.677e-01  10.801  &lt; 2e-16 ***</strong></span>
<span class="strong"><strong>modelST2000DL001   4.783e+00  6.587e-01   7.262 3.81e-13 ***</strong></span>
<span class="strong"><strong>modelST2000DL003   5.313e+00  6.296e-01   8.440  &lt; 2e-16 ***</strong></span>
<span class="strong"><strong>modelST3000DM001   4.746e+00  5.470e-01   8.677  &lt; 2e-16 ***</strong></span>
<span class="strong"><strong>modelST31500341AS  3.849e+00  5.603e-01   6.869 6.49e-12 ***</strong></span>
<span class="strong"><strong>modelST31500541AS  4.135e+00  5.598e-01   7.387 1.50e-13 ***</strong></span>
<span class="strong"><strong>modelST32000542AS  2.403e+00  5.676e-01   4.234 2.29e-05 ***</strong></span>
<span class="strong"><strong>modelST320005XXXX  1.377e-01  9.072e-01   0.152   0.8794    </strong></span>
<span class="strong"><strong>modelST33000651AS  2.470e+00  5.631e-01   4.387 1.15e-05 ***</strong></span>
<span class="strong"><strong>modelST4000DM000   3.792e+00  5.471e-01   6.931 4.17e-12 ***</strong></span>
<span class="strong"><strong>modelST4000DX000  -2.039e+01  8.138e+03  -0.003   0.9980    </strong></span>
<span class="strong"><strong>modelTOSHIBA       1.368e+00  7.687e-01   1.780   0.0751 .  </strong></span>
<span class="strong"><strong>modelWDC           2.228e+00  5.563e-01   4.006 6.19e-05 ***</strong></span>
<span class="strong"><strong>capacity_bytes     1.053e-12  5.807e-14  18.126  &lt; 2e-16 ***</strong></span>
<span class="strong"><strong>age_month          4.815e-02  2.212e-03  21.767  &lt; 2e-16 ***</strong></span>
<span class="strong"><strong>temperature       -5.427e-02  3.873e-03 -14.012  &lt; 2e-16 ***</strong></span>
<span class="strong"><strong>PendingSectoryes   2.240e-01  4.253e-02   5.267 1.39e-07 ***</strong></span>
<span class="strong"><strong>---</strong></span>
<span class="strong"><strong>Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</strong></span>

<span class="strong"><strong>(Dispersion parameter for Negative Binomial(0.8045) family taken to be 1)</strong></span>

<span class="strong"><strong>    Null deviance: 17587  on 9858  degrees of freedom</strong></span>
<span class="strong"><strong>Residual deviance: 12525  on 9840  degrees of freedom</strong></span>
<span class="strong"><strong>AIC: 23034</strong></span>

<span class="strong"><strong>Number of Fisher Scoring iterations: 1</strong></span>

<span class="strong"><strong>              Theta:  0.8045 </strong></span>
<span class="strong"><strong>          Std. Err.:  0.0525 </strong></span>

<span class="strong"><strong> 2 x log-likelihood:  -22993.8850.</strong></span>
</pre></div><p>Each predictor is significant—with a few exceptions of some contrast in model type. For example, <code class="literal">Toshiba</code> doesn't differ significantly from the reference category, <code class="literal">HGST</code>, when controlling for age, temperature, and so on.</p><p>The interpretation of the <a class="indexterm" id="id497"/>negative <a class="indexterm" id="id498"/>binomial regression parameters is similar to the Poisson model. For example, the coefficient of <code class="literal">age_month</code> is 0.048, which shows that a one month increase in age, increases the expected log-count of failures by 0.048. Or, you can opt for using exponentials as well:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; exp(data.frame(exp_coef = coef(model.negbin.2)))</strong></span>
<span class="strong"><strong>                      exp_coef</strong></span>
<span class="strong"><strong>(Intercept)       2.720600e-04</strong></span>
<span class="strong"><strong>modelHitachi      1.071430e+01</strong></span>
<span class="strong"><strong>modelST1500DL003  4.602985e+02</strong></span>
<span class="strong"><strong>modelST2000DL001  1.194937e+02</strong></span>
<span class="strong"><strong>modelST2000DL003  2.030135e+02</strong></span>
<span class="strong"><strong>modelST3000DM001  1.151628e+02</strong></span>
<span class="strong"><strong>modelST31500341AS 4.692712e+01</strong></span>
<span class="strong"><strong>modelST31500541AS 6.252061e+01</strong></span>
<span class="strong"><strong>modelST32000542AS 1.106071e+01</strong></span>
<span class="strong"><strong>modelST320005XXXX 1.147622e+00</strong></span>
<span class="strong"><strong>modelST33000651AS 1.182098e+01</strong></span>
<span class="strong"><strong>modelST4000DM000  4.436067e+01</strong></span>
<span class="strong"><strong>modelST4000DX000  1.388577e-09</strong></span>
<span class="strong"><strong>modelTOSHIBA      3.928209e+00</strong></span>
<span class="strong"><strong>modelWDC          9.283970e+00</strong></span>
<span class="strong"><strong>capacity_bytes    1.000000e+00</strong></span>
<span class="strong"><strong>age_month         1.049329e+00</strong></span>
<span class="strong"><strong>temperature       9.471743e-01</strong></span>
<span class="strong"><strong>PendingSectoryes  1.251115e+00</strong></span>
</pre></div><p>So, it seems that one month in a lifetime increases the expected number of failures by 4.9 percent, and a larger capacity also increases the number of failures. On the other hand, temperature shows a reversed effect: the exponent of the coefficient is 0.947, which says that one degree of increased warmth decreases the expected number of failures by 5.3 percent.</p><p>The effect of the model name can be judged on the basis of comparison to the reference category, which is <code class="literal">HGST</code> in our case. One may want to change this reference. For example, for the most common drive: <code class="literal">WDC</code>. This can be easily done by changing the order of the factor levels in hard drive models, or simply defining the reference category in the factor via the extremely useful <code class="literal">relevel</code> function:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; dfa$model &lt;- relevel(dfa$model, 'WDC')</strong></span>
</pre></div><p>Now, let's verify if <a class="indexterm" id="id499"/>
<code class="literal">HGST</code> indeed <a class="indexterm" id="id500"/>replaced <code class="literal">WDC</code> in the coefficients list, but instead of the lengthy output of summary, we will use the <code class="literal">tidy</code> function from <a class="indexterm" id="id501"/>the <code class="literal">broom</code> package, which can extract the most important features (for the model summary, take a look at the <code class="literal">glance</code> function) of different statistical models:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; model.negbin.3 &lt;- update(model.negbin.2, data = dfa)</strong></span>
<span class="strong"><strong>&gt; library(broom)</strong></span>
<span class="strong"><strong>&gt; format(tidy(model.negbin.3), digits = 4)</strong></span>
<span class="strong"><strong>                term   estimate std.error statistic    p.value</strong></span>
<span class="strong"><strong>1        (Intercept) -5.981e+00 2.173e-01 -27.52222 9.519e-167</strong></span>
<span class="strong"><strong>2          modelHGST -2.228e+00 5.563e-01  -4.00558  6.187e-05</strong></span>
<span class="strong"><strong>3       modelHitachi  1.433e-01 1.009e-01   1.41945  1.558e-01</strong></span>
<span class="strong"><strong>4   modelST1500DL003  3.904e+00 1.353e-01  28.84295 6.212e-183</strong></span>
<span class="strong"><strong>5   modelST2000DL001  2.555e+00 3.663e-01   6.97524  3.054e-12</strong></span>
<span class="strong"><strong>6   modelST2000DL003  3.085e+00 3.108e-01   9.92496  3.242e-23</strong></span>
<span class="strong"><strong>7   modelST3000DM001  2.518e+00 9.351e-02  26.92818 1.028e-159</strong></span>
<span class="strong"><strong>8  modelST31500341AS  1.620e+00 1.069e-01  15.16126  6.383e-52</strong></span>
<span class="strong"><strong>9  modelST31500541AS  1.907e+00 1.016e-01  18.77560  1.196e-78</strong></span>
<span class="strong"><strong>10 modelST32000542AS  1.751e-01 1.533e-01   1.14260  2.532e-01</strong></span>
<span class="strong"><strong>11 modelST320005XXXX -2.091e+00 7.243e-01  -2.88627  3.898e-03</strong></span>
<span class="strong"><strong>12 modelST33000651AS  2.416e-01 1.652e-01   1.46245  1.436e-01</strong></span>
<span class="strong"><strong>13  modelST4000DM000  1.564e+00 1.320e-01  11.84645  2.245e-32</strong></span>
<span class="strong"><strong>14  modelST4000DX000 -1.862e+01 1.101e+03  -0.01691  9.865e-01</strong></span>
<span class="strong"><strong>15      modelTOSHIBA -8.601e-01 5.483e-01  -1.56881  1.167e-01</strong></span>
<span class="strong"><strong>16    capacity_bytes  1.053e-12 5.807e-14  18.12597  1.988e-73</strong></span>
<span class="strong"><strong>17         age_month  4.815e-02 2.212e-03  21.76714 4.754e-105</strong></span>
<span class="strong"><strong>18       temperature -5.427e-02 3.873e-03 -14.01175  1.321e-44</strong></span>
<span class="strong"><strong>19  PendingSectoryes  2.240e-01 4.253e-02   5.26709  1.386e-07</strong></span>
</pre></div><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note54"/>Note</h3><p>Use the <code class="literal">broom</code> package to extract model coefficients, compare model fit, and other metrics to be passed to, for example, <code class="literal">ggplot2</code>.</p></div></div><p>The effect of temperature suggests that the higher the temperature, the lower the number of hard drive failures. However, everyday experiences show a very different picture, for example, as described at <a class="ulink" href="https://www.backblaze.com/blog/hard-drive-temperature-does-it-matter">https://www.backblaze.com/blog/hard-drive-temperature-does-it-matter</a>. Google engineers found that temperature was not a good predictor of<a class="indexterm" id="id502"/> failure, while <a class="indexterm" id="id503"/>Microsoft and the University of Virginia found that it had a significant effect. Disk drive manufacturers suggest keeping disks at cooler temperatures.</p><p>So, let's take a closer look at this interesting question, and we will have the <code class="literal">temperature</code> as a predictor of drive failure. First, let's classify temperature into six equal categories, and then we will draw a bar plot presenting the mean number of failures per categories. Note that we have to take into account the different groups' sizes, so we will weight by <code class="literal">freq</code>, and as we are doing some data aggregation, it's the right time to convert our dataset into a <code class="literal">data.table</code> object:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; library(data.table)</strong></span>
<span class="strong"><strong>&gt; dfa &lt;- data.table(dfa)</strong></span>
<span class="strong"><strong>&gt; dfa[, temp6 := cut2(temperature, g = 6)]</strong></span>
<span class="strong"><strong>&gt; temperature.weighted.mean &lt;- dfa[, .(wfailure = </strong></span>
<span class="strong"><strong>+     weighted.mean(failure, freq)), by = temp6] </strong></span>
<span class="strong"><strong>&gt; ggplot(temperature.weighted.mean, aes(x = temp6, y = wfailure)) +  </strong></span>
<span class="strong"><strong>+     geom_bar(stat = 'identity') + xlab('Categorized temperature') +</strong></span>
<span class="strong"><strong>+     ylab('Weighted mean of disk faults') + theme_bw()</strong></span>
</pre></div><div class="mediaobject"><img alt="Multivariate non-linear models" src="graphics/2028OS_05_02.jpg"/></div><p>The assumption of linear<a class="indexterm" id="id504"/> relation is clearly not supported. The bar plot<a class="indexterm" id="id505"/> suggests using the temperature in this classified form, instead of the original continuous variable when entering the model. To actually see which model is better, let's compare those! Since they are not nested, we have to use the AIC, which strongly supports the categorized version:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; model.negbin.4 &lt;- update(model.negbin.0, .~. + capacity_bytes +</strong></span>
<span class="strong"><strong>+   age_month + temp6 + PendingSector, data = dfa)</strong></span>
<span class="strong"><strong>&gt; AIC(model.negbin.3,model.negbin.4)</strong></span>
<span class="strong"><strong>               df      AIC</strong></span>
<span class="strong"><strong>model.negbin.3 20 23033.88</strong></span>
<span class="strong"><strong>model.negbin.4 24 22282.47</strong></span>
</pre></div><p>Well, it was really worth categorizing temperature! Now, let's check the other two continuous predictors as well. Again, we will use <code class="literal">freq</code> as a weighting factor:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; weighted.means &lt;- rbind(</strong></span>
<span class="strong"><strong>+     dfa[, .(l = 'capacity', f = weighted.mean(failure, freq)),</strong></span>
<span class="strong"><strong>+         by = .(v = capacity_bytes)],</strong></span>
<span class="strong"><strong>+     dfa[, .(l = 'age', f = weighted.mean(failure, freq)),</strong></span>
<span class="strong"><strong>+         by = .(v = age_month)])</strong></span>
</pre></div><p>As in the previous plots, <a class="indexterm" id="id506"/>we will use <code class="literal">ggplot2</code> to plot<a class="indexterm" id="id507"/> the distribution of these discrete variables, but instead of a bar plot, we will use a stair-line chart to overcome the issue of the fixed width of bar charts:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; ggplot(weighted.means, aes(x = l, y = f)) + geom_step() +</strong></span>
<span class="strong"><strong>+   facet_grid(. ~ v, scales = 'free_x') + theme_bw() +</strong></span>
<span class="strong"><strong>+   ylab('Weighted mean of disk faults') + xlab('')</strong></span>
</pre></div><div class="mediaobject"><img alt="Multivariate non-linear models" src="graphics/2028OS_05_03.jpg"/></div><p>The relations are again, clearly not linear. The case of <code class="literal">age</code> is particularly interesting; there seems to be highly risky periods in the hard drives' lifetime. Now, let's force R to use <code class="literal">capacity</code> as a nominal variable (it has only five values, so there is no real <a class="indexterm" id="id508"/>need to categorize it), and let's classify <code class="literal">age</code> into 8 equally sized categories: </p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; dfa[, capacity_bytes := as.factor(capacity_bytes)]</strong></span>
<span class="strong"><strong>&gt; dfa[, age8 := cut2(age_month, g = 8)]</strong></span>
<span class="strong"><strong>&gt; model.negbin.5 &lt;- update(model.negbin.0, .~. + capacity_bytes +</strong></span>
<span class="strong"><strong>+   age8 + temp6 + PendingSector, data = dfa)</strong></span>
</pre></div><p>According to the AIC, the last model with the categorized age and capacity is much better, and is the best fitting model so far:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; AIC(model.negbin.5, model.negbin.4)</strong></span>
<span class="strong"><strong>               df      AIC</strong></span>
<span class="strong"><strong>model.negbin.5 33 22079.47</strong></span>
<span class="strong"><strong>model.negbin.4 24 22282.47</strong></span>
</pre></div><p>If you look at the parameter <a class="indexterm" id="id509"/>estimates, you can see that the first dummy variable on capacity significantly differ from the reference:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; format(tidy(model.negbin.5), digits = 3)</strong></span>
<span class="strong"><strong>                          term estimate std.error statistic   p.value</strong></span>
<span class="strong"><strong>1                  (Intercept)  -6.1648  1.84e-01 -3.34e+01 2.69e-245</strong></span>
<span class="strong"><strong>2                    modelHGST  -2.4747  5.63e-01 -4.40e+00  1.10e-05</strong></span>
<span class="strong"><strong>3                 modelHitachi  -0.1119  1.21e-01 -9.25e-01  3.55e-01</strong></span>
<span class="strong"><strong>4             modelST1500DL003  31.7680  7.05e+05  4.51e-05  1.00e+00</strong></span>
<span class="strong"><strong>5             modelST2000DL001   1.5216  3.81e-01  3.99e+00  6.47e-05</strong></span>
<span class="strong"><strong>6             modelST2000DL003   2.1055  3.28e-01  6.43e+00  1.29e-10</strong></span>
<span class="strong"><strong>7             modelST3000DM001   2.4799  9.54e-02  2.60e+01 5.40e-149</strong></span>
<span class="strong"><strong>8            modelST31500341AS  29.4626  7.05e+05  4.18e-05  1.00e+00</strong></span>
<span class="strong"><strong>9            modelST31500541AS  29.7597  7.05e+05  4.22e-05  1.00e+00</strong></span>
<span class="strong"><strong>10           modelST32000542AS  -0.5419  1.93e-01 -2.81e+00  5.02e-03</strong></span>
<span class="strong"><strong>11           modelST320005XXXX  -2.8404  7.33e-01 -3.88e+00  1.07e-04</strong></span>
<span class="strong"><strong>12           modelST33000651AS   0.0518  1.66e-01  3.11e-01  7.56e-01</strong></span>
<span class="strong"><strong>13            modelST4000DM000   1.2243  1.62e-01  7.54e+00  4.72e-14</strong></span>
<span class="strong"><strong>14            modelST4000DX000 -29.6729  2.55e+05 -1.16e-04  1.00e+00</strong></span>
<span class="strong"><strong>15                modelTOSHIBA  -1.1658  5.48e-01 -2.13e+00  3.33e-02</strong></span>
<span class="strong"><strong>16 capacity_bytes1500301910016 -27.1391  7.05e+05 -3.85e-05  1.00e+00</strong></span>
<span class="strong"><strong>17 capacity_bytes2000398934016   1.8165  2.08e-01  8.73e+00  2.65e-18</strong></span>
<span class="strong"><strong>18 capacity_bytes3000592982016   2.3515  1.88e-01  1.25e+01  8.14e-36</strong></span>
<span class="strong"><strong>19 capacity_bytes4000787030016   3.6023  2.25e-01  1.60e+01  6.29e-58</strong></span>
<span class="strong"><strong>20                 age8[ 5, 9)  -0.5417  7.55e-02 -7.18e+00  7.15e-13</strong></span>
<span class="strong"><strong>21                 age8[ 9,14)  -0.0683  7.48e-02 -9.12e-01  3.62e-01</strong></span>
<span class="strong"><strong>22                 age8[14,19)   0.3499  7.24e-02  4.83e+00  1.34e-06</strong></span>
<span class="strong"><strong>23                 age8[19,25)   0.7383  7.33e-02  1.01e+01  7.22e-24</strong></span>
<span class="strong"><strong>24                 age8[25,33)   0.5896  1.14e-01  5.18e+00  2.27e-07</strong></span>
<span class="strong"><strong>25                 age8[33,43)   1.5698  1.05e-01  1.49e+01  1.61e-50</strong></span>
<span class="strong"><strong>26                 age8[43,60]   1.9105  1.06e-01  1.81e+01  3.59e-73</strong></span>
<span class="strong"><strong>27                temp6[22,24)   0.7582  5.01e-02  1.51e+01  8.37e-52</strong></span>
<span class="strong"><strong>28                temp6[24,27)   0.5005  4.78e-02  1.05e+01  1.28e-25</strong></span>
<span class="strong"><strong>29                temp6[27,30)   0.0883  5.40e-02  1.64e+00  1.02e-01</strong></span>
<span class="strong"><strong>30                temp6[30,33)  -1.0627  9.20e-02 -1.15e+01  7.49e-31</strong></span>
<span class="strong"><strong>31                temp6[33,50]  -1.5259  1.37e-01 -1.11e+01  1.23e-28</strong></span>
<span class="strong"><strong>32            PendingSectoryes   0.1301  4.12e-02  3.16e+00  1.58e-03</strong></span>
</pre></div><p>The next three capacities are more likely to cause failures, but the trend is not linear. The effect of age also does not seem to be linear. In general, aging increases the number of failures, but there are some exceptions. For example, drives are significantly more likely to have a failure in the first (reference) age group than in the second one. This finding is plausible since drives have a higher failure rate at the beginning of their operation. The effect of temperature suggests that the middle temperature (22-30 degrees Celsius) is more likely to cause failures<a class="indexterm" id="id510"/> than low or high temperatures. Remember that<a class="indexterm" id="id511"/> each effect is controlled for every other predictor.</p><p>It would also be important to judge the effect-size of different predictors, comparing them to each other. As a picture is worth a thousand words, let's summarize the coefficients with the confidence intervals in one plot.</p><p>First, we have to extract the significant terms from the model:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; tmnb5 &lt;- tidy(model.negbin.5)</strong></span>
<span class="strong"><strong>&gt; str(terms &lt;- tmnb5$term[tmnb5$p.value &lt; 0.05][-1])</strong></span>
<span class="strong"><strong> chr [1:22] "modelHGST" "modelST2000DL001" "modelST2000DL003" ...</strong></span>
</pre></div><p>Then, let's identify the confidence intervals of the coefficients using the <code class="literal">confint</code> function and the good old<a class="indexterm" id="id512"/> <code class="literal">plyr</code> package:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; library(plyr)</strong></span>
<span class="strong"><strong>&gt; ci &lt;- ldply(terms, function(t) confint(model.negbin.5, t))</strong></span>
</pre></div><p>Unfortunately, this resulting data frame is not yet complete. We need to add the term names, and also, let's extract the grouping variables via a simple, regular expression:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; names(ci) &lt;- c('min', 'max')</strong></span>
<span class="strong"><strong>&gt; ci$term &lt;- terms</strong></span>
<span class="strong"><strong>&gt; ci$variable &lt;- sub('[A-Z0-9\\]\\[,() ]*$', '', terms, perl = TRUE)</strong></span>
</pre></div><p>And now we have the <a class="indexterm" id="id513"/>confidence intervals of the<a class="indexterm" id="id514"/> coefficients in a nicely formatted dataset, which can be easily plotted by <code class="literal">ggplot</code>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; ggplot(ci, aes(x = factor(term), color = variable)) + </strong></span>
<span class="strong"><strong>+     geom_errorbar(ymin = min, ymax = max) + xlab('') +</strong></span>
<span class="strong"><strong>+     ylab('Coefficients (95% conf.int)') + theme_bw() + </strong></span>
<span class="strong"><strong>+     theme(axis.text.x = element_text(angle = 90, hjust = 1),</strong></span>
<span class="strong"><strong>+         legend.position = 'top')</strong></span>
</pre></div><div class="mediaobject"><img alt="Multivariate non-linear models" src="graphics/2028OS_05_04.jpg"/></div><p>It can be easily seen that although each predictor is significant, the size of their effects strongly differ. For <a class="indexterm" id="id515"/>example, <code class="literal">PendingSector</code> has just a slight effect on the <a class="indexterm" id="id516"/>number of failures, but <code class="literal">age</code>, <code class="literal">capacity</code>, and <code class="literal">temperature</code> have a much stronger effect, and the hard drive model is the predictor that best differentiates the number of failures.</p><p>As we have mentioned in the <span class="emphasis"><em>Logistic regression</em></span> section, different pseudo R-squared measures are available for nonlinear models as well. We again warn you to use these metrics with reservation. Anyway, in our case, they uniformly suggest the model's explanative power to be pretty good:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; PseudoR2(model.negbin.6 )</strong></span>
<span class="strong"><strong>        McFadden     Adj.McFadden        Cox.Snell       Nagelkerke </strong></span>
<span class="strong"><strong>       0.3352654        0.3318286        0.4606953        0.5474952 </strong></span>
<span class="strong"><strong>McKelvey.Zavoina           Effron            Count        Adj.Count </strong></span>
<span class="strong"><strong>              NA        0.1497521        0.9310444       -0.1943522 </strong></span>
<span class="strong"><strong>             AIC    Corrected.AIC </strong></span>
<span class="strong"><strong>   12829.5012999    12829.7044941 </strong></span>
</pre></div></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec44"/>Summary</h1></div></div></div><p>This chapter introduced three well known nonlinear regression models: the logistic, Poisson, and negative binomial models, and you became familiar with the general logic of modeling. It was also shown how the same concepts, such as effect of predictors, goodness of fit, explanative power, model comparison for nested and non-nested models, and model building are applied in different contexts. Now, having spent some time on mastering the data analysis skills, in the next chapter, we will get back to some hardcore data science problems, such as the cleansing and structuring of data.</p></div></body></html>