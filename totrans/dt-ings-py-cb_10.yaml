- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Logging and Monitoring Your Data Ingest in Airﬂow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We already know how vital logging and monitoring are to manage applications
    and systems, and Airflow is no different. In fact, **Apache Airflow** already
    has built-in modules to create logs and export them. But what about improving
    them?
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, *Putting Everything Together with Airﬂow*, we covered
    the fundamental aspects of Airflow, how to start our data ingestion, and how to
    orchestrate a pipeline and use the best data development practices. Now, let’s
    put into practice the best techniques to enhance logging and monitor Airflow pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating basic logs in Airﬂow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing log files in a remote location
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring logs in `airflow.cfg`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing advanced monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using notiﬁcation operators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using SQL operators for data quality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can find the code for this chapter in the GitHub repository here: [https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook).'
  prefs: []
  type: TYPE_NORMAL
- en: Installing and running Airflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This chapter requires that Airflow is installed on your local machine. You can
    install it directly on your **operating system** (**OS**) or by using a Docker
    image. For more information, refer to the *Configuring Docker for Airflow* recipe
    in [*Chapter 1*](B19453_01.xhtml#_idTextAnchor022).
  prefs: []
  type: TYPE_NORMAL
- en: 'After following the steps described in [*Chapter 1*](B19453_01.xhtml#_idTextAnchor022),
    ensure your Airflow runs correctly. You can do that by checking the Airflow UI
    here: `http://localhost:8080`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are using a Docker container (as I am) to host your Airflow application,
    you can check its status on the terminal by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see the command running here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1 – Airflow containers running](img/Figure_9.01_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 – Airflow containers running
  prefs: []
  type: TYPE_NORMAL
- en: 'For Docker, check the container status on **Docker Desktop**, as shown in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.2 – The Docker Desktop version of Airflow containers running](img/Figure_10.02_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 – The Docker Desktop version of Airflow containers running
  prefs: []
  type: TYPE_NORMAL
- en: Airflow environment variables in docker-compose
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section is aimed at users with Airflow running in a Docker container. If
    you install it directly on your machine, you can skip it.
  prefs: []
  type: TYPE_NORMAL
- en: We need to configure or change Airflow environment variables to complete most
    of the recipes in this chapter. This kind of configuration is supposed to be done
    by editing the `airflow.cfg` file. However, this can be tricky if you opt to run
    your Airflow application using `docker-compose`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ideally, we should be able to access the `airflow.cfg` file by mounting a volume
    in `docker-compose.yaml`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.3 – docker-compose.yaml volumes](img/Figure_10.03_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 – docker-compose.yaml volumes
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, instead of reflecting the file in the local machine, it creates
    a directory named `airflow.cfg`. It is a bug known by the community (see [https://github.com/puckel/docker-airflow/issues/571](https://github.com/puckel/docker-airflow/issues/571))
    with no resolution.
  prefs: []
  type: TYPE_NORMAL
- en: 'To work around it, we will set all the `airflow.cfg` configurations in `docker-compose.yaml`
    using the environment variables, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: For users who install and run Airflow directly on their local machine, you can
    proceed by following the steps that instruct you how to edit the `airflow.cfg`
    file.
  prefs: []
  type: TYPE_NORMAL
- en: Creating basic logs in Airﬂow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The internal Airflow logging library is based on the Python built-in logs, which
    provide flexible and configurable forms to capture and store log messages using
    different components of **directed acyclic graphs** (**DAGs**). Let’s start this
    chapter by covering the basic concepts of how Airflow logs work. This knowledge
    will allow us to apply more advanced concepts and create mature data ingestion
    pipelines in real-life projects.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will create a simple DAG to generate logs based on the default
    configurations of Airflow. We will also understand how Airflow internally sets
    the logging architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the *Technical requirements* section for this recipe, since we will
    handle it with the same technology.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we will create a new DAG, let’s create a folder under the `dag/` directory
    called `basic_logging` and a file inside it called `basic_logging_dag.py` to insert
    our script. By the end, your folder structure should look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.4 – An Airflow directory with a basic_logging DAG structure](img/Figure_10.04_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.4 – An Airflow directory with a basic_logging DAG structure
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The goal is to understand how to create logs in Airflow properly so that the
    DAG script will be pretty straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by importing the Airflow and Python libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, let’s get the log configuration we want to use:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s define `default_args` and the DAG object which Airflow can create:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Unlike in [*Chapter 9*](B19453_09.xhtml#_idTextAnchor319), here we will define
    which tasks belong to this DAG by assigning them to the operator instantiation
    in *step 5* of this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s create three example functions only to return log messages. The
    functions will be named after the ETL steps, as you can see here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Feel free to insert more log levels if you want to.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each function, we will set a task using `PythonOperator` and the execution
    order:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You can see that we referred the DAG to each task by assigning the `dag` object
    (defined in *step 4*) to a `dag` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Save the file and go to the Airflow UI.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Airflow UI, look for the **basic_logging_dag** DAG and enable it by
    clicking the toggle button. The job will start right away, and if you check the
    **Graph** vision of the DAG, you should see something similar to the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.5 – The DAG Graph view showing the successful state of the tasks](img/Figure_10.05_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.5 – The DAG Graph view showing the successful state of the tasks
  prefs: []
  type: TYPE_NORMAL
- en: It means the pipeline ran successfully!
  prefs: []
  type: TYPE_NORMAL
- en: Let’s check the `logs/` directory on our local machine. This directory is at
    the same level as the `DAGs` folder, where we put our scripts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can see more folders inside if you open the `logs/` folder. Look for the
    one beginning with `dag_id= basic_logging` and open it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.6 – The Airflow logs folder for the basic_logging DAG and its tasks](img/Figure_10.06_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.6 – The Airflow logs folder for the basic_logging DAG and its tasks
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, select the folder named `task_id=transform_data` and open the log file
    inside. You should see something like the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.7 – Log messages for the transform_data task](img/Figure_10.07_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.7 – Log messages for the transform_data task
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the logs were printed on the output and even colored accordingly
    with the log level, where **INFO** is in green and **ERROR** is in red.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This exercise was straightforward, but what if I told you that many developers
    struggle to understand how Airflow creates its logs? It often happens for two
    reasons – developers are used to inserting `print()` functions instead of logging
    methods and only check the records in the Airflow UI.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the Airflow configuration, it will not show `print()` messages
    on the UI, and messages used to debug or find where the code ran can be lost.
    Also, the Airflow UI has a limit on the number of record lines to show, and Spark
    error messages can be easily omitted in this case.
  prefs: []
  type: TYPE_NORMAL
- en: 'That’s why it is vital to understand that, by default, Airflow stores all its
    logs under a `logs/` directory, even organizing it by `dag_id`, `run_id`, and
    each task separately, as we saw in *step 7*. This folder structure can also be
    changed or improved depending on your needs, and all you need to do is alter the
    `log_filename_template` variable in `airflow.cfg`. The following is how it is
    set by default:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, looking inside the log file, you can see that it is the same as what is
    on the UI, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.8 – A complete log message stored in a log file found in the local
    Airflow log folder](img/Figure_10.08_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.8 – A complete log message stored in a log file found in the local
    Airflow log folder
  prefs: []
  type: TYPE_NORMAL
- en: In the first lines, it is possible to see the internal calls Airflow makes to
    start a task, and even the specific function names, such as `taskinstance.py`
    or `standard_task_runner.py`. Those are all internal scripts. Then, we can see
    our log messages below in the file.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you look closely, you can see that the format for our logs is similar to
    the Airflow core. It happens for two reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: 'At the beginning of our code, we used the `getLogger()` method to retrieve
    the configuration used by the `airflow.task` module, as you can see here:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`airflow.task` uses the Airflow default configuration to format all logs, which
    can also be found inside the `airflow.cfg` file. Don’t worry about this now; we
    will cover it later in the *Configuring logs in* *airflow.cfg* recipe.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After defining the `logger` variable and setting the logging class configurations,
    the rest of the script is straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can read more details about Airflow logs on the Astronomer page here: [https://docs.astronomer.io/learn/logging](https://docs.astronomer.io/learn/logging).'
  prefs: []
  type: TYPE_NORMAL
- en: Storing log files in a remote location
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By default, Airflow stores and organizes its logs in a local folder with easy
    access for developers, which facilitates the debugging process when something
    does not go as expected. However, working with larger projects or teams makes
    giving everyone access to an Airflow instance or server almost impracticable.
    Besides looking at the DAG console output, there are other ways to allow access
    to the logging folder without granting access to Airflow’s server.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most straightforward solutions is to export logs to external storage,
    such as S3 or **Google Cloud Storage**. The good news is that Airflow already
    has native support to export records to cloud resources.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will set a configuration in our `airflow.cfg` file that allows
    the use of the remote logging feature and test it using an example DAG.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the *Technical requirements* section for this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: AWS S3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To complete this exercise, it is necessary to create an **AWS S3** bucket.
    Here are the steps required to accomplish it:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create an AWS account by following the steps here: [https://docs.aws.amazon.com/accounts/latest/reference/manage-acct-creating.xhtml](https://docs.aws.amazon.com/accounts/latest/reference/manage-acct-creating.xhtml)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, proceed to create an S3 bucket, guided by the AWS documentation here:
    [https://docs.aws.amazon.com/AmazonS3/latest/userguide/creating-bucket.xhtml](https://docs.aws.amazon.com/AmazonS3/latest/userguide/creating-bucket.xhtml)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In my case, I will create an S3 bucket called `airflow-cookbook` for use in
    this recipe, as you can see in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.9 – The AWS S3 Create bucket page](img/Figure_10.09_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.9 – The AWS S3 Create bucket page
  prefs: []
  type: TYPE_NORMAL
- en: Airflow DAG code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To avoid redundancy and focus on the goal of this recipe, which is to configure
    remote logging in Airflow, we will use the same DAG as the *Creating basic logs
    in Airﬂow* recipe. However, feel free to create another DAG with a different name
    but the same code.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are the steps to perform this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s create a programmatic user in our AWS account. Airflow will use
    this user to authenticate on AWS and will be able to write the logs. On your AWS
    console, select **IAM services**, and you will be redirected to a page similar
    to this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.10 – The AWS IAM main page](img/Figure_10.10_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.10 – The AWS IAM main page
  prefs: []
  type: TYPE_NORMAL
- en: Since this is a test account with a strict purpose, I will ignore the alerts
    on the IAM dashboard.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, select **Users** and **Add users**, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.11 – The AWS IAM Users page](img/Figure_10.11_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.11 – The AWS IAM Users page
  prefs: []
  type: TYPE_NORMAL
- en: 'On the **Create user** page, insert a username that is easy to remember, as
    shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.12 – The AWS IAM new user details](img/Figure_10.12_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.12 – The AWS IAM new user details
  prefs: []
  type: TYPE_NORMAL
- en: Leave the checkbox unmarked and select **Next** to add the access policies.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the **Set permissions** page, select **Attach policies directly** and then
    look for **AmazonS3FullAccess** in the **Permission** **policies** checkbox:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.13 – AWS IAM set permissions for user creation](img/Figure_10.13_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.13 – AWS IAM set permissions for user creation
  prefs: []
  type: TYPE_NORMAL
- en: Since this is a testing exercise, we can use full access to the S3 resource.
    However, remember to attach specific policies to access the resources in a production
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: Select **Next** and then click on the **Create** **user** button.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, retrieve the access key by selecting the user you created, go to **Security
    credentials**, and scroll down until you see the **Access keys** box. Then, create
    a new one and save the CSV file in an easily accessible place:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.14 – Access key creation for a user](img/Figure_10.14_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.14 – Access key creation for a user
  prefs: []
  type: TYPE_NORMAL
- en: Now, back in Airflow, let’s configure the connection between Airflow and our
    AWS account.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a new connection using the Airflow UI, and in the **Connection Type**
    field, select **Amazon S3**. In the **Extra** field, insert the following line
    with the credentials retrieved in *step 4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Your page will look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.15 – The Airflow UI on adding a new AWS S3 connector](img/Figure_10.15_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.15 – The Airflow UI on adding a new AWS S3 connector
  prefs: []
  type: TYPE_NORMAL
- en: Save it, and open your code editor in your Airflow directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s add the configurations to our `airflow.cfg` file. If you are using
    Docker to host Airflow, add the following lines to your `docker-compose.yaml file`,
    under the environment settings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Your `docker-compose.yaml` file will look similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.16 – Remote logging configuration in docker-compose.yaml](img/Figure_10.16_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.16 – Remote logging configuration in docker-compose.yaml
  prefs: []
  type: TYPE_NORMAL
- en: 'If you installed Airflow directly on your local machine, you can instantly
    change the `airflow.cfg` file. Change the following lines in `airflow.cfg` and
    save it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: After the preceding changes, restart your Airflow application.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'With your refreshed Airflow, run `basic_logging_dag` and open your AWS S3\.
    Select the bucket you created in the *Getting ready* section, and you should see
    a new object inside of it, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.17 – The AWS S3 airflow-cookbook bucket objects](img/Figure_10.17_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.17 – The AWS S3 airflow-cookbook bucket objects
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, select the object created, and you should be able to see more folders
    related to the tasks executed, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.18 – AWS S3 airflow-cookbook showing the remote logs](img/Figure_10.18_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.18 – AWS S3 airflow-cookbook showing the remote logs
  prefs: []
  type: TYPE_NORMAL
- en: Finally, if you select one of the folders, you will see the same file you saw
    in the *Creating basic logs in Airﬂow* recipe. We successfully wrote logs in a
    remote location!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you look at this recipe overall, it may seem considerable work. However,
    remember that we are making a configuration from zero, which generally takes time.
    Since we are somewhat used to creating an AWS S3 bucket and executing DAGs (see
    [*Chapter 2*](B19453_02.xhtml#_idTextAnchor064) and [*Chapter 9*](B19453_09.xhtml#_idTextAnchor319),
    respectively), let’s focus on setting the remote log configurations.
  prefs: []
  type: TYPE_NORMAL
- en: Our first action started with creating a connection in Airflow using the access
    keys generated on AWS. This step is required because, internally, Airflow will
    use those keys to authenticate in AWS and prove its identity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we changed the following Airflow configurations as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The two first lines are string configurations to set on Airflow whether remote
    logging is enabled and which bucket path will be used. The last two lines are
    related to the name of the connection we created on the `True` if we handle sensitive
    information.
  prefs: []
  type: TYPE_NORMAL
- en: After restarting Airflow, the configurations will be reflected in our application,
    and by executing a DAG, we can already see the logs written in the S3 bucket.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned in the introduction of this recipe, this type of configuration
    is beneficial not only in big projects but also as a good practice when using
    Airflow, allowing developers to debug or retrieve information about code output
    without accessing the cluster or server.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we covered an example using AWS S3, but it is also possible to use **Google
    Cloud Storage** or **Azure Blog Storage**. You can read more here: [https://airflow.apache.org/docs/apache-airflow/1.10.13/howto/write-logs.xhtml](https://airflow.apache.org/docs/apache-airflow/1.10.13/howto/write-logs.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you don’t want to use remote logging anymore, you can simply remove the environment
    variables from your `docker-compose.yaml` or set `REMOTE_LOGGING` back to `False`.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can read more about remote logging in S3 on the Apache Airflow official
    documentation page here: [https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/logging/s3-task-handler.xhtml](https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/logging/s3-task-handler.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: Configuring logs in airflow.cfg
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We had our first contact with the `airflow.cfg` file in the *Storing log files
    in a remote location* recipe. At a glance, we saw how powerful and handy this
    configuration file is. There are many ways to customize and improve Airflow just
    by editing it.
  prefs: []
  type: TYPE_NORMAL
- en: This exercise will teach how you to enhance your logs by setting applicable
    configurations in the `airflow.cfg` file.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the *Technical requirements* section for this recipe, since we will
    handle it with the same technology.
  prefs: []
  type: TYPE_NORMAL
- en: Airflow DAG code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To avoid redundancy and focus on the goal of this recipe, which is to configure
    remote logging in Airflow, we will use the same DAG as the *Creating basic logs
    in Airﬂow* recipe. However, feel free to create another DAG with a different name
    but the same code.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since we will use the same DAG code from *Creating basic logs in Airﬂow,* let’s
    jump right to the required configuration to format our logs:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s begin by setting the configuration in our `docker-compose.yaml`. In the
    environment section, insert the following line and save the file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Your `docker-compose` file should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.19 – Formatting log configuration in docker-compose.yaml](img/Figure_10.19_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.19 – Formatting log configuration in docker-compose.yaml
  prefs: []
  type: TYPE_NORMAL
- en: 'If you directly edit the `airflow.cfg` file, search for the `log_format` variable,
    and change it to the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Your code will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.20 – log_format inside airflow.cfg](img/Figure_10.20_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.20 – log_format inside airflow.cfg
  prefs: []
  type: TYPE_NORMAL
- en: Save it, and go to the next step.
  prefs: []
  type: TYPE_NORMAL
- en: We added a few more items in the log line, which we will cover later.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Be very attentive here. In the `airflow.cfg` file, the `%` character is doubled,
    unlike in the `docker-compose` file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s restart Airflow. You can do it by stopping the Docker container
    and rerunning it with the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, let’s head up to the Airflow UI and run our DAG called `basic_logging_dag`.
    On the DAG page, look in the top-right corner and select the play button (depicted
    by an arrow), followed by **Trigger DAG**, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.21 – basic_logging_dag trigger button on the right side of the
    page](img/Figure_10.21_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.21 – basic_logging_dag trigger button on the right side of the page
  prefs: []
  type: TYPE_NORMAL
- en: The DAG will start to run immediately.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s see the logs generated by one task. I will pick the `extract_data`
    task, and the log will look like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.22 – The formatted log output for extract_data task](img/Figure_10.22_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.22 – The formatted log output for extract_data task
  prefs: []
  type: TYPE_NORMAL
- en: If you look closely, you will see that we now have the process number displayed
    on the output.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you opt to maintain continuity from the last recipe, *Storing log files in
    a remote location*, remember that your logs are stored in a remote location.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we can see, altering any logging information is simple, since Airflow uses
    the Python logging library behind the scenes. Now, let’s take a look at our output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.23 – The formatted log output for the extract_data task](img/Figure_10.23_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.23 – The formatted log output for the extract_data task
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, before the process name (for example, `airflow.task`), we also
    have the number of the running process. It can be helpful information when running
    multiple processes simultaneously, allowing us to understand which one is taking
    longer to complete and what is running.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the code we inserted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, variables such as `asctime`, `process`, and `filename` are
    identical to the ones we saw in [*Chapter 8*](B19453_08.xhtml#_idTextAnchor280).
    Also, since a core Python function operates behind the scenes, we can add more
    information based on the allowed attributes. You can find the list here: [https://docs.python.org/3/library/logging.xhtml#logrecord-attributes](https://docs.python.org/3/library/logging.xhtml#logrecord-attributes).'
  prefs: []
  type: TYPE_NORMAL
- en: Going deeper in airflow.cfg
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now, let’s go deeper into Airflow configurations. As you can observe, Airflow
    resources are orchestrated by the `airflow.cfg` file. Using a single file, we
    can determine how to send email notifications (we will cover this in the *Using
    notiﬁcations operators* recipe), when DAGs will reflect a code change, how logs
    will be displayed, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is also possible to set these configurations by exporting environment variables,
    and this has priority over the configuration setting on `airflow.cfg`. This prioritization
    happens because, internally, Airflow translates the content from `airflow.cfg`
    to environment variables, broadly speaking. You can read more here: [https://airflow.apache.org/docs/apache-airflow/stable/cli-and-env-variables-ref.xhtml#environment-variable](https://airflow.apache.org/docs/apache-airflow/stable/cli-and-env-variables-ref.xhtml#environment-variable).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the logging configuration in the Airflow **REFERENCES** section.
    We can see many other customization possibilities, such as coloring, a specific
    format for DAG processors, and extra logs for third-party applications, as shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.24 – Airflow documentation for the logging configuration](img/Figure_10.24_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.24 – Airflow documentation for the logging configuration
  prefs: []
  type: TYPE_NORMAL
- en: 'The fantastic part of this documentation is that we have references to configure
    directly in `airflow.cfg` or environment variables. You can see the complete reference
    list here: [https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.xhtml#logging](https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.xhtml#logging).'
  prefs: []
  type: TYPE_NORMAL
- en: After we get used to the Airflow dynamics, testing new configurations or formats
    is straightforward, especially when we have a testing server to do so. However,
    simultaneously, we need to be cautious when changing anything internally; otherwise,
    we can impair our whole application.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In *step 1*, we mentioned avoiding the use of double `%` characters when setting
    the variables in `docker-compose` – let’s now cover this!
  prefs: []
  type: TYPE_NORMAL
- en: 'The `string` variable we pass for `docker-compose` will be read by an internal
    Python logging function, which will not recognize the double `%` pattern. Instead,
    it will understand the default format for the logs in Airflow needs to be equal
    to that string variable, and all the DAG logs will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.25 – An error when the environment variable for log_format is not
    correctly set](img/Figure_10.25_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.25 – An error when the environment variable for log_format is not
    correctly set
  prefs: []
  type: TYPE_NORMAL
- en: Now, inside the `airflow.cfg` file, the double `%` character is a Bash format
    pattern that works like a modulo operator.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'See the whole list of configurations for Airflow here: [https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: Designing advanced monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After spending some time learning and practicing logging concepts, we can advance
    a little more in the subject of monitoring. We can monitor results from all our
    logging collection work and generate insightful monitoring dashboards and alerts,
    with the right monitoring message stored.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will cover the Airflow metrics integrated with StatsD, a
    platform that collects system statistics, and their purpose to help us achieve
    a mature pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This exercise will focus on bringing clarity to the Airflow monitoring metrics
    and how to build a robust architecture to structure it.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a requirement for this recipe, it is vital to keep in mind the following
    basic Airflow architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.26 – An Airflow high-level architecture diagram](img/Figure_10.26_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.26 – An Airflow high-level architecture diagram
  prefs: []
  type: TYPE_NORMAL
- en: 'Airflow components, from a high-level perspective, are composed of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A **web server**, where we can access the Airflow UI.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A relational database to store metadata and other helpful information for use
    in the DAGs or tasks. To keep it simple, we will work with just one type of database;
    however, there can be more than one.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **scheduler**, which will consult the information inside the database to
    send it to the workers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **Celery** application, responsible for queueing the requests sent from the
    scheduler and the workers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **workers**, which will execute the DAG and tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With this in mind, we can proceed to the next section.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s see the main items to design advanced monitoring:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Counters**: As the name suggests, this metric will provide information about
    the counts of actions inside Airflow. This metric provides a count of running
    tasks, failed tasks, and so on. In the following figure, you can see some examples:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 10.27 – A list of counter metric examples to monitor Airflow workflows](img/Figure_10.27_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.27 – A list of counter metric examples to monitor Airflow workflows
  prefs: []
  type: TYPE_NORMAL
- en: '**Timers**: This metric tells us how long a task or DAG takes to complete or
    load a file. In the following figure, you can see more:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 10.28 – A list of timer examples to monitor Airflow workflows](img/Figure_10.28_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.28 – A list of timer examples to monitor Airflow workflows
  prefs: []
  type: TYPE_NORMAL
- en: '**Gauges**: Finally, the last metric type gives us a more visual overview.
    Gauges use timers or counters metrics to illustrate whether we are reaching a
    defined threshold. In the following figure, there are some examples of gauges:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 10.29 – A list of gauge examples to be used to monitor Airflow](img/Figure_10.29_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.29 – A list of gauge examples to be used to monitor Airflow
  prefs: []
  type: TYPE_NORMAL
- en: With the metrics defined and on our radar, we can proceed with the architecture
    design to integrate it.
  prefs: []
  type: TYPE_NORMAL
- en: '**StatsD**: Now, let’s add **StatsD** to the architecture drawing we saw in
    the *Getting ready* section. You will have something like this:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 10.30 – StatsD integration and coverage for the Airflow components
    architecture](img/Figure_10.30_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.30 – StatsD integration and coverage for the Airflow components architecture
  prefs: []
  type: TYPE_NORMAL
- en: StatsD can collect the metrics from all the components inside the dotted rectangle
    and direct them to a monitoring tool.
  prefs: []
  type: TYPE_NORMAL
- en: '**Prometheus and Grafana**: Then, we can plug StatsD into Prometheus, which
    serves as one of Grafana’s data sources. Adding these tools into our architecture
    will look something like this:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 10.31 – A Prometheus and Grafana integration with StatsD and Airflow
    diagram](img/Figure_10.31_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.31 – A Prometheus and Grafana integration with StatsD and Airflow
    diagram
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s understand the components behind this architecture.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s start understanding what StatsD is. StatsD is a daemon developed by the
    Etsy company to aggregate and collect application metrics. Generally, any application
    can send metrics using a simple protocol, such as **User Datagram Protocol** (**UDP**).
    With this protocol, the sender doesn’t need to wait for a response from StatsD,
    making the process simple. After listening and aggregating data for some time,
    StatsD will send the metrics to output storage, which is Prometheus.
  prefs: []
  type: TYPE_NORMAL
- en: 'The StatsD integration and installation can be done using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to know more about it, you can refer to the Airflow documentation
    here: [https://airflow.apache.org/docs/apache-airflow/2.5.1/administration-and-deployment/logging-monitoring/metrics.xhtml#counters](https://airflow.apache.org/docs/apache-airflow/2.5.1/administration-and-deployment/logging-monitoring/metrics.xhtml#counters).'
  prefs: []
  type: TYPE_NORMAL
- en: Then, Prometheus and Grafana will gather the metrics and transform them into
    a more visual resource. You don’t need to worry about this now; we will learn
    more about it in [*Chapter 12*](B19453_12.xhtml#_idTextAnchor433).
  prefs: []
  type: TYPE_NORMAL
- en: 'For each metric we saw in the three first steps in the *How to do it…* section,
    we can set a threshold to trigger an alert when it has trespassed. All the metrics
    are presented in the *How to do it…* section, and some more can be found here:
    https://airflow.apache.org/docs/apache-airflow/2.5.1/administration-and-deployment/logging-monitoring/metrics.xhtml#counters.'
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Besides StatsD, there are other tools we can plug into Airflow to track specific
    metrics or statuses. For example, for a deep error track, we can use **Sentry**,
    a specialized tool used by IT operations teams to provide support and insights.
    You can learn more about this integration here: [https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/errors.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/errors.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, if tracking users’ activities is a concern, it is possible
    to integrate Airflow with Google Analytics. You can learn more here: [https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/tracking-user-activity.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/tracking-user-activity.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Learn more about Airflow architecture here: [https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/logging-architecture.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/logging-architecture.xhtml)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'More information about StatsD is here: [https://www.datadoghq.com/blog/statsd/](https://www.datadoghq.com/blog/statsd/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using notiﬁcation operators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have focused on ensuring that code is well logged and has enough
    information to provide valid monitoring. Nevertheless, the purpose of having mature
    and structured pipelines is to avoid the necessity of manual intervention. With
    busy agendas and other projects, it is hard to constantly look at monitoring dashboards
    to check whether everything is fine.
  prefs: []
  type: TYPE_NORMAL
- en: Thankfully, Airflow also has native operators to trigger alerts depending on
    their configured situation. In this recipe, we will configure an email operator
    to trigger a message every time a pipeline succeeds or fails, allowing us to remediate
    the problem rapidly.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the *Technical requirements* section for this recipe, since we will
    handle it with the same technology.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to that, you need to create an app password for your Google account.
    This password will allow our application to authenticate and use the **Simple
    Mail Transfer Protocol** (**SMTP**) host from Google to trigger emails. You can
    generate the app password in your Google account at the following link: [https://security.google.com/settings/security/apppasswords](https://security.google.com/settings/security/apppasswords).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you access the link, you will be asked to authenticate using your Google
    credentials, and a new page will appear, similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.32 – The Google app password generation page](img/Figure_10.32_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.32 – The Google app password generation page
  prefs: []
  type: TYPE_NORMAL
- en: In the first box, select **Mail**, and in the second box, select the device
    that will use the app password. Since I am using a Macbook, I will select **Mac**,
    as shown in the preceding screenshot. Then, click on **GENERATE**.
  prefs: []
  type: TYPE_NORMAL
- en: 'A window similar to the following will appear:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.33 – The Google generated app password pop-up window](img/Figure_10.33_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.33 – The Google generated app password pop-up window
  prefs: []
  type: TYPE_NORMAL
- en: Follow the steps on the page and save the password in a place you can remember.
  prefs: []
  type: TYPE_NORMAL
- en: Airflow DAG code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To avoid redundancy and focus on the goal of this recipe, which is to configure
    remote logging in Airflow, we will use the same DAG as the *Creating basic logs
    in Airﬂow* recipe. However, feel free to create another DAG with a different name
    but the same code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nonetheless, you can always find the final code in the GitHub repository here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_10/Using_notifications_operators](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_10/Using_notifications_operators)'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to try this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by configuring the SMTP server in Airflow. Insert the following
    lines in your `docker-compose.yaml` file under the environment section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Your file should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.34 – docker-compose.yaml with SMTP environment variables](img/Figure_10.34_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.34 – docker-compose.yaml with SMTP environment variables
  prefs: []
  type: TYPE_NORMAL
- en: 'If you directly edit the `airflow.cfg` file, edit the following lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Don’t forget to restart Airflow after these configurations are saved.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s edit our `basic_logging_dag` DAG to allow it to send emails using
    `EmailOperator`. Let’s add to our imports the following line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The imports will be organized like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'In `default_args`, we will add three new parameters – `email`, `email_on_failure`,
    and `email_on_retry`. You can see here what it looks like:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You don’t need to worry about these new parameters for now. We will cover them
    in the *How it* *works…* section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, let’s add a new task to our DAG called `success_task`. If all the other
    tasks are successful, this one will trigger `EmailOperator` to alert us. Add the
    following code to the `basic_logging_dag` script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, at the end of your script, let’s add the workflow:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Don’t forget that you can always check how the final code looks here: [https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_10/Using_noti%EF%AC%81cations_operators](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_10/Using_noti%EF%AC%81cations_operators)'
  prefs: []
  type: TYPE_NORMAL
- en: If you check your DAG graph, you can see that a new task called `success_task`
    appears. It shows our operator is ready to be used. Let’s trigger our DAG by selecting
    the play button in the top-right corner, as we did in *step 3* of the *Configuring
    logs in* *airflow.cfg* recipe.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Your Airflow UI should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.35 – basic_logging_dag showing successful runs for all the tasks](img/Figure_10.35_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.35 – basic_logging_dag showing successful runs for all the tasks
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, let’s check our email. If everything is well configured, you should see
    an email similar to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.36 – An email with a Hello World! Message, indicating that success_task
    worked](img/Figure_10.36_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.36 – An email with a Hello World! Message, indicating that success_task
    worked
  prefs: []
  type: TYPE_NORMAL
- en: Our `EmailOperator` works exactly as expected!
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s start explaining the code by defining what an SMTP server is. An SMTP
    server is a key component of an email system that enables the transmission of
    email messages between servers and from clients to servers.
  prefs: []
  type: TYPE_NORMAL
- en: In our case, Google works both as a sender and receiver. We borrow a Gmail host
    to help send an email from our local machine. However, you don’t need to worry
    about this when working on a company project; your IT operations team will take
    care of it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, back to Airflow – once we understand how the SMTP works, its configuration
    is straightforward. Consulting the reference page for the configurations in Airflow
    ([https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.xhtml)),
    we can see that there is a section dedicated to SMTP, as you can see here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.37 – The Airflow documentation page for the SMTP environment variables](img/Figure_10.37_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.37 – The Airflow documentation page for the SMTP environment variables
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, all we needed to do was to set the required parameters to allow the connection
    between the host (`smtp.gmail.com`) and Airflow, as you can see here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.38 – A close look at the docker-compose.yaml SMTP settings](img/Figure_10.38_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.38 – A close look at the docker-compose.yaml SMTP settings
  prefs: []
  type: TYPE_NORMAL
- en: 'Once this step is completed, we will go to our DAG and declare `EmailOperator`,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The parameters of the email are very intuitive and can be set accordingly to
    whatever is needed. If we delve deeper, we can see that there are plenty of possibilities
    to make those fields’ values more abstract to adapt to different function results.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is also possible to use a formatted email template in `html_content` and
    even attach a complete error or log message. You can see more of the allowed parameters
    here: [https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/email/index.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/email/index.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, this operator was triggered when all tasks successfully ran. But
    what about if there is an error? Let’s go back to *step 3* and see `default_args`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The two new parameters added (`email_on_failure` and `email_on_retry`) address
    scenarios where the DAG failed or retries a task. The values inside the `email`
    parameter list are the recipients of these emails.
  prefs: []
  type: TYPE_NORMAL
- en: 'A default email triggered by an error message looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.39 – The Airflow default email for error in a task instance](img/Figure_10.39_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.39 – The Airflow default email for error in a task instance
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Airflow notification system is not limited to sending emails and counts,
    offering useful integrations with Slack, Teams, and Telegram.
  prefs: []
  type: TYPE_NORMAL
- en: 'TowardsDataScience has a fantastic blog post about how to integrate Airflow
    with Slack, and you can find it here: [https://towardsdatascience.com/automated-alerts-for-airflow-with-slack-5c6ec766a823](https://towardsdatascience.com/automated-alerts-for-airflow-with-slack-5c6ec766a823).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Not limited to corporate tools, Airflow also has a Discord hook: [https://airflow.apache.org/docs/apache-airflow-providers-discord/stable/_api/airflow/providers/discord/hooks/discord_webhook/index.xhtml](https://airflow.apache.org/docs/apache-airflow-providers-discord/stable/_api/airflow/providers/discord/hooks/discord_webhook/index.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: The best advice I can give is always to look at Airflow community documentation.
    As an open source and active platform, there is always a new implementation to
    help automate and make our daily work easier.
  prefs: []
  type: TYPE_NORMAL
- en: Using SQL operators for data quality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Good **data quality** is crucial for an organization to ensure the effectiveness
    of its data systems. By performing quality checks within the DAG, it is possible
    to stop pipelines and notify stakeholders before erroneous data is introduced
    into a production lake or warehouse.
  prefs: []
  type: TYPE_NORMAL
- en: Although plenty of available tools in the market provide **data quality checks**,
    one of the most popular ways to do this is by running SQL queries. As you may
    have already guessed, Airflow has providers to support those operations.
  prefs: []
  type: TYPE_NORMAL
- en: This recipe will cover the data quality principal topics in the data ingestion
    process, pointing out the best `SQLOperator` type to run in those situations.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before starting our exercise, let’s create a simple `customers` table. You
    can see here how it looks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.40 – An example of customers table columns](img/Figure_10.40_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.40 – An example of customers table columns
  prefs: []
  type: TYPE_NORMAL
- en: 'And the same table is represented with its schema:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: You don’t need to worry about creating this table in a SQL database. This exercise
    will focus on the data quality factors to be checked, using this table as an example.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are the steps to perform this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by defining the essential data quality checks that apply as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.41 – Data quality essential points](img/Figure_10.41_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.41 – Data quality essential points
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s imagine implementing it using `SQLColumnCheckOperator`, integrated and
    installed in our Airflow platform. Let’s now create a simple task to check whether
    our table has unique IDs and whether all customers have `first_name`. Our example
    code looks like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s validate whether we ingest the required count of rows using `SQLTableCheckOperator`,
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, let’s ensure the customers in our database have at least one order.
    Our example code looks like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `geq_to` key stands for **great or** **equal to**.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data quality is a complex topic encompassing many variables, such as the project
    or company context, business models, and **Service Level Agreements** (**SLAs**)
    between teams. Based on this, the goal of this recipe was to offer the core concept
    of data quality and demonstrate how to first approach using Airflow SQLOperators.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with the essential topics in *step 1*, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.42 – Data quality essential points](img/Figure_10.42_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.42 – Data quality essential points
  prefs: []
  type: TYPE_NORMAL
- en: In a generic scenario, those items are the principal topics to be approached
    and implemented. They will guarantee the minimum data reliability, based on whether
    the columns are the ones we expected, creating an average value for the row count,
    ensuring the IDs are unique, and having control of the `null` and distinct values
    in specific columns.
  prefs: []
  type: TYPE_NORMAL
- en: Using Airflow, we used the SQL approach to check data. As mentioned at the beginning
    of this recipe, SQL checks are widespread and popular due to their simplicity
    and flexibility. Unfortunately, to simulate a scenario like this, we would be
    required to set up a hard-working local infrastructure, and the best we could
    come up with is simulating the tasks in Airflow.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we used two `SQLOperator` subtypes – `SQLColumnCheckOperator` and `SQLTableCheckOperator`.
    As the name suggests, the first operator is more focused on verifying the column’s
    content by checking whether there are null or distinct values. In the case of
    `customer_id`, we verified both scenarios and only null values for `first_name`,
    as you can see here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '`SQLTableCheckOperator` will perform validations across the whole table. It
    allows the insertion of a SQL query to make counts or other operations, as we
    did to validate the expected number of rows in *step 3*, as you can see in the
    piece of code here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'However, `SQLOperator` is not limited to these two. In the Airflow documentation,
    you can see other examples and the complete list of accepted parameters for these
    functions: [https://airflow.apache.org/docs/apache-airflow/2.1.4/_api/airflow/operators/sql/index.xhtml#module-airflow.operators.sql](https://airflow.apache.org/docs/apache-airflow/2.1.4/_api/airflow/operators/sql/index.xhtml#module-airflow.operators.sql).'
  prefs: []
  type: TYPE_NORMAL
- en: A fantastic operator to check out is `SQLIntervalCheckOperator`, used to validate
    historical data and ensure the stored information is concise.
  prefs: []
  type: TYPE_NORMAL
- en: In your data career, you will see that data quality is a daily topic and concern
    among teams. The best advice here is to continually search for tools and methods
    to improve this methodology.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can use additional tools to enhance our data quality checks. One of the recommended
    tools for this use is **GreatExpectations**, an open source platform made in Python
    with plenty of integrations, with resources such as Airflow, **AWS S3**, and **Databricks**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although it is a platform you can install in any cluster, **GreatExpectations**
    is expanding toward a managed cloud version. You can check more about it on the
    official page here: [https://greatexpectations.io/integrations](https://greatexpectations.io/integrations).'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Yu Ishikawa* has a nice blog post about other checks you can do using SQL
    in Airflow: [https://yu-ishikawa.medium.com/apache-airflow-as-a-data-quality-checker-416ca7f5a3ad](https://yu-ishikawa.medium.com/apache-airflow-as-a-data-quality-checker-416ca7f5a3ad)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'More information about data quality in Airflow is available here: [https://docs.astronomer.io/learn/data-quality](https://docs.astronomer.io/learn/data-quality)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[https://www.oak-tree.tech/blog/airflow-remote-logging-s3](https://www.oak-tree.tech/blog/airflow-remote-logging-s3)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/connections/aws.xhtml#examples](https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/connections/aws.xhtml#examples)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://airflow.apache.org/docs/apache-airflow/stable/howto/email-config.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/howto/email-config.xhtml)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: https://docs.astronomer.io/learn/logging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/metrics.xhtml#setup
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: https://hevodata.com/learn/airflow-monitoring/#aam
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: https://servian.dev/developing-5-step-data-quality-framework-with-apache-airflow-972488ddb65f
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
