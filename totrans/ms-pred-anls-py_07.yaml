- en: Chapter 7. Learning from the Bottom Up – Deep Networks and Unsupervised Features
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章. 从底层学习 – 深度网络和无监督特征
- en: Thus far, we have studied predictive modeling techniques that use a set of features
    (columns in a tabular dataset) that are pre-defined for the problem at hand. For
    example, a user account, an internet transaction, a product, or any other item
    that is important to a business scenario are often described using properties
    derived from domain knowledge of a particular industry. More complex data, such
    as a document, can still be transformed into a vector representing something about
    the words in the text, and images can be represented by matrix factors as we saw
    in [Chapter 6](ch06.html "Chapter 6. Words and Pixels – Working with Unstructured
    Data"), *Words and Pixels – Working with Unstructured Data*. However, with both
    simple and complex data types, we could easily imagine higher-level interactions
    between features (for example, a user in a certain country and age range using
    a particular device is more likely to click on a webpage, while none of these
    three factors alone are predictive) as well as entirely new features (such as
    image edges or sentence fragments) that would be difficult to construct without
    subject-area expertise or extensive trial and error.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们研究的是使用一组特征（表格数据集中的列）的预测建模技术，这些特征是为当前问题预先定义的。例如，用户账户、互联网交易、产品或任何对业务场景重要的项目通常使用特定行业的领域知识推导出的属性来描述。更复杂的数据，如文档，仍然可以转换成表示文本中单词的向量，如图像可以通过我们在[第6章](ch06.html
    "第6章. 文字与像素 – 处理非结构化数据")中看到的矩阵因子来表示，“文字与像素 – 处理非结构化数据”。然而，对于简单和复杂的数据类型，我们可以很容易地想象特征之间的高级交互（例如，某个国家某个年龄段的用户使用特定设备更有可能点击网页，而这三个因素中的任何一个单独都不是预测性的）以及全新的特征（如图像边缘或句子片段），这些特征在没有领域专业知识或大量试错的情况下很难构建。
- en: 'Ideally, we could automatically find the best features for a predictive modeling
    task using whatever raw inputs we have at hand, without the effort of testing
    a vast number of transformations and interactions. The ability to do this—automatically
    determine complex features from relatively raw inputs—is an attractive property
    of *deep learning* methods, a class of algorithms commonly applied to neural network
    models that have enjoyed much recent popularity. In this chapter, we will examine
    the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们可以自动找到用于预测建模任务的最佳特征，无论我们手头有什么原始输入，而无需测试大量变换和交互。这种能力——从相对原始的输入中自动确定复杂特征——是*深度学习*方法的一个吸引人的特性，这类算法通常应用于神经网络模型，近年来受到了广泛的欢迎。在本章中，我们将探讨以下主题：
- en: How a basic neural network is fit to data
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基本神经网络如何拟合数据
- en: How deep learning methods improve the performance of classical neural networks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习方法如何提高经典神经网络的性能
- en: How to perform image recognition with deep learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用深度学习进行图像识别
- en: Learning patterns with neural networks
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用神经网络学习模式
- en: The core building blocks for the deep learning algorithms we will examine are
    *Neural Networks*, a predictive model that simulates the way cells inside the
    brain fire impulses to transmit signals. By combining individual contributions
    from many inputs (for example, the many columns we might have in a tabular dataset,
    words in a document, or pixels in an image), the network integrates signals to
    predict an output of interest (whether it is price, click through rate, or some
    other response). Fitting this sort of model to data therefore involves determining
    the best parameters of the neuron to perform this mapping from input data to output
    variable.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要研究的深度学习算法的核心构建块是*神经网络*，这是一种模拟大脑内部细胞产生脉冲以传递信号的预测模型。通过结合许多输入（例如，我们可能在表格数据集中拥有的许多列、文档中的单词或图像中的像素）的个别贡献，网络整合信号以预测感兴趣的输出（无论是价格、点击率还是其他响应）。因此，将此类模型拟合到数据涉及确定神经元的最优参数，以执行从输入数据到输出变量的映射。
- en: Some common features of the deep learning models we will discuss in this chapter
    are the large number of parameters we can tune and the complexity of the models
    themselves. Whereas the regression models we have seen so far required us to determine
    the optimal value of ~50 coefficients, in deep learning models we can potentially
    have hundreds or thousands of parameters. However, despite this complexity, deep
    learning models are composed of relatively simple units, so we will start by examining
    these building blocks.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将讨论的深度学习模型的一些常见特征是我们能够调整的大量参数以及模型的复杂性。而到目前为止我们所看到的回归模型需要我们确定大约50个系数的最优值，在深度学习模型中，我们可能拥有数百或数千个参数。然而，尽管这种复杂性，深度学习模型由相对简单的单元组成，因此我们将从这些构建块开始研究。
- en: A network of one – the perceptron
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个网络——感知器
- en: 'The simplest neural network we could imagine is composed of a single linear
    function, and is known as a perceptron (Rosenblatt, Frank. The perceptron, a perceiving
    and recognizing automaton Project Para. Cornell Aeronautical Laboratory, 1957):'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能想象到的最简单的神经网络由一个线性函数组成，被称为感知器（Rosenblatt, Frank. The perceptron, a perceiving
    and recognizing automaton Project Para. Cornell Aeronautical Laboratory, 1957）：
- en: '![A network of one – the perceptron](img/B04881_07_12.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![一个网络——感知器](img/B04881_07_12.jpg)'
- en: 'Here, *w* is a set of weights for each of the input features in the column
    vector *x*, and *b* is the intercept. You may recognize this as being very similar
    to the formula for the SVM we examined in [Chapter 5](ch05.html "Chapter 5. Putting
    Data in its Place – Classification Methods and Analysis"), *Putting Data in its
    Place – Classification Methods and Analysis*, when the kernel function used is
    linear. Both the function above and the SVM separates data into two classes depending
    upon whether a point is above or below the hyperplane given by *w*. If we wanted
    to determine the optimal parameters for this perceptron using a dataset, we could
    perform the following steps:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*w* 是列向量 *x* 中每个输入特征的权重集合，而 *b* 是截距。你可能认出这与我们在[第5章](ch05.html "第5章. 将数据放在合适的位置
    – 分类方法和分析")中考察的支持向量机（SVM）的公式非常相似，*将数据放在合适的位置 – 分类方法和分析*，当使用的核函数是线性时。上述函数和SVM根据点是否位于由
    *w* 给定的超平面之上或之下将数据分为两类。如果我们想使用数据集来确定这个感知器的最优参数，我们可以执行以下步骤：
- en: Set all weights *w* to a random value. Instead of using a fixed *b* as on offset,
    we will append a column of *1s* to the data set matrix represented by the *n x
    m* matrix *X* (*n* data points with *m* features each) to represent this offset,
    and learn the optimal value along with the rest of the parameters.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有权重 *w* 设置为随机值。不同于使用固定的 *b* 作为偏移量，我们将向由 *n x m* 矩阵 *X* 表示的数据集矩阵中添加一列 *1s*
    来表示这个偏移量，并与其他参数一起学习最优值。
- en: Calculate the output of the model, *F(xi)*, for a particular observation *x*,
    in our dataset.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算模型对于数据集中特定观察值 *x* 的输出，即 *F(xi)*。
- en: Update the weights using a learning rate *α* according to the formula:![A network
    of one – the perceptron](img/B04881_07_13.jpg)
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用学习率 *α* 根据以下公式更新权重：![一个网络——感知器](img/B04881_07_13.jpg)
- en: Here, *y[i]* is the target (the real label `0` or `1` for *x[i]*). Thus, if
    *F(x[i])* is too small, we increase the weights on all features, and vice versa.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，*y[i]* 是目标（对于 *x[i]* 的真实标签 `0` 或 `1`）。因此，如果 *F(x[i])* 太小，我们将增加所有特征上的权重，反之亦然。
- en: Repeat steps 2 and 3 for each data point in our set, until we reach a maximum
    number of iterations or the average error given by:![A network of one – the perceptron](img/B04881_07_14.jpg)
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对我们集合中的每个数据点重复步骤2和3，直到达到最大迭代次数或平均误差达到：![一个网络——感知器](img/B04881_07_14.jpg)
- en: over the *n* data points drops below a given threshold *ε* (for example, *1e-6*).
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 *n* 个数据点上的误差低于给定的阈值 *ε*（例如，*1e-6*）。
- en: While this model is easy to understand, it has practical limitations for many
    problems. For one, if our data cannot be separated by a hyperplane (they are not
    linearly separable), then we will never be able to correctly classify all data
    points. Additionally, all the weights are updated with the same rule, meaning
    we need to learn a common pattern of feature importance across all data points.
    Additionally, since the outputs are only 1 or 0, the Perceptron is only suitable
    for binary classification problems. However, despite these limitations, this model
    illustrates some of the common features of more complex neural network models.
    The training algorithm given above adjusts the model weights based on the error
    of classification tuned by a learning rate, a pattern we will see in more complex
    models as well. We will frequently see a thresholded (binary) function such as
    the preceding one, though we will also relax this restriction and investigate
    the use of other functions.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个模型容易理解，但它对许多问题来说在实践上存在局限性。首先，如果我们的数据不能被超平面分离（它们不是线性可分的），那么我们就永远无法正确分类所有数据点。此外，所有权重都使用相同的规则更新，这意味着我们需要学习所有数据点的特征重要性的共同模式。此外，由于输出只有1或0，感知器仅适用于二元分类问题。然而，尽管存在这些局限性，这个模型展示了更复杂神经网络模型的一些共同特征。上面给出的训练算法根据由学习率调整的分类误差来调整模型权重，这种模式我们将在更复杂的模型中也会看到。我们经常看到像前面那样的阈值（二元）函数，尽管我们也会放宽这个限制，并研究使用其他函数。
- en: How can we develop this simple Perceptron into a more powerful model? As a first
    step, we can start by combining inputs from many individual models of this kind.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何将这个简单的感知器发展成为更强大的模型？作为第一步，我们可以从结合许多这种类型的单个模型的输入开始。
- en: Combining perceptrons – a single-layer neural network
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结合感知器——单层神经网络
- en: 'Just as a biological brain is composed of individual neuronal cells, a neural
    network model consists of a collection of functions such as the Perceptron discussed
    previously. We will refer to these individual functions within the network as
    **neurons** or **units**. By combining the inputs from several functions, blended
    using a set of weights, we can start to fit more complex patterns. We can also
    capture nonlinear patterns by using other functions than the linear decision boundary
    of the Perceptron. One popular choice is the logistic transform we previously
    saw in [Chapter 5](ch05.html "Chapter 5. Putting Data in its Place – Classification
    Methods and Analysis"), *Putting Data in its Place – Classification Methods and
    Analysis*. Recall that the logistic transform is given by:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 正如生物大脑由单个神经元细胞组成一样，神经网络模型由一系列函数组成，例如之前讨论过的感知器。我们将把这些网络中的单个函数称为**神经元**或**单元**。通过结合来自多个函数的输入，并使用一组权重进行混合，我们可以开始拟合更复杂的模式。我们还可以通过使用比感知器的线性决策边界更复杂的其他函数来捕捉非线性模式。一个流行的选择是我们之前在[第5章](ch05.html
    "第5章。将数据放在合适的位置——分类方法和分析")中看到的对数变换，*将数据放在合适的位置——分类方法和分析*。回想一下，对数变换由以下公式给出：
- en: '![Combining perceptrons – a single-layer neural network](img/B04881_07_15.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![结合感知器——单层神经网络](img/B04881_07_15.jpg)'
- en: Here, *w* is a set of weights on the elements of the vector *x*, and *b* is
    an offset or bias, just as in the Perceptron. This bias serves the same role as
    in the Perceptron model, increasing or decreasing the score calculated by the
    model by a fixed amount, while the weights are comparable to regression coefficients
    we have seen in models from [Chapter 4](ch04.html "Chapter 4. Connecting the Dots
    with Models – Regression Methods"), *Connecting the Dots with Models – Regression
    Methods*. For simpler notation in the following derivation, we can represent the
    value *wx+b* as a single variable *z*. As in the logistic regression model from
    [Chapter 5](ch05.html "Chapter 5. Putting Data in its Place – Classification Methods
    and Analysis"), *Putting Data in its Place – Classification Methods and Analysis*,
    this function maps the input *x* into the range *[0,1]* and can be visually represented
    (**Figure 1**) as an input vector *x* (consisting of three green units) connected
    by lines (representing the weights *W*) to a single blue unit (the function *F(z)*).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*w*是向量*x*的元素上的权重集合，*b*是一个偏移量或偏差，就像在感知器中一样。这个偏差在感知器模型中起着相同的作用，通过增加或减少模型计算出的分数的固定量，而权重则类似于我们在[第4章](ch04.html
    "第4章. 使用模型连接点 – 回归方法")中看到的回归系数，*使用模型连接点 – 回归方法*。为了在以下推导中简化符号，我们可以将值*wx+b*表示为一个单一变量*z*。正如在[第5章](ch05.html
    "第5章. 将数据放在合适的位置 – 分类方法和分析")中提到的逻辑回归模型，*将数据放在合适的位置 – 分类方法和分析*，这个函数将输入*x*映射到范围*[0,1]*，并且可以如图1所示（**图1**）将一个输入向量*x*（由图顶部的三个绿色单元组成）通过线（表示权重*W*）连接到一个单一的蓝色单元（函数*F(z)*）进行视觉表示。
- en: In addition to increasing the flexibility with which we can separate data through
    this nonlinear transformation, let us also adjust our definition of the target.
    In the Perceptron model, we had a single output value `0` or `1`. We could also
    think of representing this as a two-unit vector (shown in red in Figure 1), in
    which one element is set to `1` and the other `0`, representing which of the two
    categories a data point belongs to. This may seem like an unnecessary complication,
    but it will become very useful as we build increasingly complex models.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 除了通过这种非线性变换增加我们分离数据的灵活性外，让我们也调整我们的目标定义。在感知器模型中，我们有一个单一的输出值`0`或`1`。我们也可以考虑将其表示为一个由两个单元组成的向量（如图1中所示，用红色表示），其中一个元素设置为`1`，另一个设置为`0`，表示数据点属于两个类别中的哪一个。这看起来可能像是一种不必要的复杂性，但随着我们构建越来越复杂的模型，它将变得非常有用。
- en: With these modifications, our model now consists of the elements show in Figure
    1\. The logistic function takes input from the three features of *x* represented
    by the vector at the top of the diagram, combines them using the logistic function
    with the weights for each element of *x* given by *W1*, and returns an output.
    This output is then used by two additional logistic functions downstream, represented
    by the red units at the bottom of the diagram. The one on the left uses the output
    of the first function to give a score for the probability of class `1`. On the
    right, a second function uses this output to give the probability of class `0`.
    Again, the input from the blue unit to the red units is weighted by a vector *W2*.
    By tuning the value of *W2* we can increase or decrease the likelihood of *activting*
    one of the red nodes and setting its value to `1`. By taking the maximum over
    these two values, we get a binary classification.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 经过这些修改，我们的模型现在由图1中显示的元素组成。逻辑函数从图顶部的向量表示的*x*的三个特征中获取输入，使用每个*x*元素的权重*W1*通过逻辑函数进行组合，并返回一个输出。然后，这个输出被下游的两个额外的逻辑函数使用，如图中底部的红色单元所示。左侧的函数使用第一个函数的输出给出类别`1`的概率分数。在右侧，第二个函数使用这个输出给出类别`0`的概率。同样，从蓝色单元到红色单元的输入由一个向量*W2*加权。通过调整*W2*的值，我们可以增加或减少激活其中一个红色节点并将其值设置为`1`的可能性。通过对这两个值取最大值，我们得到一个二元分类。
- en: '![Combining perceptrons – a single-layer neural network](img/B04881_07_05.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![结合感知器 – 单层神经网络](img/B04881_07_05.jpg)'
- en: 'Figure 1: Architecture of a basic single-layer neural network'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：基本单层神经网络的架构
- en: This model is still relatively simple, but has several features that will also
    be present in more complex scenarios. For one, we now actually have multiple layers,
    since the input data effectively forms the top layer of this network (green nodes).
    Likewise, the two output functions form another layer (red nodes). Because it
    lies between the input data and the output response, the middle layer is also
    referred to as the *hidden layer* of the network. In contrast, the bottom-most
    level is called the *visible layer*, and the top the *output layer*.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型仍然相对简单，但具有一些将在更复杂场景中出现的特征。首先，我们现在实际上有多个层，因为输入数据有效地形成了这个网络的最顶层（绿色节点）。同样，两个输出函数形成另一个层（红色节点）。因为它位于输入数据和输出响应之间，中间层也被称为网络的
    *隐藏层*。相比之下，最底层被称为 *可见层*，最顶层是 *输出层*。
- en: 'Right now, this is not a very interesting model: while it can perform a nonlinear
    mapping from the input *x* using the logistic function, we only have a single
    set of weights to tune from the input, meaning we can effectively only extract
    one set of patterns or features from the input data by reweighting it. In a sense,
    it is very similar to the Perceptron, just with a different decision function.
    However, with only a few modifications, we can easily start to create more complex
    mappings that can accommodate interactions between the input features. For example,
    we could add two more neurons to this network in the hidden layer, as shown in
    the **Figure 2**. With these new units, we now have three potentially different
    sets of weights for the elements of the input (each representing different weighting
    of the inputs), each of which could form a different signal when integrated in
    the hidden layer. As a simple example, consider if the vector represented an image:
    the hidden neurons on the right and left could receive weights of (*1,0,0*) and
    (*0,0,1*), picking up the edges of the image, while the middle neuron could receive
    weight (*0,1,0*) and thus only consider the middle pixel. The output probabilities
    of the two classes in the output layer now receive contributions from all three
    hidden neurons. As a result, we could now adjust the weight parameters in the
    vector *W2* to pool contributions from the three hidden units to decide the probability
    of the two classes'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，这并不是一个非常有趣的模型：虽然它可以使用逻辑函数从输入 *x* 进行非线性映射，但我们只有一组权重可以调整，这意味着我们只能通过重新加权来有效地从输入数据中提取一组模式或特征。从某种意义上说，它与感知器非常相似，只是决策函数不同。然而，只需进行一些修改，我们就可以轻松地开始创建更复杂的映射，这些映射可以适应输入特征之间的交互。例如，我们可以在隐藏层中添加两个额外的神经元，如图
    **图2** 所示。有了这些新单元，我们现在有三个可能不同的权重集，用于输入元素（每个代表输入的不同加权），每个在隐藏层中整合时都可能形成不同的信号。作为一个简单的例子，考虑如果向量代表一个图像：右侧和左侧的隐藏神经元可以接收权重
    (*1,0,0*) 和 (*0,0,1*)，拾取图像的边缘，而中间的神经元可以接收权重 (*0,1,0*)，因此只考虑中间像素。输出层中两个类别的输出概率现在都受到三个隐藏神经元的影响。因此，我们现在可以调整向量
    *W2* 中的权重参数，将三个隐藏单元的贡献汇总以决定两个类别的概率
- en: '![Combining perceptrons – a single-layer neural network](img/B04881_07_06.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![结合感知器 - 单层神经网络](img/B04881_07_06.jpg)'
- en: 'Figure 2: A more complex architecture with three units in the hidden layer'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：具有隐藏层中三个单元的更复杂架构
- en: Even with these modifications, the **Figure 2** represents a relatively simple
    model. To add more flexibility to the model we could add many more units to the
    hidden layer. We could also extend this approach to a problem with more than two
    classes by adding more units to the output layer at the bottom of the diagram.
    Also, we have only considered linear and logistic transformations thus far, but
    as we will see later in this, chapter there are a wide variety of functions we
    could choose.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 即使有这些修改，**图2** 代表的模型仍然相对简单。为了增加模型的灵活性，我们可以在隐藏层中添加更多的单元。我们还可以通过在图的下部输出层添加更多单元来将这种方法扩展到具有两个以上类别的问题。此外，我们迄今为止只考虑了线性和对数逻辑变换，但正如我们将在本章后面看到的，我们有各种各样的函数可以选择。
- en: However, before we consider more complex variations on this design, let us examine
    the methodology to determine the proper parameters for this model. The insertion
    of the intermediate layers means we can no longer rely on the simple learning
    training algorithm that we used for the Perceptron model. For example, while we
    still want to adjust the weights between the input and hidden layer to optimize
    the error between the prediction and the target, the final prediction is now not
    given by the output of the hidden layer, but the output layer below it. Thus,
    our training procedure needs to incorporate errors from the output layer in tuning
    hidden levels of the network.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在我们考虑这个设计的更复杂变化之前，让我们检查确定此模型适当参数的方法。插入中间层意味着我们不能再依赖于我们为感知器模型使用的简单学习训练算法。例如，虽然我们仍然想要调整输入层和隐藏层之间的权重以优化预测和目标之间的误差，但最终的预测现在不是由隐藏层的输出给出，而是由其下面的输出层的输出给出。因此，我们的训练过程需要将输出层的误差纳入调整网络隐藏层的过程中。
- en: Parameter fitting with back-propagation
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用反向传播进行参数拟合
- en: Given the three layer network shown in Figure 2, how can we determine the best
    set of **weights** (**W**) and **offsets** (**b**) that map our input data to
    the output? As with the Perceptron algorithm, we can initially set all our weights
    to random numbers (a common strategy is to sample them from a normal distribution
    with mean of 0 and standard deviation of 1). We then follow the flow of the arrows
    in the network from top to bottom, computing the logistic transform for each node
    at each stage until we arrive at the probabilities of each of the two classes.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 给定图2中所示的三层网络，我们如何确定将我们的输入数据映射到输出的最佳权重集 **W** 和偏移量集 **b**？与感知器算法一样，我们最初可以将所有权重设置为随机数（一种常见策略是从均值为0、标准差为1的正态分布中采样）。然后我们遵循网络从上到下的箭头流向，计算每个阶段的每个节点的逻辑变换，直到我们到达两个类别的概率。
- en: 'To adjust our randomly chosen parameters to better fit the output, we can calculate
    what direction the weights should move to reduce the error between the predicted
    and observed response *y*, just as with the Perceptron learning rule. In **Figure
    2** we can see two sets of weights we need to adjust: those between the input
    and hidden layer (W1) and those between the hidden layer and the output (W2).'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了调整我们随机选择的参数以更好地拟合输出，我们可以计算权重应该移动的方向以减少预测响应和观察响应 *y* 之间的误差，就像在感知器学习规则中一样。在**图2**中，我们可以看到我们需要调整的两组权重：输入层和隐藏层之间的权重（W1）以及隐藏层和输出层之间的权重（W2）。
- en: 'Let us start with the simpler case. If we are tuning the bottom-most weights
    (between the output and the hidden layer), then we want to find the change in
    the errors (the difference between the prediction and the real value of the output)
    as we alter the weights. For now, we will use a squared error function to illustrate:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从更简单的情况开始。如果我们正在调整最底层的权重（在输出层和隐藏层之间），那么我们希望找到随着我们改变权重时误差的变化（预测值和输出真实值之间的差异）。目前，我们将使用平方误差函数来演示：
- en: '![Parameter fitting with back-propagation](img/B04881_07_16.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![使用反向传播进行参数拟合](img/B04881_07_16.jpg)'
- en: Here, yi is the actual value of the label, and `F(zi)` represents one of the
    red neurons in the output layer (in this example, a logistic function). We want
    to calculate the change in this error function when we adjust the weight (one
    of the lines connecting the hidden blue and output red neurons in the Figure 2),
    represented by the variable `Wij`, where `i` and `j` are indices for the red and
    blue neurons connected by a given weight. Recall that this weight is actually
    an argument of the variable z (since `z=wx+b`), representing the input to the
    function logistic *F*. Because the variable w is nested inside the logistic function,
    we cannot calculate the partial derivative the error function with respect to
    this weight directly to determine the weight update Δw.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，yi 是标签的实际值，而 `F(zi)` 代表输出层中的一个红色神经元（在这个例子中，是一个逻辑函数）。我们希望计算当我们调整权重（图2中连接隐藏层蓝色神经元和输出层红色神经元的一条线）时，这个误差函数的变化，这个权重由变量
    `Wij` 表示，其中 `i` 和 `j` 是通过给定权重连接的红色神经元和蓝色神经元的索引。回想一下，这个权重实际上是变量 z 的一个参数（因为 `z=wx+b`），代表函数逻辑
    *F* 的输入。由于变量 w 被嵌套在逻辑函数中，我们无法直接计算误差函数关于这个权重的偏导数来确定权重更新 Δw。
- en: Note
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: We want to calculate a partial derivative because the error is a function of
    all input weights, but we want to update each weight independently
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想计算偏导数，因为误差是所有输入权重的函数，但我们希望独立更新每个权重
- en: 'To see this, recall an example from calculus if we wanted to find the derivative
    of the function with respect to x:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这一点，请回忆一下微积分中的一个例子，如果我们想要找到函数相对于 x 的导数：
- en: '![Parameter fitting with back-propagation](img/B04881_07_17.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![参数拟合与反向传播](img/B04881_07_17.jpg)'
- en: 'We would need to first take the derivative with respect to ez, then multiply
    by the derivative of z with respect to x, where z=x2, giving a final value of
    ![Parameter fitting with back-propagation](img/B04881_07_35.jpg). This pattern,
    given more generally by:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要计算相对于 ez 的导数，然后乘以 z 相对于 x 的导数，其中 z=x^2，得到最终值为 ![参数拟合与反向传播](img/B04881_07_35.jpg)。这个模式更一般地表示为：
- en: '![Parameter fitting with back-propagation](img/B04881_07_18.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![参数拟合与反向传播](img/B04881_07_18.jpg)'
- en: This is known as the *chain rule*, since we *chain* together derivatives in
    calculations with nested functions. In fact, though are example was for only a
    single level of nesting, we could extend this to an arbitrary number, and would
    just need to insert more multiplication terms in the formula above.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为**链式法则**，因为我们把嵌套函数中的导数**链**在一起进行计算。实际上，尽管我们的例子只有单层嵌套，但我们可以将这个方法扩展到任意层数，只需要在上面的公式中插入更多的乘法项。
- en: 'Thus, to calculate the change in the error function *E* when we change a given
    weight wij, we need to take the derivative of the error function with respect
    to *F(z)* first, followed by the derivative of *F(z)* with respect to *z*, and
    finally the derivative of *z* with respect to the weight wij. When we multiply
    these three partial derivatives together and cancel terms in the numerator and
    denominator, we obtain the derivative of the error function with respect to the
    weight. This, the partial derivative of this error with respect to a particular
    weight *w* between one the outputs *i* and one of the hidden neurons *j* is given
    using the chain rule described previously:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当我们改变一个给定的权重 wij 时，要计算误差函数 E 的变化，我们首先需要计算误差函数相对于 F(z) 的导数，然后是 F(z) 相对于 z
    的导数，最后是 z 相对于权重 wij 的导数。当我们把这三个偏导数相乘并约简分子和分母中的项时，我们得到误差函数相对于权重的导数。这就是之前描述的链式法则，用于计算特定权重
    w 之间的误差相对于输出 i 和隐藏神经元 j 的偏导数：
- en: '![Parameter fitting with back-propagation](img/B04881_07_19.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![参数拟合与反向传播](img/B04881_07_19.jpg)'
- en: 'Now that we have the formula to determine the derivative of the error function
    with respect to the weight, let us determine the value of each of these three
    terms. The derivative for the first term is simply the difference between the
    prediction and the actual response variable:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了确定误差函数相对于权重的导数的公式，让我们确定这三个项中的每一个的值。第一个项的导数很简单，就是预测值和实际响应变量之间的差值：
- en: '![Parameter fitting with back-propagation](img/B04881_07_20.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![参数拟合与反向传播](img/B04881_07_20.jpg)'
- en: Note
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Please note that the subscript i here refers to the index of the output neuron,
    not the data point i as it has been used previously.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这里的下标 i 指的是输出神经元的索引，而不是之前使用的数据点 i。
- en: 'For the second term, we find that the partial derivative of the logistic function
    has a convenient form as a product of the function and 1 minus the function:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第二个项，我们发现逻辑函数的偏导数有一个方便的形式，即函数与 1 减去函数的乘积：
- en: '![Parameter fitting with back-propagation](img/B04881_07_21.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![参数拟合与反向传播](img/B04881_07_21.jpg)'
- en: Finally for the last term we have simply,
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，对于最后一个项，我们简单地，
- en: '![Parameter fitting with back-propagation](img/B04881_07_22.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![参数拟合与反向传播](img/B04881_07_22.jpg)'
- en: 'Here, `F(z)j` is the output of the hidden layer neuron `j`. To adjust the weight
    `wij`, we want to move in the opposite direction to which the error is increasing,
    just as in the stochastic gradient descent algorithm we described in [Chapter
    5](ch05.html "Chapter 5. Putting Data in its Place – Classification Methods and
    Analysis"), *Putting Data in its Place – Classification Methods and Analysis*.
    Thus, we update the value of the weight using the following equation:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`F(z)j` 是隐藏层神经元 j 的输出。为了调整权重 `wij`，我们希望朝着误差增加的反方向移动，就像我们在第 5 章中描述的随机梯度下降算法中做的那样，[第
    5 章](ch05.html "第 5 章. 将数据放在合适的位置 – 分类方法和分析")，将数据放在合适的位置 – 分类方法和分析。因此，我们使用以下方程更新权重的值：
- en: '![Parameter fitting with back-propagation](img/B04881_07_23.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![参数拟合与反向传播](img/B04881_07_23.jpg)'
- en: 'Where `α` is a learning rate. For the first set of weights (between the input
    and the hidden layer), the calculation is slightly more complicated. We start
    again with a similar formula as previously:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 `α` 是学习率。对于第一组权重（输入层和隐藏层之间），计算稍微复杂一些。我们再次从一个与之前相似的公式开始：
- en: '![Parameter fitting with back-propagation](img/B04881_07_24.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![使用反向传播进行参数拟合](img/B04881_07_24.jpg)'
- en: 'Where wjk is the weight between a hidden neuron j and a visible neuron k. Instead
    of the output, `F(z)i,` the partial derivative of the error with respect to the
    weight is now calculated with respect to the hidden neuron''s output `F(z)j`.
    Because the hidden neuron is connected to several output neurons, in the first
    term we cannot simply use the derivative of the error with respect to the output
    of the neuron, since F(z)j receives error inputs from all these connections: there
    is no direct relationship between F(z)j and the error, only through the F(z)i
    of the output layer. Thus, for hidden to visible layer weights we need to calculate
    the first term of this partial derivative by summing the results of applying the
    chain rule for the connection from each output neuron i:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 wjk 是隐藏神经元 j 和可见神经元 k 之间的权重。与输出 `F(z)i` 相比，现在计算的是误差相对于权重的偏导数，相对于隐藏神经元的输出
    `F(z)j`。由于隐藏神经元连接到多个输出神经元，在第一项中，我们不能简单地使用误差相对于神经元输出的导数，因为 F(z)j 从所有这些连接接收错误输入：F(z)j
    与误差之间没有直接关系，只有通过输出层的 F(z)i。因此，对于隐藏到可见层的权重，我们需要通过将链式法则应用于每个输出神经元 i 的连接来求和偏导数的第一个项的结果：
- en: '![Parameter fitting with back-propagation](img/B04881_07_25.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![使用反向传播进行参数拟合](img/B04881_07_25.jpg)'
- en: In other words, we sum the partial derivatives along all the arrows connecting
    wjk to the output layer. For the hidden to visible weights, this means two arrows
    (from each output to the hidden neuron j).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们沿着所有连接 wjk 到输出层的箭头求和偏导数。对于隐藏到可见层的权重，这意味着两条箭头（从每个输出到隐藏神经元 j）。
- en: 'Since the input to the hidden neuron is now the data itself in the visible
    layer, the third term in the equation becomes:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 由于隐藏神经元的输入现在是可见层中的数据本身，方程中的第三项变为：
- en: '![Parameter fitting with back-propagation](img/B04881_07_26.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![使用反向传播进行参数拟合](img/B04881_07_26.jpg)'
- en: 'This is simply an element of the data vector *x*. Plugging in these values
    for the first and third terms, and using the gradient descent update given previously,
    we now have all the ingredients we need to optimize the weights in this network.
    To train the network, we repeat the following steps:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是数据向量 *x* 的一个元素。将这组值代入第一项和第三项，并使用之前给出的梯度下降更新，我们现在拥有了优化这个网络权重所需的所有成分。为了训练网络，我们重复以下步骤：
- en: Randomly initialize the weights (again, using samples from the standard normal
    is a common approach).
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机初始化权重（再次，使用标准正态分布的样本是一种常见的方法）。
- en: From the input data, follow the arrows in **Figure 2** forward through the network
    (from top to bottom) to calculate the output in the bottom layer.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从输入数据开始，按照 **图 2** 中的箭头（从上到下）通过网络前进，以计算底层输出。
- en: Using the difference between the calculated result in step 2 and the actual
    value of the output (such as a class label), use the preceding equations to calculate
    the amount by which to change each weight.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用步骤 2 中计算的结果与实际输出值（例如类别标签）之间的差异，使用前面的方程计算每个权重需要改变的数量。
- en: Repeat steps 1–3 until the weights have reached a stable value (meaning the
    difference between the old and new values is less than some small numerical cutoff
    such as *1e-6*).
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤 1–3，直到权重达到一个稳定的值（这意味着新旧值之间的差异小于某个小的数值截止点，例如 *1e-6*）。
- en: 'This process is known as back-propagation (Bryson, Arthur E., Walter F. Denham,
    and Stewart E. Dreyfus. Optimal programming problems with inequality constraints.
    *AIAA journal 1.11 (1963): 2544-2550; Rumelhart, David E., Geoffrey E. Hinton,
    and Ronald J. Williams. Learning representations by back-propagating errors*.
    Cognitive modeling 5.3 (1988): 1; Bryson, Arthur Earl. Applied optimal control:
    optimization, estimation and control. CRC Press, 1975; Alpaydin, Ethem. Introduction
    to machine learning. MIT press, 2014.) because visually the errors in prediction
    flow backward through the network to the connection weights w from the input.
    In form, it is quite similar to the Perceptron learning rule that we discussed
    at the beginning of this chapter, but accommodates the complexity of relating
    the prediction error to the weights between the hidden and visible layers, which
    depend on all output neurons in the example we have illustrated.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '这个过程被称为反向传播（Bryson, Arthur E., Walter F. Denham, 和 Stewart E. Dreyfus. 带不等式约束的最优编程问题。*AIAA杂志1.11
    (1963): 2544-2550; Rumelhart, David E., Geoffrey E. Hinton, 和 Ronald J. Williams.
    通过反向传播错误学习表示* 认知建模5.3 (1988): 1; Bryson, Arthur Earl. 应用最优控制：优化、估计和控制。CRC出版社，1975；Alpaydin,
    Ethem. 机器学习导论。麻省理工学院出版社，2014。）因为从视觉上看，预测误差通过网络反向流动到输入的连接权重w。在形式上，它与我们在本章开头讨论的感知器学习规则非常相似，但它适应了将预测误差与隐藏层和可见层之间的权重相关联的复杂性，这些权重取决于我们展示的示例中的所有输出神经元。'
- en: Discriminative versus generative models
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 区分性模型与生成性模型
- en: 'In the examples described previously and illustrated in Figures 1 and 2, the
    arrows always point exclusively forward from the input data to the output target.
    This is known as a feed-forward network, since the movement of information is
    always in one direction (Hinton, Geoffrey, et al. *Deep neural networks for acoustic
    modeling in speech recognition: The shared views of four research groups*. IEEE
    Signal Processing Magazine 29.6 (2012): 82-97). However, this is not a hard requirement—if
    we had a model in which arrows moved both forward and backward in the visible
    layer (Figure 3), we could in a sense have a generative model not unlike the LDA
    algorithm discussed in [Chapter 6](ch06.html "Chapter 6. Words and Pixels – Working
    with Unstructured Data"), *Words and Pixels – Working with Unstructured Data*:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '在前面描述的示例以及图1和图2中展示的例子中，箭头始终只从输入数据指向输出目标。这被称为前馈网络，因为信息的流动始终是单向的（Hinton, Geoffrey,
    等人 *在语音识别中的声学建模深度神经网络：四个研究小组的共同观点* IEEE信号处理杂志29.6 (2012): 82-97）。然而，这并不是一个硬性要求——如果我们有一个箭头在可见层中既向前又向后移动的模型（图3），那么在某种意义上，我们可以拥有一个生成模型，这与在第6章中讨论的LDA算法类似，*文字与像素
    - 处理非结构化数据*：'
- en: '![Discriminative versus generative models](img/B04881_07_08.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![区分性模型与生成性模型](img/B04881_07_08.jpg)'
- en: 'Figure 3: A Restricted Boltzman Machine (RBM) as the top two levels of the
    neural network from Figure 2'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：从图2中看到的神经网络的顶层两个级别的受限玻尔兹曼机（RBM）
- en: 'Instead of simply generating a predicted target in the output layer (a discriminative
    model), such a model could be used to draw samples from the presumed distribution
    of the input data. In other words, just as we could generate documents using the
    probability model described in LDA, we could draw samples from the visible layer
    using the weights from the hidden to the visible layer as inputs to the visible
    neurons. This kind of neural network model is also known as a belief network,
    since it can be used to simulate the ''knowledge'' represented by the network
    (in the form of the input data) as well as perform classification. A visible and
    hidden layer in which there is a connection between each neuron in both layers
    is a kind of model known more generally as a Restricted Boltzman Machine (RBM)
    (Smolensky, Paul. Information processing in dynamical systems: Foundations of
    harmony theory. No. CU-CS-321-86\. COLORADO UNIV AT BOULDER DEPT OF COMPUTER SCIENCE,
    1986; Hinton, Geoffrey E., James L. Mcclelland, and David E. Rumelhart. *Distributed
    representations, Parallel distributed processing: explorations in the microstructure
    of cognition, vol. 1: foundations*. (1986).).'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '而不是简单地生成输出层中的预测目标（判别模型），这样的模型可以用来从输入数据的假设分布中抽取样本。换句话说，正如我们能够使用LDA中描述的概率模型生成文档一样，我们也可以使用从隐藏层到可见层的权重作为输入到可见神经元的输入，从可见层抽取样本。这种类型的神经网络模型也被称为信念网络，因为它可以用来模拟网络（以输入数据的形式）所表示的“知识”以及执行分类。在每一层中每个神经元之间都有连接的可见层和隐藏层是一种更普遍地被称为限制性玻尔兹曼机（RBM）的模型（Smolensky,
    Paul. Information processing in dynamical systems: Foundations of harmony theory.
    No. CU-CS-321-86\. COLORADO UNIV AT BOULDER DEPT OF COMPUTER SCIENCE, 1986; Hinton,
    Geoffrey E., James L. Mcclelland, and David E. Rumelhart. *Distributed representations,
    Parallel distributed processing: explorations in the microstructure of cognition,
    vol. 1: foundations*. (1986).)。'
- en: In addition to providing a way for us to understand the distribution of the
    data by simulating samples from the space of possible input data points that the
    network has been exposed to, RBMs can form useful building blocks in the deep
    networks that we will construct using additional hidden layers. However, we are
    presented with a number of challenges in adding these additional layers.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 除了通过模拟网络所接触到的可能输入数据点的空间中的样本来帮助我们理解数据的分布之外，RBM（限制性玻尔兹曼机）还可以成为我们构建的具有额外隐藏层的深度网络中的有用构建模块。然而，在添加这些额外层时，我们面临着许多挑战。
- en: Vanishing gradients and explaining away
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度消失和解释掉
- en: Even the architecture shown in Figures 2 and 3 is not the most complex neural
    network we could imagine. The extra hidden layer means we can add an additional
    interaction between the input features, but for very complex data types (such
    as images or documents), we could easily imagine cases where capturing all interactions
    of interests might require more than one layer of blending and recombination to
    resolve. For example, one could imagine a document dataset where individual word
    features captured by the network are merged into sentence fragments features,
    which are further merged into sentence, paragraph, and chapter patterns, giving
    potentially 5+ levels of interaction. Each of these interactions would require
    another layer of hidden neurons, with the number of connections (and weights which
    need to be tuned) consequently rising. Similarly, an image might be resolved into
    grids of different resolution that are merged into smaller and larger objects
    nested within each other. To accommodate these further levels of interaction by
    adding additional hidden layers into our network (Figure 4), we would end up creating
    an increasingly *deep* network. We could also add additional RBM layers like we
    described . Would this increased complexity help us learn a more accurate model?
    Would we still be able to compute the optimal parameters for such a system using
    the back-propagation algorithm?
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 即使图2和图3中显示的架构也不是我们能够想象的最复杂的神经网络。额外的隐藏层意味着我们可以添加输入特征之间的额外交互，但对于非常复杂的数据类型（如图像或文档），我们可以很容易地想象出可能需要超过一层混合和重组来捕捉所有感兴趣交互的情况。例如，可以想象一个文档数据集，其中网络捕获的个别单词特征被合并成句子片段特征，这些特征进一步合并成句子、段落和章节模式，从而可能提供5+级别的交互。每个这样的交互都需要另一层隐藏神经元，因此连接数（以及需要调整的权重）相应增加。同样，一个图像可能被解析成不同分辨率的网格，这些网格合并成嵌套的更小和更大的对象。为了通过在我们的网络中添加额外的隐藏层（图4）来适应这些更高级的交互，我们最终会创建一个越来越**深**的网络。我们还可以添加额外的RBM层，就像我们描述的那样。这种增加的复杂性是否有助于我们学习更准确的模式？我们是否仍然能够使用反向传播算法计算这样一个系统的最优参数？
- en: '![Vanishing gradients and explaining away](img/B04881_07_09.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![消失的梯度与解释](img/B04881_07_09.jpg)'
- en: 'Figure 4: Multilayer neural network architecture'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：多层神经网络架构
- en: 'Let us consider what happens in back-propagation when we add an extra layer.
    Recall that when we derived the expression for the change in the error rate as
    a function of the weights in the first layer (between visible and the first hidden
    layer), we ended up with an equation that was a product of the weights between
    the output and hidden layer, as well as the weights in the first layer:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑在反向传播中添加额外层时会发生什么。回想一下，当我们推导出误差率变化作为第一层（可见层和第一隐藏层之间）权重函数的表达式时，我们最终得到一个乘积形式的方程，它是输出层和隐藏层之间的权重以及第一层中的权重的乘积：
- en: '![Vanishing gradients and explaining away](img/B04881_07_27.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![消失的梯度与解释](img/B04881_07_27.jpg)'
- en: Let us consider what happens when the first term (the sum over the errors from
    the output layer) is <1\. Since this formula is a product, the value of the entire
    expression also decreases, meaning we will change the value of wjk by very small
    steps. Now recall that to calculate the change in the error with respect to a
    visible to hidden connection wjk we needed to sum over all the connections from
    the output to this weight. In our example, we had just two connections, but in
    deeper networks we would end up with extra terms such as the first to capture
    the error contribution from all the layers between the hidden and output. When
    we multiply by more terms with value < 1, the value of the total expressions will
    increasingly shrink towards 0, meaning the value of the weight will get updated
    hardly at all during the gradient step. Conversely, if all these terms have value
    > 1, they will quickly inflate the value of the whole expression, causing the
    value of the weight to change wildly between gradient update steps.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑当第一个项（输出层的误差总和）小于1时会发生什么。由于这个公式是乘积形式，整个表达式的值也会减小，这意味着我们将通过非常小的步骤改变wjk的值。现在回想一下，为了计算与可见到隐藏连接wjk相关的误差变化，我们需要对所有从输出到这个权重的连接进行求和。在我们的例子中，我们只有两个连接，但在更深层的网络中，我们最终会得到额外的项，例如第一个项，以捕捉隐藏层和输出层之间所有层的误差贡献。当我们乘以更多小于1的项时，整个表达式的值会越来越接近0，这意味着在梯度步骤中权重的值几乎不会更新。相反，如果所有这些项的值都大于1，它们将迅速增加整个表达式的值，导致权重的值在梯度更新步骤之间剧烈变化。
- en: Thus, the change in error as a function of the hidden to visible weights tends
    to approach 0 or increase in an unstable fashion, causing the weight to either
    change very slowly or oscillate wildly in magnitude. It will therefore take a
    longer time to train the network, and it will be harder to find stable values
    for weights closer to the visible layer. As we add more layers, this problem becomes
    worse as we keep adding more error terms that make it harder for the weights to
    converge to a stable value, as increasing the number of terms in the product representing
    the gradient has a greater likelihood of shrinking or exploding the value.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，误差作为隐藏到可见权重函数的变化往往接近0或以不稳定的方式增加，导致权重要么变化非常缓慢，要么在幅度上剧烈振荡。因此，训练网络将需要更长的时间，并且更难找到接近可见层的稳定权重值。随着我们添加更多层，这个问题会变得更糟，因为我们不断添加更多的误差项，这使得权重更难收敛到稳定值，因为增加表示梯度的乘积中的项数有更大的可能性缩小或爆炸值。
- en: 'Because of this behavior adding more layers and using back-propagation to train
    a deep network are not sufficient for effectively generating more complex features
    by incorporating multiple hidden layers in the network. In fact, this problem
    also known as vanishing gradients due to the fact that the gradients have a greater
    chance of shrinking to zero and disappearing as we add layers, was one of the
    major reasons why multilayer neural network remained practically infeasible for
    many years (Schmidhuber, Jürgen. *Deep learning in neural networks: An overview*.
    Neural Networks 61 (2015): 85-117.). In a sense, the problem is that the outer
    layers of the network ''absorb'' the information from the error function faster
    than the deeper layers, making the rate of learning (represented by the weight
    updates) extremely uneven.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '由于这种行为，仅仅通过添加更多层和使用反向传播来训练深度网络，并不能有效地通过在网络中包含多个隐藏层来生成更复杂的功能。事实上，这个问题也被称为梯度消失，因为随着我们添加层，梯度有更大的可能性缩小到零并消失，这是多层神经网络多年来在实际上不可行的主要原因之一（Schmidhuber,
    Jürgen. *Deep learning in neural networks: An overview*. Neural Networks 61 (2015):
    85-117.）。从某种意义上说，问题是网络的较外层比深层更快地“吸收”了误差函数的信息，使得学习率（由权重更新表示）极为不均匀。'
- en: Even if we were to assume that we are not limited by time in our back-propagation
    procedure and could run the algorithm until the weights finally converge (even
    if this amount of time were impractical for real-world use), multilayer neural
    networks present other difficulties such as explaining away.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 即使假设我们在反向传播过程中不受时间的限制，并且可以运行算法直到权重最终收敛（即使这个时间量对于实际应用来说不切实际），多层神经网络仍然存在其他困难，例如解释消除。
- en: 'The explaining away effect concerns the tendency for one input unit to overwhelm
    the effect of another. A classic example (Hinton, Geoffrey E., Simon Osindero,
    and Yee-Whye Teh. *A fast learning algorithm for deep belief nets*. Neural computation
    18.7 (2006): 1527-1554) is if our response variable is a house jumping off the
    ground. This could be explained by two potential inputs piece of evidence, whether
    a truck has hit the house and whether an earthquake has occurred nearby (Figure
    5):'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '解释消除效应涉及一个输入单元压倒另一个输入单元效应的倾向。一个经典的例子（Hinton, Geoffrey E., Simon Osindero, 和
    Yee-Whye Teh. *A fast learning algorithm for deep belief nets*. Neural computation
    18.7 (2006): 1527-1554）是，如果我们的响应变量是一栋从地面跳起来的房子。这可以通过两个潜在输入的证据来解释，即是否有卡车撞到房子以及附近是否发生了地震（图5）：'
- en: '![Vanishing gradients and explaining away](img/B04881_07_10.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![梯度消失和解释消除](img/B04881_07_10.jpg)'
- en: 'Figure 5: Explaining away causes imbalanced weighting in deep networks'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：解释消除导致深度网络中的权重不平衡
- en: If an earthquake has occurred, then the evidence for this being the cause of
    the house's movement is so strong that the truck collision evidence is minimized.
    Simply put, knowing that an earthquake has occurred means we no longer need any
    additional evidence to explain the house moving, and thus the value of the truck
    evidence becomes negligible. If we are optimizing for how we should weight these
    two sources of evidence (analogous to how we might weight the inputs from hidden
    neurons to an output unit), the weight on the truck collision evidence could be
    set to 0, as the earthquake evidence explains away the other variable. We could
    turn on both inputs, but the probability of these two co-occurring is low enough
    that our learning procedure would not optimally do so. In effect, this means that
    the value of the weights are correlated with whether the hidden neurons (represented
    by each evidence type) are turned on (set to 1). Thus, it is difficult to find
    a set of parameters that does not saturate one weight at the expense of the other.
    Given the problem of vanishing gradients and explaining away, how could we hope
    to find the optimal parameters in a deep neural network composed of many layers?
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果发生了地震，那么这种地震是房屋移动原因的证据就非常强烈，以至于卡车碰撞的证据被最小化。简单来说，知道发生了地震就意味着我们不再需要任何额外的证据来解释房屋的移动，因此卡车证据的价值变得可以忽略不计。如果我们正在优化如何权衡这两种证据来源的权重（类似于我们可能如何权衡从隐藏神经元到输出单元的输入），那么卡车碰撞证据的权重可以设置为0，因为地震证据解释了其他变量。我们可以开启这两个输入，但由于这两个事件同时发生的概率足够低，我们的学习过程不会最优地这样做。实际上，这意味着权重的值与隐藏神经元（由每种证据类型表示）是否开启（设置为1）相关。因此，很难找到一组参数，不会以牺牲另一个权重为代价而饱和。考虑到梯度消失和解释掉的问题，我们如何在由多层组成的深度神经网络中找到最优参数呢？
- en: Pretraining belief networks
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预训练信念网络
- en: Both the vanishing gradient and explaining away effects are in a sense are caused
    by the fact that it is difficult to find an optimal set of weights for large networks
    if we start from a set of random values and perform back-propagation. Unlike the
    logistic regression objective function we saw in [Chapter 5](ch05.html "Chapter 5. Putting
    Data in its Place – Classification Methods and Analysis"), *Putting Data in its
    Place – Classification Methods and Analysis*, the optimal error in a deep learning
    network is not necessarily convex. Thus, following gradient descent through back-propagation
    for many rounds is not guaranteed to converge to globally optimal value. Indeed,
    we can imagine the space of the Error function as a multidimensional landscape,
    where the elevation represents the value of the error function and the coordinates
    represent different values of the weight parameters. Back-propagation navigates
    through different parameter values by moving up or down the slopes of this landscape,
    which are represented by the *steps* taken with each weight update. If this landscape
    consisted of a single steep *peak* located at the optimal value of the weights,
    back-propagation might quickly converge to this value. More often, though, this
    multidimensional space could have many *ravines* and *valleys* (where the error
    functions dips and rises in irregular ways with particular set of weights), such
    that it is difficult for a first-order method such as back-propagation to navigate
    out of local minima/maxima. For example, the first derivative of the error function
    could change slowly over a valley in the error function landscape, as the error
    only gradually increases or decreases as we move in or around the valley. Starting
    in a random location in this landscape through the standard random initialization
    of the weight variables might leave us in a place where we are unlikely to ever
    navigate to the optimal parameter values. Thus, one possibility would be to initialize
    the weights in the network to a more favorable configuration before running back-propagation,
    giving us a better chance of actually finding the optimal weight values.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 消失梯度效应和解释消除效应在某种程度上是由以下事实引起的：如果我们从一个随机值集合开始并执行反向传播，那么在大型网络中找到一组最优权重是困难的。与我们在[第5章](ch05.html
    "第5章. 将数据放在合适的位置 – 分类方法和分析")中看到的逻辑回归目标函数不同，*将数据放在合适的位置 – 分类方法和分析*，深度学习网络中的最优误差不一定总是凸的。因此，通过反向传播进行多次梯度下降并不保证收敛到全局最优值。实际上，我们可以想象误差函数的空间是一个多维景观，其中高度代表误差函数的值，坐标代表不同的权重参数值。反向传播通过沿着这个景观的斜坡上下移动来导航不同的参数值，这些斜坡由每次权重更新所采取的*步骤*表示。如果这个景观由一个位于权重最优值的单峰*顶峰*组成，反向传播可能会快速收敛到这个值。然而，更常见的情况是多维空间中可能有许多*峡谷*和*山谷*（误差函数以不规则的方式随着特定权重集的上升和下降），这使得一阶方法如反向传播难以从局部最小值/最大值中导航出来。例如，误差函数景观中的一个山谷中，误差函数的一阶导数可能会缓慢变化，因为当我们进入或围绕山谷移动时，误差只逐渐增加或减少。通过标准随机初始化权重变量在这个景观中的随机位置开始，我们可能处于一个不太可能导航到最优参数值的位置。因此，一种可能性是在运行反向传播之前将网络中的权重初始化到一个更有利的配置，这样我们就有更大的机会找到最优的权重值。
- en: 'Indeed, this is the essence of the solution proposed by research published
    in 2006 (Hinton, Geoffrey E., Simon Osindero, and Yee-Whye Teh. *A fast learning
    algorithm for deep belief nets*. Neural computation 18.7 (2006): 1527-1554). Instead
    of fitting a multi-layer neural network directly to the response variable for
    a dataset (in this case, the digits represented by a set of images of hand-drawn
    numbers) after random initialization of the weight variables, this study suggested
    that the network weights could be initialized through a pre-training phase that
    would move them closer to the correct values before running back-propagation.
    The networks used in this study contained several RBM layers, and the proposed
    solution was to optimize one RBM at a time through the following steps, which
    are illustrated in **Figure 6**:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '的确，这是2006年发表的研究（Hinton, Geoffrey E., Simon Osindero, and Yee-Whye Teh. *A fast
    learning algorithm for deep belief nets*. Neural computation 18.7 (2006): 1527-1554）提出的解决方案的精髓。该研究不是直接将多层神经网络拟合到数据集（在这种情况下，由一组手绘数字的图像表示的数字）的响应变量上，而是在随机初始化权重变量之后，建议通过一个预训练阶段来初始化网络权重，这样可以在运行反向传播之前将它们移动到正确的值附近。本研究中使用的网络包含几个RBM层，提出的解决方案是依次优化一个RBM，具体步骤在**图6**中展示：'
- en: First, the visible layer is used to generate a set of values for the hidden
    neurons, just as in back-propagation.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，使用可见层生成隐藏神经元的一组值，就像在后向传播中一样。
- en: However, the process is then inverted, with the hidden unit values in the uppermost
    RBM being used as the starting point and the network run backward to recreate
    the input data (as in Figure 3).
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然而，然后过程被反转，最上面的RBM中的隐藏单元值用作起点，网络反向运行以重新创建输入数据（如图3所示）。
- en: The optimal weights between the layers are then calculated using the difference
    of the input data and the data sample generated by running the model backwards
    from the hidden layer.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 层与层之间的最佳权重是通过计算输入数据与从隐藏层反向运行模型生成的数据样本之间的差异来计算的。
- en: This process is iterated several times, until the inferred weights stop changing.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此过程重复多次，直到推断出的权重停止变化。
- en: 'This process is then repeated with successive layers, with each deeper hidden
    layer forming the new input. Additionally, a constraint is enforced that the weights
    between the visible and first hidden layer and the weights between the first and
    second hidden layer are matrix transposes: this is known as *tied weights*. This
    condition is enforced for every pair of weights between adjacent hidden layers:![Pretraining
    belief networks](img/B04881_07_11.jpg)'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后通过连续的层重复此过程，每一层更深的隐藏层形成新的输入。此外，强制执行一个约束，即可见层和第一隐藏层之间的权重以及第一和第二隐藏层之间的权重是矩阵转置：这被称为*绑定权重*。这个条件被强制执行在相邻隐藏层之间每对权重之间：![预训练信念网络](img/B04881_07_11.jpg)
- en: 'Figure 6: Pre-training algorithm for deep belief networks'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6：深度信念网络的预训练算法
- en: This pre-training procedure has the practical effect that the network is initialized
    with weights that are in the general *shape* of the input data. The fact that
    this procedure is employed on a single layer at a time avoids some of the vanishing
    gradient problems discussed previously, since only a single set of weights is
    considered in each step. The problem of explaining away is also minimized due
    to the matching of weights as described in step 5\. Going back to our example
    of house movement, the relative strength of the earthquake and truck weights would
    be represented in the first layer of the deep belief network. In the next phase
    of pre-training, these weights would be inverted through the matrix transpose,
    *undoing* the explaining away effect in the higher layer. This pattern is repeated
    in sequential layers, systematically removing the correlation between the value
    of the weights and the likelihood of the connected hidden unit being activated.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这种预训练程序的实际效果是网络以与输入数据的一般*形状*相似的权重初始化。由于此程序是逐层进行的，因此避免了之前讨论的一些梯度消失问题，因为在每一步中只考虑一组权重。由于第5步中描述的权重匹配，解释问题的可能性也最小化。回到我们关于房屋移动的例子，地震和卡车权重的相对强度将在深度信念网络的第一层中表示。在预训练的下一阶段，这些权重将通过矩阵转置进行反转，*取消*高层中的解释效果。这种模式在连续层中重复，系统地消除权重值与连接的隐藏单元被激活的可能性之间的相关性。
- en: Once this pretraining is complete, a back-propagation-like approach is used
    as described for simpler networks, but the weights now converge much quicker to
    stable values because they have started at a more optimal, instead of random,
    value.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成预训练，就使用与简单网络中描述的类似的后向传播方法，但现在权重更快地收敛到稳定值，因为它们是从一个更优而不是随机的值开始的。
- en: Using dropout to regularize networks
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用dropout正则化网络
- en: Even using the pretraining approach described previously, it can be computationally
    expensive to optimize a large number of parameters in deep networks. We also potentially
    suffer from the same problem as regression models with large numbers of coefficients,
    where the large number of parameters leads the network to overfit the training
    data and not generalize well to data it has not previously seen.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 即使使用之前描述的预训练方法，优化深度网络中的大量参数也可能非常耗时。我们可能还会遇到与具有大量系数的回归模型相同的问题，即大量参数导致网络过度拟合训练数据，并且无法很好地泛化到之前未见过的数据。
- en: 'While in the case of regression, we used approaches such as Ridge, Lasso, and
    Elastic Net to regularize our models, for deep networks we can use an approach
    known as Dropout to reduce overfitting (Srivastava, Nitish, et al. *Dropout: a
    simple way to prevent neural networks from overfitting.* Journal of Machine Learning
    Research 15.1 (2014): 1929-1958.). The idea is relatively is simple: at each stage
    of tuning the weights, we randomly remove some neuron units along with their connections
    from the network and only update the remaining weights. As we repeat this process,
    we effectively average over many possible network structures. This is because
    with a 50% probability of dropping any given neuron at each stage from the network,
    each stage of our training effectively samples from 2n possible network structures.
    Thus, the model is regularized because we are only fitting a subsample of parameters
    at each stage, and similar to the random forest we examined in [Chapter 5](ch05.html
    "Chapter 5. Putting Data in its Place – Classification Methods and Analysis"),
    *Putting Data in its Place – Classification Methods and Analysis*, we average
    over a larger number of randomly constructed networks. Even though dropout can
    reduce over fitting, it could potentially make the training process longer, since
    we need to average over more networks to obtain an accurate prediction.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '在回归的情况下，我们使用了如岭回归、Lasso 和弹性网络等方法来正则化我们的模型，而对于深度网络，我们可以使用称为Dropout的方法来减少过拟合（Srivastava,
    Nitish, 等人 *Dropout：一种防止神经网络过拟合的简单方法*。机器学习研究杂志 15.1 (2014): 1929-1958）。这个想法相对简单：在调整权重的每个阶段，我们随机从网络中移除一些神经元及其连接，并且只更新剩余的权重。随着我们重复这个过程，我们实际上是在许多可能的网络结构上取平均值。这是因为，在每个阶段有50%的概率从网络中丢弃任何给定的神经元，我们训练的每个阶段实际上是从2^n个可能网络结构中采样。因此，模型被正则化，因为我们只在每个阶段拟合参数的子样本，并且类似于我们在[第5章](ch05.html
    "第5章。数据定位 – 分类方法和分析")中检查的随机森林，*数据定位 – 分类方法和分析*，我们在更多随机构建的网络上取平均值。尽管Dropout可以减少过拟合，但它可能会使训练过程更长，因为我们需要平均更多的网络以获得准确的预测。'
- en: Convolutional networks and rectified units
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积网络和整流单元
- en: Even though the pre-training procedure provides a way to initialize network
    weights, as we add layers the overall model complexity increases. For larger input
    data (for example, large images), this can lead to increasing numbers of weights
    along with each additional layer, and thus the training period may take longer.
    Thus, for some applications, we might accelerate the training process by intelligently
    simplifying the structure of our network by (1) not making a connection between
    every single neuron in every layer and (2) changing the functions used for neurons.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管预训练过程提供了一种初始化网络权重的途径，但随着我们添加层，整体模型复杂性增加。对于更大的输入数据（例如，大图像），这可能导致每个额外层增加的权重数量增加，因此训练周期可能会更长。因此，对于某些应用，我们可能通过以下方式智能地简化网络结构来加速训练过程：（1）不在每一层的每个神经元之间建立连接；（2）改变神经元使用的函数。
- en: 'These kinds of modifications are common in a type of deep network also known
    as a Convotional Network (LeCun, Yann, et al. *Gradient-based learning applied
    to document recognition*. Proceedings of the IEEE 86.11 (1998): 2278-2324; Krizhevsky,
    Alex, Ilya Sutskever, and Geoffrey E. Hinton. *Imagenet classification with deep
    convolutional neural networks*. Advances in neural information processing systems.
    2012). The name convolution comes from image analysis, where a convolution operator
    such as the opening and dilation operations we used in [Chapter 6](ch06.html "Chapter 6. Words
    and Pixels – Working with Unstructured Data"), *Words and Pixels – Working with
    Unstructured Data* are applied to overlapping areas of an image. Indeed, convolutional
    Networks are commonly applied to tasks involving image recognition. While the
    number of potential configurations is large, a potential structure of a convolutional
    network might be the following (see Figure 7):'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '这些修改在一种称为卷积网络（LeCun, Yann, 等人 *基于梯度的学习应用于文档识别*。IEEE汇刊 86.11 (1998): 2278-2324；Krizhevsky,
    Alex, Ilya Sutskever 和 Geoffrey E. Hinton *使用深度卷积神经网络进行 Imagenet 分类*。神经信息处理系统进展。2012）的深度网络中很常见。卷积这个名字来源于图像分析，其中像我们在[第6章](ch06.html
    "第6章。文字与像素 – 处理非结构化数据")中使用的开运算和膨胀运算这样的卷积算子被应用于图像的重叠区域。实际上，卷积网络通常应用于涉及图像识别的任务。虽然可能的配置数量很大，但一个卷积网络的潜在结构可能如下（见图7）：'
- en: '**The visible, input layer, with width w and height h**: For color images,
    this input can be three-dimensional, with one depth layer for each of the red,
    green, and blue channels.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可见的输入层，宽度为w，高度为h**：对于彩色图像，这个输入可以是三维的，每个红色、绿色和蓝色通道都有一个深度层。'
- en: '**A convolutional layer**: Here, a single neuron could be connected to a square
    region through all three color channels (*nxnx3*). Each of these nxnx3 units has
    a weight connecting it to a neuron in the convolutional layer. Furthermore, we
    could have more than one neuron in the convolutional layer connected to each of
    these *nxnx3* units, but each with a different set of weights.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卷积层**：在这里，单个神经元可以通过所有三个颜色通道（*nxnx3*）连接到一个正方形区域。这些nxnx3单元中的每一个都有一个权重连接到卷积层中的一个神经元。此外，我们可以在卷积层中连接到每个这些*nxnx3*单元的多个神经元，但每个神经元都有不同的权重集。'
- en: '**A rectifying layer**: Using the **Rectified Linear Unit** (**ReLU**) discussed
    later in this chapter, each of the neurons outputs in the convolutional layer
    are thresholded to yield another set of neurons of the same size.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**整流层**：使用本章后面讨论的**整流线性单元**（**ReLU**），卷积层中的每个神经元输出都被阈值化，以产生另一组相同大小的神经元。'
- en: '**A downsampling layer**: This type of layer averages over a subregion in the
    previous layer to produce a layer with a smaller width and height, while leaving
    the depth unchanged.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**下采样层**：这种类型的层在上一层的子区域上平均，以产生一个宽度更小、高度更小的层，同时保持深度不变。'
- en: '**A fully connected layer**: In this layer each unit in the downsampling layer
    is connected to a vector of output (for example, a 10 unit vector representing
    10 different class labels).'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全连接层**：在这个层中，下采样层中的每个单元都连接到一个输出向量（例如，一个表示10个不同类别标签的10单元向量）。'
- en: 'This architecture exploits the structure of the data (examining local patterns
    in images), and training is faster because we only make selective connections
    between neurons in each layer, leading to fewer weights to optimize. A second
    reason that this structure can train more rapidly is due to the activation functions
    used in the rectification and pooling layers. A common choice of pooling function
    is simply the maximum of all inputs, also known as a **Rectified Linear Unit**
    (**ReLU**) (Nair, Vinod, and Geoffrey E. Hinton. *Rectified linear units improve
    restricted boltzmann machines.* Proceedings of the 27th International Conference
    on Machine Learning (ICML-10). 2010). which is:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这种架构利用了数据结构（检查图像中的局部模式），并且训练速度更快，因为我们只在每层的神经元之间进行选择性连接，导致需要优化的权重更少。这种结构可以更快地训练的第二个原因是由于整流和池化层中使用的激活函数。池化函数的一个常见选择是所有输入的最大值，也称为**整流线性单元**（**ReLU**）（Nair,
    Vinod, and Geoffrey E. Hinton. *Rectified linear units improve restricted boltzmann
    machines.* Proceedings of the 27th International Conference on Machine Learning
    (ICML-10). 2010）。它是这样的：
- en: '![Convolutional networks and rectified units](img/B04881_07_28.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![卷积网络和整流单元](img/B04881_07_28.jpg)'
- en: 'Here, `z` is the input to a given neuron. Unlike the logistic function described
    previously, the ReLU is not bounded by the range `[0,1]`, meaning that the values
    to neurons following it in the network can change more rapidly than is possible
    with logistic functions. Furthermore, the gradient of the ReLU is given by:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`z`是给定神经元的输入。与之前描述的对数函数不同，ReLU不受范围`[0,1]`的限制，这意味着网络中跟随它的神经元的值可以比对数函数更快地变化。此外，ReLU的梯度由以下给出：
- en: '![Convolutional networks and rectified units](img/B04881_07_29.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![卷积网络和整流单元](img/B04881_07_29.jpg)'
- en: 'This means that gradients do not tend to vanish (unless the neuron inputs drop
    very low such that it is always off) or explode, as the maximum change is `1`.
    In the former case, to prevent the ReLU from turning permanently off, the function
    could be modified to be leaky:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着梯度不太可能消失（除非神经元输入非常低，以至于它总是关闭）或爆炸，因为最大变化是`1`。在前一种情况下，为了防止ReLU永久关闭，函数可以被修改为漏斗状：
- en: '![Convolutional networks and rectified units](img/B04881_07_30.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![卷积网络和整流单元](img/B04881_07_30.jpg)'
- en: Here, `α` is a small value such a `0.01`, preventing the neuron from ever being
    set to 0.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`α`是一个小的值，例如`0.01`，防止神经元被设置为0。
- en: '![Convolutional networks and rectified units](img/B04881_07_36.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![卷积网络和整流单元](img/B04881_07_36.jpg)'
- en: 'Figure 7: Convolutional neural network architecture. For clarity, connections
    in convolutional layer are represented to highlighted region rather than all wxhxd
    neurons, and only a subset of the network converging to a pooling layer neuron
    is shown.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：卷积神经网络架构。为了清晰起见，卷积层的连接表示为高亮区域，而不是所有wxhxd神经元，并且只显示了网络中汇聚到池化层神经元的子集。
- en: Tip
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: '**Aside: Alternative Activation Functions**'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '**旁注：其他激活函数**'
- en: 'In addition to the linear, sigmoid, and ReLU functions discussed previously,
    other activation functions are also used in building deep networks. One is the
    hyperbolic tangent function, also known as the `tanh` function, given by:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 除了之前讨论的线性、Sigmoid和ReLU函数外，在构建深度网络时还会使用其他激活函数。其中一个是双曲正切函数，也称为`tanh`函数，其表达式为：
- en: '![Convolutional networks and rectified units](img/B04881_07_31.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![卷积网络和整流单元](img/B04881_07_31.jpg)'
- en: 'The output of this function is in the range [`–1`,`1]`, unlike the sigmoid
    or ReLU, which are in the range [0,1], and some evidence suggests that this could
    accelerate training of networks by allowing the average output of neurons to be
    zero and thus reduce bias (LeCun, Yann, Ido Kanter, and Sara A. Solla. "Second
    order properties of error surfaces: Learning time and generalization." Advances
    in neural information processing systems 3 (1991): 918-924.). Similarly, we could
    imagine using a Gaussian function such as the kernels we saw in [Chapters 3](ch03.html
    "Chapter 3. Finding Patterns in the Noise – Clustering and Unsupervised Learning"),
    *Finding Patterns in the Noise – Clustering and Unsupervised Learning*, and [Chapter
    5](ch05.html "Chapter 5. Putting Data in its Place – Classification Methods and
    Analysis"), *Putting Data in its Place – Classification Methods and Analysis*,
    in the context of spectral clustering and SVMs, respectively. The `softmax` function
    used for multinomial regression in [Chapter 5](ch05.html "Chapter 5. Putting Data
    in its Place – Classification Methods and Analysis"), *Putting Data in its Place
    – Classification Methods and Analysis*, is also a candidate; the number of potential
    functions increases the flexibility of deep models, allowing us to tune specific
    behavior according to the problem at hand.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '该函数的输出范围在`[–1,1]`之间，与Sigmoid或ReLU的输出范围在`[0,1]`之间不同，一些证据表明，这可以通过允许神经元的平均输出为零来加速网络的训练，从而减少偏差（LeCun,
    Yann, Ido Kanter, and Sara A. Solla. "Second order properties of error surfaces:
    Learning time and generalization." Advances in neural information processing systems
    3 (1991): 918-924.). 类似地，我们可以想象在谱聚类和SVMs的背景下使用高斯函数，如我们在[第3章](ch03.html "第3章. 在噪声中寻找模式
    – 聚类和无监督学习")、*在噪声中寻找模式 – 聚类和无监督学习*和[第5章](ch05.html "第5章. 将数据放在合适的位置 – 分类方法和分析")、*将数据放在合适的位置
    – 分类方法和分析*中看到的核函数。在[第5章](ch05.html "第5章. 将数据放在合适的位置 – 分类方法和分析")、*将数据放在合适的位置 –
    分类方法和分析*中用于多项式回归的`softmax`函数也是一个候选者；潜在函数数量的增加增加了深度模型的灵活性，使我们能够根据具体问题调整特定的行为。'
- en: Compressing Data with autoencoder networks
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用自动编码器网络压缩数据
- en: 'While most of our discussion in this chapter involves the use of deep learning
    for classification tasks, these models can also be used for dimensionality reduction
    in a way comparable to the matrix factorization methods we discussed in [Chapter
    6](ch06.html "Chapter 6. Words and Pixels – Working with Unstructured Data"),
    *Words and Pixels – Working with Unstructured Data*. In such an application, also
    known as an *auto-encoder* network (Hinton, Geoffrey E., and Ruslan R. Salakhutdinov.
    *Reducing the dimensionality of data with neural networks.* Science 313.5786 (2006):
    504-507), the objective is not to fit a response (such as binary label), but to
    reconstruct the data itself. Thus, the visible and output layers are always the
    same size (Figure 8), while the hidden layers are typically smaller and thus form
    a lower-dimensional representation of the data that can be used to reconstruct
    the input. Thus, like PCA or NMF, autoencoders discover a compact version of the
    input that can approximate the original (with some error). If the hidden layer
    was not smaller than the visible and output, the network might well just optimize
    the hidden layer to be identical to the input; this would allow the network to
    perfectly reconstruct the input, but at the expense of any feature extraction
    or dimensionality reduction.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管本章的大部分讨论涉及使用深度学习进行分类任务，但这些模型也可以用于降维，其方式与我们在[第 6 章](ch06.html "第 6 章。文字与像素
    – 处理非结构化数据")中讨论的矩阵分解方法相当，即 *文字与像素 – 处理非结构化数据*。在这种应用中，也称为 *自动编码器* 网络 (Hinton, Geoffrey
    E., 和 Ruslan R. Salakhutdinov. *使用神经网络降低数据维度*. 科学 313.5786 (2006): 504-507)，目标不是拟合响应（如二进制标签），而是重建数据本身。因此，可见层和输出层总是相同的大小（图
    8），而隐藏层通常较小，因此形成数据的低维表示，可用于重建输入。因此，类似于 PCA 或 NMF，自动编码器发现输入的紧凑版本，可以近似原始数据（存在一些误差）。如果隐藏层的大小不小于可见层和输出层，网络可能只是优化隐藏层以与输入相同；这将允许网络完美地重建输入，但会牺牲任何特征提取或降维。'
- en: '![Compressing Data with autoencoder networks](img/B04881_07_37.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![使用自动编码器网络压缩数据](img/B04881_07_37.jpg)'
- en: 'Figure 8: Autoencoder network architecture'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：自动编码器网络架构
- en: Optimizing the learning rate
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化学习率
- en: 'In the examples we have discussed above, the learning rate for the parameters
    at each stage is always a fixed value α. Intuitively, it makes sense that for
    some parameters we may want to adjust the value more aggressively, while others
    less. Many optimizations have been proposed for this sort of tuning. For example,
    Adative Gradient (AdaGrad) (Duchi, John, Elad Hazan, and Yoram Singer. *Adaptive
    subgradient methods for online learning and stochastic optimization*. Journal
    of Machine Learning Research 12.Jul (2011): 2121-2159.) uses a learning rate for
    each parameter based on the past history of gradients for a given parameter:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '在我们上面讨论的例子中，每个阶段的参数学习率始终是一个固定的值 α。直观上，对于某些参数我们可能希望更积极地调整其值，而对于其他参数则希望调整得较少。为此，已经提出了许多优化方法。例如，自适应梯度（Adaptive
    Gradient，AdaGrad）(Duchi, John, Elad Hazan, 和 Yoram Singer. *自适应子梯度方法用于在线学习和随机优化*.
    机器学习研究杂志 12.Jul (2011): 2121-2159.) 使用基于给定参数过去梯度历史的每个参数的学习率：'
- en: '![Optimizing the learning rate](img/B04881_07_32.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![优化学习率](img/B04881_07_32.jpg)'
- en: Where *Gt* represents the sum of squares of all gradients of a particular parameter,
    `gt` is the gradient at the current step, and ε is a smoothing parameter. Thus,
    the learning rate at each stage is the global value α, multiplied by a fraction
    that the current gradient represents of the historic variation. If the current
    gradient is high compared to historical updates, then we change the parameter
    more. Otherwise, we should change it less. Over time, most the learning rates
    will shrink toward zero, accelerating convergence.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *Gt* 代表特定参数所有梯度的平方和，`gt` 是当前步骤的梯度，ε 是平滑参数。因此，每个阶段的学习率是全球值 α，乘以当前梯度表示的历史变化的分数。如果当前梯度与历史更新相比较高，那么我们将改变参数。否则，我们应该改变得较少。随着时间的推移，大多数学习率将趋向于零，加速收敛。
- en: 'A natural extension of this idea is used in AdaDelta (Zeiler, Matthew D. *ADADELTA:
    an adaptive learning rate method*. arXiv preprint arXiv:1212.5701 (2012)), where
    instead of using the full history of gradient updates G, we, at each step, replace
    this value with the average of the current gradient and the historical average
    gradient:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '这个想法的自然扩展被用于 AdaDelta（Zeiler, Matthew D. *ADADELTA: an adaptive learning rate
    method*. arXiv preprint arXiv:1212.5701 (2012)），在这里，我们不是使用梯度更新历史记录的全集 G，而是在每一步，用当前梯度和历史平均梯度的平均值来替换这个值：'
- en: '![Optimizing the learning rate](img/B04881_07_33.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![优化学习率](img/B04881_07_33.jpg)'
- en: The expression for Adagrad then uses the above formula in the denominator instead
    of *Gt*. Like Adagrad, this will tend to reduce the learning rate for parameters
    that are not changing significantly relative to their history.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: Adagrad 的表达式在分母中使用上述公式代替 *Gt*。与 Adagrad 一样，这倾向于减少相对于其历史记录变化不大的参数的学习率。
- en: 'The `TensorFlow` library we will examine in the following also provides the
    Adaptive Moment Estimation (ADAM) method for adjusting the learning rate (Kingma,
    Diederik, and Jimmy Ba. *Adam: A method for stochastic optimization*. arXiv preprint
    arXiv:1412.6980 (2014).). In this method, like AdaDelta, we keep an average of
    the squared gradient, but also of the gradient itself. The update rule is then
    as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将在以下部分检查的 `TensorFlow` 库也提供了自适应矩估计（ADAM）方法来调整学习率（Kingma, Diederik, and Jimmy
    Ba. *Adam: A method for stochastic optimization*. arXiv preprint arXiv:1412.6980
    (2014)）。在这个方法中，与 AdaDelta 一样，我们保持平方梯度和梯度的平均值。更新规则如下：'
- en: '![Optimizing the learning rate](img/B04881_07_34.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![优化学习率](img/B04881_07_34.jpg)'
- en: Here, the weighted averages as in `AdaDelta`, normalized by dividing by a decay
    parameter (`1-β`). Many other algorithms have been proposed, but the sample of
    methods we have described should give you an idea of how the learning rate may
    be adaptively tuned to accelerate training of deep networks.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，与 `AdaDelta` 中的加权平均值一样，通过除以衰减参数（`1-β`）进行归一化。已经提出了许多其他算法，但我们所描述的方法样本应该能给你一个关于如何自适应调整学习率以加速深度网络训练的想法。
- en: Tip
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**提示**'
- en: '**Aside: alternative network architectures**'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '**旁白：替代网络架构**'
- en: 'In addition to the Convolutional, Feed Forward, and Deep Belief Networks we
    have discussed, other network architectures are tuned for particular problems.
    **Recurrent Neural Networks** (**RNNs**) have sparse two-way connections between
    layers, allowing units to exhibit reinforcing behavior through these cycles (Figure
    9). Because the network has a memory from this cycle, it can be used to process
    data for tasks as speech recognition (Graves, Alex, et al. "A novel connectionist
    system for unconstrained handwriting recognition." IEEE transactions on pattern
    analysis and machine intelligence 31.5 (2009): 855-868.), where a series of inputs
    of indeterminate length is processed, and at each point the network can produce
    a predicted label based on the current and previous inputs. Similarly, **Long
    Short Term Memory Networks** (**LSTM**) (*Hochreiter, Sepp, and Jürgen Schmidhuber.
    Long short-term memory. Neural computation 9.8 (1997): 1735-1780).* have cyclic
    elements that allow units to remember input from previously input data. In contrast
    to RNNs, they also have secondary units that can erase the values in the cyclically
    activated units, allowing the network to retain information from inputs over a
    particular window of time (see **Figure 9**, the loop represents this `forgetting`
    function which may be activated by the inputs) .'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '除了我们讨论过的卷积、前馈和深度信念网络之外，其他网络架构也被调整以解决特定问题。**循环神经网络**（**RNNs**）在层之间有稀疏的双向连接，允许单元通过这些循环表现出增强行为（见图
    9）。由于网络从这个循环中具有记忆，它可以用于处理语音识别等任务的数据（Graves, Alex, et al. "A novel connectionist
    system for unconstrained handwriting recognition." IEEE transactions on pattern
    analysis and machine intelligence 31.5 (2009): 855-868），在这些任务中，一系列不确定长度的输入被处理，并且网络可以在每个点根据当前和之前的输入产生一个预测标签。同样，**长短期记忆网络**（**LSTM**）（Hochreiter,
    Sepp, and Jürgen Schmidhuber. Long short-term memory. Neural computation 9.8 (1997):
    1735-1780）具有循环元素，允许单元记住先前输入的数据。与 RNNs 相比，它们还具有可以清除循环激活单元中值的辅助单元，允许网络在特定时间窗口内保留输入信息（见**图
    9**，循环代表这个`遗忘`功能，它可能由输入激活）。'
- en: '![Optimizing the learning rate](img/B04881_07_38.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![优化学习率](img/B04881_07_38.jpg)'
- en: 'Figure 9: Recurrent Neural Network (RNN) and Long Short Term Memory (LSTM)
    architectures.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：循环神经网络（RNN）和长短期记忆（LSTM）架构。
- en: Now that we have seen how a deep learning network is constructed, trained, and
    tuned through a variety of optimizations, let's look at a practical example of
    image recognition.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了如何通过多种优化构建、训练和调整深度学习网络，让我们看看一个图像识别的实际例子。
- en: The TensorFlow library and digit recognition
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow库和数字识别
- en: For the exercises in this chapter, we will be using the `TensorFlow` library
    open-sourced by Google (available at [https://www.tensorflow.org/](https://www.tensorflow.org/)).
    Installation instructions vary by operating system. Additionally, for Linux systems,
    it is possible to leverage both the CPU and **graphics processing unit** (**GPU**)
    on your computer to run deep learning models. Because many of the steps in training
    (such as the multiplications required to update a grid of weight values) involve
    matrix operations, they can be readily parallelized (and thus accelerated) by
    using a GPU. However, the `TensorFlow` library will work on CPU as well, so don't
    worry if you don't have access to an Nvidia GPU card.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章的练习，我们将使用由谷歌开源的`TensorFlow`库（可在[https://www.tensorflow.org/](https://www.tensorflow.org/)找到）。安装说明因操作系统而异。此外，对于Linux系统，你可以利用计算机上的CPU和**图形处理单元**（**GPU**）来运行深度学习模型。由于训练中的许多步骤（如更新权重值网格所需的乘法）涉及矩阵运算，因此它们可以通过使用GPU轻松并行化（从而加速）。然而，`TensorFlow`库也可以在CPU上运行，所以如果你没有访问Nvidia
    GPU卡也不要担心。
- en: The MNIST data
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MNIST数据
- en: The data we will be examining in this exercise is a set of images of hand-drawn
    numbers from 0 to 9 from the **Mixed National Institute of Standards and Technology**
    (**MNIST**) database (LeCun, Yann, Corinna Cortes, and Christopher JC Burges.
    *The MNIST database of handwritten digits.* (1998)). Similar to the Hello World!
    program used to introduce basic programming techniques, or the word count example
    used for demonstrating distributed computing frameworks, the MNIST data is a common
    example used to demonstrate the functions of neural network libraries. The prediction
    task associated with this data is to assign a label (digit from 0 to 9) for an
    image, given only the input pixels.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将检查的是来自**混合国家标准与技术研究院**（**MNIST**）数据库（LeCun, Yann, Corinna Cortes,
    and Christopher JC Burges. *The MNIST database of handwritten digits.* (1998)）的一组手绘数字0到9的图像。类似于用于介绍基本编程技术的Hello
    World!程序，或者用于演示分布式计算框架的单词计数示例，MNIST数据是用于演示神经网络库功能的常见示例。与这些数据相关的预测任务是给图像分配一个标签（0到9的数字），只给出输入像素。
- en: 'The `TensorFlow` library provides a `convenient` library function to load this
    data using the commands:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '`TensorFlow`库提供了一个方便的库函数，可以使用以下命令加载数据：'
- en: '[PRE0]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Note that along with specifying that we wish to load the MNIST data, we have
    indicated that the target variable (the digit represented by the image) should
    be encoded in a binary vector (for example, the number 3 is indicated by placing
    a 1 in the fourth element of this vector, since the first element encodes the
    digit 0). Once we have loaded the data, we can start examining the images themselves.
    We can see that the data has already been conveniently divided into a training
    and test set using a 4:1 split by examining the length of the training and test
    sets using the commands:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，除了指定我们希望加载MNIST数据外，我们还指明目标变量（图像表示的数字）应以二进制向量编码（例如，数字3通过在这个向量的第四个元素中放置一个1来表示，因为第一个元素编码数字0）。一旦我们加载了数据，我们就可以开始检查图像本身。我们可以看到，数据已经方便地被分为训练集和测试集，使用4:1的分割，通过检查训练集和测试集的长度使用以下命令：
- en: '[PRE1]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Each of these images is a *28*28* pixel image. In the data, these are stored
    as a one-dimensional vector of length 784, but we can use the `skimage` library
    from the previous chapter to visualize the images once we have reshaped the array
    into its original dimensions using the commands.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图像中的每一个都是一个 *28*28* 像素的图像。在数据中，这些图像被存储为一个长度为784的一维向量，但一旦我们使用命令将数组重塑为其原始维度，我们就可以使用上一章中的`skimage`库来可视化这些图像。
- en: '[PRE2]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Which displays the first image in the set:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 显示集合中的第一张图像：
- en: '![The MNIST data](img/B04881_07_39.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![MNIST数据](img/B04881_07_39.jpg)'
- en: 'This looks like the number 7: to check the label assigned to this image, we
    can examine the labels element of the train object using:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来像数字7：要检查分配给此图像的标签，我们可以使用以下命令检查train对象的labels元素：
- en: '[PRE3]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: which gives
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了
- en: '`array([ 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])`'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '`array([ 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])`'
- en: The label is a 10-element vector, representing (from left to right) the digits
    0–9, with 1 in the position associated with the label for an image. Indeed, the
    label assigned to this image is 7\. Note that the label array takes the same shape
    as the final layer of the neural network algorithms we have been examining, giving
    the convenient ability to directly compare the label to the output once we have
    calculated the prediction.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 标签是一个10个元素的向量，从左到右代表数字0到9，与图像标签关联的位置为1。实际上，分配给此图像的标签是7。请注意，标签数组与我们所检查的神经网络算法的最终层具有相同的形状，这使得我们能够方便地将标签直接与计算出的预测进行比较。
- en: Now that we have examined the data, let us use the other utilities of the `TensorFlow`
    library to develop a neural network that can predict the label of an image.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经检查了数据，让我们使用`TensorFlow`库的其他工具来开发一个可以预测图像标签的神经网络。
- en: Constructing the network
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建网络
- en: 'As you can probably appreciate by now, the structure of deep neural networks
    can be extremely complex. Thus, if we were to define variables for each layer
    of the network, we would end up with a long block of code that would need to be
    modified every time we changed the structure of the network. Because in practical
    applications we may want to experiment with many different variations of depth,
    layer size, and connectivity, we instead show in this exercise an example of how
    to make this structure generic and reusable. The key ingredients are functions
    to produce the layers, a list of the desired layers specified by these generator
    functions, and an outer process that links together the generated layers:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如您现在可能已经欣赏到的，深度神经网络的架构可以极其复杂。因此，如果我们为网络的每一层定义变量，我们最终会得到一大块代码，每次我们改变网络结构时都需要对其进行修改。因为在实际应用中，我们可能想要尝试许多不同深度的变体、层大小和连接性，所以我们在这个练习中展示了如何使这种结构通用和可重用。关键成分是生成层的函数，由这些生成函数指定的所需层列表，以及一个外部过程，它将生成的层连接起来：
- en: '[PRE4]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This format thus allows us to template the construction of the network in a
    way that is easily reconfigured and reused.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这种格式允许我们以易于重新配置和重用的方式模板化网络的构建。
- en: 'This function constructs a series of `convolutional`/`max_pooling` layers,
    followed by one or more fully connected layers, whose output is used to generate
    a prediction. At the end, we simply return the final layer prediction from the
    `softmax` function as the output. Thus, we can configure a network by setting
    a few parameters:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数构建了一系列`卷积`/`最大池化`层，随后是一或多个全连接层，其输出用于生成预测。最后，我们只需将`softmax`函数的最终层预测作为输出返回。因此，我们可以通过设置一些参数来配置网络：
- en: '[PRE5]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note that the input (`X`) and the true labels (observed) are both placeholders,
    as is the probability of dropout in a layer (`keep_prob`)—they do not contain
    actual values, but will be filled in as the network is trained and we submit batches
    of data to the algorithm.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，输入（`X`）、真实标签（观察到的）以及层中丢弃概率（`keep_prob`）都是占位符，它们不包含实际值，但将在网络训练过程中以及我们向算法提交数据批次时被填充。
- en: 'Now all we need to do is initialize a session and begin submitting batches
    of data using the following code:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要做的只是初始化一个会话，并开始使用以下代码提交数据批次：
- en: '[PRE6]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We can observe the progress of the algorithm as it trains, with the accuracy
    of every 1000th iteration printed to the console.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在算法训练过程中观察其进度，每1000次迭代的准确度都会打印到控制台。
- en: Summary
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we introduced deep neural networks as a way to generate models
    for complex data types where features are difficult to engineer. We examined how
    neural networks are trained through back-propagation, and why additional layers
    make this optimization intractable. We discussed solutions to this problem and
    demonstrated the use of the `TensorFlow` library to build an image classifier
    for hand-drawn digits.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了深度神经网络作为生成复杂数据类型模型的途径，其中特征难以工程化。我们探讨了神经网络通过反向传播进行训练的方式，以及为什么额外的层使得这种优化变得难以处理。我们讨论了这个问题的一些解决方案，并展示了如何使用`TensorFlow`库构建一个用于手绘数字的图像分类器。
- en: 'Now that you have covered a wide range of predictive models, we will turn in
    the final two chapters to the last two tasks in generating analytical pipelines:
    turning the models that we have trained into a repeatable, automated process,
    and visualizing the results for ongoing insights and monitoring.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经覆盖了广泛的预测模型，我们将转向最后两章，探讨生成分析管道的最后两个任务：将我们训练的模型转化为可重复的、自动化的流程，以及可视化结果以获取持续洞察和监控。
