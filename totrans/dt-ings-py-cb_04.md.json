["```py\n$ jupyter notebook\n```", "```py\n    import csv\n    filename = \"     path/to/spotify_data.csv\"\n    columns = []\n    rows = []\n    with open (filename, 'r', encoding=\"utf8\") as f:\n        csvreader = csv.reader(f)\n        fields = next(csvreader)\n        for row in csvreader:\n            rows.append(row)\n    ```", "```py\n    print(column for column in columns)\n    ```", "```py\n    print('First 10 rows:')\n    for row in rows[:5]:\n        for col in row:\n            print(col)\n        print('\\n')\n    ```", "```py\nimport csv\nfilename = \"spotify_data.csv\"\ncolumns = []\nrows = []\n```", "```py\n    $ pip install pandas\n    ```", "```py\n    import pandas as pd\n    spotify_df = pd.read_csv('spotify_data.csv')\n    spotify_df.head()\n    ```", "```py\n    import json\n    filename_json = 'github_events.json'\n    with open (filename_json, 'r') as f:\n        github_events = json.loads(f.read())\n    ```", "```py\n    id_list = [item['id'] for item in github_events]\n    print(id_list)\n    ```", "```py\n['25208138097',\n '25208138110',\n (...)\n '25208138008',\n '25208137998']\n```", "```py\nimport json\nfilename_json = 'github_events.json'\n```", "```py\nwith open (filename_json, 'r') as f:\n    github_events = json.loads(f.read())\n```", "```py\nid_list = [item['id'] for item in github_events]\n```", "```py\nimport pandas as pd\ngithub_events = pd.read_json('github_events.json')\ngithub_events.head(3)\n```", "```py\ngithub_events['id']\n```", "```py\n0     25208138097\n1     25208138110\n2     25208138076\n(...)\n27    25208138012\n28    25208138008\n29    25208137998\nName: id, dtype: int64\n```", "```py\n$ pyspark –version\n```", "```py\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  '_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.1.2\n      /_/\nUsing Scala version 2.12.10, OpenJDK 64-Bit Server VM, 1.8.0_342\nBranch HEAD\nCompiled by user centos on 2021-05-24T04:27:48Z\nRevision de351e30a90dd988b133b3d00fa6218bfcaba8b8\nUrl https://github.com/apache/spark\nType --help for more information.\n```", "```py\n    from pyspark.sql import SparkSession\n    spark = SparkSession.builder \\\n          .master(\"local[1]\") \\\n          .appName(\"DataIngestion\") \\\n          .config(\"spark.executor.memory\", '1g') \\\n          .config(\"spark.executor.cores\", '3') \\\n          .config(\"spark.cores.max\", '3') \\\n          .enableHiveSupport() \\\n          .getOrCreate()\n    ```", "```py\n22/11/14 11:09:55 WARN Utils: Your hostname, DESKTOP-DVUDB98 resolves to a loopback address: 127.0.1.1; using 172.27.100.10 instead (on interface eth0)\n22/11/14 11:09:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n22/11/14 11:09:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n```", "```py\n    spark\n    ```", "```py\n spark = .builder \\\n```", "```py\n       .master(\"local[1]\") \\\n      .appName(\"DataIngestion\") \\\n```", "```py\n      .config(\"spark.executor.memory\", '1g') \\\n      .config(\"spark.executor.cores\", '3') \\\n      .config(\"spark.cores.max\", '3') \\\n      .enableHiveSupport() \\\n```", "```py\n      .getOrCreate()\n```", "```py\nspark.sparkContext.getConf().getAll()\n```", "```py\n    from pyspark.sql import\n    spark = .builder \\\n          .master(\"local[1]\") \\\n          .appName(\"DataIngestion_CSV\") \\\n          .config(\"spark.executor.memory\", '3g') \\\n          .config(\"spark.executor.cores\", '1') \\\n          .config(\"spark.cores.max\", '1') \\\n          .getOrCreate()\n    ```", "```py\n    df = spark.read.option('header',True).csv('spotify_data.csv')\n    ```", "```py\n    df.show()\n    ```", "```py\ndf = spark.read.option('header',True).csv('spotify_data.csv')\n```", "```py\ndf = spark.read.options(header= 'True',\n                       sep=',',\n                       inferSchema='True') \\\n                .csv('spotify_data.csv')\n```", "```py\ndf = spark.read.options(header= 'True',\n                       sep=',',\n                       inferSchema='True') \\\n                .csv('spotify_data_pipe.csv')\n```", "```py\n    df_broken = spark.read.options(header= 'True', sep=',',\n                           inferSchema='True') \\\n                    .csv('listings.csv')\n    df_broken.show()\n    ```", "```py\ngroup = df_1.groupBy(\"room_type\").count()\ngroup.show()\n```", "```py\n    df_1 = spark.read.options(header=True, sep=',',\n                              multiLine=True, escape='\"') \\\n                    .csv('listings.csv')\n    group = df_1.groupBy(\"room_type\").count()\n    group.show()\n    ```", "```py\n    spark = .builder \\\n          .master(\"local[1]\") \\\n          .appName(\"DataIngestion_JSON\") \\\n          .config(\"spark.executor.memory\", '3g') \\\n          .config(\"spark.executor.cores\", '1') \\\n          .config(\"spark.cores.max\", '1') \\\n          .getOrCreate()\n    ```", "```py\n    df_json = spark.read.option(\"multiline\", \"true\") \\\n                        .json('github_events.json')\n    ```", "```py\n    df_json.show()\n    ```", "```py\n[\n  {\n    \"id\": \"25208138097\",\n    \"type\": \"WatchEvent\",\n    \"actor\": {...},\n    \"repo\": {...},\n    \"payload\": {\n      \"action\": \"started\"\n    },\n    \"public\": true,\n    \"created_at\": \"2022-11-13T22:52:04Z\",\n    \"org\": {...}\n  },\n  {\n    \"id\": \"25208138110\",\n    \"type\": \"IssueCommentEvent\",\n    \"actor\": {...},\n    \"repo\": {...},\n  },...\n```"]