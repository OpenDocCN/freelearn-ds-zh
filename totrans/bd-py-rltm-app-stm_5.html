<html><head></head><body>
  <div id="sbo-rt-content"><div class="chapter" title="Chapter 5. Persistence Using Redis and MongoDB"><div class="titlepage"><div><div><h1 class="title"><a id="ch05"/>Chapter 5. Persistence Using Redis and MongoDB</h1></div></div></div><p>It is often necessary to store tuples in a persistent data store, such as a NoSQL database or a fast key-value cache, in order to perform additional analysis. In this chapter, we will revisit the Twitter trending analysis topology from <a class="link" href="ch04.html" title="Chapter 4. Example Topology – Twitter">Chapter 4</a>, <span class="emphasis"><em>Example Topology – Twitter</em></span> with the help of two popular persistence media: Redis and MongoDB.</p><p>Redis (<a class="ulink" href="http://redis.io/">http://redis.io/</a>) is an <a id="id164" class="indexterm"/>open source and BSD-licensed advanced key-value cache<a id="id165" class="indexterm"/> and store. MongoDB is a cross-platform, document-oriented database (<a class="ulink" href="https://www.mongodb.org/">https://www.mongodb.org/</a>).</p><p>Here are the two problems that we will solve in this chapter:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Finding the top trending tweet topics using Redis</li><li class="listitem" style="list-style-type: disc">Computing hourly aggregates of city mentions using MongoDB</li></ul></div><div class="section" title="Finding the top n ranked topics using Redis"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec27"/>Finding the top n ranked topics using Redis</h1></div></div></div><p>The topology will <a id="id166" class="indexterm"/>compute a rolling ranking of the most popular words in the past 5 minutes. The word counts are stored in individual windows <a id="id167" class="indexterm"/>of 60 seconds in length. It consists of the<a id="id168" class="indexterm"/> following components:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Twitter stream spout (<code class="literal">twitterstream.py</code>): This reads<a id="id169" class="indexterm"/> tweets from the Twitter sample stream. This spout is unchanged from <a class="link" href="ch04.html" title="Chapter 4. Example Topology – Twitter">Chapter 4</a>, <span class="emphasis"><em>Example Topology – Twitter</em></span>.</li><li class="listitem" style="list-style-type: disc">Splitter bolt (<code class="literal">splitsentence.py</code>): This receives<a id="id170" class="indexterm"/> tweets and splits them into words. This is also identical to the one in <a class="link" href="ch04.html" title="Chapter 4. Example Topology – Twitter">Chapter 4</a>, <span class="emphasis"><em>Example Topology – Twitter</em></span>.</li><li class="listitem" style="list-style-type: disc">Rolling word count bolt (<code class="literal">rollingcount.py</code>): This receives words and counts the occurrences. The Redis<a id="id171" class="indexterm"/> keys look like <code class="literal">twitter_word_count:&lt;start time of current window in seconds&gt;</code>, and the values are stored in a hash using the following simple format:<div class="informalexample"><pre class="programlisting">{
    "word1": 5,
    "word2", 3,
}</pre></div><p>This bolt uses the Redis <code class="literal">expireat</code> command to discard old data after 5 minutes. These lines of code perform the key work:</p><div class="informalexample"><pre class="programlisting">      self.conn.zincrby(name, word)
      self.conn.expireat(name, expires)
      Total rankings bolt (totalrankings.py)</pre></div></li></ul></div><p>In this bolt, the following <a id="id172" class="indexterm"/>code does the most important<a id="id173" class="indexterm"/> work:</p><div class="informalexample"><pre class="programlisting">self.conn.zunionstore(
    'twitter_word_count',
    ['twitter_word_count:%s' % t for t in xrange(
        first_window, now_floor)])
for t in self.conn.zrevrange('twitter_word_count', 0, self.maxSize, withscores=True):
    log.info('Emitting: %s', repr(t))
    storm.emit(t)</pre></div><p>This bolt computes the top <code class="literal">maxSize</code> words across the last num_windows periods. The <code class="literal">zunionstore()</code> combines the word counts across the periods. The <code class="literal">zrevrange()</code> sorts the combined counts, returning<a id="id174" class="indexterm"/> the top <code class="literal">maxSize</code> words.</p><p>In the original Twitter example, roughly the same logic was implemented in <code class="literal">rollingcount.py</code>, <code class="literal">intermediaterankings.py</code>, and <code class="literal">totalrankings.py</code>. With Redis, we can implement the same calculations in just a few lines. The design delegates much of the work to Redis. Depending on your data volumes, this may not scale as well as the topology in the previous chapter. However, it demonstrates that Redis's capabilities go far beyond simply storing data.</p><div class="section" title="The topology configuration file – the Redis case"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec42"/>The topology configuration file – the Redis case</h2></div></div></div><p>Coming up is the topology <a id="id175" class="indexterm"/>configuration file. Depending on your Redis installation, you may need to change the value of <code class="literal">redis_url</code>.</p><p>Enter this code in <code class="literal">topology.yaml</code>:</p><div class="informalexample"><pre class="programlisting">nimbus.host: "localhost"
topology.workers: 1
oauth.consumer_key: "your-key-for-oauth-blah"
oauth.consumer_secret: "your-secret-for-oauth-blah"
oauth.access_token: "your-access-token-blah"
oauth.access_token_secret: "your-access-secret-blah"
twitter_word_count.redis_url: "redis://localhost:6379"
twitter_word_count.num_windows: 5
twitter_word_count.window_duration: 60</pre></div></div><div class="section" title="Rolling word count bolt – the Redis case"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec43"/>Rolling word count bolt – the Redis case</h2></div></div></div><p>The rolling word count bolt is similar to the word count bolt in <a class="link" href="ch03.html" title="Chapter 3. Introducing Petrel">Chapter 3</a>, <span class="emphasis"><em>Introducing Petrel</em></span>. The bolt in the earlier chapter simply accumulated the word count indefinitely. This is not good for <a id="id176" class="indexterm"/>analyzing the top words on Twitter, where the popular topics can change from one moment to the next. Rather, we want counts that reflect the latest information. As explained earlier, the rolling word count bolt stores data in time-based buckets. Then, it periodically discards buckets that exceed 5 minutes in age. Thus, the word counts from this bolt only consider the last 5 minutes of data.</p><p>Enter this code in <code class="literal">rollingcount.py</code>:</p><div class="informalexample"><pre class="programlisting">import math
import time
from collections import defaultdict

import redis

from petrel import storm
from petrel.emitter import BasicBolt

class RollingCountBolt(BasicBolt):
    def __init__(self):
        super(RollingCountBolt, self).__init__(script=__file__)

    def initialize(self, conf, context):
        self.conf = conf
        self.num_windows = self.conf['twitter_word_count.num_windows']
        self.window_duration = self.conf['twitter_word_count.window_duration']
        self.conn = redis.from_url(conf['twitter_word_count.redis_url'])

    @classmethod
    def declareOutputFields(cls):
        return ['word', 'count']

    def process(self, tup):
        word = tup.values[0]
        now = time.time()
        now_floor = int(math.floor(now / self.window_duration) * self.window_duration)
        expires = int(now_floor + self.num_windows * self.window_duration)
        name = 'twitter_word_count:%s' % now_floor
        self.conn.zincrby(name, word)
        self.conn.expireat(name, expires)

    def run():
        RollingCountBolt().run()</pre></div></div><div class="section" title="Total rankings bolt – the Redis case"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec44"/>Total rankings bolt – the Redis case</h2></div></div></div><p>Enter the following<a id="id177" class="indexterm"/> code in <code class="literal">totalrankings.py</code>:</p><div class="informalexample"><pre class="programlisting">import logging
import math
import time
import redis

from petrel import storm
from petrel.emitter import BasicBolt

log = logging.getLogger('totalrankings')

class TotalRankingsBolt(BasicBolt):
    emitFrequencyInSeconds = 15
    maxSize = 10

    def __init__(self):
        super(TotalRankingsBolt, self).__init__(script=__file__)
        self.rankedItems = {}

    def initialize(self, conf, context):
        self.conf = conf
          self.num_windows = \
            self.conf['twitter_word_count.num_windows']
        self.window_duration = \
            self.conf['twitter_word_count.window_duration']
        self.conn = redis.from_url(
            conf['twitter_word_count.redis_url'])

    def declareOutputFields(self):
        return ['word', 'count']

    def process(self, tup):
        if tup.is_tick_tuple():
            now = time.time()
            now_floor = int(math.floor(now / self.window_duration) *
                self.window_duration)
            first_window = int(now_floor - self.num_windows *
                self.window_duration)
            self.conn.zunionstore(
                'twitter_word_count',
                ['twitter_word_count:%s' % t for t in xrange(first_window, now_floor)])
            for t in self.conn.zrevrange('
                'twitter_word_count', 0,
               self.maxSize, withScores=True):
                log.info('Emitting: %s', repr(t))
                storm.emit(t)
    def getComponentConfiguration(self):
          return {"topology.tick.tuple.freq.secs":
            self.emitFrequencyInSeconds}

   def run():
       TotalRankingsBolt().run()</pre></div></div><div class="section" title="Defining the topology – the Redis case"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec45"/>Defining the topology – the Redis case</h2></div></div></div><p>Here is the <code class="literal">create.py</code> script that <a id="id178" class="indexterm"/>defines the structure of the topology:</p><div class="informalexample"><pre class="programlisting">from twitterstream import TwitterStreamSpout
from splitsentence import SplitSentenceBolt
from rollingcount import RollingCountBolt
from totalrankings import TotalRankingsBolt

def create(builder):
    spoutId = "spout"
    splitterId = "splitter"
    counterId = "counter"
    totalRankerId = "finalRanker"
    builder.setSpout(spoutId, TwitterStreamSpout(), 1)
    builder.setBolt(
        splitterId, SplitSentenceBolt(), 1).shuffleGrouping("spout")
    builder.setBolt(
        counterId, RollingCountBolt(), 4).fieldsGrouping(
            splitterId, ["word"])
    builder.setBolt(
        totalRankerId, TotalRankingsBolt()).globalGrouping(
            counterId)</pre></div></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Running the topology – the Redis case"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec28"/>Running the topology – the Redis case</h1></div></div></div><p>We have a few more small things to<a id="id179" class="indexterm"/> address before we run the topology:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Copy the <code class="literal">logconfig.ini</code> file from<a id="id180" class="indexterm"/> the second example in <a class="link" href="ch03.html" title="Chapter 3. Introducing Petrel">Chapter 3</a>, <span class="emphasis"><em>Introducing Petrel</em></span>, to this topology's directory.</li><li class="listitem">Create a file called <code class="literal">setup.sh</code>. Petrel will package this script with the topology and run it at startup. This script installs the third-party Python libraries used by the topology. The file looks like this:<div class="informalexample"><pre class="programlisting">pip install -U pip
pip install nltk==3.0.1 oauthlib==0.7.2
tweepy==3.2.0</pre></div></li><li class="listitem">Create a file called <code class="literal">manifest.txt</code> with these two lines:<div class="informalexample"><pre class="programlisting">logconfig.ini
setup.sh</pre></div></li><li class="listitem">Install the Redis server on a well-known node. All workers will store state here:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong>         sudo apt-get install redis-server</strong></span></pre></div></li><li class="listitem">Install the Python Redis client on all Storm worker machines:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong>         sudo apt-get install python-redis</strong></span></pre></div></li><li class="listitem">Before running the topology, let's review the list of files that we've created. Make sure you have created these files correctly:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">topology.yaml</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">twitterstream.py</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">splitsentence.py</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">rollingcount.py</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">totalrankings.py</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">manifest.txt</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">setup.sh</code></li></ul></div></li><li class="listitem">Run the topology with the following command:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong>petrel submit --config topology.yaml --logdir `pwd`</strong></span></pre></div></li></ol></div><p>Once the topology is running, open another terminal in the topology directory. Enter this command to see the log file for the total rankings bolt, sorted from oldest to newest:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>ls -ltr petrel*totalrankings.log</strong></span></pre></div><p>If this is the first time you are running the topology, there will be only one log file listed. A new file is created for each run. If there are several listed, choose the most recent one. Enter this command to monitor the<a id="id181" class="indexterm"/> contents of the log file (the exact filename will be different on your system):</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>tail -f petrel24748_totalrankings.log</strong></span></pre></div><p>Periodically, you will see an output like the following, listing the top 5 words in descending order of popularity:</p><p>Example output from <code class="literal">totalrankings</code>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>[2015-08-10 21:30:01,691][totalrankings][INFO]Emitting: ('love', 74.0)</strong></span>
<span class="strong"><strong>[2015-08-10 21:30:01,691][totalrankings][INFO]Emitting: ('amp', 68.0)</strong></span>
<span class="strong"><strong>[2015-08-10 21:30:01,691][totalrankings][INFO]Emitting: ('like', 67.0)</strong></span>
<span class="strong"><strong>[2015-08-10 21:30:01,692][totalrankings][INFO]Emitting: ('zaynmalik', 61.0)</strong></span>
<span class="strong"><strong>[2015-08-10 21:30:01,692][totalrankings][INFO]Emitting: ('mtvhottest', 61.0)</strong></span>
<span class="strong"><strong>[2015-08-10 21:30:01,692][totalrankings][INFO]Emitting: ('get', 58.0)</strong></span>
<span class="strong"><strong>[2015-08-10 21:30:01,692][totalrankings][INFO]Emitting: ('one', 49.0)</strong></span>
<span class="strong"><strong>[2015-08-10 21:30:01,692][totalrankings][INFO]Emitting: ('follow', 46.0)</strong></span>
<span class="strong"><strong>[2015-08-10 21:30:01,692][totalrankings][INFO]Emitting: ('u', 44.0)</strong></span>
<span class="strong"><strong>[2015-08-10 21:30:01,692][totalrankings][INFO]Emitting: ('new', 38.0)</strong></span>
<span class="strong"><strong>[2015-08-10 21:30:01,692][totalrankings][INFO]Emitting: ('much', 37.0)</strong></span></pre></div><div class="section" title="Finding the hourly count of tweets by city name using MongoDB"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec46"/>Finding the hourly count of tweets by city name using MongoDB</h2></div></div></div><p>MongoDB is a<a id="id182" class="indexterm"/> popular database for storing large <a id="id183" class="indexterm"/>amounts of data. It is designed for easy scalability across many nodes.</p><p>To run this topology, you first need to install MongoDB and configure some database-specific settings. This example uses a MongoDB database called <code class="literal">cities</code> with a collection named <code class="literal">minute</code>. In order to compute the counts by city and minute, we must create a unique index on the <code class="literal">cities.minute</code> collection. To do this, launch the MongoDB command-line client:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>mongo</strong></span></pre></div><p>Create a unique index on the <code class="literal">cities.minute</code> collection:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>use cities</strong></span>
<span class="strong"><strong>db.minute.createIndex( { minute: 1, city: 1 }, { unique: true } )</strong></span></pre></div><p>This index stores a per minute time series of city counts in MongoDB. After running the example topology to capture some data, we'll run a standalone command-line script (<code class="literal">city_report.py</code>) to sum the <a id="id184" class="indexterm"/>per minute city counts by hour and city.</p><p>This is a variant of the earlier<a id="id185" class="indexterm"/> Twitter topology. This example uses the Python geotext library (<a class="ulink" href="https://pypi.python.org/pypi/geotext">https://pypi.python.org/pypi/geotext</a>) to find city names in tweets.</p><p>Here is an overview of the topology:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Read the tweets.</li><li class="listitem" style="list-style-type: disc">Split them into words and find city names.</li><li class="listitem" style="list-style-type: disc">In MongoDB, count the number of times a city is mentioned each minute.</li><li class="listitem" style="list-style-type: disc">Twitter stream spout (<code class="literal">twitterstream.py</code>): This reads tweets from the Twitter sample stream.</li><li class="listitem" style="list-style-type: disc">City count bolt (<code class="literal">citycount.py</code>): This finds city names and writes to MongoDB. It is similar to the <code class="literal">SplitSentenceBolt</code> from the Twitter sample, but after splitting by words, it looks for city names.</li></ul></div><p>The <code class="literal">_get_words()</code> function <a id="id186" class="indexterm"/>here is slightly different from earlier examples. This is because geotext does not recognize lowercase strings as city names.</p><p>It creates or updates MongoDB records, taking advantage of the unique index on minute and city to<a id="id187" class="indexterm"/> accumulate the per minute counts.</p><p>This is a common pattern for representing time series data in MongoDB. Each record also includes an <code class="literal">hour</code> field. The <code class="literal">city_report.py</code> script uses this to compute the hourly counts.</p><p>Enter this code in <code class="literal">citycount.py</code>:</p><div class="informalexample"><pre class="programlisting">Import datetime
import logging
import geotext
import nltk.corpus
import pymongo

from petrel import storm
from petrel.emitter import BasicBolt

log = logging.getLogger('citycount')

class CityCountBolt(BasicBolt):
    def __init__(self):
        super(CityCountBolt, self).__init__(script=__file__)
        self.stop_words = set(nltk.corpus.stopwords.words('english'))
        self.stop_words.update(['http', 'https', 'rt'])
        self.stop_cities = set([
            'bay', 'best', 'deal', 'man', 'metro', 'of', 'un'])

    def initialize(self, conf, context):
        self.db = pymongo.MongoClient()

    def declareOutputFields(self):
        return []

    def process(self, tup):
        clean_text = ' '.join(w for w in self._get_words(tup.values[0]))
        places = geotext.GeoText(clean_text)
        now_minute = self._get_minute()
        now_hour = now_minute.replace(minute=0)
        for city in places.cities:
            city = city.lower()
            if city in self.stop_cities:
                continue
            log.info('Updating count: %s, %s, %s', now_hour, now_minute, city)
            self.db.cities.minute.update(
                {
                    'hour': now_hour,
                    'minute': now_minute,
                    'city': city
                },
                {'$inc': { 'count' : 1 } },
                upsert=True)

    @staticmethod
    def _get_minute():
        return datetime.datetime.now().replace(second=0, microsecond=0)

    def _get_words(self, sentence):
        for w in nltk.word_tokenize(sentence):
            wl = w.lower()
            if wl.isalpha() and wl not in self.stop_words:
                yield w

def run():
    CityCountBolt().run()</pre></div></div><div class="section" title="Defining the topology – the MongoDB case"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec47"/>Defining the topology – the MongoDB case</h2></div></div></div><p>Enter the following <a id="id188" class="indexterm"/>code in <code class="literal">create.py</code>:</p><div class="informalexample"><pre class="programlisting">from twitterstream import TwitterStreamSpout
from citycount import CityCountBolt

def create(builder):
    spoutId = "spout"
    cityCountId = "citycount"
    builder.setSpout(spoutId, TwitterStreamSpout(), 1)
    builder.setBolt(cityCountId, CityCountBolt(), 1).shuffleGrouping("spout")</pre></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Running the topology – the MongoDB case"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec29"/>Running the topology – the MongoDB case</h1></div></div></div><p>We have a few more<a id="id189" class="indexterm"/> small things to address before we run the topology:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Copy the <code class="literal">logconfig.ini</code> file from the second example in <a class="link" href="ch03.html" title="Chapter 3. Introducing Petrel">Chapter 3</a>, <span class="emphasis"><em>Introducing Petrel</em></span> to this topology's directory.</li><li class="listitem">Create a file called <code class="literal">setup.sh</code>:<div class="informalexample"><pre class="programlisting">pip install -U pip
pip install nltk==3.0.1 oauthlib==0.7.2 tweepy==3.2.0 geotext==0.1.0 pymongo==3.0.3</pre></div></li><li class="listitem">Next, create a file called <code class="literal">manifest.txt</code>. This is identical to the Redis example.<p>Install the<a id="id190" class="indexterm"/> MongoDB server. On Ubuntu, you can use the instructions given at <a class="ulink" href="http://docs.mongodb.org/manual/tutorial/install-mongodb-on-ubuntu/">http://docs.mongodb.org/manual/tutorial/install-mongodb-on-ubuntu/</a>.</p></li><li class="listitem">Install the Python MongoDB client on all Storm worker machines:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong>pip install pymongo==3.0.3</strong></span></pre></div></li><li class="listitem">To verify that <code class="literal">pymongo</code> is installed and the index is created correctly, start an interactive Python session by running <code class="literal">python</code>. Then use this code:<div class="informalexample"><pre class="programlisting">import pymongo
from pymongo import MongoClient
db = MongoClient()
for index in db.cities.minute.list_indexes():
    print index</pre></div><p>You should see the following output. The second line is the index that we added:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>SON([(u'v', 1), (u'key', SON([(u'_id', 1)])), (u'name', u'_id_'), (u'ns', u'cities.minute')])</strong></span>
<span class="strong"><strong>SON([(u'v', 1), (u'unique', True), (u'key', SON([(u'minute', 1.0), (u'city', 1.0)])), (u'name', u'minute_1_city_1'), (u'ns', u'cities.minute')])</strong></span></pre></div></li><li class="listitem">Next, install <code class="literal">geotext</code>:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong>pip install geotext==0.1.0</strong></span></pre></div></li><li class="listitem">Before running <a id="id191" class="indexterm"/>the topology, let's review the list of files that we created. Make sure you have created these files correctly:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">topology.yaml</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">twitterstream.py</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">citycount.py</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">manifest.txt</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">setup.sh</code></li></ul></div></li><li class="listitem">Run the topology with the following command:<div class="informalexample"><pre class="programlisting"><span class="strong"><strong>petrel submit --config topology.yaml --logdir `pwd`</strong></span></pre></div></li></ol></div><p>The <code class="literal">city_report.py</code> file is a standalone script that generates a simple hourly report from the data inserted by the topology. This script uses MongoDB aggregation to compute the hourly totals. As noted earlier, the report depends on the presence of an <code class="literal">hour</code> field.</p><p>Enter this code in <code class="literal">city_report.py</code>:</p><div class="informalexample"><pre class="programlisting">import pymongo

def main():
    db = pymongo.MongoClient()
    pipeline = [{
        '$group': { 
          '_id':   { 'hour': '$hour', 'city': '$city' },
          'count': { '$sum': '$count' } 
        } 
      }]
    for r in db.cities.command('aggregate', 'minute', pipeline=pipeline)['result']:
        print '%s,%s,%s' % (r['_id']['city'], r['_id']['hour'], r['count'])

if __name__ == '__main__':
    main()</pre></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec30"/>Summary</h1></div></div></div><p>In this chapter, we saw how to use two popular NoSQL storage engines (Redis and MongoDB) with Storm. We also showed you how to create data in a topology and access it from other applications, demonstrating that Storm can be an effective part of an ETL pipeline.</p></div></div>
</body></html>