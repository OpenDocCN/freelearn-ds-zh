["```py\n    mkdir /tmp/assassin/dataset\n    tar –xvf  /tmp/assassin/ 20021010_easy_ham.tar.bz2 \n    tar –xvf /tmp/assassin/ 20021010_spam.tar.bz2\n\n    ```", "```py\n    hadoop fs  -mkdir /user/hue/assassin/\n    hadoop fs –put /tmp/assassin/dataset  /user/hue/assassin \n    tar –xvf /tmp/assassin/ 20021010_spam.tar.bz2\n\n    ```", "```py\n    bin/mahout seqdirectory –i /user/hue/assassin/dataset –o /user/hue/assassinseq-out\n\n    ```", "```py\n    bin/mahout seq2sparse -i /user/hue/assassinseq-out/part-m-00000 -o /user/hue/assassinvec -lnorm -nv -wt tfidf\n\n    ```", "```py\n    bin/mahout split -i /user/hue/assassinvec/tfidf-vectors --trainingOutput /user/hue/assassindatatrain --testOutput /user/hue/assassindatatest --randomSelectionPct 20 --overwrite --sequenceFiles -xm sequential\n\n    ```", "```py\n    bin/mahout trainnb -i /user/hue/assassindatatrain -el -o /user/hue/prodmodel -li /user/hue/prodlabelindex -ow -c\n\n    ```", "```py\n    bin/mahout testnb -i /user/hue/assassindatatest -m /user/hue/prodmodel/ -l  /user/hue/prodlabelindex -ow -o /user/hue/prodresults\n\n    ```", "```py\n    public static Map<String, Integer> readDictionary(Configuration conf, Path dictionaryPath) {\n      Map<String, Integer> dictionary = new HashMap<String, Integer>();\n      for (Pair<Text, IntWritable> pair : new SequenceFileIterable<Text, IntWritable>(dictionaryPath, true, conf)) {\n        dictionary.put(pair.getFirst().toString(), pair.getSecond().get());\n      }\n      return dictionary;\n    }\n    ```", "```py\n    public static Map<Integer, Long> readDocumentFrequency(Configuration conf, Path documentFrequencyPath) {\n      Map<Integer, Long> documentFrequency = new HashMap<Integer, Long>();\n      for (Pair<IntWritable, LongWritable> pair : new SequenceFileIterable<IntWritable, LongWritable>(documentFrequencyPath, true, conf)) {\n        documentFrequency.put(pair.getFirst().get(), pair.getSecond().get());\n      }\n      return documentFrequency;\n    }\n    ```", "```py\n        public static void main(String[] args) throws Exception {\n          if (args.length < 5) {\n            System.out.println(\"Arguments: [model] [labelindex] [dictionary] [documentfrequency] [new file] \");\n            return;\n          }\n          String modelPath = args[0];\n          String labelIndexPath = args[1];\n          String dictionaryPath = args[2];\n          String documentFrequencyPath = args[3];\n          String newDataPath = args[4];\n          Configuration configuration = new Configuration(); // model is a matrix (wordId, labelId) => probability score\n          NaiveBayesModel model = NaiveBayesModel.materialize(new Path(modelPath), configuration); \n          StandardNaiveBayesClassifier classifier = new StandardNaiveBayesClassifier(model); \n          // labels is a map label => classId\n          Map<Integer, String> labels = BayesUtils.readLabelIndex(configuration, new Path(labelIndexPath));\n          Map<String, Integer> dictionary = readDictionary(configuration, new Path(dictionaryPath));\n          Map<Integer, Long> documentFrequency = readDocumentFrequency(configuration, new Path(documentFrequencyPath));\n        ```", "```py\n    Analyzer analyzer = new StandardAnalyzer(Version.LUCENE_CURRENT);\n\n    int labelCount = labels.size();\n    int documentCount = documentFrequency.get(-1).intValue();\n\n    System.out.println(\"Number of labels: \" + labelCount);\n    System.out.println(\"Number of documents in training set: \" + documentCount);\n    BufferedReader reader = new BufferedReader(new FileReader(newDataPath));\n    while(true) {\n      String line = reader.readLine();\n      if (line == null) {\n        break;\n      }\n\n      ConcurrentHashMultiset<Object> words = ConcurrentHashMultiset.create(); \n      // extract words from mail\n      TokenStream ts = analyzer.tokenStream(\"text\", new StringReader(line));\n      CharTermAttribute termAtt = ts.addAttribute(CharTermAttribute.class);\n      ts.reset();\n      int wordCount = 0;\n      while (ts.incrementToken()) {\n        if (termAtt.length() > 0) {\n          String word = ts.getAttribute(CharTermAttribute.class).toString();\n          Integer wordId = dictionary.get(word);\n          // if the word is not in the dictionary, skip it\n          if (wordId != null) {\n            words.add(word);\n            wordCount++;\n          }\n        }\n      }\n      ts.close();\n    ```", "```py\n    Vector vector = new RandomAccessSparseVector(10000);\n    TFIDF tfidf = new TFIDF();\n    for (Multiset.Entry entry:words.entrySet()) {\n      String word =  (String)entry.getElement();\n      int count = entry.getCount();\n      Integer wordId = dictionary.get(word);\n      Long freq = documentFrequency.get(wordId);\n      double tfIdfValue = tfidf.calculate(count, freq.intValue(), wordCount, documentCount);\n      vector.setQuick(wordId, tfIdfValue);\n    }\n    ```", "```py\n      Vector resultVector = classifier.classifyFull(vector);\n        double bestScore = -Double.MAX_VALUE;\n        int bestCategoryId = -1;          \n        for(int i=0 ;i<resultVector.size();i++) {\n          Element e1  = resultVector.getElement(i);\n          int categoryId = e1.index();\n          double score = e1.get();\n          if (score > bestScore) {\n            bestScore = score;\n            bestCategoryId = categoryId;\n          }\n          System.out.print(\"  \" + labels.get(categoryId) + \": \" + score);\n        }\n        System.out.println(\" => \" + labels.get(bestCategoryId));\n      }\n    }\n    ```", "```py\n    mkdir /tmp/assassinmodeltest\n\n    ```", "```py\n        hadoop fs –get /user/hue/prodmodel /tmp/assassinmodeltest\n\n        ```", "```py\n        hadoop fs –get /user/hue/prodlabelindex  /tmp/assassinmodeltest\n\n        ```", "```py\n        hadoop fs –get /user/hue/assassinvec/df-count  /tmp/assassinmodeltest\n        dictionary.file-0 from the same assassinvec folder\n        hadoop fs –get /user/hue/assassinvec/dictionary.file-0  /tmp/assassinmodeltest\n\n        ```", "```py\n    Java –cp /tmp/assassinmodeltest/spamclassifier.jar:/usr/lib/mahout/* com.packt.spamfilter.TestClassifier /tmp/assassinmodeltest /tmp/assassinmodeltest/prodlabelindex /tmp/assassinmodeltest/dictionary.file-0 /tmp/assassinmodeltest/df-count /tmp/assassinmodeltest/testemail\n\n    ```", "```py\n    mkdir /tmp/asfmail\n    tar –xvf  ibm.tar\n\n    ```", "```py\n    hadoop fs -put /tmp/asfmail/ibm/content /user/hue/asfmail\n\n    ```", "```py\n    mahout  org.apache.mahout.text.SequenceFilesFromMailArchives --charset \"UTF-8\" --body --subject --input /user/hue/asfmail/content --output /user/hue/asfmailout\n\n    ```", "```py\n    mahout  seq2sparse --input /user/hue/asfmailout --output /user/hue/asfmailseqsp --norm 2 --weight TFIDF --namedVector --maxDFPercent 90 --minSupport 2 --analyzerName org.apache.mahout.text.MailArchivesClusteringAnalyzer\n\n    ```", "```py\n    mahout  org.apache.mahout.classifier.email.PrepEmailDriver --input /user/hue/asfmailseqsp --output /user/hue/asfmailseqsplabel --maxItemsPerLabel 1000\n\n    ```", "```py\n    mahout  split --input /user/hue/asfmailseqsplabel --trainingOutput /user/hue/asfmailtrain --testOutput /user/hue/asfmailtest  --randomSelectionPct 20 --overwrite --sequenceFiles\n\n    ```", "```py\n    mahout trainnb -i /user/hue/asfmailtrain -o /user/hue/asfmailmodel -extractLabels --labelIndex /user/hue/asfmaillabels\n\n    ```", "```py\n    mahout testnb -i /user/hue/asfmailtest -m /user/hue/asfmailmodel --labelIndex /user/hue/asfmaillabels\n\n    ```"]