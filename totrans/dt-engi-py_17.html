<html><head></head><body>
		<div id="_idContainer220">
			<h1 id="_idParaDest-151"><em class="italic"><a id="_idTextAnchor153"/>Chapter 14</em>: Data Processing with Apache Spark</h1>
			<p>In the previous chapter, you learned how to add streaming data to your data pipelines. Using Python or Apache NiFi, you can extract, transform, and load streaming data. However, to perform transformations on large amounts of streaming data, data engineers turn to tools such as Apache Spark. Apache Spark is faster than most other methods – such as MapReduce on non-trivial transformations – and it allows distributed data processing.</p>
			<p>In this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li>Installing and running Spark</li>
				<li>Installing and configuring PySpark</li>
				<li>Processing data with PySpark</li>
			</ul>
			<h1 id="_idParaDest-152"><a id="_idTextAnchor154"/>Installing and running Spark</h1>
			<p>Apache Spark is a <a id="_idIndexMarker715"/>distributed data processing engine that can handle both streams and batch data, and even graphs. It has a core set of components and other libraries that are used to add functionality. A common depiction of the Spark ecosystem is shown in the following diagram:</p>
			<div>
				<div id="_idContainer211" class="IMG---Figure">
					<img src="image/Figure_14.1_B15739.jpg" alt="Figure 14.1 – The Apache Spark ecosystem&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.1 – The Apache Spark ecosystem</p>
			<p>To run Spark<a id="_idIndexMarker716"/> as a cluster, you have several options. Spark can run in a standalone mode, which uses a simple cluster manager provided by Spark. It can also run on an Amazon EC2 instance, using YARN, Mesos, or Kubernetes. In a production environment with a significant workload, you would probably not want to run in standalone mode; however, this is how we will stand up our cluster in this chapter. The principles will be the same, but the standalone cluster provides the fastest way to get you up and running without needing to dive into more complicated infrastructure. </p>
			<p>To install Apache Spark, take <a id="_idIndexMarker717"/>the following steps:</p>
			<ol>
				<li>Browse to the website at <a href="http://spark.apache.org">http://spark.apache.org</a>. From there, you can keep up to date with new versions of Apache Spark, read the documentation, learn about the libraries, and find code examples:<div id="_idContainer212" class="IMG---Figure"><img src="image/Figure_14.2_B15739.jpg" alt="Figure 14.2 – The Apache Spark website&#13;&#10;"/></div><p class="figure-caption">Figure 14.2 – The Apache Spark website</p></li>
				<li>From the website, select<a id="_idIndexMarker718"/> the <strong class="bold">Download</strong> menu option. Choose the version of Spark you want to use – at the time of writing, the newest version is 3.0.0. You will be asked to choose a package type. We will not be using Hadoop, but have to select a version or provide our own. I have chosen <strong class="bold">Pre-built for Apache Hadoop 2.7</strong>. On Windows, you may need to trick the operating system into thinking Hadoop is installed by setting environment variables, but on Linux and macOS, this should not be an issue. The download options are shown in the following screenshot:<div id="_idContainer213" class="IMG---Figure"><img src="image/Figure_14.3_B15739.jpg" alt="Figure 14.3 – Downloading Apache Spark for Hadoop 2.7&#13;&#10;"/></div><p class="figure-caption">Figure 14.3 – Downloading Apache Spark for Hadoop 2.7</p></li>
				<li>After downloading <a id="_idIndexMarker719"/>the file, you will extract it, then move it to the home directory in a directory named <strong class="source-inline">spark3</strong>. You can do this using the following commands:<p class="source-code"><strong class="bold">tar -xvzf spark-3.0.0-bin-hadoop2.7.tgz</strong></p><p class="source-code"><strong class="bold">mv spark-3.0.0-bin-hadoop2.7 ~/spark3</strong></p></li>
				<li>Next, you will need to make a cluster. As you did with Kafka, you will make a copy of the Spark directory on the same machine and make it act as another node. If you have another machine, you could also put another copy of Spark on that server. Copy the directory and rename it <strong class="source-inline">spark-node</strong>, as shown:<p class="source-code"><strong class="bold">cp -r spark3/ spark-node</strong></p></li>
				<li>To run the Spark cluster, you can use the provided scripts. The scripts to run the cluster use outdated terminology – <strong class="source-inline">master</strong> and <strong class="source-inline">slave</strong>. This language is common in the technology space; however, there have been many voices opposed to it for a long time. Finally, it appears that there is some traction being made in removing this language as GitHub will remove <strong class="source-inline">master</strong> from the branch names. I too have renamed the scripts, using the terms <strong class="source-inline">head</strong> and <strong class="source-inline">node</strong>. To do this, use the following commands:<p class="source-code"><strong class="bold">cd ~/spark3/sbin</strong></p><p class="source-code"><strong class="bold">cp start-master.sh start-head.sh</strong></p><p class="source-code"><strong class="bold">cd ~/spark-node/sbin</strong></p><p class="source-code"><strong class="bold">cp start-slave.sh start-node.sh</strong></p></li>
				<li>To start the cluster, you can now run the scripts as shown:<p class="source-code"><strong class="bold">./start-head.sh</strong></p><p class="source-code"><strong class="bold">./start-node.sh spark://pop-os.localdomain:7077 -p 9911</strong></p></li>
				<li>You can pass <a id="_idIndexMarker720"/>parameters to the scripts, and in the preceding command, you pass the port flag (<strong class="source-inline">-p</strong>) to tell the script which port you want the node to run on. You can also pass the following:<p>a) <strong class="source-inline">-h, --host</strong>: The hostname to run on. The <strong class="source-inline">i, -ip</strong> flag has been deprecated.</p><p>b) <strong class="source-inline">-p, --port</strong>: The port to listen on.</p><p>c) <strong class="source-inline">--webui-port</strong>: The port for the web GUI, which defaults to <strong class="source-inline">8080</strong>.</p><p>d) <strong class="source-inline">-c, --cores</strong>: The number of cores to use.</p><p>e) <strong class="source-inline">-m, --memory</strong>: The amount of memory to use. By default, it is 1 gigabyte less than your full memory.</p><p>f) <strong class="source-inline">-d, --work-dir</strong>: The scratch space directory for the worker only.</p><p>g) <strong class="source-inline">--properties-file</strong>: This is where you can specify several of these flags in a <strong class="source-inline">spark.conf</strong> file.</p></li>
			</ol>
			<p>The cluster will take a minute to load, and when it has finished, you can browse to the web UI at <a href="http://localhost:8080/">http://localhost:8080/</a>. You will see the details of your cluster and it will look as in the following screenshot:</p>
			<div>
				<div id="_idContainer214" class="IMG---Figure">
					<img src="image/Figure_14.4_B15739.jpg" alt="Figure 14.4 – Spark cluster web UI&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.4 – Spark cluster web UI</p>
			<p>With the cluster up and <a id="_idIndexMarker721"/>running, you will need to set up the Python environment so that you can code against it. The next section will walk you through those steps.</p>
			<h1 id="_idParaDest-153"><a id="_idTextAnchor155"/>Installing and configuring PySpark</h1>
			<p>PySpark is <a id="_idIndexMarker722"/>installed with Spark. You can see it in the <strong class="source-inline">~/spark3/bin</strong> directory, as well as other libraries and tools. To configure PySpark to run, you need to export environment variables. The variables are shown here:</p>
			<p class="source-code">export SPARK_HOME=/home/paulcrickard/spark3</p>
			<p class="source-code">export PATH=$SPARK_HOME/bin:$PATH</p>
			<p class="source-code">export PYSPARK_PYTHON=python3 </p>
			<p>The preceding command set the <strong class="source-inline">SPARK_HOME</strong> variable. This will be where you installed Spark. I have pointed the variable to the head of the Spark cluster because the node would really be on another machine. Then, it adds <strong class="source-inline">SPARK_HOME</strong> to your path. This means that when you type a command, the operating system will look for it in the directories specified in your path, so now it will search <strong class="source-inline">~/spark3/bin</strong>, which is where PySpark lives. </p>
			<p>Running the<a id="_idIndexMarker723"/> preceding commands in a terminal will allow Spark to run while the<a id="_idIndexMarker724"/> terminal is open. You will have to rerun these commands every time. To make them permanent, you can add the commands to your <strong class="source-inline">~/.bashrc</strong> file. After saving the <strong class="source-inline">.bashrc</strong> file, you need to reload it. You can do that by running the following command:</p>
			<p class="source-code">source ~/.bashrc</p>
			<p>You should now be able to open a terminal and run PySpark, and the result will be the PySpark interactive shell, as shown:</p>
			<div>
				<div id="_idContainer215" class="IMG---Figure">
					<img src="image/Figure_14.5_B15739.jpg" alt="Figure 14.5 – The interactive Spark shell&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.5 – The interactive Spark shell</p>
			<p>If you see the preceding screenshot, congratulations, you have PySpark configured. In this chapter, the examples will use PySpark in Jupyter notebooks. There are two ways to configure PySpark to work with Jupyter:</p>
			<ul>
				<li><strong class="bold">Set the drivers</strong>: To set PySpark to run in Jupyter, you can set the <strong class="source-inline">PYSPARK_DRIVER_PYTHON</strong> environment variable and the <strong class="source-inline">_OPTS</strong> variable to the Jupyter Notebook using the following commands (add this to <strong class="source-inline">~/.bashrc</strong> if you want it to be permanent):<p class="source-code"><strong class="bold">export PYSPARK_DRIVER_PYTHON=jupyter</strong></p><p class="source-code"><strong class="bold">export PYSPARK_DRIVER_PYTHON_OPTS='notebook'</strong></p></li>
				<li><strong class="bold">Find Spark</strong>: You can also use the <strong class="source-inline">findspark</strong> library and add code to your Jupyter notebook that gets the Spark information as it runs. The examples in this chapter will use this method. You can install <strong class="source-inline">findspark</strong> using <strong class="source-inline">pip</strong>, as shown:<p class="source-code"><strong class="bold">pip3 install findspark</strong></p><p>To test whether PySpark runs in a Jupyter notebook, start the notebook server using the following command:</p><p class="source-code"><strong class="bold">jupyter notebook</strong></p><p>The notebook server will open your browser. From there, you can create a new Python 3 <a id="_idIndexMarker725"/>notebook. To use the <strong class="source-inline">findspark</strong> method, add the following two lines to your <a id="_idIndexMarker726"/>notebook and run it:</p><p class="source-code"><strong class="bold">import findspark</strong></p><p class="source-code"><strong class="bold">findspark.init()</strong></p><p>If the preceding lines ran without error, then the code was able to find Spark. </p></li>
			</ul>
			<p>You can now run PySpark code on your Spark cluster. The next section will walk you through some basic PySpark examples.</p>
			<h1 id="_idParaDest-154"><a id="_idTextAnchor156"/>Processing data with PySpark</h1>
			<p>Before<a id="_idIndexMarker727"/> processing data with PySpark, let's run one of the samples <a id="_idIndexMarker728"/>to show how Spark works. Then, we will skip the boilerplate in later examples and focus on data processing. The Jupyter notebook for the <strong class="bold">Pi Estimation</strong> example from the Spark website at <a href="http://spark.apache.org/examples.html">http://spark.apache.org/examples.html</a> is shown in the following screenshot:</p>
			<div>
				<div id="_idContainer216" class="IMG---Figure">
					<img src="image/Figure_14.6_B15739.jpg" alt="Figure 14.6 – The Pi Estimation example in a Jupyter notebook&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.6 – The Pi Estimation example in a Jupyter notebook</p>
			<p>The example from the <a id="_idIndexMarker729"/>website will not run without some <a id="_idIndexMarker730"/>modifications. In the following points, I will walk through the cells:</p>
			<ol>
				<li value="1">The first cell imports <strong class="source-inline">findspark</strong> and runs the <strong class="source-inline">init()</strong> method. This was explained in the preceding section as the preferred method to include PySpark in Jupyter notebooks. The code is as follows:<p class="source-code">import findspark</p><p class="source-code">findspark.init()</p></li>
				<li>The next cell imports the <strong class="source-inline">pyspark</strong> library and <strong class="source-inline">SparkSession</strong>. It then creates the session by passing the head node of the Spark cluster. You can get the URL from the Spark web UI – you also used it to start the worker node:<p class="source-code">import pyspark</p><p class="source-code">from pyspark.sql import SparkSession</p><p class="source-code">spark=SparkSession.builder.master('spark://pop-os.localdomain:7077').appName('Pi-Estimation').getOrCreate()</p></li>
				<li>Running the first two cells, you can browse to the Spark GUI and see that there is a task running. The running task is shown in the following screenshot – notice the name of the worker is <strong class="source-inline">Pi-Estimation</strong>, which is the <strong class="source-inline">appName</strong> parameter in the preceding code:<div id="_idContainer217" class="IMG---Figure"><img src="image/Figure_14.7_B15739.jpg" alt="Figure 14.7 – The Spark web UI with a running session and two completed sessions&#13;&#10;"/></div><p class="figure-caption">Figure 14.7 – The Spark web UI with a running session and two completed sessions</p><p>The preceding<a id="_idIndexMarker731"/> code will be used in all of your Spark code. It is the boilerplate code. </p></li>
				<li>The next cell<a id="_idIndexMarker732"/> contains the work. The code that follows will estimate the value of pi. The details of the code are not important, but notice that the <strong class="source-inline">count</strong> variable uses <strong class="source-inline">sparkContext</strong> and parallelizes a task on the cluster. After the boilerplate, your Spark code will execute a task and get the results:<p class="source-code">import random</p><p class="source-code">NUM_SAMPLES=1</p><p class="source-code">def inside(p):</p><p class="source-code">    x, y = random.random(), random.random()</p><p class="source-code">    return x*x + y*y &lt; 1</p><p class="source-code">count = spark.sparkContext.parallelize(range(0,</p><p class="source-code">                     NUM_SAMPLES)).filter(inside).count()</p><p class="source-code">print('Pi is roughly {}'.format(4.0 * count / </p><p class="source-code">                                NUM_SAMPLES))</p></li>
				<li>Lastly, stop the session:<p class="source-code">spark.stop()</p></li>
			</ol>
			<p>Once the session <a id="_idIndexMarker733"/>has been stopped, it will show up as a completed<a id="_idIndexMarker734"/> application in the web UI. The next section will use Spark with DataFrames and send the data to Kafka.</p>
			<h2 id="_idParaDest-155"><a id="_idTextAnchor157"/>Spark for data engineering</h2>
			<p>The previous section<a id="_idIndexMarker735"/> showed the structure of a Spark application: we used <strong class="source-inline">findspark</strong> to get the paths, imported the libraries, created a session, did something, and stopped the session. When you do something, it will most likely involve a Spark DataFrame. This section will provide a brief overview of how Spark DataFrames work – it is slightly different than <strong class="source-inline">pandas</strong>.</p>
			<p>The first thing you have to do is use <strong class="source-inline">findspark</strong> to set up the environment. Then, you can import the required libraries. Then, create the session. The following code shows the boilerplate to get set up:</p>
			<p class="source-code">import findspark</p>
			<p class="source-code">findspark.init()</p>
			<p class="source-code">import pyspark</p>
			<p class="source-code">from pyspark.sql import SparkSession</p>
			<p class="source-code">import os</p>
			<p class="source-code">os.chdir('/home/paulcrickard')</p>
			<p class="source-code">spark=SparkSession.builder.master('spark://pop-os.localdomain:7077').appName('DataFrame-Kafka').getOrCreate()</p>
			<p>Now you are connected to a session on the Spark cluster. You can read CSV and JSON data just like you did with DataFrames in <a href="B15739_03_ePub_AM.xhtml#_idTextAnchor039"><em class="italic">Chapter 3</em></a><em class="italic">, Reading and Writing Files</em> and <a href="B15739_04_ePub_AM.xhtml#_idTextAnchor049"><em class="italic">Chapter 4</em></a><em class="italic">, Working with Databases</em>, with some slight modifications. When you read in the data, you <a id="_idIndexMarker736"/>can use <strong class="source-inline">read.csv</strong> instead of <strong class="source-inline">read_csv</strong> in <strong class="source-inline">pandas</strong>. Another difference between Spark and <strong class="source-inline">pandas</strong> is the use of <strong class="source-inline">.show()</strong> in Spark to see the DataFrame. In <strong class="source-inline">pandas</strong>, you can view <strong class="source-inline">dtypes</strong> of a DataFrame, and in Spark, you can do the same using <strong class="source-inline">printSchema()</strong>. The following code reads the <strong class="source-inline">data.csv</strong> file and prints the top five rows and the schema:</p>
			<p class="source-code">df = spark.read.csv('data.csv')</p>
			<p class="source-code">df.show(5)</p>
			<p class="source-code">df.printSchema()</p>
			<p>The output will be a DataFrame like the one shown in the following screenshot:</p>
			<div>
				<div id="_idContainer218" class="IMG---Figure">
					<img src="image/Figure_14.8_B15739.jpg" alt="Figure 14.8 – DataFrame from CSV with schema&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.8 – DataFrame from CSV with schema</p>
			<p>You will notice that<a id="_idIndexMarker737"/> the headers are the first row and there are default <strong class="source-inline">_c0</strong> column names. The output also shows that all the columns are strings. You can specify a schema and pass it as a parameter; however, you can also tell Spark to infer the schema. The following code passes that there are headers and tells Spark to infer the schema:</p>
			<p class="source-code">df = spark.read.csv('data.csv',header=True,inferSchema=True)</p>
			<p class="source-code">df.show(5)</p>
			<p>The results are what you expected: a DataFrame with the correct types. The following screenshot shows the results:</p>
			<div>
				<div id="_idContainer219" class="IMG---Figure">
					<img src="image/Figure_14.9_B15739.jpg" alt="Figure 14.9 – DataFrame with headers and correct types&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.9 – DataFrame with headers and correct types</p>
			<p>You can select a column by using <strong class="source-inline">select()</strong> and passing the column name as a parameter. Don't forget to add <strong class="source-inline">.show()</strong> or it will return a DataFrame and not display it:</p>
			<p class="source-code">df.select('name').show()</p>
			<p>You will notice that in <strong class="source-inline">pandas</strong>, you would have used <strong class="source-inline">[]</strong> and a column name and also did not need the <strong class="source-inline">select</strong> method. In <strong class="source-inline">pandas</strong>, you could also filter a DataFrame by using the <strong class="source-inline">df[(df['field']&lt; value)]</strong> format. In Spark, you can use <strong class="source-inline">select</strong> and <strong class="source-inline">filter</strong> to do the same, the difference being that a <strong class="source-inline">select</strong> method returns <strong class="source-inline">True</strong> and <strong class="source-inline">False</strong> for a condition, and <strong class="source-inline">filter</strong> will return the DataFrame for that condition. With <strong class="source-inline">filter</strong>, you can also add a <strong class="source-inline">select</strong> method and pass an array of columns to return. The code is shown as follows:</p>
			<p class="source-code">df.select(df['age']&lt;40).show()</p>
			<p class="source-code">df.filter(df['age']&lt;40).show()</p>
			<p class="source-code">df.filter('age&lt;40').select(['name','age','state']).show()</p>
			<p>Notice that in the last<a id="_idIndexMarker738"/> line, you didn't use <strong class="source-inline">df['age']</strong> but were able to just pass the column name. When you want to iterate through a DataFrame, you could use <strong class="source-inline">iterrows</strong> in <strong class="source-inline">pandas</strong>. In Spark, you create an array of rows using <strong class="source-inline">collect()</strong>. The following code will use a <strong class="source-inline">filter</strong> method to get all people under 40 and print the array:</p>
			<p class="source-code">u40=df.filter('age&lt;40').collect()</p>
			<p class="source-code">u40</p>
			<p>To get a single row, you can just pass the index. You can convert the row into different formats, and in this example, I converted it into a dictionary. As a dictionary, you can select any value by specifying the key. The code is shown as follows:</p>
			<p class="source-code">u40[0]</p>
			<p class="source-code">u40[0].asDict()</p>
			<p class="source-code">u40[0].asDict()['name']</p>
			<p>The output of the preceding code is a <strong class="source-inline">Row</strong> object, a dictionary, and a string of the value for the key name, as shown:</p>
			<p class="source-code">Row(name='Patrick Hendrix', age=23, street='5755 Jonathan Ranch', city='New Sheriland', state='Wisconsin', zip=60519, lng=103.914462, lat=-59.0094375)</p>
			<p class="source-code">{'name': 'Patrick Hendrix', 'age': 23, 'street': '5755 Jonathan Ranch', 'city': 'New Sheriland', 'state': 'Wisconsin', 'zip': 60519, 'lng': 103.914462, 'lat': -59.0094375}</p>
			<p class="source-code">'Patrick Hendrix'</p>
			<p>To iterate through the DataFrame in Spark, you call <strong class="source-inline">collect()</strong>, and then iterate through the array using a <strong class="source-inline">for</strong> loop. You can then convert each of the rows into a dictionary and do what you need for processing. The following code snippet prints the dictionary:</p>
			<p class="source-code">for x in u40:</p>
			<p class="source-code">    print(x.asDict())</p>
			<p>If you are more <a id="_idIndexMarker739"/>comfortable with SQL, you can filter a DataFrame using <strong class="source-inline">spark.sql</strong>. To use SQL, you must first create a view, then you can query it with SQL, as shown in the following code:</p>
			<p class="source-code">df.createOrReplaceTempView('people')</p>
			<p class="source-code">df_over40=spark.sql('select * from people where age &gt; 40')</p>
			<p class="source-code">df_over40.show()</p>
			<p>The results will be the same DataFrame as the <strong class="source-inline">filter</strong> method, but just a different method by which to achieve the same result.</p>
			<p>There are several functions to perform modifications or analysis on columns or data in a DataFrame. In Spark, you can use <strong class="source-inline">describe()</strong> to get a basic summary of the data in a column. The following code uses it on the <strong class="source-inline">age</strong> column:</p>
			<p class="source-code">df_over40.describe('age').show()</p>
			<p>The output is the common descriptive statistics of <strong class="source-inline">count</strong>, <strong class="source-inline">mean</strong>, <strong class="source-inline">standard deviation</strong>, <strong class="source-inline">min</strong>, and <strong class="source-inline">max</strong>. These five statistics give you a good overview of the data.</p>
			<p>You can also group and aggregate your data just like in <strong class="source-inline">pandas</strong>. To group the counts of states, you can use <strong class="source-inline">groupBy()</strong>, as shown:</p>
			<p class="source-code">df.groupBy('state').count().show()</p>
			<p>Aggregation allows you to pass in a dictionary of the field and a method. To calculate the mean of the <strong class="source-inline">age</strong> column, you would use the following code:</p>
			<p class="source-code">df.agg({'age':'mean'}).show()</p>
			<p>For both <strong class="source-inline">groupBy</strong> and <strong class="source-inline">agg</strong>, you can use <strong class="source-inline">mean</strong>, <strong class="source-inline">max</strong>, <strong class="source-inline">min</strong>, <strong class="source-inline">sum</strong>, and other methods that you can read about in the documentation. There is a large number of other functions you can use<a id="_idIndexMarker740"/> that require you to import the <strong class="source-inline">pyspark.sql.functions</strong> module. The following code imports it as <strong class="source-inline">f</strong> and demonstrates some useful functions. Again, for more information on all of the functions, you can read the Python API documents at <a href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html">https://spark.apache.org/docs/latest/api/python/pyspark.sql.html</a>:</p>
			<p class="source-code">import pyspark.sql.functions as f</p>
			<p class="source-code">df.select(f.collect_set(df['state'])).collect()</p>
			<p class="source-code"># Returns a Row of unique states which will be all 50.</p>
			<p class="source-code">df.select(f.countDistinct('state').alias('states')).show()</p>
			<p class="source-code">#returns a single column named states with a single value of 50.</p>
			<p class="source-code">df.select(f.md5('street').alias('hash')).collect()</p>
			<p class="source-code">#Returns an md5 hash of the street value for each row</p>
			<p class="source-code"># Row(hash='81576976c4903b063c46ed9fdd140d62'),</p>
			<p class="source-code">df.select(f.reverse(df.state).alias('state-reverse')).collect()</p>
			<p class="source-code"># returns each rows street value reversed</p>
			<p class="source-code"># Row(state-reverse='nisnocsiW')</p>
			<p class="source-code">select(f.soundex(df.name).alias('soundex')).collect()</p>
			<p class="source-code"># returns a soundex of the name field for each row</p>
			<p class="source-code"># Row(soundex='P362')</p>
			<p>When you have finished your data processing, stop the session using <strong class="source-inline">stop()</strong>, as shown:</p>
			<p class="source-code">spark.stop()</p>
			<p>Congratulations! You have successfully processed data with PySpark.</p>
			<h1 id="_idParaDest-156"><a id="_idTextAnchor158"/>Summary</h1>
			<p>In this chapter, you learned the basics of working with Apache Spark. First, you downloaded and installed Spark and configured PySpark to run in Jupyter notebooks. You also learned how to scale Spark horizontally by adding nodes. Spark uses DataFrames similar to those used in <strong class="source-inline">pandas</strong>. The last section taught you the basics of manipulating data in Spark.</p>
			<p>In the next chapter, you will use Spark with Apache MiNiFi to move data at the edge or on Internet-of-Things devices.</p>
		</div>
	</body></html>