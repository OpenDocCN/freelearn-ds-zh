<html><head></head><body>
		<div><h1 id="_idParaDest-151"><em class="italic"><a id="_idTextAnchor153"/>Chapter 14</em>: Data Processing with Apache Spark</h1>
			<p>In the previous chapter, you learned how to add streaming data to your data pipelines. Using Python or Apache NiFi, you can extract, transform, and load streaming data. However, to perform transformations on large amounts of streaming data, data engineers turn to tools such as Apache Spark. Apache Spark is faster than most other methods – such as MapReduce on non-trivial transformations – and it allows distributed data processing.</p>
			<p>In this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li>Installing and running Spark</li>
				<li>Installing and configuring PySpark</li>
				<li>Processing data with PySpark</li>
			</ul>
			<h1 id="_idParaDest-152"><a id="_idTextAnchor154"/>Installing and running Spark</h1>
			<p>Apache Spark is a <a id="_idIndexMarker715"/>distributed data processing engine that can handle both streams and batch data, and even graphs. It has a core set of components and other libraries that are used to add functionality. A common depiction of the Spark ecosystem is shown in the following diagram:</p>
			<div><div><img src="img/Figure_14.1_B15739.jpg" alt="Figure 14.1 – The Apache Spark ecosystem&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.1 – The Apache Spark ecosystem</p>
			<p>To run Spark<a id="_idIndexMarker716"/> as a cluster, you have several options. Spark can run in a standalone mode, which uses a simple cluster manager provided by Spark. It can also run on an Amazon EC2 instance, using YARN, Mesos, or Kubernetes. In a production environment with a significant workload, you would probably not want to run in standalone mode; however, this is how we will stand up our cluster in this chapter. The principles will be the same, but the standalone cluster provides the fastest way to get you up and running without needing to dive into more complicated infrastructure. </p>
			<p>To install Apache Spark, take <a id="_idIndexMarker717"/>the following steps:</p>
			<ol>
				<li>Browse to the website at <a href="http://spark.apache.org">http://spark.apache.org</a>. From there, you can keep up to date with new versions of Apache Spark, read the documentation, learn about the libraries, and find code examples:<div><img src="img/Figure_14.2_B15739.jpg" alt="Figure 14.2 – The Apache Spark website&#13;&#10;"/></div><p class="figure-caption">Figure 14.2 – The Apache Spark website</p></li>
				<li>From the website, select<a id="_idIndexMarker718"/> the <strong class="bold">Download</strong> menu option. Choose the version of Spark you want to use – at the time of writing, the newest version is 3.0.0. You will be asked to choose a package type. We will not be using Hadoop, but have to select a version or provide our own. I have chosen <strong class="bold">Pre-built for Apache Hadoop 2.7</strong>. On Windows, you may need to trick the operating system into thinking Hadoop is installed by setting environment variables, but on Linux and macOS, this should not be an issue. The download options are shown in the following screenshot:<div><img src="img/Figure_14.3_B15739.jpg" alt="Figure 14.3 – Downloading Apache Spark for Hadoop 2.7&#13;&#10;"/></div><p class="figure-caption">Figure 14.3 – Downloading Apache Spark for Hadoop 2.7</p></li>
				<li>After downloading <a id="_idIndexMarker719"/>the file, you will extract it, then move it to the home directory in a directory named <code>spark3</code>. You can do this using the following commands:<pre><strong class="bold">tar -xvzf spark-3.0.0-bin-hadoop2.7.tgz</strong>
<strong class="bold">mv spark-3.0.0-bin-hadoop2.7 ~/spark3</strong></pre></li>
				<li>Next, you will need to make a cluster. As you did with Kafka, you will make a copy of the Spark directory on the same machine and make it act as another node. If you have another machine, you could also put another copy of Spark on that server. Copy the directory and rename it <code>spark-node</code>, as shown:<pre><strong class="bold">cp -r spark3/ spark-node</strong></pre></li>
				<li>To run the Spark cluster, you can use the provided scripts. The scripts to run the cluster use outdated terminology – <code>master</code> and <code>slave</code>. This language is common in the technology space; however, there have been many voices opposed to it for a long time. Finally, it appears that there is some traction being made in removing this language as GitHub will remove <code>master</code> from the branch names. I too have renamed the scripts, using the terms <code>head</code> and <code>node</code>. To do this, use the following commands:<pre><strong class="bold">cd ~/spark3/sbin</strong>
<strong class="bold">cp start-master.sh start-head.sh</strong>
<strong class="bold">cd ~/spark-node/sbin</strong>
<strong class="bold">cp start-slave.sh start-node.sh</strong></pre></li>
				<li>To start the cluster, you can now run the scripts as shown:<pre><strong class="bold">./start-head.sh</strong>
<strong class="bold">./start-node.sh spark://pop-os.localdomain:7077 -p 9911</strong></pre></li>
				<li>You can pass <a id="_idIndexMarker720"/>parameters to the scripts, and in the preceding command, you pass the port flag (<code>-p</code>) to tell the script which port you want the node to run on. You can also pass the following:<p>a) <code>-h, --host</code>: The hostname to run on. The <code>i, -ip</code> flag has been deprecated.</p><p>b) <code>-p, --port</code>: The port to listen on.</p><p>c) <code>--webui-port</code>: The port for the web GUI, which defaults to <code>8080</code>.</p><p>d) <code>-c, --cores</code>: The number of cores to use.</p><p>e) <code>-m, --memory</code>: The amount of memory to use. By default, it is 1 gigabyte less than your full memory.</p><p>f) <code>-d, --work-dir</code>: The scratch space directory for the worker only.</p><p>g) <code>--properties-file</code>: This is where you can specify several of these flags in a <code>spark.conf</code> file.</p></li>
			</ol>
			<p>The cluster will take a minute to load, and when it has finished, you can browse to the web UI at <a href="http://localhost:8080/">http://localhost:8080/</a>. You will see the details of your cluster and it will look as in the following screenshot:</p>
			<div><div><img src="img/Figure_14.4_B15739.jpg" alt="Figure 14.4 – Spark cluster web UI&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.4 – Spark cluster web UI</p>
			<p>With the cluster up and <a id="_idIndexMarker721"/>running, you will need to set up the Python environment so that you can code against it. The next section will walk you through those steps.</p>
			<h1 id="_idParaDest-153"><a id="_idTextAnchor155"/>Installing and configuring PySpark</h1>
			<p>PySpark is <a id="_idIndexMarker722"/>installed with Spark. You can see it in the <code>~/spark3/bin</code> directory, as well as other libraries and tools. To configure PySpark to run, you need to export environment variables. The variables are shown here:</p>
			<pre>export SPARK_HOME=/home/paulcrickard/spark3
export PATH=$SPARK_HOME/bin:$PATH
export PYSPARK_PYTHON=python3 </pre>
			<p>The preceding command set the <code>SPARK_HOME</code> variable. This will be where you installed Spark. I have pointed the variable to the head of the Spark cluster because the node would really be on another machine. Then, it adds <code>SPARK_HOME</code> to your path. This means that when you type a command, the operating system will look for it in the directories specified in your path, so now it will search <code>~/spark3/bin</code>, which is where PySpark lives. </p>
			<p>Running the<a id="_idIndexMarker723"/> preceding commands in a terminal will allow Spark to run while the<a id="_idIndexMarker724"/> terminal is open. You will have to rerun these commands every time. To make them permanent, you can add the commands to your <code>~/.bashrc</code> file. After saving the <code>.bashrc</code> file, you need to reload it. You can do that by running the following command:</p>
			<pre>source ~/.bashrc</pre>
			<p>You should now be able to open a terminal and run PySpark, and the result will be the PySpark interactive shell, as shown:</p>
			<div><div><img src="img/Figure_14.5_B15739.jpg" alt="Figure 14.5 – The interactive Spark shell&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.5 – The interactive Spark shell</p>
			<p>If you see the preceding screenshot, congratulations, you have PySpark configured. In this chapter, the examples will use PySpark in Jupyter notebooks. There are two ways to configure PySpark to work with Jupyter:</p>
			<ul>
				<li><code>PYSPARK_DRIVER_PYTHON</code> environment variable and the <code>_OPTS</code> variable to the Jupyter Notebook using the following commands (add this to <code>~/.bashrc</code> if you want it to be permanent):<pre><strong class="bold">export PYSPARK_DRIVER_PYTHON=jupyter</strong>
<strong class="bold">export PYSPARK_DRIVER_PYTHON_OPTS='notebook'</strong></pre></li>
				<li><code>findspark</code> library and add code to your Jupyter notebook that gets the Spark information as it runs. The examples in this chapter will use this method. You can install <code>findspark</code> using <code>pip</code>, as shown:<pre><code>findspark</code> method, add the following two lines to your <a id="_idIndexMarker726"/>notebook and run it:</p><pre><strong class="bold">import findspark</strong>
<strong class="bold">findspark.init()</strong></pre><p>If the preceding lines ran without error, then the code was able to find Spark. </p></li>
			</ul>
			<p>You can now run PySpark code on your Spark cluster. The next section will walk you through some basic PySpark examples.</p>
			<h1 id="_idParaDest-154"><a id="_idTextAnchor156"/>Processing data with PySpark</h1>
			<p>Before<a id="_idIndexMarker727"/> processing data with PySpark, let's run one of the samples <a id="_idIndexMarker728"/>to show how Spark works. Then, we will skip the boilerplate in later examples and focus on data processing. The Jupyter notebook for the <strong class="bold">Pi Estimation</strong> example from the Spark website at <a href="http://spark.apache.org/examples.html">http://spark.apache.org/examples.html</a> is shown in the following screenshot:</p>
			<div><div><img src="img/Figure_14.6_B15739.jpg" alt="Figure 14.6 – The Pi Estimation example in a Jupyter notebook&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.6 – The Pi Estimation example in a Jupyter notebook</p>
			<p>The example from the <a id="_idIndexMarker729"/>website will not run without some <a id="_idIndexMarker730"/>modifications. In the following points, I will walk through the cells:</p>
			<ol>
				<li value="1">The first cell imports <code>findspark</code> and runs the <code>init()</code> method. This was explained in the preceding section as the preferred method to include PySpark in Jupyter notebooks. The code is as follows:<pre>import findspark
findspark.init()</pre></li>
				<li>The next cell imports the <code>pyspark</code> library and <code>SparkSession</code>. It then creates the session by passing the head node of the Spark cluster. You can get the URL from the Spark web UI – you also used it to start the worker node:<pre>import pyspark
from pyspark.sql import SparkSession
spark=SparkSession.builder.master('spark://pop-os.localdomain:7077').appName('Pi-Estimation').getOrCreate()</pre></li>
				<li>Running the first two cells, you can browse to the Spark GUI and see that there is a task running. The running task is shown in the following screenshot – notice the name of the worker is <code>Pi-Estimation</code>, which is the <code>appName</code> parameter in the preceding code:<div><img src="img/Figure_14.7_B15739.jpg" alt="Figure 14.7 – The Spark web UI with a running session and two completed sessions&#13;&#10;"/></div><p class="figure-caption">Figure 14.7 – The Spark web UI with a running session and two completed sessions</p><p>The preceding<a id="_idIndexMarker731"/> code will be used in all of your Spark code. It is the boilerplate code. </p></li>
				<li>The next cell<a id="_idIndexMarker732"/> contains the work. The code that follows will estimate the value of pi. The details of the code are not important, but notice that the <code>count</code> variable uses <code>sparkContext</code> and parallelizes a task on the cluster. After the boilerplate, your Spark code will execute a task and get the results:<pre>import random
NUM_SAMPLES=1
def inside(p):
    x, y = random.random(), random.random()
    return x*x + y*y &lt; 1
count = spark.sparkContext.parallelize(range(0,
                     NUM_SAMPLES)).filter(inside).count()
print('Pi is roughly {}'.format(4.0 * count / 
                                NUM_SAMPLES))</pre></li>
				<li>Lastly, stop the session:<pre>spark.stop()</pre></li>
			</ol>
			<p>Once the session <a id="_idIndexMarker733"/>has been stopped, it will show up as a completed<a id="_idIndexMarker734"/> application in the web UI. The next section will use Spark with DataFrames and send the data to Kafka.</p>
			<h2 id="_idParaDest-155"><a id="_idTextAnchor157"/>Spark for data engineering</h2>
			<p>The previous section<a id="_idIndexMarker735"/> showed the structure of a Spark application: we used <code>findspark</code> to get the paths, imported the libraries, created a session, did something, and stopped the session. When you do something, it will most likely involve a Spark DataFrame. This section will provide a brief overview of how Spark DataFrames work – it is slightly different than <code>pandas</code>.</p>
			<p>The first thing you have to do is use <code>findspark</code> to set up the environment. Then, you can import the required libraries. Then, create the session. The following code shows the boilerplate to get set up:</p>
			<pre>import findspark
findspark.init()
import pyspark
from pyspark.sql import SparkSession
import os
os.chdir('/home/paulcrickard')
spark=SparkSession.builder.master('spark://pop-os.localdomain:7077').appName('DataFrame-Kafka').getOrCreate()</pre>
			<p>Now you are connected to a session on the Spark cluster. You can read CSV and JSON data just like you did with DataFrames in <a href="B15739_03_ePub_AM.xhtml#_idTextAnchor039"><em class="italic">Chapter 3</em></a><em class="italic">, Reading and Writing Files</em> and <a href="B15739_04_ePub_AM.xhtml#_idTextAnchor049"><em class="italic">Chapter 4</em></a><em class="italic">, Working with Databases</em>, with some slight modifications. When you read in the data, you <a id="_idIndexMarker736"/>can use <code>read.csv</code> instead of <code>read_csv</code> in <code>pandas</code>. Another difference between Spark and <code>pandas</code> is the use of <code>.show()</code> in Spark to see the DataFrame. In <code>pandas</code>, you can view <code>dtypes</code> of a DataFrame, and in Spark, you can do the same using <code>printSchema()</code>. The following code reads the <code>data.csv</code> file and prints the top five rows and the schema:</p>
			<pre>df = spark.read.csv('data.csv')
df.show(5)
df.printSchema()</pre>
			<p>The output will be a DataFrame like the one shown in the following screenshot:</p>
			<div><div><img src="img/Figure_14.8_B15739.jpg" alt="Figure 14.8 – DataFrame from CSV with schema&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.8 – DataFrame from CSV with schema</p>
			<p>You will notice that<a id="_idIndexMarker737"/> the headers are the first row and there are default <code>_c0</code> column names. The output also shows that all the columns are strings. You can specify a schema and pass it as a parameter; however, you can also tell Spark to infer the schema. The following code passes that there are headers and tells Spark to infer the schema:</p>
			<pre>df = spark.read.csv('data.csv',header=True,inferSchema=True)
df.show(5)</pre>
			<p>The results are what you expected: a DataFrame with the correct types. The following screenshot shows the results:</p>
			<div><div><img src="img/Figure_14.9_B15739.jpg" alt="Figure 14.9 – DataFrame with headers and correct types&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 14.9 – DataFrame with headers and correct types</p>
			<p>You can select a column by using <code>select()</code> and passing the column name as a parameter. Don't forget to add <code>.show()</code> or it will return a DataFrame and not display it:</p>
			<pre>df.select('name').show()</pre>
			<p>You will notice that in <code>pandas</code>, you would have used <code>[]</code> and a column name and also did not need the <code>select</code> method. In <code>pandas</code>, you could also filter a DataFrame by using the <code>df[(df['field']&lt; value)]</code> format. In Spark, you can use <code>select</code> and <code>filter</code> to do the same, the difference being that a <code>select</code> method returns <code>True</code> and <code>False</code> for a condition, and <code>filter</code> will return the DataFrame for that condition. With <code>filter</code>, you can also add a <code>select</code> method and pass an array of columns to return. The code is shown as follows:</p>
			<pre>df.select(df['age']&lt;40).show()
df.filter(df['age']&lt;40).show()
df.filter('age&lt;40').select(['name','age','state']).show()</pre>
			<p>Notice that in the last<a id="_idIndexMarker738"/> line, you didn't use <code>df['age']</code> but were able to just pass the column name. When you want to iterate through a DataFrame, you could use <code>iterrows</code> in <code>pandas</code>. In Spark, you create an array of rows using <code>collect()</code>. The following code will use a <code>filter</code> method to get all people under 40 and print the array:</p>
			<pre>u40=df.filter('age&lt;40').collect()
u40</pre>
			<p>To get a single row, you can just pass the index. You can convert the row into different formats, and in this example, I converted it into a dictionary. As a dictionary, you can select any value by specifying the key. The code is shown as follows:</p>
			<pre>u40[0]
u40[0].asDict()
u40[0].asDict()['name']</pre>
			<p>The output of the preceding code is a <code>Row</code> object, a dictionary, and a string of the value for the key name, as shown:</p>
			<pre>Row(name='Patrick Hendrix', age=23, street='5755 Jonathan Ranch', city='New Sheriland', state='Wisconsin', zip=60519, lng=103.914462, lat=-59.0094375)
{'name': 'Patrick Hendrix', 'age': 23, 'street': '5755 Jonathan Ranch', 'city': 'New Sheriland', 'state': 'Wisconsin', 'zip': 60519, 'lng': 103.914462, 'lat': -59.0094375}
'Patrick Hendrix'</pre>
			<p>To iterate through the DataFrame in Spark, you call <code>collect()</code>, and then iterate through the array using a <code>for</code> loop. You can then convert each of the rows into a dictionary and do what you need for processing. The following code snippet prints the dictionary:</p>
			<pre>for x in u40:
    print(x.asDict())</pre>
			<p>If you are more <a id="_idIndexMarker739"/>comfortable with SQL, you can filter a DataFrame using <code>spark.sql</code>. To use SQL, you must first create a view, then you can query it with SQL, as shown in the following code:</p>
			<pre>df.createOrReplaceTempView('people')
df_over40=spark.sql('select * from people where age &gt; 40')
df_over40.show()</pre>
			<p>The results will be the same DataFrame as the <code>filter</code> method, but just a different method by which to achieve the same result.</p>
			<p>There are several functions to perform modifications or analysis on columns or data in a DataFrame. In Spark, you can use <code>describe()</code> to get a basic summary of the data in a column. The following code uses it on the <code>age</code> column:</p>
			<pre>df_over40.describe('age').show()</pre>
			<p>The output is the common descriptive statistics of <code>count</code>, <code>mean</code>, <code>standard deviation</code>, <code>min</code>, and <code>max</code>. These five statistics give you a good overview of the data.</p>
			<p>You can also group and aggregate your data just like in <code>pandas</code>. To group the counts of states, you can use <code>groupBy()</code>, as shown:</p>
			<pre>df.groupBy('state').count().show()</pre>
			<p>Aggregation allows you to pass in a dictionary of the field and a method. To calculate the mean of the <code>age</code> column, you would use the following code:</p>
			<pre>df.agg({'age':'mean'}).show()</pre>
			<p>For both <code>groupBy</code> and <code>agg</code>, you can use <code>mean</code>, <code>max</code>, <code>min</code>, <code>sum</code>, and other methods that you can read about in the documentation. There is a large number of other functions you can use<a id="_idIndexMarker740"/> that require you to import the <code>pyspark.sql.functions</code> module. The following code imports it as <code>f</code> and demonstrates some useful functions. Again, for more information on all of the functions, you can read the Python API documents at <a href="https://spark.apache.org/docs/latest/api/python/pyspark.sql.html">https://spark.apache.org/docs/latest/api/python/pyspark.sql.html</a>:</p>
			<pre>import pyspark.sql.functions as f
df.select(f.collect_set(df['state'])).collect()
# Returns a Row of unique states which will be all 50.
df.select(f.countDistinct('state').alias('states')).show()
#returns a single column named states with a single value of 50.
df.select(f.md5('street').alias('hash')).collect()
#Returns an md5 hash of the street value for each row
# Row(hash='81576976c4903b063c46ed9fdd140d62'),
df.select(f.reverse(df.state).alias('state-reverse')).collect()
# returns each rows street value reversed
# Row(state-reverse='nisnocsiW')
select(f.soundex(df.name).alias('soundex')).collect()
# returns a soundex of the name field for each row
# Row(soundex='P362')</pre>
			<p>When you have finished your data processing, stop the session using <code>stop()</code>, as shown:</p>
			<pre>spark.stop()</pre>
			<p>Congratulations! You have successfully processed data with PySpark.</p>
			<h1 id="_idParaDest-156"><a id="_idTextAnchor158"/>Summary</h1>
			<p>In this chapter, you learned the basics of working with Apache Spark. First, you downloaded and installed Spark and configured PySpark to run in Jupyter notebooks. You also learned how to scale Spark horizontally by adding nodes. Spark uses DataFrames similar to those used in <code>pandas</code>. The last section taught you the basics of manipulating data in Spark.</p>
			<p>In the next chapter, you will use Spark with Apache MiNiFi to move data at the edge or on Internet-of-Things devices.</p>
		</div>
	</body></html>