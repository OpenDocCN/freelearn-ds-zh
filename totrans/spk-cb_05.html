<html><head></head><body><div class="chapter" title="Chapter&#xA0;5.&#xA0;Spark Streaming"><div class="titlepage"><div><div><h1 class="title"><a id="ch05"/>Chapter 5. Spark Streaming</h1></div></div></div><p>Spark Streaming adds the holy grail of big data processing—that is, real-time analytics—to Apache Spark. It enables Spark to ingest live data streams and provides real-time intelligence at a very low latency of a few seconds.</p><p>In this chapter, we'll cover the following recipes:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Word count using Streaming</li><li class="listitem" style="list-style-type: disc">Streaming Twitter data</li><li class="listitem" style="list-style-type: disc">Streaming using Kafka</li></ul></div><div class="section" title="Introduction"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec39"/>Introduction</h1></div></div></div><p>Streaming is the <a id="id282" class="indexterm"/>process of dividing continuously flowing input data into discreet units so that it can be processed easily. Familiar examples in real life are streaming video and audio content (though a user can download the full movie before he/she can watch it, a faster solution is to stream data in small chunks that start playing for the user while the rest of the data is being downloaded in the background).</p><p>Real-world examples of streaming, besides multimedia, are the processing of market feeds, weather data, electronic stock trading data, and so on. All of these applications produce large volumes of data at very fast rates and require special handling of the data so that insights can be derived from data in real time.</p><p>Streaming has a few basic concepts, which are better to understand before we focus on Spark Streaming. The rate at which a streaming application receives data is called <span class="strong"><strong>data rate</strong></span> and <a id="id283" class="indexterm"/>is expressed<a id="id284" class="indexterm"/> in the form of <span class="strong"><strong>kilobytes per second</strong></span> (<span class="strong"><strong>kbps</strong></span>) <a id="id285" class="indexterm"/>or <span class="strong"><strong>megabytes per second</strong></span> (<span class="strong"><strong>mbps</strong></span>).</p><p>One important <a id="id286" class="indexterm"/>use case of streaming is <span class="strong"><strong>complex event processing</strong></span> (<span class="strong"><strong>CEP</strong></span>). In CEP, it is important to control the scope of the data being processed. This scope is called window, which can be either based on time or size. An example of a time-based window is to analyze data that has come in last one minute. An example of a size-based window can be the average ask price of the last 100 trades of a given stock.</p><p>Spark Streaming is<a id="id287" class="indexterm"/> Spark's library that provides support to process live data. This stream can come from any source, such as Twitter, Kafka, or Flume.</p><p>Spark Streaming has a few fundamental building blocks that we need to understand well before diving into the recipes.</p><p>Spark Streaming has a context wrapper called <code class="literal">StreamingContext</code>, which wraps around <code class="literal">SparkContext</code> and is the entry point to the Spark Streaming functionality. Streaming data, by definition, is continuous and needs to be time-sliced to process. This slice of time is called<a id="id288" class="indexterm"/> the <span class="strong"><strong>batch interval</strong></span>, which is specified when <code class="literal">StreamingContext</code> is created. There is one-to-one mapping of RDD and batch, that is, each batch results in one RDD. As you can see in the following image, Spark Streaming takes continuous data, break it into batches and feed to Spark.</p><div class="mediaobject"><img src="graphics/3056_05_01.jpg" alt="Introduction"/></div><p>Batch interval is important to optimize your streaming application. Ideally, you want to process data at least as fast as it is getting ingested; otherwise, your application will develop a backlog. Spark Streaming collects data for the duration of a batch interval, say, 2 seconds. The moment this 2 second interval is over, data collected in that interval will be given to Spark for processing and Streaming will focus on collecting data for the next batch interval. Now, this 2 second batch interval is all Spark has to process data, as it should be free to receive data from the next batch. If Spark can process the data faster, you can reduce the batch interval to, say, 1 second. If Spark is not able to keep up with this speed, you have to increase the batch interval.</p><p>The continuous stream of RDDs in Spark Streaming needs to be represented in the form of an abstraction through which it can be processed. This abstraction is called <span class="strong"><strong>Discretized Stream</strong></span> (<span class="strong"><strong>DStream</strong></span>). Any operation applied on DStream results in an operation on <a id="id289" class="indexterm"/>underlying RDDs.</p><p>Every input DStream is <a id="id290" class="indexterm"/>associated with a receiver (except for file stream). A receiver receives data from the input source and stores it in Spark's memory. There are two types of streaming sources:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Basic sources, such as file and socket connections</li><li class="listitem" style="list-style-type: disc">Advanced sources, such as Kafka and Flume</li></ul></div><p>Spark Streaming also provides windowed computations in which you can apply the transformation over a sliding window of data. A sliding window operation is based on two parameters:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Window length</strong></span>: This<a id="id291" class="indexterm"/> is the duration of the window. For example, if you want to get analytics of the last 1 minute of data, the window length will be 1 minute.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Sliding interval</strong></span>: This depicts how frequently you want to perform an operation. Say you <a id="id292" class="indexterm"/>want to perform the operation every 10 seconds; this means that every 10 seconds, 1 minute of window will have 50 seconds of data common with the last window and 10 seconds of the new data.</li></ul></div><p>Both these parameters work on underlying RDDs that, obviously, cannot be broken apart; therefore, both of these should be a multiple of the batch interval. The window length has to be a multiple of the sliding interval as well.</p><p>DStream also has output operations, which allow data to be pushed to external systems. They are similar to actions on RDDs (one higher level of abstraction of what you do at DStream happens to RDDs).</p><p>Besides print to print content of DStream, standard RDD actions, such as <code class="literal">saveAsTextFile</code>, <code class="literal">saveAsObjectFile</code>, and <code class="literal">saveAsHadoopFile</code>, are supported by similar counterparts, such as <code class="literal">saveAsTextFiles</code>, <code class="literal">saveAsObjectFiles</code>, and <code class="literal">saveAsHadoopFiles</code>, respectively.</p><p>One very useful output operation is <code class="literal">foreachRDD(func)</code>, which applies any arbitrary function to all the RDDs.</p></div></div>
<div class="section" title="Word count using Streaming"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec40"/>Word count using Streaming</h1></div></div></div><p>Let's start with a simple example of Streaming in which in one terminal, we will type some text <a id="id293" class="indexterm"/>and the Streaming application will capture it in<a id="id294" class="indexterm"/> another window.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec61"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Start the Spark shell and give it some extra memory:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell --driver-memory 1G</strong></span>
</pre></div></li><li class="listitem">Stream specific imports:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.SparkConf</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.streaming.{Seconds, StreamingContext}</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.storage.StorageLevel</strong></span>
<span class="strong"><strong>scala&gt; import StorageLevel._</strong></span>
</pre></div></li><li class="listitem">Import for an implicit conversion:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark._</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.streaming._</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.streaming.StreamingContext._</strong></span>
</pre></div></li><li class="listitem">Create <code class="literal">StreamingContext</code> with a 2 second batch interval:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val ssc = new StreamingContext(sc, Seconds(2))</strong></span>
</pre></div></li><li class="listitem">Create a <code class="literal">SocketTextStream</code> Dstream on localhost with port <code class="literal">8585</code> with the <code class="literal">MEMORY_ONLY</code> caching:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val lines = ssc.socketTextStream("localhost",8585,MEMORY_ONLY)</strong></span>
</pre></div></li><li class="listitem">Divide the lines into multiple words:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val wordsFlatMap = lines.flatMap(_.split(" "))</strong></span>
</pre></div></li><li class="listitem">Convert word to (word,1), that is, output <code class="literal">1</code> as the value for each occurrence of a word as the key:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val wordsMap = wordsFlatMap.map( w =&gt; (w,1))</strong></span>
</pre></div></li><li class="listitem">Use the <code class="literal">reduceByKey</code> method to add a number of occurrences for each word as the key (the function works on two consecutive values at a time, represented by <code class="literal">a</code> and <code class="literal">b</code>):<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val wordCount = wordsMap.reduceByKey( (a,b) =&gt; (a+b))</strong></span>
</pre></div></li><li class="listitem">Print <code class="literal">wordCount</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; wordCount.print</strong></span>
</pre></div></li><li class="listitem">Start <code class="literal">StreamingContext</code>; remember, nothing happens until <code class="literal">StreamingContext</code> is started:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; ssc.start</strong></span>
</pre></div></li><li class="listitem">Now, in a <a id="id295" class="indexterm"/>separate window, start the netcat server:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ nc -lk 8585</strong></span>
</pre></div></li><li class="listitem">Enter <a id="id296" class="indexterm"/>different lines, such as <code class="literal">to be or not to be</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>to be or not to be</strong></span>
</pre></div></li><li class="listitem">Check the Spark shell, and you will see word count results like the following screenshot:<div class="mediaobject"><img src="graphics/3056_05_02.jpg" alt="How to do it..."/></div></li></ol></div></div></div>
<div class="section" title="Streaming Twitter data"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec41"/>Streaming Twitter data</h1></div></div></div><p>Twitter is a famous microblogging platform. It produces a massive amount of data with around 500 million tweets sent each day. Twitter allows its data to be accessed by APIs and that makes it the poster child of testing any big data streaming application.</p><p>In this recipe, we <a id="id297" class="indexterm"/>will see how we can live stream data in Spark using Twitter streaming libraries. Twitter is just one source of providing the streaming data to Spark and has no special status. Therefore, there are no built-in libraries for Twitter. Spark does provide some APIs to facilitate integration with Twitter libraries, though.</p><p>An example use of live Twitter data feed can be to find trending tweets in the last 5 minutes.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec62"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Create a <a id="id298" class="indexterm"/>Twitter account if you do not already have one.</li><li class="listitem">Go to <a class="ulink" href="http://apps.twitter.com">http://apps.twitter.com</a>.</li><li class="listitem">Click on <span class="strong"><strong>Create New App</strong></span>.</li><li class="listitem">Enter <span class="strong"><strong>Name</strong></span>, <span class="strong"><strong>Description</strong></span>, <span class="strong"><strong>Website</strong></span>, and <span class="strong"><strong>Callback URL</strong></span>, and then click on <span class="strong"><strong>Create your Twitter Application</strong></span>.<div class="mediaobject"><img src="graphics/3056_05_03.jpg" alt="How to do it..."/></div></li><li class="listitem">You will <a id="id299" class="indexterm"/>reach <span class="strong"><strong>Application Management</strong></span> screen.</li><li class="listitem">Navigate to <span class="strong"><strong>Keys and Access Tokens</strong></span> | <span class="strong"><strong>Create my access Token</strong></span>.<div class="mediaobject"><img src="graphics/3056_05_04.jpg" alt="How to do it..."/></div></li><li class="listitem">Note down the four values in this screen that we will use in step 14:<p><span class="strong"><strong>Consumer Key (API Key)</strong></span></p><p><span class="strong"><strong>Consumer Secret (API Secret)</strong></span></p><p><span class="strong"><strong>Access Token</strong></span></p><p><span class="strong"><strong>Access Token Secret</strong></span></p></li><li class="listitem">We will need to provide the values in this screen in some time, but, for now, let's download the third-party libraries needed from Maven central:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ wget http://central.maven.org/maven2/org/apache/spark/spark-streaming-twitter_2.10/1.2.0/spark-streaming-twitter_2.10-1.2.0.jar</strong></span>
<span class="strong"><strong>$ wget http://central.maven.org/maven2/org/twitter4j/twitter4j-stream/4.0.2/twitter4j-stream-4.0.2.jar</strong></span>
<span class="strong"><strong>$ wget http://central.maven.org/maven2/org/twitter4j/twitter4j-core/4.0.2/twitter4j-core-4.0.2.jar</strong></span>
</pre></div></li><li class="listitem">Open the <a id="id300" class="indexterm"/>Spark shell, supplying the preceding three JARS as the dependency:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell --jars spark-streaming-twitter_2.10-1.2.0.jar, twitter4j-stream-4.0.2.jar,twitter4j-core-4.0.2.jar</strong></span>
</pre></div></li><li class="listitem">Perform imports that are Twitter-specific:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.streaming.twitter._</strong></span>
<span class="strong"><strong>scala&gt; import twitter4j.auth._</strong></span>
<span class="strong"><strong>scala&gt; import twitter4j.conf._</strong></span>
</pre></div></li><li class="listitem">Stream specific imports:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.streaming.{Seconds, StreamingContext}</strong></span>
</pre></div></li><li class="listitem">Import for an implicit conversion:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark._</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.streaming._</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.streaming.StreamingContext._</strong></span>
</pre></div></li><li class="listitem">Create <code class="literal">StreamingContext</code> with a 10 second batch interval:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val ssc = new StreamingContext(sc, Seconds(10))</strong></span>
</pre></div></li><li class="listitem">Create <code class="literal">StreamingContext</code> with a 2 second batch interval:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val cb = new ConfigurationBuilder</strong></span>
<span class="strong"><strong>scala&gt; cb.setDebugEnabled(true)</strong></span>
<span class="strong"><strong>.setOAuthConsumerKey("FKNryYEKeCrKzGV7zuZW4EKeN")</strong></span>
<span class="strong"><strong>.setOAuthConsumerSecret("x6Y0zcTBOwVxpvekSCnGzbi3NYNrM5b8ZMZRIPI1XRC3pDyOs1")</strong></span>
<span class="strong"><strong>  .setOAuthAccessToken("31548859-DHbESdk6YoghCLcfhMF88QEFDvEjxbM6Q90eoZTGl")</strong></span>
<span class="strong"><strong>.setOAuthAccessTokenSecret("wjcWPvtejZSbp9cgLejUdd6W1MJqFzm5lByUFZl1NYgrV")</strong></span>
<span class="strong"><strong>val auth = new OAuthAuthorization(cb.build)</strong></span>
</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note14"/>Note</h3><p>These are sample values and you should put your own values.</p></div></div></li><li class="listitem">Create Twitter DStream:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val tweets = TwitterUtils.createStream(ssc,auth)</strong></span>
</pre></div></li><li class="listitem">Filter out English tweets:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val englishTweets = tweets.filter(_.getLang()=="en")</strong></span>
</pre></div></li><li class="listitem">Get text out of the tweets:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val status = englishTweets.map(status =&gt; status.getText)</strong></span>
</pre></div></li><li class="listitem">Set the <a id="id301" class="indexterm"/>checkpoint directory:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; ssc.checkpoint("hdfs://localhost:9000/user/hduser/checkpoint")</strong></span>
</pre></div></li><li class="listitem">Start <code class="literal">StreamingContext</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; ssc.start</strong></span>
<span class="strong"><strong>scala&gt; ssc.awaitTermination</strong></span>
</pre></div></li><li class="listitem">You can put all these commands together using <code class="literal">:paste</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; :paste</strong></span>
<span class="strong"><strong>import org.apache.spark.streaming.twitter._</strong></span>
<span class="strong"><strong>import twitter4j.auth._</strong></span>
<span class="strong"><strong>import twitter4j.conf._</strong></span>
<span class="strong"><strong>import org.apache.spark.streaming.{Seconds, StreamingContext}</strong></span>
<span class="strong"><strong>import org.apache.spark._</strong></span>
<span class="strong"><strong>import org.apache.spark.streaming._</strong></span>
<span class="strong"><strong>import org.apache.spark.streaming.StreamingContext._</strong></span>
<span class="strong"><strong>val ssc = new StreamingContext(sc, Seconds(10))</strong></span>
<span class="strong"><strong>val cb = new ConfigurationBuilder</strong></span>
<span class="strong"><strong>cb.setDebugEnabled(true).setOAuthConsumerKey("FKNryYEKeCrKzGV7zuZW4EKeN")</strong></span>
<span class="strong"><strong>  .setOAuthConsumerSecret("x6Y0zcTBOwVxpvekSCnGzbi3NYNrM5b8ZMZRIPI1XRC3pDyOs1")</strong></span>
<span class="strong"><strong>  .setOAuthAccessToken("31548859-DHbESdk6YoghCLcfhMF88QEFDvEjxbM6Q90eoZTGl")</strong></span>
<span class="strong"><strong>  .setOAuthAccessTokenSecret("wjcWPvtejZSbp9cgLejUdd6W1MJqFzm5lByUFZl1NYgrV")</strong></span>
<span class="strong"><strong>val auth = new OAuthAuthorization(cb.build)</strong></span>
<span class="strong"><strong>val tweets = TwitterUtils.createStream(ssc,Some(auth))</strong></span>
<span class="strong"><strong>val englishTweets = tweets.filter(_.getLang()=="en")</strong></span>
<span class="strong"><strong>val status = englishTweets.map(status =&gt; status.getText)</strong></span>
<span class="strong"><strong>status.print</strong></span>
<span class="strong"><strong>ssc.checkpoint("hdfs://localhost:9000/checkpoint")</strong></span>
<span class="strong"><strong>ssc.start</strong></span>
<span class="strong"><strong>ssc.awaitTermination</strong></span>
</pre></div></li></ol></div></div></div>
<div class="section" title="Streaming using Kafka"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec42"/>Streaming using Kafka</h1></div></div></div><p>Kafka is a distributed, partitioned, and replicated commit log service. In simple words, it is a distributed<a id="id302" class="indexterm"/> messaging server. Kafka maintains the message feed in categories <a id="id303" class="indexterm"/>called <span class="strong"><strong>topics</strong></span>. An example of the topic can be a ticker symbol of a <a id="id304" class="indexterm"/>company you would like to get news about, for example, CSCO for Cisco.</p><p>Processes that produce<a id="id305" class="indexterm"/> messages are called <span class="strong"><strong>producers</strong></span> and those that consume messages<a id="id306" class="indexterm"/> are called <span class="strong"><strong>consumers</strong></span>. In traditional messaging, the messaging service has one central messaging server, also called <a id="id307" class="indexterm"/>
<span class="strong"><strong>broker</strong></span>. Since Kafka is a distributed messaging service, it has a cluster of brokers, which functionally act as one Kafka broker, as shown here:</p><div class="mediaobject"><img src="graphics/B03056_05_06.jpg" alt="Streaming using Kafka"/></div><p>For each topic, Kafka<a id="id308" class="indexterm"/> maintains the partitioned log. This partitioned log consists of one or more partitions spread across the cluster, as shown in the following figure:</p><div class="mediaobject"><img src="graphics/B03056_05_07.jpg" alt="Streaming using Kafka"/></div><p>Kafka borrows a lot <a id="id309" class="indexterm"/>of concepts from Hadoop and other big data frameworks. The concept of partition is very similar to the concept of <code class="literal">InputSplit</code> in Hadoop. In the simplest form, while using <code class="literal">TextInputFormat</code>, an <code class="literal">InputSplit</code> is same as a block. A block is read in the form of a key-value pair in <code class="literal">TextInputFormat</code>, where the key is the byte offset of a line and the value is content of the line itself. In a similar way, in a Kafka partition, records are stored and retrieved in the form of key-value pairs, where the key is a sequential ID number called the offset and the value is the actual<a id="id310" class="indexterm"/> message.</p><p>In Kafka, message retention does not depend on the consumption by a consumer. Messages are retained for a configurable period of time. Each consumer is free to read messages in any order they like. All it needs to retain is an offset. Another analogy can be reading a book in which the page number is analogous to the offset, while the page content is analogous to the message. The reader is free to read whichever way he/she wants as long as they remember the bookmark (the current offset).</p><p>To provide functionality similar to pub/sub and PTP (queues) in traditional messaging systems, Kafka has the concept of consumer groups. A consumer group is a group of consumers, which the Kafka cluster treats as a single unit. In a consumer group, only one consumer needs to receive a message. If consumer C1, in the following diagram, receives the first message for topic T1, all the following messages on that topic will also be delivered to this consumer. Using this strategy, Kafka guarantees the order of message delivery for a given topic.</p><p>In extreme cases, when all consumers are in one consumer group, the Kafka cluster acts like PTP/queue. In another extreme case, if every consumer belongs to a different group, it acts like pub/sub. In practice, each consumer group has a limited number of consumers.</p><div class="mediaobject"><img src="graphics/B03056_05_08.jpg" alt="Streaming using Kafka"/></div><p>This recipe<a id="id311" class="indexterm"/> will show you how to perform a word count using data <a id="id312" class="indexterm"/>coming from Kafka.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec63"/>Getting ready</h2></div></div></div><p>This recipe assumes Kafka is already installed. Kafka comes with ZooKeeper bundled. We are assuming Kafka's home is in <code class="literal">/opt/infoobjects/kafka</code>:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Start ZooKeeper:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ /opt/infoobjects/kafka/bin/zookeeper-server-start.sh /opt/infoobjects/kafka/config/zookeeper.properties</strong></span>
</pre></div></li><li class="listitem">Start the Kafka server:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ /opt/infoobjects/kafka/bin/kafka-server-start.sh /opt/infoobjects/kafka/config/server.properties</strong></span>
</pre></div></li><li class="listitem">Create a <code class="literal">test</code> topic:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ /opt/infoobjects/kafka/bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test</strong></span>
</pre></div></li></ol></div></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec64"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Download the <code class="literal">spark-streaming-kafka</code> library and its dependencies:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ wget http://central.maven.org/maven2/org/apache/spark/spark-streaming-kafka_2.10/1.2.0/spark-streaming-kafka_2.10-1.2.0.jar</strong></span>
<span class="strong"><strong>$ wget http://central.maven.org/maven2/org/apache/kafka/kafka_2.10/0.8.1/kafka_2.10-0.8.1.jar</strong></span>
<span class="strong"><strong>$ wget http://central.maven.org/maven2/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar</strong></span>
<span class="strong"><strong>$ wget http://central.maven.org/maven2/com/101tec/zkclient/0.4/zkclient-0.4.jar</strong></span>
</pre></div></li><li class="listitem">Start the Spark shell and provide the <code class="literal">spark-streaming-kafka</code> library:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell --jars spark-streaming-kafka_2.10-1.2.0.jar, kafka_2.10-0.8.1.jar,metrics-core-2.2.0.jar,zkclient-0.4.jar</strong></span>
</pre></div></li><li class="listitem">Stream<a id="id313" class="indexterm"/> specific imports:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.streaming.{Seconds, StreamingContext}</strong></span>
</pre></div></li><li class="listitem">Import<a id="id314" class="indexterm"/> for implicit conversion:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark._</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.streaming._</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.streaming.StreamingContext._</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.streaming.kafka.KafkaUtils</strong></span>
</pre></div></li><li class="listitem">Create <code class="literal">StreamingContext</code> with a 2 second batch interval:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val ssc = new StreamingContext(sc, Seconds(2))</strong></span>
</pre></div></li><li class="listitem">Set Kafka-specific variables:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val zkQuorum = "localhost:2181"</strong></span>
<span class="strong"><strong>scala&gt; val group = "test-group"</strong></span>
<span class="strong"><strong>scala&gt; val topics = "test"</strong></span>
<span class="strong"><strong>scala&gt; val numThreads = 1</strong></span>
</pre></div></li><li class="listitem">Create <code class="literal">topicMap</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val topicMap = topics.split(",").map((_,numThreads.toInt)).toMap</strong></span>
</pre></div></li><li class="listitem">Create Kafka DStream:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val lineMap = KafkaUtils.createStream(ssc, zkQuorum, group, topicMap)</strong></span>
</pre></div></li><li class="listitem">Pull the value out of lineMap:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val lines = lineMap.map(_._2)</strong></span>
</pre></div></li><li class="listitem">Create <code class="literal">flatMap</code> of values:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val words = lines.flatMap(_.split(" "))</strong></span>
</pre></div></li><li class="listitem">Create the key-value pair of (word,occurrence):<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val pair = words.map( x =&gt; (x,1))</strong></span>
</pre></div></li><li class="listitem">Do the word count for a sliding window:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val wordCounts = pair.reduceByKeyAndWindow(_ + _, _ - _, Minutes(10), Seconds(2), 2)</strong></span>
<span class="strong"><strong>scala&gt; wordCounts.print</strong></span>
</pre></div></li><li class="listitem">Set the <code class="literal">checkpoint</code> directory:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; ssc.checkpoint("hdfs://localhost:9000/user/hduser/checkpoint")</strong></span>
</pre></div></li><li class="listitem">Start <code class="literal">StreamingContext</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; ssc.start</strong></span>
<span class="strong"><strong>scala&gt; ssc.awaitTermination</strong></span>
</pre></div></li><li class="listitem">Publish a <a id="id315" class="indexterm"/>message on the <code class="literal">test</code> topic in Kafka in another <a id="id316" class="indexterm"/>window:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ /opt/infoobjects/kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test</strong></span>
</pre></div></li><li class="listitem">Now, publish messages on Kafka by pressing <span class="emphasis"><em>Enter</em></span> at step 15 and after every message.</li><li class="listitem">Now, as you publish messages on Kafka, you will see them in the Spark shell:<div class="mediaobject"><img src="graphics/3056_05_05.jpg" alt="How to do it..."/></div></li></ol></div></div><div class="section" title="There's more…"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec65"/>There's more…</h2></div></div></div><p>Let's say you want to maintain a running count of the occurrence of each word. Spark Streaming has a feature for this called <code class="literal">updateStateByKey</code> operation. The <code class="literal">updateStateByKey</code> operation allows you to maintain any arbitrary state while updating it with the new information supplied.</p><p>This arbitrary state can be an aggregation value, or just a change in state (like the mood of a user on twitter). Perform the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Let's call <code class="literal">updateStateByKey</code> on pairs RDD:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val runningCounts = wordCounts.updateStateByKey( (values: Seq[Int], state: Option[Int]) =&gt; Some(state.sum + values.sum))</strong></span>
</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note15"/>Note</h3><p>The <code class="literal">updateStateByKey</code> operation returns a new "state" DStream where the state for each key is updated by applying the given function on the previous state of the key and the new values for the key. This can be used to maintain arbitrary state data for each key.</p><p>There are two steps involved in making this operation work:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Define the state</li><li class="listitem" style="list-style-type: disc">Define the state <code class="literal">update</code> function</li></ul></div><p>The <code class="literal">updateStateByKey</code> operation is called once for each key, values represent the sequence of values associated with that key, very much like MapReduce and the state can be any arbitrary state, which we chose to make <code class="literal">Option[Int]</code>. With every call in step 18, the previous state gets updated by adding the sum of current values to it.</p></div></div></li><li class="listitem">Print the<a id="id317" class="indexterm"/> results:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; runningCounts.print</strong></span>
</pre></div></li><li class="listitem">The following <a id="id318" class="indexterm"/>are all the steps combined to maintain the arbitrary state using the <code class="literal">updateStateByKey</code> operation:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Scala&gt; :paste</strong></span>
<span class="strong"><strong>import org.apache.spark.streaming.{Seconds, StreamingContext}</strong></span>
<span class="strong"><strong> import org.apache.spark._</strong></span>
<span class="strong"><strong> import org.apache.spark.streaming._</strong></span>
<span class="strong"><strong> import org.apache.spark.streaming.kafka._</strong></span>
<span class="strong"><strong> import org.apache.spark.streaming.StreamingContext._</strong></span>
<span class="strong"><strong> val ssc = new StreamingContext(sc, Seconds(2))</strong></span>
<span class="strong"><strong> val zkQuorum = "localhost:2181"</strong></span>
<span class="strong"><strong> val group = "test-group"</strong></span>
<span class="strong"><strong> val topics = "test"</strong></span>
<span class="strong"><strong> val numThreads = 1</strong></span>
<span class="strong"><strong> val topicMap = topics.split(",").map((_,numThreads.toInt)).toMap</strong></span>
<span class="strong"><strong> val lineMap = KafkaUtils.createStream(ssc, zkQuorum, group, topicMap)</strong></span>
<span class="strong"><strong> val lines = lineMap.map(_._2)</strong></span>
<span class="strong"><strong> val words = lines.flatMap(_.split(" "))</strong></span>
<span class="strong"><strong> val pairs = words.map(x =&gt; (x,1))</strong></span>
<span class="strong"><strong> val runningCounts = pairs.updateStateByKey( (values: Seq[Int], state: Option[Int]) =&gt; Some(state.sum + values.sum))</strong></span>
<span class="strong"><strong> runningCounts.print</strong></span>
<span class="strong"><strong>ssc.checkpoint("hdfs://localhost:9000/user/hduser/checkpoint")</strong></span>
<span class="strong"><strong> ssc.start</strong></span>
<span class="strong"><strong> ssc.awaitTermination</strong></span>
</pre></div></li><li class="listitem">Run it by pressing <span class="emphasis"><em>Ctrl</em></span> + <span class="emphasis"><em>D</em></span> (which executes the code pasted using <code class="literal">:paste</code>).</li></ol></div></div></div></body></html>