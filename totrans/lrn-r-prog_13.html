<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch13"/>Chapter 13. High-Performance Computing</h1></div></div></div><p>In the previous chapter, you learned about a number of built-in functions and various packages tailored for data manipulation. Although these packages rely on different techniques and may be built under a different philosophy, they all make data filtering and aggregating much easier.</p><p>However, data processing is more than simple filtering and aggregating. Sometimes, it involves simulation and other computationintensive tasks. Compared to high-performance programming languages such as C and C++, R is much slower due to its dynamic design and the current implementation that prioritizes stability, ease, and power in statistical analysis and visualization over performance and language features. However, well-written R code can still be fast enough for most purposes.</p><p>In this chapter, I'll demonstrate the following techniques to help you write R code with high performance:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Measuring code performance</li><li class="listitem" style="list-style-type: disc">Profiling code to find bottleneck</li><li class="listitem" style="list-style-type: disc">Using built-in functions and vectorization</li><li class="listitem" style="list-style-type: disc">Using multiple cores by parallel computing</li><li class="listitem" style="list-style-type: disc">Writing C++ with Rcpp and related packages</li></ul></div><div><div><div><div><h1 class="title"><a id="ch13lvl1sec67"/>Understanding code performance issues</h1></div></div></div><p>From the very beginning, R is designed for statistical computing and data visualization and is widely used by academia and industry. For most data analysis purposes, correctness is more important than performance. In other words, getting a correct result in 1 minute should be better than getting an incorrect one in 20 seconds. A result that is three times faster is not automatically three times more valid than a slow but correct result. Therefore, performance should not be a concern before you are sure about the correctness of your code.</p><p>Let's assume that you are 100 percent sure that your code is correct but it runs a bit slowly. Now, is it necessary for you to optimize the code so that it can run faster. Well, it depends. Before making a decision, it is helpful to divide the time of problem solving into three parts: time of development, execution, and future maintenance.</p><p>Suppose we have been working on a problem for an hour. Since we didn't take performance into account at the beginning, the code does not run very fast. It takes us 50 minutes to think about the problem and implement the solution. Then, it takes 1 minute to run and produce an answer. Since the code aligns well with the problem and looks straightforward, future improvements can be easily integrated into the solution, so it takes us less time to maintain.</p><p>Then, suppose another developer has been working on the same problem but attempts to write an extremely high-performance code at the beginning. It takes time to work out a solution to the problem, but takes much more time to optimize the structure of the code so that it can run faster. It may take two hours to think of and implement a high-performance solution. Then, it takes 0.1 second to run and produce an answer. Since the code is particularly optimized to squeeze the hardware, it is probably not flexible for future refinement, especially when the problem is updated, which would cost more time to maintain.</p><p>The second developer can happily claim that her code has 600 times the performance of our code, but it may not be worth doing so because it may cost much more human time. In many cases, human time is more expensive than computer time.</p><p>However, if the code is frequently used, say, if billions of iterations are required, a small improvement of performance of each iteration can help save a large amount of time. In this case, code performance really matters.</p><p>Let's take an example of a simple algorithm that produces a numeric vector of cumulative sum, that is, each element of the output vector is the sum of all previous elements of the input vector. The code will be examined in different contexts in the discussion that follows.</p><p>Although R provides a built-in function, <code class="literal">cumsum</code>, to do this, we will implement an R version at the moment to help understand performance issues. The algorithm is easy to implement as follows:</p><pre class="programlisting">x &lt;- c(1, 2, 3, 4, 5) &#13;
y &lt;- numeric() &#13;
sum_x &lt;- 0 &#13;
for (xi in x) { &#13;
  sum_x &lt;- sum_x + xi &#13;
  y &lt;- c(y, sum_x) &#13;
} &#13;
y &#13;
## [1]  1  3  6 10 15 &#13;
</pre><p>The algorithm only uses a <code class="literal">for</code> loop to accumulate each element of the input vector <code class="literal">x</code> into <code class="literal">sum_x</code>. In each iteration, it appends <code class="literal">sum_x</code> to the output vector <code class="literal">y</code>. We can rewrite the algorithm as the following function:</p><pre class="programlisting">my_cumsum1 &lt;- function(x) { &#13;
  y &lt;- numeric() &#13;
  sum_x &lt;- 0 &#13;
  for (xi in x) { &#13;
    sum_x &lt;- sum_x + xi &#13;
    y &lt;- c(y, sum_x) &#13;
  } &#13;
  y &#13;
} &#13;
</pre><p>An alternative implementation is to use index to access the input vector <code class="literal">x</code> and access/modify the output vector <code class="literal">y</code>:</p><pre class="programlisting">my_cumsum2 &lt;- function(x) { &#13;
  y &lt;- numeric(length(x)) &#13;
  if (length(y)) { &#13;
    y[[1]] &lt;- x[[1]] &#13;
    for (i in 2:length(x)) { &#13;
      y[[i]] &lt;- y[[I - 1]] + x[[i]] &#13;
    } &#13;
  } &#13;
  y &#13;
} &#13;
</pre><p>We know that R provides a built-in function <code class="literal">cumsum()</code> to do exactly the same thing. The two preceding implementations should yield exactly the same results as <code class="literal">cumsum()</code>. Here, we will generate some random numbers and check whether they are consistent:</p><pre class="programlisting">x &lt;- rnorm(100) &#13;
all.equal(cumsum(x), my_cumsum1(x)) &#13;
## [1] TRUE &#13;
all.equal(cumsum(x), my_cumsum2(x)) &#13;
## [1] TRUE &#13;
</pre><p>In the preceding code, <code class="literal">all.equal()</code> checks whether all corresponding elements of two vectors are equal. From the results, we are sure that <code class="literal">my_cumsum1()</code>, <code class="literal">my_cumsum2()</code> and <code class="literal">cumsum()</code> are consistent. In the next section, we'll measure the time required for each version of <code class="literal">cumsum</code>.</p><div><div><div><div><h2 class="title"><a id="ch13lvl2sec145"/>Measuring code performance</h2></div></div></div><p>Although the three functions will output the same results given the same input, their performance difference can be quite obvious. To reveal the difference in performance, we need tools to measure the execution time of code. The simplest one is <code class="literal">system.time()</code>.</p><p>To measure the execution time of any expression, we just wrap the code with the function. Here, we will measure how much time it takes by <code class="literal">my_cumsum1()</code> to compute over a numeric vector of 100 elements:</p><pre class="programlisting">x &lt;- rnorm(100) &#13;
system.time(my_cumsum1(x)) &#13;
##    user  system elapsed  &#13;
##       0       0       0 &#13;
</pre><p>The timer results in three columns: <code class="literal">user</code>, <code class="literal">system</code>, and <code class="literal">elapsed</code>. It is the user time that we should pay more attention to. It measures the CPU time charged for executing the code. For more details, run <code class="literal">?proc.time</code> and see the difference between these measures.</p><p>The results suggest that the code simply runs too fast to measure. We can try timing <code class="literal">my_cumsum2()</code>, and the results are mostly the same:</p><pre class="programlisting">system.time(my_cumsum2(x)) &#13;
##    user  system elapsed  &#13;
##   0.000   0.000   0.001 &#13;
</pre><p>The same thing happens with the built-in function <code class="literal">cumsum()</code> too:</p><pre class="programlisting">system.time(cumsum(x)) &#13;
##    user  system elapsed  &#13;
##       0       0       0 &#13;
</pre><p>The timing does not really work because the input is too small. Now, we will generate a vector of <code class="literal">1000</code> numbers and do it again:</p><pre class="programlisting">x &lt;- rnorm(1000) &#13;
system.time(my_cumsum1(x)) &#13;
##    user  system elapsed  &#13;
##   0.000   0.000   0.003 &#13;
system.time(my_cumsum2(x)) &#13;
##    user  system elapsed  &#13;
##   0.004   0.000   0.001 &#13;
system.time(cumsum(x)) &#13;
##    user  system elapsed  &#13;
##       0       0       0 &#13;
</pre><p>Now, we are sure that <code class="literal">my_cumsum1()</code> and <code class="literal">my_cumsum2()</code> indeed take some time to compute the results but show no remarkable contrast. However, <code class="literal">cumsum()</code> is still too fast to measure.</p><p>We will again use a larger input for all three functions and see whether their performance difference can be revealed:</p><pre class="programlisting">x &lt;- rnorm(10000) &#13;
system.time(my_cumsum1(x)) &#13;
##    user  system elapsed  &#13;
##   0.208   0.000   0.211 &#13;
system.time(my_cumsum2(x)) &#13;
##    user  system elapsed  &#13;
##   0.012   0.004   0.013 &#13;
system.time(cumsum(x)) &#13;
##    user  system elapsed  &#13;
##       0       0       0 &#13;
</pre><p>The result is quite clear: <code class="literal">my_cumsum1()</code> looks more than 10 times slower than <code class="literal">my_cumsum2()</code>, and <code class="literal">cumsum()</code> is still way too fast than both our implementations.</p><p>Note that the performance difference may not be constant, especially when we provide even larger inputs as follows:</p><pre class="programlisting">x &lt;- rnorm(100000) &#13;
system.time(my_cumsum1(x)) &#13;
##    user  system elapsed  &#13;
##  25.732   0.964  26.699 &#13;
system.time(my_cumsum2(x)) &#13;
##    user  system elapsed  &#13;
##   0.124   0.000   0.123 &#13;
system.time(cumsum(x)) &#13;
##    user  system elapsed  &#13;
##       0       0       0 &#13;
</pre><p>The preceding results make quite an astonishing contrast: <code class="literal">my_cumsum1()</code> can be 200 times slower than <code class="literal">my_cumsum2()</code> when the length of input vector is at 100,000 level. The <code class="literal">cumsum()</code> function is consistently super fast in all previous results.</p><p>The <code class="literal">system.time()</code> function can help measure the execution time of a code chunk, but it is not very accurate. On the one hand, each time, the measure can result in different values so that we should repeat the timing for enough times to make a valid comparison. On the other hand, the resolution of the timer may not be high enough to address the real difference in performance of the code of interest.</p><p>A package named <code class="literal">microbenchmark</code> serves as a more accurate solution to comparing the performance of different expressions. To install the package, run the following code:</p><pre class="programlisting">install.packages("microbenchmark") &#13;
</pre><p>When the package is ready, we will load the package and call <code class="literal">microbenchmark()</code> to directly compare the performance of the three functions:</p><pre class="programlisting">library(microbenchmark) &#13;
x &lt;- rnorm(100) &#13;
microbenchmark(my_cumsum1(x), my_cumsum2(x), cumsum(x)) &#13;
## Unit: nanoseconds &#13;
##           expr    min       lq      mean   median       uq &#13;
##  my_cumsum1(x)  58250  64732.5  68353.51  66396.0  71840.0 &#13;
##  my_cumsum2(x) 120150 127634.5 131042.40 130739.5 133287.5 &#13;
##      cumsum(x)    295    376.5    593.47    440.5    537.5 &#13;
##     max neval cld &#13;
##   88228   100  b  &#13;
##  152845   100   c &#13;
##    7182   100 a &#13;
</pre><p>Note that <code class="literal">microbenchmark()</code>, by default, runs each expression 100 times so that it can provide more quantiles of the execution time. Maybe to your surprise, <code class="literal">my_cumsum1()</code> is a bit faster than <code class="literal">my_cumsum2()</code> when the input vector has 100 elements. Also, note that the unit of the time numbers is nanoseconds (1 second is 1,000,000,000 nanoseconds).</p><p>Then, we will try an input of <code class="literal">1000</code> numbers:</p><pre class="programlisting">x &lt;- rnorm(1000) &#13;
microbenchmark(my_cumsum1(x), my_cumsum2(x), cumsum(x)) &#13;
## Unit: microseconds &#13;
##           expr      min        lq       mean    median &#13;
##  my_cumsum1(x) 1600.186 1620.5190 2238.67494 1667.5605 &#13;
##  my_cumsum2(x) 1034.973 1068.4600 1145.00544 1088.4090 &#13;
##      cumsum(x)    1.806    2.1505    3.43945    3.4405 &#13;
##         uq      max neval cld &#13;
##  3142.4610 3750.516   100   c &#13;
##  1116.2280 2596.908   100  b  &#13;
##     4.0415   11.007   100 a &#13;
</pre><p>Now, <code class="literal">my_cumsum2()</code> gets a bit faster than <code class="literal">my_cumsum1()</code>, but both are way slower than the built-in <code class="literal">cumsum()</code>. Note that the unit becomes microseconds now.</p><p>For input of <code class="literal">5000</code> numbers, the performance difference between <code class="literal">my_cumsum1()</code> and <code class="literal">my_cumsum2()</code> gets even greater:</p><pre class="programlisting">x &lt;- rnorm(5000) &#13;
microbenchmark(my_cumsum1(x), my_cumsum2(x), cumsum(x)) &#13;
## Unit: microseconds &#13;
##           expr       min        lq        mean     median &#13;
##  my_cumsum1(x) 42646.201 44043.050 51715.59988 44808.9745 &#13;
##  my_cumsum2(x)  5291.242  5364.568  5718.19744  5422.8950 &#13;
##      cumsum(x)    10.183    11.565    14.52506    14.6765 &#13;
##         uq        max neval cld &#13;
##  46153.351 135805.947   100   c &#13;
##   5794.821  10619.352   100  b  &#13;
##     15.536     37.202   100 a &#13;
</pre><p>The same thing happens with an input of <code class="literal">10000</code> elements:</p><pre class="programlisting">x &lt;- rnorm(10000) &#13;
microbenchmark(my_cumsum1(x), my_cumsum2(x), cumsum(x), times = 10) &#13;
## Unit: microseconds &#13;
##           expr        min         lq        mean     median &#13;
##  my_cumsum1(x) 169609.730 170687.964 198782.7958 173248.004 &#13;
##  my_cumsum2(x)  10682.121  10724.513  11278.0974  10813.395 &#13;
##      cumsum(x)     20.744     25.627     26.0943     26.544 &#13;
##         uq        max neval cld &#13;
##  253662.89 264469.677    10   b &#13;
##   11588.99  13487.812    10  a  &#13;
##      27.64     29.163    10  a &#13;
</pre><p>In all previous benchmarks, the performance of <code class="literal">cumsum()</code> looks very stable and does not increase significantly as the length of input increases.</p><p>To better understand the performance dynamics of the three functions, we will create the following function to visualize how they perform, provided an input of different lengths:</p><pre class="programlisting">library(data.table) &#13;
benchmark &lt;- function(ns, times = 30) { &#13;
  results &lt;- lapply(ns, function(n) { &#13;
    x &lt;- rnorm(n) &#13;
    result &lt;- microbenchmark(my_cumsum1(x), my_cumsum2(x), cumsum(x),  &#13;
times = times, unit = "ms") &#13;
    data &lt;- setDT(summary(result)) &#13;
    data[, n := n] &#13;
    data &#13;
  }) &#13;
rbindlist(results) &#13;
} &#13;
</pre><p>The logic of the function is straightforward: <code class="literal">ns</code> is a vector of all lengths of input vectors we want to test with these functions. Note that <code class="literal">microbenchmark()</code> returns in a data frame of all tests results, and <code class="literal">summary(microbenchmark())</code> returns the summary table we saw previously. We tag each summary with <code class="literal">n</code>, stack all benchmark results, and use the <code class="literal">ggplot2</code> package to visualize the results.</p><p>First, we will do the benchmarking from <code class="literal">100</code> to <code class="literal">3000</code> elements of step <code class="literal">100</code>:</p><pre class="programlisting">benchmarks &lt;- benchmark(seq(100, 3000, 100)) &#13;
</pre><p>Then, we will create a plot to show contrast between the performance of the three functions:</p><pre class="programlisting">library(ggplot2) &#13;
ggplot(benchmarks, aes(x = n, color = expr)) + &#13;
  ggtitle("Microbenchmark on cumsum functions") + &#13;
  geom_point(aes(y = median)) + &#13;
  geom_errorbar(aes(ymin = lq, ymax = uq)) &#13;
</pre><p>This produces the following benchmarks of the three versions of <code class="literal">cumsum</code> we intend to compare:</p><div><img src="img/image_13_001.jpg" alt="Measuring code performance"/></div><p>In the preceding chart, we put together the results of all three functions. The dots indicate the median, and the error bar shows the 75<sup>th</sup> and 25<sup>th</sup> quantile.</p><p>It is very clear that the performance of <code class="literal">my_cumsum1()</code> decreases faster for longer input, the performance of <code class="literal">my_cumsum2()</code> almost decreases linearly as the input gets longer, while <code class="literal">cumsum(x)</code> is extremely fast and its performance does not seem to decay significantly as the input gets longer.</p><p>For small input, <code class="literal">my_cumsum1()</code> can be faster than <code class="literal">my_cumsum2()</code>, as we demonstrated earlier. We can do a benchmarking that focuses more on small input:</p><pre class="programlisting">benchmarks2 &lt;- benchmark(seq(2, 600, 10), times = 50) &#13;
</pre><p>This time, we will limit the length of input vector from <code class="literal">2</code> to <code class="literal">500</code> of stop <code class="literal">10</code>. Since the functions will be executed almost twice the number of times than the previous benchmarking, to keep the total execution time down, we will reduce <code class="literal">times</code> from the default <code class="literal">100</code> to <code class="literal">50</code>:</p><pre class="programlisting">ggplot(benchmarks2, aes(x = n, color = expr)) + &#13;
  ggtitle("Microbenchmark on cumsum functions over small input") + &#13;
  geom_point(aes(y = median)) + &#13;
  geom_errorbar(aes(ymin = lq, ymax = uq)) &#13;
</pre><p>The following graphics illustrates the performance difference at smaller inputs:</p><div><img src="img/image_13_002.jpg" alt="Measuring code performance"/></div><p>From the chart, we can see that for small input of less than around <code class="literal">400</code> numbers, <code class="literal">my_cumsum1()</code> is faster than <code class="literal">my_cumsum2()</code>. The performance of <code class="literal">my_cumsum1()</code> decays much faster than <code class="literal">my_cumsum2()</code> as the input gets more elements.</p><p>The dynamics of performance ranking can be better illustrated by a benchmarking of input from <code class="literal">10</code> to <code class="literal">800</code> elements:</p><pre class="programlisting">benchmarks3 &lt;- benchmark(seq(10, 800, 10), times = 50) &#13;
ggplot(benchmarks3, aes(x = n, color = expr)) + &#13;
  ggtitle("Microbenchmark on cumsum functions with break even") + &#13;
  geom_point(aes(y = median)) + &#13;
  geom_errorbar(aes(ymin = lq, ymax = uq)) &#13;
</pre><p>The plot generated is shown as follows:</p><div><img src="img/image_13_003.jpg" alt="Measuring code performance"/></div><p>In conclusion, a small difference in implementation may result in big performance gaps. For a small input, the gap is usually not obvious, but when the input gets larger, the performance difference can be very significant and thus should not be ignored. To compare the performance of multiple expressions, we can use <code class="literal">microbenchmark</code> instead of <code class="literal">system.time()</code> to get more accurate and more useful results.</p></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch13lvl1sec68"/>Profiling code</h1></div></div></div><p>In the previous section, you learned how to use <code class="literal">microbenchmark()</code> to benchmark expressions. This can be useful when we have several alternative solutions to a problem and want to see which has better performance and when we optimize an expression and want to see whether the performance actually gets better than the original code.</p><p>However, it is usually the case that, when we feel the code is slow, it is not easy to locate the expression that contributes most to slowing down the entire program. Such an expression is called a "performance bottleneck." To improve code performance, it is best to resolve the bottleneck first.</p><p>Fortunately, R provides profiling tools to help us find the bottleneck, that is, the code that runs most slowly, which should be the top focus for improving code performance.</p><div><div><div><div><h2 class="title"><a id="ch13lvl2sec146"/>Profiling code with Rprof</h2></div></div></div><p>R provides a built-in function, <code class="literal">Rprof()</code>, for code profiling. When profiling starts, a sampling procedure is running with all subsequent code until the profiling is ended. The sampling basically looks at which function R is executing every 20 milliseconds by default. In this way, if a function is very slow, it is likely that most of the execution time is spent on that function call.</p><p>The sampling approach may not produce very accurate results, but it serves our purpose in most cases. In the following example, we will use <code class="literal">Rprof()</code> to profile the code in which we call <code class="literal">my_cumsum1()</code> and try to find which part slows down the code.</p><p>The way of using <code class="literal">Rprof()</code> is very simple: call <code class="literal">Rprof()</code> to start profiling, run the code you want to profile, call <code class="literal">Rprof(NULL)</code> to stop profiling, and finally call <code class="literal">summaryRprof()</code> to see the profiling summary:</p><pre class="programlisting">x &lt;- rnorm(1000) &#13;
tmp &lt;- tempfile(fileext = ".out") &#13;
Rprof(tmp) &#13;
for (i in 1:1000) { &#13;
  my_cumsum1(x) &#13;
} &#13;
Rprof(NULL) &#13;
summaryRprof(tmp) &#13;
## $by.self &#13;
##              self.time self.pct total.time total.pct &#13;
## "c"               2.42    82.88       2.42     82.88 &#13;
## "my_cumsum1"      0.46    15.75       2.92    100.00 &#13;
## "+"               0.04     1.37       0.04      1.37 &#13;
## $by.total &#13;
##              total.time total.pct self.time self.pct &#13;
## "my_cumsum1"       2.92    100.00      0.46    15.75 &#13;
## "c"                2.42     82.88      2.42    82.88 &#13;
## "+"                0.04      1.37      0.04     1.37 &#13;
##  &#13;
## $sample.interval &#13;
## [1] 0.02 &#13;
##  &#13;
## $sampling.time &#13;
## [1] 2.92 &#13;
</pre><p>Note that we used <code class="literal">tempfile()</code> to create a temporary file to store profiling data. If we don't supply such a file to <code class="literal">Rprof()</code>, it will automatically create <code class="literal">Rprof.out</code> in the current working directory. The default also applies to <code class="literal">summaryRprof()</code>.</p><p>The profiling results summarize the profiling data into a readable format: <code class="literal">$by.self</code> sorts the timing by <code class="literal">self.time</code>, while <code class="literal">$by.total</code> sorts by <code class="literal">total.time</code>. More specifically, the <code class="literal">self.time</code> of a function is the time spent executing code in the function only, and the <code class="literal">total.time</code> of a function is the total execution time of the function.</p><p>To figure out which part slows down the function, we should pay more attention to <code class="literal">self.time</code> because it addresses the independent time of execution of each function.</p><p>The preceding profiling results show that <code class="literal">c</code> takes up a major part of the execution time, that is,
<code class="literal">y &lt;- c(y, sum_x)</code> contributes most to slowing down the function.</p><p>We can do the same thing to <code class="literal">my_cumsum2()</code>. The profiling results suggest that most time is spent on <code class="literal">my_cumsum2()</code>, but that is normal because that's the only thing we do in the code. No particular function in <code class="literal">my_cumsum2()</code> takes up major part of time to execute:</p><pre class="programlisting">tmp &lt;- tempfile(fileext = ".out") &#13;
Rprof(tmp) &#13;
for (i in 1:1000) { &#13;
  my_cumsum2(x) &#13;
} &#13;
Rprof(NULL) &#13;
summaryRprof(tmp) &#13;
## $by.self &#13;
##              self.time self.pct total.time total.pct &#13;
## "my_cumsum2"      1.42    97.26       1.46    100.00 &#13;
## "-"               0.04     2.74       0.04      2.74 &#13;
##  &#13;
## $by.total &#13;
##              total.time total.pct self.time self.pct &#13;
## "my_cumsum2"       1.46    100.00      1.42    97.26 &#13;
## "-"                0.04      2.74      0.04     2.74 &#13;
##  &#13;
## $sample.interval &#13;
## [1] 0.02 &#13;
##  &#13;
## $sampling.time &#13;
## [1] 1.46 &#13;
</pre><p>In a practical situation, the code we want to profile is usually complicated enough. It may involve many different functions. Such a profiling summary can be less helpful if we only see the timing of each function it tracks. Fortunately, <code class="literal">Rprof()</code> supports line profiling, that is, it can tell us the timing of each line of code when we specify <code class="literal">line.profiling = TRUE</code> and use <code class="literal">source(..., keep.source = TRUE)</code>.</p><p>We will create a script file at <code class="literal">code/my_cumsum1.R</code> with the following code:</p><pre class="programlisting">my_cumsum1 &lt;- function(x) {&#13;
   y &lt;- numeric()&#13;
   sum_x &lt;- 0&#13;
   for (xi in x) {&#13;
     sum_x &lt;- sum_x + xi&#13;
     y &lt;- c(y, sum_x)&#13;
   }&#13;
   y&#13;
 }&#13;
 &#13;
 x &lt;- rnorm(1000)&#13;
 &#13;
 for (i in 1:1000) {&#13;
   my_cumsum1(x)&#13;
 }</pre><p>Then, we will profile this script file with <code class="literal">Rprof() </code>and <code class="literal">source()</code>:</p><pre class="programlisting">tmp &lt;- tempfile(fileext = ".out") &#13;
Rprof(tmp, line.profiling = TRUE) &#13;
source("code/my_cumsum1.R", keep.source = TRUE) &#13;
Rprof(NULL) &#13;
summaryRprof(tmp, lines = "show") &#13;
## $by.self &#13;
##                self.time self.pct total.time total.pct &#13;
## my_cumsum1.R#6      2.38    88.15       2.38     88.15 &#13;
## my_cumsum1.R#5      0.26     9.63       0.26      9.63 &#13;
## my_cumsum1.R#4      0.06     2.22       0.06      2.22 &#13;
##  &#13;
## $by.total &#13;
##                 total.time total.pct self.time self.pct &#13;
## my_cumsum1.R#14       2.70    100.00      0.00     0.00 &#13;
## my_cumsum1.R#6        2.38     88.15      2.38    88.15 &#13;
## my_cumsum1.R#5        0.26      9.63      0.26     9.63 &#13;
## my_cumsum1.R#4        0.06      2.22      0.06     2.22 &#13;
##  &#13;
## $by.line &#13;
##                 self.time self.pct total.time total.pct &#13;
## my_cumsum1.R#4       0.06     2.22       0.06      2.22 &#13;
## my_cumsum1.R#5       0.26     9.63       0.26      9.63 &#13;
## my_cumsum1.R#6       2.38    88.15       2.38     88.15 &#13;
## my_cumsum1.R#14      0.00     0.00       2.70    100.00 &#13;
##  &#13;
## $sample.interval &#13;
## [1] 0.02 &#13;
##  &#13;
## $sampling.time &#13;
## [1] 2.7 &#13;
</pre><p>This time, it no longer shows function names but line numbers in the script file. We can easily locate the lines that cost most time by looking at the top rows in <code class="literal">$by.self</code>. The <code class="literal">my_cumsum1.R#6</code> file refers to <code class="literal">y &lt;- c(y, sum_x)</code>, which is consistent with the previous profiling results.</p></div><div><div><div><div><h2 class="title"><a id="ch13lvl2sec147"/>Profiling code with profvis</h2></div></div></div><p>The <code class="literal">Rprof()</code> function provides useful information to help us find which part of the code is too slow so that we can improve the implementation. RStudio also released an enhanced profiling tool, <code class="literal">profvis</code> (<a class="ulink" href="https://rstudio.github.io/profvis/">https://rstudio.github.io/profvis/</a>), which provides interactive visualization for profiling R code.</p><p>It is an R package and has been integrated into RStudio. To install the package, run the following code:</p><pre class="programlisting">install.packages("profvis") &#13;
</pre><p>As soon as the package is installed, we can use <code class="literal">profvis</code> to profile an expression and visualize the results:</p><pre class="programlisting">library(profvis)&#13;
profvis({&#13;
  my_cumsum1 &lt;- function(x) {&#13;
    y &lt;- numeric()&#13;
    sum_x &lt;- 0&#13;
    for (xi in x) {&#13;
      sum_x &lt;- sum_x + xi&#13;
      y &lt;- c(y, sum_x)&#13;
    }&#13;
    y&#13;
  }&#13;
  &#13;
  x &lt;- rnorm(1000)&#13;
  &#13;
  for (i in 1:1000) {&#13;
    my_cumsum1(x)&#13;
  }&#13;
})</pre><p>When the profiling is finished, a new tab will appear with an interactive user interface:</p><div><img src="img/image_13_004.jpg" alt="Profiling code with profvis"/></div><p>The upper pane shows the code, memory usage, and timing, whereas the lower pane shows the timeline of function calling as well as when garbage collection occurs. We can click and select a certain line of code and see the timeline of function execution. Compared with the results produced by <code class="literal">summaryRprof()</code>, this interactive visualization provides much richer information that enables us to know more about how the code is executed over a long time. In this way, we can easily identify the slow code and some patterns that may induce problems.</p><p>We can do exactly the same thing with <code class="literal">my_cumsum2()</code>:</p><pre class="programlisting">profvis({&#13;
  my_cumsum2 &lt;- function(x) {&#13;
    y &lt;- numeric(length(x))&#13;
    y[[1]] &lt;- x[[1]]&#13;
    for (i in 2:length(x)) {&#13;
      y[[i]] &lt;- y[[i-1]] + x[[i]]&#13;
    }&#13;
    y&#13;
  }&#13;
  &#13;
  x &lt;- rnorm(1000)&#13;
  &#13;
  for (i in 1:1000) {&#13;
    my_cumsum2(x)&#13;
  }&#13;
})</pre><p>This time, the profiling results in the following statistics:</p><div><img src="img/image_13_005.jpg" alt="Profiling code with profvis"/></div><p>We can easily identify which part takes the most time and decide whether it is acceptable. In all code, there is always a part that takes most time, but this does not indicate that it is too slow. If the code serves our purpose and the performance is acceptable, then there may not be a need to optimize the performance at the risk of modifying the code into an incorrect version.</p></div><div><div><div><div><h2 class="title"><a id="ch13lvl2sec148"/>Understanding why code can be slow</h2></div></div></div><p>In the previous sections, you learned about the tools for timing and profiling code. To solve the same problem, one function can be blazing fast, and the other can be ridiculously slow. It is helpful to understand what can make code slow.</p><p>First, R is a dynamic programming language. By design, it provides highly flexible data structures and code-execution mechanisms. Therefore, it is hard for the code interpreter to know in advance how to deal with the next function call until it is actually called. This is not the case for strong-typed static programming languages such as C and C++. Many things are determined at compile time rather than runtime, so the program knows a lot ahead of time, and optimization can be intensively performed. By contrast, R trades flexibility for performance, but well-written R code can exhibit acceptable, if not good, performance.</p><p>The top reason why R code can be slow is that our code may intensively create, allocate, or copy data structures. This is exactly why <code class="literal">my_cumsum1()</code> and <code class="literal">my_cumsum2()</code> show great difference in performance when the input gets longer. The <code class="literal">my_cumsum1()</code> function always grows the vector, which means that in each iteration the vector is copied to a new address and a new element is appended. As a result, the more iterations we have, the more elements it has to copy, and then the code gets slower.</p><p>This can be made explicit by the following benchmarking: <code class="literal">grow_by_index</code> means we initialize an empty list. The <code class="literal">preallocated</code> function means we initialize a list with pre-allocated positions, that is, a list of <code class="literal">n NULL</code> values with all positions allocated. In both cases, we modify the <code class="literal">i</code><sup>th</sup> element of the list, but the difference is that we'll grow the first list in each iteration, and this does not happen with the second list because it is already fully allocated:</p><pre class="programlisting">n &lt;- 10000 &#13;
microbenchmark(grow_by_index = { &#13;
  x &lt;- list() &#13;
  for (i in 1:n) x[[i]] &lt;- i &#13;
}, preallocated = { &#13;
  x &lt;- vector("list", n) &#13;
  for (i in 1:n) x[[i]] &lt;- i &#13;
}, times = 20) &#13;
## Unit: milliseconds &#13;
##           expr        min         lq       mean     median &#13;
##  grow_by_index 258.584783 261.639465 299.781601 263.896162 &#13;
##   preallocated   7.151352   7.222043   7.372342   7.257661 &#13;
##          uq        max neval cld &#13;
##  351.887538 375.447134    20   b &#13;
##    7.382103   8.612665    20  a &#13;
</pre><p>The results are clear: intensively growing a list can significantly slow down the code, while modifying a pre-allocated list within range is fast. The same logic also applies to atomic vectors and matrices. Growing a data structure in R is generally slow because it triggers reallocation, that is, copying the original data structure to a new memory address. This is very expensive in R, especially when the data is large.</p><p>However, accurate pre-allocation is not always feasible because it requires that we know the total number prior to the iteration. Sometimes, we can only ask for a result to store repeatedly without knowing the exact total number. In this case, maybe it is still a good idea to pre-allocate a list or vector with a reasonable length. When the iteration is over, if the number of iterations does not reach the pre-allocated length, we can take a subset of the list or vector. In this way, we can avoid intensive reallocation of data structures.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch13lvl1sec69"/>Boosting code performance</h1></div></div></div><p>In the previous section, we demonstrated how to use profiling tools to identify a performance bottleneck in the code. In this section, you will learn about a number of approaches to boosting code performance.</p><div><div><div><div><h2 class="title"><a id="ch13lvl2sec149"/>Using built-in functions</h2></div></div></div><p>Previously, we demonstrated the performance difference between <code class="literal">my_cumsum1()</code>, <code class="literal">my_cumsum2()</code> and the built-in function <code class="literal">cumsum()</code>. Although <code class="literal">my_cumsum2()</code> is faster than <code class="literal">my_cumsum1()</code>, when the input vector contains many numbers, <code class="literal">cumsum()</code> is much faster than them. Also, its performance does not decay significantly even as the input gets longer. If we evaluate <code class="literal">cumsum</code>, we can see that it is a primitive function:</p><pre class="programlisting">cumsum &#13;
## function (x)  .Primitive("cumsum") &#13;
</pre><p>A primitive function in R is implemented in C/C++/Fortran, compiled to native instructions, and thus, is extremely efficient. Another example is <code class="literal">diff()</code>. Here, we will implement computing vector difference sequence in R:</p><pre class="programlisting">diff_for &lt;- function(x) { &#13;
  n &lt;- length(x) - 1 &#13;
  res &lt;- numeric(n) &#13;
  for (i in seq_len(n)) { &#13;
    res[[i]] &lt;- x[[i + 1]] - x[[i]] &#13;
  } &#13;
  res &#13;
} &#13;
</pre><p>We can verify that the implementation is correct:</p><pre class="programlisting">diff_for(c(2, 3, 1, 5)) &#13;
## [1]  1 -2  4 &#13;
</pre><p>Therefore, both <code class="literal">diff_for()</code> and built-in <code class="literal">diff()</code> must return the same result for the same input:</p><pre class="programlisting">x &lt;- rnorm(1000) &#13;
all.equal(diff_for(x), diff(x)) &#13;
## [1] TRUE &#13;
</pre><p>However, there's a big gap in performance between the two functions.</p><pre class="programlisting">microbenchmark(diff_for(x), diff(x)) &#13;
## Unit: microseconds &#13;
##         expr      min        lq       mean    median &#13;
##  diff_for(x) 1034.028 1078.9075 1256.01075 1139.1270 &#13;
##      diff(x)   12.057   14.2535   21.72772   17.5705 &#13;
##         uq      max neval cld &#13;
##  1372.1145 2634.128   100   b &#13;
##    25.4525   75.850   100  a &#13;
</pre><p>Built-in functions are, in most cases, way faster than equivalent R implementations. This is true not only for vector functions, but also for matrices. For example, here is a simple 3 by 4 integer matrix:</p><pre class="programlisting">mat &lt;- matrix(1:12, nrow = 3) &#13;
mat &#13;
##      [,1] [,2] [,3] [,4] &#13;
## [1,]    1    4    7   10 &#13;
## [2,]    2    5    8   11 &#13;
## [3,]    3    6    9   12 &#13;
</pre><p>We can write a function to transpose the matrix:</p><pre class="programlisting">my_transpose &lt;- function(x) {&#13;
   stopifnot(is.matrix(x))&#13;
   res &lt;- matrix(vector(mode(x), length(x)), &#13;
     nrow = ncol(x), ncol = nrow(x), &#13;
     dimnames = dimnames(x)[c(2, 1)])&#13;
   for (i in seq_len(ncol(x))) {&#13;
     for (j in seq_len(nrow(x))) {&#13;
       res[i, j] &lt;- x[j, i]&#13;
     }&#13;
   }&#13;
   res&#13;
 }</pre><p>In the function, we will first create a matrix of the same type as the input, but with the number and names of rows and columns exchanged, respectively. Then, we will iterate over columns and rows to transpose the matrix:</p><pre class="programlisting">my_transpose(mat) &#13;
##      [,1] [,2] [,3] &#13;
## [1,]    1    2    3 &#13;
## [2,]    4    5    6 &#13;
## [3,]    7    8    9 &#13;
## [4,]   10   11   12 &#13;
</pre><p>The built-in function of matrix transpose is <code class="literal">t()</code>. We can easily verify that both functions return the same results:</p><pre class="programlisting">all.equal(my_transpose(mat), t(mat)) &#13;
## [1] TRUE &#13;
</pre><p>However, they may exhibit great difference in performance:</p><pre class="programlisting">microbenchmark(my_transpose(mat), t(mat)) &#13;
## Unit: microseconds &#13;
##               expr    min     lq     mean  median      uq &#13;
##  my_transpose(mat) 22.795 24.633 29.47941 26.0865 35.5055 &#13;
##             t(mat)  1.576  1.978  2.87349  2.3375  2.7695 &#13;
##     max neval cld &#13;
##  71.509   100   b &#13;
##  16.171   100  a &#13;
</pre><p>The performance difference gets even more significant when the input matrix is larger. Here, we will create a new matrix with <code class="literal">1000</code> rows and <code class="literal">25</code> columns. While the results are the same, the performance can be very different:</p><pre class="programlisting">mat &lt;- matrix(rnorm(25000), nrow = 1000) &#13;
all.equal(my_transpose(mat), t(mat)) &#13;
## [1] TRUE &#13;
microbenchmark(my_transpose(mat), t(mat)) &#13;
## Unit: microseconds &#13;
##               expr       min         lq      mean &#13;
##  my_transpose(mat) 21786.241 22456.3990 24466.055 &#13;
##             t(mat)    36.611    46.2045    61.047 &#13;
##      median        uq        max neval cld &#13;
##  23821.5905 24225.142 113395.811   100   b &#13;
##     57.7505    68.694    142.126   100   a &#13;
</pre><p>Note that <code class="literal">t()</code> is a generic function that works with both matrix and data frame. S3 dispatching to find the right method for the input, also has some overhead. Therefore, directly calling <code class="literal">t.default()</code> on a matrix is slightly faster:</p><pre class="programlisting">microbenchmark(my_transpose(mat), t(mat), t.default(mat)) &#13;
## Unit: microseconds &#13;
##               expr       min         lq        mean &#13;
##  my_transpose(mat) 21773.751 22498.6420 23673.26089 &#13;
##             t(mat)    37.853    48.8475    63.57713 &#13;
##     t.default(mat)    35.518    41.0305    52.97680 &#13;
##      median         uq       max neval cld &#13;
##  23848.6625 24139.7675 29034.267   100   b &#13;
##     61.3565    69.6655   140.061   100   a  &#13;
##     46.3095    54.0655   146.755   100   a &#13;
</pre><p>All previous examples show that, in most cases, it is much better to use built-in functions if provided than reinventing the wheel in R. These functions get rid of the overhead of R code and, thus, can be extremely efficient even if the input is huge.</p></div><div><div><div><div><h2 class="title"><a id="ch13lvl2sec150"/>Using vectorization</h2></div></div></div><p>A special subset of built-in functions are arithmetic operators such as <code class="literal">+</code>, <code class="literal">-</code>, <code class="literal">*</code>, <code class="literal">/</code>, <code class="literal">^</code>, and <code class="literal">%%</code>. These operators are not only extremely efficient but also vectorized.</p><p>Suppose we implement <code class="literal">+</code> in R:</p><pre class="programlisting">add &lt;- function(x, y) {&#13;
   stopifnot(length(x) == length(y),&#13;
     is.numeric(x), is.numeric(y))&#13;
   z &lt;- numeric(length(x))&#13;
   for (i in seq_along(x)) {&#13;
     z[[i]] &lt;- x[[i]] + y[[i]]&#13;
   }&#13;
   z&#13;
 }</pre><p>Then, we would randomly generate <code class="literal">x</code> and <code class="literal">y</code>. The <code class="literal">add(x, y)</code>, and <code class="literal">x + y</code> arguments should return exactly the same results:</p><pre class="programlisting">x &lt;- rnorm(10000) &#13;
y &lt;- rnorm(10000) &#13;
all.equal(add(x, y), x + y) &#13;
## [1] TRUE &#13;
</pre><p>The following benchmarking shows that the performance difference is huge:</p><pre class="programlisting">microbenchmark(add(x, y), x + y) &#13;
## Unit: microseconds &#13;
##       expr      min         lq        mean     median &#13;
##  add(x, y) 9815.495 10055.7045 11478.95003 10712.7710 &#13;
##      x + y   10.260    12.0345    17.31862    13.3995 &#13;
##         uq       max neval cld &#13;
##  12598.366 18754.504   100   b &#13;
##     22.208    56.969   100  a &#13;
</pre><p>Now, suppose we need to calculate the sum of the reciprocal of first <code class="literal">n</code> positive integers squared. We can easily implement the algorithm using a <code class="literal">for</code> loop as the following function <code class="literal">algo1_for</code>:</p><pre class="programlisting">algo1_for &lt;- function(n) { &#13;
  res &lt;- 0 &#13;
  for (i in seq_len(n)) { &#13;
    res &lt;- res + 1 /i ^ 2 &#13;
  } &#13;
  res &#13;
} &#13;
</pre><p>The function takes an input <code class="literal">n</code>, iterates <code class="literal">n</code> times to accumulate as supposed, and returns the result.</p><p>A better approach is to use vectorized calculation directly without any necessity of a <code class="literal">for</code> loop, just like how <code class="literal">algo1_vec()</code> is implemented:</p><pre class="programlisting">algo1_vec &lt;- function(n) { &#13;
  sum(1 / seq_len(n) ^ 2) &#13;
} &#13;
</pre><p>The two functions yield the same results, given an ordinary input:</p><pre class="programlisting">algo1_for(10) &#13;
## [1] 1.549768 &#13;
algo1_vec(10) &#13;
## [1] 1.549768 &#13;
</pre><p>However, their performance is very different:</p><pre class="programlisting">microbenchmark(algo1_for(200), algo1_vec(200)) &#13;
## Unit: microseconds &#13;
##            expr    min       lq      mean   median      uq &#13;
##  algo1_for(200) 91.727 101.2285 104.26857 103.6445 105.632 &#13;
##  algo1_vec(200)  2.465   2.8015   3.51926   3.0355   3.211 &#13;
##      max neval cld &#13;
##  206.295   100   b &#13;
##   19.426   100  a &#13;
microbenchmark(algo1_for(1000), algo1_vec(1000)) &#13;
## Unit: microseconds &#13;
##             expr     min       lq      mean  median &#13;
##  algo1_for(1000) 376.335 498.9320 516.63954 506.859 &#13;
##  algo1_vec(1000)   8.718   9.1175   9.82515   9.426 &#13;
##        uq      max neval cld &#13;
##  519.2420 1823.502   100   b &#13;
##    9.8955   20.564   100  a &#13;
</pre><p>Vectorization is a highly recommended way of writing R code. It is not only of high performance but, also makes the code easier to understand.</p></div><div><div><div><div><h2 class="title"><a id="ch13lvl2sec151"/>Using byte-code compiler</h2></div></div></div><p>In the previous section, we saw the power of vectorization. Sometimes, however, the problem dictates a for loop, and it is hard to vectorize the code. In this case, we may consider using R byte-code compiler to compile the function so that the function no longer needs parsing and may run faster.</p><p>First, we will load the compiler package, which is distributed along with R. We will use <code class="literal">cmpfun()</code> to compile a given R function. For example, we will compile <code class="literal">diff_for()</code> and store the compiled function as <code class="literal">diff_cmp()</code>:</p><pre class="programlisting">library(compiler) &#13;
diff_cmp &lt;- cmpfun(diff_for) &#13;
diff_cmp &#13;
## function(x) { &#13;
##   n &lt;- length(x) - 1 &#13;
##   res &lt;- numeric(n) &#13;
##   for (i in seq_len(n)) { &#13;
##     res[[i]] &lt;- x[[i + 1]] - x[[i]] &#13;
##   } &#13;
##   res &#13;
## } &#13;
## &lt;bytecode: 0x93aec08&gt; &#13;
</pre><p>When we look at <code class="literal">diff_cmp()</code>, it does not look very different from <code class="literal">diff_for()</code>, but it has an additional tag of the <code class="literal">bytecode</code> address.</p><p>Then, we will run the benchmarking again with <code class="literal">diff_cmp()</code> this time:</p><pre class="programlisting">x &lt;- rnorm(10000) &#13;
microbenchmark(diff_for(x), diff_cmp(x), diff(x)) &#13;
## Unit: microseconds &#13;
##         expr       min         lq       mean     median &#13;
##  diff_for(x) 10664.387 10940.0840 11684.3285 11357.9330 &#13;
##  diff_cmp(x)   732.110   740.7610   760.1985   751.0295 &#13;
##      diff(x)    80.824    91.2775   107.8473   103.8535 &#13;
##        uq       max neval cld &#13;
##  12179.98 16606.291   100   c &#13;
##    763.66  1015.234   100  b  &#13;
##    115.11   219.396   100 a &#13;
</pre><p>It looks amazing that the compiled version, <code class="literal">diff_cmp()</code>, is much faster than <code class="literal">diff_for()</code> even though we didn't modify anything but compiled it into bytecode.</p><p>Now, we will do the same thing with <code class="literal">algo1_for()</code>:</p><pre class="programlisting">algo1_cmp &lt;- cmpfun(algo1_for) &#13;
algo1_cmp &#13;
## function(n) { &#13;
##   res &lt;- 0 &#13;
##   for (i in seq_len(n)) { &#13;
##     res &lt;- res + 1 / i ^ 2 &#13;
##   } &#13;
##   res &#13;
## } &#13;
## &lt;bytecode: 0xa87e2a8&gt; &#13;
</pre><p>Then, we will conduct the benchmarking with the compiled version included:</p><pre class="programlisting">n &lt;- 1000 &#13;
microbenchmark(algo1_for(n), algo1_cmp(n), algo1_vec(n)) &#13;
## Unit: microseconds &#13;
##          expr     min       lq      mean   median       uq &#13;
##  algo1_for(n) 490.791 499.5295 509.46589 505.7560 517.5770 &#13;
##  algo1_cmp(n)  55.588  56.8355  58.10490  57.8270  58.7140 &#13;
##  algo1_vec(n)   8.688   9.2150   9.79685   9.4955   9.8895 &#13;
##      max neval cld &#13;
##  567.680   100   c &#13;
##   69.734   100  b  &#13;
##   19.765   100 a &#13;
</pre><p>Again, the compiled version becomes more than six times faster than the original version, even if we didn't change a bit of code.</p><p>However, compiling is no magic if it is used to compile a fully vectorized function. Here, we will compile <code class="literal">algo1_vec()</code> and compare its performance with the original version:</p><pre class="programlisting">algo1_vec_cmp &lt;- cmpfun(algo1_vec) &#13;
microbenchmark(algo1_vec(n), algo1_vec_cmp(n), times = 10000) &#13;
## Unit: microseconds &#13;
##              expr  min    lq      mean median    uq &#13;
##      algo1_vec(n) 8.47 8.678 20.454858  8.812 9.008 &#13;
##  algo1_vec_cmp(n) 8.35 8.560  9.701012  8.687 8.864 &#13;
##        max neval cld &#13;
##  96376.483 10000   a &#13;
##   1751.431 10000   a &#13;
</pre><p>Note that the compiled function shows no significant performance improvement. To know more about how the compiler works, type <code class="literal">?compile</code> and read the documentation.</p></div><div><div><div><div><h2 class="title"><a id="ch13lvl2sec152"/>Using Intel MKL-powered R distribution</h2></div></div></div><p>The R distribution we normally use is single threaded, that is, only one CPU thread is used to execute all R code. The good thing is that the execution model is simple and safe, but it does not take advantage of multicore computing.</p><p>Microsoft R Open (MRO, see <a class="ulink" href="https://mran.microsoft.com/open/">https://mran.microsoft.com/open/</a>) is an enhanced distribution of R. Powered by Intel Math Kernel Library (MKL, see <a class="ulink" href="https://software.intel.com/en-us/intel-mkl">https://software.intel.com/en-us/intel-mkl</a>), MRO enhances the matrix algorithms by automatically taking advantage of multithreading computation. On a multicore computer, MRO can be 10-80 times faster than the official R implementation at matrix multiplication, Cholesky factorization, QR decomposition, singular value decomposition, principal component analysis, and linear discriminant analysis. For more details, visit <a class="ulink" href="https://mran.microsoft.com/documents/rro/multithread/">https://mran.microsoft.com/documents/rro/multithread/</a> and see the benchmarking.</p></div><div><div><div><div><h2 class="title"><a id="ch13lvl2sec153"/>Using parallel computing</h2></div></div></div><p>As we mentioned in the previous section, R is single threaded in design but still allows multiprocessing parallel computing, that is, running multiple R sessions to compute. This technique is supported by a parallel library, which is also distributed along with R.</p><p>Suppose we need to do a simulation: we need to generate a random path that follows a certain random process and see whether at any point, the value goes beyond a fixed margin around the starting point.</p><p>The following code generates one realization:</p><pre class="programlisting">set.seed(1) &#13;
sim_data &lt;- 100 * cumprod(1 + rnorm(500, 0, 0.006)) &#13;
plot(sim_data, type = "s", ylim = c(85, 115), &#13;
  main = "A simulated random path") &#13;
abline(h = 100, lty = 2, col = "blue") &#13;
abline(h = 100 * (1 + 0.1 * c(1, -1)), lty = 3, col = "red") &#13;
</pre><p>The plot generated is shown as follows:</p><div><img src="img/image_13_006.jpg" alt="Using parallel computing"/></div><p>The preceding graph shows the path and 10 percent margin. It is clear that between index 300 and 500, the value goes beyond the upper margin multiple times.</p><p>This is just one path. A valid simulation requires that the generator run as many times as necessary to produce statistically meaningful results. The following function parameterizes the random path generator and returns a list of summary indicators of interest. Note that <code class="literal">signal</code> indicates whether any point on the path goes beyond the margin:</p><pre class="programlisting">simulate &lt;- function(i, p = 100, n = 10000, &#13;
   r = 0, sigma = 0.0005, margin = 0.1) {&#13;
   ps &lt;- p * cumprod(1 + rnorm(n, r, sigma))&#13;
   list(id = i, &#13;
     first = ps[[1]], &#13;
     high = max(ps), &#13;
     low = min(ps), &#13;
     last = ps[[n]],&#13;
     signal = any(ps &gt; p * (1 + margin) | ps &lt; p * (1 - margin)))&#13;
 }</pre><p>Then, we can run the generator for one time and see its summarized result:</p><pre class="programlisting">simulate(1) &#13;
## $id &#13;
## [1] 1 &#13;
##  &#13;
## $first &#13;
## [1] 100.0039 &#13;
##  &#13;
## $high &#13;
## [1] 101.4578 &#13;
##  &#13;
## $low &#13;
## [1] 94.15108 &#13;
##  &#13;
## $last &#13;
## [1] 96.13973 &#13;
##  &#13;
## $signal &#13;
## [1] FALSE &#13;
</pre><p>To perform the simulation, we need to run the function many times. In practice, we may need to run at least millions of realizations, which may take us a considerable amount of time. Here, we will measure how much time it costs to run ten thousand iterations of this simulation:</p><pre class="programlisting">system.time(res &lt;- lapply(1:10000, simulate)) &#13;
##    user  system elapsed  &#13;
##   8.768   0.000   8.768 &#13;
</pre><p>When the simulation is finished, we can convert all results into one data table:</p><pre class="programlisting">library(data.table) &#13;
res_table &lt;- rbindlist(res) &#13;
head(res_table) &#13;
##    id     first     high      low      last signal &#13;
## 1:  1 100.03526 100.7157 93.80330 100.55324  FALSE &#13;
## 2:  2 100.03014 104.7150 98.85049 101.97831  FALSE &#13;
## 3:  3  99.99356 104.9834 95.28500  95.59243  FALSE &#13;
## 4:  4  99.93058 103.4315 96.10691  97.22223  FALSE &#13;
## 5:  5  99.99785 100.6041 94.12958  95.97975  FALSE &#13;
## 6:  6 100.03235 102.1770 94.65729  96.49873  FALSE &#13;
</pre><p>We can calculate the realized probability of <code class="literal">signal == TRUE</code>:</p><pre class="programlisting">res_table[, sum(signal) /.N] &#13;
## [1] 0.0881 &#13;
</pre><p>What if the problem gets more practical and requires us to run millions of times? In this case, some researchers may turn to programming languages implemented with much higher performance such as C and C++, which are extremely efficient and flexible. They are great tools in implementing algorithms but require more effort to deal with the compiler, linker, and data input/output.</p><p>Note that each iteration in the preceding simulation is completely independent of each other, so it is better accomplished by parallel computing.</p><p>Since different operating systems have different implementations of process and threading model, some features that are available for Linux and MacOS are not available for Windows. Thus, performing parallel computing on Windows can be a bit more verbose.</p><div><div><div><div><h3 class="title"><a id="ch13lvl3sec70"/>Using parallel computing on Windows</h3></div></div></div><p>On Windows, we need to create a local cluster of multiple R sessions to run parallel computing:</p><pre class="programlisting">library(parallel) &#13;
cl &lt;- makeCluster(detectCores()) &#13;
</pre><p>The <code class="literal">detectCores()</code> function returns the number of cores your computer is equipped with. Creating a cluster of more than that number of nodes is allowed but usually does no good because your computer cannot perform more tasks than that simultaneously.</p><p>Then, we can call <code class="literal">parLapply()</code>, the parallel version of <code class="literal">lapply()</code>:</p><pre class="programlisting">system.time(res &lt;- parLapply(cl, 1:10000, simulate)) &#13;
##    user  system elapsed  &#13;
##   0.024   0.008   3.772 &#13;
</pre><p>Note that the time consumed is reduced to more than half of the original time. Now, we no longer need the cluster. We can call <code class="literal">stopCluster()</code> to kill the R sessions just created:</p><pre class="programlisting">stopCluster(cl) &#13;
</pre><p>When we call <code class="literal">parLapply()</code>, it automatically schedules the task for each cluster node. More specifically, all cluster nodes run <code class="literal">simulate()</code> with one of <code class="literal">1:10000</code> exclusively at the same time so that the computation is done in parallel. Finally, all results are collected so that we get a list just like the results from <code class="literal">lapply()</code>:</p><pre class="programlisting">length(res) &#13;
## [1] 10000 &#13;
res_table &lt;- rbindlist(res) &#13;
res_table[, sum(signal) /.N] &#13;
## [1] 0.0889 &#13;
</pre><p>The parallel code looks simple because <code class="literal">simulate()</code> is self-contained and does not rely on user-defined external variables or datasets. If we run a function in parallel that refers to a variable in the master session (the current session that creates the cluster), it will not find the variable:</p><pre class="programlisting">cl &lt;- makeCluster(detectCores()) &#13;
n &lt;- 1 &#13;
parLapply(cl, 1:3, function(x) x + n) &#13;
## Error in checkForRemoteErrors(val): 3 nodes produced errors; first error: object 'n' not found &#13;
stopCluster(cl) &#13;
</pre><p>All nodes fail because each of them starts as a fresh R session with no user variables defined. To let the cluster nodes get the value of the variable they need, we have to export them to all nodes.</p><p>The following example demonstrates how this works. Suppose we have a data frame of numbers. We want to take random samples from the data frame:</p><pre class="programlisting">n &lt;- 100 &#13;
data &lt;- data.frame(id = 1:n,  x = rnorm(n), y = rnorm(n)) &#13;
 &#13;
take_sample &lt;- function(n) { &#13;
  data[sample(seq_len(nrow(data)), &#13;
    size = n, replace = FALSE), ] &#13;
} &#13;
</pre><p>If we perform the sampling in parallel, all nodes must share the data frame and the function. To do this, we can use <code class="literal">clusterEvalQ()</code> to evaluate an expression on each cluster node. First, we will make a cluster just as we did earlier:</p><pre class="programlisting">cl &lt;- makeCluster(detectCores()) &#13;
</pre><p>The <code class="literal">Sys.getpid()</code> function returns the process ID of the current R session. Since there are four nodes in the cluster, each is an R session with a unique process ID. We can call <code class="literal">clusterEvalQ()</code> with <code class="literal">Sys.getpid()</code> and see the process ID of each node:</p><pre class="programlisting">clusterEvalQ(cl, Sys.getpid()) &#13;
## [[1]] &#13;
## [1] 20714 &#13;
##  &#13;
## [[2]] &#13;
## [1] 20723 &#13;
##  &#13;
## [[3]] &#13;
## [1] 20732 &#13;
##  &#13;
## [[4]] &#13;
## [1] 20741 &#13;
</pre><p>To see the variables in the global environment of each node, we can call <code class="literal">ls()</code>, just like we call in our own working environment:</p><pre class="programlisting">clusterEvalQ(cl, ls()) &#13;
## [[1]] &#13;
## character(0) &#13;
##  &#13;
## [[2]] &#13;
## character(0) &#13;
##  &#13;
## [[3]] &#13;
## character(0) &#13;
##  &#13;
## [[4]] &#13;
## character(0) &#13;
</pre><p>As we mentioned, all cluster nodes are, by default, initialized with an empty global environment. To export <code class="literal">data</code> and <code class="literal">take_sample</code> to each node, we can call <code class="literal">clusterExport()</code>:</p><pre class="programlisting">clusterExport(cl, c("data", "take_sample")) &#13;
clusterEvalQ(cl, ls()) &#13;
## [[1]] &#13;
## [1] "data"        "take_sample" &#13;
##  &#13;
## [[2]] &#13;
## [1] "data"        "take_sample" &#13;
##  &#13;
## [[3]] &#13;
## [1] "data"        "take_sample" &#13;
##  &#13;
## [[4]] &#13;
## [1] "data"        "take_sample" &#13;
</pre><p>Now, we can see that all nodes have <code class="literal">data</code> and <code class="literal">take_sample</code>. Now, we can let each node call <code class="literal">take_sample()</code>:</p><pre class="programlisting">clusterEvalQ(cl, take_sample(2)) &#13;
## [[1]] &#13;
##    id         x           y &#13;
## 88 88 0.6519981  1.43142886 &#13;
## 80 80 0.7985715 -0.04409101 &#13;
##  &#13;
## [[2]] &#13;
##    id          x          y &#13;
## 65 65 -0.4705287 -1.0859630 &#13;
## 35 35  0.6240227 -0.3634574 &#13;
##  &#13;
## [[3]] &#13;
##    id         x          y &#13;
## 75 75 0.3994768 -0.1489621 &#13;
## 8   8 1.4234844  1.8903637 &#13;
##  &#13;
## [[4]] &#13;
##    id         x         y &#13;
## 77 77 0.4458477  1.420187 &#13;
## 9   9 0.3943990 -0.196291 &#13;
</pre><p>Alternatively, we can use <code class="literal">clusterCall()</code> and <code class="literal">&lt;&lt;-</code> to create global variables in each node, while <code class="literal">&lt;-</code> only creates local variables in the function:</p><pre class="programlisting">invisible(clusterCall(cl, function() { &#13;
  local_var &lt;- 10 &#13;
  global_var &lt;&lt;- 100 &#13;
})) &#13;
clusterEvalQ(cl, ls()) &#13;
## [[1]] &#13;
## [1] "data"        "global_var"  "take_sample" &#13;
##  &#13;
## [[2]] &#13;
## [1] "data"        "global_var"  "take_sample" &#13;
##  &#13;
## [[3]] &#13;
## [1] "data"        "global_var"  "take_sample" &#13;
##  &#13;
## [[4]] &#13;
## [1] "data"        "global_var"  "take_sample" &#13;
</pre><p>Note that <code class="literal">clusterCall()</code> returns the returned value from each node. In the preceding code, we will use <code class="literal">invisible()</code> to suppress the values they return.</p><p>Since each cluster node is started in a fresh state, they only load basic packages. To let each node load the given packages, we can also use <code class="literal">clusterEvalQ()</code>. The following code lets each node attach the <code class="literal">data.table</code> package so that <code class="literal">parLapply()</code> can run a function in which <code class="literal">data.table</code> functions are used on each node:</p><pre class="programlisting">clusterExport(cl, "simulate") &#13;
invisible(clusterEvalQ(cl, { &#13;
  library(data.table) &#13;
})) &#13;
res &lt;- parLapply(cl, 1:3, function(i) { &#13;
  res_table &lt;- rbindlist(lapply(1:1000, simulate)) &#13;
  res_table[, id := NULL] &#13;
  summary(res_table) &#13;
}) &#13;
</pre><p>A list of data summary is returned:</p><pre class="programlisting">res &#13;
## [[1]] &#13;
##      first             high             low         &#13;
##  Min.   : 99.86   Min.   : 99.95   Min.   : 84.39   &#13;
##  1st Qu.: 99.97   1st Qu.:101.44   1st Qu.: 94.20   &#13;
##  Median :100.00   Median :103.32   Median : 96.60   &#13;
##  Mean   :100.00   Mean   :103.95   Mean   : 96.04   &#13;
##  3rd Qu.:100.03   3rd Qu.:105.63   3rd Qu.: 98.40   &#13;
##  Max.   :100.17   Max.   :121.00   Max.   :100.06   &#13;
##       last          signal        &#13;
##  Min.   : 84.99   Mode :logical   &#13;
##  1st Qu.: 96.53   FALSE:911       &#13;
##  Median : 99.99   TRUE :89        &#13;
##  Mean   : 99.92   NA's :0         &#13;
##  3rd Qu.:103.11                   &#13;
##  Max.   :119.66                   &#13;
##  &#13;
## [[2]] &#13;
##      first             high             low         &#13;
##  Min.   : 99.81   Min.   : 99.86   Min.   : 83.67   &#13;
##  1st Qu.: 99.96   1st Qu.:101.48   1st Qu.: 94.32   &#13;
##  Median :100.00   Median :103.14   Median : 96.42   &#13;
##  Mean   :100.00   Mean   :103.91   Mean   : 96.05   &#13;
##  3rd Qu.:100.04   3rd Qu.:105.76   3rd Qu.: 98.48   &#13;
##  Max.   :100.16   Max.   :119.80   Max.   :100.12   &#13;
##       last          signal        &#13;
##  Min.   : 85.81   Mode :logical   &#13;
##  1st Qu.: 96.34   FALSE:914       &#13;
##  Median : 99.69   TRUE :86        &#13;
##  Mean   : 99.87   NA's :0         &#13;
##  3rd Qu.:103.31                   &#13;
##  Max.   :119.39                   &#13;
##  &#13;
## [[3]] &#13;
##      first             high             low         &#13;
##  Min.   : 99.84   Min.   : 99.88   Min.   : 85.88   &#13;
##  1st Qu.: 99.97   1st Qu.:101.61   1st Qu.: 94.26   &#13;
##  Median :100.00   Median :103.42   Median : 96.72   &#13;
##  Mean   :100.00   Mean   :104.05   Mean   : 96.12   &#13;
##  3rd Qu.:100.03   3rd Qu.:105.89   3rd Qu.: 98.35   &#13;
##  Max.   :100.15   Max.   :117.60   Max.   :100.03   &#13;
##       last          signal        &#13;
##  Min.   : 86.05   Mode :logical   &#13;
##  1st Qu.: 96.70   FALSE:920       &#13;
##  Median :100.16   TRUE :80        &#13;
##  Mean   :100.04   NA's :0         &#13;
##  3rd Qu.:103.24                   &#13;
##  Max.   :114.80 &#13;
</pre><p>When we don't need the cluster any more, we will run the following code to release it:</p><pre class="programlisting">stopCluster(cl) &#13;
</pre></div><div><div><div><div><h3 class="title"><a id="ch13lvl3sec71"/>Using parallel computing on Linux and MacOS</h3></div></div></div><p>Using parallel computing on Linux and MacOS can be much easier than on Windows. Without having to manually create a socket-based cluster, <code class="literal">mclapply()</code> directly forks the current R session into multiple R sessions, with everything preserved to continue running in parallel and schedule tasks for each child R session:</p><pre class="programlisting">system.time(res &lt;- mclapply(1:10000, simulate,  &#13;
  mc.cores = detectCores())) &#13;
##    user  system elapsed  &#13;
##   9.732   0.060   3.415 &#13;
</pre><p>Therefore, we don't have to export the variables because they are immediately available in each fork process:</p><pre class="programlisting">mclapply(1:3, take_sample, mc.cores = detectCores()) &#13;
## [[1]] &#13;
##    id         x          y &#13;
## 62 62 0.1679572 -0.5948647 &#13;
##  &#13;
## [[2]] &#13;
##    id         x           y &#13;
## 56 56 1.5678983  0.08655707 &#13;
## 39 39 0.1015022 -1.98006684 &#13;
##  &#13;
## [[3]] &#13;
##    id           x          y &#13;
## 98 98  0.13892696 -0.1672610 &#13;
## 4   4  0.07533799 -0.6346651 &#13;
## 76 76 -0.57345242 -0.5234832 &#13;
</pre><p>Also, we can create jobs to be done in parallel with much flexibility. For example, we will create a job that generates <code class="literal">10</code> random numbers:</p><pre class="programlisting">job1 &lt;- mcparallel(rnorm(10), "job1") &#13;
</pre><p>As long as the job is created, we can choose to collect the results from the job with <code class="literal">mccollect()</code>. Then, the function will not return until the job is finished:</p><pre class="programlisting">mccollect(job1) &#13;
## $`20772` &#13;
##  [1]  1.1295953 -0.6173255  1.2859549 -0.9442054  0.1482608 &#13;
##  [6]  0.4242623  0.9463755  0.6662561  0.4313663  0.6231939 &#13;
</pre><p>We can also programmatically create a number of jobs to run in parallel. For example, we create <code class="literal">8</code> jobs, and each sleeps for a random time. Then, <code class="literal">mccollect()</code> won't return until all jobs are finished sleeping. Since the jobs are run in parallel, the time <code class="literal">mccollect()</code> takes won't be too long:</p><pre class="programlisting">jobs &lt;- lapply(1:8, function(i) {&#13;
   mcparallel({&#13;
     t &lt;- rbinom(1, 5, 0.6)&#13;
     Sys.sleep(t)&#13;
     t&#13;
   }, paste0("job", i))&#13;
 })&#13;
 system.time(res &lt;- mccollect(jobs))&#13;
##    user  system elapsed &#13;
##   0.012   0.040   4.852</pre><p>This allows us to customize the task-scheduling mechanism.</p></div></div><div><div><div><div><h2 class="title"><a id="ch13lvl2sec154"/>Using Rcpp</h2></div></div></div><p>As we mentioned, parallel computing works when each iteration is independent so that the final results do not rely on the order of execution. However, not all tasks are so ideal like this. Therefore, the use of parallel computing may be undermined. What if we really want the algorithm to run fast and easily interact with R? The answer is by writing the algorithm in C++ via Rcpp (<a class="ulink" href="http://www.rcpp.org/">http://www.rcpp.org/</a>).</p><p>C++ code usually runs very fast, because it is compiled to native instructions and is thus much closer to hardware level than a scripting language like R. Rcpp is a package that enables us to write C++ code with seamless R and C++ integration. With Rcpp, we can write C++ code in which we can call R functions and take advantage of R data structures. It allows us to write high-performance code and preserve the power of data manipulation in R at the same time.</p><p>To use Rcpp, we first need to ensure that the system is prepared for computing native code with the right toolchain. Under Windows, Rtools is needed and can be found at <a class="ulink" href="https://cran.r-project.org/bin/windows/Rtools/">https://cran.r-project.org/bin/windows/Rtools/</a>. Under Linux and MacOS, a properly installed C/C++ toolchain is required.</p><p>Once the toolchain is properly installed, run the following code to install the package:</p><pre class="programlisting">install.packages("Rcpp") &#13;
</pre><p>Then, we will create a C++ source file at <code class="literal">code/rcpp-demo.cpp</code> with the following code:</p><pre class="programlisting">#include &lt;Rcpp.h&gt; &#13;
usingnamespace Rcpp; &#13;
 &#13;
// [[Rcpp::export]] &#13;
NumericVector timesTwo(NumericVector x) { &#13;
  return x * 2; &#13;
} &#13;
</pre><p>The preceding code is written in C++. If you are not familiar with C++ syntax, you can quickly pick up the simplest part by going through <a class="ulink" href="http://www.learncpp.com/">http://www.learncpp.com/</a>. The language design and supported features are much richer and more complex than R. Don't expect to be an expert in a short period of time, but getting started with the basics usually allows you to write simple algorithms.</p><p>If you read the preceding code, it looks very different from typical R code. Since C++ is a strong-typed language, we need to specify the types of function arguments and the return type of functions. A function that is commented with <code class="literal">[[Rcpp::export]]</code> will be captured by Rcpp, and when we source the code in RStudio or use <code class="literal">Rcpp::sourceCpp</code> directly, these C++ functions will be automatically compiled and ported to our working environment in R.</p><p>The preceding C++ function simply takes a numeric vector and returns a new numeric vector with all <code class="literal">x</code> elements doubled. Note that the <code class="literal">NumericVector</code> class is provided by <code class="literal">Rcpp.h</code> included at the beginning of the source file. In fact, <code class="literal">Rcpp.h</code> provides the C++ proxy of all commonly used R data structures. Now, we will call <code class="literal">Rcpp::sourceCpp()</code> to compile and load the source file:</p><pre class="programlisting">Rcpp::sourceCpp("code/rcpp-demo.cpp") &#13;
</pre><p>The function compiles the source code, links it to necessary shared libraries, and exposes an R function to the environment. The beauty is that all of these are done automatically, which makes it much easier to write algorithms for non-professional C++ developers. Now, we have an R function to call it:</p><pre class="programlisting">timesTwo &#13;
## function (x)  &#13;
## .Primitive(".Call")(&lt;pointer: 0x7f81735528c0&gt;, x) &#13;
</pre><p>We can see that <code class="literal">timeTwo</code> in R does not look like an ordinary function, but performs a native call to the C++ function. The function works with single numeric input:</p><pre class="programlisting">timesTwo(10) &#13;
## [1] 20 &#13;
</pre><p>It also works with a multi-element numeric vector:</p><pre class="programlisting">timesTwo(c(1, 2, 3)) &#13;
## [1] 2 4 6 &#13;
</pre><p>Now, we can use very simple C++ language constructs to reimplement the <code class="literal">algo1_for</code> algorithm in C++. Now, we will create a C++ source file at <code class="literal">code/rcpp-algo1.cpp</code> with the following code:</p><pre class="programlisting">#include &lt;Rcpp.h&gt;&#13;
 using namespace Rcpp;&#13;
 &#13;
 // [[Rcpp::export]]&#13;
 double algo1_cpp(int n) {&#13;
   double res = 0;&#13;
   for (double i = 1; i &lt; n; i++) {&#13;
     res += 1 / (i * i);&#13;
   }&#13;
   return res;&#13;
 }</pre><p>Note that we don't use any R but C++ data structures in <code class="literal">algo1_cpp</code>. When we source the code, Rcpp will handle all the porting for us:</p><pre class="programlisting">Rcpp::sourceCpp("code/rcpp-algo1.cpp") &#13;
</pre><p>The function works with a single numeric input:</p><pre class="programlisting">algo1_cpp(10) &#13;
## [1] 1.539768 &#13;
</pre><p>If we supply a numeric vector, an error will occur:</p><pre class="programlisting">algo1_cpp(c(10, 15)) &#13;
## Error in eval(expr, envir, enclos): expecting a single value &#13;
</pre><p>Now, we can do the benchmarking again. This time, we will add <code class="literal">algo1_cpp</code> to the list of alternative implementations. Here, we will compare the version using a <code class="literal">for</code> loop in R, the byte-code compiled version using for loop in R, the vectorized version, and the C++ version:</p><pre class="programlisting">n &lt;- 1000 &#13;
microbenchmark( &#13;
  algo1_for(n),  &#13;
  algo1_cmp(n),  &#13;
  algo1_vec(n),  &#13;
  algo1_cpp(n)) &#13;
## Unit: microseconds &#13;
##          expr     min       lq      mean   median       uq &#13;
##  algo1_for(n) 493.312 507.7220 533.41701 513.8250 531.5470 &#13;
##  algo1_cmp(n)  57.262  59.1375  61.44986  60.0160  61.1190 &#13;
##  algo1_vec(n)  10.091  10.8340  11.60346  11.3045  11.7735 &#13;
##  algo1_cpp(n)   5.493   6.0765   7.13512   6.6210   7.2775 &#13;
##      max neval cld &#13;
##  789.799   100   c &#13;
##  105.260   100  b  &#13;
##   23.007   100 a   &#13;
##   22.131   100 a &#13;
</pre><p>It is amazing that the C++ version is even faster than the vectorized version. Although the functions used by the vectorized version are primitive functions and are already very fast, they still have some overhead due to method dispatching and argument checking. Our C++ version is specialized to the task, so it can be slightly faster than the vectorized version.</p><p>Another example is the C++ implementation of <code class="literal">diff_for()</code> as the following code shows:</p><pre class="programlisting">#include &lt;Rcpp.h&gt; &#13;
usingnamespace Rcpp; &#13;
 &#13;
// [[Rcpp::export]] &#13;
NumericVector diff_cpp(NumericVector x) { &#13;
  NumericVector res(x.size() - 1); &#13;
  for (int i = 0; i &lt; x.size() - 1; i++) { &#13;
    res[i] = x[i + 1] - x[i]; &#13;
  } &#13;
  return res; &#13;
} &#13;
</pre><p>In the preceding C++ code, <code class="literal">diff_cpp()</code> takes a numeric vector and returns a numeric vector. The function simply creates a new vector, and calculates and stores the differences between the consecutive two elements in <code class="literal">x</code> iteratively. Then, we will source the code file:</p><pre class="programlisting">Rcpp::sourceCpp("code/rcpp-diff.cpp") &#13;
</pre><p>It is easy to verify whether the function works as supposed:</p><pre class="programlisting">diff_cpp(c(1, 2, 3, 5)) &#13;
## [1] 1 1 2 &#13;
</pre><p>Then, we will do the benchmarking again with five different calls: the version using a for loop in R (<code class="literal">diff_for</code>), the byte-code compiled version (<code class="literal">diff_cmp</code>), the vectorized version (<code class="literal">diff</code>), the vectorized version without method dispatch (<code class="literal">diff.default</code>), and our C++ version (<code class="literal">diff_cpp</code>):</p><pre class="programlisting">x &lt;- rnorm(1000) &#13;
microbenchmark( &#13;
  diff_for(x),  &#13;
  diff_cmp(x),  &#13;
  diff(x),  &#13;
  diff.default(x),  &#13;
  diff_cpp(x)) &#13;
## Unit: microseconds &#13;
##             expr      min        lq       mean    median &#13;
##      diff_for(x) 1055.177 1113.8875 1297.82994 1282.9675 &#13;
##      diff_cmp(x)   75.511   78.4210   88.46485   88.2135 &#13;
##          diff(x)   12.899   14.9340   20.64854   18.3975 &#13;
##  diff.default(x)   10.750   11.6865   13.90939   12.6400 &#13;
##      diff_cpp(x)    5.314    6.4260    8.62119    7.5330 &#13;
##         uq      max neval cld &#13;
##  1400.8250 2930.690   100   c &#13;
##    90.3485  179.620   100  b  &#13;
##    24.2335   65.172   100 a   &#13;
##    15.3810   25.455   100 a   &#13;
##     8.9570   54.455   100 a &#13;
</pre><p>It appears that the C++ version is the fastest.</p><p>In recent years, a rapidly growing number of R packages have used Rcpp to either boost performance or directly link to popular libraries that provide high-performance algorithms. For example, RcppArmadillo and RcppEigen provide high-performance linear algebra algorithms, RcppDE provides fast implementations for global optimization by differential evolution in C++, and so on.</p><p>To know more about Rcpp and related packages, visit its official website (<a class="ulink" href="http://www.rcpp.org/">http://www.rcpp.org/</a>). I also recommend the book<em>Seamless R and C++ Integration with Rcpp</em> by Rcpp's author Dirk Eddelbuettel at <a class="ulink" href="http://www.rcpp.org/book/">http://www.rcpp.org/book/</a>.</p><div><div><div><div><h3 class="title"><a id="ch13lvl3sec72"/>OpenMP</h3></div></div></div><p>As we mentioned in the section on parallel computing, an R session runs in a single thread. However, in Rcpp code, we can use multithreading to boost the performance. One multithreading technique is OpenMP (<a class="ulink" href="http://openmp.org">http://openmp.org</a>), which is supported by most modern C++ compilers (see <a class="ulink" href="http://openmp.org/wp/openmp-compilers/">http://openmp.org/wp/openmp-compilers/</a>).</p><p>Several articles discuss and demonstrate the use of OpenMP with Rcpp at <a class="ulink" href="http://gallery.rcpp.org/tags/openmp/">http://gallery.rcpp.org/tags/openmp/</a>. Here, we will provide a simple example. We will create a C++ source file with the following code at <code class="literal">code/rcpp-diff-openmp.cpp</code>:</p><pre class="programlisting">// [[Rcpp::plugins(openmp)]] &#13;
#include &lt;omp.h&gt; &#13;
#include &lt;Rcpp.h&gt; &#13;
usingnamespace Rcpp; &#13;
 &#13;
// [[Rcpp::export]] &#13;
NumericVector diff_cpp_omp(NumericVector x) { &#13;
  omp_set_num_threads(3); &#13;
  NumericVector res(x.size() - 1); &#13;
#pragma omp parallel for  &#13;
  for (int i = 0; i &lt; x.size() - 1; i++) { &#13;
    res[i] = x[i + 1] - x[i]; &#13;
  } &#13;
  return res; &#13;
} &#13;
</pre><p>Note that Rcpp will recognize the comment in the first line and add necessary options to the compiler so that OpenMP is enabled. To use OpenMP, we need to include <code class="literal">omp.h</code>. Then, we can set the number of threads by calling <code class="literal">omp_set_num_threads(n)</code> and use <code class="literal">#pragma omp parallel for</code> to indicate that the following for loop should be parallelized. If the number of threads is set to <code class="literal">1</code>, then the code also runs normally.</p><p>We will source the C++ code file:</p><pre class="programlisting">Rcpp::sourceCpp("code/rcpp-diff-openmp.cpp") &#13;
</pre><p>First, let's see whether the function works properly:</p><pre class="programlisting">diff_cpp_omp(c(1, 2, 4, 8)) &#13;
## [1] 1 2 4 &#13;
</pre><p>Then, we will start benchmarking with a 1000-number input vector:</p><pre class="programlisting">x &lt;- rnorm(1000) &#13;
microbenchmark( &#13;
  diff_for(x),  &#13;
  diff_cmp(x),  &#13;
  diff(x),  &#13;
  diff.default(x),  &#13;
  diff_cpp(x), &#13;
  diff_cpp_omp(x)) &#13;
## Unit: microseconds &#13;
##             expr      min        lq       mean    median &#13;
##      diff_for(x) 1010.367 1097.9015 1275.67358 1236.7620 &#13;
##      diff_cmp(x)   75.729   78.6645   88.20651   88.9505 &#13;
##          diff(x)   12.615   16.4200   21.13281   20.5400 &#13;
##  diff.default(x)   10.555   12.1690   16.07964   14.8210 &#13;
##      diff_cpp(x)    5.640    6.4825    8.24118    7.5400 &#13;
##  diff_cpp_omp(x)    3.505    4.4390   26.76233    5.6625 &#13;
##         uq      max neval cld &#13;
##  1393.5430 2839.485   100   c &#13;
##    94.3970  186.660   100  b  &#13;
##    24.4260   43.893   100 a   &#13;
##    18.4635   72.940   100 a   &#13;
##     8.6365   50.533   100 a   &#13;
##    13.9585 1430.605   100 a &#13;
</pre><p>Unfortunately, even with multi-threading, <code class="literal">diff_cpp_omp()</code> is slower than its single-threaded C++ implementation. This is because using multithreading has some overhead. If the input is small, the time to initialize multiple threads may take a significant part of the whole computing time. However, if the input is large enough, the advantage of multi-threading will exceed its cost. Here, we will use <code class="literal">100000</code> numbers as the input vector:</p><pre class="programlisting">x &lt;- rnorm(100000) &#13;
microbenchmark( &#13;
  diff_for(x),  &#13;
  diff_cmp(x),  &#13;
  diff(x),  &#13;
  diff.default(x),  &#13;
  diff_cpp(x), &#13;
  diff_cpp_omp(x)) &#13;
## Unit: microseconds &#13;
##             expr        min          lq        mean &#13;
##      diff_for(x) 112216.936 114617.4975 121631.8135 &#13;
##      diff_cmp(x)   7355.241   7440.7105   8800.0184 &#13;
##          diff(x)    863.672    897.2060   1595.9434 &#13;
##  diff.default(x)    844.186    877.4030   3451.6377 &#13;
##      diff_cpp(x)    418.207    429.3125    560.3064 &#13;
##  diff_cpp_omp(x)    125.572    149.9855    237.5871 &#13;
##      median          uq        max neval cld &#13;
##  115284.377 116165.3140 214787.857   100   c &#13;
##    7537.405   8439.9260 102712.582   100  b  &#13;
##    1029.642   2195.5620   8020.990   100 a   &#13;
##     931.306   2365.6920  99832.513   100 a   &#13;
##     436.638    552.5110   2165.091   100 a   &#13;
##     166.834    190.7765   1983.299   100 a &#13;
</pre><p>The cost of creating multiple threads is small relative to the performance boost of using them. As a result, the version powered by OpenMP is even faster than the simple C++ version.</p><p>In fact, the feature set of OpenMP is much richer than we have demonstrated. For more details, read the official documentation. For more examples, I recommend <em>Guide into OpenMP: Easy multithreading programming for C++</em> by Joel Yliluoma at <a class="ulink" href="http://bisqwit.iki.fi/story/howto/openmp/">http://bisqwit.iki.fi/story/howto/openmp/</a>.</p></div><div><div><div><div><h3 class="title"><a id="ch13lvl3sec73"/>RcppParallel</h3></div></div></div><p>Another approach to taking advantage of multi-threading with Rcpp is RcppParallel (<a class="ulink" href="http://rcppcore.github.io/RcppParallel/">http://rcppcore.github.io/RcppParallel/</a>). This package includes Intel TBB (<a class="ulink" href="https://www.threadingbuildingblocks.org/">https://www.threadingbuildingblocks.org/</a>) and TinyThread (<a class="ulink" href="http://tinythreadpp.bitsnbites.eu/">http://tinythreadpp.bitsnbites.eu/</a>). It provides thread-safe vector and matrix wrapper data structures as well as high-level parallel functions.</p><p>To perform multi-threading parallel computing with RcppParallel, we need to implement a <code class="literal">Worker</code> to handle how a slice of input is transformed to the output. Then, RcppParallel will take care of the rest of the work such as multithreading task scheduling.</p><p>Here's a short demo. We will create a C++ source file with the following code at <code class="literal">code/rcpp-parallel.cpp</code>. Note that we need to declare to Rcpp that it depends on RcppParallel and uses C++ 11 for using lambda function.</p><p>Here, we will implement a <code class="literal">Worker</code> called <code class="literal">Transformer</code> that transforms each element <code class="literal">x</code> of a matrix <code class="literal">to 1 / (1 + x ^ 2)</code>. Then, in <code class="literal">par_transform</code>, we will create an instance of <code class="literal">Transformer</code> and call <code class="literal">parallelFor</code> with it so that it automatically takes advantage of multithreading:</p><pre class="programlisting">// [[Rcpp::plugins(cpp11)]]&#13;
 // [[Rcpp::depends(RcppParallel)]]&#13;
 #include &lt;Rcpp.h&gt;&#13;
 #include &lt;RcppParallel.h&gt;&#13;
 &#13;
 using namespace Rcpp;&#13;
 using namespace RcppParallel;&#13;
 &#13;
 struct Transformer : public Worker {&#13;
   const RMatrix&lt;double&gt; input;&#13;
   RMatrix&lt;double&gt; output;&#13;
   Transformer(const NumericMatrix input, NumericMatrix output)&#13;
     : input(input), output(output) {}&#13;
   void operator()(std::size_t begin, std::size_t end) {&#13;
     std::transform(input.begin() + begin, input.begin() + end,&#13;
       output.begin() + begin, [](double x) {&#13;
         return 1 / (1 + x * x);&#13;
       });&#13;
   }&#13;
 };&#13;
 &#13;
// [[Rcpp::export]]&#13;
NumericMatrix par_transform (NumericMatrix x) {&#13;
  NumericMatrix output(x.nrow(), x.ncol());&#13;
  Transformer transformer(x, output);&#13;
  parallelFor(0, x.length(), transformer);&#13;
  return output;&#13;
}</pre><p>We can easily verify that the function works with a small matrix:</p><pre class="programlisting">mat &lt;- matrix(1:12, nrow = 3) &#13;
mat &#13;
##      [,1] [,2] [,3] [,4] &#13;
## [1,]    1    4    7   10 &#13;
## [2,]    2    5    8   11 &#13;
## [3,]    3    6    9   12 &#13;
par_transform(mat) &#13;
##      [,1]       [,2]       [,3]        [,4] &#13;
## [1,]  0.5 0.05882353 0.02000000 0.009900990 &#13;
## [2,]  0.2 0.03846154 0.01538462 0.008196721 &#13;
## [3,]  0.1 0.02702703 0.01219512 0.006896552 &#13;
all.equal(par_transform(mat), 1 /(1 + mat ^ 2)) &#13;
## [1] TRUE &#13;
</pre><p>It produces exactly the same results as the vectorized R expression. Now, we can take a look at its performance when the input matrix is very large:</p><pre class="programlisting">mat &lt;- matrix(rnorm(1000 * 2000), nrow = 1000) &#13;
microbenchmark(1 /(1 + mat ^ 2), par_transform(mat)) &#13;
## Unit: milliseconds &#13;
##                expr      min        lq     mean    median &#13;
##       1/(1 + mat ^ 2) 14.50142 15.588700 19.78580 15.768088 &#13;
##  par_transform(mat)  7.73545  8.654449 13.88619  9.277798 &#13;
##        uq      max neval cld &#13;
##  18.79235 127.1912   100   b &#13;
##  11.65137 110.6236   100  a &#13;
</pre><p>It appears that the multi-threading version is almost 1x faster than the vectorized version.</p><p>RcppParallel is more powerful than we have demonstrated. For more detailed introduction and examples, visit <a class="ulink" href="http://rcppcore.github.io/RcppParallel">http://rcppcore.github.io/RcppParallel</a>.</p></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch13lvl1sec70"/>Summary</h1></div></div></div><p>In this chapter, you learned when performance may or may not matter, how to measure the performance of R code, how to use profiling tools to identify the slowest part of code, and why such code can be slow. Then, we introduced the most important ways to boost the code performance: using built-in functions if possible, taking advantage of vectorization, using the byte-code compiler, using parallel computing, writing code in C++ via Rcpp, and using multi-threading techniques in C++. High-performance computing is quite an advanced topic, and there's still a lot more to learn if you want to apply it in practice. This chapter demonstrates that using R does not always mean slow code. Instead, we can achieve high performance if we want.</p><p>In the next chapter, we will introduce another useful topic: web scraping. To scrape data from webpages, we need to understand how web pages are structured and how to extract data from their source code. You will learn the basic idea and representation of HTML, XML, and CSS, and how to analyze a target webpage so that we can correctly extract the information we want from webpages.</p></div></body></html>