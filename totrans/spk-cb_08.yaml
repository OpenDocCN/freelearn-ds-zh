- en: Chapter 8. Supervised Learning with MLlib – Classification
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter is divided into the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Doing classification using logistic regression
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Doing binary classification using SVM
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Doing classification using decision trees
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Doing classification using Random Forests
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Doing classification using Gradient Boosted Trees
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Doing classification with Naïve Bayes
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The classification problem is like the regression problem discussed in the
    previous chapter except that the outcome variable *y* takes only a few discrete
    values. In binary classification, *y* takes only two values: 0 or 1\. You can
    also think of values that the response variable can take in classification as
    representing categories.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Doing classification using logistic regression
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In classification, the response variable *y* has discreet values as opposed
    to continuous values. Some examples are e-mail (spam/non-spam), transactions (safe/fraudulent),
    and so on.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 'The *y* variable in the following equation can take on two values, 0 or 1:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_01.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
- en: Here, 0 is referred to as a negative class and 1 means a positive class. Though
    we are calling them a positive or negative class, it is only for convenience's
    sake. Algorithms are neutral about this assignment.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 'Linear regression, though it works well for regression tasks, hits a few limitations
    for classification tasks. These include:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: The fitting process is very susceptible to outliers
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no guarantee that the hypothesis function *h(x)* will fit in the range
    0 (negative class) to 1 (positive class)
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Logistic regression guarantees that *h(x)* will fit between 0 and 1\. Though
    logistic regression has the word regression in it, it is more of a misnomer and
    it is very much a classification algorithm:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_02.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
- en: 'In linear regression, the hypothesis function is as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_03.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
- en: 'In logistic regression, we slightly modify the hypothesis equation like this:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_04.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
- en: 'The *g* function is called the **sigmoid function** or **logistic function**
    and is defined as follows for a real number *t*:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_05.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
- en: 'This is what the sigmoid function looks like as a graph:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_06.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
- en: As you can see, as *t* approaches negative infinity, *g(t)* approaches 0 and,
    as *t* approaches infinity, *g(t)* approaches 1\. So, this guarantees that the
    hypothesis function output will never fall out of the 0 to 1 range.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: 'Now the hypothesis function can be rewritten as:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_07.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
- en: '*h(x)* is the estimated probability that *y = 1* for a given predictor *x*,
    so *h(x)* can also be rewritten as:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_08.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
- en: In other words, the hypothesis function is showing the probability of *y* being
    1 given feature matrix *x*, parameterized by ![Doing classification using logistic
    regression](img/3056_08_09.jpg). This probability can be any real number between
    0 and 1 but our goal of classification does not allow us to have continuous values;
    we can only have two values 0 or 1 indicating the negative or positive class.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Let's say that we predict *y = 1* if
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_10.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
- en: 'and *y = 0* otherwise. If we look at the sigmoid function graph again, we realize
    that, when the ![Doing classification using logistic regression](img/3056_08_11.jpg)
    sigmoid function is ![Doing classification using logistic regression](img/3056_08_12.jpg),
    that is, for positive values of *t*, it will predict the positive class:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: 'Since ![Doing classification using logistic regression](img/3056_08_13.jpg),
    this means for ![Doing classification using logistic regression](img/3056_08_14.jpg)
    the positive class will be predicted. To better illustrate this, let''s expand
    it to a non-matrix form for a bivariate case:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_15.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
- en: The plane represented by the equation ![Doing classification using logistic
    regression](img/3056_08_16.jpg) will decide whether a given vector belongs to
    the positive class or negative class. This line is called the decision boundary.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: 'This boundary does not have to be linear depending on the training set. If
    training data does not separate across a linear boundary, higher-level polynomial
    features can be added to facilitate it. An example can be to add two new features
    by squaring x1 and x2 as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_17.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
- en: 'Please note that, to the learning algorithm, this enhancement is exactly the
    same as the following equation:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_18.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
- en: The learning algorithm will treat the introduction of polynomials just as another
    feature. This gives you great power in the fitting process. It means any complex
    decision boundary can be created with the right choice of polynomials and parameters.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s spend some time trying to understand how we choose the right value for
    parameters like we did in the case of linear regression. The cost function *J*
    in the case of linear regression was:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_19.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
- en: 'As you know, we are averaging the cost in this cost function. Let''s represent
    this in terms of cost term:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_20.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
- en: 'In other words, the cost term is the cost the algorithm has to pay if it predicts
    *h(x)* for the real response variable value *y*:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_21.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
- en: This cost works fine for linear regression but, for logistic regression, this
    cost function is non-convex (that is, it leads to multiple local minimums) and
    we need to find a better convex way to estimate the cost.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 'The cost functions that work well for logistic regression are the following:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_22.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
- en: 'Let''s put these two cost functions into one by combining the two:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_23.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
- en: 'Let''s put back this cost function to *J*:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using logistic regression](img/3056_08_24.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
- en: 'The goal would be to minimize the cost, that is, minimize the value of ![Doing
    classification using logistic regression](img/3056_08_25.jpg). This is done using
    the gradient descent algorithm. Spark has two classes that support logistic regression:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '`LogisticRegressionWithSGD`'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LogisticRegressionWithLBFGS`'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `LogisticRegressionWithLBFGS` class is preferred as it eliminates the step
    of optimizing the step size.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In 2006, Suzuki, Tsurusaki, and Kodama did some research on the distribution
    of an endangered burrowing spider on different beaches in Japan ([https://www.jstage.jst.go.jp/article/asjaa/55/2/55_2_79/_pdf](https://www.jstage.jst.go.jp/article/asjaa/55/2/55_2_79/_pdf)).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see some data about grain size and the presence of spiders:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '| Grain size (mm) | Spider present |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
- en: '| 0.245 | Absent |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
- en: '| 0.247 | Absent |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
- en: '| 0.285 | Present |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
- en: '| 0.299 | Present |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
- en: '| 0.327 | Present |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
- en: '| 0.347 | Present |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
- en: '| 0.356 | Absent |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
- en: '| 0.36 | Present |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
- en: '| 0.363 | Absent |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
- en: '| 0.364 | Present |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
- en: '| 0.398 | Absent |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
- en: '| 0.4 | Present |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
- en: '| 0.409 | Absent |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
- en: '| 0.421 | Present |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
- en: '| 0.432 | Absent |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
- en: '| 0.473 | Present |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
- en: '| 0.509 | Present |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
- en: '| 0.529 | Present |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
- en: '| 0.561 | Absent |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
- en: '| 0.569 | Absent |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
- en: '| 0.594 | Present |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
- en: '| 0.638 | Present |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
- en: '| 0.656 | Present |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
- en: '| 0.816 | Present |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
- en: '| 0.853 | Present |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
- en: '| 0.938 | Present |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
- en: '| 1.036 | Present |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
- en: '| 1.045 | Present |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
- en: We will use this data to train the algorithm. Absent will be denoted as 0 and
    present will be denoted as 1.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Start the Spark shell:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Import statistics and related classes:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Create a `LabeledPoint` array with the presence or absence of spiders being
    the label:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Create an RDD of the preceding data:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Train a model using this data (intercept is the value when all predictors are
    zero):'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Predict the presence of spiders for grain size `0.938`:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Doing binary classification using SVM
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Classification is a technique to put data into different classes based on its
    utility. For example, an e-commerce company can apply two labels "will buy" or
    "will not buy" to potential visitors.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 'This classification is done by providing some already labeled data to machine
    learning algorithms called **training data**. The challenge is how to mark the
    boundary between two classes. Let''s take a simple example as shown in the following
    figure:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing binary classification using SVM](img/3056_08_26.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding case, we designated gray and black to the "will not buy" and
    "will buy" labels. Here, drawing a line between the two classes is as easy as
    follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing binary classification using SVM](img/3056_08_27.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
- en: 'Is this the best we can do? Not really, let''s try to do a better job. The
    black classifier is not really equidistant from the "will buy" and "will not buy"
    carts. Let''s make a better attempt like the following:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing binary classification using SVM](img/3056_08_28.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
- en: 'Now this is looking good. This in fact is what the SVM algorithm does. You
    can see in the preceding diagram that in fact there are only three carts that
    decide the slope of the line: two black carts above the line, and one gray cart
    below the line. These carts are called **support vectors** and the rest of the
    carts, that is, the vectors, are irrelevant.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes it''s not easy to draw a line and a curve may be needed to separate
    two classes like the following:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing binary classification using SVM](img/3056_08_29.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
- en: 'Sometimes even that is not enough. In that case, we need more than two dimensions
    to resolve the problem. Rather than a classified line, what we need is a hyperplane.
    In fact, whenever data is too cluttered, adding extra dimensions help to find
    a hyperplane to separate classes. The following diagram illustrates this:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing binary classification using SVM](img/3056_08_30.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
- en: This does not mean that adding extra dimensions is always a good idea. Most
    of the time, our goal is to reduce dimensions and keep only the relevant dimensions/features.
    A whole set of algorithms is dedicated to dimensionality reduction; we will cover
    these in later chapters.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Spark library comes loaded with sample `libsvm` data. We will use this
    and load the data into HDFS:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Start the Spark shell:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Perform the required imports:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Load the data as the RDD:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Count the number of records:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now let''s divide the dataset into half training data and half testing data:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Assign the `training` and `test` data:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Train the algorithm and build the model for 100 iterations (you can try different
    iterations but you will see that, at a certain point, the results start to converge
    and that is a good number to choose):'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now we can use this model to predict a label for any dataset. Let''s predict
    the label for the first point in the test data:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let''s create a tuple that has the first value as a prediction for test data
    and a second value actual label, which will help us compute the accuracy of our
    algorithm:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'You can count how many records have prediction and actual label mismatches:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Doing classification using decision trees
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Decision trees are the most intuitive among machine learning algorithms. We
    use decision trees in daily life all the time.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: 'Decision tree algorithms have a lot of useful features:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Easy to understand and interpret
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Work with both categorical and continuous features
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Work with missing features
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do not require feature scaling
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Decision tree algorithms work in an upside-down order in which an expression
    containing a feature is evaluated at every level and that splits the dataset into
    two categories. We''ll help you understand this with the simple example of a dumb
    charade, which most of us played in college. I guessed an animal and asked my
    coworker ask me questions to work out my choice. Here''s how her questioning went:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: 'Q1: Is it a big animal?'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 'A: Yes'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 'Q2: Does this animal live more than 40 years?'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 'A: Yes'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 'Q3: Is this animal an elephant?'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 'A: Yes'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 'This is an obviously oversimplified case in which she knew I had postulated
    an elephant (what else would you guess in a Big Data world?). Let''s expand this
    example to include some more animals as in the following figure (grayed boxes
    are classes):'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification using decision trees](img/3056_08_31.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
- en: The preceding example is a case of multiclass classification. In this recipe,
    we are going to focus on binary classification.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Whenever our son has to take tennis lessons in the morning, the night before
    the instructor checks the weather reports and decides whether the next morning
    would be good to play tennis. This recipe will use this example to build a decision
    tree.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s decide on the features of weather that affect the decision whether to
    play tennis in the morning or not:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Rain
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wind speed
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Temperature
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s build a table of the different combinations:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '| Rain | Windy | Temperature | Play tennis? |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
- en: '| Yes | Yes | Hot | No |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
- en: '| Yes | Yes | Normal | No |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
- en: '| Yes | Yes | Cool | No |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
- en: '| No | Yes | Hot | No |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
- en: '| No | Yes | Cool | No |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
- en: '| No | No | Hot | Yes |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
- en: '| No | No | Normal | Yes |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
- en: '| No | No | Cool | No |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
- en: 'Now how do we build a decision tree? We can start with one of three features:
    rain, windy, or temperature. The rule is to start with a feature so that the maximum
    information gain is possible.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: On a rainy day, as you can see in the table, other features do not matter and
    there is no play. The same is true for high wind velocity.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'Decision trees, like most other algorithms, take feature values only as double
    values. So, let''s do the mapping:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](img/3056_08_32.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
- en: 'The positive class is 1.0 and the negative class is 0.0\. Let''s load the data
    using the CSV format using the first value as a label:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: How to do it…
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Start the Spark shell:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Perform the required imports:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Load the file:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Parse the data and load it into `LabeledPoint`:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Train the algorithm with this data:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Create a vector for no rain, high wind, and a cool temperature:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Predict whether tennis should be played:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: How it works…
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s draw the decision tree for tennis that we created in this recipe:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/3056_08_33.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
- en: This model has a depth of three levels. Which attribute to select depends upon
    how we can maximize information gain. The way it is measured is by measuring the
    purity of the split. Purity means that, whether or not certainty is increasing,
    then that given dataset will be considered as positive or negative. In this example,
    this equates to whether the chances of play are increasing or the chances of not
    playing are increasing.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: 'Purity is measured using entropy. Entropy is a measure of disorder in a system.
    In this context, it is easier to understand it as a measure of uncertainty:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/3056_08_34.jpg)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
- en: The highest level of purity is 0 and the lowest is 1\. Let's try to determine
    the purity using the formula.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 'When rain is yes, the probability of playing tennis is *p+* is 0/3 = 0\. The
    probability of not playing tennis *p_* is 3/3 = 1:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/3056_08_35.jpg)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
- en: This is a pure set.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 'When rain is a no, the probability of playing tennis is *p+* is 2/5 = 0.4\.
    The probability of not playing tennis *p_* is 3/5 = 0.6:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/3056_08_36.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
- en: This is almost an impure set. The most impure would be the case where the probability
    is 0.5.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark uses three measures to determine impurity:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Gini impurity (classification)
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Entropy (classification)
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Variance (regression)
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Information gain is the difference between the parent node impurity and the
    weighted sum of two child node impurities. Let''s look at the first split, which
    partitions data of size eight to two datasets of size three (left) and five (right).
    Let''s call the first split *s1*, the parent node *rain*, the left child *no rain*,
    and the right child *wind*. So the information gain would be:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/3056_08_37.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
- en: 'As we calculated impurity for *no rain* and *wind* already for the entropy,
    let''s calculate the entropy for *rain*:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/3056_08_38.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
- en: 'Let''s calculate the information gain now:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/3056_08_39.jpg)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
- en: 'So the information gain is 0.2 in the first split. Is this the best we can
    achieve? Let''s see what our algorithm comes up with. First, let''s find out the
    depth of the tree:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Here, the depth is `2` compared to the `3` we intuitively built, so this model
    seems to be better optimized. Let''s look at the structure of the tree:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Let''s build it visually to get a better understanding:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/3056_08_40.jpg)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
- en: 'We will not go into detail here as we already did this with the previous model.
    We will straightaway calculate the information gain: 0.44'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in this case, the information gain is 0.44, which is more than
    double the first model.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: If you look at the second level nodes, the impurity is zero. In this case, it
    is great as we got it at a depth of 2\. Image a situation in which the depth is
    50\. In that case, the decision tree would work well for training data and would
    do badly for test data. This situation is called **overfitting**.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 'One solution to avoid overfitting is pruning. You divide your training data
    into two sets: the training set and validation set. You train the model using
    the training set. Now you test with the model against the validation set by slowly
    removing the left nodes. If removing the leaf node (which is mostly a singleton—that
    is, it contains only one data point) improves the performance of the model, this
    leaf node is pruned from the model.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Doing classification using Random Forests
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes one decision tree is not enough, so a set of decision trees is used
    to produce more powerful models. These are called **ensemble learning algorithms**.
    Ensemble learning algorithms are not limited to using decision trees as base models.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: The most popular among the ensemble learning algorithms is Random Forest. In
    Random Forest, rather than growing one single tree, *K* trees are grown. Every
    tree is given a random subset *S* of training data. To add a twist to it, every
    tree only uses a subset of features. When it comes to making predictions, a majority
    vote is done on the trees and that becomes the prediction.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Let's explain this with an example. The goal is to make a prediction for a given
    person about whether he/she has good credit or bad credit.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: To do this, we will provide labeled training data—that is, in this case, a person
    with features and labels whether he/she has good credit or bad credit. Now we
    do not want to create feature bias so we will provide a randomly selected set
    of features. There is another reason to provide a randomly selected subset of
    features and that is because most real-world data has hundreds if not thousands
    of features. Text classification algorithms, for example, typically have 50k-100k
    features.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: In this case, to add flavor to the story we are not going to provide features,
    but we will ask different people why they think a person has good or bad credit.
    Now by definition, different people are exposed to different features (sometimes
    overlapping) of a person, which gives us the same functionality as randomly selected
    features.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 'Our first example is Jack who carries a label "bad credit." We will start with
    Joey who works at Jack''s favorite bar, the Elephant Bar. The only way a person
    can deduce why a given label was given is by asking yes/no questions. Let''s see
    what Joey says:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: 'Q1: Does Jack tip well? (Feature: generosity)'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: 'A: No'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: 'Q2: Does Jack spend at least $60 per visit? (Feature: spendthrift)'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 'A: Yes'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: 'Q3: Does he tend to get into bar fights even at the smallest provocation? (Feature:
    volatile)'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: 'A: Yes'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: That explains why Jack has bad credit.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: 'We now ask Jack''s girlfriend, Stacey:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: 'Q1: When we hangout, does Jack always cover the bill? (Feature: generosity)'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: 'A: No'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 'Q2: Has Jack paid me back the $500 he owes me? (Feature: responsibility)'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 'A: No'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 'Q3: Does he overspend sometimes just to show off? (Feature: spendthrift)'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: 'A: Yes'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: That explains why Jack has bad credit.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 'We now ask Jack''s best friend George:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: 'Q1: When Jack and I hang out at my apartment, does he clean up after himself?
    (Feature: organized)'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: 'A: No'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: 'Q2: Did Jack arrive empty-handed during my Super Bowl potluck? (Feature: care)'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: 'A: Yes'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: 'Q3: Has he used the "I forgot my wallet at home" excuse for me to cover his
    tab at restaurants? (Feature: responsibility)'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: 'A: Yes'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: That explains why Jack has bad credit.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we talk about Jessica who has good credit. Let''s ask Stacey who happens
    to be Jessica''s sister:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: 'Q1: Whenever I run short of money, does Jessica offer to help? (Feature: generosity)'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: 'A: Yes'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: 'Q2: Does Jessica pay her bills on time? (Feature: responsibility)'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: 'A: Yes'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: 'Q3: Does Jessica offer to babysit my child? (Feature: care)'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: 'A: Yes'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: That explains why Jessica has good credit.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we ask George who happens to be her husband:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: 'Q1: Does Jessica keep the house tidy? (Feature: organized)'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: 'A: Yes'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: 'Q2: Does she expect expensive gifts? (Feature: spendthrift)'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: 'A: No'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: 'Q3: Does she get upset when you forget to mow the lawn? (Feature: volatile)'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: 'A: No'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: That explains why Jessica has good credit.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s ask Joey, the bartender at the Elephant Bar:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: 'Q1: Whenever she comes to the bar with friends, is she mostly the designated
    driver? (Feature: responsible)'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: 'A: Yes'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: 'Q2: Does she always take leftovers home? (Feature: spendthrift)'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: 'A: Yes'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: 'Q3: Does she tip well? (Feature: generosity)'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: 'A: Yes'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: 'The way Random Forest works is that it does random selection on two levels:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: A subset of the data
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A subset of features to split that data
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both these subsets can overlap.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: In our example, we have six features and we are going to assign three features
    to each tree. This way, there is a good chance we will have an overlap.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s add eight more people to our training dataset:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: '| Names | Label | Generosity | Responsibility | Care | Organization | Spendthrift
    | Volatile |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
- en: '| Jack | 0 | 0 | 0 | 0 | 0 | 1 | 1 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
- en: '| Jessica | 1 | 1 | 1 | 1 | 1 | 0 | 0 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
- en: '| Jenny | 0 | 0 | 0 | 1 | 0 | 1 | 1 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
- en: '| Rick | 1 | 1 | 1 | 0 | 1 | 0 | 0 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
- en: '| Pat | 0 | 0 | 0 | 0 | 0 | 1 | 1 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
- en: '| Jeb | 1 | 1 | 1 | 1 | 0 | 0 | 0 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
- en: '| Jay | 1 | 0 | 1 | 1 | 1 | 0 | 0 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
- en: '| Nat | 0 | 1 | 0 | 0 | 0 | 1 | 1 |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
- en: '| Ron | 1 | 0 | 1 | 1 | 1 | 0 | 0 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
- en: '| Mat | 0 | 1 | 0 | 0 | 0 | 1 | 1 |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
- en: Getting ready
  id: totrans-305
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s put the data we created into the `libsvm` format in the following file:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now upload it to HDFS:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: How to do it…
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Start the Spark shell:'
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Perform the required imports:'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Load and parse the data:'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Split the data into the `training` and `test` datasets:'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Create a classification as a tree strategy (Random Forest also supports regression):'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Train the model:'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Evaluate the model on test instances and compute the test error:'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Check the model:'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: How it works…
  id: totrans-327
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you can see in such a small example, three trees are using different features.
    In real-world use cases with thousands of features and training data, this would
    not happen, but most of the trees would differ in how they look at features and
    the vote of the majority will win. Please remember that, in the case of regression,
    averaging is done over trees to get a final value.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: Doing classification using Gradient Boosted Trees
  id: totrans-329
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another ensemble learning algorithm is **Gradient Boosted Trees** (**GBTs**).
    GBTs train one tree at a time, where each new tree improves upon the shortcomings
    of previously trained trees.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: As GBTs train one tree at a time, they can take longer than Random Forest.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-332
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are going to use the same data we used in the previous recipe.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-334
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Start the Spark shell:'
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Perform the required imports:'
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Load and parse the data:'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Split the data into `training` and `test` datasets:'
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Create a classification as a boosting strategy and set the number of iterations
    to `3`:'
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Train the model:'
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Evaluate the model on the test instances and compute the test error:'
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Check the model:'
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: In this case, the accuracy of the model is 0.9, which is less than what we got
    in the case of Random Forest.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: Doing classification with Naïve Bayes
  id: totrans-352
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s consider building an e-mail spam filter using machine learning. Here
    we are interested in two classes: spam for unsolicited messages and non-spam for
    regular emails:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification with Naïve Bayes](img/3056_08_42.jpg)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
- en: 'The first challenge is that, when given an e-mail, how do we represent it as
    feature vector *x*. An e-mail is just bunch of text or a collection of words (therefore,
    this problem domain falls into a broader category called **text classification**).
    Let''s represent an e-mail with a feature vector with the length equal to the
    size of the dictionary. If a given word in a dictionary appears in an e-mail,
    the value will be 1; otherwise 0\. Let''s build a vector representing e-mail with
    the content *online pharmacy sale*:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: '![Doing classification with Naïve Bayes](img/3056_08_43.jpg)'
  id: totrans-356
  prefs: []
  type: TYPE_IMG
- en: The dictionary of words in this feature vector is called *vocabulary* and the
    dimensions of the vector are the same as the size of vocabulary. If the vocabulary
    size is 10,000, the possible values in this feature vector will be 210,000.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: Our goal is to model the probability of *x* given *y*. To model *P(x|y)*, we
    will make a strong assumption, and that assumption is that *x*'s are conditionally
    independent. This assumption is called the **Naïve Bayes assumption** and the
    algorithm based on this assumption is called the **Naïve Bayes classifier**.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: For example, for *y =1*, which means spam, the probability of "online" appearing
    and "pharmacy appearing" are independent. This is a strong assumption that has
    nothing to do with reality but works out really well when it comes to getting
    good predictions.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-360
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Spark comes bundled with a sample dataset to use with Naïve Bayes. Let''s load
    this dataset to HDFS:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: How to do it…
  id: totrans-363
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'Start the Spark shell:'
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Spark shell：
- en: '[PRE46]'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Perform the required imports:'
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行所需的导入：
- en: '[PRE47]'
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Load the data into RDD:'
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据加载到RDD中：
- en: '[PRE48]'
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Parse the data into `LabeledPoint`:'
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据解析为`LabeledPoint`：
- en: '[PRE49]'
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Split the data half and half into the `training` and `test` datasets:'
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据平均分成`训练`和`测试`数据集：
- en: '[PRE50]'
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Train the model with the `training` dataset:'
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`训练`数据集训练模型：
- en: '[PRE51]'
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Predict the label of the `test` dataset:'
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测`测试`数据集的标签：
- en: '[PRE52]'
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
