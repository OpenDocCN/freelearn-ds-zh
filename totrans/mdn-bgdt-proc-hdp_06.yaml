- en: Designing Real-Time Streaming Data Pipelines
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计实时流数据处理管道
- en: The first three chapters of this book all dealt with batch data. Having learned
    about the installation of Hadoop, data ingestion tools and techniques, and data
    stores, let's turn to data streaming. Not only will we look at how we can handle
    real-time data streams, but also how to design pipelines around them.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的前三章都涉及批数据。在了解了Hadoop的安装、数据摄取工具和技术以及数据存储后，让我们转向数据流。我们不仅将探讨如何处理实时数据流，还将探讨围绕它们设计管道的方法。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Real-time streaming concepts
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时流概念
- en: Real-time streaming components
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时流组件
- en: Apache Flink versus Spark
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Flink与Spark
- en: Apache Spark versus Storm
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Spark与Storm
- en: Real-time streaming concepts
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实时流概念
- en: Let's understand a few key concepts relating to real-time streaming applications
    in the following sections.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下几节中，我们将了解与实时流应用程序相关的一些关键概念。
- en: Data stream
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据流
- en: The data stream is a continuous flow of data from one end to another end, from
    sender to receiver, from producer to consumer. The speed and volume of the data
    may vary; it may be 1 GB of data per second or it may be 1 KB of data per second
    or per minute.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 数据流是从一端到另一端、从发送者到接收者、从生产者到消费者的连续数据流动。数据的速度和量可能不同；它可能每秒1GB的数据，也可能每秒或每分钟1KB的数据。
- en: Batch processing versus real-time data processing
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 批处理与实时数据处理
- en: In batch processing, data is collected in batches and each batch is sent for
    processing. The batch interval can be anything from one day to one minute. In
    today's data analytics and business intelligence world, data will not be processed
    in a batch for more than one day. Otherwise, business teams will not have any
    insight about what's happening to the business in a day-to-day basis. For example,
    the enterprise data warehousing team may collect all the orders made during the
    last 24 hours and send all these collected orders to the analytics engine for
    reporting.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在批处理中，数据以批次的形式收集，每个批次都会被发送进行处理。批次的间隔可以是任何时间，从一天到一分钟不等。在当今的数据分析和商业智能领域，数据不会超过一天进行批处理。否则，业务团队将无法对日常业务情况有任何洞察。例如，企业数据仓库团队可能会收集过去24小时内所有订单，并将所有收集到的订单发送到分析引擎进行报告。
- en: The batch can be of one minute too. In the Spark framework (we will learn Spark
    in [Chapter 7](39cf9925-e4cc-473a-9032-a21b81e7b400.xhtml), *Large-Scale Data
    Processing Frameworks*), data is processed in micro batches.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 批次也可以是一分钟。在Spark框架中（我们将在第7章学习Spark，*大规模数据处理框架*），数据以微批次的形式进行处理。
- en: In real-time processing, data (event) is transferred (streamed) from the producer
    (sender) to the consumer (receiver) as soon as an event is produced at the source
    end. For example, on an e-commerce website, orders gets processed immediately
    in an analytics engine as soon as the customer places the same order on that website.
    The advantage is that the business team of that company gets full insights about
    its business in real time (within a few milliseconds or sub milliseconds). It
    will help them adjust their promotions to increase their revenue, all in real-time.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在实时处理中，数据（事件）在源端产生事件后立即从生产者（发送者）传输到消费者（接收者）。例如，在电子商务网站上，当客户在该网站上下单时，订单会立即在分析引擎中处理。优势是，该公司的业务团队可以实时（在几毫秒或亚毫秒内）全面了解其业务。这将帮助他们调整促销活动以增加收入，这一切都在实时进行。
- en: 'The following image explains stream-processing architecture:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图像解释了流处理架构：
- en: '![](img/01854cc6-c9ec-45f8-b35d-c5d9528176c0.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/01854cc6-c9ec-45f8-b35d-c5d9528176c0.png)'
- en: Complex event processing
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 复杂事件处理
- en: '**Complex event processing** (**CEP**) is event processing that combines data
    from multiple sources to discover complex relationships or patterns. The goal
    of CEP is to identify meaningful events (such as opportunities or threats) and
    respond to them as quickly as possible. Fundamentally, CEP is about applying business
    rules to streaming event data. For example, CEP is used in use cases, such as
    stock trading, fraud detection, medical claim processing, and so on.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**复杂事件处理**（**CEP**）是结合来自多个来源的数据以发现复杂关系或模式的事件处理。CEP的目标是尽快识别有意义的事件（如机会或威胁）并对它们做出响应。从根本上说，CEP是关于将业务规则应用于流事件数据。例如，CEP用于诸如股票交易、欺诈检测、医疗索赔处理等用例。'
- en: 'The following image explains stream-processing architecture:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图像解释了流处理架构：
- en: '![](img/c7c76b39-7e99-4115-9d6c-8b65f0dbec50.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![流处理组件](img/c7c76b39-7e99-4115-9d6c-8b65f0dbec50.png)'
- en: Continuous availability
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 持续可用性
- en: Any real-time application is expected to be available all the time with no stoppage
    whatsoever. The event collection, processing, and storage components should be
    configured with the underlined assumptions of high availability. Any failure to
    any components will cause major disruptions to the running of the business. For
    example, in a credit card fraud detection application, all the fraudulent transactions
    need to be declined. If the application stops midway and is unable to decline
    fraudulent transactions, then it will result in heavy losses.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 任何实时应用都应始终可用，没有任何中断。事件收集、处理和存储组件应配置为具有高可用性的假设。任何组件的故障都可能导致业务运行的重大中断。例如，在信用卡欺诈检测应用中，所有欺诈交易都需要被拒绝。如果应用在途中停止并且无法拒绝欺诈交易，那么这将会导致巨大的损失。
- en: Low latency
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 低延迟
- en: In any real-time application, the event should flow from source to target in
    a few milliseconds. The source collects the event, and a processing framework
    moves the event to its target data store where it can be analyzed further to find
    trends and patterns. All these should happen in real time, otherwise it may impact
    business decisions. For example, in a credit card fraud detection application,
    it is expected that all incoming transactions should be analyzed to find possible
    fraudulent transactions, if any. If the stream processing takes more than the
    desired period of time, it may be possible that these transactions may pass through
    the system, causing heavy losses to the business.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何实时应用中，事件应在几毫秒内从源头流向目标。源头收集事件，处理框架将事件移动到其目标数据存储，在那里可以进一步分析以发现趋势和模式。所有这些都应该实时发生，否则可能会影响业务决策。例如，在信用卡欺诈检测应用中，预期所有传入的交易都应该被分析以查找可能的欺诈交易，如果有。如果流处理所需的时间超过预期，那么这些交易可能通过系统，给业务造成重大损失。
- en: Scalable processing frameworks
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可扩展的处理框架
- en: Hardware failure may cause disruption to the stream processing application.
    To avoid this common scenario, we always need a processing framework that offers
    built-in APIs to support continuous computation, fault tolerant event state management,
    checkpoint features in the event of failures, in-flight aggregations, windowing,
    and so on. Fortunately, all the recent Apache projects such as Storm, Spark, Flink,
    and Kafka do support all and more of these features out of the box. The developer
    can use these APIs using Java, Python, and Scala.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件故障可能会干扰流处理应用。为了避免这种常见的场景，我们始终需要一个提供内置API以支持连续计算、容错事件状态管理、故障时的检查点功能、在途聚合、窗口等功能的处理框架。幸运的是，所有最近的Apache项目，如Storm、Spark、Flink和Kafka，都支持所有这些功能以及更多。开发者可以使用Java、Python和Scala使用这些API。
- en: Horizontal scalability
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 水平可扩展性
- en: The stream-processing platform should support horizontal scalability. That means
    adding more physical servers to the cluster in the event of a higher incoming
    data load to maintain throughput SLA. This way, the performance of processing
    can be increased by adding more nodes rather than adding more CPUs and memory
    to the existing servers; this is called **vertical scalability**.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 流处理平台应支持水平可扩展性。这意味着在数据负载更高时，向集群添加更多物理服务器以保持吞吐量SLA。这样，通过添加更多节点而不是向现有服务器添加更多CPU和内存来提高处理性能；这被称为**垂直可扩展性**。
- en: Storage
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 存储
- en: The preferable format of a stream is key-value pair. This format is very well
    represented by the JSON and Avro formats. The preferred storage to persist key-value
    type data is NoSQL data stores such as HBase and Cassandra. There are in total
    100 NoSQL open source databases in the market these days. It's very challenging
    to choose the right database, one which supports storage to real-time events,
    because all these databases offer some unique features for data persistence. A
    few examples are schema agnostic, highly distributable, commodity hardware support,
    data replication, and so on.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 流的优选格式是键值对。这种格式由JSON和Avro格式很好地表示。首选的持久化键值类型数据是NoSQL数据存储，如HBase和Cassandra。目前市场上总共有100个NoSQL开源数据库。选择正确的数据库，一个支持实时事件存储的数据库，是非常具有挑战性的，因为所有这些数据库都为数据持久性提供了一些独特的功能。一些例子包括模式无关性、高度可分布式、支持商品硬件、数据复制等。
- en: 'The following image explains all stream processing components:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图像解释了所有流处理组件：
- en: '![](img/ddc50632-2e71-4f82-805a-f60ab3170c1e.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ddc50632-2e71-4f82-805a-f60ab3170c1e.png)'
- en: In this chapter we will talk about message queue and stream processing frameworks
    in detail. In the next chapter, we will focus on data indexing techniques.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将详细讨论消息队列和流处理框架。在下一章中，我们将重点关注数据索引技术。
- en: Real-time streaming components
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实时流组件
- en: In the following sections we will walk through some important real-time streaming
    components.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下章节中，我们将介绍一些重要的实时流组件。
- en: Message queue
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 消息队列
- en: The message queue lets you publish and subscribe to a stream of events/records.
    There are various alternatives we can use as a message queue in our real-time
    stream architecture. For example, there is RabbitMQ, ActiveMQ, and Kafka. Out
    of these, Kafka has gained tremendous popularity due to its various unique features.
    Hence, we will discuss the architecture of Kafka in detail. A discussion of RabbitMQ
    and ActiveMQ is beyond the scope of this book.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 消息队列允许您发布和订阅事件/记录的流。在我们的实时流架构中，我们可以使用各种替代方案作为消息队列。例如，有RabbitMQ、ActiveMQ和Kafka。在这些中，Kafka由于其各种独特的特性而获得了巨大的流行。因此，我们将详细讨论Kafka的架构。关于RabbitMQ和ActiveMQ的讨论超出了本书的范围。
- en: So what is Kafka?
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 那么Kafka是什么呢？
- en: Kafka is a fast, scalable, durable, and fault-tolerant publish-subscribe messaging
    system. Apache Kafka is an open-source stream-processing project. It provides
    a unified, high-throughput, and is a low-latency platform for handling real-time
    data streams. It provides a distributed storage layer, which supports massively
    scalable pub/sub message queues. Kafka Connect supports data import and export
    by connecting to external systems. Kafka Streams provides Java APIs for stream
    processing. Kafka works in combination with Apache Spark, Apache Cassandra, Apache
    HBase, Apache Spark, and more for real-time stream processing.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka是一个快速、可扩展、持久和容错的发布-订阅消息系统。Apache Kafka是一个开源的流处理项目。它提供了一个统一、高吞吐量且低延迟的平台，用于处理实时数据流。它提供了一个分布式存储层，支持大规模可扩展的pub/sub消息队列。Kafka
    Connect通过连接到外部系统支持数据导入和导出。Kafka Streams提供了Java API进行流处理。Kafka与Apache Spark、Apache
    Cassandra、Apache HBase、Apache Spark等结合使用，以进行实时流处理。
- en: Apache Kafka was originally developed by LinkedIn, and was subsequently open
    sourced in early 2011\. In November 2014, several engineers who worked on Kafka
    at LinkedIn created a new company named Confluent with a focus on Kafka. Please
    use this URL [https://www.confluent.io/](https://www.confluent.io/) to learn more
    about the Confluent platform.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Kafka最初由LinkedIn开发，并在2011年初开源。2014年11月，LinkedIn上几位从事Kafka工作的工程师创建了一家名为Confluent的新公司，专注于Kafka。请使用此URL
    [https://www.confluent.io/](https://www.confluent.io/) 了解更多关于Confluent平台的信息。
- en: Kafka features
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka特性
- en: 'The following are features of Kafka:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka有以下特性：
- en: '**Kafka is scalable**: Kafka Cluster consists of more than one physical server,
    which helps to distribute the data load. It is easily scalable in the event that
    additional throughputs are required as additional servers can be added to maintain
    the SLA.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kafka是可扩展的**：Kafka集群由多个物理服务器组成，这有助于分散数据负载。在需要额外吞吐量的情况下，可以轻松扩展，因为可以添加更多服务器以保持SLA。'
- en: '**Kafka is durable**: During stream processing, Kafka persists messages on
    the persistent storage. This storage can be server local disks or Hadoop Cluster.
    In the event of message processing failure, the message can be accessed from the
    disk and replayed to process the message again. By default, the message is stored
    for seven days; this can be configured further.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kafka是持久的**：在流处理过程中，Kafka将消息持久化到持久存储中。这种存储可以是服务器本地磁盘或Hadoop集群。在消息处理失败的情况下，可以从磁盘访问消息并重新播放以再次处理消息。默认情况下，消息存储七天；这可以进一步配置。'
- en: '**Kafka is reliable**: Kafka provides message reliability with the help of
    a feature called **data replication**. Each message is replicated at least three
    times (this is configurable) so that in the event of data loss, the copy of the
    message can be used for processing.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kafka是可靠的**：Kafka通过一个名为**数据复制**的功能提供消息可靠性。每条消息至少复制三次（这是可配置的），以便在数据丢失的情况下，可以使用消息的副本进行处理。'
- en: '**Kafka supports high performance throughput**: Due to its unique architecture,
    partitioning, message storage, and horizontal scalability, Kafka helps to process
    terabytes of data per second.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kafka支持高性能吞吐量**：由于其独特的架构、分区、消息存储和水平可扩展性，Kafka有助于每秒处理数以TB计的数据。'
- en: Kafka architecture
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka架构
- en: 'The following image shows Kafka''s architecture:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像显示了Kafka的架构：
- en: '**![](img/50297cdb-a34b-439f-be63-572e999c5c87.png)**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**![](img/50297cdb-a34b-439f-be63-572e999c5c87.png)**'
- en: Kafka architecture components
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka 架构组件
- en: 'Let''s take a look at each component in detail:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细看看每个组件：
- en: '**Producers**: Producers publish messages to a specific Kafka topic. Producers
    may attach a key to each message record. By default, producers publish messages
    to topic partitions in round robin fashion. Sometimes, producers can be configured
    to write messages to a particular topic partition based on the hash value of message
    key.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生产者**：生产者将消息发布到特定的 Kafka 主题。生产者可以为每条消息记录附加一个键。默认情况下，生产者以轮询方式将消息发布到主题分区。有时，可以根据消息键的哈希值配置生产者将消息写入特定的主题分区。'
- en: '**Topic**: All messages are stored in a topic. A topic is a category or feed
    name to which records are published. A topic can be compared to a table in a relational
    database. Multiple consumers can be subscribed to a single topic to consume message
    records.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主题**：所有消息都存储在一个主题中。主题是一个类别或数据源名称，记录被发布到其中。主题可以比作关系数据库中的一个表。多个消费者可以订阅单个主题以消费消息记录。'
- en: '**Partition**: A topic is divided into multiple partitions. Kafka offers topic
    parallelism by dividing topic into partitions and by placing each partition on
    a separate broker (server) of a Kafka Cluster. Each partition has a separate partition
    log on a disk where messages are stored. Each partition contains an ordered, immutable
    sequence of messages. Each message is assigned unique sequence numbers called
    **offset**. Consumers can read messages from any point from a partition—from the
    beginning or from any offset.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分区**：主题被划分为多个分区。Kafka 通过将主题划分为分区并将每个分区放置在 Kafka 集群中单独的代理（服务器）上，提供了主题并行性。每个分区在磁盘上有一个独立的分区日志，其中存储消息。每个分区包含一个有序的、不可变的消息序列。每个消息被分配一个唯一的序列号，称为
    **偏移量**。消费者可以从分区的任何位置读取消息——从开始处或从任何偏移量。'
- en: '**Consumers**: A consumer subscribes to a topic and consumes the messages.
    In order to increase the scalability, consumers of the same application can be
    grouped into a consumer group where each consumer can read messages from a unique
    partition.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**消费者**：消费者订阅一个主题并消费消息。为了提高可伸缩性，同一应用中的消费者可以被分组到一个消费者组中，其中每个消费者可以读取来自唯一分区的消息。'
- en: '**Brokers**: Kafka is divided into multiple servers called **brokers**. All
    the brokers combined are called **Kafka Cluster**. Kafka brokers handle message
    writes from producers and message reads from consumers. Kafka Brokers stores all
    the messages coming from producers. The default period is of seven days. This
    period (retention period) can be configured based on the requirements. The retention
    period directly impacts the local storage of Kafka brokers. It takes more storage
    if a higher retention period is configured. After the retention period is over,
    the message is automatically discarded.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代理**：Kafka 被划分为多个称为 **代理** 的服务器。所有代理的总和被称为 **Kafka 集群**。Kafka 代理处理来自生产者的消息写入和来自消费者的消息读取。Kafka
    代理存储所有来自生产者的消息。默认周期为七天。这个周期（保留周期）可以根据需求进行配置。保留周期直接影响到 Kafka 代理的本地存储。如果配置了较长的保留周期，则需要更多的存储空间。在保留周期结束后，消息将被自动丢弃。'
- en: '**Kafka Connect**: According to Kafka documentation, Kafka Connect allows the
    building and running of reusable producers or consumers that connect Kafka topics
    to existing applications or data systems. For example, a connector to a relational
    database might capture every change to a table.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kafka Connect**：根据 Kafka 文档，Kafka Connect 允许构建和运行可重用的生产者或消费者，它们将 Kafka 主题连接到现有的应用或数据系统中。例如，连接到关系数据库的连接器可能会捕获表中每个更改。'
- en: '**Kafka Streams**: Stream APIs allow an application to act as a stream processor,
    consuming an input stream from one or more topics and producing an output stream
    to one or more output topics, effectively transforming the input streams to output
    streams.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kafka Streams**：流 API 允许一个应用作为流处理器，从一个或多个主题中消费输入流，并将输出流发送到一个或多个输出主题，有效地将输入流转换为输出流。'
- en: Kafka Connect deep dive
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka Connect 深入了解
- en: Kafka Connect is a part of the Confluent platform. It is integrated with Kafka.
    Using Kafka Connect, it's very easy to build data pipelines from multiple sources
    to multiple targets. **Source Connectors** import data from another system (for
    example, from a relational database into Kafka) and **Sink Connectors **export
    data (for example, the contents of a Kafka topic to an HDFS file).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka Connect是Confluent平台的一部分。它与Kafka集成。使用Kafka Connect，从多个来源到多个目标构建数据管道变得非常容易。**源连接器**从另一个系统（例如，从关系型数据库到Kafka）导入数据，而**目标连接器**导出数据（例如，Kafka主题的内容到HDFS文件）。
- en: Kafka Connect architecture
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka Connect架构
- en: 'The following image shows Kafka Connect''s architecture:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像展示了Kafka Connect的架构：
- en: '![](img/cbd6b15a-6f36-4e57-b6fd-0e01ae874bb4.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cbd6b15a-6f36-4e57-b6fd-0e01ae874bb4.png)'
- en: 'The data flow can be explained as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 数据流可以解释如下：
- en: Various sources are connected to **Kafka Connect Cluster**. **Kafka Connect
    Cluster** pulls data from the sources.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 各种来源连接到**Kafka Connect集群**。**Kafka Connect集群**从这些来源拉取数据。
- en: '**Kafka Connect Cluster** consists of a set of worker processes that are containers
    that execute connectors, and tasks automatically coordinate with each other to
    distribute work and provide scalability and fault tolerance.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kafka Connect集群**由一组执行连接器和任务的容器化工作进程组成，这些任务自动协调彼此以分配工作并提供可扩展性和容错性。'
- en: '**Kafka Connect Cluster** pushes data to **Kafka Cluster**.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kafka Connect集群**将数据推送到**Kafka集群**。'
- en: '**Kafka Cluster** persists the data on to the broker local disk or on Hadoop.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kafka集群**将数据持久化到代理本地磁盘或Hadoop。'
- en: Streams applications such as Storm, Spark Streaming, and Flink pull the data
    from **Kafka Cluster** for stream transformation, aggregation, join, and so on.
    These applications can send back the data to Kafka or persist it to external data
    stores such as HBase, Cassandra, MongoDB, HDFS, and so on.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流处理应用如Storm、Spark Streaming和Flink从**Kafka集群**中拉取数据用于流转换、聚合、连接等操作。这些应用可以将数据回传到Kafka或持久化到外部数据存储，如HBase、Cassandra、MongoDB、HDFS等。
- en: '**Kafka Connect Cluster** pulls data from **Kafka Cluster** and pushes it Sinks.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kafka Connect集群**从**Kafka集群**中拉取数据并将其推送到Sinks。'
- en: Users can extend the existing Kafka connectors or develop brand new connectors.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户可以扩展现有的Kafka连接器或开发全新的连接器。
- en: Kafka Connect workers standalone versus distributed mode
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka Connect工作进程的独立模式与分布式模式
- en: 'Users can run Kafka Connect in two ways: standalone mode or distributed mode.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 用户可以通过两种方式运行Kafka Connect：独立模式或分布式模式。
- en: In standalone mode, a single process runs all the connectors. It is not fault
    tolerant. Since it uses only a single process, it is not scalable. Generally,
    it is useful for users for development and testing purposes.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在独立模式下，单个进程运行所有连接器。它不具有容错性。由于它只使用单个进程，因此不具有可扩展性。通常，它对用户在开发和测试目的上很有用。
- en: In distributed mode, multiple workers run Kafka Connect. In this mode, Kafka
    Connect is scalable and fault tolerant, so it is used in production deployment.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式模式下，多个工作进程运行Kafka Connect。在这种模式下，Kafka Connect可扩展且容错，因此在生产部署中使用。
- en: 'Let''s learn more about Kafka and Kafka Connect (standalone mode). In this
    example, we will do the following:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地了解Kafka和Kafka Connect（独立模式）。在这个例子中，我们将执行以下操作：
- en: Install Kafka
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装Kafka
- en: Create a topic
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个主题
- en: Generate a few messages to verify the producer and consumer
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 发送几条消息以验证生产者和消费者
- en: Kafka Connect-File-source and file-sink
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kafka Connect-File-source和file-sink
- en: Kafka Connect-JDBC -Source
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kafka Connect-JDBC -Source
- en: 'The following image shows a use case using **Kafka Connect**:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像展示了使用**Kafka Connect**的用例：
- en: '![](img/a8818e04-6229-45ec-9063-ca5a4c86a8a9.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a8818e04-6229-45ec-9063-ca5a4c86a8a9.png)'
- en: Let's see how Kafka and Kafka Connect works by running a few examples. For more
    details, use the following link for Kafka Confluent's documentation: [https://docs.confluent.io/current/](https://docs.confluent.io/current/).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过运行一些示例来了解Kafka和Kafka Connect是如何工作的。有关更多详细信息，请使用以下链接访问Kafka Confluent的文档：[https://docs.confluent.io/current/](https://docs.confluent.io/current/)。
- en: Install Kafka
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装Kafka
- en: 'Let''s perform the following steps to install Kafka:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们执行以下步骤来安装Kafka：
- en: Download Confluent from [https://www.confluent.io/download/](https://www.confluent.io/download/)
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[https://www.confluent.io/download/](https://www.confluent.io/download/)下载Confluent
- en: Click on Confluent Open Source
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击Confluent开源版
- en: 'Download the file `confluent-oss-4.0.0-2.11.tar.gz` from `tar.gz` and perform
    the following:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`tar.gz`下载文件`confluent-oss-4.0.0-2.11.tar.gz`并执行以下操作：
- en: '[PRE0]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Uncomment `listeners=PLAINTEXT://:9092`
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 取消注释`listeners=PLAINTEXT://:9092`
- en: 'Start Confluent:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Confluent：
- en: '[PRE1]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Start `zookeeper`:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动`zookeeper`：
- en: '[PRE2]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Start `kafka`:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动`kafka`：
- en: '[PRE3]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Start `schema-registry`:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动 `schema-registry`：
- en: '[PRE4]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Create topics
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建主题
- en: 'Perform the following steps to create topics:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以创建主题：
- en: List the existing topics
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列出现有主题
- en: 'Open another terminal and enter the following command:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开另一个终端并输入以下命令：
- en: '[PRE5]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Create a topic:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个主题：
- en: '[PRE6]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Double check the newly created topic:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 仔细检查新创建的主题：
- en: '[PRE7]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Generate messages to verify the producer and consumer
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成消息以验证生产者和消费者
- en: 'Perform the following steps to generate messages to verify the producer and
    consumer:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以生成消息以验证生产者和消费者：
- en: 'Send messages to Kafka `my-first-topic`:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向 Kafka `my-first-topic` 发送消息
- en: '[PRE8]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Start consumer to consume messages
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动消费者以消费消息
- en: 'Open another terminal and enter the following command:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开另一个终端并输入以下命令：
- en: '[PRE9]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Go to the producer terminal and enter another message:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往生产者终端并输入另一条消息：
- en: '[PRE10]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Verify the consumer terminal to check whether you can see the message `test4`
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证消费者终端以检查是否可以看到消息 `test4`
- en: Kafka Connect using file Source and Sink
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用文件源和目标连接器的 Kafka Connect
- en: 'Let''s take a look at how to create topics using file Source and Sink, with
    the help of the following:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用文件源和目标创建主题，以下是一些帮助：
- en: '[PRE11]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Perform the following steps:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤：
- en: 'Start the Source Connector and Sink Connector:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动源连接器和目标连接器：
- en: '[PRE12]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Double check whether the Kafka topic has received the messages:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 仔细检查 Kafka 主题是否已收到消息：
- en: '[PRE13]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Verify `target-file.txt`:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证 `target-file.txt`：
- en: '[PRE14]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Kafka Connect using JDBC and file Sink Connectors
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 JDBC 和文件目标连接器的 Kafka Connect
- en: 'The following image shows how we can push all records from the database table
    to a text file:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像显示了如何将数据库表中的所有记录推送到文本文件：
- en: '![](img/6d5fb5d5-d98a-4ec5-aef5-f3207b15beab.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6d5fb5d5-d98a-4ec5-aef5-f3207b15beab.png)'
- en: 'Let''s implement the preceding example using Kafka Connect:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 Kafka Connect 实现前面的示例：
- en: 'Install SQLite:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 SQLite：
- en: '[PRE15]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Configure the JDBC Source Connector:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置 JDBC 源连接器：
- en: '[PRE16]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Configure the file Sink Connector:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置文件目标连接器：
- en: '[PRE17]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Start Kafka Connect (`.jdbs` source and file Sink):'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动 Kafka Connect（`.jdbs` 源和文件目标连接器）：
- en: '[PRE18]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Verify the consumer:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证消费者：
- en: '[PRE19]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The `--new-consumer` option is deprecated and will be removed in a future major
    release. The new consumer is used by default if the `--bootstrap-server` option
    is provided:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '`--new-consumer` 选项已弃用，将在未来的主要版本中删除。如果提供了 `--bootstrap-server` 选项，则默认使用新消费者：'
- en: '[PRE20]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Verify the target file:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证目标文件：
- en: '[PRE21]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Insert a few more records in the customer table:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在客户表中插入更多记录：
- en: '[PRE22]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Verify the target file:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证目标文件：
- en: '[PRE23]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: You will see all customer records (`cust_id`) in the target file. Using the
    preceding example, you can customize and experiment with any other Sink.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在目标文件中看到所有客户记录（`cust_id`）。使用前面的示例，你可以自定义并实验任何其他目标连接器。
- en: 'The following table presents the available Kafka connectors on the Confluent
    platform (developed and fully supported by Confluent):'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格展示了 Confluent 平台上可用的 Kafka 连接器（由 Confluent 开发并完全支持）：
- en: '| **Connector Name** | **Source/Sink** |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| **连接器名称** | **源/目标** |'
- en: '| JDBC | Source and Sink |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| JDBC | 源和目标 |'
- en: '| HDFS | Sink |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| HDFS | 目标 |'
- en: '| Elasticsearch | Sink |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| Elasticsearch | 目标 |'
- en: '| Amazon S3 | Sink |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| Amazon S3 | 目标 |'
- en: For more information on other certified Connectors by Confluent, please use
    this URL: [https://www.confluent.io/product/connectors/](https://www.confluent.io/product/connectors/).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 Confluent 其他认证连接器的更多信息，请使用此 URL：[https://www.confluent.io/product/connectors/](https://www.confluent.io/product/connectors/)。
- en: You must have observed that Kafka Connect is a configuration-based stream-processing
    framework. It means we have to configure only the Source and Sink Connector files.
    We don't need to write any code using low-level languages like Java or Scala.
    But, now, let's turn to one more popular real-time stream processing framework
    called **Apache Storm**. Let's understand some cool features of Apache Storm.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 你必须已经观察到 Kafka Connect 是一个基于配置的流处理框架。这意味着我们只需要配置源和目标连接器文件。我们不需要使用 Java 或 Scala
    等低级语言编写任何代码。但现在，让我们转向另一个流行的实时流处理框架，称为 **Apache Storm**。让我们了解一些 Apache Storm 的酷特性。
- en: Apache Storm
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Storm
- en: Apache Storm is a free and open source distributed real-time stream processing
    framework. At the time of writing this book, the stable release version of Apache
    Storm is 1.0.5\. The Storm framework is predominantly written in the Clojure programming
    language. Originally, it was created and developed by Nathan Marz and the team
    at Backtype. The project was later acquired by Twitter.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Storm是一个免费且开源的分布式实时流处理框架。在撰写本书时，Apache Storm的稳定发布版本为1.0.5。Storm框架主要使用Clojure编程语言编写。最初，它是由Nathan
    Marz和Backtype团队创建和开发的。该项目后来被Twitter收购。
- en: During one of his talks on the Storm framework, Nathan Marz talked about stream
    processing applications using any framework, such as Storm. These applications
    involved queues and worker threads. Some of the data source threads write messages
    to queues and other threads pick up these messages and write to target data stores.
    The main drawback here is that source threads and targets threads do not match
    the data load of each other and this results in data pileup. It also results in
    data loss and additional thread maintenance.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在他关于Storm框架的一次演讲中，Nathan Marz谈到了使用任何框架（如Storm）的流处理应用程序。这些应用程序涉及队列和工作线程。一些数据源线程将消息写入队列，而其他线程则从队列中取出这些消息并写入目标数据存储。这里的主要缺点是源线程和目标线程不匹配各自的数据负载，这导致数据堆积。这也导致了数据丢失和额外的线程维护。
- en: To avoid the preceding challenges, Nathan Marz came up with a great architecture
    that abstracts source threads and worker threads into Spouts and Bolts. These
    Spouts and Bolts are submitted to the Topology framework, which takes care of
    entire stream processing.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免上述挑战，Nathan Marz提出了一种优秀的架构，该架构将源线程和工作线程抽象为Spouts和Bolts。这些Spouts和Bolts被提交到拓扑框架，该框架负责整个流处理。
- en: Features of Apache Storm
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Storm的功能
- en: Apache Storm is distributed. In case of an increase in a stream's workload,
    multiple nodes can be added to the Storm Cluster to add more workers and more
    processing power to process.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Storm是分布式的。在流的工作负载增加的情况下，可以向Storm集群添加多个节点以添加更多的工作者和更多的处理能力来处理。
- en: It is a truly real-time stream processing system and supports **low-latency**.
    The event can be reached from source to target in in milliseconds, seconds, or
    minutes depending on the use cases.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 它是一个真正的实时流处理系统，支持**低延迟**。事件可以从源到目标在毫秒、秒或分钟内到达，具体取决于用例。
- en: Storm framework supports **multiple programming languages**, but Java is the
    top preference. Storm is **fault-tolerant**. It continues to operate even though
    the failure of any node in the cluster. Storm is **reliable**. It supports at
    least once or exactly-once processing.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: Storm框架支持**多种编程语言**，但Java是首选。Storm是**容错的**。即使集群中的任何节点失败，它也会继续运行。Storm是**可靠的**。它支持至少一次或恰好一次的处理。
- en: There is **no complexity** to using Storm framework. For more detail information,
    refer to the Storm documentation: [http://storm.apache.org/releases/1.0.4/index.html](http://storm.apache.org/releases/1.0.4/index.html).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Storm框架**没有复杂性**。对于更详细的信息，请参阅Storm文档：[http://storm.apache.org/releases/1.0.4/index.html](http://storm.apache.org/releases/1.0.4/index.html)。
- en: Storm topology
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Storm拓扑
- en: 'The following image shows a typical **Storm Topology**:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像显示了典型的**Storm拓扑**：
- en: '![](img/f1bc575d-ac6b-465d-aee2-2d3573f4f495.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f1bc575d-ac6b-465d-aee2-2d3573f4f495.png)'
- en: Storm topology components
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Storm拓扑组件
- en: 'The following sections explain all the components of a Storm topology:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 以下章节解释了Storm拓扑的所有组件：
- en: '**Topology**:A topology is a **DAG** (**directed acyclic graph**) of spouts
    and bolts that are connected with stream groupings. A topology runs continuously
    untill it is killed.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**拓扑**：拓扑是由流分组连接的spouts和bolts组成的**DAG（有向无环图**）。拓扑会持续运行，直到被终止。'
- en: '**Stream**:A stream is an unbounded sequence of tuples. A tuple can be of any
    data type. It supports all the Java data types.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流**：流是一个无界的元组序列。元组可以是任何数据类型。它支持所有Java数据类型。'
- en: '**Stream groupings**:Stream grouping decides which bolt receives a tuple from
    a spout. Basically, these are the strategies about how the stream will flow among
    different bolts. The following are the built-in stream groupings in Storm.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流分组**：流分组决定了哪个bolt从spout接收元组。基本上，这些都是关于流如何在不同的bolt之间流动的策略。以下是Storm中内置的流分组。'
- en: '**Shuffle grouping**:It is a default grouping strategy. Tuples are randomly
    distributed and each bolt gets an equal number of streams to process.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机分组**：这是一个默认的分组策略。元组被随机分布，每个bolt都会得到相同数量的流进行处理。'
- en: '**Field grouping**:In this strategy, the same value of a stream field will
    be sent to one bolt. For example, if all the tuples are grouped by `customer_id`,
    then all the tuples of the same `customer_id` will be sent one bolt task and all
    the tuples of another `customer_id` will be sent to another bolt task.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**字段分组**: 在这种策略中，流字段的相同值将被发送到同一个Bolt。例如，如果所有元组都是按`customer_id`分组的，那么相同`customer_id`的所有元组将被发送到同一个Bolt任务，而另一个`customer_id`的所有元组将被发送到另一个Bolt任务。'
- en: '**All grouping**:In all grouping, each tuple is sent to each bolt task. It
    can be used when two different functions have to be performed on the same set
    of data. In that case, the stream can be replicated and each function can be calculated
    on each copy of the data.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全部分组**: 在全部分组中，每个元组被发送到每个Bolt任务。当需要在同一组数据上执行两个不同的函数时，可以使用它。在这种情况下，流可以被复制，每个函数可以在数据的每个副本上计算。'
- en: '**Direct grouping**: This is a special kind of grouping. Here, the developer
    can define the grouping logic within the component where tuple is emitted itself.
    The producer of the tuple decides which task of the consumer will receive this
    tuple.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**直接分组**: 这是一个特殊的分组类型。在这里，开发者可以在元组本身发射的组件中定义分组逻辑。元组的生产者决定哪个消费者的任务将接收这个元组。'
- en: '**Custom grouping**:The developer may decide to implement his/her own grouping
    strategy by implementing the `CustomGrouping` method.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自定义分组**: 开发者可以通过实现`CustomGrouping`方法来决定实现自己的分组策略。'
- en: '**Spout**:A spout connects to the data source and ingests streams into a Storm
    topology.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spout**: Spout连接到数据源并将流数据摄入到Storm拓扑中。'
- en: '**Bolt**:A spout emits a tuple to a bolt. A bolt is responsible for event transformation,
    joining events to other events, filtering, aggregation, and windowing. It emits
    the tuple to another bolt or persists it to a target. All processing in topologies
    is done in bolts. Bolts can do anything from filtering to functions, aggregations,
    joins, talking to databases, and more.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Bolt**: 一个喷口向Bolt发送一个元组。Bolt负责事件转换、将事件与其他事件连接、过滤、聚合和窗口化。它将元组发送到另一个Bolt或持久化到目标。在拓扑中所有处理都在Bolt中完成。Bolt可以执行从过滤到函数、聚合、连接、与数据库通信等任何操作。'
- en: '**Storm Cluster**: Thefollowing image shows all the components of a **Storm
    Cluster**:'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Storm集群**: 下图显示了**Storm集群**的所有组件：'
- en: '![](img/668cfc89-82eb-40c3-89cd-a8ec15a32143.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/668cfc89-82eb-40c3-89cd-a8ec15a32143.png)'
- en: '**Storm Cluster nodes**:The three main nodes of a Storm Cluster are Nimbus,
    Supervisor, and Zookeeper. The following section explains all the components in
    detail.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Storm集群节点**: Storm集群的三个主要节点是Nimbus、Supervisor和ZooKeeper。以下部分将详细解释所有组件。'
- en: '**Nimbus node**:In Storm, this is the master node of a Storm Cluster. It distributes
    code and launches the worker tasks across the cluster. Basically, it assigns tasks
    to each node in a cluster. It also monitors the status of each job submitted.
    In the case of any job failure, Nimbus reallocates the job to a different supervisor
    within a cluster. In the case of Nimbus being unavailable, the workers will still
    continue to function. However, without Nimbus, workers won''t be reassigned to
    other machines when necessary. In the case of an unavailable node, the tasks assigned
    to that node will time-out and Nimbus will reassign those tasks to other machines. In
    the case of both Nimbus and Supervisor being unavailable, they need to be restarted
    like nothing happened and no worker processes will be affected.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Nimbus节点**: 在Storm中，这是Storm集群的主节点。它将代码分发到集群并启动工作任务。基本上，它将任务分配给集群中的每个节点。它还监控每个提交作业的状态。在作业失败的情况下，Nimbus将作业重新分配到集群中的不同管理节点。如果Nimbus不可用，工作进程仍然会继续运行。然而，没有Nimbus，当需要时，工作进程不会被重新分配到其他机器。在节点不可用的情况下，分配给该节点的任务将超时，Nimbus将把这些任务重新分配到其他机器。在Nimbus和Supervisor都不可用的情况下，它们需要像什么都没发生一样重新启动，并且不会影响任何工作进程。'
- en: '**Supervisor node**:In Storm, this is a slave node. It communicates with Nimbus
    through ZooKeeper. It starts and stops the worker processes within a supervisor
    itself. For example, if Supervisor finds that a particular worker process has
    died, then it immediately restarts that worker process. If Supervisor fails to
    restart the worker after trying few times, then it communicates this to Nimbus
    and Nimbus will restart that worker on a different Supervisor node.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**管理节点**: 在Storm中，这是一个从节点。它通过ZooKeeper与Nimbus通信。它在其自身的管理节点中启动和停止工作进程。例如，如果管理节点发现某个特定的工作进程已死亡，则它将立即重启该工作进程。如果管理节点在尝试几次后无法重启工作进程，则它将向Nimbus报告此情况，Nimbus将在不同的管理节点上重启该工作进程。'
- en: '**Zookeeper node**:It acts as a coordinator between masters (Nimbus) and slaves
    (supervisors) within a Storm Cluster. In a production environment, it is typical
    to set up a Zookeeper cluster that has three instances (nodes) of Zookeeper.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Zookeeper 节点**：它在 Storm 集群中的主节点（Nimbus）和从节点（supervisors）之间充当协调器。在生产环境中，通常设置一个包含三个
    Zookeeper 实例（节点）的 Zookeeper 集群。'
- en: Installing Storm on a single node cluster
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在单节点集群上安装 Storm
- en: 'The following are the steps to install Storm Cluster on a single machine:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在单机上安装 Storm 集群的步骤：
- en: 'Install `jdk`.Make sure you have installed 1.8:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 `jdk`。确保您已安装 1.8：
- en: '[PRE24]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'You should see the following output:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下输出：
- en: '[PRE25]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Create a folder to download the `.tar` file of Storm:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个文件夹以下载 Storm 的 `.tar` 文件：
- en: '[PRE26]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Create a folder to persist Zookeeper and Storm data:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个文件夹以持久化 Zookeeper 和 Storm 数据：
- en: '[PRE27]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Download Zookeeper and Storm:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载 Zookeeper 和 Storm：
- en: '[PRE28]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Configure Zookeeper and set the following to Zookeeper (`zoo.cfg`):'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置 Zookeeper 并将以下内容设置到 Zookeeper (`zoo.cfg`)：
- en: '[PRE29]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Configure Storm as follows:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照以下方式配置 Storm：
- en: '[PRE30]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Add the following:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加以下内容：
- en: '[PRE31]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: (for additional workers, add more ports, such as 6704 and so on)
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: （对于额外的工作者，添加更多端口，例如 6704 等）
- en: 'Start Zookeeper:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动 Zookeeper：
- en: '[PRE32]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Start Nimbus:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动 Nimbus：
- en: '[PRE33]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Start Supervisor:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动 Supervisor：
- en: '[PRE34]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Verify installation in the Storm UI:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Storm UI 中验证安装：
- en: '[PRE35]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Developing a real-time streaming pipeline with Storm
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发 Storm 的实时流式管道
- en: 'In this section, we will create the following three pipelines:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将创建以下三个管道：
- en: Streaming pipeline with Kafka - Storm - MySQL
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kafka - Storm - MySQL 的流式管道
- en: Streaming pipeline with Kafka - Storm - HDFS - Hive
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kafka - Storm - HDFS - Hive 的流式管道
- en: In this section, we will see how data streams flow from Kafka to Storm to MySQL
    table.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将了解数据流如何从 Kafka 流向 Storm 到 MySQL 表。
- en: 'The whole pipeline will work as follows:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 整个管道将按以下方式工作：
- en: We will ingest customer records (`customer_firstname` and `customer_lastname`)
    in Kafka using the Kafka console-producer API.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用 Kafka 控制台生产者 API 在 Kafka 中摄取客户记录（`customer_firstname` 和 `customer_lastname`）。
- en: After that, Storm will pull the messages from Kafka.
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，Storm 将从 Kafka 拉取消息。
- en: A connection to MySQL will be established.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将建立到 MySQL 的连接。
- en: Storm will use MySQL-Bolt to ingest records into MySQL table. MySQL will automatically
    generate `customer_id`.
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Storm 将使用 MySQL-Bolt 将记录摄取到 MySQL 表中。MySQL 将自动生成 `customer_id`。
- en: The MySQL table data (`customer_id`, `customer_firstname`, and `customer_lastname`)
    will be accessed using SQL.
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将使用 SQL 访问 MySQL 表数据（`customer_id`、`customer_firstname` 和 `customer_lastname`）。
- en: 'We will develop the following Java classes:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开发以下 Java 类：
- en: '`MysqlConnection.java`: This class will establish a connection with the local
    MySQL database.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MysqlConnection.java`：此类将与本地 MySQL 数据库建立连接。'
- en: '`MysqlPrepare.java`: This class will prepare the SQL statements to be inserted
    into the database.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MysqlPrepare.java`：此类将准备要插入数据库的 SQL 语句。'
- en: '`MysqlBolt`: This class is a storm bolt framework to emit the tuple from Kafka
    to MySQL.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MysqlBolt`：这是一个 storm bolt 框架，用于从 Kafka 发射元组到 MySQL。'
- en: '`MySQLKafkaTopology`: This is a Storm Topology Framework that builds a workflow
    to bind spouts (Kafka) to Bolts (MySQL). Here, we are using a Local Storm Cluster.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MySQLKafkaTopology`：这是一个 Storm Topology 框架，用于构建将 spouts（Kafka）绑定到 Bolts（MySQL）的工作流程。在这里，我们使用本地
    Storm 集群。'
- en: Streaming a pipeline from Kafka to Storm to MySQL
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从 Kafka 到 Storm 到 MySQL 的管道流式传输
- en: 'The following image shows the components of the pipeline. In this pipeline,
    we will learn how the messages will flow from Kafka to Storm to MySQL in real-time:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像显示了管道的组件。在这个管道中，我们将学习消息如何实时从 Kafka 流向 Storm 到 MySQL：
- en: '![](img/24ae013a-817c-478e-a2c5-04a08eb38f29.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/24ae013a-817c-478e-a2c5-04a08eb38f29.png)'
- en: 'The following is the complete Java code for `MysqlConnection.java`:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是为 `MysqlConnection.java` 编写的完整 Java 代码：
- en: '[PRE36]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The following is the complete code for `MySqlPrepare.java`:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是为 `MySqlPrepare.java` 编写的完整代码：
- en: '[PRE37]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The following is the complete code for `MySqlBolt.java`:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是为 `MySqlBolt.java` 编写的完整代码：
- en: '[PRE38]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The following is the complete code for `KafkaMySQLTopology.java`:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是为 `KafkaMySQLTopology.java` 编写的完整代码：
- en: '[PRE39]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Use the `pom.xml` file to build your project in IDE.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `pom.xml` 文件在 IDE 中构建您的项目。
- en: Streaming a pipeline with Kafka to Storm to HDFS
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Kafka 到 Storm 到 HDFS 的管道流式传输
- en: In this section, we will see how the data streams will flow from Kafka to Storm
    to HDFS and access them with a Hive external table.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将了解数据流如何从 Kafka 流向 Storm 到 HDFS，并使用 Hive 外部表访问它们。
- en: 'The following image shows the components of the pipeline. In this pipeline,
    we will learn how the messages will flow from Kafka to Storm to HDFS in real-time:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像显示了管道的组件。在这个管道中，我们将学习消息如何实时从 Kafka 流向 Storm 再流向 HDFS：
- en: '![](img/f7174858-5105-44ae-8eee-e52c6e8480af.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f7174858-5105-44ae-8eee-e52c6e8480af.png)'
- en: 'The whole pipeline will work as follows:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 整个管道将按以下方式工作：
- en: We will ingest customer records (`customer_id`, `customer_firstname`, and `customer_lastname`)
    in Kafka using the Kafka console-producer API
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用 Kafka 控制台生产者 API 在 Kafka 中导入客户记录（`customer_id`、`customer_firstname` 和
    `customer_lastname`）
- en: After that, Storm will pull the messages from Kafka
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，Storm 将从 Kafka 拉取消息
- en: A Connection to HDFS will be established
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将与 HDFS 建立连接
- en: Storm will use HDFS-Bolt to ingest records into HDFS
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Storm 将使用 HDFS-Bolt 将记录导入 HDFS
- en: Hive external table will be created to store (`customer_id`, `customer_firstname`,
    and `customer_lastname`)
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将创建 Hive 外部表以存储（`customer_id`、`customer_firstname` 和 `customer_lastname`）
- en: The Hive table data (`customer_id`, `customer_firstname`, and `customer_lastname`)
    will be accessed using SQL
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将使用 SQL 访问 Hive 表数据（`customer_id`、`customer_firstname` 和 `customer_lastname`）
- en: 'We will develop the following Java classes:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开发以下 Java 类：
- en: '`KafkaTopology.java`: This is a Storm Topology framework that builds a workflow
    to bind spouts (Kafka) to Bolts (HDFS). Here we are using a Local Storm cluster.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '`KafkaTopology.java`: 这是一个 Storm Topology 框架，用于构建工作流程以将 spouts（Kafka）绑定到 Bolts（HDFS）。在这里，我们使用的是本地
    Storm 集群。'
- en: In the previous example pipeline, multiple separate classes for data streams
    parsing and transformations can be developed to handle Kafka producers and consumers.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的示例管道中，可以开发多个独立的数据流解析和转换类来处理 Kafka 生产者和消费者。
- en: 'The following is the complete Java code for `KafkaToplogy.java`:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 `KafkaToplogy.java` 的完整 Java 代码：
- en: '[PRE40]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The Hive table for the same is as follows:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 相应的 Hive 表如下：
- en: '[PRE41]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Other popular real-time data streaming frameworks
  id: totrans-263
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他流行的实时数据流框架
- en: Apart from Apache Storm, there are quite a few other open source real-time data
    streaming frameworks. In this section, I will discuss in brief only open source
    non-commercial frameworks. But, at the end of this section, I will provide a few
    URLs for a few commercial vendor products that offer some very interesting features.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 Apache Storm 之外，还有相当多的其他开源实时数据流框架。在本节中，我将简要讨论仅限于开源非商业框架。但，在本节末尾，我将提供一些商业供应商产品的链接，这些产品提供了一些非常有趣的功能。
- en: Kafka Streams API
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka Streams API
- en: Kafka Streams is a library for building streaming applications. Kafka Streams
    is a client library for building applications and microservices, where the input
    and output data are stored in Kafka Clusters. The Kafka Streams API transforms
    and enriches the data.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka Streams 是用于构建流式应用程序的库。Kafka Streams 是用于构建应用程序和微服务的客户端库，其中输入和输出数据存储在 Kafka
    集群中。Kafka Streams API 转换并丰富了数据。
- en: 'The following are the important features of the Kafka Streams API:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 以下 Kafka Streams API 的重要特性：
- en: It is part of the open source Apache Kafka project.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是开源 Apache Kafka 项目的一部分。
- en: It supports per record streams processing with a very low latency (milliseconds).
    There is no micro- batching concept in the Kafka Streams API. Every record that
    comes into the stream is processed on its own.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它支持每条记录的低延迟流处理（毫秒级）。在 Kafka Streams API 中没有微批处理的概念。流中的每条记录都会单独处理。
- en: It supports stateless processing (filtering and mapping), stateful processing
    (joins and aggregations), and windowing operations (for example, counting the
    last minute, last 5 minutes, last 30 minutes, or last day's worth of data, and
    so on).
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它支持无状态处理（过滤和映射）、有状态处理（连接和聚合），以及窗口操作（例如，计算最后 1 分钟、最后 5 分钟、最后 30 分钟或最后一天的数据等）。
- en: To run the Kafka Streams API, there is no need to build a separate cluster that
    has multiple machines. Developers can use the Kafka Streams API in their Java
    applications or microservices to process real-time data.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要运行 Kafka Streams API，无需构建一个拥有多台机器的单独集群。开发者可以在他们的 Java 应用程序或微服务中使用 Kafka Streams
    API 来处理实时数据。
- en: The Kafka Streams API is highly scalable and fault-tolerant.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kafka Streams API 具有高度的扩展性和容错性。
- en: The Kafka Streams API is completely deployment agnostic. It can be deployed
    on a bare metal machine, VMs, Kubernetes containers, and on Cloud. There are no
    restrictions at all. Stream APIs are never deployed on Kafka Brokers. It is a
    separate application just like any other Java application, which is deployed outside
    of Kafka brokers.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kafka Streams API 完全与部署无关。它可以在裸机、虚拟机、Kubernetes 容器和云上部署。没有任何限制。流 API 从不部署在 Kafka
    代理上。它就像任何其他 Java 应用程序一样，是一个独立的应用程序，部署在 Kafka 代理之外。
- en: It uses the Kafka security model.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它使用 Kafka 安全模型。
- en: It supports exactly-once semantics since version 0.11.0.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自 0.11.0 版本以来，它支持恰好一次语义。
- en: Let's review the earlier image again to find out the exact place of the Kafka
    Streams API in the overall Kafka architecture.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次回顾一下早期的图像，以找出 Kafka Streams API 在整体 Kafka 架构中的确切位置。
- en: 'Here are a few useful URLs to understand Kafka Streams in detail:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些有用的网址，可以帮助您详细了解 Kafka Streams：
- en: '[https://kafka.apache.org/documentation/](https://kafka.apache.org/documentation/)'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://kafka.apache.org/documentation/](https://kafka.apache.org/documentation/)'
- en: '[https://www.confluent.io/blog/](https://www.confluent.io/blog/)'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.confluent.io/blog/](https://www.confluent.io/blog/)'
- en: '[https://www.confluent.io/blog/introducing-kafka-streams-stream-processing-made-simple/](https://www.confluent.io/blog/introducing-kafka-streams-stream-processing-made-simple/)'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.confluent.io/blog/introducing-kafka-streams-stream-processing-made-simple/](https://www.confluent.io/blog/introducing-kafka-streams-stream-processing-made-simple/)'
- en: '[https://docs.confluent.io/current/streams/index.html](https://docs.confluent.io/current/streams/index.html)'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://docs.confluent.io/current/streams/index.html](https://docs.confluent.io/current/streams/index.html)'
- en: Spark Streaming
  id: totrans-282
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark Streaming
- en: Please note that we will discuss Spark in [Chapter 7](39cf9925-e4cc-473a-9032-a21b81e7b400.xhtml),
    *Large-Scale Data Processing Frameworks,* which is fully dedicated to Spark. However,
    in this section, I will discuss some important features of Spark Streaming. For
    better understanding, readers are advised to study [Chapter 7](39cf9925-e4cc-473a-9032-a21b81e7b400.xhtml), *Large-Scale
    Data Processing Frameworks* first and come back to read this section further to
    understand more about Spark Streaming.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们将在第 7 章[大型数据处理框架](39cf9925-e4cc-473a-9032-a21b81e7b400.xhtml)中讨论 Spark，该章完全致力于
    Spark。然而，在本节中，我将讨论 Spark Streaming 的一些重要特性。为了更好地理解，建议读者首先学习第 7 章[大型数据处理框架]，然后再回来阅读本节以了解更多关于
    Spark Streaming 的内容。
- en: It is a general practice to use Hadoop MapReduce for batch processing and Apache
    Storm for real-time stream processing.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Hadoop MapReduce 进行批处理，使用 Apache Storm 进行实时流处理是一种通用做法。
- en: The use of these two different programming models causes an increase in code
    size, number of bugs to fix, and development effort; it also introduces a learning
    curve and causes other issues. Spark Streaming helps fix these issues and provides
    a scalable, efficient, resilient, and integrated (with batch processing) system.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这两种不同的编程模型会导致代码量增加、需要修复的错误数量以及开发工作量的增加；它还引入了学习曲线并导致其他问题。Spark Streaming 有助于解决这些问题，并提供一个可扩展的、高效的、健壮的、且与批处理集成的系统。
- en: The strength of Spark Streaming lies in its ability to combine with batch processing.
    It's possible to create a RDD using normal Spark programming and join it with
    a Spark stream. Moreover, the code base is similar and allows easy migration if
    required—and there is zero to no learning curve from Spark.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming 的优势在于其能够与批处理相结合。可以使用常规 Spark 编程创建 RDD 并将其与 Spark 流连接。此外，代码库相似，如果需要，可以轻松迁移——并且从
    Spark 来看，学习曲线几乎为零。
- en: 'Spark Streaming is an extension of the core Spark API. It extends Spark for
    doing real-time stream processing. Spark Streaming has the following features:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming 是核心 Spark API 的扩展。它扩展 Spark 以进行实时流处理。Spark Streaming 具有以下特点：
- en: It's scalable—it scales on hundreds of nodes
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是可扩展的——可以在数百个节点上进行扩展
- en: It provides high-throughput and achieves second level latency
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它提供高吞吐量并实现第二级延迟
- en: It's fault-tolerant and it efficiently receives from the failures
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它具有容错性，并且能够有效地从失败中恢复
- en: It integrates with batch and interactive data processing
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它与批处理和交互式数据处理集成
- en: Spark Streaming processes data streams application as a series of very small,
    deterministic batch jobs.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming 将数据流应用程序处理为一系列非常小、确定性的批处理作业。
- en: Spark Streaming provides an API in Scala, Java, and Python. Spark Streaming
    divides live stream of data into multiple batches based on time. The time can
    range from one second to a few minutes/hours. In general, batches are divided
    into a few seconds. Spark treats each batch as a RDD and process each based on
    RDD operations (map, filter, join flatmap, distinct, reduceByKey, and so on).
    Lastly, the processed results of RDDs are returned in batches.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming提供了Scala、Java和Python的API。Spark Streaming根据时间将实时数据流划分为多个批次。时间可以从一秒到几分钟/小时不等。通常，批次被划分为几秒。Spark将每个批次视为RDD，并根据RDD操作（map、filter、join
    flatmap、distinct、reduceByKey等）进行处理。最后，RDD的处理结果以批次的格式返回。
- en: 'The following image depicts the Spark Streaming data flow:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像展示了Spark Streaming的数据流：
- en: '![](img/5a221209-8e20-480b-8121-1d6c49e99f61.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5a221209-8e20-480b-8121-1d6c49e99f61.png)'
- en: 'Here are few useful URLs for understanding Spark Streaming in detail:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些有用的URL，用于详细了解Spark Streaming：
- en: '[https://databricks.com/blog](https://databricks.com/blog)'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://databricks.com/blog](https://databricks.com/blog)'
- en: '[https://databricks.com/blog/category/engineering/streaming](https://databricks.com/blog/category/engineering/streaming)'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://databricks.com/blog/category/engineering/streaming](https://databricks.com/blog/category/engineering/streaming)'
- en: '[https://spark.apache.org/streaming/](https://spark.apache.org/streaming/)'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://spark.apache.org/streaming/](https://spark.apache.org/streaming/)'
- en: Apache Flink
  id: totrans-300
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Flink
- en: 'Apache Flink''s documentation describes Flink in the following way: Flink is
    an open-source framework for distributed stream processing.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Flink的文档这样描述Flink：Flink是一个开源的分布式流处理框架。
- en: Flink provides accurate results and supports out-of-order or late-arriving datasets.
    It is stateful and fault-tolerant and can seamlessly recover from failures while
    maintaining an exactly-once application state. It performs at a large scale, running
    on thousands of nodes with very good throughput and latency characteristics.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: Flink提供准确的结果，并支持无序或迟到数据集。它是状态化的和容错的，可以在保持恰好一次应用程序状态的同时无缝恢复失败。它在大型规模上运行，在数千个节点上运行，具有非常好的吞吐量和延迟特性。
- en: 'The following are the features of Apache Flink:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列出的是Apache Flink的特性：
- en: Flink guarantees exactly-once semantics for stateful computations
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flink为状态化计算保证了恰好一次语义
- en: Flink supports stream processing and windowing with event time semantics
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flink支持具有事件时间语义的流处理和窗口
- en: Flink supports flexible windowing based on time, count, or sessions, in addition
    to data-driven windows
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flink支持基于时间、计数或会话的灵活窗口，除了数据驱动的窗口
- en: Flink is capable of high throughput and low latency
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flink具有高吞吐量和低延迟的能力
- en: Flink's savepoints provide a state versioning mechanism, making it possible
    to update applications or reprocess historic data with no lost state and minimal
    downtime
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flink的savepoints提供了一个状态版本化机制，使得更新应用程序或重新处理历史数据成为可能，而不会丢失状态和最小化停机时间
- en: Flink is designed to run on large-scale clusters with many thousands of nodes,
    and, in addition to a standalone cluster mode, Flink provides support for YARN
    and Mesos
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flink设计用于在具有数千个节点的超大规模集群上运行，除了独立集群模式外，Flink还提供了对YARN和Mesos的支持
- en: Flink's core is a distributed streaming dataflow engine. It supports processing
    one stream at a time rather than processing an entire batch of streams at a time.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: Flink的核心是一个分布式流数据流引擎。它支持一次处理一个流，而不是一次处理整个流的批次。
- en: 'Flink supports the following libraries:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: Flink支持以下库：
- en: CEP
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CEP
- en: Machine learning
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习
- en: Graph processing
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图形处理
- en: Apache Storm compatibility
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Storm兼容性
- en: 'Flink supports the following APIs:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: Flink支持以下API：
- en: '**DataStream API**: This API helps all the streams, transformations, that is,
    filtering, aggregations, counting, and windowing'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DataStream API**：此API帮助所有流，转换，即过滤、聚合、计数和窗口'
- en: '**DataSet API**: This API helps all the batch data transformations, that is,
    join, group, map, and filter'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DataSet API**：此API帮助所有批数据转换，即连接、分组、映射和过滤'
- en: '**Table API**: Supports SQL over relational data streams'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**表API**：支持在关系数据流上使用SQL'
- en: '**Streaming SQL**: Supports SQL over batch and streaming tables'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流式SQL**：支持在批处理和流式表上使用SQL'
- en: 'The following image describes the Flink programming model:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像描述了Flink编程模型：
- en: '![](img/d7ae2f42-7921-40b3-a961-a6a3e13bd11c.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d7ae2f42-7921-40b3-a961-a6a3e13bd11c.png)'
- en: 'The following image describes the Flink architecture:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像描述了Flink架构：
- en: '![](img/4f2dfbe0-b9ed-4693-8908-d73d07e4c0f9.png)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4f2dfbe0-b9ed-4693-8908-d73d07e4c0f9.png)'
- en: 'The following are the components of the Flink programming model:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列出的是Flink编程模型的组件：
- en: '**Source**: A data source where data is collected and sent to the Flink engine'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**源**：收集数据并发送到 Flink 引擎的数据源'
- en: '**Transformation**: In this component the whole transformation takes place'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**转换**：在这个组件中，整个转换过程发生'
- en: '**Sink**: A target where processed streams are sent'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**汇入点**：处理后的流被发送到的目标'
- en: 'Here are a few useful URLs to understand Spark Streaming in detail:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些有用的网址，可以详细了解 Spark Streaming：
- en: '[https://ci.apache.org/projects/flink/flink-docs-release-1.4/](https://ci.apache.org/projects/flink/flink-docs-release-1.4/)'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://ci.apache.org/projects/flink/flink-docs-release-1.4/](https://ci.apache.org/projects/flink/flink-docs-release-1.4/)'
- en: '[https://www.youtube.com/watch?v=ACS6OM1-xgE&amp;amp;feature=youtu.be](https://www.youtube.com/watch?v=ACS6OM1-xgE&feature=youtu.be)'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.youtube.com/watch?v=ACS6OM1-xgE&feature=youtu.be](https://www.youtube.com/watch?v=ACS6OM1-xgE&feature=youtu.be)'
- en: In the following sections, we will take a look at a comparison of various stream
    frameworks.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将探讨各种流框架的比较。
- en: Apache Flink versus Spark
  id: totrans-333
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Flink 与 Spark 的比较
- en: The main focus of Spark Streaming is stream-batching operation, called **micro-batching**.
    This programming model suits many use cases, but not all use cases require real-time
    stream processing with sub-second latency. For example, a use case such as credit
    card fraud prevention requires millisecond latency. Hence, the micro-batching
    programming model is not suited there. (But, the latest version of Spark, 2.4,
    supports millisecond data latency).
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming 的主要重点是流批处理操作，称为 **微批处理**。这种编程模型适用于许多用例，但并非所有用例都需要亚秒级延迟的实时流处理。例如，像信用卡欺诈预防这样的用例需要毫秒级延迟。因此，微批处理编程模型不适用于这种情况。（但，Spark
    的最新版本 2.4 支持毫秒级数据延迟。）
- en: Apache Flink supports millisecond latency and is suited for use cases such as
    fraud detection and like.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Flink 支持毫秒级延迟，适用于诸如欺诈检测等用例。
- en: Apache Spark versus Storm
  id: totrans-336
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark 与 Storm 的比较
- en: Spark uses micro-batches to process events while Storm processes events one
    by one. It means that Spark has a latency of seconds while Storm provides a millisecond
    of latency. Spark Streaming provides a high-level abstraction called a **Discretized
    Stream** or **DStream**, which represents a continuous sequence of RDDs. (But,
    the latest version of Spark, 2.4 supports millisecond data latency.) The latest
    Spark version supports DataFrames.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 使用微批处理来处理事件，而 Storm 则逐个处理事件。这意味着 Spark 的延迟为秒级，而 Storm 提供毫秒级延迟。Spark Streaming
    提供了一个高级抽象，称为 **离散流** 或 **DStream**，它表示 RDD 的连续序列。（但，Spark 的最新版本 2.4 支持毫秒级数据延迟。）最新的
    Spark 版本支持 DataFrame。
- en: Almost the same code (API) can be used for Spark Streaming and Spark batch jobs.
    That helps to reuse most of the code base for both programming models. Also, Spark
    supports Machine learning and the Graph API. So, again, the same codebase can
    be used for those use cases as well.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎可以使用相同的代码（API）进行 Spark Streaming 和 Spark 批处理作业。这有助于重用这两种编程模型的大部分代码库。此外，Spark
    支持机器学习和图 API。因此，同样，相同的代码库也可以用于这些用例。
- en: Summary
  id: totrans-339
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we started with a detailed understanding of real-time stream
    processing concepts, including data stream, batch vs. real-time processing, CEP,
    low latency, continuous availability, horizontal scalability, storage, and so
    on. Later, we learned about Apache Kafka, which is a very important component
    of modern real-time stream data pipelines. The main features of Kafka are scalability,
    durability, reliability, and high throughput.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首先详细了解了实时流处理概念，包括数据流、批处理与实时处理、CEP、低延迟、连续可用性、水平可扩展性、存储等。后来，我们学习了 Apache
    Kafka，它是现代实时流数据管道的重要组成部分。Kafka 的主要特性是可扩展性、持久性、可靠性和高吞吐量。
- en: We also learned about Kafka Connect; its architecture, data flow, sources, and
    connectors. We studied case studies to design a data pipeline with Kafka Connect
    using file source, file Sink, JDBC source, and file Sink Connectors.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还学习了 Kafka Connect；其架构、数据流、源和连接器。我们研究了案例研究，使用 Kafka Connect 的文件源、文件汇入点、JDBC
    源和文件汇入点连接器来设计数据管道。
- en: In the later sections, we learned about various open source real-time stream-processing
    frameworks, such as the Apache Storm framework. We have seen a few practical examples,
    as well. Apache Storm is distributed and supports low-latency and multiple programming
    languages. Storm is fault-tolerant and reliable. It supports at least once or
    exactly-once processing.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 在后面的章节中，我们学习了各种开源实时流处理框架，例如 Apache Storm 框架。我们也看到了一些实际的应用示例。Apache Storm 是一个分布式框架，支持低延迟和多种编程语言。Storm
    具有容错性和可靠性，支持至少一次或恰好一次的处理。
- en: Spark Streaming helps to fix these issues and provides a scalable, efficient,
    resilient, and integrated (with batch processing) system. The strength of Spark
    Streaming lies in its ability to combine with batch processing. Spark Streaming
    is scalable, and provides high-throughput. It supports micro-batching for second
    level latency, is fault-tolerant, and integrates with batch and interactive data
    processing.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming 有助于解决这些问题，并提供了一个可扩展、高效、弹性且与批量处理集成的系统。Spark Streaming 的优势在于其与批量处理的结合能力。Spark
    Streaming 可扩展，提供高吞吐量。它支持微批处理以实现二级延迟，具有容错性，并与批量及交互式数据处理集成。
- en: Apache Flink guarantees exactly-once semantics, supports event time semantics,
    high throughput, and low latency. It is designed to run on large-scale clusters.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Flink 保证恰好一次的语义，支持事件时间语义，高吞吐量和低延迟。它被设计用于在大型集群上运行。
