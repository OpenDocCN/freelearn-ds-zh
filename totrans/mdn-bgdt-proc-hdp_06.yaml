- en: Designing Real-Time Streaming Data Pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first three chapters of this book all dealt with batch data. Having learned
    about the installation of Hadoop, data ingestion tools and techniques, and data
    stores, let's turn to data streaming. Not only will we look at how we can handle
    real-time data streams, but also how to design pipelines around them.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Real-time streaming concepts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-time streaming components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Flink versus Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Spark versus Storm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-time streaming concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's understand a few key concepts relating to real-time streaming applications
    in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Data stream
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The data stream is a continuous flow of data from one end to another end, from
    sender to receiver, from producer to consumer. The speed and volume of the data
    may vary; it may be 1 GB of data per second or it may be 1 KB of data per second
    or per minute.
  prefs: []
  type: TYPE_NORMAL
- en: Batch processing versus real-time data processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In batch processing, data is collected in batches and each batch is sent for
    processing. The batch interval can be anything from one day to one minute. In
    today's data analytics and business intelligence world, data will not be processed
    in a batch for more than one day. Otherwise, business teams will not have any
    insight about what's happening to the business in a day-to-day basis. For example,
    the enterprise data warehousing team may collect all the orders made during the
    last 24 hours and send all these collected orders to the analytics engine for
    reporting.
  prefs: []
  type: TYPE_NORMAL
- en: The batch can be of one minute too. In the Spark framework (we will learn Spark
    in [Chapter 7](39cf9925-e4cc-473a-9032-a21b81e7b400.xhtml), *Large-Scale Data
    Processing Frameworks*), data is processed in micro batches.
  prefs: []
  type: TYPE_NORMAL
- en: In real-time processing, data (event) is transferred (streamed) from the producer
    (sender) to the consumer (receiver) as soon as an event is produced at the source
    end. For example, on an e-commerce website, orders gets processed immediately
    in an analytics engine as soon as the customer places the same order on that website.
    The advantage is that the business team of that company gets full insights about
    its business in real time (within a few milliseconds or sub milliseconds). It
    will help them adjust their promotions to increase their revenue, all in real-time.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image explains stream-processing architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/01854cc6-c9ec-45f8-b35d-c5d9528176c0.png)'
  prefs: []
  type: TYPE_IMG
- en: Complex event processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Complex event processing** (**CEP**) is event processing that combines data
    from multiple sources to discover complex relationships or patterns. The goal
    of CEP is to identify meaningful events (such as opportunities or threats) and
    respond to them as quickly as possible. Fundamentally, CEP is about applying business
    rules to streaming event data. For example, CEP is used in use cases, such as
    stock trading, fraud detection, medical claim processing, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image explains stream-processing architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c7c76b39-7e99-4115-9d6c-8b65f0dbec50.png)'
  prefs: []
  type: TYPE_IMG
- en: Continuous availability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Any real-time application is expected to be available all the time with no stoppage
    whatsoever. The event collection, processing, and storage components should be
    configured with the underlined assumptions of high availability. Any failure to
    any components will cause major disruptions to the running of the business. For
    example, in a credit card fraud detection application, all the fraudulent transactions
    need to be declined. If the application stops midway and is unable to decline
    fraudulent transactions, then it will result in heavy losses.
  prefs: []
  type: TYPE_NORMAL
- en: Low latency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In any real-time application, the event should flow from source to target in
    a few milliseconds. The source collects the event, and a processing framework
    moves the event to its target data store where it can be analyzed further to find
    trends and patterns. All these should happen in real time, otherwise it may impact
    business decisions. For example, in a credit card fraud detection application,
    it is expected that all incoming transactions should be analyzed to find possible
    fraudulent transactions, if any. If the stream processing takes more than the
    desired period of time, it may be possible that these transactions may pass through
    the system, causing heavy losses to the business.
  prefs: []
  type: TYPE_NORMAL
- en: Scalable processing frameworks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hardware failure may cause disruption to the stream processing application.
    To avoid this common scenario, we always need a processing framework that offers
    built-in APIs to support continuous computation, fault tolerant event state management,
    checkpoint features in the event of failures, in-flight aggregations, windowing,
    and so on. Fortunately, all the recent Apache projects such as Storm, Spark, Flink,
    and Kafka do support all and more of these features out of the box. The developer
    can use these APIs using Java, Python, and Scala.
  prefs: []
  type: TYPE_NORMAL
- en: Horizontal scalability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The stream-processing platform should support horizontal scalability. That means
    adding more physical servers to the cluster in the event of a higher incoming
    data load to maintain throughput SLA. This way, the performance of processing
    can be increased by adding more nodes rather than adding more CPUs and memory
    to the existing servers; this is called **vertical scalability**.
  prefs: []
  type: TYPE_NORMAL
- en: Storage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The preferable format of a stream is key-value pair. This format is very well
    represented by the JSON and Avro formats. The preferred storage to persist key-value
    type data is NoSQL data stores such as HBase and Cassandra. There are in total
    100 NoSQL open source databases in the market these days. It's very challenging
    to choose the right database, one which supports storage to real-time events,
    because all these databases offer some unique features for data persistence. A
    few examples are schema agnostic, highly distributable, commodity hardware support,
    data replication, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image explains all stream processing components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ddc50632-2e71-4f82-805a-f60ab3170c1e.png)'
  prefs: []
  type: TYPE_IMG
- en: In this chapter we will talk about message queue and stream processing frameworks
    in detail. In the next chapter, we will focus on data indexing techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Real-time streaming components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the following sections we will walk through some important real-time streaming
    components.
  prefs: []
  type: TYPE_NORMAL
- en: Message queue
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The message queue lets you publish and subscribe to a stream of events/records.
    There are various alternatives we can use as a message queue in our real-time
    stream architecture. For example, there is RabbitMQ, ActiveMQ, and Kafka. Out
    of these, Kafka has gained tremendous popularity due to its various unique features.
    Hence, we will discuss the architecture of Kafka in detail. A discussion of RabbitMQ
    and ActiveMQ is beyond the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: So what is Kafka?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kafka is a fast, scalable, durable, and fault-tolerant publish-subscribe messaging
    system. Apache Kafka is an open-source stream-processing project. It provides
    a unified, high-throughput, and is a low-latency platform for handling real-time
    data streams. It provides a distributed storage layer, which supports massively
    scalable pub/sub message queues. Kafka Connect supports data import and export
    by connecting to external systems. Kafka Streams provides Java APIs for stream
    processing. Kafka works in combination with Apache Spark, Apache Cassandra, Apache
    HBase, Apache Spark, and more for real-time stream processing.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Kafka was originally developed by LinkedIn, and was subsequently open
    sourced in early 2011\. In November 2014, several engineers who worked on Kafka
    at LinkedIn created a new company named Confluent with a focus on Kafka. Please
    use this URL [https://www.confluent.io/](https://www.confluent.io/) to learn more
    about the Confluent platform.
  prefs: []
  type: TYPE_NORMAL
- en: Kafka features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are features of Kafka:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Kafka is scalable**: Kafka Cluster consists of more than one physical server,
    which helps to distribute the data load. It is easily scalable in the event that
    additional throughputs are required as additional servers can be added to maintain
    the SLA.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kafka is durable**: During stream processing, Kafka persists messages on
    the persistent storage. This storage can be server local disks or Hadoop Cluster.
    In the event of message processing failure, the message can be accessed from the
    disk and replayed to process the message again. By default, the message is stored
    for seven days; this can be configured further.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kafka is reliable**: Kafka provides message reliability with the help of
    a feature called **data replication**. Each message is replicated at least three
    times (this is configurable) so that in the event of data loss, the copy of the
    message can be used for processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kafka supports high performance throughput**: Due to its unique architecture,
    partitioning, message storage, and horizontal scalability, Kafka helps to process
    terabytes of data per second.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kafka architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following image shows Kafka''s architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '**![](img/50297cdb-a34b-439f-be63-572e999c5c87.png)**'
  prefs: []
  type: TYPE_NORMAL
- en: Kafka architecture components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s take a look at each component in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Producers**: Producers publish messages to a specific Kafka topic. Producers
    may attach a key to each message record. By default, producers publish messages
    to topic partitions in round robin fashion. Sometimes, producers can be configured
    to write messages to a particular topic partition based on the hash value of message
    key.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Topic**: All messages are stored in a topic. A topic is a category or feed
    name to which records are published. A topic can be compared to a table in a relational
    database. Multiple consumers can be subscribed to a single topic to consume message
    records.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Partition**: A topic is divided into multiple partitions. Kafka offers topic
    parallelism by dividing topic into partitions and by placing each partition on
    a separate broker (server) of a Kafka Cluster. Each partition has a separate partition
    log on a disk where messages are stored. Each partition contains an ordered, immutable
    sequence of messages. Each message is assigned unique sequence numbers called
    **offset**. Consumers can read messages from any point from a partition—from the
    beginning or from any offset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Consumers**: A consumer subscribes to a topic and consumes the messages.
    In order to increase the scalability, consumers of the same application can be
    grouped into a consumer group where each consumer can read messages from a unique
    partition.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Brokers**: Kafka is divided into multiple servers called **brokers**. All
    the brokers combined are called **Kafka Cluster**. Kafka brokers handle message
    writes from producers and message reads from consumers. Kafka Brokers stores all
    the messages coming from producers. The default period is of seven days. This
    period (retention period) can be configured based on the requirements. The retention
    period directly impacts the local storage of Kafka brokers. It takes more storage
    if a higher retention period is configured. After the retention period is over,
    the message is automatically discarded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kafka Connect**: According to Kafka documentation, Kafka Connect allows the
    building and running of reusable producers or consumers that connect Kafka topics
    to existing applications or data systems. For example, a connector to a relational
    database might capture every change to a table.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kafka Streams**: Stream APIs allow an application to act as a stream processor,
    consuming an input stream from one or more topics and producing an output stream
    to one or more output topics, effectively transforming the input streams to output
    streams.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kafka Connect deep dive
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kafka Connect is a part of the Confluent platform. It is integrated with Kafka.
    Using Kafka Connect, it's very easy to build data pipelines from multiple sources
    to multiple targets. **Source Connectors** import data from another system (for
    example, from a relational database into Kafka) and **Sink Connectors **export
    data (for example, the contents of a Kafka topic to an HDFS file).
  prefs: []
  type: TYPE_NORMAL
- en: Kafka Connect architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following image shows Kafka Connect''s architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cbd6b15a-6f36-4e57-b6fd-0e01ae874bb4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The data flow can be explained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Various sources are connected to **Kafka Connect Cluster**. **Kafka Connect
    Cluster** pulls data from the sources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kafka Connect Cluster** consists of a set of worker processes that are containers
    that execute connectors, and tasks automatically coordinate with each other to
    distribute work and provide scalability and fault tolerance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kafka Connect Cluster** pushes data to **Kafka Cluster**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kafka Cluster** persists the data on to the broker local disk or on Hadoop.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streams applications such as Storm, Spark Streaming, and Flink pull the data
    from **Kafka Cluster** for stream transformation, aggregation, join, and so on.
    These applications can send back the data to Kafka or persist it to external data
    stores such as HBase, Cassandra, MongoDB, HDFS, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kafka Connect Cluster** pulls data from **Kafka Cluster** and pushes it Sinks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Users can extend the existing Kafka connectors or develop brand new connectors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kafka Connect workers standalone versus distributed mode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Users can run Kafka Connect in two ways: standalone mode or distributed mode.'
  prefs: []
  type: TYPE_NORMAL
- en: In standalone mode, a single process runs all the connectors. It is not fault
    tolerant. Since it uses only a single process, it is not scalable. Generally,
    it is useful for users for development and testing purposes.
  prefs: []
  type: TYPE_NORMAL
- en: In distributed mode, multiple workers run Kafka Connect. In this mode, Kafka
    Connect is scalable and fault tolerant, so it is used in production deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s learn more about Kafka and Kafka Connect (standalone mode). In this
    example, we will do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Install Kafka
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a topic
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate a few messages to verify the producer and consumer
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kafka Connect-File-source and file-sink
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kafka Connect-JDBC -Source
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following image shows a use case using **Kafka Connect**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a8818e04-6229-45ec-9063-ca5a4c86a8a9.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's see how Kafka and Kafka Connect works by running a few examples. For more
    details, use the following link for Kafka Confluent's documentation: [https://docs.confluent.io/current/](https://docs.confluent.io/current/).
  prefs: []
  type: TYPE_NORMAL
- en: Install Kafka
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s perform the following steps to install Kafka:'
  prefs: []
  type: TYPE_NORMAL
- en: Download Confluent from [https://www.confluent.io/download/](https://www.confluent.io/download/)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on Confluent Open Source
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Download the file `confluent-oss-4.0.0-2.11.tar.gz` from `tar.gz` and perform
    the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Uncomment `listeners=PLAINTEXT://:9092`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Start Confluent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Start `zookeeper`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Start `kafka`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Start `schema-registry`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Create topics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Perform the following steps to create topics:'
  prefs: []
  type: TYPE_NORMAL
- en: List the existing topics
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open another terminal and enter the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a topic:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Double check the newly created topic:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Generate messages to verify the producer and consumer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Perform the following steps to generate messages to verify the producer and
    consumer:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Send messages to Kafka `my-first-topic`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Start consumer to consume messages
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open another terminal and enter the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Go to the producer terminal and enter another message:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Verify the consumer terminal to check whether you can see the message `test4`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kafka Connect using file Source and Sink
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s take a look at how to create topics using file Source and Sink, with
    the help of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start the Source Connector and Sink Connector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Double check whether the Kafka topic has received the messages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Verify `target-file.txt`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Kafka Connect using JDBC and file Sink Connectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following image shows how we can push all records from the database table
    to a text file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d5fb5d5-d98a-4ec5-aef5-f3207b15beab.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s implement the preceding example using Kafka Connect:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install SQLite:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Configure the JDBC Source Connector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Configure the file Sink Connector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Start Kafka Connect (`.jdbs` source and file Sink):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Verify the consumer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The `--new-consumer` option is deprecated and will be removed in a future major
    release. The new consumer is used by default if the `--bootstrap-server` option
    is provided:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Verify the target file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Insert a few more records in the customer table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Verify the target file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: You will see all customer records (`cust_id`) in the target file. Using the
    preceding example, you can customize and experiment with any other Sink.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table presents the available Kafka connectors on the Confluent
    platform (developed and fully supported by Confluent):'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Connector Name** | **Source/Sink** |'
  prefs: []
  type: TYPE_TB
- en: '| JDBC | Source and Sink |'
  prefs: []
  type: TYPE_TB
- en: '| HDFS | Sink |'
  prefs: []
  type: TYPE_TB
- en: '| Elasticsearch | Sink |'
  prefs: []
  type: TYPE_TB
- en: '| Amazon S3 | Sink |'
  prefs: []
  type: TYPE_TB
- en: For more information on other certified Connectors by Confluent, please use
    this URL: [https://www.confluent.io/product/connectors/](https://www.confluent.io/product/connectors/).
  prefs: []
  type: TYPE_NORMAL
- en: You must have observed that Kafka Connect is a configuration-based stream-processing
    framework. It means we have to configure only the Source and Sink Connector files.
    We don't need to write any code using low-level languages like Java or Scala.
    But, now, let's turn to one more popular real-time stream processing framework
    called **Apache Storm**. Let's understand some cool features of Apache Storm.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Storm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Storm is a free and open source distributed real-time stream processing
    framework. At the time of writing this book, the stable release version of Apache
    Storm is 1.0.5\. The Storm framework is predominantly written in the Clojure programming
    language. Originally, it was created and developed by Nathan Marz and the team
    at Backtype. The project was later acquired by Twitter.
  prefs: []
  type: TYPE_NORMAL
- en: During one of his talks on the Storm framework, Nathan Marz talked about stream
    processing applications using any framework, such as Storm. These applications
    involved queues and worker threads. Some of the data source threads write messages
    to queues and other threads pick up these messages and write to target data stores.
    The main drawback here is that source threads and targets threads do not match
    the data load of each other and this results in data pileup. It also results in
    data loss and additional thread maintenance.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid the preceding challenges, Nathan Marz came up with a great architecture
    that abstracts source threads and worker threads into Spouts and Bolts. These
    Spouts and Bolts are submitted to the Topology framework, which takes care of
    entire stream processing.
  prefs: []
  type: TYPE_NORMAL
- en: Features of Apache Storm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Storm is distributed. In case of an increase in a stream's workload,
    multiple nodes can be added to the Storm Cluster to add more workers and more
    processing power to process.
  prefs: []
  type: TYPE_NORMAL
- en: It is a truly real-time stream processing system and supports **low-latency**.
    The event can be reached from source to target in in milliseconds, seconds, or
    minutes depending on the use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Storm framework supports **multiple programming languages**, but Java is the
    top preference. Storm is **fault-tolerant**. It continues to operate even though
    the failure of any node in the cluster. Storm is **reliable**. It supports at
    least once or exactly-once processing.
  prefs: []
  type: TYPE_NORMAL
- en: There is **no complexity** to using Storm framework. For more detail information,
    refer to the Storm documentation: [http://storm.apache.org/releases/1.0.4/index.html](http://storm.apache.org/releases/1.0.4/index.html).
  prefs: []
  type: TYPE_NORMAL
- en: Storm topology
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following image shows a typical **Storm Topology**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f1bc575d-ac6b-465d-aee2-2d3573f4f495.png)'
  prefs: []
  type: TYPE_IMG
- en: Storm topology components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following sections explain all the components of a Storm topology:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Topology**:A topology is a **DAG** (**directed acyclic graph**) of spouts
    and bolts that are connected with stream groupings. A topology runs continuously
    untill it is killed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stream**:A stream is an unbounded sequence of tuples. A tuple can be of any
    data type. It supports all the Java data types.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stream groupings**:Stream grouping decides which bolt receives a tuple from
    a spout. Basically, these are the strategies about how the stream will flow among
    different bolts. The following are the built-in stream groupings in Storm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Shuffle grouping**:It is a default grouping strategy. Tuples are randomly
    distributed and each bolt gets an equal number of streams to process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Field grouping**:In this strategy, the same value of a stream field will
    be sent to one bolt. For example, if all the tuples are grouped by `customer_id`,
    then all the tuples of the same `customer_id` will be sent one bolt task and all
    the tuples of another `customer_id` will be sent to another bolt task.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**All grouping**:In all grouping, each tuple is sent to each bolt task. It
    can be used when two different functions have to be performed on the same set
    of data. In that case, the stream can be replicated and each function can be calculated
    on each copy of the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Direct grouping**: This is a special kind of grouping. Here, the developer
    can define the grouping logic within the component where tuple is emitted itself.
    The producer of the tuple decides which task of the consumer will receive this
    tuple.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Custom grouping**:The developer may decide to implement his/her own grouping
    strategy by implementing the `CustomGrouping` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spout**:A spout connects to the data source and ingests streams into a Storm
    topology.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bolt**:A spout emits a tuple to a bolt. A bolt is responsible for event transformation,
    joining events to other events, filtering, aggregation, and windowing. It emits
    the tuple to another bolt or persists it to a target. All processing in topologies
    is done in bolts. Bolts can do anything from filtering to functions, aggregations,
    joins, talking to databases, and more.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Storm Cluster**: Thefollowing image shows all the components of a **Storm
    Cluster**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/668cfc89-82eb-40c3-89cd-a8ec15a32143.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Storm Cluster nodes**:The three main nodes of a Storm Cluster are Nimbus,
    Supervisor, and Zookeeper. The following section explains all the components in
    detail.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Nimbus node**:In Storm, this is the master node of a Storm Cluster. It distributes
    code and launches the worker tasks across the cluster. Basically, it assigns tasks
    to each node in a cluster. It also monitors the status of each job submitted.
    In the case of any job failure, Nimbus reallocates the job to a different supervisor
    within a cluster. In the case of Nimbus being unavailable, the workers will still
    continue to function. However, without Nimbus, workers won''t be reassigned to
    other machines when necessary. In the case of an unavailable node, the tasks assigned
    to that node will time-out and Nimbus will reassign those tasks to other machines. In
    the case of both Nimbus and Supervisor being unavailable, they need to be restarted
    like nothing happened and no worker processes will be affected.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Supervisor node**:In Storm, this is a slave node. It communicates with Nimbus
    through ZooKeeper. It starts and stops the worker processes within a supervisor
    itself. For example, if Supervisor finds that a particular worker process has
    died, then it immediately restarts that worker process. If Supervisor fails to
    restart the worker after trying few times, then it communicates this to Nimbus
    and Nimbus will restart that worker on a different Supervisor node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Zookeeper node**:It acts as a coordinator between masters (Nimbus) and slaves
    (supervisors) within a Storm Cluster. In a production environment, it is typical
    to set up a Zookeeper cluster that has three instances (nodes) of Zookeeper.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing Storm on a single node cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the steps to install Storm Cluster on a single machine:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install `jdk`.Make sure you have installed 1.8:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a folder to download the `.tar` file of Storm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a folder to persist Zookeeper and Storm data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Download Zookeeper and Storm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Configure Zookeeper and set the following to Zookeeper (`zoo.cfg`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Configure Storm as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: (for additional workers, add more ports, such as 6704 and so on)
  prefs: []
  type: TYPE_NORMAL
- en: 'Start Zookeeper:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Start Nimbus:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Start Supervisor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Verify installation in the Storm UI:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Developing a real-time streaming pipeline with Storm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will create the following three pipelines:'
  prefs: []
  type: TYPE_NORMAL
- en: Streaming pipeline with Kafka - Storm - MySQL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming pipeline with Kafka - Storm - HDFS - Hive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we will see how data streams flow from Kafka to Storm to MySQL
    table.
  prefs: []
  type: TYPE_NORMAL
- en: 'The whole pipeline will work as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We will ingest customer records (`customer_firstname` and `customer_lastname`)
    in Kafka using the Kafka console-producer API.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After that, Storm will pull the messages from Kafka.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A connection to MySQL will be established.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Storm will use MySQL-Bolt to ingest records into MySQL table. MySQL will automatically
    generate `customer_id`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The MySQL table data (`customer_id`, `customer_firstname`, and `customer_lastname`)
    will be accessed using SQL.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will develop the following Java classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`MysqlConnection.java`: This class will establish a connection with the local
    MySQL database.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MysqlPrepare.java`: This class will prepare the SQL statements to be inserted
    into the database.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MysqlBolt`: This class is a storm bolt framework to emit the tuple from Kafka
    to MySQL.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MySQLKafkaTopology`: This is a Storm Topology Framework that builds a workflow
    to bind spouts (Kafka) to Bolts (MySQL). Here, we are using a Local Storm Cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming a pipeline from Kafka to Storm to MySQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following image shows the components of the pipeline. In this pipeline,
    we will learn how the messages will flow from Kafka to Storm to MySQL in real-time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/24ae013a-817c-478e-a2c5-04a08eb38f29.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following is the complete Java code for `MysqlConnection.java`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the complete code for `MySqlPrepare.java`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the complete code for `MySqlBolt.java`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the complete code for `KafkaMySQLTopology.java`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Use the `pom.xml` file to build your project in IDE.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming a pipeline with Kafka to Storm to HDFS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will see how the data streams will flow from Kafka to Storm
    to HDFS and access them with a Hive external table.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image shows the components of the pipeline. In this pipeline,
    we will learn how the messages will flow from Kafka to Storm to HDFS in real-time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f7174858-5105-44ae-8eee-e52c6e8480af.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The whole pipeline will work as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We will ingest customer records (`customer_id`, `customer_firstname`, and `customer_lastname`)
    in Kafka using the Kafka console-producer API
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After that, Storm will pull the messages from Kafka
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A Connection to HDFS will be established
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Storm will use HDFS-Bolt to ingest records into HDFS
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hive external table will be created to store (`customer_id`, `customer_firstname`,
    and `customer_lastname`)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Hive table data (`customer_id`, `customer_firstname`, and `customer_lastname`)
    will be accessed using SQL
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will develop the following Java classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`KafkaTopology.java`: This is a Storm Topology framework that builds a workflow
    to bind spouts (Kafka) to Bolts (HDFS). Here we are using a Local Storm cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: In the previous example pipeline, multiple separate classes for data streams
    parsing and transformations can be developed to handle Kafka producers and consumers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the complete Java code for `KafkaToplogy.java`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The Hive table for the same is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Other popular real-time data streaming frameworks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apart from Apache Storm, there are quite a few other open source real-time data
    streaming frameworks. In this section, I will discuss in brief only open source
    non-commercial frameworks. But, at the end of this section, I will provide a few
    URLs for a few commercial vendor products that offer some very interesting features.
  prefs: []
  type: TYPE_NORMAL
- en: Kafka Streams API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kafka Streams is a library for building streaming applications. Kafka Streams
    is a client library for building applications and microservices, where the input
    and output data are stored in Kafka Clusters. The Kafka Streams API transforms
    and enriches the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the important features of the Kafka Streams API:'
  prefs: []
  type: TYPE_NORMAL
- en: It is part of the open source Apache Kafka project.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It supports per record streams processing with a very low latency (milliseconds).
    There is no micro- batching concept in the Kafka Streams API. Every record that
    comes into the stream is processed on its own.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It supports stateless processing (filtering and mapping), stateful processing
    (joins and aggregations), and windowing operations (for example, counting the
    last minute, last 5 minutes, last 30 minutes, or last day's worth of data, and
    so on).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To run the Kafka Streams API, there is no need to build a separate cluster that
    has multiple machines. Developers can use the Kafka Streams API in their Java
    applications or microservices to process real-time data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Kafka Streams API is highly scalable and fault-tolerant.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Kafka Streams API is completely deployment agnostic. It can be deployed
    on a bare metal machine, VMs, Kubernetes containers, and on Cloud. There are no
    restrictions at all. Stream APIs are never deployed on Kafka Brokers. It is a
    separate application just like any other Java application, which is deployed outside
    of Kafka brokers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It uses the Kafka security model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It supports exactly-once semantics since version 0.11.0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's review the earlier image again to find out the exact place of the Kafka
    Streams API in the overall Kafka architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few useful URLs to understand Kafka Streams in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://kafka.apache.org/documentation/](https://kafka.apache.org/documentation/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.confluent.io/blog/](https://www.confluent.io/blog/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.confluent.io/blog/introducing-kafka-streams-stream-processing-made-simple/](https://www.confluent.io/blog/introducing-kafka-streams-stream-processing-made-simple/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.confluent.io/current/streams/index.html](https://docs.confluent.io/current/streams/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark Streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Please note that we will discuss Spark in [Chapter 7](39cf9925-e4cc-473a-9032-a21b81e7b400.xhtml),
    *Large-Scale Data Processing Frameworks,* which is fully dedicated to Spark. However,
    in this section, I will discuss some important features of Spark Streaming. For
    better understanding, readers are advised to study [Chapter 7](39cf9925-e4cc-473a-9032-a21b81e7b400.xhtml), *Large-Scale
    Data Processing Frameworks* first and come back to read this section further to
    understand more about Spark Streaming.
  prefs: []
  type: TYPE_NORMAL
- en: It is a general practice to use Hadoop MapReduce for batch processing and Apache
    Storm for real-time stream processing.
  prefs: []
  type: TYPE_NORMAL
- en: The use of these two different programming models causes an increase in code
    size, number of bugs to fix, and development effort; it also introduces a learning
    curve and causes other issues. Spark Streaming helps fix these issues and provides
    a scalable, efficient, resilient, and integrated (with batch processing) system.
  prefs: []
  type: TYPE_NORMAL
- en: The strength of Spark Streaming lies in its ability to combine with batch processing.
    It's possible to create a RDD using normal Spark programming and join it with
    a Spark stream. Moreover, the code base is similar and allows easy migration if
    required—and there is zero to no learning curve from Spark.
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark Streaming is an extension of the core Spark API. It extends Spark for
    doing real-time stream processing. Spark Streaming has the following features:'
  prefs: []
  type: TYPE_NORMAL
- en: It's scalable—it scales on hundreds of nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It provides high-throughput and achieves second level latency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It's fault-tolerant and it efficiently receives from the failures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It integrates with batch and interactive data processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark Streaming processes data streams application as a series of very small,
    deterministic batch jobs.
  prefs: []
  type: TYPE_NORMAL
- en: Spark Streaming provides an API in Scala, Java, and Python. Spark Streaming
    divides live stream of data into multiple batches based on time. The time can
    range from one second to a few minutes/hours. In general, batches are divided
    into a few seconds. Spark treats each batch as a RDD and process each based on
    RDD operations (map, filter, join flatmap, distinct, reduceByKey, and so on).
    Lastly, the processed results of RDDs are returned in batches.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image depicts the Spark Streaming data flow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5a221209-8e20-480b-8121-1d6c49e99f61.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here are few useful URLs for understanding Spark Streaming in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://databricks.com/blog](https://databricks.com/blog)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://databricks.com/blog/category/engineering/streaming](https://databricks.com/blog/category/engineering/streaming)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://spark.apache.org/streaming/](https://spark.apache.org/streaming/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Flink
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Apache Flink''s documentation describes Flink in the following way: Flink is
    an open-source framework for distributed stream processing.'
  prefs: []
  type: TYPE_NORMAL
- en: Flink provides accurate results and supports out-of-order or late-arriving datasets.
    It is stateful and fault-tolerant and can seamlessly recover from failures while
    maintaining an exactly-once application state. It performs at a large scale, running
    on thousands of nodes with very good throughput and latency characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the features of Apache Flink:'
  prefs: []
  type: TYPE_NORMAL
- en: Flink guarantees exactly-once semantics for stateful computations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flink supports stream processing and windowing with event time semantics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flink supports flexible windowing based on time, count, or sessions, in addition
    to data-driven windows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flink is capable of high throughput and low latency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flink's savepoints provide a state versioning mechanism, making it possible
    to update applications or reprocess historic data with no lost state and minimal
    downtime
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flink is designed to run on large-scale clusters with many thousands of nodes,
    and, in addition to a standalone cluster mode, Flink provides support for YARN
    and Mesos
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flink's core is a distributed streaming dataflow engine. It supports processing
    one stream at a time rather than processing an entire batch of streams at a time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Flink supports the following libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: CEP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graph processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Storm compatibility
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Flink supports the following APIs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**DataStream API**: This API helps all the streams, transformations, that is,
    filtering, aggregations, counting, and windowing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DataSet API**: This API helps all the batch data transformations, that is,
    join, group, map, and filter'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Table API**: Supports SQL over relational data streams'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Streaming SQL**: Supports SQL over batch and streaming tables'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following image describes the Flink programming model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d7ae2f42-7921-40b3-a961-a6a3e13bd11c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following image describes the Flink architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4f2dfbe0-b9ed-4693-8908-d73d07e4c0f9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following are the components of the Flink programming model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Source**: A data source where data is collected and sent to the Flink engine'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transformation**: In this component the whole transformation takes place'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sink**: A target where processed streams are sent'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are a few useful URLs to understand Spark Streaming in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://ci.apache.org/projects/flink/flink-docs-release-1.4/](https://ci.apache.org/projects/flink/flink-docs-release-1.4/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.youtube.com/watch?v=ACS6OM1-xgE&amp;amp;feature=youtu.be](https://www.youtube.com/watch?v=ACS6OM1-xgE&feature=youtu.be)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following sections, we will take a look at a comparison of various stream
    frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Flink versus Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main focus of Spark Streaming is stream-batching operation, called **micro-batching**.
    This programming model suits many use cases, but not all use cases require real-time
    stream processing with sub-second latency. For example, a use case such as credit
    card fraud prevention requires millisecond latency. Hence, the micro-batching
    programming model is not suited there. (But, the latest version of Spark, 2.4,
    supports millisecond data latency).
  prefs: []
  type: TYPE_NORMAL
- en: Apache Flink supports millisecond latency and is suited for use cases such as
    fraud detection and like.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark versus Storm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark uses micro-batches to process events while Storm processes events one
    by one. It means that Spark has a latency of seconds while Storm provides a millisecond
    of latency. Spark Streaming provides a high-level abstraction called a **Discretized
    Stream** or **DStream**, which represents a continuous sequence of RDDs. (But,
    the latest version of Spark, 2.4 supports millisecond data latency.) The latest
    Spark version supports DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: Almost the same code (API) can be used for Spark Streaming and Spark batch jobs.
    That helps to reuse most of the code base for both programming models. Also, Spark
    supports Machine learning and the Graph API. So, again, the same codebase can
    be used for those use cases as well.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we started with a detailed understanding of real-time stream
    processing concepts, including data stream, batch vs. real-time processing, CEP,
    low latency, continuous availability, horizontal scalability, storage, and so
    on. Later, we learned about Apache Kafka, which is a very important component
    of modern real-time stream data pipelines. The main features of Kafka are scalability,
    durability, reliability, and high throughput.
  prefs: []
  type: TYPE_NORMAL
- en: We also learned about Kafka Connect; its architecture, data flow, sources, and
    connectors. We studied case studies to design a data pipeline with Kafka Connect
    using file source, file Sink, JDBC source, and file Sink Connectors.
  prefs: []
  type: TYPE_NORMAL
- en: In the later sections, we learned about various open source real-time stream-processing
    frameworks, such as the Apache Storm framework. We have seen a few practical examples,
    as well. Apache Storm is distributed and supports low-latency and multiple programming
    languages. Storm is fault-tolerant and reliable. It supports at least once or
    exactly-once processing.
  prefs: []
  type: TYPE_NORMAL
- en: Spark Streaming helps to fix these issues and provides a scalable, efficient,
    resilient, and integrated (with batch processing) system. The strength of Spark
    Streaming lies in its ability to combine with batch processing. Spark Streaming
    is scalable, and provides high-throughput. It supports micro-batching for second
    level latency, is fault-tolerant, and integrates with batch and interactive data
    processing.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Flink guarantees exactly-once semantics, supports event time semantics,
    high throughput, and low latency. It is designed to run on large-scale clusters.
  prefs: []
  type: TYPE_NORMAL
