- en: Designing Real-Time Streaming Data Pipelines
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first three chapters of this book all dealt with batch data. Having learned
    about the installation of Hadoop, data ingestion tools and techniques, and data
    stores, let's turn to data streaming. Not only will we look at how we can handle
    real-time data streams, but also how to design pipelines around them.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Real-time streaming concepts
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-time streaming components
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Flink versus Spark
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Spark versus Storm
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-time streaming concepts
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's understand a few key concepts relating to real-time streaming applications
    in the following sections.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Data stream
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The data stream is a continuous flow of data from one end to another end, from
    sender to receiver, from producer to consumer. The speed and volume of the data
    may vary; it may be 1 GB of data per second or it may be 1 KB of data per second
    or per minute.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Batch processing versus real-time data processing
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In batch processing, data is collected in batches and each batch is sent for
    processing. The batch interval can be anything from one day to one minute. In
    today's data analytics and business intelligence world, data will not be processed
    in a batch for more than one day. Otherwise, business teams will not have any
    insight about what's happening to the business in a day-to-day basis. For example,
    the enterprise data warehousing team may collect all the orders made during the
    last 24 hours and send all these collected orders to the analytics engine for
    reporting.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: The batch can be of one minute too. In the Spark framework (we will learn Spark
    in [Chapter 7](39cf9925-e4cc-473a-9032-a21b81e7b400.xhtml), *Large-Scale Data
    Processing Frameworks*), data is processed in micro batches.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: In real-time processing, data (event) is transferred (streamed) from the producer
    (sender) to the consumer (receiver) as soon as an event is produced at the source
    end. For example, on an e-commerce website, orders gets processed immediately
    in an analytics engine as soon as the customer places the same order on that website.
    The advantage is that the business team of that company gets full insights about
    its business in real time (within a few milliseconds or sub milliseconds). It
    will help them adjust their promotions to increase their revenue, all in real-time.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image explains stream-processing architecture:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/01854cc6-c9ec-45f8-b35d-c5d9528176c0.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
- en: Complex event processing
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Complex event processing** (**CEP**) is event processing that combines data
    from multiple sources to discover complex relationships or patterns. The goal
    of CEP is to identify meaningful events (such as opportunities or threats) and
    respond to them as quickly as possible. Fundamentally, CEP is about applying business
    rules to streaming event data. For example, CEP is used in use cases, such as
    stock trading, fraud detection, medical claim processing, and so on.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image explains stream-processing architecture:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c7c76b39-7e99-4115-9d6c-8b65f0dbec50.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
- en: Continuous availability
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Any real-time application is expected to be available all the time with no stoppage
    whatsoever. The event collection, processing, and storage components should be
    configured with the underlined assumptions of high availability. Any failure to
    any components will cause major disruptions to the running of the business. For
    example, in a credit card fraud detection application, all the fraudulent transactions
    need to be declined. If the application stops midway and is unable to decline
    fraudulent transactions, then it will result in heavy losses.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: Low latency
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In any real-time application, the event should flow from source to target in
    a few milliseconds. The source collects the event, and a processing framework
    moves the event to its target data store where it can be analyzed further to find
    trends and patterns. All these should happen in real time, otherwise it may impact
    business decisions. For example, in a credit card fraud detection application,
    it is expected that all incoming transactions should be analyzed to find possible
    fraudulent transactions, if any. If the stream processing takes more than the
    desired period of time, it may be possible that these transactions may pass through
    the system, causing heavy losses to the business.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: Scalable processing frameworks
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hardware failure may cause disruption to the stream processing application.
    To avoid this common scenario, we always need a processing framework that offers
    built-in APIs to support continuous computation, fault tolerant event state management,
    checkpoint features in the event of failures, in-flight aggregations, windowing,
    and so on. Fortunately, all the recent Apache projects such as Storm, Spark, Flink,
    and Kafka do support all and more of these features out of the box. The developer
    can use these APIs using Java, Python, and Scala.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: Horizontal scalability
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The stream-processing platform should support horizontal scalability. That means
    adding more physical servers to the cluster in the event of a higher incoming
    data load to maintain throughput SLA. This way, the performance of processing
    can be increased by adding more nodes rather than adding more CPUs and memory
    to the existing servers; this is called **vertical scalability**.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Storage
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The preferable format of a stream is key-value pair. This format is very well
    represented by the JSON and Avro formats. The preferred storage to persist key-value
    type data is NoSQL data stores such as HBase and Cassandra. There are in total
    100 NoSQL open source databases in the market these days. It's very challenging
    to choose the right database, one which supports storage to real-time events,
    because all these databases offer some unique features for data persistence. A
    few examples are schema agnostic, highly distributable, commodity hardware support,
    data replication, and so on.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image explains all stream processing components:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ddc50632-2e71-4f82-805a-f60ab3170c1e.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
- en: In this chapter we will talk about message queue and stream processing frameworks
    in detail. In the next chapter, we will focus on data indexing techniques.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Real-time streaming components
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the following sections we will walk through some important real-time streaming
    components.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Message queue
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The message queue lets you publish and subscribe to a stream of events/records.
    There are various alternatives we can use as a message queue in our real-time
    stream architecture. For example, there is RabbitMQ, ActiveMQ, and Kafka. Out
    of these, Kafka has gained tremendous popularity due to its various unique features.
    Hence, we will discuss the architecture of Kafka in detail. A discussion of RabbitMQ
    and ActiveMQ is beyond the scope of this book.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: So what is Kafka?
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kafka is a fast, scalable, durable, and fault-tolerant publish-subscribe messaging
    system. Apache Kafka is an open-source stream-processing project. It provides
    a unified, high-throughput, and is a low-latency platform for handling real-time
    data streams. It provides a distributed storage layer, which supports massively
    scalable pub/sub message queues. Kafka Connect supports data import and export
    by connecting to external systems. Kafka Streams provides Java APIs for stream
    processing. Kafka works in combination with Apache Spark, Apache Cassandra, Apache
    HBase, Apache Spark, and more for real-time stream processing.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Apache Kafka was originally developed by LinkedIn, and was subsequently open
    sourced in early 2011\. In November 2014, several engineers who worked on Kafka
    at LinkedIn created a new company named Confluent with a focus on Kafka. Please
    use this URL [https://www.confluent.io/](https://www.confluent.io/) to learn more
    about the Confluent platform.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Kafka features
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are features of Kafka:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '**Kafka is scalable**: Kafka Cluster consists of more than one physical server,
    which helps to distribute the data load. It is easily scalable in the event that
    additional throughputs are required as additional servers can be added to maintain
    the SLA.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kafka is durable**: During stream processing, Kafka persists messages on
    the persistent storage. This storage can be server local disks or Hadoop Cluster.
    In the event of message processing failure, the message can be accessed from the
    disk and replayed to process the message again. By default, the message is stored
    for seven days; this can be configured further.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kafka is reliable**: Kafka provides message reliability with the help of
    a feature called **data replication**. Each message is replicated at least three
    times (this is configurable) so that in the event of data loss, the copy of the
    message can be used for processing.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kafka supports high performance throughput**: Due to its unique architecture,
    partitioning, message storage, and horizontal scalability, Kafka helps to process
    terabytes of data per second.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kafka architecture
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following image shows Kafka''s architecture:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '**![](img/50297cdb-a34b-439f-be63-572e999c5c87.png)**'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Kafka architecture components
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s take a look at each component in detail:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '**Producers**: Producers publish messages to a specific Kafka topic. Producers
    may attach a key to each message record. By default, producers publish messages
    to topic partitions in round robin fashion. Sometimes, producers can be configured
    to write messages to a particular topic partition based on the hash value of message
    key.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Topic**: All messages are stored in a topic. A topic is a category or feed
    name to which records are published. A topic can be compared to a table in a relational
    database. Multiple consumers can be subscribed to a single topic to consume message
    records.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Partition**: A topic is divided into multiple partitions. Kafka offers topic
    parallelism by dividing topic into partitions and by placing each partition on
    a separate broker (server) of a Kafka Cluster. Each partition has a separate partition
    log on a disk where messages are stored. Each partition contains an ordered, immutable
    sequence of messages. Each message is assigned unique sequence numbers called
    **offset**. Consumers can read messages from any point from a partition—from the
    beginning or from any offset.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Consumers**: A consumer subscribes to a topic and consumes the messages.
    In order to increase the scalability, consumers of the same application can be
    grouped into a consumer group where each consumer can read messages from a unique
    partition.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Brokers**: Kafka is divided into multiple servers called **brokers**. All
    the brokers combined are called **Kafka Cluster**. Kafka brokers handle message
    writes from producers and message reads from consumers. Kafka Brokers stores all
    the messages coming from producers. The default period is of seven days. This
    period (retention period) can be configured based on the requirements. The retention
    period directly impacts the local storage of Kafka brokers. It takes more storage
    if a higher retention period is configured. After the retention period is over,
    the message is automatically discarded.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kafka Connect**: According to Kafka documentation, Kafka Connect allows the
    building and running of reusable producers or consumers that connect Kafka topics
    to existing applications or data systems. For example, a connector to a relational
    database might capture every change to a table.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kafka Streams**: Stream APIs allow an application to act as a stream processor,
    consuming an input stream from one or more topics and producing an output stream
    to one or more output topics, effectively transforming the input streams to output
    streams.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kafka Connect deep dive
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kafka Connect is a part of the Confluent platform. It is integrated with Kafka.
    Using Kafka Connect, it's very easy to build data pipelines from multiple sources
    to multiple targets. **Source Connectors** import data from another system (for
    example, from a relational database into Kafka) and **Sink Connectors **export
    data (for example, the contents of a Kafka topic to an HDFS file).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Kafka Connect architecture
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following image shows Kafka Connect''s architecture:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cbd6b15a-6f36-4e57-b6fd-0e01ae874bb4.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
- en: 'The data flow can be explained as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Various sources are connected to **Kafka Connect Cluster**. **Kafka Connect
    Cluster** pulls data from the sources.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kafka Connect Cluster** consists of a set of worker processes that are containers
    that execute connectors, and tasks automatically coordinate with each other to
    distribute work and provide scalability and fault tolerance.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kafka Connect Cluster** pushes data to **Kafka Cluster**.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kafka Cluster** persists the data on to the broker local disk or on Hadoop.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streams applications such as Storm, Spark Streaming, and Flink pull the data
    from **Kafka Cluster** for stream transformation, aggregation, join, and so on.
    These applications can send back the data to Kafka or persist it to external data
    stores such as HBase, Cassandra, MongoDB, HDFS, and so on.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kafka Connect Cluster** pulls data from **Kafka Cluster** and pushes it Sinks.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Users can extend the existing Kafka connectors or develop brand new connectors.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kafka Connect workers standalone versus distributed mode
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Users can run Kafka Connect in two ways: standalone mode or distributed mode.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: In standalone mode, a single process runs all the connectors. It is not fault
    tolerant. Since it uses only a single process, it is not scalable. Generally,
    it is useful for users for development and testing purposes.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: In distributed mode, multiple workers run Kafka Connect. In this mode, Kafka
    Connect is scalable and fault tolerant, so it is used in production deployment.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s learn more about Kafka and Kafka Connect (standalone mode). In this
    example, we will do the following:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Install Kafka
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a topic
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate a few messages to verify the producer and consumer
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kafka Connect-File-source and file-sink
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kafka Connect-JDBC -Source
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following image shows a use case using **Kafka Connect**:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a8818e04-6229-45ec-9063-ca5a4c86a8a9.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
- en: Let's see how Kafka and Kafka Connect works by running a few examples. For more
    details, use the following link for Kafka Confluent's documentation: [https://docs.confluent.io/current/](https://docs.confluent.io/current/).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Install Kafka
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s perform the following steps to install Kafka:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: Download Confluent from [https://www.confluent.io/download/](https://www.confluent.io/download/)
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on Confluent Open Source
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Download the file `confluent-oss-4.0.0-2.11.tar.gz` from `tar.gz` and perform
    the following:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Uncomment `listeners=PLAINTEXT://:9092`
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Start Confluent:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Start `zookeeper`:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Start `kafka`:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Start `schema-registry`:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Create topics
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Perform the following steps to create topics:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: List the existing topics
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open another terminal and enter the following command:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Create a topic:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Double check the newly created topic:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Generate messages to verify the producer and consumer
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Perform the following steps to generate messages to verify the producer and
    consumer:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 'Send messages to Kafka `my-first-topic`:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Start consumer to consume messages
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Open another terminal and enter the following command:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Go to the producer terminal and enter another message:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Verify the consumer terminal to check whether you can see the message `test4`
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Kafka Connect using file Source and Sink
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s take a look at how to create topics using file Source and Sink, with
    the help of the following:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Perform the following steps:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 'Start the Source Connector and Sink Connector:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Double check whether the Kafka topic has received the messages:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Verify `target-file.txt`:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Kafka Connect using JDBC and file Sink Connectors
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following image shows how we can push all records from the database table
    to a text file:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d5fb5d5-d98a-4ec5-aef5-f3207b15beab.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
- en: 'Let''s implement the preceding example using Kafka Connect:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 'Install SQLite:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Configure the JDBC Source Connector:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Configure the file Sink Connector:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Start Kafka Connect (`.jdbs` source and file Sink):'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Verify the consumer:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The `--new-consumer` option is deprecated and will be removed in a future major
    release. The new consumer is used by default if the `--bootstrap-server` option
    is provided:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Verify the target file:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Insert a few more records in the customer table:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Verify the target file:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: You will see all customer records (`cust_id`) in the target file. Using the
    preceding example, you can customize and experiment with any other Sink.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table presents the available Kafka connectors on the Confluent
    platform (developed and fully supported by Confluent):'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '| **Connector Name** | **Source/Sink** |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
- en: '| JDBC | Source and Sink |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
- en: '| HDFS | Sink |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
- en: '| Elasticsearch | Sink |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
- en: '| Amazon S3 | Sink |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
- en: For more information on other certified Connectors by Confluent, please use
    this URL: [https://www.confluent.io/product/connectors/](https://www.confluent.io/product/connectors/).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: You must have observed that Kafka Connect is a configuration-based stream-processing
    framework. It means we have to configure only the Source and Sink Connector files.
    We don't need to write any code using low-level languages like Java or Scala.
    But, now, let's turn to one more popular real-time stream processing framework
    called **Apache Storm**. Let's understand some cool features of Apache Storm.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Apache Storm
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Storm is a free and open source distributed real-time stream processing
    framework. At the time of writing this book, the stable release version of Apache
    Storm is 1.0.5\. The Storm framework is predominantly written in the Clojure programming
    language. Originally, it was created and developed by Nathan Marz and the team
    at Backtype. The project was later acquired by Twitter.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: During one of his talks on the Storm framework, Nathan Marz talked about stream
    processing applications using any framework, such as Storm. These applications
    involved queues and worker threads. Some of the data source threads write messages
    to queues and other threads pick up these messages and write to target data stores.
    The main drawback here is that source threads and targets threads do not match
    the data load of each other and this results in data pileup. It also results in
    data loss and additional thread maintenance.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: To avoid the preceding challenges, Nathan Marz came up with a great architecture
    that abstracts source threads and worker threads into Spouts and Bolts. These
    Spouts and Bolts are submitted to the Topology framework, which takes care of
    entire stream processing.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Features of Apache Storm
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Storm is distributed. In case of an increase in a stream's workload,
    multiple nodes can be added to the Storm Cluster to add more workers and more
    processing power to process.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: It is a truly real-time stream processing system and supports **low-latency**.
    The event can be reached from source to target in in milliseconds, seconds, or
    minutes depending on the use cases.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Storm framework supports **multiple programming languages**, but Java is the
    top preference. Storm is **fault-tolerant**. It continues to operate even though
    the failure of any node in the cluster. Storm is **reliable**. It supports at
    least once or exactly-once processing.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: There is **no complexity** to using Storm framework. For more detail information,
    refer to the Storm documentation: [http://storm.apache.org/releases/1.0.4/index.html](http://storm.apache.org/releases/1.0.4/index.html).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Storm topology
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following image shows a typical **Storm Topology**:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f1bc575d-ac6b-465d-aee2-2d3573f4f495.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
- en: Storm topology components
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following sections explain all the components of a Storm topology:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '**Topology**:A topology is a **DAG** (**directed acyclic graph**) of spouts
    and bolts that are connected with stream groupings. A topology runs continuously
    untill it is killed.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stream**:A stream is an unbounded sequence of tuples. A tuple can be of any
    data type. It supports all the Java data types.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stream groupings**:Stream grouping decides which bolt receives a tuple from
    a spout. Basically, these are the strategies about how the stream will flow among
    different bolts. The following are the built-in stream groupings in Storm.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Shuffle grouping**:It is a default grouping strategy. Tuples are randomly
    distributed and each bolt gets an equal number of streams to process.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Field grouping**:In this strategy, the same value of a stream field will
    be sent to one bolt. For example, if all the tuples are grouped by `customer_id`,
    then all the tuples of the same `customer_id` will be sent one bolt task and all
    the tuples of another `customer_id` will be sent to another bolt task.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**All grouping**:In all grouping, each tuple is sent to each bolt task. It
    can be used when two different functions have to be performed on the same set
    of data. In that case, the stream can be replicated and each function can be calculated
    on each copy of the data.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Direct grouping**: This is a special kind of grouping. Here, the developer
    can define the grouping logic within the component where tuple is emitted itself.
    The producer of the tuple decides which task of the consumer will receive this
    tuple.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Custom grouping**:The developer may decide to implement his/her own grouping
    strategy by implementing the `CustomGrouping` method.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spout**:A spout connects to the data source and ingests streams into a Storm
    topology.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bolt**:A spout emits a tuple to a bolt. A bolt is responsible for event transformation,
    joining events to other events, filtering, aggregation, and windowing. It emits
    the tuple to another bolt or persists it to a target. All processing in topologies
    is done in bolts. Bolts can do anything from filtering to functions, aggregations,
    joins, talking to databases, and more.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Storm Cluster**: Thefollowing image shows all the components of a **Storm
    Cluster**:'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/668cfc89-82eb-40c3-89cd-a8ec15a32143.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
- en: '**Storm Cluster nodes**:The three main nodes of a Storm Cluster are Nimbus,
    Supervisor, and Zookeeper. The following section explains all the components in
    detail.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Nimbus node**:In Storm, this is the master node of a Storm Cluster. It distributes
    code and launches the worker tasks across the cluster. Basically, it assigns tasks
    to each node in a cluster. It also monitors the status of each job submitted.
    In the case of any job failure, Nimbus reallocates the job to a different supervisor
    within a cluster. In the case of Nimbus being unavailable, the workers will still
    continue to function. However, without Nimbus, workers won''t be reassigned to
    other machines when necessary. In the case of an unavailable node, the tasks assigned
    to that node will time-out and Nimbus will reassign those tasks to other machines. In
    the case of both Nimbus and Supervisor being unavailable, they need to be restarted
    like nothing happened and no worker processes will be affected.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Supervisor node**:In Storm, this is a slave node. It communicates with Nimbus
    through ZooKeeper. It starts and stops the worker processes within a supervisor
    itself. For example, if Supervisor finds that a particular worker process has
    died, then it immediately restarts that worker process. If Supervisor fails to
    restart the worker after trying few times, then it communicates this to Nimbus
    and Nimbus will restart that worker on a different Supervisor node.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Zookeeper node**:It acts as a coordinator between masters (Nimbus) and slaves
    (supervisors) within a Storm Cluster. In a production environment, it is typical
    to set up a Zookeeper cluster that has three instances (nodes) of Zookeeper.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing Storm on a single node cluster
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the steps to install Storm Cluster on a single machine:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: 'Install `jdk`.Make sure you have installed 1.8:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'You should see the following output:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Create a folder to download the `.tar` file of Storm:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Create a folder to persist Zookeeper and Storm data:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Download Zookeeper and Storm:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Configure Zookeeper and set the following to Zookeeper (`zoo.cfg`):'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Configure Storm as follows:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Add the following:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: (for additional workers, add more ports, such as 6704 and so on)
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: 'Start Zookeeper:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Start Nimbus:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Start Supervisor:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Verify installation in the Storm UI:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Developing a real-time streaming pipeline with Storm
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will create the following three pipelines:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Streaming pipeline with Kafka - Storm - MySQL
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming pipeline with Kafka - Storm - HDFS - Hive
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we will see how data streams flow from Kafka to Storm to MySQL
    table.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 'The whole pipeline will work as follows:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: We will ingest customer records (`customer_firstname` and `customer_lastname`)
    in Kafka using the Kafka console-producer API.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After that, Storm will pull the messages from Kafka.
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A connection to MySQL will be established.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Storm will use MySQL-Bolt to ingest records into MySQL table. MySQL will automatically
    generate `customer_id`.
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The MySQL table data (`customer_id`, `customer_firstname`, and `customer_lastname`)
    will be accessed using SQL.
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will develop the following Java classes:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '`MysqlConnection.java`: This class will establish a connection with the local
    MySQL database.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MysqlPrepare.java`: This class will prepare the SQL statements to be inserted
    into the database.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MysqlBolt`: This class is a storm bolt framework to emit the tuple from Kafka
    to MySQL.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MySQLKafkaTopology`: This is a Storm Topology Framework that builds a workflow
    to bind spouts (Kafka) to Bolts (MySQL). Here, we are using a Local Storm Cluster.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming a pipeline from Kafka to Storm to MySQL
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following image shows the components of the pipeline. In this pipeline,
    we will learn how the messages will flow from Kafka to Storm to MySQL in real-time:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/24ae013a-817c-478e-a2c5-04a08eb38f29.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
- en: 'The following is the complete Java code for `MysqlConnection.java`:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The following is the complete code for `MySqlPrepare.java`:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The following is the complete code for `MySqlBolt.java`:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The following is the complete code for `KafkaMySQLTopology.java`:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Use the `pom.xml` file to build your project in IDE.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Streaming a pipeline with Kafka to Storm to HDFS
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will see how the data streams will flow from Kafka to Storm
    to HDFS and access them with a Hive external table.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image shows the components of the pipeline. In this pipeline,
    we will learn how the messages will flow from Kafka to Storm to HDFS in real-time:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f7174858-5105-44ae-8eee-e52c6e8480af.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
- en: 'The whole pipeline will work as follows:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: We will ingest customer records (`customer_id`, `customer_firstname`, and `customer_lastname`)
    in Kafka using the Kafka console-producer API
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After that, Storm will pull the messages from Kafka
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A Connection to HDFS will be established
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Storm will use HDFS-Bolt to ingest records into HDFS
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hive external table will be created to store (`customer_id`, `customer_firstname`,
    and `customer_lastname`)
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Hive table data (`customer_id`, `customer_firstname`, and `customer_lastname`)
    will be accessed using SQL
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will develop the following Java classes:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '`KafkaTopology.java`: This is a Storm Topology framework that builds a workflow
    to bind spouts (Kafka) to Bolts (HDFS). Here we are using a Local Storm cluster.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: In the previous example pipeline, multiple separate classes for data streams
    parsing and transformations can be developed to handle Kafka producers and consumers.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the complete Java code for `KafkaToplogy.java`:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The Hive table for the same is as follows:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Other popular real-time data streaming frameworks
  id: totrans-263
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apart from Apache Storm, there are quite a few other open source real-time data
    streaming frameworks. In this section, I will discuss in brief only open source
    non-commercial frameworks. But, at the end of this section, I will provide a few
    URLs for a few commercial vendor products that offer some very interesting features.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Kafka Streams API
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kafka Streams is a library for building streaming applications. Kafka Streams
    is a client library for building applications and microservices, where the input
    and output data are stored in Kafka Clusters. The Kafka Streams API transforms
    and enriches the data.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the important features of the Kafka Streams API:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: It is part of the open source Apache Kafka project.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It supports per record streams processing with a very low latency (milliseconds).
    There is no micro- batching concept in the Kafka Streams API. Every record that
    comes into the stream is processed on its own.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It supports stateless processing (filtering and mapping), stateful processing
    (joins and aggregations), and windowing operations (for example, counting the
    last minute, last 5 minutes, last 30 minutes, or last day's worth of data, and
    so on).
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To run the Kafka Streams API, there is no need to build a separate cluster that
    has multiple machines. Developers can use the Kafka Streams API in their Java
    applications or microservices to process real-time data.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Kafka Streams API is highly scalable and fault-tolerant.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Kafka Streams API is completely deployment agnostic. It can be deployed
    on a bare metal machine, VMs, Kubernetes containers, and on Cloud. There are no
    restrictions at all. Stream APIs are never deployed on Kafka Brokers. It is a
    separate application just like any other Java application, which is deployed outside
    of Kafka brokers.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It uses the Kafka security model.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It supports exactly-once semantics since version 0.11.0.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's review the earlier image again to find out the exact place of the Kafka
    Streams API in the overall Kafka architecture.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few useful URLs to understand Kafka Streams in detail:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '[https://kafka.apache.org/documentation/](https://kafka.apache.org/documentation/)'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.confluent.io/blog/](https://www.confluent.io/blog/)'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.confluent.io/blog/introducing-kafka-streams-stream-processing-made-simple/](https://www.confluent.io/blog/introducing-kafka-streams-stream-processing-made-simple/)'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.confluent.io/current/streams/index.html](https://docs.confluent.io/current/streams/index.html)'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark Streaming
  id: totrans-282
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Please note that we will discuss Spark in [Chapter 7](39cf9925-e4cc-473a-9032-a21b81e7b400.xhtml),
    *Large-Scale Data Processing Frameworks,* which is fully dedicated to Spark. However,
    in this section, I will discuss some important features of Spark Streaming. For
    better understanding, readers are advised to study [Chapter 7](39cf9925-e4cc-473a-9032-a21b81e7b400.xhtml), *Large-Scale
    Data Processing Frameworks* first and come back to read this section further to
    understand more about Spark Streaming.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: It is a general practice to use Hadoop MapReduce for batch processing and Apache
    Storm for real-time stream processing.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: The use of these two different programming models causes an increase in code
    size, number of bugs to fix, and development effort; it also introduces a learning
    curve and causes other issues. Spark Streaming helps fix these issues and provides
    a scalable, efficient, resilient, and integrated (with batch processing) system.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: The strength of Spark Streaming lies in its ability to combine with batch processing.
    It's possible to create a RDD using normal Spark programming and join it with
    a Spark stream. Moreover, the code base is similar and allows easy migration if
    required—and there is zero to no learning curve from Spark.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark Streaming is an extension of the core Spark API. It extends Spark for
    doing real-time stream processing. Spark Streaming has the following features:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: It's scalable—it scales on hundreds of nodes
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It provides high-throughput and achieves second level latency
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It's fault-tolerant and it efficiently receives from the failures
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It integrates with batch and interactive data processing
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark Streaming processes data streams application as a series of very small,
    deterministic batch jobs.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: Spark Streaming provides an API in Scala, Java, and Python. Spark Streaming
    divides live stream of data into multiple batches based on time. The time can
    range from one second to a few minutes/hours. In general, batches are divided
    into a few seconds. Spark treats each batch as a RDD and process each based on
    RDD operations (map, filter, join flatmap, distinct, reduceByKey, and so on).
    Lastly, the processed results of RDDs are returned in batches.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image depicts the Spark Streaming data flow:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5a221209-8e20-480b-8121-1d6c49e99f61.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
- en: 'Here are few useful URLs for understanding Spark Streaming in detail:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '[https://databricks.com/blog](https://databricks.com/blog)'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://databricks.com/blog/category/engineering/streaming](https://databricks.com/blog/category/engineering/streaming)'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://spark.apache.org/streaming/](https://spark.apache.org/streaming/)'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Flink
  id: totrans-300
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Apache Flink''s documentation describes Flink in the following way: Flink is
    an open-source framework for distributed stream processing.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: Flink provides accurate results and supports out-of-order or late-arriving datasets.
    It is stateful and fault-tolerant and can seamlessly recover from failures while
    maintaining an exactly-once application state. It performs at a large scale, running
    on thousands of nodes with very good throughput and latency characteristics.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the features of Apache Flink:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: Flink guarantees exactly-once semantics for stateful computations
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flink supports stream processing and windowing with event time semantics
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flink supports flexible windowing based on time, count, or sessions, in addition
    to data-driven windows
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flink is capable of high throughput and low latency
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flink's savepoints provide a state versioning mechanism, making it possible
    to update applications or reprocess historic data with no lost state and minimal
    downtime
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flink is designed to run on large-scale clusters with many thousands of nodes,
    and, in addition to a standalone cluster mode, Flink provides support for YARN
    and Mesos
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flink's core is a distributed streaming dataflow engine. It supports processing
    one stream at a time rather than processing an entire batch of streams at a time.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: 'Flink supports the following libraries:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: CEP
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graph processing
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Storm compatibility
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Flink supports the following APIs:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '**DataStream API**: This API helps all the streams, transformations, that is,
    filtering, aggregations, counting, and windowing'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DataSet API**: This API helps all the batch data transformations, that is,
    join, group, map, and filter'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Table API**: Supports SQL over relational data streams'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Streaming SQL**: Supports SQL over batch and streaming tables'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following image describes the Flink programming model:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d7ae2f42-7921-40b3-a961-a6a3e13bd11c.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
- en: 'The following image describes the Flink architecture:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4f2dfbe0-b9ed-4693-8908-d73d07e4c0f9.png)'
  id: totrans-324
  prefs: []
  type: TYPE_IMG
- en: 'The following are the components of the Flink programming model:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '**Source**: A data source where data is collected and sent to the Flink engine'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transformation**: In this component the whole transformation takes place'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sink**: A target where processed streams are sent'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are a few useful URLs to understand Spark Streaming in detail:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '[https://ci.apache.org/projects/flink/flink-docs-release-1.4/](https://ci.apache.org/projects/flink/flink-docs-release-1.4/)'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.youtube.com/watch?v=ACS6OM1-xgE&amp;amp;feature=youtu.be](https://www.youtube.com/watch?v=ACS6OM1-xgE&feature=youtu.be)'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following sections, we will take a look at a comparison of various stream
    frameworks.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: Apache Flink versus Spark
  id: totrans-333
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main focus of Spark Streaming is stream-batching operation, called **micro-batching**.
    This programming model suits many use cases, but not all use cases require real-time
    stream processing with sub-second latency. For example, a use case such as credit
    card fraud prevention requires millisecond latency. Hence, the micro-batching
    programming model is not suited there. (But, the latest version of Spark, 2.4,
    supports millisecond data latency).
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: Apache Flink supports millisecond latency and is suited for use cases such as
    fraud detection and like.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark versus Storm
  id: totrans-336
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark uses micro-batches to process events while Storm processes events one
    by one. It means that Spark has a latency of seconds while Storm provides a millisecond
    of latency. Spark Streaming provides a high-level abstraction called a **Discretized
    Stream** or **DStream**, which represents a continuous sequence of RDDs. (But,
    the latest version of Spark, 2.4 supports millisecond data latency.) The latest
    Spark version supports DataFrames.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: Almost the same code (API) can be used for Spark Streaming and Spark batch jobs.
    That helps to reuse most of the code base for both programming models. Also, Spark
    supports Machine learning and the Graph API. So, again, the same codebase can
    be used for those use cases as well.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-339
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we started with a detailed understanding of real-time stream
    processing concepts, including data stream, batch vs. real-time processing, CEP,
    low latency, continuous availability, horizontal scalability, storage, and so
    on. Later, we learned about Apache Kafka, which is a very important component
    of modern real-time stream data pipelines. The main features of Kafka are scalability,
    durability, reliability, and high throughput.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: We also learned about Kafka Connect; its architecture, data flow, sources, and
    connectors. We studied case studies to design a data pipeline with Kafka Connect
    using file source, file Sink, JDBC source, and file Sink Connectors.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: In the later sections, we learned about various open source real-time stream-processing
    frameworks, such as the Apache Storm framework. We have seen a few practical examples,
    as well. Apache Storm is distributed and supports low-latency and multiple programming
    languages. Storm is fault-tolerant and reliable. It supports at least once or
    exactly-once processing.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 在后面的章节中，我们学习了各种开源实时流处理框架，例如 Apache Storm 框架。我们也看到了一些实际的应用示例。Apache Storm 是一个分布式框架，支持低延迟和多种编程语言。Storm
    具有容错性和可靠性，支持至少一次或恰好一次的处理。
- en: Spark Streaming helps to fix these issues and provides a scalable, efficient,
    resilient, and integrated (with batch processing) system. The strength of Spark
    Streaming lies in its ability to combine with batch processing. Spark Streaming
    is scalable, and provides high-throughput. It supports micro-batching for second
    level latency, is fault-tolerant, and integrates with batch and interactive data
    processing.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming 有助于解决这些问题，并提供了一个可扩展、高效、弹性且与批量处理集成的系统。Spark Streaming 的优势在于其与批量处理的结合能力。Spark
    Streaming 可扩展，提供高吞吐量。它支持微批处理以实现二级延迟，具有容错性，并与批量及交互式数据处理集成。
- en: Apache Flink guarantees exactly-once semantics, supports event time semantics,
    high throughput, and low latency. It is designed to run on large-scale clusters.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Flink 保证恰好一次的语义，支持事件时间语义，高吞吐量和低延迟。它被设计用于在大型集群上运行。
