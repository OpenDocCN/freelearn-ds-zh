<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch06"/>Chapter 6. SciPy for Data Mining</h1></div></div></div><p>This chapter covers those branches of mathematics and statistics that treat the collection, organization, analysis, and interpretation of data. There are different applications and operations that spread over several modules and submodules: <code class="literal">scipy.stats</code> (for purely statistical tools), <code class="literal">scipy.ndimage.measurements</code> (for analysis and organization of data), <code class="literal">scipy.spatial</code> (for spatial algorithms and data structures), and finally the clustering package <code class="literal">scipy.cluster</code>. The <code class="literal">scipy.cluster</code> clustering package consists of two submodules: <code class="literal">scipy.cluster.vq</code> (vector quantization) and <code class="literal">scipy.cluster.hierarchy</code> (for hierarchical and <strong>agglomerative</strong> clustering).</p><p>As in the previous chapters, fluency with the subject matter is assumed. Our emphasis is to show you some of the SciPy functions available to perform statistical computations, not to teach it. Accordingly, you are welcome to read this chapter along side your preferred book(s) on the subject so that you can fully explore the examples provided in this chapter on additional data sets.</p><p>We should mention, however, that there are other specialized modules in Python that can be used <a id="id279" class="indexterm"/>to explore this subject from different <a id="id280" class="indexterm"/>perspectives. Some of them (not covered by any means in this book) are the <strong>Modular Toolkit for Data Processing</strong> (<strong>MDP</strong>) (<a class="ulink" href="http://mdp-toolkit.sourceforge.net/install.html">http://mdp-toolkit.sourceforge.net/install.html</a>), <strong>scikit-learn</strong> (<a class="ulink" href="http://scikit-learn.org/">http://scikit-learn.org/</a>), and <strong>Statsmodels</strong> (<a class="ulink" href="http://statsmodels.sourceforge.net/">http://statsmodels.sourceforge.net/</a>).</p><p>In this chapter<a id="id281" class="indexterm"/>, we will cover the following things:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The standard descriptive statistics measures computed via SciPy</li><li class="listitem" style="list-style-type: disc">The built-in functions in SciPy that deal with statistical distributions</li><li class="listitem" style="list-style-type: disc">The Scipy functionality to find interval estimation</li><li class="listitem" style="list-style-type: disc">Performing computations of statistical correlations and some statistical tests, the fitting of distributions, and statistical distances</li><li class="listitem" style="list-style-type: disc">A clustering example</li></ul></div><div><div><div><div><h1 class="title"><a id="ch06lvl1sec47"/>Descriptive statistics</h1></div></div></div><p>We often<a id="id282" class="indexterm"/> require the analysis of data in which certain features are grouped in different regions, each with different sizes, values, shapes, and so on. The <code class="literal">scipy.ndimage.measurements</code> submodule has the right tools for this task, and the best way to illustrate the capabilities of the module is by means of exhaustive examples. For example, for binary images of zeros and ones, it is possible to label each blob (areas of contiguous pixels with value one) and obtain the number of these with the <code class="literal">label</code> command. If we desire to obtain the center of mass of the blobs, we may do so with the <code class="literal">center_of_mass command</code>. We may see these operations in action once again in the application to obtain the structural model of oxides in <a class="link" href="ch07.html" title="Chapter 7. SciPy for Computational Geometry">Chapter 7</a>, <em>SciPy for Computational Geometry</em>.</p><p>For nonbinary data, the <code class="literal">scipy.ndimage.measurements</code> submodule provides the usual basic statistical measurements (value and location of extreme values, mean, standard deviation, sum, variance, histogram, and so on).</p><p>For more advanced statistical measurements, we must access functions from the <code class="literal">scipy.stats</code> module. We may now use geometric and harmonic means (<code class="literal">gmean</code>, <code class="literal">hmean</code>), median, mode, skewness, various moments, or kurtosis (<code class="literal">median</code>, <code class="literal">mode</code>, <code class="literal">skew</code>, <code class="literal">moment</code>, <code class="literal">kurtosis</code>). For an overview of the most significant statistical properties of the dataset, we prefer to use the <code class="literal">describe</code> routine. We may also compute item frequencies (<code class="literal">itemfreq</code>), percentiles (<code class="literal">scoreatpercentile</code>, <code class="literal">percentileofscore</code>), histograms (<code class="literal">histogram</code>, <code class="literal">histogram2</code>), cumulative and relative frequencies (<code class="literal">cumfreq</code>, <code class="literal">relfreq</code>), standard error (<code class="literal">sem</code>), and the signal-to-noise ratio (<code class="literal">signaltonoise</code>), which is always useful.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch06lvl1sec48"/>Distributions</h1></div></div></div><p>One of the<a id="id283" class="indexterm"/> main strengths of the <code class="literal">scipy.stats</code> module is the great number of distributions coded, both continuous and discrete. The list is impressively large and has at least 80 continuous distributions and 10 discrete distributions.</p><p>One of the most common ways to employ these distributions is the generation of random numbers. We have been employing this technique to <em>contaminate</em> our images with noise, for example:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; import scipy.misc </strong>
<strong>&gt;&gt;&gt; from scipy.stats import signaltonoise </strong>
<strong>&gt;&gt;&gt; from scipy.stats import norm     # Gaussian distribution</strong>
<strong>&gt;&gt;&gt; lena=scipy.misc.lena().astype(float)</strong>
<strong>&gt;&gt;&gt; lena+= norm.rvs(loc=0,scale=16,size=lena.shape)</strong>
<strong>&gt;&gt;&gt; signaltonoise(lena,axis=None)</strong>
</pre></div><p>The output is shown as follows:</p><div><pre class="programlisting"><strong>array(2.459233897516763)</strong>
</pre></div><p>Let's see the SciPy way of handling distributions. First, a random variable class is created (in SciPy there is the <code class="literal">rv_continuous</code> class for continuous random variables and the <code class="literal">rv_discrete</code> class for the discrete case). Each continuous random variable has an associated<a id="id284" class="indexterm"/> probability density function (<code class="literal">pdf</code>), a cumulative distribution function (<code class="literal">cdf</code>), a survival function along with its inverse (<code class="literal">sf</code>, <code class="literal">isf</code>), and all possible descriptive statistics. They also have associated the random variable, <code class="literal">rvs</code>, which is what we used to actually generate the random instances. For example, with a Pareto continuous random variable with parameter <em>b = 5</em>, to check these properties, we could issue the following commands:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; import numpy</strong>
<strong>&gt;&gt;&gt; from scipy.stats import pareto</strong>
<strong>&gt;&gt;&gt; import matplotlib.pyplot as plt</strong>
<strong>&gt;&gt;&gt; x=numpy.linspace(1,10,1000)</strong>
<strong>&gt;&gt;&gt; plt.subplot(131); plt.plot(pareto.pdf(x,5))</strong>
<strong>&gt;&gt;&gt; plt.subplot(132); plt.plot(pareto.cdf(x,5))</strong>
<strong>&gt;&gt;&gt; plt.subplot(133); plt.plot(pareto.rvs(5,size=1000))</strong>
<strong>&gt;&gt;&gt; plt.show()</strong>
</pre></div><p>This gives the following graphs, showing probability density function (left), cumulative distribution function (center), and random generation (right):</p><div><img src="img/7702OS_06_01.jpg" alt="Distributions"/></div></div>
<div><div><div><div><h1 class="title"><a id="ch06lvl1sec49"/>Interval estimation, correlation measures, and statistical tests</h1></div></div></div><p>We briefly<a id="id285" class="indexterm"/> covered interval estimation as an introductory example of SciPy: <code class="literal">bayes_mvs</code>, in <a class="link" href="ch01.html" title="Chapter 1. Introduction to SciPy">Chapter 1</a>, <em>Introduction to SciPy</em>, with very simple syntax, as follows:</p><div><pre class="programlisting">bayes_mvs(data, alpha=0.9)</pre></div><p>It returns a tuple of three arguments in which each argument has the form <code class="literal">(center, (lower, upper))</code>. The first argument refers to the mean; the second refers to the variance; and the third to the standard deviation. All intervals are computed according to the probability given by <code class="literal">alpha</code>, which is <code class="literal">0.9</code> by default.</p><p>We may use the <code class="literal">linregress</code> routine to compute the regression line of some two-dimensional data <em>x</em>, or two sets of one-dimensional data, <em>x</em> and <em>y</em>. We may compute different<a id="id286" class="indexterm"/> correlation coefficients, with their corresponding p-values, as well. We have the <strong>Pearson correlation coefficient</strong> (<code class="literal">pearsonr</code>), <strong>Spearman's rank-order correlation</strong> (<code class="literal">spearmanr</code>), <strong>point biserial correlation</strong> (<code class="literal">pointbiserialr</code>), and <strong>Kendall's tau</strong> for ordinal data (<code class="literal">kendalltau</code>). In all cases, the<a id="id287" class="indexterm"/> syntax is the same, as it is only required either a<a id="id288" class="indexterm"/> two-dimensional array of data, or two one-dimensional arrays of data with the same length.</p><p>SciPy also <a id="id289" class="indexterm"/>has most of the best-known statistical tests and procedures: <strong>t-tests</strong> (<code class="literal">ttest_1samp</code> for one group of scores, <code class="literal">ttest_ind</code> for two independent<a id="id290" class="indexterm"/> samples of scores, or <code class="literal">ttest_rel</code> for two related samples of scores), <strong>Kolmogorov-Smirnov tests</strong> for goodness of fit (<code class="literal">kstest</code>, <code class="literal">ks_2samp</code>), one-way <strong>Chi-square test</strong> (<code class="literal">chisquare</code>), and many more.</p><p>Let us<a id="id291" class="indexterm"/> illustrate some of the routines of this module with a textbook example, based on Timothy Sturm's studies on control design.</p><p>To turn a knob that moved an indicator by the screw action, 25 right-handed individuals were asked to use their right hands. There were two identical instruments, one with a right-handed thread where the knob turned clockwise, and the other with a left-hand thread where the knob turned counter-clockwise. The following table gives the times in seconds each subject took to move the indicator to a fixed distance:</p><div><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><tbody><tr><td style="text-align: left" valign="top">
<p>
<strong>Subject</strong>
</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>2</p>
</td><td style="text-align: left" valign="top">
<p>3</p>
</td><td style="text-align: left" valign="top">
<p>4</p>
</td><td style="text-align: left" valign="top">
<p>5</p>
</td><td style="text-align: left" valign="top">
<p>6</p>
</td><td style="text-align: left" valign="top">
<p>7</p>
</td><td style="text-align: left" valign="top">
<p>8</p>
</td><td style="text-align: left" valign="top">
<p>9</p>
</td><td style="text-align: left" valign="top">
<p>10</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<strong>Right thread</strong>
</p>
</td><td style="text-align: left" valign="top">
<p>113</p>
</td><td style="text-align: left" valign="top">
<p>105</p>
</td><td style="text-align: left" valign="top">
<p>130</p>
</td><td style="text-align: left" valign="top">
<p>101</p>
</td><td style="text-align: left" valign="top">
<p>138</p>
</td><td style="text-align: left" valign="top">
<p>118</p>
</td><td style="text-align: left" valign="top">
<p>87</p>
</td><td style="text-align: left" valign="top">
<p>116</p>
</td><td style="text-align: left" valign="top">
<p>75</p>
</td><td style="text-align: left" valign="top">
<p>96</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<strong>Left thread</strong>
</p>
</td><td style="text-align: left" valign="top">
<p>137</p>
</td><td style="text-align: left" valign="top">
<p>105</p>
</td><td style="text-align: left" valign="top">
<p>133</p>
</td><td style="text-align: left" valign="top">
<p>108</p>
</td><td style="text-align: left" valign="top">
<p>115</p>
</td><td style="text-align: left" valign="top">
<p>170</p>
</td><td style="text-align: left" valign="top">
<p>103</p>
</td><td style="text-align: left" valign="top">
<p>145</p>
</td><td style="text-align: left" valign="top">
<p>78</p>
</td><td style="text-align: left" valign="top">
<p>107</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<strong>Subject</strong>
</p>
</td><td style="text-align: left" valign="top">
<p>11</p>
</td><td style="text-align: left" valign="top">
<p>12</p>
</td><td style="text-align: left" valign="top">
<p>13</p>
</td><td style="text-align: left" valign="top">
<p>14</p>
</td><td style="text-align: left" valign="top">
<p>15</p>
</td><td style="text-align: left" valign="top">
<p>16</p>
</td><td style="text-align: left" valign="top">
<p>17</p>
</td><td style="text-align: left" valign="top">
<p>18</p>
</td><td style="text-align: left" valign="top">
<p>19</p>
</td><td style="text-align: left" valign="top">
<p>20</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<strong>Right thread</strong>
</p>
</td><td style="text-align: left" valign="top">
<p>122</p>
</td><td style="text-align: left" valign="top">
<p>103</p>
</td><td style="text-align: left" valign="top">
<p>116</p>
</td><td style="text-align: left" valign="top">
<p>107</p>
</td><td style="text-align: left" valign="top">
<p>118</p>
</td><td style="text-align: left" valign="top">
<p>103</p>
</td><td style="text-align: left" valign="top">
<p>111</p>
</td><td style="text-align: left" valign="top">
<p>104</p>
</td><td style="text-align: left" valign="top">
<p>111</p>
</td><td style="text-align: left" valign="top">
<p>89</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<strong>Left thread</strong>
</p>
</td><td style="text-align: left" valign="top">
<p>84</p>
</td><td style="text-align: left" valign="top">
<p>148</p>
</td><td style="text-align: left" valign="top">
<p>147</p>
</td><td style="text-align: left" valign="top">
<p>87</p>
</td><td style="text-align: left" valign="top">
<p>166</p>
</td><td style="text-align: left" valign="top">
<p>146</p>
</td><td style="text-align: left" valign="top">
<p>123</p>
</td><td style="text-align: left" valign="top">
<p>135</p>
</td><td style="text-align: left" valign="top">
<p>112</p>
</td><td style="text-align: left" valign="top">
<p>93</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<strong>Subject</strong>
</p>
</td><td style="text-align: left" valign="top">
<p>21</p>
</td><td style="text-align: left" valign="top">
<p>22</p>
</td><td style="text-align: left" valign="top">
<p>23</p>
</td><td style="text-align: left" valign="top">
<p>24</p>
</td><td style="text-align: left" valign="top">
<p>25</p>
</td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td></tr><tr><td style="text-align: left" valign="top">
<p>
<strong>Right thread</strong>
</p>
</td><td style="text-align: left" valign="top">
<p>78</p>
</td><td style="text-align: left" valign="top">
<p>100</p>
</td><td style="text-align: left" valign="top">
<p>89</p>
</td><td style="text-align: left" valign="top">
<p>85</p>
</td><td style="text-align: left" valign="top">
<p>88</p>
</td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td></tr><tr><td style="text-align: left" valign="top">
<p>
<strong>Left thread</strong>
</p>
</td><td style="text-align: left" valign="top">
<p>76</p>
</td><td style="text-align: left" valign="top">
<p>116</p>
</td><td style="text-align: left" valign="top">
<p>78</p>
</td><td style="text-align: left" valign="top">
<p>101</p>
</td><td style="text-align: left" valign="top">
<p>123</p>
</td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td><td style="text-align: left" valign="top"> </td></tr></tbody></table></div><p>We may <a id="id292" class="indexterm"/>perform an analysis that leads to a conclusion about right-handed people finding right-hand threads easier to use, by a simple one-sample t-statistic. We will load the data in memory, as follows:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; import numpy</strong>
<strong>&gt;&gt;&gt; data = numpy.array([[113,105,130,101,138,118,87,116,75,96, \</strong>
<strong>         122,103,116,107,118,103,111,104,111,89,78,100,89,85,88], \</strong>
<strong>         [137,105,133,108,115,170,103,145,78,107, \</strong>
<strong>         84,148,147,87,166,146,123,135,112,93,76,116,78,101,123]])</strong>
</pre></div><p>The difference of each row indicates which knob was faster, and for how much time. We can obtain that information easily and perform some basic statistical analysis on it. We will start by computing the mean, standard deviation, and a histogram with 10 bins:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; dataDiff = data[1,:]-data[0,:]</strong>
<strong>&gt;&gt;&gt; dataDiff.mean(), dataDiff.std()</strong>
</pre></div><p>The output is shown as:</p><div><pre class="programlisting"><strong>(13.32, 22.472596645692729)</strong>
</pre></div><p>Let's plot the histogram by issuing the following set of commands:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; import matplotlib.pyplot as plt</strong>
<strong>&gt;&gt;&gt; plt.hist(dataDiff)</strong>
<strong>&gt;&gt;&gt; plt.show()</strong>
</pre></div><p>This produces the following histogram:</p><div><img src="img/7702OS_06_02.jpg" alt="Interval estimation, correlation measures, and statistical tests"/></div><p>In light<a id="id293" class="indexterm"/> of this histogram, it is not far-fetched to assume a normal distribution. If we assume that this is a proper simple random sample, the use of t-statistics is justified. We would like to prove that it takes longer to turn the left thread than the right, so we set the mean of <code class="literal">dataDiff</code> to be contrasted against the zero mean (which would indicate that it takes the same time for both threads).</p><p>The two-sample t-statistics and p-value for the two-sided test are computed by the simple command, as follows:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; from scipy.stats import ttest_1samp</strong>
<strong>&gt;&gt;&gt; t_stat,p_value=ttest_1samp(dataDiff,0.0)</strong>
</pre></div><p>The p-value for the one-sided test is then calculated:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; print (p_value/2.0)</strong>
</pre></div><p>The output is shown as follows:</p><div><pre class="programlisting"><strong>0.00389575522747</strong>
</pre></div><p>Note that this p-value is much smaller than either of the usual thresholds <code class="literal">alpha = 0.05</code> or <code class="literal">alpha = 0.1</code>. We can thus guarantee that we have enough evidence to support the claim that right-handed threads take less time to turn than left-handed threads.</p></div>
<div><div><div><div><h1 class="title"><a id="ch06lvl1sec50"/>Distribution fitting</h1></div></div></div><p>In Timothy <a id="id294" class="indexterm"/>Sturm's example, we claim that the histogram of some data seemed to fit a normal distribution. SciPy has a few routines to help us approximate the best distribution to a random variable, together with the parameters that best approximate this fit. For example, for the data in that problem, the mean and standard deviation of the normal distribution that realizes the best fit can be found in the following way:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; from scipy.stats import norm     # Gaussian distribution</strong>
<strong>&gt;&gt;&gt; mean,std=norm.fit(dataDiff)</strong>
</pre></div><p>We can now plot the (<code class="literal">normed</code>) histogram of the data, together with the computed probability density function, as follows:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; plt.hist(dataDiff, normed=1)</strong>
<strong>&gt;&gt;&gt; x=numpy.linspace(dataDiff.min(),dataDiff.max(),1000)</strong>
<strong>&gt;&gt;&gt; pdf=norm.pdf(x,mean,std)</strong>
<strong>&gt;&gt;&gt; plt.plot(x,pdf)</strong>
<strong>&gt;&gt;&gt; plt.show()</strong>
</pre></div><p>We will obtain the following graph showing the maximum likelihood estimate to the normal distribution that best fits <code class="literal">dataDiff</code>:</p><div><img src="img/7702OS_06_03.jpg" alt="Distribution fitting"/></div><p>We may even fit the best probability density function without specifying any particular distribution, thanks to a non-parametric technique, <strong>kernel density estimation</strong>. We can find an<a id="id295" class="indexterm"/> algorithm to perform Gaussian kernel density estimation<a id="id296" class="indexterm"/> in the <code class="literal">scipy.stats.kde</code> submodule. Let us show by example with the same data as before:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; from scipy.stats import gaussian_kde</strong>
<strong>&gt;&gt;&gt; pdf=gaussian_kde(dataDiff)</strong>
</pre></div><p>A slightly different plotting session as given before, offers us the following graph, showing probability density function obtained by kernel density estimation on <code class="literal">dataDiff</code>:</p><div><img src="img/7702OS_06_04.jpg" alt="Distribution fitting"/></div><p>The full piece of code is as follows:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; from scipy.stats import gaussian_kde</strong>
<strong>&gt;&gt;&gt; pdf = gaussian_kde(dataDiff)</strong>
<strong>&gt;&gt;&gt; pdf = pdf.evaluate(x)</strong>
<strong>&gt;&gt;&gt; plt.hist(dataDiff, normed=1)</strong>
<strong>&gt;&gt;&gt; plt.plot(x,pdf,'k')</strong>
<strong>&gt;&gt;&gt; plt.savefig("hist2.png")</strong>
<strong>&gt;&gt;&gt; plt.show()</strong>
</pre></div><p>For comparative purposes, the last two plots can be combined into one:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; plt.hist(dataDiff, normed=1)</strong>
<strong>&gt;&gt;&gt; plt.plot(x,pdf,'k.-',label='Kernel fit')</strong>
<strong>&gt;&gt;&gt; plt.plot(x,norm.pdf(x,mean,std),'r',label='Normal fit')</strong>
<strong>&gt;&gt;&gt; plt.legend() </strong>
<strong>&gt;&gt;&gt; plt.savefig("hist3.png")</strong>
<strong>&gt;&gt;&gt; plt.show()</strong>
</pre></div><p>The <a id="id297" class="indexterm"/>output is the combined plot as follows:</p><div><img src="img/7702OS_06_05.jpg" alt="Distribution fitting"/></div></div>
<div><div><div><div><h1 class="title"><a id="ch06lvl1sec51"/>Distances</h1></div></div></div><p>In the field <a id="id298" class="indexterm"/>of data mining, it is often required to determine which members of a training set are closest to unknown test instances. It is imperative to have a good set of different distance functions for any of the algorithms that perform the search, and SciPy has, for this purpose, a huge collection of optimally coded functions in the distance submodule of the scipy.spatial module. The list is long. Besides Euclidean, squared<a id="id299" class="indexterm"/> Euclidean, or <a id="id300" class="indexterm"/>standardized Euclidean, we<a id="id301" class="indexterm"/> have many more—<strong>Bray-Curtis</strong>, <strong>Canberra</strong>, <strong>Chebyshev</strong>, <strong>Manhattan</strong>, correlation <a id="id302" class="indexterm"/>distance, cosine distance, <strong>dice dissimilarity</strong>, <strong>Hamming</strong>, <strong>Jaccard-Needham</strong>, <strong>Kulsinski</strong>, <strong>Mahalanobis</strong>, and so on. The syntax <a id="id303" class="indexterm"/>in <a id="id304" class="indexterm"/>most <a id="id305" class="indexterm"/>cases is<a id="id306" class="indexterm"/> simple:</p><div><pre class="programlisting">distance_function(first_vector, second_vector)</pre></div><p>The only <a id="id307" class="indexterm"/>three cases in which the syntax is different are the Minkowski, Mahalanobis, and standardized Euclidean distances, in which the distance function requires either an integer number (for the order of the norm in the definition of Minkowski distance), a covariance for the Mahalanobis case (but this is an optional requirement), or a variance matrix to standardize the Euclidean distance.</p><p>Let us see now a fun exercise to visualize the unit balls in Minkowski metrics:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; import numpy </strong>
<strong>&gt;&gt;&gt; from scipy.spatial.distance import minkowski </strong>
<strong>&gt;&gt;&gt; Square=numpy.mgrid[-1.1:1.1:512j,-1.1:1.1:512j]</strong>
<strong>&gt;&gt;&gt; X=Square[0]; Y=Square[1]</strong>
<strong>&gt;&gt;&gt; f=lambda x,y,p: minkowski([x,y],[0.0,0.0],p)&lt;=1.0</strong>
<strong>&gt;&gt;&gt; Ball=lambda p:numpy.vectorize(f)(X,Y,p)</strong>
</pre></div><p>We have created a function, <code class="literal">Ball</code>, which creates a grid of 512 x 512 Boolean values. The grid represents a square of length 2.2 centered at the origin, with sides parallel to the coordinate axis, and the true values on it represent all those points of the grid inside of the unit ball for the Minkowksi metric, for the parameter <code class="literal">p</code>. All we have to do is show it graphically, as in the following example:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; import matplotlib.pylab as plt</strong>
<strong>&gt;&gt;&gt; plt.imshow(Ball(3), cmap = plt.cm.gray)</strong>
<strong>&gt;&gt;&gt; plt.axis('off')</strong>
<strong>&gt;&gt;&gt; plt.subplots_adjust(left=0.0127,bottom=0.0164,\</strong>
<strong>    right=0.987,top=0.984)</strong>
<strong>&gt;&gt;&gt; plt.show()</strong>
</pre></div><p>This produces the following, where <code class="literal">Ball(3)</code> is a unit ball in the Minkowski metric with parameter <code class="literal">p = 3</code>:</p><div><img src="img/7702OS_06_06.jpg" alt="Distances"/></div><p>We feel <a id="id308" class="indexterm"/>the need to issue the following four important warnings:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>First warning</strong>: We must use these routines instead of creating our own definitions<a id="id309" class="indexterm"/> of the corresponding distance functions whenever possible. They guarantee a faster result and optimal coding to take care of situations in which the inputs are either too large or too small.</li><li class="listitem" style="list-style-type: disc"><strong>Second warning</strong>: These functions work great when comparing two vectors; however, for the pairwise computation of many vectors, we must resort to the <code class="literal">pdist</code> routine. This command takes an <em>m x n</em> array representing <em>m</em> vectors of dimension <em>n</em>, and computes the distance of each of them to each other. We indicate the distance function to be used with the option metric and additional parameters as needed. For example, for the Manhattan (<code class="literal">cityblock</code>) distance for five randomly selected randomly selected four-dimensional vectors with integer values <code class="literal">1</code>, <code class="literal">0</code>, or <code class="literal">-1</code>, we could issue the following command:<div><pre class="programlisting"><strong>&gt;&gt;&gt; import scipy.stats</strong>
<strong>&gt;&gt;&gt; from scipy.spatial.distance import pdist</strong>
<strong>&gt;&gt;&gt; V=scipy.stats.randint.rvs(0.4,3,size=(5,4))-1</strong>
<strong>&gt;&gt;&gt; print (V)</strong>
</pre></div><p>The output is shown as follows:</p><div><pre class="programlisting"><strong>[[ 1  0  1 -1]</strong>
<strong> [-1  0 -1  0]</strong>
<strong> [ 1  1  1 -1]</strong>
<strong> [ 1  1 -1  0]</strong>
<strong> [ 0  0  1 -1]]</strong>
</pre></div><p>Let's take a look at the following <code class="literal">pdist</code> command:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; pdist(V,metric='cityblock')</strong>
</pre></div><p>The output is shown as follows:</p><div><pre class="programlisting"><strong>array([ 5.,  1.,  4.,  1.,  6.,  3.,  4.,  3.,  2.,  5.])</strong>
</pre></div><p>This means, if <code class="literal">v1 = [1,0,1,-1]</code>, <code class="literal">v2 = [-1,0,-1,0]</code>, <code class="literal">v3 = [1,1,1,-1]</code>, <code class="literal">v4 = [1,1,-1,0]</code>, and <code class="literal">v5 = [0,0,1,-1]</code>, then the Manhattan distance of <code class="literal">v1</code> from <code class="literal">v2</code> is 5. The distance from <code class="literal">v1</code> to <code class="literal">v3</code> is 1; from <code class="literal">v1</code> to <code class="literal">v4</code>, 4; and from <code class="literal">v1</code> to <code class="literal">v5</code>, 1. From <code class="literal">v2</code> to <code class="literal">v3</code> the distance is 6; from <code class="literal">v2</code> to <code class="literal">v4</code>, 3; and from <code class="literal">v2</code> to <code class="literal">v5</code>, 4. From <code class="literal">v3</code> to <code class="literal">v4</code> the distance is 3; and from <code class="literal">v3</code> to <code class="literal">v5</code>, 2. And finally, the distance from <code class="literal">v4</code> to <code class="literal">v5</code> is 5, which is the last entry of the output.</p></li><li class="listitem" style="list-style-type: disc"><strong>Third warning</strong>: When computing the distance between each pair of two collections <a id="id310" class="indexterm"/>of inputs, we should use the <code class="literal">cdist</code> routine, which <a id="id311" class="indexterm"/>has a similar syntax. For instance, for the two collections of three randomly selected four-dimensional Boolean vectors, the corresponding Jaccard-Needham dissimilarities are computed, as follows:<div><pre class="programlisting"><strong>&gt;&gt;&gt; from scipy.spatial.distance import cdist</strong>
<strong>&gt;&gt;&gt; V=scipy.stats.randint.rvs(0.4, 2, size=(3,4)).astype(bool)</strong>
<strong>&gt;&gt;&gt; W=scipy.stats.randint.rvs(0.4, 3, size=(2,4)).astype(bool)</strong>
<strong>&gt;&gt;&gt; cdist(V,W,'jaccard')</strong>
<strong>array([[ 0.75      ,  1.        ],</strong>
<strong>       [ 0.75      ,  1.        ],</strong>
<strong>       [ 0.33333333,  0.5       ]])</strong>
</pre></div><p>That is, if the three vectors in <code class="literal">V</code> are labeled <code class="literal">v1</code>, <code class="literal">v2</code>, and <code class="literal">v3</code> and if the two vectors in <code class="literal">W</code> are labeled as <code class="literal">w1</code> and <code class="literal">w2</code>, then the dissimilarity between <code class="literal">v1</code> and <code class="literal">w1</code> is 0.75; between <code class="literal">v1</code> and <code class="literal">w2</code>, 1; and so on.</p></li><li class="listitem" style="list-style-type: disc"><strong>Fourth warning</strong>: When we have a large amount of data points and we need to address the problem of nearest neighbors (for example, to locate the closest element of the data to a new instance point), we seldom do it by brute force. The optimal algorithm to perform this search is based on the idea of k-dimensional<a id="id312" class="indexterm"/> trees. SciPy has two classes <a id="id313" class="indexterm"/>to handle these objects – <code class="literal">KDTree</code> and <code class="literal">cKDTree</code>. The latter is a subset of the former, a little faster since it is wrapped from C code, but with very limited use. It only has the <code class="literal">query</code> method to find the nearest neighbors of the input. The syntax is simple, as follows:<div><pre class="programlisting">KDTree(data, leafsize=10)</pre></div><p>This creates a structure containing a binary tree, very apt for the design of fast search algorithms. The <code class="literal">leafsize</code> option indicates at what level the search based on the structure of the binary tree must be abandoned in favor of brute force.</p><p>The other methods associated with the <code class="literal">KDTree</code> class are—<code class="literal">count_neighbors</code>, to compute the number of nearby pairs that can be formed with another <code class="literal">KDTree</code>; <code class="literal">query_ball_point</code>, to find all points at a given distance from the input; <code class="literal">query_ball_tree</code> and <code class="literal">query_pairs</code>, to find all pairs of points within certain distance; and <code class="literal">sparse_distance_matrix</code>, that computes a sparse matrix with the distances between two <code class="literal">KDTree</code> classes.</p><p>Let us see it in action, with a small dataset of 10 randomly generated four-dimensional points with integer entries:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; from scipy.spatial import KDTree</strong>
<strong>&gt;&gt;&gt; data=scipy.stats.randint.rvs(0.4,10,size=(10,4))</strong>
<strong>&gt;&gt;&gt; print (data)</strong>
</pre></div><p>The output is shown as follows:</p><div><pre class="programlisting"><strong>[[8 6 1 1]</strong>
<strong> [2 9 1 5]</strong>
<strong> [4 8 8 9]</strong>
<strong> [2 6 6 4]</strong>
<strong> [4 1 2 1]</strong>
<strong> [3 8 7 2]</strong>
<strong> [1 1 3 6]</strong>
<strong> [5 2 1 5]</strong>
<strong> [2 5 7 3]</strong>
<strong> [6 0 6 9]]</strong>
<strong>&gt;&gt;&gt; tree=KDTree(data)</strong>
<strong>&gt;&gt;&gt; tree.query([0,0,0,0])</strong>
</pre></div><p>The output is shown as follows:</p><div><pre class="programlisting"><strong>(4.6904157598234297, 4)</strong>
</pre></div></li></ul></div><p>This means, among <a id="id314" class="indexterm"/>all the points in the dataset, the closest one<a id="id315" class="indexterm"/> in the Euclidean distance to the origin is the fifth one (index 4), and the distance is precisely about 4.6 units.</p><p>We can have an input of more than one point; the output will still be a tuple, where the first entry is an array that indicates the smallest distance to each of the input points. The second entry is another array that indicates the indices of the nearest neighbors.</p></div>
<div><div><div><div><h1 class="title"><a id="ch06lvl1sec52"/>Clustering</h1></div></div></div><p>Another <a id="id316" class="indexterm"/>technique used in data mining is clustering. SciPy has two modules to deal with any problem in this field, each of them addressing a different clustering tool—<code class="literal">scipy.cluster.vq</code> for k-means and <code class="literal">scipy.cluster.hierarchy</code> for hierarchical clustering.</p><div><div><div><div><h2 class="title"><a id="ch06lvl2sec39"/>Vector quantization and k-means</h2></div></div></div><p>We<a id="id317" class="indexterm"/> have two routines to divide data into clusters using the<a id="id318" class="indexterm"/> k-means technique—<code class="literal">kmeans</code> and <code class="literal">kmeans2</code>. They correspond to two different implementations. The former has a very simple syntax:</p><div><pre class="programlisting">kmeans(obs, k_or_guess, iter=20, thresh=1e-05)</pre></div><p>The <code class="literal">obs</code> parameter is an <code class="literal">ndarray</code> with the data we wish to cluster. If the dimensions of the array are <em>m</em> x <em>n</em>, the algorithm interprets this data as <em>m</em> points in the n-dimensional Euclidean space. If we know the number of clusters in which this data should be divided, we enter so with the <code class="literal">k_or_guess</code> option. The output is a tuple with two elements. The first is an <code class="literal">ndarray</code> of dimension <em>k</em> x <em>n</em>, representing a collection of points—as many as clusters were indicated. Each of these locations indicates the centroid of the found clusters. The second entry of the tuple is a floating-point value indicating the distortion between the passed points, and the centroids generated previously.</p><p>If we wish to impose an initial guess for the centroids of the clusters, we may do so with the <code class="literal">k_or_guess</code> parameter again, by sending a <em>k</em> x <em>n</em> <code class="literal">ndarray</code>.</p><p>The data we pass to <code class="literal">kmeans</code> need to be normalized with the <code class="literal">whiten</code> routine.</p><p>The second option is much more flexible, as its syntax indicates:</p><div><pre class="programlisting">kmeans2(data, k, iter=10, thresh=1e-05,
minit='random', missing='warn')</pre></div><p>The <code class="literal">data</code> and <code class="literal">k</code> parameters are the same as <code class="literal">obs</code> and <code class="literal">k_or_guess</code>, respectively. The difference in this <a id="id319" class="indexterm"/>routine is the possibility of choosing<a id="id320" class="indexterm"/> among different initialization algorithms, hence providing us with the possibility to speed up the process and use fewer resources if we know some properties of our data. We do so by passing to the <code class="literal">minit</code> parameter, one of the strings such as <code class="literal">'random'</code> (initialization centroids are constructed randomly using a Gaussian), <code class="literal">'points'</code> (initialization is done by choosing points belonging to our data), or <code class="literal">'uniform'</code> (if we prefer uniform distribution to Gaussian).</p><p>In case we would like to provide the initialization centroids ourselves with the <code class="literal">k</code> parameter, we must indicate our choice to the algorithm by passing <code class="literal">'matrix'</code> to the <code class="literal">minit</code> option as well.</p><p>In any case, if we wish to classify the original data by assigning to each point the cluster to which it belongs; we do so with the <code class="literal">vq</code> routine (for vector quantization). The syntax is pretty simple as well:</p><div><pre class="programlisting">vq(obs, centroids)</pre></div><p>The output is a tuple with two entries. The first entry is a one-dimensional <code class="literal">ndarray</code> of size <em>n</em> holding for each point in <code class="literal">obs</code>, the cluster to which it belongs. The second entry is another one-dimensional <code class="literal">ndarray</code> of the same size, but containing floating-point values indicating the distance from each point to the centroid of its cluster.</p><p>Let us illustrate with a classical example, the mouse dataset. We will create a big dataset with randomly generated points in three disks, as follows:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; import numpy</strong>
<strong>&gt;&gt;&gt; from scipy.stats import norm</strong>
<strong>&gt;&gt;&gt; from numpy import array,vstack</strong>
<strong>&gt;&gt;&gt; data=norm.rvs(0,0.3,size=(10000,2))</strong>
<strong>&gt;&gt;&gt; inside_ball=numpy.hypot(data[:,0],data[:,1])&lt;1.0</strong>
<strong>&gt;&gt;&gt; data=data[inside_ball]</strong>
<strong>&gt;&gt;&gt; data = vstack((data, data+array([1,1]),data+array([-1,1])))</strong>
</pre></div><p>Once created, we will request the data to be separated into three clusters:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; from scipy.cluster.vq import *</strong>
<strong>&gt;&gt;&gt; centroids, distortion = kmeans(data,3)</strong>
<strong>&gt;&gt;&gt; cluster_assignment, distances = vq(data,centroids)</strong>
</pre></div><p>Let us present the results:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; from matplotlib.pyplot import plot</strong>
<strong>&gt;&gt;&gt; import matplotlib.pyplot as plt</strong>
<strong>&gt;&gt;&gt; plt.plot(data[cluster_assignment==0,0], \</strong>
<strong>     data[cluster_assignment==0,1], 'ro')</strong>
<strong>&gt;&gt;&gt; plt.plot(data[cluster_assignment==1,0], \</strong>
<strong>     data[cluster_assignment==1,1], 'b+')</strong>
<strong>&gt;&gt;&gt; plt.plot(data[cluster_assignment==2,0], \</strong>
<strong>     data[cluster_assignment==2,1], 'k.')</strong>
<strong>&gt;&gt;&gt; plt.show()</strong>
</pre></div><p>This gives<a id="id321" class="indexterm"/> the following plot showing the mouse dataset with <a id="id322" class="indexterm"/>three clusters from left to right—red (squares), blue (pluses), and black (dots):</p><div><img src="img/7702OS_06_07.jpg" alt="Vector quantization and k-means"/></div></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec40"/>Hierarchical clustering</h2></div></div></div><p>There <a id="id323" class="indexterm"/>are several different algorithms to perform <a id="id324" class="indexterm"/>hierarchical clustering. SciPy has routines for the following <a id="id325" class="indexterm"/>methods:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Single/min/nearest method</strong>: <code class="literal">single</code></li><li class="listitem" style="list-style-type: disc"><strong>Complete/max/farthest method</strong>: <code class="literal">complete</code></li><li class="listitem" style="list-style-type: disc"><strong>Average/UPGMA method</strong>: <code class="literal">average</code></li><li class="listitem" style="list-style-type: disc"><strong>Weighted/WPGMA method</strong>: <code class="literal">weighted</code></li><li class="listitem" style="list-style-type: disc"><strong>Centroid/UPGMC method</strong>: <code class="literal">centroid</code></li><li class="listitem" style="list-style-type: disc"><strong>Median/WPGMC method</strong>: <code class="literal">median</code></li><li class="listitem" style="list-style-type: disc"><strong>Ward's linkage method</strong>: <code class="literal">ward</code></li></ul></div><p>In any <a id="id326" class="indexterm"/>of the previous cases, the syntax is the same; the<a id="id327" class="indexterm"/> only input is the dataset, which can be either an <em>m</em> x <em>n</em> <code class="literal">ndarray</code> representing <em>m</em> points in the n-dimensional Euclidean space, or a condensed distance matrix obtained from the previous data using the <code class="literal">pdist</code> routine from <code class="literal">scipy.spatial</code>. The output is always an <code class="literal">ndarray</code> representing the corresponding linkage matrix of the clustering obtained.</p><p>Alternatively, we may call the clustering with the generic routine <code class="literal">linkage</code>. This routine accepts a dataset/distance matrix, and a string indicating the method to use. The strings coincide with the names introduced. The advantage of <code class="literal">linkage</code> over the previous routines is that we are also allowed to indicate a different metric than the usual Euclidean distance. The complete syntax for <code class="literal">linkage</code> is then as follows:</p><div><pre class="programlisting">linkage(data, method='single', metric='euclidean')</pre></div><p>Different statistics on the resulting linkage matrices may be performed with the routines such as Cophenetic distances between observations (<code class="literal">cophenet</code>); inconsistency statistics (<code class="literal">inconsistent</code>); maximum inconsistency coefficient for each non-singleton cluster with its descendants (<code class="literal">maxdists</code>); and maximum statistic for each non-singleton cluster with its descendants (<code class="literal">maxRstat</code>).</p><p>It is customary to use binary trees to represent linkage matrices, and the <code class="literal">scipy.cluster.hierachy</code> submodule has a large number of different routines to manipulate and extract information from these trees. The most useful of these routines is the visualization of these trees, often called dendrograms. The corresponding routine in SciPy is dendrogram, and has the following imposing syntax:</p><div><pre class="programlisting">dendrogram(Z, p=30, truncate_mode=None, color_threshold=None, 
get_leaves=True, orientation='top', labels=None, 
count_sort=False, distance_sort=False, 
show_leaf_counts=True, no_plot=False, no_labels=False, 
color_list=None, leaf_font_size=None, 
leaf_rotation=None, leaf_label_func=None, 
no_leaves=False, show_contracted=False,
link_color_func=None)</pre></div><p>The first obvious parameter, <code class="literal">Z</code>, is a linkage matrix. This is the only non-optional variable. The other options control the style of the output (colors, labels, rotation, and so on), and since they are technically nonmathematical in nature, we will not explore them in detail in this monograph, other than through the simple application to animal clustering shown next.</p></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec41"/>Clustering mammals by their dentition</h2></div></div></div><p>Mammals' teeth<a id="id328" class="indexterm"/> are divided into four groups <a id="id329" class="indexterm"/>such as incisors, canines, premolars, and molars. The dentition of several mammals has been collected, and is available for download at <a class="ulink" href="http://www.uni-koeln.de/themen/statistik/data/cluster/dentitio.dat">http://www.uni-koeln.de/themen/statistik/data/cluster/dentitio.dat</a>.</p><p>This file presents the name of the mammal, together with the number of top incisors, bottom incisors, top canines, bottom canines, top premolars, bottom premolars, top molars, and bottom molars.</p><p>We wish to use hierarchical clustering on that dataset to assess which species are closer to each other by these features.</p><p>We will start by preparing the dataset and store the relevant data in ndarrays. The original data is given as a text file, where each line represents a different mammal. The first four lines are as follows:</p><div><pre class="programlisting">OPOSSUM                    54113344
HAIRY TAIL MOLE            33114433
COMMON MOLE              32103333
STAR NOSE MOLE            33114433</pre></div><p>The first 27 characters of each line hold the name of the animal. The characters in positions 28 to 35 are the number of respective kinds of dentures. We need to prepare this data into something that SciPy can handle. We will collect the names apart, since we will be using them as labels in the dendrogram. The rest of the data will be forced into an array of integers:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; import numpy</strong>
<strong>&gt;&gt;&gt; file=open("dentitio.dat","r") # open the file</strong>
<strong>&gt;&gt;&gt; lines=file.readlines() # read each line in memory</strong>
<strong>&gt;&gt;&gt; file.close() # close the file</strong>
<strong>&gt;&gt;&gt; mammals=[] # this stores the names</strong>
<strong>&gt;&gt;&gt; dataset=numpy.zeros((len(lines),8)) # this stores the data</strong>
<strong>&gt;&gt;&gt; for index,line in enumerate(lines):</strong>
<strong>           mammals.append( line[0:27].rstrip(" ").capitalize() )</strong>
<strong>               for tooth in range(8):</strong>
<strong>                   dataset[index,tooth]=int(line[27+tooth])</strong>
</pre></div><p>We will proceed to compute the linkage matrix and its posterior dendrogram, making sure to use the Python list, mammals, as labels:</p><div><pre class="programlisting"><strong>&gt;&gt;&gt; import matplotlib.pyplot as plt</strong>
<strong>&gt;&gt;&gt; from scipy.cluster.hierarchy import linkage, dendrogram</strong>
<strong>&gt;&gt;&gt; Z=linkage(dataset)</strong>
<strong>&gt;&gt;&gt; dendrogram(Z, labels=mammals, orientation="right")</strong>
<strong>&gt;&gt;&gt; matplotlib.pyplot.show()</strong>
<strong>&gt;&gt;&gt; plt.show()</strong>
</pre></div><p>This gives us the following dendrogram showing clustering of mammals according to their dentition:</p><div><img src="img/7702OS_06_08.jpg" alt="Clustering mammals by their dentition"/></div><p>Note<a id="id330" class="indexterm"/> how all the bats are clustered together. The <a id="id331" class="indexterm"/>mice are also clustered together, but far from the bats. Sheep, goats, antelopes, deer, and moose have similar dentures too, and they appear clustered at the bottom of the tree, next to the opossum and the armadillo. Note how all felines are also clustered together, on the top of the tree.</p><p>Experts in data analysis can obtain more information from dendrograms; they are able to interpret the lengths of the branches or the different colors used in the composition, and give us more insightful explanations about the way the clusters differ from each other.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch06lvl1sec53"/>Summary</h1></div></div></div><p>This chapter dealt with tools appropriate for data mining and explored modules such as <code class="literal">stats</code> (for statistics), <code class="literal">spatial</code> (for data structures), and <code class="literal">cluster</code> (for clustering and vector quantization). In the next chapter, additional functionalities included in the SciPy module, <code class="literal">scipy.spatial</code>, will be studied, complementing the ones already explored in previous chapters. As usual, each function introduced will be illustrated via non-trivial examples which can be enriched  modifying the IPython Notebook corresponding to this chapter.</p></div></body></html>