- en: '*Chapter 4*: Working with Databases'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, you learned how to read and write text files. Reading
    log files or other text files from a data lake and moving them into a database
    or data warehouse is a common task for data engineers. In this chapter, you will
    use the skills you gained working with text files and learn how to move that data
    into a database. This chapter will also teach you how to extract data from relational
    and NoSQL databases. By the end of this chapter, you will have the skills needed
    to work with databases using Python, NiFi, and Airflow. It is more than likely
    that most of your data pipelines will end with a database and very likely that
    they will start with one as well. With these skills, you will be able to build
    data pipelines that can extract and load, as well as start and finish, with both
    relational and NoSQL databases.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Inserting and extracting relational data in Python
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inserting and extracting NoSQL database data in Python
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building database pipelines in Airflow
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building database pipelines in NiFi
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inserting and extracting relational data in Python
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you hear the word **database**, you probably picture a relational database
    – that is, a database made up of tables containing columns and rows with relationships
    between the tables; for example, a purchase order system that has inventory, purchases,
    and customer information. Relational databases have been around for over 40 years
    and come from the relational data model developed by E. F. Codd in the late 1970s.
    There are several vendors of relational databases – including IBM, Oracle, and
    Microsoft – but all of these databases use a similar dialect of **SQL**, which
    stands for **Structured Query Language**. In this book, you will work with a popular
    open source database – **PostgreSQL**. In the next section, you will learn how
    to create a database and tables.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: Creating a PostgreSQL database and tables
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In [*Chapter 2*](B15739_02_ePub_AM.xhtml#_idTextAnchor027), *Building Our Data
    Engineering Infrastructure*, you created a database in PostgreSQL using pgAdmin
    4\. The database was named `dataengineering` and you created a table named `users`
    with columns for name, street, city, ZIP, and ID. The database is shown in the
    following screenshot:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – The dataengineering database'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_4.1_B15739.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.1 – The dataengineering database
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: If you have the database created, you can skip this section, but if you do not,
    this section will quickly walk you through creating one.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a database in PostgreSQL with pgAdmin 4, take the following steps:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Browse to `http://localhost/pgadmin4` and log in using the account you created
    during the installation of `pgAdmin` in [*Chapter 2*](B15739_02_ePub_AM.xhtml#_idTextAnchor027),
    *Building Our Data Engineering Infrastructure*.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Expand the server icon in the **Browser** pane. Right-click on the **MyPostgreSQL**
    icon and select **Create** | **Database**.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name the database `dataengineering`. You can leave the user as `postgres`.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Expand the `dataengineering` icon, then expand **Schemas**, then **public**,
    then **Tables**. Right-click on **Tables**, then click **Create** | **Table**.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Name the table `users`. Click the `name`: `text`'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'b) `id`: `integer`'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'c) `street`: `text`'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'd) `city`: `text`'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'e) `zip`: `text`'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now you have a database and a table created in PostgreSQL and can load data
    using Python. You will populate the table in the next section.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Inserting data into PostgreSQL
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are several libraries and ways to connect to a database in Python – `pyodbc`,
    `sqlalchemy`, `psycopg2`, and using an API and requests. In this book, we will
    use the `psycopg2` library to connect to PostgreSQL because it is built specifically
    to connect to PostgreSQL. As your skills progress, you may want to look into tools
    such as **SQLAlchemy**. SQLAlchemy is a toolkit and an object-relational mapper
    for Python. It allows you to perform queries in a more Pythonic way – without
    SQL – and to map Python classes to database tables.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Installing psycopg2
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can check whether you have `psycopg2` installed by running the following
    command:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The preceding command runs `python3` with the command flag. The flag tells Python
    to run the commands as a Python program. The quoted text imports `psycopg2` and
    then prints the version. If you receive an error, it is not installed. You should
    see a version such as 2.8.4 followed by some text in parentheses. The library
    should have been installed during the installation of Apache Airflow because you
    used all the additional libraries in [*Chapter 2*](B15739_02_ePub_AM.xhtml#_idTextAnchor027),
    *Building Our Data Engineering Infrastructure*.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: 'If it is not installed, you can add it with the following command:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Using `pip` requires that there are additional dependencies present for it
    to work. If you run into problems, you can also install a precompiled binary version
    using the following command:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: One of these two methods will get the library installed and ready for us to
    start the next section.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Connecting to PostgreSQL with Python
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To connect to your database using `psycopg2`, you will need to create a connection,
    create a cursor, execute a command, and get the results. You will take these same
    steps whether you are querying or inserting data. Let''s walk through the steps
    as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the library and reference it as `db`:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Create a connection string that contains the host, database, username, and
    password:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Create the connection object by passing the connection string to the `connect()`
    method:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Next, create the cursor from the connection:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: You are now connected to the database. From here, you can issue any SQL commands.
    In the next section, you will learn how to insert data into PostgreSQL
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Inserting data
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that you have a connection open, you can insert data using SQL. To insert
    a single person, you need to format a SQL `insert` statement, as shown:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: To see what this query will look like, you can use the `mogrify()` method.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: What is mogrify?
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: According to the `psycopg2` docs, the `mogrify` method will return a query string
    after arguments binding. The string returned is exactly the one that would be
    sent to the database running the `execute()` method or similar. In short, it returns
    the formatted query. This is helpful as you can see what you are sending to the
    database, because your SQL query can often be a source of errors.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: 'Pass your query to the `mogrify` method:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The preceding code will create a proper SQL `insert` statement; however, as
    you progress, you will add multiple records in a single statement. To do so, you
    will create a tuple of tuples. To create the same SQL statement, you can use the
    following code:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Notice that in `query2`, you did not need to add quotes around strings that
    would be passed in as you did in `query` when you used `{}`. Using the preceding
    formatting, `psycopg2` will handle the mapping of types in the query string. To
    see what the query will look like when you execute it, you can use `mogrify` and
    pass the data along with the query:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The results of `mogrify` on `query` and `query2` should be identical. Now,
    you can execute the query to add it to the database:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'If you go back to pgAdmin 4, right-click on the `insert` statement, you need
    to make it permanent by committing the transaction using the following code:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, in pgAdmin 4, you should be able to see the record, as shown in the following
    screenshot:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – Record added to the database'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_4.2_B15739.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.2 – Record added to the database
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: The record is now added to the database and visible in pgAdmin 4\. Now that
    you have entered a single record, the next section will show you how to enter
    multiple records.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Inserting multiple records
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To insert multiple records, you could loop through data and use the same code
    shown in the preceding section, but this would require a transaction per record
    in the database. A better way would be to use a single transaction and send all
    the data, letting `psycopg2` handle the bulk insert. You can accomplish this by
    using the `executemany` method. The following code will use `Faker` to create
    the records and then `executemany()` to insert them:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the needed libraries:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Create the `faker` object and an array to hold all the data. You will initialize
    a variable, `i`, to hold an ID:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, you can look, iterate, and append a fake tuple to the array you created
    in the previous step. Increment `i` for the next record. Remember that in the
    previous section, you created a record for `Big Bird` with an ID of `1`. That
    is why you will start with `2` in this example. We cannot have the same primary
    key in the database table:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Convert the array into a tuple of tuples:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now, you are back to the `psycopg` code, which will be similar to the example
    from the previous section:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'You can print out what the code will send to the database using a single record
    from the `data_for_db` variable:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Lastly, use `executemany()` instead of `execute()` to let the library handle
    the multiple inserts. Then, commit the transaction:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now, you can look at pgAdmin 4 and see the 1,000 records. You will have data
    similar to what is shown in the following screenshot:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3 – 1,000 records added to the database'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_4.3_B15739.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.3 – 1,000 records added to the database
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: Your table should now have 1,001 records. Now that you can insert data into
    PostgreSQL, the next section will show you how to query it in Python.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Extracting data from PostgreSQL
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Extracting data using `psycopgs` follows the exact same procedure as inserting,
    the only difference being that you will use a `select` statement instead of `insert`.
    The following steps show you how to extract data:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the library, then set up your connection and cursor:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now, you can execute a query. In this example, you will select all records
    from the `users` table:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, you have an iterable object with the results. You can iterate over the
    cursor, as shown:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Alternatively, you could use one of the `fetch` methods:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'To grab a single record, you can assign it to a variable and look at it. Note
    that even when you select one record, the cursor returns an array:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Regardless of whether you are fetching one or many, you need to know where
    you are and how many records there are. You can get the row count of the query
    using the following code:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'You can get the current row number using `rownumber`. If you use `fetchone()`
    and then call `rownumber` again, it should increment with your new position:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The last thing to mention is that you can also query a table and write it out
    to a CSV file using the `copy_to()` method.
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create the connection and the cursor:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Open a file to write the table to:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Then, call `copy_to` and pass the file, the table name, and the separator (which
    will default to tabs if you do not include it). Close the file, and you will have
    all the rows as a CSV:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'You can verify the results by opening the file and printing the contents:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Now that you know how to read and write to a database using the `psycopg2` library,
    you can also read and write data using DataFrames, which you will learn about
    in the next section.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Extracting data with DataFrames
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You can also query data using `pandas` DataFrames. To do so, you need to establish
    a connection using `psycopg2`, and then you can skip the cursor and go straight
    to the query. DataFrames give you a lot of power in filtering, analyzing, and
    transforming data. The following steps will walk you through using DataFrames:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: 'Set up the connection:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now, you can execute the query in a DataFrame using the `pandas` `read_sql()`
    method. The method takes a query and a connection:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The result is a DataFrame, `df`, with the full table users. You now have full
    access to all the DataFrame tools for working with the data – for example, you
    can export it to JSON using the following code:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结果是一个包含完整用户表的DataFrame，`df`。现在您可以使用所有DataFrame工具来处理数据——例如，您可以使用以下代码将其导出为JSON：
- en: '[PRE33]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Now that you know how to work with data in a relational database, it is time
    to learn about NoSQL databases. The next section will show you how to use Python
    with Elasticsearch.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经知道如何处理关系型数据库中的数据，是时候学习NoSQL数据库了。下一节将向您展示如何使用Python与Elasticsearch一起工作。
- en: Inserting and extracting NoSQL database data in Python
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Python中插入和提取NoSQL数据库数据
- en: Relational databases may be what you think of when you hear the term database,
    but there are several other types of databases, such as columnar, key-value, and
    time-series. In this section, you will learn how to work with Elasticsearch, which
    is a NoSQL database. NoSQL is a generic term referring to databases that do not
    store data in rows and columns. NoSQL databases often store their data as JSON
    documents and use a query language other than SQL. The next section will teach
    you how to load data into Elasticsearch.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 当您听到数据库这个词时，可能会想到关系型数据库，但还有几种其他类型的数据库，例如列式、键值和时间序列。在本节中，您将学习如何使用Elasticsearch，它是一个NoSQL数据库。NoSQL是一个通用术语，指的是不按行和列存储数据的数据库。NoSQL数据库通常将数据存储为JSON文档，并使用除SQL之外的其他查询语言。下一节将教您如何将数据加载到Elasticsearch中。
- en: Installing Elasticsearch
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装Elasticsearch
- en: 'To install the `elasticsearch` library, you can use `pip3`, as shown:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装`elasticsearch`库，您可以使用`pip3`，如下所示：
- en: '[PRE34]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Using `pip` will install the newest version, which, if you installed Elasticsearch
    according to the instructions in [*Chapter 2*](B15739_02_ePub_AM.xhtml#_idTextAnchor027),
    *Building Our Data Engineering Infrastructure*, is what you will need. You can
    get the library for Elasticsearch versions 2, 5, 6, and 7\. To verify the installation
    and check the version, you can use the following code:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`pip`将安装最新版本，如果您按照[*第2章*](B15739_02_ePub_AM.xhtml#_idTextAnchor027)中“构建我们的数据工程基础设施”的说明安装了Elasticsearch，那么这就是您所需要的。您可以为Elasticsearch版本2、5、6和7获取库。要验证安装并检查版本，您可以使用以下代码：
- en: '[PRE35]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The preceding code should print something like the following:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码应该打印出类似以下内容：
- en: '[PRE36]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: If you have the right version for your Elasticsearch version, you are ready
    to start importing data.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有适合您Elasticsearch版本的正确版本，您就可以开始导入数据了。
- en: Inserting data into Elasticsearch
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将数据插入Elasticsearch
- en: 'Before you can query Elasticsearch, you will need to load some data into an
    index. In the previous section, you used a library, `psycopg2`, to access PostgreSQL.
    To access Elasticsearch, you will use the `elasticsearch` library. To load data,
    you need to create the connection, then you can issue commands to Elasticsearch.
    Follow the given steps to add a record to Elasticsearch:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在您能够查询Elasticsearch之前，您需要将一些数据加载到索引中。在上一个章节中，您使用了`psycopg2`库来访问PostgreSQL。要访问Elasticsearch，您将使用`elasticsearch`库。要加载数据，您需要创建连接，然后您可以向Elasticsearch发出命令。按照以下步骤向Elasticsearch添加记录：
- en: 'Import the libraries. You can also create the `Faker` object to generate random
    data:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入库。您还可以创建`Faker`对象以生成随机数据：
- en: '[PRE37]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Create a connection to Elasticsearch:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建到Elasticsearch的连接：
- en: '[PRE38]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The preceding code assumes that your `Elasticsearch` instance is running on
    `localhost`. If it is not, you can specify the IP address, as shown:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上一段代码假设您的`Elasticsearch`实例正在`localhost`上运行。如果不是，您可以指定IP地址，如下所示：
- en: '[PRE39]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Now, you can issue commands to your `Elasticsearch` instance. The `index` method
    will allow you to add data. The method takes an index name, the document type,
    and a body. The body is what is sent to Elasticsearch and is a JSON object. The
    following code creates a JSON object to add to the database, then uses `index`
    to send it to the `users` index (which will be created automatically during the
    index operation):'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以向您的`Elasticsearch`实例发出命令。`index`方法将允许您添加数据。该方法接受索引名称、文档类型和正文。正文是发送到Elasticsearch的内容，是一个JSON对象。以下代码创建一个JSON对象以添加到数据库，然后使用`index`将其发送到`users`索引（该索引将在索引操作期间自动创建）：
- en: '[PRE40]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The preceding code should print the word `created` to the console, meaning the
    document has been added. Elasticsearch returns an object with a result key that
    will let you know whether the operation failed or succeeded. `created`, in this
    case, means the index operation succeeded and created the document in the index.
    Just as with the PostgreSQL example earlier in this chapter, you could iterate
    and run the `index` command, or you can use a bulk operation to let the library
    handle all the inserts for you.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Inserting data using helpers
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Using the `bulk` method, you can insert many documents at a time. The process
    is similar to inserting a single record, except that you will generate all the
    data, then insert it. The steps are as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: 'You need to import the `helpers` library to access the `bulk` method:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The data needs to be an array of JSON objects. In the previous example, you
    created a JSON object with attributes. In this example, the object needs to have
    some additional information. You must specify the index and the type. Underscores
    in the names are used for Elasticsearch fields. The `_source` field is where you
    would put the JSON document you want to insert in the database. Outside the JSON
    is a `for` loop. This loop creates the 999 (you already added one and you index
    from 0 – to 998) documents:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Now, you can call the `bulk` method and pass it the `elasticsearch` instance
    and the array of data. You can print the results to check that it worked:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'You should now have 1,000 records in an Elasticsearch index named `users`.
    We can verify this in Kibana. To add the new index to Kibana, browse to your Kibana
    dashboard at `http://localhost:5601`. Selecting **Management** at the bottom left
    of the toolbar, you can then create an index pattern by clicking the blue **+
    Create index pattern** button, as shown in the following screenshot:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – Creating an index pattern'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_4.4_B15739.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.4 – Creating an index pattern
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 'Add an Elasticsearch index pattern to Kibana. On the next screen, enter the
    name of the index – `users`. Kibana will start pattern matching to find the index.
    Select the `users` index from the dropdown and click the `users`), as shown in
    the following screenshot; you should see your documents:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5 – All of your documents in the Discover tab'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_4.5_B15739.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.5 – All of your documents in the Discover tab
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Now that you can create a record individually or using the `bulk` method, the
    next section will teach you how you can query your data.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Querying Elasticsearch
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Querying Elasticsearch follows the exact same steps as inserting data. The
    only difference is you use a different method – `search` – to send a different
    body object. Let''s walk through a simple query on all the data:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the library and create your `elasticsearch` instance:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Create the JSON object to send to Elasticsearch. The object is a query, using
    the `match_all` search:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Pass the object to Elasticsearch using the `search` method. Pass the index
    and the return size. In this case, you will only return 10 records. The maximum
    return size is 10,000 documents:'
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`search`方法将对象传递给Elasticsearch。传递索引和返回大小。在这种情况下，你将只返回10条记录。最大返回大小是10,000个文档：
- en: '[PRE46]'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Lastly, you can print the documents:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，你可以打印文档：
- en: '[PRE47]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Or you can iterate through grabbing `_source` only:'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 或者，你可以迭代地只获取`_source`：
- en: '[PRE48]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'You can load the results of the query into a `pandas` DataFrame – it is JSON,
    and you learned how to read JSON in [*Chapter 3*](B15739_03_ePub_AM.xhtml#_idTextAnchor039),
    *Reading and Writing Files*. To load the results into a DataFrame, import `json_normalize`
    from the `pandas` `json` library, and use it (`json_normalize`) on the JSON results,
    as shown in the following code:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将查询结果加载到`pandas` DataFrame中——它是JSON格式，你已经在[*第3章*](B15739_03_ePub_AM.xhtml#_idTextAnchor039)中学习了如何读取JSON，即*读取和写入文件*。要将结果加载到DataFrame中，从`pandas`的`json`库中导入`json_normalize`，并使用它（`json_normalize`）对JSON结果进行操作，如下面的代码所示：
- en: '[PRE49]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Now you will have the results of the search in a DataFrame. In this example,
    you just grabbed all the records, but there are other queries available besides
    `match_all`.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你将在DataFrame中看到搜索结果。在这个例子中，你只是抓取了所有记录，但除了`match_all`之外，还有其他查询可用。
- en: 'Using the `match_all` query, I know I have a document with the name `Ronald
    Goodman`. You can query on a field using the `match` query:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`match_all`查询，我知道有一个名为`Ronald Goodman`的文档。你可以使用`match`查询在字段上进行查询：
- en: '[PRE50]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'You can also use a Lucene syntax for queries. In Lucene, you can specify `field:value`.
    When performing this kind of search, you do not need a document to send. You can
    pass the `q` parameter to the `search` method:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用Lucene语法进行查询。在Lucene中，你可以指定`field:value`。在执行此类搜索时，你不需要发送文档。你可以将`q`参数传递给`search`方法：
- en: '[PRE51]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Using the `City` field, you can search for `Jamesberg`. It will return two
    records: one for `Jamesberg` and one for `Lake Jamesberg`. Elasticsearch will
    tokenize strings with spaces in them, splitting them into multiple strings to
    search:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`City`字段，你可以搜索`Jamesberg`。它将返回两个记录：一个是`Jamesberg`，另一个是`Lake Jamesberg`。Elasticsearch会将包含空格的字符串进行分词，将其拆分为多个字符串进行搜索：
- en: '[PRE52]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The results are the two records in the following code block:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块中的结果是两个记录：
- en: '[PRE53]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'You can use Boolean queries to specify multiple search criteria. For example,
    you can use `must`, `must not`, and `should` before your queries. Using a Boolean
    query, you can filter out `Lake Jamesberg`. Using a `must` match on `Jamesberg`
    as the city (which will return two records), and adding a filter on the ZIP, you
    can make sure only `Jamesberg` with the ZIP `63792` is returned. You could also
    use a `must not` query on the `Lake Jameson` ZIP:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用布尔查询来指定多个搜索条件。例如，你可以在查询之前使用`must`、`must not`和`should`。使用布尔查询，你可以过滤掉`Lake
    Jamesberg`。使用`must`匹配城市为`Jamesberg`（这将返回两个记录），并添加ZIP的过滤器，你可以确保只返回ZIP为`63792`的`Jamesberg`。你还可以在`Lake
    Jameson` ZIP上使用`must not`查询：
- en: '[PRE54]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Now, you only get the single record that you wanted:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你只得到了你想要的单个记录：
- en: '[PRE55]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Your queries only returned a few documents, but in production, you will probably
    have large queries with tens of thousands of documents being returned. The next
    section will show you how to handle all that data.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 你的查询只返回了少量文档，但在实际生产中，你可能会遇到返回成千上万文档的大查询。下一节将展示如何处理所有这些数据。
- en: Using scroll to handle larger results
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用滚动处理更大的结果
- en: 'In the first example, you used a size of 10 for your search. You could have
    grabbed all 1,000 records, but what do you do when you have more than 10,000 and
    you need all of them? Elasticsearch has a scroll method that will allow you to
    iterate over the results until you get them all. To scroll through the data, follow
    the given steps:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个例子中，你为搜索使用了10的大小。你可以抓取所有1,000条记录，但当你有超过10,000条记录并且需要所有这些记录时，你该怎么办？Elasticsearch有一个滚动方法，允许你迭代结果直到获取所有结果。要滚动数据，请遵循以下步骤：
- en: 'Import the library and create your `Elasticsearch` instance:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入库并创建你的`Elasticsearch`实例：
- en: '[PRE56]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Search your data. Since you do not have over 10,000 records, you will set the
    size to `500`. This means you will be missing 500 records from your initial search.
    You will pass a new parameter to the search method – `scroll`. This parameter
    specifies how long you want to make the results available for. I am using 20 milliseconds.
    Adjust this number to make sure you have enough time to get the data – it will
    depend on the document size and network speed:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 搜索您的数据。由于您没有超过10,000条记录，您将大小设置为`500`。这意味着您将错过初始搜索中的500条记录。您将向搜索方法传递一个新参数 – `scroll`。此参数指定您希望结果可用多长时间。我使用20毫秒。调整此数字以确保您有足够的时间获取数据
    – 它将取决于文档大小和网络速度：
- en: '[PRE57]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The results will include `_scroll_id`, which you will need to pass to the `scroll`
    method later. Save the scroll ID and the size of the result set:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结果将包括`_scroll_id`，您稍后需要将其传递给`scroll`方法。保存滚动ID和结果集的大小：
- en: '[PRE58]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'To start scrolling, use a `while` loop to get records until the size is 0,
    meaning there is no more data. Inside the loop, you will call the `scroll` method
    and pass `_scroll_id` and how long to scroll. This will grab more of the results
    from the original query:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要开始滚动，使用`while`循环获取记录，直到大小为0，这意味着没有更多数据。在循环内部，您将调用`scroll`方法并传递`_scroll_id`和滚动时间。这将从原始查询中获取更多结果：
- en: '[PRE59]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Next, get the new scroll ID and the size so that you can loop through again
    if the data still exists:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，获取新的滚动ID和大小，以便在数据仍然存在时再次循环：
- en: '[PRE60]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Lastly, you can do something with the results of the scrolls. In the following
    code, you will print the source for every record:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，您可以对滚动结果进行一些操作。在以下代码中，您将打印每条记录的源：
- en: '[PRE61]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Now you know how to create documents in Elasticsearch and how to query them,
    even when there is more than the maximum return value of 10,000\. You can do the
    same using relational databases. It is now time to start putting these skills
    to use in building data pipelines. The next two sections will teach you how to
    use databases in your data pipelines using Apache Airflow and NiFi.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经知道如何在Elasticsearch中创建文档以及如何查询它们，即使返回值超过10,000个最大值。您也可以使用关系数据库做同样的事情。现在是时候开始将这些技能用于构建数据管道了。接下来的两个部分将教会您如何使用Apache
    Airflow和NiFi在数据管道中使用数据库。
- en: Building data pipelines in Apache Airflow
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Apache Airflow中构建数据管道
- en: 'In the previous chapter, you built your first Airflow data pipeline using a
    Bash and Python operator. This time, you will combine two Python operators to
    extract data from PostgreSQL, save it as a CSV file, then read it in and write
    it to an Elasticsearch index. The complete pipeline is shown in the following
    screenshot:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，您使用Bash和Python操作符构建了您的第一个Airflow数据管道。这次，您将结合两个Python操作符从PostgreSQL中提取数据，将其保存为CSV文件，然后读取并写入Elasticsearch索引。完整的管道在以下屏幕截图中显示：
- en: '![Figure 4.6 – Airflow DAG'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 4.6 – Airflow DAG'
- en: '](img/Figure_4.6_B15739.jpg)'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/Figure_4.6_B15739.jpg](img/Figure_4.6_B15739.jpg)'
- en: Figure 4.6 – Airflow DAG
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6 – Airflow DAG
- en: The preceding **Directed Acyclic Graph** (**DAG**) looks very simple; it is
    only two tasks, and you could combine the tasks into a single function. This is
    not a good idea. In *Section 2*, *Deploying Pipelines into Production*, you will
    learn about modifying your data pipelines for production. A key tenant of production
    pipelines is that each task should be atomic; that is, each task should be able
    to stand on its own. If you had a single function that read a database and inserted
    the results, when it fails, you have to track down whether the query failed or
    the insert failed. As your tasks get more complicated, it will take much more
    work to debug. The next section will walk you through building the data pipeline.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的**有向无环图**（**DAG**）看起来非常简单；它只有两个任务，您可以将任务合并成一个函数。这不是一个好主意。在*第2节*，*将管道部署到生产中*，您将学习如何修改您的数据管道以适应生产。生产管道的一个关键原则是每个任务应该是原子的；也就是说，每个任务应该能够独立存在。如果您有一个读取数据库并插入结果的单一函数，当它失败时，您必须追踪查询是否失败或插入是否失败。随着您的任务变得更加复杂，调试将需要更多的工作。下一节将引导您构建数据管道。
- en: Setting up the Airflow boilerplate
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置Airflow模板
- en: 'Every DAG is going to have some standard, boilerplate code to make it run in
    Airflow. You will always import the needed libraries, and then any other libraries
    you need for your tasks. In the following code block, you import the operator,
    `DAG`, and the time libraries for Airflow. For your tasks, you import the `pandas`,
    `psycopg2`, and `elasticsearch` libraries:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 DAG 都将有一些标准的、模板化的代码，以便在 Airflow 中运行。你将始终导入所需的库，然后导入你任务所需的任何其他库。在下面的代码块中，你导入了操作符
    `DAG` 和 Airflow 的时间库。对于你的任务，你导入了 `pandas`、`psycopg2` 和 `elasticsearch` 库：
- en: '[PRE62]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Next, you will specify the arguments for your DAG. Remember that the start
    time should be a day behind if you schedule the task to run daily:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你将为你的 DAG 指定参数。记住，如果安排任务每天运行，开始时间应该比任务运行时间晚一天：
- en: '[PRE63]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Now, you can pass the arguments to the DAG, name it, and set the run interval.
    You will define your operators here as well. In this example, you will create
    two Python operators – one to get data from PostgreSQL and one to insert data
    in to Elasticsearch. The `getData` task will be upstream and the `insertData`
    task downstream, so you will use the `>>` bit shift operator to specify this:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以将参数传递给 DAG，给它命名，并设置运行间隔。你也将在这里定义你的操作符。在这个例子中，你将创建两个 Python 操作符——一个用于从
    PostgreSQL 获取数据，另一个用于将数据插入 Elasticsearch。`getData` 任务将位于上游，而 `insertData` 任务位于下游，因此你将使用
    `>>` 位移操作符来指定这一点：
- en: '[PRE64]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Lastly, you will define the tasks. In the preceding operators, you named them
    `queryPostgresql` and `insertElasticsearch`. The code in these tasks should look
    very familiar; it is almost identical to the code from the previous sections in
    this chapter.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你将定义任务。在前面的操作符中，你将它们命名为 `queryPostgresql` 和 `insertElasticsearch`。这些任务中的代码看起来非常熟悉；它与本章前几节中的代码几乎相同。
- en: 'To query PostgreSQL, you create the connection, execute the `sql` query using
    the `pandas` `read_sql()` method, and then use the `pandas` `to_csv()` method
    to write the data to disk:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 要查询 PostgreSQL，你将创建连接，使用 `pandas` 的 `read_sql()` 方法执行 `sql` 查询，然后使用 `pandas`
    的 `to_csv()` 方法将数据写入磁盘：
- en: '[PRE65]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'To insert the data into Elasticsearch, you create the Elasticsearch object
    connecting to `localhost`. Then, read the CSV from the previous task into a DataFrame,
    iterate through the DataFrame, converting each row into JSON, and insert the data
    using the `index` method:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 要将数据插入 Elasticsearch，你将创建连接到 `localhost` 的 Elasticsearch 对象。然后，从上一个任务中读取 CSV
    到 DataFrame，遍历 DataFrame，将每一行转换为 JSON，并使用 `index` 方法插入数据：
- en: '[PRE66]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Now you have a complete data pipeline in Airflow. In the next section, you will
    run it and view the results.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经在 Airflow 中拥有了一个完整的数据管道。在下一节中，你将运行它并查看结果。
- en: Running the DAG
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行 DAG
- en: 'To run the DAG, you need to copy your code to your `$AIRFLOW_HOME/dags` folder.
    After moving the file, you can run the following commands:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行 DAG，你需要将你的代码复制到 `$AIRFLOW_HOME/dags` 文件夹。移动文件后，你可以运行以下命令：
- en: '[PRE67]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'When these commands complete, browse to `http://localhost:8080` to see the
    Airflow GUI. Select **MyDBdag**, and then select **Tree View**. You can schedule
    five runs of the DAG and click **Go**. As it runs, you should see the results
    underneath, as shown in the following screenshot:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 当这些命令完成后，浏览到 `http://localhost:8080` 以查看 Airflow GUI。选择 **MyDBdag**，然后选择 **树视图**。你可以安排
    DAG 运行五次，并点击 **Go**。运行时，你应该在下面看到结果，如下面的截图所示：
- en: '![Figure 4.7 – Task showing successful runs and queued runs'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.7 – 显示成功运行和排队运行的任务]'
- en: '](img/Figure_4.7_B15739.jpg)'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.7 – 任务显示成功运行和排队运行](img/Figure_4.7_B15739.jpg)'
- en: Figure 4.7 – Task showing successful runs and queued runs
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7 – 显示成功运行和排队运行的任务
- en: 'To verify that the data pipeline was successful, you can view the data in Elasticsearch
    using Kibana. To see the results, browse to Kibana at `http://localhost:5601`.
    You will need to create a new index in Kibana. You performed this task in the
    *Inserting data using helpers* section of this chapter. But to recap, you will
    select **Management** in Kibana from the bottom of the left-hand toolbar in Kibana,
    then create the index pattern by clicking the **Create index pattern** button.
    Start typing the name of the index and Kibana will find it, then click **Create**.
    Then, you can go to the **Discover** tab on the toolbar and view the data. You
    should see records as shown in the following screenshot:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 要验证数据管道是否成功，你可以使用 Kibana 查看 Elasticsearch 中的数据。要查看结果，浏览到 Kibana 的 `http://localhost:5601`。你需要在
    Kibana 中创建一个新的索引。你在这个章节的*使用辅助工具插入数据*部分执行了这个任务。但为了回顾，你将在 Kibana 的左侧工具栏底部选择**管理**，然后通过点击**创建索引模式**按钮创建索引模式。开始输入索引的名称，Kibana
    将找到它，然后点击**创建**。然后，你可以转到工具栏上的**发现**选项卡并查看数据。你应该看到如下所示的记录：
- en: '![Figure 4.8 – Airflow data pipeline results showing records in Elasticsearch'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.8 – 显示 Elasticsearch 中记录的 Airflow 数据管道结果'
- en: '](img/Figure_4.8_B15739.jpg)'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.8_B15739.jpg)'
- en: Figure 4.8 – Airflow data pipeline results showing records in Elasticsearch
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.8 – 显示 Elasticsearch 中记录的 Airflow 数据管道结果
- en: You will see that there are documents containing only names and cities, as specified
    in your data pipeline task. One thing to note is that we now have over 2,000 records.
    There were only 1,000 records in the PostgreSQL database, so what happened? The
    data pipeline ran multiple times, and each time, it inserted the records from
    PostgreSQL. A second tenant of data pipelines is that they should be idempotent.
    That means that no matter how many times you run it, the results are the same.
    In this case, they are not. You will learn how to fix this in *Section 2*, *Deploying
    Pipelines into Production*, in [*Chapter 7*](B15739_07_ePub_AM.xhtml#_idTextAnchor086),
    *Features of a Production Pipeline*. For now, the next section of this chapter
    will teach you how to build the same data pipeline in Apache NiFi.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 你将看到包含仅名称和城市的文档，正如你在数据管道任务中指定的那样。需要注意的是，我们现在有超过 2,000 条记录。在 PostgreSQL 数据库中之前只有
    1,000 条记录，那么发生了什么？数据管道运行了多次，每次都从 PostgreSQL 插入记录。数据管道的第二个原则是它们应该是幂等的。这意味着无论你运行多少次，结果都应该是相同的。在这种情况下，它们并不是。你将在*第
    2 节*，*在生产中部署管道*，在[*第 7 章*](B15739_07_ePub_AM.xhtml#_idTextAnchor086)，*生产管道的特点*中学习如何解决这个问题。现在，本章的下一节将教你如何在
    Apache NiFi 中构建相同的数据管道。
- en: Handling databases with NiFi processors
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 NiFi 处理器处理数据库
- en: 'In the previous sections, you learned how to read and write CSV and JSON files
    using Python. Reading files is such a common task that tools such as NiFi have
    prebuilt processors to handle it. In this section, you will build the same data
    pipeline as in the previous section. In NiFi, the data pipeline will look as shown
    in the following screenshot:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，你学习了如何使用 Python 读取和写入 CSV 和 JSON 文件。读取文件是一项如此常见的任务，以至于像 NiFi 这样的工具有预构建的处理器来处理它。在本节中，你将构建与上一节相同的数据管道。在
    NiFi 中，数据管道将看起来如下所示：
- en: '![Figure 4.9 – A NiFi data pipeline to move data from PostgreSQL to Elasticsearch'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.9 – 将数据从 PostgreSQL 移动到 Elasticsearch 的 NiFi 数据管道'
- en: '](img/Figure_4.9_B15739.jpg)'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.9_B15739.jpg)'
- en: Figure 4.9 – A NiFi data pipeline to move data from PostgreSQL to Elasticsearch
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.9 – 将数据从 PostgreSQL 移动到 Elasticsearch 的 NiFi 数据管道
- en: The data pipeline contains one more task than the Airflow version, but otherwise,
    it should look straightforward. The following sections will walk you through building
    the data pipeline.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 数据管道比 Airflow 版本多一个任务，但除此之外，它应该看起来很直接。以下几节将指导你构建数据管道。
- en: Extracting data from PostgreSQL
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从 PostgreSQL 提取数据
- en: The processor most used for handling relational databases in NiFi is the `ExecuteSQLRecord`
    processor. Drag the `ExecuteSQLRecord` processor. Once it has been added to the
    canvas, you need to configure it.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在 NiFi 中处理关系数据库最常用的处理器是 `ExecuteSQLRecord` 处理器。拖动 `ExecuteSQLRecord` 处理器。一旦它被添加到画布上，你需要配置它。
- en: Configuring the ExecuteSQLCommand processor
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置 ExecuteSQLCommand 处理器
- en: 'To configure the processor, you need to create a database connection pool,
    as shown in the following screenshot:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 要配置处理器，你需要创建一个数据库连接池，如下面的截图所示：
- en: '![Figure 4.10 – Creating a database connection pooling service'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.10 – 创建数据库连接池服务'
- en: '](img/Figure_4.10_B15739.jpg)'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.10_B15739.jpg)'
- en: Figure 4.10 – Creating a database connection pooling service
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.10 – 创建数据库连接池服务
- en: After selecting `dataengineering`. Notice how I did not name it `PostgreSQL`.
    As you add more services, you will add more PostgreSQL connections for different
    databases. It would then be hard to remember which PostgreSQL database the service
    was for.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择 `dataengineering` 后，注意我没有将其命名为 `PostgreSQL`。随着你添加更多服务，你将为不同的数据库添加更多 PostgreSQL
    连接。那时，记住哪个 PostgreSQL 数据库对应于哪个服务将变得很困难。
- en: 'To configure the service, select the arrow in the processor configuration.
    The configuration for the service should look as in the following screenshot:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 要配置服务，选择处理器配置中的箭头。服务的配置应如下截图所示：
- en: '![Figure 4.11 – Configuring the database service'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.11 – 配置数据库服务'
- en: '](img/Figure_4.11_B15739.jpg)'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.11_B15739.jpg)'
- en: Figure 4.11 – Configuring the database service
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.11 – 配置数据库服务
- en: The configuration requires you to specify the connection URL, which is a Java
    database connection string. The string specifies `PostgreSQL`. It then names the
    host, `localhost`, and the database name, `dataengineering`. The driver class
    specifies the `postgresql` driver. The location of the driver is where you downloaded
    it in [*Chapter 2*](B15739_02_ePub_AM.xhtml#_idTextAnchor027), *Building Our Data
    Engineering Infrastructure*. It should be in your home directory in the `nifi`
    folder, in a subdirectory named `drivers`. Lastly, you need to enter the username
    and password for the database.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 配置需要你指定连接 URL，这是一个 Java 数据库连接字符串。该字符串指定 `PostgreSQL`。然后命名主机，`localhost`，和数据库名称，`dataengineering`。驱动类指定
    `postgresql` 驱动。驱动程序的位置是你下载它的位置，在 [*第 2 章*](B15739_02_ePub_AM.xhtml#_idTextAnchor027)，*构建我们的数据工程基础设施*。它应该位于你的主目录中的
    `nifi` 文件夹中，在名为 `drivers` 的子目录中。最后，你需要输入数据库的用户名和密码。
- en: 'Next, you need to create a record writer service. Select **Create new service…**,
    choose **JSONRecordSetWriter**, and click the arrow to configure it. There is
    one important configuration setting that you cannot skip – **Output Grouping**.
    You must set this property to **One Line Per Object**. The finished configuration
    will look as in the following screenshot:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你需要创建一个记录写入服务。选择 **创建新服务…**，选择 **JSONRecordSetWriter**，然后点击箭头进行配置。有一个重要的配置设置你不能跳过
    – **输出分组**。你必须将此属性设置为 **每对象一行**。完成的配置将如下截图所示：
- en: '![Figure 4.13 – The JSONRecordSetWriter configuration'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.13 – JSONRecordSetWriter 配置'
- en: '](img/Figure_4.12_B15739.jpg)'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_4.12_B15739.jpg)'
- en: Figure 4.13 – The JSONRecordSetWriter configuration
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.13 – JSONRecordSetWriter 配置
- en: 'Now that you have set up the services for the processor, you need to finish
    configuring the process. The last parameter you need to configure is **SQL Select
    Query**. This is where you can specify the SQL command to run against the database.
    For example, you can enter the following:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经为处理器设置了服务，你需要完成配置过程。你需要配置的最后参数是 **SQL 选择查询**。这是你可以指定运行数据库的 SQL 命令的地方。例如，你可以输入以下内容：
- en: '[PRE68]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: This will grab all the records in the PostgreSQL database, but only the `name`
    and `city` fields. You can now move on to the next processor in the pipeline.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 这将抓取 PostgreSQL 数据库中的所有记录，但只包括 `name` 和 `city` 字段。你现在可以继续到管道中的下一个处理器。
- en: Configuring the SplitText processor
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置 SplitText 处理器
- en: Now that you have configured the **ExecuteSQLRecord** processor, you will receive
    an array of records. To process this data, you need to have a flowfile per record.
    To do that, you can use the **SplitText** processor. Drag it to the canvas and
    open the **Properties** tab by double-clicking on the processor – or right-click
    and select **Properties**. The processor defaults work, but make sure that **Line
    Split Count** is set to **1**, **Header Line Count** is **0** – your data does
    not have a header when it comes from the **ExecuteSQLRecord** processor – and
    **Remove Trailing Newlines** is **true**.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经配置了 **ExecuteSQLRecord** 处理器，你将收到一个记录数组。要处理这些数据，你需要为每条记录有一个 flowfile。为此，你可以使用
    **SplitText** 处理器。将其拖到画布上，通过双击处理器打开 **属性** 选项卡 – 或者右键单击并选择 **属性**。处理器的默认设置有效，但请确保
    **行分割计数** 设置为 **1**，**标题行计数** 为 **0** – 当数据来自 **ExecuteSQLRecord** 处理器时，你的数据没有标题，并且
    **删除尾随换行符** 为 **true**。
- en: These settings will allow the processor to take each line of the flowfile and
    split it into a new flowfile. So, your one incoming flowfile will come out of
    this processor as 1,000 flow files.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 这些设置将允许处理器将 flowfile 的每一行分割成一个新的 flowfile。因此，你的一个输入 flowfile 将从这个处理器输出 1,000
    个 flowfile。
- en: Configuring the PutElasticsearchHttp processor
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置 PutElasticsearchHttp 处理器
- en: The last step in the data pipeline is to insert the flowfiles into Elasticsearch.
    You can do that using the **PutElasticsearchHttp** processor. There are four different
    **PutElasticsearch** processors. Only two will be relevant in this book – **PutElasticsearchHttp**
    and **PutelasticsearchHttpRecord**. These are the processors to insert a single
    record or to use the bulk API. The other two processors – **Putelasticsearch**
    and **Putelasticsearch5** – are for older versions of Elasticsearch (2 and 5).
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 数据管道的最后一步是将 flowfiles 插入 Elasticsearch。您可以使用 **PutElasticsearchHttp** 处理器来完成此操作。有四个不同的
    **PutElasticsearch** 处理器。在这本书中，只有两个是相关的 – **PutElasticsearchHttp** 和 **PutelasticsearchHttpRecord**。这些处理器用于插入单个记录或使用批量
    API。其他两个处理器 – **Putelasticsearch** 和 **Putelasticsearch5** – 用于 Elasticsearch
    的旧版本（2 和 5）。
- en: To configure the processor, you must specify the URL and the port. In this example,
    you will use `http://localhost:9200`. The index will be `fromnifi`, but you can
    name it anything you would like. The type is `doc` and the index operation will
    be `index`. In *Section 2*, *Deploying Pipelines into Production*, you will use
    other index operations and you will specify the ID of the records that you insert.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 要配置处理器，您必须指定 URL 和端口。在这个例子中，您将使用 `http://localhost:9200`。索引将是 `fromnifi`，但您可以将其命名为任何您想要的名称。类型是
    `doc`，索引操作将是 `index`。在 *第 2 节*，*在生产中部署管道* 中，您将使用其他索引操作，并且您将指定您插入的记录的 ID。
- en: Running the data pipeline
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行数据管道
- en: Now that you have configured all of the processors, you can connect them by
    dragging the arrow from **ExecuteSQLRecord** to the **SplitRecord** processor
    for success. Then, connect the **SplitRecord** processor to the **PutElasticsearchHttp**
    processor for splits. Lastly, terminate the **PutElasticsearchHttp** processor
    for all relationships.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经配置了所有处理器，您可以通过将箭头从 **ExecuteSQLRecord** 拖动到 **SplitRecord** 处理器来连接它们以实现成功。然后，将
    **SplitRecord** 处理器连接到 **PutElasticsearchHttp** 处理器以进行拆分。最后，终止 **PutElasticsearchHttp**
    处理器以完成所有关系。
- en: Run each of the processors, or in the **Operations** pane, select **Run** to
    start them all. You will see one flowfile in the first queue, then it will split
    into 1,000 flowfiles in the second queue. The queue will empty in batches of 100
    as they are inserted into Elasticsearch.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 运行每个处理器，或在 **操作** 面板中，选择 **运行** 以启动它们。您将看到第一个队列中的一个 flowfile，然后它将在第二个队列中拆分为
    1,000 个 flowfiles。随着它们被插入 Elasticsearch，队列将以每批 100 个的方式清空。
- en: 'To verify the results, you can use the `elasticsearch` API, and not Kibana.
    In your browser, go to `http://localhost:9200/_cat/indices`. This is the REST
    endpoint to view the indices in your Elasticsearch database. You should see your
    new index, `fromnifi`, and the total number of documents, as shown in the following
    screenshot:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证结果，您可以使用 `elasticsearch` API，而不是 Kibana。在您的浏览器中，访问 `http://localhost:9200/_cat/indices`。这是查看您
    Elasticsearch 数据库中索引的 REST 端点。您应该看到您的新索引 `fromnifi` 和文档总数，如下面的截图所示：
- en: '![Figure 4.13 – The index contains all the records from PostgreSQL'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.13 – 索引包含 PostgreSQL 的所有记录'
- en: '](img/Figure_4.13_B15739.jpg)'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/Figure_4.13_B15739.jpg](img/Figure_4.13_B15739.jpg)'
- en: Figure 4.13 – The index contains all the records from PostgreSQL
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.13 – 索引包含 PostgreSQL 的所有记录
- en: The number of documents in the index will vary depending on whether you left
    the pipeline running or not. Just as in the Airflow example, this pipeline is
    not idempotent. As it runs, it will keep adding the same records with a different
    ID into Elasticsearch. This is not the behavior you will want in production and
    we will fix this in *Section 2*, *Deploying Pipelines into Production*.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 索引中的文档数量将根据您是否让管道运行而变化。就像在 Airflow 示例中一样，这个管道不是幂等的。在运行过程中，它将不断将具有不同 ID 的相同记录添加到
    Elasticsearch 中。这不是您在生产中想要的操作，我们将在 *第 2 节*，*在生产中部署管道* 中修复这个问题。
- en: Summary
  id: totrans-277
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned how to use Python to query and insert data into
    both relational and NoSQL databases. You also learned how to use both Airflow
    and NiFi to create data pipelines. Database skills are some of the most important
    for a data engineer. There will be very few data pipelines that do not touch on
    them in some way. The skills you learned in this chapter provide the foundation
    for the other skills you will need to learn – primarily SQL. Combining strong
    SQL skills with the data pipeline skills you learned in this chapter will allow
    you to accomplish most of the data engineering tasks you will encounter.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了如何使用Python查询和将数据插入到关系型数据库和NoSQL数据库中。你还学习了如何使用Airflow和NiFi创建数据管道。数据库技能对于数据工程师来说是最重要的技能之一。几乎所有的数据管道都会以某种方式涉及到它们。本章中你学到的技能为你将需要学习的其他技能提供了基础——主要是SQL。将强大的SQL技能与本章中学到的数据管道技能相结合，将使你能够完成你将遇到的大多数数据工程任务。
- en: In the examples, the data pipelines were not idempotent. Every time they ran,
    you got new results, and results you did not want. We will fix that in *Section
    2*, *Deploying Pipelines into Production*. But before you get to that, you will
    need to learn how to handle common data issues, and how to enrich and transform
    your data.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在示例中，数据管道不具有幂等性。每次运行时，你都会得到新的结果，以及你不想得到的结果。我们将在*第2节*，*将管道部署到生产中*中解决这个问题。但在你到达那里之前，你需要学习如何处理常见的数据问题，以及如何丰富和转换你的数据。
- en: The next chapter will teach you how to use Python to work with your data in
    between the extract and load phases of your data pipelines.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将教你如何使用Python在数据管道的提取和加载阶段之间处理你的数据。
