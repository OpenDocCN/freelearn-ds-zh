- en: '*Chapter 4*: Working with Databases'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, you learned how to read and write text files. Reading
    log files or other text files from a data lake and moving them into a database
    or data warehouse is a common task for data engineers. In this chapter, you will
    use the skills you gained working with text files and learn how to move that data
    into a database. This chapter will also teach you how to extract data from relational
    and NoSQL databases. By the end of this chapter, you will have the skills needed
    to work with databases using Python, NiFi, and Airflow. It is more than likely
    that most of your data pipelines will end with a database and very likely that
    they will start with one as well. With these skills, you will be able to build
    data pipelines that can extract and load, as well as start and finish, with both
    relational and NoSQL databases.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Inserting and extracting relational data in Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inserting and extracting NoSQL database data in Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building database pipelines in Airflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building database pipelines in NiFi
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inserting and extracting relational data in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you hear the word **database**, you probably picture a relational database
    – that is, a database made up of tables containing columns and rows with relationships
    between the tables; for example, a purchase order system that has inventory, purchases,
    and customer information. Relational databases have been around for over 40 years
    and come from the relational data model developed by E. F. Codd in the late 1970s.
    There are several vendors of relational databases – including IBM, Oracle, and
    Microsoft – but all of these databases use a similar dialect of **SQL**, which
    stands for **Structured Query Language**. In this book, you will work with a popular
    open source database – **PostgreSQL**. In the next section, you will learn how
    to create a database and tables.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a PostgreSQL database and tables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In [*Chapter 2*](B15739_02_ePub_AM.xhtml#_idTextAnchor027), *Building Our Data
    Engineering Infrastructure*, you created a database in PostgreSQL using pgAdmin
    4\. The database was named `dataengineering` and you created a table named `users`
    with columns for name, street, city, ZIP, and ID. The database is shown in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – The dataengineering database'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_4.1_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.1 – The dataengineering database
  prefs: []
  type: TYPE_NORMAL
- en: If you have the database created, you can skip this section, but if you do not,
    this section will quickly walk you through creating one.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a database in PostgreSQL with pgAdmin 4, take the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Browse to `http://localhost/pgadmin4` and log in using the account you created
    during the installation of `pgAdmin` in [*Chapter 2*](B15739_02_ePub_AM.xhtml#_idTextAnchor027),
    *Building Our Data Engineering Infrastructure*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Expand the server icon in the **Browser** pane. Right-click on the **MyPostgreSQL**
    icon and select **Create** | **Database**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name the database `dataengineering`. You can leave the user as `postgres`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Expand the `dataengineering` icon, then expand **Schemas**, then **public**,
    then **Tables**. Right-click on **Tables**, then click **Create** | **Table**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Name the table `users`. Click the `name`: `text`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'b) `id`: `integer`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'c) `street`: `text`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'd) `city`: `text`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'e) `zip`: `text`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now you have a database and a table created in PostgreSQL and can load data
    using Python. You will populate the table in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Inserting data into PostgreSQL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are several libraries and ways to connect to a database in Python – `pyodbc`,
    `sqlalchemy`, `psycopg2`, and using an API and requests. In this book, we will
    use the `psycopg2` library to connect to PostgreSQL because it is built specifically
    to connect to PostgreSQL. As your skills progress, you may want to look into tools
    such as **SQLAlchemy**. SQLAlchemy is a toolkit and an object-relational mapper
    for Python. It allows you to perform queries in a more Pythonic way – without
    SQL – and to map Python classes to database tables.
  prefs: []
  type: TYPE_NORMAL
- en: Installing psycopg2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can check whether you have `psycopg2` installed by running the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command runs `python3` with the command flag. The flag tells Python
    to run the commands as a Python program. The quoted text imports `psycopg2` and
    then prints the version. If you receive an error, it is not installed. You should
    see a version such as 2.8.4 followed by some text in parentheses. The library
    should have been installed during the installation of Apache Airflow because you
    used all the additional libraries in [*Chapter 2*](B15739_02_ePub_AM.xhtml#_idTextAnchor027),
    *Building Our Data Engineering Infrastructure*.
  prefs: []
  type: TYPE_NORMAL
- en: 'If it is not installed, you can add it with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Using `pip` requires that there are additional dependencies present for it
    to work. If you run into problems, you can also install a precompiled binary version
    using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: One of these two methods will get the library installed and ready for us to
    start the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Connecting to PostgreSQL with Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To connect to your database using `psycopg2`, you will need to create a connection,
    create a cursor, execute a command, and get the results. You will take these same
    steps whether you are querying or inserting data. Let''s walk through the steps
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the library and reference it as `db`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a connection string that contains the host, database, username, and
    password:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the connection object by passing the connection string to the `connect()`
    method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, create the cursor from the connection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You are now connected to the database. From here, you can issue any SQL commands.
    In the next section, you will learn how to insert data into PostgreSQL
  prefs: []
  type: TYPE_NORMAL
- en: Inserting data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that you have a connection open, you can insert data using SQL. To insert
    a single person, you need to format a SQL `insert` statement, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: To see what this query will look like, you can use the `mogrify()` method.
  prefs: []
  type: TYPE_NORMAL
- en: What is mogrify?
  prefs: []
  type: TYPE_NORMAL
- en: According to the `psycopg2` docs, the `mogrify` method will return a query string
    after arguments binding. The string returned is exactly the one that would be
    sent to the database running the `execute()` method or similar. In short, it returns
    the formatted query. This is helpful as you can see what you are sending to the
    database, because your SQL query can often be a source of errors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pass your query to the `mogrify` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will create a proper SQL `insert` statement; however, as
    you progress, you will add multiple records in a single statement. To do so, you
    will create a tuple of tuples. To create the same SQL statement, you can use the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that in `query2`, you did not need to add quotes around strings that
    would be passed in as you did in `query` when you used `{}`. Using the preceding
    formatting, `psycopg2` will handle the mapping of types in the query string. To
    see what the query will look like when you execute it, you can use `mogrify` and
    pass the data along with the query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The results of `mogrify` on `query` and `query2` should be identical. Now,
    you can execute the query to add it to the database:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'If you go back to pgAdmin 4, right-click on the `insert` statement, you need
    to make it permanent by committing the transaction using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, in pgAdmin 4, you should be able to see the record, as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – Record added to the database'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_4.2_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.2 – Record added to the database
  prefs: []
  type: TYPE_NORMAL
- en: The record is now added to the database and visible in pgAdmin 4\. Now that
    you have entered a single record, the next section will show you how to enter
    multiple records.
  prefs: []
  type: TYPE_NORMAL
- en: Inserting multiple records
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To insert multiple records, you could loop through data and use the same code
    shown in the preceding section, but this would require a transaction per record
    in the database. A better way would be to use a single transaction and send all
    the data, letting `psycopg2` handle the bulk insert. You can accomplish this by
    using the `executemany` method. The following code will use `Faker` to create
    the records and then `executemany()` to insert them:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the needed libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the `faker` object and an array to hold all the data. You will initialize
    a variable, `i`, to hold an ID:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, you can look, iterate, and append a fake tuple to the array you created
    in the previous step. Increment `i` for the next record. Remember that in the
    previous section, you created a record for `Big Bird` with an ID of `1`. That
    is why you will start with `2` in this example. We cannot have the same primary
    key in the database table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert the array into a tuple of tuples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, you are back to the `psycopg` code, which will be similar to the example
    from the previous section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can print out what the code will send to the database using a single record
    from the `data_for_db` variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Lastly, use `executemany()` instead of `execute()` to let the library handle
    the multiple inserts. Then, commit the transaction:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, you can look at pgAdmin 4 and see the 1,000 records. You will have data
    similar to what is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3 – 1,000 records added to the database'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_4.3_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.3 – 1,000 records added to the database
  prefs: []
  type: TYPE_NORMAL
- en: Your table should now have 1,001 records. Now that you can insert data into
    PostgreSQL, the next section will show you how to query it in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting data from PostgreSQL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Extracting data using `psycopgs` follows the exact same procedure as inserting,
    the only difference being that you will use a `select` statement instead of `insert`.
    The following steps show you how to extract data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the library, then set up your connection and cursor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, you can execute a query. In this example, you will select all records
    from the `users` table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, you have an iterable object with the results. You can iterate over the
    cursor, as shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Alternatively, you could use one of the `fetch` methods:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To grab a single record, you can assign it to a variable and look at it. Note
    that even when you select one record, the cursor returns an array:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Regardless of whether you are fetching one or many, you need to know where
    you are and how many records there are. You can get the row count of the query
    using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can get the current row number using `rownumber`. If you use `fetchone()`
    and then call `rownumber` again, it should increment with your new position:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The last thing to mention is that you can also query a table and write it out
    to a CSV file using the `copy_to()` method.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create the connection and the cursor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Open a file to write the table to:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, call `copy_to` and pass the file, the table name, and the separator (which
    will default to tabs if you do not include it). Close the file, and you will have
    all the rows as a CSV:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can verify the results by opening the file and printing the contents:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that you know how to read and write to a database using the `psycopg2` library,
    you can also read and write data using DataFrames, which you will learn about
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting data with DataFrames
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You can also query data using `pandas` DataFrames. To do so, you need to establish
    a connection using `psycopg2`, and then you can skip the cursor and go straight
    to the query. DataFrames give you a lot of power in filtering, analyzing, and
    transforming data. The following steps will walk you through using DataFrames:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Set up the connection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, you can execute the query in a DataFrame using the `pandas` `read_sql()`
    method. The method takes a query and a connection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result is a DataFrame, `df`, with the full table users. You now have full
    access to all the DataFrame tools for working with the data – for example, you
    can export it to JSON using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that you know how to work with data in a relational database, it is time
    to learn about NoSQL databases. The next section will show you how to use Python
    with Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: Inserting and extracting NoSQL database data in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Relational databases may be what you think of when you hear the term database,
    but there are several other types of databases, such as columnar, key-value, and
    time-series. In this section, you will learn how to work with Elasticsearch, which
    is a NoSQL database. NoSQL is a generic term referring to databases that do not
    store data in rows and columns. NoSQL databases often store their data as JSON
    documents and use a query language other than SQL. The next section will teach
    you how to load data into Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Elasticsearch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To install the `elasticsearch` library, you can use `pip3`, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Using `pip` will install the newest version, which, if you installed Elasticsearch
    according to the instructions in [*Chapter 2*](B15739_02_ePub_AM.xhtml#_idTextAnchor027),
    *Building Our Data Engineering Infrastructure*, is what you will need. You can
    get the library for Elasticsearch versions 2, 5, 6, and 7\. To verify the installation
    and check the version, you can use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code should print something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: If you have the right version for your Elasticsearch version, you are ready
    to start importing data.
  prefs: []
  type: TYPE_NORMAL
- en: Inserting data into Elasticsearch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before you can query Elasticsearch, you will need to load some data into an
    index. In the previous section, you used a library, `psycopg2`, to access PostgreSQL.
    To access Elasticsearch, you will use the `elasticsearch` library. To load data,
    you need to create the connection, then you can issue commands to Elasticsearch.
    Follow the given steps to add a record to Elasticsearch:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the libraries. You can also create the `Faker` object to generate random
    data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a connection to Elasticsearch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code assumes that your `Elasticsearch` instance is running on
    `localhost`. If it is not, you can specify the IP address, as shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, you can issue commands to your `Elasticsearch` instance. The `index` method
    will allow you to add data. The method takes an index name, the document type,
    and a body. The body is what is sent to Elasticsearch and is a JSON object. The
    following code creates a JSON object to add to the database, then uses `index`
    to send it to the `users` index (which will be created automatically during the
    index operation):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code should print the word `created` to the console, meaning the
    document has been added. Elasticsearch returns an object with a result key that
    will let you know whether the operation failed or succeeded. `created`, in this
    case, means the index operation succeeded and created the document in the index.
    Just as with the PostgreSQL example earlier in this chapter, you could iterate
    and run the `index` command, or you can use a bulk operation to let the library
    handle all the inserts for you.
  prefs: []
  type: TYPE_NORMAL
- en: Inserting data using helpers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Using the `bulk` method, you can insert many documents at a time. The process
    is similar to inserting a single record, except that you will generate all the
    data, then insert it. The steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You need to import the `helpers` library to access the `bulk` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The data needs to be an array of JSON objects. In the previous example, you
    created a JSON object with attributes. In this example, the object needs to have
    some additional information. You must specify the index and the type. Underscores
    in the names are used for Elasticsearch fields. The `_source` field is where you
    would put the JSON document you want to insert in the database. Outside the JSON
    is a `for` loop. This loop creates the 999 (you already added one and you index
    from 0 – to 998) documents:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, you can call the `bulk` method and pass it the `elasticsearch` instance
    and the array of data. You can print the results to check that it worked:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should now have 1,000 records in an Elasticsearch index named `users`.
    We can verify this in Kibana. To add the new index to Kibana, browse to your Kibana
    dashboard at `http://localhost:5601`. Selecting **Management** at the bottom left
    of the toolbar, you can then create an index pattern by clicking the blue **+
    Create index pattern** button, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – Creating an index pattern'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_4.4_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.4 – Creating an index pattern
  prefs: []
  type: TYPE_NORMAL
- en: 'Add an Elasticsearch index pattern to Kibana. On the next screen, enter the
    name of the index – `users`. Kibana will start pattern matching to find the index.
    Select the `users` index from the dropdown and click the `users`), as shown in
    the following screenshot; you should see your documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5 – All of your documents in the Discover tab'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_4.5_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.5 – All of your documents in the Discover tab
  prefs: []
  type: TYPE_NORMAL
- en: Now that you can create a record individually or using the `bulk` method, the
    next section will teach you how you can query your data.
  prefs: []
  type: TYPE_NORMAL
- en: Querying Elasticsearch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Querying Elasticsearch follows the exact same steps as inserting data. The
    only difference is you use a different method – `search` – to send a different
    body object. Let''s walk through a simple query on all the data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the library and create your `elasticsearch` instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the JSON object to send to Elasticsearch. The object is a query, using
    the `match_all` search:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Pass the object to Elasticsearch using the `search` method. Pass the index
    and the return size. In this case, you will only return 10 records. The maximum
    return size is 10,000 documents:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Lastly, you can print the documents:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Or you can iterate through grabbing `_source` only:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can load the results of the query into a `pandas` DataFrame – it is JSON,
    and you learned how to read JSON in [*Chapter 3*](B15739_03_ePub_AM.xhtml#_idTextAnchor039),
    *Reading and Writing Files*. To load the results into a DataFrame, import `json_normalize`
    from the `pandas` `json` library, and use it (`json_normalize`) on the JSON results,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Now you will have the results of the search in a DataFrame. In this example,
    you just grabbed all the records, but there are other queries available besides
    `match_all`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the `match_all` query, I know I have a document with the name `Ronald
    Goodman`. You can query on a field using the `match` query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also use a Lucene syntax for queries. In Lucene, you can specify `field:value`.
    When performing this kind of search, you do not need a document to send. You can
    pass the `q` parameter to the `search` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the `City` field, you can search for `Jamesberg`. It will return two
    records: one for `Jamesberg` and one for `Lake Jamesberg`. Elasticsearch will
    tokenize strings with spaces in them, splitting them into multiple strings to
    search:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are the two records in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'You can use Boolean queries to specify multiple search criteria. For example,
    you can use `must`, `must not`, and `should` before your queries. Using a Boolean
    query, you can filter out `Lake Jamesberg`. Using a `must` match on `Jamesberg`
    as the city (which will return two records), and adding a filter on the ZIP, you
    can make sure only `Jamesberg` with the ZIP `63792` is returned. You could also
    use a `must not` query on the `Lake Jameson` ZIP:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, you only get the single record that you wanted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Your queries only returned a few documents, but in production, you will probably
    have large queries with tens of thousands of documents being returned. The next
    section will show you how to handle all that data.
  prefs: []
  type: TYPE_NORMAL
- en: Using scroll to handle larger results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the first example, you used a size of 10 for your search. You could have
    grabbed all 1,000 records, but what do you do when you have more than 10,000 and
    you need all of them? Elasticsearch has a scroll method that will allow you to
    iterate over the results until you get them all. To scroll through the data, follow
    the given steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the library and create your `Elasticsearch` instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Search your data. Since you do not have over 10,000 records, you will set the
    size to `500`. This means you will be missing 500 records from your initial search.
    You will pass a new parameter to the search method – `scroll`. This parameter
    specifies how long you want to make the results available for. I am using 20 milliseconds.
    Adjust this number to make sure you have enough time to get the data – it will
    depend on the document size and network speed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The results will include `_scroll_id`, which you will need to pass to the `scroll`
    method later. Save the scroll ID and the size of the result set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To start scrolling, use a `while` loop to get records until the size is 0,
    meaning there is no more data. Inside the loop, you will call the `scroll` method
    and pass `_scroll_id` and how long to scroll. This will grab more of the results
    from the original query:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, get the new scroll ID and the size so that you can loop through again
    if the data still exists:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Lastly, you can do something with the results of the scrolls. In the following
    code, you will print the source for every record:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now you know how to create documents in Elasticsearch and how to query them,
    even when there is more than the maximum return value of 10,000\. You can do the
    same using relational databases. It is now time to start putting these skills
    to use in building data pipelines. The next two sections will teach you how to
    use databases in your data pipelines using Apache Airflow and NiFi.
  prefs: []
  type: TYPE_NORMAL
- en: Building data pipelines in Apache Airflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, you built your first Airflow data pipeline using a
    Bash and Python operator. This time, you will combine two Python operators to
    extract data from PostgreSQL, save it as a CSV file, then read it in and write
    it to an Elasticsearch index. The complete pipeline is shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6 – Airflow DAG'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_4.6_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.6 – Airflow DAG
  prefs: []
  type: TYPE_NORMAL
- en: The preceding **Directed Acyclic Graph** (**DAG**) looks very simple; it is
    only two tasks, and you could combine the tasks into a single function. This is
    not a good idea. In *Section 2*, *Deploying Pipelines into Production*, you will
    learn about modifying your data pipelines for production. A key tenant of production
    pipelines is that each task should be atomic; that is, each task should be able
    to stand on its own. If you had a single function that read a database and inserted
    the results, when it fails, you have to track down whether the query failed or
    the insert failed. As your tasks get more complicated, it will take much more
    work to debug. The next section will walk you through building the data pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the Airflow boilerplate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Every DAG is going to have some standard, boilerplate code to make it run in
    Airflow. You will always import the needed libraries, and then any other libraries
    you need for your tasks. In the following code block, you import the operator,
    `DAG`, and the time libraries for Airflow. For your tasks, you import the `pandas`,
    `psycopg2`, and `elasticsearch` libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, you will specify the arguments for your DAG. Remember that the start
    time should be a day behind if you schedule the task to run daily:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, you can pass the arguments to the DAG, name it, and set the run interval.
    You will define your operators here as well. In this example, you will create
    two Python operators – one to get data from PostgreSQL and one to insert data
    in to Elasticsearch. The `getData` task will be upstream and the `insertData`
    task downstream, so you will use the `>>` bit shift operator to specify this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: Lastly, you will define the tasks. In the preceding operators, you named them
    `queryPostgresql` and `insertElasticsearch`. The code in these tasks should look
    very familiar; it is almost identical to the code from the previous sections in
    this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'To query PostgreSQL, you create the connection, execute the `sql` query using
    the `pandas` `read_sql()` method, and then use the `pandas` `to_csv()` method
    to write the data to disk:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'To insert the data into Elasticsearch, you create the Elasticsearch object
    connecting to `localhost`. Then, read the CSV from the previous task into a DataFrame,
    iterate through the DataFrame, converting each row into JSON, and insert the data
    using the `index` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Now you have a complete data pipeline in Airflow. In the next section, you will
    run it and view the results.
  prefs: []
  type: TYPE_NORMAL
- en: Running the DAG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To run the DAG, you need to copy your code to your `$AIRFLOW_HOME/dags` folder.
    After moving the file, you can run the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'When these commands complete, browse to `http://localhost:8080` to see the
    Airflow GUI. Select **MyDBdag**, and then select **Tree View**. You can schedule
    five runs of the DAG and click **Go**. As it runs, you should see the results
    underneath, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7 – Task showing successful runs and queued runs'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_4.7_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.7 – Task showing successful runs and queued runs
  prefs: []
  type: TYPE_NORMAL
- en: 'To verify that the data pipeline was successful, you can view the data in Elasticsearch
    using Kibana. To see the results, browse to Kibana at `http://localhost:5601`.
    You will need to create a new index in Kibana. You performed this task in the
    *Inserting data using helpers* section of this chapter. But to recap, you will
    select **Management** in Kibana from the bottom of the left-hand toolbar in Kibana,
    then create the index pattern by clicking the **Create index pattern** button.
    Start typing the name of the index and Kibana will find it, then click **Create**.
    Then, you can go to the **Discover** tab on the toolbar and view the data. You
    should see records as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8 – Airflow data pipeline results showing records in Elasticsearch'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_4.8_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.8 – Airflow data pipeline results showing records in Elasticsearch
  prefs: []
  type: TYPE_NORMAL
- en: You will see that there are documents containing only names and cities, as specified
    in your data pipeline task. One thing to note is that we now have over 2,000 records.
    There were only 1,000 records in the PostgreSQL database, so what happened? The
    data pipeline ran multiple times, and each time, it inserted the records from
    PostgreSQL. A second tenant of data pipelines is that they should be idempotent.
    That means that no matter how many times you run it, the results are the same.
    In this case, they are not. You will learn how to fix this in *Section 2*, *Deploying
    Pipelines into Production*, in [*Chapter 7*](B15739_07_ePub_AM.xhtml#_idTextAnchor086),
    *Features of a Production Pipeline*. For now, the next section of this chapter
    will teach you how to build the same data pipeline in Apache NiFi.
  prefs: []
  type: TYPE_NORMAL
- en: Handling databases with NiFi processors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous sections, you learned how to read and write CSV and JSON files
    using Python. Reading files is such a common task that tools such as NiFi have
    prebuilt processors to handle it. In this section, you will build the same data
    pipeline as in the previous section. In NiFi, the data pipeline will look as shown
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9 – A NiFi data pipeline to move data from PostgreSQL to Elasticsearch'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_4.9_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.9 – A NiFi data pipeline to move data from PostgreSQL to Elasticsearch
  prefs: []
  type: TYPE_NORMAL
- en: The data pipeline contains one more task than the Airflow version, but otherwise,
    it should look straightforward. The following sections will walk you through building
    the data pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting data from PostgreSQL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The processor most used for handling relational databases in NiFi is the `ExecuteSQLRecord`
    processor. Drag the `ExecuteSQLRecord` processor. Once it has been added to the
    canvas, you need to configure it.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the ExecuteSQLCommand processor
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To configure the processor, you need to create a database connection pool,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.10 – Creating a database connection pooling service'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_4.10_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.10 – Creating a database connection pooling service
  prefs: []
  type: TYPE_NORMAL
- en: After selecting `dataengineering`. Notice how I did not name it `PostgreSQL`.
    As you add more services, you will add more PostgreSQL connections for different
    databases. It would then be hard to remember which PostgreSQL database the service
    was for.
  prefs: []
  type: TYPE_NORMAL
- en: 'To configure the service, select the arrow in the processor configuration.
    The configuration for the service should look as in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.11 – Configuring the database service'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_4.11_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.11 – Configuring the database service
  prefs: []
  type: TYPE_NORMAL
- en: The configuration requires you to specify the connection URL, which is a Java
    database connection string. The string specifies `PostgreSQL`. It then names the
    host, `localhost`, and the database name, `dataengineering`. The driver class
    specifies the `postgresql` driver. The location of the driver is where you downloaded
    it in [*Chapter 2*](B15739_02_ePub_AM.xhtml#_idTextAnchor027), *Building Our Data
    Engineering Infrastructure*. It should be in your home directory in the `nifi`
    folder, in a subdirectory named `drivers`. Lastly, you need to enter the username
    and password for the database.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, you need to create a record writer service. Select **Create new service…**,
    choose **JSONRecordSetWriter**, and click the arrow to configure it. There is
    one important configuration setting that you cannot skip – **Output Grouping**.
    You must set this property to **One Line Per Object**. The finished configuration
    will look as in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.13 – The JSONRecordSetWriter configuration'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_4.12_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.13 – The JSONRecordSetWriter configuration
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you have set up the services for the processor, you need to finish
    configuring the process. The last parameter you need to configure is **SQL Select
    Query**. This is where you can specify the SQL command to run against the database.
    For example, you can enter the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: This will grab all the records in the PostgreSQL database, but only the `name`
    and `city` fields. You can now move on to the next processor in the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the SplitText processor
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that you have configured the **ExecuteSQLRecord** processor, you will receive
    an array of records. To process this data, you need to have a flowfile per record.
    To do that, you can use the **SplitText** processor. Drag it to the canvas and
    open the **Properties** tab by double-clicking on the processor – or right-click
    and select **Properties**. The processor defaults work, but make sure that **Line
    Split Count** is set to **1**, **Header Line Count** is **0** – your data does
    not have a header when it comes from the **ExecuteSQLRecord** processor – and
    **Remove Trailing Newlines** is **true**.
  prefs: []
  type: TYPE_NORMAL
- en: These settings will allow the processor to take each line of the flowfile and
    split it into a new flowfile. So, your one incoming flowfile will come out of
    this processor as 1,000 flow files.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the PutElasticsearchHttp processor
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The last step in the data pipeline is to insert the flowfiles into Elasticsearch.
    You can do that using the **PutElasticsearchHttp** processor. There are four different
    **PutElasticsearch** processors. Only two will be relevant in this book – **PutElasticsearchHttp**
    and **PutelasticsearchHttpRecord**. These are the processors to insert a single
    record or to use the bulk API. The other two processors – **Putelasticsearch**
    and **Putelasticsearch5** – are for older versions of Elasticsearch (2 and 5).
  prefs: []
  type: TYPE_NORMAL
- en: To configure the processor, you must specify the URL and the port. In this example,
    you will use `http://localhost:9200`. The index will be `fromnifi`, but you can
    name it anything you would like. The type is `doc` and the index operation will
    be `index`. In *Section 2*, *Deploying Pipelines into Production*, you will use
    other index operations and you will specify the ID of the records that you insert.
  prefs: []
  type: TYPE_NORMAL
- en: Running the data pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that you have configured all of the processors, you can connect them by
    dragging the arrow from **ExecuteSQLRecord** to the **SplitRecord** processor
    for success. Then, connect the **SplitRecord** processor to the **PutElasticsearchHttp**
    processor for splits. Lastly, terminate the **PutElasticsearchHttp** processor
    for all relationships.
  prefs: []
  type: TYPE_NORMAL
- en: Run each of the processors, or in the **Operations** pane, select **Run** to
    start them all. You will see one flowfile in the first queue, then it will split
    into 1,000 flowfiles in the second queue. The queue will empty in batches of 100
    as they are inserted into Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: 'To verify the results, you can use the `elasticsearch` API, and not Kibana.
    In your browser, go to `http://localhost:9200/_cat/indices`. This is the REST
    endpoint to view the indices in your Elasticsearch database. You should see your
    new index, `fromnifi`, and the total number of documents, as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.13 – The index contains all the records from PostgreSQL'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_4.13_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.13 – The index contains all the records from PostgreSQL
  prefs: []
  type: TYPE_NORMAL
- en: The number of documents in the index will vary depending on whether you left
    the pipeline running or not. Just as in the Airflow example, this pipeline is
    not idempotent. As it runs, it will keep adding the same records with a different
    ID into Elasticsearch. This is not the behavior you will want in production and
    we will fix this in *Section 2*, *Deploying Pipelines into Production*.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to use Python to query and insert data into
    both relational and NoSQL databases. You also learned how to use both Airflow
    and NiFi to create data pipelines. Database skills are some of the most important
    for a data engineer. There will be very few data pipelines that do not touch on
    them in some way. The skills you learned in this chapter provide the foundation
    for the other skills you will need to learn – primarily SQL. Combining strong
    SQL skills with the data pipeline skills you learned in this chapter will allow
    you to accomplish most of the data engineering tasks you will encounter.
  prefs: []
  type: TYPE_NORMAL
- en: In the examples, the data pipelines were not idempotent. Every time they ran,
    you got new results, and results you did not want. We will fix that in *Section
    2*, *Deploying Pipelines into Production*. But before you get to that, you will
    need to learn how to handle common data issues, and how to enrich and transform
    your data.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will teach you how to use Python to work with your data in
    between the extract and load phases of your data pipelines.
  prefs: []
  type: TYPE_NORMAL
