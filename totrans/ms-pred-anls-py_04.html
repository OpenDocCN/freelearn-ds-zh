<html><head></head><body>
  <div><div><div><div><div><h1 class="title"><a id="ch04"/>Chapter 4. Connecting the Dots with Models – Regression Methods</h1></div></div></div><p>The trend line is a common feature of many business analyses. How much do purchases increase when ads are shown more often on a homepage? What is the average rating of videos on social media based on user age? What is the likelihood that a customer will buy a second product from your website if they bought their first more than 6 months ago? These sorts of questions can be answered by drawing a line representing the average change in our response (for example, purchases or ratings) as we vary the input (for example, user age or amount of past purchases) based on historical data, and using it to extrapolate the response for future data (where we only know the input, but not output yet). Calculating this line is termed <em>regression</em>, based on the hypothesis that our observations are scattered around the true relationship between the two variables, and on average future observations will regress (approach) the trend line between input and output.</p><p>Several complexities complicate this analysis in practice. First, the relationships we fit usually involve not one, but several inputs. We can no longer draw a two dimensional line to represent this multi-variate relationship, and so must increasingly rely on more advanced computational methods to calculate this trend in a high-dimensional space. Secondly, the trend we are trying to calculate may not even be a straight line – it could be a curve, a wave, or even more complex patterns. We may also have more variables than we need, and need to decide which, if any, are relevant for the problem at hand. Finally, we need to determine not just the trend that best fits the data we have, but also generalizes best to new data.</p><p>In this chapter we will learn:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">How to prepare data for a regression problem</li><li class="listitem" style="list-style-type: disc">How to choose between linear and nonlinear methods for a given problem</li><li class="listitem" style="list-style-type: disc">How to perform variable selection and assess over-fitting</li></ul></div><div><div><div><div><h1 class="title"><a id="ch04lvl1sec23"/>Linear regression</h1></div></div></div><p><strong>Ordinary Least Squares</strong> (<strong>OLS</strong>).</p><p>We will start <a id="id237" class="indexterm"/>with the <a id="id238" class="indexterm"/>simplest model of linear regression, where we will simply try to fit the best straight line through the data points we have available. Recall that the formula for linear regression is:</p><div><img src="img/B04881_04_06.jpg" alt="Linear regression"/></div><p>Where y is a vector of n responses we are trying to predict, X is a vector of our input variable also of length n, and β is the slope response (how much the response y increases for each 1-unit increase in the value of X). However, we rarely have only a single input; rather, X will represent a set of input variables, and the response y is a linear combination of these inputs. In this case, known as multiple linear regression, X is a matrix of n rows (observations) and m columns (features), and β is a vector set of slopes or coefficients which, when multiplied by the features, gives the output. In essence, it is just the trend line incorporating many inputs, but will also allow us to compare the magnitude effect of different inputs on the outcome. When we are trying to fit a <a id="id239" class="indexterm"/>model using multiple linear regression, we also assume that the response incorporates a white noise error term ε, which is a normal distribution with mean 0 and a constant variance for all data points.</p><p>To solve for the coefficients β in this model, we can perform the following calculations:</p><div><img src="img/B04881_04_03.jpg" alt="Linear regression"/></div><p>The value of β is known the ordinary least squares estimate of the coefficients. The result will be a vector of coefficients β for the input variables. We make the following assumptions about the data:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">We assume the input variables (X) are accurately measured (there is no error in the values we are given). If they were not, and incorporated error, then they represent random variables, and we would need to incorporate this error in our estimate of the response in order to be accurate.</li><li class="listitem" style="list-style-type: disc">The response is a linear combination of the input variables – in other words, we need to be able to fit a straight line through the response. As we will see later in this chapter, we can frequently perform transformations to change nonlinear data into a linear form to satisfy this assumption.</li><li class="listitem" style="list-style-type: disc">The residual (the difference between the fitted and actual response) of the response y is assumed to have constant variance over the range of its values. If this is not the case (for example, if smaller values of y have smaller errors than larger values), then it suggests we are not appropriately incorporating a source of error in our model, because the only variation left after we account for the predictors X should be the error term ε. As previously mentioned, this error term ε should have constant variance, meaning the fit should have constant residual variance.</li><li class="listitem" style="list-style-type: disc">The residuals are assumed to be un-correlated based on the value of the predictors X. This is important because we assume we are trying to fit a line that goes through the average of the response data points at each predictor value, which would be accurate if we assume that the residual error is randomly distributed about 0. If the residuals are correlated with the value of a predictor, then the line that accurately fits the data may not go through the average, but rather be determined by the underlying correlation in the data. For example, if we are looking at time-series data, days of the week may have more correlated error at a 7-day pattern, meaning that our model should fit this periodicity rather than trying to simply draw a line through the data points for all days together.</li><li class="listitem" style="list-style-type: disc">The predictors are<a id="id240" class="indexterm"/> assumed not to be collinear (correlated with one another). If two predictors are identical, then they cancel each other out when we make a linear combination of the input matrix X. As we can see in the derivation of β above, to calculate the coefficients we need to take an inverse. If columns in the matrix exactly cancel each other out, then this matrix (XTX)-1 is rank deficient and has no inverse. Recall that if a matrix is full rank, its columns (rows) cannot be represented by a linear combination of the other columns (rows). A rank-deficient does not have an inverse because if we attempt to solve the linear system represented by:<div><img src="img/B04881_04_04.jpg" alt="Linear regression"/></div><p>Where A is the inverse we are trying to solve and I is the identity matrix, we will end up with columns in the solution the exactly cancel each-other, meaning any set of coefficients will solve the equation and we cannot have a unique solution.</p></li></ul></div><p>Why does the OLS formula for β represent the best estimate of the coefficients? The reason is that this value minimizes the squared error:</p><div><img src="img/B04881_04_05.jpg" alt="Linear regression"/></div><p>While a derivation of this fact is outside the scope of this text, this result is known as the <a id="id241" class="indexterm"/>Gauss Markov Theorem, and states that the OLS estimate is the Best Linear Unbiased Estimator (BLUE) of the coefficients β. Recall that when we are estimating these coefficients, we are doing so under the assumption that our calculations have some error, and deviate from the real (unseen) values. The BLUE is then the set of coefficients β that have the smallest mean error from these real values. For more details, we refer the reader to more comprehensive texts (Greene, William H. Econometric analysis. Pearson Education India, 2003; Plackett, Ronald L. "Some theorems in least squares." Biometrika 37.1/2 (1950): 149-157).</p><p>Depending upon the <a id="id242" class="indexterm"/>problem and dataset, we can relax many of the assumptions described above using alternative methods that are extensions of the basic linear model. Before we explore these alternatives, however, let us start with a practical example. The data we will be using for this exercise is a set of news articles from the website <a class="ulink" href="http://mashable.com/">http://mashable.com/</a>. (Fernandes, Kelwin, Pedro Vinagre, and Paulo Cortez. "A Proactive Intelligent Decision Support System for Predicting the Popularity of Online News." Progress in Artificial Intelligence. Springer International Publishing, 2015. 535-546.). Each article has been annotated using a number of features such as its number of words and what day it was posted - a complete list appears in the data file associated with this exercise. The task is to predict the popularity (the share column in the dataset) using these other features. In the process of fitting this first model, we will examine some of the common feature preparation tasks that arise in such analyses.</p><div><div><div><div><h2 class="title"><a id="ch04lvl2sec39"/>Data preparation</h2></div></div></div><p>Let us start by <a id="id243" class="indexterm"/>taking a look at the data by typing the following commands:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; news = pd.read_csv('OnlineNewsPopularity.csv',sep=',')</strong>
<strong>&gt;&gt;&gt; news.columns</strong>
</pre></div><p>Which gives the output:</p><div><pre class="programlisting">
<strong>Index(['url', ' timedelta', ' n_tokens_title', ' n_tokens_content',        ' n_unique_tokens', ' n_non_stop_words', ' n_non_stop_unique_tokens',        ' num_hrefs', ' num_self_hrefs', ' num_imgs', ' num_videos',        ' average_token_length', ' num_keywords', ' data_channel_is_lifestyle',        ' data_channel_is_entertainment', ' data_channel_is_bus',        ' data_channel_is_socmed', ' data_channel_is_tech',        ' data_channel_is_world', ' kw_min_min', ' kw_max_min', ' kw_avg_min',        ' kw_min_max', ' kw_max_max', ' kw_avg_max', ' kw_min_avg',        ' kw_max_avg', ' kw_avg_avg', ' self_reference_min_shares',        ' self_reference_max_shares', ' self_reference_avg_sharess',        ' weekday_is_monday', ' weekday_is_tuesday', ' weekday_is_wednesday',        ' weekday_is_thursday', ' weekday_is_friday', ' weekday_is_saturday',        ' weekday_is_sunday', ' is_weekend', ' LDA_00', ' LDA_01', ' LDA_02',        ' LDA_03', ' LDA_04', ' global_subjectivity',        ' global_sentiment_polarity', ' global_rate_positive_words',        ' global_rate_negative_words', ' rate_positive_words',        ' rate_negative_words', ' avg_positive_polarity',        ' min_positive_polarity', ' max_positive_polarity',        ' avg_negative_polarity', ' min_negative_polarity',        ' max_negative_polarity', ' title_subjectivity',        ' title_sentiment_polarity', ' abs_title_subjectivity',        ' abs_title_sentiment_polarity', ' shares'],       dtype='object')</strong>
</pre></div><p>If you look <a id="id244" class="indexterm"/>carefully, you will realize that all the column names have a leading whitespace; you probably would have found out anyway the first time you try to extract one of the columns by using the name as an index. The first step of our data preparation is to fix this formatting using the following code to strip whitespace from every column name:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; news.columns = [ x.strip() for x in news.columns]</strong>
</pre></div><p>Now that we have correctly formatted the column headers, let us examine the distribution of the data using the <code class="literal">describe()</code> command as we have seen in previous chapters:</p><div><img src="img/B04881_04_28.jpg" alt="Data preparation"/></div><p>As you scroll from left to right along the columns, you will notice that the range of the values in each column is very different. Some columns have maximum values in the hundreds or thousands, while others are strictly between 0 and 1. In particular, the value that we are trying to predict, shares, has a very wide distribution, as we can see if we plot the distribution using the following command:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; news['shares'].plot(kind='hist',bins=100)</strong>
</pre></div><div><img src="img/B04881_04_29.jpg" alt="Data preparation"/></div><p>Why is this <a id="id245" class="indexterm"/>distribution a problem? Recall that conceptually, when we fit a line through a dataset, we are finding the solution to the equation:</p><div><img src="img/B04881_04_06.jpg" alt="Data preparation"/></div><p>Where y is a response variable (such as shares), and β is the vector slopes by which we increase/decrease the value of the response for a 1-unit change in a column of X. If our response is logarithmically distributed, then the coefficients will be biased to accommodate extremely large points in order to minimize the total error of the fit given by:</p><div><img src="img/B04881_04_07.jpg" alt="Data preparation"/></div><p>To reduce this effect, we can logarithmically transform the response variable, which as you can see through the following code makes a distribution that looks much more like a normal curve:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; news['shares'].map( lambda x: np.log10(x) ).plot(kind='hist',bins=100)</strong>
</pre></div><div><img src="img/B04881_04_30.jpg" alt="Data preparation"/></div><p>This same rule <a id="id246" class="indexterm"/>of thumb holds true for our predictor variables, X. If some predictors are much larger than others, the solution of our equation will mainly emphasize those with the largest range, as they will contribute most to the overall error. In this example, we can systemically scale all of our variables using a logarithmic transformation. First, we remove all uninformative columns, such as the URL, which simply gives website location for the article.</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; news_trimmed_features = news.ix[:,'timedelta':'shares']</strong>
</pre></div><div><div><h3 class="title"><a id="note06"/>Note</h3><p>Note that in <a class="link" href="ch06.html" title="Chapter 6. Words and Pixels – Working with Unstructured Data">Chapter 6</a>, <em>Words and Pixels – Working with Unstructured Data</em> we will explore potential ways to utilize information in textual data such as the url, but for now we will simply discard it.</p></div></div><p>Then, we identify the variables we wish to transform (here an easy rule of thumb is that their max, given by the 8th row (index 7) of the describe() data frame is &gt; 1, indicating that they are not in the range 0 to 1) and use the following code to apply the logarithmic transform. Note that we add the number 1 to each logarithmically transformed variable so that we avoid errors for taking the logarithm of 0.</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; log_values = list(news_trimmed_features.columns[news_trimmed_features.describe().reset_index().loc[7][1:]&gt;1])</strong>
<strong>&gt;&gt;&gt; for l in log_values:</strong>
<strong>…    news_trimmed_features[l] = np.log10(news_trimmed_features[l]+1)</strong>
</pre></div><p>Using the describe() command again confirms that the columns now have comparable distributions:</p><div><img src="img/B04881_04_31.jpg" alt="Data preparation"/></div><p>We also need to<a id="id247" class="indexterm"/> remove infinite or nonexistent values from the dataset. We first convert infinite values to the placeholder 'not a number', or NaN, using the following:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; news_trimmed_features = news_trimmed_features.replace([np.inf, -np.inf], np.nan)</strong>
</pre></div><p>We then use the <code class="literal">fill</code> function to substitute the <em>NaN</em> placeholder with the proceeding value in the column (we could also have specified a fixed value, or used the preceding value in the column) using the following:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; news_trimmed_features = news_trimmed_features.fillna(method='pad')</strong>
</pre></div><p>Now we can split the data into the response variable (<code class="literal">'shares'</code>) and the features (all columns from <code class="literal">'timedelta'</code> to <code class="literal">'abs_title_sentiment_polarity'</code>), which we will use as inputs in the regression models described later using the commands:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; news_response = news_trimmed_features['shares']</strong>
<strong>&gt;&gt;&gt; news_trimmed_features = news_trimmed_features.ix[:,'timedelta':'abs_title_sentiment_polarity']</strong>
</pre></div><p>Let us now also take another look at variables that we did not transform logarithmically. If you try to fit a linear model using the dataset at this point, you will find that the slopes for many of these are extremely large or small. This can be explained by looking at what the remaining variables represent. For example, one set of columns which we did not logarithmically transform encodes a <code class="literal">0/1</code> value for whether a news <a id="id248" class="indexterm"/>article was published on a given day of the week. Another (annotated LDA) gives a <code class="literal">0/1</code> indicator for whether an article was tagged with a particular algorithmically-defined topic (we will cover this algorithm, known as Latent Dirichlet Allocation, in more detail in <a class="link" href="ch06.html" title="Chapter 6. Words and Pixels – Working with Unstructured Data">Chapter 6</a>, <em>Words and Pixels – Working with Unstructured Data</em>). In both cases, any row in the dataset must have the value 1 in one of the columns of these features (for example, the day of week has to take one of the seven potential values). Why is this a problem?</p><p>Recall that in most linear fits, we have both a slope and an intercept, which is the offset of the line vertically from the origin (<em>0, 0</em>) of the <em>x-y</em> plane. In a linear model with many variables, we represent this multi-dimensional intercept by a column of all 1 in the feature matrix X, which will be added by default in many model-fitting libraries. This means that a set of columns (for example, the days of the week), since they are independent, could form a linear combination that exactly equals the intercept column, making it impossible to find a unique solution for the slopes β. This is the same issue as the last assumption of linear regression we discussed previously, in which the matrix (XTX) is not invertible, and thus we cannot obtain a numerically stable solution for the coefficients. This instability results in the unreasonably large coefficient values you will observe if you were to fit a regression model on this dataset. It is for this reason that we either need to leave out the intercept column (an option you usually need to specify in a modeling library), or leave out one of the columns for these binary variables. Here we will do the second, dropping one column from each set of binary features using the following code:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; news_trimmed_features = news_trimmed_features.drop('weekday_is_sunday',1)</strong>
<strong>&gt;&gt;&gt; news_trimmed_features = news_trimmed_features.drop('LDA_00',1)</strong>
</pre></div><p>Now that we have taken care of these feature engineering concerns, we are ready to fit a regression model to our data.</p></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec40"/>Model fitting and evaluation</h2></div></div></div><p>Now that we are <a id="id249" class="indexterm"/>ready to fit a <a id="id250" class="indexterm"/>regression model to our data, it is important to clarify the goal of our analysis. As we discussed briefly in <a class="link" href="ch01.html" title="Chapter 1. From Data to Decisions – Getting Started with Analytic Applications">Chapter 1</a>, <em>From Data to Decisions – Getting Started with Analytic Applications</em>, the goals of modeling can be either a) to predict a future response given historical data, or <code class="literal">b</code>) infer the statistical significance and effect of a given variable on an outcome.</p><p>In the first scenario, we will choose a subset of data to train our model, and then evaluate the goodness of fit of the linear model on an independent data set not used to derive the model parameters. In this case, we want to validate that the trends represented by the model generalize beyond a particular set of data points. While the coefficient outputs of the linear model are interpretable, we are still more concerned in this scenario about whether we can accurately predict future responses rather than the meaning of the coefficients.</p><p>In the second <a id="id251" class="indexterm"/>scenario, we <a id="id252" class="indexterm"/>may not use a test data set at all for validation, and instead fit a linear model using all of our data. In this case, we are more interested in the coefficients of the model and whether they are statistically significant. In this scenario, we are also frequently interested in comparing models with more or fewer coefficients to determine the most important parameters that predict an outcome.</p><p>We will return to this second case, but for now let us continue under the assumption that we are trying to predict future data. To obtain test and validation data, we use the following commands to split the response and predictor data into 60% training and 40% test splits:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; from sklearn import cross_validation</strong>
<strong>&gt;&gt;&gt; news_features_train, news_features_test, news_shares_train, news_shares_test = \</strong>
<strong>&gt;&gt;&gt; cross_validation.train_test_split(news_trimmed_features, news_response, test_size=0.4, random_state=0)</strong>
</pre></div><p>We use the 'random state' argument to set a fixed outcome for this randomization, so that we can reproduce the same train/test split if we want to rerun the analysis at later date. With these training and test sets we can then fit the model and compare the predicted and observed values visually using the following code:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; from sklearn import linear_model</strong>
<strong>&gt;&gt;&gt; lmodel = linear_model.LinearRegression().fit(news_features_train, news_shares_train)</strong>
<strong>&gt;&gt;&gt; plt.scatter(news_shares_train,lmodel.predict(news_features_train),color='black')</strong>
<strong>&gt;&gt;&gt; plt.xlabel('Observed')</strong>
<strong>&gt;&gt;&gt; plt.ylabel('Predicted')</strong>
</pre></div><p>Which gives the following plot:</p><div><img src="img/B04881_04_32.jpg" alt="Model fitting and evaluation"/></div><p>Similarly, we <a id="id253" class="indexterm"/>can look<a id="id254" class="indexterm"/> at the performance of the model on the test data set using the commands:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; plt.scatter(news_shares_test,lmodel.predict(news_features_test),color='red')</strong>
<strong>&gt;&gt;&gt; plt.xlabel('Observed')</strong>
<strong>&gt;&gt;&gt; plt.ylabel('Predicted')</strong>
</pre></div><p>Which gives the plot:</p><div><img src="img/B04881_04_33.jpg" alt="Model fitting and evaluation"/></div><p>The visual similarities <a id="id255" class="indexterm"/>are confirmed by looking at the coefficient of variation, or 'R-squared' value. This is a<a id="id256" class="indexterm"/> metric often used in regression problems, which defines how much of the variation in the response is explained by the variation in the predictors according to the model. It is computed as:</p><div><img src="img/B04881_04_08.jpg" alt="Model fitting and evaluation"/></div><p>Where <code class="literal">Cov</code> and <code class="literal">Var</code> are the <strong>Covariance</strong> and <strong>Variance</strong> (respectively) of the two variables (the observed response y, and the predicted response given by yβ). A perfect score is <code class="literal">1</code> (a straight line), while <code class="literal">0</code> represents no correlation between a predicted and observed value (an example would be a spherical cloud of points). Using scikit learn, we can obtain the <em>R<sup>2</sup></em> value using the <code class="literal">score()</code> method of the linear model, with arguments the features and response variable. Running the following for our data:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; lmodel.score(news_features_train, news_shares_train)</strong>
<strong>&gt;&gt;&gt; lmodel.score(news_features_test, news_shares_test)</strong>
</pre></div><p>We get a value of <code class="literal">0.129</code> for the training data and <code class="literal">0.109</code> for the test set. Thus, we see that while there is some relationship between the predicted and observed response captured in the news article data, though we have room for improvement.</p><p>In addition to looking for overall performance, we may also be interested in which variables from our inputs are most important in the model. We can sort the coefficients <a id="id257" class="indexterm"/>of the model by their absolute magnitude to analyse this using the following code to obtain<a id="id258" class="indexterm"/> the sorted positions of the coefficients, and reorder the column names using this new index:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; ix = np.argsort(abs(lmodel.coef_))[::-1][:]</strong>
<strong>&gt;&gt;&gt; news_trimmed_features.columns[ix]</strong>
</pre></div><p>This gives the following output:</p><div><pre class="programlisting">
<strong>Index([u'n_unique_tokens', u'n_non_stop_unique_tokens', u'n_non_stop_words',        u'kw_avg_avg', u'global_rate_positive_words',        u'self_reference_avg_sharess', u'global_subjectivity', u'LDA_02',        u'num_keywords', u'self_reference_max_shares', u'n_tokens_content',        u'LDA_03', u'LDA_01', u'data_channel_is_entertainment', u'num_hrefs',        u'num_self_hrefs', u'global_sentiment_polarity', u'kw_max_max',        u'is_weekend', u'rate_positive_words', u'LDA_04',        u'average_token_length', u'min_positive_polarity',        u'data_channel_is_bus', u'data_channel_is_world', u'num_videos',        u'global_rate_negative_words', u'data_channel_is_lifestyle',        u'num_imgs', u'avg_positive_polarity', u'abs_title_subjectivity',        u'data_channel_is_socmed', u'n_tokens_title', u'kw_max_avg',        u'self_reference_min_shares', u'rate_negative_words',        u'title_sentiment_polarity', u'weekday_is_tuesday',        u'min_negative_polarity', u'weekday_is_wednesday',        u'max_positive_polarity', u'title_subjectivity', u'weekday_is_thursday',        u'data_channel_is_tech', u'kw_min_avg', u'kw_min_max', u'kw_avg_max',        u'timedelta', u'kw_avg_min', u'kw_max_min', u'max_negative_polarity',        u'kw_min_min', u'avg_negative_polarity', u'weekday_is_saturday',        u'weekday_is_friday', u'weekday_is_monday',        u'abs_title_sentiment_polarity'],       dtype='object')</strong>
</pre></div><p>You will notice that there is no information on the variance of the parameter values. In other words, we do not know the confidence interval for a given coefficient value, nor whether it is statistically significant. In fact, the scikit-learn regression method does not calculate statistical significance measurements, and for this sort of inference analysis—the second kind of regression analysis discussed previously and in <a class="link" href="ch01.html" title="Chapter 1. From Data to Decisions – Getting Started with Analytic Applications">Chapter 1</a>, <em>From Data to Decisions – Getting Started with Analytic Applications</em>—we will turn to a second Python<a id="id259" class="indexterm"/> library, <code class="literal">statsmodels</code> (<a class="ulink" href="http://statsmodels.sourceforge.net/">http://statsmodels.sourceforge.net/</a>).</p></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec41"/>Statistical significance of regression outputs</h2></div></div></div><p>After installing the <code class="literal">statsmodels</code> library, we can perform the same linear model analysis as previously, using all of the data rather than a train/test split. With <code class="literal">statsmodels</code>, we can <a id="id260" class="indexterm"/>use two different methods to fit the linear model, <code class="literal">api</code> and <code class="literal">formula.api</code>, which we import using the following commands:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; import statsmodels</strong>
<strong>&gt;&gt;&gt; import statsmodels.api as sm</strong>
<strong>&gt;&gt;&gt; import statsmodels.formula.api as smf</strong>
</pre></div><p>The <code class="literal">api</code> methods first resembles the scikit-learn function call, except we get a lot more detailed output about the statistical significance of the model after running the following:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; results = sm.OLS(news_response, news_trimmed_features).fit()</strong>
<strong>&gt;&gt;&gt; results.summary()</strong>
</pre></div><p>Which gives the following output:</p><div><img src="img/B04881_04_34.jpg" alt="Statistical significance of regression outputs"/></div><p>What do all these parameters mean? The number of observations and number of dependent variables are probably obvious, but the others we have not seen before. Briefly, their names <a id="id261" class="indexterm"/>and interpretation are:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Df model</strong>: This is the number of independent elements in the model parameters. We have 57 columns; once we know the value of 56 of them, the last is fixed by the need to minimize the remaining error, so there are only 56 degrees of freedom overall.</li><li class="listitem" style="list-style-type: disc"><strong>Df residuals</strong>: This is the number of independent pieces of information in the error estimates of the model. Recall that we obtain the errors by <code class="literal">y-X</code>.We only have up to <code class="literal">m</code> independent columns in <code class="literal">X</code>, where <code class="literal">m</code> is the number of predictors. So our estimate of the error has <code class="literal">n-1</code> independent elements from the data itself, from which we subtract another <code class="literal">m</code> which is determined by the inputs, giving us <code class="literal">n-m-1</code>.</li><li class="listitem" style="list-style-type: disc"><strong>Covariance type</strong>: This is the kind of covariance used in the model; here we just use white noise (a mean <code class="literal">0</code>, normally distributed error), but we could just as easily have specified a particular structure that would accommodate, for example, a case where the error is correlated with the magnitude of the response.</li><li class="listitem" style="list-style-type: disc"><strong>Adj. R-squared</strong>: If we include more variables in a model, we can start to increase the R2 by simply having more degrees of freedom with which to fit the data. If we wish to fairly compare the R2 for models with different numbers of parameters, then we can adjust the R2 calculation with the following formula:<div><img src="img/B04881_04_09.jpg" alt="Statistical significance of regression outputs"/></div><p>Using this formula, for models with larger numbers of parameters, we penalize the R2 by the amount of error in the fit.</p></li><li class="listitem" style="list-style-type: disc"><strong>F-statistics</strong>: This measure is used to compare (through a Chi-squared distribution) that any of the regression coefficients are statistically different than <code class="literal">0</code>.</li><li class="listitem" style="list-style-type: disc"><strong>Prob (F-statistic)</strong>: This is the p-value (from the F-statistic) that the null hypothesis (that the coefficients are <code class="literal">0</code> and the fit is no better than the intercept-only model) is true.</li><li class="listitem" style="list-style-type: disc"><strong>Log-likelihood</strong>: Recall that we assume the error of the residuals in the linear model is normally distributed. Therefore, to determine how well our result fits this assumption, we can compute the likelihood function:<div><img src="img/B04881_04_10.jpg" alt="Statistical significance of regression outputs"/></div><p>Where σ is the <a id="id262" class="indexterm"/>standard deviation of the residuals and μ is the mean of the residuals (which we expect be very near 0 based on the linear regression assumptions above). Because the log of a product is a sum, which is easier to work with numerically, we usually take the logarithm of this value, expressed as:</p><div><img src="img/B04881_04_11.jpg" alt="Statistical significance of regression outputs"/></div><p>While this value is not very useful on its own, it can help us compare two models (for example with different numbers of coefficients). Better goodness of fit is represented by a larger log likelihood, or a lower negative log likelihood.</p><div><div><h3 class="title"><a id="note07"/>Note</h3><p>In practice, we usually minimize the negative log likelihood instead of maximizing the log likelihood, as most optimization algorithms that we might use to obtain the optimal parameters assume minimization as the default objective.</p></div></div></li><li class="listitem" style="list-style-type: disc"><strong>AIC/BIC</strong>: AIC and BIC are abbreviations for the Akaike Information Criterion and Bayes Information Criterion. These are two statistics that help to compare models with different numbers of coefficients, thus giving a sense of the benefit of greater model complexity from adding more features. AIC is given by:<div><img src="img/B04881_04_12.jpg" alt="Statistical significance of regression outputs"/></div><p>Where <em>m</em> is the number of coefficients in the model and <em>L</em> is the likelihood, as described previously. Better goodness of fit is represented by lower AIC. Thus, increasing the number of parameters penalizes the model, while improving the likelihood that it decreases the AIC. BIC is similar, but uses the formula:</p><div><img src="img/B04881_04_13.jpg" alt="Statistical significance of regression outputs"/></div></li></ul></div><p>Where <em>n</em> is the number of data points in the model. For a fuller comparison of AIC and BIC, please see (Burnham, Kenneth P., and David R. Anderson. <em>Model selection and multimodel inference: a practical information-theoretic approach</em>. Springer Science &amp; Business Media, 2003).</p><p>Along with<a id="id263" class="indexterm"/> these, we also receive an output of the statistical significance of each coefficient, as judged by a <em>t-test</em> of its standard error:</p><div><img src="img/B04881_04_35.jpg" alt="Statistical significance of regression outputs"/></div><p>We also receive a final block of statistics:</p><div><img src="img/B04881_04_36.jpg" alt="Statistical significance of regression outputs"/></div><p>Most of these are outside the scope of this volume, but the Durbin-Watson (DW) statistic will be important later, when we discuss dealing with time series data. The DW statistic is given by:</p><div><img src="img/B04881_04_14.jpg" alt="Statistical significance of regression outputs"/></div><p>Where <em>e</em> are the <a id="id264" class="indexterm"/>residuals (here <em>y-Xβ</em> for the linear model). In essence, this statistic asks whether the residuals are positively or negatively correlated. If its value is <code class="literal">&gt;2</code>, this suggests a positive correlation. Values between <code class="literal">1</code> and <code class="literal">2</code> indicate little to correlation, with 2 indicating no correlation. Values less than <code class="literal">1</code> represent negative correlation between successive residuals. For more detail please see (Chatterjee, Samprit, and Jeffrey S. Simonoff. <em>Handbook of regression analysis</em>. Vol. 5. John Wiley &amp; Sons, 2013).</p><p>We could also have fit the model using the <code class="literal">formula.api</code> commands, by constructing a string from the input data representing the formula for the linear model. We generate the formula using the following code:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; model_formula = news_response.name+" ~ "+" + ".join(news_trimmed_features.columns)</strong>
</pre></div><p>You can print this formula to the console to verify it gives the correct output:</p><div><pre class="programlisting">
<strong>shares ~ timedelta + n_tokens_title + n_tokens_content + n_unique_tokens + n_non_stop_words + n_non_stop_unique_tokens + num_hrefs + num_self_hrefs + num_imgs + num_videos + average_token_length + num_keywords + data_channel_is_lifestyle + data_channel_is_entertainment + data_channel_is_bus + data_channel_is_socmed + data_channel_is_tech + data_channel_is_world + kw_min_min + kw_max_min + kw_avg_min + kw_min_max + kw_max_max + kw_avg_max + kw_min_avg + kw_max_avg + kw_avg_avg + self_reference_min_shares + self_reference_max_shares + self_reference_avg_sharess + weekday_is_monday + weekday_is_tuesday + weekday_is_wednesday + weekday_is_thursday + weekday_is_friday + weekday_is_saturday + is_weekend + LDA_01 + LDA_02 + LDA_03 + LDA_04 + global_subjectivity + global_sentiment_polarity + global_rate_positive_words + global_rate_negative_words + rate_positive_words + rate_negative_words + avg_positive_polarity + min_positive_polarity + max_positive_polarity + avg_negative_polarity + min_negative_polarity + max_negative_polarity + title_subjectivity + title_sentiment_polarity + abs_title_subjectivity + abs_title_sentiment_polarity</strong>
</pre></div><p>We can then use this formula to fit the full pandas dataframe containing both the response and the input variables, by concatenating the response and feature variables along their columns (axis 1) and calling the <code class="literal">ols</code> method of the formula API we imported previously:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; news_all_data = pd.concat([news_trimmed_features,news_response],axis=1)</strong>
<strong>&gt;&gt;&gt; results = smf.ols(formula=model_formula,data=news_all_data).fit()</strong>
</pre></div><p>In this example, it seems reasonable to assume that the residuals in the model we fit for popularity<a id="id265" class="indexterm"/> as a function of new article features are independent. For other cases, we might make multiple observations on the same set of inputs (such as when a given customer appears more than once in a dataset), and this data may be correlated with time (as when records of a single customer are more likely to be correlated when they appear closer together in time). Both scenarios violate our assumptions of independence among the residuals in a model. In the following sections we will introduce three methods to deal with these cases.</p></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec42"/>Generalize estimating equations</h2></div></div></div><p>In our next set of<a id="id266" class="indexterm"/> exercises <a id="id267" class="indexterm"/>we will use an example of student grades in a math course in several schools recorded over three terms, expressed by the symbols (G1-3) (Cortez, Paulo, and Alice Maria Gonçalves Silva. "Using data mining to predict secondary school student performance." (2008)). It might be expected that there is a correlation between the school in which the student is enrolled and their math grades each term, and we do see some evidence of this when we plot the data using the following commands:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; students = pd.read_csv('student-mat.csv',sep=';')</strong>
<strong>&gt;&gt;&gt; students.boxplot(by='school',column=['G1','G2','G3'])</strong>
</pre></div><div><img src="img/B04881_04_37.jpg" alt="Generalize estimating equations"/></div><p>We can see that there is some correlation between a decline in math grades in terms 2 and 3, and the school. If we want to estimate the effect of other variables on student grades, then we want to account for this correlation. How we do so depends upon what our objective is. If we want to simply have an accurate estimate of the coefficients β of the model at the population level, without being able to predict individual students' responses with our model, we can use the <strong>Generalize Estimating Equations</strong> (<strong>GEE</strong>) (Liang, Kung-Yee, and Scott L. Zeger. "Longitudinal data analysis using generalized linear models." <em>Biometrika</em> 73.1 (1986): 13-22). The motivating idea of the <a id="id268" class="indexterm"/>GEE is that we treat this correlation between school and grade as an additional <a id="id269" class="indexterm"/>parameter (which we estimate by performing a linear regression on the data and calculating the residuals) in the model. By doing so, we account for the effect of this correlation on the coefficient estimate, and thus obtain a better estimate of their value. However, we still usually assume that the responses within a group are exchangeable (in other words, the order does not matter), which is not the case with clustered data that might have a time-dependent component.</p><p>Unlike the linear model, GEE parameter estimates are obtained through nonlinear optimization of the objective function <code class="literal">U(</code><strong>β</strong><code class="literal">)</code>, using the following formula:</p><div><img src="img/B04881_04_15.jpg" alt="Generalize estimating equations"/></div><p>Where <code class="literal">μ<sub>k</sub></code> is the mean response of a group <em>k</em> (such as a school, in our example), <code class="literal">V<sub>k</sub></code> is the variance matrix giving the correlation between residuals for members of the group k and <code class="literal">Y<sub>k</sub>- μ<sub>k</sub></code> is the vector of residuals within this group. This is usually solved using the Newton-Raphson equation, which we will look at in more detail in <a class="link" href="ch05.html" title="Chapter 5. Putting Data in its Place – Classification Methods and Analysis">Chapter 5</a>, <em>Putting Data in its Place – Classification Methods and Analysis</em>. Conceptually, we can estimate the variance matrix V using the residuals from a regression, and optimize the formula above until convergence. Thus, by optimizing both the correlation structure between grouped data samples given by V along with the coefficients β, we have effectively obtained an estimate β independent of V.</p><p>To apply this method to our data, we can again create the model string using the following command:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; model_formula = "G3 ~ "+" + ".join(students.columns[1:len(students.columns)-3])</strong>
</pre></div><p>We can then run the GEE using the school as the grouping variable:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; results = smf.gee(model_formula,"school",data=students).fit()</strong>
<strong>&gt;&gt;&gt; results.summary()</strong>
</pre></div><p>However, in some cases we would instead like to obtain an estimate of individual, not population-level, responses, even with the kind of group correlations we discussed <a id="id270" class="indexterm"/>previously. In <a id="id271" class="indexterm"/>this scenario, we could instead use mixed effects models.</p></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec43"/>Mixed effects models</h2></div></div></div><p>Recall that in the<a id="id272" class="indexterm"/> linear models we have fitted in this chapter, we assume the response is modeled as:</p><div><img src="img/B04881_04_16.jpg" alt="Mixed effects models"/></div><p>Where <em>ε</em> is an error term. However, when we have correlation between data points belonging to a group, we can also use a model of the form:</p><div><img src="img/B04881_04_17.jpg" alt="Mixed effects models"/></div><p>Where <code class="literal">Z</code> and <code class="literal">u</code> are group-level variables and coefficients, respectively. The coefficient <code class="literal">u</code> has a mean <code class="literal">0</code> and a variance structure that needs to be specified. It could be uncorrelated between groups, for example, or have a more complex covariance relationship where certain groups are correlated with one another more strongly than others. Unlike the GEE model, we are not attempting to simply estimate the group level effect of the coefficients (after factoring out the effect of group membership), but within-group coefficients that control for the effect of belonging to a particular group. The name of mixed effects models comes from the fact that the variables <code class="literal">β</code> are fixed effects whose value is exactly known, while <code class="literal">u</code> are random effects, where the value <code class="literal">u</code> represents an observation of a group level coefficient which is a random variable. The coefficients <code class="literal">u</code> could either be a set of group-level intercepts (random intercepts model), or combined with group-level slopes (random slopes model). Groups may even be nested within one another (hierarchical mixed effects models), such as if town-level groups capture one kind of correlated variation, while state-level groups capture another. A full discussion of the many variations of mixed effects models is outside the scope of this book, but we refer the interested reader to references such as (West, Brady T., Kathleen B. Welch, and Andrzej T. Galecki. <em>Linear mixed models: a practical guide using statistical software</em>. CRC Press, 2014; Stroup, Walter W. <em>Generalized linear mixed models: modern concepts, methods and applications</em>. CRC press, 2012). As with GEE, we can fit this model similar to the linear model by including a group-level variable using the following commands:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; results = smf.mixedlm(model_formula,groups="school",data=students).fit()</strong>
<strong>&gt;&gt;&gt; results.summary()</strong>
</pre></div></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec44"/>Time series data</h2></div></div></div><p>The last category of model assumptions that we will consider are where clustered data is temporally correlated, for example if a given customer has periodic buying activity based on the day of the <a id="id273" class="indexterm"/>week. While GEE and mixed effects models generally deal with data in which the inter-group correlations are exchangeable (the order does not matter), in time series data, the order is important for the interpretation of the data. If we assume exchangeability, then we may incorrectly estimate the error in our model, since we will assume the best fit goes through the middle of the data in a given group, rather than following the correlation structure of repeated measurements in a time series.</p><p>A particularly flexible model for time series data uses a formula known as the Kalman filter. Superficially, the Kalman filter resembles the equation for mixed effects models; consider an observation that at a given point in time has an unobserved state which we want to infer in the model (such as whether a given stock is increasing or decreasing in price), which is obscured by noise (such as market variation in stock price). The state of the data point is given by:</p><div><img src="img/B04881_04_18.jpg" alt="Time series data"/></div><p>Where <code class="literal">F</code> represents the matrix of transition probabilities between states, <em>xt-1</em> is the state at the last time step, <em>w<sub>t</sub></em> is noise, and <em>B<sub>t</sub></em> and <em>u<sub>t</sub></em> represent regression variables that could incorporate, for example, seasonal effects. In this case, <em>u</em> would be a binary indicator of a season or time of day, and <em>β</em> the amount we should add or subtract from <em>x</em> based on this indicator. The state <em>x</em> is used to predict the observed response using:</p><div><img src="img/B04881_04_19.jpg" alt="Time series data"/></div><p>Where <em>xt</em> is the state from the previous equation, <em>H</em> is a set of coefficients for each underlying state, and <em>vt</em> is noise. The Kalman filter uses the observations at time <em>t-1</em> to update our estimates of both the underlying state <em>x</em> and the response <em>y</em> at time <em>t</em>.</p><p>The family of equations given previously is also known by the more general term "Structural Time Series Equations". For the derivations of the update equations and further details on "Structural Time Series Models", we refer the reader to more advanced references (Simon, Dan. <em>Optimal state estimation: Kalman, H infinity, and nonlinear approaches</em>. John Wiley &amp; Sons, 2006; Harvey, Andrew C. <em>Forecasting, structural time series models and the Kalman filter</em>. Cambridge University Press, 1990).</p><p>In the <code class="literal">statsmodels</code> package, the Kalman filter is used<a id="id274" class="indexterm"/> in <strong>auto-regressive moving average</strong> (<strong>ARMA</strong>) models, which are fit with the following command:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; statsmodels.tsa.arima_model.ARMA()</strong>
</pre></div></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec45"/>Generalized linear models</h2></div></div></div><p>In most of the <a id="id275" class="indexterm"/>preceding <a id="id276" class="indexterm"/>examples, we assume that the response variable may be modeled as a linear combination of the responses. However, we can often relax this assumption by fitting a generalized linear model. Instead of the formula:</p><div><img src="img/B04881_04_20.jpg" alt="Generalized linear models"/></div><p>We substitute a <code class="literal">link</code> function (<em>G</em>) that transforms the nonlinear output into a linear response:</p><div><img src="img/B04881_04_21.jpg" alt="Generalized linear models"/></div><p>Examples of <code class="literal">link</code> functions include:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Logit</strong>: This <code class="literal">link</code> function <a id="id277" class="indexterm"/>maps the responses in the range <code class="literal">0</code> to <code class="literal">1</code> to a linear scale using the function <em>Xβ=ln(y/1-y)</em>, where <em>y</em> is usually a probability between <code class="literal">0</code> and <code class="literal">1</code>. This <code class="literal">link</code> function is used in logistic and multinomial regression, covered in <a class="link" href="ch05.html" title="Chapter 5. Putting Data in its Place – Classification Methods and Analysis">Chapter 5</a>, <em>Putting Data in its Place – Classification Methods and Analysis</em>.</li><li class="listitem" style="list-style-type: disc"><strong>Poisson</strong>: This <code class="literal">link</code> function<a id="id278" class="indexterm"/> maps count data to a linear scale using the relationship <em>Xβ=ln(y)</em>, where <em>y</em> is count data.</li><li class="listitem" style="list-style-type: disc"><strong>Exponential</strong>: This <code class="literal">link</code> function <a id="id279" class="indexterm"/>maps data from an exponential scale to a linear one with the formula <em>Xβ=y-1</em>.</li></ul></div><p>While these sorts of transformations make it possible to transform many nonlinear problems into linear ones, they also make it more difficult to estimate parameters of the model. Indeed, the matrix algebra used to derive the coefficients for simple linear regression do not apply, and the equations do not have any closed solution we could represent by a single step or calculation. Instead, we need iterative update equations such as those used for GEE and mixed effects models. We will cover these sorts of methods in more detail in <a class="link" href="ch05.html" title="Chapter 5. Putting Data in its Place – Classification Methods and Analysis">Chapter 5</a>, <em>Putting Data in its Place – Classification Methods and Analysis</em>.</p><p>Now we have now covered some of the diverse cases of fitting models to data that violate the linear regression assumptions in order to correctly interpret coefficients. Let us return now to the task of trying to improve the predictive performance of our linear model by selecting a subset of variables in the hope of removing correlated inputs and reducing over-fitting, an approach known as <em>regularization</em>.</p></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec46"/>Applying regularization to linear models</h2></div></div></div><p>After observing that the <a id="id280" class="indexterm"/>performance of our linear model is not optimal, one relevant question is whether all of the features in this model are necessary, or whether the coefficients we have estimated are suboptimal. For instance, two columns may be highly correlated with one another, meaning that the matrix <code class="literal">XTX</code> can be rank-deficient and thus not invertible, leading to numerical instability in calculating the coefficients. Alternatively, we may have included enough input variables to make an excellent fit on the training data, but this fit may not generalize to the test data as it precisely captures nuanced patterns present only in the training data. The high number of variables gives us great flexibility to make the predicted responses exactly match the observed responses in the training set, leading to overfitting. In both cases, it may be helpful to apply regularization to the model. Using regularization, we try to apply a penalty to the magnitude and/or number of coefficients, in order to control for over-fitting and multicollinearity. For regression models, two of the most popular forms of regularization are Ridge and Lasso Regression.</p><p>In Ridge Regression, we want to constrain the coefficient magnitude to a reasonable level, which is accomplished by applying a squared penalty to the size of the coefficients in the loss function equation:</p><div><img src="img/B04881_04_22.jpg" alt="Applying regularization to linear models"/></div><div><div><h3 class="title"><a id="note08"/>Note</h3><p>Please note this L(β), though using the same symbols, is not the same as the likelihood equations discussed earlier.</p></div></div><p>In other words, by applying the penalty <em>α</em> to the sum of squares of the coefficients, we constrain the model not only to approximate <em>y</em> as well as possible, using the slopes <em>β</em> multiplied by the features, but also constrain the size of the coefficients <em>β</em>. The effect of this penalty is controlled by the weighting factor <em>α</em>. When alpha is <code class="literal">0</code>, the model is just normal linear regression. Models with <em>α &gt; 0</em> increasingly penalizes large <em>β</em> values. How can we choose the right value for <em>α</em>? The <code class="literal">scikit-learn</code> library offers a helpful cross-validation function that can find the optimal value for <em>α</em> on the training set using the following commands:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; lmodel_ridge = linear_model.RidgeCV().fit(news_features_train, news_shares_train)</strong>
<strong>&gt;&gt;&gt; lmodel_ridge.alpha_</strong>
</pre></div><p>Which gives the optimal <code class="literal">α</code> value as <code class="literal">0.100</code>.</p><p>However, making this change does not seem to influence predictive accuracy on the test set when we evaluate the new <em>R2</em> value using the following commands:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; lmodel_ridge.score(news_features_test, news_shares_test)</strong>
</pre></div><p>In fact, we obtain the same result as the original linear model, which gave a test set <em>R2</em> of <code class="literal">0.109</code>.</p><p>Another method of regularization is referred to as Lasso, where we minimize the following equation. It is similar to the Ridge Regression formula above, except that the squared penalty on the values of <em>β</em> have been replaced with an absolute value term.</p><div><img src="img/B04881_04_23.jpg" alt="Applying regularization to linear models"/></div><p>This absolute <a id="id281" class="indexterm"/>value penalty has the practical effect that many of the slopes are optimized to be zero. This could be useful if we have many inputs and wish to select only the most important to try and derive insights. It may also help in cases where two variables are closely correlated with one another, and we will select one to include in the model. Like Ridge Regression, we can find the optimal value of <em>α</em> using the following cross validation commands:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; lmodel_lasso = linear_model.LassoCV(max_iter=10000).fit(news_features_train, news_shares_train)</strong>
<strong>&gt;&gt;&gt; lmodel_lasso.alpha_</strong>
</pre></div><p>Which suggests an optimal <em>α</em> value of <em>6.25e-5</em>.</p><p>In this case, there does not seem to be much value in applying this kind of penalization to the model, as the optimal <em>α</em> is near zero. Taken together, the analyses above suggest that modifying the coefficients themselves is not helping our model.</p><p>What might help us decide whether we would use Ridge or Lasso, besides the improvement in goodness of fit? One tradeoff is that while Lasso may generate a sparser model (more coefficients set to <code class="literal">0</code>), the values of the resulting coefficients are hard to interpret. Given two highly correlated variables, Lasso will select one, while shrinking the other to <code class="literal">0</code>, meaning with some modification to the underlying data (and thus bias to select one of these variables) we might have selected a different variable into the model. While Ridge regression does not suffer from this problem, the lack of sparsity may make it harder to interpret the outputs as well, as it does not tend to remove variables from the model.</p><p>A balance between these two choices is provided by Elastic Net Regression (Zou, Hui, and Trevor Hastie. "Regularization and variable selection via the elastic net." <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em> 67.2 (2005): 301-320.). In Elastic Net, our penalty term becomes a blend of Ridge and Lasso, with the optimal β minimizing:</p><div><img src="img/B04881_04_24.jpg" alt="Applying regularization to linear models"/></div><p>Because of this <a id="id282" class="indexterm"/>modification, Elastic Net can select groups of correlated variables, while still shrinking many to zero. Like Ridge and Lasso, Elastic Net has a CV function to choose the optimal value of the two penalty terms α using:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; from sklearn.linear_model import ElasticNetCV</strong>
<strong>&gt;&gt;&gt; lmodel_enet = ElasticNetCV().fit(news_features_train, news_shares_train)</strong>
<strong>&gt;&gt;&gt; lmodel_enet.score(news_features_test, news_shares_test)</strong>
</pre></div><p>However, this still is not significantly improving the performance of our model, as the test <em>R2</em> is still unmoved from our original least squares regression. It may be the response is not captured well by a linear trend involving a combination of the inputs. There may be interactions between features that are not represented by coefficients of any single variable, and some variables might have nonlinear responses, such as:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Nonlinear trends, such as a logarithmic increase in the response for a linear increase in the predictor</li><li class="listitem" style="list-style-type: disc">Non-monotonic (increasing or decreasing) functions such as a parabola, with a lower response in the middle of the range of predictor values and higher values at the minimum and maximum</li><li class="listitem" style="list-style-type: disc">More complex multi-modal responses, such as cubic polynomials</li></ul></div><p>While we could attempt to use generalized linear models, as described above, to capture these patterns, in large datasets we may struggle to find a transformation that effectively captures all these possibilities. We might also start constructing "interaction features" by, for example, multiplying each of our input variables to generate <em>N(N-1)/2</em> additional variables (for the pairwise products between all input variables). While this approach, sometimes called "polynomial expansion," can sometimes capture nonlinear relationships missed in the original model, with larger feature sets this can ultimately become unwieldy. Instead, we might try to explore methods that can efficiently explore the space of possible variable interactions.</p></div></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch04lvl1sec24"/>Tree methods</h1></div></div></div><p>In many datasets, the <a id="id283" class="indexterm"/>relationship between our inputs and output may not be a straight line. For example, consider the relationship between hour of the day and probability of posting on social media. If you were to draw a plot of this probability, it would likely increase during the evening and lunch break, and decline during the night, morning and workday, forming a sinusoidal wave pattern. A linear model cannot represent this kind of relationship, as the value of the response does not strictly increase or decrease with the hour of the day. What models, then, could we use to capture this relationship? In the specific case of time series models we could use approaches such as the Kalman filter described above, using the components of the structural time series equation to represent the cyclical 24-hour pattern of social media activity. In the following section we examine more general approaches that will apply both to time series data and to more general non-linear relationships.</p><div><div><div><div><h2 class="title"><a id="ch04lvl2sec47"/>Decision trees</h2></div></div></div><p>Consider a case where we assign a probability of posting on social media when hour <code class="literal">&gt; 11 am</code> and <code class="literal">&lt; 1 pm</code>, <code class="literal">&gt; 1 pm</code> and <code class="literal">&lt; 6 pm</code>, and so forth. We could visualize these as the branches of a tree, where at each branch point we have a condition (such as hour <code class="literal">&lt; 6 pm</code>), and assign <a id="id284" class="indexterm"/>our input data to one branch or another. We continue this sort of branching until we reach the<a id="id285" class="indexterm"/> end of a series of such selections, called a "leaf" of the tree; the predicted response for the tree is then the average of the value of training data points in this last group. To predict the response of new data points, we follow the branches of the tree to the bottom. To compute a decision tree, then, we need the following steps:</p><div><ol class="orderedlist arabic"><li class="listitem">Start with a training set of features <em>X</em> and responses <em>y</em>.</li><li class="listitem">Find the column of <em>X</em>, along with the dividing point, which optimizes the split between the data points. There are several criteria we could optimize, such as the variance of the target response on each side of the decision boundary (see split functions, later). (Breiman, Leo, et al. Classification and regression trees. CRC press, 1984.). We assign training data points to two new branches based on this rule.</li><li class="listitem">Repeat step 2 until a stopping rule is reached, or there is only a single value in each of the final branches of the tree.</li><li class="listitem">The predicted response is then given by the average response of the training points that end up in a particular branch of the tree.</li></ol></div><p>As mentioned previously, every time we select a split point in the tree model, we need a principled way of determining whether one candidate variable is better than another for dividing the data into groups that have more correlated responses. There are several options.</p><p><em>Variance Reduction</em> measures weather the two groups formed after splitting the data have lower variance in the response variable y than the data as a <a id="id286" class="indexterm"/>whole, and is used in the <strong>Classification and Regression Trees</strong> (CART) algorithm for decision trees (Breiman, Leo, et al. Classification and regression trees. CRC press, 1984.). It may be calculated by:</p><div><img src="img/B04881_04_25.jpg" alt="Decision trees"/></div><p>Where <em>A</em> is the set of all data points before the split, <em>L</em> is the set of values that fall to the left side of the split, and R is the set of points that falls to the right side. This formula is optimized when the combined variance of the two sides of the split point are less than the variance in the original data.</p><p>Variance reduction <a id="id287" class="indexterm"/>will work<a id="id288" class="indexterm"/> best for problems like the one we are examining in this chapter, where the output is a continuous variable. However, in classification problems with categorical outcomes, such as we will examine in <a class="link" href="ch05.html" title="Chapter 5. Putting Data in its Place – Classification Methods and Analysis">Chapter 5</a>, <em>Putting Data in its Place – Classification Methods and Analysis</em> the variance becomes less meaningful because the data can only assume fixed number of values (1 or 0 for a particular class). Another statistic we might optimize is the "information gain," which is used in the Iterative Dichotomiser 3 (ID3) and C4.5 algorithms for building decision trees (Quinlan, J. Ross. <em>C4. 5: programs for machine learning</em>. Elsevier, 2014; Quinlan, J. Ross. "Induction of decision trees." <em>Machine learning</em> 1.1 (1986): 81-106). The information gain statistic asks whether the data on the left and right sides of the decision split become more or less similar after being partitioned. If we considered the response y to be a probability, then the information gain is calculated as:</p><div><img src="img/B04881_04_26.jpg" alt="Decision trees"/></div><p>Where α is the fraction of data that is divided to the left side of the split, and f<sub>Ak</sub>, f<sub>Lk</sub>, and f<sub>Rk</sub> are the fraction of elements in class <em>k</em> among all data points, the Left side, and the Right side<a id="id289" class="indexterm"/> of the <a id="id290" class="indexterm"/>split. The three terms of this equation are known as Entropies (Borda, Monica. <em>Fundamentals in information theory and coding</em>. Springer Science &amp; Business Media, 2011). Why does the entropy reflect a good split of the data? To see this, plot the values from <code class="literal">0</code> to <code class="literal">1</code> for the function <code class="literal">ylog2y</code> using the following:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; probs = np.arange(0.01,1,0.01)</strong>
<strong>&gt;&gt;&gt; entropy = [ -1*np.log2(p)*p for p in probs]</strong>
<strong>&gt;&gt;&gt; plt.plot(probs,entropy)</strong>
<strong>&gt;&gt;&gt; plt.xlabel('y')</strong>
<strong>&gt;&gt;&gt;plt.ylabel('Entropy')</strong>
</pre></div><p>Look at the result:</p><div><img src="img/B04881_04_38.jpg" alt="Decision trees"/></div><p>You can appreciate that the entropy drops as <strong>y</strong> approaches <code class="literal">0</code> or <code class="literal">1</code>. This corresponds to a very high probability or low probability of a particular class in a classification problem, and thus trees which split data according to information gain will maximize the extent to which the left and right branches tend towards or against probability for a given class in the response.</p><p>Similarly, the CART algorithm (Breiman, Leo, et al. Classification and regression trees. CRC press, 1984.). Also use the <em>Gini impurity</em> to decide the split points, calculated as:</p><div><img src="img/B04881_04_27.jpg" alt="Decision trees"/></div><p>Inspecting this formula, you can see it will be maximized when one class is near a value of <em>f = 1</em>, while all others are <code class="literal">0</code>.</p><p>How could we deal with null values and missing data in such a model? In scikit-learn, the current decision tree implementation does not accommodate missing values, so we either need to insert a placeholder value (such as <code class="literal">-1</code>), drop missing records, or impute them (for example, replacing with the column mean) (see Aside for more details). However, some implementations (such as the R statistical programming language's <code class="literal">gbm</code> package) treat missing data as a third branch into which to sort data.</p><p>Similar diversity is <a id="id291" class="indexterm"/>present in <a id="id292" class="indexterm"/>the treatment of categorical data. The current scikit-learn implementation expects only numerical columns, meaning that categorical features such as gender or country need to be encoded as binary indicators. However, other packages, such as the implementation in R, treat categorical data by assigning data into buckets based on their feature value, then sorting the buckets by average response to determine which buckets to assign to the left and right branches of a tree.</p><div><div><h3 class="title"><a id="tip08"/>Tip</h3><p><strong>Aside</strong>: <strong>dealing with missing data</strong></p><p>When dealing with missing values in data, we need to consider several possibilities. One is whether the data is <em>missing at random</em>, or <em>missing not at random</em>. In the first case, there is a correlation between the response variable and the fact that the data is missing. We could assign a dummy value (such as <code class="literal">-1</code>), remove the whole row with missing data from our analysis, or assign the column mean or median as a placeholder. We could also think of more sophisticated approaches, such as training a regression model that uses all the other input variables as predictors and the column with the missing data as the output response, and derive imputed values using the predictions from this model. If data is missing not at random, then simply encoding the data with a placeholder is probably not sufficient, as the placeholder value is correlated with the response. In this scenario, we may remove the rows with missing data, or if this is not possible, employ the model-based approach. This will be preferable to infer the value of the missing elements in the data as it should predict values that follow the same distribution as the rest of the column.</p></div></div><p>In practice, while constructing the tree, we usually have some stopping rule, such as the minimum number of observations that are needed to form a leaf node (otherwise the predicted response could come from a small number of data points, which usually increases the error of the predictions).</p><p>It is not clear at the <a id="id293" class="indexterm"/>outset how <a id="id294" class="indexterm"/>many times the tree should branch. If there are too few splits (decision points), then few rules can be applied to subdivide the dataset and the resulting accuracy of the model may be low. If there are too many splits in a very deep tree, then the model may not generalize well to a new set of data. For our example, let us try fitting the tree to a number of different depths:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; from sklearn.tree import DecisionTreeRegressor</strong>
<strong>&gt;&gt;&gt; max_depths = [2,4,6,8,32,64,128,256]</strong>
<strong>&gt;&gt;&gt; dtrees = []</strong>
<strong>&gt;&gt;&gt; for m in max_depths:</strong>
<strong>…    dtrees.append(DecisionTreeRegressor(min_samples_leaf=20,max_depth=m).\</strong>
<strong>…    fit(news_features_train, news_shares_train))</strong>
</pre></div><p>Now, we can evaluate the results by plotting the R2 against the tree depth for each model:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; r2_values = []</strong>
<strong>&gt;&gt;&gt; for d in dtrees:</strong>
<strong>…    r2_values.append(d.score(news_features_test, news_shares_test))</strong>
<strong>&gt;&gt;&gt; plt.plot(max_depths,r2_values,color='red')</strong>
<strong>&gt;&gt;&gt; plt.xlabel('maximum depth')</strong>
<strong>&gt;&gt;&gt; plt.ylabel('r-squared')</strong>
</pre></div><p>Looking at the performance on the test set, we can see that the gains in performance drop quickly once we make the tree too deep:</p><div><img src="img/B04881_04_39.jpg" alt="Decision trees"/></div><p>Unfortunately, the tree<a id="id295" class="indexterm"/> model is still <a id="id296" class="indexterm"/>not performing much better than our basic linear regression. To try and improve this, we can try to increase the number of trees rather than the depth of the trees. The intuition here is that a set of shallower trees may in combination capture relationships that it is difficult for a single deep tree to approximate. This approach, of using a combination of smaller models to fit complex relationships, is used both in the Random Forest algorithm discussed in the following section, and in Gradient Boosted Decision Trees (<a class="link" href="ch05.html" title="Chapter 5. Putting Data in its Place – Classification Methods and Analysis">Chapter 5</a>, <em>Putting Data in its Place – Classification Methods and Analysis</em>) and, in a sense, in the deep learning models we will discuss in <a class="link" href="ch07.html" title="Chapter 7. Learning from the Bottom Up – Deep Networks and Unsupervised Features">Chapter 7</a>, <em>Learning from the Bottom Up – Deep Networks and Unsupervised Features</em>.</p></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec48"/>Random forest</h2></div></div></div><p>While the idea of capturing non-linear relationships seems reasonable, it is possible that it is difficult to <a id="id297" class="indexterm"/>construct a single tree that captures such complex relationships between input and output. What if we were to average over many simpler decision trees? This is the essence <a id="id298" class="indexterm"/>of the Random Forest algorithm (Ho, Tin Kam. "Random decision forests." <em>Document Analysis and Recognition, 1995., Proceedings of the Third International Conference on</em>. Vol. 1. IEEE, 1995.; Breiman, Leo. "Random forests." <em>Machine learning</em> 45.1 (2001): 5-32.), in which we construct several trees to try and explore the space of possible nonlinear interactions.</p><p>Random Forests are an innovation on the concept of Bootstrap Aggregation (Bagging) for tree models (Breiman, Leo. "Bagging predictors." <em>Machine learning</em> 24.2 (1996): 123-140.). In the generic Bagging algorithm, we construct a large number of trees by sampling, with replacement from the training data a small number of data points and building a tree only on this subset of data. While individual trees will be relatively weak models, by averaging over a large number of trees we can often achieve better prediction performance. Conceptually this is because instead of trying to fit a single model (such as a single line) through the response, we are approximating the response using an ensemble of small models that each fit a single simpler pattern in the input data.</p><p>Random Forests are a further development on the idea of Bagging by randomizing not just the data used to build each tree, but the variables as well. At each step we also only consider a random subset (for example, of size equal to the square root of the total number of columns) of the columns of X while constructing the splits in the tree (Hastie, Trevor, et al. "The elements of statistical learning: data mining, inference and prediction." <em>The Mathematical Intelligencer</em> 27.2 (2005): 83-85.). If we used all input columns at each round of training, we would tend to select the same variables that are most strongly correlated with the response. By instead randomly selecting a subset of variables, we can also find patterns among weaker predictors and more widely cover the space of possible feature interactions. As with Bagging, we follow this process of random data and variable selection many times, and then average together the predictions of all trees to reach an overall prediction. Again, we can explore whether varying a parameter (the number of trees) improves performance on the test set:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; from sklearn import ensemble</strong>
<strong>&gt;&gt;&gt; rforests = []</strong>
<strong>&gt;&gt;&gt; num_trees = [2,4,6,8,32,64,128,256]</strong>
<strong>&gt;&gt;&gt; for n in num_trees:</strong>
<strong> …   rforests.\</strong>
<strong> …   append(ensemble.RandomForestRegressor(n_estimators=n,min_samples_leaf=20).\</strong>
<strong> …   fit(news_features_train, news_shares_train))</strong>
</pre></div><p>Finally, we can start to see some increases in the accuracy of the model when we plot the results using the following code:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; r2_values_rforest = []</strong>
<strong>&gt;&gt;&gt; for f in rforests:</strong>
<strong> …   r2_values_rforest.append(f.score(news_features_test, news_shares_test))</strong>
<strong>&gt;&gt;&gt; plt.plot(num_trees,r2_values_rforest,color='red')</strong>
<strong>&gt;&gt;&gt; plot.xlabel('Number of Trees')</strong>
<strong>&gt;&gt;&gt; plot.ylabel('r-squared')</strong>
</pre></div><div><img src="img/B04881_04_40.jpg" alt="Random forest"/></div><p>Like the linear <a id="id299" class="indexterm"/>regression <a id="id300" class="indexterm"/>model, we can get a ranking of feature importance. While for the linear regression, it is simply the magnitude of the slopes, in the random forest model the importance of features is determined in a more complex manner. Intuitively, if we were to shuffle the values of a particular column among the rows in the dataset, it should decrease the performance of the model if the column is important. By measuring the average effect of this permutation across all trees and dividing by the standard deviation of this effect, we can get can a ranking of the magnitude and consistency of the impact of a variable on the performance of the model. By ranking variables by the degree this randomization has on accuracy, we can derive a measure of feature significance. We can examine the important variables using the following commands to select the feature importance values of the largest random forest. Since the <code class="literal">np.argsort</code> command will by default return a list in ascending order, we use the [<code class="literal">::-1</code>] slice to invert the list order to place the large coefficient values at the beginning.</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; ix = np.argsort(abs(f[5].feature_importances_))[::-1]</strong>
<strong>&gt;&gt;&gt; news_trimmed_features.columns[ix]</strong>
</pre></div><p>This gives the following result:</p><div><pre class="programlisting">
<strong> Index(['kw_avg_avg', 'self_reference_avg_sharess', 'timedelta', 'LDA_01',        'kw_max_avg', 'n_unique_tokens', 'data_channel_is_tech', 'LDA_02',        'self_reference_min_shares', 'n_tokens_content', 'LDA_03', 'kw_avg_max',        'global_rate_negative_words', 'avg_negative_polarity',        'global_rate_positive_words', 'average_token_length', 'num_hrefs',        'is_weekend', 'global_subjectivity', 'kw_avg_min',        'n_non_stop_unique_tokens', 'kw_min_max', 'global_sentiment_polarity',        'kw_max_min', 'LDA_04', 'kw_min_avg', 'min_positive_polarity',        'num_self_hrefs', 'avg_positive_polarity', 'self_reference_max_shares',        'title_sentiment_polarity', 'max_positive_polarity', 'n_tokens_title',        'abs_title_sentiment_polarity', 'abs_title_subjectivity',        'title_subjectivity', 'min_negative_polarity', 'num_imgs',        'data_channel_is_socmed', 'rate_negative_words', 'num_videos',        'max_negative_polarity', 'rate_positive_words', 'kw_min_min',        'num_keywords', 'data_channel_is_entertainment', 'weekday_is_wednesday',        'data_channel_is_lifestyle', 'weekday_is_friday', 'weekday_is_monday',        'kw_max_max', 'data_channel_is_bus', 'data_channel_is_world',        'n_non_stop_words', 'weekday_is_saturday', 'weekday_is_tuesday',        'weekday_is_thursday'],       dtype='object')</strong>
</pre></div><p>Interestingly, if you <a id="id301" class="indexterm"/>compare this <a id="id302" class="indexterm"/>list with the linear regression model, the order is quite different. Promisingly, this suggests that the random forest was able to incorporate patterns that a linear regression cannot capture, resulting in the gains in <em>R<sup>2</sup></em> seen in this section.</p><p>There is also a somewhat subtle problem in this dataset, in the sense that all the categorical variables have been encoded using a binary flag. The variable importance is thus applied individually to each member of a category. If one member of a category is highly correlated with the response while another is not, these individual variables' importance measures give an inaccurate picture of the true variable importance. One solution is to average the resulting values over all categories, a correction which we will not apply for now but raise as a consideration for your future analyses.</p><p>Here we provide a visual flowchart illustrating many of the tradeoffs we have discussed in this chapter on regression analysis. While it is difficult to provide comprehensive rules-of-thumb for all scenarios, it can serve as a starting point for diagnosing which method to apply for a given problem:</p><div><img src="img/B04881_04_41.jpg" alt="Random forest"/><div><p>Flowchart for regression analysis</p></div></div></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch04lvl1sec25"/>Scaling out with PySpark – predicting year of song release</h1></div></div></div><p>To close, let us look at<a id="id303" class="indexterm"/> another example <a id="id304" class="indexterm"/>using PySpark. With this dataset, which is a subset of the Million Song dataset (Bertin-Mahieux, Thierry, et al. "The million song dataset." <em>ISMIR 2011: Proceedings of the 12th International Society for Music Information Retrieval Conference, October 24-28, 2011, Miami, Florida</em>. University of Miami, 2011), the goal is to predict the year of a song's release based on the features of the track. The data is supplied as a comma-separated text file, which we can convert into an RDD using the Spark <code class="literal">textFile()</code> function. As before in our clustering example, we also define a parsing function with a <code class="literal">try…catch</code> block so that we do not fail on a single error in a large dataset:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; def parse_line(l):</strong>
<strong>…      try:</strong>
<strong>…            return l.split(",")</strong>
<strong>…    except:</strong>
<strong>…         print("error in processing {0}".format(l))</strong>
</pre></div><p>We then use this function to map each line to the parsed format, which splits the comma delimited text into individual fields and converts these rows into a Spark DataFrame:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; songs = sc.textFile('/Users/jbabcock/Downloads/YearPredictionMSD.txt').\</strong>
<strong>map(lambda x : parse_line(x)).\</strong>
<strong>toDF()</strong>
</pre></div><p>Since we convert the <a id="id305" class="indexterm"/>resulting RDD into a DataFrame, so that we can access its elements like a list or vector in Python. Next, we want to turn this into a <code class="literal">LabeledPoint</code> RDD, just as we did with the Streaming K-Means example in the previous chapter:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; from pyspark.mllib.regression import LabeledPoint</strong>
<strong>&gt;&gt;&gt; songs_labeled = songs.map( lambda x: LabeledPoint(x[0],x[1:]) )</strong>
</pre></div><p>As part of the documentation for this dataset, we assume that the training data (excluding tracks from artists appearing in the test set) is contained in the first 463,715 rows, while the rest is the test data. To split it, we can use the <code class="literal">zipWithIndex</code> function, which assigns an index to each element in a partition, and across partitions:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; songs_train = songs_labeled.zipWithIndex().\</strong>
<strong>filter( lambda x: x[1] &lt; 463715).\</strong>
<strong>map( lambda x: x[0] )</strong>
<strong>&gt;&gt;&gt; songs_test = songs_labeled.zipWithIndex().\</strong>
<strong>filter( lambda x: x[1] &gt;= 463715).\</strong>
<strong>map( lambda x: x[0] )</strong>
</pre></div><p>Finally, we can train a random forest model on this data using the following commands:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; from pyspark.mllib.tree import RandomForest</strong>
<strong>&gt;&gt;&gt; rf = RandomForest.trainRegressor(songs_train,{},50,"auto","variance",10,32)</strong>
<strong>&gt;&gt;&gt; prediction = rf.predict(songs_test.map(lambda x: x.features))</strong>
<strong>&gt;&gt;&gt; predictedObserved = songs_test.map(lambda lp: lp.label).zip(prediction)</strong>
</pre></div><p>To evaluate the accuracy of the resulting model, we can use the <code class="literal">RegressionMetrics</code> module:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; from pyspark.mllib.evaluation import RegressionMetrics</strong>
<strong>&gt;&gt;&gt; RegressionMetrics(predictedObserved).r2</strong>
</pre></div><p>The distributed nature of PySpark means that this analysis will run on both the single example file on your computer, and on a much larger dataset (such as the full million songs), all using the same code. If we wanted to save the random forest model (for example, if<a id="id306" class="indexterm"/> we want to store a particular day's model for future reference in a database, or distribute this model across multiple machines where it will be loaded from a serialized format), we can use to the <code class="literal">toString()</code> function, which can be potentially compressed using gzip.</p></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch04lvl1sec26"/>Summary</h1></div></div></div><p>In this chapter, we examined the fitting of several regression models, including transforming input variables to the correct scale and accounting for categorical features correctly. In interpreting the coefficients of these models, we examined both cases where the classical assumptions of linear regression are fulfilled and broken. In the latter cases, we examined generalized linear models, GEE, mixed effects models, and time series models as alternative choices for our analyses. In the process of trying to improve the accuracy of our regression model, we fit both simple and regularized linear models. We also examined the use of tree-based regression models and how to optimize parameter choices in fitting them. Finally, we examined an example of using random forest in PySpark, which can be applied to larger datasets.</p><p>In the next chapter, we will examine data that has a discrete categorical outcome, instead of a continuous response. In the process, we will examine in more detail how the likelihood functions of different models are optimized, as well as various algorithms for classification problems.</p></div></div>
</body></html>