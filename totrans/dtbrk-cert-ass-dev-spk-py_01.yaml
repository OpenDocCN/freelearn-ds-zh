- en: '1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overview of the Certification Guide and Exam
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Preparing for any task initially involves comprehending the problem at hand
    thoroughly and, subsequently, devising a strategy to tackle the challenge. Creating
    a step-by-step methodology for addressing each aspect of the challenge is an effective
    approach within this planning phase. This method enables smaller tasks to be handled
    individually, aiding in a systematic progression through the challenges without
    the need to feel overwhelmed.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter intends to demonstrate this step-by-step approach to working through
    your Spark certification exam. In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Overview of the certification exam
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different types of questions to expect in the exam
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview of the rest of the chapters in this book
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ll start by providing an overview of the certification exam.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of the certification exam
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The exam consists of **60 questions**. The time you’re given to attempt these
    questions is **120 minutes**. This gives you about **2 minutes** **per question**.
  prefs: []
  type: TYPE_NORMAL
- en: To pass the exam, you need to have a **score of 70%**, which means that you
    need to **answer 42 questions correctly** out of 60 for you to pass.
  prefs: []
  type: TYPE_NORMAL
- en: If you are well prepared, this time should be enough for you to answer the questions
    and also review them before the time finishes.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will see how the questions are distributed throughout the exam.
  prefs: []
  type: TYPE_NORMAL
- en: Distribution of questions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Exam questions are distributed into the following broad categories. The following
    table provides a breakdown of questions based on different categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Topic** | **Percentage** **of Exam** | **Number** **of Questions** |'
  prefs: []
  type: TYPE_TB
- en: '| Spark Architecture: Understanding of Concepts | 17% | 10 |'
  prefs: []
  type: TYPE_TB
- en: '| Spark Architecture: Understanding of Applications | 11% | 7 |'
  prefs: []
  type: TYPE_TB
- en: '| Spark DataFrame API Applications | 72% | 43 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1.1: Exam breakdown'
  prefs: []
  type: TYPE_NORMAL
- en: Looking at this distribution, you would want to focus on the Spark DataFrame
    API a lot more in your exam preparation since this section covers around 72% of
    the exam (about 43 questions). If you can answer these questions correctly, passing
    the exam will become easier.
  prefs: []
  type: TYPE_NORMAL
- en: But this doesn’t mean that you shouldn’t focus on the Spark architecture areas.
    Spark architecture questions have varied difficulty, and they can sometimes be
    confusing. At the same time, they allow you to score easy points as architecture
    questions are generally straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at some of the other resources available that can help you prepare
    for this exam.
  prefs: []
  type: TYPE_NORMAL
- en: Resources to prepare for the exam
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you start planning to take the certification exam, the first thing you
    must do is master Spark concepts. This book will help you with these concepts.
    Once you’ve done this, it would be useful to do mock exams. There are two mock
    exams available in this book for you to take advantage of.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, Databricks provides a practice exam, which is very useful for
    exam preparation. You can find it here: [https://files.training.databricks.com/assessments/practice-exams/PracticeExam-DCADAS3-Python.pdf](https://files.training.databricks.com/assessments/practice-exams/PracticeExam-DCADAS3-Python.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: Resources available during the exam
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: During the exam, you will be given access to the Spark documentation. This is
    done via **Webassessor** and its interface is a little different than the regular
    Spark documentation you’ll find on the internet. It would be good for you to familiarize
    yourself with this interface. You can find the interface at [https://www.webassessor.com/zz/DATABRICKS/Python_v2.html](https://www.webassessor.com/zz/DATABRICKS/Python_v2.html).
    I recommend going through it and trying to find different packages and functions
    of Spark via this documentation to make yourself comfortable navigating it during
    the exam.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will look at how we can register for the exam.
  prefs: []
  type: TYPE_NORMAL
- en: Registering for your exam
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Databricks is the company that has prepared these exams and certifications.
    Here is the link to register for the exam: [https://www.databricks.com/learn/certification/apache-spark-developer-associate](https://www.databricks.com/learn/certification/apache-spark-developer-associate).'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will look at some of the prerequisites for the exam.
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisites for the exam
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Some prerequisites are needed before you can take the exam so that you can
    be successful in passing the certification. Some of the major ones are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Grasp the fundamentals of Spark architecture, encompassing the principles of
    Adaptive Query Execution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Utilize the Spark DataFrame API proficiently for various data manipulation
    tasks, such as the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing column operations, such as selection, renaming, and manipulation
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Executing row operations, including filtering, dropping, sorting, and aggregating
    data
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Conducting DataFrame-related tasks, such as joining, reading, writing, and implementing
    partitioning strategies
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Demonstrating proficiency in working with **user-defined functions** (**UDFs**)
    and Spark SQL functions
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: While not explicitly tested, a functional understanding of either Python or
    Scala is expected. The examination is available in both programming languages.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hopefully, by the end of this book, you will be able to fully grasp all these
    concepts and have done enough practice on your own to be prepared for the exam
    with full confidence.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s discuss what to expect during the online proctored exam.
  prefs: []
  type: TYPE_NORMAL
- en: Online proctored exam
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Spark certification exam is an online proctored exam. What this means is
    that you will be taking the exam from the comfort of your home, but someone will
    be proctoring the exam online. I encourage you to understand the procedures and
    rules of the proctored exam in advance. This will save you a lot of trouble and
    anxiety at the time of the exam.
  prefs: []
  type: TYPE_NORMAL
- en: 'To give you an overview, throughout the exam session, the following procedures
    will be in place:'
  prefs: []
  type: TYPE_NORMAL
- en: Webcam monitoring will be conducted by a Webassessor proctor to ensure exam
    integrity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will need to present a valid form of identification with a photo
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will need to conduct the exam alone
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your desk needs to be decluttered and there should be no other electronic devices
    in the room except the laptop that you’ll need for the exam
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There should not be any posters or charts on the walls of the room that may
    aid you in the exam
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The proctor will be listening to you during the exam as well, so you’ll want
    to make sure that you’re sitting in a quiet and comfortable environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is recommended to **not** use your work laptop for this exam as it requires
    software to be installed and your antivirus and firewall to be disabled
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The proctor’s responsibilities are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Overseeing your exam session to maintain exam integrity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Addressing any queries related to the exam delivery process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Offering technical assistance if needed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s important to note that the proctor will not offer any form of assistance
    regarding the exam content
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I recommend that you take sufficient time before the exam to set up the environment
    where you’ll be taking the exam. This will ensure a smooth online exam procedure
    where you can focus on the questions and not worry about anything else.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s talk about the different types of questions that may appear in the
    exam.
  prefs: []
  type: TYPE_NORMAL
- en: Types of questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are different categories of questions that you will find in the exam.
    They can be broadly divided into theoretical and code questions. We will look
    at both categories and their respective subcategories in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Theoretical questions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Theoretical questions are the questions where you will be asked about the conceptual
    understanding of certain topics. Theoretical questions can be subdivided further
    into different categories. Let’s look at some of these categories, along with
    example questions taken from previous exams that fall into them.
  prefs: []
  type: TYPE_NORMAL
- en: Explanation questions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Explanation questions are ones where you need to define and explain something.
    It can also include how something works and what it does. Let’s look at an example.
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following describes a worker node?
  prefs: []
  type: TYPE_NORMAL
- en: Worker nodes are the nodes of a cluster that perform computations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Worker nodes are synonymous with executors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Worker nodes always have a one-to-one relationship with executors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Worker nodes are the most granular level of execution in the Spark execution
    hierarchy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Worker nodes are the coarsest level of execution in the Spark execution hierarchy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Connection questions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Connections questions are such questions where you need to define how different
    things are related to each other or how they differ from each other. Let’s look
    at an example to demonstrate this.
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following describes the relationship between worker nodes and executors?
  prefs: []
  type: TYPE_NORMAL
- en: An executor is a **Java Virtual Machine** (**JVM**) running on a worker node.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A worker node is a JVM running on an executor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There are always more worker nodes than executors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There are always the same number of executors and worker nodes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Executors and worker nodes are not related.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scenario question
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Scenario questions involve defining how things work in different if-else scenarios
    – for example, “If ______ occurs, then _____ happens.” Moreover, it also includes
    questions where a statement is incorrect about a scenario. Let’s look at an example
    to demonstrate this.
  prefs: []
  type: TYPE_NORMAL
- en: If Spark is running in cluster mode, which of the following statements about
    nodes is incorrect?
  prefs: []
  type: TYPE_NORMAL
- en: There is a single worker node that contains the Spark driver and the executors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Spark driver runs in its own non-worker node without any executors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each executor is a running JVM inside a worker node.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There is always more than one node.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There might be more executors than total nodes or more total nodes than executors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Categorization questions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Categorization questions are such questions where you need to describe categories
    that something belongs to. Let’s look at an example to demonstrate this.
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following statements accurately describes stages?
  prefs: []
  type: TYPE_NORMAL
- en: Tasks within a stage can be simultaneously executed by multiple machines.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Various stages within a job can run concurrently.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Stages comprise one or more jobs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Stages temporarily store transactions before committing them through actions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Configuration questions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Configuration questions are such questions where you need to outline how things
    will behave based on different cluster configurations. Let’s look at an example
    to demonstrate this.
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following statements accurately describes Spark’s cluster execution
    mode?
  prefs: []
  type: TYPE_NORMAL
- en: Cluster mode runs executor processes on gateway nodes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Cluster mode involves the driver being hosted on a gateway machine.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In cluster mode, the Spark driver and the cluster manager are not co-located.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The driver in cluster mode is located on a worker node.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we’ll look at the code-based questions and their subcategories.
  prefs: []
  type: TYPE_NORMAL
- en: Code-based questions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next category is code-based questions. A large number of Spark API-based
    questions lie in this category. Code-based questions are the questions where you
    will be given a code snippet, and you will be asked questions about it. Code-based
    questions can be subdivided further into different categories. Let’s look at some
    of these categories, along with example questions taken from previous exams that
    fall into these different subcategories.
  prefs: []
  type: TYPE_NORMAL
- en: Function identification questions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Function identification questions are such questions where you need to define
    which function does something. It is important to know the different functions
    that are available in Spark for data manipulation, along with their syntax. Let’s
    look at an example to demonstrate this.
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks returns a copy of the `df` DataFrame, where
    the `column` salary has been renamed `employeeSalary`?
  prefs: []
  type: TYPE_NORMAL
- en: '`df.withColumn(["salary", "employeeSalary"])`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.withColumnRenamed("salary").alias("employeeSalary ")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.withColumnRenamed("salary", "` `employeeSalary ")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`df.withColumn("salary", "` `employeeSalary ")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fill-in-the-blank questions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Fill-in-the-blank questions are such questions where you need to complete the
    code block by filling in the blanks. Let’s look at an example to demonstrate this.
  prefs: []
  type: TYPE_NORMAL
- en: The following code block should return a DataFrame with the `employeeId`, `salary`,
    `bonus`, and `department` columns from the `transactionsDf` DataFrame. Choose
    the answer that correctly fills the blanks to accomplish this.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '`drop`'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '`"employeeId", "salary", "``bonus", "department"`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`filter`'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '`"employeeId, salary,` `bonus, department"`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`select`'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '`["employeeId", "salary", "``bonus", "department"]`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`select`'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '`col(["employeeId", "salary", "``bonus", "department"])`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Order-lines-of-code questions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Order-lines-of-code questions are such questions where you need to place the
    lines of code in a certain order so that you can execute an operation correctly.
    Let’s look at an example to demonstrate this.
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code blocks creates a DataFrame that shows the mean of
    the `salary` column of the `salaryDf` DataFrame based on the `department` and
    `state` columns, where `age` is greater than 35?
  prefs: []
  type: TYPE_NORMAL
- en: '`salaryDf.filter(col("age") >` `35)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`.``filter(col("employeeID")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`.``filter(col("employeeID").isNotNull())`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`.``groupBy("department")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`.``groupBy("department", "state")`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`.``agg(avg("salary").alias("mean_salary"))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`.``agg(average("salary").alias("mean_salary"))`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: i, ii, v, vi
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: i, iii, v, vi
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: i, iii, vi, vii
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: i, ii, iv, vi
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter provided an overview of the certification exam. At this point,
    you know what to expect in the exam and how to best prepare for it. To do so,
    we covered different types of questions that you will encounter.
  prefs: []
  type: TYPE_NORMAL
- en: Going forward, each chapter of this book will equip you with practical knowledge
    and hands-on examples so that you can harness the power of Apache Spark for various
    data processing and analytics tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 2: Introducing Spark'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This part will offer you a comprehensive understanding of Spark’s capabilities
    and operational principles. It will cover what Spark is, why it’s important, and
    some of the applications Spark is most useful in. It will tell you about the different
    types of users who can benefit from Spark. It will also cover the basics of Spark
    architecture and how applications are navigated through in Spark. It will detail
    narrow and wide Spark transformations and discuss lazy evaluations in Spark. It’s
    important to have this understanding because Spark works differently than other
    traditional frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 2*](B19176_02.xhtml#_idTextAnchor030)*, Understanding Apache Spark
    and Its Applications*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 3*](B19176_03.xhtml#_idTextAnchor053)*, Spark Architecture and Transformations*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
