["```py\n>>> from nltk.chunk import tag_pattern2re_pattern\n>>> tag_pattern2re_pattern('<DT>?<NN.*>+')\n'(<(DT)>)?(<(NN[^\\\\{\\\\}<>]*)>)+'\n```", "```py\n>>> from nltk.chunk import RegexpParser\n>>> chunker = RegexpParser(r'''\n... NP:\n...    {<DT><NN.*><.*>*<NN.*>}\n...    }<VB.*>{\n... ''')\n>>> chunker.parse([('the', 'DT'), ('book', 'NN'), ('has', 'VBZ'), ('many', 'JJ'), ('chapters', 'NNS')])\nTree('S', [Tree('NP', [('the', 'DT'), ('book', 'NN')]), ('has', 'VBZ'), Tree('NP', [('many', 'JJ'), ('chapters', 'NNS')])])\n```", "```py\n>>> from nltk.chunk.regexp import ChunkString, ChunkRule, ChinkRule\n>>> from nltk.tree import Tree\n>>> t = Tree('S', [('the', 'DT'), ('book', 'NN'), ('has', 'VBZ'), ('many', 'JJ'), ('chapters', 'NNS')])\n>>> cs = ChunkString(t)\n>>> cs\n<ChunkString: '<DT><NN><VBZ><JJ><NNS>'>\n>>> ur = ChunkRule('<DT><NN.*><.*>*<NN.*>', 'chunk determiners and nouns')\n>>> ur.apply(cs)\n>>> cs\n<ChunkString: '{<DT><NN><VBZ><JJ><NNS>}'>\n>>> ir = ChinkRule('<VB.*>', 'chink verbs')\n>>> ir.apply(cs)\n>>> cs\n<ChunkString: '{<DT><NN>}<VBZ>{<JJ><NNS>}'>\n>>> cs.to_chunkstruct()\nTree('S', [Tree('CHUNK', [('the', 'DT'), ('book', 'NN')]), ('has', 'VBZ'), Tree('CHUNK', [('many', 'JJ'), ('chapters', 'NNS')])])\n```", "```py\n>>> from nltk.chunk import RegexpChunkParser\n>>> chunker = RegexpChunkParser([ur, ir])\n>>> chunker.parse(t)\nTree('S', [Tree('NP', [('the', 'DT'), ('book', 'NN')]), ('has', 'VBZ'), Tree('NP', [('many', 'JJ'), ('chapters', 'NNS')])])\n```", "```py\n>>> from nltk.chunk import RegexpChunkParser\n>>> chunker = RegexpChunkParser([ur, ir], chunk_node='CP')\n>>> chunker.parse(t)\nTree('S', [Tree('CP', [('the', 'DT'), ('book', 'NN')]), ('has', 'VBZ'), Tree('CP', [('many', 'JJ'), ('chapters', 'NNS')])])\n```", "```py\n>>> chunker = RegexpParser(r'''\n... NP:\n...    {<DT><NN.*>}\n...    {<JJ><NN.*>}\n... ''')\n>>> chunker.parse(t)\nTree('S', [Tree('NP', [('the', 'DT'), ('book', 'NN')]), ('has', 'VBZ'), Tree('NP', [('many', 'JJ'), ('chapters', 'NNS')])])\n```", "```py\n>>> chunker = RegexpParser(r'''\n... NP:\n...    {(<DT>|<JJ>)<NN.*>}\n... ''')\n>>> chunker.parse(t)\nTree('S', [Tree('NP', [('the', 'DT'), ('book', 'NN')]), ('has', 'VBZ'), Tree('NP', [('many', 'JJ'), ('chapters', 'NNS')])])\n```", "```py\n>>> from nltk.chunk.regexp import ChunkRuleWithContext\n>>> ctx = ChunkRuleWithContext('<DT>', '<NN.*>', '<.*>', 'chunk nouns only after determiners')\n>>> cs = ChunkString(t)\n>>> cs\n<ChunkString: '<DT><NN><VBZ><JJ><NNS>'>\n>>> ctx.apply(cs)\n>>> cs\n<ChunkString: '<DT>{<NN>}<VBZ><JJ><NNS>'>\n>>> cs.to_chunkstruct()\nTree('S', [('the', 'DT'), Tree('CHUNK', [('book', 'NN')]), ('has', 'VBZ'), ('many', 'JJ'), ('chapters', 'NNS')])\n```", "```py\n>>> chunker = RegexpParser(r'''\n... NP:\n...    <DT>{<NN.*>}\n... ''')\n>>> chunker.parse(t)\nTree('S', [('the', 'DT'), Tree('NP', [('book', 'NN')]), ('has', 'VBZ'), ('many', 'JJ'), ('chapters', 'NNS')])\n```", "```py\n>>> chunker = RegexpParser(r'''\n... NP:\n...     {<DT><.*>*<NN.*>}\n...     <NN.*>}{<.*>\n...     <.*>}{<DT>\n...     <NN.*>{}<NN.*>\n... ''')\n>>> sent = [('the', 'DT'), ('sushi', 'NN'), ('roll', 'NN'), ('was', 'VBD'), ('filled', 'VBN'), ('with', 'IN'), ('the', 'DT'), ('fish', 'NN')]\n>>> chunker.parse(sent)\nTree('S', [Tree('NP', [('the', 'DT'), ('sushi', 'NN'), ('roll', 'NN')]), Tree('NP', [('was', 'VBD'), ('filled', 'VBN'), ('with', 'IN')]), Tree('NP', [('the', 'DT'), ('fish', 'NN')])])\n```", "```py\n>>> from nltk.chunk.regexp import MergeRule, SplitRule\n>>> cs = ChunkString(Tree('S', sent))\n>>> cs\n<ChunkString: '<DT><NN><NN><VBD><VBN><IN><DT><NN>'>\n>>> ur = ChunkRule('<DT><.*>*<NN.*>', 'chunk determiner to noun')\n>>> ur.apply(cs)\n>>> cs\n<ChunkString: '{<DT><NN><NN><VBD><VBN><IN><DT><NN>}'>\n>>> sr1 = SplitRule('<NN.*>', '<.*>', 'split after noun')\n>>> sr1.apply(cs)\n>>> cs\n<ChunkString: '{<DT><NN>}{<NN>}{<VBD><VBN><IN><DT><NN>}'>\n>>> sr2 = SplitRule('<.*>', '<DT>', 'split before determiner')\n>>> sr2.apply(cs)\n>>> cs\n<ChunkString: '{<DT><NN>}{<NN>}{<VBD><VBN><IN>}{<DT><NN>}'>\n>>> mr = MergeRule('<NN.*>', '<NN.*>', 'merge nouns')\n>>> mr.apply(cs)\n>>> cs\n<ChunkString: '{<DT><NN><NN>}{<VBD><VBN><IN>}{<DT><NN>}'>\n>>> cs.to_chunkstruct()\nTree('S', [Tree('CHUNK', [('the', 'DT'), ('sushi', 'NN'), ('roll', 'NN')]), Tree('CHUNK', [('was', 'VBD'), ('filled', 'VBN'), ('with', 'IN')]), Tree('CHUNK', [('the', 'DT'), ('fish', 'NN')])])\n```", "```py\n>>> from nltk.chunk.regexp import RegexpChunkRule\n>>> RegexpChunkRule.parse('{<DT><.*>*<NN.*>}')\n<ChunkRule: '<DT><.*>*<NN.*>'>\n>>> RegexpChunkRule.parse('<.*>}{<DT>')\n<SplitRule: '<.*>', '<DT>'>\n>>> RegexpChunkRule.parse('<NN.*>{}<NN.*>')\n<MergeRule: '<NN.*>', '<NN.*>'>\n```", "```py\n>>> RegexpChunkRule.parse('{<DT><.*>*<NN.*>} # chunk everything').descr()\n'chunk everything'\n>>> RegexpChunkRule.parse('{<DT><.*>*<NN.*>}').descr()\n''\n```", "```py\n>>> from nltk.chunk.regexp import ChunkRule, ExpandLeftRule, ExpandRightRule, UnChunkRule\n>>> from nltk.chunk import RegexpChunkParser\n>>> ur = ChunkRule('<NN>', 'single noun')\n>>> el = ExpandLeftRule('<DT>', '<NN>', 'get left determiner')\n>>> er = ExpandRightRule('<NN>', '<NNS>', 'get right plural noun')\n>>> un = UnChunkRule('<DT><NN.*>*', 'unchunk everything')\n>>> chunker = RegexpChunkParser([ur, el, er, un])\n>>> sent = [('the', 'DT'), ('sushi', 'NN'), ('rolls', 'NNS')]\n>>> chunker.parse(sent)\nTree('S', [('the', 'DT'), ('sushi', 'NN'), ('rolls', 'NNS')])\n```", "```py\n>>> from nltk.chunk.regexp import ChunkString\n>>> from nltk.tree import Tree\n>>> cs = ChunkString(Tree('S', sent))\n>>> cs\n<ChunkString: '<DT><NN><NNS>'>\n>>> ur.apply(cs)\n>>> cs\n<ChunkString: '<DT>{<NN>}<NNS>'>\n>>> el.apply(cs)\n>>> cs\n<ChunkString: '{<DT><NN>}<NNS>'>\n>>> er.apply(cs)\n>>> cs\n<ChunkString: '{<DT><NN><NNS>}'>\n>>> un.apply(cs)\n>>> cs\n<ChunkString: '<DT><NN><NNS>'>\n```", "```py\n>>> chunker = RegexpParser(r'''\n... NP:\n... {<DT>?<NN.*>+}  # chunk optional determiner with nouns\n... <JJ>{}<NN.*>  # merge adjective with noun chunk\n... PP:\n... {<IN>}  # chunk preposition\n... VP:\n... {<MD>?<VB.*>}  # chunk optional modal with verb\n... ''')\n>>> from nltk.corpus import conll2000\n>>> score = chunker.evaluate(conll2000.chunked_sents())\n>>> score.accuracy()\n0.61485735457576884\n```", "```py\n>>> from nltk.corpus import treebank_chunk\n>>> treebank_score = chunker.evaluate(treebank_chunk.chunked_sents())\n>>> treebank_score.accuracy()\n0.49033970276008493\n```", "```py\n>>> score.precision()\n0.60201948127375005\n>>> score.recall()\n0.60607250250584699\n```", "```py\n>>> len(score.missed())\n47161\n>>> len(score.incorrect())\n47967\n>>> len(score.correct())\n119720\n>>> len(score.guessed())\n120526\n```", "```py\nimport nltk.chunk, itertools\nfrom nltk.tag import UnigramTagger, BigramTagger\nfrom tag_util import backoff_tagger\n\ndef conll_tag_chunks(chunk_sents):\n  tagged_sents = [nltk.chunk.tree2conlltags(tree) for tree in chunk_sents]\n  return [[(t, c) for (w, t, c) in sent] for sent in tagged_sents]\n\nclass TagChunker(nltk.chunk.ChunkParserI):\n  def __init__(self, train_chunks, tagger_classes=[UnigramTagger, BigramTagger]):\n    train_sents = conll_tag_chunks(train_chunks)\n    self.tagger = backoff_tagger(train_sents, tagger_classes)\n\n  def parse(self, tagged_sent):\n    if not tagged_sent: return None\n    (words, tags) = zip(*tagged_sent)\n    chunks = self.tagger.tag(tags)\n    wtc = itertools.izip(words, chunks)\n    return nltk.chunk.conlltags2tree([(w,t,c) for (w,(t,c)) in wtc])\n```", "```py\n>>> from chunkers import TagChunker\n>>> from nltk.corpus import treebank_chunk\n>>> train_chunks = treebank_chunk.chunked_sents()[:3000]\n>>> test_chunks = treebank_chunk.chunked_sents()[3000:]\n>>> chunker = TagChunker(train_chunks)\n>>> score = chunker.evaluate(test_chunks)\n>>> score.accuracy()\n0.97320393352514278\n>>> score.precision()\n0.91665343705350055\n>>> score.recall()\n0.9465573770491803\n```", "```py\n>>> import nltk.chunk\n>>> from nltk.tree import Tree\n>>> t = Tree('S', [Tree('NP', [('the', 'DT'), ('book', 'NN')])])\n>>> nltk.chunk.tree2conlltags(t)\n[('the', 'DT', 'B-NP'), ('book', 'NN', 'I-NP')]\n>>> nltk.chunk.conlltags2tree([('the', 'DT', 'B-NP'), ('book', 'NN', 'I-NP')])\nTree('S', [Tree('NP', [('the', 'DT'), ('book', 'NN')])])\n```", "```py\n>>> conll_tag_chunks([t])\n[[('DT', 'B-NP'), ('NN', 'I-NP')]]\n```", "```py\n>>> from nltk.corpus import conll2000\n>>> conll_train = conll2000.chunked_sents('train.txt')\n>>> conll_test = conll2000.chunked_sents('test.txt')\n>>> chunker = TagChunker(conll_train)\n>>> score = chunker.evaluate(conll_test)\n>>> score.accuracy()\n0.89505456234037617\n>>> score.precision()\n0.81148419743556754\n>>> score.recall()\n0.86441916769448635\n```", "```py\n>>> from nltk.tag import UnigramTagger\n>>> uni_chunker = TagChunker(train_chunks, tagger_classes=[UnigramTagger])\n>>> score = uni_chunker.evaluate(test_chunks)\n>>> score.accuracy()\n0.96749259243354657\n```", "```py\nimport nltk.chunk\nfrom nltk.tag import ClassifierBasedTagger\n\ndef chunk_trees2train_chunks(chunk_sents):\n  tag_sents = [nltk.chunk.tree2conlltags(sent) for sent in chunk_sents]\n  return [[((w,t),c) for (w,t,c) in sent] for sent in tag_sents]\n```", "```py\ndef prev_next_pos_iob(tokens, index, history):\n  word, pos = tokens[index]\n\n  if index == 0:\n    prevword, prevpos, previob = ('<START>',)*3\n  else:\n    prevword, prevpos = tokens[index-1]\n    previob = history[index-1]\n\n  if index == len(tokens) - 1:\n    nextword, nextpos = ('<END>',)*2\n  else:\n    nextword, nextpos = tokens[index+1]\n\n  feats = {\n    'word': word,\n    'pos': pos,\n    'nextword': nextword,\n    'nextpos': nextpos,\n    'prevword': prevword,\n    'prevpos': prevpos,\n    'previob': previob\n  }\n  return feats\n```", "```py\nclass ClassifierChunker(nltk.chunk.ChunkParserI):\n  def __init__(self, train_sents, feature_detector=prev_next_pos_iob, **kwargs):\n    if not feature_detector:\n      feature_detector = self.feature_detector\n\n    train_chunks = chunk_trees2train_chunks(train_sents)\n    self.tagger = ClassifierBasedTagger(train=train_chunks,\n      feature_detector=feature_detector, **kwargs)\n\n  def parse(self, tagged_sent):\n    if not tagged_sent: return None\n    chunks = self.tagger.tag(tagged_sent)\n    return nltk.chunk.conlltags2tree([(w,t,c) for ((w,t),c) in chunks])\n```", "```py\n>>> from chunkers import ClassifierChunker\n>>> chunker = ClassifierChunker(train_chunks)\n>>> score = chunker.evaluate(test_chunks)\n>>> score.accuracy()\n0.97217331558380216\n>>> score.precision()\n0.92588387933830685\n>>> score.recall()\n0.93590163934426229\n```", "```py\n>>> chunker = ClassifierChunker(conll_train)\n>>> score = chunker.evaluate(conll_test)\n>>> score.accuracy()\n0.92646220740021534\n>>> score.precision()\n0.87379243109102189\n>>> score.recall()\n0.90073546206203459\n```", "```py\n>>> from nltk.classify import MaxentClassifier\n>>> builder = lambda toks: MaxentClassifier.train(toks, trace=0, max_iter=10, min_lldelta=0.01)\n>>> me_chunker = ClassifierChunker(train_chunks, classifier_builder=builder)\n>>> score = me_chunker.evaluate(test_chunks)\n>>> score.accuracy()\n0.9748357452655988\n>>> score.precision()\n0.93794355504208615\n>>> score.recall()\n0.93163934426229511\n```", "```py\n>>> from nltk.chunk import ne_chunk\n>>> ne_chunk(treebank_chunk.tagged_sents()[0])\nTree('S', [Tree('PERSON', [('Pierre', 'NNP')]), Tree('ORGANIZATION', [('Vinken', 'NNP')]), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')])\n```", "```py\ndef sub_leaves(tree, node):\n  return [t.leaves() for t in tree.subtrees(lambda s: s.node == node)]\n```", "```py\n>>> tree = ne_chunk(treebank_chunk.tagged_sents()[0])\n>>> from chunkers import sub_leaves\n>>> sub_leaves(tree, 'PERSON')\n[[('Pierre', 'NNP')]]\n>>> sub_leaves(tree, 'ORGANIZATION')\n[[('Vinken', 'NNP')]]\n```", "```py\n>>> from nltk.chunk import batch_ne_chunk\n>>> trees = batch_ne_chunk(treebank_chunk.tagged_sents()[:10])\n>>> [sub_leaves(t, 'ORGANIZATION') for t in trees]\n[[[('Vinken', 'NNP')]], [[('Elsevier', 'NNP')]], [[('Consolidated', 'NNP'), ('Gold', 'NNP'), ('Fields', 'NNP')]], [], [], [[('Inc.', 'NNP')], [('Micronite', 'NN')]], [[('New', 'NNP'), ('England', 'NNP'), ('Journal', 'NNP')]], [[('Lorillard', 'NNP')]], [], []]\n```", "```py\n>>> ne_chunk(treebank_chunk.tagged_sents()[0], binary=True)\nTree('S', [Tree('NE', [('Pierre', 'NNP'), ('Vinken', 'NNP')]), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')])\n```", "```py\n>>> sub_leaves(ne_chunk(treebank_chunk.tagged_sents()[0], binary=True), 'NE')\n[[('Pierre', 'NNP'), ('Vinken', 'NNP')]]\n```", "```py\n>>> chunker = RegexpParser(r'''\n... NAME:\n...   {<NNP>+}\n... ''')\n>>> sub_leaves(chunker.parse(treebank_chunk.tagged_sents()[0]), 'NAME')\n[[('Pierre', 'NNP'), ('Vinken', 'NNP')], [('Nov.', 'NNP')]]\n```", "```py\nimport nltk.chunk\nfrom nltk.corpus import names\n\nclass PersonChunker(nltk.chunk.ChunkParserI):\n  def __init__(self):\n    self.name_set = set(names.words())\n\n  def parse(self, tagged_sent):\n    iobs = []\n    in_person = False\n\n    for word, tag in tagged_sent:\n      if word in self.name_set and in_person:\n        iobs.append((word, tag, 'I-PERSON'))\n      elif word in self.name_set:\n        iobs.append((word, tag, 'B-PERSON'))\n        in_person = True\n      else:\n        iobs.append((word, tag, 'O'))\n        in_person = False\n\n    return nltk.chunk.conlltags2tree(iobs)\n```", "```py\n>>> from chunkers import PersonChunker\n>>> chunker = PersonChunker()\n>>> sub_leaves(chunker.parse(treebank_chunk.tagged_sents()[0]), 'PERSON')\n[[('Pierre', 'NNP')]]\n```", "```py\nimport nltk.chunk\nfrom nltk.corpus import gazetteers\n\nclass LocationChunker(nltk.chunk.ChunkParserI):\n  def __init__(self):\n    self.locations = set(gazetteers.words())\n    self.lookahead = 0\n\n    for loc in self.locations:\n      nwords = loc.count(' ')\n\n      if nwords > self.lookahead:\n        self.lookahead = nwords\n\n  def iob_locations(self, tagged_sent):\n    i = 0\n    l = len(tagged_sent)\n    inside = False\n\n    while i < l:\n      word, tag = tagged_sent[i]\n      j = i + 1\n      k = j + self.lookahead\n      nextwords, nexttags = [], []\n      loc = False\n\n      while j < k:\n        if ' '.join([word] + nextwords) in self.locations:\n          if inside:\n            yield word, tag, 'I-LOCATION'\n          else:\n            yield word, tag, 'B-LOCATION'\n\n          for nword, ntag in zip(nextwords, nexttags):\n            yield nword, ntag, 'I-LOCATION'\n\n          loc, inside = True, True\n          i = j\n          break\n\n        if j < l:\n          nextword, nexttag = tagged_sent[j]\n          nextwords.append(nextword)\n          nexttags.append(nexttag)\n          j += 1\n        else:\n          break\n\n      if not loc:\n        inside = False\n        i += 1\n        yield word, tag, 'O'\n\n  def parse(self, tagged_sent):\n    iobs = self.iob_locations(tagged_sent)\n    return nltk.chunk.conlltags2tree(iobs)\n```", "```py\n>>> from chunkers import LocationChunker\n>>> t = loc.parse([('San', 'NNP'), ('Francisco', 'NNP'), ('CA', 'NNP'), ('is', 'BE'), ('cold', 'JJ'), ('compared', 'VBD'), ('to', 'TO'), ('San', 'NNP'), ('Jose', 'NNP'), ('CA', 'NNP')])\n>>> sub_leaves(t, 'LOCATION')\n[[('San', 'NNP'), ('Francisco', 'NNP'), ('CA', 'NNP')], [('San', 'NNP'), ('Jose', 'NNP'), ('CA', 'NNP')]]\n```", "```py\nimport nltk.tag, nltk.chunk, itertools\nfrom nltk.corpus import ieer\n\ndef ieertree2conlltags(tree, tag=nltk.tag.pos_tag):\n  words, ents = zip(*tree.pos())\n  iobs = []\n  prev = None\n\n  for ent in ents:\n    if ent == tree.node:\n      iobs.append('O')\n      prev = None\n    elif prev == ent:\n      iobs.append('I-%s' % ent)\n    else:\n      iobs.append('B-%s' % ent)\n      prev = ent\n\n  words, tags = zip(*tag(words))\n  return itertools.izip(words, tags, iobs)\n\ndef ieer_chunked_sents(tag=nltk.tag.pos_tag):\n  for doc in ieer.parsed_docs():\n    tagged = ieertree2conlltags(doc.text, tag)\n    yield nltk.chunk.conlltags2tree(tagged)\n```", "```py\n>>> from chunkers import ieer_chunked_sents, ClassifierChunker\n>>> from nltk.corpus import treebank_chunk\n>>> ieer_chunks = list(ieer_chunked_sents())\n>>> len(ieer_chunks)\n94\n>>> chunker = ClassifierChunker(ieer_chunks[:80])\n>>> chunker.parse(treebank_chunk.tagged_sents()[0])\nTree('S', [Tree('LOCATION', [('Pierre', 'NNP'), ('Vinken', 'NNP')]), (',', ','), Tree('DURATION', [('61', 'CD'), ('years', 'NNS')]), Tree('MEASURE', [('old', 'JJ')]), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), Tree('DATE', [('Nov.', 'NNP'), ('29', 'CD')]), ('.', '.')])\n```", "```py\n>>> score = chunker.evaluate(ieer_chunks[80:])\n>>> score.accuracy()\n0.88290183880706252\n>>> score.precision()\n0.40887174541947929\n>>> score.recall()\n0.50536352800953521\n```", "```py\n>>> from nltk.corpus import ieer\n>>> ieer.parsed_docs()[0].headline\nTree('DOCUMENT', ['Kenyans', 'protest', 'tax', 'hikes'])\n```"]