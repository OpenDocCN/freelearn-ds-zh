["```py\n>>> import execnet, remote_tag, nltk.tag, nltk.data\n>>> from nltk.corpus import treebank\n>>> import cPickle as pickle\n>>> tagger = pickle.dumps(nltk.data.load(nltk.tag._POS_TAGGER))\n>>> gw = execnet.makegateway()\n>>> channel = gw.remote_exec(remote_tag)\n>>> channel.send(tagger)\n>>> channel.send(treebank.sents()[0])\n>>> tagged_sentence = channel.receive()\n>>> tagged_sentence == treebank.tagged_sents()[0]\nTrue\n>>> gw.exit()\n```", "```py\n  import cPickle as pickle\n\n  if __name__ == '__channelexec__':\n    tagger = pickle.loads(channel.receive())\n\n    for sentence in channel:\n      channel.send(tagger.tag(sentence))\n```", "```py\n>>> import itertools\n>>> gw1 = execnet.makegateway()\n>>> gw2 = execnet.makegateway()\n>>> ch1 = gw1.remote_exec(remote_tag)\n>>> ch1.send(tagger)\n>>> ch2 = gw2.remote_exec(remote_tag)\n>>> ch2.send(tagger)\n>>> mch = execnet.MultiChannel([ch1, ch2])\n>>> queue = mch.make_receive_queue()\n>>> channels = itertools.cycle(mch)\n>>> for sentence in treebank.sents()[:4]:\n...    channel = channels.next()\n...    channel.send(sentence)\n>>> tagged_sentences = []\n>>> for i in range(4):\n...    channel, tagged_sentence = queue.get()\n...    tagged_sentences.append(tagged_sentence)\n>>> len(tagged_sentences)\n4\n>>> gw1.exit()\n>>> gw2.exit()\n```", "```py\n>>> import execnet, remote_chunk\n>>> import nltk.data, nltk.tag, nltk.chunk\n>>> import cPickle as pickle\n>>> from nltk.corpus import treebank_chunk\n>>> tagger = pickle.dumps(nltk.data.load(nltk.tag._POS_TAGGER))\n>>> chunker = pickle.dumps(nltk.data.load(nltk.chunk._MULTICLASS_NE_CHUNKER))\n>>> gw = execnet.makegateway()\n>>> channel = gw.remote_exec(remote_chunk)\n>>> channel.send(tagger)\n>>> channel.send(chunker)\n>>> channel.send(treebank_chunk.sents()[0])\n>>> chunk_tree = pickle.loads(channel.receive())\n>>> chunk_tree\nTree('S', [Tree('PERSON', [('Pierre', 'NNP')]), Tree('ORGANIZATION', [('Vinken', 'NNP')]), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')])\n>>> gw.exit()\n```", "```py\nimport cPickle as pickle\n\nif __name__ == '__channelexec__':\n  tagger = pickle.loads(channel.receive())\n  chunker = pickle.loads(channel.receive())\n\n  for sent in channel:\n    tree = chunker.parse(tagger.tag(sent))\n    channel.send(pickle.dumps(tree))\n```", "```py\nif __name__ == '__channelexec__':\n  for (i, arg) in channel:\n    channel.send((i, arg * 2))\n```", "```py\n>>> import plists, remote_double\n>>> plists.map(remote_double, range(10))\n[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n```", "```py\nimport itertools, execnet\n\ndef map(mod, args, specs=[('popen', 2)]):\n  gateways = []\n  channels = []\n\n  for spec, count in specs:\n    for i in range(count):\n      gw = execnet.makegateway(spec)\n      gateways.append(gw)\n      channels.append(gw.remote_exec(mod))\n\n  cyc = itertools.cycle(channels)\n\n  for i, arg in enumerate(args):\n    channel = cyc.next()\n    channel.send((i, arg))\n\n  mch = execnet.MultiChannel(channels)\n  queue = mch.make_receive_queue()\n  l = len(args)\n  results = [None] * l\n\n  for j in range(l):\n    channel, (i, result) = queue.get()\n    results[i] = result\n\n  for gw in gateways:\n    gw.exit()\n\n  return results\n```", "```py\n>>> plists.map(remote_double, range(10), [('popen', 4)])\n[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n```", "```py\nfrom rediscollections import RedisHashMap\n\nclass RedisHashFreqDist(RedisHashMap):\n  def inc(self, sample, count=1):\n    self._r.hincrby(self._name, sample, count)\n\n  def N(self):\n    return int(sum(self.values()))\n\n  def __getitem__(self, key):\n    return int(RedisHashMap.__getitem__(self, key) or 0)\n\n  def values(self):\n    return [int(v) for v in RedisHashMap.values(self)]\n\n  def items(self):\n    return [(k, int(v)) for (k, v) in RedisHashMap.items(self)]\n```", "```py\n>>> from redis import Redis\n>>> from redisprob import RedisHashFreqDist\n>>> r = Redis()\n>>> rhfd = RedisHashFreqDist(r, 'test')\n>>> len(rhfd)\n0\n>>> rhfd.inc('foo')\n>>> rhfd['foo']\n1\n>>> rhfd.items()\n>>> len(rhfd)\n1\n```", "```py\nimport collections, re\n\nwhite = r'[\\s&]+'\n\ndef encode_key(key):\n  return re.sub(white, '_', key.strip())\n\nclass RedisHashMap(collections.MutableMapping):\n  def __init__(self, r, name):\n    self._r = r\n    self._name = encode_key(name)\n\n  def __iter__(self):\n    return iter(self.items())\n\n  def __len__(self):\n    return self._r.hlen(self._name)\n\n  def __contains__(self, key):\n    return self._r.hexists(self._name, encode_key(key))\n\n  def __getitem__(self, key):\n    return self._r.hget(self._name, encode_key(key))\n\n  def __setitem__(self, key, val):\n    self._r.hset(self._name, encode_key(key), val)\n\n  def __delitem__(self, key):\n    self._r.hdel(self._name, encode_key(key))\n\n  def keys(self):\n    return self._r.hkeys(self._name)\n\n  def values(self):\n    return self._r.hvals(self._name)\n\n  def items(self):\n    return self._r.hgetall(self._name).items()\n\n  def get(self, key, default=0):\n    return self[key] or default\n\n  def iteritems(self):\n    return iter(self)\n\n  def clear(self):\n    self._r.delete(self._name)\n```", "```py\nfrom nltk.probability import ConditionalFreqDist\nfrom rediscollections import encode_key\n\nclass RedisConditionalHashFreqDist(ConditionalFreqDist):\n  def __init__(self, r, name, cond_samples=None):\n    self._r = r\n    self._name = name\n    ConditionalFreqDist.__init__(self, cond_samples)\n    # initialize self._fdists for all matching keys\n    for key in self._r.keys(encode_key('%s:*' % name)):\n      condition = key.split(':')[1]\n      self[condition] # calls self.__getitem__(condition)\n\n  def __contains__(self, condition):\n    return encode_key(condition) in self._fdists\n\n  def __getitem__(self, condition):\n    if condition not in self._fdists:\n      key = '%s:%s' % (self._name, condition)\n      self._fdists[condition] = RedisHashFreqDist(self._r, key)\n\n    return self._fdists[condition]\n\n  def clear(self):\n    for fdist in self._fdists.values():\n      fdist.clear()\n```", "```py\n>>> from redis import Redis\n>>> from redisprob import RedisConditionalHashFreqDist\n>>> r = Redis()\n>>> rchfd = RedisConditionalHashFreqDist(r, 'condhash')\n>>> rchfd.N()\n0\n>>> rchfd.conditions()\n[]\n>>> rchfd['cond1'].inc('foo')\n>>> rchfd.N()\n1\n>>> rchfd['cond1']['foo']\n1\n>>> rchfd.conditions()\n['cond1']\n>>> rchfd.clear()\n```", "```py\nclass RedisOrderedDict(collections.MutableMapping):\n  def __init__(self, r, name):\n    self._r = r\n    self._name = encode_key(name)\n\n  def __iter__(self):\n    return iter(self.items())\n\n  def __len__(self):\n    return self._r.zcard(self._name)\n\n  def __getitem__(self, key):\n    val = self._r.zscore(self._name, encode_key(key))\n\n    if val is None:\n      raise KeyError\n    else:\n      return val\n\n  def __setitem__(self, key, score):\n    self._r.zadd(self._name, encode_key(key), score)\n\n  def __delitem__(self, key):by brain feels dead\n\n    self._r.zrem(self._name, encode_key(key))\n\n  def keys(self, start=0, end=-1):\n    # we use zrevrange to get keys sorted by high value instead of by lowest\n    return self._r.zrevrange(self._name, start, end)\n\n  def values(self, start=0, end=-1):\n    return [v for (k, v) in self.items(start=start, end=end)]\n\n  def items(self, start=0, end=-1):\n    return self._r.zrevrange(self._name, start, end, withscores=True)\n\n  def get(self, key, default=0):\n    return self[key] or default\n\n  def iteritems(self):\n```", "```py\n    return iter(self)\n\n  def clear(self):\n    self._r.delete(self._name)\n```", "```py\n>>> from redis import Redis\n>>> from rediscollections import RedisOrderedDict\n>>> r = Redis()\n>>> rod = RedisOrderedDict(r, 'test.txt')\n>>> rod.get('bar')\n>>> len(rod)\n0\n>>> rod['bar'] = 5.2\n>>> rod['bar']\n5.2000000000000002\n>>> len(rod)\n1\n>>> rod.items()\n[('bar', 5.2000000000000002)]\n>>> rod.clear()\n```", "```py\n>>> from redis import Redis\n>>> from rediscollections import RedisOrderedDict\n>>> r = Redis()\n>>> rod = RedisOrderedDict(r, 'scores')\n>>> rod['best'] = 10\n>>> rod['worst'] = 0.1\n>>> rod['middle'] = 5\n>>> rod.keys()\n['best', 'middle', 'worst']\n>>> rod.keys(start=0, end=1)\n['best', 'middle']\n>>> rod.clear()\n```", "```py\n>>> from dist_featx import score_words\n>>> from nltk.corpus import movie_reviews\n>>> labels = movie_reviews.categories()\n>>> labelled_words = [(l, movie_reviews.words(categories=[l])) for l in labels]\n>>> word_scores = score_words(labelled_words)\n>>> len(word_scores)\n39764\n>>> topn_words = word_scores.keys(end=1000)\n>>> topn_words[0:5]\n['_', 'bad', '?', 'movie', 't']\n>>> from redis import Redis\n>>> r = Redis()\n>>> [r.delete(key) for key in ['word_fd', 'label_word_fd:neg', 'label_word_fd:pos', 'word_scores']]\n[True, True, True, True]\n```", "```py\nimport itertools, execnet, remote_word_count\nfrom nltk.metrics import BigramAssocMeasures\nfrom redis import Redis\nfrom redisprob import RedisHashFreqDist, RedisConditionalHashFreqDist\nfrom rediscollections import RedisOrderedDict\n\ndef score_words(labelled_words, score_fn=BigramAssocMeasures.chi_sq, host='localhost', specs=[('popen', 2)]):\n  gateways = []\n  channels = []\n\n  for spec, count in specs:\n    for i in range(count):\n      gw = execnet.makegateway(spec)\n      gateways.append(gw)\n      channel = gw.remote_exec(remote_word_count)\n      channel.send((host, 'word_fd', 'label_word_fd'))\n      channels.append(channel)\n\n  cyc = itertools.cycle(channels)\n\n  for label, words in labelled_words:\n    channel = cyc.next()\n    channel.send((label, list(words)))\n\n  for channel in channels:\n    channel.send('done')\n    assert 'done' == channel.receive()\n    channel.waitclose(5)\n\n  for gateway in gateways:\n    gateway.exit()\n\n  r = Redis(host)\n  fd = RedisHashFreqDist(r, 'word_fd')\n  cfd = RedisConditionalHashFreqDist(r, 'label_word_fd')\n  word_scores = RedisOrderedDict(r, 'word_scores')\n  n_xx = cfd.N()\n\n  for label in cfd.conditions():\n    n_xi = cfd[label].N()\n\n    for word, n_ii in cfd[label].iteritems():\n      n_ix = fd[word]\n\n      if n_ii and n_ix and n_xi and n_xx:\n        score = score_fn(n_ii, (n_ix, n_xi), n_xx)\n        word_scores[word] = score\n\n  return word_scores\n```", "```py\nfrom redis import Redis\nfrom redisprob import RedisHashFreqDist, RedisConditionalHashFreqDist\n\nif __name__ == '__channelexec__':\n  host, fd_name, cfd_name = channel.receive()\n  r = Redis(host)\n  fd = RedisHashFreqDist(r, fd_name)\n  cfd = RedisConditionalHashFreqDist(r, cfd_name)\n\n  for data in channel:\n    if data == 'done':\n      channel.send('done')\n      break\n\n    label, words = data\n\n    for word in words:\n      fd.inc(word)\n      cfd[label].inc(word)\n```"]