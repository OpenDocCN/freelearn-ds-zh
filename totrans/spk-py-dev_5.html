<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch05"/>Chapter 5. Streaming Live Data with Spark</h1></div></div></div><p>In this chapter, we will focus on live streaming data flowing into Spark and processing it. So far, we have discussed machine learning and data mining with batch processing. We are now looking at processing continuously flowing data and detecting facts and patterns on the fly. We are navigating from a lake to a river.</p><p>We will first investigate the challenges arising from such a dynamic and ever changing environment. After laying the grounds on the prerequisite of a streaming application, we will investigate various implementations using live sources of data such as TCP sockets to the Twitter firehose and put in place a low latency, high throughput, and scalable data pipeline combining Spark, Kafka and Flume.</p><p>In this chapter, we will cover the following points:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Analyzing a streaming application's architectural challenges, constraints, and requirements</li><li class="listitem" style="list-style-type: disc">Processing live data from a TCP socket with Spark Streaming</li><li class="listitem" style="list-style-type: disc">Connecting to the Twitter firehose directly to parse tweets in quasi real time</li><li class="listitem" style="list-style-type: disc">Establishing a reliable, fault tolerant, scalable, high throughput, low latency integrated application using Spark, Kafka, and Flume</li><li class="listitem" style="list-style-type: disc">Closing remarks on Lambda and Kappa architecture paradigms</li></ul></div><div><div><div><div><h1 class="title"><a id="ch05lvl1sec35"/>Laying the foundations of streaming architecture</h1></div></div></div><p>As customary, let's<a id="id271" class="indexterm"/> first go back to our original drawing of the data-intensive apps architecture blueprint and highlight the Spark Streaming module that will be the topic of interest.</p><p>The following diagram sets the context by highlighting the Spark Streaming module and interactions with Spark SQL and Spark MLlib within the overall data-intensive apps framework.</p><div><img src="img/B03968_05_01.jpg" alt="Laying the foundations of streaming architecture"/></div><p>Data flows<a id="id272" class="indexterm"/> from stock market time series, enterprise transactions, interactions, events, web traffic, click streams, and sensors. All events are time-stamped data and urgent. This is the case for fraud detection and prevention, mobile cross-sell and upsell, or traffic alerts. Those streams of data require immediate processing for monitoring purposes, such as detecting anomalies, outliers, spam, fraud, and intrusion; and also for providing basic statistics, insights, trends, and recommendations. In some cases, the summarized aggregated information is sufficient to be stored for later usage. From an architecture paradigm perspective, we are moving from a service-oriented architecture to an event-driven architecture.</p><p>Two models emerge for processing streams of data:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Processing <a id="id273" class="indexterm"/>one record at a time as they come in. We do not buffer the incoming records in a container before processing them. This is the case of Twitter's Storm, Yahoo's S4, and Google's MillWheel.</li><li class="listitem" style="list-style-type: disc">Micro-batching or batch computations on small intervals as performed by Spark Streaming and Storm Trident. In this case, we buffer the incoming records in a container according to the time window prescribed in the micro-batching settings.</li></ul></div><p>Spark Streaming has often been compared against Storm. They are two different models of streaming data. Spark Streaming is based on micro-batching. Storm is based on processing records as they come in. Storm also offers a micro-batching option, with its Storm Trident option.</p><p>The driving<a id="id274" class="indexterm"/> factor in a streaming application is latency. Latency<a id="id275" class="indexterm"/> varies from the milliseconds range in the case of <strong>RPC</strong> (short for <strong>Remote Procedure Call</strong>) to several seconds or minutes for micro batching solution such as Spark Streaming.</p><p>RPC allows synchronous operations between the requesting programs waiting for the results from the remote server's procedure. Threads allow concurrency of multiple RPC calls to the server.</p><p>An example of software implementing a distributed RPC model is Apache Storm.</p><p>Storm implements stateless sub millisecond latency processing of unbounded tuples using topologies or directed acyclic graphs combining spouts as source of data streams and bolts for operations such as filter, join, aggregation, and transformation. Storm also implements a <a id="id276" class="indexterm"/>higher level abstraction called <strong>Trident</strong> which, similarly to Spark, processes data streams in micro batches.</p><p>So, looking at the latency continuum, from sub millisecond to second, Storm is a good candidate. For seconds to minutes scale, Spark Streaming and Storm Trident are excellent fits. For several minutes onward, Spark and a NoSQL database such as Cassandra or HBase are adequate solutions. For ranges beyond the hour and with high volume of data, Hadoop is the ideal contender.</p><p>Although throughput is correlated to latency, it is not a simple inversely linear relationship. If processing a message takes 2 ms, which determines the latency, then one would assume the throughput is limited to 500 messages per sec. Batching messages allows for higher throughput if we allow our messages to be buffered for 8 ms more. With a latency of 10 ms, the system can buffer up to 10,000 messages. For a bearable increase in latency, we have substantially increased throughput. This is the magic of micro-batching that Spark Streaming exploits.</p><div><div><div><div><h2 class="title"><a id="ch05lvl2sec45"/>Spark Streaming inner working</h2></div></div></div><p>The Spark <a id="id277" class="indexterm"/>Streaming architecture leverages the Spark core <a id="id278" class="indexterm"/>architecture. It overlays on the <strong>SparkContext</strong> a <strong>StreamingContext</strong> as the entry point to the Stream functionality. The Cluster Manager will dedicate at least <a id="id279" class="indexterm"/>one worker node as Receiver, which will be an executor with a <em>long task</em> to process the incoming stream. The Executor creates Discretized Streams or DStreams from input data stream and replicates by default, the DStream to the cache of another worker. One receiver serves one input data stream. Multiple receivers improve parallelism and generate multiple DStreams that Spark can unite or join Resilient Distributed Datasets (RDD).</p><p>The following diagram gives an overview of the inner working of Spark Streaming. The client interacts with the Spark Cluster via the cluster manager, while Spark Streaming has a dedicated <a id="id280" class="indexterm"/>worker with a long running task ingesting the<a id="id281" class="indexterm"/> input data stream and transforming it into discretized streams or DStreams. The data is collected, buffered and replicated by a receiver and then pushed to a stream of RDDs.</p><div><img src="img/B03968_05_02.jpg" alt="Spark Streaming inner working"/></div><p>Spark receivers can ingest data from many sources. Core input sources range from TCP socket and HDFS/Amazon S3 to Akka Actors. Additional sources include Apache Kafka, Apache Flume, Amazon Kinesis, ZeroMQ, Twitter, and custom or user-defined receivers.</p><p>We distinguish between reliable resources that acknowledges receipt of data to the source and replication for possible resend, versus unreliable receivers who do not acknowledge receipt of the message. Spark scales out in terms of the number of workers, partition and receivers.</p><p>The following diagram gives an overview of Spark Streaming with the possible sources and the persistence options:</p><div><img src="img/B03968_05_03.jpg" alt="Spark Streaming inner working"/></div></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec46"/>Going under the hood of Spark Streaming</h2></div></div></div><p>Spark Streaming is <a id="id282" class="indexterm"/>composed of Receivers and powered by Discretized Streams and Spark Connectors for persistence.</p><p>As for <a id="id283" class="indexterm"/>Spark Core, the essential data structure is the RDD, the fundamental programming abstraction for Spark Streaming is the Discretized Stream or DStream.</p><p>The following diagram illustrates the Discretized Streams as continuous sequences of RDDs. The batch intervals of DStream are configurable.</p><div><img src="img/B03968_05_04.jpg" alt="Going under the hood of Spark Streaming"/></div><p>DStreams snapshots the incoming data in batch intervals. Those time steps typically range from 500 ms to several seconds. The underlying structure of a DStream is an RDD.</p><p>A DStream is<a id="id284" class="indexterm"/> essentially a continuous sequence of RDDs. This is powerful as it allows us to leverage from Spark Streaming all the traditional functions, transformations and actions available in Spark Core and allows us to dialogue with Spark SQL, performing SQL queries on incoming streams of data and Spark MLlib. Transformations similar to those on generic and key-value pair RDDs are applicable. The DStreams benefit from the inner RDDs lineage and fault tolerance. Additional transformation and output operations exist for discretized stream operations. Most generic <a id="id285" class="indexterm"/>operations on DStream are <strong>transform</strong> and <strong>foreachRDD</strong>.</p><p>The following diagram gives an overview of the lifecycle of DStreams. From creation of the micro-batches of messages materialized to RDDs on which <code class="literal">transformation</code> function and actions that trigger Spark jobs are applied. Breaking down the steps illustrated in the diagram, we read the diagram top down:</p><div><ol class="orderedlist arabic"><li class="listitem">In the Input Stream, the incoming messages are buffered in a container according to the time window allocated for the micro-batching.</li><li class="listitem">In the discretized stream step, the buffered micro-batches are transformed as DStream RDDs.</li><li class="listitem">The Mapped DStream step is obtained by applying a transformation function to the original DStream. These first three steps constitute the transformation of the original data received in predefined time windows. As the underlying data structure is the RDD, we conserve the data lineage of the transformations.</li><li class="listitem">The final step is an action on the RDD. It triggers the Spark job.</li></ol></div><div><img src="img/B03968_05_05.jpg" alt="Going under the hood of Spark Streaming"/></div><p>Transformation<a id="id286" class="indexterm"/> can be stateless or stateful. <em>Stateless</em> means that no state is maintained by the program, while <em>stateful</em> means the program keeps a state, in which case previous transactions are remembered and may affect the current transaction. A stateful operation modifies or requires some state of the system, and a stateless operation does not.</p><p>Stateless transformations process each batch in a DStream at a time. Stateful transformations process multiple batches to obtain results. Stateful transformations require the checkpoint directory to be configured. Check pointing is the main mechanism for fault tolerance in Spark Streaming to periodically save data and metadata about an application.</p><p>There are two types of stateful transformations for Spark Streaming: <code class="literal">updateStateByKey</code> and windowed transformations.</p><p>
<code class="literal">updateStateByKey</code> are transformations that maintain state for each key in a stream of Pair RDDs. It returns a new <em>state</em> DStream where the state for each key is updated by applying the given function on the previous state of the key and the new values of each key. An example would be a running count of given hashtags in a stream of tweets.</p><p>Windowed transformations are carried over multiple batches in a sliding window. A window has a defined length or duration specified in time units. It must be a multiple of a DStream batch interval. It defines how many batches are included in a windowed transformation.</p><p>A window has<a id="id287" class="indexterm"/> a sliding interval or sliding duration specified in time units. It must be a multiple of a DStream batch interval. It defines how many batches to slide a window or how frequently to compute a windowed transformation.</p><p>The following schema depicts the windowing operation on DStreams to derive window DStreams with a given length and sliding interval:</p><div><img src="img/B03968_05_06.jpg" alt="Going under the hood of Spark Streaming"/></div><p>A sample function is <code class="literal">countByWindow</code> (<code class="literal">windowLength</code>, <code class="literal">slideInterval</code>). It returns a new DStream in which each RDD has a single element generated by counting the number of elements in a sliding window over this DStream. An illustration in this case would be a running count of given hashtags in a stream of tweets every 60 seconds. The window time frame is specified.</p><p>Minute scale window length is reasonable. Hour scale window length is not recommended as it is compute and memory intensive. It would be more convenient to aggregate the data in a database such as Cassandra or HBase.</p><p>Windowed transformations compute results based on window length and window slide interval. Spark performance is primarily affected by on window length, window slide interval, and persistence.</p></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec47"/>Building in fault tolerance</h2></div></div></div><p>Real-time<a id="id288" class="indexterm"/> stream processing systems must be operational 24/7. They need to be resilient to all sorts of failures in the system. Spark and its RDD abstraction are designed to seamlessly handle failures of any worker nodes in the cluster.</p><p>Main Spark Streaming fault tolerance mechanisms are check pointing, automatic driver restart, and automatic failover. Spark enables recovery from driver failure using check pointing, which preserves the application state.</p><p>Write ahead logs, reliable receivers, and file streams guarantees zero data loss as of Spark Version 1.2. Write ahead logs represent a fault tolerant storage for received data.</p><p>Failures <a id="id289" class="indexterm"/>require recomputing results. DStream operations have exactly-one semantics. Transformations can be recomputed multiple times but will yield the same result. DStream output operations have at least once semantics. Output operations may be executed multiple times.</p></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch05lvl1sec36"/>Processing live data with TCP sockets</h1></div></div></div><p>As a<a id="id290" class="indexterm"/> stepping stone to the overall understanding of streaming operations, we will first experiment with TCP socket. TCP socket establishes two-way communication between client and server, and it can exchange data through the established connection. WebSocket connections are long lived, unlike typical HTTP connections. HTTP is not meant to keep an open connection from the server to push continuously data to the web browsers. Most web applications hence resorted to long polling <a id="id291" class="indexterm"/>via frequent <strong>Asynchronous JavaScript</strong> (<strong>AJAX</strong>) and XML requests. WebSockets, standardized and implemented in HTML5, are moving beyond web browsers and are becoming a cross-platform standard for real-time communication between client and server.</p><div><div><div><div><h2 class="title"><a id="ch05lvl2sec48"/>Setting up TCP sockets</h2></div></div></div><p>We create<a id="id292" class="indexterm"/> a TCP Socket Server by running <code class="literal">netcat</code>, a small utility found in most Linux systems, as a data server with the command <code class="literal">&gt; nc -lk 9999</code>, where <code class="literal">9999</code> is the port where we are sending data:</p><div><pre class="programlisting">#
# Socket Server
#
an@an-VB:~$ nc -lk 9999
hello world
how are you
hello  world
cool it works</pre></div><p>Once netcat is running, we will open a second console with our Spark Streaming client to receive the data and process. As soon as the Spark Streaming client console is listening, we start<a id="id293" class="indexterm"/> typing the words to be processed, that is, <code class="literal">hello world</code>.</p></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec49"/>Processing live data</h2></div></div></div><p>We <a id="id294" class="indexterm"/>will be using the example program provided in <a id="id295" class="indexterm"/>the Spark bundle for Spark Streaming called <code class="literal">network_wordcount.py</code>. It can be found on the GitHub repository under <a class="ulink" href="https://github.com/apache/spark/blob/master/examples/src/main/python/streaming/network_wordcount.py">https://github.com/apache/spark/blob/master/examples/src/main/python/streaming/network_wordcount.py</a>. The code is as follows:</p><div><pre class="programlisting">"""
 Counts words in UTF8 encoded, '\n' delimited text received from the network every second.
 Usage: network_wordcount.py &lt;hostname&gt; &lt;port&gt;
   &lt;hostname&gt; and &lt;port&gt; describe the TCP server that Spark Streaming would connect to receive data.
 To run this on your local machine, you need to first run a Netcat server
    `$ nc -lk 9999`
 and then run the example
    `$ bin/spark-submit examples/src/main/python/streaming/network_wordcount.py localhost 9999`
"""
from __future__ import print_function

import sys

from pyspark import SparkContext
from pyspark.streaming import StreamingContext

if __name__ == "__main__":
    if len(sys.argv) != 3:
        print("Usage: network_wordcount.py &lt;hostname&gt; &lt;port&gt;", file=sys.stderr)
        exit(-1)
    sc = SparkContext(appName="PythonStreamingNetworkWordCount")
    ssc = StreamingContext(sc, 1)

    lines = ssc.socketTextStream(sys.argv[1], int(sys.argv[2]))
    counts = lines.flatMap(lambda line: line.split(" "))\
                  .map(lambda word: (word, 1))\
                  .reduceByKey(lambda a, b: a+b)
    counts.pprint()

    ssc.start()
    ssc.awaitTermination()</pre></div><p>Here, we<a id="id296" class="indexterm"/> explain the steps of the program:</p><div><ol class="orderedlist arabic"><li class="listitem">The code first initializes a Spark Streaming Context with the command:<div><pre class="programlisting">
<strong>ssc = StreamingContext(sc, 1)</strong>
</pre></div></li><li class="listitem">Next, the streaming computation is set up.</li><li class="listitem">One or more DStream objects that receive data are defined to connect to localhost or <code class="literal">127.0.0.1</code> on <code class="literal">port 9999</code>:<div><pre class="programlisting">
<strong>stream = ssc.socketTextStream("127.0.0.1", 9999)</strong>
</pre></div></li><li class="listitem">The DStream computation is defined: transformations and output operations:<div><pre class="programlisting">stream.map(x: lambda (x,1))
.reduce(a+b)
.print()</pre></div></li><li class="listitem">Computation is started:<div><pre class="programlisting">
<strong>ssc.start()</strong>
</pre></div></li><li class="listitem">Program termination is pending manual or error processing completion:<div><pre class="programlisting">
<strong>ssc.awaitTermination()</strong>
</pre></div></li><li class="listitem">Manual completion is an option when a completion condition is known:<div><pre class="programlisting">
<strong>ssc.stop()</strong>
</pre></div></li></ol></div><p>We can monitor the Spark Streaming application by visiting the Spark monitoring home page at <code class="literal">localhost:4040</code>.</p><p>Here's the result of running the program and feeding the words on the <code class="literal">netcat</code> 4server console:</p><div><pre class="programlisting">#
# Socket Client
# an@an-VB:~/spark/spark-1.5.0-bin-hadoop2.6$ ./bin/spark-submit examples/src/main/python/streaming/network_wordcount.py localhost 9999</pre></div><p>Run the Spark Streaming <code class="literal">network_count</code> program by connecting to the socket localhost on <code class="literal">port 9999</code>:</p><div><pre class="programlisting">
<strong>an@an-VB:~/spark/spark-1.5.0-bin-hadoop2.6$ ./bin/spark-submit examples/src/main/python/streaming/network_wordcount.py localhost 9999</strong>
<strong>-------------------------------------------</strong>
<strong>Time: 2015-10-18 20:06:06</strong>
<strong>-------------------------------------------</strong>
<strong>(u'world', 1)</strong>
<strong>(u'hello', 1)</strong>

<strong>-------------------------------------------</strong>
<strong>Time: 2015-10-18 20:06:07</strong>
<strong>-------------------------------------------</strong>
<strong>. . .</strong>
<strong>-------------------------------------------</strong>
<strong>Time: 2015-10-18 20:06:17</strong>
<strong>-------------------------------------------</strong>
<strong>(u'you', 1)</strong>
<strong>(u'how', 1)</strong>
<strong>(u'are', 1)</strong>

<strong>-------------------------------------------</strong>
<strong>Time: 2015-10-18 20:06:18</strong>
<strong>-------------------------------------------</strong>

<strong>. . .</strong>

<strong>-------------------------------------------</strong>
<strong>Time: 2015-10-18 20:06:26</strong>
<strong>-------------------------------------------</strong>
<strong>(u'', 1)</strong>
<strong>(u'world', 1)</strong>
<strong>(u'hello', 1)</strong>

<strong>-------------------------------------------</strong>
<strong>Time: 2015-10-18 20:06:27</strong>
<strong>-------------------------------------------</strong>
<strong>. . .</strong>
<strong>-------------------------------------------</strong>
<strong>Time: 2015-10-18 20:06:37</strong>
<strong>-------------------------------------------</strong>
<strong>(u'works', 1)</strong>
<strong>(u'it', 1)</strong>
<strong>(u'cool', 1)</strong>

<strong>-------------------------------------------</strong>
<strong>Time: 2015-10-18 20:06:38</strong>
<strong>-------------------------------------------</strong>
</pre></div><p>Thus, we <a id="id297" class="indexterm"/>have established connection through the socket on <code class="literal">port 9999</code>, streamed the data sent by the <code class="literal">netcat</code> server, and performed a word count on the messages sent.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch05lvl1sec37"/>Manipulating Twitter data in real time</h1></div></div></div><p>Twitter <a id="id298" class="indexterm"/>offers two APIs. One search API that essentially allows us to retrieve past tweets based on search terms. This is how we have been collecting our data from Twitter in the previous chapters of the book. Interestingly, for our current purpose, Twitter offers a live streaming API which allows to ingest tweets as they are emitted in the blogosphere.</p><div><div><div><div><h2 class="title"><a id="ch05lvl2sec50"/>Processing Tweets in real time from the Twitter firehose</h2></div></div></div><p>The <a id="id299" class="indexterm"/>following program connects to the Twitter firehose and processes the incoming tweets to exclude deleted or invalid tweets and parses on the fly only the relevant ones to extract <code class="literal">screen name</code>, the actual tweet, or <code class="literal">tweet text</code>, <code class="literal">retweet</code> count, <code class="literal">geo-location</code> information. The processed tweets are gathered into an RDD Queue by Spark Streaming and then displayed on the console at a one-second interval:</p><div><pre class="programlisting">"""
Twitter Streaming API Spark Streaming into an RDD-Queue to process tweets live
 

 Create a queue of RDDs that will be mapped/reduced one at a time in
 1 second intervals.

 To run this example use
    '$ bin/spark-submit examples/AN_Spark/AN_Spark_Code/s07_twitterstreaming.py'

"""
#
import time
from pyspark import SparkContext
from pyspark.streaming import StreamingContext
import twitter
import dateutil.parser
import json

# Connecting Streaming Twitter with Streaming Spark via Queue
class Tweet(dict):
    def __init__(self, tweet_in):
        super(Tweet, self).__init__(self)
        if tweet_in and 'delete' not in tweet_in:
            self['timestamp'] = dateutil.parser.parse(tweet_in[u'created_at']
                                ).replace(tzinfo=None).isoformat()
            self['text'] = tweet_in['text'].encode('utf-8')
            #self['text'] = tweet_in['text']
            self['hashtags'] = [x['text'].encode('utf-8') for x in tweet_in['entities']['hashtags']]
            #self['hashtags'] = [x['text'] for x in tweet_in['entities']['hashtags']]
            self['geo'] = tweet_in['geo']['coordinates'] if tweet_in['geo'] else None
            self['id'] = tweet_in['id']
            self['screen_name'] = tweet_in['user']['screen_name'].encode('utf-8')
            #self['screen_name'] = tweet_in['user']['screen_name']
            self['user_id'] = tweet_in['user']['id']

def connect_twitter():
    twitter_stream = twitter.TwitterStream(auth=twitter.OAuth(
        token = "get_your_own_credentials",
        token_secret = "get_your_own_credentials",
        consumer_key = "get_your_own_credentials",
        consumer_secret = "get_your_own_credentials"))
    return twitter_stream

def get_next_tweet(twitter_stream):
    stream = twitter_stream.statuses.sample(block=True)
    tweet_in = None
    while not tweet_in or 'delete' in tweet_in:
        tweet_in = stream.next()
        tweet_parsed = Tweet(tweet_in)
    return json.dumps(tweet_parsed)

def process_rdd_queue(twitter_stream):
    # Create the queue through which RDDs can be pushed to
    # a QueueInputDStream
    rddQueue = []
    for i in range(3):
        rddQueue += [ssc.sparkContext.parallelize([get_next_tweet(twitter_stream)], 5)]

    lines = ssc.queueStream(rddQueue)
    lines.pprint()
    
if __name__ == "__main__":
    sc = SparkContext(appName="PythonStreamingQueueStream")
    ssc = StreamingContext(sc, 1)
    
    # Instantiate the twitter_stream
    twitter_stream = connect_twitter()
    # Get RDD queue of the streams json or parsed
    process_rdd_queue(twitter_stream)
    
    ssc.start()
    time.sleep(2)
    ssc.stop(stopSparkContext=True, stopGraceFully=True)</pre></div><p>When<a id="id300" class="indexterm"/> we run this program, it delivers the following output:</p><div><pre class="programlisting">
<strong>an@an-VB:~/spark/spark-1.5.0-bin-hadoop2.6$ bin/spark-submit examples/AN_Spark/AN_Spark_Code/s07_twitterstreaming.py</strong>
<strong>-------------------------------------------</strong>
<strong>Time: 2015-11-03 21:53:14</strong>
<strong>-------------------------------------------</strong>
<strong>{"user_id": 3242732207, "screen_name": "cypuqygoducu", "timestamp": "2015-11-03T20:53:04", "hashtags": [], "text": "RT @VIralBuzzNewss: Our Distinctive Edition Holiday break Challenge Is In this article! Hooray!... -  https://t.co/9d8wumrd5v https://t.co/\u2026", "geo": null, "id": 661647303678259200}</strong>

<strong>-------------------------------------------</strong>
<strong>Time: 2015-11-03 21:53:15</strong>
<strong>-------------------------------------------</strong>
<strong>{"user_id": 352673159, "screen_name": "melly_boo_orig", "timestamp": "2015-11-03T20:53:05", "hashtags": ["eminem"], "text": "#eminem https://t.co/GlEjPJnwxy", "geo": null, "id": 661647307847409668}</strong>

<strong>-------------------------------------------</strong>
<strong>Time: 2015-11-03 21:53:16</strong>
<strong>-------------------------------------------</strong>
<strong>{"user_id": 500620889, "screen_name": "NBAtheist", "timestamp": "2015-11-03T20:53:06", "hashtags": ["tehInterwebbies", "Nutters"], "text": "See? That didn't take long or any actual effort. This is #tehInterwebbies ... #Nutters Abound! https://t.co/QS8gLStYFO", "geo": null, "id": 661647312062709761}</strong>
</pre></div><p>So, we got an example of streaming tweets with Spark and processing them on the fly.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch05lvl1sec38"/>Building a reliable and scalable streaming app</h1></div></div></div><p>Ingesting <a id="id301" class="indexterm"/>data is the process of acquiring data from various sources and storing it for processing immediately or at a later stage. Data consuming systems are dispersed and can be physically and architecturally far from the sources. Data ingestion is often implemented manually with scripts and rudimentary automation. It actually calls for higher level frameworks like Flume and Kafka.</p><p>The challenges of data ingestion arise from the fact that the sources are physically spread out and are transient which makes the integration brittle. Data production is continuous for weather, traffic, social media, network activity, shop floor sensors, security, and surveillance. Ever increasing data volumes and rates coupled with ever changing data structure and semantics makes data ingestion ad hoc and error prone.</p><p>The aim is<a id="id302" class="indexterm"/> to become more agile, reliable, and scalable. Agility, reliability, and scalability of the data ingestion determine the overall health of the pipeline. Agility means integrating new sources as they arise and incorporating changes to existing sources as needed. In order to ensure safety and reliability, we need to protect the infrastructure against data loss and downstream applications from silent data corruption at ingress. Scalability avoids ingest bottlenecks while keeping cost tractable.</p><div><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Ingest Mode</p>
</th><th style="text-align: left" valign="bottom">
<p>Description</p>
</th><th style="text-align: left" valign="bottom">
<p>Example</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Manual or Scripted</p>
</td><td style="text-align: left" valign="top">
<p>File copy using command line interface or GUI interface</p>
</td><td style="text-align: left" valign="top">
<p>HDFS Client, Cloudera Hue</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Batch Data Transport</p>
</td><td style="text-align: left" valign="top">
<p>Bulk data transport <a id="id303" class="indexterm"/>using tools</p>
</td><td style="text-align: left" valign="top">
<p>DistCp, Sqoop</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Micro Batch</p>
</td><td style="text-align: left" valign="top">
<p>Transport of small <a id="id304" class="indexterm"/>batches of data</p>
</td><td style="text-align: left" valign="top">
<p>Sqoop, Sqoop2</p>
<p>Storm</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Pipelining</p>
</td><td style="text-align: left" valign="top">
<p>Flow like transport <a id="id305" class="indexterm"/>of event streams</p>
</td><td style="text-align: left" valign="top">
<p>Flume Scribe</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Message Queue</p>
</td><td style="text-align: left" valign="top">
<p>Publish Subscribe message <a id="id306" class="indexterm"/>bus of events</p>
</td><td style="text-align: left" valign="top">
<p>Kafka, Kinesis</p>
</td></tr></tbody></table></div><p>In order to enable an event-driven business that is able to ingest multiple streams of data, process it in flight, and make sense of it all to get to rapid decisions, the key driver is the Unified Log.</p><p>A Unified Log is a centralized enterprise structured log available for real-time subscription. All the organization's data is put in a central log for subscription. Records are numbered beginning with zero in the order that they are written. It is also known as a commit log or journal. The concept of the <em>Unified Log</em> is the central tenet of the Kappa architecture.</p><p>The properties<a id="id307" class="indexterm"/> of the Unified Log are as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Unified</strong>: There is a single deployment for the entire organization</li><li class="listitem" style="list-style-type: disc"><strong>Append only</strong>: Events are immutable and are appended</li><li class="listitem" style="list-style-type: disc"><strong>Ordered</strong>: Each <a id="id308" class="indexterm"/>event has a unique offset within a shard</li><li class="listitem" style="list-style-type: disc"><strong>Distributed</strong>: For fault tolerance purpose, the Unified Log is distributed redundantly on a cluster of computers</li><li class="listitem" style="list-style-type: disc"><strong>Fast</strong>: The <a id="id309" class="indexterm"/>systems ingests thousands of messages per second</li></ul></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec51"/>Setting up Kafka</h2></div></div></div><p>In order<a id="id310" class="indexterm"/> to isolate downstream particular consumption<a id="id311" class="indexterm"/> of data from the vagaries of upstream emission of data, we need to decouple the providers of data from the receivers or consumers of data. As they are living in two different worlds with different cycles and constraints, Kafka decouples the data pipelines.</p><p>Apache Kafka is <a id="id312" class="indexterm"/>a distributed publish subscribe messaging system rethought as a distributed commit log. The messages are stored by topic.</p><p>Apache Kafka <a id="id313" class="indexterm"/>has the following properties. It supports:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">High throughput for high volume of events feeds</li><li class="listitem" style="list-style-type: disc">Real-time processing of new and derived feeds</li><li class="listitem" style="list-style-type: disc">Large data backlogs and persistence for offline consumption</li><li class="listitem" style="list-style-type: disc">Low latency as enterprise wide messaging system</li><li class="listitem" style="list-style-type: disc">Fault tolerance thanks to its distributed nature</li></ul></div><p>Messages are stored in partition with a unique sequential ID called <code class="literal">offset</code>. Consumers track their pointers via tuple of (<code class="literal">offset</code>, <code class="literal">partition</code>, <code class="literal">topic</code>).</p><p>Let's dive deeper in the anatomy of Kafka.</p><p>Kafka has essentially three components: <em>producers</em>, <em>consumers</em> and <em>brokers</em>. Producers push and write data to brokers. Consumers pull and read data from brokers. Brokers do not push messages to consumers. Consumers pull message from brokers. The setup is distributed and coordinated by Apache Zookeeper.</p><p>The brokers manage and store the data in topics. Topics are split in replicated partitions. The data is persisted in the broker, but not removed upon consumption, but until retention period. If a consumer fails, it can always go back to the broker to fetch the data.</p><p>Kafka requires Apache ZooKeeper. ZooKeeper is a high-performance coordination service for distributed applications. It centrally manages configuration, registry or naming service, group membership, lock, and synchronization for coordination between servers. It provides a hierarchical namespace with metadata, monitoring statistics, and state of the cluster. ZooKeeper can introduce brokers and consumers on the fly and then rebalances the cluster.</p><p>Kafka producers do not need ZooKeeper. Kafka brokers use ZooKeeper to provide general state information as well elect leader in case of failure. Kafka consumers use ZooKeeper to track message offset. Newer versions of Kafka will save the consumers to go through ZooKeeper and can retrieve the Kafka special topics information. Kafka provides <a id="id314" class="indexterm"/>automatic load balancing for producers.</p><p>The following<a id="id315" class="indexterm"/> diagram gives an overview of the Kafka setup:</p><div><img src="img/B03968_05_07.jpg" alt="Setting up Kafka"/></div><div><div><div><div><h3 class="title"><a id="ch05lvl3sec08"/>Installing and testing Kafka</h3></div></div></div><p>We<a id="id316" class="indexterm"/> will <a id="id317" class="indexterm"/>download the Apache Kafka binaries from <a id="id318" class="indexterm"/>the dedicated web page at <a class="ulink" href="http://kafka.apache.org/downloads.html">http://kafka.apache.org/downloads.html</a> and install the software in our machine using the following steps:</p><div><ol class="orderedlist arabic"><li class="listitem">Download the code.</li><li class="listitem">Download the 0.8.2.0 release and <code class="literal">un-tar</code> it:<div><pre class="programlisting">
<strong>&gt; tar -xzf kafka_2.10-0.8.2.0.tgz</strong>
<strong>&gt; cd kafka_2.10-0.8.2.0</strong>
</pre></div></li><li class="listitem">Start <code class="literal">zooeeper</code>. Kafka uses ZooKeeper so we need to first start a ZooKeeper server. We will use the convenience script packaged with Kafka to get a single-node ZooKeeper instance.<div><pre class="programlisting">
<strong>&gt; bin/zookeeper-server-start.sh config/zookeeper.properties</strong>
<strong>an@an-VB:~/kafka/kafka_2.10-0.8.2.0$ bin/zookeeper-server-start.sh config/zookeeper.properties</strong>

<strong>[2015-10-31 22:49:14,808] INFO Reading configuration from: config/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)</strong>
<strong>[2015-10-31 22:49:14,816] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)...</strong>
</pre></div></li><li class="listitem">Now<a id="id319" class="indexterm"/> launch<a id="id320" class="indexterm"/> the Kafka server:<div><pre class="programlisting">
<strong>&gt; bin/kafka-server-start.sh config/server.properties</strong>

<strong>an@an-VB:~/kafka/kafka_2.10-0.8.2.0$ bin/kafka-server-start.sh config/server.properties</strong>
<strong>[2015-10-31 22:52:04,643] INFO Verifying properties (kafka.utils.VerifiableProperties)</strong>
<strong>[2015-10-31 22:52:04,714] INFO Property broker.id is overridden to 0 (kafka.utils.VerifiableProperties)</strong>
<strong>[2015-10-31 22:52:04,715] INFO Property log.cleaner.enable is overridden to false (kafka.utils.VerifiableProperties)</strong>
<strong>[2015-10-31 22:52:04,715] INFO Property log.dirs is overridden to /tmp/kafka-logs (kafka.utils.VerifiableProperties) [2013-04-22 15:01:47,051] INFO Property socket.send.buffer.bytes is overridden to 1048576 (kafka.utils.VerifiableProperties)</strong>
</pre></div></li><li class="listitem">Create a topic. Let's create a topic named test with a single partition and only one replica:<div><pre class="programlisting">
<strong>&gt; bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test</strong>
</pre></div></li><li class="listitem">We can now see that topic if we run the <code class="literal">list</code> topic command:<div><pre class="programlisting">
<strong>&gt; bin/kafka-topics.sh --list --zookeeper localhost:2181</strong>
<strong>Test</strong>
<strong>an@an-VB:~/kafka/kafka_2.10-0.8.2.0$ bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test</strong>
<strong>Created topic "test".</strong>
<strong>an@an-VB:~/kafka/kafka_2.10-0.8.2.0$ bin/kafka-topics.sh --list --zookeeper localhost:2181</strong>
<strong>test</strong>
</pre></div></li><li class="listitem">Check the Kafka installation by creating a producer and consumer. We first launch a <code class="literal">producer</code> and type a message in the console:<div><pre class="programlisting">
<strong>an@an-VB:~/kafka/kafka_2.10-0.8.2.0$ bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test</strong>
<strong>[2015-10-31 22:54:43,698] WARN Property topic is not valid (kafka.utils.VerifiableProperties)</strong>
<strong>This is a message</strong>
<strong>This is another message</strong>
</pre></div></li><li class="listitem">We<a id="id321" class="indexterm"/> then <a id="id322" class="indexterm"/>launch a consumer to check that we receive the message:<div><pre class="programlisting">
<strong>an@an-VB:~$ cd kafka/</strong>
<strong>an@an-VB:~/kafka$ cd kafka_2.10-0.8.2.0/</strong>
<strong>an@an-VB:~/kafka/kafka_2.10-0.8.2.0$ bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test --from-beginning</strong>
<strong>This is a message</strong>
<strong>This is another message</strong>
</pre></div></li></ol></div><p>The messages were appropriately received by the consumer:</p><div><ol class="orderedlist arabic"><li class="listitem">Check Kafka and Spark Streaming consumer. We will be using the Spark Streaming Kafka word count example provided in the Spark bundle. A word of caution: we have to bind the Kafka packages, <code class="literal">--packages org.apache.spark:spark-streaming-kafka_2.10:1.5.0</code>, when we submit the Spark job. The command is as follows:<div><pre class="programlisting">
<strong>./bin/spark-submit --packages org.apache.spark:spark-streaming-kafka_2.10:1.5.0 \ examples/src/main/python/streaming/kafka_wordcount.py \</strong>

<strong>localhost:2181 test</strong>
</pre></div></li><li class="listitem">When we launch the Spark Streaming word count program with Kafka, we get the following output:<div><pre class="programlisting">
<strong>an@an-VB:~/spark/spark-1.5.0-bin-hadoop2.6$ ./bin/spark-submit --packages org.apache.spark:spark-streaming-kafka_2.10:1.5.0 examples/src/main/python/streaming/kafka_wordcount.py </strong>
<strong>localhost:2181 test</strong>

<strong>-------------------------------------------</strong>
<strong>Time: 2015-10-31 23:46:33</strong>
<strong>-------------------------------------------</strong>
<strong>(u'', 1)</strong>
<strong>(u'from', 2)</strong>
<strong>(u'Hello', 2)</strong>
<strong>(u'Kafka', 2)</strong>

<strong>-------------------------------------------</strong>
<strong>Time: 2015-10-31 23:46:34</strong>
<strong>-------------------------------------------</strong>

<strong>-------------------------------------------</strong>
<strong>Time: 2015-10-31 23:46:35</strong>
<strong>-------------------------------------------</strong>
</pre></div></li><li class="listitem">Install <a id="id323" class="indexterm"/>the Kafka Python driver in order to be able to programmatically <a id="id324" class="indexterm"/>develop Producers and Consumers and interact with Kafka and Spark using Python. We will use the road-tested library<a id="id325" class="indexterm"/> from David Arthur, aka, Mumrah on GitHub (<a class="ulink" href="https://github.com/mumrah">https://github.com/mumrah</a>). We can pip install it as follows:<div><pre class="programlisting">
<strong>&gt; pip install kafka-python</strong>
<strong>an@an-VB:~$ pip install kafka-python</strong>
<strong>Collecting kafka-python</strong>
<strong>  Downloading kafka-python-0.9.4.tar.gz (63kB)</strong>
<strong>...</strong>
<strong>Successfully installed kafka-python-0.9.4</strong>
</pre></div></li></ol></div></div><div><div><div><div><h3 class="title"><a id="ch05lvl3sec09"/>Developing producers</h3></div></div></div><p>The<a id="id326" class="indexterm"/> following program creates a Simple Kafka Producer that will emit the message <em>this is a message sent from the Kafka producer:</em> five times, followed by a time stamp every second:</p><div><pre class="programlisting">#
# kafka producer
#
#
import time
from kafka.common import LeaderNotAvailableError
from kafka.client import KafkaClient
from kafka.producer import SimpleProducer
from datetime import datetime

def print_response(response=None):
    if response:
        print('Error: {0}'.format(response[0].error))
        print('Offset: {0}'.format(response[0].offset))

def main():
    kafka = KafkaClient("localhost:9092")
    producer = SimpleProducer(kafka)
    try:
        time.sleep(5)
        topic = 'test'
        for i in range(5):
            time.sleep(1)
            msg = 'This is a message sent from the kafka producer: ' \
                  + str(datetime.now().time()) + ' -- '\
                  + str(datetime.now().strftime("%A, %d %B %Y %I:%M%p"))
            print_response(producer.send_messages(topic, msg))
    except LeaderNotAvailableError:
        # https://github.com/mumrah/kafka-python/issues/249
        time.sleep(1)
        print_response(producer.send_messages(topic, msg))
 
    kafka.close()
 
if __name__ == "__main__":
    main()</pre></div><p>When<a id="id327" class="indexterm"/> we run this program, the following output is generated:</p><div><pre class="programlisting">
<strong>an@an-VB:~/spark/spark-1.5.0-bin-hadoop2.6/examples/AN_Spark/AN_Spark_Code$ python s08_kafka_producer_01.py</strong>
<strong>Error: 0</strong>
<strong>Offset: 13</strong>
<strong>Error: 0</strong>
<strong>Offset: 14</strong>
<strong>Error: 0</strong>
<strong>Offset: 15</strong>
<strong>Error: 0</strong>
<strong>Offset: 16</strong>
<strong>Error: 0</strong>
<strong>Offset: 17</strong>
<strong>an@an-VB:~/spark/spark-1.5.0-bin-hadoop2.6/examples/AN_Spark/AN_Spark_Code$</strong>
</pre></div><p>It tells<a id="id328" class="indexterm"/> us there were no errors and gives the offset of the messages given by the Kafka broker.</p></div><div><div><div><div><h3 class="title"><a id="ch05lvl3sec10"/>Developing consumers</h3></div></div></div><p>To fetch<a id="id329" class="indexterm"/> the messages from the Kafka brokers, we develop a Kafka consumer:</p><div><pre class="programlisting"># kafka consumer
# consumes messages from "test" topic and writes them to console.
#
from kafka.client import KafkaClient
from kafka.consumer import SimpleConsumer

def main():
  kafka = KafkaClient("localhost:9092")
  print("Consumer established connection to kafka")
  consumer = SimpleConsumer(kafka, "my-group", "test")
  for message in consumer:
    # This will wait and print messages as they become available
    print(message)

if __name__ == "__main__":
    main()</pre></div><p>When we run this program, we effectively confirm that the consumer received all the messages:</p><div><pre class="programlisting">
<strong>an@an-VB:~$ cd ~/spark/spark-1.5.0-bin-hadoop2.6/examples/AN_Spark/AN_Spark_Code/</strong>
<strong>an@an-VB:~/spark/spark-1.5.0-bin-hadoop2.6/examples/AN_Spark/AN_Spark_Code$ python s08_kafka_consumer_01.py</strong>
<strong>Consumer established connection to kafka</strong>
<strong>OffsetAndMessage(offset=13, message=Message(magic=0, attributes=0, key=None, value='This is a message sent from the kafka producer: 11:50:17.867309Sunday, 01 November 2015 11:50AM'))</strong>
<strong>...</strong>
<strong>OffsetAndMessage(offset=17, message=Message(magic=0, attributes=0, key=None, value='This is a message sent from the kafka producer: 11:50:22.051423Sunday, 01 November 2015 11:50AM'))</strong>
</pre></div></div><div><div><div><div><h3 class="title"><a id="ch05lvl3sec11"/>Developing a Spark Streaming consumer for Kafka</h3></div></div></div><p>Based<a id="id330" class="indexterm"/> on the example code provided in the Spark Streaming bundle, we will create a Spark Streaming consumer for Kafka and perform a word count on the messages stored with the brokers:</p><div><pre class="programlisting">#
# Kafka Spark Streaming Consumer    
#
from __future__ import print_function

import sys

from pyspark import SparkContext
from pyspark.streaming import StreamingContext
from pyspark.streaming.kafka import KafkaUtils

if __name__ == "__main__":
    if len(sys.argv) != 3:
        print("Usage: kafka_spark_consumer_01.py &lt;zk&gt; &lt;topic&gt;", file=sys.stderr)
        exit(-1)

    sc = SparkContext(appName="PythonStreamingKafkaWordCount")
    ssc = StreamingContext(sc, 1)

    zkQuorum, topic = sys.argv[1:]
    kvs = KafkaUtils.createStream(ssc, zkQuorum, "spark-streaming-consumer", {topic: 1})
    lines = kvs.map(lambda x: x[1])
    counts = lines.flatMap(lambda line: line.split(" ")) \
        .map(lambda word: (word, 1)) \
        .reduceByKey(lambda a, b: a+b)
    counts.pprint()

    ssc.start()
    ssc.awaitTermination()</pre></div><p>Run this program with the following Spark submit command:</p><div><pre class="programlisting">./bin/spark-submit --packages org.apache.spark:spark-streaming-kafka_2.10:1.5.0 examples/AN_Spark/AN_Spark_Code/s08_kafka_spark_consumer_01.py localhost:2181 test</pre></div><p>We get the following output:</p><div><pre class="programlisting">an@an-VB:~$ cd spark/spark-1.5.0-bin-hadoop2.6/
an@an-VB:~/spark/spark-1.5.0-bin-hadoop2.6$ ./bin/spark-submit \
&gt;     --packages org.apache.spark:spark-streaming-kafka_2.10:1.5.0 \
&gt;     examples/AN_Spark/AN_Spark_Code/s08_kafka_spark_consumer_01.py localhost:2181 test
...
:: retrieving :: org.apache.spark#spark-submit-parent
  confs: [default]
  0 artifacts copied, 10 already retrieved (0kB/18ms)
-------------------------------------------
Time: 2015-11-01 12:13:16
-------------------------------------------

-------------------------------------------
Time: 2015-11-01 12:13:17
-------------------------------------------

-------------------------------------------
Time: 2015-11-01 12:13:18
-------------------------------------------

-------------------------------------------
Time: 2015-11-01 12:13:19
-------------------------------------------
(u'a', 5)
(u'the', 5)
(u'11:50AM', 5)
(u'from', 5)
(u'This', 5)
(u'11:50:21.044374Sunday,', 1)
(u'message', 5)
(u'11:50:20.036422Sunday,', 1)
(u'11:50:22.051423Sunday,', 1)
(u'11:50:17.867309Sunday,', 1)
...

-------------------------------------------
Time: 2015-11-01 12:13:20
-------------------------------------------

-------------------------------------------
Time: 2015-11-01 12:13:21
-------------------------------------------</pre></div></div></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec52"/>Exploring flume</h2></div></div></div><p>Flume is<a id="id331" class="indexterm"/> a continuous ingestion system. It was originally designed to be a log aggregation system, but it evolved to handle any type of streaming event data.</p><p>Flume is a distributed, reliable, scalable, and available pipeline system for efficient collection, aggregation, and transport of large volumes of data. It has built-in support for contextual routing, filtering replication, and multiplexing. It is robust and fault tolerant, with tunable reliability mechanisms and many failover and recovery mechanisms. It uses a simple extensible <a id="id332" class="indexterm"/>data model that allows for real time analytic application.</p><p>Flume <a id="id333" class="indexterm"/>offers the following:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Guaranteed delivery semantics</li><li class="listitem" style="list-style-type: disc">Low latency reliable data transfer</li><li class="listitem" style="list-style-type: disc">Declarative configuration with no coding required</li><li class="listitem" style="list-style-type: disc">Extendable and customizable settings</li><li class="listitem" style="list-style-type: disc">Integration with most commonly used end-points</li></ul></div><p>The anatomy <a id="id334" class="indexterm"/>of Flume contains the following elements:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Event</strong>: An event is <a id="id335" class="indexterm"/>the fundamental unit of data that is transported by Flume from source to destination. It is like a message with a byte array payload opaque to Flume and optional headers used for contextual routing.</li><li class="listitem" style="list-style-type: disc"><strong>Client</strong>: A <a id="id336" class="indexterm"/>client produces and transmits events. A client decouples Flume from the data consumers. It is an entity that generates events and sends them to one or more agents. Custom client or Flume log4J append program or embedded application agent can be client.</li><li class="listitem" style="list-style-type: disc"><strong>Agent</strong>: An agent is a container hosting sources, channels, sinks, and other elements that enable the transportation of events from one place to the other. It provides configuration, life cycle management and monitoring for hosted components. An agent is a physical Java virtual machine running Flume.</li><li class="listitem" style="list-style-type: disc"><strong>Source</strong>: Source<a id="id337" class="indexterm"/> is the entity through which Flume receives events. Sources require at least one channel to function in order to either actively poll data or passively wait for data to be delivered to them. A variety of sources allow data to be collected, such as log4j logs and syslogs.</li><li class="listitem" style="list-style-type: disc"><strong>Sink</strong>: Sink is<a id="id338" class="indexterm"/> the entity that drains data from the channel and delivers it to the next destination. A variety of sinks allow data to be streamed to a range of destinations. Sinks support serialization to user's format. One example is the HDFS sink that writes events to HDFS.</li><li class="listitem" style="list-style-type: disc"><strong>Channel</strong>: Channel is the conduit between the source and the sink that buffers incoming events until drained by sinks. Sources feed events into the channel and the sinks drain the channel. Channels decouple the impedance of upstream and downstream systems. Burst of data upstream is damped by the channels. Failures downstream are transparently absorbed by the channels. Sizing the channel capacity to cope with these events is key to realizing these benefits. Channels offer two levels of persistence: either memory channel, which is volatile if the JVM crashes, or File channel backed by Write Ahead Log that stores the information to disk. Channels are fully transactional.</li></ul></div><p>Let's illustrate <a id="id339" class="indexterm"/>all <a id="id340" class="indexterm"/>these <a id="id341" class="indexterm"/>concepts:</p><div><img src="img/B03968_05_08.jpg" alt="Exploring flume"/></div></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec53"/>Developing data pipelines with Flume, Kafka, and Spark</h2></div></div></div><p>Building <a id="id342" class="indexterm"/>resilient data pipeline <a id="id343" class="indexterm"/>leverages the learnings from<a id="id344" class="indexterm"/> the previous sections. We are plumbing together data ingestion and transport with Flume, data brokerage with a reliable and sophisticated publish and subscribe messaging system such as Kafka, and finally process computation on the fly using Spark Streaming.</p><p>The following diagram illustrates the composition of streaming data pipelines as sequence of <em>connect</em>, <em>collect</em>, <em>conduct</em>, <em>compose</em>, <em>consume</em>, <em>consign</em>, and <em>control</em> activities. These activities are configurable based on the use case:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Connect establishes the binding with the streaming API.</li><li class="listitem" style="list-style-type: disc">Collect creates collection threads.</li><li class="listitem" style="list-style-type: disc">Conduct decouples the data producers from the consumers by creating a buffer queue or publish-subscribe mechanism.</li><li class="listitem" style="list-style-type: disc">Compose is focused on processing the data.</li><li class="listitem" style="list-style-type: disc">Consume provisions the processed data for the consuming systems. Consign takes care of the data persistence.</li><li class="listitem" style="list-style-type: disc">Control caters to governance and monitoring of the systems, data, and applications.</li></ul></div><div><img src="img/B03968_05_09.jpg" alt="Developing data pipelines with Flume, Kafka, and Spark"/></div><p>The <a id="id345" class="indexterm"/>following diagram illustrates<a id="id346" class="indexterm"/> the concepts of the streaming<a id="id347" class="indexterm"/> data pipelines with its key components: Spark Streaming, Kafka, Flume, and low latency databases. In the consuming or controlling applications, we are monitoring our systems in real time (depicted by a monitor) or sending real-time alerts (depicted by red lights) in case certain thresholds are crossed.</p><div><img src="img/B03968_05_10.jpg" alt="Developing data pipelines with Flume, Kafka, and Spark"/></div><p>The following diagram illustrates Spark's unique ability to process in a single platform data in motion and data at rest while seamlessly interfacing with multiple persistence data stores as per the use case requirement.</p><p>This <a id="id348" class="indexterm"/>diagram brings in one unified <a id="id349" class="indexterm"/>whole all the concepts<a id="id350" class="indexterm"/> discussed up to now. The top part describes the streaming processing pipeline. The bottom part describes the batch processing pipeline. They both share a common persistence layer in the middle of the diagram depicting the various modes of persistence and serialization.</p><div><img src="img/B03968_05_11.jpg" alt="Developing data pipelines with Flume, Kafka, and Spark"/></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch05lvl1sec39"/>Closing remarks on the Lambda and Kappa architecture</h1></div></div></div><p>Two <a id="id351" class="indexterm"/>architecture paradigms are currently in vogue: the <a id="id352" class="indexterm"/>Lambda and Kappa architectures.</p><p>Lambda is the brainchild of the Storm creator and main committer, Nathan Marz. It essentially advocates building a functional architecture on all data. The architecture has two branches. The first is a batch arm envisioned to be powered by Hadoop, where historical, high-latency, high-throughput data are pre-processed and made ready for consumption. The real-time arm is envisioned to be powered by Storm, and it processes incrementally streaming data, derives insights on the fly, and feeds aggregated information back to the batch storage.</p><p>Kappa is the<a id="id353" class="indexterm"/> brainchild of one the main committer of Kafka, Jay Kreps, and <a id="id354" class="indexterm"/>his colleagues at Confluent (previously at LinkedIn). It is advocating a full streaming pipeline, effectively implementing, at the enterprise level, the unified log enounced in the previous pages.</p><div><div><div><div><h2 class="title"><a id="ch05lvl2sec54"/>Understanding Lambda architecture</h2></div></div></div><p>Lambda<a id="id355" class="indexterm"/> architecture combines batch and streaming data to provide a unified query mechanism on all available data. Lambda architecture envisions three layers: a batch layer where precomputed information are stored, a speed layer where real-time incremental information is processed as data streams, and finally the serving layer that merges batch and real-time views for ad hoc queries. The following diagram gives an overview of the Lambda architecture:</p><div><img src="img/B03968_05_12.jpg" alt="Understanding Lambda architecture"/></div></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec55"/>Understanding Kappa architecture</h2></div></div></div><p>The <a id="id356" class="indexterm"/>Kappa architecture proposes to drive the full enterprise in streaming mode. The Kappa architecture arose from a critique from Jay Kreps and his colleagues at LinkedIn at the time. Since then, they moved and created Confluent with Apache Kafka as the main enabler of the Kappa architecture vision. The basic tenet is to move in all streaming mode with a Unified Log as the main backbone of the enterprise information architecture.</p><p>A Unified Log is a centralized enterprise structured log available for real-time subscription. All the<a id="id357" class="indexterm"/> organization's data is put in a central log for subscription. Records are numbered beginning with zero so that they are written. It is also known as a commit log or journal. The concept of the Unified Log is the central tenet of the Kappa architecture.</p><p>The<a id="id358" class="indexterm"/> properties of the unified log are as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Unified</strong>: There is a single deployment for the entire organization</li><li class="listitem" style="list-style-type: disc"><strong>Append only</strong>: Events are immutable and are appended</li><li class="listitem" style="list-style-type: disc"><strong>Ordered</strong>: Each event has a unique offset within a shard</li><li class="listitem" style="list-style-type: disc"><strong>Distributed</strong>: For fault tolerance purpose, the unified log is distributed redundantly on a cluster of computers</li><li class="listitem" style="list-style-type: disc"><strong>Fast</strong>: The systems ingests thousands of messages per second</li></ul></div><p>The following screenshot captures the moment Jay Kreps announced his reservations about the Lambda architecture. His main reservation about the Lambda architecture is implementing the same job in two different systems, Hadoop and Storm, with each of their specific idiosyncrasies, and with all the complexities that come along with it. Kappa architecture processes the real-time data and reprocesses historical data in the same framework powered by Apache Kafka.</p><div><img src="img/B03968_05_13.jpg" alt="Understanding Kappa architecture"/></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch05lvl1sec40"/>Summary</h1></div></div></div><p>In this chapter, we laid out the foundations of streaming architecture apps and described their challenges, constraints, and benefits. We went under the hood and examined the inner working of Spark Streaming and how it fits with Spark Core and dialogues with Spark SQL and Spark MLlib. We illustrated the streaming concepts with TCP sockets, followed by live tweet ingestion and processing directly from the Twitter firehose. We discussed the notions of decoupling upstream data publishing from downstream data subscription and consumption using Kafka in order to maximize the resilience of the overall streaming architecture. We also discussed Flume—a reliable, flexible, and scalable data ingestion and transport pipeline system. The combination of Flume, Kafka, and Spark delivers unparalleled robustness, speed, and agility in an ever changing landscape. We closed the chapter with some remarks and observations on two streaming architectural paradigms, the Lambda and Kappa architectures.</p><p>The Lambda architecture combines batch and streaming data in a common query front-end. It was envisioned with Hadoop and Storm in mind initially. Spark has its own batch and streaming paradigms, and it offers a single environment with common code base to effectively bring this architecture paradigm to life.</p><p>The Kappa architecture promulgates the concept of the unified log, which creates an event-oriented architecture where all events in the enterprise are channeled in a centralized commit log that is available to all consuming systems in real time.</p><p>We are now ready for the visualization of the data collected and processed so far.</p></div></body></html>