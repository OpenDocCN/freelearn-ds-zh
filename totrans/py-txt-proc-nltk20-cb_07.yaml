- en: Chapter 7. Text Classification
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章 文本分类
- en: 'In this chapter, we will cover:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖：
- en: Bag of Words feature extraction
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词袋特征提取
- en: Training a naive Bayes classifier
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练朴素贝叶斯分类器
- en: Training a decision tree classifier
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练决策树分类器
- en: Training a maximum entropy classifier
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练最大熵分类器
- en: Measuring precision and recall of a classifier
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测量分类器的精确率和召回率
- en: Calculating high information words
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算高信息词
- en: Combining classifiers with voting
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结合分类器进行投票
- en: Classifying with multiple binary classifiers
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用多个二元分类器进行分类
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介
- en: '**Text classification** is a way to categorize documents or pieces of text.
    By examining the word usage in a piece of text, classifiers can decide what *class
    label* to assign to it. A **binary classifier** decides between two labels, such
    as positive or negative. The text can either be one label or the other, but not
    both, whereas a **multi-label classifier** can assign one or more labels to a
    piece of text.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**文本分类**是将文档或文本片段进行分类的一种方法。通过检查文本中的单词使用情况，分类器可以决定将其分配给什么**类标签**。一个**二元分类器**在两个标签之间进行选择，例如正面或负面。文本可以是其中一个标签，但不能同时是两个，而**多标签分类器**可以给一个文本片段分配一个或多个标签。'
- en: Classification works by learning from *labeled feature sets*, or training data,
    to later classify an *unlabeled feature set*. A **feature set** is basically a
    key-value mapping of *feature names* to *feature values*. In the case of text
    classification, the feature names are usually words, and the values are all `True`.
    As the documents may have unknown words, and the number of possible words may
    be very large, words that don't occur in the text are omitted, instead of including
    them in a feature set with the value `False`.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 分类是通过从**标记特征集**或训练数据中学习，以后来对**未标记特征集**进行分类。**特征集**基本上是**特征名称**到**特征值**的键值映射。在文本分类的情况下，特征名称通常是单词，而值都是`True`。由于文档可能包含未知单词，并且可能的单词数量可能非常大，因此省略了未出现在文本中的单词，而不是将它们包含在具有`False`值的特征集中。
- en: An **instance** is a single feature set. It represents a single occurrence of
    a combination of features. We will use *instance* and *feature set* interchangeably.
    A *labeled feature set* is an instance with a known class label that we can use
    for training or evaluation.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '**实例**是一个单独的特征集。它代表了一个特征组合的单次出现。我们将交替使用*实例*和*特征集*。一个**标记特征集**是一个具有已知类标签的实例，我们可以用它进行训练或评估。'
- en: Bag of Words feature extraction
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词袋特征提取
- en: Text feature extraction is the process of transforming what is essentially a
    list of words into a feature set that is usable by a classifier. The NLTK classifiers
    expect `dict` style feature sets, so we must therefore transform our text into
    a `dict`. The **Bag of Words** model is the simplest method; it constructs a *word
    presence* feature set from all the words of an instance.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 文本特征提取是将本质上是一系列单词转换成分类器可用的特征集的过程。NLTK分类器期望`dict`风格的特征集，因此我们必须将我们的文本转换成`dict`。**词袋模型**是最简单的方法；它从一个实例的所有单词中构建一个**单词出现**特征集。
- en: How to do it...
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'The idea is to convert a list of words into a `dict`, where each word becomes
    a key with the value `True`. The `bag_of_words()` function in `featx.py` looks
    like this:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 理念是将单词列表转换成一个`dict`，其中每个单词成为一个键，其值为`True`。`featx.py`中的`bag_of_words()`函数看起来像这样：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can use it with a list of words, in this case the tokenized sentence "the
    quick brown fox":'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用一个单词列表使用它，在这种情况下是分词后的句子"the quick brown fox"：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The resulting `dict` is known as a *bag of words* because the words are not
    in order, and it doesn't matter where in the list of words they occurred, or how
    many times they occurred. All that matters is that the word is found at least
    once.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的`dict`被称为**词袋**，因为单词没有顺序，它们在单词列表中的位置或出现的次数无关紧要。唯一重要的是单词至少出现一次。
- en: How it works...
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The `bag_of_words()` function is a very simple *list comprehension* that constructs
    a `dict` from the given words, where every word gets the value `True`.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '`bag_of_words()`函数是一个非常简单的**列表推导**，它从给定的单词构建一个`dict`，其中每个单词都得到`True`的值。'
- en: Since we have to assign a value to each word in order to create a `dict`, `True`
    is a logical choice for the value to indicate word presence. If we knew the universe
    of all possible words, we could assign the value `False` to all the words that
    are not in the given list of words. But most of the time, we don't know all possible
    words beforehand. Plus, the `dict` that would result from assigning `False` to
    every possible word would be very large (assuming all words in the English language
    are possible). So instead, to keep feature extraction simple and use less memory,
    we stick with assigning the value `True` to all words that occur at least once.
    We don't assign the value `False` to any words since we don't know what the set
    of possible words are; we only know about the words we are given.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们必须为每个单词分配一个值以创建`dict`，所以`True`是一个逻辑上的选择，用于表示单词的存在。如果我们知道所有可能的单词的宇宙，我们可以将值`False`分配给不在给定单词列表中的所有单词。但大多数时候，我们事先不知道所有可能的单词。此外，将`False`分配给所有可能的单词所得到的`dict`将会非常大（假设所有单词都是可能的）。因此，为了保持特征提取简单并使用更少的内存，我们坚持将值`True`分配给至少出现一次的所有单词。我们不分配`False`给任何单词，因为我们不知道可能的单词集合是什么；我们只知道我们给出的单词。
- en: There's more...
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多...
- en: In the default Bag of Words model, all words are treated equally. But that's
    not always a good idea. As we already know, some words are so common that they
    are practically meaningless. If you have a set of words that you want to exclude,
    you can use the `bag_of_words_not_in_set()` function in `featx.py`.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在默认的词袋模型中，所有单词都被同等对待。但这并不总是好主意。正如我们已经知道的，有些单词非常常见，以至于它们实际上没有意义。如果你有一组想要排除的单词，你可以使用`featx.py`中的`bag_of_words_not_in_set()`函数。
- en: '[PRE2]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This function can be used, among other things, to filter stopwords. Here''s
    an example where we filter the word "the" from "the quick brown fox":'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数可用于过滤停用词。以下是一个示例，其中我们从“the quick brown fox”中过滤掉单词“the”：
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As expected, the resulting `dict` has "quick", "brown", and "fox", but not "the".
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，结果`dict`中有“quick”、“brown”和“fox”，但没有“the”。
- en: Filtering stopwords
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 过滤停用词
- en: 'Here''s an example of using the `bag_of_words_not_in_set()` function to filter
    all English stopwords:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个使用`bag_of_words_not_in_set()`函数过滤所有英语停用词的示例：
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You can pass a different language filename as the `stopfile` keyword argument
    if you are using a language other than English. Using this function produces the
    same result as the previous example:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是除英语以外的语言，你可以将不同的语言文件名作为`stopfile`关键字参数传递。使用此函数产生的结果与上一个示例相同：
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here, "the" is a stopword, so it is not present in the returned `dict`.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，“the”是一个停用词，所以它不会出现在返回的`dict`中。
- en: Including significant bigrams
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 包括重要的二元组
- en: In addition to single words, it often helps to include significant bigrams.
    As significant bigrams are less common than most individual words, including them
    in the Bag of Words can help the classifier make better decisions. We can use
    the `BigramCollocationFinder` covered in the *Discovering word collocations* recipe
    of [Chapter 1](ch01.html "Chapter 1. Tokenizing Text and WordNet Basics"), *Tokenizing
    Text and WordNet Basics*, to find significant bigrams. `bag_of_bigrams_words()`
    found in `featx.py` will return a `dict` of all words along with the 200 most
    significant bigrams.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 除了单个单词外，通常包括重要的二元组也很有帮助。因为重要的二元组比大多数单个单词更不常见，所以在词袋模型中包括它们可以帮助分类器做出更好的决策。我们可以使用[第1章](ch01.html
    "第1章. 文本分词和WordNet基础")中“发现词搭配”食谱中提到的`BigramCollocationFinder`来找到重要的二元组。`featx.py`中的`bag_of_bigrams_words()`函数将返回一个包含所有单词以及200个最显著二元组的`dict`。
- en: '[PRE6]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The bigrams will be present in the returned `dict` as `(word1, word2)` and
    will have the value as `True`. Using the same example words as before, we get
    all words plus every bigram:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 二元组将以`(word1, word2)`的形式出现在返回的`dict`中，并将具有`True`的值。使用与之前相同的示例单词，我们得到所有单词以及每个二元组：
- en: '[PRE7]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: You can change the maximum number of bigrams found by altering the keyword argument
    `n`.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过改变关键字参数`n`来更改找到的最大二元组数。
- en: See also
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: The *Discovering word collocations* recipe of [Chapter 1](ch01.html "Chapter 1. Tokenizing
    Text and WordNet Basics"), *Tokenizing Text and WordNet Basics* covers the `BigramCollocationFinder`
    in more detail. In the next recipe, we will train a `NaiveBayesClassifier` using
    feature sets created with the Bag of Words model.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '[第1章](ch01.html "第1章. 文本分词和WordNet基础")中“发现词搭配”食谱的*Tokenizing Text and WordNet
    Basics*更详细地介绍了`BigramCollocationFinder`。在下一个食谱中，我们将使用词袋模型创建的特征集来训练`NaiveBayesClassifier`。'
- en: Training a naive Bayes classifier
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练朴素贝叶斯分类器
- en: 'Now that we can extract features from text, we can train a classifier. The
    easiest classifier to get started with is the `NaiveBayesClassifier` . It uses
    **Bayes Theorem** to predict the probability that a given feature set belongs
    to a particular label. The formula is:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以从文本中提取特征，我们可以训练一个分类器。最容易开始的分类器是 `NaiveBayesClassifier`。它使用 **贝叶斯定理** 来预测给定特征集属于特定标签的概率。公式是：
- en: '[PRE8]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '`P(label)` is the prior probability of the label occurring, which is the same
    as the likelihood that a random feature set will have the label. This is based
    on the number of training instances with the label compared to the total number
    of training instances. For example, if 60/100 training instances have the label,
    the prior probability of the label is 60 percent.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`P(label)` 是标签发生的先验概率，这与随机特征集具有该标签的似然性相同。这是基于具有该标签的训练实例数与训练实例总数的比例。例如，如果有 60/100
    的训练实例具有该标签，则该标签的先验概率是 60%。'
- en: '`P(features | label)` is the prior probability of a given feature set being
    classified as that label. This is based on which features have occurred with each
    label in the training data.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`P(features | label)` 是给定特征集被分类为该标签的先验概率。这是基于在训练数据中哪些特征与每个标签一起发生的。'
- en: '`P(features)` is the prior probability of a given feature set occurring. This
    is the likelihood of a random feature set being the same as the given feature
    set, and is based on the observed feature sets in the training data. For example,
    if the given feature set occurs twice in 100 training instances, the prior probability
    is 2 percent.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`P(features)` 是给定特征集发生的先验概率。这是随机特征集与给定特征集相同的似然性，基于训练数据中观察到的特征集。例如，如果给定特征集在
    100 个训练实例中出现了两次，则先验概率是 2%。'
- en: '`P(label | features)` tells us the probability that the given features should
    have that label. If this value is high, then we can be reasonably confident that
    the label is correct for the given features.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`P(label | features)` 告诉我们给定特征应该具有该标签的概率。如果这个值很高，那么我们可以合理地确信该标签对于给定特征是正确的。'
- en: Getting ready
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We are going to be using the `movie_reviews` corpus for our initial classification
    examples. This corpus contains two categories of text: `pos` and `neg`. These
    categories are exclusive, which makes a classifier trained on them a **binary
    classifier**. Binary classifiers have only two classification labels, and will
    always choose one or the other.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 `movie_reviews` 语料库作为我们最初的分类示例。这个语料库包含两种文本类别：`pos` 和 `neg`。这些类别是互斥的，这使得在它们上训练的分类器是一个
    **二元分类器**。二元分类器只有两个分类标签，并且总是选择其中一个。
- en: Each file in the `movie_reviews` corpus is composed of either positive or negative
    movie reviews. We will be using each file as a single instance for both training
    and testing the classifier. Because of the nature of the text and its categories,
    the classification we will be doing is a form of *sentiment analysis*. If the
    classifier returns `pos`, then the text expresses *positive sentiment*, whereas
    if we get `neg`, then the text expresses *negative sentiment*.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '`movie_reviews` 语料库中的每个文件都是由正面或负面电影评论组成的。我们将使用每个文件作为训练和测试分类器的单个实例。由于文本的性质及其类别，我们将进行的分类是一种
    *情感分析*。如果分类器返回 `pos`，则文本表达 *积极情感*；而如果得到 `neg`，则文本表达 *消极情感*。'
- en: How to do it...
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'For training, we need to first create a list of labeled feature sets. This
    list should be of the form `[(featureset, label)]` where the `featureset` is a
    `dict`, and `label` is the known class label for the `featureset`. The `label_feats_from_corpus()`
    function in `featx.py` takes a corpus, such as `movie_reviews`, and a `feature_detector`
    function, which defaults to `bag_of_words`. It then constructs and returns a mapping
    of the form `{label: [featureset]}`. We can use this mapping to create a list
    of labeled *training instances* and *testing instances*. The reason to do it this
    way is because we can get a fair sample from each label.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '对于训练，我们首先需要创建一个标记特征集的列表。这个列表应该是 `[(featureset, label)]` 的形式，其中 `featureset`
    是一个 `dict`，`label` 是 `featureset` 的已知类标签。`featx.py` 中的 `label_feats_from_corpus()`
    函数接受一个语料库，例如 `movie_reviews`，以及一个 `feature_detector` 函数，默认为 `bag_of_words`。然后它构建并返回一个形式为
    `{label: [featureset]}` 的映射。我们可以使用这个映射来创建一个标记的 *训练实例* 和 *测试实例* 的列表。这样做的原因是因为我们可以从每个标签中获得一个公平的样本。'
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Once we can get a mapping of `label : feature` sets, we want to construct a
    list of labeled training instances and testing instances. The function `split_label_feats()`
    in `featx.py` takes a mapping returned from `label_feats_from_corpus()` and splits
    each list of feature sets into labeled training and testing instances.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '一旦我们能够得到`label : feature`集合的映射，我们希望构建一个标签训练实例和测试实例的列表。`featx.py`中的`split_label_feats()`函数接受从`label_feats_from_corpus()`返回的映射，并将每个特征集合列表分割成标签训练实例和测试实例。'
- en: '[PRE10]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Using these functions with the `movie_reviews` corpus gives us the lists of
    labeled feature sets we need to train and test a classifier.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些函数与`movie_reviews`语料库一起，我们可以得到训练和测试分类器所需的标签特征集合列表。
- en: '[PRE11]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: So there are 1,000 `pos` files, 1,000 `neg` files, and we end up with 1,500
    labeled training instances and 500 labeled testing instances, each composed of
    equal parts `pos` and `neg`. Now we can train a `NaiveBayesClassifier` using its
    `train()` class method,
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 因此有1,000个`pos`文件，1,000个`neg`文件，我们最终得到1,500个标签训练实例和500个标签测试实例，每个实例由等量的`pos`和`neg`组成。现在我们可以使用它的`train()`类方法来训练一个`NaiveBayesClassifier`，
- en: '[PRE12]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Let's test the classifier on a couple of made up reviews. The `classify()` method
    takes a single argument, which should be a feature set. We can use the same `bag_of_words()`
    feature detector on a made up list of words to get our feature set.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在几个虚构的评论上测试一下分类器。`classify()`方法接受一个单一参数，这个参数应该是一个特征集合。我们可以使用相同的`bag_of_words()`特征检测器对一个虚构的单词列表进行检测，以获取我们的特征集合。
- en: '[PRE13]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: How it works...
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The `label_feats_from_corpus()` assumes that the corpus is categorized, and
    that a single file represents a single instance for feature extraction. It iterates
    over each category label, and extracts features from each file in that category
    using the `feature_detector()` function, which defaults to `bag_of_words()`. It
    returns a `dict` whose keys are the category labels, and the values are lists
    of instances for that category.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`label_feats_from_corpus()`假设语料库是分类的，并且单个文件代表一个用于特征提取的实例。它遍历每个分类标签，并使用默认为`bag_of_words()`的`feature_detector()`函数从该分类中的每个文件中提取特征。它返回一个字典，其键是分类标签，值是该分类的实例列表。'
- en: Note
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: If we had the `label_feats_from_corpus()` function, return a list of labeled
    feature sets, instead of a dict, it would be much harder to get the balanced training
    data. The list would be ordered by label, and if you took a slice of it, you would
    almost certainly be getting far more of one label than another. By returning a
    `dict`, you can take slices from the feature sets of each label.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`label_feats_from_corpus()`函数返回一个标签特征集合的列表，而不是字典，那么获取平衡的训练数据将会更加困难。列表将按标签排序，如果你从中取一个子集，你几乎肯定会得到比另一个标签多得多的一个标签。通过返回一个字典，你可以从每个标签的特征集合中取子集。
- en: Now we need to split the labeled feature sets into training and testing instances
    using `split_label_feats()` . This function allows us to take a fair sample of
    labeled feature sets from each label, using the `split` keyword argument to determine
    the size of the sample. `split` defaults to `0.75`, which means the first three-fourths
    of the labeled feature sets for each label will be used for training, and the
    remaining one-fourth will be used for testing.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要使用`split_label_feats()`将标签特征集合分割成训练和测试实例。这个函数允许我们从每个标签中公平地抽取标签特征集合的样本，使用`split`关键字参数来确定样本的大小。`split`默认为`0.75`，这意味着每个标签的前三分之四的标签特征集合将用于训练，剩下的四分之一将用于测试。
- en: Once we have split up our training and testing feats, we train a classifier
    using the `NaiveBayesClassifier.train()` method. This class method builds two
    probability distributions for calculating prior probabilities. These are passed
    in to the `NaiveBayesClassifier` constructor. The `label_probdist` contains `P(label)`,
    the prior probability for each label. The `feature_probdist` contains `P(feature
    name = feature value | label)`. In our case, it will store `P(word=True | label)`.
    Both are calculated based on the frequency of occurrence of each label, and each
    feature name and value in the training data.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们将训练和测试特征分割开，我们使用`NaiveBayesClassifier.train()`方法训练一个分类器。这个类方法构建两个概率分布来计算先验概率。这些被传递给`NaiveBayesClassifier`构造函数。`label_probdist`包含`P(label)`，每个标签的先验概率。`feature_probdist`包含`P(feature
    name = feature value | label)`。在我们的情况下，它将存储`P(word=True | label)`。这两个都是基于训练数据中每个标签以及每个特征名称和值的频率来计算的。
- en: 'The `NaiveBayesClassifier` inherits from `ClassifierI`, which requires subclasses
    to provide a `labels()` method, and at least one of the `classify()` and `prob_classify()`
    methods. T he following diagram shows these and other methods, which will be covered
    shortly:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '`NaiveBayesClassifier` 继承自 `ClassifierI`，这要求子类提供 `labels()` 方法，以及至少一个 `classify()`
    或 `prob_classify()` 方法。以下图显示了这些和其他方法，稍后将会介绍：'
- en: '![How it works...](img/3609OS_07_01.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作...](img/3609OS_07_01.jpg)'
- en: There's more...
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: We can test the accuracy of the classifier using `nltk.classify.util.accuracy()`
    and the `test_feats` created previously.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `nltk.classify.util.accuracy()` 和之前创建的 `test_feats` 来测试分类器的准确度。
- en: '[PRE14]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This tells us that the classifier correctly guessed the label of nearly 73 percent
    of the testing feature sets.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们，分类器正确猜测了几乎 73% 的测试特征集的标签。
- en: Classification probability
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分类概率
- en: While the `classify()` method returns only a single label, you can use the `prob_classify()`
    method to get the classification probability of each label. This can be useful
    if you want to use probability thresholds greater than 50 percent for classification.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 `classify()` 方法只返回一个标签，但你可以使用 `prob_classify()` 方法来获取每个标签的分类概率。如果你想要使用超过
    50% 的概率阈值进行分类，这可能很有用。
- en: '[PRE15]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In this case, the classifier says that the first testing instance is nearly
    100 percent likely to be `pos`.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，分类器表示第一个测试实例几乎 100% 可能是 `pos`。
- en: Most informative features
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最具信息量的特征
- en: The `NaiveBayesClassifier` has two methods that are quite useful for learning
    about your data. Both methods take a keyword argument `n` to control how many
    results to show. The `most_informative_features()` method returns a list of the
    form `[(feature name, feature value)]` ordered by most informative to least informative.
    In our case, the feature value will always be `True`.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`NaiveBayesClassifier` 有两种非常实用的方法，可以帮助你了解你的数据。这两种方法都接受一个关键字参数 `n` 来控制显示多少个结果。`most_informative_features()`
    方法返回一个形式为 `[(feature name, feature value)]` 的列表，按信息量从大到小排序。在我们的例子中，特征值总是 `True`。'
- en: '[PRE16]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The `show_most_informative_features()` method will print out the results from
    `most_informative_features()` and will also include the probability of a feature
    pair belonging to each label.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`show_most_informative_features()` 方法将打印出 `most_informative_features()` 的结果，并包括特征对属于每个标签的概率。'
- en: '[PRE17]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The *informativeness*, or **information gain**, of each feature pair is based
    on the prior probability of the feature pair occurring for each label. More informative
    features are those that occur primarily in one label and not the other. Less informative
    features are those that occur frequently in both labels.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 每个特征对的 *信息量*，或 **信息增益**，基于每个标签发生特征对的先验概率。信息量大的特征主要出现在一个标签中，而不是另一个标签。信息量小的特征是那些在两个标签中都频繁出现的特征。
- en: Training estimator
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练估计器
- en: 'During training, the `NaiveBayesClassifier` constructs its probability distributions
    using an `estimator` parameter, which defaults to `nltk.probability.ELEProbDist`.
    But you can use any `estimator` you want, and there are quite a few to choose
    from. The only constraints are that it must inherit from `nltk.probability.ProbDistI`
    and its constructor must take a `bins` keyword argument. Here''s an example using
    the `LaplaceProdDist`:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，`NaiveBayesClassifier` 使用 `estimator` 参数构建其概率分布，该参数默认为 `nltk.probability.ELEProbDist`。但你可以使用任何你想要的
    `estimator`，并且有很多可供选择。唯一的限制是它必须继承自 `nltk.probability.ProbDistI`，并且其构造函数必须接受一个
    `bins` 关键字参数。以下是一个使用 `LaplaceProdDist` 的示例：
- en: '[PRE18]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: As you can see, accuracy is slightly lower, so choose your `estimator` carefully.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，准确度略有下降，所以请仔细选择你的 `estimator`。
- en: Note
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'You cannot use `nltk.probability.MLEProbDist` as the estimator, or any `ProbDistI`
    subclass that does not take the `bins` keyword argument. Training will fail with
    `TypeError: __init__() got an unexpected keyword argument ''bins''`.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '你不能使用 `nltk.probability.MLEProbDist` 作为估计器，或者任何不接受 `bins` 关键字参数的 `ProbDistI`
    子类。训练将因 `TypeError: __init__() got an unexpected keyword argument ''bins''` 而失败。'
- en: Manual training
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 手动训练
- en: 'You don''t have to use the `train()` class method to construct a `NaiveBayesClassifier`.
    You can instead create the `label_probdist` and `feature_probdist` manually. `label_probdist`
    should be an instance of `ProbDistI`, and should contain the prior probabilities
    for each label. `feature_probdist` should be a `dict` whose keys are tuples of
    the form `(label, feature name)` and whose values are instances of `ProbDistI`
    that have the probabilities for each feature value. In our case, each `ProbDistI`
    should have only one value, `True=1`. Here''s a very simple example using manually
    constructed `DictionaryProbDist`:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 你不必使用 `train()` 类方法来构建 `NaiveBayesClassifier`。你可以手动创建 `label_probdist` 和 `feature_probdist`。`label_probdist`
    应该是 `ProbDistI` 的一个实例，并且应该包含每个标签的先验概率。`feature_probdist` 应该是一个 `dict`，其键是形式为 `(label,
    feature name)` 的元组，其值是具有每个特征值概率的 `ProbDistI` 的实例。在我们的情况下，每个 `ProbDistI` 应该只有一个值，`True=1`。以下是一个使用手动构建的
    `DictionaryProbDist` 的非常简单的例子：
- en: '[PRE19]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: See also
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: In the next recipes, we will train two more classifiers, the `DecisionTreeClassifier`,
    and the `MaxentClassifier`. In the *Measuring precision and recall of a classifier*
    recipe in this chapter, we will use precision and recall instead of accuracy to
    evaluate the classifiers. And then in the *Calculating high information words*
    recipe, we will see how using only the most informative features can improve classifier
    performance.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的菜谱中，我们将训练另外两个分类器，即 `DecisionTreeClassifier` 和 `MaxentClassifier`。在本章的 *Measuring
    precision and recall of a classifier* 菜谱中，我们将使用精确率和召回率而不是准确率来评估分类器。然后在 *Calculating
    high information words* 菜谱中，我们将看到仅使用最有信息量的特征如何提高分类器性能。
- en: The `movie_reviews` corpus is an instance of `CategorizedPlaintextCorpusReader`,
    which is covered in the *Creating a categorized text corpus* recipe in [Chapter
    3](ch03.html "Chapter 3. Creating Custom Corpora"), *Creating Custom Corpora*.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '`movie_reviews` 语料库是 `CategorizedPlaintextCorpusReader` 的一个实例，这在 [第 3 章](ch03.html
    "第 3 章。创建自定义语料库") 的 *Creating a categorized text corpus* 菜谱中有详细说明，*Creating Custom
    Corpora*。'
- en: Training a decision tree classifier
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练决策树分类器
- en: The `DecisionTreeClassifier` works by creating a tree structure, where each
    node corresponds to a feature name, and the branches correspond to the feature
    values. Tracing down the branches, you get to the leaves of the tree, which are
    the classification labels.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '`DecisionTreeClassifier` 通过创建一个树结构来工作，其中每个节点对应一个特征名称，分支对应特征值。沿着分支追踪，你会到达树的叶子，它们是分类标签。'
- en: Getting ready
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: For the `DecisionTreeClassifier` to work for text classification, you must use
    NLTK 2.0b9 or later. This is because earlier versions are unable to deal with
    unknown features. If the `DecisionTreeClassifier` encountered a word/feature that
    it hadn't seen before, then it raised an exception. This bug has now been fixed
    by yours truly, and is included in all NLTK versions since 2.0b9.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使 `DecisionTreeClassifier` 能够用于文本分类，你必须使用 NLTK 2.0b9 或更高版本。这是因为早期版本无法处理未知特征。如果
    `DecisionTreeClassifier` 遇到了它之前没有见过的单词/特征，那么它会引发异常。这个错误已经被我修复，并包含在所有自 2.0b9 版本以来的
    NLTK 版本中。
- en: How to do it...
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Using the same `train_feats` and `test_feats` we created from the `movie_reviews`
    corpus in the previous recipe, we can call the `DecisionTreeClassifier.train()`
    class method to get a trained classifier. We pass `binary=True` because all of
    our features are binary: either the word is present or it''s not. For other classification
    use cases where you have multi-valued features, you will want to stick to the
    default `binary=False`.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们在上一个菜谱中从 `movie_reviews` 语料库创建的相同的 `train_feats` 和 `test_feats`，我们可以调用 `DecisionTreeClassifier.train()`
    类方法来获取一个训练好的分类器。我们传递 `binary=True`，因为我们的所有特征都是二元的：要么单词存在，要么不存在。对于其他具有多值特征的分类用例，你将想要坚持默认的
    `binary=False`。
- en: Note
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: In this context, `binary` refers to *feature values*, and is not to be confused
    with a *binary classifier*. Our word features are binary because the value is
    either `True`, or the word is not present. If our features could take more than
    two values, we would have to use `binary=False`. A *binary classifier*, on the
    other hand, is a classifier that only chooses between two labels. In our case,
    we are training a binary `DecisionTreeClassifier` on binary features. But it's
    also possible to have a binary classifier with non-binary features, or a non-binary
    classifier with binary features.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个上下文中，`binary`指的是*特征值*，不要与*二元分类器*混淆。我们的词特征是二元的，因为值要么是`True`，要么该词不存在。如果我们的特征可以取超过两个值，我们就必须使用`binary=False`。另一方面，*二元分类器*是一种只选择两个标签的分类器。在我们的情况下，我们正在对二元特征训练一个二元`DecisionTreeClassifier`。但也可以有一个具有非二元特征的二元分类器，或者一个具有二元特征的非二元分类器。
- en: 'Following is the code for training and evaluating the accuracy of a `DecisionTreeClassifier`:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是训练和评估`DecisionTreeClassifier`准确性的代码：
- en: '[PRE20]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Tip
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: The `DecisionTreeClassifier` can take much longer to train than the `NaiveBayesClassifier`.
    For that reason, the default parameters have been overridden so it trains faster.
    These parameters will be explained later.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`DecisionTreeClassifier`的训练时间可能比`NaiveBayesClassifier`长得多。因此，默认参数已被覆盖，以便更快地训练。这些参数将在后面解释。'
- en: How it works...
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The `DecisionTreeClassifier`, like the `NaiveBayesClassifier`, is also an instance
    of `ClassifierI`. During training, the `DecisionTreeClassifier` creates a tree
    where the child nodes are also instances of `DecisionTreeClassifier`. The leaf
    nodes contain only a single label, while the intermediate child nodes contain
    decision mappings for each feature. These decisions map each feature value to
    another `DecisionTreeClassifier`, which itself may contain decisions for another
    feature, or it may be a final leaf node with a classification label. The `train()`
    class method builds this tree from the ground up, starting with the leaf nodes.
    It then refines itself to minimize the number of decisions needed to get to a
    label by putting the most informative features at the top.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '`DecisionTreeClassifier`，就像`NaiveBayesClassifier`一样，也是`ClassifierI`的一个实例。在训练过程中，`DecisionTreeClassifier`创建一个树，其中子节点也是`DecisionTreeClassifier`的实例。叶节点只包含一个标签，而中间子节点包含每个特征的决策映射。这些决策将每个特征值映射到另一个`DecisionTreeClassifier`，该`DecisionTreeClassifier`本身可能包含对另一个特征的决策，或者它可能是一个带有分类标签的最终叶节点。`train()`类方法从叶节点开始构建这个树。然后它通过将最有信息量的特征放在顶部来优化自己，以最小化到达标签所需的决策数量。'
- en: To classify, the `DecisionTreeClassifier` looks at the given feature set and
    traces down the tree, using known feature names and values to make decisions.
    Because we are creating a *binary tree*, each `DecisionTreeClassifier` instance
    also has a *default* decision tree, which it uses when a known feature is not
    present in the feature set being classified. This is a common occurrence in text-based
    feature sets, and indicates that a known word was not in the text being classified.
    This also contributes information towards a classification decision.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 为了分类，`DecisionTreeClassifier`会查看给定的特征集，并沿着树向下追踪，使用已知的特征名称和值来做出决策。因为我们创建了一个*二叉树*，每个`DecisionTreeClassifier`实例还有一个*默认*决策树，当分类的特征集中不存在已知特征时，它就会使用这个决策树。这在基于文本的特征集中很常见，表明在分类的文本中没有出现已知单词。这也为分类决策提供了信息。
- en: There's more...
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The parameters passed in to `DecisionTreeClassifier.train()` can be tweaked
    to improve accuracy or decrease training time. Generally, if you want to improve
    accuracy, you must accept a longer training time and if you want to decrease the
    training time, the accuracy will most likely decrease as well.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 传递给`DecisionTreeClassifier.train()`的参数可以调整以提高准确性或减少训练时间。一般来说，如果你想提高准确性，你必须接受更长的训练时间，而如果你想减少训练时间，准确性很可能会降低。
- en: Entropy cutoff
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 熵截止值
- en: The `entropy_cutoff` is used during the tree refinement process. If the entropy
    of the probability distribution of label choices in the tree is greater than the
    `entropy_cutoff`, then the tree is refined further. But if the entropy is lower
    than the `entropy_cutoff`, then tree refinement is halted.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '`entropy_cutoff`在树优化过程中使用。如果树中标签选择概率分布的熵大于`entropy_cutoff`，则进一步优化树。但如果熵低于`entropy_cutoff`，则停止树优化。'
- en: '**Entropy** is the uncertainty of the outcome. As entropy approaches 1.0, uncertainty
    increases and, conversely, as entropy approaches 0.0, uncertainty decreases. In
    other words, when you have similar probabilities, the entropy will be high as
    each probability has a similar likelihood (or uncertainty of occurrence). But
    the more the probabilities differ, the lower the entropy will be.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**熵**是结果的不确定性。当熵接近1.0时，不确定性增加；相反，当熵接近0.0时，不确定性减少。换句话说，当你有相似的概率时，熵会很高，因为每个概率都有相似的似然性（或发生的不确定性）。但是，概率差异越大，熵就越低。'
- en: 'Entropy is calculated by giving `nltk.probability.entropy()` a `MLEProbDist`
    created from a `FreqDist` of label counts. Here''s an example showing the entropy
    of various `FreqDist` values:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 熵是通过给`nltk.probability.entropy()`传递一个由标签计数`FreqDist`创建的`MLEProbDist`来计算的。以下是一个显示各种`FreqDist`值的熵的示例：
- en: '[PRE21]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: What this all means is that if the label occurrence is very skewed one way or
    the other, the tree doesn't need to be refined because entropy/uncertainty is
    low. But when the entropy is greater than `entropy_cutoff` then the tree must
    be refined with further decisions to reduce the uncertainty. Higher values of
    `entropy_cutoff` will decrease both accuracy and training time.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切意味着，如果标签发生非常倾斜，树不需要被细化，因为熵/不确定性低。但是，当熵大于`entropy_cutoff`时，树必须通过进一步的决策来细化，以减少不确定性。`entropy_cutoff`的值越高，准确性和训练时间都会降低。
- en: Depth cutoff
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度截止
- en: The `depth_cutoff` is also used during refinement to control the depth of the
    tree. The final decision tree will never be deeper than the `depth_cutoff`. The
    default value is `100`, which means that classification may require up to 100
    decisions before reaching a leaf node. Decreasing the `depth_cutoff` will decrease
    the training time and most likely decrease the accuracy as well.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '`depth_cutoff`也在细化过程中使用，以控制树的深度。最终的决策树永远不会比`depth_cutoff`更深。默认值是`100`，这意味着分类可能需要最多100个决策才能到达叶节点。减少`depth_cutoff`将减少训练时间，并且很可能会降低准确性。'
- en: Support cutoff
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 支持截止
- en: The `support_` `cutoff` controls how many labeled feature sets are required
    to refine the tree. As the `DecisionTreeClassifier` refines itself, labeled feature
    sets are eliminated once they no longer provide value to the training process.
    When the number of labeled feature sets is less than or equal to `support_cutoff`,
    refinement stops, at least for that section of the tree.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '`support_` `cutoff`控制需要多少个标记的特征集来细化树。随着`DecisionTreeClassifier`的自我细化，一旦标记的特征集不再对训练过程有价值，它们就会被消除。当标记的特征集数量小于或等于`support_cutoff`时，细化停止，至少对于树的那个部分。'
- en: Another way to look at it is that `support_cutoff` specifies the minimum number
    of instances that are required to make a decision about a feature. If `support_cutoff`
    is `20`, and you have less than 20 labeled feature sets with a given feature,
    then you don't have enough instances to make a good decision, and refinement around
    that feature must come to a stop.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种看待它的方法是`support_cutoff`指定了做出关于一个特征的决定所需的最小实例数。如果`support_cutoff`是`20`，而你拥有的标记特征集少于20个，那么你没有足够的实例来做出好的决定，并且围绕该特征的细化必须停止。
- en: See also
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: The previous recipe covered the creation of training and test feature sets from
    the `movie_reviews` corpus. In the next recipe, we will cover training a `MaxentClassifier`,
    and in the *Measuring precision and recall of a classifier* recipe in this chapter,
    we will use precision and recall to evaluate all the classifiers.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的配方涵盖了从`movie_reviews`语料库创建训练和测试特征集。在下一个配方中，我们将介绍如何训练`MaxentClassifier`，在本章的*测量分类器的精确度和召回率*配方中，我们将使用精确度和召回率来评估所有分类器。
- en: Training a maximum entropy classifier
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练最大熵分类器
- en: The third classifier which we will cover is the `MaxentClassifier`, also known
    as a *conditional exponential classifier*. The **maximum entropy classifier**
    converts labeled feature sets to vectors using encoding. This encoded vector is
    then used to calculate *weights* for each feature that can then be combined to
    determine the most likely label for a feature set.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要介绍的第三个分类器是`MaxentClassifier`，也称为*条件指数分类器*。**最大熵分类器**使用编码将标记的特征集转换为向量。然后，使用这个编码向量来计算每个特征的*权重*，然后可以将这些权重组合起来，以确定特征集的最可能标签。
- en: Getting ready
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: The `MaxentClassifier` requires the `numpy` package, and optionally the `scipy`
    package. This is because the feature encodings use `numpy` arrays. Having `scipy`
    installed also means you will be able to use faster algorithms that consume less
    memory. You can find installation for both at [http://www.scipy.org/Installing_SciPy](http://www.scipy.org/Installing_SciPy).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '`MaxentClassifier` 分类器需要 `numpy` 包，并且可选地需要 `scipy` 包。这是因为特征编码使用了 `numpy` 数组。安装
    `scipy` 也意味着你可以使用更快的算法，这些算法消耗更少的内存。你可以在 [http://www.scipy.org/Installing_SciPy](http://www.scipy.org/Installing_SciPy)
    找到两者的安装方法。'
- en: Tip
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: Many of the algorithms can be quite memory hungry, so you may want to quit all
    your other programs while training a `MaxentClassifier`, just to be safe.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 许多算法可能会非常消耗内存，所以在训练 `MaxentClassifier` 时，你可能想要关闭所有其他程序，以确保安全。
- en: How to do it...
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: We will use the same `train_feats` and `test_feats` from the `movie_reviews`
    corpus that we constructed before, and call the `MaxentClassifier.train()` class
    method. Like the `DecisionTreeClassifier`, `MaxentClassifier.train()` has its
    own specific parameters that have been tweaked to speed up training. These parameters
    will be explained in more detail later.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用之前构建的 `movie_reviews` 语料库中的相同的 `train_feats` 和 `test_feats`，并调用 `MaxentClassifier.train()`
    类方法。与 `DecisionTreeClassifier` 类似，`MaxentClassifier.train()` 有其自己的特定参数，这些参数已被调整以加快训练速度。这些参数将在稍后进行更详细的解释。
- en: '[PRE22]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The reason this classifier has such a low accuracy is because the parameters
    have been set such that it is unable to learn a more accurate model. This is due
    to the time required to train a suitable model using the `iis` algorithm. Higher
    accuracy models can be learned much faster using the `scipy` algorithms.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分类器准确率如此低的原因是因为参数被设置为无法学习更准确的模型。这是由于使用 `iis` 算法训练合适模型所需的时间。使用 `scipy` 算法可以更快地学习到更高准确率的模型。
- en: Tip
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: If training is taking a long time, you can usually cut it off manually by hitting
    *Ctrl + C*. This should stop the current iteration and still return a classifier
    based on whatever state the model is in.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如果训练过程耗时过长，你通常可以通过按 *Ctrl + C* 手动中断。这应该会停止当前迭代，并基于模型当前的状态返回一个分类器。
- en: How it works...
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: Like the previous classifiers, `MaxentClassifier` inherits from `ClassifierI`.
    Depending on the algorithm, `MaxentClassifier.train()` calls one of the training
    functions in the `nltk.classify.maxent` module. If `scipy` is not installed, the
    default algorithm is `iis`, and the function used is `train_maxent_classifier_with_iis()`.
    The other algorithm that doesn't require `scipy` is `gis`, which uses the `train_maxent_classifier_with_gis()`
    function. **gis** stands for **General Iterative Scaling**, while **iis** stands
    for **Improved Iterative Scaling**. If `scipy` is installed, the `train_maxent_classifier_with_scipy()`
    function is used, and the default algorithm is `cg`. If `megam` is installed and
    you specify the `megam` algorithm, then `train_maxent_classifier_with_megam()`
    is used.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的分类器一样，`MaxentClassifier` 继承自 `ClassifierI`。根据算法的不同，`MaxentClassifier.train()`
    会调用 `nltk.classify.maxent` 模块中的某个训练函数。如果没有安装 `scipy`，则默认算法是 `iis`，使用的函数是 `train_maxent_classifier_with_iis()`。另一种不需要
    `scipy` 的算法是 `gis`，它使用 `train_maxent_classifier_with_gis()` 函数。**gis** 代表 **General
    Iterative Scaling**，而 **iis** 代表 **Improved Iterative Scaling**。如果安装了 `scipy`，则使用
    `train_maxent_classifier_with_scipy()` 函数，默认算法是 `cg`。如果安装了 `megam` 并指定了 `megam`
    算法，则使用 `train_maxent_classifier_with_megam()`。
- en: The basic idea behind the maximum entropy model is to build some probability
    distributions that fit the observed data, then choose whichever probability distribution
    has the highest entropy. The `gis` and `iis` algorithms do so by iteratively improving
    the weights used to classify features. This is where the `max_iter` and `min_lldelta`
    parameters come into play.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 最大熵模型背后的基本思想是构建一些适合观察数据的概率分布，然后选择具有最高熵的任何概率分布。`gis` 和 `iis` 算法通过迭代改进用于分类特征的权重来实现这一点。这就是
    `max_iter` 和 `min_lldelta` 参数发挥作用的地方。
- en: The `max_iter` specifies the maximum number of iterations to go through and
    update the weights. More iterations will generally improve accuracy, but only
    up to a point. Eventually, the changes from one iteration to the next will hit
    a plateau and further iterations are useless.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_iter` 指定了要遍历并更新权重的最大迭代次数。更多的迭代通常可以提高准确率，但仅限于某个程度。最终，从一个迭代到下一个迭代的改变将达到一个平台期，进一步的迭代将变得无用。'
- en: The `min_lldelta` specifies the minimum change in the *log likelihood* required
    to continue iteratively improving the weights. Before beginning training iterations,
    an instance of the `nltk.classify.util.CutoffChecker` is created. When its `check()`
    method is called, it uses functions such as `nltk.classify.util.log_likelihood()`
    to decide whether the cutoff limits have been reached. The **log** **likelihood**
    is the log (using `math.log()`) of the average label probability of the training
    data (which is the log of the average likelihood of a label). As the log likelihood
    increases, the model improves. But it too will reach a plateau where further increases
    are so small that there is no point in continuing. Specifying the `min_lldelta`
    allows you to control how much each iteration must increase the log likelihood
    before stopping iterations.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '`min_lldelta` 指定了在迭代改进权重之前所需的 *对数似然* 的最小变化。在开始训练迭代之前，创建了一个 `nltk.classify.util.CutoffChecker`
    的实例。当调用其 `check()` 方法时，它使用 `nltk.classify.util.log_likelihood()` 等函数来决定是否达到了截止限制。**对数**
    **似然** 是训练数据平均标签概率的对数（即标签平均似然的对数）。随着对数似然的增加，模型会改进。但这也将达到一个平台期，进一步的增加非常小，继续下去没有意义。指定
    `min_lldelta` 允许你控制每次迭代在停止迭代之前必须增加多少对数似然。'
- en: There's more...
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容...
- en: Like the `NaiveBayesClassifier`, you can see the most informative features by
    calling the `show_most_informative_features()` method.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 与 `NaiveBayesClassifier` 类似，你可以通过调用 `show_most_informative_features()` 方法来查看最有信息量的特征。
- en: '[PRE23]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The numbers shown are the weights for each feature. This tells us that the word
    *worst* is *negatively weighted* towards the `pos` label, and *positively weighted*
    towards the `neg` label. In other words, if the word *worst* is found in the feature
    set, then there's a strong possibility that the text should be classified `neg`.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 显示的数字是每个特征的权重。这告诉我们，单词 *worst* 对 `pos` 标签具有 *负权重*，对 `neg` 标签具有 *正权重*。换句话说，如果单词
    *worst* 出现在特征集中，那么文本被分类为 `neg` 的可能性很高。
- en: Scipy algorithms
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Scipy 算法
- en: 'The algorithms available when `scipy` is installed are:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 当安装了 `scipy` 时可用的算法有：
- en: '**CG** (**Conjugate gradient** algorithm)—the default `scipy` algorithm'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CG**（**共轭梯度** 算法）——默认的 `scipy` 算法'
- en: '**BFGS** (**Broyden-Fletcher-Goldfarb-Shanno** algorithm)—very memory hungry'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**BFGS**（**Broyden-Fletcher-Goldfarb-Shanno** 算法）——非常占用内存'
- en: Powell
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Powell
- en: LBFGSB (limited memory version of BFGS)
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LBFGSB（BFGS的内存限制版本）
- en: Nelder-Mead
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nelder-Mead
- en: 'Here''s what happens when you use the CG algorithm:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使用 CG 算法时发生的情况：
- en: '[PRE24]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This is the most accurate classifier so far.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这是迄今为止最准确的分类器。
- en: Megam algorithm
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Megam 算法
- en: If you have installed the `megam` package, then you can use the `megam` algorithm.
    It's a bit faster than the `scipy` algorithms and about as accurate. Installation
    instructions and information can be found at [http://www.cs.utah.edu/~hal/megam/](http://www.cs.utah.edu/~hal/megam/).
    The function `nltk.classify.megam.config_megam()` can be used to specify where
    the `megam` executable is found. Or, if `megam` can be found in the standard executable
    paths, NLTK will configure it automatically.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经安装了 `megam` 包，那么你可以使用 `megam` 算法。它比 `scipy` 算法快一点，并且大约一样准确。安装说明和信息可以在 [http://www.cs.utah.edu/~hal/megam/](http://www.cs.utah.edu/~hal/megam/)
    找到。可以使用 `nltk.classify.megam.config_megam()` 函数来指定 `megam` 可执行文件的位置。或者，如果 `megam`
    可以在标准可执行路径中找到，NLTK 将自动配置它。
- en: '[PRE25]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The `megam` algorithm is highly recommended for its accuracy and speed of training.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '`megam` 算法因其准确性和训练速度而被高度推荐。'
- en: See also
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: The *Bag of Words feature extraction* and the *Training a naive Bayes classifier*
    recipes in this chapter show how to construct the training and testing features
    from the `movie_reviews` corpus. In the next recipe, we will cover how and why
    to evaluate a classifier using precision and recall instead of accuracy.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中关于 *词袋特征提取* 和 *训练朴素贝叶斯分类器* 的食谱展示了如何从 `movie_reviews` 语料库中构建训练和测试特征。在下一个食谱中，我们将介绍如何以及为什么使用精确率和召回率而不是准确率来评估分类器。
- en: Measuring precision and recall of a classifier
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测量分类器的精确率和召回率
- en: In addition to accuracy, there are a number of other metrics used to evaluate
    classifiers. Two of the most common are *precision* and *recall*. To understand
    these two metrics, we must first understand *false positives* and *false negatives*.
    **False positives** happen when a classifier classifies a feature set with a label
    it shouldn't have. **False negatives** happen when a classifier doesn't assign
    a label to a feature set that should have it. In a *binary classifier*, these
    errors happen at the same time.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 除了准确率之外，还有许多其他指标用于评估分类器。其中最常见的是*精确率*和*召回率*。为了理解这两个指标，我们首先必须理解*假阳性*和*假阴性*。**假阳性**发生在分类器将一个特征集错误地分类为一个它不应该有的标签时。**假阴性**发生在分类器没有将标签分配给应该有标签的特征集时。在*二元分类器*中，这些错误同时发生。
- en: 'Here''s an example: the classifier classifies a movie review as `pos`, when
    it should have been `neg`. This counts as a *false positive* for the `pos` label,
    and a *false negative* for the `neg` label. If the classifier had correctly guessed
    `neg`, then it would count as a **true positive** for the `neg` label, and a **true
    negative** for the `pos` label.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个例子：分类器将一个电影评论错误地分类为`pos`，而它应该是`neg`。这算作`pos`标签的*假阳性*，以及`neg`标签的*假阴性*。如果分类器正确地猜测了`neg`，那么它将算作`neg`标签的**真阳性**，以及`pos`标签的**真阴性**。
- en: 'How does this apply to precision and recall? **Precision** is the *lack of
    false positives*, and **recall** is the *lack of false negatives*. As you will
    see, these two metrics are often in competition: the more precise a classifier
    is, the lower the recall, and vice versa.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这如何应用于精确率和召回率？**精确率**是*缺乏假阳性*，而**召回率**是*缺乏假阴性*。正如您将看到的，这两个指标通常存在竞争关系：一个分类器的精确率越高，召回率就越低，反之亦然。
- en: How to do it...
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何去做...
- en: 'Let''s calculate the precision and recall of the `NaiveBayesClassifier` we
    trained in the *Training a naive Bayes classifier* recipe. The `precision_recall()`
    function in `classification.py` looks like this:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算在*训练朴素贝叶斯分类器*菜谱中训练的`NaiveBayesClassifier`的精确率和召回率。`classification.py`中的`precision_recall()`函数看起来是这样的：
- en: '[PRE26]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This function takes two arguments:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数接受两个参数：
- en: The trained classifier.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练好的分类器。
- en: Labeled test features, also known as a gold standard.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 带标签的测试特征，也称为黄金标准。
- en: 'These are the same arguments you pass to the `accuracy()` function. The `precision_recall()`
    returns two dictionaries; the first holds the precision for each label, and the
    second holds the recall for each label. Here''s an example usage with the `nb_classifier`
    and the `test_feats` we created in the *Training a naive Bayes classifier* recipe
    earlier:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是您传递给`accuracy()`函数的相同参数。`precision_recall()`返回两个字典；第一个包含每个标签的精确率，第二个包含每个标签的召回率。以下是一个使用我们在*训练朴素贝叶斯分类器*菜谱中较早创建的`nb_classifier`和`test_feats`的示例用法：
- en: '[PRE27]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This tells us that while the `NaiveBayesClassifier` can correctly identify most
    of the `pos` feature sets (high recall), it also classifies many of the `neg`
    feature sets as `pos` (low precision). This behavior contributes to the high precision
    but low recall for the `neg` label—as the `neg` label isn't given often (low recall),
    and when it is, it's very likely to be correct (high precision). The conclusion
    could be that there are certain common words that are biased towards the `pos`
    label, but occur frequently enough in the `neg` feature sets to cause mis-classifications.
    To correct this behavior, we will use only the most informative words in the next
    recipe, *Calculating high information words*.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们，虽然`NaiveBayesClassifier`可以正确识别大多数`pos`特征集（高召回率），但它也将许多`neg`特征集错误地分类为`pos`（低精确率）。这种行为导致了`neg`标签的高精确率但低召回率——因为`neg`标签并不经常给出（低召回率），当它给出时，它非常可能是正确的（高精确率）。结论可能是，有一些常见的词语倾向于`pos`标签，但它们在`neg`特征集中出现的频率足够高，以至于导致错误分类。为了纠正这种行为，我们将在下一个菜谱中只使用最有信息量的词语，即*计算高信息词语*。
- en: How it works...
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: To calculate precision and recall, we must build two sets for each label. The
    first set is known as the **reference set**, and contains all the correct values.
    The second set is called the **test set** , and contains the values guessed by
    the classifier. These two sets are compared to calculate the precision or recall
    for each label.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算精确率和召回率，我们必须为每个标签构建两个集合。第一个集合被称为**参考集**，包含所有正确的值。第二个集合称为**测试集**，包含分类器猜测的值。这两个集合被比较以计算每个标签的精确率或召回率。
- en: '**Precision** is defined as the size of the intersection of both sets divided
    by the size of the test set. In other words, the percentage of the test set that
    was guessed correctly. In Python, the code is `float(len(reference.intersection(test)))
    / len(test)`.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '**精确率**定义为两个集合交集的大小除以测试集的大小。换句话说，正确猜测的测试集百分比。在 Python 中，代码是 `float(len(reference.intersection(test)))
    / len(test)`。'
- en: '**Recall** is the size of the intersection of both sets divided by the size
    of the reference set, or the percentage of the reference set that was guessed
    correctly. The Python code is `float(len(reference.intersection(test))) / len(reference)`.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '**召回率**是两个集合交集的大小除以参考集的大小，或者正确猜测的参考集百分比。Python 代码是 `float(len(reference.intersection(test)))
    / len(reference)`。'
- en: The `precision_recall()` function in `classification.py` iterates over the labeled
    test features and classifies each one. We store the *numeric index* of the feature
    set (starting with `0`) in the reference set for the known training label, and
    also store the index in the test set for the guessed label. If the classifier
    guesses `pos` but the training label is `neg`, then the index is stored in the
    *reference set* for `neg` and the *test set* for `pos`.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '`classification.py` 中的 `precision_recall()` 函数遍历标记的测试特征并对每个特征进行分类。我们将特征集的 *数值索引*（从
    `0` 开始）存储在已知训练标签的参考集中，并将索引存储在测试集中以猜测标签。如果分类器猜测为 `pos` 但训练标签是 `neg`，则索引存储在 `neg`
    的 *参考集* 和 `pos` 的 *测试集* 中。'
- en: Note
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: We use the numeric index because the feature sets aren't hashable, and we need
    a unique value for each feature set.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用数值索引，因为特征集是不可哈希的，我们需要为每个特征集提供一个唯一的值。
- en: The `nltk.metrics` package contains functions for calculating both precision
    and recall, so all we really have to do is build the sets, then call the appropriate
    function.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '`nltk.metrics` 包包含计算精确率和召回率的函数，所以我们实际上需要做的只是构建集合，然后调用适当的函数。'
- en: There's more...
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容...
- en: 'Let''s try it with the `MaxentClassifier` we trained in the previous recipe:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用之前配方中训练的 `MaxentClassifier` 来试试：
- en: '[PRE28]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This classifier is much more well-rounded than the `NaiveBayesClassifier`. In
    this case, the label bias is much less significant, and the reason is that the
    `MaxentClassifier` weighs its features according to its own internal model. Words
    that are more significant are those that occur primarily in a single label, and
    will get higher weights in the model. Words that are common to both labels will
    get lower weights, as they are less significant.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分类器比 `NaiveBayesClassifier` 更加全面。在这种情况下，标签偏差不太重要，原因是 `MaxentClassifier` 根据其内部模型来权衡其特征。更有意义的词语是那些主要出现在单个标签中的词语，它们在模型中会获得更高的权重。同时出现在两个标签中的词语会获得较低的权重，因为它们不太重要。
- en: F-measure
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F度量
- en: 'The **F-measure** is defined as the weighted harmonic mean of precision and
    recall. If `p` is the *precision*, and `r` is the *recall*, the formula is:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '**F度量**定义为精确率和召回率的加权调和平均。如果 `p` 是 *精确率*，而 `r` 是 *召回率*，则公式为：'
- en: '[PRE29]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'where `alpha` is a weighing constant that defaults to `0.5`. You can use `nltk.metrics.f_measure()`
    to get the F-measure. It takes the same arguments as for the `precision()` and
    `recall()` functions: a reference set and a test set. It''s often used instead
    of accuracy to measure a classifier. However, precision and recall are found to
    be much more useful metrics, as the F-measure can hide the kinds of imbalances
    we saw with the `NaiveBayesClassifier`.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 `alpha` 是一个默认值为 `0.5` 的权重常数。你可以使用 `nltk.metrics.f_measure()` 来获取 F度量。它接受与
    `precision()` 和 `recall()` 函数相同的参数：一个参考集和一个测试集。它通常用于代替准确率来衡量分类器。然而，精确率和召回率被发现是更有用的度量标准，因为
    F度量可以隐藏我们之前在 `NaiveBayesClassifier` 中看到的那些不平衡情况。
- en: See also
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: In the *Training a naive Bayes classifier* recipe, we collected training and
    testing feature sets, and trained the `NaiveBayesClassifier`. The `MaxentClassifier`
    was trained in the *Training a maximum entropy classifier* recipe. In the next
    recipe, we will explore eliminating the less significant words, and use only the
    high information words to create our feature sets.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *训练朴素贝叶斯分类器* 配方中，我们收集了训练和测试特征集，并训练了 `NaiveBayesClassifier`。`MaxentClassifier`
    在 *训练最大熵分类器* 配方中进行了训练。在下一个配方中，我们将探讨消除不太重要的词语，并仅使用高信息词来创建我们的特征集。
- en: Calculating high information words
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算高信息词
- en: A **high information word** is a word that is strongly biased towards a single
    classification label. These are the kinds of words we saw when we called the `show_most_informative_features()`
    method on both the `NaiveBayesClassifier` and the `MaxentClassifier`. Somewhat
    surprisingly, the top words are different for both classifiers. This discrepancy
    is due to how each classifier calculates the significance of each feature, and
    it's actually beneficial to have these different methods as they can be combined
    to improve accuracy, as we will see in the next recipe, *Combining classifiers
    with voting*.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '**高信息单词**是指强烈偏向于单个分类标签的单词。这些是我们调用 `NaiveBayesClassifier` 和 `MaxentClassifier`
    上的 `show_most_informative_features()` 方法时所看到的单词类型。有些令人惊讶的是，两个分类器的顶级单词是不同的。这种差异是由于每个分类器计算每个特征的重要性方式不同，实际上拥有这些不同的方法是有益的，因为我们可以将它们结合起来提高准确性，正如我们将在下一个配方中看到的，*使用投票结合分类器*。'
- en: The **low information words** are words that are common to all labels. It may
    be counter-intuitive, but eliminating these words from the training data can actually
    improve accuracy, precision, and recall. The reason this works is that using only
    high information words reduces the noise and confusion of a classifier's internal
    model. If all the words/features are highly biased one way or the other, it's
    much easier for the classifier to make a correct guess.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '**低信息单词**是指所有标签都共有的单词。这可能有些反直觉，但消除这些单词可以从训练数据中提高准确性、精确度和召回率。这种方法之所以有效，是因为仅使用高信息单词可以减少分类器内部模型的噪声和混淆。如果所有单词/特征都高度偏向某一方向，那么分类器做出正确猜测就更容易了。'
- en: How to do it...
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'First, we need to calculate the high information words in the `movie_review`
    corpus. We can do this using the `high_information_words()` function in `featx.py`:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要计算 `movie_review` 语料库中的高信息单词。我们可以使用 `featx.py` 中的 `high_information_words()`
    函数来完成这项工作：
- en: '[PRE30]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: It takes one argument , which is a list of 2-tuples of the form `[(label, words)]`
    where `label` is the classification label, and `words` is a list of words that
    occur under that label. It returns a list of the high information words, sorted
    from most informative to least informative.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 它需要一个参数，即形如 `[(label, words)]` 的 2-元组列表，其中 `label` 是分类标签，而 `words` 是在该标签下出现的单词列表。它返回一个按信息量从高到低排序的高信息单词列表。
- en: Once we have the high information words, we use the feature detector function
    `bag_of_words_in_set()`, also found in `featx.py`, which will let us filter out
    all low information words.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了高信息单词，我们就使用特征检测器函数 `bag_of_words_in_set()`，它也位于 `featx.py` 中，这将允许我们过滤掉所有低信息单词。
- en: '[PRE31]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: With this new feature detector, we can call `label_feats_from_corpus()` and
    get a new `train_feats` and `test_feats` using `split_label_feats()`. These two
    functions were covered in the *Training a naive Bayes classifier* recipe in this
    chapter.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个新的特征检测器，我们可以调用 `label_feats_from_corpus()` 并使用 `split_label_feats()` 获取新的
    `train_feats` 和 `test_feats`。这两个函数在本章的 *训练朴素贝叶斯分类器* 配方中已有介绍。
- en: '[PRE32]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now that we have new training and testing feature sets, let''s train and evaluate
    a `NaiveBayesClassifier`:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了新的训练和测试特征集，让我们训练并评估一个 `NaiveBayesClassifier`：
- en: '[PRE33]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: While the `neg` precision and `pos` recall have both decreased somewhat, `neg`
    recall and `pos` precision have increased drastically. Accuracy is now a little
    higher than the `MaxentClassifier`.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 `neg` 精确度和 `pos` 召回率都有所下降，但 `neg` 召回率和 `pos` 精确度都有显著提高。现在准确性略高于 `MaxentClassifier`。
- en: How it works...
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The `high_information_words()` function starts by counting the frequency of
    every word, as well as the conditional frequency for each word within each label.
    This is why we need the words to be labelled, so we know how often each word occurs
    in each label.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '`high_information_words()` 函数首先计算每个单词的频率，以及每个标签内每个单词的条件频率。这就是为什么我们需要标记单词，这样我们才知道每个单词在每个标签中出现的频率。'
- en: 'Once we have this `FreqDist` and `ConditionalFreqDist`, we can score each word
    on a per-label basis. The default `score_fn` is `nltk.metrics.BigramAssocMeasures.chi_sq()`,
    which calculates the chi-square score for each word using the following parameters:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了这个 `FreqDist` 和 `ConditionalFreqDist`，我们就可以根据每个标签对每个单词进行评分。默认的 `score_fn`
    是 `nltk.metrics.BigramAssocMeasures.chi_sq()`，它使用以下参数计算每个单词的卡方得分：
- en: '`n_ii`: The frequency of the word in the label.'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`n_ii`：单词在标签中的频率。'
- en: '`n_ix`: The total frequency of the word across all labels.'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`n_ix`：单词在所有标签中的总频率。'
- en: '`n_xi`: The total frequency of all words that occurred in the label.'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`n_xi`：在标签中出现的所有单词的总频率。'
- en: '`n_xx`: The total frequency for all words in all labels.'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`n_xx`：所有标签中所有单词的总频率。'
- en: The simplest way to think about these numbers is that the closer `n_ii` is to
    `n_ix`, the higher the score. Or, the more often a word occurs in a label, relative
    to its overall occurrence, the higher the score.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这些数字的最简单方式是，`n_ii`越接近`n_ix`，得分就越高。或者，一个单词在标签中出现的频率相对于其整体出现频率越高，得分就越高。
- en: Once we have the scores for each word in each label, we can filter out all words
    whose score is below the `min_score` threshold. We keep the words that meet or
    exceed the threshold, and return all high scoring words in each label.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们得到了每个标签中每个单词的分数，我们可以过滤掉所有得分低于`min_score`阈值的单词。我们保留满足或超过阈值的单词，并返回每个标签中所有得分高的单词。
- en: Tip
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: It is recommended to experiment with different values of `min_score` to see
    what happens. In some cases, less words may improve the metrics even more, while
    in other cases more words is better.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 建议尝试不同的`min_score`值以观察其效果。在某些情况下，更少的单词可能会使指标进一步提升，而在其他情况下，更多的单词可能更佳。
- en: There's more...
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多...
- en: There are a number of other scoring functions available in the `BigramAssocMeasures`
    class, such as `phi_sq()` for phi-square, `pmi()` for pointwise mutual information,
    and `jaccard()` for using the Jaccard index. They all take the same arguments,
    and so can be used interchangeably with `chi_sq()`.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在`BigramAssocMeasures`类中还有许多其他评分函数可用，例如`phi_sq()`用于phi-square，`pmi()`用于点互信息，以及`jaccard()`用于使用Jaccard指数。它们都接受相同的参数，因此可以与`chi_sq()`互换使用。
- en: MaxentClassifier with high information words
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 带有高信息单词的MaxentClassifier
- en: 'Let''s evaluate the `MaxentClassifier` using the high information words feature
    sets:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用高信息单词特征集来评估`MaxentClassifier`：
- en: '[PRE34]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: As you can see, the improvements are much more modest than with the `NaiveBayesClassifier`
    due to the fact that the `MaxentClassifier` already weights all features by significance.
    But using only the high information words still makes a positive difference compared
    to when we used all the words. And the precisions and recalls for each label are
    closer to each other, giving the `MaxentClassifier` even more well-rounded performance.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，由于`MaxentClassifier`已经根据重要性对所有特征进行了加权，因此与`NaiveBayesClassifier`相比，改进幅度要小得多。但仅使用高信息单词与使用所有单词相比，仍然有积极的影响。每个标签的精确率和召回率更接近，这使得`MaxentClassifier`的表现更加均衡。
- en: DecisionTreeClassifier with high information words
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 带有高信息单词的DecisionTreeClassifier
- en: 'Now, let''s evaluate the `DecisionTreeClassifier`:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们评估`DecisionTreeClassifier`：
- en: '[PRE35]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The accuracy is about the same, even with a larger `depth_cutoff`, and smaller
    `support_cutoff` and `entropy_cutoff`. The results show that the `DecisionTreeClassifier`
    was already putting the high information features at the top of the tree, and
    it will only improve if we increase the depth significantly. But that could make
    training time prohibitively long.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在更大的`depth_cutoff`、较小的`support_cutoff`和`entropy_cutoff`下，准确率也大致相同。结果表明，`DecisionTreeClassifier`已经将高信息特征置于树的最顶层，只有当我们显著增加深度时，它才会得到改善。但这可能会使训练时间变得过长。
- en: See also
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: We started this chapter with the *Bag of Words feature extraction* recipe. The
    `NaiveBayesClassifier` was originally trained in the *Training a naive Bayes classifier*
    recipe, and the `MaxentClassifier` was trained in the *Training a maximum entropy
    classifier* recipe. Details on precision and recall can be found in the *Measuring
    precision and recall of a classifier* recipe. We will be using only high information
    words in the next two recipes, where we combine classifiers.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章开始时介绍了**词袋模型特征提取**的配方。`NaiveBayesClassifier`最初是在**训练朴素贝叶斯分类器**的配方中训练的，而`MaxentClassifier`是在**训练最大熵分类器**的配方中训练的。关于精确率和召回率的详细信息可以在**测量分类器的精确率和召回率**的配方中找到。在接下来的两个配方中，我们将只使用高信息单词，我们将结合分类器。
- en: Combining classifiers with voting
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结合分类器进行投票
- en: One way to improve classification performance is to combine classifiers. The
    simplest way to combine multiple classifiers is to use voting, and choose whichever
    label gets the most votes. For this style of voting, it's best to have an odd
    number of classifiers so that there are no ties. This means combining at least
    three classifiers together. The individual classifiers should also use different
    algorithms; the idea is that multiple algorithms are better than one, and the
    combination of many can compensate for individual bias.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 提高分类性能的一种方法是将分类器组合起来。组合多个分类器最简单的方法是使用投票，并选择获得最多投票的标签。对于这种投票方式，最好有奇数个分类器，这样就没有平局。这意味着至少需要组合三个分类器。单个分类器也应该使用不同的算法；想法是多个算法比一个更好，许多算法的组合可以弥补单个偏差。
- en: Getting ready
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: As we need to have at least three trained classifiers to combine, we are going
    to use a `NaiveBayesClassifier`, a `DecisionTreeClassifier`, and a `MaxentClassifier`,
    all trained on the highest information words of the `movie_reviews` corpus. These
    were all trained in the previous recipe, so we will combine these three classifiers
    with voting.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们需要至少三个训练好的分类器来组合，我们将使用一个`NaiveBayesClassifier`、一个`DecisionTreeClassifier`和一个`MaxentClassifier`，它们都是在`movie_reviews`语料库的最高信息词上训练的。这些都是在前面的配方中训练的，所以我们将通过投票组合这三个分类器。
- en: How to do it...
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: In the `classification.py` module, there is a `MaxVoteClassifier` class.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在`classification.py`模块中，有一个`MaxVoteClassifier`类。
- en: '[PRE36]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: To create it, you pass in a list of classifiers that you want to combine. Once
    created, it works just like any other classifier. Though it may take about three
    times longer to classify, it should generally be at least as accurate as any individual
    classifier.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建它，你需要传入一个你想要组合的分类器列表。一旦创建，它的工作方式就像任何其他分类器一样。尽管分类可能需要大约三倍的时间，但它应该通常至少与任何单个分类器一样准确。
- en: '[PRE37]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: These metrics are about on par with the `MaxentClassifier` and `NaiveBayesClassifier`.
    Some numbers are slightly better, some worse. It's likely that a significant improvement
    to the `DecisionTreeClassifier` could produce some better numbers.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这些指标与`MaxentClassifier`和`NaiveBayesClassifier`大致相当。一些数字略好，一些略差。很可能对`DecisionTreeClassifier`的重大改进会产生一些更好的数字。
- en: How it works...
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'The `MaxVoteClassifier` extends the `nltk.classify.ClassifierI` interface,
    which requires implementing at least two methods:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '`MaxVoteClassifier`扩展了`nltk.classify.ClassifierI`接口，这要求实现至少两个方法：'
- en: The `labels()` function must return a list of possible labels. This will be
    the union of the `labels()` of each classifier passed in at initialization.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`labels()`函数必须返回一个可能的标签列表。这将是从初始化时传入的每个分类器的`labels()`的并集。'
- en: The `classify()` function takes a single feature set and returns a label. The
    `MaxVoteClassifier` iterates over its classifiers and calls `classify()` on each
    of them, recording their label as a vote in a `FreqDist`. The label with the most
    votes is returned using `FreqDist.max()`.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`classify()`函数接受一个特征集并返回一个标签。`MaxVoteClassifier`遍历其分类器，并对每个分类器调用`classify()`，将它们的标签记录为`FreqDist`中的投票。使用`FreqDist.max()`返回获得最多投票的标签。'
- en: While it doesn't check for this, the `MaxVoteClassifier` assumes that all the
    classifiers passed in at initialization use the same labels. Breaking this assumption
    may lead to odd behavior.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然`MaxVoteClassifier`没有检查这一点，但它假设在初始化时传入的所有分类器都使用相同的标签。违反这个假设可能会导致异常行为。
- en: See also
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: In the previous recipe, we trained a `NaiveBayesClassifier`, a `MaxentClassifier`,
    and a `DecisionTreeClassifier` using only the highest information words. In the
    next recipe, we will use the `reuters` corpus and combine many binary classifiers
    in order to create a multi-label classifier.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的配方中，我们仅使用最高信息词训练了`NaiveBayesClassifier`、`MaxentClassifier`和`DecisionTreeClassifier`。在下一个配方中，我们将使用`reuters`语料库并组合多个二元分类器来创建一个多标签分类器。
- en: Classifying with multiple binary classifiers
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用多个二元分类器进行分类
- en: So far we have focused on **binary classifiers**, which classify with *one of
    two possible labels*. The same techniques for training a binary classifier can
    also be used to create a *multi-class* classifier, which is a classifier that
    can classify with *one of many possible labels*. But there are also cases where
    you need to be able to classify with *multiple labels*. A classifier that can
    return more than one label is a **multi-label classifier**.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们专注于**二元分类器**，它们通过**两个可能标签中的一个**进行分类。用于训练二元分类器的相同技术也可以用来创建**多类分类器**，这是一种可以**通过许多可能标签中的一个**进行分类的分类器。但也有需要能够用**多个标签**进行分类的情况。能够返回多个标签的分类器被称为**多标签分类器**。
- en: A common technique for creating a multi-label classifier is to combine many
    binary classifiers, one for each label. You train each binary classifier so that
    it either returns a known label, or returns something else to signal that the
    label does not apply. Then you can run all the binary classifiers on your feature
    set to collect all the applicable labels.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 创建多标签分类器的一种常见技术是将许多二元分类器结合起来，每个标签一个。你训练每个二元分类器，使其要么返回一个已知标签，要么返回其他内容以表示该标签不适用。然后你可以在你的特征集上运行所有二元分类器以收集所有适用的标签。
- en: Getting ready
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备就绪
- en: The `reuters` corpus contains multi-labeled text that we can use for training
    and evaluation.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '`reuters`语料库包含多标签文本，我们可以用它来训练和评估。'
- en: '[PRE38]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: We will train one binary classifier per label, which means we will end up with
    90 binary classifiers.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为每个标签训练一个二元分类器，这意味着我们最终将拥有90个二元分类器。
- en: How to do it...
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: First, we should calculate the high information words in the `reuters` corpus.
    This is done with the `reuters_high_info_words()` function in `featx.py`.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们应该计算`reuters`语料库中的高信息词。这是通过`featx.py`中的`reuters_high_info_words()`函数完成的。
- en: '[PRE39]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Then we need to get training and test feature sets based on those high information
    words. This is done with the `reuters_train_test_feats()`, also found in `featx.py`.
    It defaults to using `bag_of_words()` as its `feature_detector`, but we will be
    overriding this using `bag_of_words_in_set()` to use only the high information
    words.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要根据那些高信息词来获取训练和测试特征集。这是通过`featx.py`中的`reuters_train_test_feats()`函数完成的。它默认使用`bag_of_words()`作为其`feature_detector`，但我们将使用`bag_of_words_in_set()`来仅使用高信息词。
- en: '[PRE40]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: We can use these two functions to get a list of multi-labeled training and testing
    feature sets.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这两个函数来获取多标签训练和测试特征集列表。
- en: '[PRE41]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The `multi_train_feats` and `multi_test_feats` are multi-labeled feature sets.
    That means they have a list of labels, instead of a single label, and they look
    like the `[(featureset, [label])]`, as each feature set can have one or more labels.
    With this training data, we can train multiple binary classifiers. The `train_binary_classifiers()`
    function in the `classification.py` takes a training function, a list of multi-label
    feature sets, and a set of possible labels to return a `dict` of the `label :
    binary` classifier.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '`multi_train_feats`和`multi_test_feats`是多标签特征集。这意味着它们有一个标签列表，而不是单个标签，并且它们的格式看起来像`[(featureset,
    [label])]`，因为每个特征集可以有一个或多个标签。有了这些训练数据，我们可以训练多个二元分类器。`classification.py`中的`train_binary_classifiers()`函数接受一个训练函数、一个多标签特征集列表以及一个可能的标签集合，返回一个`label
    : binary`分类器的`dict`。'
- en: '[PRE42]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: To use this function, we need to provide a training function that takes a single
    argument, which is the training data. This will be a simple `lambda` wrapper around
    the `MaxentClassifier.train()`, so we can specify extra keyword arguments.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用此函数，我们需要提供一个训练函数，它接受一个单一参数，即训练数据。这将是一个简单的`lambda`包装器，围绕`MaxentClassifier.train()`，这样我们就可以指定额外的关键字参数。
- en: '[PRE43]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Now we can define a `MultiBinaryClassifier`, which takes a list of labeled classifiers
    of the form `[(label, classifier)]` where the `classifier` is assumed to be a
    binary classifier that either returns the `label`, or something else if the label
    doesn't apply.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以定义一个`MultiBinaryClassifier`，它接受一个形式为`[(label, classifier)]`的标签化分类器列表，其中`classifier`假设是一个二元分类器，它要么返回`label`，要么在标签不适用时返回其他内容。
- en: '[PRE44]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: We can construct this class using the binary classifiers we just created.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用我们刚刚创建的二元分类器来构建这个类。
- en: '[PRE45]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: To evaluate this classifier, we can use precision and recall, but not accuracy.
    That's because the accuracy function assumes single values, and doesn't take into
    account partial matches. For example, if the multi-classifier returns three labels
    for a feature set, and two of them are correct but the third is not, then the
    `accuracy()` would mark that as incorrect. So instead of using accuracy, we will
    use the **masi distance**, which measures partial overlap between two sets. The
    lower the masi distance, the better the match. A lower average masi distance,
    therefore, means more accurate partial matches. The `multi_metrics()` function
    in the `classification.py` calculates the precision and recall of each label,
    along with the average masi distance.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估这个分类器，我们可以使用精确度和召回率，但不能使用准确率。这是因为准确率函数假设单值，并且不考虑部分匹配。例如，如果多分类器对一个特征集返回三个标签，其中两个是正确的但第三个不是，那么`accuracy()`会将其标记为不正确。因此，我们不会使用准确率，而是使用**masi距离**，它衡量两个集合之间的部分重叠。masi距离越低，匹配度越好。因此，较低的平均masi距离意味着更准确的局部匹配。`classification.py`中的`multi_metrics()`函数计算每个标签的精确度和召回率，以及平均masi距离。
- en: '[PRE46]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Using this with the `multi_classifier` we just created, gives us the following
    results:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们刚刚创建的`multi_classifier`，我们得到以下结果：
- en: '[PRE47]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'So our average masi distance is fairly low, which means our multi-label classifier
    is usually mostly accurate. Let''s take a look at a few precisions and recalls:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的平均masi距离相当低，这意味着我们的多标签分类器通常非常准确。让我们看看一些精确度和召回率的例子：
- en: '[PRE48]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: As you can see, there's quite a range of values. But, in general, the labels
    that have more feature sets will have higher precision and recall, and those with
    less feature sets will have lower performance. When there's not a lot of feature
    sets for a classifier to learn from, you can't expect it to perform well.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，存在相当大的值范围。但总的来说，具有更多特征集的标签将具有更高的精确度和召回率，而具有较少特征集的标签将性能较低。当分类器可学习的特征集不多时，您不能期望它表现良好。
- en: How it works...
  id: totrans-281
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The `reuters_high_info_words()` function is fairly simple; it constructs a list
    of `[(label, words)]` for each category of the `reuters` corpus, then passes it
    in to the `high_information_words()` function to return a list of the most informative
    words in the `reuters` corpus.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '`reuters_high_info_words()`函数相当简单；它为`reuters`语料库的每个类别构建一个`[(label, words)]`列表，然后将其传递给`high_information_words()`函数，以返回`reuters`语料库中最具信息量的单词列表。'
- en: With the resulting set of words, we create a feature detector function using
    the `bag_of_words_in_set()`. This is then passed in to the `reuters_train_test_feats()`,
    which returns two lists, the first containing `[(feats, labels)]` for all the
    training files, and the second list has the same for all the test files.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 使用生成的单词集，我们使用`bag_of_words_in_set()`创建一个特征检测函数。然后将其传递给`reuters_train_test_feats()`，该函数返回两个列表，第一个列表包含所有训练文件的`[(feats,
    labels)]`，第二个列表包含所有测试文件的相同内容。
- en: Next, we train a binary classifier for each label using `train_binary_classifiers()`.
    This function constructs two lists for each label, one containing positive training
    feature sets, the other containing negative training feature sets. The **Positive
    feature sets** are those feature sets that classify for the label. The **Negative
    feature sets** for a label comes from the positive feature sets for all other
    labels. For example, a feature set that is *positive* for `zinc` and `sunseed`
    is a *negative* example for all the other 88 labels. Once we have positive and
    negative feature sets for each label, we can train a binary classifier for each
    label using the given training function.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用`train_binary_classifiers()`为每个标签训练一个二元分类器。这个函数为每个标签构建两个列表，一个包含正训练特征集，另一个包含负训练特征集。**正特征集**是那些对标签进行分类的特征集。**负特征集**来自所有其他标签的正特征集。例如，对`zinc`和`sunseed`都是**正**的特征集是对于其他88个标签的**负**示例。一旦我们为每个标签有了正负特征集，我们就可以使用给定的训练函数为每个标签训练一个二元分类器。
- en: 'With the resulting dictionary of binary classifiers, we create an instance
    of the `MultiBinaryClassifier`. This class extends the `nltk.classify.MultiClassifierI`
    interface, which requires at least two functions:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 使用生成的二元分类器字典，我们创建了一个`MultiBinaryClassifier`的实例。这个类扩展了`nltk.classify.MultiClassifierI`接口，该接口至少需要两个函数：
- en: The `labels()` function must return a list of possible labels.
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`labels()`函数必须返回一个可能的标签列表。'
- en: The `classify()` function takes a single feature set and returns a `set` of
    labels. To create this `set`, we iterate over the binary classifiers, and any
    time a call to the `classify()` returns its label, we add it to the set. If it
    returns something else, we continue.
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`classify()`函数接受一个特征集，并返回一个`set`标签。为了创建这个`set`，我们遍历二元分类器，每次调用`classify()`返回其标签时，我们就将其添加到`set`中。如果它返回其他内容，我们就继续。'
- en: 'Finally, we evaluate the multi-label classifier using the `multi_metrics()`
    function. It is similar to the `precision_recall()` function from the *Measuring
    precision and recall of a classifier* recipe, but in this case we know the classifier
    is an instance of the `MultiClassifierI` and it can therefore return multiple
    labels. It also keeps track of the masi distance for each set of classification
    labels using the `nltk.metrics.masi_` `distance()`. The `multi_metrics()` function
    returns three values:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用`multi_metrics()`函数评估多标签分类器。它与*测量分类器的精确度和召回率*配方中的`precision_recall()`函数类似，但在这个情况下我们知道分类器是`MultiClassifierI`的一个实例，因此可以返回多个标签。它还使用`nltk.metrics.masi_`
    `distance()`跟踪每个分类标签集的masi距离。`multi_metrics()`函数返回三个值：
- en: A dictionary of precisions for each label.
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个标签的精度字典。
- en: A dictionary of recalls for each label.
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个标签的召回率字典。
- en: The average masi distance for each feature set.
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个特征集的平均masi距离。
- en: There's more...
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The nature of the `reuters` corpus introduces the **class-imbalance problem**.
    This problem occurs when some labels have very few feature sets, and other labels
    have many. The binary classifiers that have few positive instances to train on
    end up with far more negative instances, and are therefore strongly biased towards
    the negative label. There's nothing inherently wrong about this, as the bias reflects
    the data, but the negative instances can overwhelm the classifier to the point
    where it's nearly impossible to get a positive result. There are a number of advanced
    techniques for overcoming this problem, but they are out of the scope of this
    book.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '`reuters`语料库的性质引入了**类别不平衡问题**。当某些标签具有非常少的特征集，而其他标签具有很多时，就会出现这个问题。那些在训练时只有少量正例的二元分类器最终会有更多的负例，因此强烈偏向于负标签。这本身并没有什么错误，因为偏差反映了数据，但负例可能会压倒分类器，以至于几乎不可能得到一个正例。有几种高级技术可以克服这个问题，但这些技术超出了本书的范围。'
- en: See also
  id: totrans-294
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: The `MaxentClassifier` is covered in the *Training a maximum entropy classifier*
    recipe in this chapter. The *Measuring precision and recall of a classifier* recipe
    shows how to evaluate a classifier, while the *Calculating high information words*
    recipe describes how to use only the best features.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的*训练最大熵分类器*配方涵盖了`MaxentClassifier`。*测量分类器的精确度和召回率*配方展示了如何评估分类器，而*计算高信息词*配方描述了如何仅使用最佳特征。
