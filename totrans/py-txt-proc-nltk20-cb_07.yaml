- en: Chapter 7. Text Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Bag of Words feature extraction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a naive Bayes classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a decision tree classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a maximum entropy classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measuring precision and recall of a classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculating high information words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combining classifiers with voting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying with multiple binary classifiers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Text classification** is a way to categorize documents or pieces of text.
    By examining the word usage in a piece of text, classifiers can decide what *class
    label* to assign to it. A **binary classifier** decides between two labels, such
    as positive or negative. The text can either be one label or the other, but not
    both, whereas a **multi-label classifier** can assign one or more labels to a
    piece of text.'
  prefs: []
  type: TYPE_NORMAL
- en: Classification works by learning from *labeled feature sets*, or training data,
    to later classify an *unlabeled feature set*. A **feature set** is basically a
    key-value mapping of *feature names* to *feature values*. In the case of text
    classification, the feature names are usually words, and the values are all `True`.
    As the documents may have unknown words, and the number of possible words may
    be very large, words that don't occur in the text are omitted, instead of including
    them in a feature set with the value `False`.
  prefs: []
  type: TYPE_NORMAL
- en: An **instance** is a single feature set. It represents a single occurrence of
    a combination of features. We will use *instance* and *feature set* interchangeably.
    A *labeled feature set* is an instance with a known class label that we can use
    for training or evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Bag of Words feature extraction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text feature extraction is the process of transforming what is essentially a
    list of words into a feature set that is usable by a classifier. The NLTK classifiers
    expect `dict` style feature sets, so we must therefore transform our text into
    a `dict`. The **Bag of Words** model is the simplest method; it constructs a *word
    presence* feature set from all the words of an instance.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The idea is to convert a list of words into a `dict`, where each word becomes
    a key with the value `True`. The `bag_of_words()` function in `featx.py` looks
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use it with a list of words, in this case the tokenized sentence "the
    quick brown fox":'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The resulting `dict` is known as a *bag of words* because the words are not
    in order, and it doesn't matter where in the list of words they occurred, or how
    many times they occurred. All that matters is that the word is found at least
    once.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `bag_of_words()` function is a very simple *list comprehension* that constructs
    a `dict` from the given words, where every word gets the value `True`.
  prefs: []
  type: TYPE_NORMAL
- en: Since we have to assign a value to each word in order to create a `dict`, `True`
    is a logical choice for the value to indicate word presence. If we knew the universe
    of all possible words, we could assign the value `False` to all the words that
    are not in the given list of words. But most of the time, we don't know all possible
    words beforehand. Plus, the `dict` that would result from assigning `False` to
    every possible word would be very large (assuming all words in the English language
    are possible). So instead, to keep feature extraction simple and use less memory,
    we stick with assigning the value `True` to all words that occur at least once.
    We don't assign the value `False` to any words since we don't know what the set
    of possible words are; we only know about the words we are given.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the default Bag of Words model, all words are treated equally. But that's
    not always a good idea. As we already know, some words are so common that they
    are practically meaningless. If you have a set of words that you want to exclude,
    you can use the `bag_of_words_not_in_set()` function in `featx.py`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This function can be used, among other things, to filter stopwords. Here''s
    an example where we filter the word "the" from "the quick brown fox":'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As expected, the resulting `dict` has "quick", "brown", and "fox", but not "the".
  prefs: []
  type: TYPE_NORMAL
- en: Filtering stopwords
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here''s an example of using the `bag_of_words_not_in_set()` function to filter
    all English stopwords:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'You can pass a different language filename as the `stopfile` keyword argument
    if you are using a language other than English. Using this function produces the
    same result as the previous example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Here, "the" is a stopword, so it is not present in the returned `dict`.
  prefs: []
  type: TYPE_NORMAL
- en: Including significant bigrams
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In addition to single words, it often helps to include significant bigrams.
    As significant bigrams are less common than most individual words, including them
    in the Bag of Words can help the classifier make better decisions. We can use
    the `BigramCollocationFinder` covered in the *Discovering word collocations* recipe
    of [Chapter 1](ch01.html "Chapter 1. Tokenizing Text and WordNet Basics"), *Tokenizing
    Text and WordNet Basics*, to find significant bigrams. `bag_of_bigrams_words()`
    found in `featx.py` will return a `dict` of all words along with the 200 most
    significant bigrams.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The bigrams will be present in the returned `dict` as `(word1, word2)` and
    will have the value as `True`. Using the same example words as before, we get
    all words plus every bigram:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: You can change the maximum number of bigrams found by altering the keyword argument
    `n`.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Discovering word collocations* recipe of [Chapter 1](ch01.html "Chapter 1. Tokenizing
    Text and WordNet Basics"), *Tokenizing Text and WordNet Basics* covers the `BigramCollocationFinder`
    in more detail. In the next recipe, we will train a `NaiveBayesClassifier` using
    feature sets created with the Bag of Words model.
  prefs: []
  type: TYPE_NORMAL
- en: Training a naive Bayes classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we can extract features from text, we can train a classifier. The
    easiest classifier to get started with is the `NaiveBayesClassifier` . It uses
    **Bayes Theorem** to predict the probability that a given feature set belongs
    to a particular label. The formula is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '`P(label)` is the prior probability of the label occurring, which is the same
    as the likelihood that a random feature set will have the label. This is based
    on the number of training instances with the label compared to the total number
    of training instances. For example, if 60/100 training instances have the label,
    the prior probability of the label is 60 percent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`P(features | label)` is the prior probability of a given feature set being
    classified as that label. This is based on which features have occurred with each
    label in the training data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`P(features)` is the prior probability of a given feature set occurring. This
    is the likelihood of a random feature set being the same as the given feature
    set, and is based on the observed feature sets in the training data. For example,
    if the given feature set occurs twice in 100 training instances, the prior probability
    is 2 percent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`P(label | features)` tells us the probability that the given features should
    have that label. If this value is high, then we can be reasonably confident that
    the label is correct for the given features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We are going to be using the `movie_reviews` corpus for our initial classification
    examples. This corpus contains two categories of text: `pos` and `neg`. These
    categories are exclusive, which makes a classifier trained on them a **binary
    classifier**. Binary classifiers have only two classification labels, and will
    always choose one or the other.'
  prefs: []
  type: TYPE_NORMAL
- en: Each file in the `movie_reviews` corpus is composed of either positive or negative
    movie reviews. We will be using each file as a single instance for both training
    and testing the classifier. Because of the nature of the text and its categories,
    the classification we will be doing is a form of *sentiment analysis*. If the
    classifier returns `pos`, then the text expresses *positive sentiment*, whereas
    if we get `neg`, then the text expresses *negative sentiment*.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For training, we need to first create a list of labeled feature sets. This
    list should be of the form `[(featureset, label)]` where the `featureset` is a
    `dict`, and `label` is the known class label for the `featureset`. The `label_feats_from_corpus()`
    function in `featx.py` takes a corpus, such as `movie_reviews`, and a `feature_detector`
    function, which defaults to `bag_of_words`. It then constructs and returns a mapping
    of the form `{label: [featureset]}`. We can use this mapping to create a list
    of labeled *training instances* and *testing instances*. The reason to do it this
    way is because we can get a fair sample from each label.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we can get a mapping of `label : feature` sets, we want to construct a
    list of labeled training instances and testing instances. The function `split_label_feats()`
    in `featx.py` takes a mapping returned from `label_feats_from_corpus()` and splits
    each list of feature sets into labeled training and testing instances.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Using these functions with the `movie_reviews` corpus gives us the lists of
    labeled feature sets we need to train and test a classifier.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: So there are 1,000 `pos` files, 1,000 `neg` files, and we end up with 1,500
    labeled training instances and 500 labeled testing instances, each composed of
    equal parts `pos` and `neg`. Now we can train a `NaiveBayesClassifier` using its
    `train()` class method,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Let's test the classifier on a couple of made up reviews. The `classify()` method
    takes a single argument, which should be a feature set. We can use the same `bag_of_words()`
    feature detector on a made up list of words to get our feature set.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `label_feats_from_corpus()` assumes that the corpus is categorized, and
    that a single file represents a single instance for feature extraction. It iterates
    over each category label, and extracts features from each file in that category
    using the `feature_detector()` function, which defaults to `bag_of_words()`. It
    returns a `dict` whose keys are the category labels, and the values are lists
    of instances for that category.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If we had the `label_feats_from_corpus()` function, return a list of labeled
    feature sets, instead of a dict, it would be much harder to get the balanced training
    data. The list would be ordered by label, and if you took a slice of it, you would
    almost certainly be getting far more of one label than another. By returning a
    `dict`, you can take slices from the feature sets of each label.
  prefs: []
  type: TYPE_NORMAL
- en: Now we need to split the labeled feature sets into training and testing instances
    using `split_label_feats()` . This function allows us to take a fair sample of
    labeled feature sets from each label, using the `split` keyword argument to determine
    the size of the sample. `split` defaults to `0.75`, which means the first three-fourths
    of the labeled feature sets for each label will be used for training, and the
    remaining one-fourth will be used for testing.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have split up our training and testing feats, we train a classifier
    using the `NaiveBayesClassifier.train()` method. This class method builds two
    probability distributions for calculating prior probabilities. These are passed
    in to the `NaiveBayesClassifier` constructor. The `label_probdist` contains `P(label)`,
    the prior probability for each label. The `feature_probdist` contains `P(feature
    name = feature value | label)`. In our case, it will store `P(word=True | label)`.
    Both are calculated based on the frequency of occurrence of each label, and each
    feature name and value in the training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `NaiveBayesClassifier` inherits from `ClassifierI`, which requires subclasses
    to provide a `labels()` method, and at least one of the `classify()` and `prob_classify()`
    methods. T he following diagram shows these and other methods, which will be covered
    shortly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/3609OS_07_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can test the accuracy of the classifier using `nltk.classify.util.accuracy()`
    and the `test_feats` created previously.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This tells us that the classifier correctly guessed the label of nearly 73 percent
    of the testing feature sets.
  prefs: []
  type: TYPE_NORMAL
- en: Classification probability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While the `classify()` method returns only a single label, you can use the `prob_classify()`
    method to get the classification probability of each label. This can be useful
    if you want to use probability thresholds greater than 50 percent for classification.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In this case, the classifier says that the first testing instance is nearly
    100 percent likely to be `pos`.
  prefs: []
  type: TYPE_NORMAL
- en: Most informative features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `NaiveBayesClassifier` has two methods that are quite useful for learning
    about your data. Both methods take a keyword argument `n` to control how many
    results to show. The `most_informative_features()` method returns a list of the
    form `[(feature name, feature value)]` ordered by most informative to least informative.
    In our case, the feature value will always be `True`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The `show_most_informative_features()` method will print out the results from
    `most_informative_features()` and will also include the probability of a feature
    pair belonging to each label.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The *informativeness*, or **information gain**, of each feature pair is based
    on the prior probability of the feature pair occurring for each label. More informative
    features are those that occur primarily in one label and not the other. Less informative
    features are those that occur frequently in both labels.
  prefs: []
  type: TYPE_NORMAL
- en: Training estimator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'During training, the `NaiveBayesClassifier` constructs its probability distributions
    using an `estimator` parameter, which defaults to `nltk.probability.ELEProbDist`.
    But you can use any `estimator` you want, and there are quite a few to choose
    from. The only constraints are that it must inherit from `nltk.probability.ProbDistI`
    and its constructor must take a `bins` keyword argument. Here''s an example using
    the `LaplaceProdDist`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, accuracy is slightly lower, so choose your `estimator` carefully.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You cannot use `nltk.probability.MLEProbDist` as the estimator, or any `ProbDistI`
    subclass that does not take the `bins` keyword argument. Training will fail with
    `TypeError: __init__() got an unexpected keyword argument ''bins''`.'
  prefs: []
  type: TYPE_NORMAL
- en: Manual training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You don''t have to use the `train()` class method to construct a `NaiveBayesClassifier`.
    You can instead create the `label_probdist` and `feature_probdist` manually. `label_probdist`
    should be an instance of `ProbDistI`, and should contain the prior probabilities
    for each label. `feature_probdist` should be a `dict` whose keys are tuples of
    the form `(label, feature name)` and whose values are instances of `ProbDistI`
    that have the probabilities for each feature value. In our case, each `ProbDistI`
    should have only one value, `True=1`. Here''s a very simple example using manually
    constructed `DictionaryProbDist`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the next recipes, we will train two more classifiers, the `DecisionTreeClassifier`,
    and the `MaxentClassifier`. In the *Measuring precision and recall of a classifier*
    recipe in this chapter, we will use precision and recall instead of accuracy to
    evaluate the classifiers. And then in the *Calculating high information words*
    recipe, we will see how using only the most informative features can improve classifier
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: The `movie_reviews` corpus is an instance of `CategorizedPlaintextCorpusReader`,
    which is covered in the *Creating a categorized text corpus* recipe in [Chapter
    3](ch03.html "Chapter 3. Creating Custom Corpora"), *Creating Custom Corpora*.
  prefs: []
  type: TYPE_NORMAL
- en: Training a decision tree classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `DecisionTreeClassifier` works by creating a tree structure, where each
    node corresponds to a feature name, and the branches correspond to the feature
    values. Tracing down the branches, you get to the leaves of the tree, which are
    the classification labels.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the `DecisionTreeClassifier` to work for text classification, you must use
    NLTK 2.0b9 or later. This is because earlier versions are unable to deal with
    unknown features. If the `DecisionTreeClassifier` encountered a word/feature that
    it hadn't seen before, then it raised an exception. This bug has now been fixed
    by yours truly, and is included in all NLTK versions since 2.0b9.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Using the same `train_feats` and `test_feats` we created from the `movie_reviews`
    corpus in the previous recipe, we can call the `DecisionTreeClassifier.train()`
    class method to get a trained classifier. We pass `binary=True` because all of
    our features are binary: either the word is present or it''s not. For other classification
    use cases where you have multi-valued features, you will want to stick to the
    default `binary=False`.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this context, `binary` refers to *feature values*, and is not to be confused
    with a *binary classifier*. Our word features are binary because the value is
    either `True`, or the word is not present. If our features could take more than
    two values, we would have to use `binary=False`. A *binary classifier*, on the
    other hand, is a classifier that only chooses between two labels. In our case,
    we are training a binary `DecisionTreeClassifier` on binary features. But it's
    also possible to have a binary classifier with non-binary features, or a non-binary
    classifier with binary features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following is the code for training and evaluating the accuracy of a `DecisionTreeClassifier`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `DecisionTreeClassifier` can take much longer to train than the `NaiveBayesClassifier`.
    For that reason, the default parameters have been overridden so it trains faster.
    These parameters will be explained later.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `DecisionTreeClassifier`, like the `NaiveBayesClassifier`, is also an instance
    of `ClassifierI`. During training, the `DecisionTreeClassifier` creates a tree
    where the child nodes are also instances of `DecisionTreeClassifier`. The leaf
    nodes contain only a single label, while the intermediate child nodes contain
    decision mappings for each feature. These decisions map each feature value to
    another `DecisionTreeClassifier`, which itself may contain decisions for another
    feature, or it may be a final leaf node with a classification label. The `train()`
    class method builds this tree from the ground up, starting with the leaf nodes.
    It then refines itself to minimize the number of decisions needed to get to a
    label by putting the most informative features at the top.
  prefs: []
  type: TYPE_NORMAL
- en: To classify, the `DecisionTreeClassifier` looks at the given feature set and
    traces down the tree, using known feature names and values to make decisions.
    Because we are creating a *binary tree*, each `DecisionTreeClassifier` instance
    also has a *default* decision tree, which it uses when a known feature is not
    present in the feature set being classified. This is a common occurrence in text-based
    feature sets, and indicates that a known word was not in the text being classified.
    This also contributes information towards a classification decision.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The parameters passed in to `DecisionTreeClassifier.train()` can be tweaked
    to improve accuracy or decrease training time. Generally, if you want to improve
    accuracy, you must accept a longer training time and if you want to decrease the
    training time, the accuracy will most likely decrease as well.
  prefs: []
  type: TYPE_NORMAL
- en: Entropy cutoff
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `entropy_cutoff` is used during the tree refinement process. If the entropy
    of the probability distribution of label choices in the tree is greater than the
    `entropy_cutoff`, then the tree is refined further. But if the entropy is lower
    than the `entropy_cutoff`, then tree refinement is halted.
  prefs: []
  type: TYPE_NORMAL
- en: '**Entropy** is the uncertainty of the outcome. As entropy approaches 1.0, uncertainty
    increases and, conversely, as entropy approaches 0.0, uncertainty decreases. In
    other words, when you have similar probabilities, the entropy will be high as
    each probability has a similar likelihood (or uncertainty of occurrence). But
    the more the probabilities differ, the lower the entropy will be.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Entropy is calculated by giving `nltk.probability.entropy()` a `MLEProbDist`
    created from a `FreqDist` of label counts. Here''s an example showing the entropy
    of various `FreqDist` values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: What this all means is that if the label occurrence is very skewed one way or
    the other, the tree doesn't need to be refined because entropy/uncertainty is
    low. But when the entropy is greater than `entropy_cutoff` then the tree must
    be refined with further decisions to reduce the uncertainty. Higher values of
    `entropy_cutoff` will decrease both accuracy and training time.
  prefs: []
  type: TYPE_NORMAL
- en: Depth cutoff
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `depth_cutoff` is also used during refinement to control the depth of the
    tree. The final decision tree will never be deeper than the `depth_cutoff`. The
    default value is `100`, which means that classification may require up to 100
    decisions before reaching a leaf node. Decreasing the `depth_cutoff` will decrease
    the training time and most likely decrease the accuracy as well.
  prefs: []
  type: TYPE_NORMAL
- en: Support cutoff
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `support_` `cutoff` controls how many labeled feature sets are required
    to refine the tree. As the `DecisionTreeClassifier` refines itself, labeled feature
    sets are eliminated once they no longer provide value to the training process.
    When the number of labeled feature sets is less than or equal to `support_cutoff`,
    refinement stops, at least for that section of the tree.
  prefs: []
  type: TYPE_NORMAL
- en: Another way to look at it is that `support_cutoff` specifies the minimum number
    of instances that are required to make a decision about a feature. If `support_cutoff`
    is `20`, and you have less than 20 labeled feature sets with a given feature,
    then you don't have enough instances to make a good decision, and refinement around
    that feature must come to a stop.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The previous recipe covered the creation of training and test feature sets from
    the `movie_reviews` corpus. In the next recipe, we will cover training a `MaxentClassifier`,
    and in the *Measuring precision and recall of a classifier* recipe in this chapter,
    we will use precision and recall to evaluate all the classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: Training a maximum entropy classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The third classifier which we will cover is the `MaxentClassifier`, also known
    as a *conditional exponential classifier*. The **maximum entropy classifier**
    converts labeled feature sets to vectors using encoding. This encoded vector is
    then used to calculate *weights* for each feature that can then be combined to
    determine the most likely label for a feature set.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `MaxentClassifier` requires the `numpy` package, and optionally the `scipy`
    package. This is because the feature encodings use `numpy` arrays. Having `scipy`
    installed also means you will be able to use faster algorithms that consume less
    memory. You can find installation for both at [http://www.scipy.org/Installing_SciPy](http://www.scipy.org/Installing_SciPy).
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many of the algorithms can be quite memory hungry, so you may want to quit all
    your other programs while training a `MaxentClassifier`, just to be safe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use the same `train_feats` and `test_feats` from the `movie_reviews`
    corpus that we constructed before, and call the `MaxentClassifier.train()` class
    method. Like the `DecisionTreeClassifier`, `MaxentClassifier.train()` has its
    own specific parameters that have been tweaked to speed up training. These parameters
    will be explained in more detail later.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The reason this classifier has such a low accuracy is because the parameters
    have been set such that it is unable to learn a more accurate model. This is due
    to the time required to train a suitable model using the `iis` algorithm. Higher
    accuracy models can be learned much faster using the `scipy` algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If training is taking a long time, you can usually cut it off manually by hitting
    *Ctrl + C*. This should stop the current iteration and still return a classifier
    based on whatever state the model is in.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Like the previous classifiers, `MaxentClassifier` inherits from `ClassifierI`.
    Depending on the algorithm, `MaxentClassifier.train()` calls one of the training
    functions in the `nltk.classify.maxent` module. If `scipy` is not installed, the
    default algorithm is `iis`, and the function used is `train_maxent_classifier_with_iis()`.
    The other algorithm that doesn't require `scipy` is `gis`, which uses the `train_maxent_classifier_with_gis()`
    function. **gis** stands for **General Iterative Scaling**, while **iis** stands
    for **Improved Iterative Scaling**. If `scipy` is installed, the `train_maxent_classifier_with_scipy()`
    function is used, and the default algorithm is `cg`. If `megam` is installed and
    you specify the `megam` algorithm, then `train_maxent_classifier_with_megam()`
    is used.
  prefs: []
  type: TYPE_NORMAL
- en: The basic idea behind the maximum entropy model is to build some probability
    distributions that fit the observed data, then choose whichever probability distribution
    has the highest entropy. The `gis` and `iis` algorithms do so by iteratively improving
    the weights used to classify features. This is where the `max_iter` and `min_lldelta`
    parameters come into play.
  prefs: []
  type: TYPE_NORMAL
- en: The `max_iter` specifies the maximum number of iterations to go through and
    update the weights. More iterations will generally improve accuracy, but only
    up to a point. Eventually, the changes from one iteration to the next will hit
    a plateau and further iterations are useless.
  prefs: []
  type: TYPE_NORMAL
- en: The `min_lldelta` specifies the minimum change in the *log likelihood* required
    to continue iteratively improving the weights. Before beginning training iterations,
    an instance of the `nltk.classify.util.CutoffChecker` is created. When its `check()`
    method is called, it uses functions such as `nltk.classify.util.log_likelihood()`
    to decide whether the cutoff limits have been reached. The **log** **likelihood**
    is the log (using `math.log()`) of the average label probability of the training
    data (which is the log of the average likelihood of a label). As the log likelihood
    increases, the model improves. But it too will reach a plateau where further increases
    are so small that there is no point in continuing. Specifying the `min_lldelta`
    allows you to control how much each iteration must increase the log likelihood
    before stopping iterations.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Like the `NaiveBayesClassifier`, you can see the most informative features by
    calling the `show_most_informative_features()` method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The numbers shown are the weights for each feature. This tells us that the word
    *worst* is *negatively weighted* towards the `pos` label, and *positively weighted*
    towards the `neg` label. In other words, if the word *worst* is found in the feature
    set, then there's a strong possibility that the text should be classified `neg`.
  prefs: []
  type: TYPE_NORMAL
- en: Scipy algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The algorithms available when `scipy` is installed are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**CG** (**Conjugate gradient** algorithm)—the default `scipy` algorithm'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**BFGS** (**Broyden-Fletcher-Goldfarb-Shanno** algorithm)—very memory hungry'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Powell
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LBFGSB (limited memory version of BFGS)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nelder-Mead
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here''s what happens when you use the CG algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This is the most accurate classifier so far.
  prefs: []
  type: TYPE_NORMAL
- en: Megam algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you have installed the `megam` package, then you can use the `megam` algorithm.
    It's a bit faster than the `scipy` algorithms and about as accurate. Installation
    instructions and information can be found at [http://www.cs.utah.edu/~hal/megam/](http://www.cs.utah.edu/~hal/megam/).
    The function `nltk.classify.megam.config_megam()` can be used to specify where
    the `megam` executable is found. Or, if `megam` can be found in the standard executable
    paths, NLTK will configure it automatically.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The `megam` algorithm is highly recommended for its accuracy and speed of training.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Bag of Words feature extraction* and the *Training a naive Bayes classifier*
    recipes in this chapter show how to construct the training and testing features
    from the `movie_reviews` corpus. In the next recipe, we will cover how and why
    to evaluate a classifier using precision and recall instead of accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring precision and recall of a classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In addition to accuracy, there are a number of other metrics used to evaluate
    classifiers. Two of the most common are *precision* and *recall*. To understand
    these two metrics, we must first understand *false positives* and *false negatives*.
    **False positives** happen when a classifier classifies a feature set with a label
    it shouldn't have. **False negatives** happen when a classifier doesn't assign
    a label to a feature set that should have it. In a *binary classifier*, these
    errors happen at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s an example: the classifier classifies a movie review as `pos`, when
    it should have been `neg`. This counts as a *false positive* for the `pos` label,
    and a *false negative* for the `neg` label. If the classifier had correctly guessed
    `neg`, then it would count as a **true positive** for the `neg` label, and a **true
    negative** for the `pos` label.'
  prefs: []
  type: TYPE_NORMAL
- en: 'How does this apply to precision and recall? **Precision** is the *lack of
    false positives*, and **recall** is the *lack of false negatives*. As you will
    see, these two metrics are often in competition: the more precise a classifier
    is, the lower the recall, and vice versa.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s calculate the precision and recall of the `NaiveBayesClassifier` we
    trained in the *Training a naive Bayes classifier* recipe. The `precision_recall()`
    function in `classification.py` looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This function takes two arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: The trained classifier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Labeled test features, also known as a gold standard.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'These are the same arguments you pass to the `accuracy()` function. The `precision_recall()`
    returns two dictionaries; the first holds the precision for each label, and the
    second holds the recall for each label. Here''s an example usage with the `nb_classifier`
    and the `test_feats` we created in the *Training a naive Bayes classifier* recipe
    earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This tells us that while the `NaiveBayesClassifier` can correctly identify most
    of the `pos` feature sets (high recall), it also classifies many of the `neg`
    feature sets as `pos` (low precision). This behavior contributes to the high precision
    but low recall for the `neg` label—as the `neg` label isn't given often (low recall),
    and when it is, it's very likely to be correct (high precision). The conclusion
    could be that there are certain common words that are biased towards the `pos`
    label, but occur frequently enough in the `neg` feature sets to cause mis-classifications.
    To correct this behavior, we will use only the most informative words in the next
    recipe, *Calculating high information words*.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To calculate precision and recall, we must build two sets for each label. The
    first set is known as the **reference set**, and contains all the correct values.
    The second set is called the **test set** , and contains the values guessed by
    the classifier. These two sets are compared to calculate the precision or recall
    for each label.
  prefs: []
  type: TYPE_NORMAL
- en: '**Precision** is defined as the size of the intersection of both sets divided
    by the size of the test set. In other words, the percentage of the test set that
    was guessed correctly. In Python, the code is `float(len(reference.intersection(test)))
    / len(test)`.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Recall** is the size of the intersection of both sets divided by the size
    of the reference set, or the percentage of the reference set that was guessed
    correctly. The Python code is `float(len(reference.intersection(test))) / len(reference)`.'
  prefs: []
  type: TYPE_NORMAL
- en: The `precision_recall()` function in `classification.py` iterates over the labeled
    test features and classifies each one. We store the *numeric index* of the feature
    set (starting with `0`) in the reference set for the known training label, and
    also store the index in the test set for the guessed label. If the classifier
    guesses `pos` but the training label is `neg`, then the index is stored in the
    *reference set* for `neg` and the *test set* for `pos`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We use the numeric index because the feature sets aren't hashable, and we need
    a unique value for each feature set.
  prefs: []
  type: TYPE_NORMAL
- en: The `nltk.metrics` package contains functions for calculating both precision
    and recall, so all we really have to do is build the sets, then call the appropriate
    function.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s try it with the `MaxentClassifier` we trained in the previous recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: This classifier is much more well-rounded than the `NaiveBayesClassifier`. In
    this case, the label bias is much less significant, and the reason is that the
    `MaxentClassifier` weighs its features according to its own internal model. Words
    that are more significant are those that occur primarily in a single label, and
    will get higher weights in the model. Words that are common to both labels will
    get lower weights, as they are less significant.
  prefs: []
  type: TYPE_NORMAL
- en: F-measure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The **F-measure** is defined as the weighted harmonic mean of precision and
    recall. If `p` is the *precision*, and `r` is the *recall*, the formula is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'where `alpha` is a weighing constant that defaults to `0.5`. You can use `nltk.metrics.f_measure()`
    to get the F-measure. It takes the same arguments as for the `precision()` and
    `recall()` functions: a reference set and a test set. It''s often used instead
    of accuracy to measure a classifier. However, precision and recall are found to
    be much more useful metrics, as the F-measure can hide the kinds of imbalances
    we saw with the `NaiveBayesClassifier`.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the *Training a naive Bayes classifier* recipe, we collected training and
    testing feature sets, and trained the `NaiveBayesClassifier`. The `MaxentClassifier`
    was trained in the *Training a maximum entropy classifier* recipe. In the next
    recipe, we will explore eliminating the less significant words, and use only the
    high information words to create our feature sets.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating high information words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **high information word** is a word that is strongly biased towards a single
    classification label. These are the kinds of words we saw when we called the `show_most_informative_features()`
    method on both the `NaiveBayesClassifier` and the `MaxentClassifier`. Somewhat
    surprisingly, the top words are different for both classifiers. This discrepancy
    is due to how each classifier calculates the significance of each feature, and
    it's actually beneficial to have these different methods as they can be combined
    to improve accuracy, as we will see in the next recipe, *Combining classifiers
    with voting*.
  prefs: []
  type: TYPE_NORMAL
- en: The **low information words** are words that are common to all labels. It may
    be counter-intuitive, but eliminating these words from the training data can actually
    improve accuracy, precision, and recall. The reason this works is that using only
    high information words reduces the noise and confusion of a classifier's internal
    model. If all the words/features are highly biased one way or the other, it's
    much easier for the classifier to make a correct guess.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we need to calculate the high information words in the `movie_review`
    corpus. We can do this using the `high_information_words()` function in `featx.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: It takes one argument , which is a list of 2-tuples of the form `[(label, words)]`
    where `label` is the classification label, and `words` is a list of words that
    occur under that label. It returns a list of the high information words, sorted
    from most informative to least informative.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have the high information words, we use the feature detector function
    `bag_of_words_in_set()`, also found in `featx.py`, which will let us filter out
    all low information words.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: With this new feature detector, we can call `label_feats_from_corpus()` and
    get a new `train_feats` and `test_feats` using `split_label_feats()`. These two
    functions were covered in the *Training a naive Bayes classifier* recipe in this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have new training and testing feature sets, let''s train and evaluate
    a `NaiveBayesClassifier`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: While the `neg` precision and `pos` recall have both decreased somewhat, `neg`
    recall and `pos` precision have increased drastically. Accuracy is now a little
    higher than the `MaxentClassifier`.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `high_information_words()` function starts by counting the frequency of
    every word, as well as the conditional frequency for each word within each label.
    This is why we need the words to be labelled, so we know how often each word occurs
    in each label.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have this `FreqDist` and `ConditionalFreqDist`, we can score each word
    on a per-label basis. The default `score_fn` is `nltk.metrics.BigramAssocMeasures.chi_sq()`,
    which calculates the chi-square score for each word using the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`n_ii`: The frequency of the word in the label.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`n_ix`: The total frequency of the word across all labels.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`n_xi`: The total frequency of all words that occurred in the label.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`n_xx`: The total frequency for all words in all labels.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The simplest way to think about these numbers is that the closer `n_ii` is to
    `n_ix`, the higher the score. Or, the more often a word occurs in a label, relative
    to its overall occurrence, the higher the score.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have the scores for each word in each label, we can filter out all words
    whose score is below the `min_score` threshold. We keep the words that meet or
    exceed the threshold, and return all high scoring words in each label.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is recommended to experiment with different values of `min_score` to see
    what happens. In some cases, less words may improve the metrics even more, while
    in other cases more words is better.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are a number of other scoring functions available in the `BigramAssocMeasures`
    class, such as `phi_sq()` for phi-square, `pmi()` for pointwise mutual information,
    and `jaccard()` for using the Jaccard index. They all take the same arguments,
    and so can be used interchangeably with `chi_sq()`.
  prefs: []
  type: TYPE_NORMAL
- en: MaxentClassifier with high information words
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s evaluate the `MaxentClassifier` using the high information words feature
    sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the improvements are much more modest than with the `NaiveBayesClassifier`
    due to the fact that the `MaxentClassifier` already weights all features by significance.
    But using only the high information words still makes a positive difference compared
    to when we used all the words. And the precisions and recalls for each label are
    closer to each other, giving the `MaxentClassifier` even more well-rounded performance.
  prefs: []
  type: TYPE_NORMAL
- en: DecisionTreeClassifier with high information words
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now, let''s evaluate the `DecisionTreeClassifier`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: The accuracy is about the same, even with a larger `depth_cutoff`, and smaller
    `support_cutoff` and `entropy_cutoff`. The results show that the `DecisionTreeClassifier`
    was already putting the high information features at the top of the tree, and
    it will only improve if we increase the depth significantly. But that could make
    training time prohibitively long.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We started this chapter with the *Bag of Words feature extraction* recipe. The
    `NaiveBayesClassifier` was originally trained in the *Training a naive Bayes classifier*
    recipe, and the `MaxentClassifier` was trained in the *Training a maximum entropy
    classifier* recipe. Details on precision and recall can be found in the *Measuring
    precision and recall of a classifier* recipe. We will be using only high information
    words in the next two recipes, where we combine classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: Combining classifiers with voting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One way to improve classification performance is to combine classifiers. The
    simplest way to combine multiple classifiers is to use voting, and choose whichever
    label gets the most votes. For this style of voting, it's best to have an odd
    number of classifiers so that there are no ties. This means combining at least
    three classifiers together. The individual classifiers should also use different
    algorithms; the idea is that multiple algorithms are better than one, and the
    combination of many can compensate for individual bias.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we need to have at least three trained classifiers to combine, we are going
    to use a `NaiveBayesClassifier`, a `DecisionTreeClassifier`, and a `MaxentClassifier`,
    all trained on the highest information words of the `movie_reviews` corpus. These
    were all trained in the previous recipe, so we will combine these three classifiers
    with voting.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the `classification.py` module, there is a `MaxVoteClassifier` class.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: To create it, you pass in a list of classifiers that you want to combine. Once
    created, it works just like any other classifier. Though it may take about three
    times longer to classify, it should generally be at least as accurate as any individual
    classifier.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: These metrics are about on par with the `MaxentClassifier` and `NaiveBayesClassifier`.
    Some numbers are slightly better, some worse. It's likely that a significant improvement
    to the `DecisionTreeClassifier` could produce some better numbers.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `MaxVoteClassifier` extends the `nltk.classify.ClassifierI` interface,
    which requires implementing at least two methods:'
  prefs: []
  type: TYPE_NORMAL
- en: The `labels()` function must return a list of possible labels. This will be
    the union of the `labels()` of each classifier passed in at initialization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `classify()` function takes a single feature set and returns a label. The
    `MaxVoteClassifier` iterates over its classifiers and calls `classify()` on each
    of them, recording their label as a vote in a `FreqDist`. The label with the most
    votes is returned using `FreqDist.max()`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While it doesn't check for this, the `MaxVoteClassifier` assumes that all the
    classifiers passed in at initialization use the same labels. Breaking this assumption
    may lead to odd behavior.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous recipe, we trained a `NaiveBayesClassifier`, a `MaxentClassifier`,
    and a `DecisionTreeClassifier` using only the highest information words. In the
    next recipe, we will use the `reuters` corpus and combine many binary classifiers
    in order to create a multi-label classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Classifying with multiple binary classifiers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far we have focused on **binary classifiers**, which classify with *one of
    two possible labels*. The same techniques for training a binary classifier can
    also be used to create a *multi-class* classifier, which is a classifier that
    can classify with *one of many possible labels*. But there are also cases where
    you need to be able to classify with *multiple labels*. A classifier that can
    return more than one label is a **multi-label classifier**.
  prefs: []
  type: TYPE_NORMAL
- en: A common technique for creating a multi-label classifier is to combine many
    binary classifiers, one for each label. You train each binary classifier so that
    it either returns a known label, or returns something else to signal that the
    label does not apply. Then you can run all the binary classifiers on your feature
    set to collect all the applicable labels.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `reuters` corpus contains multi-labeled text that we can use for training
    and evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: We will train one binary classifier per label, which means we will end up with
    90 binary classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, we should calculate the high information words in the `reuters` corpus.
    This is done with the `reuters_high_info_words()` function in `featx.py`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Then we need to get training and test feature sets based on those high information
    words. This is done with the `reuters_train_test_feats()`, also found in `featx.py`.
    It defaults to using `bag_of_words()` as its `feature_detector`, but we will be
    overriding this using `bag_of_words_in_set()` to use only the high information
    words.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: We can use these two functions to get a list of multi-labeled training and testing
    feature sets.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The `multi_train_feats` and `multi_test_feats` are multi-labeled feature sets.
    That means they have a list of labels, instead of a single label, and they look
    like the `[(featureset, [label])]`, as each feature set can have one or more labels.
    With this training data, we can train multiple binary classifiers. The `train_binary_classifiers()`
    function in the `classification.py` takes a training function, a list of multi-label
    feature sets, and a set of possible labels to return a `dict` of the `label :
    binary` classifier.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: To use this function, we need to provide a training function that takes a single
    argument, which is the training data. This will be a simple `lambda` wrapper around
    the `MaxentClassifier.train()`, so we can specify extra keyword arguments.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Now we can define a `MultiBinaryClassifier`, which takes a list of labeled classifiers
    of the form `[(label, classifier)]` where the `classifier` is assumed to be a
    binary classifier that either returns the `label`, or something else if the label
    doesn't apply.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: We can construct this class using the binary classifiers we just created.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: To evaluate this classifier, we can use precision and recall, but not accuracy.
    That's because the accuracy function assumes single values, and doesn't take into
    account partial matches. For example, if the multi-classifier returns three labels
    for a feature set, and two of them are correct but the third is not, then the
    `accuracy()` would mark that as incorrect. So instead of using accuracy, we will
    use the **masi distance**, which measures partial overlap between two sets. The
    lower the masi distance, the better the match. A lower average masi distance,
    therefore, means more accurate partial matches. The `multi_metrics()` function
    in the `classification.py` calculates the precision and recall of each label,
    along with the average masi distance.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Using this with the `multi_classifier` we just created, gives us the following
    results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'So our average masi distance is fairly low, which means our multi-label classifier
    is usually mostly accurate. Let''s take a look at a few precisions and recalls:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, there's quite a range of values. But, in general, the labels
    that have more feature sets will have higher precision and recall, and those with
    less feature sets will have lower performance. When there's not a lot of feature
    sets for a classifier to learn from, you can't expect it to perform well.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `reuters_high_info_words()` function is fairly simple; it constructs a list
    of `[(label, words)]` for each category of the `reuters` corpus, then passes it
    in to the `high_information_words()` function to return a list of the most informative
    words in the `reuters` corpus.
  prefs: []
  type: TYPE_NORMAL
- en: With the resulting set of words, we create a feature detector function using
    the `bag_of_words_in_set()`. This is then passed in to the `reuters_train_test_feats()`,
    which returns two lists, the first containing `[(feats, labels)]` for all the
    training files, and the second list has the same for all the test files.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we train a binary classifier for each label using `train_binary_classifiers()`.
    This function constructs two lists for each label, one containing positive training
    feature sets, the other containing negative training feature sets. The **Positive
    feature sets** are those feature sets that classify for the label. The **Negative
    feature sets** for a label comes from the positive feature sets for all other
    labels. For example, a feature set that is *positive* for `zinc` and `sunseed`
    is a *negative* example for all the other 88 labels. Once we have positive and
    negative feature sets for each label, we can train a binary classifier for each
    label using the given training function.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the resulting dictionary of binary classifiers, we create an instance
    of the `MultiBinaryClassifier`. This class extends the `nltk.classify.MultiClassifierI`
    interface, which requires at least two functions:'
  prefs: []
  type: TYPE_NORMAL
- en: The `labels()` function must return a list of possible labels.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `classify()` function takes a single feature set and returns a `set` of
    labels. To create this `set`, we iterate over the binary classifiers, and any
    time a call to the `classify()` returns its label, we add it to the set. If it
    returns something else, we continue.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, we evaluate the multi-label classifier using the `multi_metrics()`
    function. It is similar to the `precision_recall()` function from the *Measuring
    precision and recall of a classifier* recipe, but in this case we know the classifier
    is an instance of the `MultiClassifierI` and it can therefore return multiple
    labels. It also keeps track of the masi distance for each set of classification
    labels using the `nltk.metrics.masi_` `distance()`. The `multi_metrics()` function
    returns three values:'
  prefs: []
  type: TYPE_NORMAL
- en: A dictionary of precisions for each label.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A dictionary of recalls for each label.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The average masi distance for each feature set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The nature of the `reuters` corpus introduces the **class-imbalance problem**.
    This problem occurs when some labels have very few feature sets, and other labels
    have many. The binary classifiers that have few positive instances to train on
    end up with far more negative instances, and are therefore strongly biased towards
    the negative label. There's nothing inherently wrong about this, as the bias reflects
    the data, but the negative instances can overwhelm the classifier to the point
    where it's nearly impossible to get a positive result. There are a number of advanced
    techniques for overcoming this problem, but they are out of the scope of this
    book.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `MaxentClassifier` is covered in the *Training a maximum entropy classifier*
    recipe in this chapter. The *Measuring precision and recall of a classifier* recipe
    shows how to evaluate a classifier, while the *Calculating high information words*
    recipe describes how to use only the best features.
  prefs: []
  type: TYPE_NORMAL
