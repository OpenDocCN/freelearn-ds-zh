<html><head></head><body>
<div id="_idContainer114" class="calibre2">
<h1 class="chapter-number" id="_idParaDest-58"><a id="_idTextAnchor058" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.1.1">4</span></h1>
<h1 id="_idParaDest-59" class="calibre5"><a id="_idTextAnchor059" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.2.1">Building Data Engineering Pipelines with Snowpark</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.3.1">Data is the heartbeat of every organization, and data engineering is the lifeblood that ensures that current, accurate data is flowing through for various consumption. </span><span class="kobospan" id="kobo.3.2">The role of a data engineer is to develop and manage the data engineering pipeline and the process that collects, transforms, and delivers data to a different </span><strong class="bold"><span class="kobospan" id="kobo.4.1">line of business</span></strong><span class="kobospan" id="kobo.5.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.6.1">LOB</span></strong><span class="kobospan" id="kobo.7.1">). </span><span class="kobospan" id="kobo.7.2">As Gartner’s research rightly mentions, “</span><em class="italic"><span class="kobospan" id="kobo.8.1">The increasing diversity of data, and the need to provide the right data to the right people at the right time, has created a demand for the data engineering practice. </span><span class="kobospan" id="kobo.8.2">Data and analytics leaders must integrate the data engineering discipline into their data management strategy.</span></em><span class="kobospan" id="kobo.9.1">” This chapter discusses a practical approach to building efficient data engineering pipelines </span><span><span class="kobospan" id="kobo.10.1">with Snowpark.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.11.1">In this chapter, we’re going to cover the following </span><span><span class="kobospan" id="kobo.12.1">main topics:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.13.1">Developing resilient data pipelines </span><span><span class="kobospan" id="kobo.14.1">with Snowpark</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.15.1">Deploying efficient DataOps </span><span><span class="kobospan" id="kobo.16.1">in Snowpark</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.17.1">Overview of tasks </span><span><span class="kobospan" id="kobo.18.1">in Snowflake</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.19.1">Implementing logging and tracing </span><span><span class="kobospan" id="kobo.20.1">in Snowpark</span></span></li>
</ul>
<h1 id="_idParaDest-60" class="calibre5"><a id="_idTextAnchor060" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.21.1">Technical requirements</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.22.1">This chapter requires an active Snowflake account and Python installed with Anaconda and configured locally. </span><span class="kobospan" id="kobo.22.2">You can sign up for a Snowflake trial account </span><span><span class="kobospan" id="kobo.23.1">at </span></span><a href="https://signup.snowflake.com/" class="calibre6 pcalibre1 pcalibre"><span><span class="kobospan" id="kobo.24.1">https://signup.snowflake.com/</span></span></a><span><span class="kobospan" id="kobo.25.1">.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.26.1">The technical requirements for environment setup are the same as in the previous chapters. </span><span class="kobospan" id="kobo.26.2">If you haven’t set up your environment yet, please refer to the previous chapter. </span><span class="kobospan" id="kobo.26.3">Supporting materials are available </span><span><span class="kobospan" id="kobo.27.1">at </span></span><a href="https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark" class="calibre6 pcalibre1 pcalibre"><span><span class="kobospan" id="kobo.28.1">https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark</span></span></a><span><span class="kobospan" id="kobo.29.1">.</span></span></p>
<h1 id="_idParaDest-61" class="calibre5"><a id="_idTextAnchor061" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.30.1">Developing resilient data pipelines with Snowpark</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.31.1">A</span><a id="_idIndexMarker210" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.32.1"> robust and resilient data pipeline will equip organizations to source, collect, analyze, and effectively use insights to grow business and deliver cost-saving business processes. </span><span class="kobospan" id="kobo.32.2">Traditional data pipelines are difficult to manage and do not support the organization’s evolving data needs. </span><span class="kobospan" id="kobo.32.3">Snowpark solves problems that conventional data pipelines have by running them natively on the Snowflake Data Cloud and making extracting information from data in the Data Cloud easier and faster. </span><span class="kobospan" id="kobo.32.4">This section will cover the various characteristics of resilient data pipelines, how to develop them in Snowpark, and </span><span><span class="kobospan" id="kobo.33.1">their benefits.</span></span></p>
<h2 id="_idParaDest-62" class="calibre7"><a id="_idTextAnchor062" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.34.1">Traditional versus modern data pipelines</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.35.1">A </span><a id="_idIndexMarker211" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.36.1">significant challenge of a traditional data pipeline is that it takes considerable time and cost to develop and manage, with high technical debt. </span><span class="kobospan" id="kobo.36.2">It also consists of multiple tools that take much time to integrate. </span><span class="kobospan" id="kobo.36.3">Due to the complexity of the solution, there is a possibility of delayed data due to latency and support for streaming data. </span><span class="kobospan" id="kobo.36.4">The following diagram highlights the </span><span><span class="kobospan" id="kobo.37.1">traditional method:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer098">
<span class="kobospan" id="kobo.38.1"><img alt="Figure 4.1 – Traditional data pipeline" src="image/B19923_04_01.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.39.1">Figure 4.1 – Traditional data pipeline</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.40.1">The architecture shows a complex of technologies and systems being stitched together to deliver data from the source to consumers, with multiple points of failure at each stage. </span><span class="kobospan" id="kobo.40.2">There are also issues of data governance and security due to data silos being present with numerous copies of the </span><span><span class="kobospan" id="kobo.41.1">same data.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.42.1">Modern </span><a id="_idIndexMarker212" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.43.1">data pipelines work based on a unified platform and multi-workload model. </span><span class="kobospan" id="kobo.43.2">They integrate data sources such as batch and streaming to enhance productivity with streamlined architecture by enabling various </span><strong class="bold"><span class="kobospan" id="kobo.44.1">business intelligence</span></strong><span class="kobospan" id="kobo.45.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.46.1">BI</span></strong><span class="kobospan" id="kobo.47.1">) and analytics workloads and supporting internal and external users. </span><span class="kobospan" id="kobo.47.2">The unified platform architecture supports continuous, extensible data processing pipelines with scalable performance. </span><span class="kobospan" id="kobo.47.3">The following diagram highlights the modern </span><span><span class="kobospan" id="kobo.48.1">Snowflake approach:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer099">
<span class="kobospan" id="kobo.49.1"><img alt="Figure 4.2 – Modern data pipeline" src="image/B19923_04_02.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.50.1">Figure 4.2 – Modern data pipeline</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.51.1">Snowpark stands out as a modern data pipeline tool due to its native integration with Snowflake, a leading cloud data warehouse, enabling seamless data processing directly within Spark applications. </span><span class="kobospan" id="kobo.51.2">Offering a unified development experience with familiar programming languages such as Scala and Java, Snowpark eliminates the complexity associated with traditional Spark setups, allowing for streamlined development and maintenance of data pipelines. </span><span class="kobospan" id="kobo.51.3">Snowpark’s optimized performance for Snowflake’s architecture ensures efficient data processing and reduced latency, enabling quick analysis of large datasets. </span><span class="kobospan" id="kobo.51.4">Moreover, its advanced analytics capabilities, scalability, and cost-effectiveness make it a compelling choice for organizations seeking to build agile, cloud-native data pipelines with enhanced productivity and flexibility compared to traditional </span><span><span class="kobospan" id="kobo.52.1">Spark setups.</span></span></p>
<h2 id="_idParaDest-63" class="calibre7"><a id="_idTextAnchor063" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.53.1">Data engineering with Snowpark</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.54.1">Snowpark </span><a id="_idIndexMarker213" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.55.1">has many data engineering capabilities, making it a</span><a id="_idIndexMarker214" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.56.1"> fast and flexible platform that enables developers to use Python for data engineering. </span><span class="kobospan" id="kobo.56.2">With the support</span><a id="_idIndexMarker215" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.57.1"> of </span><strong class="bold"><span class="kobospan" id="kobo.58.1">extract, transform, and load</span></strong><span class="kobospan" id="kobo.59.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.60.1">ETL</span></strong><span class="kobospan" id="kobo.61.1">) and </span><strong class="bold"><span class="kobospan" id="kobo.62.1">extract, load, and transform</span></strong><span class="kobospan" id="kobo.63.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.64.1">ELT</span></strong><span class="kobospan" id="kobo.65.1">), developers can use the </span><a id="_idIndexMarker216" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.66.1">Snowpark client for development and interact with the Snowflake engine for processing using their favorite developer environment. </span><span class="kobospan" id="kobo.66.2">With the support of Anaconda, you can ensure that required packages and dependencies are readily available for Snowpark scripts. </span><span class="kobospan" id="kobo.66.3">And it becomes easier to accelerate the growth of product pipelines. </span><span class="kobospan" id="kobo.66.4">Data pipelines in Snowpark can be batch or real-time, utilizing scalable, high-performant multi-cluster warehouses capable of handling complex data transformations without </span><span><span class="kobospan" id="kobo.67.1">compromising performance:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer100">
<span class="kobospan" id="kobo.68.1"><img alt="Figure 4.3 – Snowpark data engineering" src="image/B19923_04_03.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.69.1">Figure 4.3 – Snowpark data engineering</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.70.1">Snowpark enhances the entire data engineering lifecycle with an engine that enables expressiveness and flexibility for developers with the simplicity of Data Cloud operations. </span><span class="kobospan" id="kobo.70.2">Snowflake can help make data centralized, including structured, semi-structured, and unstructured data loaded into Snowflake for processing. </span><span class="kobospan" id="kobo.70.3">Transformations can be carried out with the help of the powerful Python-based Snowpark functions. </span><span class="kobospan" id="kobo.70.4">Snowpark data engineering workloads can be fully managed alongside other Snowflake objects throughout the development lifecycle with built-in monitoring and orchestration capabilities that support complex data pipelines at scale powered by the Data Cloud. </span><span class="kobospan" id="kobo.70.5">The result of advanced data transformations is stored inside Snowflake and can be used for different data consumers. </span><span class="kobospan" id="kobo.70.6">Snowpark data pipelines reduce the number of stages data needs to move to actionable insights by removing the step that moves data </span><span><span class="kobospan" id="kobo.71.1">for computation.</span></span></p>
<h2 id="_idParaDest-64" class="calibre7"><a id="_idTextAnchor064" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.72.1">Implementing programmatic ELT with Snowpark</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.73.1">Snowflake </span><a id="_idIndexMarker217" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.74.1">supports and recommends a modern ELT implementation pattern for data engineering instead of the legacy ETL process. </span><span class="kobospan" id="kobo.74.2">ETL is a </span><a id="_idIndexMarker218" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.75.1">pattern where data is extracted from various sources, transformed in the data pipeline, and then the transformed data is loaded into a destination such </span><a id="_idIndexMarker219" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.76.1">as a data warehouse or data mart. </span><span class="kobospan" id="kobo.76.2">The following diagram shows the comparison of ETL </span><span><span class="kobospan" id="kobo.77.1">versus ELT:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer101">
<span class="kobospan" id="kobo.78.1"><img alt="Figure 4.4 – ETL versus ELT" src="image/B19923_04_04.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.79.1">Figure 4.4 – ETL versus ELT</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.80.1">ELT is a pattern that</span><a id="_idIndexMarker220" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.81.1"> is more suited for the Snowflake Data Cloud, where data is extracted from the source and loaded into Snowflake. </span><span class="kobospan" id="kobo.81.2">This data is then transformed within Snowflake using Snowpark. </span><span class="kobospan" id="kobo.81.3">Snowpark pipelines are designed to extract and load the data first and then transform it in the destination as the transformation is done inside Snowflake, which provides better scalability and elasticity. </span><span class="kobospan" id="kobo.81.4">The ELT also improves performance and reduces the time it takes to ingest, transform, and analyze the data within Snowflake using Snowpark. </span><span class="kobospan" id="kobo.81.5">The following diagram shows the different layers of data </span><span><span class="kobospan" id="kobo.82.1">within Snowflake:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer102">
<span class="kobospan" id="kobo.83.1"><img alt="Figure 4.5 – Data stages in Snowflake" src="image/B19923_04_05.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.84.1">Figure 4.5 – Data stages in Snowflake</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.85.1">The</span><a id="_idIndexMarker221" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.86.1"> data pipelines build the different data stages inside Snowflake. </span><span class="kobospan" id="kobo.86.2">These stages are databases and schemas with objects such as tables </span><a id="_idIndexMarker222" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.87.1">and views inside them. </span><span class="kobospan" id="kobo.87.2">The raw data is ingested from the source systems into Snowflake with no transformations since Snowflake supports multiple data formats. </span><span class="kobospan" id="kobo.87.3">This data is then transformed using Snowpark into the conformed stage containing the de-duped and standardized data. </span><span class="kobospan" id="kobo.87.4">This becomes the data that feeds into the next step in the data pipeline. </span><span class="kobospan" id="kobo.87.5">The reference stage has the business definition and data mappings with the hierarchies and the master data. </span><span class="kobospan" id="kobo.87.6">The final stage has the modeled data, which has the clean and transformed data. </span><span class="kobospan" id="kobo.87.7">Snowpark has many functions that help with doing value-added transformations that help convert data into a business-ready format accessible to users and applications, making it more valuable for </span><span><span class="kobospan" id="kobo.88.1">the organization.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.89.1">ETL versus ELT in Snowpark</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.90.1">Snowpark </span><a id="_idIndexMarker223" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.91.1">supports both ETL and ELT workloads. </span><span class="kobospan" id="kobo.91.2">While ELT is famous for modern pipelines, the ETL pattern is also used in some scenarios. </span><span class="kobospan" id="kobo.91.3">ETL is commonly used with structured data where the total volume of data is small. </span><span class="kobospan" id="kobo.91.4">It is also used in the system for migrating legacy databases to the Data Cloud where the source and target data types differ. </span><span class="kobospan" id="kobo.91.5">ELT provides a significant advantage compared to the traditional ETL process. </span><span class="kobospan" id="kobo.91.6">ELT supports large volumes of structured, unstructured, and semi-structured data that can be processed using Snowflake. </span><span class="kobospan" id="kobo.91.7">It also allows developers and analysts to experiment with data as it is loaded into Snowflake. </span><span class="kobospan" id="kobo.91.8">ELT also maximizes the option for them to transform data to get potential insights. </span><span class="kobospan" id="kobo.91.9">It also supports low latency and real-time analytics. </span><span class="kobospan" id="kobo.91.10">ELT is better suited for Snowflake than the traditional ETL for these reasons. </span><span class="kobospan" id="kobo.91.11">The following section will cover how to develop efficient DataOps </span><span><span class="kobospan" id="kobo.92.1">in Snowpark.</span></span></p>
<h1 id="_idParaDest-65" class="calibre5"><a id="_idTextAnchor065" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.93.1">Deploying efficient DataOps in Snowpark</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.94.1">DataOps</span><a id="_idIndexMarker224" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.95.1"> helps data</span><a id="_idIndexMarker225" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.96.1"> teams reduce development times, increase data quality, and maximize the business value of data by bringing more rigor to the development and management of data pipelines. </span><span class="kobospan" id="kobo.96.2">It also ensures that the data is clean, accurate, and up-to-date in a streamlined environment with data governance. </span><span class="kobospan" id="kobo.96.3">Data engineering introduces the processes and capabilities required to effectively develop, manage, and deploy data engineering pipelines. </span><span class="kobospan" id="kobo.96.4">The following diagram highlights the </span><span><span class="kobospan" id="kobo.97.1">DataOps approach:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer103">
<span class="kobospan" id="kobo.98.1"><img alt="Figure 4.6 – DataOps process" src="image/B19923_04_06.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.99.1">Figure 4.6 – DataOps process</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.100.1">The DataOps process focuses on bringing agile development to data engineering pipelines using an iterative development, testing, and deployment process in loops. </span><span class="kobospan" id="kobo.100.2">It also</span><a id="_idIndexMarker226" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.101.1"> includes </span><strong class="bold"><span class="kobospan" id="kobo.102.1">continuous integration and continuous deployment</span></strong><span class="kobospan" id="kobo.103.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.104.1">CI/CD</span></strong><span class="kobospan" id="kobo.105.1">) for data, schema changes, and the data versioning and automation of data models and artifacts. </span><span class="kobospan" id="kobo.105.2">This section will show an example of a data engineering pipeline executed </span><span><span class="kobospan" id="kobo.106.1">in Snowpark.</span></span></p>
<h2 id="_idParaDest-66" class="calibre7"><a id="_idTextAnchor066" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.107.1">Developing a data engineering pipeline</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.108.1">Creating a </span><a id="_idIndexMarker227" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.109.1">resilient data engineering pipeline within the Snowpark framework requires integrating three core </span><span><span class="kobospan" id="kobo.110.1">components seamlessly:</span></span></p>
<ol class="calibre13">
<li class="calibre14"><span class="kobospan" id="kobo.111.1">First and foremost, data engineers must master the art of loading data into Snowflake, setting the stage for subsequent processing. </span><span class="kobospan" id="kobo.111.2">This initial step sets the foundation upon which the entire pipeline </span><span><span class="kobospan" id="kobo.112.1">is built.</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.113.1">Second, the transformative power of Snowpark data functions comes into play, enabling engineers to shape and mold the data to meet specific analytical needs. </span><a href="B19923_03.xhtml#_idTextAnchor042" class="calibre6 pcalibre1 pcalibre"><span><em class="italic"><span class="kobospan" id="kobo.114.1">Chapter 3</span></em></span></a><em class="italic"><span class="kobospan" id="kobo.115.1">,</span></em> <em class="italic"><span class="kobospan" id="kobo.116.1">Simplifying Data Processing Using Snowpark</span></em><span class="kobospan" id="kobo.117.1"> provided a detailed exploration of DataFrame operations, laying the groundwork for this pivotal </span><span><span class="kobospan" id="kobo.118.1">transformation phase.</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.119.1">Finally, the data journey culminates in bundling these operations as Snowpark stored procedures, offering efficiency and repeatability in </span><span><span class="kobospan" id="kobo.120.1">handling data.</span></span></li>
</ol>
<p class="calibre3"><span class="kobospan" id="kobo.121.1">As we </span><a id="_idIndexMarker228" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.122.1">delve into this section, building upon the knowledge garnered from </span><a href="B19923_02.xhtml#_idTextAnchor028" class="calibre6 pcalibre1 pcalibre"><span><em class="italic"><span class="kobospan" id="kobo.123.1">Chapter 2</span></em></span></a><em class="italic"><span class="kobospan" id="kobo.124.1">, Establishing a Foundation with Snowpark</span></em><span class="kobospan" id="kobo.125.1"> and </span><a href="B19923_03.xhtml#_idTextAnchor042" class="calibre6 pcalibre1 pcalibre"><span><em class="italic"><span class="kobospan" id="kobo.126.1">Chapter 3</span></em></span></a><span class="kobospan" id="kobo.127.1">, </span><em class="italic"><span class="kobospan" id="kobo.128.1">Simplifying Data Processing Using Snowpark</span></em><span class="kobospan" id="kobo.129.1"> where we elaborated on DataFrame operations and their</span><a id="_idIndexMarker229" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.130.1"> conversion into </span><strong class="bold"><span class="kobospan" id="kobo.131.1">user-defined functions</span></strong><span class="kobospan" id="kobo.132.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.133.1">UDFs</span></strong><span class="kobospan" id="kobo.134.1">) and stored procedures, we will unravel the intricate process of unifying these elements into a resilient data engineering pipeline. </span><span class="kobospan" id="kobo.134.2">This chapter is a testament to the synthesis of theory and practice, empowering data professionals to seamlessly interconnect the loading, transformation, and bundling phases, resulting in a robust framework for data processing and analysis within the </span><span><span class="kobospan" id="kobo.135.1">Snowpark ecosystem.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.136.1">With a comprehensive understanding of data loading from our discussions in the previous chapters, our journey now pivots toward strategically utilizing this data. </span><span class="kobospan" id="kobo.136.2">This pivotal transition places our emphasis on three </span><span><span class="kobospan" id="kobo.137.1">core steps:</span></span></p>
<ol class="calibre13">
<li class="calibre14"><span><span class="kobospan" id="kobo.138.1">Data preparation</span></span></li>
<li class="calibre14"><span><span class="kobospan" id="kobo.139.1">Data transformation</span></span></li>
<li class="calibre14"><span><span class="kobospan" id="kobo.140.1">Data cleanup</span></span></li>
</ol>
<p class="calibre3"><span class="kobospan" id="kobo.141.1">These stages constitute the cornerstone of our data engineering voyage, where we will sculpt, consolidate, and refine our data, revealing its true potential for analysis and valuable insights. </span><span class="kobospan" id="kobo.141.2">We’ll now transform these concepts into practical data engineering pipelines, leveraging the valuable insights from our prior discussions on stored procedure templates and transformation steps. </span><span class="kobospan" id="kobo.141.3">Our focus will center on our marketing campaign data, where the foundational loading steps have been thoughtfully outlined in </span><a href="B19923_03.xhtml#_idTextAnchor042" class="calibre6 pcalibre1 pcalibre"><span><em class="italic"><span class="kobospan" id="kobo.142.1">Chapter 3</span></em></span></a><span class="kobospan" id="kobo.143.1">, </span><em class="italic"><span class="kobospan" id="kobo.144.1">Simplifying Data Processing Using Snowpark</span></em><span class="kobospan" id="kobo.145.1"> providing a solid starting point for our </span><span><span class="kobospan" id="kobo.146.1">data preparation.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.147.1">Data preparation</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.148.1">In the</span><a id="_idIndexMarker230" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.149.1"> progression of our data engineering pipeline, the subsequent imperative phase is data preparation, which involves the integration of diverse tables. </span><span class="kobospan" id="kobo.149.2">In this section, we will explore techniques for merging these disparate data tables using various functions tailored to the task. </span><span class="kobospan" id="kobo.149.3">Additionally, we will elucidate the process of registering these functions as stored procedures, ensuring a streamlined and efficient data workflow. </span><span class="kobospan" id="kobo.149.4">The first step is joining the purchase history with the campaign information. </span><span class="kobospan" id="kobo.149.5">Both tables are entered using the </span><strong class="source-inline"><span class="kobospan" id="kobo.150.1">ID</span></strong><span class="kobospan" id="kobo.151.1"> column, and a single ID </span><span><span class="kobospan" id="kobo.152.1">is retained:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.153.1">
def combine_campaign_table(purchase_history,campaign_info):
    purchase_campaign = purchase_history.join(
        campaign_info, \
        purchase_history.ID == campaign_info.ID, \
        lsuffix="_left", rsuffix="_right"
    )
    purchase_campaign = purchase_campaign.drop("ID_RIGHT")
    return purchase_campaign</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.154.1">The resultant </span><strong class="source-inline"><span class="kobospan" id="kobo.155.1">purchase_campaign</span></strong><span class="kobospan" id="kobo.156.1"> DataFrame holds the data and is used in the next step. </span><span class="kobospan" id="kobo.156.2">In the next step, we join the purchase campaign with the complaint information using the same </span><strong class="source-inline"><span class="kobospan" id="kobo.157.1">ID</span></strong><span class="kobospan" id="kobo.158.1"> column and then create a </span><span><strong class="source-inline"><span class="kobospan" id="kobo.159.1">purchase_campaign_complain</span></strong></span><span><span class="kobospan" id="kobo.160.1"> DataFrame:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.161.1">
def combine_complain_table(purchase_campaign,complain_info):
    purchase_campaign_complain = purchase_campaign.join(
        complain_info, \
        purchase_campaign["ID_LEFT"] == complain_info.ID
    )
    purchase_campaign_complain = \
        purchase_campaign_complain.drop("ID_LEFT")
    return purchase_campaign_complain</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.162.1">The preceding code joins the column to create a </span><strong class="source-inline"><span class="kobospan" id="kobo.163.1">purchase_campaign_complain</span></strong><span class="kobospan" id="kobo.164.1"> DataFrame, which contains the mapped purchase data with complaint information. </span><span class="kobospan" id="kobo.164.2">In the final step, a marketing table is created by the union of the data between the purchase complaint and the </span><span><span class="kobospan" id="kobo.165.1">marketing table:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.166.1">
def union_marketing_additional_table(
    purchase_campaign_complain,marketing_additional):
    final_marketing_table = \
        purchase_campaign_complain.union_by_name(
            marketing_additional
        )
    return final_marketing_table</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.167.1">The </span><a id="_idIndexMarker231" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.168.1">preceding code produces a table that contains all the combined data that is the final result of the pipeline, which will be written as a table. </span><span class="kobospan" id="kobo.168.2">The Python functions representing each step are executed as part of the Snowpark stored procedure. </span><span class="kobospan" id="kobo.168.3">The stored procedures can be performed in sequence one after the other and also scheduled as Snowflake tasks. </span><span class="kobospan" id="kobo.168.4">The data preparation procedure calls the three Python methods, and the final table is written </span><span><span class="kobospan" id="kobo.169.1">to Snowflake:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.170.1">
from snowflake.snowpark.functions import sproc
import snowflake
def data_prep(session: Session):
    #### Loading Required Tables
    purchase_history = session.table("PURCHASE_HISTORY")
    campaign_info = session.table("CAMPAIGN_INFO")
    complain_info = session.table("COMPLAINT_INFO")
    marketing_additional = session.table("MARKETING_ADDITIONAL")</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.171.1">The</span><a id="_idIndexMarker232" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.172.1"> preceding code does the data preparation by loading the required data into the DataFrame. </span><span class="kobospan" id="kobo.172.2">We will now call each of the steps to execute it like </span><span><span class="kobospan" id="kobo.173.1">a pipeline:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.174.1">
    #### Calling Step 1
    purchase_campaign = combine_campaign_table(
        purchase_history, campaign_info)
    #### Calling Step 2
    purchase_campaign_complain = combine_campaign_table(
        purchase_campaign, complain_info)
    #### Calling Step 3
    final_marketing_data = union_marketing_additional_table(
        purchase_campaign_complain, marketing_additional)</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.175.1">The three previously defined step functions are executed. </span><span class="kobospan" id="kobo.175.2">The resultant data is loaded into the new </span><strong class="source-inline"><span class="kobospan" id="kobo.176.1">final_marketing_data</span></strong><span class="kobospan" id="kobo.177.1"> DataFrame, which will then be loaded to the </span><span><span class="kobospan" id="kobo.178.1">Snowflake table:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.179.1">
    #### Writing Combined Data To New Table
    final_marketing_data.write.save_as_table( \
        "FINAL_MARKETING_DATA")
    return "LOADED FINAL MARKETING DATA TABLE"</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.180.1">Now, we will create and execute a stored procedure that contains the preceding logic. </span><span class="kobospan" id="kobo.180.2">The procedure is called </span><strong class="source-inline"><span class="kobospan" id="kobo.181.1">data_prep_sproc</span></strong><span class="kobospan" id="kobo.182.1"> and is the first part of the data engineering pipeline – </span><span><span class="kobospan" id="kobo.183.1">data preparation:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.184.1">
# Create an instance of StoredProcedure using the sproc() function
from snowflake.snowpark.types import IntegerType,StringType
data_prep_sproc = sproc(
                        func= data_prep,\
                        replace=True,\
                        return_type = StringType(),\
                        stage_location="@my_stage",\
                        packages=["snowflake-snowpark-python"]
                        )</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.185.1">The</span><a id="_idIndexMarker233" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.186.1"> preceding stored procedure writes the data into the </span><strong class="source-inline"><span class="kobospan" id="kobo.187.1">Final_Marketing_Data</span></strong><span class="kobospan" id="kobo.188.1"> table, which will be used in the next step of </span><span><span class="kobospan" id="kobo.189.1">data transformation.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.190.1">Data transformation</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.191.1">The subsequent </span><a id="_idIndexMarker234" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.192.1">phase in this process involves data transformation, building upon the data prepared in the previous step. </span><span class="kobospan" id="kobo.192.2">Here, we’ll take the pivotal action of registering another stored procedure after the last stage. </span><span class="kobospan" id="kobo.192.3">This procedure applies transformation logic, molding the data into a form primed for analysis. </span><span class="kobospan" id="kobo.192.4">Leveraging Snowpark’s array of valuable aggregation and summarization functions, we will harness these capabilities to shape and enhance our data, laying a solid foundation for rigorous analysis. </span><span class="kobospan" id="kobo.192.5">The following code transforms </span><span><span class="kobospan" id="kobo.193.1">the data:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.194.1">
def data_transform(session: Session):
    #### Loading Required Tables
    marketing_final = session.table("FINAL_MARKETING_DATA")
    market_subset = marketing_final.select("EDUCATION", \
        "MARITAL_STATUS","INCOME")
    market_pivot = market_subset.pivot("EDUCATION", \
        ["Graduation","PhD","Master","Basic","2n Cycle"]
    ).sum("INCOME")
    #### Writing Transformed Data To New Table
    market_pivot.write.save_as_table("MARKETING_PIVOT")
    return "CREATED MARKETING PIVOT TABLE"
data_transform_sproc = sproc(
                        func= data_transform,\
                        replace=True,\
                        return_type = StringType(),\
                        stage_location="@my_stage",\
                        packages=["snowflake-snowpark-python"]
                        )</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.195.1">A </span><strong class="source-inline"><span class="kobospan" id="kobo.196.1">data_transform_sproc</span></strong><span class="kobospan" id="kobo.197.1"> stored </span><a id="_idIndexMarker235" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.198.1">procedure is created, which reads the </span><strong class="source-inline"><span class="kobospan" id="kobo.199.1">Final_Marketing_Data</span></strong><span class="kobospan" id="kobo.200.1"> table and creates a pivot with the education of the customer and the total income. </span><span class="kobospan" id="kobo.200.2">When the stored procedure is executed, this is then written to the </span><span><strong class="source-inline"><span class="kobospan" id="kobo.201.1">Marketing_Pivot</span></strong></span><span><span class="kobospan" id="kobo.202.1"> table.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.203.1">Data cleanup</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.204.1">In the final </span><a id="_idIndexMarker236" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.205.1">step of our data engineering process, we focus on a crucial task: cleaning up the data in the </span><strong class="source-inline"><span class="kobospan" id="kobo.206.1">Marketing_Pivot</span></strong><span class="kobospan" id="kobo.207.1"> table. </span><span class="kobospan" id="kobo.207.2">Similar to artists perfecting a masterpiece, we carefully go through our data, removing any empty values in tables that aren’t important for our analysis. </span><span class="kobospan" id="kobo.207.3">To do this, we rely on the versatile </span><strong class="source-inline"><span class="kobospan" id="kobo.208.1">dropna()</span></strong><span class="kobospan" id="kobo.209.1"> function, which acts like a precise tool to cut away </span><span><span class="kobospan" id="kobo.210.1">unnecessary data:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.211.1">
def data_cleanup(session: Session):
    #### Loading Required Tables
    market_pivot = session.table("MARKETING_PIVOT")
    market_drop_null = market_pivot.dropna(thresh=5)
    #### Writing Cleaned Data To New Table
    market_drop_null.write.save_as_table("MARKET_PIVOT_CLEANED")
    return "CREATED CLEANED TABLE"
data_cleanup_sproc = sproc(
                        func= data_cleanup,\
                        replace=True,\
                        return_type = StringType(),\
                        stage_location="@my_stage",\
                        packages=["snowflake-snowpark-python"]
                        )</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.212.1">The </span><a id="_idIndexMarker237" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.213.1">cleaned-up data from the </span><strong class="source-inline"><span class="kobospan" id="kobo.214.1">market_drop_null</span></strong><span class="kobospan" id="kobo.215.1"> DataFrame is then saved into the </span><strong class="source-inline"><span class="kobospan" id="kobo.216.1">Market_Pivot_Cleaned</span></strong><span class="kobospan" id="kobo.217.1"> table. </span><span class="kobospan" id="kobo.217.2">This data is at the last stage of the pipeline and is used </span><span><span class="kobospan" id="kobo.218.1">for analysis.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.219.1">Orchestrating the pipeline</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.220.1">The</span><a id="_idIndexMarker238" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.221.1"> data pipeline is orchestrated by calling three Snowpark procedures, which invokes the three different steps of the data engineering pipeline. </span><span class="kobospan" id="kobo.221.2">The procedures are executed in the order of </span><strong class="source-inline"><span class="kobospan" id="kobo.222.1">data_prep_sproc</span></strong><span class="kobospan" id="kobo.223.1">, </span><strong class="source-inline"><span class="kobospan" id="kobo.224.1">data_transform_sproc</span></strong><span class="kobospan" id="kobo.225.1">, </span><span><span class="kobospan" id="kobo.226.1">and </span></span><span><strong class="source-inline"><span class="kobospan" id="kobo.227.1">data_cleanup_sproc</span></strong></span><span><span class="kobospan" id="kobo.228.1">:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.229.1">
#### Calling Data Preparation Stored Procedure
data_prep_sproc()
#### Calling Data Transformation Stored Procedure
data_transform_sproc()
#### Calling Data Cleanup Stored Procedure
data_cleanup_sproc()</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.230.1">The</span><a id="_idIndexMarker239" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.231.1"> Snowpark procedure is executed, and after each step is executed, the final data is written to the </span><strong class="source-inline"><span class="kobospan" id="kobo.232.1">Market_Pivot_Cleaned</span></strong><span class="kobospan" id="kobo.233.1"> table. </span><span class="kobospan" id="kobo.233.2">Snowflake supports scheduling and orchestration through tasks. </span><span class="kobospan" id="kobo.233.3">Tasks can be scheduled using the Python API and through worksheets, and they can trigger procedures </span><span><span class="kobospan" id="kobo.234.1">in sequence:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer104">
<span class="kobospan" id="kobo.235.1"><img alt="Figure 4.7 – Stored procedure execution" src="image/B19923_04_07.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.236.1">Figure 4.7 – Stored procedure execution</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.237.1">In the following section, we will explore how we can utilize Snowflake tasks and task graphs to execute the </span><span><span class="kobospan" id="kobo.238.1">preceding pipeline.</span></span></p>
<h1 id="_idParaDest-67" class="calibre5"><a id="_idTextAnchor067" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.239.1">Overview of tasks in Snowflake</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.240.1">Tasks in </span><a id="_idIndexMarker240" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.241.1">Snowflake are powerful tools designed to streamline data processing workflows and automate various tasks within the Snowflake environment. </span><span class="kobospan" id="kobo.241.2">Offering </span><a id="_idIndexMarker241" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.242.1">a range of functionalities, tasks execute different types of SQL code, enabling users to perform diverse operations on </span><span><span class="kobospan" id="kobo.243.1">their data.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.244.1">Tasks in Snowflake can</span><a id="_idIndexMarker242" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.245.1"> execute three main types of </span><span><span class="kobospan" id="kobo.246.1">SQL code:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.247.1">Single SQL statement</span></strong><span class="kobospan" id="kobo.248.1">: Allows the execution of a single </span><span><span class="kobospan" id="kobo.249.1">SQL statement</span></span></li>
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.250.1">Call to a stored procedure</span></strong><span class="kobospan" id="kobo.251.1">: Enables the invocation of a </span><span><span class="kobospan" id="kobo.252.1">stored procedure</span></span></li>
<li class="calibre14"><strong class="bold"><span class="kobospan" id="kobo.253.1">Procedural logic using Snowflake Scripting</span></strong><span class="kobospan" id="kobo.254.1">: Supports the implementation of procedural logic using </span><span><span class="kobospan" id="kobo.255.1">Snowflake Scripting</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.256.1">Tasks can be integrated with table streams to create continuous ELT workflows. </span><span class="kobospan" id="kobo.256.2">By processing </span><a id="_idIndexMarker243" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.257.1">recently changed table rows, tasks ensure the maintenance of data integrity and provide exactly-once semantics for new or altered data. </span><span class="kobospan" id="kobo.257.2">Tasks in Snowflake can be scheduled to run at specified intervals. </span><span class="kobospan" id="kobo.257.3">Snowflake ensures that only one instance of a scheduled task is executed at a time, skipping scheduled executions if a task is </span><span><span class="kobospan" id="kobo.258.1">still running.</span></span></p>
<h2 id="_idParaDest-68" class="calibre7"><a id="_idTextAnchor068" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.259.1">Compute models for tasks</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.260.1">In the </span><a id="_idIndexMarker244" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.261.1">serverless compute model, tasks rely on compute resources managed by Snowflake. </span><span class="kobospan" id="kobo.261.2">These resources are automatically resized and scaled based on workload demands, ensuring optimal performance and resource utilization. </span><span class="kobospan" id="kobo.261.3">Snowflake dynamically determines the appropriate compute size for each task run based on </span><span><span class="kobospan" id="kobo.262.1">historical statistics.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.263.1">Alternatively, users can opt for the user-managed virtual warehouse model, where they specify an existing virtual warehouse for individual tasks. </span><span class="kobospan" id="kobo.263.2">This model provides users with more control over compute resource management but requires careful sizing to ensure efficient </span><span><span class="kobospan" id="kobo.264.1">task execution.</span></span></p>
<h2 id="_idParaDest-69" class="calibre7"><a id="_idTextAnchor069" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.265.1">Task graphs</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.266.1">Task graphs, also</span><a id="_idIndexMarker245" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.267.1"> known as </span><strong class="bold"><span class="kobospan" id="kobo.268.1">directed acyclic graphs</span></strong><span class="kobospan" id="kobo.269.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.270.1">DAGs</span></strong><span class="kobospan" id="kobo.271.1">), allow </span><a id="_idIndexMarker246" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.272.1">for the organization of tasks based on dependencies. </span><span class="kobospan" id="kobo.272.2">Each task within a task graph has predecessor and subsequent tasks, facilitating complex </span><span><span class="kobospan" id="kobo.273.1">workflow management.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.274.1">Task graphs are subject to certain limitations, including a maximum of 1,000 tasks in total, including the root task. </span><span class="kobospan" id="kobo.274.2">Individual tasks within a task graph can have a maximum of 100 predecessors and 100 </span><span><span class="kobospan" id="kobo.275.1">child tasks.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.276.1">Users can view and monitor their task graphs using SQL or Snowsight, Snowflake’s integrated development environment, providing visibility into task dependencies and </span><span><span class="kobospan" id="kobo.277.1">execution status.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.278.1">In summary, tasks in Snowflake offer robust capabilities for data processing, automation, and workflow management, making them indispensable tools for users seeking to optimize their data operations within the </span><span><span class="kobospan" id="kobo.279.1">Snowflake ecosystem.</span></span></p>
<h2 id="_idParaDest-70" class="calibre7"><a id="_idTextAnchor070" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.280.1">Managing tasks and task graphs with Python</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.281.1">Our</span><a id="_idIndexMarker247" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.282.1"> primary focus is on Snowpark. </span><span class="kobospan" id="kobo.282.2">Now, we’ll explore how we can utilize Python Snowpark to programmatically perform task graph operations instead of using </span><span><span class="kobospan" id="kobo.283.1">SQL statements.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.284.1">Now, Python can manage Snowflake tasks, allowing users to run SQL statements, procedure calls, and Snowflake Scripting logic. </span><span class="kobospan" id="kobo.284.2">The Snowflake Python API introduces </span><span><span class="kobospan" id="kobo.285.1">two types:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><strong class="source-inline1"><span class="kobospan" id="kobo.286.1">Task</span></strong><span class="kobospan" id="kobo.287.1">: This type represents a task’s properties, such as its schedule, parameters, </span><span><span class="kobospan" id="kobo.288.1">and dependencies</span></span></li>
<li class="calibre14"><strong class="source-inline1"><span class="kobospan" id="kobo.289.1">TaskResource</span></strong><span class="kobospan" id="kobo.290.1">: This type provides methods to interact with </span><strong class="source-inline1"><span class="kobospan" id="kobo.291.1">Task</span></strong><span class="kobospan" id="kobo.292.1"> objects, enabling task execution </span><span><span class="kobospan" id="kobo.293.1">and modification</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.294.1">Tasks can be grouped into task graphs, which consist of interconnected tasks arranged based on their dependencies. </span><span class="kobospan" id="kobo.294.2">To create a task graph, users first define a DAG object, specifying its name and optional properties, such as its schedule. </span><span class="kobospan" id="kobo.294.3">The scheduling of a task graph can be customized using either a </span><strong class="source-inline"><span class="kobospan" id="kobo.295.1">timedelta</span></strong><span class="kobospan" id="kobo.296.1"> value or a cron expression, allowing for flexible task execution timing and </span><span><span class="kobospan" id="kobo.297.1">recurrence patterns.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.298.1">Let’s begin by setting up the necessary functions to implement our DAG. </span><span class="kobospan" id="kobo.298.2">The examples provided in this section presuppose that you’ve already written code to establish a connection with Snowflake to utilize the Snowflake </span><span><span class="kobospan" id="kobo.299.1">Python API:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.300.1">
from snowflake.core import Root
from snowflake.core.task import StoredProcedureCall
from snowflake.core.task.dagv1 import DAG, DAGTask, DAGOperation
from snowflake.snowpark import Session
from datetime import timedelta
root = Root(session)</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.301.1">The preceding code initializes the Snowflake Python API, creating a </span><strong class="source-inline"><span class="kobospan" id="kobo.302.1">root</span></strong><span class="kobospan" id="kobo.303.1"> object for utilizing its types and methods. </span><span class="kobospan" id="kobo.303.2">Additionally, it sets up a </span><strong class="source-inline"><span class="kobospan" id="kobo.304.1">timedelta</span></strong><span class="kobospan" id="kobo.305.1"> value of 1 hour for the task’s schedule. </span><span class="kobospan" id="kobo.305.2">You can define the schedule using either a </span><strong class="source-inline"><span class="kobospan" id="kobo.306.1">timedelta</span></strong><span class="kobospan" id="kobo.307.1"> value or a Cron expression. </span><span class="kobospan" id="kobo.307.2">For one-off runs, you can omit the schedule argument to the DAG object without worrying about it running unnecessarily in </span><span><span class="kobospan" id="kobo.308.1">the background.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.309.1">Let’s</span><a id="_idIndexMarker248" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.310.1"> define a simple DAG that we’ll use to execute </span><span><span class="kobospan" id="kobo.311.1">our pipelines:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.312.1">
dag = DAG("Task_Demo",
          warehouse="COMPUTE_WH",
          schedule=timedelta(days=1),
          stage_location= \
              "SNOWPARK_DEFINITIVE_GUIDE.MY_SCHEMA.MY_STAGE",
          packages=["snowflake-snowpark-python"]
          )</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.313.1">In this DAG setup, the </span><span><span class="kobospan" id="kobo.314.1">following applies:</span></span></p>
<ul class="calibre15">
<li class="calibre14"><span class="kobospan" id="kobo.315.1">We’ve named our DAG </span><strong class="source-inline1"><span class="kobospan" id="kobo.316.1">Task_Demo</span></strong><span class="kobospan" id="kobo.317.1">, which by default runs on the </span><span><span class="kobospan" id="kobo.318.1">specified warehouse.</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.319.1">A schedule for daily execution has been defined </span><span><span class="kobospan" id="kobo.320.1">using </span></span><span><strong class="source-inline1"><span class="kobospan" id="kobo.321.1">timedelta</span></strong></span><span><span class="kobospan" id="kobo.322.1">.</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.323.1">A </span><strong class="source-inline1"><span class="kobospan" id="kobo.324.1">stage_location</span></strong><span class="kobospan" id="kobo.325.1"> attribute is necessary for storing the serialized version of the tasks via the </span><span><span class="kobospan" id="kobo.326.1">Python API.</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.327.1">All tasks under this DAG will run with the default list of packages and the specified warehouse. </span><span class="kobospan" id="kobo.327.2">However, both the warehouse and packages for individual tasks within the DAG can be overridden with </span><span><span class="kobospan" id="kobo.328.1">different values.</span></span></li>
<li class="calibre14"><span class="kobospan" id="kobo.329.1">In addition, the </span><strong class="source-inline1"><span class="kobospan" id="kobo.330.1">use_func_return_value</span></strong><span class="kobospan" id="kobo.331.1"> attribute indicates that the return value of Python functions will be treated as the return value of the task, though in our case, we’re not utilizing the </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.332.1">return</span></strong></span><span><span class="kobospan" id="kobo.333.1"> object.</span></span></li>
</ul>
<p class="calibre3"><span class="kobospan" id="kobo.334.1">We’ve now defined a series of Python functions representing a three-task pipeline, or DAG. </span><span class="kobospan" id="kobo.334.2">However, we haven’t yet created and pushed the DAG to Snowflake. </span><span class="kobospan" id="kobo.334.3">Let’s do that now using the Snowflake </span><span><span class="kobospan" id="kobo.335.1">Python API:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.336.1">
with dag:
    data_prep_task = DAGTask("Data_Prep", definition=data_prep)
    data_transform_task = DAGTask("Data_Transform", \
        definition=data_transform)
    data_cleanup_task = DAGTask("Data_Cleanup", \
        definition=data_cleanup)
    data_prep_task &gt;&gt; data_transform_task &gt;&gt; data_cleanup_task</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.337.1">In this</span><a id="_idIndexMarker249" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.338.1"> code snippet, we’ve instantiated </span><strong class="source-inline"><span class="kobospan" id="kobo.339.1">DAGTask</span></strong><span class="kobospan" id="kobo.340.1"> objects for each task in our pipeline: </span><strong class="source-inline"><span class="kobospan" id="kobo.341.1">data_prep</span></strong><span class="kobospan" id="kobo.342.1">, </span><strong class="source-inline"><span class="kobospan" id="kobo.343.1">data_transform</span></strong><span class="kobospan" id="kobo.344.1">, and </span><strong class="source-inline"><span class="kobospan" id="kobo.345.1">data_cleanup</span></strong><span class="kobospan" id="kobo.346.1">. </span><span class="kobospan" id="kobo.346.2">These tasks are then linked together using the </span><strong class="source-inline"><span class="kobospan" id="kobo.347.1">&gt;&gt;</span></strong><span class="kobospan" id="kobo.348.1"> operator to specify their </span><span><span class="kobospan" id="kobo.349.1">execution order.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.350.1">For one-off testing or running of the DAG, users can skip specifying the schedule and manually trigger a run </span><span><span class="kobospan" id="kobo.351.1">with </span></span><span><strong class="source-inline"><span class="kobospan" id="kobo.352.1">dag_op.run(dag)</span></strong></span><span><span class="kobospan" id="kobo.353.1">:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.354.1">
schema = root.databases["SNOWPARK_DEFINITIVE_GUIDE"].schemas[ \
    "MY_SCHEMA"]
dag_op = DAGOperation(schema)
dag_op.deploy(dag,mode="orReplace")
dag_op.run(dag)</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.355.1">The provided code performs several actions using Snowflake’s Snowpark library. </span><span class="kobospan" id="kobo.355.2">Firstly, it retrieves the schema named </span><strong class="source-inline"><span class="kobospan" id="kobo.356.1">MY_SCHEMA</span></strong><span class="kobospan" id="kobo.357.1"> from the </span><strong class="source-inline"><span class="kobospan" id="kobo.358.1">SNOWPARK_DEFINITIVE_GUIDE</span></strong><span class="kobospan" id="kobo.359.1"> database using the </span><strong class="source-inline"><span class="kobospan" id="kobo.360.1">root</span></strong><span class="kobospan" id="kobo.361.1"> object. </span><span class="kobospan" id="kobo.361.2">Then, it initializes a </span><strong class="source-inline"><span class="kobospan" id="kobo.362.1">DAGOperation</span></strong><span class="kobospan" id="kobo.363.1"> object named </span><strong class="source-inline"><span class="kobospan" id="kobo.364.1">dag_op</span></strong><span class="kobospan" id="kobo.365.1">, specifying the schema where the DAG will be deployed. </span><span class="kobospan" id="kobo.365.2">The </span><strong class="source-inline"><span class="kobospan" id="kobo.366.1">deploy()</span></strong><span class="kobospan" id="kobo.367.1"> method is then called on </span><strong class="source-inline"><span class="kobospan" id="kobo.368.1">dag_op</span></strong><span class="kobospan" id="kobo.369.1"> to deploy the specified DAG (named </span><strong class="source-inline"><span class="kobospan" id="kobo.370.1">dag</span></strong><span class="kobospan" id="kobo.371.1">) in the </span><span><span class="kobospan" id="kobo.372.1">specified schema.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.373.1">The </span><strong class="source-inline"><span class="kobospan" id="kobo.374.1">orReplace</span></strong><span class="kobospan" id="kobo.375.1"> mode argument indicates that if a DAG with the same name already exists in the schema, it will be replaced. </span><span class="kobospan" id="kobo.375.2">Finally, the </span><strong class="source-inline"><span class="kobospan" id="kobo.376.1">run()</span></strong><span class="kobospan" id="kobo.377.1"> method is called on </span><strong class="source-inline"><span class="kobospan" id="kobo.378.1">dag_op</span></strong><span class="kobospan" id="kobo.379.1"> to execute the deployed DAG. </span><span class="kobospan" id="kobo.379.2">This code essentially sets up and executes a DAG within Snowflake using Snowpark. </span><span class="kobospan" id="kobo.379.3">Now, you can check the graph in Snowsight to see how the graph has been </span><span><span class="kobospan" id="kobo.380.1">set up:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer105">
<span class="kobospan" id="kobo.381.1"><img alt="Figure 4.8 – Snowsight graph" src="image/B19923_04_08.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.382.1">Figure 4.8 – Snowsight graph</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.383.1">Additionally, note </span><a id="_idIndexMarker250" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.384.1">that you can pass the raw function to the dependency definition without explicitly creating a </span><strong class="source-inline"><span class="kobospan" id="kobo.385.1">DAGTask</span></strong><span class="kobospan" id="kobo.386.1"> instance, with the library automatically creating a task for you with the same name. </span><span class="kobospan" id="kobo.386.2">However, there are exceptions to this rule, such as needing to explicitly create a </span><strong class="source-inline"><span class="kobospan" id="kobo.387.1">DAGTask</span></strong><span class="kobospan" id="kobo.388.1"> instance for the first task or when utilizing </span><strong class="source-inline"><span class="kobospan" id="kobo.389.1">DAGTaskBranch</span></strong><span class="kobospan" id="kobo.390.1"> or repeating certain functions in </span><span><span class="kobospan" id="kobo.391.1">multiple tasks.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.392.1">Once you’ve deployed and run the DAG, you can easily check its status using the </span><span><span class="kobospan" id="kobo.393.1">following code:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.394.1">
current_runs = dag_op.get_current_dag_runs(dag)
for r in current_runs:
    print(f"RunId={r.run_id} State={r.state}")</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.395.1">As depicted in the screenshot, our tasks are scheduled to run daily. </span><span class="kobospan" id="kobo.395.2">Additionally, we have executed them once to verify the task </span><span><span class="kobospan" id="kobo.396.1">deployment beforehand:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer106">
<span class="kobospan" id="kobo.397.1"><img alt="Figure 4.9 – Tasks deployed" src="image/B19923_04_09.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.398.1">Figure 4.9 – Tasks deployed</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.399.1">As we progress in constructing more complex pipelines, managing and debugging become </span><a id="_idIndexMarker251" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.400.1">increasingly challenging. </span><span class="kobospan" id="kobo.400.2">It’s crucial to establish a robust logging mechanism to facilitate maintenance and streamline error resolution. </span><span class="kobospan" id="kobo.400.3">In the next section, we’ll delve into implementing logging and traceback functionalities in Snowpark to enhance our pipeline’s manageability and </span><span><span class="kobospan" id="kobo.401.1">ease troubleshooting.</span></span></p>
<h1 id="_idParaDest-71" class="calibre5"><a id="_idTextAnchor071" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.402.1">Implementing logging and tracing in Snowpark</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.403.1">Logging and </span><a id="_idIndexMarker252" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.404.1">tracing are crucial for DataOps and are necessary to monitor and fix failures in the data engineering pipeline. </span><span class="kobospan" id="kobo.404.2">Snowpark comes with logging and tracing functionality that is built in, which can help record the activity of Snowpark functions and procedures and capture those in an easy-to-access central table inside Snowflake. </span><span class="kobospan" id="kobo.404.3">Log </span><a id="_idIndexMarker253" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.405.1">messages are independent, detailed messages with information in the form of strings, providing details about the piece of code, and trace events are structured data that we can use to get information spanning and grouping multiple parts of our code. </span><span class="kobospan" id="kobo.405.2">Once logs are collected, they can be easily queried by SQL or accessed via Snowpark. </span><span class="kobospan" id="kobo.405.3">The following diagram highlights the event table </span><span><span class="kobospan" id="kobo.406.1">and alerting:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer107">
<span class="kobospan" id="kobo.407.1"><img alt="Figure 4.10 – Event table" src="image/B19923_04_10.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.408.1">Figure 4.10 – Event table</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.409.1">Snowpark stores logs and trace messages inside the event table, a unique table with a predefined set of columns. </span><span class="kobospan" id="kobo.409.2">Logs and traces are captured in this table as the code is executed. </span><span class="kobospan" id="kobo.409.3">Let’s look at the structure of event tables and how to </span><span><span class="kobospan" id="kobo.410.1">create them.</span></span></p>
<h2 id="_idParaDest-72" class="calibre7"><a id="_idTextAnchor072" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.411.1">Event tables</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.412.1">An event table</span><a id="_idIndexMarker254" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.413.1"> is native to Snowflake and needs to be created. </span><span class="kobospan" id="kobo.413.2">There can be only one event table for a Snowflake account that captures all the information, but multiple views can be made for analysis. </span><span class="kobospan" id="kobo.413.3">An event table contains the </span><span><span class="kobospan" id="kobo.414.1">following columns:</span></span></p>
<table class="no-table-style" id="table001-2">
<colgroup class="calibre10">
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
</colgroup>
<tbody class="calibre12">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><strong class="bold"><span class="kobospan" id="kobo.415.1">Column</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="bold"><span class="kobospan" id="kobo.416.1">Data Type</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="bold"><span class="kobospan" id="kobo.417.1">Description</span></strong></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.418.1">TIMESTAMP</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.419.1">TIMESTAMP_NTZ</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.420.1">The UTC timestamp when an event was created. </span><span class="kobospan" id="kobo.420.2">This is the end of the period for events representing </span><span><span class="kobospan" id="kobo.421.1">a period.</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.422.1">START_TIMESTAMP</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.423.1">TIMESTAMP_NTZ</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.424.1">For events representing a period, such as trace events as the start of the period as a </span><span><span class="kobospan" id="kobo.425.1">UTC timestamp.</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.426.1">OBSERVED_TIMESTAMP</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.427.1">TIMESTAMP_NTZ</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.428.1">A UTC timestamp is used for logs. </span><span class="kobospan" id="kobo.428.2">Currently, it has the same value </span><span><span class="kobospan" id="kobo.429.1">as </span></span><span><strong class="source-inline"><span class="kobospan" id="kobo.430.1">TIMESTAMP</span></strong></span><span><span class="kobospan" id="kobo.431.1">.</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.432.1">TRACE</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.433.1">OBJECT</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.434.1">Tracing context for all signal types. </span><span class="kobospan" id="kobo.434.2">Contains </span><strong class="source-inline"><span class="kobospan" id="kobo.435.1">trace_id</span></strong><span class="kobospan" id="kobo.436.1"> and </span><strong class="source-inline"><span class="kobospan" id="kobo.437.1">span_id</span></strong> <span><span class="kobospan" id="kobo.438.1">string values.</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.439.1">RESOURCE</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.440.1">OBJECT</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.441.1">Reserved for </span><span><span class="kobospan" id="kobo.442.1">future use.</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.443.1">RESOURCE_ATTRIBUTES</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.444.1">OBJECT</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.445.1">Attributes that identify the source of an event, such as database, schema, user, warehouse, and </span><span><span class="kobospan" id="kobo.446.1">so on.</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.447.1">SCOPE</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.448.1">OBJECT</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.449.1">Scopes for events; for example, class names </span><span><span class="kobospan" id="kobo.450.1">for logs.</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.451.1">SCOPE_ATTRIBUTES</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.452.1">OBJECT</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.453.1">Reserved for </span><span><span class="kobospan" id="kobo.454.1">future use.</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.455.1">RECORD_TYPE</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.456.1">STRING</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.457.1">The event types. </span><span class="kobospan" id="kobo.457.2">One of </span><span><span class="kobospan" id="kobo.458.1">the following:</span></span></p>
<p class="calibre3"><strong class="source-inline"><span class="kobospan" id="kobo.459.1">LOG</span></strong><span class="kobospan" id="kobo.460.1"> for a </span><span><span class="kobospan" id="kobo.461.1">log message.</span></span></p>
<p class="calibre3"><strong class="source-inline"><span class="kobospan" id="kobo.462.1">SPAN</span></strong><span class="kobospan" id="kobo.463.1"> for UDF invocations performed sequentially on the same thread. </span><strong class="source-inline"><span class="kobospan" id="kobo.464.1">SPAN_EVENT</span></strong><span class="kobospan" id="kobo.465.1"> for a single trace event. </span><span class="kobospan" id="kobo.465.2">A single query can emit more than one </span><strong class="source-inline"><span class="kobospan" id="kobo.466.1">SPAN_EVENT</span></strong> <span><span class="kobospan" id="kobo.467.1">event type.</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.468.1">RECORD</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.469.1">OBJECT</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.470.1">Fixed values for each </span><span><span class="kobospan" id="kobo.471.1">record type.</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.472.1">RECORD_ATTRIBUTES</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.473.1">OBJECT</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.474.1">Variable attributes for each </span><span><span class="kobospan" id="kobo.475.1">record type.</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.476.1">VALUE</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.477.1">VARIANT</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.478.1">Primary </span><span><span class="kobospan" id="kobo.479.1">event value.</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.480.1">EXEMPLARS</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.481.1">ARRAY</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.482.1">Reserved for </span><span><span class="kobospan" id="kobo.483.1">future use.</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.484.1">Table 4.1 – Event table columns</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.485.1">Each column can </span><a id="_idIndexMarker255" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.486.1">be queried or combined to analyze different outcomes based on the logs and the traces. </span><span class="kobospan" id="kobo.486.2">The log type describes the log levels assigned as part of the logging and can be set at both objects and the session. </span><span class="kobospan" id="kobo.486.3">The log levels can be </span><span><span class="kobospan" id="kobo.487.1">the following:</span></span></p>
<table class="no-table-style" id="table002">
<colgroup class="calibre10">
<col class="calibre11"/>
<col class="calibre11"/>
</colgroup>
<tbody class="calibre12">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><strong class="bold"><span class="kobospan" id="kobo.488.1">LOG_LEVEL </span></strong><span><strong class="bold"><span class="kobospan" id="kobo.489.1">Parameter Setting</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><strong class="bold"><span class="kobospan" id="kobo.490.1">Levels of </span></strong><span><strong class="bold"><span class="kobospan" id="kobo.491.1">Messages Ingested</span></strong></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.492.1">TRACE</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.493.1">TRACE</span></strong></span></p>
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.494.1">DEBUG</span></strong></span></p>
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.495.1">INFO</span></strong></span></p>
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.496.1">WARN</span></strong></span></p>
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.497.1">ERROR</span></strong></span></p>
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.498.1">FATAL</span></strong></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.499.1">DEBUG</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.500.1">DEBUG</span></strong></span></p>
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.501.1">INFO</span></strong></span></p>
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.502.1">WARN</span></strong></span></p>
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.503.1">ERROR</span></strong></span></p>
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.504.1">FATAL</span></strong></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.505.1">INFO</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.506.1">INFO</span></strong></span></p>
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.507.1">WARN</span></strong></span></p>
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.508.1">ERROR</span></strong></span></p>
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.509.1">FATAL</span></strong></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.510.1">WARN</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.511.1">WARN</span></strong></span></p>
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.512.1">ERROR</span></strong></span></p>
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.513.1">FATAL</span></strong></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.514.1">ERROR</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.515.1">ERROR</span></strong></span></p>
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.516.1">FATAL</span></strong></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.517.1">FATAL</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.518.1">ERROR</span></strong></span></p>
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.519.1">FATAL</span></strong></span></p>
</td>
</tr>
</tbody>
</table>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.520.1">Table 4.2 – Event table log levels</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.521.1">The log level </span><a id="_idIndexMarker256" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.522.1">is a hierarchy applied in the order presented in </span><em class="italic"><span class="kobospan" id="kobo.523.1">Table 4.2</span></em><span class="kobospan" id="kobo.524.1">. </span><span class="kobospan" id="kobo.524.2">In the next section, we will look at creating an </span><span><span class="kobospan" id="kobo.525.1">event table.</span></span></p>
<p class="callout-heading"><span class="kobospan" id="kobo.526.1">Note</span></p>
<p class="callout"><span class="kobospan" id="kobo.527.1">It is best practice to set the necessary log level based on a minor level such as </span><strong class="source-inline1"><span class="kobospan" id="kobo.528.1">FATAL </span></strong><span class="kobospan" id="kobo.529.1">and ERROR so that the number of logged messages is fewer. </span><span class="kobospan" id="kobo.529.2">In the case of logging </span><strong class="source-inline1"><span class="kobospan" id="kobo.530.1">INFO</span></strong><span class="kobospan" id="kobo.531.1">, it is usually turned on to capture the log and turned off in production to avoid catching too </span><span><span class="kobospan" id="kobo.532.1">many records.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.533.1">Creating and configuring an event table</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.534.1">The first step </span><a id="_idIndexMarker257" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.535.1">is to create an event table for the Snowflake account. </span><span class="kobospan" id="kobo.535.2">The name of the event table can be specified, and columns for the event table are not required to be set when creating the table, as Snowflake automatically creates it with the standard columns. </span><span class="kobospan" id="kobo.535.3">The event table is assigned to the account and needs to be made in a separate database that does not have Snowflake replication enabled using the </span><span><strong class="source-inline"><span class="kobospan" id="kobo.536.1">ACCOUNTADMIN</span></strong></span><span><span class="kobospan" id="kobo.537.1"> role.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.538.1">To create an event table, run the following command with the </span><strong class="source-inline"><span class="kobospan" id="kobo.539.1">ACCOUNTADMIN</span></strong><span class="kobospan" id="kobo.540.1"> role </span><span><span class="kobospan" id="kobo.541.1">using Snowpark:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.542.1">
session.sql('''CREATE EVENT TABLE MY_EVENTS;''').show()</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.543.1">An event table called </span><strong class="source-inline"><span class="kobospan" id="kobo.544.1">MY_EVENTS</span></strong><span class="kobospan" id="kobo.545.1"> is created with the default column structure. </span><span class="kobospan" id="kobo.545.2">The next step is to assign the event table as the active event table to a particular Snowflake account. </span><span class="kobospan" id="kobo.545.3">The </span><a id="_idIndexMarker258" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.546.1">event table can be assigned to an account by executing the following code with the </span><span><strong class="source-inline"><span class="kobospan" id="kobo.547.1">ACCOUNTADMIN</span></strong></span><span><span class="kobospan" id="kobo.548.1"> role:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.549.1">
session.sql('''ALTER ACCOUNT SET EVENT_TABLE = \
    SNOWPARK_DEFINITIVE_GUIDE.MY_SCHEMA.MY_EVENTS;
''').show()</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.550.1">The parameter is applied at the account level, and all events from the particular Snowflake account are captured in this event table. </span><span class="kobospan" id="kobo.550.2">This completes the event </span><span><span class="kobospan" id="kobo.551.1">table setup.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.552.1">Querying event tables</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.553.1">An</span><a id="_idIndexMarker259" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.554.1"> event table can be accessed just like any other Snowflake table. </span><span class="kobospan" id="kobo.554.2">To get the records from an event table, you can query the event table with the </span><span><span class="kobospan" id="kobo.555.1">following code:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.556.1">
session.sql('''SELECT *
    FROM SNOWPARK_DEFINITIVE_GUIDE.MY_SCHEMA.MY_EVENTS;
''').show()</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.557.1">It returns a result with empty records since no information was captured. </span><span class="kobospan" id="kobo.557.2">Records in an event table can be filtered with a specific column to get detailed information. </span><span class="kobospan" id="kobo.557.3">A Snowflake stream can be set on top of the event table to capture only new events. </span><span class="kobospan" id="kobo.557.4">A Stream can be created by running the </span><span><span class="kobospan" id="kobo.558.1">following query:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.559.1">
session.sql('''CREATE STREAM EVENT_APPEND ON EVENT TABLE MY_EVENTS APPEND_ONLY=TRUE;''').show()</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.560.1">The </span><strong class="source-inline"><span class="kobospan" id="kobo.561.1">EVENT_APPEND</span></strong><span class="kobospan" id="kobo.562.1"> stream captures the latest inserted records into the event table. </span><span class="kobospan" id="kobo.562.2">In the next section, we will set up logging and tracing to capture records in an </span><span><span class="kobospan" id="kobo.563.1">event table.</span></span></p>
<h2 id="_idParaDest-73" class="calibre7"><a id="_idTextAnchor073" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.564.1">Setting up logging in Snowpark</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.565.1">Introducing logging </span><a id="_idIndexMarker260" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.566.1">and tracing capabilities into our pipelines</span><a id="_idIndexMarker261" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.567.1"> is akin to infusing resilience into our standard data engineering processes. </span><span class="kobospan" id="kobo.567.2">This section will delve into the integration of logging functionalities into our existing data engineering pipeline. </span><span class="kobospan" id="kobo.567.3">By doing so, we not only gain the ability to monitor and trace the flow of data but also enhance the robustness of our code, fortifying</span><a id="_idIndexMarker262" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.568.1"> it against potential pitfalls. </span><span class="kobospan" id="kobo.568.2">Join us as we explore how these logging capabilities elevate our data engineering </span><a id="_idIndexMarker263" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.569.1">practices, making them more reliable </span><span><span class="kobospan" id="kobo.570.1">and fault-tolerant.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.571.1">Logging can be enabled for both Snowpark functions and procedures. </span><span class="kobospan" id="kobo.571.2">The first step in capturing logs is to set the log level in the Snowpark session. </span><span class="kobospan" id="kobo.571.3">The log level in the session can be set by running the </span><span><span class="kobospan" id="kobo.572.1">following code:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.573.1">
session.sql('''alter session set log_level = INFO;''').show()</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.574.1">This sets the log level to </span><strong class="source-inline"><span class="kobospan" id="kobo.575.1">INFO</span></strong><span class="kobospan" id="kobo.576.1"> for the particular session, so all Snowpark execution that happens for the specific session is captured with the log level as information. </span><span class="kobospan" id="kobo.576.2">Snowpark supports APIs to log messages directly from </span><span><span class="kobospan" id="kobo.577.1">the handler.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.578.1">Capturing informational logs</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.579.1">We </span><a id="_idIndexMarker264" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.580.1">will now modify the data preparation procedures from the data pipeline to capture informational logs. </span><span class="kobospan" id="kobo.580.2">We start with prepping </span><span><span class="kobospan" id="kobo.581.1">the data:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.582.1">
from snowflake.snowpark.functions import sproc
import logging
def data_prep(session: Session):
    ## Initializing Logger
    logger = logging.getLogger("My_Logger")
    logger.info("Data Preparation Pipeline Starts")
    #### Loading Required Tables
    logger.info("Loading Required Tables")
    purchase_history = session.table("PURCHASE_HISTORY")
    campaign_info = session.table("CAMPAIGN_INFO")
    complain_info = session.table("COMPLAINT_INFO")
    marketing_additional = session.table("MARKETING_ADDITIONAL")</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.583.1">The </span><a id="_idIndexMarker265" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.584.1">data from the four tables is loaded into the DataFrame. </span><span class="kobospan" id="kobo.584.2">This DataFrame is then used to execute each step. </span><span class="kobospan" id="kobo.584.3">Next, we will proceed with calling each step in order to process </span><span><span class="kobospan" id="kobo.585.1">the data:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.586.1">
    #### Calling Step 1
    purchase_campaign = combine_campaign_table(
        purchase_history,campaign_info)
    logger.info("Joined Purchase and Campaign Tables")
    #### Calling Step 2
    purchase_campaign_complain = combine_complain_table(
        purchase_campaign,complain_info)
    logger.info("Joined Complain Table")
    #### Calling Step 3
    final_marketing_data = union_marketing_additional_table(
        purchase_campaign_complain,marketing_additional)
    logger.info("Final Marketing Data Created")</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.587.1">Once all three steps are executed, we get the final marketing data ready to be loaded into a</span><a id="_idIndexMarker266" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.588.1"> Snowflake table for consumption. </span><span class="kobospan" id="kobo.588.2">The following code will load the data into a </span><span><span class="kobospan" id="kobo.589.1">Snowflake table:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.590.1">
    #### Writing Combined Data To New Table
    final_marketing_data.write.save_as_table( \
        "FINAL_MARKETING_DATA")
    logger.info("Final Marketing Data Table Created")
    return "LOADED FINAL MARKETING DATA TABLE"</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.591.1">The data is loaded into a table called </span><strong class="source-inline"><span class="kobospan" id="kobo.592.1">FINAL_MARKETING_DATA</span></strong><span class="kobospan" id="kobo.593.1">. </span><span class="kobospan" id="kobo.593.2">The table is automatically created with the data in the Snowpark DataFrame. </span><span class="kobospan" id="kobo.593.3">We will now register this as a Snowpark </span><span><span class="kobospan" id="kobo.594.1">stored procedure:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.595.1">
## Register Stored Procedure in Snowflake
### Add packages and data types
from snowflake.snowpark.types import StringType
session.add_packages('snowflake-snowpark-python')
### Upload Stored Procedure to Snowflake
session.sproc.register(
    func = data_prep
  , return_type = StringType()
  , input_types = []
  , is_permanent = True
  , name = 'DATA_PREP_SPROC_LOG'
  , replace = True
  , stage_location = '@MY_STAGE'
)</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.596.1">The </span><strong class="source-inline"><span class="kobospan" id="kobo.597.1">logging</span></strong><span class="kobospan" id="kobo.598.1"> module from Python’s standard library is used for logging. </span><span class="kobospan" id="kobo.598.2">The package is imported, and the logger’s name is specified as </span><strong class="source-inline"><span class="kobospan" id="kobo.599.1">My_Logger</span></strong><span class="kobospan" id="kobo.600.1">. </span><span class="kobospan" id="kobo.600.2">Different logger names can be set for</span><a id="_idIndexMarker267" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.601.1"> processes to help identify the particular logging application. </span><span class="kobospan" id="kobo.601.2">We can execute the stored procedure to capture logs to the event table. </span><span class="kobospan" id="kobo.601.3">The stored procedure can be executed by running the </span><span><span class="kobospan" id="kobo.602.1">following command:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.603.1">
session.sql(''' Call DATA_PREP_SPROC_LOG()''').show()</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.604.1">The stored procedure is executed, and we can see the successful output of </span><span><span class="kobospan" id="kobo.605.1">the execution:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer108">
<span class="kobospan" id="kobo.606.1"><img alt="Figure 4.11 – Stored procedure execution" src="image/B19923_04_11.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.607.1">Figure 4.11 – Stored procedure execution</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.608.1">The following section will cover how to query logs generated by the </span><span><span class="kobospan" id="kobo.609.1">stored procedure.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.610.1">Querying informational logs</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.611.1">Logs </span><a id="_idIndexMarker268" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.612.1">generated by the stored procedure can be accessed from the event table. </span><span class="kobospan" id="kobo.612.2">Records captured from the previous stored procedure can be accessed by running the </span><span><span class="kobospan" id="kobo.613.1">following query:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.614.1">
session.sql("""
    SELECT RECORD['severity_text'] AS SEVERITY,
        VALUE AS MESSAGE
    FROM MY_EVENTS
    WHERE SCOPE['name'] = 'My_Logger'
    AND RECORD_TYPE = 'LOG'
""").show()</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.615.1">We filter logs captured only from </span><strong class="source-inline"><span class="kobospan" id="kobo.616.1">My_Logger</span></strong><span class="kobospan" id="kobo.617.1"> by specifying it in the scope. </span><span class="kobospan" id="kobo.617.2">The query returns the following records that were generated from executing the </span><span><span class="kobospan" id="kobo.618.1">stored procedure:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer109">
<span class="kobospan" id="kobo.619.1"><img alt="Figure 4.12 – Querying logs" src="image/B19923_04_12.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.620.1">Figure 4.12 – Querying logs</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.621.1">In the</span><a id="_idIndexMarker269" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.622.1"> next section, we will set up logging to capture error messages and handle exceptions </span><span><span class="kobospan" id="kobo.623.1">in Snowpark.</span></span></p>
<h2 id="_idParaDest-74" class="calibre7"><a id="_idTextAnchor074" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.624.1">Handling exceptions in Snowpark</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.625.1">Exception handling</span><a id="_idIndexMarker270" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.626.1"> is integral to data pipelines as it helps identify and handle issues. </span><span class="kobospan" id="kobo.626.2">Exception handling can be done by catching exceptions</span><a id="_idIndexMarker271" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.627.1"> thrown from within the </span><strong class="source-inline"><span class="kobospan" id="kobo.628.1">try</span></strong><span class="kobospan" id="kobo.629.1"> block and capturing those error logs. </span><span class="kobospan" id="kobo.629.2">The </span><strong class="source-inline"><span class="kobospan" id="kobo.630.1">ERROR</span></strong><span class="kobospan" id="kobo.631.1"> and </span><strong class="source-inline"><span class="kobospan" id="kobo.632.1">WARN</span></strong><span class="kobospan" id="kobo.633.1"> log levels are often used when capturing exceptions, and fatal issues are logged at the </span><strong class="source-inline"><span class="kobospan" id="kobo.634.1">FATAL</span></strong><span class="kobospan" id="kobo.635.1"> level. </span><span class="kobospan" id="kobo.635.2">This section will look at capturing error logs and handling the exception on </span><span><span class="kobospan" id="kobo.636.1">the pipeline.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.637.1">Capturing error logs</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.638.1">We</span><a id="_idIndexMarker272" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.639.1"> will modify the data transformation stored procedure to capture error logs and add </span><span><span class="kobospan" id="kobo.640.1">exception handling:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.641.1">
def data_transform(session: Session):
    try:
        ## Initializing Logger
        logger = logging.getLogger("Data_Transform_Logger")
        logger.info("Data Transformation Pipeline Starts")
        ## Pivoting Process
        marketing_final = session.table("FINAL_MARKETING_DATA")
        market_subset = marketing_final.select("EDUCATION", \
            "MARITAL_STATUS","INCOME")
        market_pivot = market_subset.pivot("EDUCATION", \
            ["Graduation","PhD","Master","Basic","2n Cycle"]
        ).sum("INCOME")
        #### Writing Transformed Data To New Table
        market_pivot.write.save_as_table("MAREKTING_PIVOT")
        logger.log("MARKETING PIVOT TABLE CREATED")
        return "CREATED MARKETING PIVOT TABLE"
    except Exception as err:
        logger.error("Logging an error from Python handler: ")
        logger.error(err)
        return "ERROR"</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.642.1">The data</span><a id="_idIndexMarker273" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.643.1"> transformation logic is moved into the </span><strong class="source-inline"><span class="kobospan" id="kobo.644.1">try</span></strong><span class="kobospan" id="kobo.645.1"> block, and a logger named </span><strong class="source-inline"><span class="kobospan" id="kobo.646.1">Data_Transform_Logger</span></strong><span class="kobospan" id="kobo.647.1"> is initiated. </span><span class="kobospan" id="kobo.647.2">The exception raised by the code is captured in the exception object defined as </span><strong class="source-inline"><span class="kobospan" id="kobo.648.1">err</span></strong><span class="kobospan" id="kobo.649.1">. </span><span class="kobospan" id="kobo.649.2">This is then logged in the event table by the </span><strong class="source-inline"><span class="kobospan" id="kobo.650.1">ERROR</span></strong><span class="kobospan" id="kobo.651.1"> log level. </span><span class="kobospan" id="kobo.651.2">We will now register this stored procedure </span><span><span class="kobospan" id="kobo.652.1">in Snowpark:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.653.1">
## Register Stored Procedure in Snowflake
### Add packages and data types
from snowflake.snowpark.types import StringType
session.add_packages('snowflake-snowpark-python')
### Upload Stored Procedure to Snowflake
session.sproc.register(
    func = data_transform
  , return_type = StringType()
  , input_types = []
  , is_permanent = True
  , name = 'DATA_TRANSFORM_SPROC_LOG_ERROR'
  , replace = True   , stage_locations = "@MY_STAGE" )</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.654.1">We will </span><a id="_idIndexMarker274" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.655.1">execute the stored procedure purposely with the error to log the error. </span><span class="kobospan" id="kobo.655.2">The stored procedure can be triggered by running the </span><span><span class="kobospan" id="kobo.656.1">following command:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.657.1">
session.sql(''' Call DATA_TRANSFORM_SPROC_LOG_ERROR()''').show()</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.658.1">The stored procedure has raised an exception, and the error has been captured in the </span><span><span class="kobospan" id="kobo.659.1">event table:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer110">
<span class="kobospan" id="kobo.660.1"><img alt="Figure 4.13 – Error execution" src="image/B19923_04_13.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.661.1">Figure 4.13 – Error execution</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.662.1">The following section will cover querying error logs generated by </span><span><span class="kobospan" id="kobo.663.1">the procedure.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.664.1">Querying error logs</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.665.1">The </span><a id="_idIndexMarker275" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.666.1">error log records from the preceding stored procedure are captured under the </span><strong class="source-inline"><span class="kobospan" id="kobo.667.1">Data_Transform_Logger</span></strong><span class="kobospan" id="kobo.668.1"> logger, which can be accessed by filtering the query to return logs specific to the logger. </span><span class="kobospan" id="kobo.668.2">The following </span><a id="_idIndexMarker276" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.669.1">query can be executed to get the records from the </span><span><span class="kobospan" id="kobo.670.1">event table:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.671.1">
session.sql("""
    SELECT RECORD['severity_text'] AS SEVERITY,VALUE AS MESSAGE
    FROM MY_EVENTS
    WHERE SCOPE['name'] = 'Data_Transform_Logger'
    AND RECORD_TYPE = 'LOG'
""").collect()</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.672.1">The scope name is filtered as the </span><strong class="source-inline"><span class="kobospan" id="kobo.673.1">Data_Transform_Logger</span></strong><span class="kobospan" id="kobo.674.1"> to get the results, and errors caused by the exception are logged under </span><strong class="source-inline"><span class="kobospan" id="kobo.675.1">SEVERITY</span></strong> <span><span class="kobospan" id="kobo.676.1">as </span></span><span><strong class="source-inline"><span class="kobospan" id="kobo.677.1">ERROR</span></strong></span><span><span class="kobospan" id="kobo.678.1">:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer111">
<span class="kobospan" id="kobo.679.1"><img alt="Figure 4.14 – Error log messages" src="image/B19923_04_14.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.680.1">Figure 4.14 – Error log messages</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.681.1">Snowpark makes debugging easy by supporting logging errors and handling exceptions through event tables. </span><span class="kobospan" id="kobo.681.2">The following section will cover event traces and how to capture trace information </span><span><span class="kobospan" id="kobo.682.1">in Snowpark.</span></span></p>
<h2 id="_idParaDest-75" class="calibre7"><a id="_idTextAnchor075" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.683.1">Setting up tracing in Snowpark</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.684.1">Trace events</span><a id="_idIndexMarker277" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.685.1"> are a type of telemetry data that is captured when something has happened in the code. </span><span class="kobospan" id="kobo.685.2">It has a structured payload that helps analyze the</span><a id="_idIndexMarker278" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.686.1"> trace by aggregating this information to understand the code’s behavior at a high level. </span><span class="kobospan" id="kobo.686.2">When a procedure or function executes, trace events are emitted, which are available in the active </span><span><span class="kobospan" id="kobo.687.1">event table.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.688.1">The first step in capturing events is to turn on the tracing functionality by setting the trace level in the Snowpark session. </span><span class="kobospan" id="kobo.688.2">The trace level in the session can be set by running the </span><span><span class="kobospan" id="kobo.689.1">following code:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.690.1">
session.sql("ALTER SESSION SET TRACE_LEVEL = ALWAYS;").show()</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.691.1">In the next section, we will look at </span><span><span class="kobospan" id="kobo.692.1">capturing traces.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.693.1">Capturing traces</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.694.1">Traces</span><a id="_idIndexMarker279" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.695.1"> can be captured with the open source Snowflake </span><strong class="source-inline"><span class="kobospan" id="kobo.696.1">telemetry</span></strong><span class="kobospan" id="kobo.697.1"> Python package available in the Anaconda Snowflake channel. </span><span class="kobospan" id="kobo.697.2">The package needs to be imported into the code, and it will be executed in Snowpark. </span><span class="kobospan" id="kobo.697.3">The </span><strong class="source-inline"><span class="kobospan" id="kobo.698.1">telemetry</span></strong><span class="kobospan" id="kobo.699.1"> package can be imported by including the code in the </span><span><span class="kobospan" id="kobo.700.1">Snowpark handler:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.701.1">
from snowflake import telemetry</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.702.1">The </span><strong class="source-inline"><span class="kobospan" id="kobo.703.1">telemetry</span></strong><span class="kobospan" id="kobo.704.1"> package helped capture traces generated on the code and logged into the event table. </span><span class="kobospan" id="kobo.704.2">We will modify the data cleanup procedure by adding telemetry events to capture </span><span><span class="kobospan" id="kobo.705.1">the trace:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.706.1">
def data_cleanup(session: Session):
    #### Loading Telemetry Package
    from snowflake import telemetry
    #### Loading Required Tables
    market_pivot = session.table("MARKETING_PIVOT")
    #### Adding Trace Event
    telemetry.add_event("data_cleanup", \
        {"table_name": "MARKETING_PIVOT", \
         "count": market_pivot.count()})
    #### Dropping Null
    market_drop_null = market_pivot.dropna(thresh=5)
    #### Writing Cleaned Data To New Table
    market_drop_null.write.save_as_table("MARKET_PIVOT_CLEANED")
    #### Adding Trace Event
    telemetry.add_event("data_cleanup", \
        {"table_name": "MARKET_PIVOT_CLEANED", \
         "count": market_drop_null.count()})
    return "CREATED CLEANED TABLE"
###########################################################
## Register Stored Procedure in Snowflake
### Add packages and data types
from snowflake.snowpark.types import StringType
session.add_packages('snowflake-snowpark-python', \
    'snowflake-telemetry-python')
### Upload Stored Procedure to Snowflake
session.sproc.register(
    func = data_cleanup
  , return_type = StringType()
  , input_types = []
  , is_permanent = True
  , name = 'DATA_CLEANUP_SPROC_TRACE'
  , replace = True
  , stage_location = '@MY_STAGE'
)</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.707.1">The </span><a id="_idIndexMarker280" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.708.1">procedure is now ready to be executed. </span><span class="kobospan" id="kobo.708.2">We are passing attributes for the operations captured on the traces. </span><span class="kobospan" id="kobo.708.3">We can execute the procedure by running the </span><span><span class="kobospan" id="kobo.709.1">following code:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.710.1">
session.sql(''' Call DATA_CLEANUP_SPROC_TRACE()''').show()</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.711.1">The procedure is executed, and the data is cleaned from </span><span><span class="kobospan" id="kobo.712.1">the table:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer112">
<span class="kobospan" id="kobo.713.1"><img alt="Figure 4.15 – Data cleanup procedure execution" src="image/B19923_04_15.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.714.1">Figure 4.15 – Data cleanup procedure execution</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.715.1">The traces are now generated in the event table. </span><span class="kobospan" id="kobo.715.2">We can directly query the event table to obtain </span><span><span class="kobospan" id="kobo.716.1">trace information.</span></span></p>
<h3 class="calibre9"><span class="kobospan" id="kobo.717.1">Querying traces</span></h3>
<p class="calibre3"><span class="kobospan" id="kobo.718.1">Traces </span><a id="_idIndexMarker281" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.719.1">generated can be accessed from the event table. </span><span class="kobospan" id="kobo.719.2">Traces captured from the previous stored procedure can be accessed by running the </span><span><span class="kobospan" id="kobo.720.1">following query:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.721.1">
session.sql("""
    SELECT
        TIMESTAMP as time,
        RESOURCE_ATTRIBUTES['snow.executable.name']
            as handler_name,
        RESOURCE_ATTRIBUTES['snow.executable.type']
            as handler_type,
        RECORD['name'] as event_name,
        RECORD_ATTRIBUTES as attributes
    FROM
        MY_EVENTS
    WHERE
        EVENT_NAME ='data_cleanup'
""").show(2)</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.722.1">We are</span><a id="_idIndexMarker282" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.723.1"> querying using the </span><strong class="source-inline"><span class="kobospan" id="kobo.724.1">data_cleanup</span></strong><span class="kobospan" id="kobo.725.1"> event name. </span><span class="kobospan" id="kobo.725.2">This returns the two traces captured when the code </span><span><span class="kobospan" id="kobo.726.1">was executed:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer113">
<span class="kobospan" id="kobo.727.1"><img alt="Figure 4.16 – Trace capture information" src="image/B19923_04_16.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.728.1">Figure 4.16 – Trace capture information</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.729.1">We can see the attributes that are captured from the execution of the stored procedure. </span><span class="kobospan" id="kobo.729.2">After the </span><strong class="source-inline"><span class="kobospan" id="kobo.730.1">NULL</span></strong><span class="kobospan" id="kobo.731.1"> values have been cleaned up, the details return the total data counts, including the </span><strong class="source-inline"><span class="kobospan" id="kobo.732.1">NULL</span></strong><span class="kobospan" id="kobo.733.1"> and </span><span><strong class="source-inline"><span class="kobospan" id="kobo.734.1">count</span></strong></span><span><span class="kobospan" id="kobo.735.1"> values.</span></span></p>
<h2 id="_idParaDest-76" class="calibre7"><a id="_idTextAnchor076" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.736.1">Comparison of logs and traces</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.737.1">The following table compares logs and traces and lists scenarios to </span><span><span class="kobospan" id="kobo.738.1">use each:</span></span></p>
<table class="no-table-style" id="table003">
<colgroup class="calibre10">
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
</colgroup>
<tbody class="calibre12">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><strong class="bold"><span class="kobospan" id="kobo.739.1">Characteristic</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="bold"><span class="kobospan" id="kobo.740.1">Log entries</span></strong></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span><strong class="bold"><span class="kobospan" id="kobo.741.1">Trace events</span></strong></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span><span class="kobospan" id="kobo.742.1">Intended use</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.743.1">Record detailed but unstructured information about the state of your code. </span><span class="kobospan" id="kobo.743.2">Use this information to understand what happened during a particular invocation of your function </span><span><span class="kobospan" id="kobo.744.1">or procedure.</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.745.1">Record a brief but structured summary of each invocation of your code. </span><span class="kobospan" id="kobo.745.2">Aggregate this information to understand the behavior of your code at a </span><span><span class="kobospan" id="kobo.746.1">high level.</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.747.1">Structure as </span><span><span class="kobospan" id="kobo.748.1">a payload</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.749.1">None. </span><span class="kobospan" id="kobo.749.2">A log entry is just </span><span><span class="kobospan" id="kobo.750.1">a string.</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.751.1">Structured with attributes you can attach to trace events. </span><span class="kobospan" id="kobo.751.2">Attributes are key-value pairs that can be easily queried with a </span><span><span class="kobospan" id="kobo.752.1">SQL query.</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.753.1">Supports </span><span><span class="kobospan" id="kobo.754.1">grouping</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.755.1">No. </span><span class="kobospan" id="kobo.755.2">Each log entry is an </span><span><span class="kobospan" id="kobo.756.1">independent event.</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.757.1">Yes. </span><span class="kobospan" id="kobo.757.2">Trace events are organized into spans. </span><span class="kobospan" id="kobo.757.3">A span can have </span><span><span class="kobospan" id="kobo.758.1">its attributes.</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.759.1">Quantity </span><span><span class="kobospan" id="kobo.760.1">limits</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.761.1">Unlimited. </span><span class="kobospan" id="kobo.761.2">All log entries emitted by your code are ingested into the </span><span><span class="kobospan" id="kobo.762.1">event table.</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.763.1">The number of trace events per span is capped at 128. </span><span class="kobospan" id="kobo.763.2">There is also a limit on the number of </span><span><span class="kobospan" id="kobo.764.1">span attributes.</span></span></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.765.1">Complexity of queries against </span><span><span class="kobospan" id="kobo.766.1">recorded data</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.767.1">Relatively high. </span><span class="kobospan" id="kobo.767.2">Your queries must parse each log entry to extract meaningful information </span><span><span class="kobospan" id="kobo.768.1">from it.</span></span></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><span class="kobospan" id="kobo.769.1">Relatively low. </span><span class="kobospan" id="kobo.769.2">Your queries can take advantage of the structured nature of </span><span><span class="kobospan" id="kobo.770.1">trace events.</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.771.1">Table 4.3 – Differences between logs and traces</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.772.1">Logs and traces help debug Snowpark and are a practical feature for </span><span><span class="kobospan" id="kobo.773.1">efficient DataOps.</span></span></p>
<h1 id="_idParaDest-77" class="calibre5"><a id="_idTextAnchor077" class="calibre6 pcalibre1 pcalibre"/><span class="kobospan" id="kobo.774.1">Summary</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.775.1">In this chapter, we covered in detail building and deploying resilient data pipelines in Snowpark and also how to enable logging and tracking using event tables. </span><span class="kobospan" id="kobo.775.2">Building resilient data pipelines with effective DataOps is vital for a successful data strategy. </span><span class="kobospan" id="kobo.775.3">Snowpark supports the development of a modern data pipeline through a programmatic ELT approach along with features such as logging and tracing, making it easy for developers to implement DataOps. </span><span class="kobospan" id="kobo.775.4">We also covered how tasks and task graphs can be used in scheduling and </span><span><span class="kobospan" id="kobo.776.1">deploying pipelines.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.777.1">In the next chapter, we will cover using Snowpark to develop data </span><span><span class="kobospan" id="kobo.778.1">science workloads.</span></span></p>
</div>
</body></html>