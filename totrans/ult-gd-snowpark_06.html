<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-58"><a id="_idTextAnchor058" class="calibre6 pcalibre1 pcalibre"/>4</h1>
<h1 id="_idParaDest-59" class="calibre5"><a id="_idTextAnchor059" class="calibre6 pcalibre1 pcalibre"/>Building Data Engineering Pipelines with Snowpark</h1>
<p class="calibre3">Data is the heartbeat of every organization, and data engineering is the lifeblood that ensures that current, accurate data is flowing through for various consumption. The role of a data engineer is to develop and manage the data engineering pipeline and the process that collects, transforms, and delivers data to a different <strong class="bold">line of business</strong> (<strong class="bold">LOB</strong>). As Gartner’s research rightly mentions, “<em class="italic">The increasing diversity of data, and the need to provide the right data to the right people at the right time, has created a demand for the data engineering practice. Data and analytics leaders must integrate the data engineering discipline into their data management strategy.</em>” This chapter discusses a practical approach to building efficient data engineering pipelines with Snowpark.</p>
<p class="calibre3">In this chapter, we’re going to cover the following main topics:</p>
<ul class="calibre15">
<li class="calibre14">Developing resilient data pipelines with Snowpark</li>
<li class="calibre14">Deploying efficient DataOps in Snowpark</li>
<li class="calibre14">Overview of tasks in Snowflake</li>
<li class="calibre14">Implementing logging and tracing in Snowpark</li>
</ul>
<h1 id="_idParaDest-60" class="calibre5"><a id="_idTextAnchor060" class="calibre6 pcalibre1 pcalibre"/>Technical requirements</h1>
<p class="calibre3">This chapter requires an active Snowflake account and Python installed with Anaconda and configured locally. You can sign up for a Snowflake trial account at <a href="https://signup.snowflake.com/" class="calibre6 pcalibre1 pcalibre">https://signup.snowflake.com/</a>.</p>
<p class="calibre3">The technical requirements for environment setup are the same as in the previous chapters. If you haven’t set up your environment yet, please refer to the previous chapter. Supporting materials are available at <a href="https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark" class="calibre6 pcalibre1 pcalibre">https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark</a>.</p>
<h1 id="_idParaDest-61" class="calibre5"><a id="_idTextAnchor061" class="calibre6 pcalibre1 pcalibre"/>Developing resilient data pipelines with Snowpark</h1>
<p class="calibre3">A<a id="_idIndexMarker210" class="calibre6 pcalibre1 pcalibre"/> robust and resilient data pipeline will equip organizations to source, collect, analyze, and effectively use insights to grow business and deliver cost-saving business processes. Traditional data pipelines are difficult to manage and do not support the organization’s evolving data needs. Snowpark solves problems that conventional data pipelines have by running them natively on the Snowflake Data Cloud and making extracting information from data in the Data Cloud easier and faster. This section will cover the various characteristics of resilient data pipelines, how to develop them in Snowpark, and their benefits.</p>
<h2 id="_idParaDest-62" class="calibre7"><a id="_idTextAnchor062" class="calibre6 pcalibre1 pcalibre"/>Traditional versus modern data pipelines</h2>
<p class="calibre3">A <a id="_idIndexMarker211" class="calibre6 pcalibre1 pcalibre"/>significant challenge of a traditional data pipeline is that it takes considerable time and cost to develop and manage, with high technical debt. It also consists of multiple tools that take much time to integrate. Due to the complexity of the solution, there is a possibility of delayed data due to latency and support for streaming data. The following diagram highlights the traditional method:</p>
<div><div><img alt="Figure 4.1 – Traditional data pipeline" src="img/B19923_04_01.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 4.1 – Traditional data pipeline</p>
<p class="calibre3">The architecture shows a complex of technologies and systems being stitched together to deliver data from the source to consumers, with multiple points of failure at each stage. There are also issues of data governance and security due to data silos being present with numerous copies of the same data.</p>
<p class="calibre3">Modern <a id="_idIndexMarker212" class="calibre6 pcalibre1 pcalibre"/>data pipelines work based on a unified platform and multi-workload model. They integrate data sources such as batch and streaming to enhance productivity with streamlined architecture by enabling various <strong class="bold">business intelligence</strong> (<strong class="bold">BI</strong>) and analytics workloads and supporting internal and external users. The unified platform architecture supports continuous, extensible data processing pipelines with scalable performance. The following diagram highlights the modern Snowflake approach:</p>
<div><div><img alt="Figure 4.2 – Modern data pipeline" src="img/B19923_04_02.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 4.2 – Modern data pipeline</p>
<p class="calibre3">Snowpark stands out as a modern data pipeline tool due to its native integration with Snowflake, a leading cloud data warehouse, enabling seamless data processing directly within Spark applications. Offering a unified development experience with familiar programming languages such as Scala and Java, Snowpark eliminates the complexity associated with traditional Spark setups, allowing for streamlined development and maintenance of data pipelines. Snowpark’s optimized performance for Snowflake’s architecture ensures efficient data processing and reduced latency, enabling quick analysis of large datasets. Moreover, its advanced analytics capabilities, scalability, and cost-effectiveness make it a compelling choice for organizations seeking to build agile, cloud-native data pipelines with enhanced productivity and flexibility compared to traditional Spark setups.</p>
<h2 id="_idParaDest-63" class="calibre7"><a id="_idTextAnchor063" class="calibre6 pcalibre1 pcalibre"/>Data engineering with Snowpark</h2>
<p class="calibre3">Snowpark <a id="_idIndexMarker213" class="calibre6 pcalibre1 pcalibre"/>has many data engineering capabilities, making it a<a id="_idIndexMarker214" class="calibre6 pcalibre1 pcalibre"/> fast and flexible platform that enables developers to use Python for data engineering. With the support<a id="_idIndexMarker215" class="calibre6 pcalibre1 pcalibre"/> of <strong class="bold">extract, transform, and load</strong> (<strong class="bold">ETL</strong>) and <strong class="bold">extract, load, and transform</strong> (<strong class="bold">ELT</strong>), developers can use the <a id="_idIndexMarker216" class="calibre6 pcalibre1 pcalibre"/>Snowpark client for development and interact with the Snowflake engine for processing using their favorite developer environment. With the support of Anaconda, you can ensure that required packages and dependencies are readily available for Snowpark scripts. And it becomes easier to accelerate the growth of product pipelines. Data pipelines in Snowpark can be batch or real-time, utilizing scalable, high-performant multi-cluster warehouses capable of handling complex data transformations without compromising performance:</p>
<div><div><img alt="Figure 4.3 – Snowpark data engineering" src="img/B19923_04_03.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 4.3 – Snowpark data engineering</p>
<p class="calibre3">Snowpark enhances the entire data engineering lifecycle with an engine that enables expressiveness and flexibility for developers with the simplicity of Data Cloud operations. Snowflake can help make data centralized, including structured, semi-structured, and unstructured data loaded into Snowflake for processing. Transformations can be carried out with the help of the powerful Python-based Snowpark functions. Snowpark data engineering workloads can be fully managed alongside other Snowflake objects throughout the development lifecycle with built-in monitoring and orchestration capabilities that support complex data pipelines at scale powered by the Data Cloud. The result of advanced data transformations is stored inside Snowflake and can be used for different data consumers. Snowpark data pipelines reduce the number of stages data needs to move to actionable insights by removing the step that moves data for computation.</p>
<h2 id="_idParaDest-64" class="calibre7"><a id="_idTextAnchor064" class="calibre6 pcalibre1 pcalibre"/>Implementing programmatic ELT with Snowpark</h2>
<p class="calibre3">Snowflake <a id="_idIndexMarker217" class="calibre6 pcalibre1 pcalibre"/>supports and recommends a modern ELT implementation pattern for data engineering instead of the legacy ETL process. ETL is a <a id="_idIndexMarker218" class="calibre6 pcalibre1 pcalibre"/>pattern where data is extracted from various sources, transformed in the data pipeline, and then the transformed data is loaded into a destination such <a id="_idIndexMarker219" class="calibre6 pcalibre1 pcalibre"/>as a data warehouse or data mart. The following diagram shows the comparison of ETL versus ELT:</p>
<div><div><img alt="Figure 4.4 – ETL versus ELT" src="img/B19923_04_04.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 4.4 – ETL versus ELT</p>
<p class="calibre3">ELT is a pattern that<a id="_idIndexMarker220" class="calibre6 pcalibre1 pcalibre"/> is more suited for the Snowflake Data Cloud, where data is extracted from the source and loaded into Snowflake. This data is then transformed within Snowflake using Snowpark. Snowpark pipelines are designed to extract and load the data first and then transform it in the destination as the transformation is done inside Snowflake, which provides better scalability and elasticity. The ELT also improves performance and reduces the time it takes to ingest, transform, and analyze the data within Snowflake using Snowpark. The following diagram shows the different layers of data within Snowflake:</p>
<div><div><img alt="Figure 4.5 – Data stages in Snowflake" src="img/B19923_04_05.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 4.5 – Data stages in Snowflake</p>
<p class="calibre3">The<a id="_idIndexMarker221" class="calibre6 pcalibre1 pcalibre"/> data pipelines build the different data stages inside Snowflake. These stages are databases and schemas with objects such as tables <a id="_idIndexMarker222" class="calibre6 pcalibre1 pcalibre"/>and views inside them. The raw data is ingested from the source systems into Snowflake with no transformations since Snowflake supports multiple data formats. This data is then transformed using Snowpark into the conformed stage containing the de-duped and standardized data. This becomes the data that feeds into the next step in the data pipeline. The reference stage has the business definition and data mappings with the hierarchies and the master data. The final stage has the modeled data, which has the clean and transformed data. Snowpark has many functions that help with doing value-added transformations that help convert data into a business-ready format accessible to users and applications, making it more valuable for the organization.</p>
<h3 class="calibre9">ETL versus ELT in Snowpark</h3>
<p class="calibre3">Snowpark <a id="_idIndexMarker223" class="calibre6 pcalibre1 pcalibre"/>supports both ETL and ELT workloads. While ELT is famous for modern pipelines, the ETL pattern is also used in some scenarios. ETL is commonly used with structured data where the total volume of data is small. It is also used in the system for migrating legacy databases to the Data Cloud where the source and target data types differ. ELT provides a significant advantage compared to the traditional ETL process. ELT supports large volumes of structured, unstructured, and semi-structured data that can be processed using Snowflake. It also allows developers and analysts to experiment with data as it is loaded into Snowflake. ELT also maximizes the option for them to transform data to get potential insights. It also supports low latency and real-time analytics. ELT is better suited for Snowflake than the traditional ETL for these reasons. The following section will cover how to develop efficient DataOps in Snowpark.</p>
<h1 id="_idParaDest-65" class="calibre5"><a id="_idTextAnchor065" class="calibre6 pcalibre1 pcalibre"/>Deploying efficient DataOps in Snowpark</h1>
<p class="calibre3">DataOps<a id="_idIndexMarker224" class="calibre6 pcalibre1 pcalibre"/> helps data<a id="_idIndexMarker225" class="calibre6 pcalibre1 pcalibre"/> teams reduce development times, increase data quality, and maximize the business value of data by bringing more rigor to the development and management of data pipelines. It also ensures that the data is clean, accurate, and up-to-date in a streamlined environment with data governance. Data engineering introduces the processes and capabilities required to effectively develop, manage, and deploy data engineering pipelines. The following diagram highlights the DataOps approach:</p>
<div><div><img alt="Figure 4.6 – DataOps process" src="img/B19923_04_06.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 4.6 – DataOps process</p>
<p class="calibre3">The DataOps process focuses on bringing agile development to data engineering pipelines using an iterative development, testing, and deployment process in loops. It also<a id="_idIndexMarker226" class="calibre6 pcalibre1 pcalibre"/> includes <strong class="bold">continuous integration and continuous deployment</strong> (<strong class="bold">CI/CD</strong>) for data, schema changes, and the data versioning and automation of data models and artifacts. This section will show an example of a data engineering pipeline executed in Snowpark.</p>
<h2 id="_idParaDest-66" class="calibre7"><a id="_idTextAnchor066" class="calibre6 pcalibre1 pcalibre"/>Developing a data engineering pipeline</h2>
<p class="calibre3">Creating a <a id="_idIndexMarker227" class="calibre6 pcalibre1 pcalibre"/>resilient data engineering pipeline within the Snowpark framework requires integrating three core components seamlessly:</p>
<ol class="calibre13">
<li class="calibre14">First and foremost, data engineers must master the art of loading data into Snowflake, setting the stage for subsequent processing. This initial step sets the foundation upon which the entire pipeline is built.</li>
<li class="calibre14">Second, the transformative power of Snowpark data functions comes into play, enabling engineers to shape and mold the data to meet specific analytical needs. <a href="B19923_03.xhtml#_idTextAnchor042" class="calibre6 pcalibre1 pcalibre"><em class="italic">Chapter 3</em></a><em class="italic">,</em> <em class="italic">Simplifying Data Processing Using Snowpark</em> provided a detailed exploration of DataFrame operations, laying the groundwork for this pivotal transformation phase.</li>
<li class="calibre14">Finally, the data journey culminates in bundling these operations as Snowpark stored procedures, offering efficiency and repeatability in handling data.</li>
</ol>
<p class="calibre3">As we <a id="_idIndexMarker228" class="calibre6 pcalibre1 pcalibre"/>delve into this section, building upon the knowledge garnered from <a href="B19923_02.xhtml#_idTextAnchor028" class="calibre6 pcalibre1 pcalibre"><em class="italic">Chapter 2</em></a><em class="italic">, Establishing a Foundation with Snowpark</em> and <a href="B19923_03.xhtml#_idTextAnchor042" class="calibre6 pcalibre1 pcalibre"><em class="italic">Chapter 3</em></a>, <em class="italic">Simplifying Data Processing Using Snowpark</em> where we elaborated on DataFrame operations and their<a id="_idIndexMarker229" class="calibre6 pcalibre1 pcalibre"/> conversion into <strong class="bold">user-defined functions</strong> (<strong class="bold">UDFs</strong>) and stored procedures, we will unravel the intricate process of unifying these elements into a resilient data engineering pipeline. This chapter is a testament to the synthesis of theory and practice, empowering data professionals to seamlessly interconnect the loading, transformation, and bundling phases, resulting in a robust framework for data processing and analysis within the Snowpark ecosystem.</p>
<p class="calibre3">With a comprehensive understanding of data loading from our discussions in the previous chapters, our journey now pivots toward strategically utilizing this data. This pivotal transition places our emphasis on three core steps:</p>
<ol class="calibre13">
<li class="calibre14">Data preparation</li>
<li class="calibre14">Data transformation</li>
<li class="calibre14">Data cleanup</li>
</ol>
<p class="calibre3">These stages constitute the cornerstone of our data engineering voyage, where we will sculpt, consolidate, and refine our data, revealing its true potential for analysis and valuable insights. We’ll now transform these concepts into practical data engineering pipelines, leveraging the valuable insights from our prior discussions on stored procedure templates and transformation steps. Our focus will center on our marketing campaign data, where the foundational loading steps have been thoughtfully outlined in <a href="B19923_03.xhtml#_idTextAnchor042" class="calibre6 pcalibre1 pcalibre"><em class="italic">Chapter 3</em></a>, <em class="italic">Simplifying Data Processing Using Snowpark</em> providing a solid starting point for our data preparation.</p>
<h3 class="calibre9">Data preparation</h3>
<p class="calibre3">In the<a id="_idIndexMarker230" class="calibre6 pcalibre1 pcalibre"/> progression of our data engineering pipeline, the subsequent imperative phase is data preparation, which involves the integration of diverse tables. In this section, we will explore techniques for merging these disparate data tables using various functions tailored to the task. Additionally, we will elucidate the process of registering these functions as stored procedures, ensuring a streamlined and efficient data workflow. The first step is joining the purchase history with the campaign information. Both tables are entered using the <code>ID</code> column, and a single ID is retained:</p>
<pre class="source-code">
def combine_campaign_table(purchase_history,campaign_info):
    purchase_campaign = purchase_history.join(
        campaign_info, \
        purchase_history.ID == campaign_info.ID, \
        lsuffix="_left", rsuffix="_right"
    )
    purchase_campaign = purchase_campaign.drop("ID_RIGHT")
    return purchase_campaign</pre> <p class="calibre3">The resultant <code>purchase_campaign</code> DataFrame holds the data and is used in the next step. In the next step, we join the purchase campaign with the complaint information using the same <code>ID</code> column and then create a <code>purchase_campaign_complain</code> DataFrame:</p>
<pre class="source-code">
def combine_complain_table(purchase_campaign,complain_info):
    purchase_campaign_complain = purchase_campaign.join(
        complain_info, \
        purchase_campaign["ID_LEFT"] == complain_info.ID
    )
    purchase_campaign_complain = \
        purchase_campaign_complain.drop("ID_LEFT")
    return purchase_campaign_complain</pre> <p class="calibre3">The preceding code joins the column to create a <code>purchase_campaign_complain</code> DataFrame, which contains the mapped purchase data with complaint information. In the final step, a marketing table is created by the union of the data between the purchase complaint and the marketing table:</p>
<pre class="source-code">
def union_marketing_additional_table(
    purchase_campaign_complain,marketing_additional):
    final_marketing_table = \
        purchase_campaign_complain.union_by_name(
            marketing_additional
        )
    return final_marketing_table</pre> <p class="calibre3">The <a id="_idIndexMarker231" class="calibre6 pcalibre1 pcalibre"/>preceding code produces a table that contains all the combined data that is the final result of the pipeline, which will be written as a table. The Python functions representing each step are executed as part of the Snowpark stored procedure. The stored procedures can be performed in sequence one after the other and also scheduled as Snowflake tasks. The data preparation procedure calls the three Python methods, and the final table is written to Snowflake:</p>
<pre class="source-code">
from snowflake.snowpark.functions import sproc
import snowflake
def data_prep(session: Session):
    #### Loading Required Tables
    purchase_history = session.table("PURCHASE_HISTORY")
    campaign_info = session.table("CAMPAIGN_INFO")
    complain_info = session.table("COMPLAINT_INFO")
    marketing_additional = session.table("MARKETING_ADDITIONAL")</pre> <p class="calibre3">The<a id="_idIndexMarker232" class="calibre6 pcalibre1 pcalibre"/> preceding code does the data preparation by loading the required data into the DataFrame. We will now call each of the steps to execute it like a pipeline:</p>
<pre class="source-code">
    #### Calling Step 1
    purchase_campaign = combine_campaign_table(
        purchase_history, campaign_info)
    #### Calling Step 2
    purchase_campaign_complain = combine_campaign_table(
        purchase_campaign, complain_info)
    #### Calling Step 3
    final_marketing_data = union_marketing_additional_table(
        purchase_campaign_complain, marketing_additional)</pre> <p class="calibre3">The three previously defined step functions are executed. The resultant data is loaded into the new <code>final_marketing_data</code> DataFrame, which will then be loaded to the Snowflake table:</p>
<pre class="source-code">
    #### Writing Combined Data To New Table
    final_marketing_data.write.save_as_table( \
        "FINAL_MARKETING_DATA")
    return "LOADED FINAL MARKETING DATA TABLE"</pre> <p class="calibre3">Now, we will create and execute a stored procedure that contains the preceding logic. The procedure is called <code>data_prep_sproc</code> and is the first part of the data engineering pipeline – data preparation:</p>
<pre class="source-code">
# Create an instance of StoredProcedure using the sproc() function
from snowflake.snowpark.types import IntegerType,StringType
data_prep_sproc = sproc(
                        func= data_prep,\
                        replace=True,\
                        return_type = StringType(),\
                        stage_location="@my_stage",\
                        packages=["snowflake-snowpark-python"]
                        )</pre> <p class="calibre3">The<a id="_idIndexMarker233" class="calibre6 pcalibre1 pcalibre"/> preceding stored procedure writes the data into the <code>Final_Marketing_Data</code> table, which will be used in the next step of data transformation.</p>
<h3 class="calibre9">Data transformation</h3>
<p class="calibre3">The subsequent <a id="_idIndexMarker234" class="calibre6 pcalibre1 pcalibre"/>phase in this process involves data transformation, building upon the data prepared in the previous step. Here, we’ll take the pivotal action of registering another stored procedure after the last stage. This procedure applies transformation logic, molding the data into a form primed for analysis. Leveraging Snowpark’s array of valuable aggregation and summarization functions, we will harness these capabilities to shape and enhance our data, laying a solid foundation for rigorous analysis. The following code transforms the data:</p>
<pre class="source-code">
def data_transform(session: Session):
    #### Loading Required Tables
    marketing_final = session.table("FINAL_MARKETING_DATA")
    market_subset = marketing_final.select("EDUCATION", \
        "MARITAL_STATUS","INCOME")
    market_pivot = market_subset.pivot("EDUCATION", \
        ["Graduation","PhD","Master","Basic","2n Cycle"]
    ).sum("INCOME")
    #### Writing Transformed Data To New Table
    market_pivot.write.save_as_table("MARKETING_PIVOT")
    return "CREATED MARKETING PIVOT TABLE"
data_transform_sproc = sproc(
                        func= data_transform,\
                        replace=True,\
                        return_type = StringType(),\
                        stage_location="@my_stage",\
                        packages=["snowflake-snowpark-python"]
                        )</pre> <p class="calibre3">A <code>data_transform_sproc</code> stored <a id="_idIndexMarker235" class="calibre6 pcalibre1 pcalibre"/>procedure is created, which reads the <code>Final_Marketing_Data</code> table and creates a pivot with the education of the customer and the total income. When the stored procedure is executed, this is then written to the <code>Marketing_Pivot</code> table.</p>
<h3 class="calibre9">Data cleanup</h3>
<p class="calibre3">In the final <a id="_idIndexMarker236" class="calibre6 pcalibre1 pcalibre"/>step of our data engineering process, we focus on a crucial task: cleaning up the data in the <code>Marketing_Pivot</code> table. Similar to artists perfecting a masterpiece, we carefully go through our data, removing any empty values in tables that aren’t important for our analysis. To do this, we rely on the versatile <code>dropna()</code> function, which acts like a precise tool to cut away unnecessary data:</p>
<pre class="source-code">
def data_cleanup(session: Session):
    #### Loading Required Tables
    market_pivot = session.table("MARKETING_PIVOT")
    market_drop_null = market_pivot.dropna(thresh=5)
    #### Writing Cleaned Data To New Table
    market_drop_null.write.save_as_table("MARKET_PIVOT_CLEANED")
    return "CREATED CLEANED TABLE"
data_cleanup_sproc = sproc(
                        func= data_cleanup,\
                        replace=True,\
                        return_type = StringType(),\
                        stage_location="@my_stage",\
                        packages=["snowflake-snowpark-python"]
                        )</pre> <p class="calibre3">The <a id="_idIndexMarker237" class="calibre6 pcalibre1 pcalibre"/>cleaned-up data from the <code>market_drop_null</code> DataFrame is then saved into the <code>Market_Pivot_Cleaned</code> table. This data is at the last stage of the pipeline and is used for analysis.</p>
<h3 class="calibre9">Orchestrating the pipeline</h3>
<p class="calibre3">The<a id="_idIndexMarker238" class="calibre6 pcalibre1 pcalibre"/> data pipeline is orchestrated by calling three Snowpark procedures, which invokes the three different steps of the data engineering pipeline. The procedures are executed in the order of <code>data_prep_sproc</code>, <code>data_transform_sproc</code>, and <code>data_cleanup_sproc</code>:</p>
<pre class="source-code">
#### Calling Data Preparation Stored Procedure
data_prep_sproc()
#### Calling Data Transformation Stored Procedure
data_transform_sproc()
#### Calling Data Cleanup Stored Procedure
data_cleanup_sproc()</pre> <p class="calibre3">The<a id="_idIndexMarker239" class="calibre6 pcalibre1 pcalibre"/> Snowpark procedure is executed, and after each step is executed, the final data is written to the <code>Market_Pivot_Cleaned</code> table. Snowflake supports scheduling and orchestration through tasks. Tasks can be scheduled using the Python API and through worksheets, and they can trigger procedures in sequence:</p>
<div><div><img alt="Figure 4.7 – Stored procedure execution" src="img/B19923_04_07.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 4.7 – Stored procedure execution</p>
<p class="calibre3">In the following section, we will explore how we can utilize Snowflake tasks and task graphs to execute the preceding pipeline.</p>
<h1 id="_idParaDest-67" class="calibre5"><a id="_idTextAnchor067" class="calibre6 pcalibre1 pcalibre"/>Overview of tasks in Snowflake</h1>
<p class="calibre3">Tasks in <a id="_idIndexMarker240" class="calibre6 pcalibre1 pcalibre"/>Snowflake are powerful tools designed to streamline data processing workflows and automate various tasks within the Snowflake environment. Offering <a id="_idIndexMarker241" class="calibre6 pcalibre1 pcalibre"/>a range of functionalities, tasks execute different types of SQL code, enabling users to perform diverse operations on their data.</p>
<p class="calibre3">Tasks in Snowflake can<a id="_idIndexMarker242" class="calibre6 pcalibre1 pcalibre"/> execute three main types of SQL code:</p>
<ul class="calibre15">
<li class="calibre14"><strong class="bold">Single SQL statement</strong>: Allows the execution of a single SQL statement</li>
<li class="calibre14"><strong class="bold">Call to a stored procedure</strong>: Enables the invocation of a stored procedure</li>
<li class="calibre14"><strong class="bold">Procedural logic using Snowflake Scripting</strong>: Supports the implementation of procedural logic using Snowflake Scripting</li>
</ul>
<p class="calibre3">Tasks can be integrated with table streams to create continuous ELT workflows. By processing <a id="_idIndexMarker243" class="calibre6 pcalibre1 pcalibre"/>recently changed table rows, tasks ensure the maintenance of data integrity and provide exactly-once semantics for new or altered data. Tasks in Snowflake can be scheduled to run at specified intervals. Snowflake ensures that only one instance of a scheduled task is executed at a time, skipping scheduled executions if a task is still running.</p>
<h2 id="_idParaDest-68" class="calibre7"><a id="_idTextAnchor068" class="calibre6 pcalibre1 pcalibre"/>Compute models for tasks</h2>
<p class="calibre3">In the <a id="_idIndexMarker244" class="calibre6 pcalibre1 pcalibre"/>serverless compute model, tasks rely on compute resources managed by Snowflake. These resources are automatically resized and scaled based on workload demands, ensuring optimal performance and resource utilization. Snowflake dynamically determines the appropriate compute size for each task run based on historical statistics.</p>
<p class="calibre3">Alternatively, users can opt for the user-managed virtual warehouse model, where they specify an existing virtual warehouse for individual tasks. This model provides users with more control over compute resource management but requires careful sizing to ensure efficient task execution.</p>
<h2 id="_idParaDest-69" class="calibre7"><a id="_idTextAnchor069" class="calibre6 pcalibre1 pcalibre"/>Task graphs</h2>
<p class="calibre3">Task graphs, also<a id="_idIndexMarker245" class="calibre6 pcalibre1 pcalibre"/> known as <strong class="bold">directed acyclic graphs</strong> (<strong class="bold">DAGs</strong>), allow <a id="_idIndexMarker246" class="calibre6 pcalibre1 pcalibre"/>for the organization of tasks based on dependencies. Each task within a task graph has predecessor and subsequent tasks, facilitating complex workflow management.</p>
<p class="calibre3">Task graphs are subject to certain limitations, including a maximum of 1,000 tasks in total, including the root task. Individual tasks within a task graph can have a maximum of 100 predecessors and 100 child tasks.</p>
<p class="calibre3">Users can view and monitor their task graphs using SQL or Snowsight, Snowflake’s integrated development environment, providing visibility into task dependencies and execution status.</p>
<p class="calibre3">In summary, tasks in Snowflake offer robust capabilities for data processing, automation, and workflow management, making them indispensable tools for users seeking to optimize their data operations within the Snowflake ecosystem.</p>
<h2 id="_idParaDest-70" class="calibre7"><a id="_idTextAnchor070" class="calibre6 pcalibre1 pcalibre"/>Managing tasks and task graphs with Python</h2>
<p class="calibre3">Our<a id="_idIndexMarker247" class="calibre6 pcalibre1 pcalibre"/> primary focus is on Snowpark. Now, we’ll explore how we can utilize Python Snowpark to programmatically perform task graph operations instead of using SQL statements.</p>
<p class="calibre3">Now, Python can manage Snowflake tasks, allowing users to run SQL statements, procedure calls, and Snowflake Scripting logic. The Snowflake Python API introduces two types:</p>
<ul class="calibre15">
<li class="calibre14"><strong class="source-inline1">Task</strong>: This type represents a task’s properties, such as its schedule, parameters, and dependencies</li>
<li class="calibre14"><strong class="source-inline1">TaskResource</strong>: This type provides methods to interact with <strong class="source-inline1">Task</strong> objects, enabling task execution and modification</li>
</ul>
<p class="calibre3">Tasks can be grouped into task graphs, which consist of interconnected tasks arranged based on their dependencies. To create a task graph, users first define a DAG object, specifying its name and optional properties, such as its schedule. The scheduling of a task graph can be customized using either a <code>timedelta</code> value or a cron expression, allowing for flexible task execution timing and recurrence patterns.</p>
<p class="calibre3">Let’s begin by setting up the necessary functions to implement our DAG. The examples provided in this section presuppose that you’ve already written code to establish a connection with Snowflake to utilize the Snowflake Python API:</p>
<pre class="source-code">
from snowflake.core import Root
from snowflake.core.task import StoredProcedureCall
from snowflake.core.task.dagv1 import DAG, DAGTask, DAGOperation
from snowflake.snowpark import Session
from datetime import timedelta
root = Root(session)</pre> <p class="calibre3">The preceding code initializes the Snowflake Python API, creating a <code>root</code> object for utilizing its types and methods. Additionally, it sets up a <code>timedelta</code> value of 1 hour for the task’s schedule. You can define the schedule using either a <code>timedelta</code> value or a Cron expression. For one-off runs, you can omit the schedule argument to the DAG object without worrying about it running unnecessarily in the background.</p>
<p class="calibre3">Let’s<a id="_idIndexMarker248" class="calibre6 pcalibre1 pcalibre"/> define a simple DAG that we’ll use to execute our pipelines:</p>
<pre class="source-code">
dag = DAG("Task_Demo",
          warehouse="COMPUTE_WH",
          schedule=timedelta(days=1),
          stage_location= \
              "SNOWPARK_DEFINITIVE_GUIDE.MY_SCHEMA.MY_STAGE",
          packages=["snowflake-snowpark-python"]
          )</pre> <p class="calibre3">In this DAG setup, the following applies:</p>
<ul class="calibre15">
<li class="calibre14">We’ve named our DAG <strong class="source-inline1">Task_Demo</strong>, which by default runs on the specified warehouse.</li>
<li class="calibre14">A schedule for daily execution has been defined using <strong class="source-inline1">timedelta</strong>.</li>
<li class="calibre14">A <strong class="source-inline1">stage_location</strong> attribute is necessary for storing the serialized version of the tasks via the Python API.</li>
<li class="calibre14">All tasks under this DAG will run with the default list of packages and the specified warehouse. However, both the warehouse and packages for individual tasks within the DAG can be overridden with different values.</li>
<li class="calibre14">In addition, the <strong class="source-inline1">use_func_return_value</strong> attribute indicates that the return value of Python functions will be treated as the return value of the task, though in our case, we’re not utilizing the <strong class="source-inline1">return</strong> object.</li>
</ul>
<p class="calibre3">We’ve now defined a series of Python functions representing a three-task pipeline, or DAG. However, we haven’t yet created and pushed the DAG to Snowflake. Let’s do that now using the Snowflake Python API:</p>
<pre class="source-code">
with dag:
    data_prep_task = DAGTask("Data_Prep", definition=data_prep)
    data_transform_task = DAGTask("Data_Transform", \
        definition=data_transform)
    data_cleanup_task = DAGTask("Data_Cleanup", \
        definition=data_cleanup)
    data_prep_task &gt;&gt; data_transform_task &gt;&gt; data_cleanup_task
DAGTask</strong> objects for each task in our pipeline: <code>data_prep</code>, <code>data_transform</code>, and <code>data_cleanup</code>. These tasks are then linked together using the <code>&gt;&gt;</code> operator to specify their execution order.</pre>
<p class="calibre3">For one-off testing or running of the DAG, users can skip specifying the schedule and manually trigger a run with <code>dag_op.run(dag)</code>:</p>
<pre class="source-code">
schema = root.databases["SNOWPARK_DEFINITIVE_GUIDE"].schemas[ \
    "MY_SCHEMA"]
dag_op = DAGOperation(schema)
dag_op.deploy(dag,mode="orReplace")
dag_op.run(dag)</pre> <p class="calibre3">The provided code performs several actions using Snowflake’s Snowpark library. Firstly, it retrieves the schema named <code>MY_SCHEMA</code> from the <code>SNOWPARK_DEFINITIVE_GUIDE</code> database using the <code>root</code> object. Then, it initializes a <code>DAGOperation</code> object named <code>dag_op</code>, specifying the schema where the DAG will be deployed. The <code>deploy()</code> method is then called on <code>dag_op</code> to deploy the specified DAG (named <code>dag</code>) in the specified schema.</p>
<p class="calibre3">The <code>orReplace</code> mode argument indicates that if a DAG with the same name already exists in the schema, it will be replaced. Finally, the <code>run()</code> method is called on <code>dag_op</code> to execute the deployed DAG. This code essentially sets up and executes a DAG within Snowflake using Snowpark. Now, you can check the graph in Snowsight to see how the graph has been set up:</p>
<div><div><img alt="Figure 4.8 – Snowsight graph" src="img/B19923_04_08.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 4.8 – Snowsight graph</p>
<p class="calibre3">Additionally, note <a id="_idIndexMarker250" class="calibre6 pcalibre1 pcalibre"/>that you can pass the raw function to the dependency definition without explicitly creating a <code>DAGTask</code> instance, with the library automatically creating a task for you with the same name. However, there are exceptions to this rule, such as needing to explicitly create a <code>DAGTask</code> instance for the first task or when utilizing <code>DAGTaskBranch</code> or repeating certain functions in multiple tasks.</p>
<p class="calibre3">Once you’ve deployed and run the DAG, you can easily check its status using the following code:</p>
<pre class="source-code">
current_runs = dag_op.get_current_dag_runs(dag)
for r in current_runs:
    print(f"RunId={r.run_id} State={r.state}")</pre> <p class="calibre3">As depicted in the screenshot, our tasks are scheduled to run daily. Additionally, we have executed them once to verify the task deployment beforehand:</p>
<div><div><img alt="Figure 4.9 – Tasks deployed" src="img/B19923_04_09.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 4.9 – Tasks deployed</p>
<p class="calibre3">As we progress in constructing more complex pipelines, managing and debugging become <a id="_idIndexMarker251" class="calibre6 pcalibre1 pcalibre"/>increasingly challenging. It’s crucial to establish a robust logging mechanism to facilitate maintenance and streamline error resolution. In the next section, we’ll delve into implementing logging and traceback functionalities in Snowpark to enhance our pipeline’s manageability and ease troubleshooting.</p>
<h1 id="_idParaDest-71" class="calibre5"><a id="_idTextAnchor071" class="calibre6 pcalibre1 pcalibre"/>Implementing logging and tracing in Snowpark</h1>
<p class="calibre3">Logging and <a id="_idIndexMarker252" class="calibre6 pcalibre1 pcalibre"/>tracing are crucial for DataOps and are necessary to monitor and fix failures in the data engineering pipeline. Snowpark comes with logging and tracing functionality that is built in, which can help record the activity of Snowpark functions and procedures and capture those in an easy-to-access central table inside Snowflake. Log <a id="_idIndexMarker253" class="calibre6 pcalibre1 pcalibre"/>messages are independent, detailed messages with information in the form of strings, providing details about the piece of code, and trace events are structured data that we can use to get information spanning and grouping multiple parts of our code. Once logs are collected, they can be easily queried by SQL or accessed via Snowpark. The following diagram highlights the event table and alerting:</p>
<div><div><img alt="Figure 4.10 – Event table" src="img/B19923_04_10.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 4.10 – Event table</p>
<p class="calibre3">Snowpark stores logs and trace messages inside the event table, a unique table with a predefined set of columns. Logs and traces are captured in this table as the code is executed. Let’s look at the structure of event tables and how to create them.</p>
<h2 id="_idParaDest-72" class="calibre7"><a id="_idTextAnchor072" class="calibre6 pcalibre1 pcalibre"/>Event tables</h2>
<p class="calibre3">An event table<a id="_idIndexMarker254" class="calibre6 pcalibre1 pcalibre"/> is native to Snowflake and needs to be created. There can be only one event table for a Snowflake account that captures all the information, but multiple views can be made for analysis. An event table contains the following columns:</p>
<table class="no-table-style" id="table001-2">
<colgroup class="calibre10">
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
</colgroup>
<tbody class="calibre12">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><strong class="bold">Column</strong></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><strong class="bold">Data Type</strong></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><strong class="bold">Description</strong></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><code>TIMESTAMP</code></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><code>TIMESTAMP_NTZ</code></p>
</td>
<td class="no-table-style2">
<p class="calibre3">The UTC timestamp when an event was created. This is the end of the period for events representing a period.</p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><code>START_TIMESTAMP</code></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><code>TIMESTAMP_NTZ</code></p>
</td>
<td class="no-table-style2">
<p class="calibre3">For events representing a period, such as trace events as the start of the period as a UTC timestamp.</p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><code>OBSERVED_TIMESTAMP</code></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><code>TIMESTAMP_NTZ</code></p>
</td>
<td class="no-table-style2">
<p class="calibre3">A UTC timestamp is used for logs. Currently, it has the same value as <code>TIMESTAMP</code>.</p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><code>TRACE</code></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><code>OBJECT</code></p>
</td>
<td class="no-table-style2">
<p class="calibre3">Tracing context for all signal types. Contains <code>trace_id</code> and <code>span_id</code> string values.</p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><code>RESOURCE</code></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><code>OBJECT</code></p>
</td>
<td class="no-table-style2">
<p class="calibre3">Reserved for future use.</p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><code>RESOURCE_ATTRIBUTES</code></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><code>OBJECT</code></p>
</td>
<td class="no-table-style2">
<p class="calibre3">Attributes that identify the source of an event, such as database, schema, user, warehouse, and so on.</p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><code>SCOPE</code></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><code>OBJECT</code></p>
</td>
<td class="no-table-style2">
<p class="calibre3">Scopes for events; for example, class names for logs.</p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><code>SCOPE_ATTRIBUTES</code></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><code>OBJECT</code></p>
</td>
<td class="no-table-style2">
<p class="calibre3">Reserved for future use.</p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><code>RECORD_TYPE</code></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><code>STRING</code></p>
</td>
<td class="no-table-style2">
<p class="calibre3">The event types. One of the following:</p>
<p class="calibre3"><code>LOG</code> for a log message.</p>
<p class="calibre3"><code>SPAN</code> for UDF invocations performed sequentially on the same thread. <code>SPAN_EVENT</code> for a single trace event. A single query can emit more than one <code>SPAN_EVENT</code> event type.</p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><code>RECORD</code></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><code>OBJECT</code></p>
</td>
<td class="no-table-style2">
<p class="calibre3">Fixed values for each record type.</p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><code>RECORD_ATTRIBUTES</code></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><code>OBJECT</code></p>
</td>
<td class="no-table-style2">
<p class="calibre3">Variable attributes for each record type.</p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><code>VALUE</code></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><code>VARIANT</code></p>
</td>
<td class="no-table-style2">
<p class="calibre3">Primary event value.</p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><code>EXEMPLARS</code></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><code>ARRAY</code></p>
</td>
<td class="no-table-style2">
<p class="calibre3">Reserved for future use.</p>
</td>
</tr>
</tbody>
</table>
<p class="img---caption" lang="en-US" xml:lang="en-US">Table 4.1 – Event table columns</p>
<p class="calibre3">Each column can <a id="_idIndexMarker255" class="calibre6 pcalibre1 pcalibre"/>be queried or combined to analyze different outcomes based on the logs and the traces. The log type describes the log levels assigned as part of the logging and can be set at both objects and the session. The log levels can be the following:</p>
<table class="no-table-style" id="table002">
<colgroup class="calibre10">
<col class="calibre11"/>
<col class="calibre11"/>
</colgroup>
<tbody class="calibre12">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><strong class="bold">LOG_LEVEL </strong><strong class="bold">Parameter Setting</strong></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><strong class="bold">Levels of </strong><strong class="bold">Messages Ingested</strong></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><code>TRACE</code></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><code>TRACE</code></p>
<p class="calibre3"><code>DEBUG</code></p>
<p class="calibre3"><code>INFO</code></p>
<p class="calibre3"><code>WARN</code></p>
<p class="calibre3"><code>ERROR</code></p>
<p class="calibre3"><code>FATAL</code></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><code>DEBUG</code></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><code>DEBUG</code></p>
<p class="calibre3"><code>INFO</code></p>
<p class="calibre3"><code>WARN</code></p>
<p class="calibre3"><code>ERROR</code></p>
<p class="calibre3"><code>FATAL</code></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><code>INFO</code></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><code>INFO</code></p>
<p class="calibre3"><code>WARN</code></p>
<p class="calibre3"><code>ERROR</code></p>
<p class="calibre3"><code>FATAL</code></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><code>WARN</code></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><code>WARN</code></p>
<p class="calibre3"><code>ERROR</code></p>
<p class="calibre3"><code>FATAL</code></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><code>ERROR</code></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><code>ERROR</code></p>
<p class="calibre3"><code>FATAL</code></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><code>FATAL</code></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><code>ERROR</code></p>
<p class="calibre3"><code>FATAL</code></p>
</td>
</tr>
</tbody>
</table>
<p class="img---caption" lang="en-US" xml:lang="en-US">Table 4.2 – Event table log levels</p>
<p class="calibre3">The log level <a id="_idIndexMarker256" class="calibre6 pcalibre1 pcalibre"/>is a hierarchy applied in the order presented in <em class="italic">Table 4.2</em>. In the next section, we will look at creating an event table.</p>
<p class="callout-heading">Note</p>
<p class="callout">It is best practice to set the necessary log level based on a minor level such as <strong class="source-inline1">FATAL </strong>and ERROR so that the number of logged messages is fewer. In the case of logging <strong class="source-inline1">INFO</strong>, it is usually turned on to capture the log and turned off in production to avoid catching too many records.</p>
<h3 class="calibre9">Creating and configuring an event table</h3>
<p class="calibre3">The first step <a id="_idIndexMarker257" class="calibre6 pcalibre1 pcalibre"/>is to create an event table for the Snowflake account. The name of the event table can be specified, and columns for the event table are not required to be set when creating the table, as Snowflake automatically creates it with the standard columns. The event table is assigned to the account and needs to be made in a separate database that does not have Snowflake replication enabled using the <code>ACCOUNTADMIN</code> role.</p>
<p class="calibre3">To create an event table, run the following command with the <code>ACCOUNTADMIN</code> role using Snowpark:</p>
<pre class="source-code">
session.sql('''CREATE EVENT TABLE MY_EVENTS;''').show()</pre> <p class="calibre3">An event table called <code>MY_EVENTS</code> is created with the default column structure. The next step is to assign the event table as the active event table to a particular Snowflake account. The <a id="_idIndexMarker258" class="calibre6 pcalibre1 pcalibre"/>event table can be assigned to an account by executing the following code with the <code>ACCOUNTADMIN</code> role:</p>
<pre class="source-code">
session.sql('''ALTER ACCOUNT SET EVENT_TABLE = \
    SNOWPARK_DEFINITIVE_GUIDE.MY_SCHEMA.MY_EVENTS;
''').show()</pre> <p class="calibre3">The parameter is applied at the account level, and all events from the particular Snowflake account are captured in this event table. This completes the event table setup.</p>
<h3 class="calibre9">Querying event tables</h3>
<p class="calibre3">An<a id="_idIndexMarker259" class="calibre6 pcalibre1 pcalibre"/> event table can be accessed just like any other Snowflake table. To get the records from an event table, you can query the event table with the following code:</p>
<pre class="source-code">
session.sql('''SELECT *
    FROM SNOWPARK_DEFINITIVE_GUIDE.MY_SCHEMA.MY_EVENTS;
''').show()</pre> <p class="calibre3">It returns a result with empty records since no information was captured. Records in an event table can be filtered with a specific column to get detailed information. A Snowflake stream can be set on top of the event table to capture only new events. A Stream can be created by running the following query:</p>
<pre class="source-code">
session.sql('''CREATE STREAM EVENT_APPEND ON EVENT TABLE MY_EVENTS APPEND_ONLY=TRUE;''').show()</pre> <p class="calibre3">The <code>EVENT_APPEND</code> stream captures the latest inserted records into the event table. In the next section, we will set up logging and tracing to capture records in an event table.</p>
<h2 id="_idParaDest-73" class="calibre7"><a id="_idTextAnchor073" class="calibre6 pcalibre1 pcalibre"/>Setting up logging in Snowpark</h2>
<p class="calibre3">Introducing logging <a id="_idIndexMarker260" class="calibre6 pcalibre1 pcalibre"/>and tracing capabilities into our pipelines<a id="_idIndexMarker261" class="calibre6 pcalibre1 pcalibre"/> is akin to infusing resilience into our standard data engineering processes. This section will delve into the integration of logging functionalities into our existing data engineering pipeline. By doing so, we not only gain the ability to monitor and trace the flow of data but also enhance the robustness of our code, fortifying<a id="_idIndexMarker262" class="calibre6 pcalibre1 pcalibre"/> it against potential pitfalls. Join us as we explore how these logging capabilities elevate our data engineering <a id="_idIndexMarker263" class="calibre6 pcalibre1 pcalibre"/>practices, making them more reliable and fault-tolerant.</p>
<p class="calibre3">Logging can be enabled for both Snowpark functions and procedures. The first step in capturing logs is to set the log level in the Snowpark session. The log level in the session can be set by running the following code:</p>
<pre class="source-code">
session.sql('''alter session set log_level = INFO;''').show()</pre> <p class="calibre3">This sets the log level to <code>INFO</code> for the particular session, so all Snowpark execution that happens for the specific session is captured with the log level as information. Snowpark supports APIs to log messages directly from the handler.</p>
<h3 class="calibre9">Capturing informational logs</h3>
<p class="calibre3">We <a id="_idIndexMarker264" class="calibre6 pcalibre1 pcalibre"/>will now modify the data preparation procedures from the data pipeline to capture informational logs. We start with prepping the data:</p>
<pre class="source-code">
from snowflake.snowpark.functions import sproc
import logging
def data_prep(session: Session):
    ## Initializing Logger
    logger = logging.getLogger("My_Logger")
    logger.info("Data Preparation Pipeline Starts")
    #### Loading Required Tables
    logger.info("Loading Required Tables")
    purchase_history = session.table("PURCHASE_HISTORY")
    campaign_info = session.table("CAMPAIGN_INFO")
    complain_info = session.table("COMPLAINT_INFO")
    marketing_additional = session.table("MARKETING_ADDITIONAL")</pre> <p class="calibre3">The <a id="_idIndexMarker265" class="calibre6 pcalibre1 pcalibre"/>data from the four tables is loaded into the DataFrame. This DataFrame is then used to execute each step. Next, we will proceed with calling each step in order to process the data:</p>
<pre class="source-code">
    #### Calling Step 1
    purchase_campaign = combine_campaign_table(
        purchase_history,campaign_info)
    logger.info("Joined Purchase and Campaign Tables")
    #### Calling Step 2
    purchase_campaign_complain = combine_complain_table(
        purchase_campaign,complain_info)
    logger.info("Joined Complain Table")
    #### Calling Step 3
    final_marketing_data = union_marketing_additional_table(
        purchase_campaign_complain,marketing_additional)
    logger.info("Final Marketing Data Created")</pre> <p class="calibre3">Once all three steps are executed, we get the final marketing data ready to be loaded into a<a id="_idIndexMarker266" class="calibre6 pcalibre1 pcalibre"/> Snowflake table for consumption. The following code will load the data into a Snowflake table:</p>
<pre class="source-code">
    #### Writing Combined Data To New Table
    final_marketing_data.write.save_as_table( \
        "FINAL_MARKETING_DATA")
    logger.info("Final Marketing Data Table Created")
    return "LOADED FINAL MARKETING DATA TABLE"</pre> <p class="calibre3">The data is loaded into a table called <code>FINAL_MARKETING_DATA</code>. The table is automatically created with the data in the Snowpark DataFrame. We will now register this as a Snowpark stored procedure:</p>
<pre class="source-code">
## Register Stored Procedure in Snowflake
### Add packages and data types
from snowflake.snowpark.types import StringType
session.add_packages('snowflake-snowpark-python')
### Upload Stored Procedure to Snowflake
session.sproc.register(
    func = data_prep
  , return_type = StringType()
  , input_types = []
  , is_permanent = True
  , name = 'DATA_PREP_SPROC_LOG'
  , replace = True
  , stage_location = '@MY_STAGE'
)</pre> <p class="calibre3">The <code>logging</code> module from Python’s standard library is used for logging. The package is imported, and the logger’s name is specified as <code>My_Logger</code>. Different logger names can be set for<a id="_idIndexMarker267" class="calibre6 pcalibre1 pcalibre"/> processes to help identify the particular logging application. We can execute the stored procedure to capture logs to the event table. The stored procedure can be executed by running the following command:</p>
<pre class="source-code">
session.sql(''' Call DATA_PREP_SPROC_LOG()''').show()</pre> <p class="calibre3">The stored procedure is executed, and we can see the successful output of the execution:</p>
<div><div><img alt="Figure 4.11 – Stored procedure execution" src="img/B19923_04_11.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 4.11 – Stored procedure execution</p>
<p class="calibre3">The following section will cover how to query logs generated by the stored procedure.</p>
<h3 class="calibre9">Querying informational logs</h3>
<p class="calibre3">Logs <a id="_idIndexMarker268" class="calibre6 pcalibre1 pcalibre"/>generated by the stored procedure can be accessed from the event table. Records captured from the previous stored procedure can be accessed by running the following query:</p>
<pre class="source-code">
session.sql("""
    SELECT RECORD['severity_text'] AS SEVERITY,
        VALUE AS MESSAGE
    FROM MY_EVENTS
    WHERE SCOPE['name'] = 'My_Logger'
    AND RECORD_TYPE = 'LOG'
""").show()</pre> <p class="calibre3">We filter logs captured only from <code>My_Logger</code> by specifying it in the scope. The query returns the following records that were generated from executing the stored procedure:</p>
<div><div><img alt="Figure 4.12 – Querying logs" src="img/B19923_04_12.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 4.12 – Querying logs</p>
<p class="calibre3">In the<a id="_idIndexMarker269" class="calibre6 pcalibre1 pcalibre"/> next section, we will set up logging to capture error messages and handle exceptions in Snowpark.</p>
<h2 id="_idParaDest-74" class="calibre7"><a id="_idTextAnchor074" class="calibre6 pcalibre1 pcalibre"/>Handling exceptions in Snowpark</h2>
<p class="calibre3">Exception handling<a id="_idIndexMarker270" class="calibre6 pcalibre1 pcalibre"/> is integral to data pipelines as it helps identify and handle issues. Exception handling can be done by catching exceptions<a id="_idIndexMarker271" class="calibre6 pcalibre1 pcalibre"/> thrown from within the <code>try</code> block and capturing those error logs. The <code>ERROR</code> and <code>WARN</code> log levels are often used when capturing exceptions, and fatal issues are logged at the <code>FATAL</code> level. This section will look at capturing error logs and handling the exception on the pipeline.</p>
<h3 class="calibre9">Capturing error logs</h3>
<p class="calibre3">We<a id="_idIndexMarker272" class="calibre6 pcalibre1 pcalibre"/> will modify the data transformation stored procedure to capture error logs and add exception handling:</p>
<pre class="source-code">
def data_transform(session: Session):
    try:
        ## Initializing Logger
        logger = logging.getLogger("Data_Transform_Logger")
        logger.info("Data Transformation Pipeline Starts")
        ## Pivoting Process
        marketing_final = session.table("FINAL_MARKETING_DATA")
        market_subset = marketing_final.select("EDUCATION", \
            "MARITAL_STATUS","INCOME")
        market_pivot = market_subset.pivot("EDUCATION", \
            ["Graduation","PhD","Master","Basic","2n Cycle"]
        ).sum("INCOME")
        #### Writing Transformed Data To New Table
        market_pivot.write.save_as_table("MAREKTING_PIVOT")
        logger.log("MARKETING PIVOT TABLE CREATED")
        return "CREATED MARKETING PIVOT TABLE"
    except Exception as err:
        logger.error("Logging an error from Python handler: ")
        logger.error(err)
        return "ERROR"</pre> <p class="calibre3">The data<a id="_idIndexMarker273" class="calibre6 pcalibre1 pcalibre"/> transformation logic is moved into the <code>try</code> block, and a logger named <code>Data_Transform_Logger</code> is initiated. The exception raised by the code is captured in the exception object defined as <code>err</code>. This is then logged in the event table by the <code>ERROR</code> log level. We will now register this stored procedure in Snowpark:</p>
<pre class="source-code">
## Register Stored Procedure in Snowflake
### Add packages and data types
from snowflake.snowpark.types import StringType
session.add_packages('snowflake-snowpark-python')
### Upload Stored Procedure to Snowflake
session.sproc.register(
    func = data_transform
  , return_type = StringType()
  , input_types = []
  , is_permanent = True
  , name = 'DATA_TRANSFORM_SPROC_LOG_ERROR'
  , replace = True   , stage_locations = "@MY_STAGE" )</pre> <p class="calibre3">We will <a id="_idIndexMarker274" class="calibre6 pcalibre1 pcalibre"/>execute the stored procedure purposely with the error to log the error. The stored procedure can be triggered by running the following command:</p>
<pre class="source-code">
session.sql(''' Call DATA_TRANSFORM_SPROC_LOG_ERROR()''').show()</pre> <p class="calibre3">The stored procedure has raised an exception, and the error has been captured in the event table:</p>
<div><div><img alt="Figure 4.13 – Error execution" src="img/B19923_04_13.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 4.13 – Error execution</p>
<p class="calibre3">The following section will cover querying error logs generated by the procedure.</p>
<h3 class="calibre9">Querying error logs</h3>
<p class="calibre3">The <a id="_idIndexMarker275" class="calibre6 pcalibre1 pcalibre"/>error log records from the preceding stored procedure are captured under the <code>Data_Transform_Logger</code> logger, which can be accessed by filtering the query to return logs specific to the logger. The following <a id="_idIndexMarker276" class="calibre6 pcalibre1 pcalibre"/>query can be executed to get the records from the event table:</p>
<pre class="source-code">
session.sql("""
    SELECT RECORD['severity_text'] AS SEVERITY,VALUE AS MESSAGE
    FROM MY_EVENTS
    WHERE SCOPE['name'] = 'Data_Transform_Logger'
    AND RECORD_TYPE = 'LOG'
""").collect()</pre> <p class="calibre3">The scope name is filtered as the <code>Data_Transform_Logger</code> to get the results, and errors caused by the exception are logged under <code>SEVERITY</code> as <code>ERROR</code>:</p>
<div><div><img alt="Figure 4.14 – Error log messages" src="img/B19923_04_14.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 4.14 – Error log messages</p>
<p class="calibre3">Snowpark makes debugging easy by supporting logging errors and handling exceptions through event tables. The following section will cover event traces and how to capture trace information in Snowpark.</p>
<h2 id="_idParaDest-75" class="calibre7"><a id="_idTextAnchor075" class="calibre6 pcalibre1 pcalibre"/>Setting up tracing in Snowpark</h2>
<p class="calibre3">Trace events<a id="_idIndexMarker277" class="calibre6 pcalibre1 pcalibre"/> are a type of telemetry data that is captured when something has happened in the code. It has a structured payload that helps analyze the<a id="_idIndexMarker278" class="calibre6 pcalibre1 pcalibre"/> trace by aggregating this information to understand the code’s behavior at a high level. When a procedure or function executes, trace events are emitted, which are available in the active event table.</p>
<p class="calibre3">The first step in capturing events is to turn on the tracing functionality by setting the trace level in the Snowpark session. The trace level in the session can be set by running the following code:</p>
<pre class="source-code">
session.sql("ALTER SESSION SET TRACE_LEVEL = ALWAYS;").show()</pre> <p class="calibre3">In the next section, we will look at capturing traces.</p>
<h3 class="calibre9">Capturing traces</h3>
<p class="calibre3">Traces<a id="_idIndexMarker279" class="calibre6 pcalibre1 pcalibre"/> can be captured with the open source Snowflake <code>telemetry</code> Python package available in the Anaconda Snowflake channel. The package needs to be imported into the code, and it will be executed in Snowpark. The <code>telemetry</code> package can be imported by including the code in the Snowpark handler:</p>
<pre class="source-code">
from snowflake import telemetry</pre> <p class="calibre3">The <code>telemetry</code> package helped capture traces generated on the code and logged into the event table. We will modify the data cleanup procedure by adding telemetry events to capture the trace:</p>
<pre class="source-code">
def data_cleanup(session: Session):
    #### Loading Telemetry Package
    from snowflake import telemetry
    #### Loading Required Tables
    market_pivot = session.table("MARKETING_PIVOT")
    #### Adding Trace Event
    telemetry.add_event("data_cleanup", \
        {"table_name": "MARKETING_PIVOT", \
         "count": market_pivot.count()})
    #### Dropping Null
    market_drop_null = market_pivot.dropna(thresh=5)
    #### Writing Cleaned Data To New Table
    market_drop_null.write.save_as_table("MARKET_PIVOT_CLEANED")
    #### Adding Trace Event
    telemetry.add_event("data_cleanup", \
        {"table_name": "MARKET_PIVOT_CLEANED", \
         "count": market_drop_null.count()})
    return "CREATED CLEANED TABLE"
###########################################################
## Register Stored Procedure in Snowflake
### Add packages and data types
from snowflake.snowpark.types import StringType
session.add_packages('snowflake-snowpark-python', \
    'snowflake-telemetry-python')
### Upload Stored Procedure to Snowflake
session.sproc.register(
    func = data_cleanup
  , return_type = StringType()
  , input_types = []
  , is_permanent = True
  , name = 'DATA_CLEANUP_SPROC_TRACE'
  , replace = True
  , stage_location = '@MY_STAGE'
)</pre> <p class="calibre3">The <a id="_idIndexMarker280" class="calibre6 pcalibre1 pcalibre"/>procedure is now ready to be executed. We are passing attributes for the operations captured on the traces. We can execute the procedure by running the following code:</p>
<pre class="source-code">
session.sql(''' Call DATA_CLEANUP_SPROC_TRACE()''').show()</pre> <p class="calibre3">The procedure is executed, and the data is cleaned from the table:</p>
<div><div><img alt="Figure 4.15 – Data cleanup procedure execution" src="img/B19923_04_15.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 4.15 – Data cleanup procedure execution</p>
<p class="calibre3">The traces are now generated in the event table. We can directly query the event table to obtain trace information.</p>
<h3 class="calibre9">Querying traces</h3>
<p class="calibre3">Traces <a id="_idIndexMarker281" class="calibre6 pcalibre1 pcalibre"/>generated can be accessed from the event table. Traces captured from the previous stored procedure can be accessed by running the following query:</p>
<pre class="source-code">
session.sql("""
    SELECT
        TIMESTAMP as time,
        RESOURCE_ATTRIBUTES['snow.executable.name']
            as handler_name,
        RESOURCE_ATTRIBUTES['snow.executable.type']
            as handler_type,
        RECORD['name'] as event_name,
        RECORD_ATTRIBUTES as attributes
    FROM
        MY_EVENTS
    WHERE
        EVENT_NAME ='data_cleanup'
""").show(2)</pre> <p class="calibre3">We are<a id="_idIndexMarker282" class="calibre6 pcalibre1 pcalibre"/> querying using the <code>data_cleanup</code> event name. This returns the two traces captured when the code was executed:</p>
<div><div><img alt="Figure 4.16 – Trace capture information" src="img/B19923_04_16.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 4.16 – Trace capture information</p>
<p class="calibre3">We can see the attributes that are captured from the execution of the stored procedure. After the <code>NULL</code> values have been cleaned up, the details return the total data counts, including the <code>NULL</code> and <code>count</code> values.</p>
<h2 id="_idParaDest-76" class="calibre7"><a id="_idTextAnchor076" class="calibre6 pcalibre1 pcalibre"/>Comparison of logs and traces</h2>
<p class="calibre3">The following table compares logs and traces and lists scenarios to use each:</p>
<table class="no-table-style" id="table003">
<colgroup class="calibre10">
<col class="calibre11"/>
<col class="calibre11"/>
<col class="calibre11"/>
</colgroup>
<tbody class="calibre12">
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3"><strong class="bold">Characteristic</strong></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><strong class="bold">Log entries</strong></p>
</td>
<td class="no-table-style2">
<p class="calibre3"><strong class="bold">Trace events</strong></p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3">Intended use</p>
</td>
<td class="no-table-style2">
<p class="calibre3">Record detailed but unstructured information about the state of your code. Use this information to understand what happened during a particular invocation of your function or procedure.</p>
</td>
<td class="no-table-style2">
<p class="calibre3">Record a brief but structured summary of each invocation of your code. Aggregate this information to understand the behavior of your code at a high level.</p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3">Structure as a payload</p>
</td>
<td class="no-table-style2">
<p class="calibre3">None. A log entry is just a string.</p>
</td>
<td class="no-table-style2">
<p class="calibre3">Structured with attributes you can attach to trace events. Attributes are key-value pairs that can be easily queried with a SQL query.</p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3">Supports grouping</p>
</td>
<td class="no-table-style2">
<p class="calibre3">No. Each log entry is an independent event.</p>
</td>
<td class="no-table-style2">
<p class="calibre3">Yes. Trace events are organized into spans. A span can have its attributes.</p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3">Quantity limits</p>
</td>
<td class="no-table-style2">
<p class="calibre3">Unlimited. All log entries emitted by your code are ingested into the event table.</p>
</td>
<td class="no-table-style2">
<p class="calibre3">The number of trace events per span is capped at 128. There is also a limit on the number of span attributes.</p>
</td>
</tr>
<tr class="no-table-style1">
<td class="no-table-style2">
<p class="calibre3">Complexity of queries against recorded data</p>
</td>
<td class="no-table-style2">
<p class="calibre3">Relatively high. Your queries must parse each log entry to extract meaningful information from it.</p>
</td>
<td class="no-table-style2">
<p class="calibre3">Relatively low. Your queries can take advantage of the structured nature of trace events.</p>
</td>
</tr>
</tbody>
</table>
<p class="img---caption" lang="en-US" xml:lang="en-US">Table 4.3 – Differences between logs and traces</p>
<p class="calibre3">Logs and traces help debug Snowpark and are a practical feature for efficient DataOps.</p>
<h1 id="_idParaDest-77" class="calibre5"><a id="_idTextAnchor077" class="calibre6 pcalibre1 pcalibre"/>Summary</h1>
<p class="calibre3">In this chapter, we covered in detail building and deploying resilient data pipelines in Snowpark and also how to enable logging and tracking using event tables. Building resilient data pipelines with effective DataOps is vital for a successful data strategy. Snowpark supports the development of a modern data pipeline through a programmatic ELT approach along with features such as logging and tracing, making it easy for developers to implement DataOps. We also covered how tasks and task graphs can be used in scheduling and deploying pipelines.</p>
<p class="calibre3">In the next chapter, we will cover using Snowpark to develop data science workloads.</p>
</div>
</body></html>