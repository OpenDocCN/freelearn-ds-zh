["```py\n> library(tm)\n> getSources()\n[1] \"DataframeSource\" \"DirSource\"  \"ReutersSource\"   \"URISource\"\n[2] \"VectorSource\" \n\n```", "```py\n> getReaders()\n[1] \"readDOC\"                 \"readPDF\" \n[3] \"readPlain\"               \"readRCV1\" \n[5] \"readRCV1asPlain\"         \"readReut21578XML\" \n[7] \"readReut21578XMLasPlain\" \"readTabular\" \n[9] \"readXML\" \n\n```", "```py\n> res <- XML::readHTMLTable(paste0('http://cran.r-project.org/',\n+                   'web/packages/available_packages_by_name.html'),\n+               which = 1)\n\n```", "```py\n> v <- Corpus(VectorSource(res$V2))\n\n```", "```py\n> v\n<<VCorpus (documents: 5880, metadata (corpus/indexed): 0/0)>>\n\n```", "```py\n> inspect(head(v, 3))\n<<VCorpus (documents: 3, metadata (corpus/indexed): 0/0)>>\n\n[[1]]\n<<PlainTextDocument (metadata: 7)>>\nA3: Accurate, Adaptable, and Accessible Error Metrics for\nPredictive Models\n\n[[2]]\n<<PlainTextDocument (metadata: 7)>>\nTools for Approximate Bayesian Computation (ABC)\n\n[[3]]\n<<PlainTextDocument (metadata: 7)>>\nABCDE_FBA: A-Biologist-Can-Do-Everything of Flux Balance\nAnalysis with this package\n\n```", "```py\n> getTransformations()\n[1] \"as.PlainTextDocument\" \"removeNumbers\"\n[3] \"removePunctuation\"    \"removeWords\"\n[5] \"stemDocument\"         \"stripWhitespace\" \n\n```", "```py\n> stopwords(\"english\")\n [1] \"i\"          \"me\"         \"my\"         \"myself\"     \"we\" \n [6] \"our\"        \"ours\"       \"ourselves\"  \"you\"        \"your\" \n [11] \"yours\"      \"yourself\"   \"yourselves\" \"he\"         \"him\" \n [16] \"his\"        \"himself\"    \"she\"        \"her\"        \"hers\" \n [21] \"herself\"    \"it\"         \"its\"        \"itself\"     \"they\" \n [26] \"them\"       \"their\"      \"theirs\"     \"themselves\" \"what\" \n [31] \"which\"      \"who\"        \"whom\"       \"this\"       \"that\" \n [36] \"these\"      \"those\"      \"am\"         \"is\"         \"are\" \n [41] \"was\"        \"were\"       \"be\"         \"been\"       \"being\" \n [46] \"have\"       \"has\"        \"had\"        \"having\"     \"do\" \n [51] \"does\"       \"did\"        \"doing\"      \"would\"      \"should\" \n [56] \"could\"      \"ought\"      \"i'm\"        \"you're\"     \"he's\" \n [61] \"she's\"      \"it's\"       \"we're\"      \"they're\"    \"i've\" \n [66] \"you've\"     \"we've\"      \"they've\"    \"i'd\"        \"you'd\" \n [71] \"he'd\"       \"she'd\"      \"we'd\"       \"they'd\"     \"i'll\" \n [76] \"you'll\"     \"he'll\"      \"she'll\"     \"we'll\"      \"they'll\" \n [81] \"isn't\"      \"aren't\"     \"wasn't\"     \"weren't\"    \"hasn't\" \n [86] \"haven't\"    \"hadn't\"     \"doesn't\"    \"don't\"      \"didn't\" \n [91] \"won't\"      \"wouldn't\"   \"shan't\"     \"shouldn't\"  \"can't\" \n [96] \"cannot\"     \"couldn't\"   \"mustn't\"    \"let's\"      \"that's\" \n[101] \"who's\"      \"what's\"     \"here's\"     \"there's\"    \"when's\" \n[106] \"where's\"    \"why's\"      \"how's\"      \"a\"          \"an\" \n[111] \"the\"        \"and\"        \"but\"        \"if\"         \"or\" \n[116] \"because\"    \"as\"         \"until\"      \"while\"      \"of\" \n[121] \"at\"         \"by\"         \"for\"        \"with\"       \"about\" \n[126] \"against\"    \"between\"    \"into\"       \"through\"    \"during\" \n[131] \"before\"     \"after\"      \"above\"      \"below\"      \"to\" \n[136] \"from\"       \"up\"         \"down\"       \"in\"         \"out\" \n[141] \"on\"         \"off\"        \"over\"       \"under\"      \"again\" \n[146] \"further\"    \"then\"       \"once\"       \"here\"       \"there\" \n[151] \"when\"       \"where\"      \"why\"        \"how\"        \"all\" \n[156] \"any\"        \"both\"       \"each\"       \"few\"        \"more\" \n[161] \"most\"       \"other\"      \"some\"       \"such\"       \"no\" \n[166] \"nor\"        \"not\"        \"only\"       \"own\"        \"same\" \n[171] \"so\"         \"than\"       \"too\"        \"very\" \n\n```", "```py\n> removeWords('to be or not to be', stopwords(\"english\"))\n[1] \"     \"\n\n```", "```py\n> v <- tm_map(v, removeWords, stopwords(\"english\"))\n\n```", "```py\n> inspect(head(v, 3))\n<<VCorpus (documents: 3, metadata (corpus/indexed): 0/0)>>\n\n[[1]]\n<<PlainTextDocument (metadata: 7)>>\nA3 Accurate Adaptable Accessible Error Metrics Predictive Models\n\n[[2]]\n<<PlainTextDocument (metadata: 7)>>\nTools Approximate Bayesian Computation ABC\n\n[[3]]\n<<PlainTextDocument (metadata: 7)>>\nABCDEFBA ABiologistCanDoEverything Flux Balance Analysis package\n\n```", "```py\n> removeWords('To be or not to be.', stopwords(\"english\"))\n[1] \"To     .\"\n\n```", "```py\n> v <- tm_map(v, content_transformer(tolower))\n> v <- tm_map(v, removePunctuation)\n> v <- tm_map(v, stripWhitespace)\n> inspect(head(v, 3))\n<<VCorpus (documents: 3, metadata (corpus/indexed): 0/0)>>\n\n[[1]]\n[1] a3 accurate adaptable accessible error metrics predictive models\n\n[[2]]\n[1] tools approximate bayesian computation abc\n\n[[3]]\n[1] abcdefba abiologistcandoeverything flux balance analysis package\n\n```", "```py\n> wordcloud::wordcloud(v)\n\n```", "```py\n> v <- tm_map(v, removeNumbers)\n\n```", "```py\n> tdm <- TermDocumentMatrix(v)\n\n```", "```py\n> inspect(tdm[1:5, 1:20])\n<<TermDocumentMatrix (terms: 5, documents: 20)>>\nNon-/sparse entries: 5/95\nSparsity           : 95%\nMaximal term length: 14\nWeighting          : term frequency (tf)\n\n Docs\nTerms            1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\n aalenjohansson 0 0 0 0 0 0 0 0 0  0  0  0  0  0  0  0  0  0  0  0\n abc            0 1 0 1 1 0 1 0 0  0  0  0  0  0  0  0  0  0  0  0\n abcdefba       0 0 1 0 0 0 0 0 0  0  0  0  0  0  0  0  0  0  0  0\n abcsmc         0 0 0 0 0 0 0 0 0  0  0  0  0  0  0  0  0  0  0  0\n aberrations    0 0 0 0 0 0 0 0 0  0  0  0  0  0  0  0  0  0  0  0\n\n```", "```py\n> findFreqTerms(tdm, lowfreq = 100)\n [1] \"analysis\"     \"based\"        \"bayesian\"     \"data\" \n [5] \"estimation\"   \"functions\"    \"generalized\"  \"inference\" \n [9] \"interface\"    \"linear\"       \"methods\"      \"model\" \n[13] \"models\"       \"multivariate\" \"package\"      \"regression\" \n[17] \"series\"       \"statistical\"  \"test\"         \"tests\" \n[21] \"time\"         \"tools\"        \"using\" \n\n```", "```py\n> myStopwords <- c('package', 'based', 'using')\n> v <- tm_map(v, removeWords, myStopwords)\n\n```", "```py\n> library(SnowballC)\n> wordStem(c('cats', 'mastering', 'modelling', 'models', 'model'))\n[1] \"cat\"    \"master\" \"model\"  \"model\"  \"model\"\n\n```", "```py\n> wordStem(c('are', 'analyst', 'analyze', 'analysis'))\n[1] \"ar\"      \"analyst\" \"analyz\"  \"analysi\"\n\n```", "```py\n> d <- v\n\n```", "```py\n> v <- tm_map(v, stemDocument, language = \"english\")\n\n```", "```py\n> v <- tm_map(v, content_transformer(function(x, d) {\n+         paste(stemCompletion(\n+                 strsplit(stemDocument(x), ' ')[[1]],\n+                 d),\n+         collapse = ' ')\n+       }), d)\n\n```", "```py\n> tdm <- TermDocumentMatrix(v)\n> findFreqTerms(tdm, lowfreq = 100)\n [1] \"algorithm\"     \"analysing\"     \"bayesian\"      \"calculate\" \n [5] \"cluster\"       \"computation\"   \"data\"          \"distributed\" \n [9] \"estimate\"      \"fit\"           \"function\"      \"general\" \n[13] \"interface\"     \"linear\"        \"method\"        \"model\" \n[17] \"multivariable\" \"network\"       \"plot\"          \"random\" \n[21] \"regression\"    \"sample\"        \"selected\"      \"serial\" \n[25] \"set\"           \"simulate\"      \"statistic\"     \"test\" \n[29] \"time\"          \"tool\"          \"variable\" \n\n```", "```py\n> tdm\n<<TermDocumentMatrix (terms: 4776, documents: 5880)>>\nNon-/sparse entries: 27946/28054934\nSparsity           : 100%\nMaximal term length: 35\nWeighting          : term frequency (tf)\n\n```", "```py\n> findAssocs(tdm, 'data', 0.1)\n data\nset          0.17\nanalyzing    0.13\nlongitudinal 0.11\nbig          0.10\n\n```", "```py\n> findAssocs(tdm, 'big', 0.1)\n big\nmpi           0.38\npbd           0.33\nprogram       0.32\nunidata       0.19\ndemonstration 0.17\nnetcdf        0.15\nforest        0.13\npackaged      0.13\nbase          0.12\ndata          0.10\n\n```", "```py\n> vnchar <- sapply(v, function(x) nchar(x$content))\n> summary(vnchar)\n Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 2.00   27.00   37.00   39.85   50.00  168.00\n\n```", "```py\n> (vm <- which.min(vnchar))\n[1] 221\n\n```", "```py\n> v[[vm]]\n<<PlainTextDocument (metadata: 7)>>\nNA\n> res[vm, ]\n V1   V2\n221    <NA>\n\n```", "```py\n> hist(vnchar, main = 'Length of R package descriptions',\n+     xlab = 'Number of characters')\n\n```", "```py\n> hadleyverse <- c('ggplot2', 'dplyr', 'reshape2', 'lubridate',\n+   'stringr', 'devtools', 'roxygen2', 'tidyr')\n\n```", "```py\n> (w <- which(res$V1 %in% hadleyverse))\n[1] 1104 1230 1922 2772 4421 4658 5409 5596\n\n```", "```py\n> plot(hclust(dist(DocumentTermMatrix(v[w]))),\n+   xlab = 'Hadleyverse packages')\n\n```", "```py\n> sapply(v[w], function(x) structure(content(x),\n+   .Names = meta(x, 'id')))\n devtools \n \"tools make developing r code easier\" \n dplyr \n \"a grammar data manipulation\" \n ggplot2 \n \"an implementation grammar graphics\" \n lubridate \n \"make dealing dates little easier\" \n reshape2 \n \"flexibly reshape data reboot reshape \" \n roxygen2 \n \"insource documentation r\" \n stringr \n \"make easier work strings\" \n tidyr \n\"easily tidy data spread gather functions\"\n\n```"]