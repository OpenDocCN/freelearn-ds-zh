<html><head></head><body><div class="chapter" title="Chapter&#xA0;4.&#xA0;SciPy for Numerical Analysis"><div class="titlepage"><div><div><h1 class="title"><a id="ch04"/>Chapter 4. SciPy for Numerical Analysis</h1></div></div></div><p>Practically all the different areas of numerical analysis are contemplated in some SciPy module. For example, in order to compute values of special functions, we use the <code class="literal">scipy.special</code> module. The <code class="literal">scipy.interpolate</code> module takes care of interpolation, extrapolation, and <a id="id158" class="indexterm"/>regression. For optimization, we have the <a id="id159" class="indexterm"/>
<code class="literal">scipy.optimize</code> module, and finally, we have the <code class="literal">scipy.integrate</code> module for numerical evaluation of integrals. This last module serves as the interface to perform numerical solutions of ordinary differential equations as well.</p><p>Thus, in this chapter, we will first extensively explore how to use SciPy to numerically evaluate the special functions that are commonly found in the field of mathematical physics. Then, we will discuss the modules available in SciPy to tackle regression, interpolation, and optimization problems.</p><p>The chapter ends with a solution of the chaotic Lorenz system as an illustration of the capabilities included in SciPy to find numerical solutions of ordinary differential equations. The corresponding IPython Notebook will help you to try the functionalities of the modules involved in the computations and to modify each illustrative example according to your specific needs.</p><div class="section" title="The evaluation of special functions"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec28"/>The evaluation of special functions</h1></div></div></div><p>The <code class="literal">scipy.special</code> module contains numerically stable definitions of useful functions. Most often, the <a id="id160" class="indexterm"/>straightforward evaluation of a function at a single value is not <a id="id161" class="indexterm"/>very efficient. For instance, we would rather use a Horner scheme (<a class="ulink" href="http://en.wikipedia.org/wiki/Horner%27s_method">http://en.wikipedia.org/wiki/Horner%27s_method</a>) to find the value of a polynomial at a point than use the raw formula. The NumPy and SciPy modules ensure that this optimization is always guaranteed with the definition of all its functions, whether by means of Horner schemes or with more advanced techniques.</p></div></div>
<div class="section" title="Convenience and test functions"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec29"/>Convenience and test functions</h1></div></div></div><p>All the <a id="id162" class="indexterm"/>convenience functions are designed to facilitate a computational environment where the user does not need to worry about relative errors. The functions seem to be pointless at first sight, but behind their codes, there are state-of-the-art ideas that offer faster and more reliable results.</p><p>We have <a id="id163" class="indexterm"/>convenience functions beyond the ones defined in the NumPy libraries to find the solutions of trigonometric functions in degrees (<code class="literal">cosdg</code>, <code class="literal">sindg</code>, <code class="literal">tandg</code>, and <code class="literal">cotdg</code>); to compute angles in radians from their expressions in degrees, minutes, and seconds (<code class="literal">radian</code>); common powers (<code class="literal">exp2</code> for <span class="emphasis"><em>2**x</em></span>, and <code class="literal">exp10</code> for <span class="emphasis"><em>10**x</em></span>); and common functions for small values of the variable (<code class="literal">log1p</code> for <span class="emphasis"><em>log(1 + x)</em></span>, <code class="literal">expm1</code> for <span class="emphasis"><em>exp(x) - 1</em></span>, and <code class="literal">cosm1</code> for <span class="emphasis"><em>cos(x) - 1</em></span>).</p><p>For instance, in the following code snippet, <code class="literal">the log1p</code> function computes the natural logarithm of <span class="emphasis"><em>1 + x</em></span>. Why not simply add 1 to the value of <span class="emphasis"><em>x</em></span> and then take the logarithm instead? Let's compare:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; import numpy</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; import scipy.special</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; a=scipy.special.exp10(-16)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; numpy.log(1+a)</strong></span>
</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>0.0</strong></span>
</pre></div><p>Now let's use <code class="literal">log1p()</code> on <code class="literal">a</code>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; scipy.special.log1p(a)</strong></span>
</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>9.9999999999999998e-17</strong></span>
</pre></div><p>While the absolute error of the first computation is small, the relative error is 100 percent.</p><p>In the same way as Lena image is regarded as the performance test in image processing, we have a few functions that are used to test different algorithms in different scenarios. </p><p>For instance, it is customary to test minimization codes against the Rosenbrock's banana function (<a class="ulink" href="http://en.wikipedia.org/wiki/Rosenbrock_function">http://en.wikipedia.org/wiki/Rosenbrock_function</a>):</p><div class="mediaobject"><img src="graphics/7702OS_04_01.jpg" alt="Convenience and test functions"/></div><p>The corresponding optimization module, <code class="literal">scipy.optimize</code>, has a routine to accurately evaluate this function (<code class="literal">rosen</code>), its derivative (<code class="literal">rosen_der</code>), its <span class="strong"><strong>Hessian</strong></span> matrix (<code class="literal">rosen_hess</code>), or the <a id="id164" class="indexterm"/>product of the latter <a id="id165" class="indexterm"/>with a vector (<code class="literal">rosen_hess_prod</code>).</p></div>
<div class="section" title="Univariate polynomials"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec30"/>Univariate polynomials</h1></div></div></div><p>Polynomials <a id="id166" class="indexterm"/>are defined in SciPy as a NumPy class, <code class="literal">poly1d</code>. This class has a handful of methods associated to compute the coefficients of the polynomial (<code class="literal">coeffs</code> or simply <code class="literal">c</code>), to compute the roots of the polynomial (<code class="literal">r</code>), to compute its derivative (<code class="literal">deriv</code>), to compute the symbolic integral (<code class="literal">integ</code>), and to obtain the degree (<code class="literal">order</code> or simply <code class="literal">o</code>), as well as a method (<code class="literal">variable</code>) that provides a string holding the name of the variable we would like to use in the proper definition of the polynomial (see the example involving <code class="literal">P2</code>).</p><p>In order to define a polynomial, we must indicate either its coefficients or its roots:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; import numpy</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; P1=numpy.poly1d([1,0,1])           # using coefficients</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print (P1)</strong></span>
</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>   2</strong></span>
<span class="strong"><strong>1 x + 1</strong></span>
</pre></div><p>Now let's find roots, order, and derivative of <code class="literal">P1</code>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; print (P1.r); print (P1.o); print (P1.deriv())</strong></span>
</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>[ 0.+1.j  0.-1.j]</strong></span>
<span class="strong"><strong>2</strong></span>
<span class="strong"><strong>2 x</strong></span>
</pre></div><p>Let's use the <code class="literal">poly1d</code> class:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; P2=numpy.poly1d([1,1,1], True)     # using roots</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print (P2)</strong></span>
</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>   3     2</strong></span>
<span class="strong"><strong>1 x - 3 x + 3 x – 1</strong></span>
</pre></div><p>Let's use the <code class="literal">poly1d</code> class with the <code class="literal">variable</code> method:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; P2=numpy.poly1d([1,1,1], True, variable='z')</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print (P2)</strong></span>
</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>   3     2 </strong></span>
<span class="strong"><strong>1 z - 3 z + 3 z - 1</strong></span>
</pre></div><p>We may evaluate polynomials by treating them either as (vectorized) functions, or with the <code class="literal">__call__ method</code>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; P1( numpy.arange(10) )           # evaluate at 0,1,...,9</strong></span>
</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>array([ 1,  2,  5, 10, 17, 26, 37, 50, 65, 82])</strong></span>
</pre></div><p>Let's issue the <code class="literal">__call__</code> command:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; P1.__call__(numpy.arange(10))    # same evaluation</strong></span>
</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>array([ 1,  2,  5, 10, 17, 26, 37, 50, 65, 82])</strong></span>
</pre></div><p>An immediate <a id="id167" class="indexterm"/>application of these ideas is to verify the computation of the natural logarithm of <span class="emphasis"><em>1 + x</em></span> used in the preceding example . When <span class="emphasis"><em>x</em></span> is close to zero, the natural logarithm can be approximated by the following formula:</p><div class="mediaobject"><img src="graphics/7702OS_04_02.jpg" alt="Univariate polynomials"/></div><p>This expression can be entered and evaluated in Python using the ideas just presented, as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; import numpy</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; Px=numpy.poly1d([-(1./2.),1,0])</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print(Px)</strong></span>
</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>      2</strong></span>
<span class="strong"><strong>-0.5 x + 1 x</strong></span>
</pre></div><p>Let's have a look on the value stored in variable <code class="literal">a</code>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; a=1./10000000000000000.</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print(a)</strong></span>
</pre></div><p>The output for value stored in <code class="literal">a</code> is as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>1e-16</strong></span>
</pre></div><p>We now use <code class="literal">Px</code> (which contains one-dimensional polynomial form) on <code class="literal">a</code> in the following line of code:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; Px(a)</strong></span>
</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>9.9999999999999998e-17</strong></span>
</pre></div><p>The result is the same as that obtained before using the SciPy function <code class="literal">scipy.special.log1p</code>, which verifies the computation.</p><p>There are also a few routines associated with polynomials: <code class="literal">roots</code> (to compute zeros), <code class="literal">polyder</code> (to compute derivatives), <code class="literal">polyint</code> (to compute integrals), <code class="literal">polyadd</code> (to add polynomials), <code class="literal">polysub</code> (to subtract polynomials), <code class="literal">polymul</code> (to multiply polynomials), <code class="literal">polydiv</code> (to perform polynomial division), <code class="literal">polyval</code> (to evaluate polynomials), and <code class="literal">polyfit</code> (to compute the best fit polynomial of certain order for two given arrays of data).</p><p>The usual binary <a id="id168" class="indexterm"/>operators +, -, *, and / perform the corresponding operations with polynomials. In addition, once a polynomial is created, any list of values that interacts with them is immediately casted to a polynomial. Therefore, the following four commands are equivalent:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; P1=numpy.poly1d([1,0,1])</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print(P1)</strong></span>
</pre></div><p>The output for the preceding lines of code is as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>   2</strong></span>
<span class="strong"><strong>1 x + 1</strong></span>
</pre></div><p>Let's take a look at the following <code class="literal">print()</code> command:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; print(numpy.polyadd(P1, numpy.poly1d([2,1])))</strong></span>
</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>   2</strong></span>
<span class="strong"><strong>1 x + 2 x + 2</strong></span>
</pre></div><p>Let's take a look at the following <code class="literal">print()</code> command:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; print(numpy.polyadd(P1, [2,1]))</strong></span>
</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>   2</strong></span>
<span class="strong"><strong>1 x + 2 x + 2</strong></span>
</pre></div><p>Let's take a look at the following <code class="literal">print()</code> command:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; print(P1 + numpy.poly1d([2,1]))</strong></span>
</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>   2</strong></span>
<span class="strong"><strong>1 x + 2 x + 2</strong></span>
</pre></div><p>Let's take a look at the following <code class="literal">print()</code> command:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; print(P1 + [2,1])</strong></span>
</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>   2</strong></span>
<span class="strong"><strong>1 x + 2 x + 2</strong></span>
</pre></div><p>Note how the polynomial division offers both the quotient and reminder values, for example:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; P1/[2,1]</strong></span>
</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>(poly1d([ 0.5 , -0.25]), poly1d([ 1.25]))</strong></span>
</pre></div><p>This can also be <a id="id169" class="indexterm"/>written as follows:</p><div class="mediaobject"><img src="graphics/7702OS_04_03.jpg" alt="Univariate polynomials"/></div><p>A family of polynomials is said to be orthogonal with respect to an inner product if for any two polynomials in the family, their inner product is zero. Sequences of these functions are used as the backbone of extremely fast algorithms of quadrature (for numerical integration of general functions). The <code class="literal">scipy.special</code> module contains the <code class="literal">poly1d</code> definitions <a id="id170" class="indexterm"/>and allows fast evaluation of the families of orthogonal polynomials, such as <span class="strong"><strong>Legendre</strong></span> (<code class="literal">legendre</code>), <span class="strong"><strong>Chebyshev</strong></span> (<code class="literal">chebyt</code>, <code class="literal">chebyu</code>, <code class="literal">chebyc</code>, and <code class="literal">chebys</code>), <span class="strong"><strong>Jacobi</strong></span> (<code class="literal">jacobi</code>), <span class="strong"><strong>Laguerre</strong></span> and its generalized version (<code class="literal">laguerre</code> and <code class="literal">genlaguerre</code>), <span class="strong"><strong>Hermite</strong></span> and its normalized version (<code class="literal">hermite</code> and <code class="literal">hermitenorm</code>), and <span class="strong"><strong>Gegenbauer</strong></span> (<code class="literal">gegenbauer</code>). There are also shifted versions of some of them, such as <code class="literal">sh_legendre</code>, <code class="literal">sh_chebyt</code>, and so on.</p><p>The usual evaluation of polynomials can be improved for orthogonal polynomials, thanks to their rich mathematical structure. In such cases, we never evaluate them with the generic call methods presented previously. Instead, we employ the <code class="literal">eval_</code> syntax. For example, we use the following command for Jacobi polynomials:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; eval_jacobi(n, alpha, beta, x)</strong></span>
</pre></div><p>In order to obtain the graph of the Jacobi polynomial of order <code class="literal">n = 3</code> for <code class="literal">alpha = 0</code> and <code class="literal">beta = 1</code>, for a thousand values of <code class="literal">x</code> uniformly spaced from -1 to 1, we could issue the following commands:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; import numpy </strong></span>
<span class="strong"><strong>&gt;&gt;&gt; import scipy.special</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; import matplotlib.pyplot as plt</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; x=numpy.linspace(-1,1,1000)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; plt.plot(x,scipy.special.eval_jacobi(3,0,1,x))</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; plt.show()</strong></span>
</pre></div><p>The output is as <a id="id171" class="indexterm"/>follows:</p><div class="mediaobject"><img src="graphics/7702OS_04_04.jpg" alt="Univariate polynomials"/></div></div>
<div class="section" title="The gamma function"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec31"/>The gamma function</h1></div></div></div><p>The gamma <a id="id172" class="indexterm"/>function is a logarithmic, convex, smooth function operating on complex numbers, which interpolates the factorial function for all nonnegative integers. It <a id="id173" class="indexterm"/>is not defined at zero or any negative integer. This is the most common special function and is widely used in many different applications, either by itself or as the main ingredient in the definition of many other functions. The gamma function is used in diverse fields such as quantum physics, astrophysics, statistics, and fluid dynamics.</p><p>The gamma function is defined by the improper integral, as follows:</p><div class="mediaobject"><img src="graphics/7702OS_04_05.jpg" alt="The gamma function"/></div><p>Evaluation of gamma at integer values gives shifted factorials, and that is precisely how the factorials are coded in SciPy.</p><p>The <code class="literal">scipy.special</code> module has algorithms to obtain a fast evaluation of the gamma function at any permissible value. It also contains routines to perform evaluation of the most common compositions of the gamma functions appearing in the literature: <code class="literal">gammaln</code> for the natural logarithm of the absolute value of gamma, <code class="literal">rgamma</code> for the value one over gamma, <code class="literal">beta</code> for quotients, and <code class="literal">betaln</code> for the natural logarithm of the latter. We also have implementations of the logarithm of its derivative (<code class="literal">psi</code>).</p><p>An obvious <a id="id174" class="indexterm"/>application of gamma functions is the ability to perform computations that are virtually impossible for a computer if approached in a direct way. For <a id="id175" class="indexterm"/>instance, in statistical applications we often work with ratios of factorials. If these factorials are too large for the precision of a computer, we resort to expressions involving their logarithms instead. Even then, computing <span class="emphasis"><em>ln(a! / b!)</em></span> can prove to be an impossible task (try, for example, with <span class="emphasis"><em>a = 10**15</em></span> and <span class="emphasis"><em>b = a - 10**10</em></span>). An elegant solution uses the digamma function <code class="literal">psi</code> by an application of the mean value theorem on the <code class="literal">ln(gamma(x))</code> function. With proper estimation, we obtain the excellent approximation (for this case of choice of <span class="emphasis"><em>a</em></span> and <span class="emphasis"><em>b</em></span>):</p><div class="mediaobject"><img src="graphics/7702OS_04_06.jpg" alt="The gamma function"/></div><p>Let's take a look at the following code snippet:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; import scipy.special</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; 10**10*scipy.special.psi(10**15)</strong></span>
</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>345387763949.10681</strong></span>
</pre></div></div>
<div class="section" title="The Riemann zeta function"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec32"/>The Riemann zeta function</h1></div></div></div><p>The Riemann <a id="id176" class="indexterm"/>zeta function is very important in analytic number <a id="id177" class="indexterm"/>theory and has applications in physics and the probability theory as well. It computes the p-series for any complex value <span class="emphasis"><em>p</em></span>:</p><div class="mediaobject"><img src="graphics/7702OS_04_07.jpg" alt="The Riemann zeta function"/></div><p>The definition coded in SciPy allows a more flexible generalization of this function, as follows:</p><div class="mediaobject"><img src="graphics/7702OS_04_08.jpg" alt="The Riemann zeta function"/></div><p>Among <a id="id178" class="indexterm"/>others, this function has applications in the field of particle physics and in dynamical systems (<a class="ulink" href="http://en.wikipedia.org/wiki/Hurwitz_zeta_function">http://en.wikipedia.org/wiki/Hurwitz_zeta_function</a>)</p></div>
<div class="section" title="Airy and Bairy functions"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec33"/>Airy and Bairy functions</h1></div></div></div><p>These are <a id="id179" class="indexterm"/>solutions of the Stokes equation and are obtained by solving the <a id="id180" class="indexterm"/>following differential equation:</p><div class="mediaobject"><img src="graphics/7702OS_04_09.jpg" alt="Airy and Bairy functions"/></div><p>This equation has <a id="id181" class="indexterm"/>two linearly independent solutions, both of them defined as <a id="id182" class="indexterm"/>an improper integral for real values of the independent variable. The <code class="literal">airy</code> command computes both functions (<code class="literal">Ai</code> and <code class="literal">Bi</code>) as well as their corresponding derivatives (<code class="literal">Aip</code> and <code class="literal">Bip</code>, respectively). In the following code, we take advantage of the <code class="literal">contourf</code> command in <code class="literal">matplotlib.pyplot</code> to present an image of the real part of the output of the Bairy function <code class="literal">Bi</code> for an array of 801 x 801 complex values uniformly spaced in the square from <span class="emphasis"><em>-4 - 4j</em></span> to <span class="emphasis"><em>4 + 4j</em></span>. We also offer this graph as a surface plot using the <code class="literal">mplot3d</code> module of <code class="literal">mpl_toolkits</code>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; import numpy</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; import scipy.special</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; import  matplotlib.pyplot as plt</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; import mpl_toolkits.mplot3d</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; x=numpy.mgrid[-4:4:100j,-4:4:100j]</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; z=x[0]+1j*x[1]</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; (Ai, Aip, Bi, Bip) = scipy.special.airy(z)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; steps = range(int(Bi.real.min()), int(Bi.real.max()),6)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; fig=plt.figure()</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; subplot1=fig.add_subplot(121,aspect='equal')</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; subplot1.contourf(x[0], x[1], Bi.real, steps)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; subplot2=fig.add_subplot(122,projection='3d')</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; subplot2.plot_surface(x[0],x[1],Bi.real)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; plt.show()</strong></span>
</pre></div><p>The <a id="id183" class="indexterm"/>output is <a id="id184" class="indexterm"/>as follows:</p><div class="mediaobject"><img src="graphics/7702OS_04_10.jpg" alt="Airy and Bairy functions"/></div></div>
<div class="section" title="The Bessel and Struve functions"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec34"/>The Bessel and Struve functions</h1></div></div></div><p>
<span class="strong"><strong>Bessel</strong></span> functions <a id="id185" class="indexterm"/>are both of the canonical solutions to Bessel's <a id="id186" class="indexterm"/>homogeneous differential equation:</p><div class="mediaobject"><img src="graphics/7702OS_04_11.jpg" alt="The Bessel and Struve functions"/></div><p>These equations arise naturally in the solution of Laplace's equation in cylindrical coordinates. The solutions of the non-homogeneous Bessel differential equation shown in the following diagram <a id="id187" class="indexterm"/>are called <span class="strong"><strong>Struve</strong></span> functions:</p><div class="mediaobject"><img src="graphics/7702OS_04_12.jpg" alt="The Bessel and Struve functions"/></div><p>In either case, the <a id="id188" class="indexterm"/>order of the equation is the complex number <code class="literal">alpha</code> which acts as a parameter. Depending on the canonical solution and the order, the Bessel and Struve functions are addressed (and computed) differently.</p><p>For Bessel functions, we have algorithms to produce Bessel functions of the first kind (<code class="literal">jv</code>) and second kind (<code class="literal">yn</code> and <code class="literal">yv</code>), Hankel functions of the first and second kind (<code class="literal">hankel1</code> and <code class="literal">hankel2</code>), and the modified Bessel functions of the first and second kind (<code class="literal">iv</code>, <code class="literal">kn</code>, and <code class="literal">kv</code>). Their syntax is similar in all cases: first parameter is the order and second parameter the independent variable. The component <span class="emphasis"><em>n</em></span> in the definition indicates that an integer is to be used as the order (since they are optimally coded for that situation):</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; import numpy</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; import scipy.special</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; scipy.special.jn(5,numpy.pi)</strong></span>
</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>0.052141184367118461</strong></span>
</pre></div><p>The <code class="literal">scipy.special</code> module also contains fast versions of the most common Bessel functions (those of orders 0 and 1): <code class="literal">j0(x)</code>, <code class="literal">j1(x)</code> (first kind <code class="literal">y0(x)</code>and second kind <code class="literal">y1(x)</code>), and so on. There are definitions of the spherical Bessel functions, such as <code class="literal">sph_jn(n,z)</code> and <code class="literal">sph_yn(z)</code>; the Riccati-Bessel functions, such as <code class="literal">riccati_jn(n,x)</code> and <code class="literal">riccati_yn(n,x)</code>; and derivatives of all the basic ones, such as <code class="literal">jvp</code>, <code class="literal">yvp</code>, <code class="literal">kvp</code>, <code class="literal">ivp</code>, <code class="literal">h1vp</code>, and <code class="literal">h2vp</code>.</p><p>For Struve functions, we have fast algorithms to compute solutions of the differential equation of order <span class="emphasis"><em>v</em></span>:(<code class="literal">struve(v,x)</code> and <code class="literal">modstruve(v,x)</code>).</p></div>
<div class="section" title="Other special functions"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec35"/>Other special functions</h1></div></div></div><p>There are <a id="id189" class="indexterm"/>more special functions included in the <code class="literal">scipy.special</code> module that are of great use in many applications in both pure and applied mathematics. An exhaustive list would be too large for the scope of this chapter, and I encourage you to use the different utilities for each set of special functions. Among the most interesting ones, we <a id="id190" class="indexterm"/>have elliptic functions, <span class="strong"><strong>Gauss hypergeometric functions</strong></span>, <span class="strong"><strong>parabolic cylinder functions</strong></span>, <span class="strong"><strong>Mathieu functions</strong></span>, <span class="strong"><strong>spheroidal wave functions</strong></span>, and <span class="strong"><strong>Kelvin functions</strong></span>.</p></div>
<div class="section" title="Interpolation"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec36"/>Interpolation</h1></div></div></div><p>Interpolation is a basic method in numerical computation that is obtained from a discrete set of data points, intended to find an interpolation function which represents some higher order structure that contains the data. The best known example is the interpolation of a sequence of points (<span class="emphasis"><em>x_k</em></span> and <span class="emphasis"><em>y_k</em></span>) in a plane to obtain a curve that goes through all the points in the order dictated by the sequence. </p><p>If the points in the previous sequence are in the right position and order, it is possible to find a univariate function <span class="emphasis"><em>y = f(x)</em></span> for which <span class="emphasis"><em>y_k = f(x_k)</em></span>. It is often reasonable to request this interpolating function to be a polynomial, or a rational function, or a more complex functional object. Interpolation is also possible in higher dimensions, of course. The objective of the <code class="literal">scipy.interpolate</code> module is to offer a complete set of optimally coded applications to address this problem in different settings.</p><p>Let's address the easiest way of interpolating data to obtain a polynomial: lagrange interpolation. Given a sequence of different <span class="emphasis"><em>x</em></span> values of size <span class="emphasis"><em>n</em></span> and a sequence of arbitrary real values <span class="emphasis"><em>y</em></span> of the same size <span class="emphasis"><em>n</em></span>, we seek a polynomial <span class="emphasis"><em>p(x)</em></span> of the degree of <span class="emphasis"><em>n - 1</em></span> that satisfies the <span class="emphasis"><em>n</em></span> constraints <span class="emphasis"><em>p(x[k]) = y[k]</em></span> for all k from 0 to <span class="emphasis"><em>n - 1</em></span>. The following code illustrates how to obtain a polynomial of degree 9 that interpolates the 10 uniformly spaced values of sine in the interval (-1, 1):</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; import numpy</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; import matplotlib.pyplot as plt</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; import scipy.interpolate</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; x=numpy.linspace(-1,1,10); xn=numpy.linspace(-1,1,1000)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; y=numpy.sin(x)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; polynomial=scipy.interpolate.lagrange(x, numpy.sin(x))</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; plt.plot(xn,polynomial(xn),x,y,'or')</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; plt.show()</strong></span>
</pre></div><p>We will obtain the following <code class="literal">plot</code> showing the Lagrange interpolation:</p><div class="mediaobject"><img src="graphics/7702OS_04_13.jpg" alt="Interpolation"/></div><p>There are numerous issues with Lagrange interpolation. The first obvious drawback is that the user cannot specify the degree of the interpolation; this depends solely on the data. The procedure is also highly unstable numerically, especially for datasets with size over 20 points. This issue can be addressed by allowing the algorithm to depend on different properties of the dataset, rather than just the size and location of the points.</p><p>Also, it is inconvenient <a id="id191" class="indexterm"/>when we need to update the dataset by adding a few more instances; the procedure needs to be repeated again from the beginning. This proves impractical if the datasets are increasing in size and are updated frequently. To address this issue, <code class="literal">BarycentricInterpolator</code> has the <code class="literal">add_xi</code> and <code class="literal">set_yi</code> methods. For example, in the next session we start by interpolating 10 uniformly spaced values of the sine function between 1 and 10. Once done, we update the interpolating polynomial with 10 more uniformly spaced values between 1.5 and 10.5. As expected, this operation reduces the (percent) relative error of an interpolation computed at points within the interpolating ones. The following commands are used:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; import numpy</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; import scipy.interpolate</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; x1=numpy.linspace(1,10,10); y1=numpy.sin(x1)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; Polynomial=scipy.interpolate.BarycentricInterpolator(x1,y1)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; exactValues=numpy.sin(x1+0.3)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; exactValues</strong></span>
</pre></div><p>Here is the output for <code class="literal">exactValues</code>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>array([ 0.96355819,  0.74570521, -0.15774569, -0.91616594, </strong></span>
<span class="strong"><strong>-0.83226744,</strong></span>
<span class="strong"><strong>        0.0168139 ,  0.85043662,  0.90217183,  0.12445442, </strong></span>
<span class="strong"><strong>-0.76768581])</strong></span>
</pre></div><p>Let's find the value of <code class="literal">interpolatedValues</code> by issuing following commands:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; interpolatedValues=Polynomial(x1+0.3)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; interpolatedValues</strong></span>
</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>array([ 0.97103132,  0.74460631, -0.15742869, -0.91631362, </strong></span>
<span class="strong"><strong>-0.83216445, </strong></span>
<span class="strong"><strong>        0.01670922,  0.85059283,  0.90181323,  0.12588718, </strong></span>
<span class="strong"><strong>-0.7825744 ])</strong></span>
</pre></div><p>Let's find the value of <code class="literal">PercentRelativeError</code> by issuing following commands:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; PercentRelativeError = numpy.abs((exactValues - interpolatedValues)/interpolatedValues)*100</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; PercentRelativeError</strong></span>
</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>array([ 0.76960822,  0.14758101,  0.20136334,  0.01611703,  0.01237594, </strong></span>
<span class="strong"><strong>        0.62647084,  0.01836479,  0.0397652 ,  1.13812858,  1.90251374])</strong></span>
</pre></div><p>Then, we find <a id="id192" class="indexterm"/>what <code class="literal">interpolatedValues2</code> holds:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; x2=numpy.linspace(1.5,10.5,10); y2=numpy.sin(x2)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; Polynomial.add_xi(x2,y2)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; interpolatedValues2=Polynomial(x1+0.3)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; interpolatedValues2</strong></span>
</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>array([ 0.96355818,  0.74570521, -0.15774569, -0.91616594, -0.83226744,</strong></span>
<span class="strong"><strong>        0.0168139 ,  0.85043662,  0.90217183,  0.12445442, -0.76768581])</strong></span>
</pre></div><p>Let's find the value of <code class="literal">PercentRelativeError</code>, keeping in consideration <code class="literal">interpolatedValues2</code>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; PercentRelativeError = numpy.abs((exactValues - interpolatedValues2)/interpolatedValues2)*100</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; PercentRelativeError</strong></span>
</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>array([  1.26241742e-07,   2.02502252e-09,   5.95225989e-10,</strong></span>
<span class="strong"><strong>         1.84438143e-11,   8.75086862e-12,   4.14359323e-10,</strong></span>
<span class="strong"><strong>         1.75194631e-11,   8.52321518e-11,   9.45285176e-09,</strong></span>
<span class="strong"><strong>         1.29570657e-07])</strong></span>
</pre></div><p>It is possible to interpolate data not only by point location, but also with the derivatives at those locations. The <code class="literal">KroghInterpolator</code> command allows this by including repeated <span class="emphasis"><em>x</em></span> values and indicating the <a id="id193" class="indexterm"/>location and successive derivatives in order on the corresponding <span class="emphasis"><em>y</em></span> values. </p><p>For instance, if we desire to construct a polynomial that is zero at the origin, one at <span class="emphasis"><em>x = 1</em></span>, two at <span class="emphasis"><em>x = 2</em></span>, and has horizontal tangent lines at each of these three locations, we issue the following commands:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; import numpy</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; import matplotlib.pyplot as plt</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; import  scipy.interpolate</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; x=numpy.array([0,0,1,1,2,2]); y=numpy.array([0,0,1,0,2,0])</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; interp=scipy.interpolate.KroghInterpolator(x,y)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; xn=numpy.linspace(0,2,20)   # evaluate polynomial in larger set</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; plt.plot(x,y,'o',xn,interp(xn),'r')</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; plt.show()</strong></span>
</pre></div><p>This renders the <a id="id194" class="indexterm"/>following graph:</p><div class="mediaobject"><img src="graphics/7702OS_04_14.jpg" alt="Interpolation"/></div><p>More advanced one-dimensional interpolation is possible with piecewise polynomials (<code class="literal">PiecewisePolynomial</code>). This allows control over the degrees of different pieces as well as <a id="id195" class="indexterm"/>the derivatives at their intersections. Other interpolation options in the <code class="literal">scipy.interpolate</code> module are <span class="strong"><strong>PCHIP monotonic cubic interpolation</strong></span> (<code class="literal">pchip</code>) or even <span class="strong"><strong>univariate splines</strong></span> (<code class="literal">InterpolatedUnivariateSpline</code>).</p><p>Let's <a id="id196" class="indexterm"/>examine an example with univariate splines. Its syntax is as follows:</p><div class="informalexample"><pre class="programlisting">InterpolatedUnivariateSpline(x, y, w=None, bbox=[None, None], k=3)</pre></div><p>The <code class="literal">x</code> and <code class="literal">y</code> arrays contain dependent and independent data, respectively. The array <code class="literal">w</code> contains positive weights for spline fitting. The two-sequence <code class="literal">bbox</code>  parameter specifies the boundary of the approximation interval. The last option indicates the degree of the smoothing polynomials (<code class="literal">k</code>).</p><p>Suppose we want to interpolate five points as shown in the following example. These points are ordered <a id="id197" class="indexterm"/>by strictly increasing <code class="literal">x</code> values. We need to perform this interpolation with four cubic polynomials (one for every two consecutive points) in such a way that at least the first derivative of each two consecutive pieces agree on their intersection. We will proceed as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; import numpy</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; import matplotlib.pyplot as plt</strong></span>
<span class="strong"><strong>&gt;&gt;&gt;import scipy.interpolate</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; x=numpy.arange(5); y=numpy.sin(x)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; xn=numpy.linspace(0,4,40)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; interp=scipy.interpolate.InterpolatedUnivariateSpline(x,y)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; plt.plot(x,y,'.',xn,interp(xn))</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; plt.show()</strong></span>
</pre></div><p>This offers the following plot showing interpolation with univariate splines:</p><div class="mediaobject"><img src="graphics/7702OS_04_15.jpg" alt="Interpolation"/></div><p>SciPy excels at interpolating in two-dimensional grids as well. It performs well with simple piecewise polynomials (<code class="literal">LinearNDInterpolator</code>), piecewise constants (<code class="literal">NearestNDInterpolator</code>), or more advanced splines (<code class="literal">BivariateSpline</code>). It is capable of carrying out spline interpolation on rectangular meshes in a plane (<code class="literal">RectBivariateSpline</code>) or on the surface of a sphere (<code class="literal">RectSphereBivariateSpline</code>). For unstructured data, besides the basic <code class="literal">scipy.interpolate.BivariateSpline</code>, it is capable of computing smooth approximations (<code class="literal">SmoothBivariateSpline</code>) or more involved weighted least-squares splines (<code class="literal">LSQBivariateSpline</code>).</p><p>The following code creates a 10 x 10 grid of uniformly spaced points in the square from (0, 0) to (9, 9), and evaluates the function <code class="literal">sin(x) * cos(y)</code> on the points. We use these points to create a <code class="literal">scipy.interpolate.BivariateSpline</code> and evaluate the resulting function on the square for all values:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; import numpy</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; import scipy.interpolate</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; import matplotlib.pyplot as plt</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; from mpl_toolkits.mplot3d import Axes3D</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; x=y=numpy.arange(10)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; f=(lambda i,j: numpy.sin(i)*numpy.cos(j))  # function to interpolate</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; A=numpy.fromfunction(f, (10,10))           # generate samples</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; spline=scipy.interpolate.RectBivariateSpline(x,y,A)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; fig=plt.figure()</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; subplot=fig.add_subplot(111,projection='3d')</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; xx=numpy.mgrid[0:9:100j, 0:9:100j]     # larger grid for plotting</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; A=spline(numpy.linspace(0,9,100), numpy.linspace(0,9,100))</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; subplot.plot_surface(xx[0],xx[1],A)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; plt.show()</strong></span>
</pre></div><p>The output is as <a id="id198" class="indexterm"/>follows, and it shows the interpolation of 2D data with bivariate splines:</p><div class="mediaobject"><img src="graphics/7702OS_04_16.jpg" alt="Interpolation"/></div></div>
<div class="section" title="Regression"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec37"/>Regression</h1></div></div></div><p>Regression is similar <a id="id199" class="indexterm"/>to interpolation. In this case, we assume that the data is imprecise, and we require an object of predetermined structure to fit the data as closely as possible. The most basic example is univariate polynomial regression to a sequence of points. We obtain that with the <code class="literal">polyfit</code> command, which we discussed briefly in the <span class="emphasis"><em>Univariate polynomials</em></span> section of this chapter. For instance, if we want to compute the regression line in the least-squares sense for a sequence of 10 uniformly spaced points in the interval (0, <span class="emphasis"><em>π</em></span>/2) and their values under the <code class="literal">sin</code> function, we will issue the following commands:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; import numpy</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; import scipy</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; import matplotlib.pyplot as plt</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; x=numpy.linspace(0,1,10)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; y=numpy.sin(x*numpy.pi/2)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; line=numpy.polyfit(x,y,deg=1)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; plt.plot(x,y,'.',x,numpy.polyval(line,x),'r')</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; plt.show()</strong></span>
</pre></div><p>This gives the following plot that shows linear regression with <code class="literal">polyfit</code>:</p><div class="mediaobject"><img src="graphics/7702OS_04_17.jpg" alt="Regression"/></div><p>Curve fitting is also possible with splines if we use the parameters wisely. For example, in the case of univariate spline fitting that we introduced before, we can play around with the weights, smoothing factor, the degree of the smoothing spline, and so on. If we want to fit a parabolic spline for the same data as the previous example, we could issue the following commands:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; import numpy</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; import scipy.interpolate</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; import matplotlib.pyplot as plt</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; x=numpy.linspace(0,1,10)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; y=numpy.sin(x*numpy.pi/2)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; spline=scipy.interpolate.UnivariateSpline(x,y,k=2)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; xn=numpy.linspace(0,1,100)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; plt.plot(x,y,'.', xn, spline(xn))</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; plt.show()</strong></span>
</pre></div><p>This gives the following <a id="id200" class="indexterm"/>graph that shows curve fitting with splines:</p><div class="mediaobject"><img src="graphics/7702OS_04_18.jpg" alt="Regression"/></div><p>For regression from the point of view of curve fitting, there is a generic routine: <code class="literal">curve_fit</code> in the <code class="literal">scipy.optimize</code> module. This routine minimizes the sum of squares of a set of <a id="id201" class="indexterm"/>equations using the <span class="strong"><strong>Levenberg-Marquardt</strong></span> algorithm and offers a best fit from any kind of functions (not only polynomials or splines). The syntax is simple:</p><div class="informalexample"><pre class="programlisting">curve_fit(f, xdata, ydata, p0=None, sigma=None, **kw)</pre></div><p>The <code class="literal">f</code> parameter is a callable function that represents the function we seek, and <code class="literal">xdata</code> and <code class="literal">ydata</code> are arrays of the same length that contain the <span class="emphasis"><em>x</em></span> and <span class="emphasis"><em>y</em></span> coordinates of the points to be fit. The tuple <code class="literal">p0</code> holds an initial guess for the values to be found, and <code class="literal">sigma</code> is a vector of weights that could be used instead of the standard deviation of the data, if necessary. </p><p>We will show its usage with a good example. We will start by generating some points on a section of a sine wave with amplitude <code class="literal">A=18</code>, angular frequency <span class="emphasis"><em>w=3π</em></span>, and phase <code class="literal">h=0.5</code>. We corrupt the data in the array <code class="literal">y</code> with some small random noise:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; import numpy</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; import scipy</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; A=18; w=3*numpy.pi; h=0.5</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; x=numpy.linspace(0,1,100); y=A*numpy.sin(w*x+h)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; y += 4*((0.5-scipy.rand(100))*numpy.exp(2*scipy.rand(100)**2))</strong></span>
</pre></div><p>We want to estimate the values of <code class="literal">A</code>, <code class="literal">w</code>, and <code class="literal">h</code> from the corrupted data, hence technically finding a curve fit from the set of sine waves. We start by gathering the three parameters in a list and initializing them to some values, for example, <code class="literal">A = 20</code>, <span class="emphasis"><em>w = 2π</em></span>, and <code class="literal">h = 1</code>. We also construct a callable expression of the target function (<code class="literal">target_function</code>):</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; import scipy.optimize</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; p0 = [20, 2*numpy.pi, 1]</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; target_function = lambda x,AA,ww,hh: AA*numpy.sin(ww*x+hh)</strong></span>
</pre></div><p>We feed these, together with the fitting data, to <code class="literal">curve_fit</code> in order to find the required values:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; pF,pVar = scipy.optimize.curve_fit(target_function, x, y, p0)</strong></span>
</pre></div><p>A sample of <code class="literal">pF</code> run <a id="id202" class="indexterm"/>on any of our experiments should give an accurate result for the three requested values:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; print (pF)</strong></span>
</pre></div><p>The output for the preceding command is as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>[ 18.13799397   9.32232504   0.54808516]</strong></span>
</pre></div><p>This means that <code class="literal">A</code> was estimated to about 18.14, <code class="literal">w</code> was estimated very close to 3<span class="emphasis"><em>π</em></span>, and <code class="literal">h</code> was between 0.46 and 0.55. The output of the initial data together with a computation of the sine wave is as follows, in which original data (in blue on the left-hand side graph), corrupted (in red in both graphs), and computed sine wave (in black in the right-hand side) are shown in following plots:</p><div class="mediaobject"><img src="graphics/7702OS_04_19.jpg" alt="Regression"/></div><p>The code is too long to be included here. Instead, the full code (intermediate plots that are produced are not shown <a id="id203" class="indexterm"/>here) can be found in the corresponding electronic resource IPython Notebook for this chapter.</p></div>
<div class="section" title="Optimization"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec38"/>Optimization</h1></div></div></div><p>Optimization <a id="id204" class="indexterm"/>involves finding extreme values of functions or their roots. We have already seen the power of optimization in the curve-fitting arena, but it does not stop there. There are applications to virtually every single branch of engineering, and robust algorithms to perform these tasks are a must in every scientist's toolbox.</p><p>The <code class="literal">curve_fit</code> routine is actually syntactic sugar for the general algorithm that performs least-squares minimization, <code class="literal">leastsq</code>, with the imposing syntax:</p><div class="informalexample"><pre class="programlisting">leastsq(func, x0, args=(), Dfun=None, full_output=0,
        col_deriv=0, ftol=1.49012e-8, xtol=1.49012e-8,
        gtol=0.0, maxfev=0, epsfcn=0.0, factor=100, diag=None):</pre></div><p>For instance, the <code class="literal">curve_fit</code> routine could have been called with a <code class="literal">leastsq</code> call instead:</p><div class="informalexample"><pre class="programlisting">leastsq(error_function,p0,args=(x,y))</pre></div><p>Here, <code class="literal">error_function</code> is equal to <code class="literal">lambda p,x,y: target_function(x,p[0],p[1],p[2])-y</code>
</p><p>The implementation is given in the corresponding section on the IPython Notebook of this chapter. Most of the optimization routines in SciPy can be accessed from either native Python code, or as wrappers for Fortran or C classical implementations of their corresponding algorithms. Technically, we are still using the same packages we did under Fortran or C, but from <a id="id205" class="indexterm"/>within Python. For instance, the minimization routine that implements the truncated <code class="literal">Newton</code> method can be called with <code class="literal">fmin_ncg</code> (and this is purely Python) or as <code class="literal">fmin_tnc</code> (and this one is a wrap of a C implementation).</p><div class="section" title="Minimization"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec27"/>Minimization</h2></div></div></div><p>For general minimization <a id="id206" class="indexterm"/>problems, SciPy has many different algorithms. So far, we have covered the least-squares algorithm (<code class="literal">leastsq</code>), but we also have brute force (<code class="literal">brute</code>), <span class="strong"><strong>simulated annealing</strong></span> (<code class="literal">anneal</code>), <span class="strong"><strong>Brent</strong></span> or <span class="strong"><strong>Golden</strong></span> methods for scalar functions (<code class="literal">brent</code> or <code class="literal">golden</code>), the <span class="strong"><strong>downhill simplex</strong></span> algorithm (<code class="literal">fmin</code>), <span class="strong"><strong>Powell's</strong></span> method (<code class="literal">fmin_powell</code>), <span class="strong"><strong>nonlinear conjugate gradient</strong></span> or Newton's version of it (<code class="literal">fmin_cg</code>, <code class="literal">fmin_ncg</code>), and the <span class="strong"><strong>BFGS</strong></span> algorithm (<code class="literal">fmin_bfgs</code>).</p><p>Constrained minimization is also possible computationally, and SciPy has routines that implement the <span class="strong"><strong>L-BFGS-S</strong></span> algorithm (<code class="literal">fmin_l_bfgs_s</code>), truncated Newton's algorithm (<code class="literal">fmin_tnc</code>), <span class="strong"><strong>COBYLA</strong></span> (<code class="literal">fmin_cobyla</code>), or sequential least-squares programming (<code class="literal">fmin_slsqp</code>).</p><p>The following code, for example, compares the output of all different methods to finding a local minimum of the Rosenbrock function, <code class="literal">scipy.optimize.rosen</code>, near the origin using the downhill simplex algorithm:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; import scipy.optimize</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; scipy.optimize.fmin(scipy.optimize.rosen,[0,0])</strong></span>
</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>Optimization terminated successfully.</strong></span>
<span class="strong"><strong>         Current function value: 0.000000</strong></span>
<span class="strong"><strong>         Iterations: 79</strong></span>
<span class="strong"><strong>         Function evaluations: 146</strong></span>
<span class="strong"><strong>array([ 1.00000439,  1.00001064])</strong></span>
</pre></div><p>Since the Version 0.11 of SciPy, all minimization routines can be called from the generic <code class="literal">scipy.optimize.minimize</code>, with the <code class="literal">method</code> parameter pointing to one of the strings, such as <code class="literal">Nelder-Mead</code> (for the downhill simplex), <code class="literal">Powell</code>, <code class="literal">CG</code>, <code class="literal">Newton-CG</code>, <code class="literal">BFGS</code>, or <code class="literal">anneal</code>. For constrained minimization, the corresponding strings are one of <code class="literal">L-BFGS-S</code>, <code class="literal">TNC</code> (for truncated Newton's), <code class="literal">COBYLA</code>, or <code class="literal">SLSQP</code>:</p><div class="informalexample"><pre class="programlisting">minimize( fun, x0, args=(), method='BFGS', jac=None, hess=None, hessp=None, bounds=None, constraints=(),tol=None, callback=None, options=None)</pre></div></div><div class="section" title="Roots"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec28"/>Roots</h2></div></div></div><p>For most special <a id="id207" class="indexterm"/>functions included in the <code class="literal">scipy.specia</code>l module, we have accurate algorithms that allow us to their zeros. For instance, for the Bessel function of first kind with integer order, <code class="literal">jn_zeros</code>, offers as many roots as desired (in ascending order). We may obtain the first three roots of the Bessel J-function of order four by issuing the following command:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; import scipy.special</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print (scipy.special.jn_zeros(4,3))</strong></span>
</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>[  7.58834243  11.06470949  14.37253667]</strong></span>
</pre></div><p>For nonspecial scalar functions, the <code class="literal">scipy.optimize</code> module allows approximation to the roots through a great deal of different algorithms. For scalar functions, we have the <span class="strong"><strong>crude bisection</strong></span> method (<code class="literal">bisect</code>), the <span class="strong"><strong>classical secant</strong></span> method of <span class="strong"><strong>Newton-Raphson</strong></span> (<code class="literal">newton</code>), and more accurate and faster methods such as <span class="strong"><strong>Ridders</strong></span>' algorithm (<code class="literal">ridder</code>), and two versions of the Brent method (<code class="literal">brentq</code> and <code class="literal">brenth</code>).</p><p>Finding roots for functions of several variables is very challenging in many ways; the larger the dimension, the more difficult it is. The effectiveness of any of these algorithms depends on the problem, and it is a good idea to invest some time and resources in knowing them all. Since Version 0.11 of SciPy, it is possible to call any of the designed methods with the same routine <code class="literal">scipy.optimize.root</code>, which has the following syntax:</p><div class="informalexample"><pre class="programlisting">root(fun, x0, args=(), method='hybr', jac=None, tol=None, callback=None, options=None)</pre></div><p>The different methods are obtained upon changing the value of the <code class="literal">method</code> parameter to a method's string. We may choose from methods such as <code class="literal">'hybr'</code> for a modified hybrid Powell's method; <code class="literal">'lm'</code> for a modified least-squares method; <code class="literal">'broyden1'</code> or <code class="literal">'broyden2'</code> for Broyden's good and bad methods, respectively; <code class="literal">'diagbroyden'</code> for the diagonal Broyden Jacobian approximation; <code class="literal">'anderson'</code> for Anderson's extended mixing; <code class="literal">'Krylov'</code> for Krylov approximation of the Jacobian; <code class="literal">'linearmixing'</code> for scalar Jacobian approximation; and <code class="literal">'excitingmixing'</code> for a tuned diagonal Jacobian approximation.</p><p>For large-scale problems, both the Krylov approximation of the Jacobian or the Anderson extended mixing are usually the best options.</p><p>Let's present an illustrative example of the power of these techniques. Consider the following system of differential equations:</p><div class="mediaobject"><img src="graphics/7702OS_04_20.jpg" alt="Roots"/></div><p>We use the plot routine quiver from the matplotlib.pyplot libraries to visualize a slope field for values of <code class="literal">x</code> and <code class="literal">y</code> between -0.5 and 2.5, and hence identify the location of the possible critical points in that region:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; import numpy </strong></span>
<span class="strong"><strong>&gt;&gt;&gt; import matplotlib.pyplot as plt </strong></span>
<span class="strong"><strong>&gt;&gt;&gt; f=lambda x: [x[0]**2 - 2*x[0] - x[1] + 0.5, x[0]**2 + 4*x[1]**2 - 4]</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; x,y=numpy.mgrid[-0.5:2.5:24j,-0.5:2.5:24j]</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; U,V=f([x,y])</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; plt.quiver(x,y,U,V,color='r', \</strong></span>
<span class="strong"><strong>         linewidths=(0.2,), edgecolors=('k'), \</strong></span>
<span class="strong"><strong>         headaxislength=5)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; plt.show()</strong></span>
</pre></div><p>The output <a id="id208" class="indexterm"/>is as follows:</p><div class="mediaobject"><img src="graphics/7702OS_04_21.jpg" alt="Roots"/></div><p>Note how there is a whole region of the plane in which the slopes are extremely small. Because of the degrees of the polynomials involved, there are at most four different possible critical points. In this area, we should be able to identify two such points (as a matter of fact there are only two noncomplex solutions). One of them seems to be near (0, 1) and the second one is near (2, 0). We use these two locations as initial guesses for our searches:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; import scipy.optimize</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; f=lambda x: [x[0]**2 - 2*x[0] - x[1] + 0.5, x[0]**2 + 4*x[1]**2 - 4]</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; scipy.optimize.root(f,[0,1])</strong></span>
</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>  status: 1</strong></span>
<span class="strong"><strong> success: True</strong></span>
<span class="strong"><strong>qtf: array([ -4.81190247e-09,  -3.83395899e-09])</strong></span>
<span class="strong"><strong>nfev: 9</strong></span>
<span class="strong"><strong>       r: array([ 2.38128242, -0.60840482, -8.35489601])</strong></span>
<span class="strong"><strong>     fun: array([  3.59529073e-12,   3.85025345e-12])</strong></span>
<span class="strong"><strong>       x: array([-0.22221456,  0.99380842])</strong></span>
<span class="strong"><strong> message: 'The solution converged.'</strong></span>
<span class="strong"><strong>fjac: array([[-0.98918813, -0.14665209],</strong></span>
<span class="strong"><strong>       [ 0.14665209, -0.98918813]])</strong></span>
</pre></div><p>Let's look at second case:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; scipy.optimize.root(f,[2,0])</strong></span>
</pre></div><p>The output is as <a id="id209" class="indexterm"/>follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>  status: 1</strong></span>
<span class="strong"><strong> success: True</strong></span>
<span class="strong"><strong>qtf: array([  2.08960516e-10,   8.61298294e-11])</strong></span>
<span class="strong"><strong>nfev: 12</strong></span>
<span class="strong"><strong>       r: array([-4.56575336, -1.67067665, -1.81464307])</strong></span>
<span class="strong"><strong>     fun: array([  2.44249065e-15,   1.42996726e-13])</strong></span>
<span class="strong"><strong>       x: array([ 1.90067673,  0.31121857])</strong></span>
<span class="strong"><strong> message: 'The solution converged.'</strong></span>
<span class="strong"><strong>fjac: array([[-0.39612596, -0.91819618],</strong></span>
<span class="strong"><strong>       [ 0.91819618, -0.39612596]])</strong></span>
</pre></div><p>In the first case, we converged successfully to (-0.22221456, 0.99380842). In the second case, we converged to (1.90067673, 0.31121857). The routine gives us the details of the convergence and the properties of the approximation. For instance, <code class="literal">nfev</code> tells us about the number of function calls performed, and <code class="literal">fun</code> indicates the output of the function at the found location. The other items in the output reflect the matrices used in the procedure, such as <code class="literal">qtf</code>, <code class="literal">r</code>, and <code class="literal">fjac</code>.</p></div></div>
<div class="section" title="Integration"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec39"/>Integration</h1></div></div></div><p>SciPy is <a id="id210" class="indexterm"/>capable of performing very robust numerical integration. Definite integrals of a set of special functions are evaluated accurately with routines in the <code class="literal">scipy.special</code> module. For other functions, there are several different algorithms to obtain reliable approximations in the <code class="literal">scipy.integrate</code> module.</p><div class="section" title="Exponential/logarithm integrals"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec29"/>Exponential/logarithm integrals</h2></div></div></div><p>A <a id="id211" class="indexterm"/>summary of the indefinite and definite integrals in the <a id="id212" class="indexterm"/>category of exponential/logarithm is presented here: the exponential <a id="id213" class="indexterm"/>integrals (<code class="literal">expn</code>, <code class="literal">expi</code>, and <code class="literal">exp1</code>), <span class="strong"><strong>Dawson's</strong></span> integral (<code class="literal">dawsn</code>), and <a id="id214" class="indexterm"/>
<span class="strong"><strong>Gauss error functions</strong></span> (<code class="literal">erf</code> and <code class="literal">erfc</code>). We also have <span class="strong"><strong>Spence's</strong></span> <a id="id215" class="indexterm"/>dilogarithm (also known as Spence's integral). Let's have a look at the following formulas:</p><div class="mediaobject"><img src="graphics/7702OS_04_22.jpg" alt="Exponential/logarithm integrals"/></div></div><div class="section" title="Trigonometric and hyperbolic trigonometric integrals"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec30"/>Trigonometric and hyperbolic trigonometric integrals</h2></div></div></div><p>In the <a id="id216" class="indexterm"/>category of trigonometric and hyperbolic <a id="id217" class="indexterm"/>trigonometric integrals, we have Fresnel sine and cosine <a id="id218" class="indexterm"/>integrals, as well as the sinc and hyperbolic <a id="id219" class="indexterm"/>trigonometric integrals. Let's have a look at the following formulas:</p><div class="mediaobject"><img src="graphics/7702OS_04_23.jpg" alt="Trigonometric and hyperbolic trigonometric integrals"/></div><p>In the <a id="id220" class="indexterm"/>definitions given in the preceding list of integrals, the <a id="id221" class="indexterm"/>gamma <a id="id222" class="indexterm"/>symbol denotes the Euler-Mascheroni <a id="id223" class="indexterm"/>constant:</p><div class="mediaobject"><img src="graphics/7702OS_04_24.jpg" alt="Trigonometric and hyperbolic trigonometric integrals"/></div></div><div class="section" title="Elliptic integrals"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec31"/>Elliptic integrals</h2></div></div></div><p>Elliptic <a id="id224" class="indexterm"/>integrals arise naturally when computing the arc length of <a id="id225" class="indexterm"/>ellipses. SciPy follows the argument notation for elliptic integrals: complete (one argument) and incomplete (two arguments). Let's have a look at the following formulas:</p><div class="mediaobject"><img src="graphics/7702OS_04_25.jpg" alt="Elliptic integrals"/></div></div><div class="section" title="Gamma and beta integrals"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec32"/>Gamma and beta integrals </h2></div></div></div><p>In the <a id="id226" class="indexterm"/>category of gamma and beta integrals, we have one incomplete <a id="id227" class="indexterm"/>gamma function, one complemented incomplete gamma integral, and one incomplete beta integral. These are some of the <a id="id228" class="indexterm"/>most <a id="id229" class="indexterm"/>useful functions in this category. Let's have a look at the following formulas:</p><div class="mediaobject"><img src="graphics/7702OS_04_26.jpg" alt="Gamma and beta integrals"/></div></div><div class="section" title="Numerical integration"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec33"/>Numerical integration</h2></div></div></div><p>For any <a id="id230" class="indexterm"/>other functions, we are content with approximating <a id="id231" class="indexterm"/>definite integrals with quadrature formulae, such as <code class="literal">quad</code> (adaptive quadrature), <code class="literal">fixed_quad</code> (fixed-order Gaussian quadrature), <code class="literal">quadrature</code> (fixed-tolerance Gaussian quadrature), and <code class="literal">romberg</code>, (Romberg integration). For functions with more than one variable, we have <code class="literal">dbquad</code> (double integral) and <code class="literal">tplquad</code> (triple integral) methods. The syntax in all cases is a variation of <code class="literal">quad</code>:</p><div class="informalexample"><pre class="programlisting">quad(func, a, b, args=(), full_output=0, epsabs=1.49e-08, epsrel=1.49e-08, limit=50, points=None, weight=None, wvar=None, wopts=None, maxp1=50, limlst=50)</pre></div><p>If we have samples instead of functions, we may use routines such as <code class="literal">trapz</code>, <code class="literal">cumtrapz</code> (composite trapezoidal rule and its cumulative version), <code class="literal">romb</code> (Romberg integration again), and <code class="literal">simps</code> (Simpson's rule) instead. In these routines, the syntax is simpler and changes the order of the parameters. For example, this is how we call <code class="literal">simps</code>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; simps(y, x=None, dx=1, axis=-1, even='avg')</strong></span>
</pre></div><p>Those of us familiar with the <span class="strong"><strong>QUADPACK</strong></span> libraries will find similar syntax, usage, and performance.</p><p>For extra information, run the <code class="literal">scipy.integrate.quad_explain()</code> command. In the IPython Notebook for this chapter, the alternative help command,<code class="literal"> scipy.integrate.quad</code>, is executed and its output is displayed in the corresponding section. This explains with great detail all the different outputs of the quadrature integrals included in the module result, the estimation of absolute error and convergence, and explanation of the used weightings, if necessary. Let's give at least one meaningful example where we integrate a special function and compare the output of a quadrature formula against the more accurate value of the routines given in <code class="literal">scipy.special</code>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; f=lambda t: numpy.exp(-t)*t**4</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; from scipy.special import gammainc</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; from scipy.integrate import quad</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; from scipy.misc import factorial</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; print (gammainc(5,1))</strong></span>
</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>0.00365984682734</strong></span>
</pre></div><p>Let's take a look at following <code class="literal">print</code> command:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>print('%.19f' % gammainc(5,1))</strong></span>
</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>0.0036598468273437131</strong></span>
</pre></div><p>Let's look further into the code:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; import numpy</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; result,error=quad(f,0,1)/factorial(4)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; result</strong></span>
</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>0.0036598468273437122</strong></span>
</pre></div><p>To use a routine <a id="id232" class="indexterm"/>that integrates from samples, we have the <a id="id233" class="indexterm"/>flexibility of assigning the frequency and length of the data. For the following problem, we could try with 10,000 samples in the same interval:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; import numpy </strong></span>
<span class="strong"><strong>&gt;&gt;&gt; import scipy.integrate</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; x=numpy.linspace(0,1,10000)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; scipy.integrate.simps(f(x)/factorial(4), x)</strong></span>
</pre></div><p>The output is as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>0.0036598468273469071</strong></span>
</pre></div></div></div>
<div class="section" title="Ordinary differential equations"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec40"/>Ordinary differential equations</h1></div></div></div><p>As with <a id="id234" class="indexterm"/>integration, SciPy has some extremely accurate general-purpose solvers for systems of ordinary differential equations of first order:</p><div class="mediaobject"><img src="graphics/7702OS_04_27.jpg" alt="Ordinary differential equations"/></div><p>For real-valued functions, we have basically two flavors: <code class="literal">ode</code> (with options passed with the <code class="literal">set_integrator</code> method) and <code class="literal">odeint</code> (simpler interface). The syntax of <code class="literal">ode</code> is as follows:</p><div class="informalexample"><pre class="programlisting">ode(f,jac=None)</pre></div><p>The first parameter, <code class="literal">f</code>, is the function to be integrated, and the second parameter, <code class="literal">jac</code>, refers to the matrix of partial derivatives with respect to the dependent variables (the Jacobian). This creates an <code class="literal">ode</code> object, with different methods to indicate the algorithm to solve the system (<code class="literal">set_integrator</code>), the initial conditions (<code class="literal">set_initial_value</code>), and different parameters to be sent to the function or its Jacobian.</p><p>The options for integration algorithm are <code class="literal">'vode'</code> for real-valued variable coefficient ODE solver, with fixed-leading-coefficient implementation (it provides Adam's method for non-stiff problems and BDF for stiff); <code class="literal">'zvode'</code> for complex-valued variable coefficient ODE <a id="id235" class="indexterm"/>solver, with similar options as the preceding option; <code class="literal">'dopri5'</code> for a <span class="strong"><strong>Runge-Kutta</strong></span> method of order (4)5; <code class="literal">'dop853'</code> for a Runge-Kutta method of order 8(5, 3).</p><p>The next code <a id="id236" class="indexterm"/>snippet presents an example of usage of the <code class="literal">scipy.integrate.ode</code> to solve the initial value problem using the following formula:</p><div class="mediaobject"><img src="graphics/7702OS_04_28.jpg" alt="Ordinary differential equations"/></div><p>We compute each step sequentially and compare it with the actual solution, which is known. You will notice that virtually there is no difference:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; import numpy</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; from scipy.integrate import ode</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; f=lambda t,y: -20*y        # The ODE</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; actual_solution=lambda t:numpy.exp(-20*t)  # actual solution</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; dt=0.01            # time step</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; solver=ode(f).set_integrator('dop853')  # solver</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; solver.set_initial_value(1,0)      # initial value</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; while solver.successful() and solver.t&lt;=1+dt:</strong></span>
<span class="strong"><strong>   # solve the equation at succesive time steps,</strong></span>
<span class="strong"><strong>   # until the time is greater than 1</strong></span>
<span class="strong"><strong>   # but make sure that the solution is successful</strong></span>
<span class="strong"><strong>       print (solver.t, solver.y, actual_solution(solver.t))</strong></span>
<span class="strong"><strong>   # We compare each numerical solution with the actual</strong></span>
<span class="strong"><strong>   # solution of the ODE</strong></span>
<span class="strong"><strong>       solver.integrate(solver.t + dt)    # solve next step</strong></span>
</pre></div><p>Once run, the preceding code gives us the following output:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&lt;scipy.integrate._ode.ode at 0x10eac5e50&gt;</strong></span>
<span class="strong"><strong>0 [ 1.] 1.0</strong></span>
<span class="strong"><strong>0.01 [ 0.81873075] 0.818730753078</strong></span>
<span class="strong"><strong>0.02 [ 0.67032005] 0.670320046036</strong></span>
<span class="strong"><strong>0.03 [ 0.54881164] 0.548811636094</strong></span>
<span class="strong"><strong>0.04 [ 0.44932896] 0.449328964117</strong></span>
<span class="strong"><strong>0.05 [ 0.36787944] 0.367879441171</strong></span>
<span class="strong"><strong>0.06 [ 0.30119421] 0.301194211912</strong></span>
<span class="strong"><strong>0.07 [ 0.24659696] 0.246596963942</strong></span>
<span class="strong"><strong>0.08 [ 0.20189652] 0.201896517995</strong></span>
<span class="strong"><strong>0.09 [ 0.16529889] 0.165298888222</strong></span>
<span class="strong"><strong>0.1 [ 0.13533528] 0.135335283237</strong></span>
<span class="strong"><strong>      ...</strong></span>
<span class="strong"><strong>0.9 [  1.52299797e-08] 1.52299797447e-08</strong></span>
<span class="strong"><strong>0.91 [  1.24692528e-08] 1.24692527858e-08</strong></span>
<span class="strong"><strong>0.92 [  1.02089607e-08] 1.02089607236e-08</strong></span>
<span class="strong"><strong>0.93 [  8.35839010e-09] 8.35839010137e-09</strong></span>
<span class="strong"><strong>0.94 [  6.84327102e-09] 6.84327102222e-09</strong></span>
<span class="strong"><strong>0.95 [  5.60279644e-09] 5.60279643754e-09</strong></span>
<span class="strong"><strong>0.96 [  4.58718175e-09] 4.58718174665e-09</strong></span>
<span class="strong"><strong>0.97 [  3.75566677e-09] 3.75566676594e-09</strong></span>
<span class="strong"><strong>0.98 [  3.07487988e-09] 3.07487987959e-09</strong></span>
<span class="strong"><strong>0.99 [  2.51749872e-09] 2.51749871944e-09</strong></span>
<span class="strong"><strong>1.0   [  2.06115362e-09] 2.06115362244e-09</strong></span>
</pre></div><p>The full output is displayed on the corresponding section of the IPython Notebook for this chapter. For systems of differential equations of first order with complex-valued functions, we have a wrapper of <code class="literal">ode</code>, which we call with the <code class="literal">complex_ode</code> command. Syntax and usage are similar to those of <code class="literal">ode</code>.</p><p>The syntax of <code class="literal">odeint</code> is much more intuitive, and more Python friendly:</p><div class="informalexample"><pre class="programlisting">odeint(func, y0, t, args=(), Dfun=None, col_deriv=0, full_output=0, ml=None, mu=None, rtol=None, atol=None, tcrit=None, h0=0.0, hmax=0.0, hmin=0.0, ixpr=0, mxstep=0, mxhnil=0, mxordn=12, mxords=5, printmessg=0)</pre></div><p>The most impressive <a id="id237" class="indexterm"/>part of this routine is that one is able to indicate not only the Jacobian, but also whether this is banded and how many nonzero diagonals are under or over the main diagonal we have (with the <code class="literal">ml</code> and <code class="literal">mu</code> options). This speeds up computations by a huge factor. Another amazing feature of <code class="literal">odeint</code> is the possibility to indicate critical points for the integration (<code class="literal">tcrit</code>).</p><p>We will now introduce an application to analyze Lorentz attractors with the routines presented in this section.</p></div>
<div class="section" title="Lorenz attractors"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec41"/>Lorenz attractors</h1></div></div></div><p>No book on scientific computing is complete without revisiting Lorenz attractors; SciPy excels both at computation of solutions and presentation of ideas based upon systems of differential equations, of course, and we will show how and why in this section.</p><p>Consider a two-dimensional fluid cell that is heated from underneath and cooled from above, much like <a id="id238" class="indexterm"/>what occurs with the Earth's atmosphere. This creates convection that can be modeled by a single partial differential equation, for which a decent approximation has the form of the following system of ordinary differential equations:</p><div class="mediaobject"><img src="graphics/7702OS_04_29.jpg" alt="Lorenz attractors"/></div><p>The variable <span class="emphasis"><em>x</em></span> represents the rate of convective overturning. The variables <span class="emphasis"><em>y</em></span> and <span class="emphasis"><em>z</em></span> stand for the horizontal and vertical temperature variations, respectively. This system depends on four physical parameters, the descriptions of which are far beyond the scope of this book. The important point is that we may model Earth's atmosphere with these equations, and in that case a good choice for the parameters is given by <code class="literal">sigma = 10.0</code>, and <code class="literal">b = 8/3.0</code>. For certain values of the third parameter, we have systems for which the solutions behave chaotically. Let's explore this effect with the help of SciPy.</p><p>In the following code snippet, we will use one of the solvers in the <code class="literal">scipy.integrate</code> module as well as the plotting utilities:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; import numpy</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; from numpy import linspace</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; import scipy</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; from scipy.integrate import odeint</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; import matplotlib.pyplot as plt</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; from mpl_toolkits.mplot3d import Axes3D</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; sigma=10.0</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; b=8/3.0</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; r=28.0</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; f = lambda x,t: [sigma*(x[1]-x[0]), r*x[0]-x[1]-x[0]*x[2], x[0]*x[1]-b*x[2]]</strong></span>
</pre></div><p>Let's choose a time interval <code class="literal">t</code> large enough with a sufficiently dense partition and any initial condition, <code class="literal">y0</code>. Then, issue the following commands:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; t=linspace(0,20,2000); y0=[5.0,5.0,5.0]</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; solution=odeint(f,y0,t)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; X=solution[:,0]; Y=solution[:,1]; Z=solution[:,2]</strong></span>
</pre></div><p>If we want to plot a 3D rendering of the solution obtained, we can do so as follows:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; import matplotlib.pyplot as plt</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; plt.gca(projection='3d'); plt.plot(X,Y,Z)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; plt.show()</strong></span>
</pre></div><p>This produces the <a id="id239" class="indexterm"/>following graph that shows a Lorenz attractor:</p><div class="mediaobject"><img src="graphics/7702OS_04_30.jpg" alt="Lorenz attractors"/></div><p>This is most illustrative and shows precisely the chaotic behavior of the solutions. Let's observe the fluctuations of the vertical temperature in detail, along with the fluctuation of horizontal temperature against vertical. Issue the following commands:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt;&gt;&gt; plt.rcParams['figure.figsize'] = (10.0, 5.0)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; plt.subplot(121); plt.plot(t,Z)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; plt.subplot(122); plt.plot(Y,Z)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; plt.show()</strong></span>
</pre></div><p>This produces the following the plots that show vertical temperature with respect to time (left-hand side plot) and horizontal versus vertical temperature (right-hand side plot):</p><div class="mediaobject"><img src="graphics/7702OS_04_31.jpg" alt="Lorenz attractors"/></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec42"/>Summary</h1></div></div></div><p>This chapter explored special functions, integration, interpolation, and optimization through the corresponding modules (<code class="literal">special</code>, <code class="literal">integrate</code>, <code class="literal">interpolate</code>, and <code class="literal">optimize</code>), as well as discussed solutions of systems of ordinary differential equations.</p><p>In <a class="link" href="ch05.html" title="Chapter 5. SciPy for Signal Processing">Chapter 5</a>, <span class="emphasis"><em>SciPy for Signal Processing</em></span>, we will describe the functionality of SciPy modules to analyze processes involving time series and spatial signals, including how to perform on numerical data the discrete Fourier transform, how to construct signals, how to apply filters on data, and how to interpolate images.</p></div></body></html>