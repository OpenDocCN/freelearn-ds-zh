- en: Chapter 8. Spark Graph Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A graph is a mathematical concept and a data structure in computer science.
    It has huge applications in many real-world use cases. It is used to model a pair-wise
    relationship between entities. An entity here is known as a vertex and two vertices
    are connected by an edge. A graph comprises a collection of vertices, and the
    edges connecting them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Conceptually, it is a deceptively simple abstraction, but when it comes to
    processing a huge number of vertices and edges, it is computationally intensive
    and consumes a lot of processing time and computing resources. Here is a representation
    of a graph with four vertices and three edges:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Spark Graph Processing](img/B05289_08_01_new.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Graphs and their uses
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The GraphX library
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The PageRank algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Connected component algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GraphFrames
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graph Queries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding graphs and their usage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are numerous application constructs that can be modeled as graphs. In
    a social networking application, the relationship between users can be modeled
    as a graph in which the users form the vertices of the graph and the relationships
    between users form the edges of the graph. In a multi-stage job scheduling application,
    the individual tasks form the vertices of the graph and the sequencing of the
    tasks forms the edges. In a road traffic modeling system, the towns form the vertices
    of the graph and the roads connecting the towns form the edges.
  prefs: []
  type: TYPE_NORMAL
- en: The edges of a given graph have a very important property, namely *the direction
    of the connection*. In many use cases, the direction of the connection doesn't
    matter. The case of connectivity between cities by roads is one such example.
    But if the use case is to produce driving directions within a city, the connectivity
    between traffic junctions has a direction. Take any two traffic junctions and
    there will be road connectivity, but it is also possible that it is a one-way
    road. So it all depends on the direction the traffic is flowing. If the road is
    open to traffic from traffic junction J1 to J2 but closed from J2 to J1, then
    the graph of driving directions will have a connectivity from J1 to J2 and not
    from J2 to J1\. In such cases, the edge connecting J1 and J2 has a direction.
    If the road between J2 and J3 is open in both directions, then the edge connecting
    J2 and J3 has no direction. A graph in which all the edges have a direction is
    called a **directed graph**.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When representing a graph pictorially, it is mandatory to give the direction
    on the edges of the directed graph. If it is not a directed graph, the edge can
    be represented without any direction at all or with direction to both sides. This
    is up to the individual's choice. *Figure 1* is not a directed graph, but is represented
    with directions to both the vertices that the edge is connecting.
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 2*, the relationship between two users in a social networking application
    use case is represented as a graph. Users form the vertices and the relationships
    between the users form the edges. User A follows User B. At the same time, User
    A is the son of User B. In this graph, there are two parallel edges sharing the
    same source and destination vertices. A graph containing parallel edges is called
    a multigraph. The graph shown in *Figure 2* is also a directed graph. This is
    a good example of a **directed multigraph**.
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding graphs and their usage](img/image_08_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2
  prefs: []
  type: TYPE_NORMAL
- en: In real-world use cases, the vertices and edges of a graph represent real-world
    entities. These entities have properties. For example, in the social connectivity
    graph of users from a social networking application, the users form the vertices
    and users have many properties such as name, e-mail, phone number, and so on.
    Similarly, the relationships between the users form the edges of the graph and
    the edges connecting user vertices can have properties such as relationship. Any
    graph processing application library should be flexible enough to attach any kind
    of property to the vertices and edges of a graph.
  prefs: []
  type: TYPE_NORMAL
- en: The Spark GraphX library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For graph processing, many libraries are available in the open source world.
    Giraph, Pregel, GraphLab, and Spark GraphX are some of them. Spark GraphX is one
    of the recent entrants into this space.
  prefs: []
  type: TYPE_NORMAL
- en: What is so special about Spark GraphX? Spark GraphX is a graph processing library
    built on top of the Spark data processing framework. Compared to the other graph
    processing libraries, Spark GraphX has a real advantage. It can make use of all
    the data processing capabilities of Spark. However, in reality, the performance
    of graph processing algorithms is not the only aspect that needs consideration.
  prefs: []
  type: TYPE_NORMAL
- en: In many applications, the data that needs to be modeled as a graph does not
    exist in that form naturally. In many use cases, more than the graph processing,
    lots of processor time and other computing resources are expended to get the data
    in the right format so that the graph processing algorithms can be applied. This
    is the sweet spot where the combination of the Spark data processing framework
    and the Spark GraphX library deliver their value. The data processing jobs to
    make the data ready to be consumed by the Spark GraphX can be easily done using
    the plethora of tools available in the Spark toolkit. In summary, the Spark GraphX
    library, which is part of the Spark family, combines the power of the core data
    processing capabilities of Spark and a very easy-to-use graph processing library.
  prefs: []
  type: TYPE_NORMAL
- en: Revisit the bigger picture once again, as given in *Figure 3*, to set the context
    and see what is being discussed here before getting into the use cases. Unlike
    other chapters, in this chapter, the code samples will only be done in Scala because
    the Spark GraphX library only has a Scala API available at the moment.
  prefs: []
  type: TYPE_NORMAL
- en: '![The Spark GraphX library](img/image_08_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3
  prefs: []
  type: TYPE_NORMAL
- en: GraphX overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In any real-world use case, it is easy to understand the concept of a graph
    comprising vertices and edges. But when it comes to the implementation, this is
    not a data structure that is very well understood by even good designers and programmers.
    The reason is simple: unlike other ubiquitous data structures such as list, set,
    map, queue, and so on, graphs are not commonly used in most applications. Taking
    this into consideration, the concepts are introduced slowly and steadily, one
    step at a time, with simple and trivial examples, before taking up some real-world
    use cases.'
  prefs: []
  type: TYPE_NORMAL
- en: The most important aspect of the Spark GraphX library is a data type, Graph,
    which extends the Spark **resilient distributed dataset** (**RDD**) and introduces
    a new graph abstraction. The graph abstraction in Spark GraphX is a directed multigraph
    with properties attached to all the vertices and edges. The properties for each
    of these vertices and edges can be user defined types that are supported by the
    Scala type system. These types are parameterized in the Graph type. A given graph
    may be required to have different data types for vertices or edges. This is possible
    by using a type system related by an inheritance hierarchy. In addition to all
    these basic ground rules, the library includes a collection of graph builders
    and algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: A vertex in a graph is identified by a unique 64-bit long identifier, `org.apache.spark.graphx.VertexId`.
    Instead of the VertexId type, a simple Scala type, Long, can also be used. In
    addition to that, vertices can take any type as a property. An edge in a graph
    should have a source vertex identifier, a destination vertex identifier, and any
    type as a property.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4* shows a graph with a vertex property as a String type and an edge
    property as a String type. In addition to the properties, each vertex has a unique
    identifier and each edge has a source vertex number and destination vertex number.'
  prefs: []
  type: TYPE_NORMAL
- en: '![GraphX overview](img/image_08_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4
  prefs: []
  type: TYPE_NORMAL
- en: When processing a graph, there are methods to get the vertices and edges. But
    these independent objects of a graph in isolation may not be sufficient while
    doing processing.
  prefs: []
  type: TYPE_NORMAL
- en: A vertex has its unique identifier and a property, as stated previously. An
    edge is uniquely identified by its source and destination vertices. To easily
    process each edge in graph processing applications, the triplet abstraction of
    the Spark GraphX library provides an easy way to access the properties of the
    source vertex, destination vertex, and the edge from a single object.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following Scala code snippet is used to create the graph shown in *Figure
    4* using the Spark GraphX library. After creating the graph, many methods are
    invoked on the graph that expose various properties of the graph. At the Scala
    REPL prompt, try the following statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Readers will be familiar with Spark programming using RDDs. The preceding code
    snippet elucidated the process of constructing the vertices and edges of a graph
    using RDDs. RDDs can be constructed using data persisted in various data stores.
    In real-world use cases, most of the time the data will come from external sources,
    such as NoSQL data stores, and there are ways to construct RDDs using such data.
    Once the RDDs are constructed, graphs can be constructed using that.
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding code snippet also explained the various methods available with
    the graph to get all the required details of a given graph. The teaser use case
    covered here is a very small graph in terms of size. In real-world use cases,
    the number of vertices and edges of a graph can be in the millions. Since all
    these abstractions are implemented as RDDs, all the inherent goodness of immutability,
    partitioning, distribution, and parallel processing comes out of the box, hence
    making graph processing highly scalable. Finally, the following tables show how
    the vertices and edges are represented:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Vertex table**:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **VertexId** | **Vertex property** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Thomas |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Krish |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Mathew |'
  prefs: []
  type: TYPE_TB
- en: '**Edge table:**'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Source VertexId** | **Destination VertexId** | **Edge property** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 2 | Follows |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 2 | Son |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 3 | Follows |'
  prefs: []
  type: TYPE_TB
- en: '**Triplet table**:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Source VertexId** | **Destination VertexId** | **Source vertex Property**
    | **Edge property** | **Destination vertex property** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 2 | Thomas | Follows | Krish |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 2 | Thomas | Son | Krish |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 3 | Krish | Follows | Mathew |'
  prefs: []
  type: TYPE_TB
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is important to note that these tables are only for explanation purposes.
    The real internal representation follows the rules and regulations of RDD representation.
  prefs: []
  type: TYPE_NORMAL
- en: If anything is represented as an RDD, it is bound to get partitioned and distributed.
    But if the partitioning and distribution are done freely, without any control
    for the graph, then it is going to be suboptimal when it comes to graph processing
    performance. Because of that, the creators of the Spark GraphX library have thought
    through this problem well in advance and implemented a graph partitioning strategy
    in order to have an optimized representation of the graph as RDDs.
  prefs: []
  type: TYPE_NORMAL
- en: Graph partitioning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is important to understand a little bit about how the graph RDDs are partitioned
    and distributed across various partitions. This will be useful for advanced optimizations
    that determine the partition and distribution of the various RDDs that are the
    constituent parts of a graph.
  prefs: []
  type: TYPE_NORMAL
- en: In general, there are three RDDs for a given graph. Apart from the vertex RDD
    and the edge RDD, one more RDD is used internally, and that is the routing RDD.
    To have optimal performance, all the vertices needed to form a given edge are
    kept in the same partition where the edge is stored. If a given vertex is participating
    in multiple edges and these edges are located in different partitions, then this
    particular vertex can be stored in multiple partitions.
  prefs: []
  type: TYPE_NORMAL
- en: To keep track of the partitions where a given vertex is stored redundantly,
    a routing RDD is also maintained, containing the vertex details and the partitions
    in which each vertex is available.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 5* explains this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graph partitioning](img/image_08_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 5*, assume that the edges are partitioned into partitions 1 and 2\.
    Also assume that the vertices are partitioned into partitions 1 and 2.
  prefs: []
  type: TYPE_NORMAL
- en: In partition 1, all the vertices required for the edges are available locally.
    But in partition 2, only one vertex for the edge is available locally. So the
    missing vertex is also stored in partition 2 so that all the required vertices
    are available locally.
  prefs: []
  type: TYPE_NORMAL
- en: To keep track of the replications, the vertex routing RDD maintains the partition
    numbers where a given vertex is available. In *Figure 5*, in the vertex routing
    RDD, callout symbols are used to show the partitions in which these vertices are
    replicated. In this way, while processing the edges or triplets, all the information
    related to the constituent vertices is available locally and performance will
    be highly optimal. Since the RDDs are immutable, the problems associated with
    information getting changed are removed, even if they are stored in multiple partitions.
  prefs: []
  type: TYPE_NORMAL
- en: Graph processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The constituent elements of a graph exposed to the users are the vertex RDD
    and the edge RDD. Just like any other data structure, a graph also undergoes lots
    of changes because of the change in the underlying data. To make the required
    graph operations to support various use cases, there are many algorithms available,
    using which the data hidden in the graph data structure can be processed to produce
    the desired business outcomes. Before getting into the algorithms to process a
    graph, it is good to understand some of the basics of graph processing using an
    air travel use case.
  prefs: []
  type: TYPE_NORMAL
- en: 'Assume that a person is trying to find a cheap return air ticket from Manchester
    to Bangalore. In the travel preferences, this person has mentioned that he/she
    doesn''t care about the number of stops but the price should be the lowest. Assume
    that the air ticket reservation system has picked up the same stops for both the
    onward and the return journey and produced the following routes or journey legs
    with the cheapest price:'
  prefs: []
  type: TYPE_NORMAL
- en: Manchester → London → Colombo → Bangalore
  prefs: []
  type: TYPE_NORMAL
- en: Bangalore → Colombo → London → Manchester
  prefs: []
  type: TYPE_NORMAL
- en: 'This route plan is a perfect example of a graph. If the onward journey is considered
    as one graph and the return journey is considered as another graph, the return
    journey graph can be produced by reversing the onward journey graph. At the Scala
    REPL prompt, try the following statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The source and destination of the onward journey legs are reversed in the return
    journey legs. When a graph is reversed, only the source and destination vertices
    of the edges are reversed and the identity of the vertices remains the same.
  prefs: []
  type: TYPE_NORMAL
- en: 'In other words, the vertex identifiers of each of the vertices remain the same.
    While processing a graph, it is important to know the names of the triplet attributes.
    They are useful for writing programs and processing the graph. As a continuation
    of the same Scala REPL session, try the following statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The following table gives the list of attributes of a triplet that can be used
    to process a graph and extract the required data from the graph. The preceding
    code snippet and the following table may be cross-verified to fully understand:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Triplet attribute** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `srcId` | Source vertex identifier |'
  prefs: []
  type: TYPE_TB
- en: '| `dstId` | Destination vertex identifier |'
  prefs: []
  type: TYPE_TB
- en: '| `attr` | Edge property |'
  prefs: []
  type: TYPE_TB
- en: '| `srcAttr` | Source vertex property |'
  prefs: []
  type: TYPE_TB
- en: '| `dstAttr` | Destination vertex property |'
  prefs: []
  type: TYPE_TB
- en: In a graph, vertices are RDDs and edges are RDDs, and just by virtue of that,
    transformations are possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, to demonstrate graph transformations, the same use case is used, with
    a slight twist. Assume that a travel agent is getting special discount prices
    from the airline companies for selected routes. The travel agent decides to keep
    the discount and offer the market price to his/her customers, and for this purpose
    he/she adds 10% to the price given by the airline company. This travel agent has
    noticed that the airport names are being displayed inconsistently and wanted to
    make sure that there is consistent representation when displayed throughout the
    website and decides to change all the stop names to upper case. As a continuation
    of the same Scala REPL session, try the following statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In essence, these transformations are truly RDD transformations. If there is
    a conceptual understanding of how these different RDDs are cobbled together to
    form a graph, any programmer with RDD programming proficiency will be able to
    do graph processing very well. This is another testament to the power of the unified
    programming model of Spark.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding use case did the map transformation on vertex and edge RDDs. Similarly,
    filter transformations are another useful type that is commonly used. Apart from
    these, all the transformations and actions can be used to process the vertex and
    edge RDDs.
  prefs: []
  type: TYPE_NORMAL
- en: Graph structure processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous section, one type of graph processing is done by individually
    processing the required vertices or edges. One disadvantage of this approach is
    that the processing is going through three different stages, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Extract vertices or edges from the graph
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Process the vertices or edges
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Re-create a new graph with the processed vertices and edges
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is tedious and prone to user programming errors. To circumvent this problem,
    there are some structural operators available in the Spark GraphX library that
    let users process the graph as an individual unit that produces a new graph.
  prefs: []
  type: TYPE_NORMAL
- en: One important structural operation has already been discussed in the previous
    section, which is the reversal of a graph producing a new graph with all the directions
    of the edges reversed. Another frequently used structural operation is the extraction
    of a subgraph from a given graph. The resultant subgraph can be the entire parent
    graph itself or a subset of the parent graph, depending on the operation that
    is done on the parent graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'When creating a graph from data from external sources, there is a possibility
    that the edges may have invalid vertices. This is very much a possibility if the
    vertices and the edges are created from the data coming from two different sources
    or different applications. With these vertices and edges, if a graph is created,
    some of the edges will have invalid vertices, and processing will result in unexpected
    outcomes. The following is a use case where some of the edges containing invalid
    vertices and pruning are done to get rid of that using a structural operator.
    At the Scala REPL prompt, try the following statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In huge graphs, at times depending on the use case, there can be a whole lot
    of parallel edges. In some use cases, it is possible to combine the data of the
    parallel edges and maintain only one edge instead of maintaining lots of parallel
    edges. In the preceding use case, the final graph without any invalid edges, there
    are parallel edges, one with the property `Follows` and the other with `Son`,
    which have the same source and destination vertices.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is fine to combine these parallel edges into one single edge with the property
    concatenated from the parallel edges, which will reduce the number of edges without
    losing information. This is accomplished by the groupEdges structural operation
    of the graph. As a continuation of the same Scala REPL session, try the following
    statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The preceding structural change in the graph reduced the number of edges by
    grouping the edges. When the edge property is numerical, and if it makes sense
    to consolidate by aggregating them, then also reduce the number of edges by removing
    the parallel edges, which can reduce the graph processing time considerably.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One important point to note in this code snippet is that the graph has been
    partitioned before the group-by operation on the edges.
  prefs: []
  type: TYPE_NORMAL
- en: By default, the edges and the constituent vertices of a given graph need not
    be co-located in the same partition. For the group-by operation to work, all the
    parallel edges have to be located on the same partition. The CanonicalRandomVertexCut
    partition strategy makes sure that colocation happens for all the edges between
    two vertices, irrespective of direction.
  prefs: []
  type: TYPE_NORMAL
- en: There are some more structural operators available in the Spark GraphX library
    and a consultation of the Spark documentation will give a good insight into them.
    They can be used depending on the use case.
  prefs: []
  type: TYPE_NORMAL
- en: Tennis tournament analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since the basic graph processing fundamentals are in place, now it is time
    to take up a real-world use case that uses graphs. Here, a tennis tournament''s
    results are modeled using a graph. The Barclays ATP World Tour 2015 singles competition
    results are modeled using a graph. The vertices contain the player details and
    the edges contain the individual matches played. The edges are formed in such
    a way that the source vertex is the player who won the match and the destination
    vertex is the player who lost the match. The edge property contains the type of
    the match, the points the winner got in the match, and the head-to-head count
    of the players in the match. The points system used here is fictitious and is
    nothing but a weight earned by the winner in that particular match. The initial
    group matches carried the least weight, the semi-final matches carried more weight,
    and the final match carried the most weight. With this way of modeling the results,
    find out the following details by processing the graph:'
  prefs: []
  type: TYPE_NORMAL
- en: List all the match details.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: List all the matches with player names, the match type, and the result.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: List all the Group 1 winners with the points in the match.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: List all the Group 2 winners with the points in the match.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: List all the semi-final winners with the points in the match.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: List the final winner with the points in the match.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: List the players with the total points they earned in the whole tournament.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: List the winner of the match by finding the highest number of points scored
    by the player.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the group-based matches, because of the round robin scheme of draws, it is
    possible that the same players can meet more than once. Find if there are any
    such players who have played each other more than once in this tournament.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: List the players who have won at least one match.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: List the players who have lost at least one match.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: List the players who have won at least one match and lost at least one match.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: List the players who have no wins at all.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: List the players who have no losses at all.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Those who are not familiar with the game of tennis have no need to worry because
    the rules of the games are not discussed here and are not required to understand
    this use case. For all practical purposes, it is to be taken only as a game played
    between two people, where one wins and the other loses. At the Scala REPL prompt,
    try the following statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The graph containing the tennis tournament has been created, and from now on,
    all that is going to be done is the processing of this base graph and extracting
    information from it to fulfill the requirements of the use cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'It is worth noticing here that the usage of triplets in graphs comes in handy
    for extracting all the required data elements of a given tennis match, including
    who was playing, who won, and the match type, from a single object. The following
    implementations of analysis use cases involve filtering the tennis match records
    of the tournament. Here, only simple filtering logic is used, but in real-world
    use cases, any complex logic can be implemented in functions, and that can be
    passed as arguments to the filter transformations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The following implementations of analysis use cases involve grouping by key
    and doing summary calculations. It is not limited to just finding the sum of the
    tennis match record points, as shown in the following use case implementations;
    rather, user-defined functions can be used to do the calculations as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The following implementations of analysis use cases involve finding unique
    records from the query. The Spark distinct transformation does that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: In this use case, not much effort has been made to make the results pretty because
    they are reduced to simple RDD-based structures that can be manipulated however
    required using the RDD programming techniques that were already covered in the
    initial chapters of the book.
  prefs: []
  type: TYPE_NORMAL
- en: The highly succinct and uniform programming model of Spark, in conjunction with
    the Spark GraphX library, helps developers build real-world use cases with very
    few lines of code. This also demonstrates that once the right graph structure
    is built with the relevant data, with the supported graph operations, lots of
    truth that is hidden in the underlying data can be brought to light.
  prefs: []
  type: TYPE_NORMAL
- en: Applying the PageRank algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A research paper, titled *The Anatomy of a Large-Scale Hypertextual Web Search
    Engine,* by Sergey Brin and Lawrence Page, revolutionized web searching, and Google
    based its search engine on this concept of PageRank and came to dominate other
    web search engines.
  prefs: []
  type: TYPE_NORMAL
- en: When searching the web using Google, pages that are ranked highly by its algorithm
    are displayed. In the context of graphs, instead of web pages, if vertices are
    ranked based on the same algorithm, lots of new inferences can be made. From the
    outside, it may sound like this PageRank algorithm is useful only for web searches.
    But it has immense potential to be applied to many other areas.
  prefs: []
  type: TYPE_NORMAL
- en: In graph parlance, if there is an edge, E, connecting two vertices, from V1
    to V2, according to the PageRank algorithm, V2 is more important than V1\. In
    a huge graph of vertices and edges, it is possible to calculate the PageRank of
    each and every vertex.
  prefs: []
  type: TYPE_NORMAL
- en: The PageRank algorithm can be applied very well to the tennis tournament analysis
    use case covered in the preceding section. In the graph representation that is
    adopted here, each match is represented as an edge. The source vertex has the
    winner's details and the destination vertex has the loser's details. In the game
    of tennis, if this can be termed as some fictitious importance ranking, then in
    a given match the winner has higher importance ranking than the loser.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the graph in the previous use case is taken to demonstrate the PageRank
    algorithm, then that graph has to be reversed so that the winner of each match
    becomes the destination vertex of each and every edge. At the Scala REPL prompt,
    try the following statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: If the preceding code is scrutinized carefully, it can be seen that the highest
    ranked players have won the highest number of matches.
  prefs: []
  type: TYPE_NORMAL
- en: Connected component algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a graph, finding a subgraph consisting of connected vertices is a very common
    requirement with tremendous applications. In any graph, two vertices are that
    connected to each other by paths consisting of one or more edges, and are not
    connected to any other vertex in the same graph, are called a connected component.
    For example, in a graph, G, vertex V1 is connected to V2 by an edge and V2 is
    connected to V3 by another edge. In the same graph, G, vertex V4 is connected
    to V5 by another edge. In this case V1 and V3 are connected, V4 and V5 are connected
    and V1 and V5 are not connected. In graph G, there are two connected components.
    The Spark GraphX library has an implementation of the connected components algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: In a social networking application, if the connections between the users are
    modeled as a graph, finding whether a given user is connected to another user
    is achieved by checking whether there is a connected component with these two
    vertices. In computer games, maze traversing from point A to point B can be done
    using a connected components algorithm by modeling the maze junctions as vertices
    and the paths connecting the junctions as edges in a graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'In computer networks, checking whether packets can be sent from one IP address
    to another IP address is achieved by using a connected components algorithm. In
    logistics applications, such as a courier service, checking whether a packet can
    be sent from point A to point B is achieved by using a connected components algorithm.
    *Figure 6* shows a graph with three connected components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Connected component algorithm](img/image_08_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 6* is the pictorial representation of a graph. In it, there are three
    *clusters* of vertices connected by edges. In other words, there are three connected
    components in this graph.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The use case of users in a social networking application in which they follow
    each other is taken up here again for elucidation purposes. By extracting the
    connected components of the graph, it is possible to see whether any two users
    are connected or not. *Figure 7* shows the user graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Connected component algorithm](img/image_08_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7
  prefs: []
  type: TYPE_NORMAL
- en: 'In the graph depicted in *Figure 7*, it is clearly evident that there are two
    connected components. It is easy to say that Thomas and Mathew are connected and
    at the same time Thomas and Martin are not connected. If the connected component
    graph is extracted, it can be seen that Thomas and Martin will have the same connected
    component identifier, and at the same time, Thomas and Martin will have a different
    connected component identifiers. At the Scala REPL prompt, try the following statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: There are some more graph processing algorithms available in the Spark GraphX
    library, and a detailed treatment of the complete set of algorithms deserves book
    on its own. The point here is that the Spark GraphX library provides very easy-to-use
    graph algorithms that fit very well into Spark's uniform programming model.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding GraphFrames
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Spark GraphX library is the graph processing library that has the least
    programming language support. Scala is the only programming language supported
    by the Spark GraphX library. GraphFrames is a new graph processing library available
    as an external Spark package developed by Databricks, University of California,
    Berkley, and Massachusetts Institute of Technology, built on top of Spark DataFrames.
    Since it is built on top of DataFrames, all the operations that can be done on
    DataFrames are potentially possible on GraphFrames, with support for programming
    languages such as Scala, Java, Python, and R with a uniform API. Since GraphFrames
    is built on top of DataFrames, the persistence of data, support for numerous data
    sources, and powerful graph queries in Spark SQL are additional benefits users
    get for free.
  prefs: []
  type: TYPE_NORMAL
- en: Just like the Spark GraphX library, in GraphFrames the data is stored in vertices
    and edges. The vertices and edges use DataFrames as the data structure. The first
    use case covered in the beginning of this chapter is used again to elucidate GraphFrames-based
    graph processing.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**CAUTION**: GraphFrames is an external Spark package. It has some incompatibility
    with Spark 2.0\. Because of that, the following code snippets will not work with
    Spark 2.0\. They work with Spark 1.6\. Refer to their website to check Spark 2.0
    support.'
  prefs: []
  type: TYPE_NORMAL
- en: 'At the Scala REPL prompt of Spark 1.6, try the following statements. Since
    GraphFrames is an external Spark package, while bringing up the appropriate REPL,
    the library has to be imported and the following command is used in the terminal
    prompt to fire up the REPL and make sure that the library is loaded without any
    error messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: When creating DataFrames for the GraphFrame, the only thing to keep in mind
    is that there are some mandatory columns for the vertices and the edges. In the
    DataFrame for vertices, the id column is mandatory. In the DataFrame for edges,
    the src and dst columns are mandatory. Apart from that, any number of arbitrary
    columns can be stored with both the vertices and the edges of a GraphFrame. In
    the Spark GraphX library, the vertex identifier must be a long integer, but the
    GraphFrame doesn't have any such limitations and any type is supported as the
    vertex identifier. Readers should already be familiar with DataFrames; any operation
    that can be done on a DataFrame can be done on the vertices and edges of a GraphFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: All the graph processing algorithms supported by Spark GraphX are supported
    by GraphFrames as well.
  prefs: []
  type: TYPE_NORMAL
- en: The Python version of GraphFrames has fewer features. Since Python is not a
    supported programming language for the Spark GraphX library, GraphFrame to GraphX
    and GraphX to GraphFrame conversions are not supported in Python. Since readers
    are familiar with the creation of DataFrames in Spark using Python, the Python
    example is omitted here. Moreover, there are some pending defects in the GraphFrames
    API for Python and not all the features demonstrated previously using Scala function
    properly in Python at the time of writing.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding GraphFrames queries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Spark GraphX library is the RDD-based graph processing library, but GraphFrames
    is a Spark DataFrame-based graph processing library that is available as an external
    package. Spark GraphX supports many graph processing algorithms, but GraphFrames
    supports not only graph processing algorithms, but also graph queries. The major
    difference between graph processing algorithms and graph queries is that graph
    processing algorithms are used to process the data hidden in a graph data structure,
    while graph queries are used to search for patterns in the data hidden in a graph
    data structure. In GraphFrame parlance, graph queries are also known as motif
    finding. This has tremendous applications in genetics and other biological sciences
    that deal with sequence motifs.
  prefs: []
  type: TYPE_NORMAL
- en: From a use case perspective, take the use case of users following each other
    in a social media application. Users have relationships between them. In the previous
    sections, these relationships were modeled as graphs. In real-world use cases,
    such graphs can become really huge, and if there is a need to find users with
    relationships between them in both directions, it can be expressed as a pattern
    in graph query, and such relationships can be found using easy programmatic constructs.
    The following demonstration models the relationship between the users in a GraphFrame,
    and a pattern search is done using that.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the Scala REPL prompt of Spark 1.6, try the following statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Note that the columns in the graph query result are formed with the elements
    given in the search pattern. There is no limit to the way the patterns can be
    formed.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note the data type of the graph query result. It is a DataFrame object. That
    brings a great flexibility in processing the query results using the familiar
    Spark SQL library.
  prefs: []
  type: TYPE_NORMAL
- en: The biggest limitation of the Spark GraphX library is that its API is not currently
    supported with programming languages such as Python and R. Since GraphFrames is
    a DataFrame-based library, once it has matured, it will enable graph processing
    in all the programming languages supported by DataFrames. This Spark external
    package is definitely a potential candidate to be included as part of the Spark.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information please visit the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://spark.apache.org/docs/1.5.2/graphx-programming-guide.html](https://spark.apache.org/docs/1.5.2/graphx-programming-guide.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://en.wikipedia.org/wiki/2015_ATP_World_Tour_Finals_%E2%80%93_Singles](https://en.wikipedia.org/wiki/2015_ATP_World_Tour_Finals_%E2%80%93_Singles)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://www.protennislive.com/posting/2015/605/mds.pdf](http://www.protennislive.com/posting/2015/605/mds.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://infolab.stanford.edu/~backrub/google.html](http://infolab.stanford.edu/~backrub/google.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://graphframes.github.io/index.html](http://graphframes.github.io/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/graphframes/graphframes](https://github.com/graphframes/graphframes)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://spark-packages.org/package/graphframes/graphframes](https://spark-packages.org/package/graphframes/graphframes)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Graph is a very useful data structure that has great application potential.
    Even though it is not very commonly used in most applications, there are some
    unique application use cases where using a Graph as a data structure is essential.
    A data structure is effectively used only when it is used in conjunction with
    well tested and highly optimized algorithms. Mathematicians and computer scientists
    have come up with many algorithms to process data that is part of a graph data
    structure. The Spark GraphX library has a large number of such algorithms implemented
    on top of the Spark core. This chapter provided a whirlwind tour of the Spark
    GraphX library and covered some of the basics through use cases at an introductory
    level.
  prefs: []
  type: TYPE_NORMAL
- en: The DataFrame-based graph abstraction named GraphFrames, which comes in an external
    Spark package available separately from Spark, has tremendous potential in graph
    processing as well as graph queries. A brief introduction to this external Spark
    package has been provided in order to do graph queries to find patterns in graphs.
  prefs: []
  type: TYPE_NORMAL
- en: Any book teaching a new technology has to conclude with an application covering
    its salient features. Spark is no different. So far in this book, Spark as a next
    generation data processing platform has been covered. Now it is the time to tie
    up all the loose ends and build an end-to-end application. The next chapter is
    going to cover the design and development of a data processing application using
    Spark and the family of libraries built on top of it.
  prefs: []
  type: TYPE_NORMAL
