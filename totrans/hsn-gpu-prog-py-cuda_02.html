<html><head></head><body><div><div><h1 class="header-title">Setting Up Your GPU Programming Environment</h1>
                
            
            
                
<p>We will now see how to set up an appropriate environment for GPU programming under both Windows and Linux. In both cases, there are several steps we will have to take. We will proceed through these steps one-by-one, noting any differences between Linux and Windows as we proceed. You should, of course, feel free to skip or ignore any sections or comments that don't apply to your choice of operating system.</p>
<p>The reader should note that we will only cover two platforms for 64-bit Intel/AMD-based PCs in this chapter—Ubuntu LTS (long-term support) releases and Windows 10. Note that any Ubuntu LTS-based Linux operating systems (such as Xubuntu, Kubuntu, or Linux Mint) are also equally appropriate to the generic Unity/GNOME-based Ubuntu releases.</p>
<p>We suggest the use of Python 2.7 over Python 3.x. Python 2.7 has stable support across all libraries that we use in this text, and we have tested every example given in this book with Python 2.7 on both Windows and Linux platforms. Python 3.x users can make use of this book, but should be aware of the differences between Python 2.7 and Python 3.x. Some of the examples in this have been tested on using Python 3.7, but require standard changes, such as adding parentheses with the Python <kbd>print</kbd> function. </p>
<p>Packt author Dr. Sebastian Raschka provides a list of key differences between Python 2.7 and 3.x at <a href="https://sebastianraschka.com/Articles/2014_python_2_3_key_diff.html">https://sebastianraschka.com/Articles/2014_python_2_3_key_diff.html</a>.<a href="https://sebastianraschka.com/Articles/2014_python_2_3_key_diff.html"/></p>
<p>We suggest the Anaconda Python 2.7 distribution in particular for both Windows and Linux users, since this can be installed on a user-by-user basis without <kbd>sudo</kbd> or <kbd>administrator</kbd> access, contains all necessary data science and visualization modules needed for this text, and uses fast pre-optimized NumPy/SciPy packages that make use of Intel's <strong>Math Kernel Library</strong> (<strong>MKL</strong>). (The default Linux <kbd>/usr/bin/python</kbd> installation should also be sufficient for this text, but you may have to install some packages manually, such as NumPy and Matplotlib.) </p>
<p>Anaconda Python (both 2.7 and 3.x versions) can be downloaded for all platforms at <a href="https://www.anaconda.com/download/">https://www.anaconda.com/download/.</a></p>
<p>Users who are on other supported platforms (for example, macOS, Windows 7/8, Windows Server 2016, Red Hat/Fedora, OpenSUSE, and CENTOS) should consult the official NVIDIA CUDA documentation (<a href="https://docs.nvidia.com/cuda/">https://docs.nvidia.com/cuda/</a>) for further details. Furthermore, there are other possibilities for hardware: the reader interested in embedded systems or robotics with some experience in boards, such as the Raspberry Pi may wish to start with an ARM-based NVIDIA Jetson development board, while the reader interested in cloud computing or web programming may consider remotely using an appropriate Azure or AWS instance. In these cases, the reader is encouraged to read the official documentation to set up their drivers, compiler, and CUDA Toolkit. Some of the steps in this chapter may or may not apply.</p>
<p>The learning outcomes for this chapter are:</p>
<ul>
<li>Ensuring that we have the appropriate hardware</li>
<li>Installing the NVIDIA GPU drivers</li>
<li>Setting up an appropriate C/C++ programming environment</li>
<li>Installing the NVIDIA CUDA Toolkit</li>
<li>Setting up our Python environment for GPU programming</li>
</ul>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Technical requirements</h1>
                
            
            
                
<p>An installation of Anaconda Python 2.7 is suggested for this chapter at <a href="https://www.anaconda.com/download/">https://www.anaconda.com/download/.</a><br/>
This chapter's code is also available on GitHub at <a href="https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA">https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA.</a></p>
<p><a href="https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA"/></p>
<p>For more information about the pre-requisites, check the Preface of this book; and for the software and hardware requirements, check the README section in <a href="https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA">https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA.</a></p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Ensuring that we have the right hardware</h1>
                
            
            
                
<p>For this book, we recommend that you have the following hardware as a minimum:</p>
<ul>
<li>64-bit Intel/AMD-based PC</li>
<li>4 gigabytes (GB) of RAM</li>
<li>NVIDIA GeForce GTX 1050 GPU (or higher)</li>
</ul>
<p>This configuration will ensure that you can comfortably learn GPU programming, run all of the examples in this book, and also run some of the other newer and interesting GPU-based software, such as Google's TensorFlow (a machine learning framework) or the Vulkan SDK (a cutting-edge graphics API). </p>
<div><strong>Note that you must have an NVIDIA brand GPU to make use of this book!</strong> The CUDA Toolkit is proprietary for NVIDIA cards, so it won't work for programming Intel HD or Radeon GPUs.</div>
<p>As stated, we will be assuming that you are using either the Windows 10 or Ubuntu LTS (long-term support) release.</p>
<p>Ubuntu LTS releases generally have version numbers of the form 14.04, 16.04, 18.04, and so on.</p>
<p>Ubuntu LTS, is by and large, the most mainstream version of Linux, which ensures maximum compatibility with new software and toolkits. Keep in mind there are many variations of Linux that are based on Ubuntu, such as Linux Mint or Xubuntu, and these generally work equally well. (I have personally found that Linux Mint works fairly well out of the box for GPU-equipped laptops.)</p>
<p>We should note that we are assuming that you have at least an entry-level GTX 1050 (Pascal) GPU, or the equivalent in any newer architecture. Note that many of the examples in this book will most likely work on most older GPUs, but they have only been tested on a GTX 1050 (under Windows 10) and GTX 1070 (under Linux) by the author. While the examples haven't been tested on older GPUs, a 2014-era entry level Maxwell architecture GPU, such as a GTX 750, should also be sufficient for the purposes of this text.</p>
<p>If you are using a desktop PC, please ensure that you have physically installed your GPU by following all the included instructions before proceeding.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Checking your hardware (Linux)</h1>
                
            
            
                
<p>We will now do a few basic checks in Linux to ensure that we have the right hardware. Let's first open up a Terminal and drop to the bash command line—you can do this quickly in Ubuntu by pressing the combination <em>Ctrl</em> + <em>Alt</em> + <em>T</em>.</p>
<p>Let's now check our processor by typing <kbd>lscpu</kbd> and pressing <em>Enter</em>. A lot of information will appear, but just look at the first line and make sure that the architecture is indeed x86_64:</p>
<div><img src="img/e6983512-f18c-418c-9307-e1169baf3591.png" style="" width="387" height="221"/></div>
<p>Next, we check our memory capacity by typing <kbd>free -g</kbd> at the bash prompt and then again press <em>Enter</em>. This will tell us the total number of proper memory that we have in gigabytes in the first entry of the first row, as well as the amount of memory in swap space in the following row:</p>
<div><img src="img/c3146279-5d0d-47b3-a8d9-cc7db17d807b.png" style="" width="877" height="68"/></div>
<p class="mce-root"/>
<p>This is certainly sufficient memory.</p>
<p>Finally, let's see whether we have an appropriate GPU. NVIDIA GPUs communicate with our PC via the PCI bus, so we can use the <kbd>lspci</kbd> command to list all PCI hardware. There is usually a lot of other hardware listed, so let's use the <kbd>grep</kbd> command to filter for just NVIDIA GPUs by entering <kbd>lspci | grep -e "NVIDIA"</kbd> at the bash prompt:</p>
<div><img src="img/4365ea85-449d-4d80-b0c6-4514fdebe0e5.png" width="1052" height="23"/></div>
<p>This is a GTX 1070, which fortunately exceeds our need for at least a GTX 1050.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Checking your hardware (windows)</h1>
                
            
            
                
<p>First, we must open the Windows panel. We do this by pressing <em>Windows</em> + <em>R</em> and then entering <kbd>Control Panel</kbd> at the prompt, as demonstrated in the following screenshot:</p>
<div><img src="img/80e71c36-7fdf-4962-bb3a-107afc76022b.png" style="" width="1120" height="605"/></div>
<p>The Windows Control Panel will pop up. Now click on System and Security, and then choose System on the following screen. This will immediately tell us the amount of RAM that we have and whether we have a 64-bit processor:</p>
<div><img src="img/73eb0d90-a13d-42e1-a0ac-7941a3f8ff5e.png" style="" width="1984" height="1406"/></div>
<p>To check our GPU, click on Device Manager in the upper left-hand corner of this window. The Windows Device Manager will then pop up; you can then select the Display adapters drop-down box to check which GPUs are on your system:</p>
<div><img src="img/e084b0d1-48a8-4280-86a5-dd505ff38c0c.png" style="" width="1199" height="1113"/></div>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Installing the GPU drivers</h1>
                
            
            
                
<p>If you already have drivers for your GPU installed, you may possibly skip this step; moreover, some versions of CUDA are pre-packaged with the latest drivers. Quite often, CUDA is very particular about which driver you have installed and may not even work with the CUDA Toolkit driver, so you may have to experiment with several different drivers before you find one that works.</p>
<p>Generally speaking, Windows has better CUDA driver compatibility and a more user-friendly installation than Linux. Windows users may consider skipping this step and just use the driver that is packaged with the CUDA Toolkit, which we will install a little later in this chapter. We would strongly suggest that Linux users (particularly Linux laptop users), however, closely follow all the steps in this section before proceeding.</p>
<p class="mce-root"/>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Installing the GPU drivers (Linux)</h1>
                
            
            
                
<p>In Ubuntu, the default driver for NVIDIA GPUs is an open-source driver called Nouveau; unfortunately, this does not work with CUDA at all, so we will have to install a proprietary driver. We will have to add the special <kbd>graphics-drivers</kbd> repository to our package manager to be able to download proprietary NVIDIA drivers to our Ubuntu system. We add the repository by typing the following line into the bash prompt:</p>
<pre class="command subordinate"><strong>sudo add-apt-repository ppa:graphics-drivers/ppa</strong></pre>
<p>Since this is a <kbd>sudo</kbd> superuser command, you will have to enter your password. We now synchronize our system with the new repository by typing the following line:</p>
<pre><strong>sudo apt-get update</strong></pre>
<p>We should now be ready to install our driver. From the Ubuntu desktop, press <em>Windows</em> + <em>R</em>, and then enter <kbd>software and drivers</kbd>:</p>
<div><img src="img/33637818-a7d1-4fb9-8b47-9c09741b001c.png" style="" width="356" height="212"/></div>
<p>The Software &amp; Drivers setup menu should appear. From here, click on the tab marked Additional Drivers. You should see a selection of available stable proprietary drivers for your GPU; choose the newest one you see I(n my case, it is <kbd>nvidia-driver-396</kbd>, demonstrated as follows):</p>
<p class="mce-root"/>
<p class="mce-root"/>
<div><img src="img/d75d9ff2-f44a-459c-ba72-7ded556d763b.png" width="886" height="550"/></div>
<p>With the latest driver selected, click on Apply Changes. You will be prompted again for your <kbd>sudo</kbd> password, and then the driver will install; a progress bar should appear. Note that this process can take a long time and it may appear that your computer is <em>hanging</em>; this process can take well over an hour, so please be patient. </p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Finally, when the process is complete, reset your computer, and return to your Ubuntu desktop. Now type <em>Windows</em> + <em>A</em>, and then enter <kbd>nvidia-settings</kbd> (or alternatively, run this program from a bash prompt). The NVIDIA X Server Settings manager should appear, and indicate that you are using the appropriate driver version:</p>
<div><img src="img/bdab7346-2e9f-4615-b61b-a91f6b6b8588.png" width="851" height="513"/></div>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Installing the GPU drivers (Windows)</h1>
                
            
            
                
<p>To reiterate—it is generally suggested that the reader initially skip this step, and then install the drivers that are included with the CUDA Toolkit.</p>
<p>The latest drivers for Windows are available directly from NVIDIA at <a href="http://www.nvidia.com/Download/">http://www.nvidia.com/Download/</a>. Simply choose the appropriate Windows 10 drivers for your GPU from the drop-down menu, which are executable (<kbd>.exe</kbd>) files. Simply install the driver by double-clicking on the file from the file manager.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Setting up a C++ programming environment</h1>
                
            
            
                
<p>Now that we have our drivers installed, we have to set up our C/C++ programming environment; both Python and CUDA are particular about what compilers and IDEs they may integrate with, so you may have to be careful. In the case of Ubuntu Linux users, the standard repository compilers and IDEs generally work and integrate perfectly with the CUDA Toolkit, while Windows users might have to exercise a little more caution.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Setting up GCC, Eclipse IDE, and graphical dependencies (Linux)</h1>
                
            
            
                
<p>Open up a Terminal from the Ubuntu desktop (<em>Ctrl</em> + <em>Alt</em> + <em>T</em>). We first update the <kbd>apt</kbd> repository as follows:</p>
<pre><strong>sudo apt-get update</strong></pre>
<p>Now we can install everything we need for CUDA with one additional line:</p>
<pre><strong>sudo apt-get install build-essential binutils gdb eclipse-cdt</strong></pre>
<p>Here, <kbd>build-essential</kbd> is the package with the <kbd>gcc</kbd> and <kbd>g++</kbd> compilers, and other utilities such as make; <kbd>binutils</kbd> has some generally useful utilities, such as the LD linker, <kbd>gdb</kbd> is the debugger, and Eclipse is the IDE that we will be using.</p>
<p>Let's also install a few additional dependencies that will allow us to run some of the graphical (OpenGL) demos included with the CUDA Toolkit with this line:</p>
<pre><strong>sudo apt-get install freeglut3 freeglut3-dev libxi-dev libxmu-dev</strong></pre>
<p>Now you should be good to go to install the CUDA Toolkit.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Setting up Visual Studio (Windows)</h1>
                
            
            
                
<p>At the time of writing, only one version of Visual Studio appears to ingrate perfectly with both Python and the latest CUDA Toolkits—Visual Studio 2015; that is, Visual Studio version 14.0.</p>
<p>While it may be possible to make a sub-installation of this under a later version of Visual Studio (for example, 2017), we would suggest to the reader that you directly install Visual Studio 2015 with C/C++ support onto your system. </p>
<p>Visual Studio Community 2015, the free version of this software, can be downloaded at <a href="https://visualstudio.microsoft.com/vs/older-downloads/">https://visualstudio.microsoft.com/vs/older-downloads/</a>. </p>
<p>Here, we will do a minimalist installation, with only the necessary components for CUDA. We run the installation software, and select the Custom installation:</p>
<div><img src="img/64390e7f-5b9c-4768-9d4e-b7b4116d57c7.png" style="" width="575" height="805"/></div>
<p>Click Next, then click the drop-down box for Programming Languages, and then choose Visual C++ (feel free to select other packages or programming languages if you want or need them for other purposes, but Visual C++ is all we will need for GPU programming):</p>
<div><img src="img/63cef2a7-be52-499c-af90-810042581d5d.png" style="" width="575" height="805"/></div>
<p>This should take some time to install. After this is complete, we will be ready to install the CUDA Toolkit.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Installing the CUDA Toolkit</h1>
                
            
            
                
<p>Finally, we are beginning to get close to our goal! We now download our CUDA Toolkit by heading over to <a href="https://developer.nvidia.com/cuda-downloads">https://developer.nvidia.com/cuda-downloads</a>.<a href="https://developer.nvidia.com/cuda-downloads"> </a>Select the appropriate operating system and you will see several options. In the case of both Windows and Linux, there are both network and local installations. I tend to use the local installation option under both Windows and Linux, because I prefer to download the entire package up-front; if there are any network problems, then you can be assured they won't occur while you are installing the CUDA Toolkit. </p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Installing the CUDA Toolkit (Linux)</h1>
                
            
            
                
<p>In the case of Linux users, you will see that there are choices for using a <kbd>.deb</kbd> package and a <kbd>.run</kbd> file; for most users, I would suggest going with the <kbd>.deb</kbd> file, since this will install any missing packages that CUDA requires automatically. The <kbd>.run</kbd> file installs outside of your system's <strong>Advanced Package Tool </strong>(<strong>APT</strong>) system, which effectively just copies the appropriate files to the system's <kbd>/usr</kbd> binary and library directories. If you don't want to interfere with your system's APT system or repositories, and have a good understanding of Linux, the <kbd>.run</kbd> file may be more appropriate. In either case, carefully follow the instructions given on the site about installing the package, which can vary slightly from one version to the next.</p>
<p>After the package is finished installing, you may have to configure your <kbd>PATH</kbd> and <kbd>LD_SYSTEM_CONFIG</kbd> environment variables so that your system can find the appropriate binary executable and library files needed for CUDA. I would suggest doing this by appending the followiang lines to the end of your <kbd>.bashrc</kbd> file in your user directory. Open the <kbd>~/.bashrc</kbd> file with your favorite text editor, such as <kbd>gedit</kbd>, <kbd>nano</kbd>, <kbd>emacs</kbd>, or <kbd>vim</kbd>, and, at the very bottom of the file, add the following lines:</p>
<pre>export PATH="/usr/local/cuda/bin:${PATH}<br/>export LD_LIBRARY_PATH="/usr/local/cuda/lib64:${LD_LIBRARY_PATH}"</pre>
<p>Save the file and then exit the Terminal. You can now ensure that you've correctly installed the toolkit by opening a new Terminal and typing <kbd>nvcc --version</kbd> and then pressing <em>Enter</em>, which will give you the version information of the compiler for your toolkit. (<kbd>nvcc</kbd> is the command-line CUDA C compiler, which is analogous to the <kbd>gcc</kbd> compiler.)</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Installing the CUDA Toolkit (Windows)</h1>
                
            
            
                
<p>In the case of Windows users, you can install the package by double-clicking on the <kbd>.exe</kbd> file and following all the on-screen prompts.</p>
<p>Once the installation is complete, reset your system. We will now ensure that CUDA was installed correctly by checking the <kbd>nvcc</kbd> compiler. Under the Start menu, click on the <kbd>Visual Studio 2015</kbd> folder, and then click VS2015 x64 Native Tools Command Prompt. A Terminal window will pop up; now type <kbd>nvcc --version</kbd> and press <em>Enter</em>, which should give you the version information of the NVIDIA compiler.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Setting up our Python environment for GPU programming</h1>
                
            
            
                
<p>With our compilers, IDEs, and the CUDA Toolkit properly installed on our system, we now can set up an appropriate Python environment for GPU programming. There are many options here, but we explicitly recommend that you work with the Anaconda Python Distribution. Anaconda Python is a self-contained and user-friendly distribution that can be installed directly in your user directory, and which does not require any administrator or <kbd>sudo</kbd> level system access to install, use, or update.</p>
<p>Keep in mind that Anaconda Python comes in two flavors—Python 2.7, and Python 3. Since Python 3 is currently not as well-supported for some of the libraries we will be using, we will be using Python 2.7 in this book, which still has a broad mainstream usage.</p>
<p>You can install Anaconda Python by going to <a href="https://www.anaconda.com/download">https://www.anaconda.com/download</a>, choosing your operating system, and then by choosing to download the Python 2.7 version of the distribution. Follow the instructions given on the Anaconda site to install the distribution, which is relatively straightforward. We can now set up our local Python installation for GPU programming.</p>
<p>We will now set up what is arguably the most important Python package for this book: Andreas Kloeckner's PyCUDA package.</p>
<p class="mce-root"/>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Installing PyCUDA (Linux)</h1>
                
            
            
                
<p>Open up a command line in Linux. Ensure that your <kbd>PATH</kbd> variable is set up correctly to use the local Anaconda installation of Python (rather than the system-wide installation) by typing <kbd>which python</kbd> at the bash prompt and pressing <em>Enter</em> (Anaconda should have automatically configured your <kbd>.bashrc</kbd> during installation); this should tell you that the Python binary is in your local <kbd>~/anaconda2/bin</kbd> directory, rather than in the <kbd>/usr/bin</kbd> directory. If this isn't the case, open a text editor and put the line <kbd>export PATH="/home/${USER}/anaconda2/bin:${PATH}"</kbd> at the end of your <kbd>~/.bashrc</kbd> file, save this, open a new Terminal, and then check again.</p>
<p>There are several options for installation of PyCUDA. The easiest option is to install the latest stable version from the PyPI repository by typing <kbd>pip install pycuda</kbd>. You can also install the latest version of PyCUDA by following the instructions at the PyCUDA official website at <a href="https://mathema.tician.de/software/pycuda/">https://mathema.tician.de/software/pycuda/</a>. Please note that if you wish to re-install PyCUDA from a different source, be sure to uninstall it first with <kbd>pip uninstall pycuda</kbd>.</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Creating an environment launch script (Windows)</h1>
                
            
            
                
<p>Windows users will need to be particularly careful that both their Visual Studio and Anaconda Python environment variables are set up correctly in order to use PyCUDA; otherwise, Python will not be able to find NVIDIA's <kbd>nvcc</kbd> CUDA compiler or Microsoft's <kbd>cl.exe</kbd> C++ compiler. Fortunately, batch scripts are included that will set up these environments for us automatically, but we will have to be careful that these are executed each and every time we want to do GPU programming.</p>
<p>We will, therefore, create a batch script that will launch an appropriate IDE or command-line environment by calling these other two scripts in succession. (This script is also available at <a href="https://github.com/btuomanen/handsongpuprogramming/blob/master/2/launch-python-cuda-environment.bat">https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA/blob/master/2/launch-python-cuda-environment.bat</a>.)</p>
<p>Be sure to first open up Windows Notepad, and follow along:</p>
<p>First, find where your <kbd>vcvars.bat</kbd> file for Visual Studio is; in the case of Visual Studio 2015, it is at <kbd>C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\vcvarsall.bat</kbd>.</p>
<p>Type the following line into your text editor, and then press <em>Enter</em>: </p>
<pre>call "C:\Program Files (x86)\Microsoft Visual Studio 14.0\VC\vcvarsall.bat" amd64</pre>
<p>We now need to call the Anaconda's <kbd>activate.bat</kbd> script to set up the Anaconda Python environment variables; the standard path is <kbd>Anaconda2\Scripts\activate.bat</kbd>. We have to further indicate where the Anaconda libraries are with an argument to this script. In my case, the second line in my launch script would be <kbd>call "C:\Users\%username%\Anaconda2\Scripts\activate.bat" C:\Users\%username%\Anaconda2</kbd>.</p>
<p>Finally, the last line of our batch script will launch whatever environment—IDE or command-line prompt—you prefer to program in, which will inherit all of the necessary environment and system variables the prior two scripts will set up. If you prefer the old standard DOS-style Command Prompt, this line should just be <kbd>cmd</kbd>. If you like to work from PowerShell, change this to <kbd>powershell</kbd>. It will be necessary to use the command line in some cases, particularly for accessing the command line <kbd>pip</kbd> and <kbd>conda</kbd> for updating your Python library.</p>
<p>Finally, save this file to your desktop with the filename <kbd>launch-python-cuda-environment.bat</kbd>. You can now launch our Python GPU programming environment by double-clicking this file.</p>
<p>(Keep in mind that if you wish to use the Jupyter Notebook or Spyder Python IDEs, you can simply launch these from the command line with <kbd>jupyter-notebook</kbd> or <kbd>spyder</kbd>, or alternatively, you can make a batch script that just replaces <kbd>cmd</kbd> with the appropriate IDE launch command.)</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Installing PyCUDA (Windows)</h1>
                
            
            
                
<p>Due to the fact that most Python libraries are primarily written by and for Linux users, it is suggested that you install a pre-built PyCUDA wheel binary from Christoph Gohlke's site at the following address: <a href="https://www.lfd.uci.edu/~gohlke/pythonlibs/#pycuda">https://www.lfd.uci.edu/~gohlke/pythonlibs/#pycuda</a>. Download a file of the from <kbd>pycuda‑2017.1.1+cuda(VERSION)‑cp27‑cp27m‑win_amd64.whl</kbd><a> </a>where version is your CUDA version number. You can now install PyCUDA by typing the following on the command line, and replacing <kbd>pycuda.whl</kbd> with the full path and filename of your PyCUDA wheel:</p>
<pre><strong>pip install pycuda.whl</strong></pre>
<p>(Alternatively, you can try installing PyCUDA from the PyPI repository with <kbd>pip install pycuda</kbd> , or by following the instructions on the PyCUDA website.)</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Testing PyCUDA</h1>
                
            
            
                
<p>Finally, we're at the point where we can see whether our GPU programming environment actually works. We will run a small program from the next chapter that will query our GPU and yield some relevant information about the model number, memory, number of cores, architecture, and so forth. Get the Python file (<kbd>deviceQuery.py</kbd>) from directory <kbd>3</kbd> in the repository, which is also available at <a href="https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA/blob/master/3/deviceQuery.py">https://github.com/PacktPublishing/Hands-On-GPU-Programming-with-Python-and-CUDA/blob/master/3/deviceQuery.py</a>.</p>
<p>If you are using Windows, be sure to launch the GPU programming environment by launching the <kbd>.bat</kbd> file on our desktop we made in the last section. Otherwise, if you are using Linux, open a bash Terminal. Now type the following line and press <em>Enter—</em><kbd>python deviceQuery.py</kbd>. </p>
<p>This will output many lines of data, but the first few lines should indicate that your GPU has been detected by PyCUDA, and you should see the model number in the following line:</p>
<div><img src="img/51a20697-a1c6-435b-95a0-9845a9506ab6.png" style="" width="1074" height="250"/></div>
<p>Congratulations, you are now ready to embark upon the world of GPU programming!</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Summary</h1>
                
            
            
                
<p>Setting up your Python environment for GPU programming can be a very delicate process. The Anaconda Python 2.7 distribution is suggested for both Windows and Linux users for the purposes of this text. First, we should ensure that we have the correct hardware for GPU programming; generally speaking, a 64-bit Windows or Linux PC with 4 gigabytes of RAM and any entry-level NVIDIA GPU from 2016 or later will be sufficient for our ends. Windows users should be careful in using a version of Visual Studio that works well with both the CUDA Toolkit and Anaconda (such as VS 2015), while Linux users should be particularly careful in the installation of their GPU drivers, and set up the appropriate environment variables in their <kbd>.bashrc</kbd> file. Furthermore, Windows users should create an appropriate launch script that will set up their environment for GPU programming and should use a pre-compiled wheel file for the installation of the PyCUDA library.</p>
<p>Now, with our programming environment set up and in place, we will spend the next chapter learning the very basics of GPU programming. We will see how to write and read data to and from the GPU's memory, and how to write some very simple <em>elementwise</em> GPU functions in CUDA C. (If you have seen the classic 1980's film <em>The Karate Kid</em>, then you might think of the following chapter as the "wax on, wax off" of GPU programming.)</p>


            

            
        
    </div></div>
<div><div><h1 class="header-title">Questions</h1>
                
            
            
                
<ol>
<li>Can we run CUDA on our main processor's built-in Intel HD GPU? What about on a discrete AMD Radeon GPU?</li>
<li>Does this book use Python 2.7 or Python 3.7 for examples?</li>
<li>What program do we use in Windows to see what GPU hardware we have installed?</li>
<li>What command-line program do we use in Linux to see what GPU hardware we have installed?</li>
<li>What is the command we use in Linux to determine how much memory our system has?</li>
<li>If we don't want to alter our Linux system's APT repository, should we use the <kbd>run</kbd> or <kbd>deb</kbd> installer for CUDA?</li>
</ol>


            

            
        
    </div></div></body></html>