<html><head></head><body><div class="chapter" title="Chapter&#xA0;2.&#xA0;Replacing and Correcting Words"><div class="titlepage"><div><div><h1 class="title"><a id="ch02"/>Chapter 2. Replacing and Correcting Words</h1></div></div></div><p>In this chapter, we will cover:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Stemming words</li><li class="listitem" style="list-style-type: disc">Lemmatizing words with WordNet</li><li class="listitem" style="list-style-type: disc">Translating text with Babelfish</li><li class="listitem" style="list-style-type: disc">Replacing words matching regular expressions</li><li class="listitem" style="list-style-type: disc">Removing repeating characters</li><li class="listitem" style="list-style-type: disc">Spelling correction with Enchant</li><li class="listitem" style="list-style-type: disc">Replacing synonyms</li><li class="listitem" style="list-style-type: disc">Replacing negations with antonyms</li></ul></div><div class="section" title="Introduction"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec16"/>Introduction</h1></div></div></div><p>In this chapter, we will go over various word replacement and correction techniques. The recipes cover the gamut of linguistic compression, spelling correction, and text normalization. All of these methods can be very useful for pre-processing text before search indexing, document classification, and text analysis.</p></div></div>
<div class="section" title="Stemming words"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec17"/>Stemming words</h1></div></div></div><a id="id68" class="indexterm"/><a id="id69" class="indexterm"/><p>
<span class="strong"><strong>Stemming</strong></span> is a technique for removing <span class="emphasis"><em>affixes</em></span> from a word, ending up with the <span class="emphasis"><em>stem</em></span>. For example, the stem of "cooking" is "cook", and a good stemming algorithm knows that the "ing" <span class="emphasis"><em>suffix</em></span> can be removed. Stemming is most commonly used by search engines for indexing words. Instead of storing all forms of a word, a search engine can store only the stems, greatly reducing the size of index while increasing retrieval accuracy.</p><a id="id70" class="indexterm"/><p>One of the most common stemming algorithms is the <span class="strong"><strong>Porter Stemming Algorithm</strong></span>, by Martin Porter. It is designed to remove and replace well known suffixes of English words, and its usage in NLTK will be covered next.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note02"/>Note</h3><p>The resulting stem is not always a valid word. For example, the stem of "cookery" is "cookeri". This is a feature, not a bug.</p></div></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec41"/>How to do it...</h2></div></div></div><p>NLTK comes with an implementation of the Porter Stemming Algorithm, which is very easy to use. Simply instantiate the <code class="literal">PorterStemmer</code> class and call the <code class="literal">stem()</code> method with the word you want to stem.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from nltk.stem import PorterStemmer
&gt;&gt;&gt; stemmer = PorterStemmer()
&gt;&gt;&gt; stemmer.stem('cooking')
'cook'
&gt;&gt;&gt; stemmer.stem('cookery')
'cookeri'</pre></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec42"/>How it works...</h2></div></div></div><a id="id71" class="indexterm"/><a id="id72" class="indexterm"/><p>The <code class="literal">PorterStemmer</code> knows a number of regular word forms and suffixes, and uses that knowledge to transform your input word to a final stem through a series of steps. The resulting stem is often a shorter word, or at least a common form of the word, that has the same root meaning.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec43"/>There's more...</h2></div></div></div><a id="id73" class="indexterm"/><p>There are other stemming algorithms out there besides the Porter Stemming Algorithm, such as the <span class="strong"><strong>Lancaster Stemming Algorithm</strong></span>, developed at Lancaster University. NLTK includes it as the <a id="id74" class="indexterm"/>
<code class="literal">LancasterStemmer</code> class. At the time of writing, there is no definitive research demonstrating the superiority of one algorithm over the other. However, Porter Stemming is generally the default choice.</p><p>All the stemmers covered next inherit from the <a id="id75" class="indexterm"/>
<code class="literal">StemmerI</code> interface, which defines the <a id="id76" class="indexterm"/>
<code class="literal">stem()</code> method. The following is an inheritance diagram showing this:</p><div class="mediaobject"><img src="graphics/3609OS_02_01.jpg" alt="There's more..."/></div><div class="section" title="LancasterStemmer"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec14"/>LancasterStemmer</h3></div></div></div><p>T<a id="id77" class="indexterm"/>he <code class="literal">LancasterStemmer</code> functions just like the <code class="literal">PorterStemmer</code>, but can produce slightly different results. It is known to be slightly more aggressive than the <code class="literal">P</code>
<a id="id78" class="indexterm"/>
<code class="literal">orterStemmer</code>.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from nltk.stem import LancasterStemmer
&gt;&gt;&gt; stemmer = LancasterStemmer()
&gt;&gt;&gt; stemmer.stem('cooking')
'cook'
&gt;&gt;&gt; stemmer.stem('cookery')
'cookery'</pre></div></div><div class="section" title="RegexpStemmer"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec15"/>RegexpStemmer</h3></div></div></div><p>Y<a id="id79" class="indexterm"/>ou can also construct your own stemmer using the <code class="literal">RegexpStemmer</code>. It takes a single regular expression (either compiled or as a string) and will remove any prefix or suffix that matches.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from nltk.stem import RegexpStemmer
&gt;&gt;&gt; stemmer = RegexpStemmer('ing')
&gt;&gt;&gt; stemmer.stem('cooking')
'cook'
&gt;&gt;&gt; stemmer.stem('cookery')
'cookery'
&gt;&gt;&gt; stemmer.stem('ingleside')
'leside'</pre></div><p>A <code class="literal">RegexpStemmer</code> should only be used in very specific cases that are not covered by the <code class="literal">PorterStemmer</code> or <code class="literal">LancasterStemmer</code>.</p></div><div class="section" title="SnowballStemmer"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec16"/>SnowballStemmer</h3></div></div></div><p>N<a id="id80" class="indexterm"/>ew in NLTK 2.0b9 is the <code class="literal">SnowballStemmer</code>, which supports 13 non-English languages. To use it, you create an instance with the name of the language you are using, and then call the <code class="literal">s</code>
<a id="id81" class="indexterm"/>
<code class="literal">tem()</code> method. Here is a list of all the supported languages, and an example using the Spanish <code class="literal">SnowballStemmer</code>:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from nltk.stem import SnowballStemmer
&gt;&gt;&gt; SnowballStemmer.languages
('danish', 'dutch', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')
&gt;&gt;&gt; spanish_stemmer = SnowballStemmer('spanish')
&gt;&gt;&gt; spanish_stemmer.stem('hola')
u'hol'</pre></div></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec44"/>See also</h2></div></div></div><p>In the next recipe, we will cover lemmatization, which is quite similar to stemming, but subtly different.</p></div></div>
<div class="section" title="Lemmatizing words with WordNet"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec18"/>Lemmatizing words with WordNet</h1></div></div></div><p>
<span class="strong"><strong>Lemmatization</strong></span><a id="id82" class="indexterm"/>
<a id="id83" class="indexterm"/>
<a id="id84" class="indexterm"/> is very similar to stemming, but is more akin to synonym replacement. A<a id="id85" class="indexterm"/> <span class="emphasis"><em>lemma</em></span> is a root word, as opposed to the root <span class="emphasis"><em>stem</em></span>. So unlike stemming, you are always left with a valid word which means the same thing. But the word you end up with can be completely different. A few examples will explain lemmatization...</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec45"/>Getting ready</h2></div></div></div><p>Be sure you have unzipped the <code class="literal">wordnet</code> corpus in <code class="literal">nltk_data/corpora/wordnet</code>. This will allow the <code class="literal">WordNetLemmatizer</code> to access WordNet. You should also be somewhat familiar with the part-of-speech tags covered in the <span class="emphasis"><em>Looking up synsets for a word in WordNet</em></span> recipe of <a class="link" href="ch01.html" title="Chapter 1. Tokenizing Text and WordNet Basics">Chapter 1</a>, <span class="emphasis"><em>Tokenizing Text and WordNet Basics</em></span>.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec46"/>How to do it...</h2></div></div></div><p>We will use the <code class="literal">WordNetLemmatizer</code> to find lemmas:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from nltk.stem import WordNetLemmatizer
&gt;&gt;&gt; lemmatizer = WordNetLemmatizer()
&gt;&gt;&gt; lemmatizer.lemmatize('cooking')
'cooking'
&gt;&gt;&gt; lemmatizer.lemmatize('cooking', pos='v')
'cook'
&gt;&gt;&gt; lemmatizer.lemmatize('cookbooks')
'cookbook'</pre></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec47"/>How it works...</h2></div></div></div><p>T<a id="id86" class="indexterm"/>he <code class="literal">WordNetLemmatizer</code> <a id="id87" class="indexterm"/>is a thin wrapper around the WordNet corpus, and uses the <code class="literal">morphy()</code> function of the <code class="literal">W</code>
<a id="id88" class="indexterm"/>
<code class="literal">ordNetCorpusReader</code> to find a lemma. If no lemma is found, the word is returned as it is. Unlike with stemming, knowing the part of speech of the word is important. As demonstrated previously, "cooking" does not have a lemma unless you specify that the part of speech (<code class="literal">pos</code>) is a <span class="emphasis"><em>verb</em></span>. This is because the default part of speech is a <span class="emphasis"><em>noun</em></span>, and since "cooking" is not a noun, no lemma is found. "Cookbooks", on the other hand, is a noun, and its lemma is the singular form, "cookbook".</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec48"/>There's more...</h2></div></div></div><p>Here's an example that illustrates one of the major differences between stemming and lemmatization:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from nltk.stem import PorterStemmer
&gt;&gt;&gt; stemmer = PorterStemmer()
&gt;&gt;&gt; stemmer.stem('believes')
'believ'
&gt;&gt;&gt; lemmatizer.lemmatize('believes')
'belief'</pre></div><p>Instead of just chopping off the "es" like the <code class="literal">PorterStemmer</code>, the <code class="literal">WordNetLemmatizer</code> finds a valid root word. Where a stemmer only looks at the form of the word, the lemmatizer looks at the meaning of the word. And by returning a lemma, you will always get a valid word.</p><div class="section" title="Combining stemming with lemmatization"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec17"/>Combining stemming with lemmatization</h3></div></div></div><p>S<a id="id89" class="indexterm"/>
<a id="id90" class="indexterm"/>temming and lemmatization can be combined to compress words more than either process can by itself. These cases are somewhat rare, but they do exist:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; stemmer.stem('buses')
'buse'
&gt;&gt;&gt; lemmatizer.lemmatize('buses')
'bus'
&gt;&gt;&gt; stemmer.stem('bus')
'bu'</pre></div><p>I<a id="id91" class="indexterm"/>
<a id="id92" class="indexterm"/>n this example, stemming saves one character, lemmatizing saves two characters, and stemming the lemma saves a total of three characters out of five characters. That is nearly a 60% compression rate! This level of word compression over many thousands of words, while unlikely to always produce such high gains, can still make a huge difference.</p></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec49"/>See also</h2></div></div></div><p>In the previous recipe, we covered stemming basics and WordNet was introduced in the <span class="emphasis"><em>Looking up synsets for a word in WordNet</em></span> and <span class="emphasis"><em>Looking up lemmas</em></span> and <span class="emphasis"><em>synonyms in WordNet</em></span> recipes of <a class="link" href="ch01.html" title="Chapter 1. Tokenizing Text and WordNet Basics">Chapter 1</a>, <span class="emphasis"><em>Tokenizing Text and WordNet Basics</em></span>. Looking forward, we will cover the <span class="emphasis"><em>Using WordNet for Tagging</em></span> recipe in <a class="link" href="ch04.html" title="Chapter 4. Part-of-Speech Tagging">Chapter 4</a>, <span class="emphasis"><em>Part-of-Speech Tagging</em></span>.</p></div></div>
<div class="section" title="Translating text with Babelfish"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec19"/>Translating text with Babelfish</h1></div></div></div><p>
<span class="strong"><strong>Babelfish</strong></span><a id="id93" class="indexterm"/>
<a id="id94" class="indexterm"/>
<a id="id95" class="indexterm"/> is an online language translation API provided by Yahoo. With it, you can translate text in a <span class="emphasis"><em>source language</em></span> to a <span class="emphasis"><em>target language</em></span>. NLTK comes with a simple interface for using it.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec50"/>Getting ready</h2></div></div></div><p>Be sure you are connected to the internet first. The <a id="id96" class="indexterm"/>
<code class="literal">babelfish.translate()</code> function requires access to Yahoo's online API in order to work.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec51"/>How to do it...</h2></div></div></div><p>To translate your text, you first need to know two things:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">The language of your text or source language.</li><li class="listitem">The language you want to translate to or target language.</li></ol></div><p>Language detection is outside the scope of this recipe, so we will assume you already know the source and target languages.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from nltk.misc import babelfish
&gt;&gt;&gt; babelfish.translate('cookbook', 'english', 'spanish')
'libro de cocina'
&gt;&gt;&gt; babelfish.translate('libro de cocina', 'spanish', 'english')
'kitchen book'
&gt;&gt;&gt; babelfish.translate('cookbook', 'english', 'german')
'Kochbuch'
&gt;&gt;&gt; babelfish.translate('kochbuch', 'german', 'english')
'cook book'</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note03"/>Note</h3><p>You cannot translate using the same language for both source and target. Attempting to do so will raise a <code class="literal">BabelfishChangedError</code>.</p></div></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec52"/>How it works...</h2></div></div></div><p>The<a id="id97" class="indexterm"/>
<a id="id98" class="indexterm"/> <code class="literal">translate()</code> function is a small function that sends a <code class="literal">urllib</code><a id="id99" class="indexterm"/> request to <a class="ulink" href="http://babelfish.yahoo.com/translate_txt">http://babelfish.yahoo.com/translate_txt</a>, and then searches the response for the translated text.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note04"/>Note</h3><p>If Yahoo, for whatever reason, had changed their HTML response to the point that <code class="literal">translate()</code> cannot identify the translated text, a <code class="literal">BabelfishChangedError</code> will be raised. This is unlikely to happen, but if it does, you may need to upgrade to a newer version of NLTK and/or report the error.</p></div></div></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec53"/>There's more...</h2></div></div></div><p>T<a id="id100" class="indexterm"/>here is also a fun function called <code class="literal">babelize()</code> that translates back and forth between the source and target language until there are no more changes.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; for text in babelfish.babelize('cookbook', 'english', 'spanish'):
...  print text
cookbook
libro de cocina
kitchen book
libro de la cocina
book of the kitchen</pre></div><div class="section" title="Available languages"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec18"/>Available languages</h3></div></div></div><p>You can see all the languages available for translation by examining the <code class="literal">available_languages</code><a id="id101" class="indexterm"/> attribute.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; babelfish.available_languages
['Portuguese', 'Chinese', 'German', 'Japanese', 'French', 'Spanish', 'Russian', 'Greek', 'English', 'Korean', 'Italian']</pre></div><p>The lowercased version of each of these languages can be used as a source or target language for translation.</p></div></div></div>
<div class="section" title="Replacing words matching regular expressions"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec20"/>Replacing words matching regular expressions</h1></div></div></div><p>N<a id="id102" class="indexterm"/>ow we are going to get into the process of replacing words. Where stemming and lemmatization are a kind of <span class="emphasis"><em>linguistic compression</em></span>, and word replacement can be thought of as <span class="emphasis"><em>error correction</em></span>, or <span class="emphasis"><em>text normalization</em></span>.</p><p>For this recipe, we will be replacing words based on regular expressions, with a focus on <span class="emphasis"><em>expanding contractions</em></span>. Remember when we were tokenizing words in <a class="link" href="ch01.html" title="Chapter 1. Tokenizing Text and WordNet Basics">Chapter 1</a>, <span class="emphasis"><em>Tokenizing Text and WordNet Basics</em></span> and it was clear that most tokenizers had trouble with contractions? This recipe aims to fix that by replacing contractions with their expanded forms, such as by replacing "can't" with "cannot", or "would've" with "would have".</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec54"/>Getting ready</h2></div></div></div><p>Understanding how this recipe works will require a basic knowledge of regular expressions and the <code class="literal">re</code> module. The key things to know are <span class="emphasis"><em>matching patterns</em></span> and the <code class="literal">re.subn()</code>
<a id="id103" class="indexterm"/>
 function.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec55"/>How to do it...</h2></div></div></div><p>First, we need to define a number of replacement patterns. This will be a list of tuple pairs, where the first element is the pattern to match on, and the second element is the replacement.</p><p>Next, we will create a <code class="literal">RegexpReplacer</code> class that will compile the patterns, and provide a <code class="literal">replace()</code> method <a id="id104" class="indexterm"/>to substitute all found patterns with their replacements.</p><p>The following code can be found in the <code class="literal">replacers.py</code> module and is meant to be imported, not typed into the console:</p><div class="informalexample"><pre class="programlisting">import re

replacement_patterns = [
  (r'won\'t', 'will not'),
  (r'can\'t', 'cannot'),
  (r'i\'m', 'i am'),
  (r'ain\'t', 'is not'),
  (r'(\w+)\'ll', '\g&lt;1&gt; will'),
  (r'(\w+)n\'t', '\g&lt;1&gt; not'),
  (r'(\w+)\'ve', '\g&lt;1&gt; have'),
  (r'(\w+)\'s', '\g&lt;1&gt; is'),
  (r'(\w+)\'re', '\g&lt;1&gt; are'),
  (r'(\w+)\'d', '\g&lt;1&gt; would')

]
class RegexpReplacer(object):
  def __init__(self, patterns=replacement_patterns):
    self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]

  def replace(self, text):
    s = text
    for (pattern, repl) in self.patterns:
      (s, count) = re.subn(pattern, repl, s)
    return s</pre></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec56"/>How it works...</h2></div></div></div><p>Here is a simple usage example:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from replacers import RegexpReplacer
&gt;&gt;&gt; replacer = RegexpReplacer()
&gt;&gt;&gt; replacer.replace("can't is a contraction")
'cannot is a contraction'
&gt;&gt;&gt; replacer.replace("I should've done that thing I didn't do")
'I should have done that thing I did not do'</pre></div><p>
<code class="literal">RegexpReplacer.replace()</code><a id="id105" class="indexterm"/>
<a id="id106" class="indexterm"/> works by replacing every instance of a replacement pattern with its corresponding substitution pattern. In <code class="literal">replacement_patterns</code>, we have defined tuples such as (<code class="literal">r'(\w+)\'ve', '\g&lt;1&gt; have'</code>). The first element matches a group of ASCII characters followed by <code class="literal">'ve</code>. By grouping the characters before the <code class="literal">'ve</code> in parenthesis, a match group is found and can be used in the substitution pattern with the <code class="literal">\g&lt;1&gt;</code> reference. So we keep everything before <code class="literal">'ve</code>, then replace <code class="literal">'ve</code> with the word <code class="literal">have</code>. This is how "should've" can become "should have".</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec57"/>There's more...</h2></div></div></div><p>This replacement technique can work with any kind of regular expression, not just contractions. So you could replace any occurrence of "&amp;" with "and", or eliminate all occurrences of "-" by replacing it with the empty string. The <code class="literal">RegexpReplacer</code> can take any list of replacement patterns for whatever purpose.</p><div class="section" title="Replacement before tokenization"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec19"/>Replacement before tokenization</h3></div></div></div><p>Let us try using the <code class="literal">RegexpReplacer</code> as a preliminary step before tokenization:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from nltk.tokenize import word_tokenize
&gt;&gt;&gt; from replacers import RegexpReplacer
&gt;&gt;&gt; replacer = RegexpReplacer()
&gt;&gt;&gt; word_tokenize("can't is a contraction")
['ca', "n't", 'is', 'a', 'contraction']
&gt;&gt;&gt; word_tokenize(replacer.replace("can't is a contraction"))
['can', 'not', 'is', 'a', 'contraction']</pre></div><p>Much better! By eliminating the contractions in the first place, the tokenizer will produce cleaner results. Cleaning up text before processing is a common pattern in natural language processing.</p></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec58"/>See also</h2></div></div></div><p>For more information on tokenization, see the first three recipes in <a class="link" href="ch01.html" title="Chapter 1. Tokenizing Text and WordNet Basics">Chapter 1</a>, <span class="emphasis"><em>Tokenizing Text and WordNet Basics</em></span>. For more replacement techniques, continue reading the rest of this chapter.</p></div></div>
<div class="section" title="Removing repeating characters"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec21"/>Removing repeating characters</h1></div></div></div><p>I<a id="id107" class="indexterm"/>n everyday language, people are often not strictly grammatical. They will write things like "I looooooove it" in order to emphasize the word "love". But computers don't know that "looooooove" is a variation of "love" unless they are told. This recipe presents a method for removing those annoying repeating characters in order to end up with a "proper" English word.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec59"/>Getting ready</h2></div></div></div><p>As in the previous recipe, we will be making use of the <code class="literal">re</code> module, and more specifically, backreferences. A<a id="id108" class="indexterm"/> <span class="strong"><strong>backreference</strong></span> is a way to refer to a previously matched group in a regular expression. This is what will allow us to match and remove repeating characters.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec60"/>How to do it...</h2></div></div></div><p>We will create a class that has the same form as the <code class="literal">RegexpReplacer</code> from the previous recipe. It will have a <code class="literal">replace()</code> method that takes a single word and returns a more correct version of that word, with dubious repeating characters removed. The following code can be found in <code class="literal">replacers.py</code> and is meant to be imported:</p><div class="informalexample"><pre class="programlisting">import re

class RepeatReplacer(object):
  def __init__(self):
    self.repeat_regexp = re.compile(r'(\w*)(\w)\2(\w*)')
    self.repl = r'\1\2\3'

  def replace(self, word):
    repl_word = self.repeat_regexp.sub(self.repl, word)
    if repl_word != word:
      return self.replace(repl_word)

    else:
      return repl_word</pre></div><p>And now some example use cases:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from replacers import RepeatReplacer
&gt;&gt;&gt; replacer = RepeatReplacer()
&gt;&gt;&gt; replacer.replace('looooove')
'love'
&gt;&gt;&gt; replacer.replace('oooooh')
'oh'
&gt;&gt;&gt; replacer.replace('goose')
'gose'</pre></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec61"/>How it works...</h2></div></div></div><p>
<code class="literal">RepeatReplacer</code><a id="id109" class="indexterm"/>
<a id="id110" class="indexterm"/> starts by compiling a regular expression for matching and defining a replacement string with backreferences. The <code class="literal">repeat_regexp</code> matches three groups:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Zero or more starting characters <code class="literal">(\w*)</code>.</li><li class="listitem">A single character <code class="literal">(\w)</code>, followed by another instance of that character <code class="literal">\2</code>.</li><li class="listitem">Zero or more ending characters <code class="literal">(\w*)</code>.</li></ol></div><p>The <span class="emphasis"><em>replacement string</em></span> is then used to keep all the matched groups, while discarding the backreference to the second group. So the word "looooove" gets split into <code class="literal">(l)(o)o(ooove)</code> and then recombined as "loooove", discarding the second "o". This continues until only one "o" remains, when <code class="literal">repeat_regexp</code> no longer matches the string, and no more characters are removed.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec62"/>There's more...</h2></div></div></div><p>In the preceding examples, you can see that the <code class="literal">RepeatReplacer</code> is a bit too greedy and ends up changing "goose" into "gose". To correct this issue, we can augment the <code class="literal">replace()</code> function with a WordNet lookup. If WordNet recognizes the word, then we can stop replacing characters. Here is the WordNet augmented version:</p><div class="informalexample"><pre class="programlisting">import re
from nltk.corpus import wordnet

class RepeatReplacer(object):
  def __init__(self):
    self.repeat_regexp = re.compile(r'(\w*)(\w)\2(\w*)')
    self.repl = r'\1\2\3'

  def replace(self, word):
    if wordnet.synsets(word):
      return word
    repl_word = self.repeat_regexp.sub(self.repl, word)

    if repl_word != word:
      return self.replace(repl_word)
    else:
      return repl_word</pre></div><p>Now, "goose" will be found in WordNet, and no character replacement will take place. And "oooooh" will become "ooh" instead of "oh", because "ooh" is actually a word in WordNet, defined as an expression of admiration or pleasure.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec63"/>See also</h2></div></div></div><p>Read the next recipe to learn how to correct misspellings. And for more on WordNet, refer to the WordNet recipes in <a class="link" href="ch01.html" title="Chapter 1. Tokenizing Text and WordNet Basics">Chapter 1</a>, <span class="emphasis"><em>Tokenizing Text and WordNet Basics</em></span>. We will also be using WordNet for antonym replacement later in this chapter.</p></div></div>
<div class="section" title="Spelling correction with Enchant"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec22"/>Spelling correction with Enchant</h1></div></div></div><p>R<a id="id111" class="indexterm"/>
<a id="id112" class="indexterm"/>eplacing repeating characters is actually an extreme form of spelling correction. In this recipe, we will take on the less extreme case of correcting minor spelling issues using <span class="strong"><strong>Enchant</strong></span>—a spelling correction API.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec64"/>Getting ready</h2></div></div></div><p>You will need to install Enchant, and a dictionary for it to use. Enchant is an offshoot of the "Abiword" open source word processor, and more information can be found at <a class="ulink" href="http://www.abisource.com/projects/enchant/">http://www.abisource.com/projects/enchant/</a>.</p><p>For dictionaries, <span class="strong"><strong>aspell</strong></span> <a id="id113" class="indexterm"/>is a good open source spellchecker and dictionary that can be found at <a class="ulink" href="http://aspell.net/">http://aspell.net/</a>.</p><p>Finally, you will need the <span class="strong"><strong>pyenchant</strong></span><a id="id114" class="indexterm"/> library, which can be found at <a class="ulink" href="http://www.rfk.id.au/software/pyenchant/">http://www.rfk.id.au/software/pyenchant/</a>. You should be able to install it with the <code class="literal">easy_install</code> command that comes with <span class="emphasis"><em>python-setuptools</em></span>, such as by doing <code class="literal">sudo easy_install pyenchant</code> on Linux or Unix.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec65"/>How to do it...</h2></div></div></div><p>We will create a new class called <code class="literal">SpellingReplacer</code> in <code class="literal">replacers.py</code>, and this time the <code class="literal">replace()</code> method will check Enchant to see whether the word is valid or not. If not, we will look up suggested alternatives and return the best match using <code class="literal">nltk.metrics.edit_distance()</code>:</p><div class="informalexample"><pre class="programlisting">import enchant
from nltk.metrics import edit_distance

class SpellingReplacer(object):
  def __init__(self, dict_name='en', max_dist=2):
    self.spell_dict = enchant.Dict(dict_name)
    self.max_dist = 2

  def replace(self, word):
    if self.spell_dict.check(word):
      return word
    suggestions = self.spell_dict.suggest(word)

    if suggestions and edit_distance(word, suggestions[0]) &lt;= self.max_dist:
      return suggestions[0]
    else:
      return word</pre></div><p>The preceding class can be used to correct English spellings as follows:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from replacers import SpellingReplacer
&gt;&gt;&gt; replacer = SpellingReplacer()
&gt;&gt;&gt; replacer.replace('cookbok')
'cookbook'</pre></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec66"/>How it works...</h2></div></div></div><a id="id115" class="indexterm"/><a id="id116" class="indexterm"/><p>
<code class="literal">SpellingReplacer</code> starts by creating a reference to an <code class="literal">enchant</code> dictionary. Then, in the <code class="literal">replace()</code> method, it first checks whether the given <code class="literal">word</code> is present in the dictionary or not. If it is, no spelling correction is necessary, and the word is returned. But if the word is not found, it looks up a list of suggestions and returns the first suggestion, as long as its edit distance is less than or equal to <code class="literal">max_dist</code>. The <a id="id117" class="indexterm"/>
<span class="strong"><strong>edit distance</strong></span> is the number of character changes necessary to transform the given word into the suggested word. <code class="literal">max_dist</code> then acts as a constraint on the Enchant <code class="literal">suggest()</code> function to ensure that no unlikely replacement words are returned. Here is an example showing all the suggestions for "languege", a misspelling of "language":</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; import enchant
&gt;&gt;&gt; d = enchant.Dict('en')
&gt;&gt;&gt; d.suggest('languege')
['language', 'languisher', 'languish', 'languor', 'languid']</pre></div><p>Except for the correct suggestion, "language", all the other words have an edit distance of three or greater.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec67"/>There's more...</h2></div></div></div><p>You can use language dictionaries other than <code class="literal">'</code>
<code class="literal">en'</code>, such as <code class="literal">'en_GB'</code>, assuming the dictionary has already been installed. To check which other languages are available, use <code class="literal">enchant.list_languages()</code>:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; enchant.list_languages()
['en_AU', 'en_GB', 'en_US', 'en_ZA', 'en_CA', 'en']</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note05"/>Note</h3><p>If you try to use a dictionary that doesn't exist, you will get <code class="literal">enchant.DictNotFoundError</code>. You can first check whether the dictionary exists using <code class="literal">enchant.dict_exists()</code>, which will return <code class="literal">True</code> if the named dictionary exists, or <code class="literal">False</code> otherwise.</p></div></div><div class="section" title="en_GB dictionary"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec20"/>en_GB dictionary</h3></div></div></div><a id="id118" class="indexterm"/><p>Always be sure to use the correct dictionary for whichever language you are doing spelling correction on. <code class="literal">'en_US'</code> can give you different results than <code class="literal">'en_GB'</code>, such as for the word "theater". "Theater" is the American English spelling, whereas the British English spelling is "Theatre":</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; import enchant
&gt;&gt;&gt; dUS = enchant.Dict('en_US')
&gt;&gt;&gt; dUS.check('theater')
True
&gt;&gt;&gt; dGB = enchant.Dict('en_GB')
&gt;&gt;&gt; dGB.check('theater')
False
&gt;&gt;&gt; from replacers import SpellingReplacer
&gt;&gt;&gt; us_replacer = SpellingReplacer('en_US')
&gt;&gt;&gt; us_replacer.replace('theater')
'theater'
&gt;&gt;&gt; gb_replacer = SpellingReplacer('en_GB')
&gt;&gt;&gt; gb_replacer.replace('theater')
'theatre'</pre></div></div><div class="section" title="Personal word lists"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec21"/>Personal word lists</h3></div></div></div><a id="id119" class="indexterm"/><p>Enchant also supports personal word lists. These can be combined with an existing dictionary, allowing you to augment the dictionary with your own words. So let us say you had a file named <code class="literal">mywords.txt</code> that had <code class="literal">nltk</code> on one line. You could then create a dictionary augmented with your personal word list as follows:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; d = enchant.Dict('en_US')
&gt;&gt;&gt; d.check('nltk')
False
&gt;&gt;&gt; d = enchant.DictWithPWL('en_US', 'mywords.txt')
&gt;&gt;&gt; d.check('nltk')
True</pre></div><p>To use an augmented dictionary with our <code class="literal">SpellingReplacer</code>, we can create a subclass in <code class="literal">replacers.py</code> that takes an existing spelling dictionary.</p><div class="informalexample"><pre class="programlisting">class CustomSpellingReplacer(SpellingReplacer):
  def __init__(self, spell_dict, max_dist=2):
    self.spell_dict = spell_dict
    self.max_dist = max_dist</pre></div><p>This <code class="literal">CustomSpellingReplacer</code> will not replace any words that you put into <code class="literal">mywords.txt</code>.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from replacers import CustomSpellingReplacer
&gt;&gt;&gt; d = enchant.DictWithPWL('en_US', 'mywords.txt')
&gt;&gt;&gt; replacer = CustomSpellingReplacer(d)
&gt;&gt;&gt; replacer.replace('nltk')
'nltk'</pre></div></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec68"/>See also</h2></div></div></div><p>The previous recipe covered an extreme form of spelling correction by replacing repeating characters. You could also do spelling correction by simple word replacement as discussed in the next recipe.</p></div></div>
<div class="section" title="Replacing synonyms"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec23"/>Replacing synonyms</h1></div></div></div><a id="id120" class="indexterm"/><p>It is often useful to reduce the vocabulary of a text by replacing words with common synonyms. By compressing the vocabulary without losing meaning, you can save memory in cases such as <span class="emphasis"><em>frequency analysis</em></span> and <span class="emphasis"><em>text indexing</em></span>. Vocabulary reduction can also increase the occurrence of significant collocations, which was covered in the <span class="emphasis"><em>Discovering word collocations</em></span> recipe of <a class="link" href="ch01.html" title="Chapter 1. Tokenizing Text and WordNet Basics">Chapter 1</a>, <span class="emphasis"><em>Tokenizing Text and WordNet Basics</em></span>.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec69"/>Getting ready</h2></div></div></div><p>You will need to have a defined mapping of a word to its synonym. This is a simple <span class="emphasis"><em>controlled vocabulary</em></span>. We will start by hardcoding the synonyms as a Python dictionary, then explore other options for storing synonym maps.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec70"/>How to do it...</h2></div></div></div><p>We'll first create a <code class="literal">WordReplacer</code> class in <code class="literal">replacers.py</code> that takes a word replacement mapping:</p><div class="informalexample"><pre class="programlisting">class WordReplacer(object):
  def __init__(self, word_map):
    self.word_map = word_map
  def replace(self, word):
    return self.word_map.get(word, word)</pre></div><p>Then we can demonstrate its usage for simple word replacement:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from replacers import wordReplacer
&gt;&gt;&gt; replacer = WordReplacer({'bday': 'birthday'})
&gt;&gt;&gt; replacer.replace('bday')
'birthday'
&gt;&gt;&gt; replacer.replace('happy')
'happy'</pre></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec71"/>How it works...</h2></div></div></div><a id="id121" class="indexterm"/><a id="id122" class="indexterm"/><p>
<code class="literal">WordReplacer</code> is simply a class wrapper around a Python dictionary. The <code class="literal">replace()</code> method looks up the given word in its <code class="literal">word_map</code> and returns the replacement synonym if it exists. Otherwise, the given word is returned as is.</p><p>If you were only using the <code class="literal">word_map</code> dictionary, you would have no need for the <code class="literal">WordReplacer</code> class, and could instead call <code class="literal">word_map.get()</code> directly. But <code class="literal">WordReplacer</code> can act as a base class for other classes that construct the <code class="literal">word_map</code> from various file formats. Read on for more information.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec72"/>There's more...</h2></div></div></div><p>Hardcoding synonyms as a Python dictionary is not a good long-term solution. Two better alternatives are to store the synonyms in a CSV file or in a YAML file. Choose whichever format is easiest for whoever will be maintaining your synonym vocabulary. Both of the classes outlined in the following section inherit the <code class="literal">replace()</code> method from <code class="literal">WordReplacer</code>.</p><div class="section" title="CSV synonym replacement"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec22"/>CSV synonym replacement</h3></div></div></div><a id="id123" class="indexterm"/><a id="id124" class="indexterm"/><p>The <code class="literal">CsvWordReplacer</code> class extends <code class="literal">WordReplacer</code> in <code class="literal">replacers.py</code> in order to construct the <code class="literal">word_map</code> from a CSV file:</p><div class="informalexample"><pre class="programlisting">import csv

class CsvWordReplacer(WordReplacer):
  def __init__(self, fname):
    word_map = {}
    for line in csv.reader(open(fname)):
      word, syn = line
      word_map[word] = syn
    super(CsvWordReplacer, self).__init__(word_map)</pre></div><p>Your CSV file should be two columns, where the first column is the word, and the second column is the synonym meant to replace it. If this file is called <code class="literal">synonyms.csv</code> and the first line is <code class="literal">bday</code>, <code class="literal">birthday</code>, then you can do:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from replacers import CsvWordReplacer
&gt;&gt;&gt; replacer = CsvWordReplacer('synonyms.csv')
&gt;&gt;&gt; replacer.replace('bday')
'birthday'
&gt;&gt;&gt; replacer.replace('happy')
'happy'</pre></div></div><div class="section" title="YAML synonym replacement"><div class="titlepage"><div><div><h3 class="title"><a id="ch02lvl3sec23"/>YAML synonym replacement</h3></div></div></div><a id="id125" class="indexterm"/><p>If you have PyYAML installed, you can create a <code class="literal">YamlWordReplacer</code> in <code class="literal">replacers.py</code>. Download and installation instructions for PyYAML are located at <a class="ulink" href="http://pyyaml.org/wiki/PyYAML">http://pyyaml.org/wiki/PyYAML</a>.</p><div class="informalexample"><pre class="programlisting">import yaml

class YamlWordReplacer(WordReplacer):
  def __init__(self, fname):
    word_map = yaml.load(open(fname))
    super(YamlWordReplacer, self).__init__(word_map)</pre></div><p>Your YAML file should be a simple mapping of "word: synonym", such as <code class="literal">bday: birthday</code>. Note that the YAML syntax is very particular, and the space after the colon is required. If the file is named <code class="literal">synonyms.yaml</code>, you can do:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from replacers import YamlWordReplacer
&gt;&gt;&gt; replacer = YamlWordReplacer('synonyms.yaml')
&gt;&gt;&gt; replacer.replace('bday')
'birthday'
&gt;&gt;&gt; replacer.replace('happy')
'happy'</pre></div></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec73"/>See also</h2></div></div></div><p>You can use the <code class="literal">WordReplacer</code> to do any kind of word replacement, even spelling correction for more complicated words that can't be automatically corrected, as we did in the previous recipe. In the next recipe, we will cover antonym replacement.</p></div></div>
<div class="section" title="Replacing negations with antonyms"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec24"/>Replacing negations with antonyms</h1></div></div></div><a id="id126" class="indexterm"/><a id="id127" class="indexterm"/><a id="id128" class="indexterm"/><p>The opposite of synonym replacement is <span class="emphasis"><em>antonym</em></span> replacement. An <span class="strong"><strong>antonym</strong></span> is the opposite meaning of a word. This time, instead of creating custom word mappings, we can use WordNet to replace words with unambiguous antonyms. Refer to the <span class="emphasis"><em>Looking up lemmas and synonyms in WordNet</em></span> recipe in <a class="link" href="ch01.html" title="Chapter 1. Tokenizing Text and WordNet Basics">Chapter 1</a>, <span class="emphasis"><em>Tokenizing Text and WordNet Basics </em></span>for more details on antonym lookups.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec74"/>How to do it...</h2></div></div></div><p>Let us say you have a sentence such as "let's not uglify our code". With antonym replacement, you can replace "not uglify" with "beautify", resulting in the sentence "let's beautify our code". To do this, we will need to create an <code class="literal">AntonymReplacer</code> in <code class="literal">replacers.py</code> as follows:</p><div class="informalexample"><pre class="programlisting">from nltk.corpus import wordnet
class AntonymReplacer(object):
  def replace(self, word, pos=None):
    antonyms = set()
    for syn in wordnet.synsets(word, pos=pos):
      for lemma in syn.lemmas:
        for antonym in lemma.antonyms():
          antonyms.add(antonym.name)
    if len(antonyms) == 1:
      return antonyms.pop()
    else:
      return None

  def replace_negations(self, sent):
    i, l = 0, len(sent)
    words = []
    while i &lt; l:
      word = sent[i]
      if word == 'not' and i+1 &lt; l:
        ant = self.replace(sent[i+1])
        if ant:
          words.append(ant)
          i += 2
          continue
      words.append(word)
      i += 1
    return words</pre></div><p>Now we can tokenize the original sentence into <code class="literal">["let's", 'not', 'uglify', 'our', 'code']</code>, and pass this to the <code class="literal">replace_negations()</code> function. Here are some examples:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from replacers import AntonymReplacer
&gt;&gt;&gt; replacer = AntonymReplacer()
&gt;&gt;&gt; replacer.replace('good')
&gt;&gt;&gt; replacer.replace('uglify')
'beautify'
&gt;&gt;&gt; sent = ["let's", 'not', 'uglify', 'our', 'code']
&gt;&gt;&gt; replacer.replace_negations(sent)
["let's", 'beautify', 'our', 'code']</pre></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec75"/>How it works...</h2></div></div></div><a id="id129" class="indexterm"/><a id="id130" class="indexterm"/><p>The <a id="id131" class="indexterm"/>
<code class="literal">AntonymReplacer</code> has two methods: <code class="literal">replace()</code>
<a id="id132" class="indexterm"/> and <code class="literal">replace_negations()</code>. The <code class="literal">replace()</code> method takes a single <code class="literal">word</code> and an optional part of speech tag, then looks up the synsets for the word in WordNet. Going through all the synsets and every lemma of each synset, it creates a <code class="literal">set</code> of all antonyms found. If only one antonym is found, then it is an <span class="emphasis"><em>unambiguous replacement</em></span>. If there is more than one antonym found, which can happen quite often, then we don't know for sure which antonym is correct. In the case of multiple antonyms (or no antonyms), <code class="literal">replace()</code> returns <code class="literal">None</code> since it cannot make a decision.</p><a id="id133" class="indexterm"/><p>In <code class="literal">replace_negations()</code>, we look through a tokenized sentence for the word "<code class="literal">not</code>". If "<code class="literal">not</code>" is found, then we try to find an antonym <a id="id134" class="indexterm"/>
<a id="id135" class="indexterm"/>for the next word using <code class="literal">replace()</code>. If we find an antonym, then it is appended to the list of <code class="literal">words</code>, replacing "<code class="literal">not</code>" and the original word. All other words are appended as it is, resulting in a tokenized sentence with unambiguous negations replaced by their antonyms.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec76"/>There's more...</h2></div></div></div><p>Since unambiguous antonyms aren't very common in WordNet, you may want to create a custom antonym mapping the same way we did for synonyms. This <code class="literal">AntonymWordReplacer</code> could be constructed by inheriting from both <code class="literal">WordReplacer</code> and <code class="literal">AntonymReplacer</code>:</p><div class="informalexample"><pre class="programlisting">class AntonymWordReplacer(WordReplacer, AntonymReplacer):
  pass</pre></div><p>The order of inheritance is very important, as we want the initialization and <code class="literal">replace()</code> function of <code class="literal">WordReplacer</code> combined with the <code class="literal">replace_negations()</code> function from <code class="literal">AntonymReplacer</code>. The result is a replacer that can do the following:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from replacers import AntonymWordReplacer
&gt;&gt;&gt; replacer = AntonymWordReplacer({'evil': 'good'})
&gt;&gt;&gt; replacer.replace_negations(['good', 'is', 'not', 'evil'])
['good', 'is', 'good']</pre></div><p>Of course, you could also inherit from <code class="literal">CsvWordReplacer</code> or <code class="literal">YamlWordReplacer</code> instead of <code class="literal">WordReplacer</code> if you want to load the antonym word mappings from a file.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec77"/>See also</h2></div></div></div><p>The previous recipe covers the <code class="literal">WordReplacer</code> from the perspective of synonym replacement. And in <a class="link" href="ch01.html" title="Chapter 1. Tokenizing Text and WordNet Basics">Chapter 1</a>, <span class="emphasis"><em>Tokenizing Text and WordNet Basics</em></span> Wordnet usage is covered in detail in the <span class="emphasis"><em>Looking up synsets for a word in Wordnet</em></span> and <span class="emphasis"><em>Looking up lemmas and synonyms in Wordnet</em></span> recipes.</p></div></div></body></html>