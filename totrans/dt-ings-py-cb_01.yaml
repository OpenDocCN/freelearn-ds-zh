- en: '1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction to Data Ingestion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Welcome to the fantastic world of data! Are you ready to embark on a thrilling
    journey into data ingestion? If so, this is the perfect book to start! Ingesting
    data is the first step into the big data world.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data ingestion** is a process that involves gathering and importing data
    and also storing it properly so that the subsequent **extract, transform, and
    load** (**ETL**) pipeline can utilize the data. To make it happen, we must be
    cautious about the tools we will use and how to configure them properly.'
  prefs: []
  type: TYPE_NORMAL
- en: In our book journey, we will use **Python** and **PySpark** to retrieve data
    from different data sources and learn how to store them properly. To orchestrate
    all this, the basic concepts of **Airflow** will be implemented, along with efficient
    monitoring to guarantee that our pipelines are covered.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will introduce some basic concepts about data ingestion and how
    to set up your environment to start the tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will build and learn the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up Python and the environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing PySpark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring Docker for MongoDB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring Docker for Airflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logging libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating schemas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying data governance in ingestion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing data replication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The commands inside the recipes of this chapter use Linux syntax. If you don’t
    use a Linux-based system, you may need to adapt the commands:'
  prefs: []
  type: TYPE_NORMAL
- en: Docker or Docker Desktop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The SQL client of your choice (recommended); we recommend DBeaver, since it
    has a community-free version
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can find the code from this chapter in this GitHub repository: [https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook).'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'Windows users might get an error message such as **Docker Desktop requires
    a newer WSL kernel version.** This can be fixed by following the steps here: [https://docs.docker.com/desktop/windows/wsl/](https://docs.docker.com/desktop/windows/wsl/).'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up Python and its environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the data world, languages such as **Java**, **Scala**, or **Python** are
    commonly used. The first two languages are used due to their compatibility with
    the big data tools environment, such as **Hadoop** and **Spark**, the central
    core of which runs on a **Java Virtual Machine** (**JVM**). However, in the past
    few years, the use of Python for data engineering and data science has increased
    significantly due to the language’s versatility, ease of understanding, and many
    open source libraries built by the community.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s create a folder for our project:'
  prefs: []
  type: TYPE_NORMAL
- en: First, open your system command line. Since I use the **Windows Subsystem for
    Linux** (**WSL**), I will open the WSL application.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Go to your home directory and create a folder as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Go inside this folder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Check your Python version on your operating system as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Depending on your operational system, you might or might not have output here
    – for example, WSL 20.04 users might have the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'If your Python path is configured to use the `python` command, you will see
    output similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Sometimes, your Python path might be configured to be invoked using `python3`.
    You can try it using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be similar to the `python` command, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let’s check our `pip` version. This check is essential, since some operating
    systems have more than one Python version installed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see similar output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: If your `3.8x` or doesn’t have the language installed, proceed to the *How to
    do it* steps; otherwise, you are ready to start the following *Installing* *PySpark*
    recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We are going to use the official installer from Python.org. You can find the
    link for it here: [https://www.python.org/downloads/](https://www.python.org/downloads/):'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: For Windows users, it is important to check your OS version, since Python 3.10
    may not be yet compatible with Windows 7, or your processor type (32-bits or 64-bits).
  prefs: []
  type: TYPE_NORMAL
- en: Download one of the stable versions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'At the time of writing, the stable recommended versions compatible with the
    tools and resources presented here are `3.8`, `3.9`, and `3.10`. I will use the
    `3.9` version and download it using the following link: [https://www.python.org/downloads/release/python-390/](https://www.python.org/downloads/release/python-390/).
    Scrolling down the page, you will find a list of links to Python installers according
    to OS, as shown in the following screenshot.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.1 – Python.org download files for version 3.9](img/Figure_1.1_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.1 – Python.org download files for version 3.9
  prefs: []
  type: TYPE_NORMAL
- en: After downloading the installation file, double-click it and follow the instructions
    in the wizard window. To avoid complexity, choose the recommended settings displayed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following screenshot shows how it looks on Windows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.2 – The Python Installer for Windows](img/Figure_1.2_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.2 – The Python Installer for Windows
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are a Linux user, you can install it from the source using the following
    commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After installing Python, you should be able to execute the `pip` command. If
    not, refer to the `pip` official documentation page here: [https://pip.pypa.io/en/stable/installation/](https://pip.pypa.io/en/stable/installation/).'
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Python is an **interpreted language**, and its interpreter extends several functions
    made with **C** or **C++**. The language package also comes with several built-in
    libraries and, of course, the interpreter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The interpreter works like a Unix shell and can be found in the `usr/local/bin`
    directory: [https://docs.python.org/3/tutorial/interpreter.xhtml](https://docs.python.org/3/tutorial/interpreter.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, note that many Python third-party packages in this book require the
    `pip` command to be installed. This is because `pip` (an acronym for **Pip Installs
    Packages**) is the default package manager for Python; therefore, it is used to
    install, upgrade, and manage the Python packages and dependencies from the **Python
    Package** **Index** (**PyPI**).
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Even if you don’t have any Python versions on your machine, you can still install
    them using the command line or **HomeBrew** (for **macOS** users). Windows users
    can also download them from the MS Windows Store.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you choose to download Python from the Windows Store, ensure you use an application
    made by the Python Software Foundation.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can use `pip` to install convenient third-party applications, such as Jupyter.
    This is an open source, web-based, interactive (and user-friendly) computing platform,
    often used by data scientists and data engineers. You can install it from the
    official website here: [https://jupyter.org/install](https://jupyter.org/install).'
  prefs: []
  type: TYPE_NORMAL
- en: Installing PySpark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To process, clean, and transform vast amounts of data, we need a tool that provides
    resilience and distributed processing, and that’s why **PySpark** is a good fit.
    It gets an API over the Spark library that lets you use its applications.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before starting the PySpark installation, we need to check our Java version
    in our operational system:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we check the Java version:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see output similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: If everything is correct, you should see the preceding message as the output
    of the command and the **OpenJDK 18** version or higher. However, some systems
    don’t have any Java version installed by default, and to cover this, we need to
    proceed to *step 2*.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we download the **Java Development** **Kit** (**JDK**).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go to [https://www.oracle.com/java/technologies/downloads/](https://www.oracle.com/java/technologies/downloads/),
    select your **OS**, and download the most recent version of JDK. At the time of
    writing, it is JDK 19.
  prefs: []
  type: TYPE_NORMAL
- en: 'The download page of the JDK will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.3 – The JDK 19 downloads official web page](img/Figure_1.3_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.3 – The JDK 19 downloads official web page
  prefs: []
  type: TYPE_NORMAL
- en: 'Execute the downloaded application. Click on the application to start the installation
    process. The following window will appear:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Depending on your OS, the installation window may appear slightly different.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.4 – The Java installation wizard window](img/Figure_1.4_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.4 – The Java installation wizard window
  prefs: []
  type: TYPE_NORMAL
- en: Click **Next** for the following two questions, and the application will start
    the installation. You don’t need to worry about where the JDK will be installed.
    By default, the application is configured, as standard, to be compatible with
    other tools’ installations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we again check our Java version. When executing the command again, you
    should see the following version:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are the steps to perform this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install PySpark from PyPi:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If the command runs successfully, the installation output’s last line will
    look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the `pyspark` command to open the interactive shell. When executing
    the `pyspark` command in your command line, you should see this message:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You can observe some interesting messages here, such as the Spark version and
    the Python used from PySpark.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we exit the interactive shell as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As seen at the beginning of this recipe, Spark is a robust framework that runs
    on top of the JVM. It is also an open source tool for creating resilient and distributed
    processing output from vast data. With the growth in popularity of the Python
    language in the past few years, it became necessary to have a solution that adapts
    Spark to run alongside Python.
  prefs: []
  type: TYPE_NORMAL
- en: PySpark is an interface that interacts with **Spark APIs via Py4J**, dynamically
    allowing Python code to interact with the JVM. We first need to have Java installed
    on our OS to use Spark. When we install PySpark, it already comes with Spark and
    Py4J components installed, making it easy to start the application and build the
    code.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Anaconda is a convenient way to install PySpark and other data science tools.
    This tool encapsulates all manual processes and has a friendly interface for interacting
    with and installing Python components, such as **NumPy**, **pandas**, or **Jupyter**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To install Anaconda, go to the official website and select **Products** | **Anaconda**
    **Distribution**: [https://www.anaconda.com/products/distribution](https://www.anaconda.com/products/distribution).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download the distribution according to your OS.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For more detailed information about how to install Anaconda and other powerful
    commands, refer to [https://docs.anaconda.com/](https://docs.anaconda.com/).
  prefs: []
  type: TYPE_NORMAL
- en: Using virtualenv with PySpark
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is possible to configure and use `virtualenv` with PySpark, and Anaconda
    does it automatically if you choose this type of installation. However, for the
    other installation methods, we need to make some additional steps to make our
    Spark cluster (locally or on the server) run it, which includes indicating the
    `virtualenv /bin/` folder and where your PySpark path is.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There is a nice article about this topic, *Using VirtualEnv with PySpark*,
    by jzhang, here: [https://community.cloudera.com/t5/Community-Articles/Using-VirtualEnv-with-PySpark/ta-p/245932](https://community.cloudera.com/t5/Community-Articles/Using-VirtualEnv-with-PySpark/ta-p/245932).'
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Docker for MongoDB
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**MongoDB** is a **Not Only SQL** (**NoSQL**) document-oriented database, widely
    used to store **Internet of Things** (**IoT**) data, application logs, and so
    on. A NoSQL database is a non-relational database that stores unstructured data
    differently from relational databases such as MySQL or PostgreSQL. Don’t worry
    too much about this now; we will cover it in more detail in [*Chapter 5*](B19453_05.xhtml#_idTextAnchor161).'
  prefs: []
  type: TYPE_NORMAL
- en: Your cluster production environment can handle huge amounts of data and create
    resilient data storage.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Following the good practice of code organization, let’s start creating a folder
    inside our project to store the Docker image:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a folder inside our project directory to store the MongoDB Docker image
    and data as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are the steps to try out this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we pull the Docker image from Docker Hub as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following message in your command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are a WSL user, an error might occur if you use the WSL 1 version instead
    of version 2\. You can easily fix this by following the steps here: [https://learn.microsoft.com/en-us/windows/wsl/install](https://learn.microsoft.com/en-us/windows/wsl/install).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we run the MongoDB server as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then check our server. To do this, we can use the command line to see which
    Docker images are running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We then see this on the screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.5 – MongoDB and Docker running container](img/Figure_1.5_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.5 – MongoDB and Docker running container
  prefs: []
  type: TYPE_NORMAL
- en: 'We can even check on the Docker Desktop application to see whether our container
    is running:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.6 – The Docker Desktop vision of the MongoDB container running](img/Figure_1.6_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.6 – The Docker Desktop vision of the MongoDB container running
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we need to stop our container. We need to use `Container ID` to stop
    the container, which we previously saw when checking the Docker running images.
    We will rerun it in [*Chapter 5*](B19453_05.xhtml#_idTextAnchor161):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: MongoDB’s architecture uses the concept of `main` node interacts with clients’
    requests, such as queries and document manipulation. It distributes the requests
    automatically among its shards, which are a subset of a larger data collection
    here.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.7 – MongoDB architecture](img/Figure_1.7_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.7 – MongoDB architecture
  prefs: []
  type: TYPE_NORMAL
- en: Since we may also have other running projects or software applications inside
    our machine, isolating any database or application server used in development
    is a good practice. In this way, we ensure nothing interferes with our local servers,
    and the debug process can be more manageable.
  prefs: []
  type: TYPE_NORMAL
- en: This Docker image setting creates a MongoDB server locally and even allows us
    to make additional changes if we want to simulate any other scenario for testing
    or development.
  prefs: []
  type: TYPE_NORMAL
- en: 'The commands we used are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The `--name` command defines the name we give to our container.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `-p` command specifies the port our container will open so that we can access
    it via `localhost:27017`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-e` command defines the environment variables. In this case, we set the `root`
    username and password for our MongoDB container.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-d` is detached mode – that is, the Docker process will run in the background,
    and we will not see input or output. However, we can still use `docker ps` to
    check the container status.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mongo:latest` indicates Docker pulling this image’s latest version.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For frequent users, manually configuring other parameters for the MongoDB container,
    such as the version, image port, database name, and database credentials, is also
    possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'A version of this image with example values is also available as a `docker-compose`
    file in the official documentation here: [https://hub.docker.com/_/mongo](https://hub.docker.com/_/mongo).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `docker-compose` file for MongoDB looks similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can check out MongoDB at the complete Docker Hub documentation here: [https://hub.docker.com/_/mongo](https://hub.docker.com/_/mongo).'
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Docker for Airflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this book, we will use **Airflow** to orchestrate data ingests and provide
    logs to monitor our pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Airflow can be installed directly on your local machine and any server using
    PyPi([https://pypi.org/project/apache-airflow/](https://pypi.org/project/apache-airflow/))
    or a Docker container ([https://hub.docker.com/r/apache/airflow](https://hub.docker.com/r/apache/airflow)).
    An official and supported version of Airflow can be found on Docker Hub, and the
    **Apache Foundation** community maintains it.
  prefs: []
  type: TYPE_NORMAL
- en: However, there are some additional steps to configure our Airflow. Thankfully,
    the Apache Foundation also has a `docker-compose` file that contains all other
    requirements to make Airflow work. We just need to complete a few more steps.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s start by initializing our Docker application on our machine. You can use
    the desktop version or the CLI command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Make sure you are inside your project folder for this. Create a folder to store
    Airflow internal components and the `docker-compose.yaml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we fetch the `docker-compose.yaml` file directly from the Airflow official
    docs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see output like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.8 – Airflow container image download progress](img/Figure_1.8_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.8 – Airflow container image download progress
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Check the most stable version of this `docker-compose` file when you download
    it, since new, more appropriate versions may be available after this book is published.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we create the `dags`, `logs`, and `plugins` folders as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we create and set the Airflow user as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you have any error messages related to the `AIRFLOW_UID` variable, you can
    create a `.env` file in the same folder where your `docker-compose.yaml` file
    is and define the variable as `AIRFLOW_UID=50000`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we initialize the database:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After executing the command, you should see output similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we start the Airflow service:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we need to check the Docker processes. Using the following CLI command,
    you will see the Docker images running:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'These are the images we see:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.9 – The docker ps command output](img/Figure_1.9_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.9 – The docker ps command output
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Docker Desktop application, you can also see the same containers running
    but with a more friendly interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.10 – A Docker desktop view of the Airflow containers running](img/Figure_1.10_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.10 – A Docker desktop view of the Airflow containers running
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we access Airflow in a web browser:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In your preferred browser, type `http://localhost:8080/home`. The following
    screen will appear:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.11 – The Airflow UI login page](img/Figure_1.11_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.11 – The Airflow UI login page
  prefs: []
  type: TYPE_NORMAL
- en: Then, we log in to the Airflow platform. Since it’s a local application used
    for testing and learning, the default credentials (username and password) for
    administrative access in Airflow are `airflow`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When logged in, the following screen will appear:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.12 – The Airflow UI main page](img/Figure_1.12_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.12 – The Airflow UI main page
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we stop our containers. We can stop our containers until we reach [*Chapter
    9*](B19453_09.xhtml#_idTextAnchor319), when we will explore data ingest in Airflow:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Airflow is an open source platform that allows batch data pipeline development,
    monitoring, and scheduling. However, it requires other components, such as an
    internal database, to store metadata to work correctly. In this example, we use
    PostgreSQL to store the metadata and **Redis** to cache information.
  prefs: []
  type: TYPE_NORMAL
- en: All this can be installed directly in our machine environment one by one. Even
    though it seems quite simple, it may not be due to compatibility issues with OS,
    other software versions, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Docker can create an isolated environment and provide all the requirements to
    make it work. With `docker-compose`, it becomes even simpler, since we can create
    dependencies between the components that can only be created if the others are
    healthy.
  prefs: []
  type: TYPE_NORMAL
- en: You can also open the `docker-compose.yaml` file we downloaded for this recipe
    and take a look to explore it better. We will also cover it in detail in [*Chapter
    9*](B19453_09.xhtml#_idTextAnchor319).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you want to learn more about how this `docker-compose` file works, you can
    look at the Apache Airflow official Docker documentation on the Apache Airflow
    documentation page: [https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.xhtml](https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: Creating schemas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Schemas** are considered blueprints of a database or table. While some databases
    strictly require schema definition, others can work without it. However, in some
    cases, it is advantageous to work with data schemas to ensure that the application
    data architecture is maintained and can receive the desired data input.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s imagine we need to create a database for a school to store information
    about the students, the courses, and the instructors. With this information, we
    know we have at least three tables so far.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.13 – A table diagram for three entities](img/Figure_1.13_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.13 – A table diagram for three entities
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will cover how schemas work using the **Entity Relationship
    Diagram** (**ERD**), a visual representation of relationships between entities
    in a database, to exemplify how schemas are connected.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are the steps to try this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We define the type of schema. The following figure helps us understand how
    to go about this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.14 – A diagram to help you decide which schema to use](img/Figure_1.14_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.14 – A diagram to help you decide which schema to use
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we define the fields and the data type for each table column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.15 – A definition of the columns of each table](img/Figure_1.15_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.15 – A definition of the columns of each table
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we define which fields can be empty or `NULL`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.16 – A definition of which columns can be NULL](img/Figure_1.16_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.16 – A definition of which columns can be NULL
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we create the relationship between the tables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.17 – A relationship diagram of the tables](img/Figure_1.17_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.17 – A relationship diagram of the tables
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When designing data schemas, the first thing we need to do is define their type.
    As we can see in the diagram in *step 1*, applying the schema architecture depends
    on the data’s purpose.
  prefs: []
  type: TYPE_NORMAL
- en: After that, the tables are designed. Deciding how to define data types can vary,
    depending project or purpose, but deciding what values a column can receive is
    important. For instance, the `officeRoom` on `Teacher` table can be an `Integer`
    type if we know the room’s identification is always numeric, or a `String` type
    if it is unsure how identifications are made (for example, `Room 3-D`).
  prefs: []
  type: TYPE_NORMAL
- en: Another important topic covered in *step 3* is how to define which of the columns
    can accept `NULL` fields. Can a field for a student’s name be empty? If not, we
    need to create a constraint to forbid this type of insert.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, based on the type of schema, a definition of the relationship between
    the tables is made.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you want to know more about database schema designs and their application,
    read this article by Mark Smallcombe: [https://www.integrate.io/blog/database-schema-examples/](https://www.integrate.io/blog/database-schema-examples/).'
  prefs: []
  type: TYPE_NORMAL
- en: Applying data governance in ingestion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Data governance** is a set of methodologies that ensure that data is secure,
    available, well-stored, documented, private, and accurate.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Data ingestion** is the beginning of the data pipeline process, but it doesn’t
    mean data governance is not heavily applied. The governance status in the final
    data pipeline output depends on how it was implemented during the ingestion.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows how data ingestion is commonly conducted:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.18 – The data ingestion process](img/Figure_1.18_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.18 – The data ingestion process
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s analyze the steps in the diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Getting data from the source**: The first step is to define the type of data,
    its periodicity, where we will gather it, and why we need it.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Writing the scripts to ingest data**: Based on the answers to the previous
    step, we can begin planning how our code will behave and some basic steps.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Storing data in a temporary database or other types of storage**: Between
    the ingest and the transformation phase, data is typically stored in a temporary
    database or repository.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.19 – Data governance pillars](img/Figure_1.19_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.19 – Data governance pillars
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Step by step, let’s attribute the pillars in *Figure 1**.19* to the ingestion
    phase:'
  prefs: []
  type: TYPE_NORMAL
- en: A concern for accessibility needs to be applied at the data source level, defining
    the individuals that are allowed to see or retrieve data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, it is necessary to catalog our data to understand it better. Since data
    ingestion is only covered here, it is more relevant to cover the data sources.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The quality pillar will be applied to the ingestion and staging area, where
    we control the data and keep its quality aligned with the source.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, let’s define ownership. We know the data source *belongs* to a business
    area or a company. However, when we ingested the data and put it in temporary
    or staging storage, it becomes our responsibility to maintain it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The last pillar involves keeping data secure for the whole pipeline. Security
    is vital in all steps, since we may be handling private or sensitive information.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.20 – Adding to data ingestion](img/Figure_1.20_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.20 – Adding to data ingestion
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While some articles define “pillars” to create governance good practices, the
    best way to understand how to apply them is to understand how they are composed.
    As you saw in the previous *How to do it…* section, we attributed some items to
    our pipeline, and now we can understand how they are connected to the following
    topics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data accessibility**: Data accessibility is how people from a group, organization,
    or project can see and use data. The information needs to be readily available
    for use. At the same time, it needs to be available for the people involved in
    the process. For example, sensitive data accessibility should be restricted to
    some people or programs. In the diagram we built, we applied it to our data sources,
    since we need to understand and retrieve data. For the same reason, it can be
    applied for temporary storage needs as well.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data catalog**: Cataloging and documenting data are essential for business
    and engineering teams. When we know what types of information rely on our databases
    or data lakes and have quick access to these documents, the action time to solve
    a problem becomes short.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Again, documenting our data sources can make the ingest process quicker, since
    we need to make a discovery every time we need to ingest data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data quality**: Quality is constantly preoccupied with ingesting, processing,
    and loading data. Tracking and monitoring data’s expected income and outcome by
    its periodicity is essential. For example, if we expect to ingest 300 GB of data
    per day and suddenly it drops to 1 GB, something is very wrong and will affect
    the quality of our final output. Other quality parameters can be the number of
    columns, partitioning, and so on, which we will explore later in this book.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ownership**: Who is responsible for the data? This definition is crucial
    to make contact with the owner if there are problems or attribute responsibility
    to keep and maintain data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security**: A concerning topic nowadays is data security. With so many regulations
    about data privacy, it became an obligation of data engineers and scientists to
    know, at least, the basics of encryption, sensitive data, and how to avoid data
    leaks. Even languages and libraries that are used for work need to be evaluated.
    That’s why this item is attributed to the three steps in *Figure 1**.19*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to the topics we explored, a global data governance project has
    a vital role called a **data steward**, which is responsible for managing an organization’s
    data assets and ensuring that data is accurate, consistent, and secure. In summary,
    data stewardship is managing and overseeing an organization’s data assets.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can read more about a recent vulnerability found in one of the most used
    tools for data engineering here: [https://www.ncsc.gov.uk/information/log4j-vulnerability-what-everyone-needs-to-know](https://www.ncsc.gov.uk/information/log4j-vulnerability-what-everyone-needs-to-know).'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing data replication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Data replication** is a process applied in data environments to create multiple
    copies of data and store them on different locations, servers, or sites. This
    technique is commonly implemented to create better availability and avoid data
    loss if there is downtime, or even a natural disaster that affects a data center.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You will find across papers and articles different types (or even names) on
    the best way for **data replication** decision. In this recipe, you will learn
    how to decide which kind of replication better suits your application or software.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s begin to build our fundamental pillars to implement data replication:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we need to decide the size of our replication, and it can be done using
    a portion or all the stored data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The next step is to consider when replication will take place. It can be done
    synchronously when new data arrives in storage or within a specific timeframe.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The last fundamental pillar is whether the data is incremented or in a bulk
    form.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the end, we will have a diagram that looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.21 – A data replication model decision diagram](img/Figure_1.21_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.21 – A data replication model decision diagram
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Analyzing the preceding figure, we have three main questions to answer, regarding
    the extension, the frequency, and whether our replication will be incremental
    or bulk.
  prefs: []
  type: TYPE_NORMAL
- en: For the first question, we decide whether the replication will be complete or
    partial. In other words, either the data will consistently be replicated no matter
    what type of transaction or change was made, or just a portion of the data will
    be replicated. A real example of this would be keeping track of all store sales
    or just the most expensive ones.
  prefs: []
  type: TYPE_NORMAL
- en: The second question, related to the frequency, is to decide when a replication
    needs to be done. This question also needs to take into consideration related
    costs. Real-time replication is often more expensive, but the synchronicity guarantees
    almost no data inconsistency.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, it is relevant to consider how data will be transported to the replication
    site. In most cases, a scheduler with a script can replicate small data batches
    and reduce transportation costs. However, a bulk replication can be used in the
    data ingestion process, such as copying all the current batch’s raw data from
    a source to cold storage.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One method of data replication that has seen an increase in use in the past
    few years is **cold storage**, which is used to retain data used infrequently
    or is even inactive. The costs related to this type of replication are meager
    and guarantee data longevity. You can find cold storage solutions in all cloud
    providers, such as **Amazon Glacier**, **Azure Cool Blob**, and **Google Cloud**
    **Storage Nearline**.
  prefs: []
  type: TYPE_NORMAL
- en: Besides replication, regulatory compliance such as **General Data Protection
    Regulation** (**GDPR**) laws benefit from this type of storage, since, for some
    case scenarios, users’ data need to be kept for some years.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we explored the basic concepts and laid the foundation for
    the following chapters and recipes in this book. We started with a Python installation,
    prepared our Docker containers, and saw data governance and replication concepts.
    You will observe over the upcoming chapters that almost all topics interconnect,
    and you will understand the relevance of understanding them at the beginning of
    the ETL process.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[https://www.manageengine.com/device-control/data-replication.xhtml](https://www.manageengine.com/device-control/data-replication.xhtml)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
