<html><head></head><body>
		<div><h1 id="_idParaDest-71" class="chapter-number"><a id="_idTextAnchor071"/><a id="_idTextAnchor072"/>4</h1>
			<h1 id="_idParaDest-72"><a id="_idTextAnchor073"/>Spark DataFrames and their Operations</h1>
			<p>In this chapter, we will learn about a few different APIs in Spark and talk about their features. We will <a id="_idIndexMarker151"/>also get started with Spark’s DataFrame operations and look at different data viewing and manipulation techniques such as filtering, adding, renaming, and dropping columns available in Spark.</p>
			<p>We will cover these concepts under the following topics:</p>
			<ul>
				<li>The Spark DataFrame API</li>
				<li>Creating DataFrames</li>
				<li>Viewing DataFrames</li>
				<li>Manipulating DataFrames</li>
				<li>Aggregating DataFrames</li>
			</ul>
			<p>By the end of this chapter, you will know how to work with PySpark DataFrames. You’ll also discover various data manipulation techniques and see how you can view data after manipulating it.</p>
			<h1 id="_idParaDest-73"><a id="_idTextAnchor074"/>Getting Started in PySpark</h1>
			<p>In the previous chapters, we discussed that Spark primarily uses four languages, which are Scala, Python, R, and SQL. When any of these languages are used, the underlying execution engine <a id="_idIndexMarker152"/>is the same. This provides the necessary unification we talked about in <a href="B19176_02.xhtml#_idTextAnchor030"><em class="italic">Chapter 2</em></a>. This means that developers can use any language of their choice and can also switch between different APIs in applications.</p>
			<p>For the context of this book, we’re going to focus on Python as the primary language. Spark used with Python is called <strong class="bold">PySpark</strong>.</p>
			<p>Let’s get started with the installation of Spark.</p>
			<h2 id="_idParaDest-74"><a id="_idTextAnchor075"/>Installing Spark</h2>
			<p>To get started with Spark, you would have to first install it on your computer. There are a few ways <a id="_idIndexMarker153"/>to install Spark. We will focus on just one in this section.</p>
			<p>PySpark provides <strong class="bold">pip</strong> installation from <strong class="bold">PyPI</strong>. You can install it as follows:</p>
			<pre class="source-code">
pip install pyspark</pre>			<p>Once Spark is installed, you will need to create a Spark session.</p>
			<h2 id="_idParaDest-75"><a id="_idTextAnchor076"/>Creating a Spark session</h2>
			<p>Once you have installed Spark on your system, you can get started with creating a Spark session. A Spark <a id="_idIndexMarker154"/>session is the entry point of any Spark application. To create a Spark session, you will initialize it in the following way:</p>
			<pre class="source-code">
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()</pre>			<p>When you are running your code in the Spark shell, the Spark session is automatically created for you so you don’t have to manually execute this code to create a Spark session. This session is usually created in a variable called <code>spark</code>.</p>
			<p>It is important to note that we can only create a single spark session at any given time. Duplicating a Spark session is not possible in Spark.</p>
			<p>Now, let’s take a look at different data APIs in Spark DataFrames.</p>
			<h1 id="_idParaDest-76"><a id="_idTextAnchor077"/>Dataset API</h1>
			<p>Dataset is <a id="_idIndexMarker155"/>a newer interface added to <em class="italic">Spark 1.6</em>. It is a distributed collection of data. The Dataset API is available in Java and Scala, but not in Python and R. The Dataset <a id="_idIndexMarker156"/>API uses <strong class="bold">Resilient Distributed Datasets</strong> (<strong class="bold">RDDs</strong>) and hence provides additional features of RDDs, such as fixed typing. It also uses Spark SQL’s optimized engine for faster queries.</p>
			<p>Since a lot of the data engineering and data science community is already familiar with Python and uses it extensively for data architectures in production, PySpark also provides an <a id="_idIndexMarker157"/>equivalent API for DataFrames for this purpose. Let’s take a look at it in the next section.</p>
			<h1 id="_idParaDest-77"><a id="_idTextAnchor078"/>DataFrame API</h1>
			<p>The motivation of Spark DataFrames comes from Pandas DataFrames in Python. A DataFrame is essentially a set of rows and columns. You can think of it like a table where you have <a id="_idIndexMarker158"/>table headers as column names and below these headers are data arranged accordingly. This table-like format has been part of computations for a long time in tools such as relational databases and comma-separated files.</p>
			<p>Spark’s DataFrame API is built on top of RDDs. The underlying structures to store the data are still RDDs but DataFrames create an abstraction on top of the RDDs to hide its complexity. Just as RDDs are lazily evaluated and are immutable, DataFrames are also evaluated lazily and are immutable. If you can remember from previous chapters, lazy evaluation gives Spark performance gains and optimization by running the computations only when needed. This also gives Spark a large number of optimizations in its DataFrames by planning how to best compute the operations. The computations start when an action is called on a DataFrame. There are a lot of different ways to create Spark DataFrames. We will learn about some of those in this chapter.</p>
			<p>Let’s take a look at what a DataFrame is in Spark.</p>
			<h1 id="_idParaDest-78"><a id="_idTextAnchor079"/>Creating DataFrame operations</h1>
			<p>As we have already discussed, DataFrames are the main building blocks of Spark data. They consist <a id="_idIndexMarker159"/>of rows and column data structures.</p>
			<p>DataFrames in PySpark are created using the <code>pyspark.sql.SparkSession.createDataFrame</code> function. You can use lists, lists of lists, tuples, dictionaries, Pandas DataFrames, RDDs, and <code>pyspark.sql.Rows</code> to create DataFrames.</p>
			<p>Spark DataFrames also has an argument named <strong class="bold">schema</strong> that specifies the schema of the DataFrame. You can either choose to specify the schema explicitly or let Spark infer the schema from the DataFrame itself. If you don’t specify this argument in the code, Spark will infer the schema on its own.</p>
			<p>There are <a id="_idIndexMarker160"/>different ways to create DataFrames in Spark. Some of them are explained in the following sections.</p>
			<h2 id="_idParaDest-79"><a id="_idTextAnchor080"/>Using a list of rows</h2>
			<p>The first way to create DataFrames we see is by using rows of data. You can think of rows of <a id="_idIndexMarker161"/>data as lists. They would share common header values for each of the values in the list.</p>
			<p>Here’s the code to use when creating a new DataFrame using rows of data:</p>
			<pre class="source-code">
import pandas as pd
from datetime import datetime, date
from pyspark.sql import Row
data_df = spark.createDataFrame([
    Row(col_1=100, col_2=200., col_3='string_test_1', col_4=date(2023, 1, 1), col_5=datetime(2023, 1, 1, 12, 0)),
    Row(col_1=200, col_2=300., col_3='string_test_2', col_4=date(2023, 2, 1), col_5=datetime(2023, 1, 2, 12, 0)),
    Row(col_1=400, col_2=500., col_3='string_test_3', col_4=date(2023, 3, 1), col_5=datetime(2023, 1, 3, 12, 0))
])</pre>			<p>As a result, you will see a DataFrame with our specified columns and their data types:</p>
			<pre class="source-code">
DataFrame[col_1: bigint, col_2: double, col_3: string, col_4: date, col_5: timestamp]</pre>			<p>Now, we’ll see how we can specify the schema for a Spark DataFrame explicitly.</p>
			<h2 id="_idParaDest-80"><a id="_idTextAnchor081"/>Using a list of rows with schema</h2>
			<p>The schema of a DataFrame defines what would be the different data types present in each of <a id="_idIndexMarker162"/>the rows and columns of a DataFrame. Explicitly defining schema helps in cases where we want to enforce certain data types to our datasets.</p>
			<p>Now, we will explicitly tell Spark which schema to use for the DataFrame that we’re creating. Notice that the majority of the code remains the same—we’re simply adding another argument named <code>schema</code> in the code for creating the DataFrame to explicitly tell which columns would have what kind of datatypes:</p>
			<pre class="source-code">
import pandas as pd
from datetime import datetime, date
from pyspark.sql import Row
data_df = spark.createDataFrame([
    Row(col_1=100, col_2=200., col_3='string_test_1', col_4=date(2023, 1, 1), col_5=datetime(2023, 1, 1, 12, 0)),
    Row(col_1=200, col_2=300., col_3='string_test_2', col_4=date(2023, 2, 1), col_5=datetime(2023, 1, 2, 12, 0)),
    Row(col_1=400, col_2=500., col_3='string_test_3', col_4=date(2023, 3, 1), col_5=datetime(2023, 1, 3, 12, 0))
], schema=' col_1 long, col_2 double, col_3 string, col_4 date, col_5 timestamp')</pre>			<p>As a result, you will see a DataFrame with our specified columns and their data types:</p>
			<pre class="source-code">
data_df
DataFrame[col_1: bigint, col_2: double, col_3: string, col_4: date, col_5: timestamp]</pre>			<p>Now, let’s take a look at how we can create DataFrames using Pandas DataFrames.</p>
			<h2 id="_idParaDest-81"><a id="_idTextAnchor082"/>Using Pandas DataFrames</h2>
			<p>DataFrames can also be created using Pandas DataFrames. To achieve this, you would need to <a id="_idIndexMarker163"/>create a DataFrame in Pandas first. Once that is created, you would then convert that DataFrame to a PySpark DataFrame. The following code demonstrates this process:</p>
			<pre class="source-code">
from datetime import datetime, date
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()
rdd = spark.sparkContext.parallelize([
    (100, 200., 'string_test_1', date(2023, 1, 1), datetime(2023, 1, 1, 12, 0)),
    (200, 300., 'string_test_2', date(2023, 2, 1), datetime(2023, 1, 2, 12, 0)),
    (300, 400., 'string_test_3', date(2023, 3, 1), datetime(2023, 1, 3, 12, 0))
])
data_df = spark.createDataFrame(rdd, schema=['col_1', 'col_2', 'col_3', 'col_4', 'col_5'])</pre>			<p>As a result, you will see a DataFrame with our specified columns and their data types:</p>
			<pre class="source-code">
DataFrame[col_1: bigint, col_2: double, col_3: string, col_4: date, col_5: timestamp]</pre>			<p>Now, let’s take a look at how we can create DataFrames using tuples.</p>
			<h2 id="_idParaDest-82"><a id="_idTextAnchor083"/>Using tuples</h2>
			<p>Another <a id="_idIndexMarker164"/>way to create DataFrames is through tuples. This means that we can create a tuple as a row and add each tuple as a separate row in the DataFrame. Each tuple would contain the data for each of the columns of the DataFrame. The following code demonstrates this:</p>
			<pre class="source-code">
import pandas as pd
from datetime import datetime, date
from pyspark.sql import Row
rdd = spark.sparkContext.parallelize([
    (100, 200., 'string_test_1', date(2023, 1, 1), datetime(2023, 1, 1, 12, 0)),
    (200, 300., 'string_test_2', date(2023, 2, 1), datetime(2023, 1, 2, 12, 0)),
    (300, 400., 'string_test_3', date(2023, 3, 1), datetime(2023, 1, 3, 12, 0))
])
data_df = spark.createDataFrame(rdd, schema=['col_1', 'col_2', 'col_3', 'col_4', 'col_5'])</pre>			<p>As a result, you <a id="_idIndexMarker165"/>will see a DataFrame with our specified columns and their data types:</p>
			<pre class="source-code">
DataFrame[col_1: bigint, col_2: double, col_3: string, col_4: date, col_5: timestamp]</pre>			<p>Now, let’s take a look at different ways we can view the DataFrames in Spark and see the results of the DataFrames that we just created.</p>
			<h1 id="_idParaDest-83"><a id="_idTextAnchor084"/>How to view the DataFrames</h1>
			<p>There are <a id="_idIndexMarker166"/>different statements in Spark to view data. The DataFrames that we created in the previous section through different methods all yield the same result as the DataFrame. Let’s look at a few different ways to view DataFrames.</p>
			<h2 id="_idParaDest-84"><a id="_idTextAnchor085"/>Viewing DataFrames</h2>
			<p>The first way to show a DataFrame is through the <code>DataFrame.show()</code> statement. Here’s an example:</p>
			<pre class="source-code">
data_df.show()</pre>			<p>As a result, you will <a id="_idIndexMarker167"/>see a DataFrame with our specified columns and the data inside this DataFrame:</p>
			<pre class="source-code">
+-----+-----+-------------+----------+-------------------+
|col_1|col_2|   col_3     |  col_4   |       col_5       |
+-----+-----+-------------+----------+-------------------+
|  100|200.0|string_test_1|2023-01-01|2023-01-01 12:00:00|
|  200|300.0|string_test_2|2023-02-01|2023-01-02 12:00:00|
|  300|400.0|string_test_3|2023-03-01|2023-01-03 12:00:00|
+-----+-----+-------------+----------+-------------------+</pre>			<p>We can also select the total rows that can be viewed in a single statement. Let’s see how we can do that in the next topic.</p>
			<h2 id="_idParaDest-85"><a id="_idTextAnchor086"/>Viewing top n rows</h2>
			<p>We can also be selective in the number of rows that can be viewed in a single statement. We <a id="_idIndexMarker168"/>can control that using a parameter in <code>DataFrame.show()</code>. Here’s an example of looking at only the top two rows of the DataFrame.</p>
			<p>If you specify <em class="italic">n</em> to be a specific number, then only those sets of rows would be shown. Here’s an example:</p>
			<pre class="source-code">
data_df.show(2)</pre>			<p>As a result, you will see a DataFrame with its top two rows:</p>
			<pre class="source-code">
+-----+-----+-------------+----------+-------------------+
|col_1|col_2|    col_3    |   col_4  |        col_5      |
+------+------+-----------+----------+-------------------+
|  100|200.0|string_test_1|2023-01-01|2023-01-01 12:00:00|
|  200|300.0|string_test_2|2023-02-01|2023-01-02 12:00:00|
------+-----+-------------+----------+-------------------+
only showing top 2 rows.</pre>			<h2 id="_idParaDest-86"><a id="_idTextAnchor087"/>Viewing DataFrame schema</h2>
			<p>We can also <a id="_idIndexMarker169"/>choose to see the schema of the DataFrame using the <code>printSchema()</code> function:</p>
			<pre class="source-code">
data_df.printSchema()</pre>			<p>As a result, you will see the schema of a DataFrame with our specified columns and their data types:</p>
			<pre class="source-code">
root
 |-- col_1: long (nullable = true)
 |-- col_2: double (nullable = true)
 |-- col_3: string (nullable = true)
 |-- col_4: date (nullable = true)
 |-- col_5: timestamp (nullable = true)</pre>			<h2 id="_idParaDest-87"><a id="_idTextAnchor088"/>Viewing data vertically</h2>
			<p>When the <a id="_idIndexMarker170"/>data becomes too long to fit into the screen, it’s sometimes useful to see the data in a vertical format instead of a horizontal table view. Here’s an example of how you can view the data in a vertical format:</p>
			<pre class="source-code">
data_df.show(1, vertical=True)</pre>			<p>As a result, you will <a id="_idIndexMarker171"/>see a DataFrame with our specified columns and their data but in a vertical format:</p>
			<pre class="source-code">
-RECORD 0------------------
 col_1   | 100
 col_2   | 200.0
 col_3   | string_test_1
 col_4   | 2023-01-01
 col_5   | 2023-01-01 12:00:00
only showing top 1 row</pre>			<h2 id="_idParaDest-88"><a id="_idTextAnchor089"/>Viewing columns of data</h2>
			<p>When we <a id="_idIndexMarker172"/>just need to view the columns that exist in a DataFrame, we would use the following:</p>
			<pre class="source-code">
data_df.columns</pre>			<p>As a result, you will see a list of the columns in the DataFrame:</p>
			<pre class="source-code">
['col_1', 'col_2', 'col_3', 'col_4', 'col_5']</pre>			<h2 id="_idParaDest-89"><a id="_idTextAnchor090"/>Viewing summary statistics</h2>
			<p>Now, let’s <a id="_idIndexMarker173"/>take a look at how we can view the summary statistics of a DataFrame:</p>
			<pre class="source-code">
Show the summary of the DataFrame
data_df.select('col_1', 'col_2', 'col_3').describe().show()</pre>			<p>As a result, you will see a DataFrame with its summary statistics for each column defined:</p>
			<pre class="source-code">
+-------+-------+-------+-------------+
|summary| col_1 | col_2 |    col_3    |
+-------+-------+-------+-------------+
|  count|   3   |   3   |            3|
|   mean| 200.0 | 300.0 |         null|
| stddev| 100.0 | 100.0 |         null|
|    min| 100   | 200.0 |string_test_1|
|    max| 300   | 400.0 |string_test_3|
+-------+-------+-------+-------------+</pre>			<p>Now, let’s <a id="_idIndexMarker174"/>take a look at the collect statement.</p>
			<h1 id="_idParaDest-90"><a id="_idTextAnchor091"/>Collecting the data</h1>
			<p>A collect statement <a id="_idIndexMarker175"/>is used when we want to get all the data that is being processed in different clusters back to the driver. When using a collect statement, we need to make sure that the driver has enough memory to hold the processed data. If the driver doesn’t have enough memory to hold the data, we will get out-of-memory errors.</p>
			<p>This is how you show the collect statement:</p>
			<pre class="source-code">
data_df.collect()</pre>			<p>This statement will then show result as follows:</p>
			<pre class="source-code">
[Row(col_1=100, col_2=200.0, col_3='string_test_1', col_4=datetime.date(2023, 1, 1), col_5=datetime.datetime(2023, 1, 1, 12, 0)),
 Row(col_1=200, col_2=300.0, col_3='string_test_2', col_4=datetime.date(2023, 2, 1), col_5=datetime.datetime(2023, 1, 2, 12, 0)),
 Row(col_1=300, col_2=400.0, col_3='string_test_3', col_4=datetime.date(2023, 3, 1), col_5=datetime.datetime(2023, 1, 3, 12, 0))]</pre>			<p>There are a few ways to avoid out-of-memory errors. We will explore some of the options that avoid out-of-memory errors such as take, tail, and head statements. These statements return only a subset of the data and not all of the data in a DataFrame, therefore, they are very useful to inspect the data without having to lead all the data in driver memory.</p>
			<p>Now, let’s take a look at the take statement.</p>
			<h2 id="_idParaDest-91"><a id="_idTextAnchor092"/>Using take</h2>
			<p>A take <a id="_idIndexMarker176"/>statement takes an argument for a number <a id="_idIndexMarker177"/>of elements to return from the top of a DataFrame. We will see how it is used in the following code example:</p>
			<pre class="source-code">
data_df.take(1)</pre>			<p>As a result, you will see a DataFrame with its top row:</p>
			<pre class="source-code">
[Row(col_1=100, col_2=200.0, col_3='string_test_1', col_4=datetime.date(2023, 1, 1), col_5=datetime.datetime(2023, 1, 1, 12, 0))]</pre>			<p>In this example, we’re only returning the first element of the DataFrame by giving <code>1</code> as an argument value to the <code>take()</code> function. Therefore, only one row is returned in the result.</p>
			<p>Now, let’s take a look at the tail statement.</p>
			<h2 id="_idParaDest-92"><a id="_idTextAnchor093"/>Using tail</h2>
			<p>The tail <a id="_idIndexMarker178"/>statement takes an argument for a number of elements to return from <a id="_idIndexMarker179"/>the bottom of a DataFrame. We will see how it is used in the following code example:</p>
			<pre class="source-code">
data_df.tail(1)</pre>			<p>As a result, you will see a DataFrame with its last row of data:</p>
			<pre class="source-code">
[Row(col_1=300, col_2=400.0, col_3='string_test_3', col_4=datetime.date(2023, 3, 1), col_5=datetime.datetime(2023, 1, 3, 12, 0))]</pre>			<p>In this example, we’re only returning the last element of the DataFrame by giving <code>1</code> as an argument value to the <code>tail()</code> function. Therefore, only one row is returned in the result.</p>
			<p>Now, let’s take a look at the head statement.</p>
			<h2 id="_idParaDest-93"><a id="_idTextAnchor094"/>Using head</h2>
			<p>The head <a id="_idIndexMarker180"/>statement takes an argument for a <a id="_idIndexMarker181"/>number of elements to return from the top of a DataFrame. We will see how it is used in the following code example:</p>
			<pre class="source-code">
data_df.head(1)</pre>			<p>As a result, you will see a DataFrame with its top row of data:</p>
			<pre class="source-code">
[Row(col_1=100, col_2=200.0, col_3='string_test_1', col_4=datetime.date(2023, 1, 1), col_5=datetime.datetime(2023, 1, 1, 12, 0))]</pre>			<p>In this example, we’re only returning the first element of the DataFrame by giving <code>1</code> as an argument value to the <code>head()</code> function. Therefore, only one row is returned in the result.</p>
			<p>Now, let’s take a look at how we can count the number of rows in a DataFrame.</p>
			<h2 id="_idParaDest-94"><a id="_idTextAnchor095"/>Counting the number of rows of data</h2>
			<p>When we just <a id="_idIndexMarker182"/>need to count the number of rows in a DataFrame, we <a id="_idIndexMarker183"/>would use the following:</p>
			<pre class="source-code">
data_df.count()</pre>			<p>As a result, you will see the total count of rows in a DataFrame:</p>
			<pre class="source-code">
3</pre>			<p>In PySpark, several methods are available for retrieving data from a DataFrame or RDD, each with its own characteristics and use cases. Here’s a summary of the major differences between take, collect, show, head, and tail that we used earlier in this section for data retrieval.</p>
			<h3>take(n)</h3>
			<p>This function <a id="_idIndexMarker184"/>returns an array containing the first <em class="italic">n</em> elements from the DataFrame or RDD</p>
			<ul>
				<li>It is useful for quickly inspecting a small subset of the data</li>
				<li>It performs a lazy evaluation, meaning it only computes the required number of elements</li>
			</ul>
			<h3>collect()</h3>
			<p>This function <a id="_idIndexMarker185"/>retrieves all elements from the DataFrame or RDD and returns them as a list</p>
			<ul>
				<li>It should be used with caution as it brings all data to the driver node, which can lead to out-of-memory errors for large datasets</li>
				<li>It is suitable for small datasets or when working with aggregated results that fit into memory</li>
			</ul>
			<h3>show(n)</h3>
			<p>This function <a id="_idIndexMarker186"/>displays the first <em class="italic">n</em> rows of the DataFrame in a tabular format</p>
			<ul>
				<li>It is primarily <a id="_idIndexMarker187"/>used for visual inspection of data during <strong class="bold">exploratory data analysis</strong> (<strong class="bold">EDA</strong>) or debugging</li>
				<li>It provides a user-friendly display of data with column headers and formatting</li>
			</ul>
			<h3>head(n)</h3>
			<p>This function <a id="_idIndexMarker188"/>returns the first <em class="italic">n</em> rows of the DataFrame as a list of <code>Row</code> objects</p>
			<ul>
				<li>It is similar to <code>take(n)</code> but it returns <code>Row</code> objects instead of simple values</li>
				<li>It is often used when you need access to specific column values while working with structured data</li>
			</ul>
			<h3>tail(n)</h3>
			<p>This function <a id="_idIndexMarker189"/>returns the last <em class="italic">n</em> rows of the DataFrame</p>
			<ul>
				<li>It is useful for examining the end of the dataset, especially in cases where data is sorted in descending order</li>
				<li>It performs a more expensive operation compared to <code>head(n)</code> as it may involve scanning the entire dataset</li>
			</ul>
			<p>In summary, <code>take</code> and <code>collect</code> are used to retrieve data elements, with <code>take</code> being more suitable for small subsets and <code>collect</code> for retrieving all data (with caution). <code>show</code> is used for <a id="_idIndexMarker190"/>visual inspection, <code>head</code> retrieves the first rows as <code>Row</code> objects, and <code>tail</code> retrieves the last rows of the dataset. Each method serves different purposes and should be chosen based on the specific requirements of the data analysis task.</p>
			<p>When working with data in PySpark, sometimes, you will need to use some Python functions on the DataFrames. To achieve that, you will have to convert PySpark DataFrames to Pandas DataFrames. Now, let’s take a look at how we can convert a Pyspark DataFrame to a Pandas DataFrame.</p>
			<h1 id="_idParaDest-95"><a id="_idTextAnchor096"/>Converting a PySpark DataFrame to a Pandas DataFrame</h1>
			<p>At various times in your workflow, you will want to switch from a Pyspark DataFrame to a Pandas <a id="_idIndexMarker191"/>DataFrame. There <a id="_idIndexMarker192"/>are options to convert a PySpark DataFrame to a Pandas DataFrame. This option is <code>toPandas()</code>.</p>
			<p>One thing to note here is that Python inherently is not distributed. Therefore, when a PySpark DataFrame is converted to Pandas, the driver would need to collect all the data in its memory. We need to make sure that the driver’s memory is able to collect the data in itself. If the data is not able to fit in the driver’s memory, it will cause an out-of-memory error.</p>
			<p>Here’s an example to see how we can convert a PySpark DataFrame to a Pandas DataFrame:</p>
			<pre class="source-code">
data_df.toPandas()</pre>			<p>As a result, you will see a DataFrame with our specified columns and their data types:</p>
			<table id="table001-3" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p><code>col_1</code></p>
						</td>
						<td class="No-Table-Style">
							<p><code>col_2</code></p>
						</td>
						<td class="No-Table-Style">
							<p><code>col_3</code></p>
						</td>
						<td class="No-Table-Style">
							<p><code>col_4</code></p>
						</td>
						<td class="No-Table-Style">
							<p><code>col_5</code></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><code>0</code></p>
						</td>
						<td class="No-Table-Style">
							<p><code>100</code></p>
						</td>
						<td class="No-Table-Style">
							<p><code>200.0</code></p>
						</td>
						<td class="No-Table-Style">
							<p><code>String_test_1</code></p>
						</td>
						<td class="No-Table-Style">
							<p><code>2023-01-01</code></p>
						</td>
						<td class="No-Table-Style">
							<p><code>2023-01-01 12:00:00</code></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><code>1</code></p>
						</td>
						<td class="No-Table-Style">
							<p><code>200</code></p>
						</td>
						<td class="No-Table-Style">
							<p><code>300.0</code></p>
						</td>
						<td class="No-Table-Style">
							<p><code>String_test_2</code></p>
						</td>
						<td class="No-Table-Style">
							<p><code>2023-02-01</code></p>
						</td>
						<td class="No-Table-Style">
							<p><code>2023-01-02 12:00:00</code></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><code>2</code></p>
						</td>
						<td class="No-Table-Style">
							<p><code>300</code></p>
						</td>
						<td class="No-Table-Style">
							<p><code>400.0</code></p>
						</td>
						<td class="No-Table-Style">
							<p><code>String_test_3</code></p>
						</td>
						<td class="No-Table-Style">
							<p><code>2023-03-01</code></p>
						</td>
						<td class="No-Table-Style">
							<p><code>2023-01-03 12:00:00</code></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 4.1: DataFrame with the columns and data types specified by us</p>
			<p>In the <a id="_idIndexMarker193"/>next section, we will learn <a id="_idIndexMarker194"/>about different data manipulation techniques. You will need to filter, slice, and dice the data based on different criteria for different purposes. Therefore, data manipulation is essential in working with data.</p>
			<h1 id="_idParaDest-96"><a id="_idTextAnchor097"/>How to manipulate data on rows and columns</h1>
			<p>In this section, we <a id="_idIndexMarker195"/>will learn how to do different data manipulation operations on Spark DataFrames rows and columns.</p>
			<p>We will start by looking at how we can select columns in a Spark DataFrame.</p>
			<h2 id="_idParaDest-97"><a id="_idTextAnchor098"/>Selecting columns</h2>
			<p>We can use <a id="_idIndexMarker196"/>column functions for data manipulation at the column level in a Spark DataFrame. To select a column in a DataFrame, we would use the <code>select()</code> function like so:</p>
			<pre class="source-code">
from pyspark.sql import Column
data_df.select(data_df.col_3).show()</pre>			<p>As a result, you will see only one column of the DataFrame with its data:</p>
			<pre class="source-code">
+-------------+
|    col_3    |
+-------------+
|string_test_1|
|string_test_2|
|string_test_3|
+-------------+
The important thing to note here is that the resulting DataFrame with one column is a new DataFrame. Recalling what we discussed in <a href="B19176_03.xhtml#_idTextAnchor053"><em class="italic">Chapter 3</em></a>, RDDs are immutable. The underlying <a id="_idIndexMarker197"/>data structure for DataFrames is RDDs, therefore, DataFrames are also immutable. This means every time you make a change to a DataFrame, a new DataFrame would be created out of it. You would either have to assign the resultant DataFrame to a new DataFrame or overwrite the original DataFrame.</pre>			<p>There are some other ways to achieve the same result in PySpark as well. Some of those are demonstrated here:</p>
			<pre class="source-code">
data_df.select('col_3').show()
data_df.select(data_df['col_3']).show()</pre>			<p>Once we select the required columns, there will be instances where you will need to add new columns to a DataFrame. We will now take a look at how we can create columns in a Spark DataFrame.</p>
			<h2 id="_idParaDest-98"><a id="_idTextAnchor099"/>Creating columns</h2>
			<p>We can use a <code>withColumn()</code> function to create a new column in a DataFrame. To create a <a id="_idIndexMarker198"/>new column, we would need to pass the column name and column values to fill the column with. In the following example, we’re creating a new column named <code>col_6</code> and putting a constant literal <code>A</code> in this column:</p>
			<pre class="source-code">
from pyspark.sql import functions as F
data_df = data_df.withColumn("col_6", F.lit("A"))
data_df.show()</pre>			<p>As a result, you will see a DataFrame with an additional column named <code>col_6</code> filled with multiple <code>A</code> instances:</p>
			<div><div><img src="img/B19176_04_01.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p>The <code>lit()</code> function is <a id="_idIndexMarker199"/>used to fill constant values in a column.</p>
			<p>You can <a id="_idIndexMarker200"/>also delete columns that are no longer needed in a DataFrame. We will now take a look at how we can drop columns in a Spark DataFrame.</p>
			<h2 id="_idParaDest-99"><a id="_idTextAnchor100"/>Dropping columns</h2>
			<p>If we need to drop a column from a Spark DataFrame, we would use the <code>drop()</code> function. We <a id="_idIndexMarker201"/>need to provide the name of the column to be dropped from the DataFrame. Here’s an example of how to use this function:</p>
			<pre class="source-code">
data_df = data_df.drop("col_5")
data_df.show()</pre>			<p>As a result, you will see that <code>col_5</code> is dropped from the DataFrame:</p>
			<pre class="source-code">
+-----+-----+-------------+----------+------+
|col_1|col_2|    col_3    |  col_4   | col_6|
+-----+-----+-------------+----------+------+
|  100|200.0|string_test_1|2023-01-01|     A|
|  200|300.0|string_test_2|2023-02-01|     A|
|  300|400.0|string_test_3|2023-03-01|     A|
+-----+-----+-------------+----------+------+</pre>			<p>We have successfully dropped <code>col_5</code> from this DataFrame.</p>
			<p>You can <a id="_idIndexMarker202"/>also drop multiple columns in the same drop statement as well:</p>
			<pre class="console">
<code>data_df = data_df.drop("col_4", "col_5")</code></pre>			<p>Also note that if we drop a column that does not exist in the DataFrame, it will not result in any errors. The resulting DataFrame would remain as it is.</p>
			<p>Just like dropping columns, you can also update columns in Spark DataFrames. Now, we will take a look at how we can update columns in a Spark DataFrame.</p>
			<h2 id="_idParaDest-100"><a id="_idTextAnchor101"/>Updating columns</h2>
			<p>Updating columns can also be done with the help of the <code>withColumn()</code> function in Spark. We need <a id="_idIndexMarker203"/>to provide the name of the column to be updated along with the updated value. Notice that we can also use this function to calculate some new values for the columns. Here’s an example:</p>
			<pre class="source-code">
data_df.withColumn("col_2", F.col("col_2") / 100).show()</pre>			<p>This will give us the following updated frame:</p>
			<pre class="source-code">
+-----+-----+-------------+----------+------+
|col_1|col_2|    col_3    |   col_4  | col_6|
+-----+-----+-------------+----------+------+
|  100|200.0|string_test_1|2023-01-01|     A|
|  200|300.0|string_test_2|2023-02-01|     A|
|  300|400.0|string_test_3|2023-03-01|     A|
+-----+-----+-------------+----------+------+</pre>			<p>One thing to note here is the use of the <code>col</code> function when updating the column. This function is used for column-wise operators. If we don’t use this function, our code will return an error.</p>
			<p>You don’t always <a id="_idIndexMarker204"/>have to update a column in a DataFrame if you only need to rename a column. Now, we will see how we can rename columns in a Spark DataFrame.</p>
			<h2 id="_idParaDest-101"><a id="_idTextAnchor102"/>Renaming columns</h2>
			<p>For changing the name of a column, we would use the <code>withColumnRenamed()</code> function <a id="_idIndexMarker205"/>in Spark. We would need to provide the column name that needs to be changed along with the new column name. Here’s the code to illustrate this:</p>
			<pre class="source-code">
data_df = data_df.withColumnRenamed("col_3", "string_col")
data_df.show()</pre>			<p>As a result, we’ll see the following change:</p>
			<pre class="source-code">
+-----+-----+-------------+----------+------+
|col_1|col_2|  string_col |   col_4  | col_6|
+-----+-----+-------------+----------+------+
|  100|200.0|string_test_1|2023-01-01|   A  |
|  200|300.0|string_test_2|2023-02-01|   A  |
|  300|400.0|string_test_3|2023-03-01|   A  |
+-----+-----+-------------+----------+------+</pre>			<p>Notice that <code>col_3</code> is now called <code>string_col</code> after making the change.</p>
			<p>Now, let’s shift our focus to some data manipulation techniques in Spark DataFrames. You can have search-like functionality in Spark DataFrames for finding different values in a column. Now, let’s take a look at how we can find unique values in a column of a Spark DataFrame.</p>
			<h2 id="_idParaDest-102"><a id="_idTextAnchor103"/>Finding unique values in a column</h2>
			<p>Finding <a id="_idIndexMarker206"/>unique values is a very useful function that would give us distinct values in a column. For this purpose, we can use the <code>distinct()</code> function of the Spark DataFrame like so:</p>
			<pre class="source-code">
data_df.select("col_6").distinct().show()</pre>			<p>Here is the result:</p>
			<pre class="source-code">
+------+
|col_6 |
+------+
|   A  |
+------+</pre>			<p>We applied a <code>distinct</code> function on <code>col_6</code> to get all the unique values in this column. In our case, the column just had one distinct value, <code>A</code>, so that was shown.</p>
			<p>We can also use it to find the count of distinct values in a given column. Here’s an example of how to use this function:</p>
			<pre class="source-code">
data_df.select(F.countDistinct("col_6").alias("Total_Unique")).show()</pre>			<p>Here is the result:</p>
			<pre class="source-code">
+------+
|col_6 |
+------+
|  1   |
+------+</pre>			<p>In this example, we can see the total count of distinct values in <code>col_6</code>. Currently, it is the only type of distinct value present in this column, therefore, it returned <code>1</code>.</p>
			<p>One other useful function in Spark data manipulation is changing the case of a column. Now, let’s take a look at how we can change the case of a column in a Spark DataFrame.</p>
			<h2 id="_idParaDest-103"><a id="_idTextAnchor104"/>Changing the case of a column</h2>
			<p>There is also a function that exists in Spark to change the case of a column. We don’t need to <a id="_idIndexMarker207"/>specify each value of the column separately to make use of the function. Once applied, the whole column’s values would change case. One such example is as follows:</p>
			<pre class="source-code">
from pyspark.sql.functions import upper
data_df.withColumn('upper_string_col', upper(data_df.string_col)).show()</pre>			<p>Here is the result:</p>
			<div><div><img src="img/B19176_04_2.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p>In this example, we change the case of <code>string_col</code> to all caps. We need to assign this to a new column, so, we create a column called <code>upper_string_col</code> to store these upper-case values. Also, note that this column is not added to the original <code>data_df</code> because we did not save the results back in <code>data_df</code>.</p>
			<p>A lot of times in data manipulation, we would need functions to filter DataFrames. We will now take a look at how we can filter data in a Spark DataFrame.</p>
			<h2 id="_idParaDest-104"><a id="_idTextAnchor105"/>Filtering a DataFrame</h2>
			<p>Filtering a DataFrame means that we can get a subset of rows or columns from a DataFrame. There <a id="_idIndexMarker208"/>are different ways of filtering a Spark DataFrame. We will take a look at one example here:</p>
			<pre class="source-code">
data_df.filter(data_df.col_1 == 100).show()</pre>			<p>Here is the result:</p>
			<pre class="source-code">
+-----+-----+-------------+----------+------+
|col_1|col_2| string_col  |   col_4  |col_6 |
+-----+-----+-------------+----------+------+
|  100|200.0|string_test_1|2023-01-01|   A  |
+-----+-----+-------------+----------+------+</pre>			<p>In this example, we filter <code>data_df</code> to only include rows where the column value of <code>col_1</code> is equal to <code>100</code>. This criterion is met by only one row, therefore, a single row is returned in the resulting DataFrame.</p>
			<p>You can use this function to slice and dice your data in a number of different ways based on the requirements.</p>
			<p>Since we are talking about data filtering, we can also filter data based on logical operators as well. We will now take a look at how we can use logical operators in DataFrames to filter data.</p>
			<h2 id="_idParaDest-105"><a id="_idTextAnchor106"/>Logical operators in a DataFrame</h2>
			<p>Another important set of operators that we can combine with filter operations in a DataFrame <a id="_idIndexMarker209"/>are the logical operators. These consist of AND and OR operators, amongst others. These are used to filter DataFrames based on complex conditions. Let’s take a look at how we use the AND operator here:</p>
			<pre class="source-code">
data_df.filter((data_df.col_1 == 100)
                  &amp; (data_df.col_6 == 'A')).show()</pre>			<p>Here is the result:</p>
			<pre class="source-code">
+-----+------+-------------+----------+------+
|col_1| col_2|  string_col |   col_4  |col_6 |
+-----+------+-------------+----------+------+
|  100| 200.0|string_test_1|2023-01-01|   A  |
+-----+------+-------------+----------+------+</pre>			<p>In this example, we’re trying to get the rows where the value of <code>col_1</code> is equal to <code>100</code> and the value of <code>col_6</code> is <code>A</code>. Currently, only one row fulfills this condition, therefore, one row is returned as a result.</p>
			<p>Now, let’s <a id="_idIndexMarker210"/>see how we can use the OR operator to combine conditions:</p>
			<pre class="source-code">
data_df.filter((data_df.col_1 == 100)
                  | (data_df.col_2 == 300.00)).show()</pre>			<p>This statement will give the following result:</p>
			<pre class="source-code">
+-----+------+-------------+----------+------+
|col_1| col_2|  string_col |   col_4  | col_6|
+-----+------+-------------+----------+------+
|  100| 200.0|string_test_1|2023-01-01|   A  |
|  200| 300.0|string_test_2|2023-02-01|   A  |
+-----+------+-------------+----------+------+</pre>			<p>In this example, we’re trying to get the rows where the value of <code>col_1</code> is equal to <code>100</code> or the value of <code>col_2</code> is equal to <code>300.0</code>. Currently, two rows fulfill this condition, therefore, they are returned as a result.</p>
			<p>In data filtering, there is another important function to find values in a list. Now, we will see how you use the <code>isin()</code> function in PySpark.</p>
			<h2 id="_idParaDest-106"><a id="_idTextAnchor107"/>Using isin()</h2>
			<p>The <code>isin()</code> function <a id="_idIndexMarker211"/>is used to find <a id="_idIndexMarker212"/>values in a DataFrame column that exist in a list. To do this, we would create a list with some values in it. Once we have the list, then we would use the <code>isin()</code> function to see whether some of the values that are in the list exist in the DataFrame. Here’s an example to demonstrate this:</p>
			<pre class="source-code">
list = [100, 200]
data_df.filter(data_df.col_1.isin(list)).show()</pre>			<p>Here is the result:</p>
			<pre class="source-code">
+-----+-----+-------------+----------+------+
|col_1|col_2|  string_col |   col_4  |col_6 |
+-----+-----+-------------+----------+------+
|  100|200.0|string_test_1|2023-01-01|   A  |
|  200|300.0|string_test_2|2023-02-01|   A  |
+-----+-----+-------------+----------+------+</pre>			<p>In this <a id="_idIndexMarker213"/>example, we see that the values <code>100</code> and <code>200</code> are present in the <code>data_df</code> DataFrame in two of its rows, therefore, both rows are returned.</p>
			<p>We can also convert data types for different columns in Spark DataFrames. Now, let’s look at how we can convert different data types in Spark DataFrames.</p>
			<h2 id="_idParaDest-107"><a id="_idTextAnchor108"/>Datatype conversions</h2>
			<p>In this <a id="_idIndexMarker214"/>section, we’ll see different ways of converting data types in Spark DataFrame columns.</p>
			<p>We will start by using the <code>cast</code> function in Spark. The following code illustrates this:</p>
			<pre class="source-code">
from pyspark.sql.functions import col
from pyspark.sql.types import StringType,BooleanType,DateType,IntegerType
data_df_2 = data_df.withColumn("col_4",col("col_4").cast(StringType())) \
    .withColumn("col_1",col("col_1").cast(IntegerType()))
data_df_2.printSchema()
data_df.show()</pre>			<p>Here <a id="_idIndexMarker215"/>is the result:</p>
			<pre class="source-code">
root
 |-- col_1: integer (nullable = true)
 |-- col_2: double (nullable = true)
 |-- string_col: string (nullable = true)
 |-- col_4: string (nullable = true)
 |-- col_6: string (nullable = false)
+-----+-----+-------------+----------+-----+
|col_1|col_2|   string_col|     col_4|col_6|
+-----+-----+-------------+----------+-----+
|  100|200.0|string_test_1|2023-01-01|    A|
|  200|300.0|string_test_2|2023-02-01|    A|
|  300|400.0|string_test_3|2023-03-01|    A|
+-----+-----+-------------+----------+-----+</pre>			<p>In the preceding code, we see that we’re changing the data types of two columns, namely <code>col_4</code> and <code>col_1</code>. First, we change <code>col_4</code> to string type. This column was previously a date column. Then, we change <code>col_1</code> to integer type from <code>long</code>.</p>
			<p>Here is the schema of <code>data_df</code> for reference:</p>
			<pre class="source-code">
root
 |-- col_1: long (nullable = true)
 |-- col_2: double (nullable = true)
 |-- string_col: string (nullable = true)
 |-- col_4: date (nullable = true)
 |-- col_5: timestamp (nullable = true)
 |-- col_6: string (nullable = false)</pre>			<p>We see that <code>col_1</code> and <code>col_4</code> were different data types.</p>
			<p>The next <a id="_idIndexMarker216"/>example of changing the data types of the columns is by using the <code>selectExpr()</code> function. The following code illustrates this:</p>
			<pre class="source-code">
data_df_3 = data_df_2.selectExpr("cast(col_4 as date) col_4",
    "cast(col_1 as long) col_1")
data_df_3.printSchema()
data_df_3.show(truncate=False)</pre>			<p>Here is the result:</p>
			<pre class="source-code">
root
 |-- col_4: date (nullable = true)
 |-- col_1: long (nullable = true)
+----------+-----+
|  col_4   |col_1|
+----------+-----+
|2023-01-01|  100|
|2023-02-01|  200|
|2023-03-01|  300|
+----------+-----+</pre>			<p>In the preceding code, we see that we’re changing the data types of two columns, namely <code>col_4</code> and <code>col_1</code>. First, we change <code>col_4</code> back to the <code>date</code> type. Then, we change <code>col_1</code> to the <code>long</code> type.</p>
			<p>The next example of changing the data types of the columns is by using SQL. The following code illustrates this:</p>
			<pre class="source-code">
data_df_3.createOrReplaceTempView("CastExample")
data_df_4 = spark.sql("SELECT DOUBLE(col_1), DATE(col_4) from CastExample")
data_df_4.printSchema()
data_df_4.show(truncate=False)</pre>			<p>Here <a id="_idIndexMarker217"/>is the result:</p>
			<pre class="source-code">
root
 |-- col_1: double (nullable = true)
 |-- col_4: date (nullable = true)
+-----+----------+
|col_1|  col_4   |
+-----+----------+
|100.0|2023-01-01|
|200.0|2023-02-01|
|300.0|2023-03-01|
+-----+----------+</pre>			<p>In the preceding code, we see that we’re changing the data types of two columns, namely <code>col_4</code> and <code>col_1</code>. First, we use <code>createOrReplaceTempView()</code> to create a table named <code>CastExample</code>. Then, we use this table to change <code>col_4</code> back to the <code>date</code> type. Then, we change <code>col_1</code> to the <code>double</code> type.</p>
			<p>In the data analysis world, working with null values is very valuable. Now, let’s take a look at how we can drop null values from a DataFrame.</p>
			<h2 id="_idParaDest-108"><a id="_idTextAnchor109"/>Dropping null values from a DataFrame</h2>
			<p>Sometimes, in the data, there exist null values that can make clean data messy. Dropping <a id="_idIndexMarker218"/>nulls is an essential exercise that a lot of data analysts and data engineers need to do. Pyspark provides us with relevant functions to do this.</p>
			<p>Let’s create another DataFrame called <code>salary_data</code> to show some of the next operations:</p>
			<pre class="source-code">
salary_data = [("John", "Field-eng", 3500),
    ("Michael", "Field-eng", 4500),
    ("Robert", None, 4000),
    ("Maria", "Finance", 3500),
    ("John", "Sales", 3000),
    ("Kelly", "Finance", 3500),
    ("Kate", "Finance", 3000),
    ("Martin", None, 3500),
    ("Kiran", "Sales", 2200),
    ("Michael", "Field-eng", 4500)
  ]
columns= ["Employee", "Department", "Salary"]
salary_data = spark.createDataFrame(data = salary_data, schema = columns)
salary_data.printSchema()
salary_data.show()</pre>			<p>Here is the result:</p>
			<pre class="source-code">
root
 |-- Employee: string (nullable = true)
 |-- Department: string (nullable = true)
 |-- Salary: long (nullable = true)
+--------+----------+------+
|Employee|Department|Salary|
+--------+----------+------+
|    John| Field-eng| 3500 |
| Michael| Field-eng| 4500 |
|  Robert|      null| 4000 |
|   Maria|   Finance| 3500 |
|    John|     Sales| 3000 |
|   Kelly|   Finance| 3500 |
|    Kate|   Finance| 3000 |
|  Martin|      null| 3500 |
|   Kiran|     Sales| 2200 |
| Michael| Field-eng| 4500 |
+--------+----------+------+</pre>			<p>Now, let’s <a id="_idIndexMarker219"/>take a look at the <code>dropna()</code> function; this will help us drop null values from our DataFrame:</p>
			<pre class="source-code">
salary_data.dropna().show()</pre>			<p>Here is the result:</p>
			<pre class="source-code">
+--------+----------+------+
|Employee|Department|Salary|
+--------+----------+------+
| John   | Field-eng| 3500 |
| Michael| Field-eng| 4500 |
| Maria  |   Finance| 3500 |
| John   |     Sales| 3000 |
| Kelly  |   Finance| 3500 |
| Kate   |   Finance| 3000 |
| Kiran  |     Sales| 2200 |
| Michael| Field-eng| 4500 |
+--------+----------+------+</pre>			<p>We <a id="_idIndexMarker220"/>see in the resulting DataFrame that rows with <code>Robert</code> and <code>Martin</code> are deleted from the new DataFrame when we use the <code>dropna()</code> function.</p>
			<p>Deduplicating data is another useful technique that is often required in data analysis tasks. Now, let’s take a look at how we can drop duplicate values from a DataFrame.</p>
			<h2 id="_idParaDest-109"><a id="_idTextAnchor110"/>Dropping duplicates from a DataFrame</h2>
			<p>Sometimes, in <a id="_idIndexMarker221"/>the data, there are redundant values present that would make clean data messy. Dropping these values might be needed in a lot of use cases. PySpark provides us with the <code>dropDuplicates()</code> function to do this. Here’s the code to illustrate this:</p>
			<pre class="source-code">
new_salary_data = salary_data.dropDuplicates().show()</pre>			<p>Here is the result:</p>
			<pre class="source-code">
+--------+----------+------+
|Employee|Department|Salary|
+--------+----------+------+
|    John| Field-eng|  3500|
| Michael| Field-eng|  4500|
|   Maria|   Finance|  3500|
|    John|     Sales|  3000|
|   Kelly|   Finance|  3500|
|    Kate|   Finance|  3000|
|   Kiran|     Sales|  2200|
+--------+----------+------+</pre>			<p>We <a id="_idIndexMarker222"/>see in this example that employees named Michael are only shown once in the resulting DataFrame after we apply the <code>dropDuplicates()</code> function to the original DataFrame. This name and its corresponding values exist in the original DataFrame twice.</p>
			<p>Now that we have learned about different data filtering techniques, we will now see how we can aggregate data in Pyspark DataFrames.</p>
			<h2 id="_idParaDest-110"><a id="_idTextAnchor111"/>Using aggregates in a DataFrame</h2>
			<p>Some of <a id="_idIndexMarker223"/>the methods <a id="_idIndexMarker224"/>available in Spark for aggregating data are as follows:</p>
			<ul>
				<li><code>agg</code></li>
				<li><code>avg</code></li>
				<li><code>count</code></li>
				<li><code>max</code></li>
				<li><code>mean</code></li>
				<li><code>min</code></li>
				<li><code>pivot</code></li>
				<li><code>sum</code></li>
			</ul>
			<p>We will see some of them in action in the following code examples.</p>
			<h3>Average (avg)</h3>
			<p>In the <a id="_idIndexMarker225"/>following example, we see how to use aggregate functions in Spark. We will start by calculating the average of all the values in a column:</p>
			<pre class="source-code">
from pyspark.sql.functions import countDistinct, avg
salary_data.select(avg('Salary')).show()</pre>			<p>Here is the result:</p>
			<pre class="source-code">
+-----------+
|avg(Salary)|
+-----------+
|     3520.0|
+-----------+</pre>			<p>This example calculates the average of the salary column of the <code>salary_data</code> DataFrame. We have passed the <code>Salary</code> column to the <code>avg</code> function and it has calculated the average of that column for us.</p>
			<p>Now, let’s take a look at how to count different elements in a PySpark DataFrame.</p>
			<h3>Count</h3>
			<p>In the <a id="_idIndexMarker226"/>following code example, we can see how you use aggregate functions in Spark:</p>
			<pre class="source-code">
salary_data.agg({'Salary':'count'}).show()</pre>			<p>Here is the result:</p>
			<pre class="source-code">
+-------------+
|count(Salary)|
+-------------+
|           10|
+-------------+</pre>			<p>This example calculates the total count of the values in the <code>Salary</code> column of the <code>salary_data</code> DataFrame. We have passed the <code>Salary</code> column to the <code>agg</code> function with <code>count</code> as its other parameter, and it has calculated the count of that column for us.</p>
			<p>Now, let’s <a id="_idIndexMarker227"/>take a look at how to count distinct elements in a PySpark DataFrame.</p>
			<h3>Count distinct values</h3>
			<p>In the <a id="_idIndexMarker228"/>following example, we will look at how to count distinct elements in a PySpark DataFrame:</p>
			<pre class="source-code">
salary_data.select(countDistinct("Salary").alias("Distinct Salary")).show()</pre>			<p>Here is the result:</p>
			<pre class="source-code">
+---------------+
|Distinct Salary|
+---------------+
|              5|
+---------------+</pre>			<p>This example calculates total distinct values in the salary column of the <code>salary_data</code> DataFrame. We have passed the <code>Salary</code> column to the <code>countDistinct</code> function and it has calculated the count of that column for us.</p>
			<p>Now, let’s take a look at how to find maximum values in a PySpark DataFrame.</p>
			<h3>Finding maximums (max)</h3>
			<p>In the <a id="_idIndexMarker229"/>following code example, we will take a look at how to find maximum values in a column of a PySpark DataFrame:</p>
			<pre class="source-code">
salary_data.agg({'Salary':'max'}).show()</pre>			<p>Here is the result:</p>
			<pre class="source-code">
+-----------+
|max(Salary)|
+-----------+
|       4500|
+-----------+</pre>			<p>This example calculates the maximum value out of all the values in the <code>Salary</code> column of the <code>salary_data</code> DataFrame. We have passed the <code>Salary</code> column to the <code>agg</code> function <a id="_idIndexMarker230"/>with <code>max</code> as its other parameter, and it has calculated the maximum of that column for us.</p>
			<p>Now, let’s take a look at how to get the sum of all elements in a PySpark DataFrame.</p>
			<h3>Sum</h3>
			<p>In the <a id="_idIndexMarker231"/>following code example, we will look at how to sum all values in a PySpark DataFrames:</p>
			<pre class="source-code">
salary_data.agg({'Salary':'sum'}).show()</pre>			<p>Here is the result:</p>
			<pre class="source-code">
+-----------+
|sum(Salary)|
+-----------+
|      35200|
+-----------+</pre>			<p>This example calculates the sum of all the values in the <code>Salary</code> column of the <code>salary_data</code> DataFrame. We have passed the <code>Salary</code> column to the <code>agg</code> function with <code>sum</code> as its other parameter, and it has calculated the sum of that column for us.</p>
			<p>Now, let’s take a look at how to sort data in a PySpark DataFrame.</p>
			<h3>Sort data with OrderBy</h3>
			<p>In the <a id="_idIndexMarker232"/>following code example, we will look at how can we sort data in ascending order in a PySpark DataFrame:</p>
			<pre class="source-code">
salary_data.orderBy("Salary").show()</pre>			<p>Here is the result:</p>
			<pre class="source-code">
+--------+----------+------+
|Employee|Department|Salary|
+--------+----------+------+
| Kiran  | Sales    | 2200 |
| Kate   | Finance  | 3000 |
| John   | Sales    | 3000 |
| John   | Field-eng| 3500 |
| Martin | null     | 3500 |
| Kelly  | Finance  | 3500 |
| Maria  | Finance  | 3500 |
| Robert | null     | 4000 |
| Michael| Field-eng| 4500 |
| Michael| Field-eng| 4500 |
+--------+----------+------+</pre>			<p>This example <a id="_idIndexMarker233"/>sorts the full DataFrame based on the values in the <code>Salary</code> column of the <code>salary_data</code> DataFrame. We have passed the <code>Salary</code> column to the <code>orderBy</code> function and it has sorted the DataFrame based on this column.</p>
			<p>We can also sort the data in descending format by adding another function, <code>desc()</code>, to the original <code>orderBy</code> function. The following example illustrates this:</p>
			<pre class="source-code">
salary_data.orderBy(salary_data["Salary"].desc()).show()</pre>			<p>Here is the result:</p>
			<pre class="source-code">
+--------+----------+------+
|Employee|Department|Salary|
+--------+----------+------+
| Michael| Field-eng| 4500 |
| Michael| Field-eng| 4500 |
| Robert | null     | 4000 |
| Martin | null     | 3500 |
| Kelly  | Finance  | 3500 |
| Maria  | Finance  | 3500 |
| John   | Field-eng| 3500 |
| John   | Sales    | 3000 |
| Kate   | Finance  | 3000 |
| Kiran  | Sales    | 2200 |
+--------+----------+------+</pre>			<p>This example <a id="_idIndexMarker234"/>is sorting the full DataFrame in descending order based on the values in the <code>Salary</code> column of the <code>salary_data</code> DataFrame. We have passed the <code>Salary</code> column to the <code>orderBy</code> function with <code>desc()</code> as an additional function call and it has sorted the DataFrame in descending order based on this column.</p>
			<h1 id="_idParaDest-111"><a id="_idTextAnchor112"/>Summary</h1>
			<p>Over the course of this chapter, we have learned how to manipulate data in Spark DataFrames.</p>
			<p>We talked about the Spark DataFrame API and what different data types are in Spark. We also learned how to create DataFrames in Spark and how we can view these DataFrames once they’ve been created. Finally, we learned about different data manipulation and data aggregation functions.</p>
			<p>In the next chapter, we will cover some advanced operations in Spark with respect to data manipulation.</p>
			<h1 id="_idParaDest-112"><a id="_idTextAnchor113"/>Sample question</h1>
			<p>1. Which of the following operations will trigger evaluation?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>DataFrame.filter()</code></li>
				<li class="Alphabets"><code>DataFrame.distinct()</code></li>
				<li class="Alphabets"><code>DataFrame.intersect()</code></li>
				<li class="Alphabets"><code>DataFrame.join()</code></li>
				<li class="Alphabets"><code>DataFrame.count()</code></li>
			</ol>
			<h2 id="_idParaDest-113"><a id="_idTextAnchor114"/>Answer</h2>
			<ol>
				<li>E</li>
			</ol>
		</div>
	</body></html>