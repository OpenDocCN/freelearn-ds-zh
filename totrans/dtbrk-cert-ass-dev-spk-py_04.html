<html><head></head><body>
		<div id="_idContainer025">
			<h1 id="_idParaDest-71" class="chapter-number"><a id="_idTextAnchor071"/><a id="_idTextAnchor072"/>4</h1>
			<h1 id="_idParaDest-72"><a id="_idTextAnchor073"/>Spark DataFrames and their Operations</h1>
			<p>In this chapter, we will learn about a few different APIs in Spark and talk about their features. We will <a id="_idIndexMarker151"/>also get started with Spark’s DataFrame operations and look at different data viewing and manipulation techniques such as filtering, adding, renaming, and dropping columns available <span class="No-Break">in Spark.</span></p>
			<p>We will cover these concepts under the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>The Spark <span class="No-Break">DataFrame API</span></li>
				<li><span class="No-Break">Creating DataFrames</span></li>
				<li><span class="No-Break">Viewing DataFrames</span></li>
				<li><span class="No-Break">Manipulating DataFrames</span></li>
				<li><span class="No-Break">Aggregating DataFrames</span></li>
			</ul>
			<p>By the end of this chapter, you will know how to work with PySpark DataFrames. You’ll also discover various data manipulation techniques and see how you can view data after <span class="No-Break">manipulating it.</span></p>
			<h1 id="_idParaDest-73"><a id="_idTextAnchor074"/>Getting Started in PySpark</h1>
			<p>In the previous chapters, we discussed that Spark primarily uses four languages, which are Scala, Python, R, and SQL. When any of these languages are used, the underlying execution engine <a id="_idIndexMarker152"/>is the same. This provides the necessary unification we talked about in <a href="B19176_02.xhtml#_idTextAnchor030"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>. This means that developers can use any language of their choice and can also switch between different APIs <span class="No-Break">in applications.</span></p>
			<p>For the context of this book, we’re going to focus on Python as the primary language. Spark used with Python is <span class="No-Break">called </span><span class="No-Break"><strong class="bold">PySpark</strong></span><span class="No-Break">.</span></p>
			<p>Let’s get started with the installation <span class="No-Break">of Spark.</span></p>
			<h2 id="_idParaDest-74"><a id="_idTextAnchor075"/>Installing Spark</h2>
			<p>To get started with Spark, you would have to first install it on your computer. There are a few ways <a id="_idIndexMarker153"/>to install Spark. We will focus on just one in <span class="No-Break">this section.</span></p>
			<p>PySpark provides <strong class="bold">pip</strong> installation from <strong class="bold">PyPI</strong>. You can install it <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
pip install pyspark</pre>			<p>Once Spark is installed, you will need to create a <span class="No-Break">Spark session.</span></p>
			<h2 id="_idParaDest-75"><a id="_idTextAnchor076"/>Creating a Spark session</h2>
			<p>Once you have installed Spark on your system, you can get started with creating a Spark session. A Spark <a id="_idIndexMarker154"/>session is the entry point of any Spark application. To create a Spark session, you will initialize it in the <span class="No-Break">following way:</span></p>
			<pre class="source-code">
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()</pre>			<p>When you are running your code in the Spark shell, the Spark session is automatically created for you so you don’t have to manually execute this code to create a Spark session. This session is usually created in a variable <span class="No-Break">called </span><span class="No-Break"><strong class="source-inline">spark</strong></span><span class="No-Break">.</span></p>
			<p>It is important to note that we can only create a single spark session at any given time. Duplicating a Spark session is not possible <span class="No-Break">in Spark.</span></p>
			<p>Now, let’s take a look at different data APIs in <span class="No-Break">Spark DataFrames.</span></p>
			<h1 id="_idParaDest-76"><a id="_idTextAnchor077"/>Dataset API</h1>
			<p>Dataset is <a id="_idIndexMarker155"/>a newer interface added to <em class="italic">Spark 1.6</em>. It is a distributed collection of data. The Dataset API is available in Java and Scala, but not in Python and R. The Dataset <a id="_idIndexMarker156"/>API uses <strong class="bold">Resilient Distributed Datasets</strong> (<strong class="bold">RDDs</strong>) and hence provides additional features of RDDs, such as fixed typing. It also uses Spark SQL’s optimized engine for <span class="No-Break">faster queries.</span></p>
			<p>Since a lot of the data engineering and data science community is already familiar with Python and uses it extensively for data architectures in production, PySpark also provides an <a id="_idIndexMarker157"/>equivalent API for DataFrames for this purpose. Let’s take a look at it in the <span class="No-Break">next section.</span></p>
			<h1 id="_idParaDest-77"><a id="_idTextAnchor078"/>DataFrame API</h1>
			<p>The motivation of Spark DataFrames comes from Pandas DataFrames in Python. A DataFrame is essentially a set of rows and columns. You can think of it like a table where you have <a id="_idIndexMarker158"/>table headers as column names and below these headers are data arranged accordingly. This table-like format has been part of computations for a long time in tools such as relational databases and <span class="No-Break">comma-separated files.</span></p>
			<p>Spark’s DataFrame API is built on top of RDDs. The underlying structures to store the data are still RDDs but DataFrames create an abstraction on top of the RDDs to hide its complexity. Just as RDDs are lazily evaluated and are immutable, DataFrames are also evaluated lazily and are immutable. If you can remember from previous chapters, lazy evaluation gives Spark performance gains and optimization by running the computations only when needed. This also gives Spark a large number of optimizations in its DataFrames by planning how to best compute the operations. The computations start when an action is called on a DataFrame. There are a lot of different ways to create Spark DataFrames. We will learn about some of those in <span class="No-Break">this chapter.</span></p>
			<p>Let’s take a look at what a DataFrame is <span class="No-Break">in Spark.</span></p>
			<h1 id="_idParaDest-78"><a id="_idTextAnchor079"/>Creating DataFrame operations</h1>
			<p>As we have already discussed, DataFrames are the main building blocks of Spark data. They consist <a id="_idIndexMarker159"/>of rows and column <span class="No-Break">data structures.</span></p>
			<p>DataFrames in PySpark are created using the <strong class="source-inline">pyspark.sql.SparkSession.createDataFrame</strong> function. You can use lists, lists of lists, tuples, dictionaries, Pandas DataFrames, RDDs, and <strong class="source-inline">pyspark.sql.Rows</strong> to <span class="No-Break">create DataFrames.</span></p>
			<p>Spark DataFrames also has an argument named <strong class="bold">schema</strong> that specifies the schema of the DataFrame. You can either choose to specify the schema explicitly or let Spark infer the schema from the DataFrame itself. If you don’t specify this argument in the code, Spark will infer the schema on <span class="No-Break">its own.</span></p>
			<p>There are <a id="_idIndexMarker160"/>different ways to create DataFrames in Spark. Some of them are explained in the <span class="No-Break">following sections.</span></p>
			<h2 id="_idParaDest-79"><a id="_idTextAnchor080"/>Using a list of rows</h2>
			<p>The first way to create DataFrames we see is by using rows of data. You can think of rows of <a id="_idIndexMarker161"/>data as lists. They would share common header values for each of the values in <span class="No-Break">the list.</span></p>
			<p>Here’s the code to use when creating a new DataFrame using rows <span class="No-Break">of data:</span></p>
			<pre class="source-code">
import pandas as pd
from datetime import datetime, date
from pyspark.sql import Row
data_df = spark.createDataFrame([
    Row(col_1=100, col_2=200., col_3='string_test_1', col_4=date(2023, 1, 1), col_5=datetime(2023, 1, 1, 12, 0)),
    Row(col_1=200, col_2=300., col_3='string_test_2', col_4=date(2023, 2, 1), col_5=datetime(2023, 1, 2, 12, 0)),
    Row(col_1=400, col_2=500., col_3='string_test_3', col_4=date(2023, 3, 1), col_5=datetime(2023, 1, 3, 12, 0))
])</pre>			<p>As a result, you will see a DataFrame with our specified columns and their <span class="No-Break">data types:</span></p>
			<pre class="source-code">
DataFrame[col_1: bigint, col_2: double, col_3: string, col_4: date, col_5: timestamp]</pre>			<p>Now, we’ll see how we can specify the schema for a Spark <span class="No-Break">DataFrame explicitly.</span></p>
			<h2 id="_idParaDest-80"><a id="_idTextAnchor081"/>Using a list of rows with schema</h2>
			<p>The schema of a DataFrame defines what would be the different data types present in each of <a id="_idIndexMarker162"/>the rows and columns of a DataFrame. Explicitly defining schema helps in cases where we want to enforce certain data types to <span class="No-Break">our datasets.</span></p>
			<p>Now, we will explicitly tell Spark which schema to use for the DataFrame that we’re creating. Notice that the majority of the code remains the same—we’re simply adding another argument named <strong class="source-inline">schema</strong> in the code for creating the DataFrame to explicitly tell which columns would have what kind <span class="No-Break">of datatypes:</span></p>
			<pre class="source-code">
import pandas as pd
from datetime import datetime, date
from pyspark.sql import Row
data_df = spark.createDataFrame([
    Row(col_1=100, col_2=200., col_3='string_test_1', col_4=date(2023, 1, 1), col_5=datetime(2023, 1, 1, 12, 0)),
    Row(col_1=200, col_2=300., col_3='string_test_2', col_4=date(2023, 2, 1), col_5=datetime(2023, 1, 2, 12, 0)),
    Row(col_1=400, col_2=500., col_3='string_test_3', col_4=date(2023, 3, 1), col_5=datetime(2023, 1, 3, 12, 0))
], schema=' col_1 long, col_2 double, col_3 string, col_4 date, col_5 timestamp')</pre>			<p>As a result, you will see a DataFrame with our specified columns and their <span class="No-Break">data types:</span></p>
			<pre class="source-code">
data_df
DataFrame[col_1: bigint, col_2: double, col_3: string, col_4: date, col_5: timestamp]</pre>			<p>Now, let’s take a look at how we can create DataFrames using <span class="No-Break">Pandas DataFrames.</span></p>
			<h2 id="_idParaDest-81"><a id="_idTextAnchor082"/>Using Pandas DataFrames</h2>
			<p>DataFrames can also be created using Pandas DataFrames. To achieve this, you would need to <a id="_idIndexMarker163"/>create a DataFrame in Pandas first. Once that is created, you would then convert that DataFrame to a PySpark DataFrame. The following code demonstrates <span class="No-Break">this process:</span></p>
			<pre class="source-code">
from datetime import datetime, date
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()
rdd = spark.sparkContext.parallelize([
    (100, 200., 'string_test_1', date(2023, 1, 1), datetime(2023, 1, 1, 12, 0)),
    (200, 300., 'string_test_2', date(2023, 2, 1), datetime(2023, 1, 2, 12, 0)),
    (300, 400., 'string_test_3', date(2023, 3, 1), datetime(2023, 1, 3, 12, 0))
])
data_df = spark.createDataFrame(rdd, schema=['col_1', 'col_2', 'col_3', 'col_4', 'col_5'])</pre>			<p>As a result, you will see a DataFrame with our specified columns and their <span class="No-Break">data types:</span></p>
			<pre class="source-code">
DataFrame[col_1: bigint, col_2: double, col_3: string, col_4: date, col_5: timestamp]</pre>			<p>Now, let’s take a look at how we can create DataFrames <span class="No-Break">using tuples.</span></p>
			<h2 id="_idParaDest-82"><a id="_idTextAnchor083"/>Using tuples</h2>
			<p>Another <a id="_idIndexMarker164"/>way to create DataFrames is through tuples. This means that we can create a tuple as a row and add each tuple as a separate row in the DataFrame. Each tuple would contain the data for each of the columns of the DataFrame. The following code <span class="No-Break">demonstrates this:</span></p>
			<pre class="source-code">
import pandas as pd
from datetime import datetime, date
from pyspark.sql import Row
rdd = spark.sparkContext.parallelize([
    (100, 200., 'string_test_1', date(2023, 1, 1), datetime(2023, 1, 1, 12, 0)),
    (200, 300., 'string_test_2', date(2023, 2, 1), datetime(2023, 1, 2, 12, 0)),
    (300, 400., 'string_test_3', date(2023, 3, 1), datetime(2023, 1, 3, 12, 0))
])
data_df = spark.createDataFrame(rdd, schema=['col_1', 'col_2', 'col_3', 'col_4', 'col_5'])</pre>			<p>As a result, you <a id="_idIndexMarker165"/>will see a DataFrame with our specified columns and their <span class="No-Break">data types:</span></p>
			<pre class="source-code">
DataFrame[col_1: bigint, col_2: double, col_3: string, col_4: date, col_5: timestamp]</pre>			<p>Now, let’s take a look at different ways we can view the DataFrames in Spark and see the results of the DataFrames that we <span class="No-Break">just created.</span></p>
			<h1 id="_idParaDest-83"><a id="_idTextAnchor084"/>How to view the DataFrames</h1>
			<p>There are <a id="_idIndexMarker166"/>different statements in Spark to view data. The DataFrames that we created in the previous section through different methods all yield the same result as the DataFrame. Let’s look at a few different ways to <span class="No-Break">view DataFrames.</span></p>
			<h2 id="_idParaDest-84"><a id="_idTextAnchor085"/>Viewing DataFrames</h2>
			<p>The first way to show a DataFrame is through the <strong class="source-inline">DataFrame.show()</strong> statement. Here’s <span class="No-Break">an example:</span></p>
			<pre class="source-code">
data_df.show()</pre>			<p>As a result, you will <a id="_idIndexMarker167"/>see a DataFrame with our specified columns and the data inside <span class="No-Break">this DataFrame:</span></p>
			<pre class="source-code">
+-----+-----+-------------+----------+-------------------+
|col_1|col_2|   col_3     |  col_4   |       col_5       |
+-----+-----+-------------+----------+-------------------+
|  100|200.0|string_test_1|2023-01-01|2023-01-01 12:00:00|
|  200|300.0|string_test_2|2023-02-01|2023-01-02 12:00:00|
|  300|400.0|string_test_3|2023-03-01|2023-01-03 12:00:00|
+-----+-----+-------------+----------+-------------------+</pre>			<p>We can also select the total rows that can be viewed in a single statement. Let’s see how we can do that in the <span class="No-Break">next topic.</span></p>
			<h2 id="_idParaDest-85"><a id="_idTextAnchor086"/>Viewing top n rows</h2>
			<p>We can also be selective in the number of rows that can be viewed in a single statement. We <a id="_idIndexMarker168"/>can control that using a parameter in <strong class="source-inline">DataFrame.show()</strong>. Here’s an example of looking at only the top two rows of <span class="No-Break">the DataFrame.</span></p>
			<p>If you specify <em class="italic">n</em> to be a specific number, then only those sets of rows would be shown. Here’s <span class="No-Break">an example:</span></p>
			<pre class="source-code">
data_df.show(2)</pre>			<p>As a result, you will see a DataFrame with its top <span class="No-Break">two rows:</span></p>
			<pre class="source-code">
+-----+-----+-------------+----------+-------------------+
|col_1|col_2|    col_3    |   col_4  |        col_5      |
+------+------+-----------+----------+-------------------+
|  100|200.0|string_test_1|2023-01-01|2023-01-01 12:00:00|
|  200|300.0|string_test_2|2023-02-01|2023-01-02 12:00:00|
------+-----+-------------+----------+-------------------+
only showing top 2 rows.</pre>			<h2 id="_idParaDest-86"><a id="_idTextAnchor087"/>Viewing DataFrame schema</h2>
			<p>We can also <a id="_idIndexMarker169"/>choose to see the schema of the DataFrame using the <span class="No-Break"><strong class="source-inline">printSchema()</strong></span><span class="No-Break"> function:</span></p>
			<pre class="source-code">
data_df.printSchema()</pre>			<p>As a result, you will see the schema of a DataFrame with our specified columns and their <span class="No-Break">data types:</span></p>
			<pre class="source-code">
root
 |-- col_1: long (nullable = true)
 |-- col_2: double (nullable = true)
 |-- col_3: string (nullable = true)
 |-- col_4: date (nullable = true)
 |-- col_5: timestamp (nullable = true)</pre>			<h2 id="_idParaDest-87"><a id="_idTextAnchor088"/>Viewing data vertically</h2>
			<p>When the <a id="_idIndexMarker170"/>data becomes too long to fit into the screen, it’s sometimes useful to see the data in a vertical format instead of a horizontal table view. Here’s an example of how you can view the data in a <span class="No-Break">vertical format:</span></p>
			<pre class="source-code">
data_df.show(1, vertical=True)</pre>			<p>As a result, you will <a id="_idIndexMarker171"/>see a DataFrame with our specified columns and their data but in a <span class="No-Break">vertical format:</span></p>
			<pre class="source-code">
-RECORD 0------------------
 col_1   | 100
 col_2   | 200.0
 col_3   | string_test_1
 col_4   | 2023-01-01
 col_5   | 2023-01-01 12:00:00
only showing top 1 row</pre>			<h2 id="_idParaDest-88"><a id="_idTextAnchor089"/>Viewing columns of data</h2>
			<p>When we <a id="_idIndexMarker172"/>just need to view the columns that exist in a DataFrame, we would use <span class="No-Break">the following:</span></p>
			<pre class="source-code">
data_df.columns</pre>			<p>As a result, you will see a list of the columns in <span class="No-Break">the DataFrame:</span></p>
			<pre class="source-code">
['col_1', 'col_2', 'col_3', 'col_4', 'col_5']</pre>			<h2 id="_idParaDest-89"><a id="_idTextAnchor090"/>Viewing summary statistics</h2>
			<p>Now, let’s <a id="_idIndexMarker173"/>take a look at how we can view the summary statistics of <span class="No-Break">a DataFrame:</span></p>
			<pre class="source-code">
Show the summary of the DataFrame
data_df.select('col_1', 'col_2', 'col_3').describe().show()</pre>			<p>As a result, you will see a DataFrame with its summary statistics for each <span class="No-Break">column defined:</span></p>
			<pre class="source-code">
+-------+-------+-------+-------------+
|summary| col_1 | col_2 |    col_3    |
+-------+-------+-------+-------------+
|  count|   3   |   3   |            3|
|   mean| 200.0 | 300.0 |         null|
| stddev| 100.0 | 100.0 |         null|
|    min| 100   | 200.0 |string_test_1|
|    max| 300   | 400.0 |string_test_3|
+-------+-------+-------+-------------+</pre>			<p>Now, let’s <a id="_idIndexMarker174"/>take a look at the <span class="No-Break">collect statement.</span></p>
			<h1 id="_idParaDest-90"><a id="_idTextAnchor091"/>Collecting the data</h1>
			<p>A collect statement <a id="_idIndexMarker175"/>is used when we want to get all the data that is being processed in different clusters back to the driver. When using a collect statement, we need to make sure that the driver has enough memory to hold the processed data. If the driver doesn’t have enough memory to hold the data, we will get <span class="No-Break">out-of-memory errors.</span></p>
			<p>This is how you show the <span class="No-Break">collect statement:</span></p>
			<pre class="source-code">
data_df.collect()</pre>			<p>This statement will then show result <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
[Row(col_1=100, col_2=200.0, col_3='string_test_1', col_4=datetime.date(2023, 1, 1), col_5=datetime.datetime(2023, 1, 1, 12, 0)),
 Row(col_1=200, col_2=300.0, col_3='string_test_2', col_4=datetime.date(2023, 2, 1), col_5=datetime.datetime(2023, 1, 2, 12, 0)),
 Row(col_1=300, col_2=400.0, col_3='string_test_3', col_4=datetime.date(2023, 3, 1), col_5=datetime.datetime(2023, 1, 3, 12, 0))]</pre>			<p>There are a few ways to avoid out-of-memory errors. We will explore some of the options that avoid out-of-memory errors such as take, tail, and head statements. These statements return only a subset of the data and not all of the data in a DataFrame, therefore, they are very useful to inspect the data without having to lead all the data in <span class="No-Break">driver memory.</span></p>
			<p>Now, let’s take a look at the <span class="No-Break">take statement.</span></p>
			<h2 id="_idParaDest-91"><a id="_idTextAnchor092"/>Using take</h2>
			<p>A take <a id="_idIndexMarker176"/>statement takes an argument for a number <a id="_idIndexMarker177"/>of elements to return from the top of a DataFrame. We will see how it is used in the following <span class="No-Break">code example:</span></p>
			<pre class="source-code">
data_df.take(1)</pre>			<p>As a result, you will see a DataFrame with its <span class="No-Break">top row:</span></p>
			<pre class="source-code">
[Row(col_1=100, col_2=200.0, col_3='string_test_1', col_4=datetime.date(2023, 1, 1), col_5=datetime.datetime(2023, 1, 1, 12, 0))]</pre>			<p>In this example, we’re only returning the first element of the DataFrame by giving <strong class="source-inline">1</strong> as an argument value to the <strong class="source-inline">take()</strong> function. Therefore, only one row is returned in <span class="No-Break">the result.</span></p>
			<p>Now, let’s take a look at the <span class="No-Break">tail statement.</span></p>
			<h2 id="_idParaDest-92"><a id="_idTextAnchor093"/>Using tail</h2>
			<p>The tail <a id="_idIndexMarker178"/>statement takes an argument for a number of elements to return from <a id="_idIndexMarker179"/>the bottom of a DataFrame. We will see how it is used in the following <span class="No-Break">code example:</span></p>
			<pre class="source-code">
data_df.tail(1)</pre>			<p>As a result, you will see a DataFrame with its last row <span class="No-Break">of data:</span></p>
			<pre class="source-code">
[Row(col_1=300, col_2=400.0, col_3='string_test_3', col_4=datetime.date(2023, 3, 1), col_5=datetime.datetime(2023, 1, 3, 12, 0))]</pre>			<p>In this example, we’re only returning the last element of the DataFrame by giving <strong class="source-inline">1</strong> as an argument value to the <strong class="source-inline">tail()</strong> function. Therefore, only one row is returned in <span class="No-Break">the result.</span></p>
			<p>Now, let’s take a look at the <span class="No-Break">head statement.</span></p>
			<h2 id="_idParaDest-93"><a id="_idTextAnchor094"/>Using head</h2>
			<p>The head <a id="_idIndexMarker180"/>statement takes an argument for a <a id="_idIndexMarker181"/>number of elements to return from the top of a DataFrame. We will see how it is used in the following <span class="No-Break">code example:</span></p>
			<pre class="source-code">
data_df.head(1)</pre>			<p>As a result, you will see a DataFrame with its top row <span class="No-Break">of data:</span></p>
			<pre class="source-code">
[Row(col_1=100, col_2=200.0, col_3='string_test_1', col_4=datetime.date(2023, 1, 1), col_5=datetime.datetime(2023, 1, 1, 12, 0))]</pre>			<p>In this example, we’re only returning the first element of the DataFrame by giving <strong class="source-inline">1</strong> as an argument value to the <strong class="source-inline">head()</strong> function. Therefore, only one row is returned in <span class="No-Break">the result.</span></p>
			<p>Now, let’s take a look at how we can count the number of rows in <span class="No-Break">a DataFrame.</span></p>
			<h2 id="_idParaDest-94"><a id="_idTextAnchor095"/>Counting the number of rows of data</h2>
			<p>When we just <a id="_idIndexMarker182"/>need to count the number of rows in a DataFrame, we <a id="_idIndexMarker183"/>would use <span class="No-Break">the following:</span></p>
			<pre class="source-code">
data_df.count()</pre>			<p>As a result, you will see the total count of rows in <span class="No-Break">a DataFrame:</span></p>
			<pre class="source-code">
3</pre>			<p>In PySpark, several methods are available for retrieving data from a DataFrame or RDD, each with its own characteristics and use cases. Here’s a summary of the major differences between take, collect, show, head, and tail that we used earlier in this section for <span class="No-Break">data retrieval.</span></p>
			<h3>take(n)</h3>
			<p>This function <a id="_idIndexMarker184"/>returns an array containing the first <em class="italic">n</em> elements from the DataFrame <span class="No-Break">or RDD</span></p>
			<ul>
				<li>It is useful for quickly inspecting a small subset of <span class="No-Break">the data</span></li>
				<li>It performs a lazy evaluation, meaning it only computes the required number <span class="No-Break">of elements</span></li>
			</ul>
			<h3>collect()</h3>
			<p>This function <a id="_idIndexMarker185"/>retrieves all elements from the DataFrame or RDD and returns them as <span class="No-Break">a list</span></p>
			<ul>
				<li>It should be used with caution as it brings all data to the driver node, which can lead to out-of-memory errors for <span class="No-Break">large datasets</span></li>
				<li>It is suitable for small datasets or when working with aggregated results that fit <span class="No-Break">into memory</span></li>
			</ul>
			<h3>show(n)</h3>
			<p>This function <a id="_idIndexMarker186"/>displays the first <em class="italic">n</em> rows of the DataFrame in a <span class="No-Break">tabular format</span></p>
			<ul>
				<li>It is primarily <a id="_idIndexMarker187"/>used for visual inspection of data during <strong class="bold">exploratory data analysis</strong> (<strong class="bold">EDA</strong>) <span class="No-Break">or debugging</span></li>
				<li>It provides a user-friendly display of data with column headers <span class="No-Break">and formatting</span></li>
			</ul>
			<h3>head(n)</h3>
			<p>This function <a id="_idIndexMarker188"/>returns the first <em class="italic">n</em> rows of the DataFrame as a list of <span class="No-Break"><strong class="source-inline">Row</strong></span><span class="No-Break"> objects</span></p>
			<ul>
				<li>It is similar to <strong class="source-inline">take(n)</strong> but it returns <strong class="source-inline">Row</strong> objects instead of <span class="No-Break">simple values</span></li>
				<li>It is often used when you need access to specific column values while working with <span class="No-Break">structured data</span></li>
			</ul>
			<h3>tail(n)</h3>
			<p>This function <a id="_idIndexMarker189"/>returns the last <em class="italic">n</em> rows of <span class="No-Break">the DataFrame</span></p>
			<ul>
				<li>It is useful for examining the end of the dataset, especially in cases where data is sorted in <span class="No-Break">descending order</span></li>
				<li>It performs a more expensive operation compared to <strong class="source-inline">head(n)</strong> as it may involve scanning the <span class="No-Break">entire dataset</span></li>
			</ul>
			<p>In summary, <strong class="source-inline">take</strong> and <strong class="source-inline">collect</strong> are used to retrieve data elements, with <strong class="source-inline">take</strong> being more suitable for small subsets and <strong class="source-inline">collect</strong> for retrieving all data (with caution). <strong class="source-inline">show</strong> is used for <a id="_idIndexMarker190"/>visual inspection, <strong class="source-inline">head</strong> retrieves the first rows as <strong class="source-inline">Row</strong> objects, and <strong class="source-inline">tail</strong> retrieves the last rows of the dataset. Each method serves different purposes and should be chosen based on the specific requirements of the data <span class="No-Break">analysis task.</span></p>
			<p>When working with data in PySpark, sometimes, you will need to use some Python functions on the DataFrames. To achieve that, you will have to convert PySpark DataFrames to Pandas DataFrames. Now, let’s take a look at how we can convert a Pyspark DataFrame to a <span class="No-Break">Pandas DataFrame.</span></p>
			<h1 id="_idParaDest-95"><a id="_idTextAnchor096"/>Converting a PySpark DataFrame to a Pandas DataFrame</h1>
			<p>At various times in your workflow, you will want to switch from a Pyspark DataFrame to a Pandas <a id="_idIndexMarker191"/>DataFrame. There <a id="_idIndexMarker192"/>are options to convert a PySpark DataFrame to a Pandas DataFrame. This option <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">toPandas()</strong></span><span class="No-Break">.</span></p>
			<p>One thing to note here is that Python inherently is not distributed. Therefore, when a PySpark DataFrame is converted to Pandas, the driver would need to collect all the data in its memory. We need to make sure that the driver’s memory is able to collect the data in itself. If the data is not able to fit in the driver’s memory, it will cause an <span class="No-Break">out-of-memory error.</span></p>
			<p>Here’s an example to see how we can convert a PySpark DataFrame to a <span class="No-Break">Pandas DataFrame:</span></p>
			<pre class="source-code">
data_df.toPandas()</pre>			<p>As a result, you will see a DataFrame with our specified columns and their <span class="No-Break">data types:</span></p>
			<table id="table001-3" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline" lang="en-US" xml:lang="en-US">col_1</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline" lang="en-US" xml:lang="en-US">col_2</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline" lang="en-US" xml:lang="en-US">col_3</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline" lang="en-US" xml:lang="en-US">col_4</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline" lang="en-US" xml:lang="en-US">col_5</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="source-inline" lang="en-US" xml:lang="en-US">0</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline" lang="en-US" xml:lang="en-US">100</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline" lang="en-US" xml:lang="en-US">200.0</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline" lang="en-US" xml:lang="en-US">String_test_1</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline" lang="en-US" xml:lang="en-US">2023-01-01</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline" lang="en-US" xml:lang="en-US">2023-01-01 12:00:00</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="source-inline" lang="en-US" xml:lang="en-US">1</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline" lang="en-US" xml:lang="en-US">200</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline" lang="en-US" xml:lang="en-US">300.0</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline" lang="en-US" xml:lang="en-US">String_test_2</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline" lang="en-US" xml:lang="en-US">2023-02-01</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline" lang="en-US" xml:lang="en-US">2023-01-02 12:00:00</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="source-inline" lang="en-US" xml:lang="en-US">2</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline" lang="en-US" xml:lang="en-US">300</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline" lang="en-US" xml:lang="en-US">400.0</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline" lang="en-US" xml:lang="en-US">String_test_3</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline" lang="en-US" xml:lang="en-US">2023-03-01</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline" lang="en-US" xml:lang="en-US">2023-01-03 12:00:00</strong></span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 4.1: DataFrame with the columns and data types specified by us</p>
			<p>In the <a id="_idIndexMarker193"/>next section, we will learn <a id="_idIndexMarker194"/>about different data manipulation techniques. You will need to filter, slice, and dice the data based on different criteria for different purposes. Therefore, data manipulation is essential in working <span class="No-Break">with data.</span></p>
			<h1 id="_idParaDest-96"><a id="_idTextAnchor097"/>How to manipulate data on rows and columns</h1>
			<p>In this section, we <a id="_idIndexMarker195"/>will learn how to do different data manipulation operations on Spark DataFrames rows <span class="No-Break">and columns.</span></p>
			<p>We will start by looking at how we can select columns in a <span class="No-Break">Spark DataFrame.</span></p>
			<h2 id="_idParaDest-97"><a id="_idTextAnchor098"/>Selecting columns</h2>
			<p>We can use <a id="_idIndexMarker196"/>column functions for data manipulation at the column level in a Spark DataFrame. To select a column in a DataFrame, we would use the <strong class="source-inline">select()</strong> function <span class="No-Break">like so:</span></p>
			<pre class="source-code">
from pyspark.sql import Column
data_df.select(data_df.col_3).show()</pre>			<p>As a result, you will see only one column of the DataFrame with <span class="No-Break">its data:</span></p>
			<pre class="source-code">
+-------------+
|    col_3    |
+-------------+
|string_test_1|
|string_test_2|
|string_test_3|
+-------------+
The important thing to note here is that the resulting DataFrame with one column is a new DataFrame. Recalling what we discussed in <a href="B19176_03.xhtml#_idTextAnchor053"><em class="italic">Chapter 3</em></a>, RDDs are immutable. The underlying <a id="_idIndexMarker197"/>data structure for DataFrames is RDDs, therefore, DataFrames are also immutable. This means every time you make a change to a DataFrame, a new DataFrame would be created out of it. You would either have to assign the resultant DataFrame to a new DataFrame or overwrite the original DataFrame.</pre>			<p>There are some other ways to achieve the same result in PySpark as well. Some of those are <span class="No-Break">demonstrated here:</span></p>
			<pre class="source-code">
data_df.select('col_3').show()
data_df.select(data_df['col_3']).show()</pre>			<p>Once we select the required columns, there will be instances where you will need to add new columns to a DataFrame. We will now take a look at how we can create columns in a <span class="No-Break">Spark DataFrame.</span></p>
			<h2 id="_idParaDest-98"><a id="_idTextAnchor099"/>Creating columns</h2>
			<p>We can use a <strong class="source-inline">withColumn()</strong> function to create a new column in a DataFrame. To create a <a id="_idIndexMarker198"/>new column, we would need to pass the column name and column values to fill the column with. In the following example, we’re creating a new column named <strong class="source-inline">col_6</strong> and putting a constant literal <strong class="source-inline">A</strong> in <span class="No-Break">this column:</span></p>
			<pre class="source-code">
from pyspark.sql import functions as F
data_df = data_df.withColumn("col_6", F.lit("A"))
data_df.show()</pre>			<p>As a result, you will see a DataFrame with an additional column named <strong class="source-inline">col_6</strong> filled with multiple <span class="No-Break"><strong class="source-inline">A</strong></span><span class="No-Break"> instances:</span></p>
			<div>
				<div id="_idContainer023" class="IMG---Figure">
					<img src="image/B19176_04_01.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p>The <strong class="source-inline">lit()</strong> function is <a id="_idIndexMarker199"/>used to fill constant values in <span class="No-Break">a column.</span></p>
			<p>You can <a id="_idIndexMarker200"/>also delete columns that are no longer needed in a DataFrame. We will now take a look at how we can drop columns in a <span class="No-Break">Spark DataFrame.</span></p>
			<h2 id="_idParaDest-99"><a id="_idTextAnchor100"/>Dropping columns</h2>
			<p>If we need to drop a column from a Spark DataFrame, we would use the <strong class="source-inline">drop()</strong> function. We <a id="_idIndexMarker201"/>need to provide the name of the column to be dropped from the DataFrame. Here’s an example of how to use <span class="No-Break">this function:</span></p>
			<pre class="source-code">
data_df = data_df.drop("col_5")
data_df.show()</pre>			<p>As a result, you will see that <strong class="source-inline">col_5</strong> is dropped from <span class="No-Break">the DataFrame:</span></p>
			<pre class="source-code">
+-----+-----+-------------+----------+------+
|col_1|col_2|    col_3    |  col_4   | col_6|
+-----+-----+-------------+----------+------+
|  100|200.0|string_test_1|2023-01-01|     A|
|  200|300.0|string_test_2|2023-02-01|     A|
|  300|400.0|string_test_3|2023-03-01|     A|
+-----+-----+-------------+----------+------+</pre>			<p>We have successfully dropped <strong class="source-inline">col_5</strong> from <span class="No-Break">this DataFrame.</span></p>
			<p>You can <a id="_idIndexMarker202"/>also drop multiple columns in the same drop statement <span class="No-Break">as well:</span></p>
			<pre class="console">
<strong class="source-inline">data_df = data_df.drop("col_4", "col_5")</strong></pre>			<p>Also note that if we drop a column that does not exist in the DataFrame, it will not result in any errors. The resulting DataFrame would remain as <span class="No-Break">it is.</span></p>
			<p>Just like dropping columns, you can also update columns in Spark DataFrames. Now, we will take a look at how we can update columns in a <span class="No-Break">Spark DataFrame.</span></p>
			<h2 id="_idParaDest-100"><a id="_idTextAnchor101"/>Updating columns</h2>
			<p>Updating columns can also be done with the help of the <strong class="source-inline">withColumn()</strong> function in Spark. We need <a id="_idIndexMarker203"/>to provide the name of the column to be updated along with the updated value. Notice that we can also use this function to calculate some new values for the columns. Here’s <span class="No-Break">an example:</span></p>
			<pre class="source-code">
data_df.withColumn("col_2", F.col("col_2") / 100).show()</pre>			<p>This will give us the following <span class="No-Break">updated frame:</span></p>
			<pre class="source-code">
+-----+-----+-------------+----------+------+
|col_1|col_2|    col_3    |   col_4  | col_6|
+-----+-----+-------------+----------+------+
|  100|200.0|string_test_1|2023-01-01|     A|
|  200|300.0|string_test_2|2023-02-01|     A|
|  300|400.0|string_test_3|2023-03-01|     A|
+-----+-----+-------------+----------+------+</pre>			<p>One thing to note here is the use of the <strong class="source-inline">col</strong> function when updating the column. This function is used for column-wise operators. If we don’t use this function, our code will return <span class="No-Break">an error.</span></p>
			<p>You don’t always <a id="_idIndexMarker204"/>have to update a column in a DataFrame if you only need to rename a column. Now, we will see how we can rename columns in a <span class="No-Break">Spark DataFrame.</span></p>
			<h2 id="_idParaDest-101"><a id="_idTextAnchor102"/>Renaming columns</h2>
			<p>For changing the name of a column, we would use the <strong class="source-inline">withColumnRenamed()</strong> function <a id="_idIndexMarker205"/>in Spark. We would need to provide the column name that needs to be changed along with the new column name. Here’s the code to <span class="No-Break">illustrate this:</span></p>
			<pre class="source-code">
data_df = data_df.withColumnRenamed("col_3", "string_col")
data_df.show()</pre>			<p>As a result, we’ll see the <span class="No-Break">following change:</span></p>
			<pre class="source-code">
+-----+-----+-------------+----------+------+
|col_1|col_2|  string_col |   col_4  | col_6|
+-----+-----+-------------+----------+------+
|  100|200.0|string_test_1|2023-01-01|   A  |
|  200|300.0|string_test_2|2023-02-01|   A  |
|  300|400.0|string_test_3|2023-03-01|   A  |
+-----+-----+-------------+----------+------+</pre>			<p>Notice that <strong class="source-inline">col_3</strong> is now called <strong class="source-inline">string_col</strong> after making <span class="No-Break">the change.</span></p>
			<p>Now, let’s shift our focus to some data manipulation techniques in Spark DataFrames. You can have search-like functionality in Spark DataFrames for finding different values in a column. Now, let’s take a look at how we can find unique values in a column of a <span class="No-Break">Spark DataFrame.</span></p>
			<h2 id="_idParaDest-102"><a id="_idTextAnchor103"/>Finding unique values in a column</h2>
			<p>Finding <a id="_idIndexMarker206"/>unique values is a very useful function that would give us distinct values in a column. For this purpose, we can use the <strong class="source-inline">distinct()</strong> function of the Spark DataFrame <span class="No-Break">like so:</span></p>
			<pre class="source-code">
data_df.select("col_6").distinct().show()</pre>			<p>Here is <span class="No-Break">the result:</span></p>
			<pre class="source-code">
+------+
|col_6 |
+------+
|   A  |
+------+</pre>			<p>We applied a <strong class="source-inline">distinct</strong> function on <strong class="source-inline">col_6</strong> to get all the unique values in this column. In our case, the column just had one distinct value, <strong class="source-inline">A</strong>, so that <span class="No-Break">was shown.</span></p>
			<p>We can also use it to find the count of distinct values in a given column. Here’s an example of how to use <span class="No-Break">this function:</span></p>
			<pre class="source-code">
data_df.select(F.countDistinct("col_6").alias("Total_Unique")).show()</pre>			<p>Here is <span class="No-Break">the result:</span></p>
			<pre class="source-code">
+------+
|col_6 |
+------+
|  1   |
+------+</pre>			<p>In this example, we can see the total count of distinct values in <strong class="source-inline">col_6</strong>. Currently, it is the only type of distinct value present in this column, therefore, it <span class="No-Break">returned </span><span class="No-Break"><strong class="source-inline">1</strong></span><span class="No-Break">.</span></p>
			<p>One other useful function in Spark data manipulation is changing the case of a column. Now, let’s take a look at how we can change the case of a column in a <span class="No-Break">Spark DataFrame.</span></p>
			<h2 id="_idParaDest-103"><a id="_idTextAnchor104"/>Changing the case of a column</h2>
			<p>There is also a function that exists in Spark to change the case of a column. We don’t need to <a id="_idIndexMarker207"/>specify each value of the column separately to make use of the function. Once applied, the whole column’s values would change case. One such example is <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
from pyspark.sql.functions import upper
data_df.withColumn('upper_string_col', upper(data_df.string_col)).show()</pre>			<p>Here is <span class="No-Break">the result:</span></p>
			<div>
				<div id="_idContainer024" class="IMG---Figure">
					<img src="image/B19176_04_2.jpg" alt="" role="presentation"/>
				</div>
			</div>
			<p>In this example, we change the case of <strong class="source-inline">string_col</strong> to all caps. We need to assign this to a new column, so, we create a column called <strong class="source-inline">upper_string_col</strong> to store these upper-case values. Also, note that this column is not added to the original <strong class="source-inline">data_df</strong> because we did not save the results back <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">data_df</strong></span><span class="No-Break">.</span></p>
			<p>A lot of times in data manipulation, we would need functions to filter DataFrames. We will now take a look at how we can filter data in a <span class="No-Break">Spark DataFrame.</span></p>
			<h2 id="_idParaDest-104"><a id="_idTextAnchor105"/>Filtering a DataFrame</h2>
			<p>Filtering a DataFrame means that we can get a subset of rows or columns from a DataFrame. There <a id="_idIndexMarker208"/>are different ways of filtering a Spark DataFrame. We will take a look at one <span class="No-Break">example here:</span></p>
			<pre class="source-code">
data_df.filter(data_df.col_1 == 100).show()</pre>			<p>Here is <span class="No-Break">the result:</span></p>
			<pre class="source-code">
+-----+-----+-------------+----------+------+
|col_1|col_2| string_col  |   col_4  |col_6 |
+-----+-----+-------------+----------+------+
|  100|200.0|string_test_1|2023-01-01|   A  |
+-----+-----+-------------+----------+------+</pre>			<p>In this example, we filter <strong class="source-inline">data_df</strong> to only include rows where the column value of <strong class="source-inline">col_1</strong> is equal to <strong class="source-inline">100</strong>. This criterion is met by only one row, therefore, a single row is returned in the <span class="No-Break">resulting DataFrame.</span></p>
			<p>You can use this function to slice and dice your data in a number of different ways based on <span class="No-Break">the requirements.</span></p>
			<p>Since we are talking about data filtering, we can also filter data based on logical operators as well. We will now take a look at how we can use logical operators in DataFrames to <span class="No-Break">filter data.</span></p>
			<h2 id="_idParaDest-105"><a id="_idTextAnchor106"/>Logical operators in a DataFrame</h2>
			<p>Another important set of operators that we can combine with filter operations in a DataFrame <a id="_idIndexMarker209"/>are the logical operators. These consist of AND and OR operators, amongst others. These are used to filter DataFrames based on complex conditions. Let’s take a look at how we use the AND <span class="No-Break">operator here:</span></p>
			<pre class="source-code">
data_df.filter((data_df.col_1 == 100)
                  &amp; (data_df.col_6 == 'A')).show()</pre>			<p>Here is <span class="No-Break">the result:</span></p>
			<pre class="source-code">
+-----+------+-------------+----------+------+
|col_1| col_2|  string_col |   col_4  |col_6 |
+-----+------+-------------+----------+------+
|  100| 200.0|string_test_1|2023-01-01|   A  |
+-----+------+-------------+----------+------+</pre>			<p>In this example, we’re trying to get the rows where the value of <strong class="source-inline">col_1</strong> is equal to <strong class="source-inline">100</strong> and the value of <strong class="source-inline">col_6</strong> is <strong class="source-inline">A</strong>. Currently, only one row fulfills this condition, therefore, one row is returned as <span class="No-Break">a result.</span></p>
			<p>Now, let’s <a id="_idIndexMarker210"/>see how we can use the OR operator to <span class="No-Break">combine conditions:</span></p>
			<pre class="source-code">
data_df.filter((data_df.col_1 == 100)
                  | (data_df.col_2 == 300.00)).show()</pre>			<p>This statement will give the <span class="No-Break">following result:</span></p>
			<pre class="source-code">
+-----+------+-------------+----------+------+
|col_1| col_2|  string_col |   col_4  | col_6|
+-----+------+-------------+----------+------+
|  100| 200.0|string_test_1|2023-01-01|   A  |
|  200| 300.0|string_test_2|2023-02-01|   A  |
+-----+------+-------------+----------+------+</pre>			<p>In this example, we’re trying to get the rows where the value of <strong class="source-inline">col_1</strong> is equal to <strong class="source-inline">100</strong> or the value of <strong class="source-inline">col_2</strong> is equal to <strong class="source-inline">300.0</strong>. Currently, two rows fulfill this condition, therefore, they are returned as <span class="No-Break">a result.</span></p>
			<p>In data filtering, there is another important function to find values in a list. Now, we will see how you use the <strong class="source-inline">isin()</strong> function <span class="No-Break">in PySpark.</span></p>
			<h2 id="_idParaDest-106"><a id="_idTextAnchor107"/>Using isin()</h2>
			<p>The <strong class="source-inline">isin()</strong> function <a id="_idIndexMarker211"/>is used to find <a id="_idIndexMarker212"/>values in a DataFrame column that exist in a list. To do this, we would create a list with some values in it. Once we have the list, then we would use the <strong class="source-inline">isin()</strong> function to see whether some of the values that are in the list exist in the DataFrame. Here’s an example to <span class="No-Break">demonstrate this:</span></p>
			<pre class="source-code">
list = [100, 200]
data_df.filter(data_df.col_1.isin(list)).show()</pre>			<p>Here is <span class="No-Break">the result:</span></p>
			<pre class="source-code">
+-----+-----+-------------+----------+------+
|col_1|col_2|  string_col |   col_4  |col_6 |
+-----+-----+-------------+----------+------+
|  100|200.0|string_test_1|2023-01-01|   A  |
|  200|300.0|string_test_2|2023-02-01|   A  |
+-----+-----+-------------+----------+------+</pre>			<p>In this <a id="_idIndexMarker213"/>example, we see that the values <strong class="source-inline">100</strong> and <strong class="source-inline">200</strong> are present in the <strong class="source-inline">data_df</strong> DataFrame in two of its rows, therefore, both rows <span class="No-Break">are returned.</span></p>
			<p>We can also convert data types for different columns in Spark DataFrames. Now, let’s look at how we can convert different data types in <span class="No-Break">Spark DataFrames.</span></p>
			<h2 id="_idParaDest-107"><a id="_idTextAnchor108"/>Datatype conversions</h2>
			<p>In this <a id="_idIndexMarker214"/>section, we’ll see different ways of converting data types in Spark <span class="No-Break">DataFrame columns.</span></p>
			<p>We will start by using the <strong class="source-inline">cast</strong> function in Spark. The following code <span class="No-Break">illustrates this:</span></p>
			<pre class="source-code">
from pyspark.sql.functions import col
from pyspark.sql.types import StringType,BooleanType,DateType,IntegerType
data_df_2 = data_df.withColumn("col_4",col("col_4").cast(StringType())) \
    .withColumn("col_1",col("col_1").cast(IntegerType()))
data_df_2.printSchema()
data_df.show()</pre>			<p>Here <a id="_idIndexMarker215"/>is <span class="No-Break">the result:</span></p>
			<pre class="source-code">
root
 |-- col_1: integer (nullable = true)
 |-- col_2: double (nullable = true)
 |-- string_col: string (nullable = true)
 |-- col_4: string (nullable = true)
 |-- col_6: string (nullable = false)
+-----+-----+-------------+----------+-----+
|col_1|col_2|   string_col|     col_4|col_6|
+-----+-----+-------------+----------+-----+
|  100|200.0|string_test_1|2023-01-01|    A|
|  200|300.0|string_test_2|2023-02-01|    A|
|  300|400.0|string_test_3|2023-03-01|    A|
+-----+-----+-------------+----------+-----+</pre>			<p>In the preceding code, we see that we’re changing the data types of two columns, namely <strong class="source-inline">col_4</strong> and <strong class="source-inline">col_1</strong>. First, we change <strong class="source-inline">col_4</strong> to string type. This column was previously a date column. Then, we change <strong class="source-inline">col_1</strong> to integer type <span class="No-Break">from </span><span class="No-Break"><strong class="source-inline">long</strong></span><span class="No-Break">.</span></p>
			<p>Here is the schema of <strong class="source-inline">data_df</strong> <span class="No-Break">for reference:</span></p>
			<pre class="source-code">
root
 |-- col_1: long (nullable = true)
 |-- col_2: double (nullable = true)
 |-- string_col: string (nullable = true)
 |-- col_4: date (nullable = true)
 |-- col_5: timestamp (nullable = true)
 |-- col_6: string (nullable = false)</pre>			<p>We see that <strong class="source-inline">col_1</strong> and <strong class="source-inline">col_4</strong> were different <span class="No-Break">data types.</span></p>
			<p>The next <a id="_idIndexMarker216"/>example of changing the data types of the columns is by using the <strong class="source-inline">selectExpr()</strong> function. The following code <span class="No-Break">illustrates this:</span></p>
			<pre class="source-code">
data_df_3 = data_df_2.selectExpr("cast(col_4 as date) col_4",
    "cast(col_1 as long) col_1")
data_df_3.printSchema()
data_df_3.show(truncate=False)</pre>			<p>Here is <span class="No-Break">the result:</span></p>
			<pre class="source-code">
root
 |-- col_4: date (nullable = true)
 |-- col_1: long (nullable = true)
+----------+-----+
|  col_4   |col_1|
+----------+-----+
|2023-01-01|  100|
|2023-02-01|  200|
|2023-03-01|  300|
+----------+-----+</pre>			<p>In the preceding code, we see that we’re changing the data types of two columns, namely <strong class="source-inline">col_4</strong> and <strong class="source-inline">col_1</strong>. First, we change <strong class="source-inline">col_4</strong> back to the <strong class="source-inline">date</strong> type. Then, we change <strong class="source-inline">col_1</strong> to the <span class="No-Break"><strong class="source-inline">long</strong></span><span class="No-Break"> type.</span></p>
			<p>The next example of changing the data types of the columns is by using SQL. The following code <span class="No-Break">illustrates this:</span></p>
			<pre class="source-code">
data_df_3.createOrReplaceTempView("CastExample")
data_df_4 = spark.sql("SELECT DOUBLE(col_1), DATE(col_4) from CastExample")
data_df_4.printSchema()
data_df_4.show(truncate=False)</pre>			<p>Here <a id="_idIndexMarker217"/>is <span class="No-Break">the result:</span></p>
			<pre class="source-code">
root
 |-- col_1: double (nullable = true)
 |-- col_4: date (nullable = true)
+-----+----------+
|col_1|  col_4   |
+-----+----------+
|100.0|2023-01-01|
|200.0|2023-02-01|
|300.0|2023-03-01|
+-----+----------+</pre>			<p>In the preceding code, we see that we’re changing the data types of two columns, namely <strong class="source-inline">col_4</strong> and <strong class="source-inline">col_1</strong>. First, we use <strong class="source-inline">createOrReplaceTempView()</strong> to create a table named <strong class="source-inline">CastExample</strong>. Then, we use this table to change <strong class="source-inline">col_4</strong> back to the <strong class="source-inline">date</strong> type. Then, we change <strong class="source-inline">col_1</strong> to the <span class="No-Break"><strong class="source-inline">double</strong></span><span class="No-Break"> type.</span></p>
			<p>In the data analysis world, working with null values is very valuable. Now, let’s take a look at how we can drop null values from <span class="No-Break">a DataFrame.</span></p>
			<h2 id="_idParaDest-108"><a id="_idTextAnchor109"/>Dropping null values from a DataFrame</h2>
			<p>Sometimes, in the data, there exist null values that can make clean data messy. Dropping <a id="_idIndexMarker218"/>nulls is an essential exercise that a lot of data analysts and data engineers need to do. Pyspark provides us with relevant functions to <span class="No-Break">do this.</span></p>
			<p>Let’s create another DataFrame called <strong class="source-inline">salary_data</strong> to show some of the <span class="No-Break">next operations:</span></p>
			<pre class="source-code">
salary_data = [("John", "Field-eng", 3500),
    ("Michael", "Field-eng", 4500),
    ("Robert", None, 4000),
    ("Maria", "Finance", 3500),
    ("John", "Sales", 3000),
    ("Kelly", "Finance", 3500),
    ("Kate", "Finance", 3000),
    ("Martin", None, 3500),
    ("Kiran", "Sales", 2200),
    ("Michael", "Field-eng", 4500)
  ]
columns= ["Employee", "Department", "Salary"]
salary_data = spark.createDataFrame(data = salary_data, schema = columns)
salary_data.printSchema()
salary_data.show()</pre>			<p>Here is <span class="No-Break">the result:</span></p>
			<pre class="source-code">
root
 |-- Employee: string (nullable = true)
 |-- Department: string (nullable = true)
 |-- Salary: long (nullable = true)
+--------+----------+------+
|Employee|Department|Salary|
+--------+----------+------+
|    John| Field-eng| 3500 |
| Michael| Field-eng| 4500 |
|  Robert|      null| 4000 |
|   Maria|   Finance| 3500 |
|    John|     Sales| 3000 |
|   Kelly|   Finance| 3500 |
|    Kate|   Finance| 3000 |
|  Martin|      null| 3500 |
|   Kiran|     Sales| 2200 |
| Michael| Field-eng| 4500 |
+--------+----------+------+</pre>			<p>Now, let’s <a id="_idIndexMarker219"/>take a look at the <strong class="source-inline">dropna()</strong> function; this will help us drop null values from <span class="No-Break">our DataFrame:</span></p>
			<pre class="source-code">
salary_data.dropna().show()</pre>			<p>Here is <span class="No-Break">the result:</span></p>
			<pre class="source-code">
+--------+----------+------+
|Employee|Department|Salary|
+--------+----------+------+
| John   | Field-eng| 3500 |
| Michael| Field-eng| 4500 |
| Maria  |   Finance| 3500 |
| John   |     Sales| 3000 |
| Kelly  |   Finance| 3500 |
| Kate   |   Finance| 3000 |
| Kiran  |     Sales| 2200 |
| Michael| Field-eng| 4500 |
+--------+----------+------+</pre>			<p>We <a id="_idIndexMarker220"/>see in the resulting DataFrame that rows with <strong class="source-inline">Robert</strong> and <strong class="source-inline">Martin</strong> are deleted from the new DataFrame when we use the <span class="No-Break"><strong class="source-inline">dropna()</strong></span><span class="No-Break"> function.</span></p>
			<p>Deduplicating data is another useful technique that is often required in data analysis tasks. Now, let’s take a look at how we can drop duplicate values from <span class="No-Break">a DataFrame.</span></p>
			<h2 id="_idParaDest-109"><a id="_idTextAnchor110"/>Dropping duplicates from a DataFrame</h2>
			<p>Sometimes, in <a id="_idIndexMarker221"/>the data, there are redundant values present that would make clean data messy. Dropping these values might be needed in a lot of use cases. PySpark provides us with the <strong class="source-inline">dropDuplicates()</strong> function to do this. Here’s the code to <span class="No-Break">illustrate this:</span></p>
			<pre class="source-code">
new_salary_data = salary_data.dropDuplicates().show()</pre>			<p>Here is <span class="No-Break">the result:</span></p>
			<pre class="source-code">
+--------+----------+------+
|Employee|Department|Salary|
+--------+----------+------+
|    John| Field-eng|  3500|
| Michael| Field-eng|  4500|
|   Maria|   Finance|  3500|
|    John|     Sales|  3000|
|   Kelly|   Finance|  3500|
|    Kate|   Finance|  3000|
|   Kiran|     Sales|  2200|
+--------+----------+------+</pre>			<p>We <a id="_idIndexMarker222"/>see in this example that employees named Michael are only shown once in the resulting DataFrame after we apply the <strong class="source-inline">dropDuplicates()</strong> function to the original DataFrame. This name and its corresponding values exist in the original <span class="No-Break">DataFrame twice.</span></p>
			<p>Now that we have learned about different data filtering techniques, we will now see how we can aggregate data in <span class="No-Break">Pyspark DataFrames.</span></p>
			<h2 id="_idParaDest-110"><a id="_idTextAnchor111"/>Using aggregates in a DataFrame</h2>
			<p>Some of <a id="_idIndexMarker223"/>the methods <a id="_idIndexMarker224"/>available in Spark for aggregating data are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="source-inline">agg</strong></span></li>
				<li><span class="No-Break"><strong class="source-inline">avg</strong></span></li>
				<li><span class="No-Break"><strong class="source-inline">count</strong></span></li>
				<li><span class="No-Break"><strong class="source-inline">max</strong></span></li>
				<li><span class="No-Break"><strong class="source-inline">mean</strong></span></li>
				<li><span class="No-Break"><strong class="source-inline">min</strong></span></li>
				<li><span class="No-Break"><strong class="source-inline">pivot</strong></span></li>
				<li><span class="No-Break"><strong class="source-inline">sum</strong></span></li>
			</ul>
			<p>We will see some of them in action in the following <span class="No-Break">code examples.</span></p>
			<h3>Average (avg)</h3>
			<p>In the <a id="_idIndexMarker225"/>following example, we see how to use aggregate functions in Spark. We will start by calculating the average of all the values in <span class="No-Break">a column:</span></p>
			<pre class="source-code">
from pyspark.sql.functions import countDistinct, avg
salary_data.select(avg('Salary')).show()</pre>			<p>Here is <span class="No-Break">the result:</span></p>
			<pre class="source-code">
+-----------+
|avg(Salary)|
+-----------+
|     3520.0|
+-----------+</pre>			<p>This example calculates the average of the salary column of the <strong class="source-inline">salary_data</strong> DataFrame. We have passed the <strong class="source-inline">Salary</strong> column to the <strong class="source-inline">avg</strong> function and it has calculated the average of that column <span class="No-Break">for us.</span></p>
			<p>Now, let’s take a look at how to count different elements in a <span class="No-Break">PySpark DataFrame.</span></p>
			<h3>Count</h3>
			<p>In the <a id="_idIndexMarker226"/>following code example, we can see how you use aggregate functions <span class="No-Break">in Spark:</span></p>
			<pre class="source-code">
salary_data.agg({'Salary':'count'}).show()</pre>			<p>Here is <span class="No-Break">the result:</span></p>
			<pre class="source-code">
+-------------+
|count(Salary)|
+-------------+
|           10|
+-------------+</pre>			<p>This example calculates the total count of the values in the <strong class="source-inline">Salary</strong> column of the <strong class="source-inline">salary_data</strong> DataFrame. We have passed the <strong class="source-inline">Salary</strong> column to the <strong class="source-inline">agg</strong> function with <strong class="source-inline">count</strong> as its other parameter, and it has calculated the count of that column <span class="No-Break">for us.</span></p>
			<p>Now, let’s <a id="_idIndexMarker227"/>take a look at how to count distinct elements in a <span class="No-Break">PySpark DataFrame.</span></p>
			<h3>Count distinct values</h3>
			<p>In the <a id="_idIndexMarker228"/>following example, we will look at how to count distinct elements in a <span class="No-Break">PySpark DataFrame:</span></p>
			<pre class="source-code">
salary_data.select(countDistinct("Salary").alias("Distinct Salary")).show()</pre>			<p>Here is <span class="No-Break">the result:</span></p>
			<pre class="source-code">
+---------------+
|Distinct Salary|
+---------------+
|              5|
+---------------+</pre>			<p>This example calculates total distinct values in the salary column of the <strong class="source-inline">salary_data</strong> DataFrame. We have passed the <strong class="source-inline">Salary</strong> column to the <strong class="source-inline">countDistinct</strong> function and it has calculated the count of that column <span class="No-Break">for us.</span></p>
			<p>Now, let’s take a look at how to find maximum values in a <span class="No-Break">PySpark DataFrame.</span></p>
			<h3>Finding maximums (max)</h3>
			<p>In the <a id="_idIndexMarker229"/>following code example, we will take a look at how to find maximum values in a column of a <span class="No-Break">PySpark DataFrame:</span></p>
			<pre class="source-code">
salary_data.agg({'Salary':'max'}).show()</pre>			<p>Here is <span class="No-Break">the result:</span></p>
			<pre class="source-code">
+-----------+
|max(Salary)|
+-----------+
|       4500|
+-----------+</pre>			<p>This example calculates the maximum value out of all the values in the <strong class="source-inline">Salary</strong> column of the <strong class="source-inline">salary_data</strong> DataFrame. We have passed the <strong class="source-inline">Salary</strong> column to the <strong class="source-inline">agg</strong> function <a id="_idIndexMarker230"/>with <strong class="source-inline">max</strong> as its other parameter, and it has calculated the maximum of that column <span class="No-Break">for us.</span></p>
			<p>Now, let’s take a look at how to get the sum of all elements in a <span class="No-Break">PySpark DataFrame.</span></p>
			<h3>Sum</h3>
			<p>In the <a id="_idIndexMarker231"/>following code example, we will look at how to sum all values in a <span class="No-Break">PySpark DataFrames:</span></p>
			<pre class="source-code">
salary_data.agg({'Salary':'sum'}).show()</pre>			<p>Here is <span class="No-Break">the result:</span></p>
			<pre class="source-code">
+-----------+
|sum(Salary)|
+-----------+
|      35200|
+-----------+</pre>			<p>This example calculates the sum of all the values in the <strong class="source-inline">Salary</strong> column of the <strong class="source-inline">salary_data</strong> DataFrame. We have passed the <strong class="source-inline">Salary</strong> column to the <strong class="source-inline">agg</strong> function with <strong class="source-inline">sum</strong> as its other parameter, and it has calculated the sum of that column <span class="No-Break">for us.</span></p>
			<p>Now, let’s take a look at how to sort data in a <span class="No-Break">PySpark DataFrame.</span></p>
			<h3>Sort data with OrderBy</h3>
			<p>In the <a id="_idIndexMarker232"/>following code example, we will look at how can we sort data in ascending order in a <span class="No-Break">PySpark DataFrame:</span></p>
			<pre class="source-code">
salary_data.orderBy("Salary").show()</pre>			<p>Here is <span class="No-Break">the result:</span></p>
			<pre class="source-code">
+--------+----------+------+
|Employee|Department|Salary|
+--------+----------+------+
| Kiran  | Sales    | 2200 |
| Kate   | Finance  | 3000 |
| John   | Sales    | 3000 |
| John   | Field-eng| 3500 |
| Martin | null     | 3500 |
| Kelly  | Finance  | 3500 |
| Maria  | Finance  | 3500 |
| Robert | null     | 4000 |
| Michael| Field-eng| 4500 |
| Michael| Field-eng| 4500 |
+--------+----------+------+</pre>			<p>This example <a id="_idIndexMarker233"/>sorts the full DataFrame based on the values in the <strong class="source-inline">Salary</strong> column of the <strong class="source-inline">salary_data</strong> DataFrame. We have passed the <strong class="source-inline">Salary</strong> column to the <strong class="source-inline">orderBy</strong> function and it has sorted the DataFrame based on <span class="No-Break">this column.</span></p>
			<p>We can also sort the data in descending format by adding another function, <strong class="source-inline">desc()</strong>, to the original <strong class="source-inline">orderBy</strong> function. The following example <span class="No-Break">illustrates this:</span></p>
			<pre class="source-code">
salary_data.orderBy(salary_data["Salary"].desc()).show()</pre>			<p>Here is <span class="No-Break">the result:</span></p>
			<pre class="source-code">
+--------+----------+------+
|Employee|Department|Salary|
+--------+----------+------+
| Michael| Field-eng| 4500 |
| Michael| Field-eng| 4500 |
| Robert | null     | 4000 |
| Martin | null     | 3500 |
| Kelly  | Finance  | 3500 |
| Maria  | Finance  | 3500 |
| John   | Field-eng| 3500 |
| John   | Sales    | 3000 |
| Kate   | Finance  | 3000 |
| Kiran  | Sales    | 2200 |
+--------+----------+------+</pre>			<p>This example <a id="_idIndexMarker234"/>is sorting the full DataFrame in descending order based on the values in the <strong class="source-inline">Salary</strong> column of the <strong class="source-inline">salary_data</strong> DataFrame. We have passed the <strong class="source-inline">Salary</strong> column to the <strong class="source-inline">orderBy</strong> function with <strong class="source-inline">desc()</strong> as an additional function call and it has sorted the DataFrame in descending order based on <span class="No-Break">this column.</span></p>
			<h1 id="_idParaDest-111"><a id="_idTextAnchor112"/>Summary</h1>
			<p>Over the course of this chapter, we have learned how to manipulate data in <span class="No-Break">Spark DataFrames.</span></p>
			<p>We talked about the Spark DataFrame API and what different data types are in Spark. We also learned how to create DataFrames in Spark and how we can view these DataFrames once they’ve been created. Finally, we learned about different data manipulation and data <span class="No-Break">aggregation functions.</span></p>
			<p>In the next chapter, we will cover some advanced operations in Spark with respect to <span class="No-Break">data manipulation.</span></p>
			<h1 id="_idParaDest-112"><a id="_idTextAnchor113"/>Sample question</h1>
			<p>1. Which of the following operations will <span class="No-Break">trigger evaluation?</span></p>
			<ol class="margin-left">
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">DataFrame.filter()</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">DataFrame.distinct()</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">DataFrame.intersect()</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">DataFrame.join()</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">DataFrame.count()</strong></span></li>
			</ol>
			<h2 id="_idParaDest-113"><a id="_idTextAnchor114"/>Answer</h2>
			<ol>
				<li>E</li>
			</ol>
		</div>
	</body></html>