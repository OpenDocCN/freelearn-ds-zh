<html><head></head><body>
        

                            
                    <h1 class="header-title">Maintenance, Optimization, and Performance Tuning</h1>
                
            
            
                
<p>In this chapter, we will cover the following recipes:</p>
<ul>
<li>Organizing the database</li>
<li>Setting up the correct data privilege mechanism</li>
<li>Backing up the database</li>
<li>Using indexes</li>
<li>Clustering for efficiency</li>
<li>Optimizing SQL queries</li>
<li>Migrating a PostGIS database to a different server</li>
<li>Replicating a PostGIS database with streaming replication</li>
<li>Geospatial sharding</li>
<li>Paralellizing in PosgtreSQL</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Introduction</h1>
                
            
            
                
<p>Unlike prior chapters, this chapter does not discuss the capabilities or applications of PostGIS. Instead, it focuses on the techniques for organizing the database, improving the query performance, and ensuring the long-term viability of the spatial data.</p>
<p>These techniques are frequently ignored by most PostGIS users until it is too late - for example, when data has already been lost because of users' actions or the performance has already decreased as the volume of data or number of users increased.</p>
<p>Such neglect is often due to the amount of time required to learn about each technique, as well as the time it takes implement them. This chapter attempts to demonstrate each technique in a distilled manner that minimizes the learning curve and maximizes the benefits.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Organizing the database</h1>
                
            
            
                
<p>One of the most important things to consider when creating and using a database is how to organize the data. The layout should be decided when you first establish the database. The layout can be decided on or changed at a later date, but this is almost guaranteed to be a tedious, if not difficult, task. If it is never decided on, a database will become disorganized over time and introduce significant hurdles when upgrading components or running backups.</p>
<p>By default, a new PostgreSQL database has only one <strong>schemaÂ </strong>- namely, <kbd>public</kbd>. Most users place all the data (their own and third-party modules, such as PostGIS) in the <kbd>public</kbd> schema. Doing so mixes different information from various origins. An easy method with which to separate the information is by using schemas. This enables us to use one schema for our data and a separate schema for everything else.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Getting ready</h1>
                
            
            
                
<p>In this recipe, we will create a database and install PostGIS in its own schema. We will also load some geometries and rasters for future use by other recipes in this chapter.</p>
<p>The following are the two methods to create a PostGIS-enabled database:</p>
<ul>
<li>Using the <kbd>CREATE EXTENSION</kbd> statement</li>
<li>Running the installation SQL scripts with a PostgreSQL client</li>
</ul>
<p>The <kbd>CREATE EXTENSION</kbd> method is available if you are running PostgreSQL 9.1 or a later version and is the recommended method for installing PostGIS:</p>
<ol>
<li>Visit the following link to download the shapefiles for California schools and police stations: <a href="http://scec.usc.edu/internships/useit/content/california-emergency-facilities-shapefile">http://scec.usc.edu/internships/useit/content/california-emergency-facilities-shapefile</a>.<a href="http://scec.usc.edu/internships/useit/content/california-emergency-facilities-shapefile"/></li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">How to do it...</h1>
                
            
            
                
<p>Carry out the following steps to create and organize a database:</p>
<ol>
<li>Create a database named <kbd>chapter10</kbd> by executing the following command:</li>
</ol>
<pre>      <strong>CREATE DATABASE chapter10;</strong></pre>
<ol start="2">
<li>Create a schema named <kbd>postgis</kbd> in the <kbd>chapter10</kbd> database, where we will install PostGIS. Execute the following command:</li>
</ol>
<pre>      <strong>CREATE SCHEMA postgis;</strong></pre>
<ol start="3">
<li>Install PostGIS in the <kbd>postgis</kbd> schema of the <kbd>chapter10</kbd> database:
<ol>
<li>If you are running PostgreSQL 9.1 or a newer version, use the <kbd>CREATE EXTENSION</kbd> statement:</li>
</ol>
</li>
</ol>
<pre>               <strong>CREATE EXTENSION postgis WITH SCHEMA postgis;</strong></pre>
<p style="padding-left: 120px">The <kbd>WITH SCHEMA</kbd> clause of the <kbd>CREATE EXTENSION</kbd> statement instructs PostgreSQL to install PostGIS and its objects in the <kbd>postgis</kbd> schema.</p>
<ol start="4">
<li>Check whether or not the PostGIS installation has succeeded by running the following commands:</li>
</ol>
<pre>      <strong>&gt; psql -U me -d chapter10 </strong>
      <strong>&gt; chapter10=# SET search_path = public, postgis;</strong></pre>
<div><strong><strong><img src="img/f3eebd3c-c546-4bef-ad81-18acf1521025.png"/></strong></strong></div>
<p class="mce-root CDPAlignCenter CDPAlign CDPAlignLeft" style="padding-left: 90px">Verify the list of relations in the schema, which should include all the ones created by the extension:</p>
<div><strong><img src="img/60d51d76-e3f4-419e-b465-4e6c9568bb35.png" style="width:27.58em;height:9.33em;"/></strong></div>
<p class="mce-root" style="padding-left: 90px">If you are using <kbd>pgAdmin</kbd> or a similar database system, you can also check on the graphical interface whether the schemas, views, and table were created successfully.</p>
<p style="padding-left: 90px">The <kbd>SET</kbd> statement instructs PostgreSQL to consider the <kbd>public</kbd> and <kbd>postgis</kbd> schemas when processing any SQL statements from our client connection. Without the <kbd>SET</kbd> statement, the <kbd>\d</kbd> command will not return any relation from the <kbd>postgis</kbd> schema.</p>
<ol start="5">
<li>To prevent the need to manually use the <kbd>SET</kbd> statement every time a client connects to the <kbd>chapter10</kbd> database, alter the database by executing the following command:</li>
</ol>
<pre>      <strong>ALTER DATABASE chapter10 SET search_path = public, postgis;</strong></pre>
<p style="padding-left: 90px">All future connections and queries to <kbd>chapter10</kbd> will result in PostgreSQL automatically using both <kbd>public</kbd> and <kbd>postgis</kbd> schemas.</p>
<p style="padding-left: 90px">Note: It may be the case that, for Windows users, this option may not work well; in version 9.6.7 it worked but not in version 9.6.3. If it does not work, you may need to clearly define the <kbd>search_path</kbd> on every command. Both versions are provided.</p>
<ol start="6">
<li>Load the PRISM rasters and San Francisco boundaries geometry, which we used in <a href="9e0e214c-c084-42dc-afdf-e58e021f9094.xhtml">Chapter 5</a>, <em>Working with Raster Data</em>, by executing the following command:</li>
</ol>
<pre>      <strong>&gt; raster2pgsql -s 4322 -t 100x100 -I -F -C -Y <br/>      C:\postgis_cookbook\data\chap5<br/>      \PRISM\    PRISM_tmin_provisional_4kmM2_201703_asc.asc <br/>      prism | psql -d chapter10 -U me</strong></pre>
<p style="padding-left: 60px">Then, define the search path:</p>
<pre>      <strong>&gt; raster2pgsql -s 4322 -t 100x100 -I -F -C -Y <br/>      C\:postgis_cookbook\data\chap5<br/>      \PRISM\PRISM_tmin_provisional_4kmM2_201703_asc.asc <br/>      prism | psql "dbname=chapter10 options=--search_path=postgis" me </strong></pre>
<ol start="7">
<li>As we did in <a href="9e0e214c-c084-42dc-afdf-e58e021f9094.xhtml">Chapter 5</a>, <em>Working with Raster Data</em>, we will postprocess the raster filenames to a <kbd>date</kbd> column by executing the following command:</li>
</ol>
<pre><strong>      ALTER TABLE postgis.prism ADD COLUMN month_year DATE;</strong><br/><strong>      UPDATE postgis.prism SET month_year = ( </strong><br/><strong>        SUBSTRING(split_part(filename, '_', 5), 0, 5) || '-' || </strong><br/><strong>        SUBSTRING(split_part(filename, '_', 5), 5, 4) || '-01' </strong><br/><strong>      ) :: DATE;</strong></pre>
<ol start="8">
<li>Then, we load the San Francisco boundaries by executing the following command:</li>
</ol>
<pre>      <strong>&gt; shp2pgsql -s 3310 -I<br/>      C\:postgis_cookbook\data\chap5\SFPoly\sfpoly.shp sfpoly |<br/>      psql -d chapter10 -U me</strong></pre>
<p class="mce-root" style="padding-left: 60px">Then, define the search path:</p>
<pre>      <strong>&gt; shp2pgsql -s 3310 -I <br/>      C\:postgis_cookbook\data\chap5\SFPoly\sfpoly.shp <br/>      sfpoly | psql "dbname=chapter10 options=--search_path=postgis" me</strong></pre>
<ol start="9">
<li>Copy this chapter's dataset to its own directory by executing the following commands:</li>
</ol>
<pre>      <strong>&gt; mkdir C:\postgis_cookbook\data\chap10</strong>
      <strong>&gt; cp -r /path/to/book_dataset/chap10 <br/>      C\:postgis_cookbook\data\chap10</strong></pre>
<p style="padding-left: 60px">We will use the shapefiles for California schools and police stations provided by the USEIT program at the University of Southern California. Import the shapefiles by executing the following commands; use the spatial index flag <kbd>-I</kbd> only for the police stations shapefile:</p>
<pre>      <strong>&gt; shp2pgsql -s 4269 -I <br/>      C\:postgis_cookbook\data\chap10\CAEmergencyFacilities\CA_police.shp <br/>      capolice | psql -d chapter10 -U me</strong></pre>
<p style="padding-left: 60px">Then, define the search path:</p>
<pre>      <strong>&gt; shp2pgsql -s 4269 C\:postgis_cookbook\data\chap10<br/>      \CAEmergencyFacilities\CA_schools.shp <br/>      caschools | psql -d chapter10 -U me</strong></pre>
<p style="padding-left: 60px">Then, define the search path:</p>
<pre>      <strong>shp2pgsql -s 4269 -I C\:postgis_cookbook\data\chap10<br/>      \CAEmergencyFacilities\CA_schools.shp <br/>      caschools | psql "dbname=chapter10 options=--search_path=postgis" <br/>      me </strong><strong>shp2pgsql -s 4269 -I <br/>      C\:postgis_cookbook\data\chap10\CAEmergencyFacilities\CA_police.shp <br/>      capolice | psql "dbname=chapter10 options=--search_path=postgis" me</strong>
    
  </pre>


            

            
        
    

        

                            
                    <h1 class="header-title">How it works...</h1>
                
            
            
                
<p>In this recipe, we created a new database and installed PostGIS in its own schema. We kept the PostGIS objects separate from our geometries and rasters without installing PostGIS in the <kbd>public</kbd> schema. This separation keeps the <kbd>public</kbd> schema tidy and reduces the accidental modification or deletion of the PostGIS objects. If the definition of the search path did not work, then use the explicit definition of the schema in all the commands, as shown.</p>
<p>In the following recipes, we will see that our decision to install PostGIS in its own schema results in fewer problems when maintaining the database.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Setting up the correct data privilege mechanism</h1>
                
            
            
                
<p>PostgreSQL provides a fine-grained privilege system that dictates who can use a particular set of data and how that set of data can be accessed by an approved user. Because of its granular nature, creating an effective set of privileges can be confusing, and may result in undesired behavior. There are different levels of access that can be provided, from controlling who can connect to the database server itself, to who can query a view, to who can execute a PostGIS function.</p>
<p>The challenges of establishing a good set of privileges can be minimized by thinking of the database as an onion. The outermost layer has generic rules and each layer inward applies rules that are more specific than the last. An example of this is a company's database server that only the company's network can access.</p>
<p>Only one of the company's divisions can access database A, which contains a schema for each department. Within one schema, all users can run the <kbd>SELECT</kbd> queries against views, but only specific users can add, update, or delete records from tables.</p>
<p>In PostgreSQL, users and groups are known as <strong>roles</strong>. A role can be parent to other roles that are themselves parents to even more roles.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Getting ready</h1>
                
            
            
                
<p>In this recipe, we focus on establishing the best set of privileges for the <kbd>postgis</kbd> schema created in the previous recipe. With the right selection of privileges, we can control who can use the contents of and apply operations to a geometry, geography, or raster column.</p>
<p>One aspect worth mentioning is that the owner of a database object (such as the database itself, a schema, or a table) always has full control over that object. Unless someone changes the owner, the user who created the database object is typically the owner of the object.</p>
<p>Again, when tested in Windows, the functionalities regarding the granting of permission worked on version 9.6.7 and did not work in version 9.6.3.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How to do it...</h1>
                
            
            
                
<p>In the preceding recipe, we imported several rasters and shapefiles to their respective tables. By default, access to those tables is restricted to only the user who performed the import operation, also known as the owner. The following steps permit other users to access those tables:</p>
<ol>
<li>We need to create several groups and users in order for this recipe to demonstrate and test the privileges set in the <kbd>chapter10</kbd> database by executing the following commands:</li>
</ol>
<pre>      <strong>CREATE ROLE group1 NOLOGIN;</strong>
      <strong>CREATE ROLE group2 NOLOGIN;</strong>
      <strong>CREATE ROLE user1 LOGIN PASSWORD 'pass1' IN ROLE group1;</strong>
      <strong>CREATE ROLE user2 LOGIN PASSWORD 'pass2' IN ROLE group1;</strong>
      <strong>CREATE ROLE user3 LOGIN PASSWORD 'pass3' IN ROLE group2;</strong></pre>
<p style="padding-left: 60px">The first two <kbd>CREATE ROLE</kbd> statements create the groups <kbd>group1</kbd> and <kbd>group2</kbd>. The last three <kbd>CREATE ROLE</kbd> statements create three users, with the <kbd>user1</kbd> and <kbd>user2</kbd> users assigned to <kbd>group1</kbd> and the <kbd>user3</kbd> user assigned to <kbd>group2</kbd>.</p>
<ol start="2">
<li>We want <kbd>group1</kbd> and <kbd>group2</kbd> to have access to the <kbd>chapter10</kbd> database. We want <kbd>group1</kbd> to be permitted to connect to the database and create temporary tables, while <kbd>group2</kbd> should be granted all database-level privileges, so we use the <kbd>GRANT</kbd> statement as follows:</li>
</ol>
<pre>      <strong>GRANT CONNECT, TEMP ON DATABASE chapter10 TO GROUP group1;</strong>
      <strong>GRANT ALL ON DATABASE chapter10 TO GROUP group2;</strong></pre>
<ol start="3">
<li>Let's check whether or not the <kbd>GRANT</kbd> statement worked by executing the following commands:</li>
</ol>
<pre>      <strong>&gt; psql -U me -d chapter10</strong></pre>
<div><strong><img src="img/f1835127-0ac0-44f9-b76f-1956fc2ac59c.png" style="width:43.50em;height:15.75em;"/></strong></div>
<p style="padding-left: 60px">As you can see, <kbd>group1</kbd> and <kbd>group2</kbd> are present in the <kbd>Access privileges</kbd> column of the <kbd>chapter10</kbd> database:</p>
<pre>      <strong>group1=Tc/postgres</strong>
      <strong>group2=CTc/postgres</strong></pre>
<ol start="4">
<li>There is one thing in the privileges of <kbd>chapter10</kbd> that may be of concern to us:</li>
</ol>
<pre>      <strong>=Tc/postgres</strong></pre>
<p style="padding-left: 60px">Unlike the privilege listings for <kbd>group1</kbd> and <kbd>group2</kbd>, this listing has no value before the equal sign (<kbd><em>=</em></kbd>). This listing is for the special metagroup <kbd>public</kbd>, which is built into PostgreSQL and to which all users and groups automatically belong.</p>
<ol start="5">
<li>We don't want everyone to have access to the <kbd>chapter10</kbd> database, so we need to use the <kbd>REVOKE</kbd> statement to remove privileges from the <kbd>public</kbd> metagroup by executing the following command:</li>
</ol>
<pre>      <strong>REVOKE ALL ON DATABASE chapter10 FROM public;</strong></pre>
<ol start="6">
<li>Let's see what the initial privileges are for the schemas of the <kbd>chapter10</kbd> database by executing the following command:</li>
</ol>
<div><img src="img/ba689d1e-20bb-4ea4-8f09-2a12352cbf16.png" style="text-align: center;font-size: 1em;width:48.08em;height:10.75em;"/></div>
<ol start="7">
<li>The <kbd>postgis</kbd> schema has no privileges listed. However, this does not mean that no one can access the <kbd>postgis</kbd> schema. Only the owner of the schema -<kbd>postgres</kbd>, in this case - can access it. We will grant access to the <kbd>postgis</kbd> schema to both <kbd>group1</kbd> and <kbd>group2</kbd> by executing the following command:</li>
</ol>
<pre>      <strong>GRANT USAGE ON SCHEMA postgis TO group1, group2;</strong></pre>
<p style="padding-left: 60px">We generally do not want to grant the <kbd>CREATE</kbd> privilege in the <kbd>postgis</kbd> schema to any user or group. New objects (such as functions, views, and tables) should not be added to the <kbd>postgis</kbd> schema.</p>
<ol start="8">
<li>If we want all users and groups to have access to the <kbd>postgis</kbd> schema, we can grant the <kbd>USAGE</kbd> privilege to the metagroup <kbd>public</kbd> by executing the following command:</li>
</ol>
<pre>      <strong>GRANT USAGE ON SCHEMA postgis TO public;</strong></pre>
<p style="padding-left: 60px">If you want to revoke this privilege, use the following command:</p>
<pre>      <strong>REVOKE USAGE ON SCHEMA postgis FROM public;</strong></pre>
<ol start="9">
<li>Before continuing further, we should check that our privileges have been reflected in the database:</li>
</ol>
<div><strong><img src="img/49120cf3-41f7-4a23-8d50-fd4f88fa7982.png" style="width:36.00em;height:11.50em;"/></strong></div>
<p style="padding-left: 60px">Granting the <kbd>USAGE</kbd> privilege to a schema does not allow the granted users and groups to use any objects in the schema. The <kbd>USAGE</kbd> privilege only permits the users and groups to view the schema's child objects. Each child object has its own set of privileges, which we establish in the remaining steps.</p>
<p style="padding-left: 60px">PostGIS comes with more than 1,000 functions. It would be unreasonable to individually set privileges for each of those functions. Instead, we grant the <kbd>EXECUTE</kbd> privilege to the metagroup public and then grant and/or revoke privileges to specific functions, such as management functions.</p>
<ol start="10">
<li>First, grant the <kbd>EXECUTE</kbd> privilege to the metagroup <kbd>public</kbd> by executing the following command:</li>
</ol>
<pre>      <strong>GRANT EXECUTE ON ALL FUNCTIONS IN SCHEMA postgis TO public;</strong></pre>
<ol start="11">
<li>Now, revoke the <kbd>EXECUTE</kbd> privileges of the <kbd>public</kbd> metagroup for some functions, such as <kbd>postgis_full_version()</kbd>, by executing the following command:</li>
</ol>
<pre>      <strong>REVOKE ALL ON FUNCTION postgis_full_version() FROM public;</strong></pre>
<p style="padding-left: 60px">If there are problems accessing the functions on the <kbd>postgis</kbd> schema, use the following command:</p>
<pre>      <strong>REVOKE ALL ON FUNCTION postgis.postgis_full_version() FROM public;</strong></pre>
<p style="padding-left: 60px">The <kbd>GRANT</kbd> and <kbd>REVOKE</kbd> statements do not differentiate between tables and views, so care must be taken to ensure that the applied privileges are appropriate for the object.</p>
<ol start="12">
<li>We will grant the <kbd>SELECT</kbd>, <kbd>REFERENCES</kbd>, and <kbd>TRIGGER</kbd> privileges to the <kbd>public</kbd> metagroup on all <kbd>postgis</kbd> tables and views by executing the following command; none of these privileges gives the <kbd>public</kbd> metagroup the ability to alter the tables' or views' contents:</li>
</ol>
<pre>      <strong>GRANT SELECT, REFERENCES, TRIGGER <br/>      ON ALL TABLES IN SCHEMA postgis TO public;</strong></pre>
<ol start="13">
<li>We want to allow <kbd>group1</kbd> to be able to insert new records into the <kbd>spatial_ref_sys</kbd> table, so we must execute the following command:</li>
</ol>
<pre>      <strong>GRANT INSERT ON spatial_ref_sys TO group1;</strong></pre>
<p style="padding-left: 60px">Groups and users that are not part of <kbd>group1</kbd> (such as <kbd>group2</kbd>) can only use the <kbd>SELECT</kbd> statements on <kbd>spatial_ref_sys</kbd>. Groups and users that are part of <kbd>group1</kbd> can now use the <kbd>INSERT</kbd> statement to add new spatial reference systems.</p>
<ol start="14">
<li>Let's give <kbd>user2</kbd>, which is a member of <kbd>group1</kbd>, the ability to use the <kbd>UPDATE</kbd> and <kbd>DELETE</kbd> statements on <kbd>spatial_ref_sys</kbd> by executing the following command; we are not going to give anyone the privilege to use the <kbd>TRUNCATE</kbd> statement on <kbd>spatial_ref_sys</kbd>:</li>
</ol>
<pre>      <strong>GRANT UPDATE, DELETE ON spatial_ref_sys TO user2;</strong></pre>
<ol start="15">
<li>After establishing the privileges, it is always good practice to check that they actually work. The best way to do so is by logging into the database as one of the users. We will use the <kbd>user3</kbd> user to do this by executing the following command:</li>
</ol>
<pre>      <strong>&gt; psql -d chapter10 -u user3</strong></pre>
<ol start="16">
<li>Now, check that we can run a <kbd>SELECT</kbd> statement on the <kbd>spatial_ref_sys</kbd> table by executing the following commands:</li>
</ol>
<pre>      <strong>chapter10=# SELECT count(*) FROM spatial_ref_sys;</strong></pre>
<p style="padding-left: 60px">Of if the schema need to be defined, use the following sentence:</p>
<div><strong><img src="img/3bd3c608-e4e1-409e-a288-db8316e507e8.png" style="width:36.58em;height:7.42em;"/></strong></div>
<ol start="17">
<li>Let's try inserting a new record in <kbd>spatial_ref_sys</kbd> by executing the following commands:</li>
</ol>
<pre>      <strong>chapter10=# INSERT INTO spatial_ref_sys <br/>      VALUES (99999, 'test', 99999, '', '');</strong>
      <strong>ERROR:  permission denied for relation spatial_ref_sys</strong></pre>
<ol start="18">
<li>Excellent! Now update the records in <kbd>spatial_ref_sys</kbd> by executing the following commands:</li>
</ol>
<pre>      <strong>chapter10=# UPDATE spatial_ref_sys SET srtext = 'Lorum ipsum';</strong>
      <strong>ERROR:  permission denied for relation spatial_ref_sys</strong></pre>
<ol start="19">
<li>Run a final check on the <kbd>postgis_full_version()</kbd> function by executing the following commands:</li>
</ol>
<pre>      <strong>chapter10=# SELECT postgis_full_version();</strong>
      <strong>ERROR:  permission denied for function postgis_full_version</strong></pre>


            

            
        
    

        

                            
                    <h1 class="header-title">How it works...</h1>
                
            
            
                
<p>In this recipe, we granted and revoked privileges based on the group or user, with security increasing as a group or user descends into the database. This resulted in <kbd>group1</kbd> and <kbd>group2</kbd> being able to connect to the <kbd>chapter10</kbd> database and use objects found in the <kbd>postgis</kbd> schema. <kbd>group1</kbd> could also insert new records into the <kbd>spatial_ref_sys</kbd> table. Only <kbd>user2</kbd> was permitted to update or delete the records of <kbd>spatial_ref_sys</kbd>.</p>
<p>The <kbd>GRANT</kbd> and <kbd>REVOKE</kbd> statements used in this recipe work, but they can be tedious to use with a command-line utility, such as <kbd>psql</kbd>. Instead, use a graphical tool, such as pgAdmin, that provides a grant wizard. Such tools also make it easier to check the behavior of the database after granting and revoking privileges.</p>
<p>For additional practice, set up the privileges on the <kbd>public</kbd> schema and child objects so that, although <kbd>group1</kbd> and <kbd>group2</kbd> will be able to run the <kbd>SELECT</kbd> queries on the tables, only <kbd>group2</kbd> will be able to use the <kbd>INSERT</kbd> statement on the <kbd>caschools</kbd> table. You will also want to make sure that an <kbd>INSERT</kbd> statement executed by a user of <kbd>group2</kbd> actually works.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Backing up the database</h1>
                
            
            
                
<p>Maintaining functional backups of your data and work is probably the least appreciated, yet the most important thing you can do to improve your productivity (and stress levels). You may think that you don't need to have backups of your PostGIS database because you have the original data imported to the database, but do you remember all the work you did to develop the final product? How about the intermediary products? Even if you remember every step in the process, how much time will it take to create the intermediary and final products?</p>
<p>If any of these questions gives you pause, you need to create a backup for your data. Fortunately, PostgreSQL makes the backup process painless, or at least less painful than the alternatives.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Getting ready</h1>
                
            
            
                
<p>In this recipe, we use PostgreSQL's <kbd>pg_dump</kbd> utility. The <kbd>pg_dump</kbd> utility ensures that the data being backed up is consistent, even if it is currently in use.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How to do it...</h1>
                
            
            
                
<p>Use the following steps to back up a database:</p>
<ol>
<li>Start backing up the <kbd>chapter10</kbd> database by executing the following command:</li>
</ol>
<pre>      <strong>&gt; pg_dump -f chapter10.backup -F custom chapter10</strong></pre>
<p style="padding-left: 60px">We use the <kbd>-f</kbd> flag to specify that the backup should be placed in the <kbd>chapter10.backup</kbd> file. We also use the <kbd>-F</kbd> flag to set the format of the backup output as custom - the most flexible and compressed of <kbd>pg_dump's</kbd> output formats by default.</p>
<ol start="2">
<li>Inspect the backup file by outputting the contents onto a SQL file by executing the following command:</li>
</ol>
<pre>      <strong>&gt; pg_restore -f chapter10.sql chapter10.backup</strong></pre>
<p style="padding-left: 60px">After creating a backup, it is good practice to make sure that the backup is valid. We do so with the <kbd>pg_restore</kbd> PostgreSQL tool. The <kbd>-f</kbd> flag instructs <kbd>pg_restore</kbd> to emit the restored output to a file instead of a database. The emitted output comprises standard SQL statements.</p>
<ol start="3">
<li>Use a text editor to view <kbd>chapter10.sql</kbd>. You should see blocks of SQL statements for creating tables, filling created tables, and setting privileges, as shown here:</li>
</ol>
<div><strong><strong><img src="img/e2db1646-4fff-42f8-aa63-7c2bda5be17e.png" style="width:34.83em;height:32.50em;"/></strong></strong></div>
<p style="padding-left: 60px">And the files continue to show information about tables, sequences, and so on:</p>
<div><strong><img src="img/d95a1938-6903-40d5-8808-d5b54a8b0d3d.png" style="width:34.75em;height:28.25em;"/></strong></div>
<ol start="4">
<li>Because we backed up the <kbd>chapter10</kbd> database using the custom format, we have fine-grained control over how <kbd>pg_restore</kbd> behaves and what it restores. Let's extract only the <kbd>public</kbd> schema using the <kbd>-n</kbd> flag, as follows:</li>
</ol>
<pre>      <strong>&gt; pg_restore -f chapter10_public.sql -n public chapter10.backup</strong></pre>
<p style="padding-left: 60px">If you compare <kbd>chapter10_public.sql</kbd> to the <kbd>chapter10.sql</kbd> file exported in the preceding step, you will see that the <kbd>postgis</kbd> schema is not restored.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How it works...</h1>
                
            
            
                
<p>As you can see, backing up your database is easy in PostgreSQL. Unfortunately, backups are meaningless if they are not performed on a regular schedule. If the database is lost or corrupted, any work done since the last backup is also lost. It is recommended that you perform backups at intervals that minimize the amount of work lost. The ideal interval will depend on the frequency of changes made to the database.</p>
<p>The <kbd>pg_dump</kbd> utility can be scheduled to run at regular intervals by adding a job to the operating system's task scheduler; the instructions for doing this are available in the PostgreSQL wiki at <a href="http://wiki.postgresql.org/wiki/Automated_Backup_on_Windows">http://wiki.postgresql.org/wiki/Automated_Backup_on_Windows</a> and <a href="http://wiki.postgresql.org/wiki/Automated_Backup_on_Linux">http://wiki.postgresql.org/wiki/Automated_Backup_on_Linux</a>.</p>
<p>The <kbd>pg_dump</kbd> utility is not adequate for all situations. If you have a database undergoing constant changes or that is larger than a few tens of gigabytes, you will need a backup mechanism far more robust than that discussed in this recipe. Information regarding these robust mechanisms can be found in the PostgreSQL documentation at <a href="http://www.postgresql.org/docs/current/static/backup.html">http://www.postgresql.org/docs/current/static/backup.html</a>.</p>
<p>The following are several third-party backup tools available for establishing robust and advanced backup schemes:</p>
<ul>
<li>Barman, which is available at <a href="http://www.pgbarman.org">http://www.pgbarman.org</a></li>
<li>pg-rman, which is available at <a href="http://code.google.com/p/pg-rman">http://code.google.com/p/pg-rman</a></li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Using indexes</h1>
                
            
            
                
<p>A database index is very much like the index of a book (such as this one). While a book's index indicates the pages on which a word is present, a database column index indicates the rows in a table that contain a searched-for value. Just as a book's index does not indicate exactly where on the page a word is located, the database index may not be able to denote the exact location of the searched-for value in a row's column.</p>
<p>PostgreSQL has several types of index, such as <kbd>B-Tree</kbd>, <kbd>Hash</kbd>, <kbd>GIST</kbd>, <kbd>SP-GIST</kbd>, and <kbd>GIN</kbd>. All of these index types are designed to help queries find matching rows faster. What makes the indices different are the underlying algorithms. Generally, to keep things simple, almost all PostgreSQL indexes are of the <kbd>B-Tree</kbd> type. PostGIS (spatial) indices are of the <kbd>GIST</kbd> type.</p>
<p>Geometries, geographies, and rasters are all large, complex objects, and relating to or among these objects takes time. Spatial indices are added to the PostGIS data types to improve search performance. The performance improvement comes not from comparing actual, potentially complex, spatial objects, but rather the simple bounding boxes of those objects.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Getting ready</h1>
                
            
            
                
<p>For this recipe, <kbd>psql</kbd> will be used as follows to time the queries:</p>
<pre><strong>&gt; psql -U me -d chapter10</strong>
<strong>chapter10=# \timing on</strong></pre>
<p>We will use the <kbd>caschools</kbd> and <kbd>sfpoly</kbd> tables loaded in this chapter's first recipe.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How to do it...</h1>
                
            
            
                
<p>The best way to see how a query can be affected by an index is by running the query before and after the addition of an index. In this recipe, in order to avoid the need to define the schema, all the tables are assumed to be on the public schema. The following steps will guide you through the process of optimizing a query with an index:</p>
<ol>
<li>Run the following query, which returns the names of all the schools found in San Francisco:</li>
</ol>
<pre>      <strong>SELECT</strong> <strong>schoolid</strong> <strong>FROM caschools sc J</strong><strong>OIN sfpoly sf <br/></strong><strong>      ON ST_Intersects(sf.geom, ST_Transform(sc.geom, 3310));</strong></pre>
<ol start="2">
<li>The results from the query do not matter. We are more interested in the time it took to run the query. When we run the query three times, it runs with the following elapsed times; your numbers may be different from these numbers:</li>
</ol>
<pre>      <strong>Time: 136.643 ms</strong>
      <strong>Time: 140.863 ms</strong>
      <strong>Time: 135.859 ms</strong></pre>
<ol start="3">
<li>The query ran quickly. But, if the query needs to be run many times (say 1,000 times), it will take more than 500 seconds to run it that number of times. Can the query run faster? Use <kbd>EXPLAIN ANALYZE</kbd> to see how PostgreSQL runs the query, as follows:</li>
</ol>
<pre>      <strong>EXPLAIN ANALYZE</strong>
      <strong>SELECT</strong> <strong>schoolid</strong> <strong>FROM caschools sc </strong><strong>JOIN sfpoly sf <br/></strong><strong>      ON ST_Intersects(sf.geom, ST_Transform(sc.geom, 3310));</strong></pre>
<p style="padding-left: 60px">Adding <kbd>EXPLAIN ANALYZE</kbd> before the query instructs PostgreSQL to return the actual plan used to execute the query, as follows:</p>
<div><img src="img/1cedc1ca-7e3d-4fb2-8ac5-5a26b408acc9.png"/></div>
<p style="padding-left: 60px">What is significant in the preceding <kbd>QUERY PLAN</kbd> is <kbd>Join Filter</kbd>, which has consumed most of the execution time. This may be happening because the <kbd>caschools</kbd> table does not have a spatial index on the <kbd>geom</kbd> column.</p>
<ol start="4">
<li>Add a spatial index to the <kbd>geom</kbd> column, as follows:</li>
</ol>
<pre>      <strong>CREATE INDEX caschools_geom_idx </strong><strong>ON caschools</strong>
    <strong>  USING gist </strong><strong>(geom);</strong></pre>
<ol start="5">
<li>Rerun the query from step 1 three times so as to minimize runtime variations. With a spatial index, the query ran with the following elapsed query times:</li>
</ol>
<pre>      <strong>Time: 95.807 ms</strong>
      <strong>Time: 101.626 ms</strong>
      <strong>Time: 103.748 ms</strong></pre>
<p style="padding-left: 60px">The query did not run much faster with the spatial index. What happened? We need to check the <kbd>QUERY PLAN</kbd>.</p>
<ol start="6">
<li>You can see whether or not, or even how the <kbd>QUERY PLAN</kbd> changed in PostgreSQL using <kbd>EXPLAIN ANALYZE</kbd>, as follows:</li>
</ol>
<div><img src="img/beca8da3-a674-440e-b267-f282a6c016d5.png"/></div>
<p style="padding-left: 60px">The <kbd>QUERY PLAN</kbd> table is the same as that found in <em>step 4</em>. The query is not using the spatial index. Why?</p>
<p style="padding-left: 60px">If you look at the query, we used <kbd>ST_Transform()</kbd> to reproject <kbd>caschools.geom</kbd> on the spatial reference system of <kbd>sfpoly.geom</kbd>. The <kbd>ST_Transform()</kbd> geometries used in the <kbd>ST_Intersects()</kbd> spatial test were in SRID 3310, but the geometries used for the <kbd>caschools_geom_idx</kbd> index were in SRID 4269. This difference in spatial reference systems prevented the use of the index in the query.</p>
<ol start="7">
<li>We can create a spatial index that uses geometries projected in the desired spatial reference system. An index that uses a function is known as a <strong>functional index</strong>. It can be created as follows:</li>
</ol>
<pre><strong>      CREATE INDEX caschools_geom_3310_idx ON caschools</strong><br/><strong>      USING gist (ST_Transform(geom, 3310)); </strong></pre>
<ol start="8">
<li>Rerun the query from step 1 three times to get the following output :</li>
</ol>
<pre>      <strong>Time: 63.359 ms</strong>
      <strong>Time: 64.611 ms</strong>
      <strong>Time: 56.485 ms</strong></pre>
<p style="padding-left: 60px">That's better! The duration of the process has decreased from about 135 ms to 60 ms.</p>
<ol start="9">
<li>Check the <kbd>QUERY PLAN</kbd> table as follows:</li>
</ol>
<div><strong><img src="img/b78beb79-9794-4c27-9580-9892ae640541.png"/></strong></div>
<p style="padding-left: 60px">The plan shows that the query used the <kbd>caschools_geom_3310_idx</kbd> index. The <kbd>Index Scan</kbd> command was significantly faster than the previously used <kbd>Join Filter</kbd> command.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How it works...</h1>
                
            
            
                
<p>Database indices help us quickly and efficiently find the values we are interested in. Generally, a query using an index is faster than one that is not, but the performance improvement may not be to the degree found in this recipe.</p>
<p>Additional information about PostgreSQL and PostGIS indices can be found at the following links:</p>
<ul>
<li><a href="https://www.postgresql.org/docs/9.6/static/indexes.html">https://www.postgresql.org/docs/9.6/static/indexes.html</a></li>
<li><a href="https://postgis.net/docs/using_postgis_dbmanagement.html#idm2267">https://postgis.net/docs/using_postgis_dbmanagement.html#idm2267</a></li>
</ul>
<p>We will discuss query plans in greater detail in a later recipe in this chapter. By understanding query plans, it becomes possible to optimize the performance of deficient queries.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Clustering for efficiency</h1>
                
            
            
                
<p>Most users stop optimizing the performance of a table after adding the appropriate indices. This usually happens because the performance reaches a point where it is good enough. But what if the table has millions or billions of records? This amount of information may not fit in the database server's RAM, thereby forcing hard drive access. Generally, table records are stored sequentially on the hard drive. But the data being fetched for a query from the hard drive may be accessing many different parts of the hard drive. Having to access different parts of a hard drive is a known performance limitation.</p>
<p>To mitigate hard drive performance issues, a database table can have its records reordered on the hard drive so that similar record data is stored next to or near each other. The reordering of a database table is known as <strong>clustering</strong> and is used with the <kbd>CLUSTER</kbd> statement in PostgreSQL.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Getting ready</h1>
                
            
            
                
<p>We will use the California schools (<kbd>caschools</kbd>) and San Francisco boundaries (<kbd>sfpoly</kbd>) tables for this recipe. If neither table is available, refer to the first recipe of this chapter.</p>
<p>The <kbd>psql</kbd> utility will be used for this recipe's queries, as shown here:</p>
<pre><strong>&gt; psql -U me -d chapter10</strong>
<strong>chapter10=# \timing on</strong></pre>


            

            
        
    

        

                            
                    <h1 class="header-title">How to do it...</h1>
                
            
            
                
<p>Use the following steps to cluster a table:</p>
<ol>
<li>Before using the <kbd>CLUSTER</kbd> statement, check the time at which the query used in the previous recipe was executed by executing the following commands:</li>
</ol>
<pre>      <strong>SELECT</strong><strong> schoolid</strong> <strong>FROM caschools sc </strong><strong>JOIN sfpoly sf <br/></strong><strong>      ON ST_Intersects(sf.geom, ST_Transform(sc.geom, 3310));</strong></pre>
<ol start="2">
<li>We get the following performance numbers for three query runs:</li>
</ol>
<pre>      <strong>Time: 80.746 ms</strong>
      <strong>Time: 80.172 ms</strong>
      <strong>Time: 80.004 ms</strong></pre>
<ol start="3">
<li>Cluster the <kbd>caschools</kbd> table using the <kbd>caschools_geom_3310_idx</kbd> index as follows:</li>
</ol>
<pre>      <strong>CLUSTER caschools </strong><strong>USING caschools_geom_3310_idx;</strong></pre>
<ol start="4">
<li>Rerun the query from the first step three times for the following performance timings:</li>
</ol>
<pre>      <strong>Time: 57.880 ms</strong>
      <strong>Time: 55.939 ms</strong>
      <strong>Time: 53.107 ms</strong></pre>
<p style="padding-left: 60px">The performance improvements were not significant.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How it works...</h1>
                
            
            
                
<p>Using the <kbd>CLUSTER</kbd> statement on the <kbd>caschools</kbd> table did not result in a significant performance boost. The lesson here is that, despite the fact that the data is physically reordered based on the index information in order to optimize searching, there is no guarantee that query performance will improve on a clustered table. Clustering should be reserved for tables with many large records only after adding the appropriate indices to and optimizing queries for the tables in question.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Optimizing SQL queries</h1>
                
            
            
                
<p>When an SQL query is received, PostgreSQL runs the query through its planner to decide the best execution plan. The best execution plan generally results in the fastest query performance. Though the planner usually makes the correct choices, on occasion, a specific query will have a suboptimal execution plan.</p>
<p>For these situations, the following are several things that can be done to change the behavior of the PostgreSQL planner:</p>
<ul>
<li>Add appropriate column indices to the tables in question</li>
<li>Update the statistics of the database tables</li>
<li>Rewrite the SQL query by evaluating the query's execution plan and using capabilities available in your PostgreSQL installation</li>
<li>Consider changing or adding the layout of the database tables</li>
<li>Change the query planner's configuration</li>
</ul>
<p>Adding indices (the first bullet point) is discussed in a separate recipe found in this chapter. Updating statistics (the second point) is generally done automatically by PostgreSQL after a certain amount of table activity, but the statistics can be manually updated using the <kbd>ANALYZE</kbd> statement. Changing the database layout and the query planner's configuration (the fourth and fifth bullet point, respectively) are advanced operations used only when the first three points have already been attempted and, thus, will not be discussed further.</p>
<p>This recipe only discusses the third option - that is, optimizing performance by rewriting SQL queries.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Getting ready</h1>
                
            
            
                
<p>For this recipe, we will find the nearest police station to every school and the distance in meters between each school in San Francisco and its nearest station; we will attempt to do this as fast as possible. This will require us to rewrite our query many times to be more efficient and take advantage of the new PostgreSQL capabilities. For this recipe, ensure that you also include the <kbd>capolice</kbd> table.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How to do it...</h1>
                
            
            
                
<p>The following steps will guide you through the iterative process required to improve query performance:</p>
<ol>
<li>To find a school's nearest police station and the distance between each school in San Francisco and its nearest station, we will start by executing the following query:</li>
</ol>
<pre>      <strong>SELECT</strong>
    <strong>    di.school,</strong>
    <strong>    police_address,</strong>
    <strong>    distance</strong>
      <strong>FROM ( -- for each school, get the minimum distance to a <br/>            -- police station</strong>
    <strong>    SELECT</strong>
    <strong>      gid,</strong>
    <strong>      school,</strong>
    <strong>      min(distance) AS distance<br/>        FROM ( -- get distance between every school and every police <br/>               -- station in San Francisco</strong>
    <strong>      SELECT</strong>
    <strong>        sc.gid,</strong>
    <strong>        sc.name AS school,</strong>
    <strong>        po.address AS police_address,</strong>
    <strong>        ST_Distance(po.geom_3310, sc.geom_3310) AS distance</strong>
    <strong>      FROM ( -- get schools in San Francisco</strong>
    <strong>        SELECT</strong>
    <strong>          ca.gid,</strong>
    <strong>          ca.name,</strong>
    <strong>          ST_Transform(ca.geom, 3310) AS geom_3310</strong>
    <strong>        FROM sfpoly sf</strong>
    <strong>        JOIN caschools ca</strong>
    <strong>          ON ST_Intersects(sf.geom, ST_Transform(ca.geom, 3310))</strong>
    <strong>      ) sc</strong>
    <strong>      CROSS JOIN ( -- get police stations in San Francisco<br/>            SELECT</strong>
    <strong>          ca.address,</strong>
    <strong>          ST_Transform(ca.geom, 3310) AS geom_3310<br/>            FROM sfpoly sf</strong>
    <strong>        JOIN capolice ca</strong>
    <strong>          ON ST_Intersects(sf.geom, ST_Transform(ca.geom, 3310))</strong>
    <strong>      ) po </strong><strong>ORDER BY 1, 2, 4</strong>
    <strong>    ) scpo</strong>
    <strong>    GROUP BY 1, 2</strong>
    <strong>    ORDER BY 2</strong>
    <strong>) di JOIN ( -- for each school, collect the police station      </strong>
    <strong>           -- addresses ordered by distance</strong>
    <strong>  SELECT</strong>
    <strong>    gid,</strong>
    <strong>    school,</strong>
    <strong>    (array_agg(police_address))[1] AS police_address</strong>
    <strong>  FROM (-- get distance between every school and <br/>            every police station in San Francisco</strong>
    <strong>    SELECT</strong>
    <strong>      sc.gid,</strong>
    <strong>      sc.name AS school,</strong>
    <strong>      po.address AS police_address,</strong>
    <strong>      ST_Distance(po.geom_3310, sc.geom_3310) AS distance<br/>        FROM ( -- get schools in San Francisco</strong>
    <strong>      SELECT</strong>
    <strong>        ca.gid,</strong>
    <strong>        ca.name,</strong>
    <strong>        ST_Transform(ca.geom, 3310) AS geom_3310<br/>          FROM sfpoly sf</strong>
    <strong>      JOIN caschools ca</strong>
    <strong>        ON ST_Intersects(sf.geom, ST_Transform(ca.geom, 3310))</strong>
    <strong>    ) sc</strong>
    <strong>    CROSS JOIN ( -- get police stations in San Francisco</strong>
    <strong>      SELECT</strong>
    <strong>        ca.address,</strong>
    <strong>        ST_Transform(ca.geom, 3310) AS geom_3310</strong>
    <strong>      FROM sfpoly sf </strong><strong>JOIN capolice ca <br/></strong><strong>          ON ST_Intersects(sf.geom, ST_Transform(ca.geom, 3310))</strong>
    <strong>    ) po</strong>
    <strong>    ORDER BY 1, 2, 4</strong>
    <strong>  ) scpo</strong>
    <strong>  GROUP BY 1, 2</strong>
    <strong>  ORDER BY 2</strong>
      <strong>) po</strong>
    <strong>    ON di.gid = po.gid</strong>
      <strong>ORDER BY di.school;</strong>
  </pre>
<ol start="2">
<li>Generally speaking, this is a crude and simplistic query. The subquery <kbd>scpo</kbd> occurs twice in the query because it needs to compute the shortest distance from a school to its nearest police station and the name of the police station closest to each school. If each instance of <kbd>scpo</kbd> took 10 seconds to compute, two instances of <kbd>scpo</kbd> would take 20 seconds. This is very detrimental to performance.</li>
</ol>
<p style="padding-left: 60px">Note: the time may vary substantially between experiments, depending on the machine configuration, database usage, and so on. However, the changes in the duration of the experiments will be noticeable and should follow the same improvement ratio presented in this section.</p>
<p style="padding-left: 60px">The query output looks as follows:</p>
<div><strong><img src="img/2bd4c201-1c99-4e8b-9d4f-19788c436949.png"/></strong></div>
<p>...</p>
<div><strong><img src="img/f80ecca2-dd69-41bc-8517-8620721fe083.png"/></strong></div>
<ol start="3">
<li>The query results provide the addresses of the schools in San Francisco, the addresses of the closest police station to each of those schools, and the distance from each school to its closest police station. However, we are also interested in getting the answer as fast as possible. With timing turned on in <kbd>psql</kbd>, we get the following performance numbers for three runs of the query:</li>
</ol>
<pre>      <strong>Time: 5076.363 ms</strong>
      <strong>Time: 4974.282 ms</strong>
      <strong>Time: 5027.721 ms</strong></pre>
<ol start="4">
<li>Just by looking at the query in step 1, we can see that there are redundant subqueries. Let's get rid of those duplicates using <strong>common table expressions</strong> (<strong>CTEs</strong>), introduced in PostgreSQL 8.4. CTEs are used to logically and syntactically separate a block of SQL from subsequent parts of the query. Since CTEs are logically separated, they are run at the start of the query execution and their results are cached for subsequent use:</li>
</ol>
<pre>      <strong>WITH scpo AS ( -- get distance between every school and every <br/>                    -- police station in San Francisco</strong>
    <strong>  SELECT</strong>
    <strong>    sc.gid,</strong>
    <strong>    sc.name AS school,</strong>
    <strong>    po.address AS police_address,</strong>
    <strong>    ST_Distance(po.geom_3310, sc.geom_3310) AS distance<br/>      FROM ( -- get schools in San Francisco</strong>
    <strong>    SELECT</strong>
    <strong>      ca.*,</strong>
    <strong>      ST_Transform(ca.geom, 3310) AS geom_3310</strong>
    <strong>    FROM sfpoly sf</strong>
    <strong>    JOIN caschools ca</strong>
    <strong>      ON ST_Intersects(sf.geom, ST_Transform(ca.geom, 3310))</strong>
    <strong>  ) sc</strong>
    <strong>  CROSS JOIN ( -- get police stations in San Francisco<br/>        SELECT</strong>
    <strong>      ca.*,</strong>
    <strong>      ST_Transform(ca.geom, 3310) AS geom_3310</strong>
    <strong>    FROM sfpoly sf</strong>
    <strong>    JOIN capolice ca</strong>
    <strong>      ON ST_Intersects(sf.geom, ST_Transform(ca.geom, 3310))</strong>
    <strong>  ) po</strong>
    <strong>  ORDER BY 1, 2, 4</strong>
      <strong>)</strong>
    
      <strong>SELECT</strong>
    <strong>    di.school,</strong>
    <strong>    police_address,</strong>
    <strong>    distance</strong>
      <strong>FROM ( -- for each school, get the minimum distance to a                      </strong>
    <strong>         -- police station</strong>
    <strong>    SELECT</strong>
    <strong>      gid,</strong>
    <strong>      school,</strong>
    <strong>      min(distance) AS distance<br/>        FROM scpo</strong>
    <strong>    GROUP BY 1, 2</strong>
     <strong>   ORDER BY 2</strong>
      <strong>) di</strong>
      <strong>JOIN ( -- for each school, collect the police station <br/>             -- addresses ordered by distance</strong>
    <strong>    SELECT</strong>
    <strong>      gid,</strong>
    <strong>      school,</strong>
    <strong>      (array_agg(police_address))[1] AS police_address</strong>
    <strong>    FROM scpo</strong>
    <strong>    GROUP BY 1, 2</strong>
    <strong>    ORDER BY 2</strong>
      <strong>) po</strong>
    <strong>    ON di.gid = po.gid</strong>
      <strong>ORDER BY 1;</strong></pre>
<ol start="5">
<li>Not only is the query syntactically cleaner, but the performance is improved, as shown here:</li>
</ol>
<pre>      <strong>Time: 2803.923 ms</strong>
      <strong>Time: 2798.105 ms</strong>
      <strong>Time: 2796.481 ms</strong></pre>
<p style="padding-left: 60px">The execution times went from more than 5 seconds to less than 3 seconds.</p>
<ol start="6">
<li>Though some may stop optimizing this query at this point, we will continue to improve the query performance. We can use the window functions, which are another PostgreSQL capability introduced in v8.4. Using the window functions as follows, we can get rid of the <kbd>JOIN</kbd> expression:</li>
</ol>
<pre>      <strong>WITH scpo AS ( -- get distance between every school and every<br/>                     -- police station in San Francisco<br/>        SELECT</strong>
    <strong>      sc.name AS school,</strong>
    <strong>      po.address AS police_address,</strong>
    <strong>      ST_Distance(po.geom_3310, sc.geom_3310) AS distance<br/>        FROM ( -- get schools in San Francisco</strong>
    <strong>      SELECT</strong>
    <strong>        ca.name,</strong>
    <strong>        ST_Transform(ca.geom, 3310) AS geom_3310</strong>
    <strong>      FROM sfpoly sf</strong>
    <strong>      JOIN caschools ca</strong>
    <strong>        ON ST_Intersects(sf.geom, ST_Transform(ca.geom, 3310))<br/>        ) sc</strong>
    <strong>    CROSS JOIN ( -- get police stations in San Francisco</strong>
    <strong>      SELECT</strong>
    <strong>        ca.address,</strong>
    <strong>        ST_Transform(ca.geom, 3310) AS geom_3310</strong>
    <strong>      FROM sfpoly sf</strong>
    <strong>      JOIN capolice ca</strong>
    <strong>        ON ST_Intersects(sf.geom, ST_Transform(ca.geom, 3310))</strong>
    <strong>    ) po</strong>
    <strong>    ORDER BY 1, 3, 2</strong>
      <strong>)</strong>
      <strong>SELECT</strong>
    <strong>    DISTINCT school,</strong>
    <strong>    first_value(police_address) <br/>          OVER (PARTITION BY school ORDER BY distance),</strong>
    <strong>    first_value(distance) <br/>          OVER (PARTITION BY school ORDER BY distance)</strong>
      <strong>FROM scpo</strong>
      <strong>ORDER BY 1;</strong></pre>
<ol start="7">
<li>We use the <kbd>first_value()</kbd> window function to extract the first <kbd>police_address</kbd> and <kbd>distance</kbd> values for each school sorted by the distance between the school and a police station. The improvement is considerable, reducing from almost 3 seconds to around 1.2 seconds:</li>
</ol>
<pre>      <strong>Time: 1261.473 ms</strong>
      <strong>Time: 1217.843 ms</strong>
      <strong>Time: 1215.086 ms</strong></pre>
<ol start="8">
<li>However, it is worth to inspect the execution plan with <kbd>EXPLAIN ANALYZE VERBOSE</kbd> to see what is decreasing the query performance. Because of the verbosity of the output, we've trimmed it to just the following lines of interest:</li>
</ol>
<div><strong><img src="img/9d14f781-b3a1-4456-8c64-bf3e42a3c659.png"/></strong></div>
<pre class="mce-root">      <strong>...</strong>
    
      <strong>-&gt;  Nested Loop  (cost=0.15..311.48 rows=1 width=48) <br/>          (actual time=15.047..1186.907 rows=7956 loops=1)</strong>
    <strong>      Output: ca.name, ca_1.address, <br/>          st_distance(st_transform(ca_1.geom, 3310), <br/>          st_transform(ca.geom, 3310))</strong></pre>
<ol start="9">
<li>In the <kbd>EXPLAIN ANALYZE VERBOSE</kbd> output, we want to inspect the values for the actual time, which provide the actual start and end times for that part of the query. Of all the actual time ranges, the actual time value of 15.047..1186.907 for the <kbd>Nested Loop</kbd> (highlighted in the preceding output) is the worst. This query step consumes at least 80 percent of the total execution time, so any work done to improve performance must be done in this step.</li>
</ol>
<ol start="10">
<li>The columns returned from the slow <kbd>Nested Loop</kbd> utility are found in the value for the output. Of these columns, <kbd>st_distance()</kbd> is present only in this step and not in any inner step. This means we will need to mitigate the number of calls to <kbd>ST_Distance()</kbd>.</li>
<li>At this step, further query improvements are not possible without running PostgreSQL 9.1 or a later version. PostgreSQL 9.1 introduced indexed nearest-neighbor searches using the <kbd>&lt;-&gt;</kbd> and <kbd>&lt;#&gt;</kbd> operators to compare the geometries' convex hulls and bounding boxes, respectively. For point geometries, both operators result in the same answer.</li>
<li>Let's rewrite the query to take advantage of the <kbd>&lt;-&gt;</kbd> operator. The following query still uses the CTEs and window functions:</li>
</ol>
<pre>      <strong>WITH sc AS ( -- get schools in San Francisco</strong>
    <strong>    SELECT</strong>
    <strong>      ca.gid,</strong>
    <strong>      ca.name,</strong>
    <strong>      ca.geom</strong>
    <strong>    FROM sfpoly sf</strong>
    <strong>    JOIN caschools ca</strong>
    <strong>      ON ST_Intersects(sf.geom, ST_Transform(ca.geom, 3310))</strong>
      <strong>), po AS ( -- get police stations in San Francisco</strong>
    <strong>    SELECT</strong>
    <strong>      ca.gid,</strong>
    <strong>      ca.address,</strong>
    <strong>      ca.geom</strong>
    <strong>    FROM sfpoly sf</strong>
    <strong>    JOIN capolice ca</strong>
    <strong>      ON ST_Intersects(sf.geom, ST_Transform(ca.geom, 3310))</strong>
      <strong>)</strong>
      <strong>SELECT</strong>
    <strong>    school,</strong>
    <strong>    police_address,</strong>
    <strong>    ST_Distance(ST_Transform(school_geom, 3310), <br/>        ST_Transform(police_geom, 3310)) AS distance</strong>
      <strong>FROM ( -- for each school, number and order the police<br/>             -- stations by how close each station is to the school</strong>
    <strong>    SELECT</strong>
    <strong>      ROW_NUMBER() OVER (<br/>            PARTITION BY sc.gid ORDER BY sc.geom &lt;-&gt; po.geom<br/>          ) AS r,</strong>
    <strong>      sc.name AS school,</strong>
    <strong>      sc.geom AS school_geom,</strong>
    <strong>      po.address AS police_address,</strong>
    <strong>      po.geom AS police_geom</strong>
    <strong>    FROM sc</strong>
    <strong>    CROSS JOIN po</strong>
      <strong>) scpo</strong>
      <strong>WHERE r &lt; 2</strong>
      <strong>ORDER BY 1;</strong></pre>
<ol start="13">
<li>The query has the following performance numbers:</li>
</ol>
<pre>      <strong>Time: 83.002 ms</strong>
      <strong>Time: 82.586 ms</strong>
      <strong>Time: 83.327 ms</strong></pre>
<p style="padding-left: 60px">Wow! Using indexed nearest-neighbor searches with the <kbd>&lt;-&gt;</kbd> operator, we reduced our initial query from one second to less than a tenth of a second.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How it works...</h1>
                
            
            
                
<p>In this recipe, we optimized a query that users may commonly encounter while using PostGIS. We started by taking advantage of the PostgreSQL capabilities to improve the performance and syntax of our query. When performance could no longer improve, we ran <kbd>EXPLAIN ANALYZE VERBOSE</kbd> to find out what was consuming most of the query-execution time. We learned that the <kbd>ST_Distance()</kbd> function consumed the most time from the execution plan. We finally used the <kbd>&lt;-&gt;</kbd> operator of PostgreSQL 9.1 to dramatically improve the query-execution time to under a second.</p>
<p>The output of <kbd>EXPLAIN ANALYZE VERBOSE</kbd> used in this recipe is not easy to understand. For complex queries, it is recommended that you use the visual output in pgAdmin (discussed in a separate chapter's recipe) or the color coding provided by the <a href="http://explain.depesz.com/">http://explain.depesz.com/</a> web service, as shown in the following screenshot:</p>
<div><img src="img/26713564-98ba-4901-ae34-3e187fd27ab1.png"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title">Migrating a PostGIS database to a different server</h1>
                
            
            
                
<p>At some point, user databases need to be migrated to a different server. This need for server migration could be due to new hardware or a database-server software upgrade.</p>
<p>The following are the three methods available for migrating a database:</p>
<ul>
<li>Dumping and restoring the database with <kbd>pg_dump</kbd> and <kbd>pg_restore</kbd></li>
<li>Performing an in-place upgrade of the database with <kbd>pg_upgrade</kbd></li>
<li>Performing streaming replication from one server to another</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Getting ready</h1>
                
            
            
                
<p>In this recipe, we will use the <kbd>dump</kbd> and <kbd>restore</kbd> methods to move user data to a new database with a new PostGIS installation. Unlike the other methods, this method is the most foolproof, works in all situations, and stores a backup in case things don't work as expected.</p>
<p>As mentioned before, creating a schema specifically to work with PostGIS may not work properly for Windows users. Working on the <kbd>public</kbd> schema is an option in order to test the results.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How to do it...</h1>
                
            
            
                
<p>On the command line, perform the following steps:</p>
<ol>
<li>Even though a backup file was created in this chapter's third recipe, create a new backup file by executing the following command:</li>
</ol>
<pre>      <strong>&gt; pg_dump -U me -f chapter10.backup -F custom chapter10</strong></pre>
<ol start="2">
<li>Create a new database to which the backup file will be restored by executing the following commands:</li>
</ol>
<pre>      <strong>&gt; psql -d postgres -U me</strong>
      <strong>postgres=# CREATE DATABASE new10;</strong></pre>
<ol start="3">
<li>Connect to the <kbd>new10</kbd>, database and create a <kbd>postgis</kbd> schema as follows:</li>
</ol>
<pre>      <strong>postgres=# \c new10</strong>
      <strong>new10=# CREATE SCHEMA postgis;</strong></pre>
<ol start="4">
<li>Execute the <kbd>CREATE EXTENSION command to install the Postgis extension in the postgis schema</kbd>:</li>
</ol>
<pre>      <strong>new10=# CREATE EXTENSION postgis WITH SCHEMA postgis;</strong></pre>
<ol start="5">
<li>Make sure you set the <kbd>search_path</kbd> parameter to include the <kbd>postgis</kbd> schema, as follows:</li>
</ol>
<pre>      <strong>new10=# ALTER DATABASE new10 SET search_path = public, postgis;</strong></pre>
<ol start="6">
<li>Restore only the <kbd>public</kbd> schema from the backup file to the <kbd>new10</kbd> database by executing the following command:</li>
</ol>
<pre>      <strong>&gt; pg_restore -U me -d new10 --schema=public chapter10.backup</strong></pre>
<ol start="7">
<li>The <kbd>restore</kbd> method runs and should not generate errors. If it does, an error message such as the following will appear:</li>
</ol>
<pre>      <strong>pg_restore: [archiver (db)] Error while PROCESSING TOC:</strong>
      <strong>pg_restore: [archiver (db)] Error from TOC entry 3781; 03496229 <br/>                  TABLE DATA prism postgres</strong>
      <strong>pg_restore: [archiver (db)] COPY failed for table "prism": <br/>      ERROR:  function st_bandmetadata(postgis.raster, integer[]) <br/>              does not exist</strong>
      <strong>LINE 1:  SELECT array_agg(pixeltype)::text[] <br/>               FROM st_bandmetadata($1...</strong></pre>
<p style="padding-left: 60px">We have now installed PostGIS in the <kbd>postgis</kbd> schema, but the database server can't find the <kbd>ST_BandMetadata()</kbd> function. If a function cannot be found, it is usually an issue with <kbd>search_path</kbd>. We will fix this issue in the next step.</p>
<ol start="8">
<li>Check what <kbd>pg_restore</kbd> actually does by executing the following command:</li>
</ol>
<pre>      <strong>pg_restore -f chapter10.sql --schema=public chapter10.backup</strong></pre>
<ol start="9">
<li>Looking at the <kbd>COPY</kbd> statement for the prism table, everything looks fine. But the <kbd>search_path</kbd> method preceding the table does not include the <kbd>postgis</kbd> schema as shown here:</li>
</ol>
<pre>      <strong>SET search_path = public, pg_catalog;</strong></pre>
<ol start="10">
<li>Change the <kbd>search_path</kbd> value in <kbd>chapter10.sql</kbd> to include the <kbd>postgis</kbd> schema by executing the following command:</li>
</ol>
<pre>      <strong>SET search_path = public, postgis, pg_catalog;</strong></pre>
<ol start="11">
<li>Run <kbd>chapter10.sql</kbd> with <kbd>psql</kbd>, as follows; the original <kbd>chapter10.backup</kbd> file can't be used because the necessary change can't be applied to <kbd>pg_restore</kbd>:</li>
</ol>
<pre>      <strong>&gt; psql -U me -d new10 -f chapter10.sql</strong>  </pre>


            

            
        
    

        

                            
                    <h1 class="header-title">How it works...</h1>
                
            
            
                
<p>This procedure is essentially the standard PostgreSQL backup and restore cycle. It may not be simple, but has the benefit of being accessible in terms of the tools used and the control available in each step of the process. Though the other migration methods may be convenient, they typically require faith in an opaque process or the installation of additional software.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Replicating a PostGIS database with streaming replication</h1>
                
            
            
                
<p>The reality of the world is that, given enough time, everything will break. This includes the hardware and software of computers running PostgreSQL. To protect data in PostgreSQL from corruption or loss, backups are taken using tools such as <kbd>pg_dump</kbd>. However, restoring a database backup can take a very long time, during which users cannot use the database.</p>
<p>When downtime must be kept to a minimum or is not acceptable, one or more standby servers are used to compensate for the failed primary PostgreSQL server. The data on the standby server is kept in sync with the primary PostgreSQL server by streaming data as frequently as possible.</p>
<p>In addition, you are strongly discouraged from trying to mix different PostgreSQL versions. Primary and standby servers must run the same PostgreSQL version.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Getting ready</h1>
                
            
            
                
<p>In this recipe, we will use the streaming replication capability introduced in PostgreSQL 9.X. This recipe will use one server with two parallel PostgreSQL installations instead of the typical two or more servers, each with one PostgreSQL installation. We will use two new database clusters in order to keep things simple.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How to do it...</h1>
                
            
            
                
<p>Use the following steps to replicate a PostGIS database:</p>
<ol>
<li>Create directories for the primary and standby database clusters by executing the following commands:</li>
</ol>
<pre>      <strong>&gt; mkdir postgis_cookbook/db</strong>
      <strong>&gt; mkdir postgis_cookbook/db/primary</strong>
      <strong>&gt; mkdir postgis_cookbook/db/standby</strong></pre>
<ol start="2">
<li>Initialize the database clusters with <kbd>initdb</kbd> as follows, defining the user <kbd>me</kbd> as the owner of the database:</li>
</ol>
<pre>      <strong>&gt; cd postgis_cookbook/db</strong>
      <strong>&gt; initdb --encoding=utf8 --locale=en_US.utf-8 -U me -D primary</strong>
      <strong>&gt; initdb --encoding=utf8 --locale=en_US.utf-8 -U me -D standby</strong></pre>
<ol start="3">
<li>You may opt for avoiding the <kbd>--locale=en_US.utf-8</kbd> option if an error occurs; in that case, the system will adopt the default locale on your computer.</li>
<li>Create directories for the archives of the primary and standby database clusters by executing the following commands:</li>
</ol>
<pre><strong>      &gt; mkdir postgis_cookbook/db/primary/archive</strong><br/><strong>      &gt; mkdir postgis_cookbook/db/standby/archive</strong></pre>
<ol start="5">
<li>Open the <kbd>pg_hba.conf</kbd> authentication file of the primary cluster with your preferred editing application.</li>
</ol>
<p>Â </p>
<ol start="6">
<li>If you're running PostgreSQL 9.0, add the following text to the end of <kbd>pg_hba.conf</kbd>:</li>
</ol>
<div><strong><img src="img/09148717-56b5-4ed5-8162-a265d8531a68.png" style="width:42.83em;height:13.50em;"/></strong></div>
<p>For PostgreSQL 9.1 or a later version, the configuration lines are already part of the <kbd>pg_hba.conf</kbd> file. You just need to remove the comment character (<em>#</em>) from the beginning of each matching line.</p>
<ol start="7">
<li>Edit the primary cluster's <kbd>postgresql.conf</kbd> configuration file to set the streaming replication parameters. Search for each parameter, uncomment and replace the assigned value to the following:</li>
</ol>
<pre>      <strong>port = 5433</strong>
      <strong>wal_level = hot_standby </strong>
      <strong>max_wal_senders = 5</strong>
      <strong>wal_keep_segments = 32</strong>
      <strong>archive_mode = on</strong>
      <strong>archive_command = 'copy "%p" <br/>      "C:\\postgis_cookbook\\db\\primary\\archive\\%f"' # for Windows</strong></pre>
<p style="padding-left: 60px">A relative location could also be used:</p>
<pre>      <strong>archive_command = 'copy "%p" "archive\\%f" "%p"'</strong></pre>
<p style="padding-left: 60px">When using Linux or macOS type instead:</p>
<pre>      <strong>archive_command = 'cp %p archive\/%f'</strong></pre>
<ol start="8">
<li>Start PostgreSQL on the primary database cluster by executing the following command:</li>
</ol>
<pre>      <strong>&gt; pg_ctl start -D primary -l primary\postgres.log</strong></pre>
<ol start="9">
<li>Create a base backup of the primary database cluster and copy it to the standby database cluster. Before performing the backup, create an exclusion list file for <kbd>xcopy</kbd> (Windows only) by executing the following command:</li>
</ol>
<pre>      <strong>&gt; notepad exclude.txt</strong></pre>
<ol start="10">
<li>Add the following to <kbd>exclude.txt</kbd>:</li>
</ol>
<pre>      <strong>postmaster.pid</strong>
      <strong>pg_xlog</strong></pre>
<ol start="11">
<li>Run the base backup and copy the directory contents from the primary to the standby database cluster, as follows:</li>
</ol>
<pre>      <strong>&gt; psql -p 5433 -U me -c "SELECT pg_start_backup('base_backup', true)"</strong>
      <strong>&gt; xcopy primary\* standby\ /e /exclude:primary\exclude.txt</strong>
      <strong>&gt; psql -p 5433 -U me -c "SELECT pg_stop_backup()"</strong></pre>
<ol start="12">
<li>Make the following changes to the standby cluster's <kbd>postgresql.conf</kbd> configuration file uncommenting these parameters and adjusting the values:</li>
</ol>
<pre>      <strong>port = 5434</strong>
      <strong>hot_standby = on</strong>
      <strong>archive_command = 'copy "%p" <br/>      "C:\\postgis_cookbook\\db\\standby\\archive\\%f"' # for Windows</strong></pre>
<p style="padding-left: 60px">A relative location could also be used:</p>
<pre>      <strong>archive_command = 'copy ".\\archive\\%f" "%p"'</strong></pre>
<p style="padding-left: 60px">When using Linux or macOS type instead:</p>
<pre>      <strong>archive_command = 'cp %p archive\/%f'</strong></pre>
<ol start="13">
<li>Create the <kbd>recovery.conf</kbd> configuration file in the standby cluster directory by executing the following command for Windows:</li>
</ol>
<pre>      <strong>&gt; notepad standby\recovery.conf</strong></pre>
<p style="padding-left: 60px">For Linux or macOS:</p>
<pre>      <strong>&gt; nano standby\recovery.conf</strong></pre>
<ol start="14">
<li>Enter the following in the <kbd>recovery.conf</kbd> configuration file and save the changes:</li>
</ol>
<pre>      <strong>standby_mode = 'on'</strong>
      <strong>primary_conninfo = 'port=5433 user=me'</strong>
      <strong>restore_command = 'copy <br/>      "C:\\postgis_cookbook\\db\\standby\\archive\\%f" "%p"'</strong></pre>
<p style="padding-left: 60px">Or a relative location could be used also:</p>
<pre>      <strong>restore_command = 'copy ".\\archive\\%f" "%p"'</strong></pre>
<p style="padding-left: 60px">For Linux or macOS use:</p>
<pre>      <strong>restore_command = 'cp %p \archive\/%f"'</strong></pre>
<ol start="15">
<li>Start PostgreSQL on the standby database cluster by executing the following command:</li>
</ol>
<pre>      <strong>&gt; pg_ctl start -U me -D standby -l standby\postgres.log</strong></pre>
<ol start="16">
<li>Run some simple tests to make sure the replication is working.</li>
<li>Create the <kbd>test</kbd> database and the <kbd>test</kbd> table on the primary database server by executing the following commands:</li>
</ol>
<pre>      <strong>&gt; psql -p 5433 -U me</strong>
      <strong>postgres=# CREATE DATABASE test;</strong>
      <strong>postgres=# \c test</strong>
      <strong>test=# CREATE TABLE test AS SELECT 1 AS id, 'one'::text AS value;</strong></pre>
<ol start="18">
<li>Connect to the standby database server by executing the following command:</li>
</ol>
<pre>      <strong>&gt; psql -p 5434 -U me</strong></pre>
<ol start="19">
<li>See if the <kbd>test</kbd> database is present by executing the following command:</li>
</ol>
<pre>      <strong>postgres=# \l</strong>  </pre>
<div><strong><strong><img src="img/bddc7f6b-50a0-4d2b-99ff-29ae5fc3cb10.png" style="width:34.58em;height:8.08em;"/></strong></strong></div>
<ol start="20">
<li>Connect to the <kbd>test</kbd> database and get the list of tables by executing the following command:</li>
</ol>
<pre>      <strong>postgres=# \c test</strong>  </pre>
<div><strong><img src="img/49c1d6eb-9f00-4596-aeca-aba9044138f6.png" style="width:20.50em;height:6.25em;"/></strong></div>
<ol start="21">
<li>Get the records, if any, in the <kbd>test</kbd> table by executing the following commands:</li>
</ol>
<div><strong><img src="img/24afe91f-4db6-429d-9d15-c831d6f64781.png" style="width:19.08em;height:6.42em;"/></strong></div>
<p style="padding-left: 60px">Congratulations! The streaming replication works.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How it works...</h1>
                
            
            
                
<p>As demonstrated in this recipe, the basic setup for streaming replication is straightforward. Changes made to the primary database server are quickly pushed to the standby database server.</p>
<p>There are third-party applications to help establish, administer, and maintain streaming replication on production servers. These applications permit complex replication strategies, including multimaster, multistandby, and proper failover. A few of these applications include the following:</p>
<ul>
<li>Pgpool-II, which is available at <a href="http://www.pgpool.net">http://www.pgpool.net</a></li>
<li>Bucardo, which is available at <a href="http://bucardo.org/wiki/Bucardo">http://bucardo.org/wiki/Bucardo</a></li>
<li>Postgres-XC, which is available at <a href="http://postgresxc.wikia.com/wiki/Postgres-XC_Wiki">http://postgresxc.wikia.com/wiki/Postgres-XC_Wiki</a></li>
<li>Slony-I, which is available at <a href="http://slony.info">http://slony.info</a> <a href="http://slony.info"/></li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Geospatial sharding</h1>
                
            
            
                
<p>Working with large datasets can be challenging for the database engine, especially when they are stored in a single table or in a single database. PostgreSQL offers an option to split the data into several external databases, with smaller tables, that work logically as one. Sharding allows distributing the load of storage and processing of a large dataset so that the impact of large local tables is reduced.</p>
<p>One of the most important issues to make it work is the definition of a function to classify and evenly distribute the data. Given that this function can be a geographical property, sharding can be applied to geospatial data.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Getting ready</h1>
                
            
            
                
<p>In this recipe, we will use the <kbd>postgres_fdw</kbd> extension that allows the creation of foreign data wrappers, needed to access data stored in external PostgreSQL databases. In order to use this extension, we will need the combination of several concepts: server, foreign data wrapper, user mapping, foreign table and table inheritance. We will see them in action in this recipe, and you are welcome to explore them in detail on the PostgreSQL documentation.</p>
<p>We will use the fire hotspot dataset and the world country borders shapefile used in <a href="38f20dd1-ca55-47e8-80cd-21670bcb32b2.xhtml">Chapter 1</a>, <em>Moving Data in and out of PostGIS</em>, in order to distribute the records for the hotspot data based on a geographical criteria, we will create a new distributed version of the hotspot dataset.</p>
<p>We will use the <kbd>postgis_cookbook</kbd> database for this recipe.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How to do it...</h1>
                
            
            
                
<p>If you did not follow the recipes in <a href="38f20dd1-ca55-47e8-80cd-21670bcb32b2.xhtml">Chapter 1</a>, <em>Moving Data in and out of PostGIS</em>, be sure to import the hotspots (<kbd>Global_24h.csv</kbd>) in PostGIS. The following steps explain how to do it with <kbd>ogr2ogr</kbd> (you should import the dataset in their original SRID, 4326, to make spatial operations faster):</p>
<ol>
<li>Start a session in the <kbd>postgis_cookbook</kbd> database:</li>
</ol>
<pre><strong>      &gt; psql -d postgis_cookbook -U me</strong></pre>
<ol start="2">
<li>Create a new schema <kbd>chp10</kbd> in the <kbd>postgis_cookbook</kbd> database:</li>
</ol>
<pre><strong>      postgis_cookbook=# CREATE SCHEMA chp10;</strong></pre>
<ol start="3">
<li>We need to create the <kbd>hotspots_dist</kbd> table, that will serve as parent for the foreign tables:</li>
</ol>
<pre><strong>      postgis_cookbook =# CREATE TABLE chp10.hotspots_dist (id serial <br/>      PRIMARY KEY, the_geom public.geometry(Point,4326));</strong></pre>
<ol start="4">
<li>Exit the <kbd>psql</kbd> environment:</li>
</ol>
<pre><strong>      postgis_cookbook=# \q</strong></pre>
<ol start="5">
<li>Connect to the psql environment as the postgres user:</li>
</ol>
<pre><strong>      &gt; psql -U me</strong></pre>
<ol start="6">
<li>Create the remote databases, connect them, create the <kbd>postgis</kbd> extension and create the foreign tables that will receive the sharded data. Then, exit the <kbd>psql</kbd> environment. For this, execute the following SQL commands:</li>
</ol>
<pre><strong>      postgres=# CREATE DATABASE quad_NW;</strong><br/><strong>      CREATE DATABASE quad_NE;</strong><br/><strong>      CREATE DATABASE quad_SW;</strong><br/><strong>      CREATE DATABASE quad_SE;</strong><br/><strong>      postgres=# \c quad_NW;</strong><br/><strong>      quad_NW =# CREAT EXTENSION postgis;</strong><br/><strong>      quad_NW =# CREATE TABLE hotspots_quad_NW (</strong><br/><strong>        id serial PRIMARY KEY, </strong><br/><strong>        the_geom public.geometry(Point,4326)</strong><br/><strong>      );</strong><br/><strong>      quad_NW =# \c quad_NE;</strong><br/><strong>      quad_NE =# CREAT EXTENSION postgis;</strong><br/><strong>      quad_NE =# CREATE TABLE hotspots_quad_NE (</strong><br/><strong>        id serial PRIMARY KEY, </strong><br/><strong>        the_geom public.geometry(Point,4326)</strong><br/><strong>      );</strong><br/><strong>      quad_NW =# \c quad_SW;</strong><br/><strong>      quad_SW =# CREAT EXTENSION postgis;</strong><br/><strong>      quad_SW =# CREATE TABLE hotspots_quad_SW (</strong><br/><strong>        id serial PRIMARY KEY, </strong><br/><strong>        the_geom public.geometry(Point,4326)</strong><br/><strong>      );</strong><br/><strong>      quad_SW =# \c quad_SE;</strong><br/><strong>      quad_SE =# CREAT EXTENSION postgis;</strong><br/><strong>      quad_SE =# CREATE TABLE hotspots_quad_SE (</strong><br/><strong>        id serial PRIMARY KEY, </strong><br/><strong>        the_geom public.geometry(Point,4326)</strong><br/><strong>      );</strong><br/><strong>      quad_SE =# \q</strong></pre>
<ol start="7">
<li>In order to import the fire dataset, create a GDAL virtual data source composed of just one layer derived from the <kbd>Global_24h.csv</kbd> file. To do so, create a text file named <kbd>global_24h.vrt</kbd> in the same directory where the CSV file is and edit it as follows:</li>
</ol>
<pre>        &lt;OGRVRTDataSource&gt; 
          &lt;OGRVRTLayer name="Global_24h"&gt; 
            &lt;SrcDataSource&gt;Global_24h.csv&lt;/SrcDataSource&gt; 
            &lt;GeometryType&gt;wkbPoint&lt;/GeometryType&gt; 
            &lt;LayerSRS&gt;EPSG:4326&lt;/LayerSRS&gt; 
            &lt;GeometryField encoding="PointFromColumns" <br/>             x="longitude" y="latitude"/&gt; 
          &lt;/OGRVRTLayer&gt; 
        &lt;/OGRVRTDataSource&gt; </pre>
<ol start="8">
<li>Import in PostGIS the <kbd>Global_24h.csv</kbd> file using the <kbd>global_24.vrt</kbd> virtual driver you created in a previous recipe:</li>
</ol>
<pre>      <strong>$ ogr2ogr -f PostgreSQL PG:"dbname='postgis_cookbook' user='me' <br/>      password='mypassword'" -lco SCHEMA=chp10 global_24h.vrt <br/>      -lco OVERWRITE=YES -lco GEOMETRY_NAME=the_geom -nln hotspots</strong></pre>
<ol start="9">
<li>Create the extension <kbd>postgres_fdw</kbd> in the database:</li>
</ol>
<pre>      <strong>postgis_cookbook =# CREATE EXTENSION postgres_fdw;</strong></pre>
<ol start="10">
<li>Define the servers that will host the external databases. You need to define the name of the database, the host address and the port in which the database will receive connections. In this case we will create 4 databases, one per global quadrant, according to latitude and longitude in the Mercator SRID. Execute the following commands to create the four servers:</li>
</ol>
<pre>      <strong>postgis_cookbook =# CREATE SERVER quad_NW <br/>        FOREIGN DATA WRAPPER postgres_fdw OPTIONS <br/>        (dbname 'quad_NW', host 'localhost', port '5432');</strong>
      <strong>CREATE SERVER quad_SW FOREIGN DATA WRAPPER postgres_fdw OPTIONS <br/>        (dbname 'quad_SW', host 'localhost', port '5432');</strong>
      <strong>CREATE SERVER quad_NE FOREIGN DATA WRAPPER postgres_fdw OPTIONS <br/>        (dbname 'quad_NE', host 'localhost', port '5432');</strong>
      <strong>CREATE SERVER quad_SE FOREIGN DATA WRAPPER postgres_fdw OPTIONS <br/>        (dbname 'quad_SE', host 'localhost', port '5432');</strong></pre>
<ol start="11">
<li>For this example, we will be using local databases, but the host parameter can be either an IP address or a database file. The user who creates these commands will be defined as the local owner of the servers.</li>
<li>Create the user mapping in order to be able to connect to the foreign databases. For this, you need to write the login information of the owner of the foreign database in their local server:</li>
</ol>
<pre>      <strong>postgis_cookbook =# CREATE USER MAPPING FOR POSTGRES SERVER quad_NW <br/>        OPTIONS (user 'remoteme1', password 'myPassremote1');</strong>
      <strong>CREATE USER MAPPING FOR POSTGRES SERVER quad_SW <br/>        OPTIONS (user 'remoteme2', password 'myPassremote2');</strong>
      <strong>CREATE USER MAPPING FOR POSTGRES SERVER quad_NE <br/>        OPTIONS (user 'remoteme3', password 'myPassremote3');</strong>
      <strong>CREATE USER MAPPING FOR POSTGRES SERVER quad_SE <br/>        OPTIONS (user 'remoteme4', password 'myPassremote4');</strong>
  </pre>
<ol start="13">
<li>Create the tables in the foreign databases, based on the local table <kbd>chp10.hotspots_dist</kbd>:</li>
</ol>
<pre>    <strong>postgis_cookbook =# CREATE FOREIGN TABLE hotspots_quad_NW () <br/>      INHERITS (chp10.hotspots_dist) SERVER quad_NW <br/>      OPTIONS (table_name 'hotspots_quad_sw');</strong>
    <strong>CREATE FOREIGN TABLE hotspots_quad_SW () INHERITS (chp10.hotspots_dist) <br/>      SERVER quad_SW OPTIONS (table_name 'hotspots_quad_sw');</strong>
    <strong>CREATE FOREIGN TABLE hotspots_quad_NE () INHERITS (chp10.hotspots_dist) <br/>      SERVER quad_NE OPTIONS (table_name 'hotspots_quad_ne');</strong>
    <strong>CREATE FOREIGN TABLE hotspots_quad_SE () INHERITS (chp10.hotspots_dist) <br/>      SERVER quad_SE OPTIONS (table_name 'hotspots_quad_se');</strong>
  </pre>
<ol start="14">
<li>The name of the table name should preferably be written in lowercase.</li>
<li>Create a function that will calculate the quadrant of the point to be inserted in the database:</li>
</ol>
<pre><strong>      postgis_cookbook=# CREATE OR REPLACE <br/>      FUNCTION __trigger_users_before_insert(</strong><strong>) RETURNS trigger AS $__$</strong><br/><strong>      DECLARE</strong><br/><strong>      angle integer;</strong><br/><strong>      BEGIN</strong><br/><strong>        EXECUTE $$ select (st_azimuth(ST_geomfromtext('Point(0 0)',4326), <br/>         $1)</strong><br/><strong>        /(2*PI()))*360 $$ INTO angle</strong><br/><strong>        USING NEW.the_geom;</strong><br/><strong>        IF (angle &gt;= 0 AND angle&lt;90) THEN</strong><br/><strong>          EXECUTE $$</strong><br/><strong>          INSERT INTO hotspots_quad_ne (the_geom) VALUES ($1)</strong><br/><strong>          $$ USING</strong><br/><strong>          NEW.the_geom;</strong><br/><strong>        END IF;</strong><br/><strong>        IF (angle &gt;= 90 AND angle &lt;180) THEN</strong><br/><strong>          EXECUTE $$ INSERT INTO hotspots_quad_NW (the_geom) VALUES ($1)</strong><br/><strong>          $$ USING NEW.the_geom;</strong><br/><strong>        END IF;</strong><br/><strong>        IF (angle &gt;= 180 AND angle &lt;270) THEN</strong><br/><strong>          EXECUTE $$ INSERT INTO hotspots_quad_SW (the_geom) VALUES ($1)</strong><br/><strong>          $$ USING NEW.the_geom;</strong><br/><strong>        END IF;</strong><br/><strong>        IF (angle &gt;= 270 AND angle &lt;360) THEN</strong><br/><strong>          EXECUTE $$ INSERT INTO hotspots_quad_SE (the_geom) VALUES ($1)</strong><br/><strong>          $$ USING NEW.the_geom;</strong><br/><strong>        END IF;</strong><br/><strong>        RETURN null;</strong><br/><strong>      END;</strong><br/><strong>      $__$ LANGUAGE plpgsql;</strong><br/><strong>      CREATE TRIGGER users_before_insert </strong><br/><strong>      BEFORE INSERT ON chp10.hotspots_dist </strong><br/><strong>      FOR EACH ROW EXECUTE PROCEDURE __trigger_users_before_insert();</strong></pre>
<ol start="16">
<li>Insert the test coordinates (10, 10), (-10, 10) and (-10 -10). The first one should be stored in the NE quadrant, the second on the SE quadrant and the third on the SW quadrant.</li>
</ol>
<pre><strong>      postgis_cookbook=# INSERT INTO CHP10.hotspots_dist (the_geom)<br/>        VALUES (0, st_geomfromtext('POINT (10 10)',4326));</strong><br/><strong>      INSERT INTO CHP10.hotspots_dist (the_geom) <br/>        VALUES ( st_geomfromtext('POINT (-10 10)',4326));</strong><br/><strong>      INSERT INTO CHP10.hotspots_dist (the_geom) <br/>        VALUES ( st_geomfromtext('POINT (-10 -10)',4326));</strong></pre>
<ol start="17">
<li>Check the data insertion in the tables, both the local view and the external database <kbd>hotspots_quad_NE</kbd>:</li>
</ol>
<pre><strong>      postgis_cookbook=# SELECT ST_ASTEXT(the_geom) <br/>      FROM CHP10.hotspots_dist;</strong></pre>
<div><img src="img/73fec4b3-0929-4392-a90e-5c99400559aa.png" style="width:15.50em;height:6.92em;"/></div>
<ol start="18">
<li>As can be seen, the local version shows all the points that were inserted. Now, execute the query over a remote database:</li>
</ol>
<pre><strong>      postgis_cookbook=# SELECT ST_ASTEXT(the_geom) FROM hotspots_quad_ne;</strong></pre>
<div><img src="img/f93746d9-a205-44a3-a32d-f313abcd8e9d.png" style="width:16.75em;height:4.42em;"/></div>
<p style="padding-left: 60px">The remote databases only has the point that it should store, based on the trigger function defined earlier.</p>
<ol start="19">
<li>Now, insert all the points from the original hotspot table, imported in step 8. For this test, we will just insert the geometry information. Execute the following SQL sentence:</li>
</ol>
<pre><strong>      postgis_cookbook=# insert into CHP10.hotspots_dist<br/>        (the_geom, quadrant)</strong><br/><strong>      select the_geom, 0 as geom from chp10.hotspots;</strong></pre>
<ol start="20">
<li>As in <em>step 15</em>, in order to check if the results were classified and stored correctly, execute the following queries, to the local table <kbd>hotspots_dist</kbd> and the remote table <kbd>hotsports_quad_ne</kbd>:</li>
</ol>
<pre><strong>      postgis_cookbook=# SELECT ST_ASTEXT(the_geom) <br/>      FROM CHP10.hotspots_dist;</strong></pre>
<div><img src="img/1d1d5449-6471-42c7-ac60-4122c03183a9.png" style="width:36.42em;height:16.08em;"/></div>
<ol start="21">
<li>The results show the first 10 points stored in the local logical version of the database.</li>
</ol>
<pre><strong>      postgis_cookbook=# SELECT ST_ASTEXT(the_geom) FROM hotspots_quad_ne;</strong></pre>
<div><img src="img/5232c5f1-508d-4e2a-ba05-33b76503e49b.png" style="width:35.25em;height:15.83em;"/></div>
<ol start="22">
<li>The results show the first 10 points stored in the remote database with all the points in the NE quadrant. The points indeed show that they all have positive latitude and longitude values. When presented in a GIS application, the results is the following:</li>
</ol>
<div><img src="img/db9c50fd-d050-4be5-a352-e1f6b3566399.png" style="width:63.58em;height:34.00em;"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title">How it works...</h1>
                
            
            
                
<p>In this recipe, a basic setup for geographical sharding is demonstrated. More sophisticated functions can be implemented easily on the same proposed structure. In addition, for heavy lifting applications purposes, there are some products in the market that could be explored, if considered necessary.</p>
<p>The example shown was based partly on a GitHub implementation found at the following link: <a href="https://gist.github.com/sylr/623bab09edd04d53ee4e">https://gist.github.com/sylr/623bab09edd04d53ee4e</a>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Paralellizing in PosgtreSQL</h1>
                
            
            
                
<p>Similar to sharding, working with a large amount of rows within a geospatial table in postgres, will cause a lot of processing time for a single worker. With the release of postgres 9.6, the server is capable of executing queries which can be processed by multiple CPUs for a faster answer. According to the postgres documentation, depending of the table size and the query plan, there might not be a considerable benefit when implementing a parallel query, instead of a serial query.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Getting ready</h1>
                
            
            
                
<p>For this recipe, we need a specific version of postgres. It is not mandatory for you to download and install the postgres version that will be used. The reason is that, some developers might have an already configured postgres database version with data, and having multiple servers running within a computer might cause issues later.</p>
<p>To overcome this problem, we will make use of a <strong>docker container</strong>. A container could be defined as a lightweight instantiation of a software application that is isolated from other containers and your computer host. Similar to a virtual machine, you could have multiple versions of your software stored inside your host, and start multiple containers whenever necessary.</p>
<p>First, we will download docker from <a href="https://docs.docker.com/install/">https://docs.docker.com/install/</a> and install the <strong>Community Edition</strong> (<strong>CE</strong>) version. Then, we will pull an already precompiled docker image. Start a Terminal and run the following command:</p>
<pre><strong>$ docker pull shongololo/postgis</strong></pre>
<p>This docker image has PostgreSQL 10 with Postgis 2.4 and SFCGAL plugin. Now we need to start an instance given the image. An important part corresponds to the <kbd>-p 5433:5432</kbd>. These arguments maps every connection and request that is received at port <kbd>5433</kbd> in your host (local) computer to the <kbd>5432</kbd> port of your container:</p>
<pre><strong>$ docker run --name parallel -p 5433:5432 -v &lt;SHP_PATH&gt;:/data shongololo/postgis</strong></pre>
<p>Now, you can connect to your PostgreSQL container:</p>
<pre><strong>$ docker exec -it parallel /bin/bash</strong><br/><strong>root@d842288536c9:/# psql -U postgres</strong><br/><strong>psql (10.1)</strong><br/><strong>Type "help" for help.</strong><br/><strong>postgres=#</strong></pre>
<p>Where root and <kbd>d842288536c9</kbd> corresponds to your container username and group respectively.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">How to do it...</h1>
                
            
            
                
<p>Because we created an isolated instance of your postgres database, we have to recreate to use, database name and schema. These operations are optional. However, we encourage you to follow this to make this recipe consistent with the rest of the book:</p>
<ol>
<li>Create the user <kbd>me</kbd> in your container:</li>
</ol>
<pre><strong>      root@d842288536c9:/# psql -U postgres</strong><br/><strong>      psql (10.1)</strong><br/><strong>      Type "help" for help.</strong><br/><strong>      postgres=# CREATE USER me WITH PASSWORD 'me';</strong><br/><strong>      CREATE ROLE</strong><br/><strong>      postgres=# ALTER USER me WITH SUPERUSER;</strong><br/><strong>      ALTER ROLE</strong></pre>
<ol start="2">
<li>Reconnect to the database but now as user <kbd>me</kbd> to create database and schema:</li>
</ol>
<pre><strong>      root@d842288536c9:/# PGPASSWORD=me psql -U me -d postgres</strong><br/><strong>      postgres=# CREATE DATABASE "postgis-cookbook";</strong><br/><strong>      CREATE DATABASE</strong><br/><strong>      postgres=# \c postgis-cookbook</strong></pre>
<p style="padding-left: 60px">You are now connected to database <kbd>postgis-cookbook</kbd> as user <kbd>me</kbd>:</p>
<pre><strong>      postgis-cookbook=# CREATE SCHEMA chp10;</strong><br/><strong>      CREATE SCHEMA</strong><br/><strong>      postgis-cookbook=# CREATE EXTENSION postgis;</strong><br/><strong>      CREATE EXTENSION</strong></pre>
<ol start="3">
<li>Insert a layer into the database. In this case, we will make use of the <kbd>gis.osm_buildings_a_free_1 shapefile</kbd> from Colombia. Make sure you have these files within the <kbd>SHP_PATH</kbd> before starting the container. This database insertion could be run in two forms: First one is inside your docker container:</li>
</ol>
<pre><strong>      root@d842288536c9:/# /usr/lib/postgresql/10/bin/shp2pgsql -s 3734 <br/>      -W latin1 /data/gis.osm_buildings_a_free_1.shp chp10.buildings | <br/>      PGPASSWORD=me psql -U me -h localhost -p 5432 -d postgis-cookbook</strong></pre>
<p style="padding-left: 60px">The second option is in your host computer. Make sure to correctlyÂ set your shapefiles path and host port that maps to the <kbd>5432</kbd> container port. Also, your host must have <kbd>postgresql-client</kbd> installed:</p>
<pre><strong>      $ shp2pgsql -s 3734 -W latin1 &lt;SHP_PATH&gt;<br/>      /gis.osm_buildings_a_free_1.shp chp10.buildings | PGPASSWORD=me <br/>      psql -U me -h localhost -p 5433 -d postgis-cookbook</strong></pre>
<ol start="4">
<li>Execute parallel query. Using the building table we can execute a <kbd>postgis</kbd> command in parallel. To check how many workers are created, we make use of the <kbd>EXPLAIN ANALYZE</kbd> command. So, for example, if we want to calculate the sum of all geometries from the table in a serial query:</li>
</ol>
<pre><strong>      postgis-cookbook=# EXPLAIN ANALYZE SELECT Sum(ST_Area(geom)) <br/>      FROM chp10.buildings;</strong></pre>
<p style="padding-left: 60px">We get the following result:</p>
<pre><strong>      Aggregate (cost=35490.10..35490.11 rows=1 width=8) <br/>        (actual time=319.299..319.2 </strong><strong>99 rows=1 loops=1)</strong><br/><strong>      -&gt; Seq Scan on buildings (cost=0.00..19776.16 rows=571416 width=142) <br/>        (actua</strong><strong>l time=0.017..68.961 rows=571416 loops=1)</strong><br/><strong>      Planning time: 0.088 ms</strong><br/><strong>      Execution time: 319.358 ms</strong><br/><strong>      (4 rows)</strong></pre>
<p style="padding-left: 60px">Now, if we modify the <kbd>max_parallel_workers</kbd> and <kbd>max_parallel_workers_per_gather</kbd> parameters, we activate the parallel query capability of PostgreSQL:</p>
<pre><strong>      Aggregate (cost=35490.10..35490.11 rows=1 width=8) <br/>        (actual time=319.299..319.2</strong><strong>99 rows=1 loops=1)</strong><br/><strong>      -&gt; Seq Scan on buildings (cost=0.00..19776.16 rows=571416 width=142) <br/>        (actua</strong><strong>l time=0.017..68.961 rows=571416 loops=1)</strong><br/><strong>      Planning time: 0.088 ms</strong><br/><strong>      Execution time: 319.358 ms</strong><br/><strong>      (4 rows)</strong></pre>
<p style="padding-left: 60px">This command prints in Terminal:</p>
<pre><strong>      Finalize Aggregate (cost=21974.61..21974.62 rows=1 width=8) <br/>        (actual time=232.0</strong><strong>81..232.081 rows=1 loops=1)</strong><br/><strong>      -&gt; Gather (cost=21974.30..21974.61 rows=3 width=8) <br/>        (actual time=232.074..23</strong><strong>2.078 rows=4 loops=1)</strong><br/><strong>      Workers Planned: 3 </strong><br/><strong>      Workers Launched: 3 </strong><br/><strong>      -&gt; Partial Aggregate (cost=20974.30..20974.31 rows=1 width=8) <br/>        (actual </strong><strong>time=151.785..151.785 rows=1 loops=4)</strong><br/><strong>      -&gt; Parallel Seq Scan on buildings <br/>        (cost=0.00..15905.28 rows=184</strong><strong>328 width=142) <br/>        (actual time=0.017..58.480 rows=142854 loops=4)</strong><br/><strong>      Planning time: 0.086 ms</strong><br/><strong>      Execution time: 239.393 ms</strong><br/><strong>      (8 rows)</strong></pre>
<ol start="5">
<li>Execute parallel scans. For example, if we want to select polygons whose area is higher than a value:</li>
</ol>
<pre><strong>      postgis-cookbook=# EXPLAIN ANALYZE SELECT * FROM chp10.buildings <br/>      WHERE ST_Area(geom) &gt; 10000;</strong></pre>
<p style="padding-left: 60px">We get the following result:</p>
<pre><strong>      Seq Scan on buildings (cost=0.00..35490.10 rows=190472 width=190) <br/>        (actual time</strong><strong>=270.904..270.904 rows=0 loops=1)</strong><br/><strong>      Filter: (st_area(geom) &gt; '10000'::double precision)</strong><br/><strong>      Rows Removed by Filter: 571416</strong><br/><strong>      Planning time: 0.279 ms</strong><br/><strong>      Execution time: 270.937 ms</strong><br/><strong>      (5 rows)</strong></pre>
<p style="padding-left: 60px">This query is not executed in parallel. This happens because <kbd>ST_Area</kbd> function is defined with a <kbd>COST</kbd> value of <kbd>10</kbd>. A <kbd>COST</kbd> for PostgreSQL is a positive number giving the estimated execution cost for a function. If we increase this value to <kbd>100</kbd>, we can get a parallel plan:</p>
<pre><strong>      postgis-cookbook=# ALTER FUNCTION ST_Area(geometry) COST 100;</strong><br/><strong>      postgis-cookbook=# EXPLAIN ANALYZE SELECT * FROM chp10.buildings </strong><br/><strong>        WHERE ST_Area(geom) &gt; 10000;</strong></pre>
<p style="padding-left: 60px">Now we have a parallel plan and 3 workers are executing the query:</p>
<pre><strong>      Gather (cost=1000.00..82495.23 rows=190472 width=190) <br/>        (actual time=189.748..18</strong><strong>9.748 rows=0 loops=1)</strong><br/><strong>      Workers Planned: 3</strong><br/><strong>      Workers Launched: 3</strong><br/><strong>      -&gt; Parallel Seq Scan on buildings <br/>        (cost=0.00..62448.03 rows=61443 width=190</strong><strong>)<br/>        (actual time=130.117..130.117 rows=0 loops=4)</strong><br/><strong>      Filter: (st_area(geom) &gt; '10000'::double precision)</strong><br/><strong>      Rows Removed by Filter: 142854</strong><br/><strong>      Planning time: 0.165 ms</strong><br/><strong>      Execution time: 190.300 ms</strong><br/><strong>      (8 rows)</strong></pre>
<ol start="6">
<li>Execute parallel joins. First, we create a point table where we create randomly <kbd>10</kbd> points per polygon:</li>
</ol>
<pre><strong>      postgis-cookbook=# DROP TABLE IF EXISTS chp10.pts_10;</strong><br/><strong>      postgis-cookbook=# CREATE TABLE chp10.pts_10 AS</strong><br/><br/><strong>      SELECT (ST_Dump(ST_GeneratePoints(geom, 10))).geom<br/>        ::Geometry(point, 3734) AS geom,</strong><br/><strong>        gid, osm_id, code, fclass, name, type FROM chp10.buildings;</strong><br/><strong>      postgis-cookbook=# CREATE INDEX pts_10_gix</strong><br/><strong>      ON chp10.pts_10 USING GIST (geom);</strong></pre>
<p style="padding-left: 60px">Now, we can run a table join between two tables, which does not give us a parallel plan:</p>
<pre><strong>      Nested Loop (cost=0.41..89034428.58 rows=15293156466 width=269)</strong><br/><strong>      -&gt; Seq Scan on buildings (cost=0.00..19776.16 rows=571416 width=190)</strong><br/><strong>      -&gt; Index Scan using pts_10_gix on pts_10 <br/>         (cost=0.41..153.88 rows=190 width=79)</strong><br/><strong>      Index Cond: (buildings.geom &amp;&amp; geom)</strong><br/><strong>      Filter: _st_intersects(buildings.geom, geom)</strong></pre>
<p style="padding-left: 60px">For this case, we need to modify the parameter <kbd>parallel_tuple_cost</kbd> which sets the planner's estimate of the cost of transferring one tuple from a parallel worker process to another process. Setting the value to <kbd>0.001</kbd> gives us a parallel plan:</p>
<pre><strong>      Nested Loop (cost=0.41..89034428.58 rows=15293156466 width=269)</strong><br/><strong>      -&gt; Seq Scan on buildings (cost=0.00..19776.16 rows=571416 width=190)</strong><br/><strong>      -&gt; Index Scan using pts_10_gix on pts_10 <br/>         (cost=0.41..153.88 rows=190 width=79)</strong><br/><strong>      Index Cond: (buildings.geom &amp;&amp; geom)</strong><br/><strong>      Filter: _st_intersects(buildings.geom, geom)</strong></pre>


            

            
        
    

        

                            
                    <h1 class="header-title">How it works...</h1>
                
            
            
                
<p>As demonstrated in this recipe, parallelizing queries in PostgreSQL allows the optimization of operations that involve a large dataset. The database engine is already capable of implementing parallelism, but defining the proper configuration is crucial in order to take advantage of the functionality.</p>
<p>In this recipe, we used the <kbd>max_parallel_workers</kbd> and the <kbd>parallel_tuple_cost</kbd> to configure the desired amount a parallelism. We could evaluate the performance with the <kbd>ANALYZE</kbd> function.</p>


            

            
        
    </body></html>