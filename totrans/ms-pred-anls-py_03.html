<html><head></head><body>
  <div id="sbo-rt-content"><div class="chapter" title="Chapter 3. Finding Patterns in the Noise – Clustering and Unsupervised Learning"><div class="titlepage"><div><div><h1 class="title"><a id="ch03"/>Chapter 3. Finding Patterns in the Noise – Clustering and Unsupervised Learning</h1></div></div></div><p>One of the natural questions to ask about a dataset is if it contains groups. For example, if we examine financial market data consisting of stock price fluctuations over time, are there groups of stocks that fall and rise with a similar pattern? Similarly, for a set of customer transactions from an e-commerce business we might ask if are there groups of user accounts distinguished by patterns of similar purchasing activity? By identifying groups of related items using the methods described in this chapter, we can understand data as a set of general patterns rather than just individual points. These patterns can help in making high-level summaries at the outset of a predictive modeling project, or as an ongoing way to report on the shape of the data we are modeling. The groupings produced can serve as insights themselves, or they can provide starting points for the models we will cover in later chapters. For example, the group to which a datapoint is assigned can become a feature of this observation, adding additional information beyond its individual values. Additionally, we can potentially calculate statistics (such as mean and standard deviation) item features within these groups, which may be more robust as model features than individual entries.</p><p>In contrast to the methods we will discuss in later chapters, grouping or <span class="emphasis"><em>clustering</em></span> algorithms are known as <a id="id133" class="indexterm"/>
<span class="strong"><strong>unsupervised learning</strong></span>, meaning we have no response value, such as a sale price or click-through rate, which is used to determine the optimal parameters of the algorithm. Rather, we identify similar datapoints using only the features, and as a secondary analysis might ask whether the clusters we identify share a common pattern in their responses (and thus suggest the cluster is useful in finding groups associated with the outcome we are interested in).</p><p>The task of finding these groups, or <span class="emphasis"><em>clusters</em></span>, has a few common ingredients that vary between algorithms. One is a notion of distance or similarity between items in the dataset, which will allow us to quantitatively compare them. A second is the number of groups we wish to identify; this can be specified initially using domain knowledge, or determined by running an algorithm with different numbers of clusters. We can then identify the best number of clusters that describes a dataset through statistics, such as examining numerical variance within the groups determined by the algorithm, or by visual inspection. In this chapter we will dive into:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">How to normalize data for use in a clustering algorithm and to compute similarity measurements for both categorical and numerical data</li><li class="listitem" style="list-style-type: disc">How to use k-means clustering to identify an optimal number of clusters by examining the squared error function</li><li class="listitem" style="list-style-type: disc">How to use agglomerative clustering to identify clusters at different scales</li><li class="listitem" style="list-style-type: disc">Using affinity propagation to automatically identify the number of clusters in a dataset</li><li class="listitem" style="list-style-type: disc">How to use spectral methods to cluster data with nonlinear relationships between points</li></ul></div><div class="section" title="Similarity and distance metrics"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec17"/>Similarity and distance metrics</h1></div></div></div><p>The first step in <a id="id134" class="indexterm"/>clustering any new dataset is to decide how to compare the similarity (or dissimilarity) between items. Sometimes the choice is dictated by what kinds of <a id="id135" class="indexterm"/>similarity we are most interested in trying to measure, in others it is restricted by the properties of the dataset. In the following sections we illustrate several kinds of distance for numerical, categorical, time series, and set-based data—while this list is not exhaustive, it should cover many of the common use cases you will encounter in business analysis. We will also cover normalizations that may be needed for different data types prior to running clustering algorithms.</p><div class="section" title="Numerical distance metrics"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec34"/>Numerical distance metrics</h2></div></div></div><p>Let us begin by <a id="id136" class="indexterm"/>exploring the data in the <code class="literal">wine.data</code> file. It contains a set of chemical measurements describing <a id="id137" class="indexterm"/>the properties of different kinds of wines, and the quality level (I-III) to which the wines are assigned (Forina, M., et al. <span class="emphasis"><em>PARVUS An Extendible Package for Data Exploration</em></span>. Classification and Correla (1988)). Open the file in an iPython notebook and look at the first few rows with the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; df = pd.read_csv("wine.data",header=None)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; df.head()</strong></span>
</pre></div><div class="mediaobject"><img src="images/B04881_03_01.jpg" alt="Numerical distance metrics"/></div><p>Notice that in this<a id="id138" class="indexterm"/> dataset <a id="id139" class="indexterm"/>we have no column descriptions, which makes the data hard to understand since we do not know what the features are. We need to parse the column names from the dataset description file <code class="literal">wine.names</code>, which in addition to the column names contains additional information about the dataset.With the following code, we generate a regular expression that will match a column name (using a pattern where a number followed by a parenthesis has a column name after it, as you can see in the list of column names in the file):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; import re</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; expr = re.compile('.*[0-9]+\)\s?(\w+).*')</strong></span>
</pre></div><p>We then create an array where the first element is the class label of the wine (whether it belongs to quality category I-III).</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; header_names = ['Class']</strong></span>
</pre></div><p>Iterating through the lines in the file, we extract those that match our regular expression:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; df_header = open("wine.names")</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; for l in df_header.readlines():</strong></span>
<span class="strong"><strong>        if len(expr.findall(l.strip()))!=0:</strong></span>
<span class="strong"><strong>            header_names.append(expr.findall(l.strip())[0])</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; df_header.close()</strong></span>
</pre></div><p>We then assign this list to the dataframe columns property, which contains the names of the columns:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; df.columns = header_names</strong></span>
</pre></div><p>Now that we have appended the column names, we can look at a summary of the dataset using the <code class="literal">df.describe()</code> method:</p><div class="mediaobject"><img src="images/B04881_03_02.jpg" alt="Numerical distance metrics"/></div><p>Having performed<a id="id140" class="indexterm"/> some <a id="id141" class="indexterm"/>cleanup on the data, how can we calculate a similarity measurement between wines based on the information in each row? One option would be to consider each of the wines as a point in a thirteen-dimensional space specified by its dimensions (each of the columns other than <code class="literal">Class</code>). Since the resulting space has thirteen dimensions, we cannot directly visualize the datapoints using a scatterplot to see if they are nearby, but we can calculate distances just the same as with a more familiar 2- or 3-dimensional space using the Euclidean distance formula, which is simply the length of the straight line between two points. This formula for this length can be used whether the points are in a 2-dimensional plot or a more complex space such as this example, and is given by:</p><div class="mediaobject"><img src="images/B04881_03_04.jpg" alt="Numerical distance metrics"/></div><p>Here <span class="emphasis"><em>x</em></span> and <span class="emphasis"><em>y</em></span> are rows of the dataset and <span class="emphasis"><em>n</em></span> is the number of columns. One important aspect of the Euclidean distance formula is that columns whose scale is much different from others can dominate the overall result of the calculation. In our example, the values describing the magnesium content of each wine are <code class="literal">~100x</code> greater than the magnitude of features describing the alcohol content or ash percentage.</p><p>If we were to calculate the distance between these datapoints, it would largely be determined by the magnesium concentration (as even small differences on this scale overwhelmingly determine the value of the distance calculation), rather than any of its other properties. While this might sometimes be desirable (for example, if the column with the<a id="id142" class="indexterm"/> largest <a id="id143" class="indexterm"/>numerical value is the one we most care about for judging similarity), in most applications we do not favor one feature over another and want to give equal weight to all columns. To get a fair distance comparison between these points, we need to normalize the columns so that they fall into the same numerical range (have similar maximal and minimal values). We can do so using the <code class="literal">scale()</code> function in scikit-learn and the following commands, which uses the array <code class="literal">header_names</code> we constructed previously to access all columns but the class label (the first element of the array):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; from sklearn import preprocessing</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; df_normalized = pd.DataFrame(preprocessing.scale(df[header_names[1:]]))</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; df_normalized.columns = header_names[1:]</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; df_normalized.describe()</strong></span>
</pre></div><div class="mediaobject"><img src="images/B04881_03_03.jpg" alt="Numerical distance metrics"/></div><p>This function will subtract the mean value of a column from each element and then divide each point by the standard deviation of the column. This normalization centers the data in each column at mean 0 with variance 1, and in the case of normally distributed data this results in a standard normal distribution. Also, note that the <code class="literal">scale()</code> function returns a <code class="literal">numpy array</code>, which is why we must call <code class="literal">dataframe</code> on the output to use the pandas function <code class="literal">describe()</code>.</p><p>Now that we have scaled the data, we can calculate Euclidean distances between the rows using the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; import sklearn.metrics.pairwise as pairwise</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; distances = pairwise.euclidean_distances(df_normalized)</strong></span>
</pre></div><p>You can verify that this command produces a square matrix of dimension 178 x 178 (the number of rows in the original dataset by the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; distances.shape</strong></span>
</pre></div><p>We have now<a id="id144" class="indexterm"/> converted <a id="id145" class="indexterm"/>our dataset of 178 rows and 13 columns into a square matrix, giving the distance between each of these rows. In other words, row i, column j in this matrix represents the Euclidean distance between rows i and j in our dataset. This 'distance matrix' is the input we will use for clustering inputs in the following section.</p><p>If want to get a sense of how the points are distributed relative to one another using a given distance metric, we can use <a id="id146" class="indexterm"/>
<span class="strong"><strong>multidimensional scaling</strong></span> (<span class="strong"><strong>MDS</strong></span>)—(Borg, Ingwer, and Patrick JF Groenen. Modern multidimensional scaling: Theory and applications. Springer Science &amp; Business Media, 2005; Kruskal, Joseph B. <span class="emphasis"><em>Nonmetric multidimensional scaling: a numerical method.</em></span> Psychometrika 29.2 (1964): 115-129; Kruskal, Joseph B. <span class="emphasis"><em>Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis.</em></span> Psychometrika 29.1 (1964): 1-27) to create a visualization. MDS attempts to find the set of lower dimensional coordinates (here, we will use two dimensions) that best represents the distances between points in the original, higher dimensions of a dataset (here, the pairwise Euclidean distances we calculated from the 13 dimensions). MDS finds the optimal 2-d coordinates according to the strain function:</p><div class="mediaobject"><img src="images/B04881_03_09.jpg" alt="Numerical distance metrics"/></div><p>Where <code class="literal">D</code> are the distances we calculated between points. The 2-d coordinates that minimize this function are found using <a id="id147" class="indexterm"/>
<span class="strong"><strong>Singular Value Decomposition</strong></span> (<span class="strong"><strong>SVD</strong></span>), which we will discuss in more detail in <a class="link" href="ch06.html" title="Chapter 6. Words and Pixels – Working with Unstructured Data">Chapter 6</a>, <span class="emphasis"><em>Words and Pixels – Working with Unstructured Data</em></span>. After obtaining the coordinates from MDS, we can then plot the results using the <code class="literal">wine</code> class to color points in the diagram. Note that the coordinates themselves have <a id="id148" class="indexterm"/>no interpretation (in fact, they could change each time we run the algorithm due to numerical<a id="id149" class="indexterm"/> randomness in the algorithm). Rather, it is the relative position of points that we are interested in:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; from sklearn.manifold import MDS</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; mds_coords = MDS().fit_transform(distances)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; pd.DataFrame(mds_coords).plot(kind='scatter',x=1,y=0,color=df.Class[:],colormap='Reds')</strong></span>
</pre></div><div class="mediaobject"><img src="images/B04881_03_08.jpg" alt="Numerical distance metrics"/></div><p>Given that there are many ways we could have calculated the distance between datapoints, is the Euclidean distance a good choice here? Visually, based on the multidimensional scaling plot, we can see there is separation between the classes based on the features we have used to calculate distance, so conceptually it appears that this is a reasonable choice in this case. However, the decision also depends on what we are trying to compare. If we are interested in detecting wines with similar attributes in absolute values, then it is a good metric. However, what if we're not interested so much in the absolute composition of the wine, but whether its variables follow similar trends among wines with different alcohol contents? In this case, we would not be interested in the absolute difference in values, but rather the <span class="emphasis"><em>correlation</em></span> between the columns. This sort of comparison is common for time series, which we consider next.</p></div><div class="section" title="Correlation similarity metrics and time series"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec35"/>Correlation similarity metrics and time series</h2></div></div></div><p>For time series data, we are <a id="id150" class="indexterm"/>often <a id="id151" class="indexterm"/>concerned with whether the patterns between series exhibit the same<a id="id152" class="indexterm"/> variation over time, rather than their absolute differences in value. For example, if we <a id="id153" class="indexterm"/>were to compare stocks, we might want to identify groups of stocks whose prices move up and down in similar patterns over time. The absolute price is of less interest than this pattern of increase and decrease. Let us look at an example using the variation in prices of stocks in the <span class="strong"><strong>Dow Jones Industrial Average</strong></span> (<span class="strong"><strong>DJIA</strong></span>) over<a id="id154" class="indexterm"/> time (Brown, Michael Scott, Michael J. Pelosi, and Henry Dirska. <span class="emphasis"><em>Dynamic-radius species-conserving genetic algorithm for the financial forecasting of Dow Jones index stocks.</em></span> Machine Learning and Data Mining in Pattern Recognition. Springer Berlin Heidelberg, 2013. 27-41.). Start by importing the data and examining the first rows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; df = pd.read_csv("dow_jones_index/dow_jones_index.data")</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; df.head()</strong></span>
</pre></div><div class="mediaobject"><img src="images/B04881_03_10.jpg" alt="Correlation similarity metrics and time series"/></div><p>This data contains the daily stock price (for 6 months) for a set of 30 stocks. Because all of the numerical values (the prices) are on the same scale, we won't normalize this data as we did with the wine dimensions.</p><p>We notice two things about this data. First, the closing price per week (the variable we will use to calculate correlation) is presented as a string. Second, the date is not in the correct format for plotting. We will process both columns to fix this, converting these  columns to a <code class="literal">float</code> and <code class="literal">datetime</code> object, respectively, using the following commands.</p><p>To convert the closing price to a number, we apply an anonymous function that takes all characters but the dollar sign and casts it as a float.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; df.close = df.close.apply( lambda x: float(x[1:]))</strong></span>
</pre></div><p>To convert the <a id="id155" class="indexterm"/>date, we<a id="id156" class="indexterm"/> also use <a id="id157" class="indexterm"/>an anonymous function on each row of the date column, splitting the string <a id="id158" class="indexterm"/>in to year, month, and day elements and casting them as integers to form a tuple input for a <code class="literal">datetime</code> object:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; import datetime</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; df.date = df.date.apply( lambda x: datetime.\</strong></span>
<span class="strong"><strong>   datetime(int(x.split('/')[2]),int(x.split('/')[0]),int(x.split('/')[1])))</strong></span>
</pre></div><p>With this transformation, we can now make a pivot table (as we covered in <a class="link" href="ch02.html" title="Chapter 2. Exploratory Data Analysis and Visualization in Python">Chapter 2</a>, <span class="emphasis"><em>Exploratory Data Analysis and Visualization in Python</em></span>) to place the closing prices for each week as columns and individual stocks as rows using the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; df_pivot = df.pivot('stock','date','close').reset_index()</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; df_pivot.head()</strong></span>
</pre></div><div class="mediaobject"><img src="images/B04881_03_11.jpg" alt="Correlation similarity metrics and time series"/></div><p>As we can see, we only need columns after 2 to calculate correlations between rows, as the first two columns are the index and stock ticker symbol. Let us now calculate the correlation between these time series of stock prices by selecting the second column to end <a id="id159" class="indexterm"/>columns <a id="id160" class="indexterm"/>of the data frame for each row, calculating the pairwise correlations distance <a id="id161" class="indexterm"/>metric, and <a id="id162" class="indexterm"/>visualizing it using MDS, as before:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; import numpy as np</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; correlations = np.corrcoef(np.float64(np.array(df_pivot)[:,2:]))</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; mds_coords = MDS().fit_transform(correlations)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; pd.DataFrame(mds_coords).plot(kind='scatter',x=1,y=0)</strong></span>
</pre></div><div class="mediaobject"><img src="images/B04881_03_12.jpg" alt="Correlation similarity metrics and time series"/></div><p>It is important to note that the Pearson coefficient, which we have calculated here, is a measure of linear correlation between these time series. In other words, it captures the linear increase (or decrease) of the trend in one price relative to another, but won't necessarily capture nonlinear trends (such as a parabola or sigmoidal pattern). We can see this by looking at the formula for the Pearson correlation, which is given by:</p><div class="mediaobject"><img src="images/B04881_03_17.jpg" alt="Correlation similarity metrics and time series"/></div><p>Here μ and σ represent the mean and standard deviation of series <span class="emphasis"><em>a</em></span> and <span class="emphasis"><em>b</em></span>. This value varies from 1 (highly correlated) to -1 (inversely correlated), with 0 representing no correlation (such as a spherical cloud of points). You might recognize the numerator of this equation as the <a id="id163" class="indexterm"/>
<span class="strong"><strong>covariance</strong></span>, which is a measure of how much two datasets, <span class="emphasis"><em>a</em></span> and <span class="emphasis"><em>b</em></span>, vary in synch with one another. You can understand this by considering that the numerator is maximized when corresponding points in both datasets are above or below their mean value. However, whether this accurately captures the similarity in the data depends upon the scale. In data that is distributed in regular intervals between a maximum and<a id="id164" class="indexterm"/> minimum, with<a id="id165" class="indexterm"/> roughly the same difference between consecutive values it <a id="id166" class="indexterm"/>captures this <a id="id167" class="indexterm"/>pattern well. However, consider a case in which the data is exponentially distributed, with orders of magnitude differences between the minimum and maximum, and the difference between consecutive datapoints also varying widely. Here, the Pearson correlation would be numerically dominated by only the largest values in the series, which might or might not represent the overall similarity in the data. This numerical sensitivity also occurs in the denominator, which represents the product of the standard deviations of both datasets. The value of the correlation is maximized when the variation in the two datasets is roughly explained by the product of their individual variations; there is no left-over variation between the datasets that is not explained by their respective standard deviations. By extracting data for the first two stocks in this collection and plotting their pairwise values, we see that this assumption of linearity appears to be a valid one for comparing datapoints:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; df_pivot.iloc[0:2].transpose().iloc[2:].plot(kind='scatter',x=0,y=1)</strong></span>
</pre></div><div class="mediaobject"><img src="images/B04881_03_13.jpg" alt="Correlation similarity metrics and time series"/></div><p>In addition to verifying that these stocks have a roughly linear correlation, this command introduces some new functions in pandas you may find useful. The first is <code class="literal">iloc</code>, which allows you to select indexed rows from a dataframe. The second is <code class="literal">transpose</code>, which inverts the rows and columns. Here, we select the first two rows, transpose, and then select all rows (prices) after the second (since the first is the index and the second is the <span class="emphasis"><em>Ticker</em></span> symbol).</p><p>Despite the <a id="id168" class="indexterm"/>trend we <a id="id169" class="indexterm"/>see in this <a id="id170" class="indexterm"/>example, we could imagine there might a nonlinear trend between prices. In <a id="id171" class="indexterm"/>these cases, it might be better to measure not the linear correlation of the prices themselves, but whether the high prices for one stock coincide with another. In other words, the rank of market days by price should be the same, even if the prices are nonlinearly related. We can also calculate this rank correlation, also known as the Spearman's Rho, using SciPy, with the following formula:</p><div class="mediaobject"><img src="images/B04881_03_20.jpg" alt="Correlation similarity metrics and time series"/></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note03"/>Note</h3><p>Note that this formula assumes that the ranks are distinct (no ties); in the case of ties, we can instead calculate the Pearson correlation using the ranks of the datasets instead of their raw values.</p></div></div><p>Where <span class="emphasis"><em>n</em></span> is the number of datapoints in each of two sets <span class="emphasis"><em>a</em></span> and <span class="emphasis"><em>b</em></span>, and <span class="emphasis"><em>d</em></span> is the difference in ranks between each pair of datapoints <span class="emphasis"><em>ai</em></span> and <span class="emphasis"><em>bi</em></span>. Because we only compare the ranks of the data, not their actual values, this measure can capture variations between two datasets, even if their numerical value vary over wide ranges. Let us see if plotting the results using the Spearman correlation metric generates any differences in the pairwise distance of the stocks computed from MDS, using the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; import scipy.stats</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; correlations2 = scipy.stats.spearmanr(np.float64(np.array(df_pivot)[:,1:]))</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; mds_coords = MDS().fit_transform(correlations2.correlation)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; pd.DataFrame(mds_coords).plot(kind='scatter',x=1,y=0)</strong></span>
</pre></div><div class="mediaobject"><img src="images/B04881_03_14.jpg" alt="Correlation similarity metrics and time series"/></div><p>The Spearman <a id="id172" class="indexterm"/>correlation <a id="id173" class="indexterm"/>distances, based on the <span class="emphasis"><em>x</em></span> and <span class="emphasis"><em>y</em></span> axes, appear closer to each other than <a id="id174" class="indexterm"/>the Pearson <a id="id175" class="indexterm"/>distances, suggesting from the perspective of rank correlation that the time series are more similar.</p><p>Though they differ in their assumptions about how two datasets are distributed numerically, Pearson and Spearman correlations share the requirement that the two sets are of the same length. This is usually a reasonable assumption, and will be true of most of the examples we consider in this book. However, for cases where we wish to compare time series of unequal lengths, we can use <a id="id176" class="indexterm"/>
<span class="strong"><strong>Dynamic Time Warping</strong></span> (<span class="strong"><strong>DTW</strong></span>). Conceptually, the idea of DTW is to <span class="emphasis"><em>warp</em></span> one time series to align with a second, by allowing us to open gaps in either dataset so that it becomes the same size as the second. What the algorithm needs to resolve is where the most similar points of the two series are, so that gaps can be places in the appropriate locations. In the simplest implementation, DTW consists of the following steps (see the following diagram):</p><div class="mediaobject"><img src="images/B04881_03_15.jpg" alt="Correlation similarity metrics and time series"/></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">For dataset <span class="emphasis"><em>a</em></span> of length <span class="emphasis"><em>n</em></span> and a dataset <span class="emphasis"><em>b</em></span> of length <span class="emphasis"><em>m</em></span>, construct a matrix of size <span class="emphasis"><em>n</em></span> by <span class="emphasis"><em>m</em></span>.</li><li class="listitem">Set the top row and the leftmost column of this matrix to both be infinity (left, in figure above).</li><li class="listitem">For each point <span class="emphasis"><em>i</em></span> in set <span class="emphasis"><em>a</em></span>, and point <span class="emphasis"><em>j</em></span> in set <span class="emphasis"><em>b</em></span>, compare their similarity using a cost function. To this cost function, add the minimum of the element (<span class="emphasis"><em>i-1, j-1</em></span>), (<span class="emphasis"><em>i-1, j</em></span>), and (<span class="emphasis"><em>j-1, i</em></span>)—that is, from moving up and left, left, or up in the matrix). These conceptually represent the costs of opening a gap in one of the series, versus aligning the same element in both (middle, above).</li><li class="listitem">At the end of step 2, we will have traced the minimum cost path to align the two series, and the DTW distance will be represented by the bottommost corner of the matrix, (<span class="emphasis"><em>n.m</em></span>) (right, above).</li></ol></div><p>A negative<a id="id177" class="indexterm"/> aspect <a id="id178" class="indexterm"/>of this <a id="id179" class="indexterm"/>algorithm<a id="id180" class="indexterm"/> is that step 3 involves computing a value for every element of series <span class="emphasis"><em>a</em></span> and <span class="emphasis"><em>b</em></span>. For large time series or large datasets, this can be computationally prohibitive. While a full discussion of algorithmic improvements is beyond the scope of our present examples, we refer interested readers to FastDTW (which we will use in our example) and SparseDTW as examples of improvements that can be evaluated using many fewer calculations (Al-Naymat, Ghazi, Sanjay Chawla, and Javid Taheri. <span class="emphasis"><em>Sparsedtw: A novel approach to speed up dynamic time warping</em></span>. Proceedings of the Eighth Australasian Data Mining Conference-Volume 101. Australian Computer Society, Inc., 2009. Salvador, Stan, and Philip Chan. <span class="emphasis"><em>Toward accurate dynamic time warping in linear time and space</em></span>. Intelligent Data Analysis 11.5 (2007): 561-580.).</p><p>We can use the FastDTW algorithm to compare the stocks data and plot the results again using MDS. First we will compare pairwise each pair of stocks and record their DTW distance in a matrix:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; from fastdtw import fastdtw</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; dtw_matrix = np.zeros(shape=(df_pivot.shape[0],df_pivot.shape[0]))</strong></span>
<span class="strong"><strong>…  for i in np.arange(0,df_pivot.shape[0]):</strong></span>
<span class="strong"><strong>…     for j in np.arange(i+1,df_pivot.shape[0]):</strong></span>
<span class="strong"><strong>       …      dtw_matrix[i,j] = fastdtw(df_pivot.iloc[i,2:],df_pivot.iloc[j,2:])[0]</strong></span>
</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note04"/>Note</h3><p>This function is found in the fastdtw library, which you can install using pip or <code class="literal">easy_install</code>.</p></div></div><p>For computational efficiency (because the distance between <code class="literal">i</code> and <code class="literal">j</code> equals the distance between stocks <code class="literal">j</code> and <code class="literal">i</code>), we calculate only the upper triangle of this matrix. We then <a id="id181" class="indexterm"/>add the <a id="id182" class="indexterm"/>transpose (for example, the lower triangle) to this result to get the full <a id="id183" class="indexterm"/>distance <a id="id184" class="indexterm"/>matrix.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; dtw_matrix+=dtw_matrix.transpose()</strong></span>
</pre></div><p>Finally, we can use MDS again to plot the results:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; mds_coords = MDS().fit_transform(dtw_matrix)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; pd.DataFrame(mds_coords).plot(kind='scatter',x=1,y=0) </strong></span>
</pre></div><div class="mediaobject"><img src="images/B04881_03_16.jpg" alt="Correlation similarity metrics and time series"/></div><p>Compared to the distribution of coordinates along the <span class="emphasis"><em>x</em></span> and <span class="emphasis"><em>y</em></span> axis for Pearson correlation and rank correlation, the DTW distances appear to span a wider range, picking up more nuanced differences between the time series of stock prices.</p><p>Now that we have looked at numerical and time series data, as a last example let us examine calculating similarity measurements for categorical datasets.</p></div><div class="section" title="Similarity metrics for categorical data"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec36"/>Similarity metrics for categorical data</h2></div></div></div><p>Text represents one <a id="id185" class="indexterm"/>class of <a id="id186" class="indexterm"/>categorical data: for instance, we might have use a vector to represent the presence or absence of a given keyword for a set of papers submitted to an academic conference, as in our example dataset (Moran, Kelly H., Byron C. Wallace, and Carla E. Brodley. <span class="emphasis"><em>Discovering Better AAAI Keywords via Clustering with Community-sourced Constraints</em></span>. Twenty-Eighth AAAI Conference on Artificial Intelligence. 2014.). If we open the data, we see that the keywords are represented as a string in one column, which we will need to convert into a binary vector:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; df2 = pd.read_csv("Papers.csv",sep=",")</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; df2.head()</strong></span>
</pre></div><div class="mediaobject"><img src="images/B04881_03_18.jpg" alt="Similarity metrics for categorical data"/></div><p>While in <a class="link" href="ch06.html" title="Chapter 6. Words and Pixels – Working with Unstructured Data">Chapter 6</a>, <span class="emphasis"><em>Words and Pixels – Working with Unstructured Data</em></span>, we will examine special functions to do this conversion from text to vector, for illustrative purposes we will now code the solution ourselves. We need to gather all the unique keywords, and assign a unique index to each of them to generate a new column name <code class="literal">'keword_n</code>' for each keyword:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; keywords_mapping = {}</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; keyword_index = 0</strong></span>

<span class="strong"><strong>&gt;&gt;&gt; for k in df2.keywords:</strong></span>
<span class="strong"><strong>…    k = k.split('\n')</strong></span>
<span class="strong"><strong>…    for kw in k:</strong></span>
<span class="strong"><strong>…        if keywords_mapping.get(kw,None) is None:</strong></span>
<span class="strong"><strong>…           keywords_mapping[kw]='keyword_'+str(keyword_index)</strong></span>
<span class="strong"><strong>…           keyword_index+=1</strong></span>
</pre></div><p>We then generate <a id="id187" class="indexterm"/>a new set of columns using this keyword to column name mapping, to set a 1 in <a id="id188" class="indexterm"/>each row where the keyword appears in that article's keywords:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt;for (k,v) in keywords_mapping.items():</strong></span>
<span class="strong"><strong>…        df2[v] = df2.keywords.map( lambda x: 1 if k in x.split('\n') else 0 ) Image_B04881_03_18.png</strong></span>
</pre></div><p>These columns will be appended to the right of the existing columns and we can select out these binary indicators using the <code class="literal">iloc</code> command, as before:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; df2.head().iloc[:,6:]</strong></span>
</pre></div><div class="mediaobject"><img src="images/B04881_03_19.jpg" alt="Similarity metrics for categorical data"/></div><p>In this case, a Euclidean distance between articles could be computed, but because each coordinate is either 0 or 1, it does not provide the continuous distribution of distances we would like (we will get many ties, since there are only so many ways to add and subtract 1 and 0). Similarly, measurements of correlation between these binary vectors are less than ideal because the values can only be identical or non-identical, again leading to many duplicate correlation values.</p><p>What kinds of similarity metric could we use instead? One is the Jaccard coefficient:</p><div class="mediaobject"><img src="images/B04881_03_32.jpg" alt="Similarity metrics for categorical data"/></div><p>This is the number of intersecting items (positions where both <span class="emphasis"><em>a</em></span> and <span class="emphasis"><em>b</em></span> are set to <span class="emphasis"><em>1</em></span> in our example) divided by the union (the total number of positions where either <span class="emphasis"><em>a</em></span> or <span class="emphasis"><em>b</em></span> are set to 1).This measure could be biased, however, if the articles have very different numbers of keywords, as a larger set of words will have a greater probability of being similar to another article. If we are concerned about such bias, we could use the cosine similarity, which measure the angle between vectors and is sensitive to the number of <a id="id189" class="indexterm"/>elements in each:</p><div class="mediaobject"><img src="images/B04881_03_34.jpg" alt="Similarity metrics for categorical data"/></div><p>Where:</p><div class="mediaobject"><img src="images/B04881_03_35.jpg" alt="Similarity metrics for categorical data"/></div><p>We could also use<a id="id190" class="indexterm"/> the Hamming distance (Hamming, Richard W. <span class="emphasis"><em>Error detecting and error correcting codes</em></span>. Bell System technical journal 29.2 (1950): 147-160.), which simply sums whether the elements of two sets are identical or not:</p><div class="mediaobject"><img src="images/B04881_03_37.jpg" alt="Similarity metrics for categorical data"/></div><p>Clearly, this measure will be best if we are primarily looking for matches and mismatches. It is also, like the Jaccard coefficient, sensitive to the number of items in each set, as simply increasing the number of elements increases the possible upper bound of the distance. Similar to Hamming is the Manhattan distance, which does not require the data to be binary:</p><div class="mediaobject"><img src="images/B04881_03_38.jpg" alt="Similarity metrics for categorical data"/></div><p>If we use the Manhattan distance as an example, we can use MDS again to plot the arrangement of the documents in keyword space using the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; distances = pairwise.pairwise_distances(np.float64(np.array(df2)[:,6:]),metric='manhattan')</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; mds_coords = MDS().fit_transform(distances)</strong></span>
<span class="strong"><strong>pd.DataFrame(mds_coords).plot(kind='scatter',x=1,y=0)</strong></span>
</pre></div><div class="mediaobject"><img src="images/B04881_03_23.jpg" alt="Similarity metrics for categorical data"/></div><p>We see a number <a id="id191" class="indexterm"/>of groups <a id="id192" class="indexterm"/>of papers, suggesting that a simple comparison of common keywords provides a way to distinguish between articles.</p><p>The diagram below provides a summary of the different distance methods we have discussed and the decision process for choosing one over another for a particular problem. While it is not exhaustive, we hope it provides a starting point for your own clustering applications.</p><div class="mediaobject"><img src="images/B04881_03_24.jpg" alt="Similarity metrics for categorical data"/></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip06"/>Tip</h3><p><span class="strong"><strong>Aside: Normalizing categorical data</strong></span></p><p>As you may have <a id="id193" class="indexterm"/>noted, we don't normalize categorical data in the same way that we used the <code class="literal">scale()</code> function for numerical data. The reason for this is twofold. First, with categorical data we are usually dealing with data in the range <code class="literal">[0,1]</code>, so the problem of one column of the dataset containing wildly larger values that overwhelm the distance metric is minimized. Secondly, the notion of the scale() function is that the data in the column is biased, and we are removing that bias by subtracting the mean. For categorical data, the 'mean' has a less clear interpretation, as the data can only take the value of 0 or 1, while the mean might be somewhere between the two (for example, 0.25). Subtracting this value doesn't make sense as it would make the data elements no longer binary indicators.</p><p><span class="strong"><strong>Aside: Blending distance metrics</strong></span></p><p>In the examples <a id="id194" class="indexterm"/>considered so far in this chapter, we have dealt with data that may be described as either numerical, time series, or categorical. However, we might easily find examples where this is not true. For instance, we could have a dataset of stock values over time that also contains categorical information about which industry the stock belongs to and numerical information about the size and revenue of the company. In this case, it would be difficult to choose a single distance metric that adequately handles all of these features. Instead, we could calculate a different distance metric for each of the three sets of features (time-series, categorical, and numerical) and blend them (by taking their average, sum, or product, for instance). Since these distances might cover very different numerical scales, we might need to normalize them, for instance using the <code class="literal">scale()</code> function discussed above to convert each distance metric into a distribution with mean 0, standard deviation 1 before combining them.</p></div></div><p>Now that we <a id="id195" class="indexterm"/>have some <a id="id196" class="indexterm"/>ways to compare the similarity of items in a dataset, let us start implementing some clustering pipelines.</p></div><div class="section" title="K-means clustering"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec37"/>K-means clustering</h2></div></div></div><p>K-means clustering<a id="id197" class="indexterm"/> is the classical divisive clustering algorithm. The idea is relatively simple: the <span class="strong"><strong>k</strong></span> in the title comes from the number of clusters we wish to identify, which we need to decide before running the algorithm, while <span class="strong"><strong>means</strong></span> denotes the fact that the algorithm attempts to assign datapoints to clusters where they are closest to the average value of the cluster. Thus a given datapoint chooses among k different means in order to be assigned to the most appropriate cluster. The basic steps of the simplest version of this algorithm are (MacKay, David. <span class="emphasis"><em>An example inference task: clustering</em></span>. Information theory, inference and learning algorithms (2003): 284-292):</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Choose a desired number of groups <span class="emphasis"><em>k</em></span>.<p>Assign k cluster centers; these could simply be k random points from the dataset, which is known as the Forgy method ( Hamerly, Greg, and Charles Elkan. <span class="emphasis"><em>Alternatives to the k-means algorithm that find better clusterings</em></span>. Proceedings of the eleventh international conference on Information and knowledge management. ACM, 2002.). Alternatively, we could assign a random cluster to each datapoint, and compute the average of the datapoints assigned to the same cluster as the k centers, a method called Random Partitioning (Hamerly, Greg, and Charles Elkan. <span class="emphasis"><em>Alternatives to the k-means algorithm that find better clusterings</em></span>. Proceedings of the eleventh international conference on Information and knowledge management. ACM, 2002). More sophisticated methods are also possible, as we will see shortly.</p></li><li class="listitem">Assign any remaining datapoints to the nearest cluster, based on some similarity measure (such as the squared Euclidean distance).</li><li class="listitem">Recalculate the center of each of the <span class="emphasis"><em>k</em></span> groups by averaging the points assigned to them. Note that at this point the center may no longer represent the location of a single datapoint, but is the weighted center of mass of all points in the group.</li><li class="listitem">Repeat 3 and 4 until no points change cluster assignment or the maximum number <a id="id198" class="indexterm"/>of iterations is reached.<div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip07"/>Tip</h3><p><span class="strong"><strong>K-means ++</strong></span></p><p>In the initialization of the algorithm in step 2 above, there are two potential problems. If we <a id="id199" class="indexterm"/>simple choose random points as cluster centers, they may not be optimally distributed within the data (particularly if the size of the clusters is unequal). The k points may not actually end up in the k clusters in the data (for example, multiple random points may reside within the largest cluster in the dataset, as in the figure below, top middle panel), which means the algorithm may not converge to the 'correct' solution or may take a long time to do so. Similarly, the Random Partitioning method will tend to place all the centers near the greatest mass of datapoints (see figure below, top right panel), as any random set of points will be dominated by the largest cluster. To improve the initial choice of parameters, we can use instead the k++ initialization proposed in 2007 (Arthur, David, and Sergei Vassilvitskii. "k-means++: The advantages of careful seeding." Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms. Society for Industrial and Applied Mathematics, 2007.). In this algorithm, we choose an initial datapoint at random to be the center of the first cluster. We then calculate the squared distance from every other datapoint to the chosen datapoint, and choose the next cluster center with probability proportional to this distance. Subsequently, we choose the remaining clusters by calculating this squared distance to the previously selected centers for a given datapoint. Thus, this initialization will choose points with higher probability that are far from any previously chosen point, and spread the initial centers more evenly in the space. This algorithm is the default used in scikit-learn.</p></div></div></li></ol></div><div class="mediaobject"><img src="images/B04881_03_25.jpg" alt="K-means clustering"/><div class="caption"><p>Kmeans++ clustering. (Top, left): Example data with three clusters of unequal size. (Top, middle). Random choice of cluster centers is biased towards points in the largest underlying cluster. (Top, right): Random partitioning results in center of mass for all three random clusters near the bottom of the plot. (Bottom panels). Kmeans++ results in sequential selection of three cluster centers that are evenly spaced across the dataset.</p></div></div><p>Let's think for a<a id="id200" class="indexterm"/> moment about why this works; even if we start with random group centers, once we assign points to these groups the centers are pulled towards the average position of observations in our dataset. The updated center is nearer to this average value. After many iterations, the center of each group will be dominated by the average position of datapoints near the randomly chosen starting point. If the center was poorly chosen to begin with, it will be dragged towards this average, while datapoints that were perhaps incorrectly assigned to this group will gradually be reassigned. During this process, the overall value that is minimized is typically the sum of squared errors (when we are using the Euclidean distance metric), given by:</p><div class="mediaobject"><img src="images/B04881_03_44.jpg" alt="K-means clustering"/></div><p>Where D is the Euclidean distance and c is the cluster center for the cluster to which a point is assigned. This value is also sometimes referred to as the inertia. If we think about this for a moment, we can see that this has the effect that the algorithm works best for data that is composed of circles (or spheres in higher dimensions); the overall SSE for a cluster is minimized when points are uniformly distant from it in a spherical cloud. In contrast, a non-uniform shape (such as an ellipse) will tend to have higher SSE values, and the algorithm will be optimized by splitting the data into two clusters, even if visually we can see that they appear to be represented well by one. This fact reinforces why normalization is often beneficial (as the 0 mean, 1 standard deviation normalization attempts to approximate the shape of a normal distribution for all dimensions, thus forming circles of spheres of data), and the important role of data visualization in addition to numerical statistics in judging the quality of clusters.</p><p>It is also important to consider the implications of this minimization criteria for step 3. The SSE is equivalent to the summed squared Euclidean distance between cluster points and their centroid. Thus, using the squared Euclidean distance as the metric for comparison, we guarantee that the cluster assignment is also optimizing the minimization criteria. We could use other distance metrics, but then this will not be guaranteed. If we <a id="id201" class="indexterm"/>are using Manhattan or Hamming distance, we could instead make our minimization criteria the sum of distances to the cluster center, which we term the k-median, since the value that optimizes this statistic is the cluster median (Jain, Anil K., and Richard C. Dubes. Algorithms for clustering data. Prentice-Hall, Inc., 1988.). Alternatively, we could use an arbitrary distance metric with an algorithm such as k-medoids (see below).</p><p>Clearly, this method will be sensitive to our initial choice of group centers, so we will usually run the algorithm many times and use the best result.</p><p>Let's look at an example: type the following commands in the notebook to read in a sample dataset.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; df = pd.read_csv('kmeans.txt',sep='\t')</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; df.plot(kind='scatter',x='x_coord',y='y_coord')</strong></span>
</pre></div><p> </p><div class="mediaobject"><img src="images/B04881_03_26.jpg" alt="K-means clustering"/></div><p>By visual inspection, this dataset clearly has a number of clusters in it. Let's try clustering with <span class="emphasis"><em>k=5</em></span>.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; from sklearn.cluster import KMeans</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; kmeans_clusters = KMeans(5).fit_predict(X=np.array(df)[:,1:])</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; df.plot(kind='scatter', x='x_coord', y='y_coord', c=kmeans_clusters)</strong></span>
</pre></div><p> </p><div class="mediaobject"><img src="images/B04881_03_27.jpg" alt="K-means clustering"/></div><p>You will notice<a id="id202" class="indexterm"/> that we use the slice operators '<code class="literal">[]</code>' to index a <code class="literal">numpy</code> array that we create from the input dataframe, and select all rows and the columns after the first (the first contains the label, so we don't need it as it isn't part of the data being used for clustering). We call the KMeans model using a pattern that will become familiar for many algorithms in scikit-learn and PySpark: we create model object (KMeans) with parameters (here, 5, which is the number of clusters), and call 'fit_predict' to both calibrate the model parameters and apply the model to the input data. Here, applying the model generates cluster centers, while in regression or classification models that we will discuss in <a class="link" href="ch04.html" title="Chapter 4. Connecting the Dots with Models – Regression Methods">Chapters 4</a>, <span class="emphasis"><em>Connecting the Dots with Models – Regression Methods</em></span> and <a class="link" href="ch05.html" title="Chapter 5. Putting Data in its Place – Classification Methods and Analysis">Chapter 5</a>, <span class="emphasis"><em>Putting Data in its Place – Classification Methods and Analysis</em></span>, 'predict' will yield an estimated continuous response or class label, respectively, for each data point. We could also simply call the <code class="literal">fit</code> method for KMeans, which would simply return an object describing the cluster centers and the statistics resulting from fitting the model, such as the inertia measure we describe above.</p><p>Is this a good number of clusters to fit to the data or not? We can explore this question if we cluster using several values of <code class="literal">k</code> and plot the inertia at each. In Python we can use the following commands.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; inertias = []</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; ks = np.arange(1,20)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; for k in ks:</strong></span>
<span class="strong"><strong> …      inertias.append(KMeans(k).fit(X=np.array(df)[:,1:]).inertia_)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; results = pd.DataFrame({"num_clusters": ks, "sum_distance": inertias})</strong></span>
</pre></div><p>Recall that the<a id="id203" class="indexterm"/> inertia is defined as the sum of squared distance points in a cluster to the center of the cluster to which they are assigned, which is the objective we are trying to optimize in k-means. By visualizing this inertia value at each cluster number <span class="emphasis"><em>k</em></span>, we can get a feeling for the number of clusters that best fits the data:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; results.plot(kind='scatter', x='num_clusters', y='sum_distance')</strong></span>
</pre></div><div class="mediaobject"><img src="images/B04881_03_28.jpg" alt="K-means clustering"/></div><p>We notice there is an <span class="emphasis"><em>elbow</em></span> around the five-cluster mark, which fortunately was the value we initially selected. This elbow indicates that after five clusters we do not significantly decrease the inertia as we add more clusters, suggesting that at k=5 we have captured the important group structure in the data.</p><p>This exercise also illustrates some problems: as you can see from the plot, some of our clusters might be formed from what appear to be overlapping segments, forming a cross shape. Is this a single cluster or two mixed clusters? Unfortunately, without a specification in our cluster model of what shape the clusters should conform to, the results are driven entirely by distance metrics, not pattern  which you might notice yourself visually. This <a id="id204" class="indexterm"/>underscores the importance of visualizing the results and examining them with domain experts to judge whether the obtained clusters makes sense. In the absence of a domain expert, we could also see whether the obtained clusters contain all points labeled with a known assignment—if a high percentage of the clusters are enriched for a single label, this indicates the clusters are of good conceptual quality as well as minimizing our distance metric.</p><p>We could also try using a method that will automatically calculate the best number of clusters for a dataset.</p></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Affinity propagation – automatically choosing cluster numbers"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec18"/>Affinity propagation – automatically choosing cluster numbers</h1></div></div></div><p>One of the <a id="id205" class="indexterm"/>weaknesses of the k-means algorithm is that we need to define upfront the number of clusters we expect to find in the data. When we are not sure what an appropriate choice is, we may need to run many iterations to find a reasonable value. In contrast, the Affinity Propagation algorithm (Frey, Brendan J., and Delbert Dueck. <span class="emphasis"><em>Clustering by passing messages between data points</em></span>. science 315.5814 (2007): 972-976.) finds the number of clusters automatically from a dataset. The algorithm takes a similarity matrix as input (S) (which might be the inverse Euclidean distance, for example – thus, closer points have larger values in S), and performs the following steps after initializing a matrix of <span class="emphasis"><em>responsibilit</em></span>y and <span class="emphasis"><em>availability</em></span> values with all zeroes. It calculates the <span class="emphasis"><em>responsibility</em></span> for one datapoint <span class="emphasis"><em>k</em></span> to be the cluster center for another datapoint <span class="emphasis"><em>i</em></span>. This is represented numerically by the similarity between the two datapoints. Since all availabilities begin at zero, in the first round we simply subtract the highest similarity to any other point (k') for <span class="emphasis"><em>i</em></span>. Thus, a high responsibility score occurs when point k is much more similar to <span class="emphasis"><em>i</em></span> than any other point.</p><div class="mediaobject"><img src="images/B04881_03_52.jpg" alt="Affinity propagation – automatically choosing cluster numbers"/></div><p>Where <span class="emphasis"><em>i</em></span> is the point for which we are trying to find the cluster center, <span class="emphasis"><em>k</em></span> is a potential cluster center to which point <span class="emphasis"><em>i</em></span> might be assigned, s is their similarity, and a is the 'availability' described below. In the next step, the algorithm calculates the availability of the datapoint <span class="emphasis"><em>k</em></span> as a cluster center for point <span class="emphasis"><em>i</em></span>, which represents how appropriate k is <a id="id206" class="indexterm"/>as a cluster center for <span class="emphasis"><em>i</em></span> by judging if it is the center for other points as well. Points which are chosen with high responsibility by many other points have high availability, as per the formula:</p><div class="mediaobject"><img src="images/B04881_03_53.jpg" alt="Affinity propagation – automatically choosing cluster numbers"/></div><p>Where <span class="emphasis"><em>r</em></span> is the responsibility given above. If <span class="emphasis"><em>i=k</em></span>, then this formula is:</p><div class="mediaobject"><img src="images/B04881_03_54.jpg" alt="Affinity propagation – automatically choosing cluster numbers"/></div><p>These steps are sometimes described as <a id="id207" class="indexterm"/>
<span class="strong"><strong>message passing</strong></span>, as they represent information being <span class="emphasis"><em>exchanged</em></span> by the two datapoints about the relative probability of one being a cluster center for another. Looking at steps 1 and 2, you can see that as the algorithm proceeds the responsibility will drop for many of the datapoints to a negative number (as we subtract not only the highest similarity of other datapoints, but also the availability score of these points, leaving only a few positives that will determine the cluster centers. At the end of the algorithm (once the responsibilities and availabilities are no longer changing by an appreciable numerical amount), each datapoint points at another as a cluster center, meaning the number of clusters is automatically determined from the data. This method has the advantage that we don't need to know the number of clusters in advance, but does not scale as well as other methods, since in the simplest implantation we need an n-by-n similarity matrix as the input. If we fit this algorithm on our dataset from before, we see that it detects far more clusters than our elbow plot suggests, since running the following commands gives a cluster number of 309.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; affinity_p_clusters = sklearn.cluster.AffinityPropagation().fit_predict(X=np.array(df)[:,1:]) </strong></span>
<span class="strong"><strong>&gt;&gt;&gt; len(np.unique(affinity_p_clusters))</strong></span>
</pre></div><p>However, if we look at a histogram of the number of datapoints in each cluster, using the command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; pd.DataFrame(affinity_p_clusters).plot(kind='hist',bins=np.unique(affinity_p_clusters))</strong></span>
</pre></div><p>We can see that only a few clusters are large, while many points are identified as belonging to their own group:</p><div class="mediaobject"><img src="images/B04881_03_29.jpg" alt="Affinity propagation – automatically choosing cluster numbers"/><div class="caption"><p>Where K-Means Fails: Clustering Concentric Circles</p></div></div><p>So far, our data<a id="id208" class="indexterm"/> has been well clustered using k-means or variants such as affinity propagation. What examples might this algorithm perform poorly on? Let's take one example by loading our second example dataset and plotting it using the commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; df = pd.read_csv("kmeans2.txt",sep="\t")</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; df.plot(x='x_coord',y='y_coord',kind='scatter')</strong></span>
</pre></div><div class="mediaobject"><img src="images/B04881_03_30.jpg" alt="Affinity propagation – automatically choosing cluster numbers"/></div><p>By eye alone, you can clearly see that there are two groups: two circles nested one within the other. However, if we try to run k-means clustering on this data, we get an unsatisfactory result, as you can see from running the following commands and plotting the result:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; kmeans_clusters = KMeans(2).fit_predict(X=np.array(df)[:,1:])</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; df.plot(kind='scatter', x='x_coord', y='y_coord', c=kmeans_clusters)</strong></span>
</pre></div><div class="mediaobject"><img src="images/B04881_03_31.jpg" alt="Affinity propagation – automatically choosing cluster numbers"/></div><p>In this case, the <a id="id209" class="indexterm"/>algorithm was unable to identify the two natural clusters in the data—because the center ring of data is the same distance to the outer ring at many points, the randomly assigned cluster center (which is more likely to land somewhere in the outer ring) is a mathematically sound choice for the nearest cluster. This example suggests that, in some circumstances, we may need to change our strategy and use a conceptually different algorithm. Maybe our objective of squared error (inertia) is incorrect, for example. In this case, we might try k-medoids.</p></div></div>


  <div id="sbo-rt-content"><div class="section" title="k-medoids"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec19"/>k-medoids</h1></div></div></div><p>As we have described earlier, the k-means (medians) algorithm is best suited to particular distance metrics, the squared Euclidean and Manhattan distance (respectively), since<a id="id210" class="indexterm"/> these distance metrics are equivalent to the optimal value for the statistic (such as total squared distance or total distance) that these algorithms are attempting to minimize. In cases where we might have other distance metrics (such as correlations), we might also use the k-medoid method (Theodoridis, Sergios, and Konstantinos Koutroumbas. <span class="emphasis"><em>Pattern recognition.</em></span> (2003).), which consists of the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Select <span class="emphasis"><em>k</em></span> initial points as the initial cluster centers.</li><li class="listitem">Calculate the nearest cluster center for each datapoint by any distance metric and assign it to that cluster.</li><li class="listitem">For each point and each cluster center, swap the cluster center with the point and calculate the reduction in overall distances to the cluster center across all cluster members using this swap. If it doesn't improve, undo it. Iterate step 3 for all points.</li></ol></div><p>This is obviously not an exhaustive search (since we don't repeat step 1), but has the advantage that the optimality criterion is not a specific optimization function but rather improving the compactness of the clusters by a flexible distance metric. Can k-medoids improve our clustering of concentric circles? Let's try running using the following commands and plotting the result:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; from pyclust import KMedoids</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; kmedoids_clusters = KMedoids(2).fit_predict(np.array(df)[:,1:])</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; df.plot(kind='scatter', x='x_coord', y='y_coord', c=kmedoids_clusters)</strong></span>
</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note05"/>Note</h3><p>Note that k-medoids is not included in sci-kit learn, so you will need to install the pyclust library using <code class="literal">easy_install</code> or <code class="literal">pip</code>.</p></div></div><div class="mediaobject"><img src="images/B04881_03_33.jpg" alt="k-medoids"/></div><p>There isn't much improvement over k-means, so perhaps we need to change our clustering algorithm entirely. Perhaps instead of generating a similarity between datapoints in a<a id="id211" class="indexterm"/> single stage, we could examine hierarchical measures of similarity and clustering, which is the goal of the agglomerative clustering algorithms we will examine next.</p></div></div>


  <div id="sbo-rt-content"><div class="section" title="Agglomerative clustering"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec20"/>Agglomerative clustering</h1></div></div></div><p>In contrast to <a id="id212" class="indexterm"/>algorithms, such as k-means, where the dataset is partitioned into individual groups, <span class="strong"><strong>agglomerative</strong></span> or <span class="strong"><strong>hierarchical</strong></span> clustering<a id="id213" class="indexterm"/> techniques start by considering each datapoint as its own cluster and merging them together into larger groups from the bottom up (Maimon, Oded, and Lior Rokach, eds. <span class="emphasis"><em>Data mining and knowledge discovery handbook</em></span>. Vol. 2. New York: Springer, 2005). The classical application of this idea is in phylogenetic trees in evolution, where common ancestors connect individual organisms. Indeed, these methods organize the data into tree diagrams, known as <a id="id214" class="indexterm"/>
<span class="strong"><strong>dendrograms</strong></span>, which visualize how the data is sequentially merged into larger groups.</p><p>The basic steps of an agglomerative algorithm are (diagrammed visually in the figure below):</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Start with each point in its own cluster.</li><li class="listitem">Compare each pair of datapoints using a distance metric. This could be any of the methods discussed above.</li><li class="listitem">Use a linkage criterion to merge datapoints (at the first stage) or clusters (in subsequent phases), where linkage is represented by a function such as:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The maximum (also known as complete linkage) distance between two sets of points.The minimum (also known as single linkage) distance between two sets of points.</li><li class="listitem" style="list-style-type: disc">The <a id="id215" class="indexterm"/>average distance between two sets of points, also known as <span class="strong"><strong>Unweighted Pair Group Method with Arithmetic Mean</strong></span> (<span class="strong"><strong>UPGMA</strong></span>). The points in each group could also be weighted to give a weighted average, or WUPGMA.</li><li class="listitem" style="list-style-type: disc">The difference between centroids (centers of mass), or UPGMC.</li><li class="listitem" style="list-style-type: disc">The squared Euclidean distance between two sets of points, or Ward's Method (Ward Jr, Joe H. <span class="emphasis"><em>Hierarchical grouping to optimize an objective function</em></span>. <span class="emphasis"><em>Journal of the American statistical association</em></span> 58.301 (1963): 236-244).</li><li class="listitem" style="list-style-type: disc">Repeat steps 2-3 until there is only a single cluster containing all data points. Note that following the first round, the first stage of clusters become a new point to compare with all other clusters, and at each stage the clusters formed become larger as the algorithm proceeds. Along the way, we will construct a tree diagram as we sequentially merge clusters from the prior steps together.</li></ul></div><div class="mediaobject"><img src="images/B04881_03_40.jpg" alt="Agglomerative clustering"/><div class="caption"><p>Agglomerative clustering: From top to bottom, example of tree construction (right) from a dataset (left) by sequentially merging closest points</p></div></div></li></ol></div><p>Note that we could <a id="id216" class="indexterm"/>also run this process in reverse, taking an initial dataset and splitting it into individual points, which would also construct a tree diagram. In either case, we could then find clusters at several levels of resolution by choosing a cutoff depth of the tree and assigning points to the largest cluster they have been assigned to below that cutoff. This depth is often calculated using the linkage score given in step 3, allowing us conceptually to choose an appropriate distance between groups to consider a cluster (either relatively close or far away as we move up the tree).</p><div class="section" title="Where agglomerative clustering fails"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec38"/>Where agglomerative clustering fails</h2></div></div></div><p>Agglomerative <a id="id217" class="indexterm"/>algorithms have many of the same ingredients as k-means; we choose a number of clusters (which will determine where we cut the tree generated by clustering—in the most extreme case, all points become members of a single cluster) and a similarity metric. We also need to choose a <a id="id218" class="indexterm"/>
<span class="strong"><strong>linkage metric</strong></span> for step 3, which determines the rules for merging individual branches of our tree. Can agglomerative clustering succeed where k-means failed? Trying this approach on the circular data suggests otherwise, as shown by plotting results of the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; from sklearn.cluster import AgglomerativeClustering</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; agglomerative_clusters = AgglomerativeClustering(2,linkage='ward').fit_predict(X=np.array(df)[:,1:])</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; df.plot(kind='scatter', x='x_coord', y='y_coord', c=agglomerative_clusters)</strong></span>
</pre></div><div class="mediaobject"><img src="images/B04881_03_41.jpg" alt="Where agglomerative clustering fails"/></div><p>In order to correctly group the inner and outer circle, we can attempt to modify our notion of similarity using connectivity, a concept taken from graph analysis in which a set of nodes are connected by edges, and connectivity refers to whether two nodes share an edge between them. Here, we essentially construct a graph between pairs of points by thresholding the number of points that can be considered similar to one another, rather than <a id="id219" class="indexterm"/>measuring a continuous distance metric between each pair of points. This potentially reduces our difficulties with the concentric circles data, since if we set a very small value (say 10 nearby points), the uniform distance from the middle to the outer ring is no longer problematic because the central points will always be closer to each other than to the periphery. To construct this connectivity-based similarity, we could take a distance matrix such as those we've already calculated, and threshold it for some value of similarity by which we think points are connected, giving us a binary matrix of 0 and 1. This kind of matrix, representing the presence or absence of an edge between nodes in a graph, is also known as an <span class="strong"><strong>adjacency matrix</strong></span>. We<a id="id220" class="indexterm"/> could choose this value through inspecting the distribution of pairwise similarity scores, or based on prior knowledge. We can then supply this matrix as an argument to our agglomerative clustering routine, providing a neighborhood of points to be considered when comparing datapoints, which gives an initial structure to the clusters. We can see that this makes a huge difference to the algorithms results after running the following commands. Note that when we generate the adjacency matrix L, we may end up with an asymmetric matrix since we threshold the ten most similar points for each member of the data. This could lead to situations where two points are not mutually closest to each other, leading to an edge represented in only the top or bottom triangle of the adjacency matrix. To generate a symmetric input for our clustering algorithm, we take the average of the matrix L added to its transpose, which effectively adds edges in both directions between two points.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; from sklearn.cluster import AgglomerativeClustering</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; from sklearn.neighbors import kneighbors_graph</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; L = kneighbors_graph(np.array(df)[:,1:], n_neighbors=10, include_self=False)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; L = 0.5 * (L + L.T)</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; agglomerative_clusters = AgglomerativeClustering(n_clusters=2,connectivity=L,linkage='average').fit_predict(X=np.array(df)[:,1:])</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; df.plot(kind='scatter', x='x_coord', y='y_coord', c=agglomerative_clusters)</strong></span>
</pre></div><p>Now, as you can see, this algorithm can correctly identify and separate two clusters:</p><p>Interestingly, constructing this neighborhood graph and partitioning it into sub graphs (splitting the whole graph into a set of nodes and edges that are primarily connected to each other, rather than to other elements of the network) is equivalent to performing k-means clustering on a transformed distance matrix, an approach known as Spectral Clustering (Von Luxburg, Ulrike. <span class="emphasis"><em>A tutorial on spectral clustering.</em></span> Statistics and computing 17.4 (2007): 395-416). The transformation here is from taking the Euclidean distance D we calculated earlier and calculating the kernel score—the Gaussian kernel given by:</p><div class="mediaobject"><img src="images/B04881_03_70.jpg" alt="Where agglomerative clustering fails"/></div><p>between each pair <a id="id221" class="indexterm"/>of points i and j, with a bandwidth γ instead of making a hard threshold as when we constructed the neighborhood before. Using the pairwise kernel matrix K calculated from all points i and j, we can then construct the Laplacian matrix of a graph, which is given by:</p><div class="mediaobject"><img src="images/B04881_03_71.jpg" alt="Where agglomerative clustering fails"/></div><p>Here, <span class="emphasis"><em>I</em></span> is the identity matrix (with a one along the diagonal and zero elsewhere), and <span class="emphasis"><em>D</em></span> is the diagonal matrix whose elements are :</p><div class="mediaobject"><img src="images/B04881_03_72.jpg" alt="Where agglomerative clustering fails"/></div><p>Giving the number of neighbors for each point <span class="emphasis"><em>i</em></span>. In essence by calculating L we now represent the dataset as a series of nodes (points) connected by edges (the elements of this matrix), which have been normalized so that the total value of all edges for each node sums to 1. Since the Gaussian kernel score is continuous, in this normalization divides pairwise distances between a given point and all others into a probability distribution where the distances (edges) sum to 1.</p><p>You may recall from linear algebra that Eigenvectors of a matrix A are vectors v for which, if we multiply the matrix by the eigenvector v, we get the same result as if we had multiplied the vector by a constant amount λ: <span class="inlinemediaobject"><img src="images/B04881_03_73.jpg" alt="Where agglomerative clustering fails"/></span>. Thus, the matrix here represents a kind of operation on the vector. For example, the identity matrix gives an eigenvalue of 1, since multiplying v by the identity gives v itself. We could also have a matrix such as:</p><div class="mediaobject"><img src="images/B04881_03_74.jpg" alt="Where agglomerative clustering fails"/></div><p>which doubles the <a id="id222" class="indexterm"/>value of the vector with which it is multiplied, suggesting the matrix acts as a 'stretching' operation on the vector. From this perspective, larger eigenvalues correspond to greater stretching of the vector, while the eigenvector gives the direction in which the stretching occurs. This is useful because it gives us the primary axes along which the matrix operation acts. In our example, if we take the two eigenvectors with the largest eigenvalues (in essence, the directions in which the matrix represents the greatest transformation of a vector), we are extracting the two greatest axes of variation in the matrix. We will return to this concept in more detail when we discuss <span class="emphasis"><em>Principal components</em></span> in <a class="link" href="ch06.html" title="Chapter 6. Words and Pixels – Working with Unstructured Data">Chapter 6</a>, <span class="emphasis"><em>Words and Pixels – Working with Unstructured Data</em></span>, but suffice to say that if we run <code class="literal">run-kmeans</code> on these eigenvectors (an approach known as <a id="id223" class="indexterm"/>
<span class="strong"><strong>spectral clustering</strong></span>, since the eigenvalues of a matrix that we cluster are known as the spectrum of a matrix), we get a result very similar to the previous agglomerative clustering approach using neighborhoods, as we can see from executing the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; spectral_clusters = sklearn.cluster.SpectralClustering(2).fit_predict(np.array(df)[:,1:])</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; df.plot(kind='scatter', x='x_coord', y='y_coord', c=spectral_clusters)</strong></span>
</pre></div><div class="mediaobject"><img src="images/B04881_03_42.jpg" alt="Where agglomerative clustering fails"/></div><p>We can successfully capture this nonlinear separation boundary because we've represented the points in the space of the greatest variation in pairwise distance, which is the difference between the inner and outermost circle in the data:</p><p>The preceding <a id="id224" class="indexterm"/>examples should have given you a number of approaches you can use to tackle clustering problems, and as a rule of thumb guide, the following diagram illustrates the decision process for choosing between them:</p><div class="mediaobject"><img src="images/B04881_03_46.jpg" alt="Where agglomerative clustering fails"/></div><p>For the last part of our exploration of clustering, let's look at an example application utilizing Spark Streaming and k-means, which will allow us to incrementally update our clusters as they are received.</p></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Streaming clustering in Spark"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec21"/>Streaming clustering in Spark</h1></div></div></div><p>Up to this point, we <a id="id225" class="indexterm"/>have mainly <a id="id226" class="indexterm"/>demonstrated examples for ad hoc exploratory analysis. In building up analytical applications, we need to begin putting these into a more robust framework. As an example, we will demonstrate the use of a streaming clustering pipeline using PySpark. This application will potentially scale to very large datasets, and we will compose the pieces of the analysis in such a way that it is robust to failure in the case of malformed data.</p><p>As we will be using similar examples with PySpark in the following chapters, let's review the key ingredients we need in such application, some of which we already saw in <a class="link" href="ch02.html" title="Chapter 2. Exploratory Data Analysis and Visualization in Python">Chapter 2</a>, <span class="emphasis"><em>Exploratory Data Analysis and Visualization in Python</em></span>. Most PySpark jobs we will create in<a id="id227" class="indexterm"/> this book <a id="id228" class="indexterm"/>consist of the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Construct a Spark context. The context contains information about the name of the application, and parameters such as memory and number of tasks.</li><li class="listitem">The Spark context may be used to construct secondary context objects, such as the streaming context we will use in this example. This context object contains parameters specifically about a particular kind of task, such as a streaming dataset, and inherits all the information we have previously initialized in the base Spark context.</li><li class="listitem">Construct a <a id="id229" class="indexterm"/>dataset, which is represented in Spark as a <span class="strong"><strong>Resilient Distributed Dataset</strong></span> (<span class="strong"><strong>RDD</strong></span>). While from a programmatic standpoint we can operate on this RDD just as we do with, for example, a pandas dataframe, under the hood it is potentially parallelized across many machines during analysis. We may parallelize the data after reading it from a source file, or reading from parallel file systems such as Hadoop. Ideally we don't want to fail our whole job if one line of data is erroneous, so we would like to place an error handling mechanism here that will alert us to a failure to parse a row without blocking the whole job.</li><li class="listitem">We frequently need to transform our input dataset into a subclass of RDD known as a <a id="id230" class="indexterm"/><span class="strong"><strong>Labeled RDD</strong></span>. Labeled RDDs contain a label (such as the cluster label for the clustering algorithms we have been studying in this chapter) and a set of features. For our clustering problems, we will only perform this transformation when we predict (as we usually don't know the cluster ahead of time), but for the regression and classification models we will look at in <a class="link" href="ch04.html" title="Chapter 4. Connecting the Dots with Models – Regression Methods">Chapter 4</a>, <span class="emphasis"><em>Connecting the Dots with Models – Regression Methods</em></span>, and <a class="link" href="ch05.html" title="Chapter 5. Putting Data in its Place – Classification Methods and Analysis">Chapter 5</a>, <span class="emphasis"><em>Putting Data in its Place – Classification Methods and Analysis</em></span>, the label is used as a part of fitting the model.</li><li class="listitem">We'll frequently want a way to save the output of our modeling to be used by downstream applications, either on disk or in a database, where we can later query models indexed by history.</li></ol></div><p>Let's look at some of these components using the Python notebook. Assuming we have Spark installed on our system, we'll start by importing the required dependencies:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; from pyspark import SparkContext</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; from pyspark.streaming import StreamingContext</strong></span>
</pre></div><p>We can then test starting the <code class="literal">SparkContext</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; sc = SparkContext( 'local', 'streaming-kmeans')</strong></span>
</pre></div><p>Recall that the first argument gives the URL for our Spark master, the machine that coordinates execution of Spark jobs and distributes tasks to the worker machines in a cluster. In this case, we will run it locally, so give this argument as <code class="literal">localhost</code>, but otherwise this could be the URL of a remote machine in our cluster. The second argument is just <a id="id231" class="indexterm"/>the name <a id="id232" class="indexterm"/>we give to our application. With a context running, we can also generate the streaming context, which contains information about our streaming application, using the following:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; ssc = StreamingContext(sc, 10)</strong></span>
</pre></div><p>The first argument is simply the <code class="literal">SparkContext</code> used as a parent of the <code class="literal">StreamingContext</code>: the second is the frequency in seconds at which we will check our streaming data source for new data. If we expect regularly arriving data we could make this lower, or make it higher if we expect new data to be available less frequently.</p><p>Now that we have a <code class="literal">StreamingContext</code>, we can add data sources. Let's assume for now we'll have two sources for training data (which could be historical). We want the job not to die if we give one line of bad data, and so we use a <code class="literal">Parser</code> class that gives this flexibility:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; class Parser():</strong></span>
<span class="strong"><strong>    def __init__(self,type='train',delimiter=',',num_elements=5, job_uuid=''):</strong></span>
<span class="strong"><strong>        self.type=type</strong></span>
<span class="strong"><strong>        self.delimiter=delimiter</strong></span>
<span class="strong"><strong>        self.num_elements=num_elements</strong></span>
<span class="strong"><strong>        self.job_uuid=job_uuid</strong></span>
<span class="strong"><strong>        </strong></span>
<span class="strong"><strong>    def parse(self,l):</strong></span>
<span class="strong"><strong>        try:</strong></span>
<span class="strong"><strong>            line = l.split(self.delimiter)    </strong></span>
<span class="strong"><strong>            if self.type=='train':</strong></span>
<span class="strong"><strong>                category = float(line[0])</strong></span>
<span class="strong"><strong>                feature_vector = Vectors.dense(line[1:])</strong></span>
<span class="strong"><strong>                return LabeledPoint(category, feature_vector)</strong></span>
<span class="strong"><strong>            elif self.type=='test':</strong></span>
<span class="strong"><strong>                category = -1</strong></span>
<span class="strong"><strong>                feature_vector = Vectors.dense(line)</strong></span>
<span class="strong"><strong>                return LabeledPoint(category, feature_vector)</strong></span>
<span class="strong"><strong>            else:</strong></span>
<span class="strong"><strong>                # log exceptions</strong></span>
<span class="strong"><strong>                f = open('/errors_events/{0}.txt'.format(self.job_uuid),'a')</strong></span>
<span class="strong"><strong>                f.write('Unknown type: {0}'.format(self.type))</strong></span>
<span class="strong"><strong>                f.close()</strong></span>
<span class="strong"><strong>        except:</strong></span>
<span class="strong"><strong>            # log errors</strong></span>
<span class="strong"><strong>            f = open('/error_events/{0}.txt'.format(self.job_uuid),'a')</strong></span>
<span class="strong"><strong>            f.write('Error parsing line: {0}'.format)</strong></span>
<span class="strong"><strong>            f.close()   </strong></span>
</pre></div><p>We log error lines to <a id="id233" class="indexterm"/>a file<a id="id234" class="indexterm"/> with the name of our job ID, which will allow us to locate them later if we need to. We can then use this parser to train and evaluate the model. To train the model, we move files with three columns (a label and the data to be clustered) into the training directory. We can also add to the test data directory files with two columns only the coordinate features:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; num_features = 2</strong></span>
<span class="strong"><strong>num_clusters = 3</strong></span>

<span class="strong"><strong>training_parser = Parser('train',',',num_features+1,job_uuid)</strong></span>
<span class="strong"><strong>test_parser = Parser('test',',',num_features,job_uuid)</strong></span>

<span class="strong"><strong>trainingData = ssc.textFileStream("/training_data").\</strong></span>
<span class="strong"><strong>    map(lambda x: training_parser.parse(x)).map(lambda x: x.features)</strong></span>
<span class="strong"><strong>testData = ssc.textFileStream("/test_data").\</strong></span>
<span class="strong"><strong>    map(lambda x: test_parser.parse(x)).map(lambda x: x.features)</strong></span>
<span class="strong"><strong>streaming_clustering = StreamingKMeans(k=num_clusters, decayFactor=1.0).\</strong></span>
<span class="strong"><strong>    setRandomCenters(num_features,0,0)</strong></span>
<span class="strong"><strong>streaming_clustering.trainOn(trainingData)</strong></span>
<span class="strong"><strong>streaming_clustering.predictOn(testData).\</strong></span>
<span class="strong"><strong>    pprint()</strong></span>
<span class="strong"><strong>ssc.start()  </strong></span>
</pre></div><p>The decay factor in the parameters gives the recipe for combining current cluster centers and old ones. For parameter 1.0, we use an equal weight between old and new, while for <a id="id235" class="indexterm"/>the other<a id="id236" class="indexterm"/> extreme, at 0, we only use the new data. If we stop the model at any point we, can inspect it using the <code class="literal">lastestModel()</code> function:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt;  streaming_clustering.latestModel().clusterCenters</strong></span>
</pre></div><p>We could also predict using the <code class="literal">predict()</code> function on an appropriately sized vector:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt; streaming_clustering.latestModel().predict([ … ])</strong></span>
</pre></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec22"/>Summary</h1></div></div></div><p>In this section, we learned how to identify groups of similar items in a dataset, an exploratory analysis that we might frequently use as a first step in deciphering new datasets. We explored different ways of calculating the similarity between datapoints and described what kinds of data these metrics might best apply to. We examined both divisive clustering algorithms, which split the data into smaller components starting from a single group, and agglomerative methods, where every datapoint starts as its own cluster. Using a number of datasets, we showed examples where these algorithms will perform better or worse, and some ways to optimize them. We also saw our first (small) data pipeline, a clustering application in PySpark using streaming data.</p></div></div>
</body></html>