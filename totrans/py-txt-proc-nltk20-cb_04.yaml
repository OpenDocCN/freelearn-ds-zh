- en: Chapter 4. Part-of-Speech Tagging
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Default tagging
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a unigram part-of-speech tagger
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combining taggers with backoff tagging
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training and combining Ngram taggers
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a model of likely word tags
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tagging with regular expressions
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Affix tagging
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a Brill tagger
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training the TnT tagger
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using WordNet for tagging
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tagging proper names
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifier-based tagging
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Part-of-speech tagging** is the process of converting a sentence, in the
    form of a list of words, into a list of tuples, where each tuple is of the form
    `(word, tag)`. The **tag** is a part-of-speech tag and signifies whether the word
    is a noun, adjective, verb, and so on.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Most of the taggers we will cover are *trainable*. They use a list of tagged
    sentences as their training data, such as what you get from the `tagged_sents()`
    function of a `TaggedCorpusReader` (see the *Creating a part-of-speech tagged
    word corpus* recipe in [Chapter 3](ch03.html "Chapter 3. Creating Custom Corpora"),
    *Creating Custom Corpora* for more details). With these training sentences, the
    tagger generates an internal model that will tell them how to tag a word. Other
    taggers use external data sources or match word patterns to choose a tag for a
    word.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: All taggers in NLTK are in the `nltk.tag` package and inherit from the `TaggerI`
    base class. `TaggerI` requires all subclasses to implement a `tag()` method, which
    takes a list of words as input, and returns a list of tagged words as output.
    `TaggerI` also provides an `evaluate()` method for evaluating the accuracy of
    the tagger (covered at the end of the *Default tagging* recipe). Many taggers
    can also be combined into a backoff chain, so that if one tagger cannot tag a
    word, the next tagger is used, and so on.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Part-of-speech tagging is a necessary step before *chunking*, which is covered
    in [Chapter 5](ch05.html "Chapter 5. Extracting Chunks"), *Extracting Chunks*.
    Without the part-of-speech tags, a chunker cannot know how to extract phrases
    from a sentence. But with part-of-speech tags, you can tell a chunker how to identify
    phrases based on tag patterns.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Default tagging
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Default tagging provides a baseline for part-of-speech tagging. It simply assigns
    the same part-of-speech tag to every token. We do this using the `DefaultTagger`.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are going to use the `treebank` corpus for most of this chapter because it's
    a common standard and is quick to load and test. But everything we do should apply
    equally well to `brown`, `conll2000`, and any other part-of-speech tagged corpus.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `DefaultTagger` takes a single argument—the tag you want to apply. We will
    give it `'NN'`, which is the tag for a singular noun.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Every tagger has a `tag()` method that takes a list of tokens, where each token
    is a single word. This list of tokens is usually a list of words produced by a
    word tokenizer (see [Chapter 1](ch01.html "Chapter 1. Tokenizing Text and WordNet
    Basics"), *Tokenizing Text and WordNet Basics* for more on tokenization). As you
    can see, `tag()` returns a list of tagged tokens, where a **tagged token** is
    a tuple of `(word, tag)`.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`DefaultTagger` is a subclass of `SequentialBackoffTagger`. Every subclass
    of `SequentialBackoffTagger` must implement the `choose_tag()` method, which takes
    three arguments:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: The list of `tokens`.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `index` of the current token whose tag we want to choose.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `history`, which is a list of the previous tags.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`SequentialBackoffTagger` implements the `tag()` method, which calls the `choose_tag()`
    of the subclass for each index in the tokens list, while accumulating a history
    of the previously tagged tokens. This history is the reason for the *Sequential*
    in `SequentialBackoffTagger`. We will get to the *Backoff* portion of the name
    in the *Combining taggers with backoff tagging* recipe. The following is a diagram
    showing the inheritance tree:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/3609OS_04_01.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
- en: The `choose_tag()` method of `DefaultTagger` is very simple—it returns the tag
    we gave it at initialization time. It does not care about the current token or
    the history.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are a lot of different tags you could give to the `DefaultTagger`. You
    can find a complete list of possible tags for the `treebank` corpus at [http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html](http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html).
    These tags are also documented in [Appendix](apa.html "Appendix A. Penn Treebank
    Part-of-Speech Tags"), *Penn Treebank Part-of-Speech Tags*.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating accuracy
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To know how accurate a tagger is, you can use the `evaluate()` method, which
    takes a list of tagged tokens as a gold standard to evaluate the tagger. Using
    our default tagger created earlier, we can evaluate it against a subset of the
    `treebank` corpus tagged sentences.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: So by just choosing `'NN'` for every tag, we can achieve 14% accuracy testing
    on ¼th of the `treebank` corpus. We will be reusing these same `test_sents` for
    evaluating more taggers in upcoming recipes.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Batch tagging sentences
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`TaggerI` also implements a `batch_tag()` method that can be used to tag a
    list of sentences, instead of a single sentence. Here''s an example of tagging
    two simple sentences:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The result is a list of two tagged sentences, and of course every tag is `NN`
    because we are using the `DefaultTagger`. The `batch_tag()` method can be quite
    useful if you have many sentences you wish to tag all at once.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Untagging a tagged sentence
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Tagged sentences can be untagged using `nltk.tag.untag()`. Calling this function
    with a tagged sentence will return a list of words without the tags.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: See also
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For more on tokenization, see [Chapter 1](ch01.html "Chapter 1. Tokenizing Text
    and WordNet Basics"), *Tokenizing Text and WordNet Basics*. And to learn more
    about tagged sentences, see the *Creating a part-of-speech tagged word corpus*
    recipe in [Chapter 3](ch03.html "Chapter 3. Creating Custom Corpora"), *Creating
    Custom Corpora*. For a complete list of part-of-speech tags found in the treebank
    corpus, see [Appendix](apa.html "Appendix A. Penn Treebank Part-of-Speech Tags"),
    *Penn Treebank Part-of-Speech Tags*.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Training a unigram part-of-speech tagger
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **unigram** generally refers to a single token. Therefore, a *unigram tagger*
    only uses a single word as its *context* for determining the part-of-speech tag.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: The `UnigramTagger` inherits from `NgramTagger`, which is a subclass of `ContextTagger`,
    which inherits from `SequentialBackoffTagger`. In other words, the `UnigramTagger`
    is a *context-based tagger* whose context is a single word, or unigram.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`UnigramTagger` can be trained by giving it a list of tagged sentences at initialization.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We use the first 3,000 tagged sentences of the `treebank` corpus as the training
    set to initialize the `UnigramTagger`. Then we see the first sentence as a list
    of words, and can see how it is transformed by the `tag()` function into a list
    of tagged tokens.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `UnigramTagger` builds a *context model* from the list of tagged sentences.
    Because `UnigramTagger` inherits from `ContextTagger`, instead of providing a
    `choose_tag()` method, it must implement a `context()` method, which takes the
    same three arguments as `choose_tag()`. The result of `context()` is, in this
    case, the word token. The context token is used to create the model, and also
    to look up the best tag once the model is created. Here''s an inheritance diagram
    showing each class, starting at `SequentialBackoffTagger`:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/3609OS_04_02.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
- en: Let's see how accurate the `UnigramTagger` is on the test sentences (see the
    previous recipe for how `test_sents` is created).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: It has almost 86% accuracy for a tagger that only uses single word lookup to
    determine the part-of-speech tag. All accuracy gains from here on will be much
    smaller.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The model building is actually implemented in `ContextTagger`. Given the list
    of tagged sentences, it calculates the frequency that a tag has occurred for each
    context. The tag with the highest frequency for a context is stored in the model.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Overriding the context model
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: All taggers that inherit from `ContextTagger` can take a pre-built model instead
    of training their own. This model is simply a Python `dict` mapping a context
    key to a tag. The context keys will depend on what the `ContextTagger` subclass
    returns from its `context()` method. For `UnigramTagger`, context keys are individual
    words. But for other `NgramTagger` subclasses, the context keys will be tuples.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s an example where we pass a very simple model to the `UnigramTagger`
    instead of a training set:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Since the model only contained the context key, `'Pierre'`, only the first word
    got a tag. Every other word got `None` as the tag since the context word was not
    in the model. So unless you know exactly what you are doing, let the tagger train
    its own model instead of passing in your own.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: One good case for passing a self-created model to the `UnigramTagger` is for
    when you have a dictionary of words and tags, and you know that every word should
    always map to its tag. Then, you can put this `UnigramTagger` as your first backoff
    tagger (covered in the next recipe), to look up tags for unambiguous words.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: Minimum frequency cutoff
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `ContextTagger` uses frequency of occurrence to decide which tag is most
    likely for a given context. By default, it will do this even if the context word
    and tag occurs only once. If you would like to set a minimum frequency threshold,
    then you can pass a `cutoff` value to the `UnigramTagger`.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In this case, using `cutoff=3` has decreased accuracy, but there may be times
    when a cutoff is a good idea.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the next recipe, we will cover backoff tagging to combine taggers. And in
    the *Creating a model of likely word tags* recipe, we will learn how to statistically
    determine tags for very common words.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Combining taggers with backoff tagging
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Backoff tagging** is one of the core features of `SequentialBackoffTagger`.
    It allows you to chain taggers together so that if one tagger doesn''t know how
    to tag a word, it can pass the word on to the next backoff tagger. If that one
    can''t do it, it can pass the word on to the next backoff tagger, and so on until
    there are no backoff taggers left to check.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Every subclass of `SequentialBackoffTagger` can take a `backoff` keyword argument
    whose value is another instance of a `SequentialBackoffTagger`. So we will use
    the `DefaultTagger` from the *Default tagging* recipe as the `backoff` to the
    `UnigramTagger` from the *Training a unigram part-of-speech tagger* recipe. Refer
    to both recipes for details on `train_sents` and `test_sents`.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: By using a default tag of `NN` whenever the `UnigramTagger` is unable to tag
    a word, we have increased the accuracy by almost 2%!
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When a `SequentialBackoffTagger` is initialized, it creates an internal list
    of backoff taggers with itself as the first element. If a `backoff` tagger is
    given, then the backoff tagger''s internal list of taggers is appended. Here''s
    some code to illustrate this:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The `_taggers` is the internal list of backoff taggers that the `SequentialBackoffTagger`
    uses when the `tag()` method is called. It goes through its list of taggers, calling
    `choose_tag()` on each one. As soon as a tag is found, it stops and returns that
    tag. This means that if the primary tagger can tag the word, then that's the tag
    that will be returned. But if it returns `None`, then the next tagger is tried,
    and so on until a tag is found, or else `None` is returned. Of course, `None`
    will never be returned if your final backoff tagger is a `DefaultTagger`.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While most of the taggers included in NLTK are subclasses of `SequentialBackoffTagger`,
    not all of them are. There's a few taggers that we will cover in later recipes
    that cannot be used as part of a backoff tagging chain, such as the `BrillTagger`.
    However, these taggers generally take another tagger to use as a baseline, and
    a `SequentialBackoffTagger` is often a good choice for that baseline.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: Pickling and unpickling a trained tagger
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Since training a tagger can take a while, and you generally only need to do
    the training once, pickling a trained tagger is a useful way to save it for later
    usage. If your trained tagger is called `tagger`, then here''s how to dump and
    load it with `pickle`:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: If your tagger pickle file is located in a NLTK data directory, you could also
    use `nltk.data.load('tagger.pickle')` to load the tagger.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the next recipe, we will combine more taggers with backoff tagging. Also
    see the previous two recipes for details on the `DefaultTagger` and `UnigramTagger`.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Training and combining Ngram taggers
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In addition to `UnigramTagger`, there are two more `NgramTagger` subclasses:
    `BigramTagger` and `TrigramTagger` . `BigramTagger` uses the previous tag as part
    of its context, while `TrigramTagger` uses the previous two tags. An **ngram**
    is a subsequence of *n* items, so the `BigramTagger` looks at two items (the previous
    tag and word), and the `TrigramTagger` looks at three items.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: These two taggers are good at handling words whose part-of-speech tag is context
    dependent. Many words have a different part-of-speech depending on how they are
    used. For example, we have been talking about taggers that "tag" words. In this
    case, "tag" is used as a verb. But the result of tagging is a part-of-speech tag,
    so "tag" can also be a noun. The idea with the `NgramTagger` subclasses is that
    by looking at the previous words and part-of-speech tags, we can better guess
    the part-of-speech tag for the current word.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the first two recipes of this chapter for details on constructing `train_sents`
    and `test_sents`.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By themselves, `BigramTagger` and `TrigramTagger` perform quite poorly. This
    is partly because they cannot learn context from the first word(s) in a sentence.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Where they can make a contribution is when we combine them with backoff tagging.
    This time, instead of creating each tagger individually, we will create a function
    that will take `train_sents`, a list of `SequentialBackoffTagger` classes, and
    an optional final backoff tagger, and then train each tagger with the previous
    tagger as a backoff. Here''s code from `tag_util.py`:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'And to use it, we can do the following:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: So we have gained almost 1% accuracy by including the `BigramTagger` and `TrigramTagger`
    in the backoff chain. For corpora other than `treebank`, the accuracy gain may
    be more significant.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `backoff_tagger` function creates an instance of each tagger class in the
    list, giving it the `train_sents` and the previous tagger as a backoff. The order
    of the list of tagger classes is quite important—the first class in the list will
    be trained first, and be given the initial backoff tagger. This tagger will then
    become the backoff tagger for the next tagger class in the list. The final tagger
    returned will be an instance of the last tagger class in the list. Here''s some
    code to clarify this chain:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: So we end up with a `TrigramTagger`, whose first backoff is a `BigramTagger`.
    Then the next backoff will be a `UnigramTagger`, whose backoff is the `DefaultTagger`.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `backoff_tagger` function doesn't just work with `NgramTagger` classes.
    It can be used for constructing a chain containing any subclasses of `SequentialBackoffTagger`.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '`BigramTagger` and `TrigramTagger`, because they are subclasses of `NgramTagger`
    and `ContextTagger`, can also take a model and cutoff argument, just like the
    `UnigramTagger`. But unlike for `UnigramTagger`, the context keys of the model
    must be 2-tuples, where the first element is a section of the history, and the
    second element is the current token. For the `BigramTagger`, an appropriate context
    key looks like `((prevtag,), word)`, and for `TrigramTagger` it looks like `((prevtag1,
    prevtag2), word)`.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Quadgram Tagger
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `NgramTagger` class can be used by itself to create a tagger that uses Ngrams
    longer than three for its context key.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'It''s even worse than the `TrigramTagger`! Here''s an alternative implementation
    of a `QuadgramTagger` that we can include in a list to `backoff_tagger`. This
    code can be found in `taggers.py`:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This is essentially how `BigramTagger` and `TrigramTagger` are implemented;
    simple subclasses of `NgramTagger` that pass in the number of *ngrams* to look
    at in the `history` argument of the `context()` method.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s see how it does as part of a backoff chain:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: It's actually slightly worse than before when we stopped with the `TrigramTagger`.
    So the lesson is that too much context can have a negative effect on accuracy.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The previous two recipes cover the `UnigramTagger` and backoff tagging.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: Creating a model of likely word tags
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned earlier in this chapter in the *Training a unigram part-of-speech
    tagger* recipe, using a custom model with a `UnigramTagger` should only be done
    if you know exactly what you are doing. In this recipe, we are going to create
    a model for the most common words, most of which always have the same tag no matter
    what.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To find the most common words, we can use `nltk.probability.FreqDist` to count
    word frequencies in the `treebank` corpus. Then, we can create a `ConditionalFreqDist`
    for tagged words, where we count the frequency of every tag for every word. Using
    these counts, we can construct a model of the 200 most frequent words as keys,
    with the most frequent tag for each word as a value. Here''s the model creation
    function defined in `tag_util.py`:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'And to use it with a `UnigramTagger`, we can do the following:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'An accuracy of almost 56% is ok, but nowhere near as good as the trained `UnigramTagger`.
    Let''s try adding it to our backoff chain:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The final accuracy is exactly the same as without the `likely_tagger`. This
    is because the frequency calculations we did to create the model are almost exactly
    what happens when we train a `UnigramTagger`.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `word_tag_model()` function takes a list of all words, a list of all tagged
    words, and the maximum number of words we want to use for our model. We give the
    list of words to a `FreqDist`, which counts the frequency of each word. Then we
    get the top 200 words from the `FreqDist` by calling `fd.keys()`, which returns
    all words ordered by highest frequency to lowest. We give the list of tagged words
    to a `ConditionalFreqDist`, which creates a `FreqDist` of tags for each word,
    with the word as the *condition*. Finally, we return a `dict` of the top 200 words
    mapped to their most likely tag.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It may seem useless to include this tagger as it does not change the accuracy.
    But the point of this recipe is to demonstrate how to construct a useful model
    for a `UnigramTagger`. Custom model construction is a way to create a manual override
    of trained taggers that are otherwise black boxes. And by putting the likely tagger
    in the front of the chain, we can actually improve accuracy a little bit:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Putting custom model taggers at the front of the backoff chain gives you complete
    control over how specific words are tagged, while letting the trained taggers
    handle everything else.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Training a unigram part-of-speech tagger* recipe has details on the `UnigramTagger`
    and a simple custom model example. See the earlier recipes *Combining taggers
    with backoff tagging* and *Training and combining Ngram taggers* for details on
    backoff tagging.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: Tagging with regular expressions
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can use regular expression matching to tag words. For example, you can match
    numbers with `\d` to assign the tag **CD** (which refers to a **Cardinal number**).
    Or you could match on known word patterns, such as the suffix "ing". There's lot
    of flexibility here, but be careful of over-specifying since language is naturally
    inexact, and there are always exceptions to the rule.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this recipe to make sense, you should be familiar with regular expression
    syntax and Python's `re` module.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `RegexpTagger` expects a list of 2-tuples, where the first element in the
    tuple is a regular expression, and the second element is the tag. The following
    patterns can be found in `tag_util.py`:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Once you have constructed this list of patterns, you can pass it into `RegexpTagger`.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: So it's not too great with just a few patterns, but since `RegexpTagger` is
    a subclass of `SequentialBackoffTagger`, it can be useful as part of a backoff
    chain, especially if you are able to come up with more word patterns.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `RegexpTagger` saves the `patterns` given at initialization, then on each
    call to `choose_tag()`, it iterates over the patterns and returns the tag for
    the first expression that matches the current word using `re.match()`. This means
    that if you have two expressions that could match, the tag of the first one will
    always be returned, and the second expression won't even be tried.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `RegexpTagger` can replace the `DefaultTagger` if you give it a pattern
    such as `(r'.*', 'NN')`. This pattern should, of course, be last in the list of
    patterns, otherwise no other patterns will match.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the next recipe, we will cover the `AffixTagger`, which learns how to tag
    based on prefixes and suffixes of words. And see the *Default tagging* recipe
    for details on the `DefaultTagger`.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Affix tagging
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `AffixTagger` is another `ContextTagger` subclass, but this time the *context*
    is either the *prefix* or the *suffix* of a word. This means the `AffixTagger`
    is able to learn tags based on fixed-length substrings of the beginning or ending
    of a word.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The default arguments for an `AffixTagger` specify three-character suffixes,
    and that words must be at least five characters long. If a word is less than five
    characters long, then `None` is returned as the tag.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'So it does ok by itself with the default arguments. Let''s try it by specifying
    three-character prefixes:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'To learn on two-character suffixes, the code looks like this:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: How it works...
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A positive value for `affix_length` means that the `AffixTagger` will learn
    word prefixes, essentially `word[:affix_length]`. If the `affix_length` is negative,
    then suffixes are learned using `word[affix_length:]`.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can combine multiple affix taggers in a backoff chain if you want to learn
    about multiple character length affixes. Here''s an example of four `AffixTagger`
    classes learning about two and three-character prefixes and suffixes:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: As you can see, the accuracy goes up each time.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The preceding ordering is not the best, nor is it the worst. I will leave it
    to you to explore the possibilities and discover the best backoff chain of `AffixTagger`
    and `affix_length` values.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Min stem length
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`AffixTagger` also takes a `min_stem_length` keyword argument with a default
    value of `2`. If the word length is less than `min_stem_length` plus the absolute
    value of `affix_length`, then `None` is returned by the `context()` method. Increasing
    `min_stem_length` forces the `AffixTagger` to only learn on longer words, while
    decreasing `min_stem_length` will allow it to learn on shorter words. Of course,
    for shorter words, the `affix_length` could be equal to or greater than the word
    length, and `AffixTagger` would essentially be acting like a `UnigramTagger`.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can manually specify prefixes and suffixes using regular expressions, as
    shown in the previous recipe. The *Training a unigram part-of-speech tagger* and
    *Training and combining Ngram taggers* recipes have details on `NgramTagger` subclasses,
    which are also subclasses of `ContextTagger`.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Training a Brill tagger
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `BrillTagger` is a transformation-based tagger. It is the first tagger that
    is not a subclass of `SequentialBackoffTagger`. Instead, the `BrillTagger` uses
    a series of rules to correct the results of an *initial tagger*. These rules are
    scored based on how many errors they correct minus the number of new errors they
    produce.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here's a function from `tag_util.py` that trains a `BrillTagger` using `FastBrillTaggerTrainer`.
    It requires an `initial_tagger` and `train_sents`.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: To use it, we can create our `initial_tagger` from a backoff chain of `NgramTagger`
    classes, then pass that into the `train_brill_tagger()` function to get a `BrillTagger`
    back.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: So the `BrillTagger` has slightly increased accuracy over the `initial_tagger`.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `FastBrillTaggerTrainer` takes an `initial_tagger` and a list of `templates`.
    These templates must implement the `BrillTemplateI` interface. The two template
    implementations included with NLTK are `ProximateTokensTemplate` and `SymmetricProximateTokensTemplate`.
    Each template is used to generate a list of `BrillRule` subclasses. The actual
    class of the rules produced is passed in to the template at initialization. The
    basic workflow looks like this:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/3609OS_04_03.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
- en: The two `BrillRule` subclasses used are `ProximateTagsRule` and `ProximateWordsRule`,
    which are both subclasses of `ProximateTokensRule`. `ProximateTagsRule` looks
    at surrounding tags to do error correction, and `ProximateWordsRule` looks at
    the surrounding words.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: The *bounds* that we pass in to each template are lists of `(start, end)` tuples
    that get passed in to each rule as *conditions*. The conditions tell the rule
    which tokens it can look at. For example, if the condition is `(1, 1)`, then the
    rule will only look at the next token. But if the condition is `(1, 2)`, then
    the rule will look at both the next token and the token after it. For `(-1, -1)`
    the rule will look only at the previous token.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '`ProximateTokensTemplate` produces `ProximateTokensRule` that look at each
    token for its given conditions to do error correction. Positive and negative conditions
    must be explicitly specified. `SymmetricProximateTokensTemplate`, on the other
    hand, produces pairs of `ProximateTokensRule`, where one rule uses the given conditions,
    and the other rule uses the negative of the conditions. So when we pass a list
    of positive `(start, end)` tuples to a `SymmetricProximateTokensTemplate`, it
    will also produce a `ProximateTokensRule` that uses `(-start, -end`). This is
    why it''s *symmetric*—it produces rules that look on both sides of the token.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unlike with `ProximateTokensTemplate`, you should not give negative bounds to
    `SymmetricProximateTokensTemplate`, since it will produce those itself. Only use
    positive number bounds with `SymmetricProximateTokensTemplate`.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can control the number of rules generated using the `max_rules` keyword
    argument to the `FastBrillTaggerTrainer.train()` method. The default value is
    `200`. You can also control the quality of rules used with the `min_score` keyword
    argument. The default value is `2`, though `3` can be a good choice as well.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Increasing `max_rules` or `min_score` will greatly increase training time, without
    necessarily increasing accuracy. Change these values with care.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: Tracing
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can watch the `FastBrillTaggerTrainer` do its work by passing `trace=1`
    into the constructor. This can give you output such as:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This means it found `10709` rules with a score of at least `min_score`, and
    then it selects the best rules, keeping no more than `max_rules`.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: The default is `trace=0`, which means the trainer will work silently without
    printing its status.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Training and combining Ngram taggers* recipe details the construction of
    the `initial_tagger` used previously, and the *Default tagging* recipe explains
    the `default_tagger`.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Training the TnT tagger
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**TnT** stands for **Trigrams''n''Tags**. It is a statistical tagger based
    on second order Markov models. You can read the original paper that lead to the
    implementation at [http://acl.ldc.upenn.edu/A/A00/A00-1031.pdf](http://acl.ldc.upenn.edu/A/A00/A00-1031.pdf).'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `TnT` tagger has a slightly different API than previous taggers we have
    encountered. You must explicitly call the `train()` method after you have created
    it. Here''s a basic example:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: It's quite a good tagger all by itself, only slightly less accurate than the
    `BrillTagger` from the previous recipe. But if you do not call `train()` before
    `evaluate()`, you will get an accuracy of 0%.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`TnT` maintains a number of internal `FreqDist` and `ConditionalFreqDist` instances
    based on the training data. These frequency distributions count unigrams, bigrams,
    and trigrams. Then, during tagging, the frequencies are used to calculate the
    probabilities of possible tags for each word. So instead of constructing a backoff
    chain of `NgramTagger` subclasses, the `TnT` tagger uses all the ngram models
    together to choose the best tag. It also tries to guess the tags for the whole
    sentence at once, by choosing the most likely model for the entire sentence, based
    on the probabilities of each possible tag.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Training is fairly quick, but tagging is significantly slower than the other
    taggers we have covered. This is due to all the floating point math that must
    be done to calculate the tag probabilities of each word.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`TnT` accepts a few optional keyword arguments. You can pass in a tagger for
    unknown words as `unk`. If this tagger is already trained, then you must also
    pass in `Trained=True`. Otherwise it will call `unk.train(data)` with the same
    data you pass in to the `train()` method. Since none of the previous taggers have
    a public `train()` method, we recommend always passing `Trained=True` if you also
    pass an `unk` tagger. Here''s an example using a `DefaultTagger`, which does not
    require any training:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: So we got an almost 2% increase in accuracy! You must use a tagger that can
    tag a single word without having seen that word before. This is because the unknown
    tagger's `tag()` method is only called with a single word sentence. Other good
    candidates for an unknown tagger are `RegexpTagger` or `AffixTagger`. Passing
    in a `UnigramTagger` that's been trained on the same data is pretty much useless,
    as it will have seen the exact same words, and therefore have the same unknown
    word blind spots.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Controlling the beam search
  id: totrans-222
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Another parameter you can modify for `TnT` is `N`, which controls the number
    of possible solutions the tagger maintains while trying to guess the tags for
    a sentence. `N` defaults to 1,000\. Increasing it will greatly increase the amount
    of memory used during tagging, without necessarily increasing accuracy. Decreasing
    `N` will decrease memory usage, but could also decrease accuracy. Here''s what
    happens when you set `N=100`:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: So the accuracy is exactly the same, but we use significantly less memory to
    achieve it. However, don't assume that accuracy will not change if you decrease
    `N`; experiment with your own data to be sure.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: Capitalization significance
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can pass `C=True` if you want capitalization of words to be significant.
    The default is `C=False`, which means all words are lowercased. The documentation
    on `C` says that treating capitalization as significant probably will not increase
    accuracy. In my own testing, there was a very slight (< 0.01%) increase in accuracy
    with `C=True`, probably because case-sensitivity can help identify proper nouns.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We covered the `DefaultTagger` in the *Default tagging* recipe, backoff tagging
    in the *Combining taggers with backoff tagging* recipe, `NgramTagger` subclasses
    in the *Training a unigram part-of-speech tagger* and *Training combining Ngram
    taggers* recipes, `RegexpTagger` in the *Tagging with regular expressions* recipe,
    and the `AffixTagger` in the *Affix tagging* recipe.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Using WordNet for tagging
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you remember from the *Looking up synsets for a word in Wordnet* recipe in
    [Chapter 1](ch01.html "Chapter 1. Tokenizing Text and WordNet Basics"), *Tokenizing
    Text and WordNet Basics*, WordNet synsets specify a part-of-speech tag. It's a
    very restricted set of possible tags, and many words have multiple synsets with
    different part-of-speech tags, but this information can be useful for tagging
    unknown words. WordNet is essentially a giant dictionary, and it's likely to contain
    many words that are not in your training data.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, we need to decide how to map WordNet part-of-speech tags to the Penn
    Treebank part-of-speech tags we have been using. The following is a table mapping
    one to the other. See the *Looking up synsets for a word in Wordnet* recipe in
    [Chapter 1](ch01.html "Chapter 1. Tokenizing Text and WordNet Basics"), *Tokenizing
    Text and WordNet Basics* for more details. The "s", which was not shown before,
    is just another kind of adjective, at least for tagging purposes.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '| WordNet Tag | Treebank Tag |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
- en: '| n | NN |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
- en: '| a | JJ |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
- en: '| s | JJ |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
- en: '| r | RB |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
- en: '| v | VB |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
- en: How to do it...
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now we can create a class that will look up words in WordNet, then chose the
    most common tag from the synsets it finds. The `WordNetTagger` defined next can
    be found in `taggers.py`:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: How it works...
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `WordNetTagger` simply counts the number of each part-of-speech tag found
    in the synsets for a word. The most common tag is then mapped to a `treebank`
    tag using an internal mapping. Here''s some sample usage code:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: So it's not too accurate, but that's to be expected. We only have enough information
    to produce four different kinds of tags, while there are 36 possible tags in `treebank`.
    And many words can have different part-of-speech tags depending on their context.
    But if we put the `WordNetTagger` at the end of an `NgramTagger` backoff chain,
    then we can improve accuracy over the `DefaultTagger`.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: See also
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Looking up synsets for a word in Wordnet* recipe in [Chapter 1](ch01.html
    "Chapter 1. Tokenizing Text and WordNet Basics"), *Tokenizing Text and WordNet
    Basics* details how to use the `wordnet` corpus and what kinds of part-of-speech
    tags it knows about. And in the *Combining taggers with backoff tagging* and *Training
    and combining Ngram taggers* recipes, we went over backoff tagging with ngram
    taggers.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Tagging proper names
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using the included `names` corpus, we can create a simple tagger for tagging
    names as *proper nouns*.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `NamesTagger` is a subclass of `SequentialBackoffTagger` as it''s probably
    only useful near the end of a backoff chain. At initialization, we create a set
    of all names in the `names` corpus, lowercasing each name to make lookup easier.
    Then we implement the `choose_tag()` method, which simply checks if the current
    word is in the `names_set`. If it is, we return the tag *NNP* (which is the tag
    for *proper nouns*). If it isn''t, we return `None` so the next tagger in the
    chain can tag the word. The following code can be found in `taggers.py`:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: How it works...
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`NamesTagger` should be pretty self-explanatory. Its usage is also simple:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: It's probably best to use the `NamesTagger` right before a `DefaultTagger`,
    so it's at the end of a backoff chain. But it could probably go anywhere in the
    chain since it's unlikely to mistag a word.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Combining taggers with backoff tagging* recipe goes over the details of
    using `SequentialBackoffTagger` subclasses.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: Classifier based tagging
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `ClassifierBasedPOSTagger` uses *classification* to do part-of-speech tagging.
    **Features** are extracted from words, then passed to an internal classifier.
    The classifier classifies the features and returns a label; in this case, a part-of-speech
    tag. Classification will be covered in detail in [Chapter 7](ch07.html "Chapter 7. Text
    Classification"), *Text Classification*.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '`ClassifierBasedPOSTagger` is a subclass of `ClassifierBasedTagger` that implements
    a **feature detector** that combines many of the techniques of previous taggers
    into a single **feature set** . The feature detector finds multiple length suffixes,
    does some regular expression matching, and looks at the unigram, bigram, and trigram
    history to produce a fairly complete set of features for each word. The feature
    sets it produces are used to train the internal classifier, and are used for classifying
    words into part-of-speech tags.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Basic usage of the `ClassifierBasedPOSTagger` is much like any other `SequentialBackoffTaggger`.
    You pass in training sentences, it trains an internal classifier, and you get
    a very accurate tagger.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Note
  id: totrans-268
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Notice a slight modification to initialization—`train_sents` must be passed
    in as the `train` keyword argument.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`ClassifierBasedPOSTagger` inherits from `ClassifierBasedTagger` and only implements
    a `feature_detector()` method. All the training and tagging is done in `ClassifierBasedTagger`.
    It defaults to training a `NaiveBayesClassifier` with the given training data.
    Once this classifier is trained, it is used to classify word features produced
    by the `feature_detector()` method.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `ClassifierBasedTagger` is often the most accurate tagger, but it's also
    one of the slowest taggers. If speed is an issue, you should stick with a `BrillTagger`
    based on a backoff chain of `NgramTagger` subclasses and other simple taggers.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: 'The `ClassifierBasedTagger` also inherits from `FeatursetTaggerI` (which is
    just an empty class), creating an inheritance tree that looks like this:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/3609OS_04_04.jpg)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
- en: There's more...
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can use a different classifier instead of `NaiveBayesClassifier` by passing
    in your own `classifier_builder` function. For example, to use a `MaxentClassifier`,
    you would do the following:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Note
  id: totrans-279
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `MaxentClassifier` takes even longer to train than `NaiveBayesClassifier`.
    If you have `scipy` and `numpy` installed, training will be faster than normal,
    but still slower than `NaiveBayesClassifier`.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: Custom feature detector
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you want to do your own feature detection, there are two ways to do it.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: Subclass `ClassifierBasedTagger` and implement a `feature_detector()` method.
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pass a method as the `feature_detector` keyword argument into `ClassifierBasedTagger`
    at initialization.
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Either way, you need a feature detection method that can take the same arguments
    as `choose_tag()`: `tokens`, `index`, and `history`. But instead of returning
    a tag, you return a `dict` of key-value features, where the key is the feature
    name, and the value is the feature value. A very simple example would be a unigram
    feature detector (found in `tag_util.py`).'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Then using the second method, you would pass the following into `ClassifierBasedTagger`
    as `feature_detector`:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Cutoff probability
  id: totrans-289
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Because a classifier will always return the best result it can, passing in
    a backoff tagger is useless unless you also pass in a `cutoff_prob` to specify
    the probability threshold for classification. Then, if the probability of the
    chosen tag is less than `cutoff_prob`, the backoff tagger will be used. Here''s
    an example using the `DefaultTagger` as the `backoff`, and setting `cutoff_prob`
    to `0.3`:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: So we get a slight increase in accuracy if the `ClassifierBasedPOSTagger` uses
    the `DefaultTagger` whenever its tag probability is less than 30%.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: Pre-trained classifier
  id: totrans-293
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you want to use a classifier that's already been trained, then you can pass
    that in to `ClassifierBasedTagger` or `ClassifierBasedPOSTagger` as `classifier`.
    In this case, the `classifier_builder` argument is ignored and no training takes
    place. However, you must ensure that the classifier has been trained on and can
    classify feature sets produced by whatever `feature_detector()` method you use.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Chapter 7](ch07.html "Chapter 7. Text Classification"), *Text Classification*
    will cover classification in depth.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '[第7章](ch07.html "第7章。文本分类")，*文本分类*将深入探讨分类问题。'
