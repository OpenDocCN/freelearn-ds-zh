- en: Chapter 4. Part-of-Speech Tagging
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四章 词性分词
- en: 'In this chapter, we will cover:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍：
- en: Default tagging
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认分词
- en: Training a unigram part-of-speech tagger
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练单词词性分词器
- en: Combining taggers with backoff tagging
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将分词器与回退分词结合
- en: Training and combining Ngram taggers
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练和组合 Ngram 分词器
- en: Creating a model of likely word tags
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建可能的单词标记模型
- en: Tagging with regular expressions
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用正则表达式进行分词
- en: Affix tagging
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词缀分词
- en: Training a Brill tagger
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练 Brill 分词器
- en: Training the TnT tagger
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练 TnT 分词器
- en: Using WordNet for tagging
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 WordNet 进行分词
- en: Tagging proper names
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分词专有名词
- en: Classifier-based tagging
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于分类器的分词
- en: Introduction
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介
- en: '**Part-of-speech tagging** is the process of converting a sentence, in the
    form of a list of words, into a list of tuples, where each tuple is of the form
    `(word, tag)`. The **tag** is a part-of-speech tag and signifies whether the word
    is a noun, adjective, verb, and so on.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**词性分词** 是将句子（以单词列表的形式）转换为元组列表的过程，其中每个元组的形式为 `(word, tag)`。**标记** 是词性标记，表示单词是名词、形容词、动词等。'
- en: Most of the taggers we will cover are *trainable*. They use a list of tagged
    sentences as their training data, such as what you get from the `tagged_sents()`
    function of a `TaggedCorpusReader` (see the *Creating a part-of-speech tagged
    word corpus* recipe in [Chapter 3](ch03.html "Chapter 3. Creating Custom Corpora"),
    *Creating Custom Corpora* for more details). With these training sentences, the
    tagger generates an internal model that will tell them how to tag a word. Other
    taggers use external data sources or match word patterns to choose a tag for a
    word.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要介绍的多数分词器都是可训练的。它们使用标记过的句子列表作为训练数据，例如从 `TaggedCorpusReader` 的 `tagged_sents()`
    函数中获取的数据（参见第 3 章 *创建自定义语料库* 中的 *创建词性标记的词语语料库* 菜单，更多详情请参阅 *创建自定义语料库*）。有了这些训练句子，分词器会生成一个内部模型，告诉它们如何标记一个单词。其他分词器使用外部数据源或匹配单词模式来为单词选择一个标记。
- en: All taggers in NLTK are in the `nltk.tag` package and inherit from the `TaggerI`
    base class. `TaggerI` requires all subclasses to implement a `tag()` method, which
    takes a list of words as input, and returns a list of tagged words as output.
    `TaggerI` also provides an `evaluate()` method for evaluating the accuracy of
    the tagger (covered at the end of the *Default tagging* recipe). Many taggers
    can also be combined into a backoff chain, so that if one tagger cannot tag a
    word, the next tagger is used, and so on.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK 中的所有分词器都在 `nltk.tag` 包中，并继承自 `TaggerI` 基类。`TaggerI` 要求所有子类实现一个 `tag()`
    方法，该方法接收一个单词列表作为输入，并返回一个标记过的单词列表作为输出。`TaggerI` 还提供了一个 `evaluate()` 方法来评估分词器的准确性（在
    *默认分词* 菜单的末尾介绍）。许多分词器也可以组合成一个回退链，这样如果某个分词器无法标记一个单词，则使用下一个分词器，依此类推。
- en: Part-of-speech tagging is a necessary step before *chunking*, which is covered
    in [Chapter 5](ch05.html "Chapter 5. Extracting Chunks"), *Extracting Chunks*.
    Without the part-of-speech tags, a chunker cannot know how to extract phrases
    from a sentence. But with part-of-speech tags, you can tell a chunker how to identify
    phrases based on tag patterns.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 词性分词是 *短语提取* 之前的一个必要步骤，*短语提取* 在第 5 章 *提取短语* 中介绍。没有词性标记，短语提取器无法知道如何从句子中提取短语。但是有了词性标记，你可以告诉短语提取器如何根据标记模式来识别短语。
- en: Default tagging
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 默认分词
- en: Default tagging provides a baseline for part-of-speech tagging. It simply assigns
    the same part-of-speech tag to every token. We do this using the `DefaultTagger`.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 默认分词为词性分词提供了一个基线。它简单地将相同的词性标记分配给每个标记。我们使用 `DefaultTagger` 来完成这个操作。
- en: Getting ready
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We are going to use the `treebank` corpus for most of this chapter because it's
    a common standard and is quick to load and test. But everything we do should apply
    equally well to `brown`, `conll2000`, and any other part-of-speech tagged corpus.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 `treebank` 语料库来完成本章的大部分内容，因为它是一个常见的标准，加载和测试都很快速。但我们所做的一切都应该同样适用于 `brown`、`conll2000`
    和任何其他词性标记语料库。
- en: How to do it...
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: The `DefaultTagger` takes a single argument—the tag you want to apply. We will
    give it `'NN'`, which is the tag for a singular noun.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '`DefaultTagger` 接收一个单一参数——你想要应用的标记。我们将给它 `''NN''`，这是单数名词的标记。'
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Every tagger has a `tag()` method that takes a list of tokens, where each token
    is a single word. This list of tokens is usually a list of words produced by a
    word tokenizer (see [Chapter 1](ch01.html "Chapter 1. Tokenizing Text and WordNet
    Basics"), *Tokenizing Text and WordNet Basics* for more on tokenization). As you
    can see, `tag()` returns a list of tagged tokens, where a **tagged token** is
    a tuple of `(word, tag)`.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 每个标签器都有一个`tag()`方法，它接受一个标记列表，其中每个标记是一个单词。这个标记列表通常是由单词分词器（见[第1章](ch01.html "第1章. Tokenizing
    Text and WordNet Basics")，*Tokenizing Text and WordNet Basics* 中有关分词的更多信息）生成的单词列表。正如你所看到的，`tag()`返回一个标记标记列表，其中标记标记是一个`(word,
    tag)`元组。
- en: How it works...
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何工作...
- en: '`DefaultTagger` is a subclass of `SequentialBackoffTagger`. Every subclass
    of `SequentialBackoffTagger` must implement the `choose_tag()` method, which takes
    three arguments:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '`DefaultTagger`是`SequentialBackoffTagger`的子类。`SequentialBackoffTagger`的每个子类都必须实现`choose_tag()`方法，该方法接受三个参数：'
- en: The list of `tokens`.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`tokens`的列表。'
- en: The `index` of the current token whose tag we want to choose.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当前标记的`index`，我们想要选择其标签。
- en: The `history`, which is a list of the previous tags.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`history`，它是一个先前标签的列表。'
- en: '`SequentialBackoffTagger` implements the `tag()` method, which calls the `choose_tag()`
    of the subclass for each index in the tokens list, while accumulating a history
    of the previously tagged tokens. This history is the reason for the *Sequential*
    in `SequentialBackoffTagger`. We will get to the *Backoff* portion of the name
    in the *Combining taggers with backoff tagging* recipe. The following is a diagram
    showing the inheritance tree:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '`SequentialBackoffTagger`实现了`tag()`方法，该方法对标记列表中的每个索引调用子类的`choose_tag()`，同时累积先前标记的标记历史。这就是`SequentialBackoffTagger`中“Sequential”的原因。我们将在*Combining
    taggers with backoff tagging*配方中介绍名称中的*Backoff*部分。以下是一个显示继承树的图表：'
- en: '![How it works...](img/3609OS_04_01.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作...](img/3609OS_04_01.jpg)'
- en: The `choose_tag()` method of `DefaultTagger` is very simple—it returns the tag
    we gave it at initialization time. It does not care about the current token or
    the history.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '`DefaultTagger`的`choose_tag()`方法非常简单——它返回我们在初始化时给出的标签。它不关心当前标记或历史记录。'
- en: There's more...
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容...
- en: There are a lot of different tags you could give to the `DefaultTagger`. You
    can find a complete list of possible tags for the `treebank` corpus at [http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html](http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html).
    These tags are also documented in [Appendix](apa.html "Appendix A. Penn Treebank
    Part-of-Speech Tags"), *Penn Treebank Part-of-Speech Tags*.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以为`DefaultTagger`分配很多不同的标签。你可以在[http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html](http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)找到`treebank`语料库中所有可能标签的完整列表。这些标签也在[附录](apa.html
    "附录 A. Penn Treebank Part-of-Speech Tags") *Penn Treebank Part-of-Speech Tags*
    中进行了说明。
- en: Evaluating accuracy
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估准确性
- en: To know how accurate a tagger is, you can use the `evaluate()` method, which
    takes a list of tagged tokens as a gold standard to evaluate the tagger. Using
    our default tagger created earlier, we can evaluate it against a subset of the
    `treebank` corpus tagged sentences.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解标签器的准确性，你可以使用`evaluate()`方法，该方法接受一个标记标记列表作为黄金标准来评估标签器。使用我们之前创建的默认标签器，我们可以将其与`treebank`语料库的标记句子子集进行比较。
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: So by just choosing `'NN'` for every tag, we can achieve 14% accuracy testing
    on ¼th of the `treebank` corpus. We will be reusing these same `test_sents` for
    evaluating more taggers in upcoming recipes.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，只需为每个标签选择`'NN'`，我们就可以在`treebank`语料库的四分之一上实现14%的准确性测试。我们将在未来的配方中重复使用这些相同的`test_sents`来评估更多的标签器。
- en: Batch tagging sentences
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 批量标签化句子
- en: '`TaggerI` also implements a `batch_tag()` method that can be used to tag a
    list of sentences, instead of a single sentence. Here''s an example of tagging
    two simple sentences:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`TaggerI`还实现了一个`batch_tag()`方法，可以用来对一系列句子进行标签化，而不是单个句子。以下是对两个简单句子进行标签化的示例：'
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The result is a list of two tagged sentences, and of course every tag is `NN`
    because we are using the `DefaultTagger`. The `batch_tag()` method can be quite
    useful if you have many sentences you wish to tag all at once.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是两个已标记句子的列表，当然，每个标签都是`NN`，因为我们正在使用`DefaultTagger`。如果你有很多句子需要一次性进行标签化，`batch_tag()`方法非常有用。
- en: Untagging a tagged sentence
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解除已标记句子的标签
- en: Tagged sentences can be untagged using `nltk.tag.untag()`. Calling this function
    with a tagged sentence will return a list of words without the tags.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`nltk.tag.untag()`来解除已标记句子的标签。调用此函数并传入一个已标记句子将返回一个不带标签的单词列表。
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: See also
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考信息
- en: For more on tokenization, see [Chapter 1](ch01.html "Chapter 1. Tokenizing Text
    and WordNet Basics"), *Tokenizing Text and WordNet Basics*. And to learn more
    about tagged sentences, see the *Creating a part-of-speech tagged word corpus*
    recipe in [Chapter 3](ch03.html "Chapter 3. Creating Custom Corpora"), *Creating
    Custom Corpora*. For a complete list of part-of-speech tags found in the treebank
    corpus, see [Appendix](apa.html "Appendix A. Penn Treebank Part-of-Speech Tags"),
    *Penn Treebank Part-of-Speech Tags*.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 关于分词的更多信息，请参阅[第1章](ch01.html "第1章. 分词和WordNet基础知识")，*分词和WordNet基础知识*。要了解更多关于标记句子的信息，请参阅[第3章](ch03.html
    "第3章. 创建自定义语料库")中的*创建自定义语料库*配方。要查看在树库语料库中找到的所有词性标记的完整列表，请参阅[附录](apa.html "附录A.
    Penn Treebank词性标记")，*Penn Treebank词性标记*。
- en: Training a unigram part-of-speech tagger
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练单语词性标记器
- en: A **unigram** generally refers to a single token. Therefore, a *unigram tagger*
    only uses a single word as its *context* for determining the part-of-speech tag.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**单语**通常指单个标记。因此，*单语标记器*仅使用单个单词作为其*上下文*来确定词性标记。'
- en: The `UnigramTagger` inherits from `NgramTagger`, which is a subclass of `ContextTagger`,
    which inherits from `SequentialBackoffTagger`. In other words, the `UnigramTagger`
    is a *context-based tagger* whose context is a single word, or unigram.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`UnigramTagger`继承自`NgramTagger`，而`NgramTagger`是`ContextTagger`的子类，`ContextTagger`继承自`SequentialBackoffTagger`。换句话说，`UnigramTagger`是一个*基于上下文*的标记器，其上下文是一个单词，或单语。'
- en: How to do it...
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: '`UnigramTagger` can be trained by giving it a list of tagged sentences at initialization.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '`UnigramTagger`可以通过在初始化时提供标记句子列表来训练。'
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We use the first 3,000 tagged sentences of the `treebank` corpus as the training
    set to initialize the `UnigramTagger`. Then we see the first sentence as a list
    of words, and can see how it is transformed by the `tag()` function into a list
    of tagged tokens.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`treebank`语料库的前3,000个标记句子作为训练集来初始化`UnigramTagger`。然后我们将第一句话视为单词列表，并可以看到它是如何通过`tag()`函数转换为标记标记列表的。
- en: How it works...
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'The `UnigramTagger` builds a *context model* from the list of tagged sentences.
    Because `UnigramTagger` inherits from `ContextTagger`, instead of providing a
    `choose_tag()` method, it must implement a `context()` method, which takes the
    same three arguments as `choose_tag()`. The result of `context()` is, in this
    case, the word token. The context token is used to create the model, and also
    to look up the best tag once the model is created. Here''s an inheritance diagram
    showing each class, starting at `SequentialBackoffTagger`:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`UnigramTagger`从标记句子列表中构建一个*上下文模型*。因为`UnigramTagger`继承自`ContextTagger`，所以它必须实现一个`context()`方法，该方法接受与`choose_tag()`相同的三个参数。在这种情况下，`context()`的结果是单词标记。上下文标记用于创建模型，并在模型创建后查找最佳标记。以下是一个继承图，显示了从`SequentialBackoffTagger`开始的每个类：'
- en: '![How it works...](img/3609OS_04_02.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![工作原理...](img/3609OS_04_02.jpg)'
- en: Let's see how accurate the `UnigramTagger` is on the test sentences (see the
    previous recipe for how `test_sents` is created).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看`UnigramTagger`在测试句子上的准确率如何（参见前面的配方了解`test_sents`是如何创建的）。
- en: '[PRE5]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: It has almost 86% accuracy for a tagger that only uses single word lookup to
    determine the part-of-speech tag. All accuracy gains from here on will be much
    smaller.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 对于仅使用单个单词查找来确定词性标记的标记器来说，其准确率高达86%。从现在开始，所有准确率的提升都将非常小。
- en: There's more...
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容...
- en: The model building is actually implemented in `ContextTagger`. Given the list
    of tagged sentences, it calculates the frequency that a tag has occurred for each
    context. The tag with the highest frequency for a context is stored in the model.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 模型构建实际上是在`ContextTagger`中实现的。给定标记句子列表，它计算每个上下文中标记出现的频率。上下文中频率最高的标记存储在模型中。
- en: Overriding the context model
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 覆盖上下文模型
- en: All taggers that inherit from `ContextTagger` can take a pre-built model instead
    of training their own. This model is simply a Python `dict` mapping a context
    key to a tag. The context keys will depend on what the `ContextTagger` subclass
    returns from its `context()` method. For `UnigramTagger`, context keys are individual
    words. But for other `NgramTagger` subclasses, the context keys will be tuples.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 所有继承自`ContextTagger`的标记器都可以使用预构建的模型，而不是自己训练。这个模型只是一个将上下文键映射到标记的Python `dict`。上下文键将取决于`ContextTagger`子类从其`context()`方法返回的内容。对于`UnigramTagger`，上下文键是单个单词。但对于其他`NgramTagger`子类，上下文键将是元组。
- en: 'Here''s an example where we pass a very simple model to the `UnigramTagger`
    instead of a training set:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个例子，我们向`UnigramTagger`传递一个非常简单的模型，而不是训练集：
- en: '[PRE6]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Since the model only contained the context key, `'Pierre'`, only the first word
    got a tag. Every other word got `None` as the tag since the context word was not
    in the model. So unless you know exactly what you are doing, let the tagger train
    its own model instead of passing in your own.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 由于模型只包含上下文键`'Pierre'`，因此只有第一个单词得到了标签。由于上下文词不在模型中，其他每个单词的标签都是`None`。所以除非你确切知道你在做什么，否则让标记器自己训练模型，而不是传递你自己的模型。
- en: One good case for passing a self-created model to the `UnigramTagger` is for
    when you have a dictionary of words and tags, and you know that every word should
    always map to its tag. Then, you can put this `UnigramTagger` as your first backoff
    tagger (covered in the next recipe), to look up tags for unambiguous words.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 将自定义模型传递给`UnigramTagger`的一个好例子是当你有一个单词和标签的字典，并且你知道每个单词都应该始终映射到其标签。然后，你可以将这个`UnigramTagger`作为你的第一个回退标记器（在下一个菜谱中介绍），以查找无歧义单词的标签。
- en: Minimum frequency cutoff
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最小频率截止值
- en: The `ContextTagger` uses frequency of occurrence to decide which tag is most
    likely for a given context. By default, it will do this even if the context word
    and tag occurs only once. If you would like to set a minimum frequency threshold,
    then you can pass a `cutoff` value to the `UnigramTagger`.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '`ContextTagger`使用出现频率来决定给定上下文中最可能的标签。默认情况下，即使上下文词和标签只出现一次，它也会这样做。如果你想设置一个最小频率阈值，那么你可以向`UnigramTagger`传递一个`cutoff`值。'
- en: '[PRE7]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In this case, using `cutoff=3` has decreased accuracy, but there may be times
    when a cutoff is a good idea.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，使用`cutoff=3`降低了准确率，但有时设置截止值可能是个好主意。
- en: See also
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: In the next recipe, we will cover backoff tagging to combine taggers. And in
    the *Creating a model of likely word tags* recipe, we will learn how to statistically
    determine tags for very common words.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个菜谱中，我们将介绍回退标记以结合标记器。在*创建可能的单词标签模型*菜谱中，我们将学习如何统计地确定非常常见单词的标签。
- en: Combining taggers with backoff tagging
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结合标记器与回退标记
- en: '**Backoff tagging** is one of the core features of `SequentialBackoffTagger`.
    It allows you to chain taggers together so that if one tagger doesn''t know how
    to tag a word, it can pass the word on to the next backoff tagger. If that one
    can''t do it, it can pass the word on to the next backoff tagger, and so on until
    there are no backoff taggers left to check.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**回退标记**是`SequentialBackoffTagger`的核心功能之一。它允许你将标记器链接在一起，以便如果一个标记器不知道如何标记一个单词，它可以将其传递给下一个回退标记器。如果那个标记器也不能这样做，它可以将其传递给下一个回退标记器，依此类推，直到没有回退标记器可以检查。'
- en: How to do it...
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: Every subclass of `SequentialBackoffTagger` can take a `backoff` keyword argument
    whose value is another instance of a `SequentialBackoffTagger`. So we will use
    the `DefaultTagger` from the *Default tagging* recipe as the `backoff` to the
    `UnigramTagger` from the *Training a unigram part-of-speech tagger* recipe. Refer
    to both recipes for details on `train_sents` and `test_sents`.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '`SequentialBackoffTagger`的每个子类都可以接受一个`backoff`关键字参数，其值是另一个`SequentialBackoffTagger`实例。因此，我们将使用*默认标记*菜谱中的`DefaultTagger`作为*训练单语词性标记器*菜谱中的`UnigramTagger`的`backoff`。请参阅这两个菜谱以了解`train_sents`和`test_sents`的详细信息。'
- en: '[PRE8]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: By using a default tag of `NN` whenever the `UnigramTagger` is unable to tag
    a word, we have increased the accuracy by almost 2%!
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在`UnigramTagger`无法对单词进行标记时，我们使用默认标签`NN`，从而将准确率提高了近2%！
- en: How it works...
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'When a `SequentialBackoffTagger` is initialized, it creates an internal list
    of backoff taggers with itself as the first element. If a `backoff` tagger is
    given, then the backoff tagger''s internal list of taggers is appended. Here''s
    some code to illustrate this:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当`SequentialBackoffTagger`初始化时，它会创建一个包含自身作为第一个元素的内部回退标记器列表。如果提供了一个`backoff`标记器，那么回退标记器的内部标记器列表将被附加。以下是一些代码示例来说明这一点：
- en: '[PRE9]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The `_taggers` is the internal list of backoff taggers that the `SequentialBackoffTagger`
    uses when the `tag()` method is called. It goes through its list of taggers, calling
    `choose_tag()` on each one. As soon as a tag is found, it stops and returns that
    tag. This means that if the primary tagger can tag the word, then that's the tag
    that will be returned. But if it returns `None`, then the next tagger is tried,
    and so on until a tag is found, or else `None` is returned. Of course, `None`
    will never be returned if your final backoff tagger is a `DefaultTagger`.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`_taggers`是`SequentialBackoffTagger`在调用`tag()`方法时使用的内部回退标记器列表。它遍历其标记器列表，对每个标记器调用`choose_tag()`。一旦找到标记，它就停止并返回该标记。这意味着如果主要标记器可以标记单词，那么返回的将是该标记。但如果返回`None`，则尝试下一个标记器，依此类推，直到找到标记，或者返回`None`。当然，如果您的最终回退标记器是`DefaultTagger`，则永远不会返回`None`。'
- en: There's more...
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: While most of the taggers included in NLTK are subclasses of `SequentialBackoffTagger`,
    not all of them are. There's a few taggers that we will cover in later recipes
    that cannot be used as part of a backoff tagging chain, such as the `BrillTagger`.
    However, these taggers generally take another tagger to use as a baseline, and
    a `SequentialBackoffTagger` is often a good choice for that baseline.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然NLTK中包含的大多数标记器都是`SequentialBackoffTagger`的子类，但并非所有都是。在后面的菜谱中，我们将介绍一些标记器，它们不能作为回退标记链的一部分使用，例如`BrillTagger`。然而，这些标记器通常需要另一个标记器作为基线，而`SequentialBackoffTagger`通常是一个很好的选择。
- en: Pickling and unpickling a trained tagger
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 序列化和反序列化训练好的标记器
- en: 'Since training a tagger can take a while, and you generally only need to do
    the training once, pickling a trained tagger is a useful way to save it for later
    usage. If your trained tagger is called `tagger`, then here''s how to dump and
    load it with `pickle`:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 由于训练标记器可能需要一段时间，而且您通常只需要进行一次训练，因此将训练好的标记器进行序列化是一个有用的方法来保存它以供以后使用。如果您的训练好的标记器名为`tagger`，那么以下是如何使用`pickle`进行序列化和反序列化的方法：
- en: '[PRE10]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: If your tagger pickle file is located in a NLTK data directory, you could also
    use `nltk.data.load('tagger.pickle')` to load the tagger.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的标记器pickle文件位于NLTK数据目录中，您也可以使用`nltk.data.load('tagger.pickle')`来加载标记器。
- en: See also
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: In the next recipe, we will combine more taggers with backoff tagging. Also
    see the previous two recipes for details on the `DefaultTagger` and `UnigramTagger`.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个菜谱中，我们将结合更多的标记器与回退标记。同时，查看前两个菜谱以获取关于`DefaultTagger`和`UnigramTagger`的详细信息。
- en: Training and combining Ngram taggers
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练和组合Ngram标记器
- en: 'In addition to `UnigramTagger`, there are two more `NgramTagger` subclasses:
    `BigramTagger` and `TrigramTagger` . `BigramTagger` uses the previous tag as part
    of its context, while `TrigramTagger` uses the previous two tags. An **ngram**
    is a subsequence of *n* items, so the `BigramTagger` looks at two items (the previous
    tag and word), and the `TrigramTagger` looks at three items.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`UnigramTagger`之外，还有两个额外的`NgramTagger`子类：`BigramTagger`和`TrigramTagger`。`BigramTagger`使用前一个标记作为其上下文的一部分，而`TrigramTagger`使用前两个标记。**ngram**是*n*个项的子序列，因此`BigramTagger`查看两个项（前一个标记和单词），而`TrigramTagger`查看三个项。
- en: These two taggers are good at handling words whose part-of-speech tag is context
    dependent. Many words have a different part-of-speech depending on how they are
    used. For example, we have been talking about taggers that "tag" words. In this
    case, "tag" is used as a verb. But the result of tagging is a part-of-speech tag,
    so "tag" can also be a noun. The idea with the `NgramTagger` subclasses is that
    by looking at the previous words and part-of-speech tags, we can better guess
    the part-of-speech tag for the current word.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个标记器擅长处理词性标记依赖于上下文的单词。许多单词的词性根据其使用方式不同而不同。例如，我们一直在谈论标记单词的标记器。在这种情况下，“标记”被用作动词。但标记的结果是词性标记，因此“标记”也可以作为名词。`NgramTagger`子类中的想法是，通过查看前一个单词和词性标记，我们可以更好地猜测当前单词的词性标记。
- en: Getting ready
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Refer to the first two recipes of this chapter for details on constructing `train_sents`
    and `test_sents`.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅本章的前两个菜谱，以获取构建`train_sents`和`test_sents`的详细信息。
- en: How to do it...
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: By themselves, `BigramTagger` and `TrigramTagger` perform quite poorly. This
    is partly because they cannot learn context from the first word(s) in a sentence.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 单独使用时，`BigramTagger`和`TrigramTagger`的表现相当差。这主要是因为它们无法从句子的第一个单词（s）中学习上下文。
- en: '[PRE11]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Where they can make a contribution is when we combine them with backoff tagging.
    This time, instead of creating each tagger individually, we will create a function
    that will take `train_sents`, a list of `SequentialBackoffTagger` classes, and
    an optional final backoff tagger, and then train each tagger with the previous
    tagger as a backoff. Here''s code from `tag_util.py`:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 它们可以做出贡献的地方在于，当我们将它们与回退标记结合使用时。这次，我们不会单独创建每个标记器，而是创建一个函数，该函数将接受`train_sents`、一个`SequentialBackoffTagger`类列表和一个可选的最终回退标记器，然后使用上一个标记器作为回退来训练每个标记器。以下是来自`tag_util.py`的代码：
- en: '[PRE12]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'And to use it, we can do the following:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 使用它，我们可以这样做：
- en: '[PRE13]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: So we have gained almost 1% accuracy by including the `BigramTagger` and `TrigramTagger`
    in the backoff chain. For corpora other than `treebank`, the accuracy gain may
    be more significant.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过在回退链中包含`BigramTagger`和`TrigramTagger`，我们几乎提高了1%的准确率。对于除`treebank`之外的其他语料库，准确率的提高可能更为显著。
- en: How it works...
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'The `backoff_tagger` function creates an instance of each tagger class in the
    list, giving it the `train_sents` and the previous tagger as a backoff. The order
    of the list of tagger classes is quite important—the first class in the list will
    be trained first, and be given the initial backoff tagger. This tagger will then
    become the backoff tagger for the next tagger class in the list. The final tagger
    returned will be an instance of the last tagger class in the list. Here''s some
    code to clarify this chain:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '`backoff_tagger`函数在列表中的每个标记器类中创建一个实例，给它`train_sents`和上一个标记器作为回退。标记器类列表的顺序非常重要——列表中的第一个类将首先进行训练，并得到初始回退标记器。然后，该标记器将成为列表中下一个标记器类的回退标记器。返回的最后一个标记器将是列表中最后一个标记器类的实例。以下是一些澄清此链的代码：'
- en: '[PRE14]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: So we end up with a `TrigramTagger`, whose first backoff is a `BigramTagger`.
    Then the next backoff will be a `UnigramTagger`, whose backoff is the `DefaultTagger`.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们最终得到一个`TrigramTagger`，其第一个回退是`BigramTagger`。然后下一个回退将是`UnigramTagger`，其回退是`DefaultTagger`。
- en: There's more...
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The `backoff_tagger` function doesn't just work with `NgramTagger` classes.
    It can be used for constructing a chain containing any subclasses of `SequentialBackoffTagger`.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '`backoff_tagger`函数不仅与`NgramTagger`类一起工作。它可以用于构建包含任何`SequentialBackoffTagger`子类的链。'
- en: '`BigramTagger` and `TrigramTagger`, because they are subclasses of `NgramTagger`
    and `ContextTagger`, can also take a model and cutoff argument, just like the
    `UnigramTagger`. But unlike for `UnigramTagger`, the context keys of the model
    must be 2-tuples, where the first element is a section of the history, and the
    second element is the current token. For the `BigramTagger`, an appropriate context
    key looks like `((prevtag,), word)`, and for `TrigramTagger` it looks like `((prevtag1,
    prevtag2), word)`.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '`BigramTagger`和`TrigramTagger`，因为它们是`NgramTagger`和`ContextTagger`的子类，也可以接受一个模型和截断参数，就像`UnigramTagger`一样。但与`UnigramTagger`不同，模型的上下文键必须是2元组，其中第一个元素是历史的一部分，第二个元素是当前标记。对于`BigramTagger`，适当上下文键看起来像`((prevtag,),
    word)`，而对于`TrigramTagger`，它看起来像`((prevtag1, prevtag2), word)`。'
- en: Quadgram Tagger
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 四元组标记器
- en: The `NgramTagger` class can be used by itself to create a tagger that uses Ngrams
    longer than three for its context key.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`NgramTagger`类可以单独使用来创建一个标记器，该标记器使用超过三个Ngrams作为其上下文键。'
- en: '[PRE15]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'It''s even worse than the `TrigramTagger`! Here''s an alternative implementation
    of a `QuadgramTagger` that we can include in a list to `backoff_tagger`. This
    code can be found in `taggers.py`:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 它甚至比`TrigramTagger`还要糟糕！这里有一个`QuadgramTagger`的替代实现，我们可以将其包含在`backoff_tagger`列表中。此代码可在`taggers.py`中找到：
- en: '[PRE16]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This is essentially how `BigramTagger` and `TrigramTagger` are implemented;
    simple subclasses of `NgramTagger` that pass in the number of *ngrams* to look
    at in the `history` argument of the `context()` method.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上是`BigramTagger`和`TrigramTagger`的实现方式；它们是`NgramTagger`的简单子类，通过在`context()`方法的`history`参数中传递要查看的*ngrams*数量。
- en: 'Now let''s see how it does as part of a backoff chain:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看它作为回退链的一部分是如何表现的：
- en: '[PRE17]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: It's actually slightly worse than before when we stopped with the `TrigramTagger`.
    So the lesson is that too much context can have a negative effect on accuracy.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，当我们停止使用`TrigramTagger`时，它比之前稍微差一点。所以教训是，过多的上下文可能会对准确率产生负面影响。
- en: See also
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: The previous two recipes cover the `UnigramTagger` and backoff tagging.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个配方涵盖了`UnigramTagger`和回退标记。
- en: Creating a model of likely word tags
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建可能的词标记模型
- en: As mentioned earlier in this chapter in the *Training a unigram part-of-speech
    tagger* recipe, using a custom model with a `UnigramTagger` should only be done
    if you know exactly what you are doing. In this recipe, we are going to create
    a model for the most common words, most of which always have the same tag no matter
    what.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如本章 *训练一个一元词性标注器* 菜谱中提到的，使用自定义模型与 `UnigramTagger` 结合使用仅应在确切了解自己在做什么的情况下进行。在这个菜谱中，我们将为最常见的单词创建一个模型，其中大部分单词无论什么情况下总是有相同的标记。
- en: How to do it...
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'To find the most common words, we can use `nltk.probability.FreqDist` to count
    word frequencies in the `treebank` corpus. Then, we can create a `ConditionalFreqDist`
    for tagged words, where we count the frequency of every tag for every word. Using
    these counts, we can construct a model of the 200 most frequent words as keys,
    with the most frequent tag for each word as a value. Here''s the model creation
    function defined in `tag_util.py`:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 要找到最常见的单词，我们可以使用 `nltk.probability.FreqDist` 来统计 `treebank` 语料库中的单词频率。然后，我们可以为标记过的单词创建一个
    `ConditionalFreqDist`，其中我们统计每个单词每个标记的频率。使用这些计数，我们可以构建一个以 200 个最频繁单词作为键，每个单词的最频繁标记作为值的模型。以下是在
    `tag_util.py` 中定义的模型创建函数：
- en: '[PRE18]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'And to use it with a `UnigramTagger`, we can do the following:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 要与 `UnigramTagger` 一起使用，我们可以这样做：
- en: '[PRE19]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'An accuracy of almost 56% is ok, but nowhere near as good as the trained `UnigramTagger`.
    Let''s try adding it to our backoff chain:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率接近 56% 还可以，但远远不如训练好的 `UnigramTagger`。让我们尝试将其添加到我们的回退链中：
- en: '[PRE20]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The final accuracy is exactly the same as without the `likely_tagger`. This
    is because the frequency calculations we did to create the model are almost exactly
    what happens when we train a `UnigramTagger`.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的准确率与没有 `likely_tagger` 时完全相同。这是因为我们为了创建模型而进行的频率计算几乎与训练 `UnigramTagger` 时发生的情况完全相同。
- en: How it works...
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The `word_tag_model()` function takes a list of all words, a list of all tagged
    words, and the maximum number of words we want to use for our model. We give the
    list of words to a `FreqDist`, which counts the frequency of each word. Then we
    get the top 200 words from the `FreqDist` by calling `fd.keys()`, which returns
    all words ordered by highest frequency to lowest. We give the list of tagged words
    to a `ConditionalFreqDist`, which creates a `FreqDist` of tags for each word,
    with the word as the *condition*. Finally, we return a `dict` of the top 200 words
    mapped to their most likely tag.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '`word_tag_model()` 函数接受所有单词的列表、所有标记单词的列表以及我们想要用于模型的最大单词数。我们将单词列表传递给 `FreqDist`，该
    `FreqDist` 统计每个单词的频率。然后，我们通过调用 `fd.keys()` 从 `FreqDist` 中获取前 200 个单词，该函数返回按最高频率到最低频率排序的所有单词。我们将标记单词列表传递给
    `ConditionalFreqDist`，该 `ConditionalFreqDist` 为每个单词创建一个标记的 `FreqDist`，其中单词作为 *条件*。最后，我们返回一个字典，将前
    200 个单词映射到它们最可能的标记。'
- en: There's more...
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'It may seem useless to include this tagger as it does not change the accuracy.
    But the point of this recipe is to demonstrate how to construct a useful model
    for a `UnigramTagger`. Custom model construction is a way to create a manual override
    of trained taggers that are otherwise black boxes. And by putting the likely tagger
    in the front of the chain, we can actually improve accuracy a little bit:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来包含这个标注器似乎没有用处，因为它没有改变准确率。但这个菜谱的目的是演示如何为 `UnigramTagger` 构建一个有用的模型。自定义模型构建是一种创建手动覆盖训练标注器（否则是黑盒子）的方法。通过将可能的标注器放在链的前端，我们实际上可以略微提高准确率：
- en: '[PRE21]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Putting custom model taggers at the front of the backoff chain gives you complete
    control over how specific words are tagged, while letting the trained taggers
    handle everything else.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 将自定义模型标注器放在回退链的前端，可以让你完全控制特定单词的标注方式，同时让训练好的标注器处理其他所有事情。
- en: See also
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: The *Training a unigram part-of-speech tagger* recipe has details on the `UnigramTagger`
    and a simple custom model example. See the earlier recipes *Combining taggers
    with backoff tagging* and *Training and combining Ngram taggers* for details on
    backoff tagging.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '*训练一个一元词性标注器* 菜单中详细介绍了 `UnigramTagger` 和一个简单的自定义模型示例。有关回退标注的详细信息，请参阅之前的菜谱 *结合回退标注的标注器*
    和 *训练和组合 Ngram 标注器*。'
- en: Tagging with regular expressions
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用正则表达式进行标注
- en: You can use regular expression matching to tag words. For example, you can match
    numbers with `\d` to assign the tag **CD** (which refers to a **Cardinal number**).
    Or you could match on known word patterns, such as the suffix "ing". There's lot
    of flexibility here, but be careful of over-specifying since language is naturally
    inexact, and there are always exceptions to the rule.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用正则表达式匹配来标记单词。例如，你可以使用 `\d` 匹配数字，分配标签 **CD**（指的是 **基数词**）。或者你可以匹配已知的单词模式，例如后缀
    "ing"。这里有很多灵活性，但要注意不要过度指定，因为语言本身是不精确的，并且总有例外。
- en: Getting ready
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: For this recipe to make sense, you should be familiar with regular expression
    syntax and Python's `re` module.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这个配方有意义，你应该熟悉正则表达式语法和 Python 的 `re` 模块。
- en: How to do it...
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'The `RegexpTagger` expects a list of 2-tuples, where the first element in the
    tuple is a regular expression, and the second element is the tag. The following
    patterns can be found in `tag_util.py`:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '`RegexpTagger` 预期的是一个包含 2-元组的列表，其中元组的第一个元素是一个正则表达式，第二个元素是标签。以下模式可以在 `tag_util.py`
    中找到：'
- en: '[PRE22]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Once you have constructed this list of patterns, you can pass it into `RegexpTagger`.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦构建了这个模式列表，就可以将其传递给 `RegexpTagger`。
- en: '[PRE23]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: So it's not too great with just a few patterns, but since `RegexpTagger` is
    a subclass of `SequentialBackoffTagger`, it can be useful as part of a backoff
    chain, especially if you are able to come up with more word patterns.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 所以仅凭几个模式并不太出色，但既然 `RegexpTagger` 是 `SequentialBackoffTagger` 的子类，它可以作为回退链的一部分很有用，特别是如果你能够想出更多的单词模式。
- en: How it works...
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The `RegexpTagger` saves the `patterns` given at initialization, then on each
    call to `choose_tag()`, it iterates over the patterns and returns the tag for
    the first expression that matches the current word using `re.match()`. This means
    that if you have two expressions that could match, the tag of the first one will
    always be returned, and the second expression won't even be tried.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '`RegexpTagger` 保存了初始化时给出的 `patterns`，然后在每次调用 `choose_tag()` 时，它会遍历这些模式，并使用
    `re.match()` 返回与当前单词匹配的第一个表达式的标签。这意味着如果有两个表达式可以匹配，第一个表达式的标签将始终返回，第二个表达式甚至不会被尝试。'
- en: There's more...
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: The `RegexpTagger` can replace the `DefaultTagger` if you give it a pattern
    such as `(r'.*', 'NN')`. This pattern should, of course, be last in the list of
    patterns, otherwise no other patterns will match.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你提供一个模式，例如 `(r'.*', 'NN')`，`RegexpTagger` 可以替换 `DefaultTagger`。这个模式当然应该在模式列表的末尾，否则其他模式将不会匹配。
- en: See also
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: In the next recipe, we will cover the `AffixTagger`, which learns how to tag
    based on prefixes and suffixes of words. And see the *Default tagging* recipe
    for details on the `DefaultTagger`.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个配方中，我们将介绍 `AffixTagger`，它学习如何根据单词的前缀和后缀进行标记。并且查看 *默认标记* 配方以了解 `DefaultTagger`
    的详细信息。
- en: Affix tagging
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前缀标记
- en: The `AffixTagger` is another `ContextTagger` subclass, but this time the *context*
    is either the *prefix* or the *suffix* of a word. This means the `AffixTagger`
    is able to learn tags based on fixed-length substrings of the beginning or ending
    of a word.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '`AffixTagger` 是另一个 `ContextTagger` 子类，但这次上下文是单词的 *前缀* 或 *后缀*。这意味着 `AffixTagger`
    能够根据单词开头或结尾的固定长度子串来学习标签。'
- en: How to do it...
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: The default arguments for an `AffixTagger` specify three-character suffixes,
    and that words must be at least five characters long. If a word is less than five
    characters long, then `None` is returned as the tag.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '`AffixTagger` 的默认参数指定了三个字符的后缀，并且单词长度至少为五个字符。如果一个单词的长度小于五个字符，那么返回的标签将是 `None`。'
- en: '[PRE24]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'So it does ok by itself with the default arguments. Let''s try it by specifying
    three-character prefixes:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 所以它使用默认参数本身做得还不错。让我们通过指定三个字符的前缀来试一试：
- en: '[PRE25]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'To learn on two-character suffixes, the code looks like this:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 要在双字符后缀上学习，代码看起来是这样的：
- en: '[PRE26]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: How it works...
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: A positive value for `affix_length` means that the `AffixTagger` will learn
    word prefixes, essentially `word[:affix_length]`. If the `affix_length` is negative,
    then suffixes are learned using `word[affix_length:]`.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '`affix_length` 的正值意味着 `AffixTagger` 将学习单词前缀，本质上就是 `word[:affix_length]`。如果
    `affix_length` 是负值，那么将使用 `word[affix_length:]` 来学习后缀。'
- en: There's more...
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'You can combine multiple affix taggers in a backoff chain if you want to learn
    about multiple character length affixes. Here''s an example of four `AffixTagger`
    classes learning about two and three-character prefixes and suffixes:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想学习多个字符长度的前缀，可以将多个 affix taggers 结合在一个回退链中。以下是一个示例，四个 `AffixTagger` 类学习两个和三个字符的前缀和后缀：
- en: '[PRE27]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: As you can see, the accuracy goes up each time.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，每次训练后准确性都会提高。
- en: Note
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The preceding ordering is not the best, nor is it the worst. I will leave it
    to you to explore the possibilities and discover the best backoff chain of `AffixTagger`
    and `affix_length` values.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的顺序既不是最好的，也不是最差的。我将把它留给你去探索可能性，并发现最佳的 `AffixTagger` 和 `affix_length` 值的回退链。
- en: Min stem length
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最小词干长度
- en: '`AffixTagger` also takes a `min_stem_length` keyword argument with a default
    value of `2`. If the word length is less than `min_stem_length` plus the absolute
    value of `affix_length`, then `None` is returned by the `context()` method. Increasing
    `min_stem_length` forces the `AffixTagger` to only learn on longer words, while
    decreasing `min_stem_length` will allow it to learn on shorter words. Of course,
    for shorter words, the `affix_length` could be equal to or greater than the word
    length, and `AffixTagger` would essentially be acting like a `UnigramTagger`.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '`AffixTagger` 也接受一个具有默认值 `2` 的 `min_stem_length` 关键字参数。如果单词长度小于 `min_stem_length`
    加上 `affix_length` 的绝对值，则 `context()` 方法返回 `None`。增加 `min_stem_length` 会迫使 `AffixTagger`
    只在较长的单词上学习，而减少 `min_stem_length` 将允许它在较短的单词上学习。当然，对于较短的单词，`affix_length` 可能等于或大于单词长度，此时
    `AffixTagger` 实际上就像一个 `UnigramTagger`。'
- en: See also
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: You can manually specify prefixes and suffixes using regular expressions, as
    shown in the previous recipe. The *Training a unigram part-of-speech tagger* and
    *Training and combining Ngram taggers* recipes have details on `NgramTagger` subclasses,
    which are also subclasses of `ContextTagger`.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用正则表达式手动指定前缀和后缀，如前一个食谱所示。`训练单语素词性标注器` 和 `训练和组合 Ngram 标注器` 食谱中有关于 `NgramTagger`
    子类的详细信息，这些子类也是 `ContextTagger` 的子类。
- en: Training a Brill tagger
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练 Brill 标签器
- en: The `BrillTagger` is a transformation-based tagger. It is the first tagger that
    is not a subclass of `SequentialBackoffTagger`. Instead, the `BrillTagger` uses
    a series of rules to correct the results of an *initial tagger*. These rules are
    scored based on how many errors they correct minus the number of new errors they
    produce.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '`BrillTagger` 是一个基于转换的标注器。它是第一个不是 `SequentialBackoffTagger` 子类的标注器。相反，`BrillTagger`
    使用一系列规则来纠正初始标注器的结果。这些规则根据它们纠正的错误数量减去它们产生的错误数量来评分。'
- en: How to do it...
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: Here's a function from `tag_util.py` that trains a `BrillTagger` using `FastBrillTaggerTrainer`.
    It requires an `initial_tagger` and `train_sents`.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个来自 `tag_util.py` 的函数，它使用 `FastBrillTaggerTrainer` 训练 `BrillTagger`。它需要一个
    `initial_tagger` 和一个 `train_sents` 列表。
- en: '[PRE28]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: To use it, we can create our `initial_tagger` from a backoff chain of `NgramTagger`
    classes, then pass that into the `train_brill_tagger()` function to get a `BrillTagger`
    back.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用它，我们可以从 `NgramTagger` 类的回退链中创建我们的 `initial_tagger`，然后将它传递给 `train_brill_tagger()`
    函数以获取 `BrillTagger`。
- en: '[PRE29]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: So the `BrillTagger` has slightly increased accuracy over the `initial_tagger`.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，`BrillTagger` 在 `initial_tagger` 上略微提高了准确性。
- en: How it works...
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何工作...
- en: 'The `FastBrillTaggerTrainer` takes an `initial_tagger` and a list of `templates`.
    These templates must implement the `BrillTemplateI` interface. The two template
    implementations included with NLTK are `ProximateTokensTemplate` and `SymmetricProximateTokensTemplate`.
    Each template is used to generate a list of `BrillRule` subclasses. The actual
    class of the rules produced is passed in to the template at initialization. The
    basic workflow looks like this:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '`FastBrillTaggerTrainer` 接受一个 `initial_tagger` 和一个 `templates` 列表。这些模板必须实现
    `BrillTemplateI` 接口。NLTK 中包含的两个模板实现是 `ProximateTokensTemplate` 和 `SymmetricProximateTokensTemplate`。每个模板都用于生成
    `BrillRule` 子类的列表。实际生成的规则类在初始化时传递给模板。基本工作流程如下：'
- en: '![How it works...](img/3609OS_04_03.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作...](img/3609OS_04_03.jpg)'
- en: The two `BrillRule` subclasses used are `ProximateTagsRule` and `ProximateWordsRule`,
    which are both subclasses of `ProximateTokensRule`. `ProximateTagsRule` looks
    at surrounding tags to do error correction, and `ProximateWordsRule` looks at
    the surrounding words.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 使用的两个 `BrillRule` 子类是 `ProximateTagsRule` 和 `ProximateWordsRule`，它们都是 `ProximateTokensRule`
    的子类。`ProximateTagsRule` 通过查看周围的标签来进行错误纠正，而 `ProximateWordsRule` 通过查看周围的单词。
- en: The *bounds* that we pass in to each template are lists of `(start, end)` tuples
    that get passed in to each rule as *conditions*. The conditions tell the rule
    which tokens it can look at. For example, if the condition is `(1, 1)`, then the
    rule will only look at the next token. But if the condition is `(1, 2)`, then
    the rule will look at both the next token and the token after it. For `(-1, -1)`
    the rule will look only at the previous token.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们传递给每个模板的 *边界* 是 `(start, end)` 元组的列表，这些元组作为 *条件* 传递给每个规则。条件告诉规则它可以查看哪些标记。例如，如果条件是
    `(1, 1)`，则规则将只查看下一个标记。但如果条件是 `(1, 2)`，则规则将查看下一个标记和它后面的标记。对于 `(-1, -1)`，规则将只查看前面的标记。
- en: '`ProximateTokensTemplate` produces `ProximateTokensRule` that look at each
    token for its given conditions to do error correction. Positive and negative conditions
    must be explicitly specified. `SymmetricProximateTokensTemplate`, on the other
    hand, produces pairs of `ProximateTokensRule`, where one rule uses the given conditions,
    and the other rule uses the negative of the conditions. So when we pass a list
    of positive `(start, end)` tuples to a `SymmetricProximateTokensTemplate`, it
    will also produce a `ProximateTokensRule` that uses `(-start, -end`). This is
    why it''s *symmetric*—it produces rules that look on both sides of the token.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '`ProximateTokensTemplate` 生成 `ProximateTokensRule`，它会检查每个标记的给定条件以进行错误纠正。必须显式指定正负条件。另一方面，`SymmetricProximateTokensTemplate`
    生成一对 `ProximateTokensRule`，其中一条规则使用给定条件，另一条规则使用条件的负值。因此，当我们向 `SymmetricProximateTokensTemplate`
    传递一个包含正 `(start, end)` 元组的列表时，它也会生成一个使用 `(-start, -end)` 的 `ProximateTokensRule`。这就是它为什么是
    *对称的*——它生成检查标记两边的规则。'
- en: Note
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Unlike with `ProximateTokensTemplate`, you should not give negative bounds to
    `SymmetricProximateTokensTemplate`, since it will produce those itself. Only use
    positive number bounds with `SymmetricProximateTokensTemplate`.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 与 `ProximateTokensTemplate` 不同，你不应该给 `SymmetricProximateTokensTemplate` 提供负边界，因为它会自行生成。仅使用正数边界与
    `SymmetricProximateTokensTemplate`。
- en: There's more...
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多...
- en: You can control the number of rules generated using the `max_rules` keyword
    argument to the `FastBrillTaggerTrainer.train()` method. The default value is
    `200`. You can also control the quality of rules used with the `min_score` keyword
    argument. The default value is `2`, though `3` can be a good choice as well.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 `FastBrillTaggerTrainer.train()` 方法的 `max_rules` 关键字参数来控制生成的规则数量。默认值是
    `200`。您还可以使用 `min_score` 关键字参数来控制使用的规则质量。默认值是 `2`，尽管 `3` 也是一个不错的选择。
- en: Note
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Increasing `max_rules` or `min_score` will greatly increase training time, without
    necessarily increasing accuracy. Change these values with care.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 增加 `max_rules` 或 `min_score` 将大大增加训练时间，但不一定提高准确性。请谨慎更改这些值。
- en: Tracing
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 跟踪
- en: 'You can watch the `FastBrillTaggerTrainer` do its work by passing `trace=1`
    into the constructor. This can give you output such as:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过将 `trace=1` 传递给构造函数来观察 `FastBrillTaggerTrainer` 的工作。这可以给出如下输出：
- en: '[PRE30]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This means it found `10709` rules with a score of at least `min_score`, and
    then it selects the best rules, keeping no more than `max_rules`.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着它找到了至少 `min_score` 分数的 `10709` 条规则，然后它选择最佳规则，保留不超过 `max_rules`。
- en: The default is `trace=0`, which means the trainer will work silently without
    printing its status.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 默认是 `trace=0`，这意味着训练器将静默工作，不打印其状态。
- en: See also
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考也
- en: The *Training and combining Ngram taggers* recipe details the construction of
    the `initial_tagger` used previously, and the *Default tagging* recipe explains
    the `default_tagger`.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: “训练和组合 Ngram 标签器”配方详细说明了之前使用的 `initial_tagger` 的构建，而“默认标签”配方解释了 `default_tagger`。
- en: Training the TnT tagger
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练 TnT 标签器
- en: '**TnT** stands for **Trigrams''n''Tags**. It is a statistical tagger based
    on second order Markov models. You can read the original paper that lead to the
    implementation at [http://acl.ldc.upenn.edu/A/A00/A00-1031.pdf](http://acl.ldc.upenn.edu/A/A00/A00-1031.pdf).'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '**TnT** 代表 **Trigrams''n''Tags**。它是一个基于二阶马尔可夫模型的统计标签器。您可以在 [http://acl.ldc.upenn.edu/A/A00/A00-1031.pdf](http://acl.ldc.upenn.edu/A/A00/A00-1031.pdf)
    阅读导致其实施的原始论文。'
- en: How to do it...
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'The `TnT` tagger has a slightly different API than previous taggers we have
    encountered. You must explicitly call the `train()` method after you have created
    it. Here''s a basic example:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '`TnT` 标签器的 API 与我们之前遇到的标签器略有不同。创建它之后，你必须显式调用 `train()` 方法。以下是一个基本示例：'
- en: '[PRE31]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: It's quite a good tagger all by itself, only slightly less accurate than the
    `BrillTagger` from the previous recipe. But if you do not call `train()` before
    `evaluate()`, you will get an accuracy of 0%.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 它本身就是一个相当好的标记器，只比之前配方中的 `BrillTagger` 稍微不准确。但如果你在 `evaluate()` 之前没有调用 `train()`，你将得到
    0% 的准确率。
- en: How it works...
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: '`TnT` maintains a number of internal `FreqDist` and `ConditionalFreqDist` instances
    based on the training data. These frequency distributions count unigrams, bigrams,
    and trigrams. Then, during tagging, the frequencies are used to calculate the
    probabilities of possible tags for each word. So instead of constructing a backoff
    chain of `NgramTagger` subclasses, the `TnT` tagger uses all the ngram models
    together to choose the best tag. It also tries to guess the tags for the whole
    sentence at once, by choosing the most likely model for the entire sentence, based
    on the probabilities of each possible tag.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '`TnT` 根据训练数据维护了多个内部 `FreqDist` 和 `ConditionalFreqDist` 实例。这些频率分布计算单语元、双语元和三元语。然后，在标记过程中，使用频率来计算每个单词可能标记的概率。因此，而不是构建
    `NgramTagger` 子类的回退链，`TnT` 标记器使用所有 n-gram 模型一起选择最佳标记。它还尝试一次性猜测整个句子的标签，通过选择整个句子最可能的模型，基于每个可能标记的概率。'
- en: Note
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Training is fairly quick, but tagging is significantly slower than the other
    taggers we have covered. This is due to all the floating point math that must
    be done to calculate the tag probabilities of each word.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 训练相当快，但标记的速度比我们之前提到的其他标记器慢得多。这是因为必须进行所有浮点数学计算，以计算每个单词的标记概率。
- en: There's more...
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: '`TnT` accepts a few optional keyword arguments. You can pass in a tagger for
    unknown words as `unk`. If this tagger is already trained, then you must also
    pass in `Trained=True`. Otherwise it will call `unk.train(data)` with the same
    data you pass in to the `train()` method. Since none of the previous taggers have
    a public `train()` method, we recommend always passing `Trained=True` if you also
    pass an `unk` tagger. Here''s an example using a `DefaultTagger`, which does not
    require any training:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '`TnT` 接受一些可选的关键字参数。你可以传递一个用于未知单词的标记器作为 `unk`。如果这个标记器已经训练过，那么你也必须传递 `Trained=True`。否则，它将使用与传递给
    `train()` 方法的相同数据调用 `unk.train(data)`。由于之前的标记器都没有公开的 `train()` 方法，我们建议如果你也传递了一个
    `unk` 标记器，总是传递 `Trained=True`。以下是一个使用 `DefaultTagger` 的示例，它不需要任何训练：'
- en: '[PRE32]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: So we got an almost 2% increase in accuracy! You must use a tagger that can
    tag a single word without having seen that word before. This is because the unknown
    tagger's `tag()` method is only called with a single word sentence. Other good
    candidates for an unknown tagger are `RegexpTagger` or `AffixTagger`. Passing
    in a `UnigramTagger` that's been trained on the same data is pretty much useless,
    as it will have seen the exact same words, and therefore have the same unknown
    word blind spots.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的准确率提高了近 2%！你必须使用一个可以在之前没有见过该单词的情况下标记单个单词的标记器。这是因为未知标记器的 `tag()` 方法只与单个单词的句子一起调用。其他好的未知标记器候选人是
    `RegexpTagger` 或 `AffixTagger`。传递一个在相同数据上训练过的 `UnigramTagger` 几乎没有用处，因为它将看到完全相同的单词，因此有相同的未知单词盲点。
- en: Controlling the beam search
  id: totrans-222
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 控制束搜索
- en: 'Another parameter you can modify for `TnT` is `N`, which controls the number
    of possible solutions the tagger maintains while trying to guess the tags for
    a sentence. `N` defaults to 1,000\. Increasing it will greatly increase the amount
    of memory used during tagging, without necessarily increasing accuracy. Decreasing
    `N` will decrease memory usage, but could also decrease accuracy. Here''s what
    happens when you set `N=100`:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以修改 `TnT` 的另一个参数是 `N`，它控制标记器在尝试猜测句子标签时保持的可能解决方案的数量。`N` 的默认值是 1,000。增加它将大大增加标记过程中使用的内存量，但并不一定增加准确性。减少
    `N` 将减少内存使用，但可能会降低准确性。以下是将 `N` 设置为 100 时发生的情况：
- en: '[PRE33]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: So the accuracy is exactly the same, but we use significantly less memory to
    achieve it. However, don't assume that accuracy will not change if you decrease
    `N`; experiment with your own data to be sure.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，准确率完全相同，但我们使用显著更少的内存来实现它。然而，不要假设减少 `N` 不会改变准确率；通过在自己的数据上实验来确保。
- en: Capitalization significance
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 大写字母的重要性
- en: You can pass `C=True` if you want capitalization of words to be significant.
    The default is `C=False`, which means all words are lowercased. The documentation
    on `C` says that treating capitalization as significant probably will not increase
    accuracy. In my own testing, there was a very slight (< 0.01%) increase in accuracy
    with `C=True`, probably because case-sensitivity can help identify proper nouns.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你希望单词的大小写有意义，可以传递`C=True`。默认是`C=False`，这意味着所有单词都转换为小写。关于`C`的文档表示，将大小写视为重要可能不会提高准确性。在我的测试中，使用`C=True`时准确性略有增加（<
    0.01%），这可能是由于大小写敏感性可以帮助识别专有名词。
- en: See also
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: We covered the `DefaultTagger` in the *Default tagging* recipe, backoff tagging
    in the *Combining taggers with backoff tagging* recipe, `NgramTagger` subclasses
    in the *Training a unigram part-of-speech tagger* and *Training combining Ngram
    taggers* recipes, `RegexpTagger` in the *Tagging with regular expressions* recipe,
    and the `AffixTagger` in the *Affix tagging* recipe.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*默认标记*配方中介绍了`DefaultTagger`，在*结合标记和回退标记*配方中介绍了回退标记，在*训练单语词性标记器*和*训练组合Ngram标记器*配方中介绍了`NgramTagger`子类，在*使用正则表达式标记*配方中介绍了`RegexpTagger`，在*词缀标记*配方中介绍了`AffixTagger`。
- en: Using WordNet for tagging
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用WordNet进行标记
- en: If you remember from the *Looking up synsets for a word in Wordnet* recipe in
    [Chapter 1](ch01.html "Chapter 1. Tokenizing Text and WordNet Basics"), *Tokenizing
    Text and WordNet Basics*, WordNet synsets specify a part-of-speech tag. It's a
    very restricted set of possible tags, and many words have multiple synsets with
    different part-of-speech tags, but this information can be useful for tagging
    unknown words. WordNet is essentially a giant dictionary, and it's likely to contain
    many words that are not in your training data.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你记得[第1章](ch01.html "第1章. 文本分词和WordNet基础知识")中关于“在Wordnet中查找一个词的synsets”的配方，WordNet
    synsets指定了一个词性标签。这是一个非常有限的标签集合，许多词有多个synsets，具有不同的词性标签，但这些信息对于标记未知词可能是有用的。WordNet本质上是一个巨大的字典，它可能包含许多不在你的训练数据中的词。
- en: Getting ready
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: First, we need to decide how to map WordNet part-of-speech tags to the Penn
    Treebank part-of-speech tags we have been using. The following is a table mapping
    one to the other. See the *Looking up synsets for a word in Wordnet* recipe in
    [Chapter 1](ch01.html "Chapter 1. Tokenizing Text and WordNet Basics"), *Tokenizing
    Text and WordNet Basics* for more details. The "s", which was not shown before,
    is just another kind of adjective, at least for tagging purposes.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要决定如何将WordNet词性标签映射到我们一直在使用的宾州树库词性标签。以下是一个将一个映射到另一个的表格。有关更多详细信息，请参阅[第1章](ch01.html
    "第1章. 文本分词和WordNet基础知识")中关于“在Wordnet中查找一个词的synsets”的配方，*文本分词和WordNet基础知识*。其中，“s”，之前未展示，只是另一种形容词，至少在标记目的上是这样。
- en: '| WordNet Tag | Treebank Tag |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| WordNet 标签 | Treebank 标签 |'
- en: '| --- | --- |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| n | NN |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| n | NN |'
- en: '| a | JJ |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| a | JJ |'
- en: '| s | JJ |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| s | JJ |'
- en: '| r | RB |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| r | RB |'
- en: '| v | VB |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| v | VB |'
- en: How to do it...
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'Now we can create a class that will look up words in WordNet, then chose the
    most common tag from the synsets it finds. The `WordNetTagger` defined next can
    be found in `taggers.py`:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以创建一个类，该类将在WordNet中查找单词，然后从它找到的synsets中选择最常见的标签。下面定义的`WordNetTagger`可以在`taggers.py`中找到：
- en: '[PRE34]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: How it works...
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'The `WordNetTagger` simply counts the number of each part-of-speech tag found
    in the synsets for a word. The most common tag is then mapped to a `treebank`
    tag using an internal mapping. Here''s some sample usage code:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '`WordNetTagger`简单地计算一个词的synsets中每个词性标签的数量。然后，最常见的标签通过内部映射映射到一个`treebank`标签。以下是一些示例用法代码：'
- en: '[PRE35]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: So it's not too accurate, but that's to be expected. We only have enough information
    to produce four different kinds of tags, while there are 36 possible tags in `treebank`.
    And many words can have different part-of-speech tags depending on their context.
    But if we put the `WordNetTagger` at the end of an `NgramTagger` backoff chain,
    then we can improve accuracy over the `DefaultTagger`.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这并不太准确，但这是可以预料的。我们只有足够的信息来产生四种不同的标签，而`treebank`中有36种可能的标签。而且许多词根据其上下文可以有不同的词性标签。但如果我们将`WordNetTagger`放在`NgramTagger`回退链的末尾，那么我们可以提高`DefaultTagger`的准确性。
- en: '[PRE36]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: See also
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: The *Looking up synsets for a word in Wordnet* recipe in [Chapter 1](ch01.html
    "Chapter 1. Tokenizing Text and WordNet Basics"), *Tokenizing Text and WordNet
    Basics* details how to use the `wordnet` corpus and what kinds of part-of-speech
    tags it knows about. And in the *Combining taggers with backoff tagging* and *Training
    and combining Ngram taggers* recipes, we went over backoff tagging with ngram
    taggers.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '第 1 章 [文本分词和 WordNet 基础](ch01.html "第 1 章。文本分词和 WordNet 基础") 中的 *查找 Wordnet
    中的单词同义词集* 配方详细介绍了如何使用 `wordnet` 语料库以及它了解的词性标签类型。在 *结合标签器和回退标记* 和 *训练和组合 Ngram
    标签器* 配方中，我们讨论了使用 ngram 标签器的回退标记。 '
- en: Tagging proper names
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标记专有名词
- en: Using the included `names` corpus, we can create a simple tagger for tagging
    names as *proper nouns*.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 使用包含的 `names` 语料库，我们可以创建一个简单的标签器，用于将名称标记为 *专有名词*。
- en: How to do it...
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'The `NamesTagger` is a subclass of `SequentialBackoffTagger` as it''s probably
    only useful near the end of a backoff chain. At initialization, we create a set
    of all names in the `names` corpus, lowercasing each name to make lookup easier.
    Then we implement the `choose_tag()` method, which simply checks if the current
    word is in the `names_set`. If it is, we return the tag *NNP* (which is the tag
    for *proper nouns*). If it isn''t, we return `None` so the next tagger in the
    chain can tag the word. The following code can be found in `taggers.py`:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '`NamesTagger` 是 `SequentialBackoffTagger` 的子类，因为它可能只在对冲链的末尾有用。在初始化时，我们创建了一个包含
    `names` 语料库中所有名称的集合，并将每个名称转换为小写以简化查找。然后我们实现了 `choose_tag()` 方法，该方法简单地检查当前单词是否在
    `names_set` 中。如果是，我们返回标签 *NNP*（这是 *专有名词* 的标签）。如果不是，我们返回 `None`，以便链中的下一个标签器可以对单词进行标记。以下代码可以在
    `taggers.py` 中找到：'
- en: '[PRE37]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: How it works...
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: '`NamesTagger` should be pretty self-explanatory. Its usage is also simple:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '`NamesTagger` 应该相当直观。其用法也很简单：'
- en: '[PRE38]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: It's probably best to use the `NamesTagger` right before a `DefaultTagger`,
    so it's at the end of a backoff chain. But it could probably go anywhere in the
    chain since it's unlikely to mistag a word.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 可能最好在 `DefaultTagger` 之前使用 `NamesTagger`，因此它位于对冲链的末尾。但它可能可以在链中的任何位置，因为它不太可能错误地标记单词。
- en: See also
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: The *Combining taggers with backoff tagging* recipe goes over the details of
    using `SequentialBackoffTagger` subclasses.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '*结合标签器和回退标记* 的配方详细介绍了使用 `SequentialBackoffTagger` 子类的细节。'
- en: Classifier based tagging
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于分类器的标记
- en: The `ClassifierBasedPOSTagger` uses *classification* to do part-of-speech tagging.
    **Features** are extracted from words, then passed to an internal classifier.
    The classifier classifies the features and returns a label; in this case, a part-of-speech
    tag. Classification will be covered in detail in [Chapter 7](ch07.html "Chapter 7. Text
    Classification"), *Text Classification*.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '`ClassifierBasedPOSTagger` 使用 *分类* 来进行词性标注。**特征**从单词中提取，然后传递给内部分类器。分类器对特征进行分类并返回一个标签；在这种情况下，是一个词性标签。分类将在第
    7 章 [文本分类](ch07.html "第 7 章。文本分类") 中详细介绍。'
- en: '`ClassifierBasedPOSTagger` is a subclass of `ClassifierBasedTagger` that implements
    a **feature detector** that combines many of the techniques of previous taggers
    into a single **feature set** . The feature detector finds multiple length suffixes,
    does some regular expression matching, and looks at the unigram, bigram, and trigram
    history to produce a fairly complete set of features for each word. The feature
    sets it produces are used to train the internal classifier, and are used for classifying
    words into part-of-speech tags.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '`ClassifierBasedPOSTagger` 是 `ClassifierBasedTagger` 的子类，它实现了一个 **特征检测器**，将先前标签器中的许多技术结合成一个单一的
    **特征集**。特征检测器找到多个长度后缀，进行一些正则表达式匹配，并查看一元组、二元组和三元组历史记录以生成每个单词的相当完整的一组特征。它生成的特征集用于训练内部分类器，并用于将单词分类为词性标签。'
- en: How to do it...
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: Basic usage of the `ClassifierBasedPOSTagger` is much like any other `SequentialBackoffTaggger`.
    You pass in training sentences, it trains an internal classifier, and you get
    a very accurate tagger.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '`ClassifierBasedPOSTagger` 的基本用法与其他 `SequentialBackoffTaggger` 类似。您传入训练句子，它训练一个内部分类器，然后您得到一个非常准确的标签器。'
- en: '[PRE39]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Note
  id: totrans-268
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Notice a slight modification to initialization—`train_sents` must be passed
    in as the `train` keyword argument.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 注意初始化的微小修改——`train_sents` 必须作为 `train` 关键字参数传递。
- en: How it works...
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: '`ClassifierBasedPOSTagger` inherits from `ClassifierBasedTagger` and only implements
    a `feature_detector()` method. All the training and tagging is done in `ClassifierBasedTagger`.
    It defaults to training a `NaiveBayesClassifier` with the given training data.
    Once this classifier is trained, it is used to classify word features produced
    by the `feature_detector()` method.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '`ClassifierBasedPOSTagger` 继承自 `ClassifierBasedTagger` 并仅实现一个 `feature_detector()`
    方法。所有的训练和标记都在 `ClassifierBasedTagger` 中完成。它默认使用给定的训练数据训练一个 `NaiveBayesClassifier`。一旦这个分类器被训练，它就被用来分类由
    `feature_detector()` 方法产生的单词特征。'
- en: Note
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The `ClassifierBasedTagger` is often the most accurate tagger, but it's also
    one of the slowest taggers. If speed is an issue, you should stick with a `BrillTagger`
    based on a backoff chain of `NgramTagger` subclasses and other simple taggers.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '`ClassifierBasedTagger` 通常是最准确的标记器，但也是最慢的标记器之一。如果速度是一个问题，你应该坚持使用基于 `NgramTagger`
    子类和其它简单标记器的 `BrillTagger`。'
- en: 'The `ClassifierBasedTagger` also inherits from `FeatursetTaggerI` (which is
    just an empty class), creating an inheritance tree that looks like this:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '`ClassifierBasedTagger` 还继承自 `FeatursetTaggerI`（它只是一个空类），创建了一个看起来像这样的继承树：'
- en: '![How it works...](img/3609OS_04_04.jpg)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![工作原理...](img/3609OS_04_04.jpg)'
- en: There's more...
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'You can use a different classifier instead of `NaiveBayesClassifier` by passing
    in your own `classifier_builder` function. For example, to use a `MaxentClassifier`,
    you would do the following:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过传递自己的 `classifier_builder` 函数来使用不同的分类器而不是 `NaiveBayesClassifier`。例如，要使用
    `MaxentClassifier`，你会这样做：
- en: '[PRE40]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Note
  id: totrans-279
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The `MaxentClassifier` takes even longer to train than `NaiveBayesClassifier`.
    If you have `scipy` and `numpy` installed, training will be faster than normal,
    but still slower than `NaiveBayesClassifier`.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '`MaxentClassifier` 的训练时间甚至比 `NaiveBayesClassifier` 更长。如果你已经安装了 `scipy` 和 `numpy`，训练将比正常更快，但仍然比
    `NaiveBayesClassifier` 慢。'
- en: Custom feature detector
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自定义特征检测器
- en: If you want to do your own feature detection, there are two ways to do it.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想自己进行特征检测，有两种方法可以做到。
- en: Subclass `ClassifierBasedTagger` and implement a `feature_detector()` method.
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 子类 `ClassifierBasedTagger` 并实现一个 `feature_detector()` 方法。
- en: Pass a method as the `feature_detector` keyword argument into `ClassifierBasedTagger`
    at initialization.
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在初始化 `ClassifierBasedTagger` 时，将一个方法作为 `feature_detector` 关键字参数传递。
- en: 'Either way, you need a feature detection method that can take the same arguments
    as `choose_tag()`: `tokens`, `index`, and `history`. But instead of returning
    a tag, you return a `dict` of key-value features, where the key is the feature
    name, and the value is the feature value. A very simple example would be a unigram
    feature detector (found in `tag_util.py`).'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 无论哪种方式，你需要一个可以接受与 `choose_tag()` 相同参数的特征检测方法：`tokens`、`index` 和 `history`。但是，你返回的是一个键值特征的
    `dict`，其中键是特征名称，值是特征值。一个非常简单的例子是一个单语素特征检测器（在 `tag_util.py` 中找到）。
- en: '[PRE41]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Then using the second method, you would pass the following into `ClassifierBasedTagger`
    as `feature_detector`:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用第二种方法，你会将以下内容传递给 `ClassifierBasedTagger` 作为 `feature_detector`：
- en: '[PRE42]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Cutoff probability
  id: totrans-289
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 截断概率
- en: 'Because a classifier will always return the best result it can, passing in
    a backoff tagger is useless unless you also pass in a `cutoff_prob` to specify
    the probability threshold for classification. Then, if the probability of the
    chosen tag is less than `cutoff_prob`, the backoff tagger will be used. Here''s
    an example using the `DefaultTagger` as the `backoff`, and setting `cutoff_prob`
    to `0.3`:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 因为分类器总是会返回它能提供的最佳结果，所以除非你同时传递一个 `cutoff_prob` 来指定分类的概率阈值，否则传递一个回退标记器是没有用的。然后，如果所选标记的概率小于
    `cutoff_prob`，则使用回退标记器。以下是一个使用 `DefaultTagger` 作为回退，并将 `cutoff_prob` 设置为 `0.3`
    的示例：
- en: '[PRE43]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: So we get a slight increase in accuracy if the `ClassifierBasedPOSTagger` uses
    the `DefaultTagger` whenever its tag probability is less than 30%.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果 `ClassifierBasedPOSTagger` 在其标记概率小于 30% 时始终使用 `DefaultTagger`，则我们可以得到略微提高的准确度。
- en: Pre-trained classifier
  id: totrans-293
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预训练分类器
- en: If you want to use a classifier that's already been trained, then you can pass
    that in to `ClassifierBasedTagger` or `ClassifierBasedPOSTagger` as `classifier`.
    In this case, the `classifier_builder` argument is ignored and no training takes
    place. However, you must ensure that the classifier has been trained on and can
    classify feature sets produced by whatever `feature_detector()` method you use.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要使用一个已经训练好的分类器，那么你可以将其传递给 `ClassifierBasedTagger` 或 `ClassifierBasedPOSTagger`
    作为 `classifier`。在这种情况下，`classifier_builder` 参数被忽略，并且不会进行训练。然而，你必须确保该分类器已经使用你使用的任何
    `feature_detector()` 方法训练过，并且可以分类特征集。
- en: See also
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[Chapter 7](ch07.html "Chapter 7. Text Classification"), *Text Classification*
    will cover classification in depth.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '[第7章](ch07.html "第7章。文本分类")，*文本分类*将深入探讨分类问题。'
