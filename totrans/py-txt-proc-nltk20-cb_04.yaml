- en: Chapter 4. Part-of-Speech Tagging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Default tagging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a unigram part-of-speech tagger
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combining taggers with backoff tagging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training and combining Ngram taggers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a model of likely word tags
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tagging with regular expressions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Affix tagging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a Brill tagger
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training the TnT tagger
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using WordNet for tagging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tagging proper names
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifier-based tagging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Part-of-speech tagging** is the process of converting a sentence, in the
    form of a list of words, into a list of tuples, where each tuple is of the form
    `(word, tag)`. The **tag** is a part-of-speech tag and signifies whether the word
    is a noun, adjective, verb, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: Most of the taggers we will cover are *trainable*. They use a list of tagged
    sentences as their training data, such as what you get from the `tagged_sents()`
    function of a `TaggedCorpusReader` (see the *Creating a part-of-speech tagged
    word corpus* recipe in [Chapter 3](ch03.html "Chapter 3. Creating Custom Corpora"),
    *Creating Custom Corpora* for more details). With these training sentences, the
    tagger generates an internal model that will tell them how to tag a word. Other
    taggers use external data sources or match word patterns to choose a tag for a
    word.
  prefs: []
  type: TYPE_NORMAL
- en: All taggers in NLTK are in the `nltk.tag` package and inherit from the `TaggerI`
    base class. `TaggerI` requires all subclasses to implement a `tag()` method, which
    takes a list of words as input, and returns a list of tagged words as output.
    `TaggerI` also provides an `evaluate()` method for evaluating the accuracy of
    the tagger (covered at the end of the *Default tagging* recipe). Many taggers
    can also be combined into a backoff chain, so that if one tagger cannot tag a
    word, the next tagger is used, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Part-of-speech tagging is a necessary step before *chunking*, which is covered
    in [Chapter 5](ch05.html "Chapter 5. Extracting Chunks"), *Extracting Chunks*.
    Without the part-of-speech tags, a chunker cannot know how to extract phrases
    from a sentence. But with part-of-speech tags, you can tell a chunker how to identify
    phrases based on tag patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Default tagging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Default tagging provides a baseline for part-of-speech tagging. It simply assigns
    the same part-of-speech tag to every token. We do this using the `DefaultTagger`.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are going to use the `treebank` corpus for most of this chapter because it's
    a common standard and is quick to load and test. But everything we do should apply
    equally well to `brown`, `conll2000`, and any other part-of-speech tagged corpus.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `DefaultTagger` takes a single argument—the tag you want to apply. We will
    give it `'NN'`, which is the tag for a singular noun.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Every tagger has a `tag()` method that takes a list of tokens, where each token
    is a single word. This list of tokens is usually a list of words produced by a
    word tokenizer (see [Chapter 1](ch01.html "Chapter 1. Tokenizing Text and WordNet
    Basics"), *Tokenizing Text and WordNet Basics* for more on tokenization). As you
    can see, `tag()` returns a list of tagged tokens, where a **tagged token** is
    a tuple of `(word, tag)`.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`DefaultTagger` is a subclass of `SequentialBackoffTagger`. Every subclass
    of `SequentialBackoffTagger` must implement the `choose_tag()` method, which takes
    three arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: The list of `tokens`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `index` of the current token whose tag we want to choose.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `history`, which is a list of the previous tags.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`SequentialBackoffTagger` implements the `tag()` method, which calls the `choose_tag()`
    of the subclass for each index in the tokens list, while accumulating a history
    of the previously tagged tokens. This history is the reason for the *Sequential*
    in `SequentialBackoffTagger`. We will get to the *Backoff* portion of the name
    in the *Combining taggers with backoff tagging* recipe. The following is a diagram
    showing the inheritance tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/3609OS_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The `choose_tag()` method of `DefaultTagger` is very simple—it returns the tag
    we gave it at initialization time. It does not care about the current token or
    the history.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are a lot of different tags you could give to the `DefaultTagger`. You
    can find a complete list of possible tags for the `treebank` corpus at [http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html](http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html).
    These tags are also documented in [Appendix](apa.html "Appendix A. Penn Treebank
    Part-of-Speech Tags"), *Penn Treebank Part-of-Speech Tags*.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating accuracy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To know how accurate a tagger is, you can use the `evaluate()` method, which
    takes a list of tagged tokens as a gold standard to evaluate the tagger. Using
    our default tagger created earlier, we can evaluate it against a subset of the
    `treebank` corpus tagged sentences.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: So by just choosing `'NN'` for every tag, we can achieve 14% accuracy testing
    on ¼th of the `treebank` corpus. We will be reusing these same `test_sents` for
    evaluating more taggers in upcoming recipes.
  prefs: []
  type: TYPE_NORMAL
- en: Batch tagging sentences
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`TaggerI` also implements a `batch_tag()` method that can be used to tag a
    list of sentences, instead of a single sentence. Here''s an example of tagging
    two simple sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The result is a list of two tagged sentences, and of course every tag is `NN`
    because we are using the `DefaultTagger`. The `batch_tag()` method can be quite
    useful if you have many sentences you wish to tag all at once.
  prefs: []
  type: TYPE_NORMAL
- en: Untagging a tagged sentence
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Tagged sentences can be untagged using `nltk.tag.untag()`. Calling this function
    with a tagged sentence will return a list of words without the tags.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For more on tokenization, see [Chapter 1](ch01.html "Chapter 1. Tokenizing Text
    and WordNet Basics"), *Tokenizing Text and WordNet Basics*. And to learn more
    about tagged sentences, see the *Creating a part-of-speech tagged word corpus*
    recipe in [Chapter 3](ch03.html "Chapter 3. Creating Custom Corpora"), *Creating
    Custom Corpora*. For a complete list of part-of-speech tags found in the treebank
    corpus, see [Appendix](apa.html "Appendix A. Penn Treebank Part-of-Speech Tags"),
    *Penn Treebank Part-of-Speech Tags*.
  prefs: []
  type: TYPE_NORMAL
- en: Training a unigram part-of-speech tagger
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **unigram** generally refers to a single token. Therefore, a *unigram tagger*
    only uses a single word as its *context* for determining the part-of-speech tag.
  prefs: []
  type: TYPE_NORMAL
- en: The `UnigramTagger` inherits from `NgramTagger`, which is a subclass of `ContextTagger`,
    which inherits from `SequentialBackoffTagger`. In other words, the `UnigramTagger`
    is a *context-based tagger* whose context is a single word, or unigram.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`UnigramTagger` can be trained by giving it a list of tagged sentences at initialization.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We use the first 3,000 tagged sentences of the `treebank` corpus as the training
    set to initialize the `UnigramTagger`. Then we see the first sentence as a list
    of words, and can see how it is transformed by the `tag()` function into a list
    of tagged tokens.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `UnigramTagger` builds a *context model* from the list of tagged sentences.
    Because `UnigramTagger` inherits from `ContextTagger`, instead of providing a
    `choose_tag()` method, it must implement a `context()` method, which takes the
    same three arguments as `choose_tag()`. The result of `context()` is, in this
    case, the word token. The context token is used to create the model, and also
    to look up the best tag once the model is created. Here''s an inheritance diagram
    showing each class, starting at `SequentialBackoffTagger`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/3609OS_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Let's see how accurate the `UnigramTagger` is on the test sentences (see the
    previous recipe for how `test_sents` is created).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: It has almost 86% accuracy for a tagger that only uses single word lookup to
    determine the part-of-speech tag. All accuracy gains from here on will be much
    smaller.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The model building is actually implemented in `ContextTagger`. Given the list
    of tagged sentences, it calculates the frequency that a tag has occurred for each
    context. The tag with the highest frequency for a context is stored in the model.
  prefs: []
  type: TYPE_NORMAL
- en: Overriding the context model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: All taggers that inherit from `ContextTagger` can take a pre-built model instead
    of training their own. This model is simply a Python `dict` mapping a context
    key to a tag. The context keys will depend on what the `ContextTagger` subclass
    returns from its `context()` method. For `UnigramTagger`, context keys are individual
    words. But for other `NgramTagger` subclasses, the context keys will be tuples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s an example where we pass a very simple model to the `UnigramTagger`
    instead of a training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Since the model only contained the context key, `'Pierre'`, only the first word
    got a tag. Every other word got `None` as the tag since the context word was not
    in the model. So unless you know exactly what you are doing, let the tagger train
    its own model instead of passing in your own.
  prefs: []
  type: TYPE_NORMAL
- en: One good case for passing a self-created model to the `UnigramTagger` is for
    when you have a dictionary of words and tags, and you know that every word should
    always map to its tag. Then, you can put this `UnigramTagger` as your first backoff
    tagger (covered in the next recipe), to look up tags for unambiguous words.
  prefs: []
  type: TYPE_NORMAL
- en: Minimum frequency cutoff
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `ContextTagger` uses frequency of occurrence to decide which tag is most
    likely for a given context. By default, it will do this even if the context word
    and tag occurs only once. If you would like to set a minimum frequency threshold,
    then you can pass a `cutoff` value to the `UnigramTagger`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In this case, using `cutoff=3` has decreased accuracy, but there may be times
    when a cutoff is a good idea.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the next recipe, we will cover backoff tagging to combine taggers. And in
    the *Creating a model of likely word tags* recipe, we will learn how to statistically
    determine tags for very common words.
  prefs: []
  type: TYPE_NORMAL
- en: Combining taggers with backoff tagging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Backoff tagging** is one of the core features of `SequentialBackoffTagger`.
    It allows you to chain taggers together so that if one tagger doesn''t know how
    to tag a word, it can pass the word on to the next backoff tagger. If that one
    can''t do it, it can pass the word on to the next backoff tagger, and so on until
    there are no backoff taggers left to check.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Every subclass of `SequentialBackoffTagger` can take a `backoff` keyword argument
    whose value is another instance of a `SequentialBackoffTagger`. So we will use
    the `DefaultTagger` from the *Default tagging* recipe as the `backoff` to the
    `UnigramTagger` from the *Training a unigram part-of-speech tagger* recipe. Refer
    to both recipes for details on `train_sents` and `test_sents`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: By using a default tag of `NN` whenever the `UnigramTagger` is unable to tag
    a word, we have increased the accuracy by almost 2%!
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When a `SequentialBackoffTagger` is initialized, it creates an internal list
    of backoff taggers with itself as the first element. If a `backoff` tagger is
    given, then the backoff tagger''s internal list of taggers is appended. Here''s
    some code to illustrate this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The `_taggers` is the internal list of backoff taggers that the `SequentialBackoffTagger`
    uses when the `tag()` method is called. It goes through its list of taggers, calling
    `choose_tag()` on each one. As soon as a tag is found, it stops and returns that
    tag. This means that if the primary tagger can tag the word, then that's the tag
    that will be returned. But if it returns `None`, then the next tagger is tried,
    and so on until a tag is found, or else `None` is returned. Of course, `None`
    will never be returned if your final backoff tagger is a `DefaultTagger`.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While most of the taggers included in NLTK are subclasses of `SequentialBackoffTagger`,
    not all of them are. There's a few taggers that we will cover in later recipes
    that cannot be used as part of a backoff tagging chain, such as the `BrillTagger`.
    However, these taggers generally take another tagger to use as a baseline, and
    a `SequentialBackoffTagger` is often a good choice for that baseline.
  prefs: []
  type: TYPE_NORMAL
- en: Pickling and unpickling a trained tagger
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Since training a tagger can take a while, and you generally only need to do
    the training once, pickling a trained tagger is a useful way to save it for later
    usage. If your trained tagger is called `tagger`, then here''s how to dump and
    load it with `pickle`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: If your tagger pickle file is located in a NLTK data directory, you could also
    use `nltk.data.load('tagger.pickle')` to load the tagger.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the next recipe, we will combine more taggers with backoff tagging. Also
    see the previous two recipes for details on the `DefaultTagger` and `UnigramTagger`.
  prefs: []
  type: TYPE_NORMAL
- en: Training and combining Ngram taggers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In addition to `UnigramTagger`, there are two more `NgramTagger` subclasses:
    `BigramTagger` and `TrigramTagger` . `BigramTagger` uses the previous tag as part
    of its context, while `TrigramTagger` uses the previous two tags. An **ngram**
    is a subsequence of *n* items, so the `BigramTagger` looks at two items (the previous
    tag and word), and the `TrigramTagger` looks at three items.'
  prefs: []
  type: TYPE_NORMAL
- en: These two taggers are good at handling words whose part-of-speech tag is context
    dependent. Many words have a different part-of-speech depending on how they are
    used. For example, we have been talking about taggers that "tag" words. In this
    case, "tag" is used as a verb. But the result of tagging is a part-of-speech tag,
    so "tag" can also be a noun. The idea with the `NgramTagger` subclasses is that
    by looking at the previous words and part-of-speech tags, we can better guess
    the part-of-speech tag for the current word.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Refer to the first two recipes of this chapter for details on constructing `train_sents`
    and `test_sents`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By themselves, `BigramTagger` and `TrigramTagger` perform quite poorly. This
    is partly because they cannot learn context from the first word(s) in a sentence.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Where they can make a contribution is when we combine them with backoff tagging.
    This time, instead of creating each tagger individually, we will create a function
    that will take `train_sents`, a list of `SequentialBackoffTagger` classes, and
    an optional final backoff tagger, and then train each tagger with the previous
    tagger as a backoff. Here''s code from `tag_util.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'And to use it, we can do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: So we have gained almost 1% accuracy by including the `BigramTagger` and `TrigramTagger`
    in the backoff chain. For corpora other than `treebank`, the accuracy gain may
    be more significant.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `backoff_tagger` function creates an instance of each tagger class in the
    list, giving it the `train_sents` and the previous tagger as a backoff. The order
    of the list of tagger classes is quite important—the first class in the list will
    be trained first, and be given the initial backoff tagger. This tagger will then
    become the backoff tagger for the next tagger class in the list. The final tagger
    returned will be an instance of the last tagger class in the list. Here''s some
    code to clarify this chain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: So we end up with a `TrigramTagger`, whose first backoff is a `BigramTagger`.
    Then the next backoff will be a `UnigramTagger`, whose backoff is the `DefaultTagger`.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `backoff_tagger` function doesn't just work with `NgramTagger` classes.
    It can be used for constructing a chain containing any subclasses of `SequentialBackoffTagger`.
  prefs: []
  type: TYPE_NORMAL
- en: '`BigramTagger` and `TrigramTagger`, because they are subclasses of `NgramTagger`
    and `ContextTagger`, can also take a model and cutoff argument, just like the
    `UnigramTagger`. But unlike for `UnigramTagger`, the context keys of the model
    must be 2-tuples, where the first element is a section of the history, and the
    second element is the current token. For the `BigramTagger`, an appropriate context
    key looks like `((prevtag,), word)`, and for `TrigramTagger` it looks like `((prevtag1,
    prevtag2), word)`.'
  prefs: []
  type: TYPE_NORMAL
- en: Quadgram Tagger
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `NgramTagger` class can be used by itself to create a tagger that uses Ngrams
    longer than three for its context key.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s even worse than the `TrigramTagger`! Here''s an alternative implementation
    of a `QuadgramTagger` that we can include in a list to `backoff_tagger`. This
    code can be found in `taggers.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This is essentially how `BigramTagger` and `TrigramTagger` are implemented;
    simple subclasses of `NgramTagger` that pass in the number of *ngrams* to look
    at in the `history` argument of the `context()` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s see how it does as part of a backoff chain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: It's actually slightly worse than before when we stopped with the `TrigramTagger`.
    So the lesson is that too much context can have a negative effect on accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The previous two recipes cover the `UnigramTagger` and backoff tagging.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a model of likely word tags
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned earlier in this chapter in the *Training a unigram part-of-speech
    tagger* recipe, using a custom model with a `UnigramTagger` should only be done
    if you know exactly what you are doing. In this recipe, we are going to create
    a model for the most common words, most of which always have the same tag no matter
    what.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To find the most common words, we can use `nltk.probability.FreqDist` to count
    word frequencies in the `treebank` corpus. Then, we can create a `ConditionalFreqDist`
    for tagged words, where we count the frequency of every tag for every word. Using
    these counts, we can construct a model of the 200 most frequent words as keys,
    with the most frequent tag for each word as a value. Here''s the model creation
    function defined in `tag_util.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'And to use it with a `UnigramTagger`, we can do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'An accuracy of almost 56% is ok, but nowhere near as good as the trained `UnigramTagger`.
    Let''s try adding it to our backoff chain:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The final accuracy is exactly the same as without the `likely_tagger`. This
    is because the frequency calculations we did to create the model are almost exactly
    what happens when we train a `UnigramTagger`.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `word_tag_model()` function takes a list of all words, a list of all tagged
    words, and the maximum number of words we want to use for our model. We give the
    list of words to a `FreqDist`, which counts the frequency of each word. Then we
    get the top 200 words from the `FreqDist` by calling `fd.keys()`, which returns
    all words ordered by highest frequency to lowest. We give the list of tagged words
    to a `ConditionalFreqDist`, which creates a `FreqDist` of tags for each word,
    with the word as the *condition*. Finally, we return a `dict` of the top 200 words
    mapped to their most likely tag.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It may seem useless to include this tagger as it does not change the accuracy.
    But the point of this recipe is to demonstrate how to construct a useful model
    for a `UnigramTagger`. Custom model construction is a way to create a manual override
    of trained taggers that are otherwise black boxes. And by putting the likely tagger
    in the front of the chain, we can actually improve accuracy a little bit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Putting custom model taggers at the front of the backoff chain gives you complete
    control over how specific words are tagged, while letting the trained taggers
    handle everything else.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Training a unigram part-of-speech tagger* recipe has details on the `UnigramTagger`
    and a simple custom model example. See the earlier recipes *Combining taggers
    with backoff tagging* and *Training and combining Ngram taggers* for details on
    backoff tagging.
  prefs: []
  type: TYPE_NORMAL
- en: Tagging with regular expressions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can use regular expression matching to tag words. For example, you can match
    numbers with `\d` to assign the tag **CD** (which refers to a **Cardinal number**).
    Or you could match on known word patterns, such as the suffix "ing". There's lot
    of flexibility here, but be careful of over-specifying since language is naturally
    inexact, and there are always exceptions to the rule.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this recipe to make sense, you should be familiar with regular expression
    syntax and Python's `re` module.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `RegexpTagger` expects a list of 2-tuples, where the first element in the
    tuple is a regular expression, and the second element is the tag. The following
    patterns can be found in `tag_util.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Once you have constructed this list of patterns, you can pass it into `RegexpTagger`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: So it's not too great with just a few patterns, but since `RegexpTagger` is
    a subclass of `SequentialBackoffTagger`, it can be useful as part of a backoff
    chain, especially if you are able to come up with more word patterns.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `RegexpTagger` saves the `patterns` given at initialization, then on each
    call to `choose_tag()`, it iterates over the patterns and returns the tag for
    the first expression that matches the current word using `re.match()`. This means
    that if you have two expressions that could match, the tag of the first one will
    always be returned, and the second expression won't even be tried.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `RegexpTagger` can replace the `DefaultTagger` if you give it a pattern
    such as `(r'.*', 'NN')`. This pattern should, of course, be last in the list of
    patterns, otherwise no other patterns will match.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the next recipe, we will cover the `AffixTagger`, which learns how to tag
    based on prefixes and suffixes of words. And see the *Default tagging* recipe
    for details on the `DefaultTagger`.
  prefs: []
  type: TYPE_NORMAL
- en: Affix tagging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `AffixTagger` is another `ContextTagger` subclass, but this time the *context*
    is either the *prefix* or the *suffix* of a word. This means the `AffixTagger`
    is able to learn tags based on fixed-length substrings of the beginning or ending
    of a word.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The default arguments for an `AffixTagger` specify three-character suffixes,
    and that words must be at least five characters long. If a word is less than five
    characters long, then `None` is returned as the tag.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'So it does ok by itself with the default arguments. Let''s try it by specifying
    three-character prefixes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'To learn on two-character suffixes, the code looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A positive value for `affix_length` means that the `AffixTagger` will learn
    word prefixes, essentially `word[:affix_length]`. If the `affix_length` is negative,
    then suffixes are learned using `word[affix_length:]`.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can combine multiple affix taggers in a backoff chain if you want to learn
    about multiple character length affixes. Here''s an example of four `AffixTagger`
    classes learning about two and three-character prefixes and suffixes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the accuracy goes up each time.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The preceding ordering is not the best, nor is it the worst. I will leave it
    to you to explore the possibilities and discover the best backoff chain of `AffixTagger`
    and `affix_length` values.
  prefs: []
  type: TYPE_NORMAL
- en: Min stem length
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`AffixTagger` also takes a `min_stem_length` keyword argument with a default
    value of `2`. If the word length is less than `min_stem_length` plus the absolute
    value of `affix_length`, then `None` is returned by the `context()` method. Increasing
    `min_stem_length` forces the `AffixTagger` to only learn on longer words, while
    decreasing `min_stem_length` will allow it to learn on shorter words. Of course,
    for shorter words, the `affix_length` could be equal to or greater than the word
    length, and `AffixTagger` would essentially be acting like a `UnigramTagger`.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can manually specify prefixes and suffixes using regular expressions, as
    shown in the previous recipe. The *Training a unigram part-of-speech tagger* and
    *Training and combining Ngram taggers* recipes have details on `NgramTagger` subclasses,
    which are also subclasses of `ContextTagger`.
  prefs: []
  type: TYPE_NORMAL
- en: Training a Brill tagger
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `BrillTagger` is a transformation-based tagger. It is the first tagger that
    is not a subclass of `SequentialBackoffTagger`. Instead, the `BrillTagger` uses
    a series of rules to correct the results of an *initial tagger*. These rules are
    scored based on how many errors they correct minus the number of new errors they
    produce.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here's a function from `tag_util.py` that trains a `BrillTagger` using `FastBrillTaggerTrainer`.
    It requires an `initial_tagger` and `train_sents`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: To use it, we can create our `initial_tagger` from a backoff chain of `NgramTagger`
    classes, then pass that into the `train_brill_tagger()` function to get a `BrillTagger`
    back.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: So the `BrillTagger` has slightly increased accuracy over the `initial_tagger`.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `FastBrillTaggerTrainer` takes an `initial_tagger` and a list of `templates`.
    These templates must implement the `BrillTemplateI` interface. The two template
    implementations included with NLTK are `ProximateTokensTemplate` and `SymmetricProximateTokensTemplate`.
    Each template is used to generate a list of `BrillRule` subclasses. The actual
    class of the rules produced is passed in to the template at initialization. The
    basic workflow looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/3609OS_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The two `BrillRule` subclasses used are `ProximateTagsRule` and `ProximateWordsRule`,
    which are both subclasses of `ProximateTokensRule`. `ProximateTagsRule` looks
    at surrounding tags to do error correction, and `ProximateWordsRule` looks at
    the surrounding words.
  prefs: []
  type: TYPE_NORMAL
- en: The *bounds* that we pass in to each template are lists of `(start, end)` tuples
    that get passed in to each rule as *conditions*. The conditions tell the rule
    which tokens it can look at. For example, if the condition is `(1, 1)`, then the
    rule will only look at the next token. But if the condition is `(1, 2)`, then
    the rule will look at both the next token and the token after it. For `(-1, -1)`
    the rule will look only at the previous token.
  prefs: []
  type: TYPE_NORMAL
- en: '`ProximateTokensTemplate` produces `ProximateTokensRule` that look at each
    token for its given conditions to do error correction. Positive and negative conditions
    must be explicitly specified. `SymmetricProximateTokensTemplate`, on the other
    hand, produces pairs of `ProximateTokensRule`, where one rule uses the given conditions,
    and the other rule uses the negative of the conditions. So when we pass a list
    of positive `(start, end)` tuples to a `SymmetricProximateTokensTemplate`, it
    will also produce a `ProximateTokensRule` that uses `(-start, -end`). This is
    why it''s *symmetric*—it produces rules that look on both sides of the token.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unlike with `ProximateTokensTemplate`, you should not give negative bounds to
    `SymmetricProximateTokensTemplate`, since it will produce those itself. Only use
    positive number bounds with `SymmetricProximateTokensTemplate`.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can control the number of rules generated using the `max_rules` keyword
    argument to the `FastBrillTaggerTrainer.train()` method. The default value is
    `200`. You can also control the quality of rules used with the `min_score` keyword
    argument. The default value is `2`, though `3` can be a good choice as well.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Increasing `max_rules` or `min_score` will greatly increase training time, without
    necessarily increasing accuracy. Change these values with care.
  prefs: []
  type: TYPE_NORMAL
- en: Tracing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can watch the `FastBrillTaggerTrainer` do its work by passing `trace=1`
    into the constructor. This can give you output such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: This means it found `10709` rules with a score of at least `min_score`, and
    then it selects the best rules, keeping no more than `max_rules`.
  prefs: []
  type: TYPE_NORMAL
- en: The default is `trace=0`, which means the trainer will work silently without
    printing its status.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Training and combining Ngram taggers* recipe details the construction of
    the `initial_tagger` used previously, and the *Default tagging* recipe explains
    the `default_tagger`.
  prefs: []
  type: TYPE_NORMAL
- en: Training the TnT tagger
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**TnT** stands for **Trigrams''n''Tags**. It is a statistical tagger based
    on second order Markov models. You can read the original paper that lead to the
    implementation at [http://acl.ldc.upenn.edu/A/A00/A00-1031.pdf](http://acl.ldc.upenn.edu/A/A00/A00-1031.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `TnT` tagger has a slightly different API than previous taggers we have
    encountered. You must explicitly call the `train()` method after you have created
    it. Here''s a basic example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: It's quite a good tagger all by itself, only slightly less accurate than the
    `BrillTagger` from the previous recipe. But if you do not call `train()` before
    `evaluate()`, you will get an accuracy of 0%.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`TnT` maintains a number of internal `FreqDist` and `ConditionalFreqDist` instances
    based on the training data. These frequency distributions count unigrams, bigrams,
    and trigrams. Then, during tagging, the frequencies are used to calculate the
    probabilities of possible tags for each word. So instead of constructing a backoff
    chain of `NgramTagger` subclasses, the `TnT` tagger uses all the ngram models
    together to choose the best tag. It also tries to guess the tags for the whole
    sentence at once, by choosing the most likely model for the entire sentence, based
    on the probabilities of each possible tag.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Training is fairly quick, but tagging is significantly slower than the other
    taggers we have covered. This is due to all the floating point math that must
    be done to calculate the tag probabilities of each word.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`TnT` accepts a few optional keyword arguments. You can pass in a tagger for
    unknown words as `unk`. If this tagger is already trained, then you must also
    pass in `Trained=True`. Otherwise it will call `unk.train(data)` with the same
    data you pass in to the `train()` method. Since none of the previous taggers have
    a public `train()` method, we recommend always passing `Trained=True` if you also
    pass an `unk` tagger. Here''s an example using a `DefaultTagger`, which does not
    require any training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: So we got an almost 2% increase in accuracy! You must use a tagger that can
    tag a single word without having seen that word before. This is because the unknown
    tagger's `tag()` method is only called with a single word sentence. Other good
    candidates for an unknown tagger are `RegexpTagger` or `AffixTagger`. Passing
    in a `UnigramTagger` that's been trained on the same data is pretty much useless,
    as it will have seen the exact same words, and therefore have the same unknown
    word blind spots.
  prefs: []
  type: TYPE_NORMAL
- en: Controlling the beam search
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Another parameter you can modify for `TnT` is `N`, which controls the number
    of possible solutions the tagger maintains while trying to guess the tags for
    a sentence. `N` defaults to 1,000\. Increasing it will greatly increase the amount
    of memory used during tagging, without necessarily increasing accuracy. Decreasing
    `N` will decrease memory usage, but could also decrease accuracy. Here''s what
    happens when you set `N=100`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: So the accuracy is exactly the same, but we use significantly less memory to
    achieve it. However, don't assume that accuracy will not change if you decrease
    `N`; experiment with your own data to be sure.
  prefs: []
  type: TYPE_NORMAL
- en: Capitalization significance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can pass `C=True` if you want capitalization of words to be significant.
    The default is `C=False`, which means all words are lowercased. The documentation
    on `C` says that treating capitalization as significant probably will not increase
    accuracy. In my own testing, there was a very slight (< 0.01%) increase in accuracy
    with `C=True`, probably because case-sensitivity can help identify proper nouns.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We covered the `DefaultTagger` in the *Default tagging* recipe, backoff tagging
    in the *Combining taggers with backoff tagging* recipe, `NgramTagger` subclasses
    in the *Training a unigram part-of-speech tagger* and *Training combining Ngram
    taggers* recipes, `RegexpTagger` in the *Tagging with regular expressions* recipe,
    and the `AffixTagger` in the *Affix tagging* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Using WordNet for tagging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you remember from the *Looking up synsets for a word in Wordnet* recipe in
    [Chapter 1](ch01.html "Chapter 1. Tokenizing Text and WordNet Basics"), *Tokenizing
    Text and WordNet Basics*, WordNet synsets specify a part-of-speech tag. It's a
    very restricted set of possible tags, and many words have multiple synsets with
    different part-of-speech tags, but this information can be useful for tagging
    unknown words. WordNet is essentially a giant dictionary, and it's likely to contain
    many words that are not in your training data.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, we need to decide how to map WordNet part-of-speech tags to the Penn
    Treebank part-of-speech tags we have been using. The following is a table mapping
    one to the other. See the *Looking up synsets for a word in Wordnet* recipe in
    [Chapter 1](ch01.html "Chapter 1. Tokenizing Text and WordNet Basics"), *Tokenizing
    Text and WordNet Basics* for more details. The "s", which was not shown before,
    is just another kind of adjective, at least for tagging purposes.
  prefs: []
  type: TYPE_NORMAL
- en: '| WordNet Tag | Treebank Tag |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| n | NN |'
  prefs: []
  type: TYPE_TB
- en: '| a | JJ |'
  prefs: []
  type: TYPE_TB
- en: '| s | JJ |'
  prefs: []
  type: TYPE_TB
- en: '| r | RB |'
  prefs: []
  type: TYPE_TB
- en: '| v | VB |'
  prefs: []
  type: TYPE_TB
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now we can create a class that will look up words in WordNet, then chose the
    most common tag from the synsets it finds. The `WordNetTagger` defined next can
    be found in `taggers.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `WordNetTagger` simply counts the number of each part-of-speech tag found
    in the synsets for a word. The most common tag is then mapped to a `treebank`
    tag using an internal mapping. Here''s some sample usage code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: So it's not too accurate, but that's to be expected. We only have enough information
    to produce four different kinds of tags, while there are 36 possible tags in `treebank`.
    And many words can have different part-of-speech tags depending on their context.
    But if we put the `WordNetTagger` at the end of an `NgramTagger` backoff chain,
    then we can improve accuracy over the `DefaultTagger`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Looking up synsets for a word in Wordnet* recipe in [Chapter 1](ch01.html
    "Chapter 1. Tokenizing Text and WordNet Basics"), *Tokenizing Text and WordNet
    Basics* details how to use the `wordnet` corpus and what kinds of part-of-speech
    tags it knows about. And in the *Combining taggers with backoff tagging* and *Training
    and combining Ngram taggers* recipes, we went over backoff tagging with ngram
    taggers.
  prefs: []
  type: TYPE_NORMAL
- en: Tagging proper names
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using the included `names` corpus, we can create a simple tagger for tagging
    names as *proper nouns*.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `NamesTagger` is a subclass of `SequentialBackoffTagger` as it''s probably
    only useful near the end of a backoff chain. At initialization, we create a set
    of all names in the `names` corpus, lowercasing each name to make lookup easier.
    Then we implement the `choose_tag()` method, which simply checks if the current
    word is in the `names_set`. If it is, we return the tag *NNP* (which is the tag
    for *proper nouns*). If it isn''t, we return `None` so the next tagger in the
    chain can tag the word. The following code can be found in `taggers.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`NamesTagger` should be pretty self-explanatory. Its usage is also simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: It's probably best to use the `NamesTagger` right before a `DefaultTagger`,
    so it's at the end of a backoff chain. But it could probably go anywhere in the
    chain since it's unlikely to mistag a word.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Combining taggers with backoff tagging* recipe goes over the details of
    using `SequentialBackoffTagger` subclasses.
  prefs: []
  type: TYPE_NORMAL
- en: Classifier based tagging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `ClassifierBasedPOSTagger` uses *classification* to do part-of-speech tagging.
    **Features** are extracted from words, then passed to an internal classifier.
    The classifier classifies the features and returns a label; in this case, a part-of-speech
    tag. Classification will be covered in detail in [Chapter 7](ch07.html "Chapter 7. Text
    Classification"), *Text Classification*.
  prefs: []
  type: TYPE_NORMAL
- en: '`ClassifierBasedPOSTagger` is a subclass of `ClassifierBasedTagger` that implements
    a **feature detector** that combines many of the techniques of previous taggers
    into a single **feature set** . The feature detector finds multiple length suffixes,
    does some regular expression matching, and looks at the unigram, bigram, and trigram
    history to produce a fairly complete set of features for each word. The feature
    sets it produces are used to train the internal classifier, and are used for classifying
    words into part-of-speech tags.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Basic usage of the `ClassifierBasedPOSTagger` is much like any other `SequentialBackoffTaggger`.
    You pass in training sentences, it trains an internal classifier, and you get
    a very accurate tagger.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Notice a slight modification to initialization—`train_sents` must be passed
    in as the `train` keyword argument.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`ClassifierBasedPOSTagger` inherits from `ClassifierBasedTagger` and only implements
    a `feature_detector()` method. All the training and tagging is done in `ClassifierBasedTagger`.
    It defaults to training a `NaiveBayesClassifier` with the given training data.
    Once this classifier is trained, it is used to classify word features produced
    by the `feature_detector()` method.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `ClassifierBasedTagger` is often the most accurate tagger, but it's also
    one of the slowest taggers. If speed is an issue, you should stick with a `BrillTagger`
    based on a backoff chain of `NgramTagger` subclasses and other simple taggers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `ClassifierBasedTagger` also inherits from `FeatursetTaggerI` (which is
    just an empty class), creating an inheritance tree that looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/3609OS_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can use a different classifier instead of `NaiveBayesClassifier` by passing
    in your own `classifier_builder` function. For example, to use a `MaxentClassifier`,
    you would do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `MaxentClassifier` takes even longer to train than `NaiveBayesClassifier`.
    If you have `scipy` and `numpy` installed, training will be faster than normal,
    but still slower than `NaiveBayesClassifier`.
  prefs: []
  type: TYPE_NORMAL
- en: Custom feature detector
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you want to do your own feature detection, there are two ways to do it.
  prefs: []
  type: TYPE_NORMAL
- en: Subclass `ClassifierBasedTagger` and implement a `feature_detector()` method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pass a method as the `feature_detector` keyword argument into `ClassifierBasedTagger`
    at initialization.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Either way, you need a feature detection method that can take the same arguments
    as `choose_tag()`: `tokens`, `index`, and `history`. But instead of returning
    a tag, you return a `dict` of key-value features, where the key is the feature
    name, and the value is the feature value. A very simple example would be a unigram
    feature detector (found in `tag_util.py`).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Then using the second method, you would pass the following into `ClassifierBasedTagger`
    as `feature_detector`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Cutoff probability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Because a classifier will always return the best result it can, passing in
    a backoff tagger is useless unless you also pass in a `cutoff_prob` to specify
    the probability threshold for classification. Then, if the probability of the
    chosen tag is less than `cutoff_prob`, the backoff tagger will be used. Here''s
    an example using the `DefaultTagger` as the `backoff`, and setting `cutoff_prob`
    to `0.3`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: So we get a slight increase in accuracy if the `ClassifierBasedPOSTagger` uses
    the `DefaultTagger` whenever its tag probability is less than 30%.
  prefs: []
  type: TYPE_NORMAL
- en: Pre-trained classifier
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you want to use a classifier that's already been trained, then you can pass
    that in to `ClassifierBasedTagger` or `ClassifierBasedPOSTagger` as `classifier`.
    In this case, the `classifier_builder` argument is ignored and no training takes
    place. However, you must ensure that the classifier has been trained on and can
    classify feature sets produced by whatever `feature_detector()` method you use.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Chapter 7](ch07.html "Chapter 7. Text Classification"), *Text Classification*
    will cover classification in depth.'
  prefs: []
  type: TYPE_NORMAL
