<html><head></head><body>
  <div id="sbo-rt-content"><div class="chapter" title="Chapter 4. Prepare Data for Modeling"><div class="titlepage"><div><div><h1 class="title"><a id="ch04"/>Chapter 4. Prepare Data for Modeling</h1></div></div></div><p>All data is dirty, irrespective of what the source of the data might lead you to believe: it might be your colleague, a telemetry system that monitors your environment, a dataset you download from the web, or some other source. Until you have tested and proven to yourself that your data is in a clean state (we will get to what clean state means in a second), you should neither trust it nor use it for modeling.</p><p>Your data can be stained with duplicates, missing observations and outliers, non-existent addresses, wrong phone numbers and area codes, inaccurate geographical coordinates, wrong dates, incorrect labels, mixtures of upper and lower cases, trailing spaces, and many other more subtle problems. It is your job to clean it, irrespective of whether you are a data scientist or data engineer, so you can build a statistical or machine learning model.</p><p>Your dataset is considered technically clean if none of the aforementioned problems can be found. However, to clean the dataset for modeling purposes, you also need to check the distributions of your features and confirm they fit the predefined criteria.</p><p>As a data scientist, you can expect to spend 80-90% of your time <span class="emphasis"><em>massaging</em></span> your data and getting familiar with all the features. This chapter will guide you through that process, leveraging Spark capabilities.</p><p>In this chapter, you will learn how to do the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Recognize and handle duplicates, missing observations, and outliers</li><li class="listitem" style="list-style-type: disc">Calculate descriptive statistics and correlations</li><li class="listitem" style="list-style-type: disc">Visualize your data with matplotlib and Bokeh</li></ul></div><div class="section" title="Checking for duplicates, missing observations, and outliers"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec28"/>Checking for duplicates, missing observations, and outliers</h1></div></div></div><p>Until you have fully tested the data and proven it worthy of your time, you should neither trust it nor use it. In this section, we will show you how to deal with duplicates, missing observations, and outliers.</p><div class="section" title="Duplicates"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec47"/>Duplicates</h2></div></div></div><p>Duplicates are <a id="id183" class="indexterm"/>observations that appear as distinct rows in your dataset, but which, upon closer inspection, look the same. That is, if you looked at them side by side, all the features in these two (or more) rows would have exactly the same values.</p><p>On the other hand, if your data has some form of an ID to distinguish between records (or associate them with certain users, for example), then what might initially appear as a duplicate may not be; sometimes systems fail and produce erroneous IDs. In such a situation, you need to either check whether the same ID is a real duplicate, or you need to come up with a new ID system.</p><p>Consider the following example:</p><div class="informalexample"><pre class="programlisting">df = spark.createDataFrame([
        (1, 144.5, 5.9, 33, 'M'),
        (2, 167.2, 5.4, 45, 'M'),
        (3, 124.1, 5.2, 23, 'F'),
        (4, 144.5, 5.9, 33, 'M'),
        (5, 133.2, 5.7, 54, 'F'),
        (3, 124.1, 5.2, 23, 'F'),
        (5, 129.2, 5.3, 42, 'M'),
    ], ['id', 'weight', 'height', 'age', 'gender'])</pre></div><p>As you can see, we have several issues here:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">We have two rows with IDs equal to <code class="literal">3</code> and they are exactly the same</li><li class="listitem" style="list-style-type: disc">Rows with IDs <code class="literal">1</code> and <code class="literal">4</code> are the same — the only thing that's different is their IDs, so we can safely assume that they are the same person</li><li class="listitem" style="list-style-type: disc">We have two rows with IDs equal to <code class="literal">5</code>, but that seems to be a recording issue, as they do not seem to be the same person</li></ul></div><p>This is a very easy dataset with only seven rows. What do you do when you have millions of observations? The first thing I normally do is to check if I have any duplicates: I compare the counts of the full dataset with the one that I get after running a <code class="literal">.distinct()</code> method:</p><div class="informalexample"><pre class="programlisting">print('Count of rows: {0}'.format(df.count()))
print('Count of distinct rows: {0}'.format(df.distinct().count()))</pre></div><p>Here's what <a id="id184" class="indexterm"/>you get back for our DataFrame:</p><div class="mediaobject"><img src="images/B05793_04_01.jpg" alt="Duplicates"/></div><p>If these two numbers differ, then you know you have, what I like to call, pure duplicates: rows that are exact copies of each other. We can drop these rows by using the <code class="literal">.dropDuplicates(...)</code> method:</p><div class="informalexample"><pre class="programlisting">df = df.dropDuplicates()</pre></div><p>Our dataset will then look as follows (once you run <code class="literal">df.show()</code>):</p><div class="mediaobject"><img src="images/B05793_04_02.jpg" alt="Duplicates"/></div><p>We dropped one of the rows with ID <code class="literal">3</code>. Now let's check whether there are any duplicates in the data irrespective of ID. We can quickly repeat what we have done earlier, but using only columns other than the ID column:</p><div class="informalexample"><pre class="programlisting">print('Count of ids: {0}'.format(df.count()))
print('Count of distinct ids: {0}'.format(
    df.select([
        c for c in df.columns if c != 'id'
    ]).distinct().count())
)</pre></div><p>We should see one more row that is a duplicate:</p><div class="mediaobject"><img src="images/B05793_04_03.jpg" alt="Duplicates"/></div><p>We can still <a id="id185" class="indexterm"/>use the <code class="literal">.dropDuplicates(...)</code>, but will add the <code class="literal">subset</code> parameter that specifies only the columns other than the <code class="literal">id</code> column:</p><div class="informalexample"><pre class="programlisting">df = df.dropDuplicates(subset=[
    c for c in df.columns if c != 'id'
])</pre></div><p>The <code class="literal">subset </code>parameter instructs the <code class="literal">.dropDuplicates(...) </code>method to look for duplicated rows using only the columns specified via the <code class="literal">subset</code> parameter; in the preceding example, we will drop the duplicated records with the same <code class="literal">weight</code>, <code class="literal">height</code>, <code class="literal">age</code>, and <code class="literal">gender</code> but not <code class="literal">id</code>. Running the <code class="literal">df.show()</code>, we get the following cleaner dataset as we dropped the row with <code class="literal">id = 1 </code>since it was identical to the record with <code class="literal">id = 4</code>:</p><div class="mediaobject"><img src="images/B05793_04_04.jpg" alt="Duplicates"/></div><p>Now that we know there are no full rows duplicated, or any identical rows differing only by ID, let's check if there are any duplicated IDs. To calculate the total and distinct number of IDs in one step, we can use the <code class="literal">.agg(...)</code> method:</p><div class="informalexample"><pre class="programlisting">import pyspark.sql.functions as fn

df.agg(
    fn.count('id').alias('count'),
    fn.countDistinct('id').alias('distinct')
).show()</pre></div><p>Here's the output of the preceding code:</p><div class="mediaobject"><img src="images/B05793_04_05.jpg" alt="Duplicates"/></div><p>In the previous example, we first import all the functions from the <code class="literal">pyspark.sql</code> module.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip19"/>Tip</h3><p>This gives us access to a vast array of various functions, too many to list here. However, we <a id="id186" class="indexterm"/>strongly encourage you to study the PySpark's documentation at <a class="ulink" href="http://spark.apache.org/docs/2.0.0/api/python/pyspark.sql.html#module-pyspark.sql.functions">http://spark.apache.org/docs/2.0.0/api/python/pyspark.sql.html#module-pyspark.sql.functions</a>.</p></div></div><p>Next, we use the <code class="literal">.count(...)</code> and <code class="literal">.countDistinct(...)</code> to, respectively, calculate the number of rows <a id="id187" class="indexterm"/>and the number of distinct <code class="literal">ids</code> in our DataFrame. The <code class="literal">.alias(...)</code> method allows us to specify a friendly name to the returned column.</p><p>As you can see, we have five rows in total, but only four distinct IDs. Since we have already dropped all the duplicates, we can safely assume that this might just be a fluke in our ID data, so we will give each row a unique ID:</p><div class="informalexample"><pre class="programlisting">df.withColumn('new_id', fn.monotonically_increasing_id()).show()</pre></div><p>The preceding code snippet produced the following output:</p><div class="mediaobject"><img src="images/B05793_04_06.jpg" alt="Duplicates"/></div><p>The <code class="literal">.monotonicallymonotonically_increasing_id()</code> method gives each record a unique and increasing ID. According to the documentation, as long as your data is put into less than<a id="id188" class="indexterm"/> roughly 1 billion partitions with less than 8 billions records in each, the ID is guaranteed to be unique.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note34"/>Note</h3><p>A word of caution: in earlier versions of Spark the <code class="literal">.monotonicallymonotonically_increasing_id() </code>method would not necessarily return the same IDs across multiple evaluations of the same DataFrame. This, however, has been fixed in Spark 2.0.</p></div></div></div><div class="section" title="Missing observations"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec48"/>Missing observations</h2></div></div></div><p>You will frequently encounter datasets with <span class="emphasis"><em>blanks</em></span> in them. The missing values can happen for a<a id="id189" class="indexterm"/> variety of reasons: systems failure, people error, data schema changes, just to name a few.</p><p>The simplest way to deal with missing values, if your data can afford it, is to drop the whole observation when any missing value is found. You have to be careful not to drop too many: depending on the distribution of the missing values across your dataset it might severely affect the usability of your dataset. If, after dropping the rows, I end up with a very small dataset, or find that the reduction in data size is more than 50%, I start checking my data to see what features have the most holes in them and perhaps exclude those altogether; if a feature has most of its values missing (unless a missing value bears a meaning), from a modeling point of view, it is fairly useless.</p><p>The other way to deal with the observations with missing values is to impute some value in place of those <code class="literal">Nones</code>. Given the type of your data, you have several options to choose from:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">If your data is a discrete Boolean, you can turn it into a categorical variable by adding a third category — <code class="literal">Missing</code></li><li class="listitem" style="list-style-type: disc">If your data is already categorical, you can simply extend the number of levels and add the <code class="literal">Missing</code> category as well</li><li class="listitem" style="list-style-type: disc">If you're dealing with ordinal or numerical data, you can impute either mean, median, or some other predefined value (for example, first or third quartile, depending on the distribution shape of your data)</li></ul></div><p>Consider a similar example to the one we presented previously:</p><div class="informalexample"><pre class="programlisting">df_miss = spark.createDataFrame([         (1, 143.5, 5.6, 28,   'M',  100000),
        (2, 167.2, 5.4, 45,   'M',  None),
        (3, None , 5.2, None, None, None),
        (4, 144.5, 5.9, 33,   'M',  None),
        (5, 133.2, 5.7, 54,   'F',  None),
        (6, 124.1, 5.2, None, 'F',  None),
        (7, 129.2, 5.3, 42,   'M',  76000),
    ], ['id', 'weight', 'height', 'age', 'gender', 'income'])</pre></div><p>In our example, we<a id="id190" class="indexterm"/> deal with a number of missing values categories.</p><p>Analyzing <span class="emphasis"><em>rows</em></span>, we see the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The row with ID <code class="literal">3</code> has only one useful piece of information—the <code class="literal">height</code></li><li class="listitem" style="list-style-type: disc">The row with ID <code class="literal">6</code> has only one missing value—the <code class="literal">age</code></li></ul></div><p>Analyzing <span class="emphasis"><em>columns</em></span>, we can see the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <code class="literal">income</code> column, since it is a very personal thing to disclose, has most of its values missing</li><li class="listitem" style="list-style-type: disc">The <code class="literal">weight</code> and <code class="literal">gender</code> columns have only one missing value each</li><li class="listitem" style="list-style-type: disc">The <code class="literal">age</code> column has two missing values</li></ul></div><p>To find the number of missing observations per row, we can use the following snippet:</p><div class="informalexample"><pre class="programlisting">df_miss.rdd.map(
    lambda row: (row['id'], sum([c == None for c in row]))
).collect()</pre></div><p>It produces the following output:</p><div class="mediaobject"><img src="images/B05793_04_07.jpg" alt="Missing observations"/></div><p>It tells us that, for example, the row with ID <code class="literal">3</code> has four missing observations, as we observed earlier.</p><p>Let's see what values are missing so that when we count missing observations in columns, we can decide whether to drop the observation altogether or impute some of the observations:</p><div class="informalexample"><pre class="programlisting">df_miss.where('id == 3').show()</pre></div><p>Here's what we get:</p><div class="mediaobject"><img src="images/B05793_04_08.jpg" alt="Missing observations"/></div><p>Let's now check<a id="id191" class="indexterm"/> what percentage of missing observations are there in each column:</p><div class="informalexample"><pre class="programlisting">df_miss.agg(*[
    (1 - (fn.count(c) / fn.count('*'))).alias(c + '_missing')
    for c in df_miss.columns
]).show()</pre></div><p>This generates the following output:</p><div class="mediaobject"><img src="images/B05793_04_09.jpg" alt="Missing observations"/></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note35"/>Note</h3><p>The <code class="literal">*</code> argument to the <code class="literal">.count(...)</code> method (in place of a column name) instructs the method to count all rows. On the other hand, the <code class="literal">*</code> preceding the list declaration instructs the <code class="literal">.agg(...)</code> method to treat the list as a set of separate parameters passed to the function.</p></div></div><p>So, we have 14% of missing observations in the <code class="literal">weight</code> and <code class="literal">gender</code> columns, twice as much in the <code class="literal">height</code> column, and almost 72% of missing observations in the <code class="literal">income</code> column. Now we know what to do.</p><p>First, we will drop the <code class="literal">'income' </code>feature, as most of its values are missing.</p><div class="informalexample"><pre class="programlisting">df_miss_no_income = df_miss.select([
    c for c in df_miss.columns if c != 'income'
])</pre></div><p>We now see that we do not need to drop the row with ID <code class="literal">3</code> as the coverage in the <code class="literal">'weight'</code> and <code class="literal">'age'</code> columns has enough observations (in our simplified example) to calculate the mean and impute it in the place of the missing values.</p><p>However, if you<a id="id192" class="indexterm"/> decide to drop the observations instead, you can use the <code class="literal">.dropna(...)</code> method, as shown here. Here, we will also use the <code class="literal">thresh</code> parameter, which allows us to specify a threshold on the number of missing observations per row that would qualify the row to be dropped. This is useful if you have a dataset with tens or hundreds of features and you only want to drop those rows that exceed a certain threshold of missing values:</p><div class="informalexample"><pre class="programlisting">df_miss_no_income.dropna(thresh=3).show()</pre></div><p>The preceding code produces the following output:</p><div class="mediaobject"><img src="images/B05793_04_10.jpg" alt="Missing observations"/></div><p>On the other hand, if you wanted to impute the observations, you can use the <code class="literal">.fillna(...)</code> method. This method accepts a single integer (long is also accepted), float, or string; all missing values in the whole dataset will then be filled in with that value. You can also pass a dictionary of a form <code class="literal">{'&lt;colName&gt;': &lt;value_to_impute&gt;}</code>. This has the same limitation, in that, as the <code class="literal">&lt;value_to_impute&gt;</code>, you can only pass an integer, float, or string.</p><p>If you want to impute a mean, median, or other calculated value, you need to first calculate the value, create a dictionary with such values, and then pass it to the <code class="literal">.fillna(...)</code> method.</p><p>Here's how we do it:</p><div class="informalexample"><pre class="programlisting">means = df_miss_no_income.agg(
    *[fn.mean(c).alias(c) 
        for c in df_miss_no_income.columns if c != 'gender']
).toPandas().to_dict('records')[0]

means['gender'] = 'missing'

df_miss_no_income.fillna(means).show()</pre></div><p>The <a id="id193" class="indexterm"/>preceding code will produce the following output:</p><div class="mediaobject"><img src="images/B05793_04_11.jpg" alt="Missing observations"/></div><p>We omit the <code class="literal">gender</code> column as one cannot calculate a mean of a categorical variable, obviously.</p><p>We use a double conversion here. Taking the output of the <code class="literal">.agg(...)</code> method (a PySpark DataFrame), we first convert it into a pandas' DataFrame and then once more to a dictionary.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip20"/>Tip</h3><p>Note that calling the <code class="literal">.toPandas()</code> can be problematic, as the method works essentially in the same way as <code class="literal">.collect()</code> in RDDs. It collects all the information from the workers and brings it over to the driver. It is unlikely to be a problem with the preceding dataset, unless you have thousands upon thousands of features.</p></div></div><p>The <code class="literal">records</code> parameter to the <code class="literal">.to_dict(...)</code> method of pandas instructs it to create the following dictionary:</p><div class="mediaobject"><img src="images/B05793_04_12.jpg" alt="Missing observations"/></div><p>Since we <a id="id194" class="indexterm"/>cannot calculate the average (or any other numeric metric of a categorical variable), we added the <code class="literal">missing</code> category to the dictionary for the <code class="literal">gender</code> feature. Note that, even though the mean of the age column is 40.40, when imputed, the type of the <code class="literal">df_miss_no_income.age</code> column was preserved—it is still an integer.</p></div><div class="section" title="Outliers"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec49"/>Outliers</h2></div></div></div><p>Outliers are those observations that deviate significantly from the distribution of the rest of your <a id="id195" class="indexterm"/>sample. The definitions of <span class="emphasis"><em>significance</em></span> vary, but in the most general form, you can accept that there are no outliers if all the values are roughly within the Q1−1.5IQR and Q3+1.5IQR range, where IQR is the interquartile range; the IQR is defined as a difference between the upper- and lower-quartiles, that is, the 75th percentile (the Q3) and 25th percentile (the Q1), respectively.</p><p>Let's, again, consider a simple example:</p><div class="informalexample"><pre class="programlisting">df_outliers = spark.createDataFrame([
        (1, 143.5, 5.3, 28),
        (2, 154.2, 5.5, 45),
        (3, 342.3, 5.1, 99),
        (4, 144.5, 5.5, 33),
        (5, 133.2, 5.4, 54),
        (6, 124.1, 5.1, 21),
        (7, 129.2, 5.3, 42),
    ], ['id', 'weight', 'height', 'age'])</pre></div><p>Now we can use the definition we outlined previously to flag the outliers.</p><p>First, we calculate the lower and upper cut off points for each feature. We will use the <code class="literal">.approxQuantile(...)</code> method. The first parameter specified is the name of the column, the second parameter can be either a number between <code class="literal">0</code> or <code class="literal">1</code> (where <code class="literal">0.5</code> means to calculated median) or a list (as in our case), and the third parameter specifies the acceptable level of an error for each metric (if set to <code class="literal">0</code>, it will calculate an exact value for the metric, but it can be really expensive to do so):</p><div class="informalexample"><pre class="programlisting">cols = ['weight', 'height', 'age']
bounds = {}

for col in cols:
    quantiles = df_outliers.approxQuantile(
        col, [0.25, 0.75], 0.05
    )
    
    IQR = quantiles[1] - quantiles[0]
    
    bounds[col] = [
        quantiles[0] - 1.5 * IQR, 
        quantiles[1] + 1.5 * IQR
]</pre></div><p>The <code class="literal">bounds</code> dictionary<a id="id196" class="indexterm"/> holds the lower and upper bounds for each feature:</p><div class="mediaobject"><img src="images/B05793_04_13.jpg" alt="Outliers"/></div><p>Let's now use it to flag our outliers:</p><div class="informalexample"><pre class="programlisting">outliers = df_outliers.select(*['id'] + [
    (
        (df_outliers[c] &lt; bounds[c][0]) | 
        (df_outliers[c] &gt; bounds[c][1])
    ).alias(c + '_o') for c in cols
])
outliers.show()</pre></div><p>The preceding code produces the following output:</p><div class="mediaobject"><img src="images/B05793_04_14.jpg" alt="Outliers"/></div><p>We have two<a id="id197" class="indexterm"/> outliers in the <code class="literal">weight</code> feature and two in the <code class="literal">age</code> feature. By now you should know how to extract these, but here is a snippet that lists the values significantly differing from the rest of the distribution:</p><div class="informalexample"><pre class="programlisting">df_outliers = df_outliers.join(outliers, on='id')
df_outliers.filter('weight_o').select('id', 'weight').show()
df_outliers.filter('age_o').select('id', 'age').show()</pre></div><p>The preceding code will give you the following output:</p><div class="mediaobject"><img src="images/B05793_04_15.jpg" alt="Outliers"/></div><p>Equipped with<a id="id198" class="indexterm"/> the methods described in this section, you can quickly clean up even the biggest of datasets.</p></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Getting familiar with your data"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec29"/>Getting familiar with your data</h1></div></div></div><p>Although we <a id="id199" class="indexterm"/>would strongly discourage such behavior, you can build a model without knowing your data; it will most likely take you longer, and the quality of the resulting model might be less than optimal, but it is doable.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note36"/>Note</h3><p>In this section, <a id="id200" class="indexterm"/>we will use the dataset we downloaded from <a class="ulink" href="http://packages.revolutionanalytics.com/datasets/ccFraud.csv">http://packages.revolutionanalytics.com/datasets/ccFraud.csv</a>. We did not alter the dataset itself, but it was GZipped and uploaded to <a class="ulink" href="http://tomdrabas.com/data/LearningPySpark/ccFraud.csv.gz">http://tomdrabas.com/data/LearningPySpark/ccFraud.csv.gz</a>. Please download the file first and save it in the same folder that contains your notebook for this chapter.</p></div></div><p>The head of the dataset looks as follows:</p><div class="mediaobject"><img src="images/B05793_04_16.jpg" alt="Getting familiar with your data"/></div><p>Thus, any serious data scientist or data modeler will become acquainted with the dataset before starting any modeling. As a first thing, we normally start with some descriptive statistics to get a feeling for what we are dealing with.</p><div class="section" title="Descriptive statistics"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec50"/>Descriptive statistics</h2></div></div></div><p>Descriptive statistics, in the simplest sense, will tell you the basic information about your dataset: how <a id="id201" class="indexterm"/>many non-missing observations there <a id="id202" class="indexterm"/>are in your dataset, the mean and the standard deviation for the column, as well as the min and max values.</p><p>However, first things first—let's load our data and convert it to a Spark DataFrame:</p><div class="informalexample"><pre class="programlisting">import pyspark.sql.types as typ</pre></div><p>First, we load the only module we will need. The <code class="literal">pyspark.sql.types</code> exposes all the data types we can use, such as <code class="literal">IntegerType() </code>or <code class="literal">FloatType()</code>.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note37"/>Note</h3><p>For a <a id="id203" class="indexterm"/>full list of available types check <a class="ulink" href="http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types">http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types</a>.</p></div></div><p>Next, we <a id="id204" class="indexterm"/>read the data in and remove the header line <a id="id205" class="indexterm"/>using the <code class="literal">.filter(...)</code> method. This is followed by splitting the row on each comma (since this is a <code class="literal">.csv</code> file) and converting each element to an integer:</p><div class="informalexample"><pre class="programlisting">fraud = sc.textFile('ccFraud.csv.gz')
header = fraud.first()

fraud = fraud \
    .filter(lambda row: row != header) \
    .map(lambda row: [int(elem) for elem in row.split(',')])</pre></div><p>Next, we create the schema for our DataFrame:</p><div class="informalexample"><pre class="programlisting">fields = [
    *[
        typ.StructField(h[1:-1], typ.IntegerType(), True)
        for h in header.split(',')
    ]
]
schema = typ.StructType(fields)</pre></div><p>Finally, we create our DataFrame:</p><div class="informalexample"><pre class="programlisting">fraud_df = spark.createDataFrame(fraud, schema)</pre></div><p>Having created our <code class="literal">fraud_df</code> DataFrame, we can calculate the basic descriptive statistics for our dataset. However, you need to remember that even though all of our features appear as numeric in nature, some of them are categorical (for example, <code class="literal">gender</code> or <code class="literal">state</code>).</p><p>Here's the schema of our DataFrame:</p><div class="informalexample"><pre class="programlisting">fraud_df.printSchema()</pre></div><p>The representation is shown here:</p><div class="mediaobject"><img src="images/B05793_04_17.jpg" alt="Descriptive statistics"/></div><p>Also, no information would be gained from calculating the mean and standard deviation of the <code class="literal">custId </code>column, so we will not be doing that.</p><p>For a better<a id="id206" class="indexterm"/> understanding of categorical columns, we will <a id="id207" class="indexterm"/>count the frequencies of their values using the <code class="literal">.groupby(...)</code> method. In this example, we will count the frequencies of the <code class="literal">gender</code> column:</p><div class="informalexample"><pre class="programlisting">fraud_df.groupby('gender').count().show()</pre></div><p>The preceding code will produce the following output:</p><div class="mediaobject"><img src="images/B05793_04_18.jpg" alt="Descriptive statistics"/></div><p>As you can see, we are dealing with a fairly imbalanced dataset. What you would expect to see is an equal distribution for both genders.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note38"/>Note</h3><p>It goes beyond the scope of this chapter, but if we were building a statistical model, you<a id="id208" class="indexterm"/> would need to take care of these kinds of biases. You can read more at <a class="ulink" href="http://www.va.gov/VETDATA/docs/SurveysAndStudies/SAMPLE_WEIGHT.pdf">http://www.va.gov/VETDATA/docs/SurveysAndStudies/SAMPLE_WEIGHT.pdf</a>.</p></div></div><p>For the truly<a id="id209" class="indexterm"/> numerical features, we can use the <code class="literal">.describe()</code> method:</p><div class="informalexample"><pre class="programlisting">numerical = ['balance', 'numTrans', 'numIntlTrans']
desc = fraud_df.describe(numerical)
desc.show()</pre></div><p>The <code class="literal">.show() </code>method <a id="id210" class="indexterm"/>will produce the following output:</p><div class="mediaobject"><img src="images/B05793_04_19.jpg" alt="Descriptive statistics"/></div><p>Even from these relatively few numbers we can tell quite a bit:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">All of the features are positively skewed. The maximum values are a number of times larger than the average.</li><li class="listitem" style="list-style-type: disc">The coefficient of variation (the ratio of mean to standard deviation) is very high (close or greater than <code class="literal">1</code>), suggesting a wide spread of observations.</li></ul></div><p>Here's how you check the <code class="literal">skeweness</code> (we will do it for the <code class="literal">'balance'</code> feature only):</p><div class="informalexample"><pre class="programlisting">fraud_df.agg({'balance': 'skewness'}).show()</pre></div><p>The preceding code produces the following output:</p><div class="mediaobject"><img src="images/B05793_04_20.jpg" alt="Descriptive statistics"/></div><p>A list<a id="id211" class="indexterm"/> of aggregation functions (the names are<a id="id212" class="indexterm"/> fairly self-explanatory) includes: <code class="literal">avg()</code>,<code class="literal"> count()</code>, <code class="literal">countDistinct()</code>, <code class="literal">first()</code>, <code class="literal">kurtosis()</code>, <code class="literal">max()</code>, <code class="literal">mean()</code>, <code class="literal">min()</code>, <code class="literal">skewness()</code>, <code class="literal">stddev()</code>, <code class="literal">stddev_pop()</code>, <code class="literal">stddev_samp()</code>, <code class="literal">sum()</code>, <code class="literal">sumDistinct()</code>, <code class="literal">var_pop()</code>, <code class="literal">var_samp()</code> and <code class="literal">variance()</code>.</p></div><div class="section" title="Correlations"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec51"/>Correlations</h2></div></div></div><p>Another highly <a id="id213" class="indexterm"/>useful measure of mutual relationships<a id="id214" class="indexterm"/> between features is correlation. Your model would normally include only those features that are highly correlated with your target. However, it is almost equally important to check the correlation between the features; including features that are highly correlated among them (that is, are <span class="emphasis"><em>collinear</em></span>) may lead to unpredictable behavior of your model, or might unnecessarily complicate it.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note39"/>Note</h3><p>I talk more about multicollinearity in my other book, <span class="emphasis"><em>Practical Data Analysis Cookbook, Packt Publishing</em></span> (<a class="ulink" href="https://www.packtpub.com/big-data-and-business-intelligence/practical-data-analysis-cookbook)">https://www.packtpub.com/big-data-and-business-intelligence/practical-data-analysis-cookbook)</a>, in <a class="link" href="ch05.html" title="Chapter 5. Introducing MLlib">Chapter 5</a>, <span class="emphasis"><em>Introducing MLlib</em></span>, under the section titled <span class="emphasis"><em>Identifying and tackling multicollinearity</em></span>.</p></div></div><p>Calculating correlations in PySpark is very easy once your data is in a DataFrame form. The only difficulties are that the <code class="literal">.corr(...)</code> method supports the Pearson correlation coefficient at the moment, and it can only calculate pairwise correlations, such as the following:</p><div class="informalexample"><pre class="programlisting">fraud_df.corr('balance', 'numTrans')</pre></div><p>In order to create a correlations matrix, you can use the following script:</p><div class="informalexample"><pre class="programlisting">n_numerical = len(numerical)

corr = []

for i in range(0, n_numerical):
    temp = [None] * i
    
    for j in range(i, n_numerical):
        temp.append(fraud_df.corr(numerical[i], numerical[j]))
    corr.append(temp)</pre></div><p>The preceding code will create the following output:</p><div class="mediaobject"><img src="images/B05793_04_21.jpg" alt="Correlations"/></div><p>As you can see, the correlations between the numerical features in the credit card fraud dataset are pretty much non-existent. Thus, all these features can be used in our models, should they<a id="id215" class="indexterm"/> turn out to be statistically sound in explaining our <a id="id216" class="indexterm"/>target.</p><p>Having checked the correlations, we can now move on to visually inspecting our data.</p></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Visualization"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec30"/>Visualization</h1></div></div></div><p>There are multiple visualization packages, but in this section we will be using <code class="literal">matplotlib</code> and Bokeh <a id="id217" class="indexterm"/>exclusively to give you the best tools for your needs.</p><p>Both of the packages come preinstalled with Anaconda. First, let's load the modules and set them up:</p><div class="informalexample"><pre class="programlisting">%matplotlib inline
import matplotlib.pyplot as plt
plt.style.use('ggplot')

import bokeh.charts as chrt
from bokeh.io import output_notebook

output_notebook()</pre></div><p>The <code class="literal">%matplotlib inline</code> and the <code class="literal">output_notebook()</code> commands will make every chart generated with <code class="literal">matplotlib</code> or Bokeh, respectively, appear within the notebook and not as a separate window.</p><div class="section" title="Histograms"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec52"/>Histograms</h2></div></div></div><p>Histograms <a id="id218" class="indexterm"/>are by far the easiest way to visually gauge the distribution <a id="id219" class="indexterm"/>of your features. There are three ways you can generate histograms in PySpark (or a Jupyter notebook):</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Aggregate the data in workers and return an aggregated list of bins and counts in each bin of the histogram to the driver</li><li class="listitem" style="list-style-type: disc">Return all the data points to the driver and allow the plotting libraries' methods to do the job for you</li><li class="listitem" style="list-style-type: disc">Sample your data and then return them to the driver for plotting.</li></ul></div><p>If the number<a id="id220" class="indexterm"/> of rows in your dataset is counted in billions, then the <a id="id221" class="indexterm"/>second option might not be attainable. Thus, you need to aggregate the data first:</p><div class="informalexample"><pre class="programlisting">hists = fraud_df.select('balance').rdd.flatMap(
    lambda row: row
).histogram(20)</pre></div><p>To plot the histogram, you can simply call <code class="literal">matplotlib</code>, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">data = {
    'bins': hists[0][:-1],
    'freq': hists[1]
}
plt.bar(data['bins'], data['freq'], width=2000)
plt.title('Histogram of \'balance\'')</pre></div><p>This will produce the following chart:</p><div class="mediaobject"><img src="images/B05793_04_22.jpg" alt="Histograms"/></div><p>In <a id="id222" class="indexterm"/>a similar<a id="id223" class="indexterm"/> manner, a histogram can be created with Bokeh:</p><div class="informalexample"><pre class="programlisting">b_hist = chrt.Bar(
    data, 
    values='freq', label='bins', 
    title='Histogram of \'balance\'')
chrt.show(b_hist)</pre></div><p>Since Bokeh uses D3.js in the background, the resulting chart is interactive:</p><div class="mediaobject"><img src="images/B05793_04_23.jpg" alt="Histograms"/></div><p>If your data <a id="id224" class="indexterm"/>is small enough to fit on the driver (although we would <a id="id225" class="indexterm"/>argue it would normally be faster to use the previous method), you can bring the data and use the <code class="literal">.hist(...)</code> (from <code class="literal">matplotlib</code>) or <code class="literal">.Histogram(...)</code> (from Bokeh) methods:</p><div class="informalexample"><pre class="programlisting">data_driver = {
    'obs': fraud_df.select('balance').rdd.flatMap(
        lambda row: row
    ).collect()
}
plt.hist(data_driver['obs'], bins=20)
plt.title('Histogram of \'balance\' using .hist()')
b_hist_driver = chrt.Histogram(
    data_driver, values='obs', 
    title='Histogram of \'balance\' using .Histogram()', 
    bins=20
)
chrt.show(b_hist_driver)</pre></div><p>This will <a id="id226" class="indexterm"/>produce<a id="id227" class="indexterm"/> the following chart for <code class="literal">matplotlib</code>:</p><div class="mediaobject"><img src="images/B05793_04_24.jpg" alt="Histograms"/></div><p>For Bokeh, the following chart will be generated:</p><div class="mediaobject"><img src="images/B05793_04_25.jpg" alt="Histograms"/></div></div><div class="section" title="Interactions between features"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec53"/>Interactions between features</h2></div></div></div><p>Scatter charts<a id="id228" class="indexterm"/> allow us to visualize interactions between up to three variables at a time (although we will be only presenting a 2D interaction in this section).</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip21"/>Tip</h3><p>You should rarely revert to 3D visualizations unless you are dealing with some temporal data and you want to observe changes over time. Even then, we would rather discretize the time data and present a series of 2D charts, as interpreting 3D charts is somewhat more complicated and (most of the time) confusing.</p></div></div><p>Since PySpark does not offer any visualization modules on the server side, and trying to plot billions of observations at the same time would be highly impractical, in this section we will sample the dataset at 0.02% (roughly 2,000 observations).</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip22"/>Tip</h3><p>Unless you chose a stratified sampling, you should create at least three to five samples at a predefined sampling fraction so you can check if your sample is somewhat representative of your dataset—that is, that the differences between your samples are not big.</p></div></div><p>In this example, we <a id="id229" class="indexterm"/>will sample our fraud dataset at 0.02% given <code class="literal">'gender'</code> as a strata:</p><div class="informalexample"><pre class="programlisting">data_sample = fraud_df.sampleBy(
    'gender', {1: 0.0002, 2: 0.0002}
).select(numerical)</pre></div><p>To put multiple 2D charts in one go, you can use the following code:</p><div class="informalexample"><pre class="programlisting">data_multi = dict([
    (elem, data_sample.select(elem).rdd \
        .flatMap(lambda row: row).collect()) 
    for elem in numerical
])
sctr = chrt.Scatter(data_multi, x='balance', y='numTrans')
chrt.show(sctr)</pre></div><p>The preceding code will produce the following chart:</p><div class="mediaobject"><img src="images/B05793_04_26.jpg" alt="Interactions between features"/></div><p>As you can <a id="id230" class="indexterm"/>see, there are plenty of fraudulent transactions that had 0 balance but many transactions—that is, a fresh card and big spike of transactions. However, no specific pattern can be shown apart from some <span class="emphasis"><em>banding</em></span> occurring at $1,000 intervals.</p></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec31"/>Summary</h1></div></div></div><p>In this chapter, we looked at how to clean and prepare your dataset for modeling by identifying and tackling datasets with missing values, duplicates, and outliers. We also looked at how to get a bit more familiar with your data using tools from PySpark (although this is by no means a full manual on how to analyze your datasets). Finally, we showed you how to chart your data.</p><p>We will use these (and more) techniques in the next two chapters, where we will be building machine learning models.</p></div></div>
</body></html>