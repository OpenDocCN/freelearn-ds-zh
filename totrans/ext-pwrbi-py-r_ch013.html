<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xml:lang="en-US">
<head>
<meta charset="utf-8"/>
<meta name="generator" content="packt"/>
<title>12 Adding Statistics Insights: Outliers and Missing Values</title>



</head>
<body>


<h1 data-number="13">12 Adding Statistics Insights: Outliers and Missing Values</h1>
<p>In an effort to extend the data enrichment possibilities in Power BI through statistical functions, we will explore some methodologies to detect univariate and multivariate outliers in your dataset. In addition, advanced methodologies to impute possible missing values in datasets and time-series will be explored. Knowledge of these techniques is critical for the experienced analyst because Power BI does not provide useful tools for this purpose by default.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>What outliers are and how to deal with them</li>
<li>Identifying outliers</li>
<li>Implementing outlier detection algorithms</li>
<li>What missing values are and how to deal with them</li>
<li>Diagnosing missing values</li>
<li>Implementing missing value imputation algorithms</li>
</ul>

<h2 data-number="13.1">Technical requirements</h2>
<p>This chapter requires you to have a working internet connection and <strong>Power BI Desktop</strong> installed on your machine. You must have properly configured the R and Python engines and IDEs as outlined in <em>Chapter 2, Configuring R With Power BI</em>, and <em>Chapter 3, Configuring Python with Power BI</em>.</p>


<h2 data-number="13.2">What outliers are and how to deal with them</h2>
<p>Generally, outliers are defined as those observations that lie at an <em>abnormal distance</em> from other observations in a data sample. In other words, they are <em>uncommon values</em> in a dataset. The abnormal distance we're talking about obviously doesn't have a fixed measurement but is strictly dependent on the dataset you're analyzing. Simply put, it will be the analyst who decides the distance beyond which to consider other abnormal distances based on their experience and functional knowledge of the business reality represented by the dataset.</p>
<blockquote>
<p><strong>Important Note</strong></p>
<p>It makes sense to talk about outliers for numeric variables or for numeric variables grouped by elements of categorical variables. It makes no sense to talk about outliers for categorical variables only.</p>
</blockquote>
<p>But why is there so much focus on managing outliers? The answer is that very often they cause undesirable macroscopic effects on some statistical operations. The most striking example is that of a linear correlation in the presence of an outlier in an "uncomfortable" position and the same calculated by eliminating the outlier:</p>
<figure>
<img src="img/file304.png" alt="Figure 12.1 – A simple scatterplot" /><figcaption aria-hidden="true">Figure 12.1 – A simple scatterplot</figcaption>
</figure>
<p>As is evident in <em>Figure 12.1</em> and from what you learned from <em>Chapter 11, Adding Statistics Insights: Associations</em>, Pearson's correlation <em>r</em> suffers greatly from outliers.</p>
<p>But then, is it always sufficient to remove outliers <em>apriori</em> in order to solve any problems you might find downstream in your analysis? As you can imagine, the answer is “no,” because it all depends on the type of outlier you are dealing with.</p>

<h3 data-number="13.2.1">The causes of outliers</h3>
<p>Before considering any action to be applied to the outliers of a variable, it is necessary to consider what might have generated them. Once the cause is established, it may be immediate to fix the outliers. Here is a possible categorization of the causes of outliers:</p>
<ul>
<li><strong>Data entry errors</strong>: There may be an analyst collecting the data who made mistakes compiling it. If the analyst is collecting the birth dates of a group of people, it may be, for example, that instead of writing 1977, they write 177. If the dates that they have collected belong to the range from 1900 to 2100, it is quite easy to correct the outlier that has been created due to the entry error. Other times, it is not possible to recover the correct value.</li>
<li><strong>Intentional outliers</strong>: Very often, the introduction of "errors" is intentional by the individuals to whom the measurements apply. For example, adolescents typically do not accurately report the amount of alcohol they consume.</li>
<li><strong>Data processing errors</strong>: Data transformation processes that are usually applied to analytics solutions can introduce unintended errors, which in turn can give rise to possible outliers.</li>
<li><strong>Sampling errors</strong>: Sometimes, the data on which you perform your analysis must be sampled from a much larger dataset. It may be in this case that the analyst does not select a subset of data representing the entire population of data. For example, you need to measure the height of athletes, and, by mistake, you include some basketball players in your dataset.</li>
<li><strong>Natural outliers</strong>: So-called “natural” outliers exist because they are part of the nature of business and are not the result of any kind of error. For example, it's pretty much a given that shopping malls sell more products at Christmas time.</li>
</ul>
<p>Once the nature of specific outliers is identified, it is certainly easier to try to correct them as much as possible. How do we proceed? There are a few common ways to correct outliers that can be considered.</p>


<h3 data-number="13.2.2">Dealing with outliers</h3>
<p>The most widely used approaches to deal with outliers are as follows:</p>
<ul>
<li><strong>Dropping them</strong>: The analyst concludes that eliminating the outliers altogether guarantees better results in the final analysis.</li>
<li><strong>Capping them</strong>: It is common to use the strategy of assigning a fixed extreme value (cap) to all those observations that exceed it (in absolute value) when it is certain that all extreme observations behave in the same way as those with the cap value.</li>
<li><strong>Assigning a new value</strong>: In this case, outliers are eliminated by replacing them with null values, and these null values are imputed using one of the simplest techniques: the replacement of null values with a fixed value that could be, for example, the mean or median of the variable in question. You'll see more complex imputation strategies in the next sections.</li>
<li><strong>Transforming the data</strong>: When the analyst is dealing with natural outliers, very often the histogram of the variable's distribution takes on a skewed shape. Right-skewed distributions are very common, and if they were used as they appeared, many statistical tests that assume a normal distribution would give incorrect results. In this case, it is often used to transform the variable by applying a monotonic function, which in some way "straightens out" the imbalance (this is the case of the <code>log()</code> function, for example). Once transformed, the new variable satisfies the requirements of the tests and can therefore be analyzed without errors. Once the results have been obtained from the transformed variable, they must be transformed again by the inverse function of the one used at the beginning (if <code>log()</code> was used, then the inverse is <code>exp()</code>) in order to have values that are consistent with the business variable under analysis.</li>
</ul>
<p>Now that you know the most common ways of dealing with outliers, you need to figure out how to identify them.</p>



<h2 data-number="13.3">Identifying outliers</h2>
<p>There are different methods used to detect outliers depending on whether you are analyzing one variable at a time (<strong>univariate analysis</strong>) or multiple variables at once (<strong>multivariate analysis</strong>). In the univariate case, the analysis is fairly straightforward. The multivariate case, however, is more complex. Let's examine them in detail.</p>

<h3 data-number="13.3.1">Univariate outliers</h3>
<p>One of the most direct and widely used ways to identify outliers for a single variable is to make use of boxplots, which you learned about in <em>Chapter 11, Adding Statistics Insights: Associations</em>. Some of the key points of a boxplot are the <strong>interquartile range</strong> (<strong>IQR</strong>), defined as the distance from the <strong>first quartile</strong> (<strong>Q1</strong>) to the <strong>third quartile</strong> (<strong>Q3</strong>), the <strong>lower whisker</strong> (Q1 - 1.5 x IQR), and the <strong>upper whisker</strong> (Q3 + 1.5 x IQR):</p>
<figure>
<img src="img/file305.png" alt="Figure 12.2 – Boxplot’s main characteristics" /><figcaption aria-hidden="true">Figure 12.2 – Boxplot’s main characteristics</figcaption>
</figure>
<p>Specifically, all observations that are before the lower whisker and after the upper whisker are identified as outliers. This one is also known as <strong>Tukey’s method</strong>.</p>
<p>Identification becomes more complicated when dealing with more than one variable.</p>


<h3 data-number="13.3.2">Multivariate outliers</h3>
<p>Identifying outliers when you are handling more than one variable (<strong>multivariate outliers</strong>) is not always straightforward. It depends on the number of variables in play and their data type.</p>

<h4 data-number="13.3.2.1">Numeric variable and categorical variable</h4>
<p>As long as you need to analyze how a numerical variable is distributed among the different elements of a categorical variable, it is still feasible with the tools seen so far. In fact, just plot a boxplot for the values of the numeric variable grouped by each element of the categorical variable:</p>
<figure>
<img src="img/file306.png" alt="Figure 12.3 – Numeric versus categorical variables" /><figcaption aria-hidden="true">Figure 12.3 – Numeric versus categorical variables</figcaption>
</figure>
<p>In fact, it may be that the numerical variable does not present any outliers but reveals some when it is broken down by the elements of the categorical variable.</p>


<h4 data-number="13.3.2.2">All numeric variables</h4>
<p>Generally, the inexperienced analyst tends to simplify the determination of outliers in multidimensional cases when there are only numeric variables.</p>
<blockquote>
<p><strong>Important Note</strong></p>
<p>One might assume that an observation that is extreme in any variable is also a multivariate outlier, and this is often true. However, the opposite is not true: when variables are correlated, one can have a multivariate outlier that is not a univariate outlier in any variable.</p>
</blockquote>
<p>When dealing with only numeric variables, it is still possible to use algorithms that measure the distance from the center of the distribution. Let's take the case of two numerical variables, which allows us to visualize the outliers using a scatterplot:</p>
<figure>
<img src="img/file307.png" alt="Figure 12.4 – Scatterplot of two numeric variables" /><figcaption aria-hidden="true">Figure 12.4 – Scatterplot of two numeric variables</figcaption>
</figure>
<p>As you can see in <em>Figure 12.4</em>, we have also added in the margin the two boxplots for every single variable under analysis to verify that for each of them there are no outliers, except for the one detected at the bottom. You can also see that there is one outlier that is clearly different from all other observations but is not detected as an outlier by the two boxplots.</p>
<p>Imagine fixing a hypothetical center of the distribution and defining a distance from the center (Euclidean distance) above which the observations are to be considered outliers:</p>
<figure>
<img src="img/file308.png" alt="Figure 12.5 – Euclidean distance from the center of the distribution" /><figcaption aria-hidden="true">Figure 12.5 – Euclidean distance from the center of the distribution</figcaption>
</figure>
<p>The above rule defines a circle centered at the center of the distribution. Considering a circle with the radius you see in <em>Figure 12.5</em>, you are going to identify several outliers (perhaps false positives?) that were not identified by looking at the boxplots alone, but the outlier you failed to identify before remains unidentified in this case as well. As you can well understand, the problem is that the distribution has an ellipsoidal shape that is distributed along the main diagonal of the Cartesian plane. Using a circle is certainly ill-suited to fit a distribution of a different shape.</p>
<p>What if there was a distance that also considered the shape of the distribution? This is precisely the case with the <strong>Mahalanobis distance</strong>. This new distance differs from the others because it considers the <strong>covariance</strong> between the two variables. Covariance and Pearson’s correlation are two quantities associated with very similar concepts, so in some cases they are interchangeable (take a look at the references). The fact that the Mahalanobis distance accounts for the correlation between the two variables is evident in <em>Figure 12.6</em>:</p>
<figure>
<img src="img/file309.png" alt="Figure 12.6 – Mahalanobis distance from the center of the distribution" /><figcaption aria-hidden="true">Figure 12.6 – Mahalanobis distance from the center of the distribution</figcaption>
</figure>
<p>For the curious, this is the formula to calculate it:</p>
<figure>
<img src="img/file310.png" alt="Figure 12.7 – Mahalanobis distance formula" /><figcaption aria-hidden="true">Figure 12.7 – Mahalanobis distance formula</figcaption>
</figure>
<p>is a multivariate observation, is the multivariate mean of all observations, and <em>S</em> is the covariance matrix. The fact that the Mahalanobis distance depends on the mean of all observations (a very unstable measure, very sensitive to outliers) and the covariance matrix makes you realize that the same limitations encountered for the Pearson’s coefficient apply, which are as follows:</p>
<ul>
<li>Possible outliers at inconvenient locations could greatly affect the multivariate center defined by the mean of all observations. If the center is not well calculated, it is very likely that the resulting application of the Mahalanobis distance will identify erroneous outliers. This is easily solved by computing the center via a median-based formula, which is much more robust to the presence of extreme observations.</li>
<li>If extreme outliers are present, the covariance matrix may also be negatively affected. This problem is also solved by adopting a robust version of the covariance matrix using the <strong>Minimum Covariance Determinant</strong> (<strong>MCD</strong>). This method, in addition to providing a robust covariance matrix, also returns a robust estimate of the center of the observations.</li>
<li>It is very likely that the usage of the Mahalanobis distance will return false outliers in case of <em>skewed</em>, <em>nonlinear</em>, or <em>heteroscedastic</em> distributions. These are the cases in which it is necessary to resort to transformations of the variables involved, as far as possible, before applying distance calculations. The goal of the transformations is to obtain distributions that are as normal as possible and to make the associations between the various variables as linear as possible. In these cases, <strong>Box-Cox transformations</strong> or <strong>Yeo-Johnson transformations</strong> are used.</li>
</ul>
<p>The identification of outliers becomes much more complicated when dealing with mixed variables (numerical and categorical) in numbers greater than two. It is necessary to use different data science techniques (feature engineering techniques for categorical variables, handling of unbalanced datasets, and so on) and to apply specific machine learning anomaly detection algorithms. For this reason, cases of this type are out of scope.</p>
<p>Once the outliers (both univariate and multivariate) are identified, it is up to the analyst to decide which method to adopt to try to fix it, if possible.</p>
<p>Let's now see how to implement outlier detection algorithms according to what you learned in the previous sections.</p>




<h2 data-number="13.4">Implementing outlier detection algorithms</h2>
<p>The first thing you'll do is implement what you've just studied in Python.</p>

<h3 data-number="13.4.1">Implementing outlier detection in Python</h3>
<p>In this section, we will use the <em>Wine Quality</em> dataset created by Paulo Cortez et al. (<a href="https://archive.ics.uci.edu/ml/datasets/wine+quality">https://archive.ics.uci.edu/ml/datasets/wine+quality</a>) to show how to detect outliers in Python. The dataset contains as many observations as the different types of red wine, each described by the organoleptic properties measured by the variables, except for the <code>quality</code> one, which provides a measure of the quality of the product using a discrete grade scale from 1 to 10.</p>
<p>You'll find the code used in this section in the <code>01-detect-outliers-in-python.py</code> file into the <code>Chapter12\Python</code> folder.</p>
<p>Once you have loaded the data from the <code>winequality-red.csv</code> file directly from the web into the <code>df</code> variable, let's start by examining the <code>sulphates</code> variable. Let's check if it contains any outliers by displaying its boxplot, which was obtained through a wrapper function that we have defined in the code:</p>
<figure>
<img src="img/file311.png" alt="Figure 12.8 – Sulphates boxplot" /><figcaption aria-hidden="true">Figure 12.8 – Sulphates boxplot</figcaption>
</figure>
<p>Apparently there are plenty of values after 1.0. To be able to locate them in the dataset, we created a function that accepts a dataframe as input and the name of the numeric column to be considered, and as output returns the dataframe with the addition of a column of Boolean values, containing <code>True</code> when the value of the column is an outlier, and <code>False</code> otherwise:</p>
<pre><code>def add_is_outlier_IQR(data, col_name):
    col_values = data[col_name]
    
    Q1=col_values.quantile(0.25)
    Q3=col_values.quantile(0.75)
    IQR=Q3-Q1
    
    outliers_col_name = f&#39;is_{col_name.replace(&quot; &quot;, &quot;_&quot;)}_outlier&#39;
    data[outliers_col_name] = ((col_values &lt; (Q1 - 1.5 * IQR)) | (col_values &gt; (Q3 + 1.5 * IQR)))
    
    return data
add_is_outlier_IQR(df, &#39;sulphates&#39;)</code></pre>
<p>Once we have identified the outliers of the initial distribution of the <code>sulphates</code> variable, we can draw its boxplot by removing the outliers to see what changes:</p>
<pre><code>df_no_outliers = df.loc[~df[&#39;is_sulphates_outlier&#39;]]</code></pre>
<p>The resulting boxplot is as follows:</p>
<figure>
<img src="img/file312.png" alt="Figure 12.9 – Sulphates boxplot once outliers were removed" /><figcaption aria-hidden="true">Figure 12.9 – Sulphates boxplot once outliers were removed</figcaption>
</figure>
<p>As you can see, some outliers are still visible. This is due to the fact that removing the outliers from the initial distribution caused the distribution to change (its statistical properties changed). So, what you see in <em>Figure 12.9</em> are the outliers of the new distribution that was created.</p>
<p>As already explained, it is up to the analyst to figure out if the outliers identified can be corrected in some way, eliminated, or left where they are. Suppose in this case that the outliers in the second distribution are natural outliers. Let's try to break the new distribution down to the individual values of the <code>quality</code> variable and draw a boxplot for each of them:</p>
<figure>
<img src="img/file313.png" alt="Figure 12.10 – Sulphates boxplots for each quality value" /><figcaption aria-hidden="true">Figure 12.10 – Sulphates boxplots for each quality value</figcaption>
</figure>
<p>As shown in <em>Figure 12.10</em>, the distribution of sulphates for wines that received a grade of 5 has several outliers. This could induce the analyst to try to understand how much the presence of sulphates affects the final rating given by users to the wine, paying particular attention to the case of a wine considered of average quality.</p>
<p>If, on the other hand, we wanted to identify multivariate outliers for all numerical variables in the dataset, excluding the quality variable, we need to change our approach by trying to apply the Mahalanobis distance, as you learned in the previous section. We assume that the elimination of outliers for each individual variable has been validated. So, let’s now try to figure out if multivariate outliers are present for the numeric variables in the <code>df_no_outliers’</code> dataframe. First, however, it is necessary to check whether the distributions of the variables under analysis are skewed. Therefore, we try to draw a histogram for each of the variables:</p>
<pre><code>df_no_outliers.drop(&#39;quality&#39;, axis=1).hist(figsize=(10,10))
plt.tight_layout()
plt.show()</code></pre>
<p>The resulting plot is as follows:</p>
<figure>
<img src="img/file314.png" alt="Figure 12.11 – Histograms of all the wine quality variables without outliers" /><figcaption aria-hidden="true">Figure 12.11 – Histograms of all the wine quality variables without outliers</figcaption>
</figure>
<p>It is evident that some of them are extremely right-skewed (<em>residual sugar</em>, <em>chlorides</em>, <em>total sulfur dioxide</em>, etc.), therefore it is necessary to try to apply transformations that attempt to “normalize” the single distributions. Generally, <em>Box-Cox transformations</em> are applied. But since in this case some values of the distributions are not positive, it is not possible to apply them. It is therefore necessary to use other transformations that have the same objective, named <em>Yeo-Johnson</em>. For more details on these transformations, check out the references.</p>
<p>For convenience, we created a wrapper function that transforms a pandas dataframe of only numeric variables by applying Yeo-Johnson transformations and also returns the corresponding lambda values:</p>
<pre><code>from sklearn.preprocessing import PowerTransformer
def yeo_johnson_transf(data):
    pt = PowerTransformer(method=&#39;yeo-johnson&#39;, standardize=True)
    pt.fit(data)
    lambdas = pt.lambdas_
    df_yeojohnson = pd.DataFrame( pt.transform(data), columns=data.columns.values ) 
    return df_yeojohnson, lambdas</code></pre>
<p>Then, once you've transformed the dataframe into an object, you can try drawing histograms of the distributions of the transformed variables to see if the skewness has been smoothed out:</p>
<pre><code>df_transf, lambda_arr = yeo_johnson_transf(df_no_outliers[numeric_col_names])
df_transf.hist(figsize=(10,10))
plt.tight_layout()
plt.show()</code></pre>
<p>This is the plot you get:</p>
<figure>
<img src="img/file315.png" alt="Figure 12.12 – Histograms of all the wine quality variables transformed" /><figcaption aria-hidden="true">Figure 12.12 – Histograms of all the wine quality variables transformed</figcaption>
</figure>
<p>It is quite evident that now the distributions look more like the "bells" of normal distributions. You can now calculate Mahalanobis distances with the surety that it will detect outliers with fewer errors.</p>
<p>The identification of outliers is done using a robust estimator of covariance, the <em>MCD</em>. Since the squared Mahalanobis distance behaves as a Chi-Squared distribution (see the references), we can calculate the threshold value above which to consider an observation an outlier thanks to this distribution, passing the desired cutoff value to its <em>percent point function</em> (<code>ppf()</code>):</p>
<pre><code>from sklearn.covariance import MinCovDet
robust_cov = MinCovDet(support_fraction=0.7).fit(df_transf)
center = robust_cov.location_
D = robust_cov.mahalanobis(df_transf - center)
cutoff = 0.98
degrees_of_freedom = df_transf.shape[1]
cut = chi2.ppf(cutoff, degrees_of_freedom)</code></pre>
<p>Once you have determined the threshold value, you can add two columns to the dataframe: a column that identifies whether the observation (the row) is an outlier according to Mahalanobis and a column that reports the probability that an observation is not an outlier by chance:</p>
<pre><code>is_outlier_arr = (D &gt; cut)
outliers_stat_proba = np.zeros(len(is_outlier_arr))
for i in range(len(is_outlier_arr)):
    outliers_stat_proba[i] = chi2.cdf(D[i], degrees_of_freedom)
df[&#39;is_mahalanobis_outlier&#39;] = is_outlier_arr
df[&#39;mahalanobis_outlier_stat_sign&#39;] = outliers_stat_proba
df[df[&#39;is_mahalanobis_outlier&#39;]]</code></pre>
<p>You'll see a dataframe chunk like this:</p>
<figure>
<img src="img/file316.png" alt="Figure 12.13 – Outliers information shown in the dataframe" /><figcaption aria-hidden="true">Figure 12.13 – Outliers information shown in the dataframe</figcaption>
</figure>
<p>Wow! With a minimum of statistics knowledge, you were able to identify multivariate outliers of numeric variables in Python.</p>
<p>It is possible to get the same results with R. Let's see how.</p>


<h3 data-number="13.4.2">Implementing outlier detection in R</h3>
<p>You'll find the code used in this section in the <code>01-detect-outliers-in-r.R</code> file in the <code>Chapter12\R</code> folder. In order to run it properly, you need to install new packages:</p>
<ol>
<li>Open RStudio and make sure it is referencing your latest CRAN R (version 4.0.2 in our case).</li>
<li>Click on the <strong>Console</strong> window and enter this command: <code>install.packages('robust')</code>. Then press <em>Enter</em>.</li>
<li>Enter this command: <code>install.packages('recipes')</code>. Then press <em>Enter</em>.</li>
</ol>
<p>Once you have loaded the data from the <code>winequality-red.csv</code> file directly from the web into the <code>df</code> variable, you’ll draw the boxplot of the <code>sulphates</code> variable using the <code>boxPlot()</code> wrapper function:</p>
<figure>
<img src="img/file317.png" alt="Figure 12.14 – Boxplot of the sulphates variable" /><figcaption aria-hidden="true">Figure 12.14 – Boxplot of the sulphates variable</figcaption>
</figure>
<p>Since there are many outliers visible in <em>Figure 12.14</em>, they are identified using the <code>add_is_outlier_IQR()</code> function, which adds an identifier column to the dataframe. As the name indicates, the function determines the outliers based on the interquartile range. At this point, the boxplot of the same variable is drawn again, this time eliminating the previously identified outliers:</p>
<figure>
<img src="img/file318.png" alt="Figure 12.15 – Boxplot of the sulphates variable after removing outliers" /><figcaption aria-hidden="true">Figure 12.15 – Boxplot of the sulphates variable after removing outliers</figcaption>
</figure>
<p>Assuming you now want to identify multivariate outliers, it is worthwhile to first look at the histograms of the individual variables to see if significant skewness is present. The histograms are drawn using the following <code>dataframeHist()</code> function:</p>
<pre><code>dataframeHist &lt;- function(data, bins = 10) {
    data %&gt;% 
        tidyr::pivot_longer( cols = everything() ) %&gt;% 
        ggplot( aes(value) ) +
        geom_histogram( fill=&#39;orange&#39;, na.rm = TRUE, bins = bins )+ 
        theme( ... ) +
        facet_wrap(~ name, scales = &quot;free&quot;)
}</code></pre>
<p>A special feature of this function is the use of the <code>pivot_longer()</code> function of the <code>tidyr</code> package on all columns to verticalize their names in the new <code>name</code> column, to which correspond the initial values in the new <code>value</code> column. The result is as follows:</p>
<figure>
<img src="img/file319.png" alt="Figure 12.16 – Multiple histograms for each numeric variable" /><figcaption aria-hidden="true">Figure 12.16 – Multiple histograms for each numeric variable</figcaption>
</figure>
<p>Since the skewness is obvious, you can apply Yeo-Johnson transformations thanks to the <code>yeo_johnson_transf()</code> wrapper function we created for you. The peculiarity of this function is that it makes use of a ready-made step in the <code>recipes</code> package, which facilitates the whole pre-processing phase. To learn more about the use of <code>recipes</code>, take a look at the references.</p>
<p>As you learned in the previous section, the Yeo-Johnson transformations solve the skewness problem quite well in this case. Therefore, it is possible to try applying Mahalanobis distance to detect outliers via the following code:</p>
<pre><code>data &lt;- df_transf %&gt;%
    select( numeric_col_names )
cov_obj &lt;- data %&gt;% 
    covRob( estim=&quot;mcd&quot;, alpha=0.7 )
center &lt;- cov_obj$center
cov &lt;- cov_obj$cov
distances &lt;- data %&gt;%
    mahalanobis( center=center, cov=cov )</code></pre>
<p>At this point, given a cutoff value associated with the statistical significance with which we want to determine outliers, you can obtain the corresponding threshold value above which to consider an observation an outlier. Once the threshold is calculated, it is trivial to create an indicator column for the outliers. It is also possible to add a column indicating the probability with which an observation can be considered an outlier not by chance thanks to the <code>pchisq()</code> function:</p>
<pre><code>cutoff &lt;- 0.98
degrees_of_freedom &lt;- ncol(data)
outliers_value_cutoff &lt;- qchisq(cutoff, degrees_of_freedom)
data &lt;- data %&gt;% 
    mutate(
        is_mahalanobis_outlier    = distances &gt; outliers_value_cutoff,
        mahalanobis_outlier_proba = pchisq(distances, ncol(data)) )
data %&gt;% filter( is_mahalanobis_outlier == TRUE )</code></pre>
<p>The final result partially presents this output:</p>
<figure>
<img src="img/file320.png" alt="Figure 12.17 – Final tibble containing the multivariate outliers’ information" /><figcaption aria-hidden="true">Figure 12.17 – Final tibble containing the multivariate outliers’ information</figcaption>
</figure>
<p>Way to go! You were able to identify multivariate outliers in R as well.</p>
<p>At this point, it is trivial to apply the Python and R code seen so far to Power BI.</p>


<h3 data-number="13.4.3">Implementing outlier detection in Power BI</h3>
<p>Power BI has tools that allow you to view outliers graphically and then analyze them by hovering. One of these was introduced in the November 2020 release and is the <strong>Anomaly Detection</strong> feature. The other one is the <strong>Outliers Detection</strong> custom visual. Let's see what the main differences are:</p>
<ul>
<li><strong>Anomaly Detection</strong> is available directly in Power BI once you enable it as a preview feature (at the moment, it is in preview). It is <em>only supported for line chart visuals</em> containing <em>time-series data</em> in the Axis field.</li>
<li><strong>Outliers Detection</strong> is an open source R custom visual, and it has to be installed separately (<a href="https://bit.ly/power-bi-outliers-detection">https://bit.ly/power-bi-outliers-detection</a>). There are five different implemented methods to detect outliers, and it works well for univariate and bivariate datasets. Multivariate datasets are to be avoided.</li>
</ul>
<p>As you may have noticed, both of these tools are Power BI visuals.</p>
<blockquote>
<p><strong>Important Note</strong></p>
<p>All transformations performed within Python or R visuals modify the dataframe that will later be the object of visualization, but the changes cannot be persisted in the data model in any way.</p>
</blockquote>
<p>It is precisely for this reason that we have decided to illustrate some methods for detecting outliers that can be applied in Power Query using Python or R. This way, you can identify observations that are outliers by simply filtering your data model tables appropriately. Due to the simplicity of the code, in this case we will also implement the correlation coefficients in both Python and R in one project.</p>
<p>First, make sure that Power BI Desktop references the correct versions of Python and R in the <strong>Options</strong>. Then follow these steps:</p>
<ol>
<li>Click on <strong>Get Data</strong>, search for <code>web</code>, select <strong>Web</strong>, and click on <strong>Connect</strong>:</li>
<li>Enter <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv">https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv</a> into the URL textbox and click <strong>OK</strong>.</li>
<li>You’ll see a preview of the data. Then click <strong>Transform Data</strong>.</li>
<li>Click <strong>Transform</strong> on the ribbon and then <strong>Run Python script</strong>.</li>
<li>Enter the script you can find in the <code>02-detect-outliers-in-power-bi-with-python copy.py</code> file in the <code>Chapter12\Python</code> folder.</li>
<li>We are only interested in the data in <code>dataset</code>. So, click on its <strong>Table</strong> value.</li>
<li>You'll see a preview of the dataset that also has the new columns to identify outliers.</li>
<li>Click <strong>Home</strong> on the ribbon and then click <strong>Close &amp; Apply</strong>.</li>
<li>Repeat steps 1 to 3.</li>
<li>Click <strong>Transform</strong> on the ribbon and then <strong>Run R script</strong>.</li>
<li>Enter the script you can find in the <code>02-detect-outliers-in-power-bi-with-r.R</code> file in the <code>Chapter12\R</code> folder.</li>
<li>We are only interested in the <code>data</code> tibble. So, click on its <strong>Table</strong> value.</li>
<li>You’ll see the preview of the dataset that also has the new columns to identify outliers.</li>
<li>Click <strong>Home</strong> on the ribbon and then click <strong>Close &amp; Apply</strong>.</li>
</ol>
<p>Amazing! You have just identified outliers of a numeric dataset in Power BI with both Python and R!</p>



<h2 data-number="13.5">What missing values are and how to deal with them</h2>
<p>Data describing real-world phenomena often has a lot of missing data. Lack of data is a fact that cannot be overlooked, especially if the analyst wants to do an advanced study of the dataset to understand how much the variables in it are correlated.</p>
<p>The consequences of mishandling missing values can be many:</p>
<ul>
<li>The <em>statistical power</em> of variables with missing values is diminished, especially when a substantial number of values is missing for a single variable.</li>
<li>The <em>representativeness of the dataset</em> subject to missing values may also be diminished, and thus the dataset in question may not correctly represent the substantive characteristics of the set of all observations of a phenomenon.</li>
<li>Any statistical estimates may not converge to whole population values, thus <em>generating bias</em>.</li>
<li>The results of the analysis conducted may not be correct.</li>
</ul>
<p>But let's see what the causes could be that generate missing values in a dataset.</p>

<h3 data-number="13.5.1">The causes of missing values</h3>
<p>There can be many causes for a lack of values, determined by intentional or unintentional behaviors. Here is a non-exhaustive list:</p>
<ul>
<li>Corruption of data due to errors in writing, reading, or transmitting it</li>
<li>Replacing outliers that excessively skew the dataset with null values</li>
<li>Refusing to answer a question on a survey</li>
<li>Lack of knowledge of the issues asked in a survey question</li>
</ul>
<p>However, all of these causes can be summarized into <em>four types of cases</em>:</p>
<ul>
<li><strong>Missing Completely at Random</strong> (<strong>MCAR</strong>): The causes that generate the null values are totally independent both of the hypothetical values they would have if valorized (Y) and of the values of the other variables in the dataset (X). They just depend on external variables (Z). The advantage of data that is MCAR from a statistical perspective is that the dataset consisting of only complete values for both variables X and Y is an <em>unbiased sample</em> of the entire population. Unfortunately, MCAR cases are rarely found in real-world data.</li>
<li><strong>Missing At Random</strong> (<strong>MAR</strong>): The missing data of a partially incomplete variable (Y) is related to some other variables in the dataset that do not have null values (X), but not to the values of the incomplete variable itself (Y). The dataset consisting of only complete values for both variables X and Y in the MAR case constitutes a <em>biased sample</em> of the entire population because it will surely miss all those values of X on which the null values of Y depend, resulting in a dataset that is not representative of the entire phenomenon. MAR is a more realistic assumption than MCAR.</li>
<li><strong>Missing Not at Random due to external variables Z</strong> (<strong>MNAR Z</strong>): The missing data of a partially incomplete variable (Y) depends on variables not included in the dataset (external variables). For example, given a dataset without the variable “sex,” there may be observations for which the age value is zero. It could be possible that the respondents not providing this information are mostly women, since they stereotypically do not want to reveal their age. Therefore, eliminating observations that have non-null values for the “age” variable would generate a <em>biased dataset</em>.</li>
<li><strong>Missing Not at Random due to missing values Y</strong> (<strong>MNAR Y</strong>): The missing data of a partially incomplete variable (Y) depends on the hypothetical values they would have if valorized. For example, it is well known that adolescents tend to never disclose the fact that they consume alcohol. Therefore, if we remove from the dataset those observations for which the value for alcohol consumption is null, implicitly we risk removing from the dataset most of the observations pertaining to adolescents, thus obtaining a <em>biased dataset</em>.</li>
</ul>
<p>There are also statistical tests that allow you to understand if the distribution of missing data is MCAR (look at the references). But, as already mentioned, cases of MCAR are so rare that it is better to assume that the distribution of missing values of a dataset under consideration is either MAR or MNAR.</p>
<p>Depending on the type of missing data distribution, specific strategies can be employed to sanitize the missing values.</p>


<h3 data-number="13.5.2">Handling missing values</h3>
<p>The first thing to do, when possible, is to understand together with the referent of the data the reason for the missing values in the dataset, and whether it is possible to recover them. Unfortunately, most of the time it is not possible to recover missing data from the source and therefore different strategies must be adopted, depending on the case.</p>

<h4 data-number="13.5.2.1">Easy imputation by hand</h4>
<p>There may be cases of variables that are <em>obvious to impute by hand</em>. For example, in correspondence of the “blue” value of the variable “color” you will notice that the variable “weight” always takes the value of 2.4, except in a few cases where it is null. In those cases, it is easy to impute the missing values of the variable “weight” in relation to the “blue” color with the value 2.4.</p>


<h4 data-number="13.5.2.2">Discarding data</h4>
<p>The first solution to the missing values that comes to the analyst's mind is surely to eliminate the problem at the source, that is, to eliminate missing values. There are several ways to eliminate them:</p>
<ul>
<li><strong>Listwise</strong> or <strong>Complete-Case Analysis</strong> (<strong>CCA</strong>) <strong>deletion</strong>: This method involves <em>deleting any observation (row) that contains at least one missing data element in any variable</em>. It is often applied when the number of missing values is low, and the number of observations is sufficiently large. As you have seen in the classification of the four types of missing data, the only case in which adopting this solution doesn’t result in a biased dataset is the MCAR, a very rare case among datasets describing real-world phenomena. Listwise deletion is therefore not a good strategy when you are not faced with a case of MCAR with a sufficiently high number of observations in the dataset.</li>
<li><strong>Pairwise</strong> or <strong>Available-Case Analysis</strong> (<strong>ACA</strong>) <strong>deletion</strong>: Depending on the variables considered in a statistical analysis, this method <em>eliminates only those observations (rows) that have null values for the only variables involved</em>. Null values present in variables that are not involved in the analysis are not a reason to eliminate observations. Again, adopting this method does not generate a biased dataset only if the case under analysis is MCAR. The most obvious disadvantage of this method is that if you need to compare different analyses, you cannot apply it because the number of observations in the sample varies as the variables involved in the different analyses vary.</li>
<li><strong>Variable deletion</strong>: This method considers <em>removing the entire variable from the analysis</em> under study (and not from the dataset a priori!) when the proportion of missing values ranges is 60% and above. It makes sense to eliminate a variable if, after careful study, it is concluded that it does not contain important information for the analysis at hand. Otherwise, it is always preferable to try a method of imputation. Generally, the elimination of a variable is always the last option and should only be considered if the final analysis actually benefits from it.</li>
</ul>
<p>When the analyst still has to heal the problem of missing values, even after trying to apply these elimination techniques, they must then resort to imputation techniques. Let's look at the most commonly used methods.</p>


<h4 data-number="13.5.2.3">Mean, median, and mode imputation</h4>
<p>This is an intuitively attractive method, also known as <strong>single imputation</strong>, for which you fill missing values with predefined values. Simplicity is unfortunately countered by some not negligible issues.</p>
<p>Perhaps the most common substitution of null values is with the <strong>mean</strong> value of the variable's distribution resulting from ignoring missing values. The motivation behind this choice is that <em>the mean is a reasonable estimate of an observation drawn at random from a normal distribution</em>. However, if the distribution in question is skewed, the analyst runs the risk of making severely biased estimates even if the dataset’s missing value distribution is MCAR.</p>
<p>The skewness problem can be solved by using the <strong>median</strong> of the variable. However, the fact remains that the common problem in single imputation is replacing a missing value with a single value and then treating it as if it were a true value. As a result, single imputation ignores uncertainty and almost always underestimates variance (remember that variance is synonymous with information; a variable with 0 variance is a constant value variable that usually does not enrich statistical analyses).</p>
<p><strong>Mode</strong> (the value that is repeated most often) imputation is often used with categorical data represented as numbers. Even this method, when used without having strong theoretical grounds, introduces bias, so much so that sometimes analysts prefer to create a new category specifically for missing values.</p>
<p>Multiple imputation is often preferable to single imputation as it overcomes the problems of underestimating variance by considering both within-imputation and between-imputation variance. Let's see what this is all about.</p>


<h4 data-number="13.5.2.4">Multiple imputation</h4>
<p>It is thanks to Donald B. Rubin that in 1987 a methodology to deal with the problem of underestimation of variance in the case of single imputation was made public. This methodology goes by the name of <strong>multiple imputation</strong> and consists of the following steps:</p>
<ol>
<li><strong>Imputation</strong>: This step is very similar to the single imputation step, except that this time, values are extracted <em>m</em> times from a distribution for each missing value. The result of this operation is a set of <em>m</em> imputed datasets, for which all observed values are always the same, with different imputed values depending on the uncertainty of the respective distributions.</li>
<li><strong>Analysis</strong>: You use all <em>m</em> imputed datasets for the statistical analysis you need to do. The result of this step is a set of <em>m</em> results (or analyses) obtained by applying the analysis in question to each of the <em>m</em> imputed datasets.</li>
<li><strong>Pooling</strong>: The <em>m</em> results are combined in order to obtain unbiased estimates with the correct statistical properties. The <em>m</em> estimates of each missing value are pooled in order to have an estimated variance that combines the usual sampling variance (<strong>within-imputation variance</strong>) and the extra variance caused by missing data (<strong>between-imputation variance</strong>).</li>
</ol>
<p>The whole process can be summarized by <em>Figure 12.18</em>:</p>
<figure>
<img src="img/file321.png" alt="Figure 12.18 – Multiple imputation process" /><figcaption aria-hidden="true">Figure 12.18 – Multiple imputation process</figcaption>
</figure>
<p>Multiple imputation can be used in cases where the data is MCAR, MAR, and even when the data is MNAR if there are enough auxiliary variables.</p>
<p>The most common implementations of multiple imputation are as follows:</p>
<ul>
<li><strong>Multivariate Imputation by Chained Equations</strong> (<strong>MICE</strong>): This imputes missing values focusing on one variable at a time. Once the focus is on one variable, MICE uses all other variables in the dataset (or an appropriately chosen subset of those variables) to predict missing values in that variable. Predictions of missing values are based on linear regression models for numerical variables and logistic regression models for categorical variables.</li>
<li><strong>Amelia II</strong>: This is named after Amelia Mary Earhart, an American aviation pioneer who, during an attempt to become the first woman to complete a global circumnavigation flight in 1937, disappeared over the central Pacific Ocean. Amelia II combines a bootstrapping-based algorithm and an <strong>Expectation–Maximization</strong> (<strong>EM</strong>) algorithm, making it fast and reliable. It also works very well for time-series data.</li>
</ul>
<p>Recently, multiple imputation has been implemented using deep learning algorithms as well. In particular, the <strong>Multiple Imputation with Denoising Autoencoders</strong> (<strong>MIDAS</strong>) algorithm offers significant advantages in terms of accuracy and efficiency over other multiple imputation strategies, particularly when applied to large datasets with complex features.</p>


<h4 data-number="13.5.2.5">Univariate time-series imputation</h4>
<p>The problem of missing data afflicts not only multivariate tabular datasets, but also time-series datasets. For example, sensors that constantly collect data about a phenomenon could stop working at any time, generating holes in the series. Often, the analyst is faced with a time-series that has missing values and must somehow impute these values because the processes to which to submit the series do not handle null values.</p>
<p>The constraint of the consequentiality of events given by the temporal dimension forces the analyst to use specific imputation methods for time-series. Let's look at the most commonly used methods:</p>
<ul>
<li><strong>Last Observation Carried Forward (LOCF), Next Observation Carried Backward (NOCB)</strong>: In the LOCF method, the last observed (that is, non-null) measure of the variable in question is used for all subsequent missing values. The only condition in which LOCF is unbiased is when the missing data is completely random, and the data used as the basis for LOCF imputation has exactly the same distribution as the unknown missing data. Since it can never be proven that these distributions are exactly the same, all analyses that make use of LOCF are suspect and will almost certainly generate biased results.The NOCB method is a similar approach to LOCF, but works in the opposite direction, taking the first (non-null) observation after the missing value and replacing it with the missing value. It obviously has the same limitations as LOCF.</li>
<li><strong>Exponentially Weighted Moving Average (EMWA)</strong>: In general, the moving average is commonly used in time-series to smooth out fluctuations due to short-term effects and to highlight long-term trends or cycles. EWMA is designed such that older observations are given lower weights. The weights decrease exponentially as the observation gets older (hence the name “exponentially weighted”). Missing values are imputed using the values of the resulting “smoothed” time-series.</li>
<li><strong>Interpolation</strong>: The interpolation technique is one of the most widely used techniques to impute missing data from a time-series. The basic idea is to use a simple function (such as a linear function, a polynomial function, or a spline function) that fits with the non-zero points near the missing value, then interpolate the value for the missing observation.</li>
<li><strong>Seasonally Decomposed Imputation</strong>: If the time-series under analysis has seasonality, this method could give very good results. The procedure adopted is to remove the seasonal component from the time-series, perform the imputation on the seasonally adjusted series, and then add the seasonal component back.</li>
</ul>
<p>There are also algorithms to impute missing values for multivariate time-series.</p>


<h4 data-number="13.5.2.6">Multivariate time-series imputation</h4>
<p>This topic is beyond the scope of this chapter, but we simply wanted to specify that the <em>Amelia II</em> algorithm we discussed earlier is also used to impute missing values in a multivariate time-series, whereas it is not suitable for imputation on univariate time-series.</p>
<p>In order to figure out whether to impute missing values, we must first identify them in the dataset. Let's see how to do that.</p>




<h2 data-number="13.6">Diagnosing missing values in R and Python</h2>
<p>Before thinking about imputing missing values in a dataset, we must first know the extent to which the missing values affect each individual variable.</p>
<p>You can find the code used in this section in the <code>Chapter12\R\03-diagnose-missing-values-in-r.R</code> and <code>Chapter12\Python\03-diagnose-missing-values-in-python.py</code> files. In order to properly run the code and the code of the following sections, you need to install the requisite R and Python packages as follows:</p>
<ol>
<li>Open the Anaconda prompt.</li>
<li>Enter the <code>conda activate pbi_powerquery_env</code> command.</li>
<li>Enter the <code>pip install missingno</code> command.</li>
<li>Enter the <code>pip install upsetplot</code> command.</li>
<li>Then, open RStudio and make sure it is referencing your latest CRAN R (version 4.0.2 in our case).</li>
<li>Click on the <strong>Console</strong> window and enter <code>install.packages('naniar')</code>. Then press <em>Enter</em>.</li>
<li>Enter <code>install.packages('imputeTS')</code>. Then press <em>Enter</em>.</li>
<li>Enter <code>install.packages('forecast')</code>. Then press <em>Enter</em>.</li>
<li>Enter <code>install.packages('ggpubr')</code>. Then press <em>Enter</em>.</li>
<li>Enter <code>install.packages('missForest')</code>. Then press <em>Enter</em>.</li>
<li>Enter <code>install.packages('mice')</code>. Then press <em>Enter</em>.</li>
<li>Enter <code>install.packages('miceadds')</code>. Then press <em>Enter</em>.</li>
</ol>
<p>Let's see at this point what features will come in handy when you face the analysis of missing values in a dataset.</p>
<p>The R package <code>naniar</code> provides the <code>vis_miss()</code> function, which displays in a single image the missing values of the whole dataframe:</p>
<figure>
<img src="img/file322.png" alt="Figure 12.19 – Plot of missing values in the entire dataset" /><figcaption aria-hidden="true">Figure 12.19 – Plot of missing values in the entire dataset</figcaption>
</figure>
<p>You can draw similar graphs in Python thanks to the <code>missingno</code> library (<a href="https://github.com/ResidentMario/missingno">https://github.com/ResidentMario/missingno</a>).</p>
<p>Knowing only the percentage value of the number of missing values compared to the total number of values of the variable under consideration can be limiting. That's why it's often useful to also know the details for each column via the <code>miss_var_summary()</code> function:</p>
<figure>
<img src="img/file323.png" alt="Figure 12.20 – Missing values summary" /><figcaption aria-hidden="true">Figure 12.20 – Missing values summary</figcaption>
</figure>
<p>We developed a similar function in Python in the code you can find in the repository.</p>
<p>It would be interesting to be able to visualize combinations of missing values and missing intersections between variables. The R package <code>naniar</code> (<a href="https://github.com/njtierney/naniar">https://github.com/njtierney/naniar</a>) allows you to do just this kind of analysis thanks to the <code>gg_miss_upset()</code> function:</p>
<figure>
<img src="img/file324.png" alt="Figure 12.21 – UpSet plot of dataset’s missing values" /><figcaption aria-hidden="true">Figure 12.21 – UpSet plot of dataset’s missing values</figcaption>
</figure>
<p>To achieve the same plot in Python, the process is a bit more complicated. You must first use the <code>upsetplot</code> module (<a href="https://github.com/jnothman/UpSetPlot">https://github.com/jnothman/UpSetPlot</a>). The problem lies in providing the <code>UpSet()</code> function exposed by this package with an input dataframe in the required format. For this reason, we made the helper function <code>upsetplot_miss()</code> that you will find in the code to easily create the upset plot of missing values in Python as well.</p>
<p>In case you need to get an idea of the missing values in a time-series, the <code>imputeTS</code> R package provides the <code>ggplot_na_distribution()</code> function, which shows very clearly the holes in the time-series:</p>
<figure>
<img src="img/file325.png" alt="Figure 12.22 – Detecting missing values in a time-series" /><figcaption aria-hidden="true">Figure 12.22 – Detecting missing values in a time-series</figcaption>
</figure>
<p>If, on the other hand, you need to get more complete details about the statistics of missing values in a time-series, the <code>statsNA()</code> function is for you:</p>
<figure>
<img src="img/file326.png" alt="Figure 12.23 – Statistics of missing values in a time-series" /><figcaption aria-hidden="true">Figure 12.23 – Statistics of missing values in a time-series</figcaption>
</figure>
<p>Once you have carefully studied the distributions of the missing values of each variable and their intersections, you can decide which variables to leave in the dataset and which to submit to the various imputation strategies. Let's see how to do imputation in R and Python.</p>


<h2 data-number="13.7">Implementing missing value imputation algorithms</h2>
<p>From here on, all missing value analysis will be done in R because very statistically specialized and simple-to-use packages that do not exist in the Python ecosystem have been developed for this language.</p>
<p>Suppose we need to calculate the Pearson correlation coefficient between the two numerical variables, <code>Age</code> and <code>Fare</code>, of the Titanic disaster dataset. Let's first consider the case where missing values are eliminated.</p>

<h3 data-number="13.7.1">Removing missing values</h3>
<p>The impact of applying listwise and pairwise deletion techniques is evident in the calculation of Pearson's correlation between numerical variables in the Titanic dataset. Let's load the data and select only numeric features:</p>
<pre><code>library(dplyr)
dataset_url &lt;- &#39;http://bit.ly/titanic-data-csv&#39;
tbl &lt;- readr::read_csv(dataset_url)
tbl_num &lt;- tbl %&gt;% 
  select( where(is.numeric) )</code></pre>
<p>If you now calculate the correlation matrix for the two techniques separately, you will notice the differences:</p>
<pre><code># Listwise deletion
cor( tbl_num, method = &#39;pearson&#39;, use = &#39;complete.obs&#39; )
# Pairwise deletion
cor( tbl_num, method = &#39;pearson&#39;, use = &#39;pairwise.complete.obs&#39; )</code></pre>
<p>You will see the following result:</p>
<figure>
<img src="img/file327.png" alt="Figure 12.24 – Correlation matrix calculated using listwise and pairwise deletion" /><figcaption aria-hidden="true">Figure 12.24 – Correlation matrix calculated using listwise and pairwise deletion</figcaption>
</figure>
<p>Let's see how to impute missing values in the case of a tabular dataset.</p>


<h3 data-number="13.7.2">Imputing tabular data</h3>
<p>You can find the code used in this section in the <code>Chapter12\R\04-handle-tabular-missing-values-in-r.R</code> file.</p>
<p>Again, starting with the Titanic disaster dataset, the first thing you need to do is remove the <code>Name</code> and <code>Ticket</code> columns because they have a high number of distinct values.</p>
<blockquote>
<p><strong>Important Note</strong></p>
<p>It is important to eliminate categorical variables that have a high number of distinct values because otherwise, the MICE algorithm would fail due to the excessive RAM required. Generally, variables with high cardinality are not useful for the imputation of null values of other variables. There are cases in which the information contained in these variables could be fundamental for the imputation (for example, zip codes). In this case, it is necessary to use transformations that reduce the cardinality without losing the information contained in the variables. For further information, take a look at the references.</p>
</blockquote>
<p>Since the missing values in the <code>Cabin</code> column represent more than 70% of all values, we decided to remove that as well. After that, the categorical variables <code>Survived</code>, <code>Sex</code>, and <code>Embarked</code> are transformed as factors:</p>
<pre><code>tbl_cleaned &lt;- tbl %&gt;% 
  select( -Cabin, -Name, -Ticket ) %&gt;% 
  mutate(
    Survived = as.factor(Survived),
    Sex = as.factor(Sex),
    Embarked = as.factor(Embarked)
  )</code></pre>
<p>At this point, it is possible to calculate the Pearson correlation for each pair of numerical variables by applying the pooling technique provided by Rubin in multiple imputations. The <code>miceadds</code> package exposes wrapper functions that simplify this operation for the most common statistical analysis given the result of the <code>mice()</code> function as a parameter. In our case, the function of interest is <code>micombine.cor()</code>, and we use it in our <code>corr_impute_missing_values()</code> function:</p>
<pre><code>corr_impute_missing_values &lt;- function(df, m = 5, variables, method = c(&#39;pearson&#39;, &#39;spearman&#39;)) {
  method &lt;- method[1]
  df_imp_lst &lt;- mice(df, m = m, printFlag = FALSE)
  corr_tbl &lt;- miceadds::micombine.cor(df_imp_lst, variables = variables, method = method) %&gt;% 
    as_tibble() %&gt;% 
    arrange( variable1, variable2 )
  return( corr_tbl )
}</code></pre>
<p>It is easy therefore to obtain the aforementioned correlations:</p>
<pre><code># Get the indexes of numeric columns
numeric_col_idxs &lt;- which(sapply(tbl_cleaned, is.numeric))
corr_tbl &lt;- corr_impute_missing_values(tbl_cleaned, variables = numeric_col_idxs, method = &#39;pearson&#39;)
corr_tbl</code></pre>
<p>Here’s the result:</p>
<figure>
<img src="img/file328.png" alt="Figure 12.25 – Statistical inference for correlations for multiple imputed datasets" /><figcaption aria-hidden="true">Figure 12.25 – Statistical inference for correlations for multiple imputed datasets</figcaption>
</figure>
<p>Without going into too much detail about the other fields, the correlation coefficient between the variables is given by the <code>r</code> column. Since the <em>r</em> coefficient is the result of a process of inference, the <code>lower95</code> and <code>upper95</code> columns define the upper and lower 95% confidence interval bounds.</p>
<blockquote>
<p><strong>Important Note</strong></p>
<p>If you get an error such as <strong>Error in matchindex(yhatobs, yhatmis, donors) : function 'Rcpp_precious_remove' not provided by package 'Rcpp'</strong>, it is likely that you are running a recent version of a package compiled with an earlier version of <code>Rcpp</code>. Updating <code>Rcpp</code> with the <code>install.packages('Rcpp')</code> command should fix it.</p>
</blockquote>
<p>Sometimes, the goal of your analysis is not to get results from statistical functions, but to simply fill in the holes left by missing values because the dataset in question must then be used to train a machine learning algorithm that does not admit null values. The latest versions of scikit-learn (currently in the experimental phase) expose the impute module with its <code>SimpleImputer</code>, <code>KNNImputer</code>, and <code>IterativeImputer</code> methods. In this way, it is possible to impute the missing values of a dataset through machine learning algorithms (k-nearest neighbors; linear regression) among other more naïve methods (substitutions with fixed values, mean, median, or mode), and also to have an average score of how the algorithm performs in general (cross-validated mean squared error). You’ll see an example of one of these methods in <em>Chapter 13, Using Machine Learning without Premium or Embedded Capacity</em>.</p>
<p>If, on the other hand, you need to impute missing values from a univariate time-series, how would you proceed? Let's see it.</p>


<h3 data-number="13.7.3">Imputing time-series data</h3>
<p>You can find the code used in this section in the <code>Chapter12\R\05-handle-time-series-missing-values-in-r.R</code> file.</p>
<p>We consider a time-series of the average number of aircraft passengers per month. Let's duplicate it and eliminate from it 10% of the values randomly and, in addition, let's eliminate a couple of duplicated values by hand. Then, we’ll merge the two time-series into a single tibble:</p>
<pre><code>air_df &lt;- read.csv(&#39;https://bit.ly/airpassengers&#39;)
# Create 10% of missing values in the vector
set.seed(57934)
value_missing &lt;- missForest::prodNA(air_df[&#39;value&#39;], noNA = 0.1)
# Force a larger gap in the vector
value_missing[67:68,] &lt;- NA
# Add the vector with missing values to the dataframe
air_missing_df &lt;- air_df %&gt;% 
    mutate( date = ymd(date) ) %&gt;% 
    rename( complete = value ) %&gt;% 
    bind_cols( value = value_missing )</code></pre>
<p>The result can be seen in <em>Figure 12.22</em>. The <code>imputeTS</code> package exposes convenient functions that implement the missing value imputations already described in the <em>Univariate time-series imputation</em> section. Once the values are imputed using the different algorithms and parameters, it is possible to calculate the accuracy because you also know the complete time-series. We use the <code>accuracy()</code> function exposed by the <code>forecast</code> package to calculate the final accuracy using various metrics, such as <em>mean absolute error</em> and <em>root mean squared error</em>:</p>
<figure>
<img src="img/file329.png" alt="Figure 12.24 – Error metrics for imputed values in a time-series" /><figcaption aria-hidden="true">Figure 12.24 – Error metrics for imputed values in a time-series</figcaption>
</figure>
<p>The <strong>Seasonally Decomposed Imputation</strong> (<strong>seadec</strong>) strategy seems to be the best one. Here’s the plot of missing values according to this strategy:</p>
<figure>
<img src="img/file330.png" alt="Figure 12.25 – Representation of imputed values in the time-series" /><figcaption aria-hidden="true">Figure 12.25 – Representation of imputed values in the time-series</figcaption>
</figure>
<p>Let's now look at how to use what we've learned so far about missing values in Power BI.</p>


<h3 data-number="13.7.4">Imputing missing values in Power BI</h3>
<p>We have delved into the theory and techniques of imputing missing values, whether you are dealing with a tabular dataset or a time-series, precisely because in Power BI there is no way to adopt them through native tools, except for the naïve solution of replacing them with a default value (such as fixed value, mean, or median). In fact, when the business analyst finds themselves needing to fill in the gaps in the data, they often ask for the help of a data scientist or someone with the statistical knowledge to tackle the problem. Now that you’ve studied this chapter, you’re able to tackle it on your own!</p>
<p>Let’s apply what we did in the previous sections for tabular and time-series data in Power BI:</p>
<ol>
<li>Open Power BI Desktop and make sure it is referencing the latest engine.</li>
<li>Click <strong>Get data</strong>, search for <code>web</code>, and double-click on the <strong>Web</strong> connector.</li>
<li>Enter the following URL and click <strong>OK</strong>: <a href="http://bit.ly/titanic-dataset-csv">http://bit.ly/titanic-dataset-csv</a>.</li>
<li>On the next import screen, click <strong>Transform Data</strong>.</li>
<li>Go to the <strong>Transform</strong> tab, click on <strong>Run R script</strong>, copy the script in the <code>06-impute-tabular-missing-values-in-power-bi-with-r.R</code> file in the <code>Chapter12\R</code> folder, paste it into the editor, and then click <strong>OK</strong>.</li>
<li>You may be asked to configure the privacy levels of the R script and the CSV file. In this case, select both the <strong>Organizational</strong> and <strong>Public</strong> level.</li>
<li>We are only interested in the data in <code>corr_tbl</code>. So, click on its <strong>Table</strong> value.</li>
<li><p>As a result, you’ll see the table containing the correlation coefficients calculated using MICE and the pooling method provided by the multivariate imputation technique:</p>
<figure>
<img src="img/file331.png" alt="Figure 12.26 – Correlation table calculated with the multivariate imputation technique" /><figcaption aria-hidden="true">Figure 12.26 – Correlation table calculated with the multivariate imputation technique</figcaption>
</figure></li>
<li>Go to the <strong>Home</strong> tab and click <strong>Close &amp; Apply</strong>.</li>
<li>Click <strong>Get data</strong> and double-click on the <strong>Text/CSV</strong> connector.</li>
<li>Select the <code>air.csv</code> file you can find in the <code>Chapter12</code> folder and click <strong>Open</strong>.</li>
<li>On the next import screen, click <strong>Transform Data</strong>.</li>
<li><p>Power BI automatically interprets the <code>date</code> text field as a date field, and therefore applies a <strong>Changed Type</strong> operation from Text to Date. In order to correctly process dates within an R script with the <code>lubridate</code> package, you must delete the <strong>Changed Type</strong> step by clicking on the red cross before inserting the R script:</p>
<figure>
<img src="img/file332.png" alt="Figure 12.27 – Delete the Changed Type step" /><figcaption aria-hidden="true">Figure 12.27 – Delete the Changed Type step</figcaption>
</figure></li>
<li>Go to the <strong>Transform</strong> tab, click on <strong>Run R script</strong>, copy the script in the <code>07-impute-time-series-missing-values-in-power-bi-with-r.R</code> file in the <code>Chapter12\R</code> folder, paste it into the editor, and then click <strong>OK</strong>.</li>
<li>You may be asked to configure the privacy levels of the R script and the CSV file. In this case, select both the <strong>Organizational</strong> and <strong>Public</strong> level.</li>
<li><p>As a result, you’ll see the table containing the original time-series (the <code>value</code> column) and other time-series obtained through different imputation algorithms (each in a different column):</p>
<figure>
<img src="img/file333.png" alt="Figure 12.28 – Table with imputed time-series" /><figcaption aria-hidden="true">Figure 12.28 – Table with imputed time-series</figcaption>
</figure></li>
<li>Go to the <strong>Home</strong> tab and click <strong>Close &amp; Apply</strong>.</li>
</ol>
<p>Impressive! You managed to apply the most complex missing value imputation algorithms to a tabular dataset and a time-series in Power BI with minimal effort. Congratulations!</p>



<h2 data-number="13.8">Summary</h2>
<p>In this chapter, you learned what outliers are, what causes them generally, and how they are treated. You also learned how to identify them based on the number of variables involved and their given type, both in Python and in R.</p>
<p>Another important topic you covered was how to impute missing values in tabular and time-series datasets. You learned how to diagnose them and impute them with R.</p>
<p>After that, you implemented the value imputation algorithms in Power BI.</p>
<p>In the next chapter, you will see how to use machine learning algorithms in Power BI without the need for Premium or Embedded capabilities.</p>


<h2 data-number="13.9">References</h2>
<p>For additional reading, check out the following books and articles:</p>
<ol>
<li><em>Add Marginal Plot to ggplot2 Scatterplot Using ggExtra Package in R</em> (<a href="https://statisticsglobe.com/ggplot2-graphic-with-marginal-plot-in-r">https://statisticsglobe.com/ggplot2-graphic-with-marginal-plot-in-r</a>)</li>
<li><em>5 Things You Should Know About Covariance</em> (<a href="https://towardsdatascience.com/5-things-you-should-know-about-covariance-26b12a0516f1">https://towardsdatascience.com/5-things-you-should-know-about-covariance-26b12a0516f1</a>)</li>
<li><em>Mahalanobis Distance and its Limitations</em> (<a href="https://rpubs.com/jjsuarestra99/mahalanobis">https://rpubs.com/jjsuarestra99/mahalanobis</a>)</li>
<li><em>Box-Cox Transformation Explained</em> (<a href="https://towardsdatascience.com/box-cox-transformation-explained-51d745e34203">https://towardsdatascience.com/box-cox-transformation-explained-51d745e34203</a>)</li>
<li><em>How to Use Power Transforms for Machine Learning</em> (<a href="https://machinelearningmastery.com/power-transforms-with-scikit-learn/">https://machinelearningmastery.com/power-transforms-with-scikit-learn/</a>)</li>
<li><em>The Relationship between the Mahalanobis Distance and the Chi-Squared Distribution</em> (<a href="https://markusthill.github.io/mahalanbis-chi-squared/">https://markusthill.github.io/mahalanbis-chi-squared/</a>)</li>
<li><em>Using the recipes package for easy pre-processing</em> (<a href="http://www.rebeccabarter.com/blog/2019-06-06_pre_processing/">http://www.rebeccabarter.com/blog/2019-06-06_pre_processing/</a>)</li>
<li><em>Anomaly detection</em> (<a href="https://docs.microsoft.com/en-us/power-bi/visuals/power-bi-visualization-anomaly-detection">https://docs.microsoft.com/en-us/power-bi/visuals/power-bi-visualization-anomaly-detection</a>)</li>
<li><em>Missing data: mechanisms, methods, and messages</em> (<a href="http://www.i-deel.org/uploads/5/2/4/1/52416001/chapter_4.pdf">http://www.i-deel.org/uploads/5/2/4/1/52416001/chapter_4.pdf</a>)</li>
<li><em>Multiple imputation by chained equations: what is it and how does it work?</em> (<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3074241/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3074241/</a>)</li>
<li><em>Amelia II: A Program for Missing Data</em> (<a href="https://www.jstatsoft.org/article/view/v045i07">https://www.jstatsoft.org/article/view/v045i07</a>)</li>
<li><em>All about Categorical Variable Encoding</em> (<a href="https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02">https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02</a>)</li>
</ol>


</body>
</html>
