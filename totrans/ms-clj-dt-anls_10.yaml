- en: Chapter 10. Modeling Stock Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Automated stock analysis has gotten a lot of press recently. High-frequency
    trading firms are a flashpoint. People either believe that they're great for the
    markets and increasing liquidity, or that they're precursors to the apocalypse.
    Smaller traders have also gotten into the mix in a slower fashion. Some sites,
    such as Quantopian ([https://www.quantopian.com/](https://www.quantopian.com/))
    and AlgoTrader ([http://www.algotrader.ch/](http://www.algotrader.ch/)) provide
    services that allow you to create models for automated trading. Many others allow
    you to use automated analysis to inform your trading decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Whatever your view of this phenomena, it's an area with a lot of data begging
    to be analyzed. It's also a nice domain in which to experiment with some analysis
    and machine learning techniques.
  prefs: []
  type: TYPE_NORMAL
- en: For this chapter, we're going to look for relationships between news articles
    and stock prices in the future.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the course of this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Learn about financial data analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set up our project and acquire our data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prepare the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyze the text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyze the stock prices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn patterns in both text and stock prices with neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use this system to predict the future
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Talk about the limitations of these systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning about financial data analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finance has always relied heavily on data. Earnings statements, forecasting,
    and portfolio management are just some of the areas that make use of data to quantify
    their decisions. Because of this, financial data analysis and its related field,
    financial engineering, are extremely broad fields that are difficult to summarize
    in a short amount of space.
  prefs: []
  type: TYPE_NORMAL
- en: However, lately, quantitative finance, high-frequency trading, and similar fields
    have gotten a lot of press and really come into their own. As I mentioned, some
    people hate them and the added volatility that the markets seem to have. Others
    maintain that they bring the necessary liquidity that helps the market function
    better.
  prefs: []
  type: TYPE_NORMAL
- en: All of these fields apply statistical or machine learning methods to financial
    data. Some of these techniques can be quite simple. Others are more sophisticated.
    Some of these analyses are used to inform a human analyst or manager to make better
    financial decisions. Others are used as inputs to automated algorithmic processes
    that operate with varying degrees of human oversight, but perhaps with little
    to no intervention.
  prefs: []
  type: TYPE_NORMAL
- en: For this chapter, we'll focus on adding information to the human analyst's repertoire.
    We'll develop a simple machine learning system to look at past, current, and future
    stock prices, alongside the text of news articles, in order to identify potentially
    interesting articles that may indicate future fluctuations in stock price. These
    articles, with the possible future price vector, could provide important information
    to an investor or analyst attempting to decide how to shuffle his/her money around.
    We'll talk more about the purpose and limitations of this system toward the end
    of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we really dig into the project and the data, we need to prepare. We'll
    set up the code and the library, and then we'll download the data.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the library
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, we'll need to initialize the library. We can do this using Leiningen
    2 ([http://leiningen.org/](http://leiningen.org/)) and Stuart Sierra's reloaded
    plugin for it ([https://github.com/stuartsierra/reloaded](http://https://github.com/stuartsierra/reloaded)).
    This will initialize the development environment and project.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, just execute the following command at the prompt (I''ve named the
    project `financial` in this case):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can specify the libraries that we''ll need to use. We can do this in
    the `project.clj` file. Open it and replace its current contents with the following
    lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The primary library that we'll use is Enclog ([https://github.com/jimpil/enclog](https://github.com/jimpil/enclog)).
    This is a Clojure wrapper around the Java library Encog ([http://www.heatonresearch.com/encog](http://www.heatonresearch.com/encog)),
    which is a machine learning library, including classes for artificial neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: We now have the basics in place. We can get the data at this point.
  prefs: []
  type: TYPE_NORMAL
- en: Getting the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We'll need data from two different sources. To begin with, we'll focus on getting
    the stock data.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, we're going to use the historical stock data for Dominion Resources,
    Inc. They're a power company that operates in the eastern United States. Their
    New York Stock Exchange symbol is D. Focusing on one stock like this will reduce
    possible noise and allow us to focus on the simple system that we'll be working
    on in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: To download the stock data, I went to Google Finance ([https://finance.google.com/](https://finance.google.com/)).
    In the search box, I entered `NYSE:D`. On the left-hand side menu bar, there is
    an option to download **Historical prices**. Click on it.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the table header, set the date range to be from `Sept 1, 1995` to `Jan 1,
    2001`. Refer to the following screenshot as an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting the data](img/4139OS_10_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If you look at the lower-right corner of the screenshot, there's a link that
    reads **Download to spreadsheet**. Click on this link to download the data. By
    default, the filename is `d.csv`. I moved it into a directory named `d` inside
    my project folder and renamed it to `d-1995-2001.csv`.
  prefs: []
  type: TYPE_NORMAL
- en: We'll also need some news article data to correlate with the stock data. Freely
    available news articles are difficult to come by. There are good corpora available
    for modest fees (several hundred dollars). However, in order to make this exercise
    as accessible as possible, I've limited the data to what's freely available.
  prefs: []
  type: TYPE_NORMAL
- en: At the moment, the best collection appears to be the journalism segment of the
    Open American National Corpus ([http://www.anc.org/data/oanc/](http://www.anc.org/data/oanc/)).
    The **American National Corpus** (**ANC**) is a collection of texts from a variety
    of registers and genres that are assembled for linguistic research. The **Open
    ANC** (**OANC**) is the subset of the ANC that is available for open access downloading.
    The journalism genre is represented by articles from Slate ([http://www.slate.com/](http://www.slate.com/)).
    This has some benefits and introduces some problems. The primary benefit is that
    the data will be quite manageable. It means that we won't have a lot of documents
    to use for training and testing, and we'll need to be pickier about what features
    we pull from the documents. We'll see how we need to handle this later.
  prefs: []
  type: TYPE_NORMAL
- en: To download the dataset, visit the download page at [http://www.anc.org/data/oanc/download/](http://www.anc.org/data/oanc/download/)
    and get the data in your preferred format, either a TAR ball or a ZIP file. I
    decompressed that data into the `d` directory. It created a directory named `OANC-GrAF`
    that contained the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Your `d` directory should now look something as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting the data](img/4139OS_10_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Getting prepared with data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As usual, now we need to clean up the data and put it into a shape that we can
    work with. The news article dataset particularly will require some attention,
    so let's turn our attention to it first.
  prefs: []
  type: TYPE_NORMAL
- en: Working with news articles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The OANC is published in an XML format that includes a lot of information and
    annotations about the data. Specifically, this marks off:'
  prefs: []
  type: TYPE_NORMAL
- en: Sections and chapters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentences
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Words with part-of-speech lemma
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Noun chunks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Verb chunks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Named entities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, we want the option to use raw text later when the system is actually
    being used. Because of that, we will ignore the annotations and just extract the
    raw tokens. In fact, all we're really interested in is each document's text—either
    as a raw string or a feature vector—and the date it was published. Let's create
    a record type for this.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll put this into the `types.clj` file in `src/financial/`. Put this simple
    namespace header into the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This data record will be similarly simple. It can be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: So let's see what the XML looks like and what we need to do to get it to work
    with the data structures we just defined.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Slate data is in the `OANC-GrAF/data/written_1/journal/slate/` directory.
    The data files are spread through 55 subdirectories as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Digging in deeper, each document is represented by a number of files. From
    the `slate` directory, we can see the following details:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: So we can see the different annotations files are the files with the `xml` extension.
    The ANC file contains metadata about the file. We'll need to access that file
    for the date and other information. But most importantly, there's also a `.txt`
    file containing the raw text of the document. That will make working with this
    dataset much easier!
  prefs: []
  type: TYPE_NORMAL
- en: But let's take a minute to write some functions that will help us work with
    each document's text and its metadata as an entity. These will represent the knowledge
    we've just gained about the directory and file structure of the OANC corpus.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll call this file `src/financial/oanc.clj`, and its namespace header should
    look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: If we examine the directory structure that the OANC uses, we can see that it's
    divided into a clear hierarchy. Let's trace that structure in the `slate` directory
    that we discussed earlier, `OANC-GrAF/data/written_1/journal/slate/`. In this
    example, `written_1` represents a category, `journal` is a genre, and `slate`
    is a source. We can leverage this information as we walk the directory structure
    to get to the data files.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our first bit of code contains four functions. Let''s list them first, and
    then we can talk about them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The functions used in the preceding code are described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The first of these functions, `list-category-genre`, takes a category directory
    (`OANC-GrAF/data/written_1/`) and returns the genres that it contains. This could
    be `journal`, as in our example here, or fiction, letters, or a number of other
    options. Each item returned is a hash map of the full directory and the name of
    the genre.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second function is `list-genres`. It lists all of the genres within the
    OANC data directory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third function is `find-genre-dir`. It looks for one particular genre and
    returns the full directory for it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we have `find-source-data`. This takes a genre and source and lists
    all of the files with an `anc` extension.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using these functions, we can iterate over the documents for a source. We can
    see how to do that in the next function, `find-slate-files`, which returns a sequence
    of maps pointing to each document''s metadata ANC file and to its raw text file,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can get at the metadata in the ANC file. We''ll use the `clojure.data.xml`
    library to parse the file, and we''ll define a couple of utility functions to
    make descending into the file easier. Look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The first utility function, `find-all`, lazily walks the XML document and returns
    all elements with a given tag name. The second function, `content-str`, returns
    all the text children of a tag.
  prefs: []
  type: TYPE_NORMAL
- en: Also, we'll need to parse the date from the `pubDate` elements. Some of these
    have a `value` attribute, but this isn't consistent. Instead, we'll parse the
    elements' content directly using the `clj-time` library ([https://github.com/clj-time/clj-time](https://github.com/clj-time/clj-time)),
    which is a wrapper over the Joda time library for Java ([http://joda-time.sourceforge.net/](http://joda-time.sourceforge.net/)).
    From our end, we'll use a few functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we do, though, we''ll need to define a date format string. The dates
    inside the `pubDate` functions look like *2/13/97 4:30:00 PM*. The formatting
    string, then, should look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use this formatter to pull data out of a `pubDate` element and parse
    it into an `org.joda.time.DateTime` object as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Unfortunately, some of these dates are about 2000 years off. We can normalize
    the dates and correct these errors fairly quickly, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'With all of these parts in place, we can write a function that takes the XML
    from an ANC file and returns date and time for the publication date as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The other piece of data that we''ll load from the ANC metadata XML is the title.
    We get that from the `title` element, of course, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, loading a `NewsArticle` object is straightforward. In fact, it''s so simple
    that we''ll also include a version of this that reads in the text from a plain
    file. Look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'And using these functions to load all of the Slate articles just involves repeating
    the earlier steps, as shown in the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The last command in the preceding code just prints the title, publication date,
    and the length of the text in the document.
  prefs: []
  type: TYPE_NORMAL
- en: With these functions in place, we now have access to the article dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Working with stock data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Loading the news articles was complicated. Fortunately, the stock price data
    is in **comma-separated values** (**CSV**) format. Although not the richest data
    format, it is very popular, and `clojure.data.csv` ([https://github.com/clojure/data.csv/](https://github.com/clojure/data.csv/))
    is an excellent library for loading it.
  prefs: []
  type: TYPE_NORMAL
- en: As I just mentioned, though, CSV isn't the richest data format. We will want
    to convert this data into a richer format, so we'll still create a record type
    and some wrapper functions to make it easier to work with the data as we read
    it in.
  prefs: []
  type: TYPE_NORMAL
- en: 'The data in this will closely follow the columns in the CSV file that we downloaded
    from Google Finance earlier. Open `src/financial/types.clj` again and add the
    following line to represent the data type for the stock data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'For the rest of the code in this section, we''ll use a new namespace. Open
    the `src/financial/cvs_data.clj` file and add the following namespace declaration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Just like the Slate news article data, this data also has a field with a date,
    which we''ll need to parse. Unlike the Slate data, this value is formatted differently.
    Glancing at the first few lines of the file gives us all the information that
    we need, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'To parse dates in this format (29-Dec-00), we can use the following format
    specification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we build on this and a few other function—which you can find in the code
    download in the file `src/financial/utils.clj`—to create a `StockData` instance
    from a row of data, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This is all straightforward. Basically, every value in the row must be converted
    to a native Clojure/Java type, and then all of those values are used to create
    the `StockData` instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'To read in an entire file, we just do this for every row returned by the CSV
    library as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The only wrinkle is that we have to drop the first row, since it's the header.
  prefs: []
  type: TYPE_NORMAL
- en: 'And now, to load the data, we just call the following function (we''ve aliased
    the `financial.csv-data` namespace to `csvd`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Everything appears to be working correctly. Let's turn our attention back to
    the news article dataset and begin analyzing it.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our goal for analyzing the news articles is to generate a vector space model
    of the collection of documents. This attempts to pull the salient features for
    the documents into a vector of floating-point numbers. Features can be words or
    information from the documents' metadata encoded for the vector. The feature values
    can be 0 or 1 for presence, an integer for raw frequency, or the frequency scaled
    in some form.
  prefs: []
  type: TYPE_NORMAL
- en: In our case, we'll use the feature vector to represent a selection of the tokens
    in a document. Often, we can use all the tokens, or all the tokens that occur
    more than once or twice. However, in this case, we don't have a lot of data, so
    we'll need to be more selective in the features that we include. We'll consider
    how we select these in a few sections.
  prefs: []
  type: TYPE_NORMAL
- en: For the feature values, we'll use a scaled version of the token frequency called
    **term frequency-inverse document frequency** (**tf-idf**). There are good libraries
    for this, but this is a basic metric in working with free text data, so we'll
    take this algorithm apart and implement it ourselves for this chapter. That way,
    we'll understand it better.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the rest of this section, we''ll put the code into `src/financial/nlp.clj`.
    Open this file and add the following for the namespace header:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: With this in place, we can now start to pick the documents apart.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing vocabulary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step for analyzing a document, of course, is tokenizing. We'll use
    a simple tokenize function that just pulls out sequences of letters or numbers,
    including any single punctuation marks.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we can use this function to see what words are present in the text and
    how frequent they are. The core Clojure function, `frequencies`, makes this especially
    easy, but we do still need to pull out the data that we'll use.
  prefs: []
  type: TYPE_NORMAL
- en: For each step, we'll first work on raw input, and then we'll write an additional
    utility function that modifies the `:text` property of the input `NewsArticle`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To tokenize the text, we''ll search for the matches for a regular expression
    and convert the output to lowercase. This won''t work well for a lot of cases—contractions,
    abbreviations, and hyphenations in English, for example—but it will take care
    of simple needs. Look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The actual tokenization is handled by the `tokenize` function. The `tokenize-text`
    function takes a `NewsArticle` instance and replaces its raw text property with
    the sequence of tokens generated from the text.
  prefs: []
  type: TYPE_NORMAL
- en: 'The function `token-freqs` replaces the sequence of tokens with a mapping of
    their frequencies. It uses the Clojure core function frequencies as shown in the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then take a sequence of `NewsArticle` instances that contain the token
    frequencies and generate the frequencies for the entire corpus. The function `corpus-freqs`
    takes care of that. Look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s use the following functions to get the frequencies:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll get the tokens for each article. Then we''ll print out the first ten
    tokens from the first article, as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we''ll get the frequencies of the tokens in each document and print out
    ten of the token-frequency pairs from the first document, as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we can reduce those down into one set of frequencies over the entire
    collection. I''ve pretty-printed out the top ten most frequent tokens. Look at
    the following code:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can see that the most frequent words are common words with little semantic
    value. In the next section, we'll see what we need to do with them.
  prefs: []
  type: TYPE_NORMAL
- en: Stop lists
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The words identified as the most common words in the code in the previous section
    are often referred to as **function** words, because they're performing functions
    in the sentence, but not really carrying meaning. For some kinds of analyses,
    such as grammatical and stylistic analyses, these are vitally important. However,
    for this particular chapter, we're more interested in the documents' content words,
    or the words that carry semantic meaning.
  prefs: []
  type: TYPE_NORMAL
- en: To filter these out, the typical technique is to use a stop word list. This
    is a list of common words to remove from the list of tokens.
  prefs: []
  type: TYPE_NORMAL
- en: If you type `english stop list` into Google, you'll get a lot of workable stop
    lists. I've downloaded one from [http://jmlr.org/papers/volume5/lewis04a/a11-smart-stop-list/english.stop](http://jmlr.org/papers/volume5/lewis04a/a11-smart-stop-list/english.stop).
    Download this file too, and place it into the `d` directory along with the data
    files.
  prefs: []
  type: TYPE_NORMAL
- en: To load the stop words, we'll use the following function. It simply takes the
    filename and returns a set of the tokens in it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Using this set directly is easy enough on raw strings. However, we''ll want
    a function to make calling it on `NewsArticle` instances easier. Look at the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can load those words and remove them from the lists of tokens. We''ll
    start with the definition of tokens that we just created. Look at the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: First, we can tell that we've removed a number of tokens that weren't really
    adding much. `You`, `re`, and `s` were all taken out, along with others.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s regenerate the corpus frequencies with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: This list seems much more reasonable. It focuses on Bill Clinton, who was the
    US President during this period.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another way of dealing with this is to use a white list. This would be a set
    of words or features that represent the entire collection of those that we want
    to deal with. We could implement this as a simple function, `keep-white-list`,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: This function seems academic now, but we'll need it before we're done.
  prefs: []
  type: TYPE_NORMAL
- en: Hapax and Dis Legomena
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let''s look at a graph of the frequencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hapax and Dis Legomena](img/4139OS_10_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: That's a lot of words that don't occur very much. This is actually expected.
    A few words occur a lot, but most just don't.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can get another view of the data by looking at the log-log plot of the frequencies
    and ranks. Functions that represent a value raised to a power should be linear
    in these line charts. We can see that the relationship isn''t quite on a line
    in this plot, but it''s very close. Look at the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hapax and Dis Legomena](img/4139OS_10_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In fact, let''s turn the frequency mapping around in the following code to
    look at how often different frequencies occur:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: So there are more than 23,000 words that only occur once and more than 8,000
    words that only occur twice. Words like these are very interesting for authorship
    studies. The words that are found only once are referred to as **hapax legomena**,
    from Greek for "said once", and words that occur only twice are **dis legomena**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at a random 10 hapax legomena gives us a good indication of the types
    of words these are. The 10 words are: shanties, merrifield, cyberguru, alighting,
    roomfor, sciaretto, borisyeltsin, vermes, fugs, and gandhian. Some of these appear
    to be unusual or rare words. Others are mistakes or two words that were joined
    together for some reason, possibly by a dash.'
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, they do not contribute much to our study, since they don't occur
    often enough to contribute to the results statistically. In fact, we'll just get
    rid of any words that occur less than 10 times. This will form a second stop list,
    this time of rare words. Let's generate this list. Another, probably better performing,
    option is to create a whitelist of the words that aren't rare, but we can easily
    integrate this with our existing stop-list infrastructure, so we'll do it by just
    creating another list here.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create it from the frequencies, we''ll define a `make-rare-word-list` function.
    It takes a frequency mapping and returns the items with fewer than *n* occurrences,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now use this function to generate the `d/english.rare` file. We can
    use this file just like we used the stop list to remove items that aren''t common
    and to further clean up the tokens that we''ll have to deal with (you can also
    find this file in the code download for this chapter):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Now, we have a list of more than 48,000 tokens that will get removed. For perspective,
    after removing the common stop words, there were more than 71,000 token types.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now use that just like we did for the previous stop word list. Starting
    from `filtered`, which we defined in the earlier code after removing the common
    stop words, we''ll now define `filtered2` and recalculate the frequencies as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: So we can see that the process has removed some uncommon words, such as `harmonic`
    and `convergences`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This process is pretty piecemeal so far, but it''s one that we would need to
    do multiple times, probably. Many natural language processing and text analysis
    tasks begin by taking a text, converting it to a sequence of features (tokenization,
    normalization, and filtering), and then counting them. Let''s package that into
    one function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The preceding function allows us to call it with just a list of articles. We
    can also specify a list of stop word files. The entries in all the lists are added
    together to create a master list of stop words. Then the articles' text is tokenized,
    filtered by the stop words, and counted. Doing it this way should save on creating
    and possibly hanging on to multiple lists of intermediate processing stages that
    we won't ever use later.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can skip to the document-level frequencies with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we''ve filtered these out, let''s look at the graph of token frequencies
    again:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hapax and Dis Legomena](img/4139OS_10_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The distribution stayed the same, as we would expect, but the number of words
    should be more manageable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, we can see from the following log-log plot that the previous power relationship—almost,
    but not quite, linear—holds for this frequency distribution as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hapax and Dis Legomena](img/4139OS_10_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Another way to approach this would be to use a whitelist, as we mentioned earlier.
    We could load files and keep only the tokens that we''ve seen before by using
    the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Again, this will come up later. We'll find this necessary when we need to load
    unseen documents to analyze.
  prefs: []
  type: TYPE_NORMAL
- en: TF-IDF
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The frequencies as we currently have them will present a couple of difficulties.
    For one thing, if we have a document with 100 words and one with 500 words, we
    can't really compare the frequencies. For another thing, if a word occurs three
    times in every document, say in a header, it's not as interesting as one that
    occurs in only a few documents three times and nowhere else.
  prefs: []
  type: TYPE_NORMAL
- en: In order to work around both of these, we'll use a metric called **term frequency-inverse
    document frequency** (**TF-IDF**). This combines some kind of document-term frequency
    with the log of the percentage of documents that contain that term.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the first part, term frequency, we could use a number of metrics. We could
    use a boolean 0 or 1 to show absence or presence. We could use the raw frequency
    or the raw frequency scaled. In this case, we''re going to use an augmented frequency
    that scales the raw frequency by the maximum frequency of any word in the document.
    Look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The first function in the preceding code, `tf`, is a basic augmented frequency
    equation and takes the raw values as parameters. The second function, `tf-article`,
    wraps `tf` but takes a `NewsArticle` instance and a word and generates the TF
    value for that pair.
  prefs: []
  type: TYPE_NORMAL
- en: For the second part of this equation, the inverse document frequency, we'll
    use the log of the total number of documents divided by the number of documents
    containing that term. We'll also add one to the last number to protect against
    division-by-zero errors.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `idf` function calculates the inverse document frequency for a term over
    the given corpus, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The IDF for a word won''t change between different documents. Because of this,
    we can calculate all of the IDF values for all the words represented in the corpus
    once and cache them. The following two functions take care of this scenario:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: The first function in the preceding code, `get-vocabulary`, returns a set of
    all the words used in the corpus. The next function, `get-idf-cache`, iterates
    over the vocabulary set to construct a mapping of the cached IDF values. We'll
    use this cache to generate the TF-IDF values for each document.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `tf-idf` function combines the output of `tf` and `idf` (via `get-idf-cache`)
    to calculate the TF-IDF value. In this case, we simply take the raw frequencies
    and the IDF value and multiply them together as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: This works at the most basic level; however, we'll want some adapters to work
    with `NewsArticle` instances and higher-level Clojure data structures.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first level up will take the IDF cache and a map of frequencies and return
    a new map of TF-IDF values based off of those frequencies. To do this, we have
    to find the maximum frequency represented in the mapping. Then we can calculate
    the TF-IDF for each token type in the frequency map as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The `tf-idf-freqs` function does most of the work for us. Now we can build
    on it further. First, we''ll write `tf-idf-over` to calculate the TF-IDF values
    for all the tokens in a `NewsArticle` instance. Then we''ll write `tf-idf-cached`,
    which takes a cache of IDF values for each word in a corpus. It returns those
    documents with their frequencies converted if TF-IDF. Finally, `tf-idf-all` will
    call this function on a collection of `NewsArticle` instances as shown in the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: We've implemented TF-IDF, but now we should play with it some more to get a
    feel for how it works in practice.
  prefs: []
  type: TYPE_NORMAL
- en: We'll start with the definition of `filtered2` that we implemented in the *Hapax
    and Dis Legomena* section. This section contained the corpus of `NewsArticles`
    instances, and the `:text` property is the frequency of tokens without the tokens
    from the stop word lists of both rare and common words.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can generate the scaled TF-IDF frequencies for these articles by calling
    `tf-idf-all` on them. Once we have that, we can compare the frequencies for one
    article. Look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The table''s too long to reproduce here (176 tokens). Instead, I''ll just pick
    10 interesting terms to look at more closely. The following table includes not
    only each term''s raw frequencies and TF-IDF scores, but also the number of documents
    that they are found in:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Token | Raw frequency | Document frequency | TF-IDF |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| sillier | 1 | 8 | 3.35002 |'
  prefs: []
  type: TYPE_TB
- en: '| politics | 1 | 749 | 0.96849 |'
  prefs: []
  type: TYPE_TB
- en: '| british | 1 | 594 | 1.09315 |'
  prefs: []
  type: TYPE_TB
- en: '| reason | 2 | 851 | 0.96410 |'
  prefs: []
  type: TYPE_TB
- en: '| make | 2 | 2,350 | 0.37852 |'
  prefs: []
  type: TYPE_TB
- en: '| military | 3 | 700 | 1.14842 |'
  prefs: []
  type: TYPE_TB
- en: '| time | 3 | 2,810 | 0.29378 |'
  prefs: []
  type: TYPE_TB
- en: '| mags | 4 | 18 | 3.57932 |'
  prefs: []
  type: TYPE_TB
- en: '| women | 11 | 930 | 1.46071 |'
  prefs: []
  type: TYPE_TB
- en: '| men | 13 | 856 | 1.66526 |'
  prefs: []
  type: TYPE_TB
- en: The tokens in the preceding table are ordered by their raw frequencies. However,
    notice how badly that correlates with the TF-IDF.
  prefs: []
  type: TYPE_NORMAL
- en: First, notice the numbers for "sillier" and "politics". Both are found once
    in this document. But "sillier" probably doesn't occur much in the entire collection,
    and it has a TF-IDF score of more than 3\. However, "politics" is common, so it
    scores slightly less than 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, notice the numbers for "time" (raw frequency of 3) and "mags" (4). "Time"
    is a very common word that kind of straddles the categories of function words
    and content words. On the one hand, you can be using expressions like "time after
    time", but you can also talk about "time" as an abstract concept. "Mags" is a
    slangy version of "magazines", and it occurs roughly the same number of times
    as "time". However, since "mags" is rarely found in the entire corpus (only 18
    times), it has the highest TF-IDF score of any word in this document.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, look at "women" and "men". These are the two most common words in this
    article. However, because they're found in so many documents, both are given TF-IDF
    scores of around 1.5.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What we wind up with is a measure of how important a term is in that document.
    Words that are more common have to appear more to be considered significant. Words
    that are found in only a few documents can be important with just one mention.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a final step before we move on, we can also write a utility function that
    loads a set of articles, given a token whitelist and an IDF cache. This will be
    important }after we''ve trained the neural network when we''re actually using
    it. That''s because we will need to keep the same features, in the same order,
    and to scale between the two runs. Thus, it''s important to scale by the same
    IDF values. Look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code will allow us to analyze documents and actually use our neural
    network after we've trained it.
  prefs: []
  type: TYPE_NORMAL
- en: Inspecting the stock prices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have some hold on the textual data, let''s turn our attention to
    the stock prices. Previously, we loaded it from the CSV file using the `financial.csv-data/read-stock-prices`
    function. Let''s reload that data with the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s start with a graph that shows how the closing price has changed over
    the years:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Inspecting the stock prices](img/4139OS_10_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: So the price started in the low 30s, fluctuated a bit, and finished in the low
    20s. During that time, there were some periods where it climbed rapidly. Hopefully,
    we'll be able to capture and predict those changes.
  prefs: []
  type: TYPE_NORMAL
- en: Merging text and stock features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we can start to train the neural network, however, we'll need to figure
    out how we need to represent the data and what information the neural network
    needs to have.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for this section will be present in the `src/financial/nn.clj` file.
    Open it up and add the following namespace header:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: However, we first need to be clear about what we're trying to do. That will
    allow us to properly format and present the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s break it down like this: for each document, based on the previous stock
    prices and the tokens in a document, can we predict the direction of future stock
    prices.'
  prefs: []
  type: TYPE_NORMAL
- en: So one set of features will be the tokens in the document. We already have those
    identified earlier. Other features can represent the stock prices. Since we're
    actually interested in the direction of the future prices, we can actually use
    the difference between the stock prices of a point in the past and of the day
    the article was published. Offhand, we're not sure what time frames will be helpful,
    so we can select several and include them all.
  prefs: []
  type: TYPE_NORMAL
- en: The output is another difference in stock prices. Again, we don't know at what
    difference in time we'll be able to get good results (if any!), so we'll try to
    look out into the future at various distances.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the ranges of time, we''ll use some standard time periods, gradually getting
    further and further out: a day, two days, three days, four days, five days, two
    weeks, three weeks, one month, two months, six months, and one year. Days that
    fall on a weekend have the value of the previous business day. Months will be
    30 days, and a year is 365 days. This way, the time periods will be more or less
    regular.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can represent those periods in Clojure using the `clj-time` library ([https://github.com/clj-time/clj-time](https://github.com/clj-time/clj-time))
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'For the features, we''ll use the difference in price over those periods. The
    easiest way to get at that information would be to index the stock prices by date
    and then access the prices from there using some utility functions. Let''s see
    what that would look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: We can use `index-by` to index a collection of anything into a map. The other
    function, `get-stock-date`, then attempts to get the `StockData` instance from
    the index. If it doesn't find one, it tries the previous day. If it ever works
    its way before 1990, it just returns nil.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's get the input feature vector from a `NewsArticle` instance and the
    stock index.
  prefs: []
  type: TYPE_NORMAL
- en: 'The easy part of this will be getting the token vector. Getting the price vector
    will be more complicated, and we''ll be doing almost the same thing twice: once
    looking backward from the article for the input vector, and once looking forward
    from the article for the output vector. Since generating these two vectors will
    be mostly the same, we''ll write a function that does it and accepts function
    parameters for the differences, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: The `make-price-vector` function gets the base price from the day the article
    was published. It then gets the day offsets that we outlined previously and finds
    the closing stock price for each of those days. It finds the difference between
    the two prices.
  prefs: []
  type: TYPE_NORMAL
- en: The parameter for this function is `date-op`, which gets the second day to find
    the stock price for. It will either add the period to the article's publish date
    or subtract it, depending on whether we're looking in the future or the past.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can build on this to make the input vector, which will contain the token
    vector and the price vector, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: For the token vector, we get the frequencies from the `NewsArticle` instance
    in the order given by the vocab collection. This should be the same across all
    `NewsArticle` instances. We call `make-price-vector` to get the prices for the
    offset days. Then we concatenate all of them into one (Clojure) vector.
  prefs: []
  type: TYPE_NORMAL
- en: The following code gives us the input vector. However, we'll also want to have
    future prices as the output vector.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code is just a thin wrapper over `make-price-vector`. It calls
    this function with the appropriate arguments to get the future stock price.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we''ll write a function that takes a stock index, a vocabulary, and
    a collection of articles. It will generate both the input vector and the expected
    output vector, and it will return both stored in a hash map. The code for this
    function is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: This code will make it easy to generate a training set from the data that we've
    been working with.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing both text and stock features together with neural nets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We now have everything ready to perform the analysis, except for the engine
    that will actually attempt to learn the training data.
  prefs: []
  type: TYPE_NORMAL
- en: In this instance, we're going to try to train an artificial neural network to
    learn the direction of change of the future prices of the input data. In other
    words, we'll try to train it to tell whether the price will go up or down in the
    near future. We want to create a simple binary classifier from the past price
    changes and the text of an article.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding neural nets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As the name implies, artificial neural networks are machine learning structures
    modeled on the architecture and behavior of neurons, such as the ones found in
    the human brain. Artificial neural networks come in many forms, but today we''re
    going to use one of the oldest and most common forms: the three-layer feed-forward
    network.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see the structure of a unit outlined in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding neural nets](img/4139OS_10_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Each unit is able to realize linearly separable functions. That is, functions
    that divide their n-dimensional output space along a hyperplane. To emulate more
    complex functions, however, we have to go beyond a single unit and create a network
    of them.
  prefs: []
  type: TYPE_NORMAL
- en: 'These networks have three layers: an input layer, a hidden layer, and an output
    layer. Each layer is made up of one or more neurons. Each neuron takes one or
    more inputs and produces an output, which is broadcast to one or more outputs.
    The inputs are weighted, and each input is weighted individually. All of the inputs
    are added together, and the sum is passed through an activation function that
    normalizes and scales the input. The inputs are *x*, the weights are *w*, and
    the outputs are *y*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple schematic of this structure is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding neural nets](img/4139OS_10_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The network operates in a fairly simple manner, following the process called
    feed forward activation. It is described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The input vector is fed to the input layer of the network. Depending on how
    the network is set up, these may be passed through the activation function for
    each neuron. This determines the activation of each neuron, or the amount of signal
    coming into it from that channel and how excited it is.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The weighted connections between the input and hidden layers are then activated
    and used to excite the nodes in the hidden layer. This is done by getting the
    dot product of the input neurons with the weights going into each hidden node.
    These values are then passed through the activation function for the hidden neurons.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The forward propagation process is repeated again between the hidden layer and
    the output layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The activation of the neurons in the output layer is the output of the network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initially, the weights are usually randomly selected. Then the weights are trained
    using a variety of techniques. A common one is called backward propagation. This
    involves computing the error between the output neurons and the desired outputs.
    This error is then fed backward into the network. This is used to dampen some
    weights and increase others. The net effect is to nudge the output of the network
    slightly closer to the target.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other training methods work differently, but attempt to do the same thing:
    each tries to modify the weights so that the outputs are close to the targets
    for each input in the training set.'
  prefs: []
  type: TYPE_NORMAL
- en: Note that I said *close to the targets*. When training a neural network, you
    don't want the outputs to align exactly. When this happens, the network is said
    to have memorized the training set. This means that the network will perform great
    for inputs that it has seen previously. But when it encounters new inputs, it
    is brittle and won't perform well. It has learned the training set too well, but
    it won't be able to generalize that information to new inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the neural net
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Implementing neural networks isn't difficult—and doing so is a useful exercise—but
    there are good libraries for neural networks available for Java and Clojure, and
    we'll choose one of those here. For our case, we'll use the Encog Machine Learning
    Framework ([http://www.heatonresearch.com/encog](http://www.heatonresearch.com/encog)),
    which specializes in neural networks. But we'll primarily be using it through
    the Clojure wrapper library Enclog ([https://github.com/jimpil/enclog/](https://github.com/jimpil/enclog/)).
    We'll build on these to write some facade functions over Enclog to customize this
    library for our processes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to create the neural network. The `make-network` function
    takes the vocabulary size and the number of hidden nodes (the variables for our
    purposes), but it defines the rest of the parameters internally, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: The number of input nodes is a function of the size of the vocabulary in addition
    to the number of periods. (Periods is a non-dynamic, namespace-level binding.
    We may want to rethink that and make it dynamic to provide a little more flexibility,
    but for our needs right now this is sufficient.) And since from the output node,
    we just want a single value indicating whether the stock went up or down, we hardcoded
    the number of output nodes to one. However, the number of hidden nodes that will
    perform best is an open question. We'll include that as a parameter so we can
    experiment with it.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the output, we''ll need a way to take our expected output and run it through
    the same activation function as that output. That way, we can directly compare
    the two as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: The activated function takes an object that implements `org.encog.engine.network.activation.ActivationFunction`.
    We can get these from the neural network. It then puts the output for a period
    into a double array. The activation function scales this and then returns the
    array.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will also need to prepare the data and insert it into a data structure that
    Encog can work with. The primary transformation in the following code is pulling
    out the output prices for the period that we''re training for:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: There's nothing particularly exciting here. We pull the inputs and the outputs
    for one time period out into two separate vectors and create a dataset with them.
  prefs: []
  type: TYPE_NORMAL
- en: Training the neural net
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now we have a neural network, but it's been initialized to random weights, so
    it will perform very, very poorly. We'll need to train it immediately.
  prefs: []
  type: TYPE_NORMAL
- en: To do this, we will put the training set together with the network in the following
    code. Like the previous functions, `train-for` accepts the parameters that we're
    interested in being able to change, uses reasonable defaults for ones that we'll
    probably leave alone, but hardcodes parameters that we won't touch. The function
    creates a trainer object and calls its `train` method. Finally, we return the
    neural network, which was modified in place.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'When it is time to validate a network, it will be a little easier to combine
    creating a network with training it into one function. We''ll do that with `make-train`
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: This allows us to train a new neural network in one call.
  prefs: []
  type: TYPE_NORMAL
- en: Running the neural net
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once we''ve trained the network, we''ll want to run it on new inputs, ones
    for which we don''t know the expected output. We can do that with the `run-network`
    function. This takes a trained network and an input collection and returns an
    array of the network''s output as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use this function in one of two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: We can pass it data that we don't know the output for to see how the network
    classifies it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can pass it input data that we do know the output for in order to evaluate
    how well this network performs against data it hasn't previously encountered.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We'll see an example of the latter in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Validating the neural net
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can build on all of these functions to validate the neural network, train
    it, test it against new data, and evaluate how it does.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `test-on` utility gets the **sum of squared errors** (**SSE**) for running
    the network on a test set for a given period. This trains and runs a neural network
    on the training set for a given period. It then returns the SSE for that run as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Running this train-test combination once gives us a very rough idea of how the
    network will perform with those parameters. However, if we want a better idea,
    we can use K-fold cross-validation. This divides the data into *K* equally sized
    groups. It then runs the train-test combination *K* times. Each time, it holds
    out a different partition as a test group. It trains the network on the rest of
    the partitions and evaluates it on the test group. The errors returned by `test-on`
    can be averaged to get a better idea of how the network will perform with those
    parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, say we use K=4\. We''ll divide the training input into four groups:
    A, B, C, and D. This means that we''ll train the following four different classifiers:'
  prefs: []
  type: TYPE_NORMAL
- en: We'll use A as the test set and train on B, C, and D combined
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We'll use B as the test set and train on A, C, and D
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We'll use C as the test set and train on A, B, and D
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We'll use D as the test set and train on A, B, and C
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For each classifier, we'll compute the SSE, and we'll take the mean of these
    to see how well the classification should perform with those parameters, on average.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: I've defined the K-fold function in the `validate.clj` file at `src/financial/`.
    You can see how it's implemented in the source code download. I've also aliased
    that namespace to `v` in the current namespace.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `x-validate` function will perform the cross validation on the inputs.
    The other function, `accum`, is simply a small utility that accumulates the error
    values into a vector. The `v/k-fold` function expects the accumulator to return
    the base case (an empty vector) when called with no arguments, as shown in the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: The `x-validate` function uses `make-train` to create a new network and train
    it. It tests that network using `test-on`, and it gathers the resulting error
    rates together with `accum`.
  prefs: []
  type: TYPE_NORMAL
- en: Finding the best parameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We've defined this system to let us play with a couple of parameters. First,
    we can set the number of neurons in the hidden layer. Also, we can set the time
    period that we are to predict for into the future (one day, two days, three days,
    a month, a year, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: These parameters create a large space of possible solutions, some of which may
    perform better than others. We can make some educated guesses about some of the
    parameters—that it will predict the movement of the stock prices one day in the
    future better than it will the movement a year in the future—but we don't know
    that, and we should perhaps try it out.
  prefs: []
  type: TYPE_NORMAL
- en: These parameters present a search space. It would take too much time to try
    all the combinations, but we can try a number of them, just to see how they perform.
    This lets us tune the neural network to get the best results.
  prefs: []
  type: TYPE_NORMAL
- en: 'To explore this search space, let''s first define what happens when we test
    one point, one combination of time period in the future, and a number of hidden
    nodes. The `explore-point` function will take care of this in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code basically just takes the information and passes it to `x-validate`.
    It returns that function's return value (`error`) too. Along the way, it prints
    out a number of status messages. Then we need something that walks over the search
    space, calls `explore-point`, and collects the error rates returned for the output.
  prefs: []
  type: TYPE_NORMAL
- en: We'll define a dynamic global called `*hidden-counts*` that defines the range
    of hidden neuron counts that we're interested in exploring. The `periods` value
    that we bound earlier will define the search space for how far to look into the
    future.
  prefs: []
  type: TYPE_NORMAL
- en: To make sure that we don't train the networks too specifically on the data that
    we're using to find the best parameters, we'll first break the data into a development
    set and a test set. We'll use the development set to try out the different parameters,
    further breaking it up into a training set and a development-test set. At the
    end, we'll take the best set of parameters and test those against the test set
    that we originally held out. This will give us a better idea of how the neural
    network performs. The `final-eval` function will perform this last set and return
    the information that it creates.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following function walks over these values and is named `explore-params`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: I've made a slightly unusual design decision in writing `explore-params`. Instead
    of initializing a hash map to contain the period-hidden count pairs and their
    associated error rates, I need the caller to pass in a reference containing a
    hash map. During the course of the processing, `explore-params` fills the hash
    map and finally returns it.
  prefs: []
  type: TYPE_NORMAL
- en: 'I''ve done this for one reason: exploring this search space still takes a long
    time. Over the course of writing this chapter, I needed to stop the validation,
    tweak the possible parameter values, and start it again. Setting up the function
    this way allowed me to be able to stop the processing, but still have access to
    what''s happened thus far. I can look at the values, play around with them, and
    allow a more thorough examination of them to influence my decisions about what
    direction to take.'
  prefs: []
  type: TYPE_NORMAL
- en: Predicting the future
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now is the time to bring together everything that we've assembled over the course
    of this chapter, so it seems appropriate to start over from scratch, just using
    the Clojure source code that we've written over the course of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: We'll take this one block at a time, loading and processing the data, creating
    training and test sets, training and validating the neural network, and finally
    viewing and analyzing its results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we do any of this, we''ll need to load the proper namespaces into the
    REPL. We can do that with the following `require` statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: This will give us access to everything that we've implemented so far.
  prefs: []
  type: TYPE_NORMAL
- en: Loading stock prices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we''ll load the stock prices with the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code loads the stock prices from the CSV file and indexes them
    by date. This will make it easy to integrate them with the new article data in
    a few steps.
  prefs: []
  type: TYPE_NORMAL
- en: Loading news articles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now we can load the news articles. We''ll need two pieces of data from them:
    the TF-IDF scaled frequencies and the vocabulary list. Look at the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: This code binds the frequencies as `freqs` and the vocabulary as `vocab`.
  prefs: []
  type: TYPE_NORMAL
- en: Creating training and test sets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since we bundled the entire process into one function, merging our two data
    sources together into one training set is simple, as shown in the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: Now, for each article, we have an input vector and a series of output for different
    stock prices related to the data.
  prefs: []
  type: TYPE_NORMAL
- en: Finding the best parameters for the neural network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The training data and the parameters'' value ranges are the input for exploring
    the network parameter space. Look at the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: This takes a very long time to run. Actually, I looked at the output it was
    producing and realized that it wouldn't be able to predict well beyond a day or
    two, so I stopped it after that. Thanks to my decision to pass in a reference,
    I was able to stop it and still have access to the results generated by that point.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output is a mapping from the period and number of hidden nodes to a list
    of SSE values generated from each partition in the K-fold cross-validation. A
    more meaningful metric would be the average of the errors. We can generate that
    here and print out the results as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: So the squared sum of errors for predicting one day ahead go from about 1 for
    10 hidden units to 100 for 300 hidden units. So, based on that, we'll only train
    a network to predict one day into the future and to use 10 hidden nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Training and validating the neural network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Actually, training the neural network is pretty easy from our end, but it does
    take a while. The following commands should somewhat produce better results than
    we saw before, but at the cost of some time. Remember that the training process
    may not take this long, but we should probably be prepared.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: Well, that was quick.
  prefs: []
  type: TYPE_NORMAL
- en: This gives us a trained, ready-to-use neural network bound to the name `nn`.
  prefs: []
  type: TYPE_NORMAL
- en: Running the network on new data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can now run our trained network on some new data. Just to have something
    to look at, I downloaded 10 articles off the Slate website and saved them to files
    in the directory `d/slate/`. I also downloaded the stock prices for Dominion,
    Inc.
  prefs: []
  type: TYPE_NORMAL
- en: Now, how would I analyze this data?
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we really start, we''ll need to pull some data from the processes we''ve
    been using, and we''ll need to set up some reference values, such as the date
    of the documents. Look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: So we get the IDF cache, the date the articles were downloaded on, and the vocabulary
    that we used in training. That vocabulary set will serve as the token whitelist
    for loading the news articles.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how to get the documents ready to analyze. Look at the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: This is a little more complicated than it was when we loaded them earlier. Basically,
    we just read the directory list and load the text from each one. Then we tokenize
    and filter it before determining the TF-IDF value for each token.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, reading the stocks is very similar to what we just did.
    Look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'With these in hand, we can put both together to make the input vectors as shown
    in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s see how to run the network and see what happens. Look at the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: These items are very consistent. To quite a few decimal places, they're all
    clustered right around 0.5\. From the sigmoid function, this means that it doesn't
    really anticipate a stock change over the next day.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, this tracks what actually happened fairly well. On March 20, the stock
    closed at $69.77, and on March 21, it closed at $70.06\. This was a gain of $0.29.
  prefs: []
  type: TYPE_NORMAL
- en: Taking it with a grain of salt
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Any analysis like the one presented in this chapter has a number of things that
    we need to question. This chapter is no exception.
  prefs: []
  type: TYPE_NORMAL
- en: Related to this project
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The main weakness of this project was that it was carried out on far too little
    data. This cuts in several ways:'
  prefs: []
  type: TYPE_NORMAL
- en: We need articles from a number of data sources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We need articles from a wider range of time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We need more density of articles in the time period
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For all of these, there are reasons we didn't address the issues in this chapter.
    However, if you plan to take this further, you'd need to figure out some way around
    these.
  prefs: []
  type: TYPE_NORMAL
- en: There are several ways to look at the results too. The day we looked at, the
    results all clustered close to zero. In fact, this stock if relatively stable,
    so if it always indicated little change, then it would always have a fairly low
    SSE. Large changes seem to happen occasionally, and the error from not predicting
    them has a low impact on the SSE.
  prefs: []
  type: TYPE_NORMAL
- en: Related to machine learning and market modeling in general
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Second, and more importantly, simply putting some stock data into a jar with
    some machine learning and shaking it is a risky endeavor. This isn't a get-rich-quick
    scheme, and by approaching it so naively, you're asking for trouble. In this case,
    that means losing money.
  prefs: []
  type: TYPE_NORMAL
- en: For one thing, there's not much noise in news articles, and the relationship
    between their content and stock prices is tenuous enough that in general, stock
    prices may not be predictable from news reports in the first place, whatever results
    we achieve is this study, particularly given how small it is.
  prefs: []
  type: TYPE_NORMAL
- en: 'Really, to do this well, you need to understand at least two things:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Financial modeling**: You need to understand how to model financial transactions
    and dynamics mathematically'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Machine learning**: You need to understand how machine learning works and
    how it models things'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With this knowledge, you should be able to formulate a better model of how the
    stock prices change and which prices you should pay attention to.
  prefs: []
  type: TYPE_NORMAL
- en: But keep in mind, André Christoffer Andersen and Stian Mikelsen have published
    a master's thesis in 2012 showing that it's very, very difficult to do better
    than buying and holding index funds ([http://blog.andersen.im/wp-content/uploads/2012/12/ANovelAlgorithmicTradingFramework.pdf](http://blog.andersen.im/wp-content/uploads/2012/12/ANovelAlgorithmicTradingFramework.pdf)).
    So, if you do try this route, you have a hard, hard task in front of you.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Over the course of this chapter, we've gotten a hold of some news articles and
    some stock prices, and we've managed to train a neural network that projects just
    a little into the future. This is a risky thing to put into production, but we've
    also outlined what we'd need to learn to do this correctly.
  prefs: []
  type: TYPE_NORMAL
- en: And this is also the end of this book. Thank you for staying with me this far.
    You've been a great reader. I hope that you've learned something as we've looked
    at the 10 data analysis projects that we've covered. If programming and data are
    both eating this world, hopefully you've seen how to have fun with both.
  prefs: []
  type: TYPE_NORMAL
