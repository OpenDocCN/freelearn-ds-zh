- en: 'Chapter 9: spaCy and Transformers'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you will learn about the latest hot topic in NLP, transformers,
    and how to use them with TensorFlow and spaCy.
  prefs: []
  type: TYPE_NORMAL
- en: First, you will learn about transformers and transfer learning. Second, you'll
    learn about the architecture details of the commonly used Transformer architecture
    – **Bidirectional Encoder Representations from Transformers** (**BERT**). You'll
    also learn how **BERT Tokenizer** and **WordPiece** algorithms work. Then you
    will learn how to quickly get started with pre-trained transformer models of the
    **HuggingFace** library. Next, you'll practice how to fine-tune HuggingFace Transformers
    with TensorFlow and Keras. Finally, you'll learn how *spaCy v3.0* integrates transformer
    models as pre-trained pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will be completing the statistical NLP topics
    of this book. You will add your knowledge of transformers to the knowledge of
    Keras and TensorFlow that you acquired in [*Chapter 8*](B16570_08_Final_JM_ePub.xhtml#_idTextAnchor137),
    *Text Classification with spaCy*. You'll be able to build state-of-the-art NLP
    models with just a few lines of code with the power of Transformer models and
    transfer learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Transformers and transfer learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding BERT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformers and TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformers and spaCy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll use the `transformers` and `tensorflow` Python libraries
    along with `spaCy`. You can install these libraries via `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The chapter code can be found at the book''s GitHub repository: [https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter09](https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter09).'
  prefs: []
  type: TYPE_NORMAL
- en: Transformers and transfer learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A milestone in NLP happened in 2017 with the release of the research paper
    *Attention Is All You Need*, by Vaswani et al. ([https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)),
    which introduced a brand-new machine learning idea and architecture – transformers.
    Transformers in NLP is a fresh idea that aims to solve sequential modeling tasks
    and targets some problems introduced by **long short-term memory** (**LSTM**)
    architecture (recall LSTM architecture from [*Chapter 8*](B16570_08_Final_JM_ePub.xhtml#_idTextAnchor137),
    *Text Classification with spaCy*). Here''s how the paper explains how transformers
    work:'
  prefs: []
  type: TYPE_NORMAL
- en: '"The Transformer is the first transduction model relying entirely on self-attention
    to compute representations of its input and output without using sequence-aligned
    RNNs or convolution."'
  prefs: []
  type: TYPE_NORMAL
- en: Transduction in this context means transforming input words to output words
    by transforming input words and sentences into vectors. Typically, a transformer
    is trained on a huge corpus such as Wiki or news. Then, in our downstream tasks,
    we use these vectors as they carry information regarding the word semantics, sentence
    structure, and sentence semantics (we'll see how to use the vectors precisely
    in our code in the *Transformers and TensorFlow* section).
  prefs: []
  type: TYPE_NORMAL
- en: We already explored the idea of pre-trained word vectors in [*Chapter 5*](B16570_05_Final_JM_ePub.xhtml#_idTextAnchor087),
    *Working with Word Vectors and Semantic Similarity*. Word vectors such as Glove
    and FastText vectors are already trained on the Wikipedia corpus and we used them
    directly for our semantic similarity calculations. In this way, we imported information
    about word semantics from the Wiki corpus into our semantic similarity calculations.
    Importing knowledge from pre-trained word vectors or pre-trained statistical models
    is called **transfer learning**.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers offer thousands of pre-trained models to perform NLP tasks, such
    as text classification, text summarization, question answering, machine translation,
    and natural language generation in more than 100 languages. Transformers aim to
    make state-of-the-art NLP accessible to everyone.
  prefs: []
  type: TYPE_NORMAL
- en: The following screenshot shows a list of the Transformer models provided by
    HuggingFace (we'll learn about HuggingFace Transformers in the *HuggingFace Transformers*
    section). Each model is named with a combination of the architecture name (*Bert,
    DistilBert*, and so on), possibly the language code (*en*, *de*, *multilingual*,
    and similar, given on the left side of the following screenshot), and information
    regarding whether the model is cased or uncased (the model distinguishes between
    uppercase and lowercase characters).
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, on the left-hand side of *Figure 9.1*, we see the task names. Each model
    is labeled with a task name. We select a model that is suitable for our task,
    such as text classification or machine translation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – A list of HuggingFace Transformers, taken from the HuggingFace
    website'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_9_1.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.1 – A list of HuggingFace Transformers, taken from the HuggingFace
    website
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand what''s great about transformers, we''ll first revisit LSTM architecture
    from [*Chapter 8*](B16570_08_Final_JM_ePub.xhtml#_idTextAnchor137), *Text Classification
    with spaCy*. In the previous chapter, we already stepped into the statistical
    modeling world with Keras and LSTM architecture. LSTMs are great for modeling
    text; however, they have some shortcomings too:'
  prefs: []
  type: TYPE_NORMAL
- en: LSTM architecture sometimes has difficulties with learning long text. Statistical
    dependencies in a long text can be difficult to represent by an LSTM because,
    as the time steps pass, LSTM can forget some of the words that were processed
    at earlier time steps.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The nature of LSTMs is sequential. We process one word at each time step. Obviously,
    parallelizing the learning process is not possible; we have to process sequentially.
    Not allowing parallelization creates a performance bottleneck.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Transformers address these problems by not using recurrent layers at all. If
    we have a look at the following, the architecture looks completely different from
    an LSTM architecture. Transformer architecture consists of two parts – an input
    encoder (called the **Encoder**) block on the left, and the output decoder (called
    the **Decoder**) block on the right. The following diagram is taken from this
    paper and exhibits the transformer architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 – Transformer architecture from the paper entitled "Attention
    is All You Need"'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_9_2.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.2 – Transformer architecture from the paper entitled "Attention is
    All You Need"
  prefs: []
  type: TYPE_NORMAL
- en: The preceding architecture is invented for a machine translation task; hence,
    the input is a sequence of words from the source language, and the output is a
    sequence of words in the target language. The encoder generates a vector representation
    of the input words and passes them to the decoder (the word vector transfer is
    represented by the arrow from the encoder block in the direction of the decoder
    block). The decoder takes these input word vectors, transforms the output words
    into word vectors, and finally generates the probability of each output word (labeled
    in *Figure 9.2* as **Output Probabilities**).
  prefs: []
  type: TYPE_NORMAL
- en: Inside the encoder and decoder blocks, we see feedforward layers, which are
    basically a dense layer we used in [*Chapter 8*](B16570_08_Final_JM_ePub.xhtml#_idTextAnchor137),
    *Text Classification with spaCy*. The innovation transformers bring lies in the
    **Multi-Head Attention** block. This block creates a dense representation for
    each word by using a self-attention mechanism. The **Self-attention** mechanism
    relates each word in the input sentence to the other words in the input sentence.
    The word embedding of each word is calculated by taking a weighted average of
    the other words' embeddings. This way, the importance of each word in the input
    sentence is calculated, so the architecture focuses its *attention* on each input
    word in turn.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram is taken from the original paper and illustrates self-attention.
    The diagram illustrates how the input words on the left-hand side attend the input
    word "**it**" on the right-hand side. Darker colors mean more relevance, hence
    the words "**the animal**" are more related to "**it**" rather than the other
    words in this sentence. What does this mean? This means that the transformer can
    resolve what the pronoun "**it**" refers to precisely in this sentence, which
    is the phrase "**the animal**." This is a great accomplishment of transformers;
    they can resolve many semantic dependencies in a given sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.3 – Illustration of the self-attention mechanism'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_9_3.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.3 – Illustration of the self-attention mechanism
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to learn about the details of the Transformer architecture, you
    can visit http://jalammar.github.io/illustrated-transformer/. This talk on YouTube
    also explains the self-attention mechanism and transformers for all levels of
    NLP developers: [https://www.youtube.com/watch?v=rBCqOTEfxvg](https://www.youtube.com/watch?v=rBCqOTEfxvg).'
  prefs: []
  type: TYPE_NORMAL
- en: We have already seen that there is a variety of transformer architectures and,
    depending on the task, we use different types of transformers for different tasks,
    such as text classification and machine translation. In the rest of this chapter,
    we'll work with a very popular transformer architecture – BERT. Let's see the
    BERT architecture and how to use it in our NLP applications in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding BERT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we''ll explore the most influential and commonly used Transformer
    model, BERT. BERT is introduced in Google''s research paper here: [https://arxiv.org/pdf/1810.04805.pdf](https://arxiv.org/pdf/1810.04805.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: 'What does BERT do exactly? To understand what BERT outputs, let''s dissect
    the name:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bidirectional**: Training on the text data is bi-directional, which means
    each input sentence is processed from left to right as well as from right to left.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Encoder**: An encoder encodes the input sentence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Representations**: A representation is a word vector.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transformers**: The architecture is transformer-based.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BERT is essentially a trained transformer encoder stack. Input into BERT is
    a sentence, and the output is a sequence of word vectors. The word vectors are
    contextual, which means that a word vector is assigned to a word based on the
    input sentence. In short, BERT outputs **contextual word representations**.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have already seen a number of issues that transformers aim to solve in the
    previous section. Another problem that transformers address concerns word vectors.
    In [*Chapter 5*](B16570_05_Final_JM_ePub.xhtml#_idTextAnchor087)*, Working with
    Word Vectors and Semantic Similarity*, we saw that word vectors are context-free;
    the word vector for a word is *always* the same independent of the sentence it
    is used in. The following diagram explains this problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.4 – Word vector for the word "bank"'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_9_4.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.4 – Word vector for the word "bank"
  prefs: []
  type: TYPE_NORMAL
- en: Here, even though the word **bank** has two completely different meanings in
    these two sentences, the word vectors are the same, because Glove and FastText
    are **static**. Each word has only one vector and vectors are saved to a file
    following training. Then, we download these pre-trained vectors and load them
    into our application.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the contrary, BERT word vectors are *dynamic*. BERT can generate different
    word vectors for the same word depending on the input sentence. The following
    diagram shows the word vectors generated by BERT, in contrast to the word vector
    in *Figure 9.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.5 – Two distinct word vectors generated by BERT for the same word,
    "bank," in two different contexts'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_9_5.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.5 – Two distinct word vectors generated by BERT for the same word,
    "bank," in two different contexts
  prefs: []
  type: TYPE_NORMAL
- en: How does BERT generate these word vectors? In the next section, we'll explore
    the details of the BERT architecture.
  prefs: []
  type: TYPE_NORMAL
- en: BERT architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As has already been remarked in the previous section, BERT is a transformer
    encoder stack, which means that several encoder layers are stacked on top of each
    other. The first layer initializes the word vectors randomly, and then each encoder
    layer transforms the output of the previous encoder layer. The paper introduces
    two model sizes for BERT: BERT Base and BERT Large. The following diagram shows
    the BERT architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.6 – BERT Base and Large architectures, having 12 and 24 encoder
    layers, respectively'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_9_6.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.6 – BERT Base and Large architectures, having 12 and 24 encoder layers,
    respectively
  prefs: []
  type: TYPE_NORMAL
- en: Both BERT models have a huge number of encoder layers. BERT Base has 12 encoder
    layers and BERT Large has 24 encoder layers. The dimensions of the resulting word
    vectors are different too; BERT Base generates word vectors of size 768 and BERT
    Large generates word vectors of size 1024.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we remarked in the previous section, BERT outputs word vectors for each
    input word. The following diagram exhibits a high-level overview of BERT inputs
    and outputs (discard the CLS token for now; you''ll learn about it in the *BERT
    input format* section):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.7 – BERT model input word and output word vectors'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_9_7.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.7 – BERT model input word and output word vectors
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, we can see a high-level overview of BERT inputs and
    outputs. Indeed, BERT input has to be in a special format and includes some special
    tokens, such as CLS, in *Figure 9.7*. In the next section, you'll learn about
    the details of the BERT input format.
  prefs: []
  type: TYPE_NORMAL
- en: BERT input format
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have covered BERT architecture, so let's now understand how to generate the
    output vectors using BERT. For this purpose, we'll get to know the BERT input
    data format. The BERT input format can represent a single sentence, as well as
    a pair of sentences (for tasks such as question answering and semantic similarity,
    we input two sentences to the model) in a single sequence of tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'BERT works with a class of `[CLS]`, `[SEP]`, and `[PAD]`:'
  prefs: []
  type: TYPE_NORMAL
- en: The first special token of BERT is `[CLS]`. The first token of every input sequence
    has to be `[CLS]`. We use this token in classification tasks as an aggregate of
    the input sentence. We ignore this token in non-classification tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`[SEP]` means a `[CLS] sentence [SEP]`, and for two sentences, the input looks
    like `[CLS] sentence1 [SEP] sentence2 [SEP]`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`[PAD]` is a special token meaning **padding**. Recall from the previous chapter
    that we use padding values to make sentences in our dataset of equal length. BERT
    receives sentences of a fixed length; hence, we pad the short sentences before
    feeding them to BERT. The maximum length of tokens we can feed to BERT is **512**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How about tokenizing the words? Recall from the previous section that we fed
    a sentence to our Keras model one word at a time. We tokenized our input sentences
    into words using the spaCy tokenizer. BERT works slightly differently, BERT uses
    WordPiece tokenization. A "word piece" is literally a piece of a word. The WordPiece
    algorithm breaks words down into several subwords. The idea is to break down complex/long
    tokens into simpler tokens. For example, the word `playing` is tokenized as `play`
    and `##ing`. A `##` character is placed before every word piece to indicate that
    this token is not a word from the language's vocabulary but that it's a word piece.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at some more examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This way, we represent the language vocabulary more compactly as WordPiece groups
    common subwords. WordPiece tokenization creates wonders on rare/unseen words,
    as these words are broken down into their subwords.
  prefs: []
  type: TYPE_NORMAL
- en: After tokenizing the input sentence and adding the special tokens, each token
    is converted to its ID. After that, as a final step, we feed the sequence of token
    IDs to BERT.
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize, this is how we transform a sentence into BERT input format:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.8 – Transforming an input sentence into BERT input format'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_9_8.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.8 – Transforming an input sentence into BERT input format
  prefs: []
  type: TYPE_NORMAL
- en: BERT Tokenizer has different methods for performing all the tasks described
    previously, but it also has an encoding method that combines these steps into
    a single step. We'll see how to use BERT Tokenizer in detail in the *Transformers
    and TensorFlow* section. Before that, we'll learn about the algorithms that are
    used to train BERT.
  prefs: []
  type: TYPE_NORMAL
- en: How is BERT trained?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'BERT is trained on a large unlabeled Wiki corpus and a huge book corpus. Creators
    of BERT stated the following in Google Research''s BERT GitHub repository, [https://github.com/google-research/bert](https://github.com/google-research/bert),
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '"We then train a large model (12-layer to 24-layer Transformer) on a large
    corpus (Wikipedia + BookCorpus) for a long time (1M update steps), and that''s
    BERT."'
  prefs: []
  type: TYPE_NORMAL
- en: BERT is trained with two training methods, **masked language model** (**MLM**)
    and **next sentence prediction** (**NSP**). Let's first go over the details of
    masked language modeling.
  prefs: []
  type: TYPE_NORMAL
- en: Language modeling is the task of predicting the next token given the sequence
    of previous tokens. For example, given the sequence of words *Yesterday I visited*,
    a language model can predict the next token as one of the tokens *church*, *hospital*,
    *school*, and so on. Masked language modeling is a bit different. In this approach,
    we mask a percentage of the tokens randomly by replacing them with a `[MASK]`
    token and expect MLM to predict the masked words.
  prefs: []
  type: TYPE_NORMAL
- en: 'The masked language model in BERT is implemented as follows: First, 15 of the
    input tokens are chosen at random. Then, the following happens:'
  prefs: []
  type: TYPE_NORMAL
- en: 80% of the tokens chosen are replaced with `[MASK]`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 20% of the tokens chosen are replaced with another token from the vocabulary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The remaining 10% are left unchanged.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A training example sentence to LMM looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will look into the details of the other algorithm, NSP.
  prefs: []
  type: TYPE_NORMAL
- en: As the name suggests, NSP is the task of predicting the next sentence given
    an input sentence. In this approach, we feed two sentences to BERT and expect
    BERT to predict the order of the sentences, more specifically, if the second sentence
    is the sentence following the first sentence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s make an example input to NSP. We''ll feed two sentences separated by
    the `[SEP]` token as input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, the second sentence can follow the first sentence; hence,
    the predicted label is `IsNext`. How about this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This example pair of sentences generate the `NotNext` label, as obviously they
    are not contextually or semantically related.
  prefs: []
  type: TYPE_NORMAL
- en: That's it! We have learned about BERT architecture; we also learned the details
    of BERT input data format and how BERT is trained. Now, we're ready to dive into
    TensorFlow code. In the next section, we'll see how to apply what we learned so
    far in our TensorFlow code.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers and TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll dive into transformers code with TensorFlow. Pre-trained
    transformer models are provided to the developer community as open source by many
    organizations, including Google ([https://github.com/google-research/bert](https://github.com/google-research/bert)),
    Facebook ([https://github.com/pytorch/fairseq/blob/master/examples/language_model/README.md](https://github.com/pytorch/fairseq/blob/master/examples/language_model/README.md)),
    and HuggingFace ([https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)).
    All the listed organizations offer pre-trained models and nice interfaces to integrate
    transformers into our Python code. The interfaces are compatible with either PyTorch
    or Tensorflow or both.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this chapter, we'll be using HuggingFace's pre-trained transformers
    and their TensorFlow interface to the transformer models. HuggingFace is an AI
    company with a focus on NLP and quite devoted to open source. In the next section,
    we'll take a closer look at what is available in HuggingFace Transformers.
  prefs: []
  type: TYPE_NORMAL
- en: HuggingFace Transformers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the first section, we'll discover HuggingFace's pre-trained models, the TensorFlow
    interface for using these models, and HuggingFace model conventions in general.
    We saw in *Figure 9.1* that HuggingFace offers different sorts of models. Each
    model is dedicated to a task such as text classification, question answering,
    and sequence-to-sequence modeling.
  prefs: []
  type: TYPE_NORMAL
- en: The following diagram is taken from the HuggingFace documentation and shows
    details of the distilbert-base-uncased-distilled-squad model. In the documentation,
    first, the task is tagged (upper-left corner of the diagram; the *Question Answering*
    tag), followed by supported deep learning libraries (PyTorch, TensorFlow, TFLite,
    TFSavedModel for this model), the dataset it trained on (squad, in this instance),
    the model language (*en* for English), and the license and base model's name (DistilBERT
    in this case).
  prefs: []
  type: TYPE_NORMAL
- en: 'Some models are trained with similar algorithms, and so belong to the same
    model family. By way of an example, the DistilBERT family includes many models,
    such as distilbert-base-uncased and distilbert-multilingual-cased. Each model
    name also includes some information, such as casing (the model recognizes uppercase/lowercase
    differences) or the model language, such as `en`, `de`, or `multilingual`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.9 – Documentation of the distilbert-base-uncased-distilled-squad
    model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_9_9.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.9 – Documentation of the distilbert-base-uncased-distilled-squad model
  prefs: []
  type: TYPE_NORMAL
- en: 'We already explored BERT in detail in the previous section. HuggingFace documentation
    provides information about each model family and an individual model''s API in
    detail. *Figure 9.10* shows a list of available models and a list of BERT model
    architecture variations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.10 – List of the available models on the left-hand side, with a
    list of the BERT model variations on the right-hand side'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_9_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.10 – List of the available models on the left-hand side, with a list
    of the BERT model variations on the right-hand side
  prefs: []
  type: TYPE_NORMAL
- en: The BERT model has many variations for a variety of tasks, such as text classification,
    question answering, and next sentence prediction. Each of these models is obtained
    by placing some extra layers on top of the BERT output. Recall from the previous
    section that the BERT output is a sequence of word vectors for each word of the
    input sentence. For example, the BERTForSequenceClassification model is obtained
    by placing a dense layer (we covered dense layers in the previous chapter) on
    top of the BERT word vectors.
  prefs: []
  type: TYPE_NORMAL
- en: In the rest of this chapter, we'll explore how to use some of these architectures
    for our tasks as well as how to use BERT word vectors with Keras. Before all of
    these tasks, we will start with the basic task of tokenization to prepare our
    input sentences. Let's see the tokenizer code in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Using the BERT tokenizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the *Understanding BERT* section, we already saw that BERT uses the WordPiece
    algorithm for tokenization. Every input word is broken down into subwords. Let's
    see how to prepare our input data with the HuggingFace library.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following lines exhibit the basic usage of the tokenizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are the steps we followed in the preceding code block:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we imported `BertTokenizer`. Different models have different tokenizers;
    for instance, XLNet model's tokenizer is called `XLNetTokenizer`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Second, we called the `from_pretrained` method on the tokenizer object and provided
    the model's name. Note that we don't need to download the pre-trained bert-base-uncased
    model manually; this method downloads the model by itself.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we called the `tokenize` method. `tokenize` basically tokenizes the sentence
    by breaking all the words down into subwords.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We print tokens to examine the subwords. The words "he," "lived," "idle," and
    so on exist in Tokenizer's vocabulary, and so are kept as they are. "Characteristically"
    is a rare word, so does not exist in Tokenizer's vocabulary. Then, Tokenizer splits
    this word into the subwords "characteristic" and "##ally." Notice that "##ally"
    starts with the characters "##" to emphasize the fact that this is a piece of
    word.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we convert tokens to their token IDs by calling `convert_tokens_to_ids`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'How about `[CLS]` and `[SEP]` tokens? In the previous section, we already saw
    that we have to add these two special tokens to the beginning and end of the input
    sentence. For the preceding code, we need to involve one more step and add our
    special tokens manually. Can we do all of these preprocessing steps in a single
    step, perhaps? The answer is yes; BERT provides a method called `encode` that
    does the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Adds `CLS` and `SEP` tokens to the input sentence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tokenizes the sentence by breaking the tokens down into subwords
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converts the tokens to their token IDs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We call the `encode` method directly on the input sentence as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This code segment outputs the token IDs in just a single step, instead of calling
    `tokenize` and `convert_tokens_to_ids` one after the other. The result is a Python
    list.
  prefs: []
  type: TYPE_NORMAL
- en: 'How about padding the sentence? We already saw in the previous section that
    all the input sentences in a dataset should be of equal length because BERT cannot
    process variable-length sentences. Hence, we need to pad the short sentences to
    the length of the longest sentence available in the dataset. Also, if we want
    to use a TensorFlow tensor instead of a plain list, we need to write some conversion
    code. The HuggingFace library provides `encode_plus` to make our life easier and
    combine all these steps into one method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Here, we called `encode_plus` on our input sentence directly. Our sentence is
    now padded to a length of 12 (the two `0` IDs at the end of the sequence are the
    pad tokens), while the special tokens `[CLS]` and `[SEP]` are also added to the
    sentence. The output is directly a TensorFlow tensor, including the token IDs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `encode_plus` method takes the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`text`: Input sentence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`add_special_tokens`: Add `CLS` and `SEP` tokens.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_length`: The maximum length you want your sentence to be. If the sentence
    is shorter than `max_length` tokens, we want to pad the sentence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pad_to_max_length`: We feed `True` if we want to pad the sentence, otherwise
    `False`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_tensors`: We pass this parameter if we want the output to be a tensor,
    otherwise the output is a Python list. The available options are `tf` and `pt`
    for TensorFlow and PyTorch, respectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we can see, BERT Tokenizer provides several methods on input sentences. Preparing
    data is not so straightforward, but you'll get used to it by practicing. We always
    encourage you to try out the code examples with your own text.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we're ready to process the transformed input sentences. Let's go ahead
    and provide our input sentences to the BERT model to obtain BERT word vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Always check the name of the tokenizer class that you should use with your transformer.
    A list of models and their corresponding tokenizers is available at [https://huggingface.co/transformers/](https://huggingface.co/transformers/).
  prefs: []
  type: TYPE_NORMAL
- en: Obtaining BERT word vectors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we'll examine the output of the BERT model. As we stated in
    the *Understanding BERT* section, the output of the BERT model is a sequence of
    word vectors, one vector per input word. BERT has a special output format and,
    in this section, we'll examine BERT outputs in detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see the code first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This code is very similar to the code from the previous section. Here, we also
    imported `TFBertModel`. After that, we initialized out BERT model with the pre-trained
    model, `bert-base-uncased`. Then, we transformed our input sentence to BERT input
    format with `encode_plus` and captured the result, `tf.tensor`, in the input variable.
    We fed our sentence to the BERT model and captured this output with the `outputs`
    variable. What's inside the `outputs` variable then?
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the BERT model is a tuple of two elements. Let''s print the shapes
    of the output pair:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The first element of the output is the shape `(batch size, sequence length,
    hidden size)`. We fed only one sentence, hence the batch size here is 1 (the batch
    size is how many sentences we feed to the model at once). The sequence length
    here is 10 because we fed `max_length=10` to the tokenizer and padded our sentence
    to a length of 10\. `hidden_size` is a parameter of BERT. In the *BERT Architecture*
    section, we already remarked that BERT's hidden layer size is 768, and so produces
    word vectors with a dimension of 768\. So, the first output element contains 768-dimensional
    vectors per word, hence it contains 10 words x 768-dimensional vectors.
  prefs: []
  type: TYPE_NORMAL
- en: The second output is only one vector of 768-dimension. This vector is basically
    the word embedding of the `[CLS]` token. Recall from the *BERT input format* section
    that the `[CLS]` token is an aggregate of the whole sentence. You can think of
    `[CLS]` token's embedding as the pooled version of embeddings of all the words
    in the sentence. The shape of the second element of the output tuple is always
    (`batch size,` `hidden_size`). Basically, we collect the `[CLS]` token's embedding
    per input sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Great! We extracted the BERT embeddings. Next, we'll use these embeddings to
    train our text classification model with TensorFlow and tf.keras.
  prefs: []
  type: TYPE_NORMAL
- en: Using BERT for text classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we''ll train a binary text classifier with BERT and tf.keras.
    We''ll reuse some of the code from the previous chapter, but this time the code
    will be much shorter because we''ll replace the embedding and LSTM layers with
    BERT. The complete code is available at the GitHub repository: [https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter09/BERT_spam.ipynb](https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter09/BERT_spam.ipynb).ipynb.
    In this section, we''ll skip the data preparation. We used the SMS Spam Collection
    dataset from Kaggle. You can find the dataset under the `data/` directory at the
    GitHub repository, too.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s get started by importing the BERT models and tokenizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We have imported the `BertTokenizer` tokenizer and the BERT model, `TFBertModel`.
    We initialized both the tokenizer and the BERT model with the pre-trained bert-base-uncased
    model. Notice that the model's name starts with TF – the names of all of the HuggingFace
    pre-trained models for TensorFlow start with TF. Please pay attention to this
    detail when you want to play with other transformer models in the future.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll also import Keras layers and functions, together with `numpy`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we''re ready to process the input data with `BertTokenizer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: As we saw in the *Using the BERT tokenizer* section, this code segment will
    generate token IDs for each input sentence of the dataset and append them to a
    list. Labels are the list of class labels and consist of 0 and 1s. Then we convert
    the Python lists, `input_ids`, and labels to `numpy` arrays to feed them to our
    Keras model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we define our Keras model by means of the following lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'That''s it! We defined our BERT-based text classifier in only five lines of
    code! Let''s dissect the code:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we defined the input layer, which inputs the sentences to our model.
    The shape is `(64,)` because each input sentence is 64 tokens in length. We padded
    each sentence to the length of 64 tokens when we called the `encode_plus` method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we fed the input sentences to the BERT model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At the third line, we extracted the second output of the BERT output. Recall
    from the previous section that the BERT model's output is a tuple. The first element
    of the output tuple is a sequence of word vectors, and the second element is a
    single vector that represents the whole sentence, called `bert[1]` extracts the
    pooled output vector; this is a vector of shape (1, 768).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we squashed the pooled output vector to a vector of shape 1 by a sigmoid
    function, which is the class label.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We defined our Keras model with the inputs and outputs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Here, the BERT model takes only one line but can transfer the enormous knowledge
    of the Wiki corpus to your model. At the end of the training, this model obtains
    an accuracy of `0.96`. We usually fit the model for one epoch due to fact that
    BERT overfits easily even on a moderate size corpus.
  prefs: []
  type: TYPE_NORMAL
- en: 'The rest of the code handles compiling and fitting the Keras model. Note that
    BERT has huge memory requirements also. You can see how much RAM is required from
    Google Research''s GitHub link: [https://github.com/google-research/bert#out-of-memory-issues](https://github.com/google-research/bert#out-of-memory-issues).'
  prefs: []
  type: TYPE_NORMAL
- en: If you have trouble running this section's code on your machine, you can use
    **Google Colab**, which provides a Jupyter notebook environment through your browser.
    You can start using Google Colab immediately through [https://colab.research.google.com/notebooks/intro.ipynb](https://colab.research.google.com/notebooks/intro.ipynb).
    Our training code runs on Google Colab for around 1.5 hours, while bigger datasets
    can take more time even though it's just one epoch.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we learned how to train a Keras model with BERT from scratch.
    Now, we'll switch to an easier task. We'll explore how to use a pre-trained transformer
    pipeline. Let's move on to the next section for the details.
  prefs: []
  type: TYPE_NORMAL
- en: Using Transformer pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The HuggingFace Transformers library provides pipelines to help developers benefit
    from transformer code immediately without any custom training. A **pipeline**
    is a tokenizer and a pre-trained model combined.
  prefs: []
  type: TYPE_NORMAL
- en: 'HuggingFace provides a variety of models for a variety of NLP tasks. Here are
    some tasks that HuggingFace pipelines offer:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sentiment analysis**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Question answering**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NER**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text summarization**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Translation**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can see the full list of tasks at the Huggingface documentation: [https://huggingface.co/transformers/task_summary.html](https://huggingface.co/transformers/task_summary.html).
    In this section, we''ll explore the pipelines for sentiment analysis and question
    answering (the use of pipelines with other tasks is similar).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s go through some examples. We''ll start with sentiment analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code snippet, we took the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we imported the pipeline function from the `transformers` library. This
    function creates pipeline objects with the task name given as a parameter. Hence,
    we created our sentiment analysis pipeline object, `nlp`, by calling this function
    on the second line.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we define two example sentences with negative and positive sentiment,
    respectively.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then we feed these sentences to the pipeline object, `nlp`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This worked great! Next, we''ll play with question answering. Let''s see the
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Again, we imported the pipeline function and used it to create a pipeline object,
    `nlp`. In *question-answering* tasks, we need to provide a context (the same background
    information for the model to work on) to the model as well as our question. We
    asked the model about the name of this book after giving the information that
    our new publication will be out soon. The answer is `Mastering spaCy`; the transformer
    worked wonders on this pair! We encourage you to try out your own examples.
  prefs: []
  type: TYPE_NORMAL
- en: We have completed our exploration of HuggingFace transformers. Now, we will
    move on to our final section of this chapter and see what spaCy offers us as regards
    transformers.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers and spaCy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'spaCy v3.0 was released with great new features and components. The most exciting
    new feature is undoubtedly **transformer-based pipelines**. The new transformer-based
    pipelines bring spaCy''s accuracy to the state of the art. Integrating transformers
    into the spaCy NLP pipeline introduced one more pipeline component called **Transformer**.
    This component allows us to use all HuggingFace models with spaCy pipelines. If
    we recall from [*Chapter 2*](B16570_02_Final_JM_ePub.xhtml#_idTextAnchor037),
    *Core Operations with spaCy*, this is what the spaCy NLP pipeline looks like without
    transformers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ Figure 9.11 – Vector-based spaCy pipeline components'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_9_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.11 – Vector-based spaCy pipeline components
  prefs: []
  type: TYPE_NORMAL
- en: 'With the release of v3.0, v2 style spaCy models are still supported and transformer-based
    models are introduced. A transformer-based pipeline component looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.12 – Transformed-based spaCy pipeline components'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_9_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.12 – Transformed-based spaCy pipeline components
  prefs: []
  type: TYPE_NORMAL
- en: 'For each supported language, transformer-based models and v2 style models are
    listed under the Models page of the documentation (English for an example: [https://spacy.io/models/en](https://spacy.io/models/en)).
    Transformer-based models can have different size and pipeline components, just
    like v2 style models. Also, each model has corpus and genre information as well,
    just like the v2 style models. Here is an example of an English transformer-based
    language model from the Models page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.13 – spaCy English transformer-based language models'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_9_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.13 – spaCy English transformer-based language models
  prefs: []
  type: TYPE_NORMAL
- en: As we see from the preceding screenshot, the first pipeline component is a transformer
    and the rest of the pipeline components are the ones we already covered in [*Chapter
    3*](B16570_03_Final_JM_ePub.xhtml#_idTextAnchor055), *Linguistic Features*. The
    transformer component generates the word representations and deals with the WordPiece
    algorithm to tokenize words into subwords. The word vectors are fed to the rest
    of the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'Downloading, loading, and using transformer-based models are identical to v2
    style models. Currently, English has two pre-trained transformer-based models,
    `en_core_web_trf` and `en_core_web_lg`. Let''s get started by downloading the
    `en_core_web_trf` model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This should produce output similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the model download is complete, the following output should be generated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Loading a transformer-based model is identical to what we do for v2 style models,
    too:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'After loading our model and initializing the pipeline, we can use this model
    in the same way we used v2 style models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'So far so good, but what''s new then? Let''s examine some features that come
    from the transformer component. We can access the features related to the transformer
    component via `doc._.trf_data.trf_data`, which contains the word pieces, input
    `ids`, and vectors that are generated by the transformer. Let''s examine the features
    one by one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding output, we see five elements: word pieces, input IDs, attention
    masks, lengths, and token type IDs. Word pieces are the subwords that are generated
    by the WordPiece algorithm. The word pieces of this sentence are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Here, `<s>` and `</s>` are special tokens and are used in the sentence at the
    beginning and end. The word `unwillingly` is divided into three subwords – `unw`,
    `ill`, and `ingly`. The `G` character is used to mark the word boundaries. The
    tokens without `G` are subwords, such as `ill` and `ingly` in the preceding word
    piece list (with the exception of the first word in the sentence, the first word
    of the sentence is marked by `<s>`).
  prefs: []
  type: TYPE_NORMAL
- en: Next, we have to take a look at `input_ids.` Input IDs have the same meaning
    as the input IDs we introduced in the *Using the BERT tokenizer* section. These
    are basically the subword IDs assigned by the transformer's tokenizer.
  prefs: []
  type: TYPE_NORMAL
- en: The attention mask is a list of 0s and 1s for pointing the transformer to those
    tokens it should pay attention to. `0` corresponds to `PAD` tokens, while all
    the other tokens should have a corresponding `1`.
  prefs: []
  type: TYPE_NORMAL
- en: '`lengths` is the length of the sentence after breaking it down into subwords.
    Here, it''s `9` obviously, but notice that `len(doc)` outputs `5`, while spaCy
    always operates on linguistic words.'
  prefs: []
  type: TYPE_NORMAL
- en: '`token_type_ids` are used by transformer tokenizers to mark the sentence boundaries
    for two sentence input tasks, such as question and answering. Here, we provide
    only one text, hence this feature is not applicable.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that the token vectors generated by the transformer, `doc._.trf_data.tensors`,
    contain the output of the transformer, a sequence of word vectors per word, and
    the pooled output vector (we introduced these concepts in the *Obtaining BERT
    word vectors* section. If you need to refresh your memory, please refer to this
    section):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The first element of the tuple is the vectors for the tokens. Each vector is
    `768`-dimensional; hence `9` words produce `9` x `768`-dimensional vectors. The
    second element of the tuple is the pooled output vector, which is an aggregate
    representation for the input sentence, and so is of the shape `1`x`768`.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our exploration of spaCy transformer-based pipelines. Once again,
    we saw that spaCy provides user-friendly API and packaging, even for complicated
    models such as transformers. Transformer integration is yet another great reason
    to use spaCy for NLP.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You have completed an exhaustive chapter about a very hot topic in NLP. Congratulations!
    In this chapter, you started by learning what sort of models transformers are
    and what transfer learning is. Then, you learned about the commonly used Transformer
    architecture, BERT. You learned the architecture details and the specific input
    format, as well as the BERT Tokenizer and WordPiece algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you became familiar with BERT code by using the popular HuggingFace Transformers
    library. You practiced fine-tuning BERT on a custom dataset for a sentiment analysis
    task with TensorFlow and Keras. You also practiced using pre-trained HuggingFace
    pipelines for a variety of NLP tasks, such as text classification and question
    answering. Finally, you explored the spaCy and Transformers integration of the
    new spaCy release, spaCy v3.0.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you had completed the statistical NLP sections of
    this book. Now you're ready to put everything you learned together to build a
    modern NLP pipeline. Let's move on to the next chapter and see how we use our
    new statistical skills!
  prefs: []
  type: TYPE_NORMAL
