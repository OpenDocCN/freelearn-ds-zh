- en: 'Chapter 9: spaCy and Transformers'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章：spaCy和Transformers
- en: In this chapter, you will learn about the latest hot topic in NLP, transformers,
    and how to use them with TensorFlow and spaCy.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将了解自然语言处理（NLP）的最新热门话题——transformers，以及如何使用TensorFlow和spaCy来使用它们。
- en: First, you will learn about transformers and transfer learning. Second, you'll
    learn about the architecture details of the commonly used Transformer architecture
    – **Bidirectional Encoder Representations from Transformers** (**BERT**). You'll
    also learn how **BERT Tokenizer** and **WordPiece** algorithms work. Then you
    will learn how to quickly get started with pre-trained transformer models of the
    **HuggingFace** library. Next, you'll practice how to fine-tune HuggingFace Transformers
    with TensorFlow and Keras. Finally, you'll learn how *spaCy v3.0* integrates transformer
    models as pre-trained pipelines.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你将了解transformers和迁移学习。其次，你将学习常用Transformer架构——**双向Transformer编码器表示**（**BERT**）的架构细节。你还将了解**BERT分词器**和**WordPiece**算法是如何工作的。然后，你将学习如何快速开始使用HuggingFace库中预训练的transformers模型。接下来，你将练习如何使用TensorFlow和Keras微调HuggingFace
    Transformers。最后，你将了解*spaCy v3.0*如何将transformers模型作为预训练管道集成。
- en: By the end of this chapter, you will be completing the statistical NLP topics
    of this book. You will add your knowledge of transformers to the knowledge of
    Keras and TensorFlow that you acquired in [*Chapter 8*](B16570_08_Final_JM_ePub.xhtml#_idTextAnchor137),
    *Text Classification with spaCy*. You'll be able to build state-of-the-art NLP
    models with just a few lines of code with the power of Transformer models and
    transfer learning.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将完成本书的统计自然语言处理（NLP）主题。你将把你在[*第8章*](B16570_08_Final_JM_ePub.xhtml#_idTextAnchor137)“使用spaCy进行文本分类”中获得的Keras和TensorFlow知识与你对transformers的知识相结合。你将能够仅用几行代码，借助Transformer模型和迁移学习的能力，构建最先进的NLP模型。
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Transformers and transfer learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformers和迁移学习
- en: Understanding BERT
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解BERT
- en: Transformers and TensorFlow
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformers和TensorFlow
- en: Transformers and spaCy
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformers和spaCy
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, we''ll use the `transformers` and `tensorflow` Python libraries
    along with `spaCy`. You can install these libraries via `pip`:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用`transformers`和`tensorflow` Python库以及`spaCy`。你可以通过`pip`安装这些库：
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The chapter code can be found at the book''s GitHub repository: [https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter09](https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter09).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在本书的GitHub仓库中找到：[https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter09](https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter09)。
- en: Transformers and transfer learning
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformers和迁移学习
- en: 'A milestone in NLP happened in 2017 with the release of the research paper
    *Attention Is All You Need*, by Vaswani et al. ([https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)),
    which introduced a brand-new machine learning idea and architecture – transformers.
    Transformers in NLP is a fresh idea that aims to solve sequential modeling tasks
    and targets some problems introduced by **long short-term memory** (**LSTM**)
    architecture (recall LSTM architecture from [*Chapter 8*](B16570_08_Final_JM_ePub.xhtml#_idTextAnchor137),
    *Text Classification with spaCy*). Here''s how the paper explains how transformers
    work:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年，随着Vaswani等人发表的研究论文《Attention Is All You Need》的发布，自然语言处理（NLP）领域发生了一个里程碑事件（[https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)），该论文介绍了一种全新的机器学习思想和架构——transformers。在NLP中的Transformers是一个新颖的想法，旨在解决序列建模任务，并针对**长短期记忆**（**LSTM**）架构提出的一些问题（回想一下[*第8章*](B16570_08_Final_JM_ePub.xhtml#_idTextAnchor137)“使用spaCy进行文本分类”中的LSTM架构）。以下是论文如何解释transformers的工作原理：
- en: '"The Transformer is the first transduction model relying entirely on self-attention
    to compute representations of its input and output without using sequence-aligned
    RNNs or convolution."'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: “Transformer是第一个完全依赖自注意力来计算其输入和输出表示的转导模型，而不使用序列对齐的RNN或卷积。”
- en: Transduction in this context means transforming input words to output words
    by transforming input words and sentences into vectors. Typically, a transformer
    is trained on a huge corpus such as Wiki or news. Then, in our downstream tasks,
    we use these vectors as they carry information regarding the word semantics, sentence
    structure, and sentence semantics (we'll see how to use the vectors precisely
    in our code in the *Transformers and TensorFlow* section).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个上下文中，转导意味着通过将输入单词和句子转换为向量来转换输入单词和句子。通常，一个变压器在像 Wiki 或新闻这样的大型语料库上训练。然后，在我们的下游任务中，我们使用这些向量，因为它们携带有关词义、句子结构和句子语义的信息（我们将在
    *Transformers 和 TensorFlow* 部分中看到如何精确地使用这些向量）。
- en: We already explored the idea of pre-trained word vectors in [*Chapter 5*](B16570_05_Final_JM_ePub.xhtml#_idTextAnchor087),
    *Working with Word Vectors and Semantic Similarity*. Word vectors such as Glove
    and FastText vectors are already trained on the Wikipedia corpus and we used them
    directly for our semantic similarity calculations. In this way, we imported information
    about word semantics from the Wiki corpus into our semantic similarity calculations.
    Importing knowledge from pre-trained word vectors or pre-trained statistical models
    is called **transfer learning**.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在 [*第 5 章*](B16570_05_Final_JM_ePub.xhtml#_idTextAnchor087)，*使用词向量与语义相似性*
    中探讨了预训练词向量的想法。Glove 和 FastText 等词向量已经在维基百科语料库上进行了训练，我们直接使用它们进行我们的语义相似性计算。通过这种方式，我们将来自
    Wiki 语料库的词义信息导入到我们的语义相似性计算中。从预训练词向量或预训练统计模型中导入知识被称为 **迁移学习**。
- en: Transformers offer thousands of pre-trained models to perform NLP tasks, such
    as text classification, text summarization, question answering, machine translation,
    and natural language generation in more than 100 languages. Transformers aim to
    make state-of-the-art NLP accessible to everyone.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers 提供了数千个预训练模型来执行 NLP 任务，例如文本分类、文本摘要、问答、机器翻译以及超过 100 种语言的自然语言生成。Transformers
    的目标是使最先进的 NLP 对每个人可访问。
- en: The following screenshot shows a list of the Transformer models provided by
    HuggingFace (we'll learn about HuggingFace Transformers in the *HuggingFace Transformers*
    section). Each model is named with a combination of the architecture name (*Bert,
    DistilBert*, and so on), possibly the language code (*en*, *de*, *multilingual*,
    and similar, given on the left side of the following screenshot), and information
    regarding whether the model is cased or uncased (the model distinguishes between
    uppercase and lowercase characters).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了 HuggingFace 提供的 Transformer 模型列表（我们将在 *HuggingFace Transformers* 部分中了解
    HuggingFace Transformers）。每个模型都由架构名称（*Bert、DistilBert* 等）、可能的语言代码（*en*、*de*、*multilingual*
    等，如以下截图左侧所示）以及有关模型是否区分大小写的信息（模型区分大小写字符）的组合命名。
- en: 'Also, on the left-hand side of *Figure 9.1*, we see the task names. Each model
    is labeled with a task name. We select a model that is suitable for our task,
    such as text classification or machine translation:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在 *图 9.1* 的左侧，我们看到任务名称。每个模型都标有任务名称。我们选择适合我们任务的模型，例如文本分类或机器翻译：
- en: '![Figure 9.1 – A list of HuggingFace Transformers, taken from the HuggingFace
    website'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.1 – HuggingFace Transformers 列表，摘自 HuggingFace 网站'
- en: '](img/B16570_9_1.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16570_9_1.jpg)'
- en: Figure 9.1 – A list of HuggingFace Transformers, taken from the HuggingFace
    website
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1 – HuggingFace Transformers 列表，摘自 HuggingFace 网站
- en: 'To understand what''s great about transformers, we''ll first revisit LSTM architecture
    from [*Chapter 8*](B16570_08_Final_JM_ePub.xhtml#_idTextAnchor137), *Text Classification
    with spaCy*. In the previous chapter, we already stepped into the statistical
    modeling world with Keras and LSTM architecture. LSTMs are great for modeling
    text; however, they have some shortcomings too:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解变压器的好处，我们首先将回顾来自 [*第 8 章*](B16570_08_Final_JM_ePub.xhtml#_idTextAnchor137)，*使用
    spaCy 进行文本分类* 的 LSTM 架构。在前一章中，我们已经使用 Keras 和 LSTM 架构进入了统计建模的世界。LSTM 在建模文本方面非常出色；然而，它们也有一些缺点：
- en: LSTM architecture sometimes has difficulties with learning long text. Statistical
    dependencies in a long text can be difficult to represent by an LSTM because,
    as the time steps pass, LSTM can forget some of the words that were processed
    at earlier time steps.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSTM 架构在处理长文本时有时会遇到学习困难。由于随着时间步的推移，LSTM 可能会忘记之前处理的一些单词，因此在长文本中的统计依赖关系可能难以通过
    LSTM 来表示。
- en: The nature of LSTMs is sequential. We process one word at each time step. Obviously,
    parallelizing the learning process is not possible; we have to process sequentially.
    Not allowing parallelization creates a performance bottleneck.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSTM的本质是序列性的。我们每个时间步处理一个单词。显然，并行化学习过程是不可能的；我们必须按顺序处理。不允许并行化造成性能瓶颈。
- en: 'Transformers address these problems by not using recurrent layers at all. If
    we have a look at the following, the architecture looks completely different from
    an LSTM architecture. Transformer architecture consists of two parts – an input
    encoder (called the **Encoder**) block on the left, and the output decoder (called
    the **Decoder**) block on the right. The following diagram is taken from this
    paper and exhibits the transformer architecture:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Transformers通过完全不使用循环层来解决这些问题。如果我们看一下以下内容，其架构与LSTM架构完全不同。Transformer架构由两部分组成
    – 左侧的输入编码器块（称为**编码器**）和右侧的输出解码器块（称为**解码器**）。以下图表取自这篇论文，展示了Transformer架构：
- en: '![Figure 9.2 – Transformer architecture from the paper entitled "Attention
    is All You Need"'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.2 – 来自论文“Attention is All You Need”的Transformer架构'
- en: '](img/B16570_9_2.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16570_9_2.jpg'
- en: Figure 9.2 – Transformer architecture from the paper entitled "Attention is
    All You Need"
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2 – 来自论文“Attention is All You Need”的Transformer架构
- en: The preceding architecture is invented for a machine translation task; hence,
    the input is a sequence of words from the source language, and the output is a
    sequence of words in the target language. The encoder generates a vector representation
    of the input words and passes them to the decoder (the word vector transfer is
    represented by the arrow from the encoder block in the direction of the decoder
    block). The decoder takes these input word vectors, transforms the output words
    into word vectors, and finally generates the probability of each output word (labeled
    in *Figure 9.2* as **Output Probabilities**).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 上述架构是为机器翻译任务发明的；因此，输入是源语言的单词序列，输出是目标语言的单词序列。编码器生成输入单词的向量表示，并将它们传递给解码器（单词向量传递由编码器块指向解码器块的箭头表示）。解码器接收这些输入单词向量，将输出单词转换为单词向量，并最终生成每个输出单词的概率（在*图9.2*中标为**输出概率**）。
- en: Inside the encoder and decoder blocks, we see feedforward layers, which are
    basically a dense layer we used in [*Chapter 8*](B16570_08_Final_JM_ePub.xhtml#_idTextAnchor137),
    *Text Classification with spaCy*. The innovation transformers bring lies in the
    **Multi-Head Attention** block. This block creates a dense representation for
    each word by using a self-attention mechanism. The **Self-attention** mechanism
    relates each word in the input sentence to the other words in the input sentence.
    The word embedding of each word is calculated by taking a weighted average of
    the other words' embeddings. This way, the importance of each word in the input
    sentence is calculated, so the architecture focuses its *attention* on each input
    word in turn.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在编码器和解码器块内部，我们看到前馈层，这基本上是我们用于[*第8章*](B16570_08_Final_JM_ePub.xhtml#_idTextAnchor137)中*Text
    Classification with spaCy*的密集层。Transformer带来的创新在于**多头注意力**块。该块通过使用自注意力机制为每个单词创建一个密集表示。**自注意力**机制将输入句子中的每个单词与输入句子中的其他单词相关联。每个单词的词嵌入是通过取其他单词嵌入的加权平均来计算的。这样，就可以计算输入句子中每个单词的重要性，因此架构依次关注每个输入单词。
- en: 'The following diagram is taken from the original paper and illustrates self-attention.
    The diagram illustrates how the input words on the left-hand side attend the input
    word "**it**" on the right-hand side. Darker colors mean more relevance, hence
    the words "**the animal**" are more related to "**it**" rather than the other
    words in this sentence. What does this mean? This means that the transformer can
    resolve what the pronoun "**it**" refers to precisely in this sentence, which
    is the phrase "**the animal**." This is a great accomplishment of transformers;
    they can resolve many semantic dependencies in a given sentence:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表取自原始论文，并说明了自注意力机制。该图表展示了左侧的输入单词如何关注右侧的输入单词“**it**”。较深的颜色表示相关性更高，因此单词“**the
    animal**”与“**it**”比句子中的其他单词更相关。这意味着什么？这意味着Transformer可以精确地解析这个句子中代词“**it**”所指的内容，即短语“**the
    animal**”。这是Transformer的一个重大成就；它们可以在给定的句子中解析许多语义依赖关系：
- en: '![Figure 9.3 – Illustration of the self-attention mechanism'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.3 – 自注意力机制的示意图'
- en: '](img/B16570_9_3.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16570_9_3.jpg'
- en: Figure 9.3 – Illustration of the self-attention mechanism
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3 – 自注意力机制的示意图
- en: 'If you want to learn about the details of the Transformer architecture, you
    can visit http://jalammar.github.io/illustrated-transformer/. This talk on YouTube
    also explains the self-attention mechanism and transformers for all levels of
    NLP developers: [https://www.youtube.com/watch?v=rBCqOTEfxvg](https://www.youtube.com/watch?v=rBCqOTEfxvg).'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解转换器架构的细节，你可以访问http://jalammar.github.io/illustrated-transformer/。这个YouTube上的演讲也解释了所有NLP开发者的自注意力机制和转换器：[https://www.youtube.com/watch?v=rBCqOTEfxvg](https://www.youtube.com/watch?v=rBCqOTEfxvg)。
- en: We have already seen that there is a variety of transformer architectures and,
    depending on the task, we use different types of transformers for different tasks,
    such as text classification and machine translation. In the rest of this chapter,
    we'll work with a very popular transformer architecture – BERT. Let's see the
    BERT architecture and how to use it in our NLP applications in the next section.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，有各种各样的转换器架构，根据任务的不同，我们使用不同类型的转换器来完成不同的任务，例如文本分类和机器翻译。在本章的剩余部分，我们将使用一个非常流行的转换器架构——BERT。让我们在下一节中看看BERT的架构以及如何在我们的NLP应用中使用它。
- en: Understanding BERT
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解BERT
- en: 'In this section, we''ll explore the most influential and commonly used Transformer
    model, BERT. BERT is introduced in Google''s research paper here: [https://arxiv.org/pdf/1810.04805.pdf](https://arxiv.org/pdf/1810.04805.pdf).'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨最具有影响力和最常用的转换器模型——BERT。BERT在这里的谷歌研究论文中介绍：[https://arxiv.org/pdf/1810.04805.pdf](https://arxiv.org/pdf/1810.04805.pdf)。
- en: 'What does BERT do exactly? To understand what BERT outputs, let''s dissect
    the name:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: BERT究竟做了什么？为了理解BERT的输出，让我们剖析一下这个名字：
- en: '**Bidirectional**: Training on the text data is bi-directional, which means
    each input sentence is processed from left to right as well as from right to left.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**双向**：在文本数据上的训练是双向的，这意味着每个输入句子都从左到右以及从右到左进行处理。'
- en: '**Encoder**: An encoder encodes the input sentence.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码器**：编码器将输入句子进行编码。'
- en: '**Representations**: A representation is a word vector.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**表示**：表示是一个词向量。'
- en: '**Transformers**: The architecture is transformer-based.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**转换器**：其架构基于转换器。'
- en: BERT is essentially a trained transformer encoder stack. Input into BERT is
    a sentence, and the output is a sequence of word vectors. The word vectors are
    contextual, which means that a word vector is assigned to a word based on the
    input sentence. In short, BERT outputs **contextual word representations**.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: BERT本质上是一个经过训练的转换器编码器堆栈。BERT的输入是一个句子，输出是一系列词向量。这些词向量是上下文相关的，这意味着一个词向量是根据输入句子分配给一个词的。简而言之，BERT输出**上下文词表示**。
- en: 'We have already seen a number of issues that transformers aim to solve in the
    previous section. Another problem that transformers address concerns word vectors.
    In [*Chapter 5*](B16570_05_Final_JM_ePub.xhtml#_idTextAnchor087)*, Working with
    Word Vectors and Semantic Similarity*, we saw that word vectors are context-free;
    the word vector for a word is *always* the same independent of the sentence it
    is used in. The following diagram explains this problem:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上一节中已经看到了转换器试图解决的问题。转换器解决的另一个问题与词向量有关。在[*第五章*](B16570_05_Final_JM_ePub.xhtml#_idTextAnchor087)*，处理词向量和语义相似性*中，我们看到了词向量是无上下文的；一个词的词向量*总是*相同的，不依赖于它所在的句子。以下图表解释了这个问题：
- en: '![Figure 9.4 – Word vector for the word "bank"'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.4 – “bank”这个词的词向量'
- en: '](img/B16570_9_4.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16570_9_4.jpg)'
- en: Figure 9.4 – Word vector for the word "bank"
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4 – “bank”这个词的词向量
- en: Here, even though the word **bank** has two completely different meanings in
    these two sentences, the word vectors are the same, because Glove and FastText
    are **static**. Each word has only one vector and vectors are saved to a file
    following training. Then, we download these pre-trained vectors and load them
    into our application.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，尽管这两个句子中的“bank”这个词有两个完全不同的含义，但词向量是相同的，因为Glove和FastText是**静态的**。每个词只有一个向量，向量在训练后保存到文件中。然后，我们下载这些预训练的向量并将它们加载到我们的应用程序中。
- en: 'On the contrary, BERT word vectors are *dynamic*. BERT can generate different
    word vectors for the same word depending on the input sentence. The following
    diagram shows the word vectors generated by BERT, in contrast to the word vector
    in *Figure 9.4*:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，BERT的词向量是**动态的**。BERT可以根据输入句子为同一个词生成不同的词向量。以下图表显示了BERT生成的词向量，与*图9.4*中的词向量形成对比：
- en: '![Figure 9.5 – Two distinct word vectors generated by BERT for the same word,
    "bank," in two different contexts'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.5 – BERT在两个不同语境下为同一单词“bank”生成的两个不同的词向量'
- en: '](img/B16570_9_5.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16570_9_5.jpg)'
- en: Figure 9.5 – Two distinct word vectors generated by BERT for the same word,
    "bank," in two different contexts
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5 – BERT在两个不同语境下为同一单词“bank”生成的两个不同的词向量
- en: How does BERT generate these word vectors? In the next section, we'll explore
    the details of the BERT architecture.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: BERT是如何生成这些词向量的？在下一节中，我们将探讨BERT架构的细节。
- en: BERT architecture
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BERT架构
- en: 'As has already been remarked in the previous section, BERT is a transformer
    encoder stack, which means that several encoder layers are stacked on top of each
    other. The first layer initializes the word vectors randomly, and then each encoder
    layer transforms the output of the previous encoder layer. The paper introduces
    two model sizes for BERT: BERT Base and BERT Large. The following diagram shows
    the BERT architecture:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 正如上一节中已经提到的，BERT是一个transformer编码器堆叠，这意味着几个编码器层堆叠在一起。第一层随机初始化词向量，然后每个编码器层转换前一个编码器层的输出。论文介绍了BERT的两个模型大小：BERT
    Base和BERT Large。以下图表展示了BERT架构：
- en: '![Figure 9.6 – BERT Base and Large architectures, having 12 and 24 encoder
    layers, respectively'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.6 – BERT Base和Large架构，分别有12和24个编码器层'
- en: '](img/B16570_9_6.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16570_9_6.jpg)'
- en: Figure 9.6 – BERT Base and Large architectures, having 12 and 24 encoder layers,
    respectively
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6 – BERT Base和Large架构，分别有12和24个编码器层
- en: Both BERT models have a huge number of encoder layers. BERT Base has 12 encoder
    layers and BERT Large has 24 encoder layers. The dimensions of the resulting word
    vectors are different too; BERT Base generates word vectors of size 768 and BERT
    Large generates word vectors of size 1024.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 两个BERT模型都有大量的编码器层。BERT Base有12个编码器层，BERT Large有24个编码器层。生成的词向量维度也不同；BERT Base生成768大小的词向量，而BERT
    Large生成1024大小的词向量。
- en: 'As we remarked in the previous section, BERT outputs word vectors for each
    input word. The following diagram exhibits a high-level overview of BERT inputs
    and outputs (discard the CLS token for now; you''ll learn about it in the *BERT
    input format* section):'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一节中提到的，BERT为每个输入单词输出词向量。以下图表展示了BERT输入和输出的高级概述（现在暂时忽略CLS标记；你将在*BERT输入格式*部分了解它）：
- en: '![Figure 9.7 – BERT model input word and output word vectors'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.7 – BERT模型输入词和输出词向量'
- en: '](img/B16570_9_7.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16570_9_7.jpg)'
- en: Figure 9.7 – BERT model input word and output word vectors
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7 – BERT模型输入词和输出词向量
- en: In the preceding diagram, we can see a high-level overview of BERT inputs and
    outputs. Indeed, BERT input has to be in a special format and includes some special
    tokens, such as CLS, in *Figure 9.7*. In the next section, you'll learn about
    the details of the BERT input format.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图表中，我们可以看到BERT输入和输出的高级概述。实际上，BERT输入必须以特殊格式存在，并包括一些特殊标记，如*图9.7*中的CLS。在下一节中，你将了解BERT输入格式的细节。
- en: BERT input format
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BERT输入格式
- en: We have covered BERT architecture, so let's now understand how to generate the
    output vectors using BERT. For this purpose, we'll get to know the BERT input
    data format. The BERT input format can represent a single sentence, as well as
    a pair of sentences (for tasks such as question answering and semantic similarity,
    we input two sentences to the model) in a single sequence of tokens.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了BERT架构，现在让我们了解如何使用BERT生成输出向量。为此，我们将了解BERT输入数据格式。BERT输入格式可以表示一个句子，也可以表示一对句子（对于如问答和语义相似度等任务，我们输入两个句子到模型）作为一个标记序列。
- en: 'BERT works with a class of `[CLS]`, `[SEP]`, and `[PAD]`:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: BERT与`[CLS]`、`[SEP]`和`[PAD]`类一起工作：
- en: The first special token of BERT is `[CLS]`. The first token of every input sequence
    has to be `[CLS]`. We use this token in classification tasks as an aggregate of
    the input sentence. We ignore this token in non-classification tasks.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT的第一个特殊标记是`[CLS]`。每个输入序列的第一个标记必须是`[CLS]`。我们在分类任务中使用此标记作为输入句子的聚合。在非分类任务中，我们忽略此标记。
- en: '`[SEP]` means a `[CLS] sentence [SEP]`, and for two sentences, the input looks
    like `[CLS] sentence1 [SEP] sentence2 [SEP]`.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[SEP]`表示`[CLS]句子[SEP]`，对于两个句子，输入看起来像`[CLS]句子1[SEP]句子2[SEP]`。'
- en: '`[PAD]` is a special token meaning **padding**. Recall from the previous chapter
    that we use padding values to make sentences in our dataset of equal length. BERT
    receives sentences of a fixed length; hence, we pad the short sentences before
    feeding them to BERT. The maximum length of tokens we can feed to BERT is **512**.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[PAD]`是一个特殊标记，表示**填充**。回顾前一章，我们使用填充值来使我们的数据集中的句子长度相等。BERT接收固定长度的句子；因此，我们在将句子输入BERT之前对其进行填充。我们可以输入BERT的标记最大长度是**512**。'
- en: How about tokenizing the words? Recall from the previous section that we fed
    a sentence to our Keras model one word at a time. We tokenized our input sentences
    into words using the spaCy tokenizer. BERT works slightly differently, BERT uses
    WordPiece tokenization. A "word piece" is literally a piece of a word. The WordPiece
    algorithm breaks words down into several subwords. The idea is to break down complex/long
    tokens into simpler tokens. For example, the word `playing` is tokenized as `play`
    and `##ing`. A `##` character is placed before every word piece to indicate that
    this token is not a word from the language's vocabulary but that it's a word piece.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 关于分词单词呢？回顾前一小节，我们一次将一个单词输入到我们的Keras模型中。我们使用spaCy分词器将输入句子分词成单词。BERT的工作方式略有不同，BERT使用WordPiece分词。一个“词片”字面上是一个单词的一部分。WordPiece算法将单词分解成几个子词。想法是将复杂/长的标记分解成更简单的标记。例如，单词`playing`被分词为`play`和`##ing`。一个`##`字符放在每个词片之前，表示这个标记不是语言词汇中的单词，而是一个词片。
- en: 'Let''s take a look at some more examples:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看更多的例子：
- en: '[PRE1]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This way, we represent the language vocabulary more compactly as WordPiece groups
    common subwords. WordPiece tokenization creates wonders on rare/unseen words,
    as these words are broken down into their subwords.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们通过WordPiece分组常见的子词更紧凑地表示语言词汇。WordPiece分词在罕见/未见过的单词上创造了奇迹，因为这些单词被分解成它们的子词。
- en: After tokenizing the input sentence and adding the special tokens, each token
    is converted to its ID. After that, as a final step, we feed the sequence of token
    IDs to BERT.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在对输入句子进行分词并添加特殊标记后，每个标记被转换为它的ID。之后，作为最后一步，我们将标记ID序列输入BERT。
- en: 'To summarize, this is how we transform a sentence into BERT input format:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，这是我们如何将句子转换为BERT输入格式的：
- en: '![Figure 9.8 – Transforming an input sentence into BERT input format'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.8 – 将输入句子转换为BERT输入格式'
- en: '](img/B16570_9_8.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16570_9_8.jpg)'
- en: Figure 9.8 – Transforming an input sentence into BERT input format
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.8 – 将输入句子转换为BERT输入格式
- en: BERT Tokenizer has different methods for performing all the tasks described
    previously, but it also has an encoding method that combines these steps into
    a single step. We'll see how to use BERT Tokenizer in detail in the *Transformers
    and TensorFlow* section. Before that, we'll learn about the algorithms that are
    used to train BERT.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: BERT分词器有不同方法来执行之前描述的所有任务，但它还有一个编码方法，将这些步骤合并为单个步骤。我们将在*Transformers和TensorFlow*部分详细说明如何使用BERT分词器。在那之前，我们将学习用于训练BERT的算法。
- en: How is BERT trained?
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BERT是如何训练的？
- en: 'BERT is trained on a large unlabeled Wiki corpus and a huge book corpus. Creators
    of BERT stated the following in Google Research''s BERT GitHub repository, [https://github.com/google-research/bert](https://github.com/google-research/bert),
    as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: BERT是在一个大型未标记的维基百科语料库和庞大的书籍语料库上训练的。BERT的创造者在谷歌研究GitHub仓库中这样描述了BERT，[https://github.com/google-research/bert](https://github.com/google-research/bert)，如下所示：
- en: '"We then train a large model (12-layer to 24-layer Transformer) on a large
    corpus (Wikipedia + BookCorpus) for a long time (1M update steps), and that''s
    BERT."'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: “然后我们在一个大型语料库（维基百科 + BookCorpus）上训练了一个大型模型（12层到24层的Transformer），长时间（1M更新步骤），这就是BERT。”
- en: BERT is trained with two training methods, **masked language model** (**MLM**)
    and **next sentence prediction** (**NSP**). Let's first go over the details of
    masked language modeling.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: BERT使用两种训练方法进行训练，**掩码语言模型**（**MLM**）和**下一句预测**（**NSP**）。让我们首先了解掩码语言模型的细节。
- en: Language modeling is the task of predicting the next token given the sequence
    of previous tokens. For example, given the sequence of words *Yesterday I visited*,
    a language model can predict the next token as one of the tokens *church*, *hospital*,
    *school*, and so on. Masked language modeling is a bit different. In this approach,
    we mask a percentage of the tokens randomly by replacing them with a `[MASK]`
    token and expect MLM to predict the masked words.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型是预测给定前一个标记序列的下一个标记的任务。例如，给定单词序列“Yesterday I visited”，语言模型可以预测下一个标记为“church”、“hospital”、“school”等中的一个。掩码语言模型略有不同。在这个方法中，我们随机将一定比例的标记用`[MASK]`替换，并期望MLM（Masked
    Language Modeling）能够预测被掩码的单词。
- en: 'The masked language model in BERT is implemented as follows: First, 15 of the
    input tokens are chosen at random. Then, the following happens:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: BERT中的掩码语言模型实现如下：首先，随机选择输入标记中的15个。然后，发生以下情况：
- en: 80% of the tokens chosen are replaced with `[MASK]`.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在选择的标记中，有80%被替换成了`[MASK]`。
- en: 20% of the tokens chosen are replaced with another token from the vocabulary.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在选择的标记中，有20%被替换成了词汇表中的另一个标记。
- en: The remaining 10% are left unchanged.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 剩余的10%保持不变。
- en: 'A training example sentence to LMM looks like the following:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: LMM（Language Modeling Masked）的训练示例句子如下：
- en: '[PRE2]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Next, we will look into the details of the other algorithm, NSP.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨其他算法NSP的细节。
- en: As the name suggests, NSP is the task of predicting the next sentence given
    an input sentence. In this approach, we feed two sentences to BERT and expect
    BERT to predict the order of the sentences, more specifically, if the second sentence
    is the sentence following the first sentence.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名所示，NSP（Next Sentence Prediction）是指根据输入句子预测下一个句子的任务。在这个方法中，我们将两个句子输入到BERT中，并期望BERT能够预测句子的顺序，更具体地说，是第二个句子是否是紧跟在第一个句子之后的句子。
- en: 'Let''s make an example input to NSP. We''ll feed two sentences separated by
    the `[SEP]` token as input:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们做一个NSP的示例输入。我们将使用`[SEP]`标记分隔的两个句子作为输入：
- en: '[PRE3]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In this example, the second sentence can follow the first sentence; hence,
    the predicted label is `IsNext`. How about this example:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，第二个句子可以跟在第一个句子后面；因此，预测的标签是`IsNext`。那么这个例子呢：
- en: '[PRE4]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This example pair of sentences generate the `NotNext` label, as obviously they
    are not contextually or semantically related.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这对句子生成了`NotNext`标签，因为很明显它们在上下文或语义上并不相关。
- en: That's it! We have learned about BERT architecture; we also learned the details
    of BERT input data format and how BERT is trained. Now, we're ready to dive into
    TensorFlow code. In the next section, we'll see how to apply what we learned so
    far in our TensorFlow code.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！我们已经了解了BERT的架构；我们还学习了BERT输入数据格式的细节以及BERT是如何被训练的。现在，我们准备好深入TensorFlow代码了。在下一节中，我们将看到如何将我们迄今为止所学的内容应用到TensorFlow代码中。
- en: Transformers and TensorFlow
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformers和TensorFlow
- en: In this section, we'll dive into transformers code with TensorFlow. Pre-trained
    transformer models are provided to the developer community as open source by many
    organizations, including Google ([https://github.com/google-research/bert](https://github.com/google-research/bert)),
    Facebook ([https://github.com/pytorch/fairseq/blob/master/examples/language_model/README.md](https://github.com/pytorch/fairseq/blob/master/examples/language_model/README.md)),
    and HuggingFace ([https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)).
    All the listed organizations offer pre-trained models and nice interfaces to integrate
    transformers into our Python code. The interfaces are compatible with either PyTorch
    or Tensorflow or both.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用TensorFlow深入研究transformers代码。许多组织，包括Google ([https://github.com/google-research/bert](https://github.com/google-research/bert))、Facebook
    ([https://github.com/pytorch/fairseq/blob/master/examples/language_model/README.md](https://github.com/pytorch/fairseq/blob/master/examples/language_model/README.md))和HuggingFace
    ([https://github.com/huggingface/transformers](https://github.com/huggingface/transformers))，都将预训练的transformer模型以开源的形式提供给开发者社区。所有列出的组织都提供了预训练模型和良好的接口，以便将transformers集成到我们的Python代码中。这些接口与PyTorch或TensorFlow或两者都兼容。
- en: Throughout this chapter, we'll be using HuggingFace's pre-trained transformers
    and their TensorFlow interface to the transformer models. HuggingFace is an AI
    company with a focus on NLP and quite devoted to open source. In the next section,
    we'll take a closer look at what is available in HuggingFace Transformers.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用HuggingFace的预训练transformers及其TensorFlow接口来访问transformer模型。HuggingFace是一家专注于NLP的AI公司，并且非常致力于开源。在下一节中，我们将更详细地了解HuggingFace
    Transformers中可用的内容。
- en: HuggingFace Transformers
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: HuggingFace Transformers
- en: In the first section, we'll discover HuggingFace's pre-trained models, the TensorFlow
    interface for using these models, and HuggingFace model conventions in general.
    We saw in *Figure 9.1* that HuggingFace offers different sorts of models. Each
    model is dedicated to a task such as text classification, question answering,
    and sequence-to-sequence modeling.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一部分，我们将了解 HuggingFace 的预训练模型，使用这些模型的 TensorFlow 接口，以及一般性的 HuggingFace 模型约定。我们在
    *图 9.1* 中看到，HuggingFace 提供了不同种类的模型。每个模型都针对一个任务，如文本分类、问答和序列到序列建模。
- en: The following diagram is taken from the HuggingFace documentation and shows
    details of the distilbert-base-uncased-distilled-squad model. In the documentation,
    first, the task is tagged (upper-left corner of the diagram; the *Question Answering*
    tag), followed by supported deep learning libraries (PyTorch, TensorFlow, TFLite,
    TFSavedModel for this model), the dataset it trained on (squad, in this instance),
    the model language (*en* for English), and the license and base model's name (DistilBERT
    in this case).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表来自 HuggingFace 文档，展示了 distilbert-base-uncased-distilled-squad 模型的详细信息。在文档中，首先在图表的左上角标记了任务（*问答*标签），然后是支持此模型的深度学习库（PyTorch、TensorFlow、TFLite、TFSavedModel），训练所用的数据集（在这个例子中是
    squad），模型语言（*en* 表示英语），以及许可证和基础模型名称（在这种情况下是 DistilBERT）。
- en: 'Some models are trained with similar algorithms, and so belong to the same
    model family. By way of an example, the DistilBERT family includes many models,
    such as distilbert-base-uncased and distilbert-multilingual-cased. Each model
    name also includes some information, such as casing (the model recognizes uppercase/lowercase
    differences) or the model language, such as `en`, `de`, or `multilingual`:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 一些模型使用类似的算法进行训练，因此属于同一个模型家族。以 DistilBERT 家族为例，包括许多模型，如 distilbert-base-uncased
    和 distilbert-multilingual-cased。每个模型名称也包含一些信息，例如大小写（模型识别大写/小写差异）或模型语言，如 `en`、`de`
    或 `multilingual`：
- en: '![Figure 9.9 – Documentation of the distilbert-base-uncased-distilled-squad
    model'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.9 – distilbert-base-uncased-distilled-squad 模型的文档'
- en: '](img/B16570_9_9.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16570_9_9.jpg](img/B16570_9_9.jpg)'
- en: Figure 9.9 – Documentation of the distilbert-base-uncased-distilled-squad model
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.9 – distilbert-base-uncased-distilled-squad 模型的文档
- en: 'We already explored BERT in detail in the previous section. HuggingFace documentation
    provides information about each model family and an individual model''s API in
    detail. *Figure 9.10* shows a list of available models and a list of BERT model
    architecture variations:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上一节中已经详细探讨了 BERT。HuggingFace 文档提供了每个模型家族和单个模型的 API 的详细信息。*图 9.10* 展示了可用的模型列表和
    BERT 模型架构变体的列表：
- en: '![Figure 9.10 – List of the available models on the left-hand side, with a
    list of the BERT model variations on the right-hand side'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.10 – 左侧列出可用的模型，右侧列出 BERT 模型变体'
- en: '](img/B16570_9_10.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16570_9_10.jpg](img/B16570_9_10.jpg)'
- en: Figure 9.10 – List of the available models on the left-hand side, with a list
    of the BERT model variations on the right-hand side
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.10 – 左侧列出可用的模型，右侧列出 BERT 模型变体
- en: The BERT model has many variations for a variety of tasks, such as text classification,
    question answering, and next sentence prediction. Each of these models is obtained
    by placing some extra layers on top of the BERT output. Recall from the previous
    section that the BERT output is a sequence of word vectors for each word of the
    input sentence. For example, the BERTForSequenceClassification model is obtained
    by placing a dense layer (we covered dense layers in the previous chapter) on
    top of the BERT word vectors.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 模型有多种变体，适用于各种任务，如文本分类、问答和下一句预测。这些模型中的每一个都是通过在 BERT 输出之上添加一些额外的层获得的。回想一下，BERT
    输出是输入句子中每个单词的词向量序列。例如，BERTForSequenceClassification 模型是通过在 BERT 词向量之上放置一个密集层（我们在上一章中介绍了密集层）获得的。
- en: In the rest of this chapter, we'll explore how to use some of these architectures
    for our tasks as well as how to use BERT word vectors with Keras. Before all of
    these tasks, we will start with the basic task of tokenization to prepare our
    input sentences. Let's see the tokenizer code in the next section.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的剩余部分，我们将探讨如何使用这些架构中的某些部分来完成我们的任务，以及如何使用 Keras 与 BERT 词向量结合使用。在所有这些任务之前，我们将从基本的分词任务开始，为我们的输入句子做准备。让我们在下一节中查看分词器的代码。
- en: Using the BERT tokenizer
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 BERT 分词器
- en: In the *Understanding BERT* section, we already saw that BERT uses the WordPiece
    algorithm for tokenization. Every input word is broken down into subwords. Let's
    see how to prepare our input data with the HuggingFace library.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在*理解BERT*部分，我们已经看到BERT使用WordPiece算法进行分词。每个输入单词都被分解成子词。让我们看看如何使用HuggingFace库准备我们的输入数据。
- en: 'The following lines exhibit the basic usage of the tokenizer:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 以下行展示了分词器的基本用法：
- en: '[PRE5]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here are the steps we followed in the preceding code block:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们遵循的步骤如下：
- en: First, we imported `BertTokenizer`. Different models have different tokenizers;
    for instance, XLNet model's tokenizer is called `XLNetTokenizer`.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们导入了`BertTokenizer`。不同的模型有不同的分词器；例如，XLNet模型的分词器被称为`XLNetTokenizer`。
- en: Second, we called the `from_pretrained` method on the tokenizer object and provided
    the model's name. Note that we don't need to download the pre-trained bert-base-uncased
    model manually; this method downloads the model by itself.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 其次，我们在分词器对象上调用`from_pretrained`方法，并提供了模型名称。请注意，我们不需要手动下载预训练的`bert-base-uncased`模型；此方法会自动下载模型。
- en: Then, we called the `tokenize` method. `tokenize` basically tokenizes the sentence
    by breaking all the words down into subwords.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们调用了`tokenize`方法。`tokenize`基本上通过将所有单词分解成子词来分词句子。
- en: We print tokens to examine the subwords. The words "he," "lived," "idle," and
    so on exist in Tokenizer's vocabulary, and so are kept as they are. "Characteristically"
    is a rare word, so does not exist in Tokenizer's vocabulary. Then, Tokenizer splits
    this word into the subwords "characteristic" and "##ally." Notice that "##ally"
    starts with the characters "##" to emphasize the fact that this is a piece of
    word.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们打印标记以检查子词。单词“he”、“lived”、“idle”等存在于分词器的词汇表中，因此保持不变。“Characteristically”是一个罕见单词，因此不在分词器的词汇表中。然后，分词器将这个单词分解成子词“characteristic”和“##ally”。请注意，“##ally”以“##”字符开头，以强调这是一个单词的一部分。
- en: Next, we convert tokens to their token IDs by calling `convert_tokens_to_ids`.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们通过调用`convert_tokens_to_ids`将标记转换为它们的标记ID。
- en: 'How about `[CLS]` and `[SEP]` tokens? In the previous section, we already saw
    that we have to add these two special tokens to the beginning and end of the input
    sentence. For the preceding code, we need to involve one more step and add our
    special tokens manually. Can we do all of these preprocessing steps in a single
    step, perhaps? The answer is yes; BERT provides a method called `encode` that
    does the following:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 那么`[CLS]`和`[SEP]`标记呢？在前面的章节中，我们已经看到我们必须在输入句子的开头和结尾添加这两个特殊标记。对于前面的代码，我们需要进行一个额外的步骤，并手动添加我们的特殊标记。我们能否将这些预处理步骤合并为一步呢？答案是肯定的；BERT提供了一个名为`encode`的方法，它执行以下操作：
- en: Adds `CLS` and `SEP` tokens to the input sentence
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在输入句子中添加`CLS`和`SEP`标记
- en: Tokenizes the sentence by breaking the tokens down into subwords
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过将标记分解成子词来分词句子
- en: Converts the tokens to their token IDs
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将标记转换为它们的标记ID
- en: 'We call the `encode` method directly on the input sentence as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们直接在输入句子上调用`encode`方法，如下所示：
- en: '[PRE6]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This code segment outputs the token IDs in just a single step, instead of calling
    `tokenize` and `convert_tokens_to_ids` one after the other. The result is a Python
    list.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码段在单步中输出标记ID，而不是依次调用`tokenize`和`convert_tokens_to_ids`。结果是Python列表。
- en: 'How about padding the sentence? We already saw in the previous section that
    all the input sentences in a dataset should be of equal length because BERT cannot
    process variable-length sentences. Hence, we need to pad the short sentences to
    the length of the longest sentence available in the dataset. Also, if we want
    to use a TensorFlow tensor instead of a plain list, we need to write some conversion
    code. The HuggingFace library provides `encode_plus` to make our life easier and
    combine all these steps into one method as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 关于填充句子呢？在前面章节中，我们已经看到数据集中的所有输入句子都应该具有相同的长度，因为BERT无法处理变长句子。因此，我们需要将短句子填充到数据集中最长句子的长度。此外，如果我们想使用TensorFlow张量而不是普通列表，我们需要编写一些转换代码。HuggingFace库提供了`encode_plus`来简化我们的工作，并将所有这些步骤合并为一个方法，如下所示：
- en: '[PRE7]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Here, we called `encode_plus` on our input sentence directly. Our sentence is
    now padded to a length of 12 (the two `0` IDs at the end of the sequence are the
    pad tokens), while the special tokens `[CLS]` and `[SEP]` are also added to the
    sentence. The output is directly a TensorFlow tensor, including the token IDs.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们直接在我们的输入句子上调用`encode_plus`。现在，我们的句子被填充到长度为12（序列末尾的两个`0` ID是填充标记），同时特殊标记`[CLS]`和`[SEP]`也被添加到句子中。输出直接是一个TensorFlow张量，包括标记ID。
- en: 'The `encode_plus` method takes the following parameters:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '`encode_plus`方法接受以下参数：'
- en: '`text`: Input sentence.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`text`：输入句子。'
- en: '`add_special_tokens`: Add `CLS` and `SEP` tokens.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`add_special_tokens`：添加`CLS`和`SEP`标记。'
- en: '`max_length`: The maximum length you want your sentence to be. If the sentence
    is shorter than `max_length` tokens, we want to pad the sentence.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_length`：您希望句子达到的最大长度。如果句子短于`max_length`标记，我们希望填充句子。'
- en: '`pad_to_max_length`: We feed `True` if we want to pad the sentence, otherwise
    `False`.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pad_to_max_length`：如果我们想填充句子，则提供`True`，否则提供`False`。'
- en: '`return_tensors`: We pass this parameter if we want the output to be a tensor,
    otherwise the output is a Python list. The available options are `tf` and `pt`
    for TensorFlow and PyTorch, respectively.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`return_tensors`：如果我们希望输出是一个张量，则传递此参数，否则输出是一个Python列表。可用的选项是`tf`和`pt`，分别对应TensorFlow和PyTorch。'
- en: As we can see, BERT Tokenizer provides several methods on input sentences. Preparing
    data is not so straightforward, but you'll get used to it by practicing. We always
    encourage you to try out the code examples with your own text.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，BERT分词器为输入句子提供了几种方法。准备数据并不那么直接，但通过练习你会习惯的。我们总是鼓励你尝试使用自己的文本运行代码示例。
- en: Now, we're ready to process the transformed input sentences. Let's go ahead
    and provide our input sentences to the BERT model to obtain BERT word vectors.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好处理转换后的输入句子。让我们继续将我们的输入句子提供给BERT模型以获得BERT词向量。
- en: Tip
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Always check the name of the tokenizer class that you should use with your transformer.
    A list of models and their corresponding tokenizers is available at [https://huggingface.co/transformers/](https://huggingface.co/transformers/).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 总是检查您应该与您的transformer一起使用的分词器类的名称。有关模型及其对应分词器的列表，可在[https://huggingface.co/transformers/](https://huggingface.co/transformers/)找到。
- en: Obtaining BERT word vectors
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取BERT词向量
- en: In this section, we'll examine the output of the BERT model. As we stated in
    the *Understanding BERT* section, the output of the BERT model is a sequence of
    word vectors, one vector per input word. BERT has a special output format and,
    in this section, we'll examine BERT outputs in detail.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将检查BERT模型的输出。正如我们在*理解BERT*部分所述，BERT模型的输出是一系列词向量，每个输入词一个向量。BERT有一个特殊的输出格式，在本节中，我们将详细检查BERT输出。
- en: 'Let''s see the code first:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看看代码：
- en: '[PRE8]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This code is very similar to the code from the previous section. Here, we also
    imported `TFBertModel`. After that, we initialized out BERT model with the pre-trained
    model, `bert-base-uncased`. Then, we transformed our input sentence to BERT input
    format with `encode_plus` and captured the result, `tf.tensor`, in the input variable.
    We fed our sentence to the BERT model and captured this output with the `outputs`
    variable. What's inside the `outputs` variable then?
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码与上一节的代码非常相似。在这里，我们也导入了`TFBertModel`。之后，我们使用预训练模型`bert-base-uncased`初始化我们的BERT模型。然后，我们使用`encode_plus`将我们的输入句子转换为BERT输入格式，并将结果捕获在输入变量`tf.tensor`中。我们将我们的句子输入到BERT模型中，并用`outputs`变量捕获这个输出。那么`outputs`变量中有什么呢？
- en: 'The output of the BERT model is a tuple of two elements. Let''s print the shapes
    of the output pair:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: BERT模型的输出是两个元素的元组。让我们打印输出对的形状：
- en: '[PRE9]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The first element of the output is the shape `(batch size, sequence length,
    hidden size)`. We fed only one sentence, hence the batch size here is 1 (the batch
    size is how many sentences we feed to the model at once). The sequence length
    here is 10 because we fed `max_length=10` to the tokenizer and padded our sentence
    to a length of 10\. `hidden_size` is a parameter of BERT. In the *BERT Architecture*
    section, we already remarked that BERT's hidden layer size is 768, and so produces
    word vectors with a dimension of 768\. So, the first output element contains 768-dimensional
    vectors per word, hence it contains 10 words x 768-dimensional vectors.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的第一个元素是形状 `(batch size, sequence length, hidden size)`。我们只输入了一个句子，因此这里的批大小是
    1（批大小是我们一次输入给模型的句子数量）。这里的序列长度是 10，因为我们向分词器输入了 `max_length=10`，并将我们的句子填充到长度为 10。`hidden_size`
    是 BERT 的一个参数。在 *BERT 架构* 部分，我们已经提到 BERT 的隐藏层大小是 768，因此产生具有 768 维度的词向量。所以，第一个输出元素包含每个词的
    768 维向量，因此它包含 10 个词 x 768 维向量。
- en: The second output is only one vector of 768-dimension. This vector is basically
    the word embedding of the `[CLS]` token. Recall from the *BERT input format* section
    that the `[CLS]` token is an aggregate of the whole sentence. You can think of
    `[CLS]` token's embedding as the pooled version of embeddings of all the words
    in the sentence. The shape of the second element of the output tuple is always
    (`batch size,` `hidden_size`). Basically, we collect the `[CLS]` token's embedding
    per input sentence.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个输出是一个 768 维的向量。这个向量基本上是 `[CLS]` 标记的词嵌入。回想一下 *BERT 输入格式* 部分，`[CLS]` 标记是整个句子的聚合。你可以将
    `[CLS]` 标记的嵌入视为句子中所有词嵌入的汇总版本。输出元组的第二个元素的形状始终是 (`batch size,` `hidden_size`)。基本上，我们为每个输入句子收集
    `[CLS]` 标记的嵌入。
- en: Great! We extracted the BERT embeddings. Next, we'll use these embeddings to
    train our text classification model with TensorFlow and tf.keras.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！我们已经提取了 BERT 嵌入。接下来，我们将使用这些嵌入来用 TensorFlow 和 tf.keras 训练我们的文本分类模型。
- en: Using BERT for text classification
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 BERT 进行文本分类
- en: 'In this section, we''ll train a binary text classifier with BERT and tf.keras.
    We''ll reuse some of the code from the previous chapter, but this time the code
    will be much shorter because we''ll replace the embedding and LSTM layers with
    BERT. The complete code is available at the GitHub repository: [https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter09/BERT_spam.ipynb](https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter09/BERT_spam.ipynb).ipynb.
    In this section, we''ll skip the data preparation. We used the SMS Spam Collection
    dataset from Kaggle. You can find the dataset under the `data/` directory at the
    GitHub repository, too.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用 BERT 和 tf.keras 训练一个二进制文本分类器。我们将重用前一章中的一些代码，但这次代码会更短，因为我们将以 BERT
    替换嵌入和 LSTM 层。完整的代码可在 GitHub 仓库中找到：[https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter09/BERT_spam.ipynb](https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter09/BERT_spam.ipynb).ipynb。在本节中，我们将跳过数据准备。我们使用了
    Kaggle 的 SMS Spam Collection 数据集。你还可以在 GitHub 仓库的 `data/` 目录下找到该数据集。
- en: 'Let''s get started by importing the BERT models and tokenizer:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从导入 BERT 模型和分词器开始：
- en: '[PRE10]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We have imported the `BertTokenizer` tokenizer and the BERT model, `TFBertModel`.
    We initialized both the tokenizer and the BERT model with the pre-trained bert-base-uncased
    model. Notice that the model's name starts with TF – the names of all of the HuggingFace
    pre-trained models for TensorFlow start with TF. Please pay attention to this
    detail when you want to play with other transformer models in the future.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经导入了 `BertTokenizer` 分词器和 BERT 模型，`TFBertModel`。我们使用预训练的 bert-base-uncased
    模型初始化了分词器和 BERT 模型。请注意，模型的名称以 TF 开头——所有 HuggingFace 预训练模型的 TensorFlow 版本名称都以 TF
    开头。当你未来想要玩转其他 transformer 模型时，请注意这个细节。
- en: 'We''ll also import Keras layers and functions, together with `numpy`:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将导入 Keras 层和函数，以及 `numpy`：
- en: '[PRE11]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, we''re ready to process the input data with `BertTokenizer`:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备好使用 `BertTokenizer` 处理输入数据：
- en: '[PRE12]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As we saw in the *Using the BERT tokenizer* section, this code segment will
    generate token IDs for each input sentence of the dataset and append them to a
    list. Labels are the list of class labels and consist of 0 and 1s. Then we convert
    the Python lists, `input_ids`, and labels to `numpy` arrays to feed them to our
    Keras model.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在 *使用 BERT 分词器* 部分中看到的，这段代码将为数据集中的每个输入句子生成标记 ID，并将它们追加到一个列表中。标签是类标签的列表，由
    0 和 1 组成。然后我们将 Python 列表 `input_ids` 和标签转换为 `numpy` 数组，以便将它们输入到我们的 Keras 模型中。
- en: 'Finally, we define our Keras model by means of the following lines:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过以下几行代码定义了我们的Keras模型：
- en: '[PRE13]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'That''s it! We defined our BERT-based text classifier in only five lines of
    code! Let''s dissect the code:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！我们只用了五行代码就定义了基于BERT的文本分类器！让我们分析一下代码：
- en: First, we defined the input layer, which inputs the sentences to our model.
    The shape is `(64,)` because each input sentence is 64 tokens in length. We padded
    each sentence to the length of 64 tokens when we called the `encode_plus` method.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们定义了输入层，它将句子输入到我们的模型中。形状是`(64,)`，因为每个输入句子长度为64个标记。当我们调用`encode_plus`方法时，我们将每个句子填充到64个标记的长度。
- en: Next, we fed the input sentences to the BERT model.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将输入句子输入到BERT模型中。
- en: At the third line, we extracted the second output of the BERT output. Recall
    from the previous section that the BERT model's output is a tuple. The first element
    of the output tuple is a sequence of word vectors, and the second element is a
    single vector that represents the whole sentence, called `bert[1]` extracts the
    pooled output vector; this is a vector of shape (1, 768).
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第三行，我们提取了BERT输出的第二个输出。回想一下，BERT模型的输出是一个元组。输出元组的第一个元素是一系列词向量，第二个元素是一个代表整个句子的单个向量，称为`bert[1]`提取了池化后的输出向量；这是一个形状为(1,
    768)的向量。
- en: Next, we squashed the pooled output vector to a vector of shape 1 by a sigmoid
    function, which is the class label.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们通过sigmoid函数将池化后的输出向量压缩为形状为1的向量，这就是类别标签。
- en: We defined our Keras model with the inputs and outputs.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用输入和输出定义了我们的Keras模型。
- en: Here, the BERT model takes only one line but can transfer the enormous knowledge
    of the Wiki corpus to your model. At the end of the training, this model obtains
    an accuracy of `0.96`. We usually fit the model for one epoch due to fact that
    BERT overfits easily even on a moderate size corpus.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，BERT模型只需一行代码就能将维基语料库的巨大知识迁移到你的模型中。在训练结束时，这个模型达到了`0.96`的准确率。我们通常只进行一个epoch的训练，因为BERT即使在中等规模的语料库上也容易过拟合。
- en: 'The rest of the code handles compiling and fitting the Keras model. Note that
    BERT has huge memory requirements also. You can see how much RAM is required from
    Google Research''s GitHub link: [https://github.com/google-research/bert#out-of-memory-issues](https://github.com/google-research/bert#out-of-memory-issues).'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 其余的代码处理了Keras模型的编译和拟合。请注意，BERT有巨大的内存需求。您可以从谷歌研究GitHub链接中查看所需的RAM量：[https://github.com/google-research/bert#out-of-memory-issues](https://github.com/google-research/bert#out-of-memory-issues)。
- en: If you have trouble running this section's code on your machine, you can use
    **Google Colab**, which provides a Jupyter notebook environment through your browser.
    You can start using Google Colab immediately through [https://colab.research.google.com/notebooks/intro.ipynb](https://colab.research.google.com/notebooks/intro.ipynb).
    Our training code runs on Google Colab for around 1.5 hours, while bigger datasets
    can take more time even though it's just one epoch.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在机器上运行本节代码时遇到困难，可以使用**Google Colab**，它通过浏览器提供Jupyter笔记本环境。您可以通过[https://colab.research.google.com/notebooks/intro.ipynb](https://colab.research.google.com/notebooks/intro.ipynb)立即开始使用Google
    Colab。我们的训练代码在Google Colab上大约运行1.5小时，尽管只是进行一个epoch，但更大的数据集可能需要更多时间。
- en: In this section, we learned how to train a Keras model with BERT from scratch.
    Now, we'll switch to an easier task. We'll explore how to use a pre-trained transformer
    pipeline. Let's move on to the next section for the details.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何从头开始训练一个Keras模型使用BERT。现在，我们将转向一个更简单的任务。我们将探讨如何使用预训练的transformer流水线。让我们进入下一节以获取详细信息。
- en: Using Transformer pipelines
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Transformer流水线
- en: The HuggingFace Transformers library provides pipelines to help developers benefit
    from transformer code immediately without any custom training. A **pipeline**
    is a tokenizer and a pre-trained model combined.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: HuggingFace Transformers库提供了流水线，帮助开发者立即从transformer代码中受益，而无需任何自定义训练。**流水线**是一个分词器和一个预训练模型的组合。
- en: 'HuggingFace provides a variety of models for a variety of NLP tasks. Here are
    some tasks that HuggingFace pipelines offer:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: HuggingFace为各种NLP任务提供了各种模型。以下是一些HuggingFace流水线提供的任务：
- en: '**Sentiment analysis**'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**情感分析**'
- en: '**Question answering**'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问答**'
- en: '**NER**'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**命名实体识别**'
- en: '**Text summarization**'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本摘要**'
- en: '**Translation**'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**翻译**'
- en: 'You can see the full list of tasks at the Huggingface documentation: [https://huggingface.co/transformers/task_summary.html](https://huggingface.co/transformers/task_summary.html).
    In this section, we''ll explore the pipelines for sentiment analysis and question
    answering (the use of pipelines with other tasks is similar).'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 Huggingface 文档中查看所有任务的完整列表：[https://huggingface.co/transformers/task_summary.html](https://huggingface.co/transformers/task_summary.html)。在本节中，我们将探讨情感分析和问答（与其他任务的管道使用类似）的管道。
- en: 'Let''s go through some examples. We''ll start with sentiment analysis:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一些例子来了解一下。我们将从情感分析开始：
- en: '[PRE14]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In the preceding code snippet, we took the following steps:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们采取了以下步骤：
- en: First, we imported the pipeline function from the `transformers` library. This
    function creates pipeline objects with the task name given as a parameter. Hence,
    we created our sentiment analysis pipeline object, `nlp`, by calling this function
    on the second line.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们从 `transformers` 库中导入了管道函数。此函数通过将任务名称作为参数创建管道对象。因此，我们在第二行通过调用此函数创建了我们的情感分析管道对象
    `nlp`。
- en: Next, we define two example sentences with negative and positive sentiment,
    respectively.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义了两个具有负面和正面情感的示例句子。
- en: Then we feed these sentences to the pipeline object, `nlp`.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将这些句子输入到管道对象 `nlp` 中。
- en: 'Here is the output:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出结果：
- en: '[PRE15]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This worked great! Next, we''ll play with question answering. Let''s see the
    code:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这效果非常好！接下来，我们将尝试问答。让我们看看代码：
- en: '[PRE16]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Again, we imported the pipeline function and used it to create a pipeline object,
    `nlp`. In *question-answering* tasks, we need to provide a context (the same background
    information for the model to work on) to the model as well as our question. We
    asked the model about the name of this book after giving the information that
    our new publication will be out soon. The answer is `Mastering spaCy`; the transformer
    worked wonders on this pair! We encourage you to try out your own examples.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们导入了管道函数并使用它创建了一个管道对象，`nlp`。在*问答*任务中，我们需要向模型提供上下文（模型工作的相同背景信息）以及我们的问题。在提供我们即将出版的新作品即将问世的信息后，我们询问了模型关于这本书的名称。答案是
    `Mastering spaCy`；在这个对儿上，transformer 真是创造了奇迹！我们鼓励您尝试自己的例子。
- en: We have completed our exploration of HuggingFace transformers. Now, we will
    move on to our final section of this chapter and see what spaCy offers us as regards
    transformers.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了对 HuggingFace Transformers 的探索。现在，我们将进入本章的最后一部分，看看 spaCy 在 Transformer
    方面能为我们提供什么。
- en: Transformers and spaCy
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Transformers 和 spaCy
- en: 'spaCy v3.0 was released with great new features and components. The most exciting
    new feature is undoubtedly **transformer-based pipelines**. The new transformer-based
    pipelines bring spaCy''s accuracy to the state of the art. Integrating transformers
    into the spaCy NLP pipeline introduced one more pipeline component called **Transformer**.
    This component allows us to use all HuggingFace models with spaCy pipelines. If
    we recall from [*Chapter 2*](B16570_02_Final_JM_ePub.xhtml#_idTextAnchor037),
    *Core Operations with spaCy*, this is what the spaCy NLP pipeline looks like without
    transformers:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy v3.0 伴随着许多新功能和组件的发布。无疑，最令人兴奋的新功能是**基于 Transformer 的管道**。新的基于 Transformer
    的管道将 spaCy 的准确性提升到了业界领先水平。将 Transformer 集成到 spaCy NLP 管道中引入了一个名为**Transformer**的额外管道组件。此组件允许我们使用所有
    HuggingFace 模型与 spaCy 管道一起使用。如果我们回想一下[*第 2 章*](B16570_02_Final_JM_ePub.xhtml#_idTextAnchor037)，*spaCy
    的核心操作*，这是没有 Transformer 的 spaCy NLP 管道的样子：
- en: '![ Figure 9.11 – Vector-based spaCy pipeline components'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 9.11 – Vector-based spaCy pipeline components](img/B16570_9_11.jpg)'
- en: '](img/B16570_9_11.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16570_9_11.jpg](img/B16570_9_11.jpg)'
- en: Figure 9.11 – Vector-based spaCy pipeline components
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 9.11 – Vector-based spaCy pipeline components](img/B16570_9_11.jpg)'
- en: 'With the release of v3.0, v2 style spaCy models are still supported and transformer-based
    models are introduced. A transformer-based pipeline component looks like the following:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 v3.0 的发布，v2 风格的 spaCy 模型仍然得到支持，并引入了基于 Transformer 的模型。一个基于 Transformer 的管道组件看起来如下：
- en: '![Figure 9.12 – Transformed-based spaCy pipeline components'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 9.12 – Based-on-Transformer spaCy pipeline components](img/B16570_9_12.jpg)'
- en: '](img/B16570_9_12.jpg)'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16570_9_12.jpg](img/B16570_9_12.jpg)'
- en: Figure 9.12 – Transformed-based spaCy pipeline components
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 9.12 – Based-on-Transformer spaCy pipeline components](img/B16570_9_12.jpg)'
- en: 'For each supported language, transformer-based models and v2 style models are
    listed under the Models page of the documentation (English for an example: [https://spacy.io/models/en](https://spacy.io/models/en)).
    Transformer-based models can have different size and pipeline components, just
    like v2 style models. Also, each model has corpus and genre information as well,
    just like the v2 style models. Here is an example of an English transformer-based
    language model from the Models page:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个支持的语言，基于Transformer的模型和v2风格模型都在文档的“模型”页面下列出（以英语为例：[https://spacy.io/models/en](https://spacy.io/models/en)）。基于Transformer的模型可以有不同的尺寸和管道组件，就像v2风格模型一样。此外，每个模型也有语料库和体裁信息，就像v2风格模型一样。以下是“模型”页面上的一个英语基于Transformer的语言模型示例：
- en: '![Figure 9.13 – spaCy English transformer-based language models'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.13 – spaCy英语基于Transformer的语言模型](img/B16570_9_13.jpg)'
- en: '](img/B16570_9_13.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16570_9_13.jpg](img/B16570_9_13.jpg)'
- en: Figure 9.13 – spaCy English transformer-based language models
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.13 – spaCy英语基于Transformer的语言模型
- en: As we see from the preceding screenshot, the first pipeline component is a transformer
    and the rest of the pipeline components are the ones we already covered in [*Chapter
    3*](B16570_03_Final_JM_ePub.xhtml#_idTextAnchor055), *Linguistic Features*. The
    transformer component generates the word representations and deals with the WordPiece
    algorithm to tokenize words into subwords. The word vectors are fed to the rest
    of the pipeline.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们从前面的截图中所见，第一个管道组件是一个Transformer，其余的管道组件是我们已经在[*第3章*](B16570_03_Final_JM_ePub.xhtml#_idTextAnchor055)、“语言特性”中介绍过的。Transformer组件生成词表示，并处理WordPiece算法将单词分解为子词。词向量被输入到管道的其余部分。
- en: 'Downloading, loading, and using transformer-based models are identical to v2
    style models. Currently, English has two pre-trained transformer-based models,
    `en_core_web_trf` and `en_core_web_lg`. Let''s get started by downloading the
    `en_core_web_trf` model:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 下载、加载和使用基于Transformer的模型与v2风格模型相同。目前，英语有两种预训练的基于Transformer的模型，`en_core_web_trf`和`en_core_web_lg`。让我们从下载`en_core_web_trf`模型开始：
- en: '[PRE17]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This should produce output similar to the following:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该产生类似于以下输出的结果：
- en: '[PRE18]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Once the model download is complete, the following output should be generated:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型下载完成，应该生成以下输出：
- en: '[PRE19]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Loading a transformer-based model is identical to what we do for v2 style models,
    too:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 加载基于Transformer的模型与v2风格模型的做法相同：
- en: '[PRE20]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'After loading our model and initializing the pipeline, we can use this model
    in the same way we used v2 style models:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载我们的模型并初始化管道后，我们可以像使用v2风格模型一样使用这个模型：
- en: '[PRE21]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'So far so good, but what''s new then? Let''s examine some features that come
    from the transformer component. We can access the features related to the transformer
    component via `doc._.trf_data.trf_data`, which contains the word pieces, input
    `ids`, and vectors that are generated by the transformer. Let''s examine the features
    one by one:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止一切顺利，但接下来有什么新内容呢？让我们来检查一些来自Transformer组件的新特性。我们可以通过`doc._.trf_data.trf_data`访问与Transformer组件相关的特性，它包含由Transformer生成的词片段、输入`ids`和向量。让我们逐一检查这些特性：
- en: '[PRE22]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'In the preceding output, we see five elements: word pieces, input IDs, attention
    masks, lengths, and token type IDs. Word pieces are the subwords that are generated
    by the WordPiece algorithm. The word pieces of this sentence are as follows:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的输出中，我们看到五个元素：词片段、输入ID、注意力掩码、长度和标记类型ID。词片段是由WordPiece算法生成的子词。这个句子的词片段如下：
- en: '[PRE23]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Here, `<s>` and `</s>` are special tokens and are used in the sentence at the
    beginning and end. The word `unwillingly` is divided into three subwords – `unw`,
    `ill`, and `ingly`. The `G` character is used to mark the word boundaries. The
    tokens without `G` are subwords, such as `ill` and `ingly` in the preceding word
    piece list (with the exception of the first word in the sentence, the first word
    of the sentence is marked by `<s>`).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`<s>`和`</s>`是特殊标记，用于句子的开头和结尾。单词`unwillingly`被分解为三个子词 – `unw`、`ill`和`ingly`。字符`G`用于标记词边界。没有`G`的标记是子词，例如前一个词片段列表中的`ill`和`ingly`（除了句子中的第一个单词，句子的第一个单词由`<s>`标记）。
- en: Next, we have to take a look at `input_ids.` Input IDs have the same meaning
    as the input IDs we introduced in the *Using the BERT tokenizer* section. These
    are basically the subword IDs assigned by the transformer's tokenizer.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们必须查看`input_ids`。输入ID与我们之前在“使用BERT分词器”部分介绍的输入ID具有相同的意义。这些基本上是由Transformer的分词器分配的子词ID。
- en: The attention mask is a list of 0s and 1s for pointing the transformer to those
    tokens it should pay attention to. `0` corresponds to `PAD` tokens, while all
    the other tokens should have a corresponding `1`.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力掩码是一个由 0 和 1 组成的列表，用于指示 Transformer 应关注哪些标记。`0` 对应于 `PAD` 标记，而所有其他标记都应该有一个相应的
    `1`。
- en: '`lengths` is the length of the sentence after breaking it down into subwords.
    Here, it''s `9` obviously, but notice that `len(doc)` outputs `5`, while spaCy
    always operates on linguistic words.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '`lengths` 是将句子分解为子词后的句子长度。在这里，它显然是 `9`，但请注意，`len(doc)` 输出 `5`，而 spaCy 总是操作语言词汇。'
- en: '`token_type_ids` are used by transformer tokenizers to mark the sentence boundaries
    for two sentence input tasks, such as question and answering. Here, we provide
    only one text, hence this feature is not applicable.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '`token_type_ids` 由 Transformer 标记化器用于标记两个句子输入任务（如问答）的句子边界。在这里，我们只提供了一个文本，因此此功能不适用。'
- en: 'We can see that the token vectors generated by the transformer, `doc._.trf_data.tensors`,
    contain the output of the transformer, a sequence of word vectors per word, and
    the pooled output vector (we introduced these concepts in the *Obtaining BERT
    word vectors* section. If you need to refresh your memory, please refer to this
    section):'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，由 Transformer 生成的标记向量 `doc._.trf_data.tensors` 包含了 Transformer 的输出，每个单词的单词向量序列，以及池化输出向量（我们在
    *获取 BERT 单词向量* 部分介绍了这些概念。如果你需要刷新记忆，请参考本节）：
- en: '[PRE24]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The first element of the tuple is the vectors for the tokens. Each vector is
    `768`-dimensional; hence `9` words produce `9` x `768`-dimensional vectors. The
    second element of the tuple is the pooled output vector, which is an aggregate
    representation for the input sentence, and so is of the shape `1`x`768`.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 元组的第一个元素是标记的向量。每个向量是 `768` 维的；因此 `9` 个单词产生 `9` x `768` 维的向量。元组的第二个元素是池化输出向量，它是输入句子的聚合表示，因此其形状为
    `1`x`768`。
- en: This concludes our exploration of spaCy transformer-based pipelines. Once again,
    we saw that spaCy provides user-friendly API and packaging, even for complicated
    models such as transformers. Transformer integration is yet another great reason
    to use spaCy for NLP.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 这标志着我们对基于 spaCy 的 Transformer 管道的探索结束。再次强调，spaCy 提供了用户友好的 API 和打包，即使是像 transformers
    这样的复杂模型也不例外。Transformer 集成是使用 spaCy 进行 NLP 的另一个很好的理由。
- en: Summary
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: You have completed an exhaustive chapter about a very hot topic in NLP. Congratulations!
    In this chapter, you started by learning what sort of models transformers are
    and what transfer learning is. Then, you learned about the commonly used Transformer
    architecture, BERT. You learned the architecture details and the specific input
    format, as well as the BERT Tokenizer and WordPiece algorithm.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 你完成了一章关于 NLP 中一个非常热门主题的详尽章节。恭喜！在本章中，你首先学习了 Transformer 模型是什么以及迁移学习是什么。然后，你了解了常用的
    Transformer 架构，BERT。你学习了架构细节和特定的输入格式，以及 BERT Tokenizer 和 WordPiece 算法。
- en: Next, you became familiar with BERT code by using the popular HuggingFace Transformers
    library. You practiced fine-tuning BERT on a custom dataset for a sentiment analysis
    task with TensorFlow and Keras. You also practiced using pre-trained HuggingFace
    pipelines for a variety of NLP tasks, such as text classification and question
    answering. Finally, you explored the spaCy and Transformers integration of the
    new spaCy release, spaCy v3.0.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你通过使用流行的 HuggingFace Transformers 库熟悉了 BERT 代码。你练习了在自定义数据集上使用 TensorFlow
    和 Keras 对 BERT 进行微调，以完成情感分析任务。你还练习了使用预训练的 HuggingFace 管道来完成各种 NLP 任务，例如文本分类和问答。最后，你探索了新版本
    spaCy（spaCy v3.0）与 Transformers 的集成。
- en: By the end of this chapter, you had completed the statistical NLP sections of
    this book. Now you're ready to put everything you learned together to build a
    modern NLP pipeline. Let's move on to the next chapter and see how we use our
    new statistical skills!
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你已经完成了这本书中关于统计 NLP 的章节。现在你准备好将所学的一切结合起来构建一个现代 NLP 管道。让我们继续到下一章，看看我们如何使用我们新的统计技能！
