<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch01"/>Chapter 1. Hello, Data!</h1></div></div></div><p>Most projects in R start with loading at least some data into the running R session. As R supports a variety of file formats and database backend, there are several ways to do so. In this chapter, we will not deal with basic data structures, which are already familiar to you, but will concentrate on the performance issue of loading larger datasets and dealing with special file formats.</p><div><div><h3 class="title"><a id="note02"/>Note</h3><p>For a quick overview on the standard tools and to refresh your knowledge on importing <a class="indexterm" id="id0"/>general data, please see <em>Chapter 7</em> of the official <em>An Introduction to R</em> manual of CRAN at <a class="ulink" href="http://cran.r-project.org/doc/manuals/R-intro.html#Reading-data-from-files">http://cran.r-project.org/doc/manuals/R-intro.html#Reading-data-from-files</a> or Rob Kabacoff's Quick-R site, which offers keywords and cheat-sheets for most general tasks<a class="indexterm" id="id1"/> in R at <a class="ulink" href="http://www.statmethods.net/input/importingdata.html">http://www.statmethods.net/input/importingdata.html</a>. For further materials, please see the <em>References</em> section in the <em>Appendix</em>.</p></div></div><p>Although R has its own (serialized) binary <code class="literal">RData</code> and <code class="literal">rds</code> file formats, which are extremely convenient to use for all R users as these also store R object meta-information in an efficient way, most of the time we have to deal with other input formats—provided by our employer or client.</p><p>One of the most popular data file formats is flat files, which are simple text files in which the values are separated by white-space, the pipe character, commas, or more often by semi-colon in Europe. This chapter will discuss several options R has to offer to load these kinds of documents, and we will benchmark which of these is the most efficient approach to import larger files.</p><p>Sometimes we are only interested in a subset of a dataset; thus, there is no need to load all the data from the sources. In such cases, database backend can provide the best performance, where the data is stored in a structured way preloaded on our system, so we can query any subset of that with simple and efficient commands. The second section of this chapter will focus on the three most popular databases (MySQL, PostgreSQL, and Oracle Database), and how to interact with those in R.</p><p>Besides some other helper tools and a quick overview on other database backend, we will also discuss how to load Excel spreadsheets into R—without the need to previously convert those to text files in Excel or Open/LibreOffice.</p><p>Of course this chapter is not just about data file formats, database connections, and such boring internals. But please bear in mind that data analytics always starts with loading data. This is unavoidable, so that our computer and statistical environment know the structure of the data before doing some real analytics.</p><div><div><div><div><h1 class="title"><a id="ch01lvl1sec08"/>Loading text files of a reasonable size</h1></div></div></div><p>The title of this chapter might also be <em>Hello, Big Data!</em>, as now we concentrate on loading relatively large amount of data in an R session. But what is Big Data, and what amount of data is problematic to handle in R? What is reasonable size?</p><p>R was designed to process data that fits in the physical memory of a single computer. So handling datasets that are smaller than the actual accessible RAM should be fine. But please note that the memory required to process data might become larger while doing some computations, such as principal component analysis, which should be also taken into account. I will refer to this amount of data as reasonable sized datasets.</p><p>Loading data<a class="indexterm" id="id2"/> from text files is pretty simple with R, and loading any reasonable sized dataset can be achieved by calling the good old <code class="literal">read.table</code> function. The only issue here might be the performance: how long does it take to read, for example, a quarter of a million rows of data? Let's see:</p><div><pre class="programlisting"><strong>&gt; library('hflights')</strong>
<strong>&gt; write.csv(hflights, 'hflights.csv', row.names = FALSE)</strong>
</pre></div><div><div><h3 class="title"><a id="note03"/>Note</h3><p>As a reminder, please note that all R commands and the returned output are formatted as earlier in this book. The commands starts with <code class="literal">&gt;</code> on the first line, and the remainder of multi-line expressions starts with <code class="literal">+</code>, just as in the R console. To copy and paste these commands on your machine, please download the code examples from the Packt homepage. For more details, please see the <em>What you need for this book</em> section in the <em>Preface</em>.</p></div></div><p>Yes, we have just written an 18.5 MB text file to your disk<a class="indexterm" id="id3"/> from the <code class="literal">hflights</code> package, which includes some data on all flights departing from Houston in 2011:</p><div><pre class="programlisting"><strong>&gt; str(hflights)</strong>
<strong>'data.frame':  227496 obs. of  21 variables:</strong>
<strong> $ Year             : int  2011 2011 2011 2011 2011 2011 2011 ...</strong>
<strong> $ Month            : int  1 1 1 1 1 1 1 1 1 1 ...</strong>
<strong> $ DayofMonth       : int  1 2 3 4 5 6 7 8 9 10 ...</strong>
<strong> $ DayOfWeek        : int  6 7 1 2 3 4 5 6 7 1 ...</strong>
<strong> $ DepTime          : int  1400 1401 1352 1403 1405 1359 1359 ...</strong>
<strong> $ ArrTime          : int  1500 1501 1502 1513 1507 1503 1509 ...</strong>
<strong> $ UniqueCarrier    : chr  "AA" "AA" "AA" "AA" ...</strong>
<strong> $ FlightNum        : int  428 428 428 428 428 428 428 428 428 ...</strong>
<strong> $ TailNum          : chr  "N576AA" "N557AA" "N541AA" "N403AA" ...</strong>
<strong> $ ActualElapsedTime: int  60 60 70 70 62 64 70 59 71 70 ...</strong>
<strong> $ AirTime          : int  40 45 48 39 44 45 43 40 41 45 ...</strong>
<strong> $ ArrDelay         : int  -10 -9 -8 3 -3 -7 -1 -16 44 43 ...</strong>
<strong> $ DepDelay         : int  0 1 -8 3 5 -1 -1 -5 43 43 ...</strong>
<strong> $ Origin           : chr  "IAH" "IAH" "IAH" "IAH" ...</strong>
<strong> $ Dest             : chr  "DFW" "DFW" "DFW" "DFW" ...</strong>
<strong> $ Distance         : int  224 224 224 224 224 224 224 224 224 ...</strong>
<strong> $ TaxiIn           : int  7 6 5 9 9 6 12 7 8 6 ...</strong>
<strong> $ TaxiOut          : int  13 9 17 22 9 13 15 12 22 19 ...</strong>
<strong> $ Cancelled        : int  0 0 0 0 0 0 0 0 0 0 ...</strong>
<strong> $ CancellationCode : chr  "" "" "" "" ...</strong>
<strong> $ Diverted         : int  0 0 0 0 0 0 0 0 0 0 ...</strong>
</pre></div><div><div><h3 class="title"><a id="note04"/>Note</h3><p>The <code class="literal">hflights</code> package <a class="indexterm" id="id4"/>provides an easy way to load a subset of the huge Airline Dataset of the <a class="indexterm" id="id5"/>Research and Innovation Technology Administration at the Bureau of Transportation Statistics. The original database includes the scheduled and actual departure/arrival times of all US flights along with some other interesting information since 1987, and is often used to demonstrate machine learning and Big Data technologies. For more details on the<a class="indexterm" id="id6"/> dataset, please see the column description and other meta-data at <a class="ulink" href="http://www.transtats.bts.gov/DatabaseInfo.asp?DB_ID=120&amp;Link=0">http://www.transtats.bts.gov/DatabaseInfo.asp?DB_ID=120&amp;Link=0</a>.</p></div></div><p>We will use this 21-column data to benchmark data import times. For example, let's see how long it takes to import the CSV file with <code class="literal">read.csv</code>:</p><div><pre class="programlisting"><strong>&gt; system.time(read.csv('hflights.csv'))</strong>
<strong>   user  system elapsed </strong>
<strong>  1.730   0.007   1.738</strong>
</pre></div><p>It took a bit more than one and a half seconds to load the data from an SSD here. It's quite okay, but we can achieve far better results by identifying then specifying the classes of the columns instead of calling the default <code class="literal">type.convert</code> (see the docs in <code class="literal">read.table</code> for more details or search on StackOverflow, where the performance of <code class="literal">read.csv</code> seems to be a rather frequent and popular question):</p><div><pre class="programlisting"><strong>&gt; colClasses &lt;- sapply(hflights, class)</strong>
<strong>&gt; system.time(read.csv('hflights.csv', colClasses = colClasses))</strong>
<strong>   user  system elapsed </strong>
<strong>  1.093   0.000   1.092</strong>
</pre></div><p>It's much better! But should we trust this one observation? On our way to mastering data analysis in R, we should implement some more reliable tests—by simply replicating the task <em>n</em> times and providing a summary on the results of the simulation. This approach provides us with<a class="indexterm" id="id7"/> performance data with multiple observations, which can be used to identify statistically significant differences in the results. The <a class="indexterm" id="id8"/>
<code class="literal">microbenchmark</code> package provides a nice framework for such tasks:</p><div><pre class="programlisting"><strong>&gt; library(microbenchmark)</strong>
<strong>&gt; f &lt;- function() read.csv('hflights.csv')</strong>
<strong>&gt; g &lt;- function() read.csv('hflights.csv', colClasses = colClasses,</strong>
<strong>+                        nrows = 227496, comment.char = '')</strong>
<strong>&gt; res &lt;- microbenchmark(f(), g())</strong>
<strong>&gt; res</strong>
<strong>Unit: milliseconds</strong>
<strong> expr       min        lq   median       uq      max neval</strong>
<strong>  f() 1552.3383 1617.8611 1646.524 1708.393 2185.565   100</strong>
<strong>  g()  928.2675  957.3842  989.467 1044.571 1284.351   100</strong>
</pre></div><p>So we defined two functions: <code class="literal">f</code> stands for the default settings of <code class="literal">read.csv</code> while, in the <code class="literal">g</code> function, we passed the aforementioned column classes along with two other parameters for increased performance. The <code class="literal">comment.char</code> argument tells R not to look for comments in the imported data file, while the <code class="literal">nrows</code> parameter defined the exact number of rows to read from the file, which saves some time and space on memory allocation. Setting <code class="literal">stringsAsFactors</code> to <code class="literal">FALSE</code> might also speed up importing a bit.</p><div><div><h3 class="title"><a id="note05"/>Note</h3><p>Identifying the number of lines in the text file could be done with some third-party tools, such as <code class="literal">wc</code> on Unix, or a slightly slower alternative would be the <code class="literal">countLines</code> function from the <a class="indexterm" id="id9"/>
<code class="literal">R.utils</code> package.</p></div></div><p>But back to the results. Let's also visualize the median and related descriptive statistics of the test cases, which was run 100 times by default:</p><div><pre class="programlisting"><strong>&gt; boxplot(res, xlab  = '',</strong>
<strong>+   main = expression(paste('Benchmarking ', italic('read.table'))))</strong>
</pre></div><div><img alt="Loading text files of a reasonable size" src="img/2028OS_01_01.jpg"/></div><p>The difference seems to be significant (please feel free to do some statistical tests to verify that), so we made a 50+ percent performance boost simply by fine-tuning the parameters of <code class="literal">read.table</code>.</p><div><div><div><div><h2 class="title"><a id="ch01lvl2sec08"/>Data files larger than the physical memory</h2></div></div></div><p>Loading a larger amount of data into R from CSV files<a class="indexterm" id="id10"/> that would not fit in the memory could be done with custom packages created for such cases. For example, both the <a class="indexterm" id="id11"/>
<code class="literal">sqldf</code> package and the <a class="indexterm" id="id12"/>
<code class="literal">ff</code> package have their own solutions to load data from chunk to chunk in a custom data format. The first uses SQLite or another SQL-like database backend, while the latter creates a custom data frame with the <code class="literal">ffdf</code> class that can be stored on disk. The <a class="indexterm" id="id13"/>
<code class="literal">bigmemory</code> package provides a similar approach. Usage examples (to be benchmarked) later:</p><div><pre class="programlisting"><strong>&gt; library(sqldf)</strong>
<strong>&gt; system.time(read.csv.sql('hflights.csv'))</strong>
<strong>   user  system elapsed </strong>
<strong>  2.293   0.090   2.384 </strong>
<strong>&gt; library(ff)</strong>
<strong>&gt; system.time(read.csv.ffdf(file = 'hflights.csv'))</strong>
<strong>   user  system elapsed </strong>
<strong>  1.854   0.073   1.918 </strong>
<strong>&gt; library(bigmemory)</strong>
<strong>&gt; system.time(read.big.matrix('hflights.csv', header = TRUE))</strong>
<strong>   user  system elapsed </strong>
<strong>  1.547   0.010   1.559</strong>
</pre></div><p>Please note that the header defaults to <code class="literal">FALSE</code> with <code class="literal">read.big.matrix</code> from the <code class="literal">bigmemory</code> package, so<a class="indexterm" id="id14"/> be sure to read the manual of the referenced functions before doing your own <a class="indexterm" id="id15"/>benchmarks. Some of these functions also support performance tuning like <code class="literal">read.table</code>. For further examples and use cases, please see the <em>Large memory and out-of-memory data</em> section of the <em>High-Performance and Parallel Computing with R</em> <a class="indexterm" id="id16"/>CRAN Task View at <a class="ulink" href="http://cran.r-project.org/web/views/HighPerformanceComputing.html">http://cran.r-project.org/web/views/HighPerformanceComputing.html</a>.</p></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch01lvl1sec09"/>Benchmarking text file parsers</h1></div></div></div><p>Another notable alternative<a class="indexterm" id="id17"/> for handling and loading reasonable sized data from flat files to R is the <a class="indexterm" id="id18"/>
<code class="literal">data.table</code> package. Although it has a unique syntax differing from the traditional S-based R markup, the package comes with great documentation, vignettes, and case studies on the indeed impressive speedup it can offer for various database actions. Such uses cases and examples will be discussed in the <a class="link" href="ch03.html" title="Chapter 3. Filtering and Summarizing Data">Chapter 3</a>, <em>Filtering and Summarizing Data</em> and <a class="link" href="ch04.html" title="Chapter 4. Restructuring Data">Chapter 4</a>, <em>Restructuring Data</em>.</p><p>The package ships a custom R function to read text files with improved performance:</p><div><pre class="programlisting"><strong>&gt; library(data.table)</strong>
<strong>&gt; system.time(dt &lt;- fread('hflights.csv'))</strong>
<strong>   user  system elapsed </strong>
<strong>  0.153   0.003   0.158</strong>
</pre></div><p>Loading the data was extremely quick compared to the preceding examples, although it resulted in an R object with a custom <code class="literal">data.table</code> class, which can be easily transformed to the traditional <code class="literal">data.frame</code> if needed:</p><div><pre class="programlisting"><strong>&gt; df &lt;- as.data.frame(dt)</strong>
</pre></div><p>Or by using the <code class="literal">setDF</code> function, which provides a very fast and in-place method of object conversion without actually copying the data in the memory. Similarly, please note:</p><div><pre class="programlisting"><strong>&gt; is.data.frame(dt)</strong>
<strong>[1] TRUE</strong>
</pre></div><p>This means that a <code class="literal">data.table</code> object can fall back to act as a <code class="literal">data.frame</code> for traditional usage. Leaving the imported data as is or transforming it to <code class="literal">data.frame</code> depends on the latter usage. Aggregating, merging, and restructuring data with the first is faster compared to the standard data frame format in R. On the other hand, the user has to learn the custom syntax of <code class="literal">data.table</code>—for example, <code class="literal">DT[i, j, by]</code> stands for "from DT subset by <code class="literal">i</code>, then do <code class="literal">j</code> grouped by by". We will discuss it later in the <a class="link" href="ch03.html" title="Chapter 3. Filtering and Summarizing Data">Chapter 3</a>, <em>Filtering and Summarizing Data</em>.</p><p>Now, let's compare all the aforementioned data import methods: how fast are they? The final winner seems to<a class="indexterm" id="id19"/> be <code class="literal">fread</code> from <code class="literal">data.table</code> anyway. First, we define some methods to be benchmarked by declaring the test functions:</p><div><pre class="programlisting"><strong>&gt; .read.csv.orig   &lt;- function() read.csv('hflights.csv')</strong>
<strong>&gt; .read.csv.opt    &lt;- function() read.csv('hflights.csv',</strong>
<strong>+     colClasses = colClasses, nrows = 227496, comment.char = '',</strong>
<strong>+     stringsAsFactors = FALSE)</strong>
<strong>&gt; .read.csv.sql    &lt;- function() read.csv.sql('hflights.csv')</strong>
<strong>&gt; .read.csv.ffdf   &lt;- function() read.csv.ffdf(file = 'hflights.csv')</strong>
<strong>&gt; .read.big.matrix &lt;- function() read.big.matrix('hflights.csv',</strong>
<strong>+     header = TRUE)</strong>
<strong>&gt; .fread           &lt;- function() fread('hflights.csv')</strong>
</pre></div><p>Now, let's run all these functions 10 times each instead of several hundreds of iterations like previously—simply to save some time:</p><div><pre class="programlisting"><strong>&gt; res &lt;- microbenchmark(.read.csv.orig(), .read.csv.opt(),</strong>
<strong>+   .read.csv.sql(), .read.csv.ffdf(), .read.big.matrix(), .fread(),</strong>
<strong>+   times = 10)</strong>
</pre></div><p>And print the results of the benchmark with a predefined number of digits:</p><div><pre class="programlisting"><strong>&gt; print(res, digits = 6)</strong>
<strong>Unit: milliseconds</strong>
<strong>               expr      min      lq   median       uq      max neval</strong>
<strong>   .read.csv.orig() 2109.643 2149.32 2186.433 2241.054 2421.392    10</strong>
<strong>    .read.csv.opt() 1525.997 1565.23 1618.294 1660.432 1703.049    10</strong>
<strong>    .read.csv.sql() 2234.375 2265.25 2283.736 2365.420 2599.062    10</strong>
<strong>   .read.csv.ffdf() 1878.964 1901.63 1947.959 2015.794 2078.970    10</strong>
<strong> .read.big.matrix() 1579.845 1603.33 1647.621 1690.067 1937.661    10</strong>
<strong>           .fread()  153.289  154.84  164.994  197.034  207.279    10</strong>
</pre></div><p>Please note that now we were dealing with datasets fitting in actual physical memory, and some of the benchmarked packages are designed and optimized for far larger databases. So it seems that optimizing the <code class="literal">read.table</code> function gives a great performance boost over the default settings, although if we are after really fast importing of reasonable sized data, using the <code class="literal">data.table</code> package<a class="indexterm" id="id20"/> is the optimal solution.</p></div>
<div><div><div><div><h1 class="title"><a id="ch01lvl1sec10"/>Loading a subset of text files</h1></div></div></div><p>Sometimes we<a class="indexterm" id="id21"/> only need some parts of the dataset for an analysis, stored in a database backend or in flat files. In such situations, loading only the relevant subset of the data frame will result in much more speed improvement compared to any performance tweaks and custom packages discussed earlier.</p><p>Let's imagine we are only interested in flights to Nashville, where the annual <em>useR!</em> conference took place in 2012. This means we need only those rows of the CSV file where the <code class="literal">Dest</code> equals <code class="literal">BNA</code> (this International Air Transport Association airport code stands for Nashville International Airport).</p><p>Instead of loading the <a class="indexterm" id="id22"/>whole dataset in 160 to 2,000 milliseconds (see the previous section) and then dropping the unrelated rows (see in <a class="link" href="ch03.html" title="Chapter 3. Filtering and Summarizing Data">Chapter 3</a>, <em>Filtering and Summarizing Data</em>), let's see the possible ways of filtering the data while loading it.</p><p>The already mentioned <a class="indexterm" id="id23"/>
<code class="literal">sqldf</code> package can help with this task by specifying a SQL statement to be run on the temporary SQLite database created for the importing task:</p><div><pre class="programlisting"><strong>&gt; df &lt;- read.csv.sql('hflights.csv',</strong>
<strong>+   sql = "select * from file where Dest = '\"BNA\"'")</strong>
</pre></div><p>This <code class="literal">sql</code> argument defaults to <code class="literal">"select * from file"</code>, which means loading all fields of each row without any filters. Now we extended that with a <code class="literal">filter</code> statement. Please note that in our updated SQL statements, we also added the double quotes to the search term, as <code class="literal">sqldf</code> does not automatically recognize the quotes as special; it regards them as part of the fields. One may overcome this issue also by providing a custom filter argument, such as the following example on Unix-like systems:</p><div><pre class="programlisting"><strong>&gt; df &lt;- read.csv.sql('hflights.csv',</strong>
<strong>+   sql = "select * from file where Dest = 'BNA'",</strong>
<strong>+   filter = 'tr -d ^\\" ')</strong>
</pre></div><p>The resulting data frame holds only 3,481 observations out of the 227,496 cases in the original dataset, and filtering inside the temporary SQLite database of course speeds up data importing a bit:</p><div><pre class="programlisting"><strong>&gt; system.time(read.csv.sql('hflights.csv'))</strong>
<strong>   user  system elapsed </strong>
<strong>  2.117   0.070   2.191 </strong>
<strong>&gt; system.time(read.csv.sql('hflights.csv',</strong>
<strong>+   sql = "select * from file where Dest = '\"BNA\"'"))</strong>
<strong>   user  system elapsed </strong>
<strong>  1.700   0.043   1.745</strong>
</pre></div><p>The slight improvement is due to the fact that both R commands first loaded the CSV file to a temporary SQLite database; this process of course takes some time and cannot be eliminated from this process. To speed up this part of the evaluation, you can specify <code class="literal">dbname</code> as <code class="literal">NULL</code> for a performance boost. This way, the SQLite database would be created in memory instead of a <code class="literal">tempfile</code>, which might not be an optimal solution for larger datasets.</p><div><div><div><div><h2 class="title"><a id="ch01lvl2sec09"/>Filtering flat files before loading to R</h2></div></div></div><p>Is there a faster or smarter way to load only a portion of such a text file? One might apply some regular<a class="indexterm" id="id24"/> expression-based filtering on the flat files before passing them to R. For example, <code class="literal">grep</code> or <code class="literal">ack</code> might be a great tool to do so in a Unix environment, but it's not available by default on Windows machines, and parsing CSV files by regular expressions might result in some unexpected side-effects as well. Believe me, you never want to write a CSV, JSON, or XML parser from scratch!</p><p>Anyway, a data scientist nowadays should be a real jack-of-all-trades when it comes to processing data, so here comes a quick and dirty example to show how one could read the filtered data in less than 100 milliseconds:</p><div><pre class="programlisting"><strong>&gt; system.time(system('cat hflights.csv | grep BNA', intern = TRUE))</strong>
<strong>   user  system elapsed </strong>
<strong>  0.040   0.050   0.082</strong>
</pre></div><p>Well, that's a really great running time compared to any of our previous results! But what if we want to filter for flights with an arrival delay of more than 13.5 minutes?</p><p>Another way, and probably a more maintainable approach, would be to first load the data into a database backend, and query that when any subset of the data is needed. This way we could for example, simply populate a SQLite database in a file only once, and then later we could fetch any subsets in a fragment of <code class="literal">read.csv.sql</code>'s default run time.</p><p>So let's create a persistent SQLite database:</p><div><pre class="programlisting"><strong>&gt; sqldf("attach 'hflights_db' as new")</strong>
</pre></div><p>This command has just created a file named to <code class="literal">hflights_db</code> in the current working directory. Next, let's create a table named <code class="literal">hflights</code> and populate the content of the CSV file to the database created earlier:</p><div><pre class="programlisting"><strong>&gt; read.csv.sql('hflights.csv',</strong>
<strong>+   sql = 'create table hflights as select * from file',</strong>
<strong>+   dbname = 'hflights_db')</strong>
</pre></div><p>No benchmarking was made so far, as these steps will be run only once, while the queries for sub-parts of the dataset will probably run multiple times later:</p><div><pre class="programlisting"><strong>&gt; system.time(df &lt;- sqldf(</strong>
<strong>+   sql = "select * from hflights where Dest = '\"BNA\"'",</strong>
<strong>+   dbname = "hflights_db"))</strong>
<strong>   user  system elapsed </strong>
<strong>  0.070   0.027   0.097</strong>
</pre></div><p>And we have just loaded the required subset of the database in less than 100 milliseconds! But we can do a lot better if we plan to often query the persistent database: why not dedicate a real database instance for our dataset instead of a simple file-based and server-less<a class="indexterm" id="id25"/> SQLite backend?</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch01lvl1sec11"/>Loading data from databases</h1></div></div></div><p>The great advantage <a class="indexterm" id="id26"/>of using a dedicated database backend instead of loading data from the disk on demand is that databases provide:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Faster access to<a class="indexterm" id="id27"/> the whole or selected parts of large tables</li><li class="listitem" style="list-style-type: disc">Powerful and quick ways to aggregate and filter data before loading it to R</li><li class="listitem" style="list-style-type: disc">Infrastructure to store data in a relational, more structured scheme compared to the traditional matrix model of spreadsheets and R objects</li><li class="listitem" style="list-style-type: disc">Procedures to join and merge related data</li><li class="listitem" style="list-style-type: disc">Concurrent and network access from multiple clients at the same time</li><li class="listitem" style="list-style-type: disc">Security policies and limits to access the data</li><li class="listitem" style="list-style-type: disc">A scalable and configurable backend to store data</li></ul></div><p>The <code class="literal">DBI</code> package provides<a class="indexterm" id="id28"/> a database interface, a communication channel between R and various<a class="indexterm" id="id29"/> <strong>relational database management systems</strong> (<strong>RDBMS</strong>), such as MySQL, PostgreSQL, MonetDB, Oracle, and for example Open Document Databases, and so on. There is no real need<a class="indexterm" id="id30"/> to install the package on its own because, acting as an interface, it will be installed anyway as a dependency, if needed.</p><p>Connecting to a database and fetching data is pretty similar with all these backends, as all are based on the relational model and using SQL to manage and query data. Please be advised that there are some important differences between the aforementioned database engines and that several more open-source and commercial alternatives also exist. But we will not dig into the <a class="indexterm" id="id31"/>details on how to choose a database backend or how to build a <a class="indexterm" id="id32"/>data warehouse and <strong>extract, transform, and load</strong> (<strong>ETL</strong>) workflows, but we will only concentrate on making connections and managing data from R.</p><div><div><h3 class="title"><a id="note06"/>Note</h3><p>SQL, originally developed at IBM, with its more than 40 years of history, is one of the most important programming languages nowadays—with various dialects and implementations. Being one of the most popular declarative languages all over the world, there are many online tutorials and free courses to learn how to query and manage data with SQL, which is definitely one of the most important tools in every data scientist's Swiss army knife.</p><p>So, besides R, it's really worth knowing your way around RDBMS, which are extremely common in any industry you may be working at as a data analyst or in a similar position.</p></div></div><div><div><div><div><h2 class="title"><a id="ch01lvl2sec10"/>Setting up the test environment</h2></div></div></div><p>Database backends<a class="indexterm" id="id33"/> usually run on servers remote from the users doing data analysis, but for testing purposes, it might be a good idea to install local instances on the machine running R. As the installation process can be extremely different on various operating systems, we will not enter into any details of the installation steps, but <a class="indexterm" id="id34"/>we will rather refer to where the software can be downloaded from and some further links to great resources and documentation for installation.</p><p>Please note that installing and actually trying to load data from these databases is totally optional and you do not have to follow each step—the rest of the book will not depend on any database knowledge or prior experience with databases. On the other hand, if you do not want to mess your workspace with temporary installation of multiple database applications for testing purposes, using<a class="indexterm" id="id35"/> virtual machines might be an optimal workaround. Oracle's <code class="literal">VirtualBox</code> provides a free and easy way of running multiple virtual machines with their dedicated operating system and userspace.</p><div><div><h3 class="title"><a id="note07"/>Note</h3><p>For detailed instructions on how to download then import a <code class="literal">VirtualBox</code> image, see the <em>Oracle</em> section.</p></div></div><p>This way you can quickly deploy a fully functional, but disposable, database environment to test-drive the following examples of this chapter. In the following image, you can see <code class="literal">VirtualBox</code> with four installed virtual machines, of which three are running in the background to provide some database backends for testing purposes:</p><div><img alt="Setting up the test environment" src="img/2028OS_01_02.jpg"/></div><div><div><h3 class="title"><a id="note08"/>Note</h3><p>
<code class="literal">VirtualBox</code> can be installed by your operating system's package manager on Linux or by downloading the installation binary/sources from <a class="ulink" href="https://www.virtualbox.org/wiki/Downloads">https://www.virtualbox.org/wiki/Downloads</a>. For detailed and operating-system specific installation information, please refer to the <em>Chapter 2</em>, <em>Installation details</em> of the manual: <a class="ulink" href="http://www.virtualbox.org/manual/">http://www.virtualbox.org/manual/</a>.</p></div></div><p>Nowadays, setting<a class="indexterm" id="id36"/> up and running a virtual machine is really intuitive<a class="indexterm" id="id37"/> and easy; basically you only need a virtual machine image to be loaded and launched. Some <a class="indexterm" id="id38"/>virtual machines, so called appliances, include the operating system, with a number of further software usually already configured to work, for simple, easy and quick distribution.</p><div><div><h3 class="title"><a id="tip02"/>Tip</h3><p>Once again, if you do not enjoy installing and testing new software or spending time on learning about the infrastructure empowering your data needs, the following steps are not necessary and you can freely skip these optional tasks primarily described for full-stack developers/data scientists.</p></div></div><p>Such pre-configured virtual machines to be run on any computer can be downloaded from various providers on the Internet in multiple file formats, such as OVF or OVA. General purpose <code class="literal">VirtualBox</code> virtual appliances can be downloaded for example from <a class="ulink" href="http://virtualboximages.com/vdi/index">http://virtualboximages.com/vdi/index</a> or <a class="ulink" href="http://virtualboxes.org/images/">http://virtualboxes.org/images/</a>.</p><div><div><h3 class="title"><a id="note09"/>Note</h3><p>Virtual appliances should be imported in VirtualBox, while non-OVF/OVA disk images should be attached to newly created<a class="indexterm" id="id39"/> virtual machines; thus, some extra manual configuration might also be needed.</p></div></div><p>Oracle also has a repository with a bunch of useful virtual images for data scientist apprentices and other developers at <a class="ulink" href="http://www.oracle.com/technetwork/community/developer-vm/index.html">http://www.oracle.com/technetwork/community/developer-vm/index.html</a>, with for example the Oracle Big Data Lite VM developer virtual appliance featuring the following most<a class="indexterm" id="id40"/> important components:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Oracle Database</li><li class="listitem" style="list-style-type: disc">Apache Hadoop and various tools in Cloudera distribution</li><li class="listitem" style="list-style-type: disc">The Oracle R Distribution</li><li class="listitem" style="list-style-type: disc">Build on<a class="indexterm" id="id41"/> Oracle Enterprise Linux</li></ul></div><p>Disclaimer: Oracle wouldn't be my first choice personally, but they did a great job with their platform-independent virtualization environment, just like with providing free developer VMs based on their commercial products. In short, it's definitely worth using the provided Oracle tools.</p><div><div><h3 class="title"><a id="note10"/>Note</h3><p>If you cannot reach your installed<a class="indexterm" id="id42"/> virtual machines on the network, please update your network settings to use <em>Host-only adapter</em> if no Internet connection is needed, or <em>Bridged networking</em> for a more robust setup. The latter setting will reserve an extra IP on your local network for the virtual machine; this way, it becomes accessible easily. Please find more details and examples with screenshots in the <em>Oracle database</em> section.</p></div></div><p>Another <a class="indexterm" id="id43"/>good source of virtual appliances created for open-source database engines<a class="indexterm" id="id44"/> is the Turnkey GNU/Linux repository at <a class="ulink" href="http://www.turnkeylinux.org/database">http://www.turnkeylinux.org/database</a>. These images are based on<a class="indexterm" id="id45"/> Debian Linux, are totally free to use, and currently support the MySQL, PostgreSQL, MongoDB, and <a class="indexterm" id="id46"/>CouchDB databases.</p><p>A great advantage of the Turnkey Linux media is that it includes only open-source, free software and non-proprietary stuff. Besides, the disk images are a lot smaller and include only the required components for one dedicated database engine. This also results in far faster installation with less overhead in terms of the required disk and memory space.</p><p>Further similar virtual appliances are available at <a class="ulink" href="http://www.webuzo.com/sysapps/databases">http://www.webuzo.com/sysapps/databases</a> with a <a class="indexterm" id="id47"/>wider range of database backends, such as <a class="indexterm" id="id48"/>Cassandra, HBase, Neo4j, Hypertable, or <a class="indexterm" id="id49"/>Redis, although some of the Webuzo <a class="indexterm" id="id50"/>appliances<a class="indexterm" id="id51"/> might require<a class="indexterm" id="id52"/> a paid subscription for deployment.</p><p>And as the new cool being Docker, I even more suggest you to get familiar with its concept on deploying software containers incredibly fast. Such container can be described as a standalone filesystem including the operating system, libraries, tools, data and so is based on abstraction layers of Docker images. In practice this means that you can fire up a database including some demo data with a one-liner command on your localhost, and developing such custom images is similarly easy. Please see some simple examples and <a class="indexterm" id="id53"/>further references at my R and Pandoc-related <a class="indexterm" id="id54"/>Docker images described at <a class="ulink" href="https://github.com/cardcorp/card-rocker">https://github.com/cardcorp/card-rocker</a>.</p></div><div><div><div><div><h2 class="title"><a id="ch01lvl2sec11"/>MySQL and MariaDB</h2></div></div></div><p>MySQL is the most popular <a class="indexterm" id="id55"/>open-source database engine all over the world based on the number of<a class="indexterm" id="id56"/> mentions, job offers, Google searches, and so on, summarized by the <a class="indexterm" id="id57"/>DB-Engines Ranking: <a class="ulink" href="http://db-engines.com/en/ranking">http://db-engines.com/en/ranking</a>. Mostly used in Web development, the high popularity is probably due to the fact that MySQL is free, platform-independent, and relatively easy to set up and configure—just like its drop-in replacement fork called<a class="indexterm" id="id58"/> <strong>MariaDB</strong>.</p><div><div><h3 class="title"><a id="note11"/>Note</h3><p>MariaDB is <a class="indexterm" id="id59"/>a community-developed, fully open-source fork of MySQL, started and led by the founder of MySQL, Michael Widenius. It was later merged with SkySQL; thus further ex-MySQL executives and investors joined the fork. MariaDB was created after Sun Microsystems bought MySQL, currently owned by Oracle, and the development of the database engine changed.</p></div></div><p>We will refer to both engines as MySQL in the book to keep it simple, as MariaDB can be considered as a drop-in replacement for MySQL, so please feel free to reproduce the following examples with either MySQL or MariaDB.</p><p>Although the installation of a MySQL server is<a class="indexterm" id="id60"/> pretty straightforward on most operating systems (<a class="ulink" href="https://dev.mysql.com/downloads/mysql/">https://dev.mysql.com/downloads/mysql/</a>), one might rather prefer to have the database installed in a virtual machine. Turnkey Linux provides small but fully configured, virtual appliances for free: <a class="ulink" href="http://www.turnkeylinux.org/mysql">http://www.turnkeylinux.org/mysql</a>.</p><p>R provides multiple ways to query data from a MySQL database. One option is to use the <code class="literal">RMySQL</code> package, which<a class="indexterm" id="id61"/> might be a bit tricky for some users to install. If you are on Linux, please be sure to install the development packages of MySQL along with the MySQL client, so that the package can compile on your system. And, as there are no binary packages available on CRAN for Windows installation due to the high variability of MySQL versions, Windows users should also compile the package from source:</p><div><pre class="programlisting"><strong>&gt; install.packages('RMySQL', type = 'source')</strong>
</pre></div><p>Windows users might find the<a class="indexterm" id="id62"/> following blog post useful about the detailed installation steps: <a class="ulink" href="http://www.ahschulz.de/2013/07/23/installing-rmysql-under-windows/">http://www.ahschulz.de/2013/07/23/installing-rmysql-under-windows/</a>.</p><div><div><h3 class="title"><a id="note12"/>Note</h3><p>For the sake of simplicity, we will refer to the MySQL server as <code class="literal">localhost</code> listening on the default 3306 port; user will stand as <code class="literal">user</code> and password as <code class="literal">password</code> in all database connections. We will work with the <code class="literal">hflights</code> table in the <code class="literal">hflights_db</code> database, just like in the SQLite examples a few pages earlier. If you are working in a remote or virtual server, please modify the <code class="literal">host</code>, <code class="literal">username</code>, and so on arguments of the following code examples accordingly.</p></div></div><p>After successfully<a class="indexterm" id="id63"/> installing and starting the MySQL server, we have to set up a test database, which<a class="indexterm" id="id64"/> we could later populate in R. To this end, let us start the MySQL command-line tool to create the database and a test user.</p><p>Please note that the<a class="indexterm" id="id65"/> following example was run on Linux, and a Windows user might have<a class="indexterm" id="id66"/> to also provide the path and probably the <code class="literal">exe</code> file extension to start the MySQL command-line tool:</p><div><img alt="MySQL and MariaDB" src="img/2028OS_01_03.jpg"/></div><p>This quick session can be seen in the previous screenshot, where we first connected to the MySQL server in the command-line as the <code class="literal">root</code> (admin) user. Then we created a database named <code class="literal">hflights_db</code>, and granted all privileges and permissions of that database to a new user called <code class="literal">user</code> with the <a class="indexterm" id="id67"/>password set to <code class="literal">password</code>. Then we simply verified whether we could connect to the database with the newly created user, and we exited the command-line MySQL client.</p><p>To load data from <a class="indexterm" id="id68"/>a MySQL database into R, first we have to connect and also <a class="indexterm" id="id69"/>often authenticate with the server. This can be done with the automatically loaded <a class="indexterm" id="id70"/>
<code class="literal">DBI</code> package when attaching <code class="literal">RMySQL</code>:</p><div><pre class="programlisting"><strong>&gt; library(RMySQL)</strong>
<strong>Loading required package: DBI</strong>
<strong>&gt; con &lt;- dbConnect(dbDriver('MySQL'),</strong>
<strong>+   user = 'user', password = 'password', dbname = 'hflights_db')</strong>
</pre></div><p>Now we can refer to our MySQL connection as <code class="literal">con</code>, where we want to deploy the <code class="literal">hflights</code> dataset for later access:</p><div><pre class="programlisting"><strong>&gt; dbWriteTable(con, name = 'hflights', value = hflights)</strong>
<strong>[1] TRUE</strong>
<strong>&gt; dbListTables(con)</strong>
<strong>[1] "hflights"</strong>
</pre></div><p>The <code class="literal">dbWriteTable</code> function wrote the <code class="literal">hflights</code> data frame with the same name to the previously defined connection. The latter command shows all the tables in the currently used databases, equivalent to the <code class="literal">SHOW TABLES </code>SQL command. Now that we have our original CVS file imported to MySQL, let's see how long it takes to read the whole dataset:</p><div><pre class="programlisting"><strong>&gt; system.time(dbReadTable(con, 'hflights'))</strong>
<strong>   user  system elapsed </strong>
<strong>  0.993   0.000   1.058</strong>
</pre></div><p>Or we can do so with a direct SQL command passed to <code class="literal">dbGetQuery</code> from the <a class="indexterm" id="id71"/>same <code class="literal">DBI</code> package:</p><div><pre class="programlisting"><strong>&gt; system.time(dbGetQuery(con, 'select * from hflights'))</strong>
<strong>   user  system elapsed </strong>
<strong>  0.910   0.000   1.158</strong>
</pre></div><p>And, just to keep further examples simpler, let's get back to the <a class="indexterm" id="id72"/>
<code class="literal">sqldf</code> package, which stands for "SQL select on data frames". As a matter of fact, <code class="literal">sqldf</code> is a convenient wrapper around DBI's <code class="literal">dbSendQuery</code> function with some useful defaults, and returns <code class="literal">data.frame</code>. This wrapper can query various database engines, such as SQLite, MySQL, H2, or PostgreSQL, and defaults to the one specified in the global <code class="literal">sqldf.driver</code> option; or, if that's <code class="literal">NULL</code>, it will then check if any R packages have been loaded for the aforementioned backends.</p><p>As we have already loaded <code class="literal">RMySQL</code>, now <code class="literal">sqldf</code> will default to using MySQL instead of SQLite. But we still have to specify which connection to use; otherwise the function will try to open a new one—without any idea about our complex username and password combination, not<a class="indexterm" id="id73"/> to mention the mysterious database name. The connection can<a class="indexterm" id="id74"/> be passed in each <code class="literal">sqldf</code> expression or defined once in a global option:</p><div><pre class="programlisting"><strong>&gt; options('sqldf.connection' = con)</strong>
<strong>&gt; system.time(sqldf('select * from hflights'))</strong>
<strong>   user  system elapsed </strong>
<strong>  0.807   0.000   1.014</strong>
</pre></div><p>The difference in the<a class="indexterm" id="id75"/> preceding three versions of the same task does not seem to be significant. That 1-second timing seems to be a pretty okay result compared to our previously tested methods—although loading the whole dataset with <code class="literal">data.table</code> still beats this result. What about if we only need a subset of the dataset? Let's fetch only those flights ending in Nashville, just like in our previous SQLite example:</p><div><pre class="programlisting"><strong>&gt; system.time(sqldf('SELECT * FROM hflights WHERE Dest = "BNA"'))</strong>
<strong>   user  system elapsed </strong>
<strong>  0.000   0.000   0.281</strong>
</pre></div><p>This does not seem to be very convincing compared to our previous SQLite test, as the latter could reproduce the same result in less than 100 milliseconds. But please also note that that both the user and system elapsed times are zero, which was not the case with SQLite.</p><div><div><h3 class="title"><a id="note13"/>Note</h3><p>The returned elapsed time by <code class="literal">system.time</code> means the number of milliseconds passed since the start of the evaluation. The user and system times are a bit trickier to understand; they are reported by the operating system. More or less, <code class="literal">user</code> means the CPU time spent by the called process (like R or the MySQL server), while <code class="literal">system</code> reports the CPU time required by the kernel and other operating system processes (such as opening a file for reading). See <code class="literal">?proc.time</code> for further details.</p></div></div><p>This means that no CPU time was used at all to return the required subset of data, which took almost 100 milliseconds with SQLite. How is it possible? What if we index the database on <code class="literal">Dest</code>?</p><div><pre class="programlisting"><strong>&gt; dbSendQuery(con, 'CREATE INDEX Dest_idx ON hflights (Dest(3));')</strong>
</pre></div><p>This SQL query stands for creating an index named <code class="literal">Dest_idx</code> in our table based on the <code class="literal">Dest</code> column's first three letters.</p><div><div><h3 class="title"><a id="note14"/>Note</h3><p>SQL index can seriously boost the performance of a <code class="literal">SELECT</code> statement with <code class="literal">WHERE</code> clauses, as MySQL this way does not have to read through the entire database to match each row, but it can determine the position of the relevant search results. This<a class="indexterm" id="id76"/> performance boost becomes more and more spectacular with larger databases, although it's also worth mentioning that indexing only makes sense if subsets of data are queried most of the time. If most or all data is needed, sequential reads would be faster.</p></div></div><p>Live example:</p><div><pre class="programlisting"><strong>&gt; system.time(sqldf('SELECT * FROM hflights WHERE Dest = "BNA"'))</strong>
<strong>   user  system elapsed </strong>
<strong>  0.024   0.000   0.034</strong>
</pre></div><p>It seems to be a lot better! Well, of course, we could have also indexed the SQLite database, not just the MySQL instance. To test it again, we have to revert the default <code class="literal">sqldf</code> driver to SQLite, which was overridden by loading the <a class="indexterm" id="id77"/>
<code class="literal">RMySQL</code> package:</p><div><pre class="programlisting"><strong>&gt; options(sqldf.driver = 'SQLite')</strong>
<strong>&gt; sqldf("CREATE INDEX Dest_idx ON hflights(Dest);",</strong>
<strong>+   dbname = "hflights_db"))</strong>
<strong>NULL</strong>
<strong>&gt; system.time(sqldf("select * from hflights where</strong>
<strong>+   Dest = '\"BNA\"'", dbname = "hflights_db"))</strong>
<strong>   user  system elapsed </strong>
<strong>  0.034   0.004   0.036</strong>
</pre></div><p>So it seems that both<a class="indexterm" id="id78"/> database engines are capable of returning the required subset of data in a fraction of a second, which is a lot better even compared to what we achieved with the impressive <code class="literal">data.table</code> before.</p><p>Although SQLite<a class="indexterm" id="id79"/> proved to be faster than MySQL in some earlier examples, there are many reasons to choose the latter in most situations. First, SQLite is a file-based database, which simply means that the database should be on a filesystem attached to the computer running R. This usually means having the SQLite database and the running R session on the same computer. Similarly, MySQL can handle larger amount of data; it has user management and rule-based control on what they can do, and concurrent access to the same dataset. The smart data scientist knows how to choose his weapon—depending on the task, another database backend might be the optimal solution. Let's see what other options we have in R!</p></div><div><div><div><div><h2 class="title"><a id="ch01lvl2sec12"/>PostgreSQL</h2></div></div></div><p>While MySQL is said to<a class="indexterm" id="id80"/> be the most popular open-source relational database<a class="indexterm" id="id81"/> management system, PostgreSQL is famous for being "the world's most advanced open source database". This means that PostgreSQL is often considered to have more features compared to the simpler but faster MySQL, including analytic functions, which has led to PostgreSQL often being described as the open-source version of Oracle.</p><p>This sounds<a class="indexterm" id="id82"/> rather funny now, as Oracle owns MySQL today. So a bunch of things have changed in the past 20-30 years of RDBMS history, and PostgreSQL is not so slow any more. On the other hand, MySQL has also gained some nice new features—for example MySQL also became ACID-compliant with the <code class="literal">InnoDB</code> engine, allowing rollback to previous states of the database. There are some other differences between the two popular database servers that might support choosing either of them. Now let's see what happens if our data provider has a liking for PostgreSQL instead of MySQL!</p><p>Installing PostgreSQL is similar to<a class="indexterm" id="id83"/> MySQL. One may install the software with the operating system's package <a class="indexterm" id="id84"/>manager, download a graphical installer from <a class="ulink" href="http://www.enterprisedb.com/products-services-training/pgdownload">http://www.enterprisedb.com/products-services-training/pgdownload</a>, or run a virtual appliance with, for example, the free Turnkey Linux, which provides a small but fully configured disk image for free at <a class="ulink" href="http://www.turnkeylinux.org/postgresql">http://www.turnkeylinux.org/postgresql</a>.</p><div><div><h3 class="title"><a id="tip03"/>Tip</h3><p>
<strong>Downloading the example code</strong>
</p><p>You can download the example code files from your account at <a class="ulink" href="http://www.packtpub.com">http://www.packtpub.com</a> for all the Packt Publishing books you have purchased. If you purchased this book elsewhere, you can visit <a class="ulink" href="http://www.packtpub.com/support">http://www.packtpub.com/support</a> and register to have the files e-mailed directly to you.</p></div></div><p>After successfully installing and starting the server, let's set up the test database—just like we did after the MySQL installation:</p><div><img alt="PostgreSQL" src="img/2028OS_01_04.jpg"/></div><p>The syntax is a bit different in some cases, and we have used some command-line tools for the user and <a class="indexterm" id="id85"/>database creation. These helper programs are shipped with PostgreSQL by default, and MySQL also have some similar functionality with <code class="literal">mysqladmin</code>.</p><p>After setting up<a class="indexterm" id="id86"/> the initial test environment, or if we already have a working database instance to connect, we can repeat the previously described data management tasks with the help of the<a class="indexterm" id="id87"/> <code class="literal">RPostgreSQL</code> package:</p><div><pre class="programlisting"><strong>&gt; library(RPostgreSQL)</strong>
<strong>Loading required package: DBI</strong>
</pre></div><div><div><h3 class="title"><a id="note15"/>Note</h3><p>If your R session starts to throw strange error messages in the following examples, it's highly possible that the loaded R packages are conflicting. You could simply start a clean R session, or detach the previously attached packages—for example, <code class="literal">detach('package:RMySQL', unload = TRUE)</code>.</p></div></div><p>Connecting to the database (listening on the default port number 5432) is again familiar:</p><div><pre class="programlisting"><strong>&gt; con &lt;- dbConnect(dbDriver('PostgreSQL'), user = 'user',</strong>
<strong>+   password = 'password', dbname = 'hflights_db')</strong>
</pre></div><p>Let's verify that we are connected to the right database instance, which should be currently empty without the <code class="literal">hflights</code> table:</p><div><pre class="programlisting"><strong>&gt; dbListTables(con)</strong>
<strong>character(0)</strong>
<strong>&gt; dbExistsTable(con, 'hflights')</strong>
<strong>[1] FALSE</strong>
</pre></div><p>Then let's write our demo table in PostgreSQL and see if the old rumor about it being slower than MySQL is still true:</p><div><pre class="programlisting"><strong>&gt; dbWriteTable(con, 'hflights', hflights)</strong>
<strong>[1] TRUE</strong>
<strong>&gt; system.time(dbReadTable(con, 'hflights'))</strong>
<strong>   user  system elapsed </strong>
<strong>  0.590   0.013   0.921</strong>
</pre></div><p>Seems to be<a class="indexterm" id="id88"/> impressive! What about loading partial data?</p><div><pre class="programlisting"><strong>&gt; system.time(dbGetQuery(con,</strong>
<strong>+ statement = "SELECT * FROM hflights WHERE \"Dest\" = 'BNA';"))</strong>
<strong>   user  system elapsed </strong>
<strong>  0.026   0.000   0.082</strong>
</pre></div><p>Just under 100 milliseconds without indexing! Please note the extra escaped quotes around <code class="literal">Dest</code>, as the default PostgreSQL behavior folds unquoted column names to lower case, which would result in a column <code class="literal">dest</code> does not exist error. Creating an index and running the preceding query with much improved speed can be easily reproduced based on the MySQL example.</p></div><div><div><div><div><h2 class="title"><a id="ch01lvl2sec13"/>Oracle database</h2></div></div></div><p>Oracle Database <a class="indexterm" id="id89"/>Express Edition can be downloaded and installed from <a class="ulink" href="http://www.oracle.com/technetwork/database/database-technologies/express-edition/downloads/index.html">http://www.oracle.com/technetwork/database/database-technologies/express-edition/downloads/index.html</a>. Although this is not a full-featured Oracle database, and<a class="indexterm" id="id90"/> it suffers from serious limitations, the Express<a class="indexterm" id="id91"/> Edition is a free and not too resource-hungry way to build a test environment at home.</p><div><div><h3 class="title"><a id="note16"/>Note</h3><p>Oracle database is said to be the most popular database management system in the world, although it is available only with a proprietary license, unlike the previous two discussed RDBMSs, which means that Oracle offers the product with term licensing. On the other hand, the paid license also comes with priority support from the developer company, which is often a strict requirement in enterprise environments. Oracle Database has supported a variety of nice features since its first release in 1980, such as sharding, master-master replication, and full ACID properties.</p></div></div><p>Another way of getting a working Oracle database for testing purposes is to download an Oracle<a class="indexterm" id="id92"/> Pre-Built Developer VM from <a class="ulink" href="http://www.oracle.com/technetwork/community/developer-vm/index.html">http://www.oracle.com/technetwork/community/developer-vm/index.html</a>, or a much smaller image custom created for <em>Hands-on Database Application Development</em> at <em>Oracle Technology Network Developer Day</em>: <a class="ulink" href="http://www.oracle.com/technetwork/database/enterprise-edition/databaseappdev-vm-161299.html">http://www.oracle.com/technetwork/database/enterprise-edition/databaseappdev-vm-161299.html</a>. We will follow the instructions from the latter source.</p><p>After accepting the License <a class="indexterm" id="id93"/>Agreement and registering for free at Oracle, we can download the <code class="literal">OTN_Developer_Day_VM.ova</code> virtual appliance. Let's import it to VirtualBox via <strong>Import appliance</strong> in the <strong>File</strong> menu, then choose the <code class="literal">ova</code> file, and click <strong>Next</strong>:</p><div><img alt="Oracle database" src="img/2028OS_01_05.jpg"/></div><p>After clicking<a class="indexterm" id="id94"/> <strong>Import</strong>, you will have to agree again to the Software License Agreement. Importing the virtual disk image (15 GB) might take a few minutes:</p><div><img alt="Oracle database" src="img/2028OS_01_06.jpg"/></div><p>After importing has<a class="indexterm" id="id95"/> finished, we should first update the networking configuration so that we can access the internal database of the virtual machine from outside. So let's switch from <strong>NAT</strong> to <strong>Bridged Adapter</strong> in the settings:</p><div><img alt="Oracle database" src="img/2028OS_01_07.jpg"/></div><p>Then we can simply start the newly created virtual machine in VirtualBox. After Oracle Linux has booted, we can log in with the default <code class="literal">oracle</code> password.</p><p>Although we have set a<a class="indexterm" id="id96"/> bridged networking interface for our virtual machine, which means that the VM is directly connected to our real sub-network with a real IP address, the machine is <a class="indexterm" id="id97"/>not yet accessible over the network. To connect with the default DHCP settings, simply navigate to the top red bar and look for the networking icon, then select <strong>System eth0</strong>. After a few seconds the VM is accessible from your host machine, as the guest system should be connected to your network. You can verify that by running the <code class="literal">ifconfig</code> or <code class="literal">ip addr show eth0</code> command in the already running console:</p><div><img alt="Oracle database" src="img/2028OS_01_08.jpg"/></div><p>Unfortunately, this already<a class="indexterm" id="id98"/> running Oracle database is not yet accessible outside the guest machine. The developer VM comes with a rather strict firewall by default, which should be disabled first. To see the rules in effect, run the standard <code class="literal">iptables -L -n</code> command and, to flush all rules, execute <code class="literal">iptables -F</code>:</p><div><img alt="Oracle database" src="img/2028OS_01_09.jpg"/></div><p>Now that we have <a class="indexterm" id="id99"/>a running and remotely accessible Oracle database, let's prepare the R client side. Installing the<a class="indexterm" id="id100"/> <code class="literal">ROracle</code> package might get tricky on some operating systems, as there are no <a class="indexterm" id="id101"/>prebuilt binary packages and you have to manually install the Oracle Instant Client Lite and SDK libraries before compiling the package from source. If the compiler complained about the path of your previously installed Oracle libraries, please pass the <code class="literal">--with-oci-lib</code> and <code class="literal">--with-oci-inc</code> arguments with your custom paths with the <code class="literal">--configure-args</code> parameter. More details can be found in<a class="indexterm" id="id102"/> the package installation document: <a class="ulink" href="http://cran.r-project.org/web/packages/ROracle/INSTALL">http://cran.r-project.org/web/packages/ROracle/INSTALL</a>.</p><p>For example, on Arch Linux you can install the Oracle libs from AUR, then run the following command in <code class="literal">bash</code> after downloading the R package from CRAN:</p><div><pre class="programlisting"><strong># R CMD INSTALL --configure-args='--with-oci-lib=/usr/include/    \</strong>
<strong>&gt;  --with-oci-inc=/usr/share/licenses/oracle-instantclient-basic' \</strong>
<strong>&gt;  ROracle_1.1-11.tar.gz</strong>
</pre></div><p>After installing and loading the package, opening a connection is extremely similar to the pervious examples with <code class="literal">DBI::dbConnect</code>. We only pass an extra parameter here. First, let us specify the<a class="indexterm" id="id103"/> hostname or direct IP address of the Oracle database included in the <code class="literal">dbname</code> argument. Then we can connect to the already existing PDB1 database of the developer machine instead of the previously used <code class="literal">hflights_db</code>—just to save some time and space in the book on slightly off-topic database management tasks:</p><div><pre class="programlisting"><strong>&gt; library(ROracle)</strong>
<strong>Loading required package: DBI</strong>
<strong>&gt; con &lt;- dbConnect(dbDriver('Oracle'), user = 'pmuser',</strong>
<strong>+   password = 'oracle', dbname = '//192.168.0.16:1521/PDB1')</strong>
</pre></div><p>And we have a working connection to Oracle RDBMS:</p><div><pre class="programlisting"><strong>&gt; summary(con)</strong>
<strong>User name:             pmuser </strong>
<strong>Connect string:        //192.168.0.16:1521/PDB1 </strong>
<strong>Server version:        12.1.0.1.0 </strong>
<strong>Server type:           Oracle RDBMS </strong>
<strong>Results processed:     0 </strong>
<strong>OCI prefetch:          FALSE </strong>
<strong>Bulk read:             1000 </strong>
<strong>Statement cache size:  0 </strong>
<strong>Open results:          0 </strong>
</pre></div><p>Let's see what we have in the bundled database on the development VM:</p><div><pre class="programlisting"><strong>&gt; dbListTables(con)</strong>
<strong>[1] "TICKER_G" "TICKER_O" "TICKER_A" "TICKER"  </strong>
</pre></div><p>So it seems that we have a table called <code class="literal">TICKER</code> with three views on tick data of three symbols. Saving the <code class="literal">hflights</code> table in the same database will not do any harm, and we can also instantly test the speed of the Oracle database when reading the whole table:</p><div><pre class="programlisting"><strong>&gt; dbWriteTable(con, 'hflights', hflights)</strong>
<strong>[1] TRUE</strong>
<strong>&gt; system.time(dbReadTable(con, 'hflights'))</strong>
<strong>   user  system elapsed </strong>
<strong>  0.980   0.057   1.256</strong>
</pre></div><p>And the extremely familiar subset with 3,481 cases:</p><div><pre class="programlisting"><strong>&gt; system.time(dbGetQuery(con,</strong>
<strong>+ "SELECT * FROM \"hflights\" WHERE \"Dest\" = 'BNA'"))</strong>
<strong>   user  system elapsed</strong>
<strong>  0.046   0.003   0.131</strong>
</pre></div><p>Please note the quotes around the table name. In the previous examples with MySQL and PostgreSQL, the SQL statements run fine without those. However, the quotes are needed in the Oracle database, as we have saved the table with an all-lowercase name, and the default rule in Oracle DB is to store object names in upper case. The only other option is to use double quotes to create them, which is what we did; thus we have to refer to the table with quotes around the lowercase name.</p><div><div><h3 class="title"><a id="note17"/>Note</h3><p>We started <a class="indexterm" id="id104"/>with unquoted table and column names in MySQL, then had to add escaped quotes around the variable name in the PostgreSQL query run from R, and now in Oracle database we have to put both names between quotes—which demonstrates the slight differences in the various SQL flavors (such as MySQL, PostgreSQL, PL/SQL of Oracle or Microsoft's Transact-SQL) on top of ANSI SQL.</p><p>And more importantly: do not stick to one database engine with all your projects, but rather choose the optimal DB for the task if company policy doesn't stop you doing so.</p></div></div><p>These results were not so impressive compared to what we have seen by PostgreSQL, so let's also see the results of an indexed query:</p><div><pre class="programlisting"><strong>&gt; dbSendQuery(con, 'CREATE INDEX Dest_idx ON "hflights" ("Dest")')</strong>
<strong>Statement:            CREATE INDEX Dest_idx ON "hflights" ("Dest") </strong>
<strong>Rows affected:        0 </strong>
<strong>Row count:            0 </strong>
<strong>Select statement:     FALSE </strong>
<strong>Statement completed:  TRUE </strong>
<strong>OCI prefetch:         FALSE </strong>
<strong>Bulk read:            1000 </strong>
<strong>&gt; system.time(dbGetQuery(con, "SELECT * FROM \"hflights\"</strong>
<strong>+ WHERE \"Dest\" = 'BNA'"))</strong>
<strong>   user  system elapsed </strong>
<strong>  0.023   0.000   0.069</strong>
</pre></div><p>I leave the full-scale comparative testing and benchmarking to you, so that you can run custom queries in the tests fitting your exact needs. It is highly possible that the different database engines perform differently in special use cases.</p><p>To make this process a bit more seamless and easier to implement, let's check out another R way of connecting to databases, although probably with a slight performance trade-off. For a quick scalability and performance comparison on connecting to Oracle databases with different approaches in R, please see <a class="ulink" href="https://blogs.oracle.com/R/entry/r_to_oracle_database_connectivity">https://blogs.oracle.com/R/entry/r_to_oracle_database_connectivity</a>.</p></div><div><div><div><div><h2 class="title"><a id="ch01lvl2sec14"/>ODBC database access</h2></div></div></div><p>As mentioned earlier, installing<a class="indexterm" id="id105"/> the native client software, libraries, and header files for the different databases so that the custom R packages can be built from source can be tedious and rather tricky in some cases. Fortunately, we can also try to do the opposite of this process. An alternative solution can be installing a middleware <strong>Application Programming Interface</strong> (<strong>API</strong>) in the databases, so that R, or as a matter of fact any <a class="indexterm" id="id106"/>other tool, could communicate with them in a standardized and more convenient way. However, please be advised that this more convenient way impairs performance due to the translation layer between the application and the DBMS.</p><p>The <code class="literal">RODBC</code> package<a class="indexterm" id="id107"/> implements access to such a layer. The<a class="indexterm" id="id108"/> <strong>Open Database Connectivity</strong> (<strong>ODBC</strong>) driver is available for most database management systems, even for CSV and Excel files, so <code class="literal">RODBC</code> provides a standardized way to access data in almost any databases if the ODBC driver is installed. This<a class="indexterm" id="id109"/> platform-independent interface is available for SQLite, MySQL, MariaDB, PostgreSQL, Oracle <a class="indexterm" id="id110"/>database, Microsoft SQL Server, Microsoft Access, and IBM DB2 on Windows and on Linux.</p><p>For a quick example, let's connect to MySQL running on <code class="literal">localhost</code> (or on a virtual machine). First, we have to set up a <a class="indexterm" id="id111"/>
<strong>Database Source Name</strong> (<strong>DSN</strong>) with the connection details, such as:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Database driver</li><li class="listitem" style="list-style-type: disc">Host name or address and port number, optionally a Unix socket</li><li class="listitem" style="list-style-type: disc">Database name</li><li class="listitem" style="list-style-type: disc">Optionally the username and password to be used for the connection</li></ul></div><p>This can be done in the command line by editing the <code class="literal">odbc.ini</code> and <code class="literal">odbcinst.ini</code> files on Linux after installing the <a class="indexterm" id="id112"/>
<code class="literal">unixODBC</code> program. The latter should include the following configuration for the MySQL driver in your <code class="literal">/etc</code> folder:</p><div><pre class="programlisting"><strong>[MySQL]</strong>
<strong>Description     = ODBC Driver for MySQL</strong>
<strong>Driver          = /usr/lib/libmyodbc.so</strong>
<strong>Setup           = /usr/lib/libodbcmyS.so</strong>
<strong>FileUsage       = 1</strong>
</pre></div><p>The <code class="literal">odbc.ini</code> file includes the aforementioned DSN configuration for the exact database and server:</p><div><pre class="programlisting"><strong>[hflights]</strong>
<strong>Description     = MySQL hflights test</strong>
<strong>Driver          = MySQL</strong>
<strong>Server          = localhost</strong>
<strong>Database        = hflights_db</strong>
<strong>Port            = 3306</strong>
<strong>Socket          = /var/run/mysqld/mysqld.sock</strong>
</pre></div><p>Or use a graphical user interface on Mac OS or Windows, as shown in the following screenshot:</p><div><img alt="ODBC database access" src="img/2028OS_01_10.jpg"/></div><p>After configuring a<a class="indexterm" id="id113"/> DSN, we can connect with a one-line <a class="indexterm" id="id114"/>command:</p><div><pre class="programlisting"><strong>&gt; library(RODBC)</strong>
<strong>&gt; con &lt;- odbcConnect("hflights", uid = "user", pwd = "password")</strong>
</pre></div><p>Let's fetch the data we saved in the database before:</p><div><pre class="programlisting"><strong>&gt; system.time(hflights &lt;- sqlQuery(con, "select * from hflights"))</strong>
<strong>   user  system elapsed </strong>
<strong>  3.180   0.000   3.398</strong>
</pre></div><p>Well, it took a few seconds to finish. That's the trade-off for using a more convenient and high-level interface to interact with the database. Removing and uploading data to the database can be done with similar high-level functions (such as <code class="literal">sqlFetch</code>) besides the <code class="literal">odbc*</code> functions, providing low-level access to the database. Quick examples:</p><div><pre class="programlisting"><strong>&gt; sqlDrop(con, 'hflights')</strong>
<strong>&gt; sqlSave(con, hflights, 'hflights')</strong>
</pre></div><p>You can use the exact same commands to query any of the other supported database engines; just be sure to set up the DSN for each backend, and to close your connections if not needed any more:</p><div><pre class="programlisting"><strong>&gt; close(con)</strong>
</pre></div><p>The<a class="indexterm" id="id115"/> <code class="literal">RJDBC</code> package<a class="indexterm" id="id116"/> can provide a similar interface to database<a class="indexterm" id="id117"/> management systems <a class="indexterm" id="id118"/>with a <strong>Java Database Connectivity</strong> (<strong>JDBC</strong>) driver.</p></div><div><div><div><div><h2 class="title"><a id="ch01lvl2sec15"/>Using a graphical user interface to connect to databases</h2></div></div></div><p>Speaking <a class="indexterm" id="id119"/>of high-level interfaces, R also has a<a class="indexterm" id="id120"/> graphical user interface to connect to MySQL in<a class="indexterm" id="id121"/> the <code class="literal">dbConnect</code> package:</p><div><pre class="programlisting"><strong>&gt; library(dbConnect)</strong>
<strong>Loading required package: RMySQL</strong>
<strong>Loading required package: DBI</strong>
<strong>Loading required package: gWidgets</strong>
<strong>&gt; DatabaseConnect()</strong>
<strong>Loading required package: gWidgetsRGtk2</strong>
<strong>Loading required package: RGtk2</strong>
</pre></div><p>No parameters, no custom configuration in the console, just a simple dialog window:</p><div><img alt="Using a graphical user interface to connect to databases" src="img/2028OS_01_11.jpg"/></div><p>After providing the<a class="indexterm" id="id122"/> required connection information, we can easily view the raw data and the column/variable types, and run custom SQL queries. A basic query builder can also help novice users to<a class="indexterm" id="id123"/> fetch subsamples from the database:</p><div><img alt="Using a graphical user interface to connect to databases" src="img/2028OS_01_12.jpg"/></div><p>The package ships with a handy function called <code class="literal">sqlToR</code>, which can turn the SQL results into R objects with a<a class="indexterm" id="id124"/> click in the GUI. Unfortunately, <code class="literal">dbConnect</code> relies heavily on <code class="literal">RMySQL</code>, which means it's a MySQL-only package, and there is no plan to extend the functionality of this interface.</p></div><div><div><div><div><h2 class="title"><a id="ch01lvl2sec16"/>Other database backends</h2></div></div></div><p>Besides the previously mentioned popular databases, there are several other implementations<a class="indexterm" id="id125"/> that we cannot discuss here in detail.</p><p>For example, column-oriented database management systems, such as<a class="indexterm" id="id126"/> MonetDB, are often<a class="indexterm" id="id127"/> used to store large datasets with millions of rows and thousands of columns to provide the backend for high-performance data mining. It also has great R support with the <code class="literal">MonetDB.R</code> package, which<a class="indexterm" id="id128"/> was among the most exciting talks at the useR! 2013 conference.</p><p>The ever-growing popularity of the NoSQL ecosystem also provides similar approaches, although usually without supporting SQL and providing a schema-free data storage. Apache Cassandra is a<a class="indexterm" id="id129"/> good example of such a similar, column-oriented, and primarily distributed database management system with high availably and performance, run on commodity hardware. The <a class="indexterm" id="id130"/>
<code class="literal">RCassandra</code> package provides access to the basic Cassandra features and the Cassandra Query Language in a convenient way with the <code class="literal">RC.*</code> function family. Another Google Bigtable-inspired and similar database engine<a class="indexterm" id="id131"/> is HBase, which is supported by the <a class="indexterm" id="id132"/>
<code class="literal">rhbase</code> package, part of the<a class="indexterm" id="id133"/> <code class="literal">RHadoop</code> project: <a class="ulink" href="https://github.com/RevolutionAnalytics/RHadoop/wiki">https://github.com/RevolutionAnalytics/RHadoop/wiki</a>.</p><p>Speaking of Massively Parallel <a class="indexterm" id="id134"/>Processing, HP's Vertica and<a class="indexterm" id="id135"/> Cloudera's open-source<a class="indexterm" id="id136"/> Impala are also accessible from R, so you can easily access and query large amount of data with relatively good performance.</p><p>One of the most popular NoSQL databases is <a class="indexterm" id="id137"/>MongoDB, which provides document-oriented data storage in a JSON-like format, providing an infrastructure to dynamic schemas. MongoDB is actively developed and has some SQL-like features, such as a query language and indexing, also with multiple R packages providing access to this backend. The <a class="indexterm" id="id138"/>
<code class="literal">RMongo</code> package uses the <em>mongo-java-driver</em> and thus depends on Java, but provides a rather high-level interface to the database. Another implementation, the <code class="literal">rmongodb</code> package, is<a class="indexterm" id="id139"/> developed and maintained by the MongoDB Team. The latter has more frequent updates and more detailed documentation, but the R integration seems to be a lot more seamless with the first package as <code class="literal">rmongodb</code> provides access to the raw MongoDB functions and BSON objects, instead of concentrating on a translation layer for general R users. A more recent and really promising package supporting MongoDB<a class="indexterm" id="id140"/> is <code class="literal">mongolite</code> <a class="indexterm" id="id141"/>developed by Jeroen Ooms.</p><p>CouchDB, my personal<a class="indexterm" id="id142"/> favorite for most schema-less projects, provides very convenient document storage with JSON objects and HTTP API, which means that integrating in applications, such as any R script, is really easy with, for example, the <a class="indexterm" id="id143"/>
<code class="literal">RCurl</code> package, although you may find the<a class="indexterm" id="id144"/> <code class="literal">R4CouchDB</code> more quick to act in interacting with the database.</p><p>Google BigQuery<a class="indexterm" id="id145"/> also provides a similar, REST-based HTTP API to query even terabytes of data hosted in the Google infrastructure with an SQL-like language. Although the <a class="indexterm" id="id146"/>
<code class="literal">bigrquery</code> package is not available on CRAN yet, you may easily install it from GitHub with the<a class="indexterm" id="id147"/> <code class="literal">devtools</code> package from the same author, Hadley Wickham:</p><div><pre class="programlisting"><strong>&gt; library(devtools)</strong>
<strong>&gt; install_github('bigrquery', 'hadley')</strong>
</pre></div><p>To test-drive the features of this package and Google BigQuery, you can sign up for a free account to fetch and process the demo dataset provided by Google, respecting the 10,000 requests per day limitation for free usage. Please note that the current implementation is a read-only interface to the database.</p><p>For rather similar database engines <a class="indexterm" id="id148"/>and comparisons, see for example <a class="ulink" href="http://db-engines.com/en/systems">http://db-engines.com/en/systems</a>. Most of the popular databases already have R support but, if not, I am pretty<a class="indexterm" id="id149"/> sure that someone is already working on it. It's worth checking the CRAN packages at <a class="ulink" href="http://cran.r-project.org/web/packages/available_packages_by_name.html">http://cran.r-project.org/web/packages/available_packages_by_name.html</a> or searching on GitHub or on <a class="ulink" href="http://R-bloggers.com">http://R-bloggers.com</a> to<a class="indexterm" id="id150"/> see how other R users manage to interact with your database<a class="indexterm" id="id151"/> of choice.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch01lvl1sec12"/>Importing data from other statistical systems</h1></div></div></div><p>In a recent academic <a class="indexterm" id="id152"/>project, where my task was to implement some financial models in R, I got the demo dataset to be analyzed as Stata <code class="literal">dta</code> files. Working as a contractor at the university, without access to any Stata installations, it might have been problematic to read the binary file format of another statistical software, but as the <code class="literal">dta</code> file format is documented and the specification is publicly available<a class="indexterm" id="id153"/> at <a class="ulink" href="http://www.stata.com/help.cgi?dta">http://www.stata.com/help.cgi?dta</a>, some members of the Core R Team have already implemented an R parser in the form of the <code class="literal">read.dta</code> function in<a class="indexterm" id="id154"/> the <code class="literal">foreign</code> package.</p><p>To this end, loading (and often writing)<a class="indexterm" id="id155"/> Stata—or for example SPSS, SAS, Weka, Minitab, Octave, or dBase files—just cannot be easier in R. Please see the complete list of supported file formats and examples in the package <a class="indexterm" id="id156"/>documentation or in the <em>R Data Import/Export</em> manual: <a class="ulink" href="http://cran.r-project.org/doc/manuals/r-release/R-data.html#Importing-from-other-statistical-systems">http://cran.r-project.org/doc/manuals/r-release/R-data.html#Importing-from-other-statistical-systems</a>.</p></div>
<div><div><div><div><h1 class="title"><a id="ch01lvl1sec13"/>Loading Excel spreadsheets</h1></div></div></div><p>One of the <a class="indexterm" id="id157"/>most popular file formats to store and transfer relatively small amounts of data in academic institutions and businesses (besides CSV files) is still Excel <code class="literal">xls</code> (or <code class="literal">xlsx</code>, more recently). The first is a proprietary binary file format from Microsoft, which is exhaustively documented (the <code class="literal">xls</code> specification is available in a document of more than 1,100 pages and 50 megabytes!), but importing multiple sheets, macros, and formulas is not straightforward even nowadays. This section will only cover the most used platform-independent packages to interact with Excel.</p><p>One option is to use the previously discussed<a class="indexterm" id="id158"/> <code class="literal">RODBC</code> package with the Excel driver to query an Excel spreadsheet. Other ways of accessing Excel data depend on third-party tools, such as using Perl to automatically convert the Excel file to CSV then importing it into R as the <code class="literal">read.xls</code> function from the<a class="indexterm" id="id159"/> <code class="literal">gdata</code> package. But installing Perl on Windows sometimes seems to be tedious; thus, <code class="literal">RODBC</code> might be a more convenient method on that platform.</p><p>Some platform-independent, Java-based solutions also provide a way to not just read, but also write Excel files, especially to the <code class="literal">xlsx</code>, the Office Open XML file, format. Two separate implementations exist on CRAN to read and write Excel 2007 and the 97/2000/XP/2003 file formats: the<a class="indexterm" id="id160"/> <code class="literal">xlConnect</code> and the<a class="indexterm" id="id161"/> <code class="literal">xlsx</code> packages. Both are actively maintained, and use the Apache POI Java API project. This latter means that it runs on any platform that supports Java, and there is no need to have Microsoft Excel or Office on the computer; both packages can read and write Excel files on their own.</p><p>On the other hand, if you would rather not depend on Perl or Java, the recently published <code class="literal">openxlsx</code> package provides a platform-independent (C++-powered) way of reading and writing <code class="literal">xlsx</code> files. Hadley Wickham released a similar package, but with a slightly modified scope: the<a class="indexterm" id="id162"/> <code class="literal">readxl</code> package <a class="indexterm" id="id163"/>can read (but not write) both the <code class="literal">xls</code> and <code class="literal">xlsx</code> file formats.</p><p>Remember: pick the most appropriate tool for your needs! For example to read Excel files without many external dependencies, I'd choose <code class="literal">readxl</code>; but, for writing Excel 2003 spreadsheets with cell formatting and more advanced features, probably we cannot save the Java dependency and should<a class="indexterm" id="id164"/> use the <code class="literal">xlConnect</code> or<a class="indexterm" id="id165"/> <code class="literal">xlsx</code> packages over the <code class="literal">xlsx</code>-only <a class="indexterm" id="id166"/>
<code class="literal">openxlsx</code> package.</p></div>
<div><div><div><div><h1 class="title"><a id="ch01lvl1sec14"/>Summary</h1></div></div></div><p>This chapter focused on some rather boring, but important tasks that we usually do every day. Importing data is among the first steps of every data science projects, thus mastering data analysis should start with how to load data into the R session in an efficient way.</p><p>But efficiency is an ambiguous term in this sense: loading data should be quick in a technical point of view so as not to waste our time, although coding for long hours to speed up the importing process does not make much sense either.</p><p>The chapter gave a general overview on the most popular available options to read text files, to interact with databases, and to query subsets of data in R. Now you should be able to deal with all the most often used different data sources, and probably you can also choose which data source would be the ideal candidate in your projects and then do the benchmarks on your own, as we did previously.</p><p>The next chapter will extend this knowledge further by providing use cases for fetching data from the Web and different APIs. This simply means that you will be able to use public data in your projects, even if you do not yet have those in binary dataset files or on database backends.</p></div></body></html>