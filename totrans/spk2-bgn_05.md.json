["```py\n$ cd $SPARK_HOME\n$ ./bin/pyspark\n>>> # Import all the required libraries \n>>> from pyspark.sql import Row\n>>> import matplotlib.pyplot as plt\n>>> import numpy as np\n>>> import matplotlib.pyplot as plt\n>>> import pylab as P\n>>> plt.rcdefaults()\n>>> # TODO - The following location has to be changed to the appropriate data file location\n>>> dataDir = \"/Users/RajT/Documents/Writing/SparkForBeginners/SparkDataAnalysisWithPython/Data/ml-100k/\">>> # Create the DataFrame of the user dataset\n>>> lines = sc.textFile(dataDir + \"u.user\")\n>>> splitLines = lines.map(lambda l: l.split(\"|\"))\n>>> usersRDD = splitLines.map(lambda p: Row(id=p[0], age=int(p[1]), gender=p[2], occupation=p[3], zipcode=p[4]))\n>>> usersDF = spark.createDataFrame(usersRDD)\n>>> usersDF.createOrReplaceTempView(\"users\")\n>>> usersDF.show()\n      +---+------+---+-------------+-------+\n\n      |age|gender| id|   occupation|zipcode|\n\n      +---+------+---+-------------+-------+\n\n      | 24|     M|  1|   technician|  85711|\n\n      | 53|     F|  2|        other|  94043|\n\n      | 23|     M|  3|       writer|  32067|\n\n      | 24|     M|  4|   technician|  43537|\n\n      | 33|     F|  5|        other|  15213|\n\n      | 42|     M|  6|    executive|  98101|\n\n      | 57|     M|  7|administrator|  91344|\n\n      | 36|     M|  8|administrator|  05201|\n\n      | 29|     M|  9|      student|  01002|\n\n      | 53|     M| 10|       lawyer|  90703|\n\n      | 39|     F| 11|        other|  30329|\n\n      | 28|     F| 12|        other|  06405|\n\n      | 47|     M| 13|     educator|  29206|\n\n      | 45|     M| 14|    scientist|  55106|\n\n      | 49|     F| 15|     educator|  97301|\n\n      | 21|     M| 16|entertainment|  10309|\n\n      | 30|     M| 17|   programmer|  06355|\n\n      | 35|     F| 18|        other|  37212|\n\n      | 40|     M| 19|    librarian|  02138|\n\n      | 42|     F| 20|    homemaker|  95660|\n\n      +---+------+---+-------------+-------+\n\n      only showing top 20 rows\n    >>> # Create the DataFrame of the user dataset with only one column age\n\t>>> ageDF = spark.sql(\"SELECT age FROM users\")\n\t>>> ageList = ageDF.rdd.map(lambda p: p.age).collect()\n\t>>> ageDF.describe().show()\n      +-------+------------------+\n\n      |summary|               age|\n\n      +-------+------------------+\n\n      |  count|               943|\n\n      |   mean| 34.05196182396607|\n\n      | stddev|12.186273150937206|\n\n      |    min|                 7|\n\n      |    max|                73|\n\n      +-------+------------------+\n >>> # Age distribution of the users\n >>> plt.hist(ageList)\n >>> plt.title(\"Age distribution of the users\\n\")\n >>> plt.xlabel(\"Age\")\n >>> plt.ylabel(\"Number of users\")\n >>> plt.show(block=False)\n\n```", "```py\n>>> # Draw a density plot\n>>> from scipy.stats import gaussian_kde\n>>> density = gaussian_kde(ageList)\n>>> xAxisValues = np.linspace(0,100,1000)\n>>> density.covariance_factor = lambda : .5\n>>> density._compute_covariance()\n>>> plt.title(\"Age density plot of the users\\n\")\n>>> plt.xlabel(\"Age\")\n>>> plt.ylabel(\"Density\")\n>>> plt.plot(xAxisValues, density(xAxisValues))\n>>> plt.show(block=False)\n\n```", "```py\n>>> # The following example demonstrates the creation of multiple diagrams\n        in one figure\n\t\t>>> # There are two plots on one row\n\t\t>>> # The first one is the histogram of the distribution \n\t\t>>> # The second one is the boxplot containing the summary of the \n        distribution\n\t\t>>> plt.subplot(121)\n\t\t>>> plt.hist(ageList)\n\t\t>>> plt.title(\"Age distribution of the users\\n\")\n\t\t>>> plt.xlabel(\"Age\")\n\t\t>>> plt.ylabel(\"Number of users\")\n\t\t>>> plt.subplot(122)\n\t\t>>> plt.title(\"Summary of distribution\\n\")\n\t\t>>> plt.xlabel(\"Age\")\n\t\t>>> plt.boxplot(ageList, vert=False)\n\t\t>>> plt.show(block=False)\n\n```", "```py\n>>> occupationsTop10 = spark.sql(\"SELECT occupation, count(occupation) as usercount FROM users GROUP BY occupation ORDER BY usercount DESC LIMIT 10\")\n>>> occupationsTop10.show()\n      +-------------+---------+\n\n      |   occupation|usercount|\n\n      +-------------+---------+\n\n      |      student|      196|\n\n      |        other|      105|\n\n      |     educator|       95|\n\n      |administrator|       79|\n\n      |     engineer|       67|\n\n      |   programmer|       66|\n\n      |    librarian|       51|\n\n      |       writer|       45|\n\n      |    executive|       32|\n\n      |    scientist|       31|\n\n      +-------------+---------+\n\t  >>> occupationsTop10Tuple = occupationsTop10.rdd.map(lambda p:\n\t  (p.occupation,p.usercount)).collect()\n\t  >>> occupationsTop10List, countTop10List = zip(*occupationsTop10Tuple)\n\t  >>> occupationsTop10Tuple\n\t  >>> # Top 10 occupations in terms of the number of users having that\n\t  occupation who have rated movies\n\t  >>> y_pos = np.arange(len(occupationsTop10List))\n\t  >>> plt.barh(y_pos, countTop10List, align='center', alpha=0.4)\n\t  >>> plt.yticks(y_pos, occupationsTop10List)\n\t  >>> plt.xlabel('Number of users')\n\t  >>> plt.title('Top 10 user types\\n')\n\t  >>> plt.gcf().subplots_adjust(left=0.15)\n\t  >>> plt.show(block=False)\n\n```", "```py\n>>> occupationsGender = spark.sql(\"SELECT occupation, gender FROM users\")>>> occupationsGender.show()\n      +-------------+------+\n\n      |   occupation|gender|\n\n      +-------------+------+\n\n      |   technician|     M|\n\n      |        other|     F|\n\n      |       writer|     M|\n\n      |   technician|     M|\n\n      |        other|     F|\n\n      |    executive|     M|\n\n      |administrator|     M|\n\n      |administrator|     M|\n\n      |      student|     M|\n\n      |       lawyer|     M|\n\n      |        other|     F|\n\n      |        other|     F|\n\n      |     educator|     M|\n\n      |    scientist|     M|\n\n      |     educator|     F|\n\n      |entertainment|     M|\n\n      |   programmer|     M|\n\n      |        other|     F|\n\n      |    librarian|     M|\n\n      |    homemaker|     F|\n\n      +-------------+------+\n\n      only showing top 20 rows\n    >>> occCrossTab = occupationsGender.stat.crosstab(\"occupation\", \"gender\")>>> occCrossTab.show()\n      +-----------------+---+---+\n\n      |occupation_gender|  M|  F|\n\n      +-----------------+---+---+\n\n      |        scientist| 28|  3|\n\n      |          student|136| 60|\n\n      |           writer| 26| 19|\n\n      |         salesman|  9|  3|\n\n      |          retired| 13|  1|\n\n      |    administrator| 43| 36|\n\n      |       programmer| 60|  6|\n\n      |           doctor|  7|  0|\n\n      |        homemaker|  1|  6|\n\n      |        executive| 29|  3|\n\n      |         engineer| 65|  2|\n\n      |    entertainment| 16|  2|\n\n      |        marketing| 16| 10|\n\n      |       technician| 26|  1|\n\n      |           artist| 15| 13|\n\n      |        librarian| 22| 29|\n\n      |           lawyer| 10|  2|\n\n      |         educator| 69| 26|\n\n      |       healthcare|  5| 11|\n\n      |             none|  5|  4|\n\n      +-----------------+---+---+\n\n      only showing top 20 rows\n      >>> occupationsCrossTuple = occCrossTab.rdd.map(lambda p:\n\t (p.occupation_gender,p.M, p.F)).collect()\n\t >>> occList, mList, fList = zip(*occupationsCrossTuple)\n\t >>> N = len(occList)\n\t >>> ind = np.arange(N) # the x locations for the groups\n\t >>> width = 0.75 # the width of the bars\n\t >>> p1 = plt.bar(ind, mList, width, color='r')\n\t >>> p2 = plt.bar(ind, fList, width, color='y', bottom=mList)\n\t >>> plt.ylabel('Count')\n\t >>> plt.title('Gender distribution by occupation\\n')\n\t >>> plt.xticks(ind + width/2., occList, rotation=90)\n\t >>> plt.legend((p1[0], p2[0]), ('Male', 'Female'))\n\t >>> plt.gcf().subplots_adjust(bottom=0.25)\n\t >>> plt.show(block=False)\n\n```", "```py\n>>> occCrossTab.describe('M', 'F').show()\n      +-------+------------------+------------------+\n\n      |summary|                 M|                 F|\n\n      +-------+------------------+------------------+\n\n      |  count|                21|                21|\n\n      |   mean|31.904761904761905|              13.0|\n\n      | stddev|31.595516200735347|15.491933384829668|\n\n      |    min|                 1|                 0|\n\n      |    max|               136|                60|\n\n      +-------+------------------+------------------+\n    >>> occCrossTab.stat.cov('M', 'F')\n      381.15\n    >>> occCrossTab.stat.corr('M', 'F')\n      0.7416099517313641 \n\n```", "```py\n>>> occupationsBottom10 = spark.sql(\"SELECT occupation, count(occupation) as usercount FROM users GROUP BY occupation ORDER BY usercount LIMIT 10\")\n>>> occupationsBottom10.show()\n      +-------------+---------+\n\n      |   occupation|usercount|\n\n      +-------------+---------+\n\n      |    homemaker|        7|\n\n      |       doctor|        7|\n\n      |         none|        9|\n\n      |     salesman|       12|\n\n      |       lawyer|       12|\n\n      |      retired|       14|\n\n      |   healthcare|       16|\n\n      |entertainment|       18|\n\n      |    marketing|       26|\n\n      |   technician|       27|\n\n      +-------------+---------+\n    >>> occupationsBottom10Tuple = occupationsBottom10.rdd.map(lambda p: (p.occupation,p.usercount)).collect()\n\t>>> occupationsBottom10List, countBottom10List = zip(*occupationsBottom10Tuple)\n\t>>> # Bottom 10 occupations in terms of the number of users having that occupation who have rated movies\n\t>>> explode = (0, 0, 0, 0,0.1,0,0,0,0,0.1)\n\t>>> plt.pie(countBottom10List, explode=explode, labels=occupationsBottom10List, autopct='%1.1f%%', shadow=True, startangle=90)\n\t>>> plt.title('Bottom 10 user types\\n')\n\t>>> plt.show(block=False)\n\n```", "```py\n>>> zipTop10 = spark.sql(\"SELECT zipcode, count(zipcode) as usercount FROM users GROUP BY zipcode ORDER BY usercount DESC LIMIT 10\")\n>>> zipTop10.show()\n      +-------+---------+\n\n      |zipcode|usercount|\n\n      +-------+---------+\n\n      |  55414|        9|\n\n      |  55105|        6|\n\n      |  20009|        5|\n\n      |  55337|        5|\n\n      |  10003|        5|\n\n      |  55454|        4|\n\n      |  55408|        4|\n\n      |  27514|        4|\n\n      |  11217|        3|\n\n      |  14216|        3|\n\n      +-------+---------+\n    >>> zipTop10Tuple = zipTop10.rdd.map(lambda p: (p.zipcode,p.usercount)).collect()\n\t>>> zipTop10List, countTop10List = zip(*zipTop10Tuple)\n\t>>> # Top 10 zipcodes in terms of the number of users living in that zipcode who have rated movies>>> explode = (0.1, 0, 0, 0,0,0,0,0,0,0)  # explode a slice if required\n\t>>> plt.pie(countTop10List, explode=explode, labels=zipTop10List, autopct='%1.1f%%', shadow=True)\n\t>>> #Draw a circle at the center of pie to make it look like a donut\n\t>>> centre_circle = plt.Circle((0,0),0.75,color='black', fc='white',linewidth=1.25)\n\t>>> fig = plt.gcf()\n\t>>> fig.gca().add_artist(centre_circle)\n\t>>> # The aspect ratio is to be made equal. This is to make sure that pie chart is coming perfectly as a circle.\n\t>>> plt.axis('equal')\n\t>>> plt.text(- 0.25,0,'Top 10 zip codes')\n\t>>> plt.show(block=False)\n\n```", "```py\n>>> ages = spark.sql(\"SELECT occupation, age FROM users WHERE occupation ='administrator' ORDER BY age\")\n>>> adminAges = ages.rdd.map(lambda p: p.age).collect()\n>>> ages.describe().show()\n      +-------+------------------+\n\n      |summary|               age|\n\n      +-------+------------------+\n\n      |  count|                79|\n\n      |   mean| 38.74683544303797|\n\n      | stddev|11.052771408491363|\n\n      |    min|                21|\n\n      |    max|                70|\n\n      +-------+------------------+\n    >>> ages = spark.sql(\"SELECT occupation, age FROM users WHERE occupation ='engineer' ORDER BY age\")>>> engAges = ages.rdd.map(lambda p: p.age).collect()\n\t>>> ages.describe().show()\n      +-------+------------------+\n\n      |summary|               age|\n\n      +-------+------------------+\n\n      |  count|                67|\n\n      |   mean| 36.38805970149254|\n\n      | stddev|11.115345348003853|\n\n      |    min|                22|\n\n      |    max|                70|\n\n      +-------+------------------+\n    >>> ages = spark.sql(\"SELECT occupation, age FROM users WHERE occupation ='programmer' ORDER BY age\")>>> progAges = ages.rdd.map(lambda p: p.age).collect()\n\t>>> ages.describe().show()\n      +-------+------------------+\n\n      |summary|               age|\n\n      +-------+------------------+\n\n      |  count|                66|\n\n      |   mean|33.121212121212125|\n\n      | stddev| 9.551320948648684|\n\n      |    min|                20|\n\n      |    max|                63|\n\n      +-------+------------------+\n >>> # Box plots of the ages by profession\n >>> boxPlotAges = [adminAges, engAges, progAges]\n >>> boxPlotLabels = ['administrator','engineer', 'programmer' ]\n >>> x = np.arange(len(boxPlotLabels))\n >>> plt.figure()\n >>> plt.boxplot(boxPlotAges)\n >>> plt.title('Age summary statistics\\n')\n >>> plt.ylabel(\"Age\")\n >>> plt.xticks(x + 1, boxPlotLabels, rotation=0)\n >>> plt.show(block=False)\n\n```", "```py\n>>> movieLines = sc.textFile(dataDir + \"u.item\")\n>>> splitMovieLines = movieLines.map(lambda l: l.split(\"|\"))\n>>> moviesRDD = splitMovieLines.map(lambda p: Row(id=p[0], title=p[1], releaseDate=p[2], videoReleaseDate=p[3], url=p[4], unknown=int(p[5]),action=int(p[6]),adventure=int(p[7]),animation=int(p[8]),childrens=int(p[9]),comedy=int(p[10]),crime=int(p[11]),documentary=int(p[12]),drama=int(p[13]),fantasy=int(p[14]),filmNoir=int(p[15]),horror=int(p[16]),musical=int(p[17]),mystery=int(p[18]),romance=int(p[19]),sciFi=int(p[20]),thriller=int(p[21]),war=int(p[22]),western=int(p[23])))\n>>> moviesDF = spark.createDataFrame(moviesRDD)\n>>> moviesDF.createOrReplaceTempView(\"movies\")\n>>> genreDF = spark.sql(\"SELECT sum(unknown) as unknown, sum(action) as action,sum(adventure) as adventure,sum(animation) as animation, sum(childrens) as childrens,sum(comedy) as comedy,sum(crime) as crime,sum(documentary) as documentary,sum(drama) as drama,sum(fantasy) as fantasy,sum(filmNoir) as filmNoir,sum(horror) as horror,sum(musical) as musical,sum(mystery) as mystery,sum(romance) as romance,sum(sciFi) as sciFi,sum(thriller) as thriller,sum(war) as war,sum(western) as western FROM movies\")\n>>> genreList = genreDF.collect()\n>>> genreDict = genreList[0].asDict()\n>>> labelValues = list(genreDict.keys())\n>>> countList = list(genreDict.values())\n>>> genreDict\n      {'animation': 42, 'adventure': 135, 'romance': 247, 'unknown': 2, 'musical': 56, 'western': 27, 'comedy': 505, 'drama': 725, 'war': 71, 'horror': 92, 'mystery': 61, 'fantasy': 22, 'childrens': 122, 'sciFi': 101, 'filmNoir': 24, 'action': 251, 'documentary': 50, 'crime': 109, 'thriller': 251}\n    >>> # Movie types and the counts\n\t>>> x = np.arange(len(labelValues))\n\t>>> plt.title('Movie types\\n')\n\t>>> plt.ylabel(\"Count\")\n\t>>> plt.bar(x, countList)\n\t>>> plt.xticks(x + 0.5, labelValues, rotation=90)\n\t>>> plt.gcf().subplots_adjust(bottom=0.20)\n\t>>> plt.show(block=False)\n\n```", "```py\n>>> yearDF = spark.sql(\"SELECT substring(releaseDate,8,4) as releaseYear, count(*) as movieCount FROM movies GROUP BY substring(releaseDate,8,4) ORDER BY movieCount DESC LIMIT 10\")\n>>> yearDF.show()\n      +-----------+----------+\n\n      |releaseYear|movieCount|\n\n      +-----------+----------+\n\n      |       1996|       355|\n\n      |       1997|       286|\n\n      |       1995|       219|\n\n      |       1994|       214|\n\n      |       1993|       126|\n\n      |       1998|        65|\n\n      |       1992|        37|\n\n      |       1990|        24|\n\n      |       1991|        22|\n\n      |       1986|        15|\n\n      +-----------+----------+\n    >>> yearMovieCountTuple = yearDF.rdd.map(lambda p: (int(p.releaseYear),p.movieCount)).collect()\n\t>>> yearList,movieCountList = zip(*yearMovieCountTuple)\n\t>>> countArea = yearDF.rdd.map(lambda p: np.pi * (p.movieCount/15)**2).collect()\n\t>>> plt.title('Top 10 movie release by year\\n')\n\t>>> plt.xlabel(\"Year\")\n\t>>> plt.ylabel(\"Number of movies released\")\n\t>>> plt.ylim([0,max(movieCountList) + 20])\n\t>>> colors = np.random.rand(10)\n\t>>> plt.scatter(yearList, movieCountList,c=colors)\n\t>>> plt.show(block=False)\n\n```", "```py\n>>> # Top 10 years where the most number of movies have been released\n>>> plt.title('Top 10 movie release by year\\n')\n>>> plt.xlabel(\"Year\")\n>>> plt.ylabel(\"Number of movies released\")\n>>> plt.ylim([0,max(movieCountList) + 100])\n>>> colors = np.random.rand(10)\n>>> plt.scatter(yearList, movieCountList,c=colors, s=countArea)\n>>> plt.show(block=False)\n\n```", "```py\n>>> yearActionDF = spark.sql(\"SELECT substring(releaseDate,8,4) as actionReleaseYear, count(*) as actionMovieCount FROM movies WHERE action = 1 GROUP BY substring(releaseDate,8,4) ORDER BY actionReleaseYear DESC LIMIT 10\")\n>>> yearActionDF.show()\n      +-----------------+----------------+\n\n      |actionReleaseYear|actionMovieCount|\n\n      +-----------------+----------------+\n\n      |             1998|              12|\n\n      |             1997|              46|\n\n      |             1996|              44|\n\n      |             1995|              40|\n\n      |             1994|              30|\n\n      |             1993|              20|\n\n      |             1992|               8|\n\n      |             1991|               2|\n\n      |             1990|               7|\n\n      |             1989|               6|\n\n      +-----------------+----------------+\n    >>> yearActionDF.createOrReplaceTempView(\"action\")\n\t>>> yearDramaDF = spark.sql(\"SELECT substring(releaseDate,8,4) as dramaReleaseYear, count(*) as dramaMovieCount FROM movies WHERE drama = 1 GROUP BY substring(releaseDate,8,4) ORDER BY dramaReleaseYear DESC LIMIT 10\")\n\t>>> yearDramaDF.show()\n      +----------------+---------------+\n\n      |dramaReleaseYear|dramaMovieCount|\n\n      +----------------+---------------+\n\n      |            1998|             33|\n\n      |            1997|            113|\n\n      |            1996|            170|\n\n      |            1995|             89|\n\n      |            1994|             97|\n\n      |            1993|             64|\n\n      |            1992|             14|\n\n      |            1991|             11|\n\n      |            1990|             12|\n\n      |            1989|              8|\n\n      +----------------+---------------+\n    >>> yearDramaDF.createOrReplaceTempView(\"drama\")\n\t>>> yearCombinedDF = spark.sql(\"SELECT a.actionReleaseYear as releaseYear, a.actionMovieCount, d.dramaMovieCount FROM action a, drama d WHERE a.actionReleaseYear = d.dramaReleaseYear ORDER BY a.actionReleaseYear DESC LIMIT 10\")\n\t>>> yearCombinedDF.show()\n      +-----------+----------------+---------------+\n\n      |releaseYear|actionMovieCount|dramaMovieCount|\n\n      +-----------+----------------+---------------+\n\n      |       1998|              12|             33|\n\n      |       1997|              46|            113|\n\n      |       1996|              44|            170|\n\n      |       1995|              40|             89|\n\n      |       1994|              30|             97|\n\n      |       1993|              20|             64|\n\n      |       1992|               8|             14|\n\n      |       1991|               2|             11|\n\n      |       1990|               7|             12|\n\n      |       1989|               6|              8|\n\n      +-----------+----------------+---------------+\n   >>> yearMovieCountTuple = yearCombinedDF.rdd.map(lambda p: (p.releaseYear,p.actionMovieCount, p.dramaMovieCount)).collect()\n   >>> yearList,actionMovieCountList,dramaMovieCountList = zip(*yearMovieCountTuple)\n   >>> plt.title(\"Movie release by year\\n\")\n   >>> plt.xlabel(\"Year\")\n   >>> plt.ylabel(\"Movie count\")\n   >>> line_action, = plt.plot(yearList, actionMovieCountList)\n   >>> line_drama, = plt.plot(yearList, dramaMovieCountList)\n   >>> plt.legend([line_action, line_drama], ['Action Movies', 'Drama Movies'],loc='upper left')\n   >>> plt.gca().get_xaxis().get_major_formatter().set_useOffset(False)\n   >>> plt.show(block=False)\n\n```"]