- en: Chapter 8. TensorFrames
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will provide a high-level primer on the burgeoning field of Deep
    Learning and the reasons why it is important. It will provide the fundamentals
    surrounding feature learning and neural networks required for deep learning. As
    well, this chapter will provide a quick start for TensorFrames for Apache Spark.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn about:'
  prefs: []
  type: TYPE_NORMAL
- en: What is Deep Learning?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A primer on feature learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is feature engineering?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is TensorFlow?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing TensorFrames
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFrames – quick start
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see in the preceding breakdown, we will be initially discussing deep
    learning – more specifically we will start with neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: What is Deep Learning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep Learning is part of a family of machine learning techniques based on *learning*
    representations of data. Deep Learning is loosely based on our brain's own neural
    networks, the purpose of this structure is to provide a large number of highly
    interconnected elements (in biological systems, this would be the neurons in our
    brains); there are approximately 100 billion neurons in our brain, each connected
    to approximately 10,000 other neurons, resulting in a mind-boggling 1015 synaptic
    connections. These elements work together to solve problems through learning processes
    – examples include pattern recognition and data classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'Learning within this architecture involves modifications of the connections
    between the interconnected elements similar to how our own brains make adjustments
    to the synaptic connections between neurons:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What is Deep Learning?](img/B05793_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: *Wikimedia Commons: File: Réseau de neurones.jpg*; [https://commons.wikimedia.org/wiki/File:Réseau_de_neurones.jpg](https://commons.wikimedia.org/wiki/File:R%C3%A9seau_de_neurones.jpg).'
  prefs: []
  type: TYPE_NORMAL
- en: The traditional algorithmic approach involves programming known steps or quantities,
    that is, you already know the steps to solve a specific problem, now repeat the
    solution and make it run faster. Neural networks are an interesting paradigm because
    neural networks learn by example and are not actually programmed to perform a
    specific task per se. This makes the training process in neural networks (and
    Deep Learning) very important in that you must provide good examples for the neural
    network to learn from otherwise it will "learn" the wrong thing (that is, provide
    unpredictable results).
  prefs: []
  type: TYPE_NORMAL
- en: 'The most common approach to building an artificial neural network involves
    the creation of three layers: input, hidden, and outer; as noted in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What is Deep Learning?](img/B05793_08_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Each layer is comprised of one or more nodes with connections (that is, flow
    of data) between each of these nodes, as noted in the preceding diagram. Input
    nodes are passive in that they receive data, but do not modify the information.
    The nodes in the hidden and output layers will actively modify the data. For example,
    the connections from the three nodes in the input layer to one of the nodes in
    the first hidden layer is noted in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What is Deep Learning?](img/B05793_08_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Referring to a signal processing neural network example, each input (denoted
    as ![What is Deep Learning?](img/B05793_08_22.jpg)) has a weight applied to it
    (![What is Deep Learning?](img/B05793_08_23.jpg)), which produces a new value.
    In this case, one of the hidden nodes (![What is Deep Learning?](img/B05793_08_24.jpg))
    is the result of three modified input nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What is Deep Learning?](img/B05793_08_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'There is also a bias applied to the sum in a form of a constant that also gets
    adjusted during the training process. The sum (the *h* *1* in our example) passes
    through so-called activation function that determines the output of the neuron.
    Some examples of such activation functions are presented in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What is Deep Learning?](img/B05793_08_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This process is repeated for each node in the hidden layers as well as the output
    layer. The output node is the accumulation of all the weights applied to the input
    values for every active layer node. The learning process is the result of many
    iterations running in parallel, applying and reapplying these weights (in this
    scenario).
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks appear in all the different sizes and shapes. The most popular
    are single- and multi-layer feedforward networks that resemble the one presented
    earlier; such structures (even with two layers and one neuron!) neuron in the
    output layer are capable of solving simple regression problems (such as linear
    and logistic) to highly complex regression and classification tasks (with many
    hidden layers and a number of neurons). Another type commonly used are self-organizing
    maps, sometimes referred to as Kohonen networks, due to Teuvo Kohonen, a Finnish
    researcher who first proposed such structures. The structures are trained *without-a-teacher*,
    that is, they do not require a target (an unsupervised learning paradigm). Such
    structures are used most commonly to solve clustering problems where the aim is
    to find an underlying pattern in the data.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For more information about neural network types, we suggest checking this document:
    [http://www.ieee.cz/knihovna/Zhang/Zhang100-ch03.pdf](http://www.ieee.cz/knihovna/Zhang/Zhang100-ch03.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Note that there are many other interesting deep learning libraries in addition
    to TensorFlow; including, but not limited, to Theano, Torch, Caffe, Microsoft
    Cognitive Toolkit (CNTK), mxnet, and DL4J.
  prefs: []
  type: TYPE_NORMAL
- en: The need for neural networks and Deep Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are many potential applications with neural networks (and Deep Learning).
    Some of the more popular ones include facial recognition, handwritten digit identification,
    game playing, speech recognition, language translation, and object classification.
    The key aspect here is that it involves learning and pattern recognition.
  prefs: []
  type: TYPE_NORMAL
- en: 'While neural networks have been around for a long time (at least within the
    context of the history of computer science), they have become more popular now
    because of the overarching themes: advances and availability of distributed computing
    and advances in research:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Advances and availability of distributed computing and hardware**: Distributed
    computing frameworks such as Apache Spark allows you to complete more training
    iterations faster by being able to run more models in parallel to determine the
    optimal parameters for your machine learning models. With the prevalence of GPUs
    – graphic processing units that were originally designed for displaying graphics
    – these processors are adept at performing the resource intensive mathematical
    computations required for machine learning. Together with cloud computing, it
    becomes easier to harness the power of distributed computing and GPUs due to the
    lower up-front costs, minimal time to deployment, and easier to deploy elastic
    scale.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Advances in deep learning research**: These hardware advances have helped
    return neural networks to the forefront of data sciences with projects such as
    TensorFlow as well as other popular ones such as Theano, Caffe, Torch, Microsoft
    Cognitive Toolkit (CNTK), mxnet, and DL4J.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To dive deeper into these topics, two good references include:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Lessons Learned from Deploying Deep Learning at Scale* ([http://blog.algorithmia.com/deploying-deep-learning-cloud-services/](http://blog.algorithmia.com/deploying-deep-learning-cloud-services/)):
    This blog post by the folks at Algorithmia discuss their learnings on deploying
    deep learning solutions at scale.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Neural Networks by Christos Stergio and Dimitrios Siganos* ([http://bit.ly/2hNSWar](http://bit.ly/2hNSWar)):
    A great primer on neural networks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As noted previously, Deep Learning is part of a family of machine learning
    methods based on learning representations of data. In the case of learning representations,
    this can also be defined as *feature learning*. What makes Deep Learning so exciting
    is that it has the potential to replace or minimize the need for *manual* feature
    engineering. Deep Learning will allow the machine to not just learn a specific
    task, but also learn the *features* needed for that task. More succinctly, automating
    feature engineering or teaching machines *to learn how to learn* (a great reference
    on feature learning is Stanford''s Unsupervised Feature Learning and Deep Learning
    tutorial: [http://deeplearning.stanford.edu/tutorial/](http://deeplearning.stanford.edu/tutorial/)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Breaking these concepts down to the fundamentals, let''s start with a *feature*.
    As observed in Christopher Bishop''s *Pattern Recognition and machine learning*
    (Berlin: Springer. ISBN 0-387-31073-8\. 2006) and as noted in the previous chapters
    on MLlib and ML, a feature is a measurable property of the phenomenon being observed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are more familiar in the domain of statistics, a *feature* would be
    in reference to the independent variables (*x[1], x[2], …, x[n]*) within a stochastic
    linear regression model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The need for neural networks and Deep Learning](img/B05793_08_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this specific example, *y* is the dependent variable and *x* *i* are the
    independent variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Within the context of machine learning scenarios, examples of features include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Restaurant recommendations**: Features include the reviews, ratings, and
    other content and user profile attributes related to the restaurant. A good example
    of this model is the *Yelp Food Recommendation System*: [http://cs229.stanford.edu/proj2013/SawantPai-YelpFoodRecommendationSystem.pdf](http://cs229.stanford.edu/proj2013/SawantPai-YelpFoodRecommendationSystem.pdf)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Handwritten Digit recognition**: Features include block wise histograms (count
    of pixels along 2D directions), holes, stroke detection, and so on. Examples include:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Handwritten Digit Classification*: [http://ttic.uchicago.edu/~smaji/projects/digits/](http://ttic.uchicago.edu/~smaji/projects/digits/)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Recognizing Handwritten Digits and Characters*: [http://cs231n.stanford.edu/reports/vishnu_final.pdf](http://cs231n.stanford.edu/reports/vishnu_final.pdf)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image Processing**: Features include the points, edges, and objects within
    the image; some good examples include:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Seminar: Feature extraction by André Aichert, [http://home.in.tum.de/~aichert/featurepres.pdf](http://home.in.tum.de/~aichert/featurepres.pdf)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'University of Washington Computer Science & Engineering CSE455: Computer Vision
    Lecture 6, [https://courses.cs.washington.edu/courses/cse455/09wi/Lects/lect6.pdf](https://courses.cs.washington.edu/courses/cse455/09wi/Lects/lect6.pdf)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature engineering is about determining which of these features (for example,
    in statistics, the independent variables) are important in defining the model
    that you are creating. Typically, it involves the process of using domain knowledge
    to create the features to allow the ML models to work.
  prefs: []
  type: TYPE_NORMAL
- en: Coming up with features is difficult, time-consuming, requires expert knowledge.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '"Applied machine learning" is basically feature engineering.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: —Andrew Ng, Machine Learning and AI via Brain simulations ([http://helper.ipam.ucla.edu/publications/gss2012/gss2012_10595.pdf](http://helper.ipam.ucla.edu/publications/gss2012/gss2012_10595.pdf))
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What is feature engineering?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Typically, performing feature engineering involves concepts such as feature
    selection (selecting a subset of the original feature set) or feature extraction
    (building a new set of features from the original feature set):'
  prefs: []
  type: TYPE_NORMAL
- en: In *feature selection*, based on domain knowledge, you can filter the variables
    that you think define the model (for example, predicting football scores based
    on number of turnovers). Often data analysis techniques such as regression and
    classification can also be used to help you determine this.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In *feature extraction*, the idea is to transform the data from a high dimensional
    space (that is, many different independent variables) to a smaller space of fewer
    dimensions. Continuing the football analogy, this would be the quarterback rating,
    which is based on several selected features (e.g. completions, touchdowns, interceptions,
    average gain per pass attempt, etc.). A common approach for feature extraction
    within the linear data transformation space is **principal component analysis**
    (**PCA**): [http://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html#principal-component-analysis-pca](http://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html#principal-component-analysis-pca).
    Other common mechanisms include:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Nonlinear dimensionality reduction*: [https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction](https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Multilinear subspace learning*: [https://en.wikipedia.org/wiki/Multilinear_subspace_learning](https://en.wikipedia.org/wiki/Multilinear_subspace_learning)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: A good reference on the topic of feature selection versus feature extraction
    is *What is dimensionality reduction?* *What is the difference between feature
    selection and extraction?* [http://datascience.stackexchange.com/questions/130/what-is-dimensionality-reduction-what-is-the-difference-between-feature-selecti/132#132](http://datascience.stackexchange.com/questions/130/what-is-dimensionality-reduction-what-is-the-difference-between-feature-selecti/132#132)
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Bridging the data and algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s bridge the feature and feature engineering definitions within the context
    of feature selection using the example of restaurant recommendations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bridging the data and algorithm](img/B05793_08_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: While this is a simplified model, the analogy describes the basic premise of
    applied machine learning. It would be up to a data scientist to analyze the data
    to determine the key features of this restaurant recommendation model.
  prefs: []
  type: TYPE_NORMAL
- en: In our restaurant recommendation case, while it may be easy to presume that
    geolocation and cuisine type are major factors, it will require some digging into
    the data to understand how the user (that is, restaurant-goer) has chosen their
    preference for a restaurant. Different restaurants often have different characteristics
    or weights for the mode.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the key features for high-end restaurant catering businesses are
    often related to location (that is, proximity to their customer''s location),
    the ability to make reservations for large parties, and the diversity of the wine
    list:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bridging the data and algorithm](img/B05793_08_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Meanwhile, for specialty restaurants, often few of those previous factors are
    involved; instead, the focus is on the reviews, ratings, social media buzz, and,
    possibly whether the restaurant is good for kids:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bridging the data and algorithm](img/B05793_08_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The ability to segment these different restaurants (and their target audience)
    is a key facet of applied machine learning. It can be an arduous process where
    you try different models and algorithms with different variables and weights and
    then retry after iteratively training and testing many different combinations.
    But note how this time consuming iterative approach itself is its own process
    that can potentially be automated? This is the key context of building algorithms
    of helping machines *learn to learn*: Deep Learning has the potential to automating
    the learning process when building our models.'
  prefs: []
  type: TYPE_NORMAL
- en: What is TensorFlow?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow is a Google open source software library for numerical computation
    using data flow graphs; that is, an open source machine learning library focusing
    on Deep Learning. Based loosely on neural networks, TensorFlow is the culmination
    of the work of Google's Brain Team researchers and engineers to apply Deep Learning
    to Google products and build production models for various Google teams including
    (but not limited to) search, photos, and speech.
  prefs: []
  type: TYPE_NORMAL
- en: 'Built on C++ with a Python interface, it has quickly become one of the most
    popular Deep Learning projects in a short amount of time. The following screenshot
    denotes a Google Trends comparison between four popular deep learning libraries;
    note the spike around November 8th - 14th, 2015 (when TensorFlow was announced)
    and the rapid rise over the last year (this snapshot was taken late December 2016):'
  prefs: []
  type: TYPE_NORMAL
- en: '![What is TensorFlow?](img/B05793_08_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Another way to measure the popularity of TensorFlow is to note that TensorFlow
    is the most popular machine learning framework on GitHub per [http://www.theverge.com/2016/4/13/11420144/google-machine-learning-tensorflow-upgrade](http://www.theverge.com/2016/4/13/11420144/google-machine-learning-tensorflow-upgrade).
    Note that TensorFlow was only released in November 2015 and in two months it had
    already become the most popular forked ML GitHub repository. In the following
    diagram, you can review the GitHub Repositories Created in 2015 (Interactive Visualization)
    per [http://donnemartin.com/viz/pages/2015](http://donnemartin.com/viz/pages/2015):'
  prefs: []
  type: TYPE_NORMAL
- en: '![What is TensorFlow?](img/B05793_08_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As noted previously, TensorFlow performs numerical computation using data flow
    graphs. When thinking about graph (as per the previous chapter on GraphFrames),
    the node (or vertices) of this graph represent mathematical operations while the
    graph edges represent the multidimensional arrays (that is, tensors) that communicate
    between the different nodes (that is, mathematical operations).
  prefs: []
  type: TYPE_NORMAL
- en: 'Referring to the following diagram, `t` `1` is a **2x3** matrix while `t` `2`
    is a **3x2** matrix; these are the tensors (or edges of the tensor graph). The
    node is the mathematical operations represented as `op` `1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What is TensorFlow?](img/B05793_08_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In this example, `op` `1` is a matrix multiplication operation represented
    by the following diagram, though this could be any of the many mathematics operations
    available in TensorFlow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What is TensorFlow?](img/B05793_08_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Together, to perform your numerical computations within the graph, there is
    a flow of multidimensional arrays (that is, tensors) between the mathematical
    operations (nodes) - that is, the flow of tensors, or *TensorFlow*.
  prefs: []
  type: TYPE_NORMAL
- en: 'To better understand how TensorFlow works, let''s start by installing TensorFlow
    within your Python environment (initially sans Spark). For the full instructions,
    please refer to TensorFlow | Download and Setup: [https://www.tensorflow.org/versions/r0.12/get_started/os_setup.html](https://www.tensorflow.org/versions/r0.12/get_started/os_setup.html).'
  prefs: []
  type: TYPE_NORMAL
- en: For this chapter, let's focus on the Python `pip` package management system
    installation on Linux or Mac OS.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Pip
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Ensure that you have installed `pip`; if have not, please use the following
    commands to install the Python package installation manager for Ubuntu/Linux:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'For Mac OS, you would use the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Note, for Ubuntu/Linux, you may also want to upgrade `pip` as the pip within
    the Ubuntu repository is old and may not be compatible with newer packages. To
    do this, you can run the command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Installing TensorFlow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To install TensorFlow (with `pip` already installed), you only need to execute
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'If you have a computer that has GPU support, you can *instead* use the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Note that if the preceding command does not work, there are specific instructions
    to install TensorFlow with GPU support based on your Python version (that is,
    2.7, 3.4, or 3.5) and GPU support.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if I wanted to install TensorFlow on Python 2.7 with GPU enabled
    on Mac OS, execute the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Please refer to [https://www.tensorflow.org/versions/r0.12/get_started/os_setup.html](https://www.tensorflow.org/versions/r0.12/get_started/os_setup.html)
    for the latest installation instructions.
  prefs: []
  type: TYPE_NORMAL
- en: Matrix multiplication using constants
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To better describe tensors and how TensorFlow works, let''s start with a matrix
    multiplication calculation involving two constants. As noted in the following
    diagram, we have `c` `1` (**3x1** matrix) and `c` `2` (**1x3** matrix), where
    the operation (`op` `1`) is a matrix multiplication:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Matrix multiplication using constants](img/B05793_08_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We will now define `c` `1` (**1x3** matrix) and `c` `2` (**3x1** matrix) using
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have our constants, let''s run our matrix multiplication using
    the following code. Within the context of a TensorFlow graph, recall that the
    nodes in the graph are called operations (or `ops`). The following matrix multiplication
    is the `ops`, while the two matrices (`c` `1`, `c` `2`) are the tensors (typed
    multi-dimensional array). An `op` takes zero or more tensors as its input, performs
    the operation such as a mathematical calculation, with the output being zero or
    more tensors in the form of `numpy ndarray` objects ([http://www.numpy.org/](http://www.numpy.org/))
    or `tensorflow::Tensor interfaces` in C, C++:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that this TensorFlow graph has been established, execution of this operation
    (for example, in this case, the matrix multiplication) is done within the context
    of a `session`; the `session` places the graph `ops` into the CPU or GPU (that
    is, devices) to be executed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'With the output being:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you have completed your operations, you can close the session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Matrix multiplication using placeholders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now we will perform the same task as before, except this time, we will use
    tensors instead of constants. As noted in the following diagram, we will start
    off with two matrices (`m1: 3x1, m2: 1x3`) using the same values as in the previous
    section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Matrix multiplication using placeholders](img/B05793_08_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Within TensorFlow, we will use `placeholder` to define our two tensors as per
    the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The advantage of this approach is that, with placeholders you can use the same
    operations (that is, in this case, the matrix multiplication) with tensors of
    different sizes and shape (provided they meet the criteria of the operation).
    Like the operations in the previous section, let's define two matrices and execute
    the graph (with a simplified session execution).
  prefs: []
  type: TYPE_NORMAL
- en: Running the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following code snippet is similar to the code snippet in the previous section,
    except that it now uses placeholders instead of constants:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'With the output being both the value, as well as the data type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Running another model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that we have a graph (albeit a simple one) using `placeholders`, we can
    use different tensors to perform the same operation using different input matrices.
    As noted in the following figure, we have `m1` (4x1) and `m2` (1x4):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Running another model](img/B05793_08_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Because we''re using `placeholders`, we can easily reuse the same graph within
    a new session using new input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'With the output being:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As noted previously, TensorFlow provides users with the ability to perform deep
    learning using Python libraries by representing computations as graphs where the
    tensors represent the data (edges of the graph) and operations represent what
    is to be executed (for example, mathematical computations) (vertices of the graph).
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information, please refer to:'
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow | Get Started | Basic Usage [https://www.tensorflow.org/get_started/get_started#basic_usage](https://www.tensorflow.org/get_started/get_started#basic_usage)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shannon McCormick's Neural Network and Google TensorFlow [http://www.slideshare.net/ShannonMcCormick4/neural-networks-and-google-tensor-flow](http://www.slideshare.net/ShannonMcCormick4/neural-networks-and-google-tensor-flow)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing TensorFrames
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At the time of writing, TensorFrames is an experimental binding for Apache
    Spark; it was introduced in early 2016, shortly after the release of TensorFlow.
    With TensorFrames, one can manipulate Spark DataFrames with TensorFlow programs.
    Referring to the tensor diagrams in the previous section, we have updated the
    figure to show how Spark DataFrames work with TensorFlow, as shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Introducing TensorFrames](img/B05793_08_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As noted in the preceding diagram, TensorFrames provides a bridge between Spark
    DataFrames and TensorFlow. This allows you to take your DataFrames and apply them
    as input into your TensorFlow computation graph. TensorFrames also allows you
    to take the TensorFlow computation graph output and push it back into DataFrames
    so you can continue your downstream Spark processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of common usage scenarios for TensorFrames, these typically include
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Utilize TensorFlow with your data**'
  prefs: []
  type: TYPE_NORMAL
- en: The integration of TensorFlow and Apache Spark with TensorFrames allows data
    scientists to expand their analytics, streaming, graph, and machine learning capabilities
    to include Deep Learning via TensorFlow. This allows you to both train and deploy
    models at scale.
  prefs: []
  type: TYPE_NORMAL
- en: '**Parallel training to determine optimal hyperparameters**'
  prefs: []
  type: TYPE_NORMAL
- en: When building deep learning models, there are several configuration parameters
    (that is, hyperparameters) that impact on how the model is trained. Common in
    Deep Learning/artificial neural networks are hyperparameters that define the learning
    rate (if the rate is high it will learn quickly, but it may not take into account
    highly variable input - that is, it will not learn well if the rate and variability
    in the data is too high) and the number of neurons in each layer of your neural
    network (too many neurons results in noisy estimates, while too few neurons will
    result in the network not learning well).
  prefs: []
  type: TYPE_NORMAL
- en: As observed in *Deep Learning with Apache Spark and TensorFlow* ([https://databricks.com/blog/2016/01/25/deep-learning-with-apache-spark-and-tensorflow.html](https://databricks.com/blog/2016/01/25/deep-learning-with-apache-spark-and-tensorflow.html)),
    using Spark with TensorFlow to help find the best set of hyperparameters for neural
    network training resulted in an order of magnitude reduction in training time
    and a 34% lower error rate for the handwritten digit recognition dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information on Deep Learning and hyperparameters, please refer to:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Optimizing Deep Learning Hyper-Parameters Through an Evolutionary Algorithm*
    [http://ornlcda.github.io/MLHPC2015/presentations/4-Steven.pdf](http://ornlcda.github.io/MLHPC2015/presentations/4-Steven.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*CS231n Convolutional Network Networks for Visual Recognition* [http://cs231n.github.io/](http://cs231n.github.io/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Deep Learning with Apache Spark and TensorFlow* [https://databricks.com/blog/2016/01/25/deep-learning-with-apache-spark-and-tensorflow.html](https://databricks.com/blog/2016/01/25/deep-learning-with-apache-spark-and-tensorflow.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the time of writing, TensorFrames is officially supported as of Apache Spark
    1.6 (Scala 2.10), though most contributions are currently focused on Spark 2.0
    (Scala 2.11). The easiest way to use TensorFrames is to access it via Spark Packages
    ([https://spark-packages.org](https://spark-packages.org)).
  prefs: []
  type: TYPE_NORMAL
- en: TensorFrames – quick start
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After all this preamble, let's jump start our use of TensorFrames with this
    quick start tutorial. You can download and use the full notebook within Databricks
    Community Edition at [http://bit.ly/2hwGyuC](http://bit.ly/2hwGyuC).
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also run this from the PySpark shell (or other Spark environments),
    like any other Spark package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Note, you will only use one of the above commands (that is, not both). For more
    information, please refer to the `databricks/tensorframes` GitHub repository ([https://github.com/databricks/tensorframes](https://github.com/databricks/tensorframes)).
  prefs: []
  type: TYPE_NORMAL
- en: Configuration and setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Please follow the configuration and setup steps in the following order:'
  prefs: []
  type: TYPE_NORMAL
- en: Launching a Spark cluster
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Launch a Spark cluster using Spark 1.6 (Hadoop 1) and Scala 2.10\. This has
    been tested with Spark 1.6, Spark 1.6.2, and Spark 1.6.3 (Hadoop 1) on Databricks
    Community Edition ([http://databricks.com/try-databricks](http://databricks.com/try-databricks)).
  prefs: []
  type: TYPE_NORMAL
- en: Creating a TensorFrames library
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Create a library to attach TensorFrames 0.2.2 to your cluster: `tensorframes-0.2.2-s_2.10`.
    Refer to [Chapter 7](ch07.html "Chapter 7. GraphFrames"), *GraphFrames* to recall
    how to create a library.'
  prefs: []
  type: TYPE_NORMAL
- en: Installing TensorFlow on your cluster
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In a notebook, run one of the following commands to install TensorFlow. This
    has been tested with TensorFlow 0.9 CPU edition:'
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow 0.9, Ubuntu/Linux 64-bit, CPU only, Python 2.7:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'TensorFlow 0.9, Ubuntu/Linux 64-bit, GPU enabled, Python 2.7:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following is the `pip` install command that will install TensorFlow on
    to the Apache Spark driver:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'A successful installation should have something similar to the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Upon successful installation of TensorFlow, detach and reattach the notebook
    where you just ran this command. Your cluster is now configured; you can run pure
    TensorFlow programs on the driver, or TensorFrames examples on the whole cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Using TensorFlow to add a constant to an existing column
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is a simple TensorFrames program where the `op` is to perform a simple
    addition. Note that the original source code can be found in the `databricks/tensorframes`
    GitHub repository. This is in reference to the TensorFrames `Readme.md` | *How
    to Run in Python* section ([https://github.com/databricks/tensorframes#how-to-run-in-python](https://github.com/databricks/tensorframes#how-to-run-in-python)).
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing we will do is import `TensorFlow`, `TensorFrames`, and `pyspark.sql.row`
    to create a DataFrame based on an RDD of floats:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'To view the `df` DataFrame generated by the RDD of floats, we can use the `show`
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using TensorFlow to add a constant to an existing column](img/B05793_08_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Executing the Tensor graph
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As noted previously, this tensor graph consists of adding 3 to the tensor created
    by the `df` DataFrame generated by the RDD of floats. We will now execute the
    following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are some specific call outs for the preceding code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '`x` utilizes `tfs.block`, where the `block` builds a block placeholder based
    on the content of a column in a DataFrame'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`z` is the output tensor from the TensorFlow `add` method (`tf.add`)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`df2` is the new DataFrame, which adds an extra column to the `df` DataFrame
    with the `z` tensor block by block'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'While `z` is the tensor (as noted in the preceding output), for us to work
    with the results of the TensorFlow program, we will utilize the `df2` dataframe.
    The output from `df2.show()` is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Executing the Tensor graph](img/B05793_08_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Blockwise reducing operations example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this next section, we will show how to work with blockwise reducing operations.
    Specifically, we will compute the sum and min of field vectors, working with blocks
    of rows for more efficient processing.
  prefs: []
  type: TYPE_NORMAL
- en: Building a DataFrame of vectors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, we will create a one-column DataFrame of vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Building a DataFrame of vectors](img/B05793_08_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Analysing the DataFrame
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We need to analyze the DataFrame to determine what its shape is (that is, dimensions
    of the vectors). For example, in the following snippet, we use the `tfs.print_schema`
    command for the `df` DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice the `double[?,?]`, meaning that TensorFlow does not know the dimensions
    of the vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Upon analysis of the `df2` DataFrame, TensorFlow has inferred that `y` contains
    vectors of size 2\. For small tensors (scalars and vectors), TensorFrames usually
    infers the shapes of the tensors without requiring a preliminary analysis. If
    it cannot do so, an error message will indicate that you need to run the DataFrame
    through `tfs.analyze()` first.
  prefs: []
  type: TYPE_NORMAL
- en: Computing elementwise sum and min of all vectors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now, let''s analyze the `df` DataFrame to compute the `sum` and the elementwise
    `min` of all the vectors using `tf.reduce_sum` and `tf.reduce_min`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`tf.reduce_sum`: Computes the sum of elements across dimensions of a tensor,
    for example, if `x = [[3, 2, 1], [-1, 2, 1]]` then `tf.reduce_sum(x) ==> 8`. More
    information can be found at: `https://www.tensorflow.org/api_docs/python/tf/reduce_sum`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tf.reduce_min`: Computes the minimum of elements across dimensions of a tensor,
    for example, if `x = [[3, 2, 1], [-1, 2, 1]]` then `tf.reduce_min(x) ==> -1`.
    More information can be found at: `https://www.tensorflow.org/api_docs/python/tf/reduce_min`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code snippet allows us to perform efficient elementwise reductions
    using TensorFlow, where the source data is within a DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: With a few lines of TensorFlow code with TensorFrames, we can take the data
    stored within the `df` DataFrame and execute a Tensor Graph to perform element
    wise sum and min, merge the data back into a DataFrame, and (in our case) print
    out the final values.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have reviewed the fundamentals of neural networks and Deep
    Learning, including the components of feature engineering. With all this new excitement
    in Deep Learning, we introduced TensorFlow and how it can work closely together
    with Apache Spark through TensorFrames.
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFrames is a powerful deep learning tool that allows data scientists and
    engineers to work with TensorFlow with data stored in Spark DataFrames. This allows
    you to expand the capabilities of Apache Spark to a powerful deep learning toolset
    that is based on the learning process of neural networks. To help continue your
    Deep Learning journey, the following are some great TensorFlow and TensorFrames
    resources:'
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow: [https://www.tensorflow.org/](https://www.tensorflow.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'TensorFlow | Get Started: [https://www.tensorflow.org/get_started/get_started](https://www.tensorflow.org/get_started/get_started)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'TensorFlow | Guides: [https://www.tensorflow.org/tutorials/](https://www.tensorflow.org/tutorials/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deep Learning on Databricks: [https://databricks.com/blog/2016/12/21/deep-learning-on-databricks.html](https://databricks.com/blog/2016/12/21/deep-learning-on-databricks.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'TensorFrames (GitHub): [https://github.com/databricks/tensorframes](https://github.com/databricks/tensorframes)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'TensorFrames User Guide: [https://github.com/databricks/tensorframes/wiki/TensorFrames-user-guide](https://github.com/databricks/tensorframes/wiki/TensorFrames-user-guide)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
