- en: 'Chapter 2: Core Operations with spaCy'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章：使用spaCy的核心操作
- en: In this chapter, you will learn the core operations with spaCy, such as creating
    a language pipeline, tokenizing the text, and breaking the text into its sentences.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习使用spaCy的核心操作，例如创建语言管道、分词文本以及将文本分解成句子。
- en: First, you'll learn what a language processing pipeline is and the pipeline
    components. We'll continue with general spaCy conventions – important classes
    and class organization – to help you to better understand spaCy library organization
    and develop a solid understanding of the library itself.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你将学习什么是语言处理管道以及管道组件。我们将继续介绍spaCy的通用约定——重要的类和类组织——以帮助你更好地理解spaCy库的组织结构，并对你对库本身有一个坚实的理解。
- en: You will then learn about the first pipeline component – **Tokenizer**. You'll
    also learn about an important linguistic concept – **lemmatization** – along with
    its applications in **natural language understanding** (**NLU**). Following that,
    we will cover **container classes** and **spaCy data structures** in detail. We
    will finish the chapter with useful spaCy features that you'll use in everyday
    NLP development.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 你将接着学习第一个管道组件——**分词器**。你还将了解一个重要的语言学概念——**词形还原**——以及它在**自然语言理解**（**NLU**）中的应用。随后，我们将详细介绍**容器类**和**spaCy数据结构**。我们将以有用的spaCy特性结束本章，这些特性你将在日常NLP开发中使用。
- en: 'We''re going to cover the following main topics in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Overview of spaCy conventions
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: spaCy约定的概述
- en: Introducing tokenization
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍分词
- en: Understanding lemmatization
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解词形还原
- en: spaCy container objects
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: spaCy容器对象
- en: More spaCy features
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更多spaCy特性
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The chapter code can be found at the book''s GitHub repository: [https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter02](https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter02)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在本书的GitHub仓库中找到：[https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter02](https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter02)
- en: Overview of spaCy conventions
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: spaCy约定的概述
- en: Every NLP application consists of several steps of processing the text. As you
    can see in the first chapter, we have always created instances called `nlp` and
    `doc`. But what did we do exactly?
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 每个NLP应用程序都包含对文本进行处理的几个步骤。正如你在第一章中看到的，我们总是创建了名为`nlp`和`doc`的实例。但我们到底做了什么呢？
- en: 'When we call `nlp` on our text, spaCy applies some processing steps. The first
    step is tokenization to produce a `Doc` object. The `Doc` object is then processed
    further with a `Doc` and then passes it to the next component:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在文本上调用`nlp`时，spaCy会应用一些处理步骤。第一步是分词，以生成一个`Doc`对象。然后，`Doc`对象会进一步通过`Doc`处理，然后传递给下一个组件：
- en: '![Figure 2.1 – A high-level view of the processing pipeline'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.1 – 处理管道的高级视图'
- en: '](img/B16570_02_01.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16570_02_01.jpg)'
- en: Figure 2.1 – A high-level view of the processing pipeline
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 – 处理管道的高级视图
- en: 'A spaCy pipeline object is created when we load a language model. We load an
    English model and initialize a pipeline in the following code segment:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们加载语言模型时，会创建一个spaCy管道对象。在以下代码段中，我们加载了一个英语模型并初始化了一个管道：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'What happened exactly in the preceding code is as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中实际上发生了以下情况：
- en: We started by importing `spaCy`.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先导入了`spaCy`。
- en: In the second line, `spacy.load()` returned a `Language` class instance, `nlp`.
    The `Language` class is *the text processing pipeline*.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第二行，`spacy.load()`返回了一个`Language`类实例，`nlp`。`Language`类是*文本处理管道*。
- en: After that, we applied `nlp` on the sample sentence `I went there` and got a
    `Doc` class instance, `doc`.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将`nlp`应用于示例句子`我去那里`，并得到了一个`Doc`类实例，`doc`。
- en: 'The `Language` class applies all of the preceding pipeline steps to your input
    sentence behind the scenes. After applying `nlp` to the sentence, the `Doc` object
    contains tokens that are tagged, lemmatized, and marked as entities if the token
    is an entity (we will go into detail about what are those and how it''s done later).
    Each pipeline component has a well-defined task:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '`Language`类在幕后将所有先前的管道步骤应用于你的输入句子。在将`nlp`应用于句子后，`Doc`对象包含标记、词形还原，如果标记是实体，则标记为实体（我们将在稍后详细介绍这些是什么以及如何实现）。每个管道组件都有一个明确定义的任务：'
- en: '![Figure 2.2 – Pipeline components and tasks'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.2 – 管道组件和任务'
- en: '](img/B16570_02_02.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16570_02_02.jpg)'
- en: Figure 2.2 – Pipeline components and tasks
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2 – 管道组件和任务
- en: The spaCy language processing pipeline always *depends on the statistical model*
    and its capabilities. This is why we always load a language model with `spacy.load()`
    as the first step in our code.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy语言处理管道始终**依赖于统计模型**及其能力。这就是为什么我们总是将语言模型作为代码中的第一步通过`spacy.load()`加载。
- en: 'Each component corresponds to a `spaCy` class. `spaCy` classes have self-explanatory
    names such as `Language` and `Doc` classes – let''s see all of the processing
    pipeline classes and their duties:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 每个组件都对应一个`spaCy`类。`spaCy`类有自解释的名称，如`Language`和`Doc`类 – 让我们看看所有处理管道类及其职责：
- en: '![Figure 2.3 – spaCy processing pipeline classes'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.3 – spaCy处理管道类'
- en: '](img/B16570_02_03.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16570_02_03.jpg)'
- en: Figure 2.3 – spaCy processing pipeline classes
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3 – spaCy处理管道类
- en: Don't be intimated by the number of classes; each class has unique features
    to help you process your text better.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 不要被类的数量吓倒；每个类都有独特的功能，可以帮助你更好地处理文本。
- en: 'There are more data structures to represent text data and language data. Container
    classes such as Doc hold information about sentences, words, and the text. There
    are also container classes other than Doc:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 有更多数据结构可以表示文本数据和语言数据。例如，Doc容器类包含关于句子、单词和文本的信息。除了Doc之外，还有其他容器类：
- en: '![Figure 2.4 – spaCy container classes'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.4 – spaCy容器类'
- en: '](img/B16570_02_04.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16570_02_04.jpg)'
- en: Figure 2.4 – spaCy container classes
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4 – spaCy容器类
- en: 'Finally, spaCy provides helper classes for vectors, language vocabulary, and
    annotations. We''ll see the `Vocab` class often in this book. `Vocab` represents
    a language''s vocabulary. Vocab contains all the words of the language model we
    loaded:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，spaCy为向量、语言词汇和注释提供了辅助类。在这本书中，我们将经常看到`Vocab`类。`Vocab`代表一种语言的词汇。Vocab包含我们加载的语言模型中的所有单词：
- en: '![Figure 2.5 – spaCy helper classes'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.5 – spaCy辅助类'
- en: '](img/B16570_02_05.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16570_02_05.jpg)'
- en: Figure 2.5 – spaCy helper classes
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5 – spaCy辅助类
- en: 'The spaCy library''s backbone data structures are `Doc` and `Vocab`. The `Doc`
    object abstracts the text by owning the sequence of tokens and all their properties.
    The `Vocab` object provides a centralized set of strings and lexical attributes
    to all the other classes. This way spaCy avoids storing multiple copies of linguistic
    data:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy库的核心数据结构是`Doc`和`Vocab`。`Doc`对象通过拥有标记序列及其所有属性来抽象文本。`Vocab`对象为所有其他类提供集中式的字符串和词汇属性。这样spaCy就避免了存储多个语言数据的副本：
- en: '![Figure 2.6 – spaCy architecture'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.6 – spaCy架构'
- en: '](img/B16570_02_06.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16570_02_06.jpg)'
- en: Figure 2.6 – spaCy architecture
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.6 – spaCy架构
- en: 'You can divide the objects composing the preceding spaCy architecture into
    two: **containers** and **processing pipeline components**. In this chapter, we''ll
    first learn about two basic components, **Tokenizer** and **Lemmatizer**, then
    we''ll explore **Container** objects further.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将组成前面spaCy架构的对象分为两类：**容器**和**处理管道组件**。在本章中，我们将首先了解两个基本组件，**分词器**和**词形还原器**，然后我们将进一步探索**容器对象**。
- en: spaCy does all these operations for us behind the scenes, allowing us to concentrate
    on our own application's development. With this level of abstraction, using spaCy
    for NLP application development is no coincidence. Let's start with the `Tokenizer`
    class and see what it offers for us; then we will explore all the container classes
    one by one throughout the chapter.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy在幕后为我们执行所有这些操作，使我们能够专注于我们自己的应用程序开发。在这个抽象级别上，使用spaCy进行NLP应用程序开发并非巧合。让我们从`Tokenizer`类开始，看看它为我们提供了什么；然后我们将在本章中逐个探索所有容器类。
- en: Introducing tokenization
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍分词
- en: We saw in *Figure 2.1* that the first step in a text processing pipeline is
    tokenization. Tokenization is always the first operation because all the other
    operations require the tokens.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*图2.1*中看到，文本处理管道的第一步是分词。分词总是第一个操作，因为所有其他操作都需要标记单元。
- en: 'Tokenization simply means splitting the sentence into its tokens. A **token**
    is a unit of semantics. You can think of a token as the smallest meaningful part
    of a piece of text. Tokens can be words, numbers, punctuation, currency symbols,
    and any other meaningful symbols that are the building blocks of a sentence. The
    following are examples of tokens:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 分词简单来说就是将句子拆分成其标记单元。**标记单元**是语义的一个单位。你可以将标记单元想象为文本中最小的有意义的部分。标记单元可以是单词、数字、标点符号、货币符号以及任何其他构成句子的有意义的符号。以下是一些标记单元的例子：
- en: '`USA`'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`USA`'
- en: '`N.Y.`'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`N.Y.`'
- en: '`city`'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`city`'
- en: '`33`'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`33`'
- en: '`3rd`'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`3rd`'
- en: '`!`'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`!`'
- en: '`…`'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`…`'
- en: '`?`'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`?`'
- en: '`''s`'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''s'''
- en: 'Input to the spaCy tokenizer is a Unicode text and the result is a `Doc` object.
    The following code shows the tokenization process:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 输入到 spaCy 标记化器的是 Unicode 文本，结果是 `Doc` 对象。以下代码显示了标记化过程：
- en: '[PRE1]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The following is what we just did:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们刚刚所做的事情：
- en: We start by importing `spaCy`.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先导入 `spaCy`。
- en: Then we loaded the English language model via the `en` shortcut to create an
    instance of the `nlp` `Language` class.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们通过 `en` 快捷方式加载了英语语言模型以创建 `nlp` `Language` 类的实例。
- en: Next, we apply the `nlp` object to the input sentence to create a `Doc` object,
    `doc`. A `Doc` object is a container for a sequence of `Token` objects. spaCy
    generates the `Token` objects implicitly when we created the `Doc` object.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将 `nlp` 对象应用于输入句子以创建 `Doc` 对象，`doc`。`Doc` 对象是一个 `Token` 对象序列的容器。当我们创建 `Doc`
    对象时，spaCy 隐式地生成 `Token` 对象。
- en: Finally, we print a list of the preceding sentence's tokens.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们打印出前一句子的标记列表。
- en: 'That''s it, we made the tokenization with just three lines of code. You can
    visualize the tokenization with indexing as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样，我们只用了三行代码就完成了标记化。你可以通过以下方式可视化标记化：
- en: '![Figure 2.7 – Tokenization of “I own a ginger cat.”'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.7 – “我有一只姜黄色猫。”的标记化]'
- en: '](img/B16570_02_07.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16570_02_07.jpg](img/B16570_02_07.jpg)'
- en: Figure 2.7 – Tokenization of "I own a ginger cat."
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.7 – “我有一只姜黄色猫。”的标记化
- en: 'As the examples suggest, tokenization can indeed be tricky. There are many
    aspects we should pay attention to: punctuation, whitespaces, numbers, and so
    on. Splitting from the whitespaces with `text.split(" ")` might be tempting and
    looks like it is working for the example sentence *I own a ginger cat*.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如示例所示，标记化确实可能很棘手。有许多方面我们应该注意：标点符号、空白、数字等等。使用 `text.split(" ")` 从空白处分割可能很有吸引力，看起来它对于示例句子
    *我有一只姜黄色猫* 是有效的。
- en: 'How about the sentence `"It''s been a crazy week!!!"`? If we make a `split("
    ")` the resulting tokens would be `It''s`, `been`, `a`, `crazy`, `week!!!`, which
    is not what you want. First of all, `It''s` is not one token, it''s two tokens:
    `it` and `''s`. `week!!!` is not a valid token as the punctuation is not split
    correctly. Moreover, `!!!` should be tokenized per symbol and should generate
    three *!*''s. (This may not look like an important detail, but trust me, it is
    important for *sentiment analysis*. We''ll cover sentiment analysis in [*Chapter
    8*](B16570_08_Final_JM_ePub.xhtml#_idTextAnchor137)*, Text Classification with
    spaCy*.) Let''s see what spaCy tokenizer has generated:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，“It's been a crazy week!!!” 这个句子呢？如果我们使用 `split(" ")`，得到的标记将是 `It's`、`been`、`a`、`crazy`、`week!!!`，这并不是你想要的。首先，`It's`
    不是一个标记，它是两个标记：`it` 和 `'s'`。`week!!!` 不是一个有效的标记，因为标点符号没有被正确分割。此外，`!!!` 应该按符号标记化，并生成三个
    *!*。 (这可能看起来不是一个重要的细节，但请相信我，这对 *情感分析* 非常重要。我们将在 [*第 8 章*](B16570_08_Final_JM_ePub.xhtml#_idTextAnchor137)*，使用
    spaCy 进行文本分类* 中介绍情感分析。) 让我们看看 spaCy 标记化器生成了什么：
- en: '[PRE2]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This time the sentence is split as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这次句子是这样分割的：
- en: '![Figure 2.8 – Tokenization of apostrophe and punctuations marks'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.8 – 引号和标点符号的标记化]'
- en: '](img/B16570_02_08.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16570_02_08.jpg](img/B16570_02_08.jpg)'
- en: Figure 2.8 – Tokenization of apostrophe and punctuations marks
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.8 – 引号和标点符号的标记化
- en: 'How does spaCy know where to split the sentence? Unlike other parts of the
    pipeline, the tokenizer doesn''t need a statistical model. Tokenization is based
    on language-specific rules. You can see examples the language specified data here:
    [https://github.com/explosion/spaCy/tree/master/spacy/lang](https://github.com/explosion/spaCy/tree/master/spacy/lang).'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 是如何知道在哪里分割句子的？与其他管道部分不同，标记化器不需要统计模型。标记化基于语言特定的规则。你可以在这里看到指定语言的数据示例：[https://github.com/explosion/spaCy/tree/master/spacy/lang](https://github.com/explosion/spaCy/tree/master/spacy/lang)。
- en: 'Tokenizer exceptions define rules for exceptions, such as `it''s` , `don''t`
    , `won''t`, abbreviations, and so on. If you look at the rules for English: [https://github.com/explosion/spaCy/blob/master/spacy/lang/en/tokenizer_exceptions.py](https://github.com/explosion/spaCy/blob/master/spacy/lang/en/tokenizer_exceptions.py),
    you will see that rules look like `{ORTH: "n''t", LEMMA: "not"}`, which describes
    the splitting rule for `n''t` to the tokenizer.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '标记化器异常定义了异常的规则，例如 `it''s`、`don''t`、`won''t`、缩写等等。如果你查看英语的规则：[https://github.com/explosion/spaCy/blob/master/spacy/lang/en/tokenizer_exceptions.py](https://github.com/explosion/spaCy/blob/master/spacy/lang/en/tokenizer_exceptions.py)，你会看到规则看起来像
    `{ORTH: "n''t", LEMMA: "not"}`，这描述了 `n''t` 对标记化器的分割规则。'
- en: 'The prefixes, suffixes, and infixes mostly describe how to deal with punctuation
    – for example, we split at a period if it is at the end of the sentence, otherwise,
    most probably it''s part of an abbreviation such as N.Y. and we shouldn''t touch
    it. Here, `ORTH` means the text and `LEMMA` means the base word form without any
    inflections. The following example shows you the execution of the spaCy tokenization
    algorithm:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 前缀、后缀和内嵌词主要描述了如何处理标点符号 – 例如，如果句尾有一个句号，我们就将其分割，否则，它很可能是缩写的一部分，如 N.Y.，我们不应该对其进行操作。在这里，`ORTH`
    表示文本，而 `LEMMA` 表示不带任何屈折变化的词的基本形式。以下示例展示了 spaCy 分词算法的执行过程：
- en: '![Figure 2.9 – spaCy performing tokenization with exception rules (image taken
    from spaCy tokenization guidelines (https://spacy.io/usage/linguistic-features#tokenization))](img/B16570_02_09.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.9 – spaCy 使用异常规则进行分词（图片来自 spaCy 分词指南（https://spacy.io/usage/linguistic-features#tokenization）)](img/B16570_02_09.jpg)'
- en: Figure 2.9 – spaCy performing tokenization with exception rules (image taken
    from spaCy tokenization guidelines (https://spacy.io/usage/linguistic-features#tokenization))
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.9 – spaCy 使用异常规则进行分词（图片来自 spaCy 分词指南（https://spacy.io/usage/linguistic-features#tokenization））
- en: Tokenization rules depend on the grammatical rules of the individual language.
    Punctuation rules such as splitting periods, commas, or exclamation marks are
    more or less similar for many languages; however, some rules are specific to the
    individual language, such as abbreviation words and apostrophe usage. spaCy supports
    each language having its own specific rules by allowing hand-coded data and rules,
    as each language has its own subclass.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 分词规则取决于个别语言的语法规则。像分割句号、逗号或感叹号这样的标点符号规则在许多语言中或多或少是相似的；然而，一些规则是特定于个别语言的，例如缩写词和撇号的使用。spaCy
    通过允许手动编码的数据和规则来支持每个语言都有其特定的规则，因为每个语言都有自己的子类。
- en: Tip
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: spaCy provides non-destructive tokenization, which means that we always will
    be able to recover the original text from the tokens. Whitespace and punctuation
    information is preserved during tokenization, so the input text is preserved as
    it is.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 提供了非破坏性分词，这意味着我们总是可以从标记中恢复原始文本。在分词过程中，空白和标点信息被保留，因此输入文本保持原样。
- en: Every `Language` object contains a `Tokenizer` object. The `Tokenizer` class
    is the class that performs the tokenization. You don't often call this class directly
    when you create a `Doc` class instance, while Tokenizer class acts behind the
    scenes. When we want to customize the tokenization, we need to interact with this
    class. Let's see how it is done.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 `Language` 对象都包含一个 `Tokenizer` 对象。`Tokenizer` 类是执行分词的类。当你创建 `Doc` 类实例时，通常不直接调用这个类，而
    `Tokenizer` 类在幕后工作。当我们想要自定义分词时，我们需要与这个类进行交互。让我们看看它是如何完成的。
- en: Customizing the tokenizer
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义分词器
- en: 'When we work with a specific domain such as medicine, insurance, or finance,
    we often come across words, abbreviations, and entities that needs special attention.
    Most domains that you''ll process have characteristic words and phrases that need
    custom tokenization rules. Here''s how to add a special case rule to an existing
    `Tokenizer` class instance:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们处理特定领域，如医学、保险或金融时，我们经常会遇到需要特别注意的单词、缩写和实体。你将处理的多数领域都有其特有的单词和短语，需要自定义分词规则。以下是如何向现有的
    `Tokenizer` 类实例添加特殊案例规则的方法：
- en: '[PRE3]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Here is what we did:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们所做的工作：
- en: We again started by importing `spacy`.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们再次从导入 `spacy` 开始。
- en: Then, we imported the `ORTH` symbol, which means orthography; that is, text.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们导入了 `ORTH` 符号，这意味着正字法；即文本。
- en: We continued with creating a `Language` class object, `nlp`, and created a Doc
    object, `doc`.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们继续创建一个 `Language` 类对象，`nlp`，并创建了一个 `Doc` 对象，`doc`。
- en: We defined a special case, where the word `lemme` should tokenize as two tokens,
    `lem` and `me`.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义了一个特殊案例，其中单词 `lemme` 应该分词为两个标记，`lem` 和 `me`。
- en: We added the rule to the `nlp` object's tokenizer.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将规则添加到了 `nlp` 对象的分词器中。
- en: The last line exhibits how the fresh rule works.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后一行展示了新规则是如何工作的。
- en: 'When we define custom rules, punctuation splitting rules will still apply.
    Our special case will be recognized as a result, even if it''s surrounded by punctuation.
    The tokenizer will divide punctuation step by step, and apply the same process
    to the remaining substring:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们定义自定义规则时，标点分割规则仍然适用。我们的特殊案例将被识别为结果，即使它被标点符号包围。分词器将逐步分割标点符号，并将相同的处理过程应用于剩余的子串：
- en: '[PRE4]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'If you define a special case rule with punctuation, the special case rule will
    take precedence over the punctuation splitting:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你定义了一个带有标点的特殊规则，则特殊规则将优先于标点分割：
- en: '[PRE5]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Pro tip
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: Modify the tokenizer by adding new rules only if you really need to. Trust me,
    you can get quite unexpected results with custom rules. One of the cases where
    you really need it is when working with Twitter text, which is usually full of
    hashtags and special symbols. If you have social media text, first feed some sentences
    into the spaCy NLP pipeline and see how the tokenization works out.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 只有在真正需要的时候才通过添加新规则来修改分词器。相信我，使用自定义规则可能会得到相当意外的结果。真正需要这种情况之一是处理Twitter文本，它通常充满了标签和特殊符号。如果你有社交媒体文本，首先将一些句子输入到spaCy
    NLP管道中，看看分词是如何进行的。
- en: Debugging the tokenizer
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调试分词器
- en: 'The spaCy library has a tool for debugging: `nlp.tokenizer.explain(sentence`).
    It returns (`tokenizer rule/pattern, token`) **tuples** to help us understand
    what happened exactly during the tokenization. Let''s see an example:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy库有一个用于调试的工具：`nlp.tokenizer.explain(sentence)`。它返回（`tokenizer rule/pattern,
    token`）**元组**，帮助我们了解分词过程中确切发生了什么。让我们看一个例子：
- en: '[PRE6]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In the preceding code, we imported `spacy` and created a `Language` class instance,
    `nlp`, as usual. Then we created a `Doc` class instance with the sentence `Let's
    go!`. After that, we asked the `Tokenizer` class instance, `tokenizer`, of `nlp`
    for an explanation of the tokenization of this sentence. `nlp.tokenizer.explain()`
    explained the rules that the tokenizer used one by one.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们导入了`spacy`，并像往常一样创建了一个`Language`类实例，`nlp`。然后我们使用句子`Let's go!`创建了一个`Doc`类实例。之后，我们向`nlp`的`Tokenizer`类实例`tokenizer`请求对这句话分词的解释。`nlp.tokenizer.explain()`逐个解释了分词器使用的规则。
- en: After splitting a sentence into its tokens, it's time to split a text into its
    sentences.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在将句子分割成词素之后，现在是时候将文本分割成句子了。
- en: Sentence segmentation
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 句子分割
- en: We saw that breaking a sentence into its tokens is not a straightforward task
    at all. How about breaking a text into sentences? It's indeed a bit more complicated
    to mark where a sentence starts and ends due to the same reasons of punctuation,
    abbreviations, and so on.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到将句子分割成词素并不是一个简单直接的任务。那么将文本分割成句子呢？由于标点、缩写等原因，标记句子开始和结束的位置确实要复杂一些。
- en: 'A Doc object''s sentences are available via the `doc.sents` property:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 一个Doc对象的句子可以通过`doc.sents`属性访问：
- en: '[PRE7]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Determining sentence boundaries is a more complicated task than tokenization.
    As a result, spaCy uses the dependency parser to perform sentence segmentation.
    This is a unique feature of spaCy – no other library puts such a sophisticated
    idea into practice. The results are very accurate in general, unless you process
    text of a very specific genre, such as from the conversation domain, or social
    media text.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 确定句子边界比分词更复杂。因此，spaCy使用依存句法分析器来执行句子分割。这是spaCy的独特功能——没有其他库将如此复杂的思想付诸实践。一般来说，结果非常准确，除非你处理的是非常特定类型的文本，例如来自对话领域或社交媒体文本。
- en: Now we know how to segment a text into sentences and tokenize the sentences.
    We're ready to process the tokens one by one. Let's start with lemmatization,
    a commonly used operation in semantics including sentiment analysis.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了如何将文本分割成句子并将句子分词。我们准备好逐个处理词素了。让我们从词元化开始，这是语义分析中常用的一种操作。
- en: Understanding lemmatization
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解词元化
- en: 'A **lemma** is the base form of a token. You can think of a lemma as the form
    in which the token appears in a dictionary. For instance, the lemma of *eating*
    is *eat*; the lemma of *eats* is *eat*; *ate* similarly maps to *eat*. Lemmatization
    is the process of reducing the word forms to their lemmas. The following code
    is a quick example of how to do lemmatization with spaCy:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '**词元**是词素的基本形式。你可以将词元想象成词典中词素出现的形态。例如，*eating*的词元是*eat*；*eats*的词元也是*eat*；*ate*同样映射到*eat*。词元化是将词形还原到其词元的过程。以下是一个使用spaCy进行词元化的快速示例：'
- en: '[PRE8]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: By now, you should be familiar with what the first three lines of the code do.
    Recall that we import the `spacy` library, load an English model using `spacy.load`,
    create a pipeline, and apply the pipeline to the preceding sentence to get a Doc
    object. Here we iterated over tokens to get their text and lemmas.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，你应该熟悉代码的前三行做了什么。回想一下，我们导入了`spacy`库，使用`spacy.load`加载了一个英语模型，创建了一个管道，并将管道应用于前面的句子以获取一个Doc对象。在这里，我们遍历了词素以获取它们的文本和词元。
- en: 'In the first line you see `–PRON-`, which doesn''t look like a real token.
    This is a **pronoun lemma**, a special token for lemmas of personal pronouns.
    This is an exception for semantic purposes: the personal pronouns *you*, *I*,
    *me*, *him*, *his*, and so on look different, but in terms of meaning, they''re
    in the same group. spaCy offers this trick for the pronoun lemmas.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一行中，你看到 `–PRON-`，这看起来不像一个真正的标记。这是一个 **代词词元**，一个用于个人代词词元的特殊标记。这是出于语义目的的一个例外：个人代词
    *you*、*I*、*me*、*him*、*his* 等看起来不同，但在意义上，它们属于同一组。spaCy 为代词词元提供了这个技巧。
- en: No worries if all of this sounds too abstract – let's see lemmatization in action
    with a real-world example.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这一切听起来过于抽象，请不要担心——让我们通过一个现实世界的例子来看看词元化的实际应用。
- en: Lemmatization in NLU
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NLU 中的词元化
- en: Lemmatization is an important step in NLU. We'll go over an example in this
    subsection. Suppose that you design an NLP pipeline for a ticket booking system.
    Your application processes a customer's sentence, extracts necessary information
    from it, and then passes it to the booking API.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 词元化是 NLU 的重要步骤。我们将在本小节中通过一个示例来讲解。假设你为票务预订系统设计了一个 NLP 管道。你的应用程序处理客户的句子，从中提取必要的信息，然后将它传递给预订
    API。
- en: 'The NLP pipeline wants to extract the form of the travel (a flight, bus, or
    train), the destination city, and the date. The first thing the application needs
    to verify is the means of travel:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: NLP 管道旨在提取旅行的形式（航班、巴士或火车）、目的地城市和日期。应用程序需要验证的第一件事是旅行方式：
- en: '[PRE9]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We have this list of keywords and we want to recognize the means of travel
    by searching the tokens in the keywords list. The most compact way of doing this
    search is by looking up the token''s lemma. Consider the following customer sentences:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有这个关键词列表，并希望通过在关键词列表中搜索标记来识别旅行方式。进行此搜索的最紧凑方式是查找标记的词元。考虑以下客户句子：
- en: '[PRE10]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Here, we don't need to include all word forms of the verb *fly* (*fly*, *flying*,
    *flies*, *flew*, and *flown*) in the keywords list and similar for the word `flight`;
    we reduced all possible variants to the base forms – *fly* and *flight*. Don't
    think of English only; languages such as Spanish, German, and Finnish have many
    word forms from a single lemma as well.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们不需要在关键词列表中包含动词 *fly* 的所有词形（*fly*、*flying*、*flies*、*flew* 和 *flown*），对于单词
    `flight` 也是如此；我们将所有可能的变体都缩减到了基本形式 – *fly* 和 *flight*。不要只考虑英语；像西班牙语、德语和芬兰语这样的语言也有许多来自单个词元的词形。
- en: 'Lemmatization also comes in handy when we want to recognize the destination
    city. There are many nicknames available for global cities and the booking API
    can process only the official names. The default tokenizer and lemmatizer won''t
    know the difference between the official name and the nickname. In this case,
    you can add special rules, as we saw in the *Introducing tokenization* section.
    The following code plays a small trick:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们想要识别目的地城市时，词元化也很有用。全球城市有许多昵称，而预订 API 只能处理官方名称。默认的分词器和词元化器不会区分官方名称和昵称。在这种情况下，你可以添加特殊规则，就像我们在
    *介绍分词* 部分中看到的那样。以下代码玩了一个小把戏：
- en: '[PRE11]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We defined a special case for the word `Angeltown` by replacing its lemma with
    the official name `Los Angeles`. Then we added this special case to the `Tokenizer`
    instance. When we print the token lemmas, we see that `Angeltown` maps to `Los
    Angeles` as we wished.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过将 `Angeltown` 的词元替换为官方名称 `Los Angeles` 来为该词定义了一个特殊情况。然后我们将这个特殊情况添加到 `Tokenizer`
    实例中。当我们打印标记词元时，我们看到 `Angeltown` 正如我们希望的那样映射到 `Los Angeles`。
- en: Understanding the difference between lemmatization and stemming
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解词元化与词干提取的区别
- en: 'A lemma is the base form of a word and is always a member of the language''s
    vocabulary. The stem does not have to be a valid word at all. For instance, the
    lemma of *improvement* is *improvement*, but the stem is *improv*. You can think
    of the stem as the smallest part of the word that carries the meaning. Compare
    the following examples:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 词元是单词的基本形式，并且总是语言词汇的一部分。词干不一定必须是有效的单词。例如，*improvement* 的词元是 *improvement*，但词干是
    *improv*。你可以把词干看作是承载意义的单词的最小部分。比较以下示例：
- en: '[PRE12]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The preceding word-lemma examples show how lemma is calculated by following
    the grammatical rules of the language. Here, the lemma of a plural form is the
    singular form, and the lemma of a third-person verb is the base form of the verb.
    Let''s compare them to the following examples of word-stem pairs:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 上述词元-词元示例展示了如何通过遵循语言的语法规则来计算词元。在这里，复数形式的词元是其单数形式，第三人称动词的词元是动词的基本形式。让我们将它们与以下词根-词对示例进行比较：
- en: '[PRE13]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The first and the most important point to notice in the preceding examples is
    that the lemma does not have to be a valid word in the language. The second point
    is that many words can map to the same stem. Also, words from different grammatical
    categories can map to the same stem; here for instance, the noun *improvement*
    and the verb *improves* both map to *improv*.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，需要注意的第一个和最重要的点是词元不必是语言中的有效单词。第二个点是许多单词可以映射到同一个词干。此外，来自不同语法类别的单词也可以映射到同一个词干；例如，名词
    *improvement* 和动词 *improves* 都映射到 *improv*。
- en: Though stems are not valid words, they still carry meaning. That's why stemming
    is commonly used in NLU applications.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管词干不是有效单词，但它们仍然承载着意义。这就是为什么词干提取在 NLU 应用中通常被使用的原因。
- en: Stemming algorithms don't know anything about the grammar of the language. This
    class of algorithms works rather by trimming some common suffixes and prefixes
    from the beginning or end of the word.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 词干提取算法对语言的语法一无所知。这类算法主要通过从词的起始或结束部分修剪一些常见的后缀和前缀来工作。
- en: Stemming algorithms are rough, they cut the word from head and tail. There are
    several stemming algorithms available for English, including Porter and Lancaster.
    You can play with different stemming algorithms on NLTK's demo page at [https://text-processing.com/demo/stem/](https://text-processing.com/demo/stem/).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 词干提取算法比较粗糙，它们从词的首部和尾部切割单词。对于英语，有几种可用的词干提取算法，包括 Porter 和 Lancaster。你可以在 NLTK
    的演示页面上尝试不同的词干提取算法：[https://text-processing.com/demo/stem/](https://text-processing.com/demo/stem/)。
- en: Lemmatization, on the other hand, takes the morphological analysis of the words
    into consideration. To do so, it is important to obtain the dictionaries for the
    algorithm to look through in order to link the form back to its lemma.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，词形还原考虑了单词的形态分析。为此，重要的是要获得算法查找的词典，以便将形式与其词元联系起来。
- en: spaCy provides lemmatization via dictionary lookup and each language has its
    own dictionary.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 通过字典查找提供词形还原，每种语言都有自己的字典。
- en: Tip
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: Both stemming and lemmatization have their own advantages. Stemming gives very
    good results if you apply only statistical algorithms to the text, without further
    semantic processing such as pattern lookup, entity extraction, coreference resolution,
    and so on. Also stemming can trim a big corpus to a more moderate size and give
    you a compact representation. If you also use linguistic features in your pipeline
    or make a keyword search, include lemmatization. Lemmatization algorithms are
    accurate but come with a cost in terms of computation.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 词干提取和词形还原都有它们自己的优势。如果你只对文本应用统计算法，而不进行进一步的语义处理（如模式查找、实体提取、指代消解等），词干提取会给出非常好的结果。此外，词干提取可以将大型语料库缩减到更适中的大小，并提供紧凑的表示。如果你在管道中使用语言特征或进行关键词搜索，请包括词形还原。词形还原算法准确，但计算成本较高。
- en: spaCy container objects
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: spaCy 容器对象
- en: At the beginning of this chapter, we saw a list of container objects including
    **Doc**, **Token**, **Span**, and **Lexeme**. We already used Token and Doc in
    our code. In this subsection, we'll see the properties of the container objects
    in detail.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的开头，我们看到了一个包含 **Doc**、**Token**、**Span** 和 **Lexeme** 的容器对象列表。我们已经在代码中使用了
    Token 和 Doc。在本小节中，我们将详细查看容器对象的属性。
- en: Using container objects, we can access the linguistic properties that spaCy
    assigns to the text. A container object is a logical representation of the text
    units such as a document, a token, or a slice of the document.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 使用容器对象，我们可以访问 spaCy 分配给文本的语言属性。容器对象是文本单元（如文档、标记或文档的片段）的逻辑表示。
- en: 'Container objects in spaCy follow the natural structure of the text: a document
    is composed of sentences and sentences are composed of tokens.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: spaCy 中的容器对象遵循文本的自然结构：文档由句子组成，句子由标记组成。
- en: We most widely use Doc, Token, and Span objects in development, which represent
    a document, a single token, and a phrase, respectively. A container can contain
    other containers, for instance a document contains tokens and spans.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在开发中最广泛地使用 Doc、Token 和 Span 对象，分别代表文档、单个标记和短语。容器可以包含其他容器，例如文档包含标记和片段。
- en: Let's explore each class and its useful properties one by one.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一探索每个类及其有用的属性。
- en: Doc
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Doc
- en: We created Doc objects in our code to represent the text, so you might have
    already figured out that Doc represents a text.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在代码中创建了 Doc 对象来表示文本，所以你可能已经意识到 Doc 代表文本。
- en: 'We already know how to create a Doc object:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经知道如何创建 Doc 对象：
- en: '[PRE14]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '`doc.text` returns a Unicode representation of the document text:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '`doc.text` 返回文档文本的 Unicode 表示：'
- en: '[PRE15]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The building block of a Doc object is Token, hence when you iterate a Doc you
    get Token objects as items:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: Doc 对象的构建块是 Token，因此当您迭代 Doc 时，您会得到 Token 对象作为项目：
- en: '[PRE16]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The same logic applies to indexing:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的逻辑也适用于索引：
- en: '[PRE17]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The length of a Doc is the number of tokens it includes:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: Doc 的长度是它包含的标记数量：
- en: '[PRE18]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We already saw how to get the text''s sentences. `doc.sents` returns an iterator
    to the list of sentences. Each sentence is a Span object:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了如何获取文本的句子。`doc.sents` 返回句子列表的迭代器。每个句子都是一个 Span 对象：
- en: '[PRE19]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '`doc.ents` gives named entities of the text. The result is a list of Span objects.
    We''ll see named entities in detail later – for now, think of them as proper nouns:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '`doc.ents` 提供文本中的命名实体。结果是 Span 对象的列表。我们将在稍后详细讨论命名实体——现在，请将它们视为专有名词：'
- en: '[PRE20]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Another syntactic property is `doc.noun_chunks`. It yields the noun phrases
    found in the text:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个句法属性是 `doc.noun_chunks`。它产生文本中找到的名词短语：
- en: '[PRE21]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '`doc.lang_` returns the language that `doc` created:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '`doc.lang_` 返回 `doc` 创建的语言：'
- en: '[PRE22]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'A useful method for serialization is `doc.to_json`. This is how to convert
    a Doc object to JSON:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有用的序列化方法是 `doc.to_json`。这是将 Doc 对象转换为 JSON 的方法：
- en: '[PRE23]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Pro tip
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: You might have noticed that we call `doc.lang_`, not `doc.lang`. `doc.lang`
    returns the language ID, whereas `doc.lang_` returns the Unicode string of the
    language, that is, the name of the language. You can see the same convention with
    Token features in the following, for instance, `token.lemma_`, `token.tag_`, and
    `token.pos_`.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到我们调用 `doc.lang_`，而不是 `doc.lang`。`doc.lang` 返回语言 ID，而 `doc.lang_` 返回语言的
    Unicode 字符串，即语言名称。您可以在以下 Token 特性中看到相同的约定，例如，`token.lemma_`、`token.tag_` 和 `token.pos_`。
- en: The Doc object has very useful properties with which you can understand a sentence's
    syntactic properties and use them in your own applications. Let's move on to the
    Token object and see what it offers.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: Doc 对象具有非常实用的属性，您可以使用这些属性来理解句子的句法属性，并在您自己的应用程序中使用它们。让我们继续了解 Token 对象及其提供的功能。
- en: Token
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Token
- en: 'A Token object represents a word. Token objects are the building blocks of
    Doc and Span objects. In this section, we will cover the following properties
    of the `Token` class:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: Token 对象代表一个单词。Token 对象是 Doc 和 Span 对象的构建块。在本节中，我们将介绍 `Token` 类的以下属性：
- en: '`token.text`'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token.text`'
- en: '`token.text_with_ws`'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token.text_with_ws`'
- en: '`token.i`'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token.i`'
- en: '`token.idx`'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token.idx`'
- en: '`token.doc`'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token.doc`'
- en: '`token.sent`'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token.sent`'
- en: '`token.is_sent_start`'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token.is_sent_start`'
- en: '`token.ent_type`'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`token.ent_type`'
- en: 'We usually don''t construct a Token object directly, rather we construct a
    Doc object then access its tokens:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常不会直接构建 Token 对象，而是先构建 Doc 对象，然后访问其标记：
- en: '[PRE24]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '`token.text` is similar to `doc.text` and provides the underlying Unicode string:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '`token.text` 与 `doc.text` 类似，并提供底层 Unicode 字符串：'
- en: '[PRE25]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '`token.text_with_ws` is a similar property. It provides the text with a trailing
    whitespace if present in the `doc`:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '`token.text_with_ws` 是一个类似的属性。如果 `doc` 中存在，它将提供带有尾随空白的文本：'
- en: '[PRE26]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Finding the length of a token is similar to finding the length of a Python
    string:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 找到标记的长度与找到 Python 字符串的长度类似：
- en: '[PRE27]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '`token.i` gives the index of the token in `doc`:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '`token.i` 给出标记在 `doc` 中的索引：'
- en: '[PRE28]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '`token.idx` provides the token''s character offset (the character position)
    in `doc`:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '`token.idx` 提供了标记在 `doc` 中的字符偏移量（字符位置）：'
- en: '[PRE29]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We can also access the `doc` that created the `token` as follows:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以按照以下方式访问创建 `token` 的 `doc`：
- en: '[PRE30]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Getting the sentence that the `token` belongs to is done in a similar way to
    accessing the `doc` that created the `token`:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 获取 `token` 所属的句子与访问创建 `token` 的 `doc` 的方式类似：
- en: '[PRE31]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '`token.is_sent_start` is another useful property; it returns a Boolean indicating
    whether the token starts a sentence:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '`token.is_sent_start` 是另一个有用的属性；它返回一个布尔值，指示标记是否开始句子：'
- en: '[PRE32]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'These are the basic properties of the Token object that you''ll use every day.
    There is another set of properties that are more related to syntax and semantics.
    We already saw how to calculate the token lemma in the previous section:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是您每天都会使用的 Token 对象的基本属性。还有另一组属性，它们与句法和语义更相关。我们已经在上一节中看到了如何计算标记的词元：
- en: '[PRE33]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'You already learned that `doc.ents` gives the named entities of the document.
    If you want to learn what sort of entity the token is, use `token.ent_type_`:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经了解到 `doc.ents` 提供了文档中的命名实体。如果您想了解标记是哪种类型的实体，请使用 `token.ent_type_`：
- en: '[PRE34]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Two syntactic features related to POS tagging are `token.pos_` and `token.tag`.
    We'll learn what they are and how to use them in the next chapter.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 与词性标注相关的两个语法功能是`token.pos_`和`token.tag`。我们将在下一章学习它们是什么以及如何使用它们。
- en: Another set of syntactic features comes from the dependency parser. These features
    are `dep_`, `head_`, `conj_`, `lefts_`, `rights_`, `left_edge_`, and `right_edge_`.
    We'll cover them in the next chapter as well.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 另一套语法功能来自依存句法分析器。这些功能包括`dep_`、`head_`、`conj_`、`lefts_`、`rights_`、`left_edge_`和`right_edge_`。我们将在下一章中也介绍它们。
- en: Tip
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: It is totally normal if you don't remember all the features afterward. If you
    don't remember the name of a feature, you can always do `dir(token)` or `dir(doc)`.
    Calling `dir()` will print all the features and methods available on the object.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你之后不记得所有功能，这是完全正常的。如果你不记得一个功能的名称，你总是可以执行`dir(token)`或`dir(doc)`。调用`dir()`将打印出对象上所有可用的功能和方法的名称。
- en: The Token object has a rich set of features, enabling us to process the text
    from head to toe. Let's move on to the Span object and see what it offers for
    us.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: Token对象具有丰富的功能集，使我们能够从头到尾处理文本。让我们继续到Span对象，看看它为我们提供了什么。
- en: Span
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Span
- en: 'Span objects represent phrases or segments of the text. Technically, a Span
    has to be a contiguous sequence of tokens. We usually don''t initialize Span objects,
    rather we slice a Doc object:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: Span对象代表文本的短语或片段。技术上，Span必须是一系列连续的标记。我们通常不会初始化Span对象，而是通过切片Doc对象来创建：
- en: '[PRE35]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Trying to slice an invalid index will raise an `IndexError`. Most indexing
    and slicing rules of Python strings are applicable to Doc slicing as well:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试切片一个无效的索引将引发一个`IndexError`。Python字符串的大多数索引和切片规则也适用于文档切片：
- en: '[PRE36]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'There is one more way to create a Span – we can make a character-level slice
    of a Doc object with `char_span` :'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 创建Span还有另一种方法——我们可以使用`char_span`对Doc对象进行字符级别的切片：
- en: '[PRE37]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The building blocks of a Span object are Token objects. If you iterate over
    a Span object you get Token objects:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: Span对象的基本构建块是Token对象。如果你迭代一个Span对象，你会得到Token对象：
- en: '[PRE38]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'You can think of the Span object as a *junior* Doc object, indeed it''s a view
    of the Doc object it''s created from. Hence most of the features of Doc are applicable
    to Span as well. For instance, `len` is identical:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将Span对象视为一个*初级*文档对象，实际上它是由它创建的文档对象的一个视图。因此，文档的大多数功能也适用于Span。例如，`len`是相同的：
- en: '[PRE39]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Span object also supports indexing. The result of slicing a Span object is
    another Span object:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: Span对象也支持索引。切片Span对象的结果是另一个Span对象：
- en: '[PRE40]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '`char_spans` also works on Span objects. Remember the Span class is a junior
    Doc class, so we can create character-indexed spans on Span objects as well:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '`char_spans`也适用于Span对象。记住，Span类是一个初级文档类，因此我们也可以在Span对象上创建字符索引的跨度：'
- en: '[PRE41]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Just like a Token knows the Doc object it''s created from; Span also knows
    the Doc object it''s created from:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 就像Token知道它创建的文档对象一样；Span也知道它创建的文档对象：
- en: '[PRE42]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We can also locate the `Span` in the original `Doc`:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以在原始的`Doc`中定位`Span`：
- en: '[PRE43]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '`span.start` is the index of the first token of the Span and `span.start_char`
    is the start offset of the Span at character level.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '`span.start`是Span的第一个标记的索引，而`span.start_char`是Span在字符级别上的起始偏移量。'
- en: 'If you want a brand-new Doc object, you can call `span.as_doc()`. It copies
    the data into a new Doc object:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要一个新的Doc对象，你可以调用`span.as_doc()`。它将数据复制到一个新的Doc对象中：
- en: '[PRE44]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '`span.ents`, `span.sent`, `span.text`, and `span.text_wth_ws` are similar to
    their corresponding Doc and Token methods.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '`span.ents`、`span.sent`、`span.text`和`span.text_wth_ws`与它们对应的文档和标记方法类似。'
- en: Dear readers, we have reached the end of an exhaustive section. We'll now go
    through a few more features and methods for more detailed text analysis in the
    next section.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 亲爱的读者，我们已经到达了详尽章节的结尾。接下来，我们将在本节中探讨更多关于详细文本分析的功能和方法。
- en: More spaCy features
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多spaCy功能
- en: 'Most of the NLP development is token and span oriented; that is, it processes
    tags, dependency relations, tokens themselves, and phrases. Most of the time we
    eliminate small words and words without much meaning; we process URLs differently,
    and so on. What we do sometimes depends on the **token shape** (token is a short
    word or token looks like an URL string) or more semantical features (such as the
    token is an article, or the token is a conjunction). In this section, we will
    see these features of tokens with examples. We''ll start with features related
    to the token shape:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数 NLP 开发都是基于标记和跨度进行的；也就是说，它处理标签、依存关系、标记本身和短语。大多数时候我们会消除没有太多意义的短词；我们以不同的方式处理
    URL，等等。我们有时做的事情取决于 **标记形状**（标记是一个短词或标记看起来像 URL 字符串）或更语义的特征（例如，标记是一个冠词，或标记是一个连词）。在本节中，我们将通过示例查看这些标记特征。我们将从与标记形状相关的特征开始：
- en: '[PRE45]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '`token.lower_` returns the token in lowercase. The return value is a Unicode
    string and this feature is equivalent to `token.text.lower()`.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '`token.lower_` 返回标记的小写形式。返回值是一个 Unicode 字符串，这个特征等同于 `token.text.lower()`。'
- en: '`is_lower` and `is_upper` are similar to their Python string method counterparts,
    `islower()` and `isupper()`. `is_lower` returns `True` if all the characters are
    lowercase, while `is_upper` does the same with uppercase:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '`is_lower` 和 `is_upper` 与它们的 Python 字符串方法 `islower()` 和 `isupper()` 类似。`is_lower`
    返回 `True` 如果所有字符都是小写，而 `is_upper` 则对大写字母做同样处理：'
- en: '[PRE46]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '`is_alpha` returns `True` if all the characters of the token are alphabetic
    letters. Examples of nonalphabetic characters are numbers, punctuation, and whitespace:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '`is_alpha` 返回 `True` 如果标记的所有字符都是字母。非字母字符的例子包括数字、标点和空白字符：'
- en: '[PRE47]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '`is_ascii` returns `True` if all the characters of token are ASCII characters.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '`is_ascii` 返回 `True` 如果标记的所有字符都是 ASCII 字符。'
- en: '[PRE48]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '`is_digit` returns `True` if all the characters of the token are numbers:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '`is_digit` 返回 `True` 如果标记的所有字符都是数字：'
- en: '[PRE49]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '`is_punct` returns `True` if the token is a punctuation mark:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '`is_punct` 返回 `True` 如果标记是标点符号：'
- en: '[PRE50]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '`is_left_punct` and `is_right_punct` return `True` if the token is a left punctuation
    mark or right punctuation mark, respectively. A right punctuation mark can be
    any mark that closes a left punctuation mark, such as right brackets, > or ».
    Left punctuation marks are similar, with the left brackets < and « as some examples:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '`is_left_punct` 和 `is_right_punct` 分别返回 `True` 如果标记是左标点符号或右标点符号。右标点符号可以是任何关闭左标点符号的标记，例如右括号、>
    或 ». 左标点符号类似，左括号 < 和 « 是一些例子：'
- en: '[PRE51]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '`is_space` returns `True` if the token is only whitespace characters:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '`is_space` 返回 `True` 如果标记仅包含空白字符：'
- en: '[PRE52]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '`is_bracket` returns `True` for bracket characters:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '`is_bracket` 返回 `True` 对于括号字符：'
- en: '[PRE53]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '`is_quote` returns `True` for quotation marks:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '`is_quote` 返回 `True` 对于引号：'
- en: '[PRE54]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '`is_currency` returns `True` for currency symbols such as `$` and `€` (this
    method was implemented by myself):'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '`is_currency` 返回 `True` 对于货币符号，如 `$` 和 `€`（此方法由我实现）：'
- en: '[PRE55]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '`like_url`, `like_num`, and `like_email` are methods about the token shape
    and return `True` if the token looks like a URL, a number, or an email, respectively.
    These methods are very handy when we want to process social media text and scraped
    web pages:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '`like_url`、`like_num` 和 `like_email` 是关于标记形状的方法，分别返回 `True` 如果标记看起来像 URL、数字或电子邮件。当我们要处理社交媒体文本和抓取的网页时，这些方法非常方便：'
- en: '[PRE56]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '`token.shape_` is an unusual feature – there is nothing similar in other NLP
    libraries. It returns a string that shows a token''s orthographic features. Numbers
    are replaced with `d`, uppercase letters are replaced with `X`, and lowercase
    letters are replaced with `x`. You can use the result string as a feature in your
    machine learning algorithms, and token shapes can be correlated to text sentiment:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '`token.shape_` 是一个不寻常的特征——在其他 NLP 库中没有类似的东西。它返回一个字符串，显示标记的 orthographic 特征。数字被替换为
    `d`，大写字母被替换为 `X`，小写字母被替换为 `x`。您可以将结果字符串用作机器学习算法中的特征，并且标记形状可以与文本情感相关联：'
- en: '[PRE57]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '`is_oov` and `is_stop` are semantic features, as opposed to the preceding shape
    features. `is_oov` returns `True` if the token is **Out Of Vocabulary** (**OOV**),
    that is, not in the Doc object''s vocabulary. OOV words are unknown words to the
    language model, and thus also to the processing pipeline components:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '`is_oov` 和 `is_stop` 是语义特征，与前面的形状特征相对。`is_oov` 返回 `True` 如果标记是 **Out Of Vocabulary**
    (**OOV**)，即不在 Doc 对象的词汇表中。OOV 单词对于语言模型以及处理管道组件都是未知的：'
- en: '[PRE58]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '`is_stop` is a feature that is frequently used by machine learning algorithms.
    Often, we filter words that do not carry much meaning, such as *the*, *a*, *an*,
    *and*, *just*, *with*, and so on. Such words are called stop words. Each language
    has their own stop word list, and you can access English stop words here [https://github.com/explosion/spaCy/blob/master/spacy/lang/en/stop_words.py](https://github.com/explosion/spaCy/blob/master/spacy/lang/en/stop_words.py):'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '`is_stop` 是机器学习算法中经常使用的一个特征。通常，我们会过滤掉那些意义不大的词，例如 *the*，*a*，*an*，*and*，*just*，*with*
    等等。这样的词被称为停用词。每种语言都有自己的停用词列表，你可以在这里访问英语停用词：[https://github.com/explosion/spaCy/blob/master/spacy/lang/en/stop_words.py](https://github.com/explosion/spaCy/blob/master/spacy/lang/en/stop_words.py)'
- en: '[PRE59]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: We have exhausted the list of spaCy's syntactic, semantic, and orthographic
    features. Unsurprisingly, many methods focused on the Token object as a token
    is the syntactic unit of a text.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经详尽地列出了 spaCy 的句法、语义和正字法特性。不出所料，许多方法都集中在 Token 对象上，因为分词是文本的句法单位。
- en: Summary
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'We have now reached the end of an exhaustive chapter of spaCy core operations
    and the basic features of spaCy. This chapter gave you a comprehensive picture
    of spaCy library classes and methods. We made a deep dive into language processing
    pipelining and learned about pipeline components. We also covered a basic yet
    important syntactic task: tokenization. We continued with the linguistic concept
    of lemmatization and you learned a real-world application of a spaCy feature.
    We explored spaCy container classes in detail and finalized the chapter with precise
    and useful spaCy features. At this point, you have a good grasp of spaCy language
    pipelining and you are confident about accomplishing bigger tasks.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经到达了 spaCy 核心操作和基本特性的详尽章节的结尾。这一章节为你提供了 spaCy 库类和方法的全面概述。我们深入探讨了语言处理管道，并了解了管道组件。我们还覆盖了一个基本但重要的句法任务：分词。我们继续探讨了词形还原这一语言概念，并学习了
    spaCy 功能的一个实际应用。我们详细探讨了 spaCy 容器类，并以精确且实用的 spaCy 特性结束了这一章节。到目前为止，你对 spaCy 语言管道有了很好的掌握，并且对完成更复杂的任务充满信心。
- en: 'In the next chapter, we will dive into spaCy''s full linguistic power. You''ll
    discover linguistic features including spaCy''s most used features: the POS tagger,
    dependency parser, named entities, and entity linking.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入挖掘 spaCy 的全部语言能力。你将发现包括 spaCy 最常用的功能：词性标注器、依存句法分析器、命名实体和实体链接在内的语言特性。
