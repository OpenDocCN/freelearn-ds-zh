- en: 'Chapter 2: Core Operations with spaCy'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you will learn the core operations with spaCy, such as creating
    a language pipeline, tokenizing the text, and breaking the text into its sentences.
  prefs: []
  type: TYPE_NORMAL
- en: First, you'll learn what a language processing pipeline is and the pipeline
    components. We'll continue with general spaCy conventions – important classes
    and class organization – to help you to better understand spaCy library organization
    and develop a solid understanding of the library itself.
  prefs: []
  type: TYPE_NORMAL
- en: You will then learn about the first pipeline component – **Tokenizer**. You'll
    also learn about an important linguistic concept – **lemmatization** – along with
    its applications in **natural language understanding** (**NLU**). Following that,
    we will cover **container classes** and **spaCy data structures** in detail. We
    will finish the chapter with useful spaCy features that you'll use in everyday
    NLP development.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''re going to cover the following main topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Overview of spaCy conventions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing tokenization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding lemmatization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: spaCy container objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More spaCy features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The chapter code can be found at the book''s GitHub repository: [https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter02](https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter02)'
  prefs: []
  type: TYPE_NORMAL
- en: Overview of spaCy conventions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Every NLP application consists of several steps of processing the text. As you
    can see in the first chapter, we have always created instances called `nlp` and
    `doc`. But what did we do exactly?
  prefs: []
  type: TYPE_NORMAL
- en: 'When we call `nlp` on our text, spaCy applies some processing steps. The first
    step is tokenization to produce a `Doc` object. The `Doc` object is then processed
    further with a `Doc` and then passes it to the next component:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1 – A high-level view of the processing pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_02_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.1 – A high-level view of the processing pipeline
  prefs: []
  type: TYPE_NORMAL
- en: 'A spaCy pipeline object is created when we load a language model. We load an
    English model and initialize a pipeline in the following code segment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'What happened exactly in the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We started by importing `spaCy`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the second line, `spacy.load()` returned a `Language` class instance, `nlp`.
    The `Language` class is *the text processing pipeline*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After that, we applied `nlp` on the sample sentence `I went there` and got a
    `Doc` class instance, `doc`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The `Language` class applies all of the preceding pipeline steps to your input
    sentence behind the scenes. After applying `nlp` to the sentence, the `Doc` object
    contains tokens that are tagged, lemmatized, and marked as entities if the token
    is an entity (we will go into detail about what are those and how it''s done later).
    Each pipeline component has a well-defined task:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2 – Pipeline components and tasks'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_02_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.2 – Pipeline components and tasks
  prefs: []
  type: TYPE_NORMAL
- en: The spaCy language processing pipeline always *depends on the statistical model*
    and its capabilities. This is why we always load a language model with `spacy.load()`
    as the first step in our code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each component corresponds to a `spaCy` class. `spaCy` classes have self-explanatory
    names such as `Language` and `Doc` classes – let''s see all of the processing
    pipeline classes and their duties:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3 – spaCy processing pipeline classes'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_02_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.3 – spaCy processing pipeline classes
  prefs: []
  type: TYPE_NORMAL
- en: Don't be intimated by the number of classes; each class has unique features
    to help you process your text better.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are more data structures to represent text data and language data. Container
    classes such as Doc hold information about sentences, words, and the text. There
    are also container classes other than Doc:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4 – spaCy container classes'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_02_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.4 – spaCy container classes
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, spaCy provides helper classes for vectors, language vocabulary, and
    annotations. We''ll see the `Vocab` class often in this book. `Vocab` represents
    a language''s vocabulary. Vocab contains all the words of the language model we
    loaded:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.5 – spaCy helper classes'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_02_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.5 – spaCy helper classes
  prefs: []
  type: TYPE_NORMAL
- en: 'The spaCy library''s backbone data structures are `Doc` and `Vocab`. The `Doc`
    object abstracts the text by owning the sequence of tokens and all their properties.
    The `Vocab` object provides a centralized set of strings and lexical attributes
    to all the other classes. This way spaCy avoids storing multiple copies of linguistic
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.6 – spaCy architecture'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_02_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.6 – spaCy architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'You can divide the objects composing the preceding spaCy architecture into
    two: **containers** and **processing pipeline components**. In this chapter, we''ll
    first learn about two basic components, **Tokenizer** and **Lemmatizer**, then
    we''ll explore **Container** objects further.'
  prefs: []
  type: TYPE_NORMAL
- en: spaCy does all these operations for us behind the scenes, allowing us to concentrate
    on our own application's development. With this level of abstraction, using spaCy
    for NLP application development is no coincidence. Let's start with the `Tokenizer`
    class and see what it offers for us; then we will explore all the container classes
    one by one throughout the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing tokenization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We saw in *Figure 2.1* that the first step in a text processing pipeline is
    tokenization. Tokenization is always the first operation because all the other
    operations require the tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tokenization simply means splitting the sentence into its tokens. A **token**
    is a unit of semantics. You can think of a token as the smallest meaningful part
    of a piece of text. Tokens can be words, numbers, punctuation, currency symbols,
    and any other meaningful symbols that are the building blocks of a sentence. The
    following are examples of tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '`USA`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`N.Y.`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`city`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`33`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`3rd`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`!`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`…`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`?`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`''s`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Input to the spaCy tokenizer is a Unicode text and the result is a `Doc` object.
    The following code shows the tokenization process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is what we just did:'
  prefs: []
  type: TYPE_NORMAL
- en: We start by importing `spaCy`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then we loaded the English language model via the `en` shortcut to create an
    instance of the `nlp` `Language` class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, we apply the `nlp` object to the input sentence to create a `Doc` object,
    `doc`. A `Doc` object is a container for a sequence of `Token` objects. spaCy
    generates the `Token` objects implicitly when we created the `Doc` object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we print a list of the preceding sentence's tokens.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'That''s it, we made the tokenization with just three lines of code. You can
    visualize the tokenization with indexing as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.7 – Tokenization of “I own a ginger cat.”'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_02_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.7 – Tokenization of "I own a ginger cat."
  prefs: []
  type: TYPE_NORMAL
- en: 'As the examples suggest, tokenization can indeed be tricky. There are many
    aspects we should pay attention to: punctuation, whitespaces, numbers, and so
    on. Splitting from the whitespaces with `text.split(" ")` might be tempting and
    looks like it is working for the example sentence *I own a ginger cat*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'How about the sentence `"It''s been a crazy week!!!"`? If we make a `split("
    ")` the resulting tokens would be `It''s`, `been`, `a`, `crazy`, `week!!!`, which
    is not what you want. First of all, `It''s` is not one token, it''s two tokens:
    `it` and `''s`. `week!!!` is not a valid token as the punctuation is not split
    correctly. Moreover, `!!!` should be tokenized per symbol and should generate
    three *!*''s. (This may not look like an important detail, but trust me, it is
    important for *sentiment analysis*. We''ll cover sentiment analysis in [*Chapter
    8*](B16570_08_Final_JM_ePub.xhtml#_idTextAnchor137)*, Text Classification with
    spaCy*.) Let''s see what spaCy tokenizer has generated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This time the sentence is split as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.8 – Tokenization of apostrophe and punctuations marks'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16570_02_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.8 – Tokenization of apostrophe and punctuations marks
  prefs: []
  type: TYPE_NORMAL
- en: 'How does spaCy know where to split the sentence? Unlike other parts of the
    pipeline, the tokenizer doesn''t need a statistical model. Tokenization is based
    on language-specific rules. You can see examples the language specified data here:
    [https://github.com/explosion/spaCy/tree/master/spacy/lang](https://github.com/explosion/spaCy/tree/master/spacy/lang).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Tokenizer exceptions define rules for exceptions, such as `it''s` , `don''t`
    , `won''t`, abbreviations, and so on. If you look at the rules for English: [https://github.com/explosion/spaCy/blob/master/spacy/lang/en/tokenizer_exceptions.py](https://github.com/explosion/spaCy/blob/master/spacy/lang/en/tokenizer_exceptions.py),
    you will see that rules look like `{ORTH: "n''t", LEMMA: "not"}`, which describes
    the splitting rule for `n''t` to the tokenizer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The prefixes, suffixes, and infixes mostly describe how to deal with punctuation
    – for example, we split at a period if it is at the end of the sentence, otherwise,
    most probably it''s part of an abbreviation such as N.Y. and we shouldn''t touch
    it. Here, `ORTH` means the text and `LEMMA` means the base word form without any
    inflections. The following example shows you the execution of the spaCy tokenization
    algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.9 – spaCy performing tokenization with exception rules (image taken
    from spaCy tokenization guidelines (https://spacy.io/usage/linguistic-features#tokenization))](img/B16570_02_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.9 – spaCy performing tokenization with exception rules (image taken
    from spaCy tokenization guidelines (https://spacy.io/usage/linguistic-features#tokenization))
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization rules depend on the grammatical rules of the individual language.
    Punctuation rules such as splitting periods, commas, or exclamation marks are
    more or less similar for many languages; however, some rules are specific to the
    individual language, such as abbreviation words and apostrophe usage. spaCy supports
    each language having its own specific rules by allowing hand-coded data and rules,
    as each language has its own subclass.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: spaCy provides non-destructive tokenization, which means that we always will
    be able to recover the original text from the tokens. Whitespace and punctuation
    information is preserved during tokenization, so the input text is preserved as
    it is.
  prefs: []
  type: TYPE_NORMAL
- en: Every `Language` object contains a `Tokenizer` object. The `Tokenizer` class
    is the class that performs the tokenization. You don't often call this class directly
    when you create a `Doc` class instance, while Tokenizer class acts behind the
    scenes. When we want to customize the tokenization, we need to interact with this
    class. Let's see how it is done.
  prefs: []
  type: TYPE_NORMAL
- en: Customizing the tokenizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When we work with a specific domain such as medicine, insurance, or finance,
    we often come across words, abbreviations, and entities that needs special attention.
    Most domains that you''ll process have characteristic words and phrases that need
    custom tokenization rules. Here''s how to add a special case rule to an existing
    `Tokenizer` class instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is what we did:'
  prefs: []
  type: TYPE_NORMAL
- en: We again started by importing `spacy`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we imported the `ORTH` symbol, which means orthography; that is, text.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We continued with creating a `Language` class object, `nlp`, and created a Doc
    object, `doc`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We defined a special case, where the word `lemme` should tokenize as two tokens,
    `lem` and `me`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We added the rule to the `nlp` object's tokenizer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The last line exhibits how the fresh rule works.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When we define custom rules, punctuation splitting rules will still apply.
    Our special case will be recognized as a result, even if it''s surrounded by punctuation.
    The tokenizer will divide punctuation step by step, and apply the same process
    to the remaining substring:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'If you define a special case rule with punctuation, the special case rule will
    take precedence over the punctuation splitting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Pro tip
  prefs: []
  type: TYPE_NORMAL
- en: Modify the tokenizer by adding new rules only if you really need to. Trust me,
    you can get quite unexpected results with custom rules. One of the cases where
    you really need it is when working with Twitter text, which is usually full of
    hashtags and special symbols. If you have social media text, first feed some sentences
    into the spaCy NLP pipeline and see how the tokenization works out.
  prefs: []
  type: TYPE_NORMAL
- en: Debugging the tokenizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The spaCy library has a tool for debugging: `nlp.tokenizer.explain(sentence`).
    It returns (`tokenizer rule/pattern, token`) **tuples** to help us understand
    what happened exactly during the tokenization. Let''s see an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we imported `spacy` and created a `Language` class instance,
    `nlp`, as usual. Then we created a `Doc` class instance with the sentence `Let's
    go!`. After that, we asked the `Tokenizer` class instance, `tokenizer`, of `nlp`
    for an explanation of the tokenization of this sentence. `nlp.tokenizer.explain()`
    explained the rules that the tokenizer used one by one.
  prefs: []
  type: TYPE_NORMAL
- en: After splitting a sentence into its tokens, it's time to split a text into its
    sentences.
  prefs: []
  type: TYPE_NORMAL
- en: Sentence segmentation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We saw that breaking a sentence into its tokens is not a straightforward task
    at all. How about breaking a text into sentences? It's indeed a bit more complicated
    to mark where a sentence starts and ends due to the same reasons of punctuation,
    abbreviations, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'A Doc object''s sentences are available via the `doc.sents` property:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Determining sentence boundaries is a more complicated task than tokenization.
    As a result, spaCy uses the dependency parser to perform sentence segmentation.
    This is a unique feature of spaCy – no other library puts such a sophisticated
    idea into practice. The results are very accurate in general, unless you process
    text of a very specific genre, such as from the conversation domain, or social
    media text.
  prefs: []
  type: TYPE_NORMAL
- en: Now we know how to segment a text into sentences and tokenize the sentences.
    We're ready to process the tokens one by one. Let's start with lemmatization,
    a commonly used operation in semantics including sentiment analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding lemmatization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A **lemma** is the base form of a token. You can think of a lemma as the form
    in which the token appears in a dictionary. For instance, the lemma of *eating*
    is *eat*; the lemma of *eats* is *eat*; *ate* similarly maps to *eat*. Lemmatization
    is the process of reducing the word forms to their lemmas. The following code
    is a quick example of how to do lemmatization with spaCy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: By now, you should be familiar with what the first three lines of the code do.
    Recall that we import the `spacy` library, load an English model using `spacy.load`,
    create a pipeline, and apply the pipeline to the preceding sentence to get a Doc
    object. Here we iterated over tokens to get their text and lemmas.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first line you see `–PRON-`, which doesn''t look like a real token.
    This is a **pronoun lemma**, a special token for lemmas of personal pronouns.
    This is an exception for semantic purposes: the personal pronouns *you*, *I*,
    *me*, *him*, *his*, and so on look different, but in terms of meaning, they''re
    in the same group. spaCy offers this trick for the pronoun lemmas.'
  prefs: []
  type: TYPE_NORMAL
- en: No worries if all of this sounds too abstract – let's see lemmatization in action
    with a real-world example.
  prefs: []
  type: TYPE_NORMAL
- en: Lemmatization in NLU
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Lemmatization is an important step in NLU. We'll go over an example in this
    subsection. Suppose that you design an NLP pipeline for a ticket booking system.
    Your application processes a customer's sentence, extracts necessary information
    from it, and then passes it to the booking API.
  prefs: []
  type: TYPE_NORMAL
- en: 'The NLP pipeline wants to extract the form of the travel (a flight, bus, or
    train), the destination city, and the date. The first thing the application needs
    to verify is the means of travel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We have this list of keywords and we want to recognize the means of travel
    by searching the tokens in the keywords list. The most compact way of doing this
    search is by looking up the token''s lemma. Consider the following customer sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Here, we don't need to include all word forms of the verb *fly* (*fly*, *flying*,
    *flies*, *flew*, and *flown*) in the keywords list and similar for the word `flight`;
    we reduced all possible variants to the base forms – *fly* and *flight*. Don't
    think of English only; languages such as Spanish, German, and Finnish have many
    word forms from a single lemma as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lemmatization also comes in handy when we want to recognize the destination
    city. There are many nicknames available for global cities and the booking API
    can process only the official names. The default tokenizer and lemmatizer won''t
    know the difference between the official name and the nickname. In this case,
    you can add special rules, as we saw in the *Introducing tokenization* section.
    The following code plays a small trick:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We defined a special case for the word `Angeltown` by replacing its lemma with
    the official name `Los Angeles`. Then we added this special case to the `Tokenizer`
    instance. When we print the token lemmas, we see that `Angeltown` maps to `Los
    Angeles` as we wished.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the difference between lemmatization and stemming
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A lemma is the base form of a word and is always a member of the language''s
    vocabulary. The stem does not have to be a valid word at all. For instance, the
    lemma of *improvement* is *improvement*, but the stem is *improv*. You can think
    of the stem as the smallest part of the word that carries the meaning. Compare
    the following examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding word-lemma examples show how lemma is calculated by following
    the grammatical rules of the language. Here, the lemma of a plural form is the
    singular form, and the lemma of a third-person verb is the base form of the verb.
    Let''s compare them to the following examples of word-stem pairs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The first and the most important point to notice in the preceding examples is
    that the lemma does not have to be a valid word in the language. The second point
    is that many words can map to the same stem. Also, words from different grammatical
    categories can map to the same stem; here for instance, the noun *improvement*
    and the verb *improves* both map to *improv*.
  prefs: []
  type: TYPE_NORMAL
- en: Though stems are not valid words, they still carry meaning. That's why stemming
    is commonly used in NLU applications.
  prefs: []
  type: TYPE_NORMAL
- en: Stemming algorithms don't know anything about the grammar of the language. This
    class of algorithms works rather by trimming some common suffixes and prefixes
    from the beginning or end of the word.
  prefs: []
  type: TYPE_NORMAL
- en: Stemming algorithms are rough, they cut the word from head and tail. There are
    several stemming algorithms available for English, including Porter and Lancaster.
    You can play with different stemming algorithms on NLTK's demo page at [https://text-processing.com/demo/stem/](https://text-processing.com/demo/stem/).
  prefs: []
  type: TYPE_NORMAL
- en: Lemmatization, on the other hand, takes the morphological analysis of the words
    into consideration. To do so, it is important to obtain the dictionaries for the
    algorithm to look through in order to link the form back to its lemma.
  prefs: []
  type: TYPE_NORMAL
- en: spaCy provides lemmatization via dictionary lookup and each language has its
    own dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Both stemming and lemmatization have their own advantages. Stemming gives very
    good results if you apply only statistical algorithms to the text, without further
    semantic processing such as pattern lookup, entity extraction, coreference resolution,
    and so on. Also stemming can trim a big corpus to a more moderate size and give
    you a compact representation. If you also use linguistic features in your pipeline
    or make a keyword search, include lemmatization. Lemmatization algorithms are
    accurate but come with a cost in terms of computation.
  prefs: []
  type: TYPE_NORMAL
- en: spaCy container objects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the beginning of this chapter, we saw a list of container objects including
    **Doc**, **Token**, **Span**, and **Lexeme**. We already used Token and Doc in
    our code. In this subsection, we'll see the properties of the container objects
    in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Using container objects, we can access the linguistic properties that spaCy
    assigns to the text. A container object is a logical representation of the text
    units such as a document, a token, or a slice of the document.
  prefs: []
  type: TYPE_NORMAL
- en: 'Container objects in spaCy follow the natural structure of the text: a document
    is composed of sentences and sentences are composed of tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: We most widely use Doc, Token, and Span objects in development, which represent
    a document, a single token, and a phrase, respectively. A container can contain
    other containers, for instance a document contains tokens and spans.
  prefs: []
  type: TYPE_NORMAL
- en: Let's explore each class and its useful properties one by one.
  prefs: []
  type: TYPE_NORMAL
- en: Doc
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We created Doc objects in our code to represent the text, so you might have
    already figured out that Doc represents a text.
  prefs: []
  type: TYPE_NORMAL
- en: 'We already know how to create a Doc object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '`doc.text` returns a Unicode representation of the document text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The building block of a Doc object is Token, hence when you iterate a Doc you
    get Token objects as items:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The same logic applies to indexing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The length of a Doc is the number of tokens it includes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We already saw how to get the text''s sentences. `doc.sents` returns an iterator
    to the list of sentences. Each sentence is a Span object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '`doc.ents` gives named entities of the text. The result is a list of Span objects.
    We''ll see named entities in detail later – for now, think of them as proper nouns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Another syntactic property is `doc.noun_chunks`. It yields the noun phrases
    found in the text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '`doc.lang_` returns the language that `doc` created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'A useful method for serialization is `doc.to_json`. This is how to convert
    a Doc object to JSON:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Pro tip
  prefs: []
  type: TYPE_NORMAL
- en: You might have noticed that we call `doc.lang_`, not `doc.lang`. `doc.lang`
    returns the language ID, whereas `doc.lang_` returns the Unicode string of the
    language, that is, the name of the language. You can see the same convention with
    Token features in the following, for instance, `token.lemma_`, `token.tag_`, and
    `token.pos_`.
  prefs: []
  type: TYPE_NORMAL
- en: The Doc object has very useful properties with which you can understand a sentence's
    syntactic properties and use them in your own applications. Let's move on to the
    Token object and see what it offers.
  prefs: []
  type: TYPE_NORMAL
- en: Token
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A Token object represents a word. Token objects are the building blocks of
    Doc and Span objects. In this section, we will cover the following properties
    of the `Token` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '`token.text`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token.text_with_ws`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token.i`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token.idx`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token.doc`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token.sent`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token.is_sent_start`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`token.ent_type`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We usually don''t construct a Token object directly, rather we construct a
    Doc object then access its tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '`token.text` is similar to `doc.text` and provides the underlying Unicode string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '`token.text_with_ws` is a similar property. It provides the text with a trailing
    whitespace if present in the `doc`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Finding the length of a token is similar to finding the length of a Python
    string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '`token.i` gives the index of the token in `doc`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '`token.idx` provides the token''s character offset (the character position)
    in `doc`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also access the `doc` that created the `token` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Getting the sentence that the `token` belongs to is done in a similar way to
    accessing the `doc` that created the `token`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '`token.is_sent_start` is another useful property; it returns a Boolean indicating
    whether the token starts a sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'These are the basic properties of the Token object that you''ll use every day.
    There is another set of properties that are more related to syntax and semantics.
    We already saw how to calculate the token lemma in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'You already learned that `doc.ents` gives the named entities of the document.
    If you want to learn what sort of entity the token is, use `token.ent_type_`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Two syntactic features related to POS tagging are `token.pos_` and `token.tag`.
    We'll learn what they are and how to use them in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Another set of syntactic features comes from the dependency parser. These features
    are `dep_`, `head_`, `conj_`, `lefts_`, `rights_`, `left_edge_`, and `right_edge_`.
    We'll cover them in the next chapter as well.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: It is totally normal if you don't remember all the features afterward. If you
    don't remember the name of a feature, you can always do `dir(token)` or `dir(doc)`.
    Calling `dir()` will print all the features and methods available on the object.
  prefs: []
  type: TYPE_NORMAL
- en: The Token object has a rich set of features, enabling us to process the text
    from head to toe. Let's move on to the Span object and see what it offers for
    us.
  prefs: []
  type: TYPE_NORMAL
- en: Span
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Span objects represent phrases or segments of the text. Technically, a Span
    has to be a contiguous sequence of tokens. We usually don''t initialize Span objects,
    rather we slice a Doc object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Trying to slice an invalid index will raise an `IndexError`. Most indexing
    and slicing rules of Python strings are applicable to Doc slicing as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'There is one more way to create a Span – we can make a character-level slice
    of a Doc object with `char_span` :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The building blocks of a Span object are Token objects. If you iterate over
    a Span object you get Token objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'You can think of the Span object as a *junior* Doc object, indeed it''s a view
    of the Doc object it''s created from. Hence most of the features of Doc are applicable
    to Span as well. For instance, `len` is identical:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Span object also supports indexing. The result of slicing a Span object is
    another Span object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '`char_spans` also works on Span objects. Remember the Span class is a junior
    Doc class, so we can create character-indexed spans on Span objects as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Just like a Token knows the Doc object it''s created from; Span also knows
    the Doc object it''s created from:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also locate the `Span` in the original `Doc`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '`span.start` is the index of the first token of the Span and `span.start_char`
    is the start offset of the Span at character level.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want a brand-new Doc object, you can call `span.as_doc()`. It copies
    the data into a new Doc object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '`span.ents`, `span.sent`, `span.text`, and `span.text_wth_ws` are similar to
    their corresponding Doc and Token methods.'
  prefs: []
  type: TYPE_NORMAL
- en: Dear readers, we have reached the end of an exhaustive section. We'll now go
    through a few more features and methods for more detailed text analysis in the
    next section.
  prefs: []
  type: TYPE_NORMAL
- en: More spaCy features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Most of the NLP development is token and span oriented; that is, it processes
    tags, dependency relations, tokens themselves, and phrases. Most of the time we
    eliminate small words and words without much meaning; we process URLs differently,
    and so on. What we do sometimes depends on the **token shape** (token is a short
    word or token looks like an URL string) or more semantical features (such as the
    token is an article, or the token is a conjunction). In this section, we will
    see these features of tokens with examples. We''ll start with features related
    to the token shape:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '`token.lower_` returns the token in lowercase. The return value is a Unicode
    string and this feature is equivalent to `token.text.lower()`.'
  prefs: []
  type: TYPE_NORMAL
- en: '`is_lower` and `is_upper` are similar to their Python string method counterparts,
    `islower()` and `isupper()`. `is_lower` returns `True` if all the characters are
    lowercase, while `is_upper` does the same with uppercase:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '`is_alpha` returns `True` if all the characters of the token are alphabetic
    letters. Examples of nonalphabetic characters are numbers, punctuation, and whitespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '`is_ascii` returns `True` if all the characters of token are ASCII characters.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '`is_digit` returns `True` if all the characters of the token are numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '`is_punct` returns `True` if the token is a punctuation mark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '`is_left_punct` and `is_right_punct` return `True` if the token is a left punctuation
    mark or right punctuation mark, respectively. A right punctuation mark can be
    any mark that closes a left punctuation mark, such as right brackets, > or ».
    Left punctuation marks are similar, with the left brackets < and « as some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '`is_space` returns `True` if the token is only whitespace characters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '`is_bracket` returns `True` for bracket characters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '`is_quote` returns `True` for quotation marks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '`is_currency` returns `True` for currency symbols such as `$` and `€` (this
    method was implemented by myself):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '`like_url`, `like_num`, and `like_email` are methods about the token shape
    and return `True` if the token looks like a URL, a number, or an email, respectively.
    These methods are very handy when we want to process social media text and scraped
    web pages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '`token.shape_` is an unusual feature – there is nothing similar in other NLP
    libraries. It returns a string that shows a token''s orthographic features. Numbers
    are replaced with `d`, uppercase letters are replaced with `X`, and lowercase
    letters are replaced with `x`. You can use the result string as a feature in your
    machine learning algorithms, and token shapes can be correlated to text sentiment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '`is_oov` and `is_stop` are semantic features, as opposed to the preceding shape
    features. `is_oov` returns `True` if the token is **Out Of Vocabulary** (**OOV**),
    that is, not in the Doc object''s vocabulary. OOV words are unknown words to the
    language model, and thus also to the processing pipeline components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '`is_stop` is a feature that is frequently used by machine learning algorithms.
    Often, we filter words that do not carry much meaning, such as *the*, *a*, *an*,
    *and*, *just*, *with*, and so on. Such words are called stop words. Each language
    has their own stop word list, and you can access English stop words here [https://github.com/explosion/spaCy/blob/master/spacy/lang/en/stop_words.py](https://github.com/explosion/spaCy/blob/master/spacy/lang/en/stop_words.py):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: We have exhausted the list of spaCy's syntactic, semantic, and orthographic
    features. Unsurprisingly, many methods focused on the Token object as a token
    is the syntactic unit of a text.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have now reached the end of an exhaustive chapter of spaCy core operations
    and the basic features of spaCy. This chapter gave you a comprehensive picture
    of spaCy library classes and methods. We made a deep dive into language processing
    pipelining and learned about pipeline components. We also covered a basic yet
    important syntactic task: tokenization. We continued with the linguistic concept
    of lemmatization and you learned a real-world application of a spaCy feature.
    We explored spaCy container classes in detail and finalized the chapter with precise
    and useful spaCy features. At this point, you have a good grasp of spaCy language
    pipelining and you are confident about accomplishing bigger tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will dive into spaCy''s full linguistic power. You''ll
    discover linguistic features including spaCy''s most used features: the POS tagger,
    dependency parser, named entities, and entity linking.'
  prefs: []
  type: TYPE_NORMAL
