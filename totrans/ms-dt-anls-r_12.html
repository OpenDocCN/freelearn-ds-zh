<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch12"/>Chapter 12. Analyzing Time-series</h1></div></div></div><p>A time-series<a class="indexterm" id="id865"/> is a sequence of data points ordered in time, often used in economics or, for example, in social sciences. The great advantage of collecting data over a long period of time compared to cross-sectional observations is that we can analyze the collected values of the exact same object over time instead of comparing different observations.</p><p>This special characteristic of the data requires new methods and data structures for time-series analysis. We will cover these in this chapter:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">First, we learn how to load or transform observations into time-series objects</li><li class="listitem" style="list-style-type: disc">Then we visualize them and try to improve the plots by smoothing and filtering the observations</li><li class="listitem" style="list-style-type: disc">Besides seasonal decomposition, we introduce forecasting methods based on time-series models, and we also cover methods to identify outliers, extreme values, and anomalies in time-series</li></ul></div><div><div><div><div><h1 class="title"><a id="ch12lvl1sec79"/>Creating time-series objects</h1></div></div></div><p>Most tutorials on<a class="indexterm" id="id866"/> time-series analysis start with the <code class="literal">ts</code> function of the <code class="literal">stats</code> package, which can create time-series objects in a very straightforward way. Simply pass a vector or matrix of numeric values (time-series analysis mostly deals with continuous variables), specify the frequency of your data, and it's all set!</p><p>The frequency refers to the natural time-span of the data. Thus, for monthly data, you should set it to 12, 4 for quarterly and 365 or 7 for daily data, depending on the most characteristic seasonality of the events. For example, if your data has a significant weekly seasonality, which is pretty usual in social sciences, it should be 7, but if the calendar date is the main differentiator, such as with weather data, it should be 365.</p><p>In the forthcoming pages, let's use daily summary statistics from the <code class="literal">hflights</code> dataset. First let's load the related dataset and transform it to <code class="literal">data.table</code> for easy aggregation. We also have to<a class="indexterm" id="id867"/> create a date variable from the provided <code class="literal">Year</code>, <code class="literal">Month</code>, and <code class="literal">DayofMonth</code> columns:</p><div><pre class="programlisting">
<strong>&gt; library(hflights)</strong>
<strong>&gt; library(data.table)</strong>
<strong>&gt; dt &lt;- data.table(hflights)</strong>
<strong>&gt; dt[, date := ISOdate(Year, Month, DayofMonth)]</strong>
</pre></div><p>Now let's compute the number of flights and the overall sum of arrival delays, number of cancelled flights and the average distance of the related flights for each day in 2011:</p><div><pre class="programlisting">
<strong>&gt; daily &lt;- dt[, list(</strong>
<strong>+     N         = .N,</strong>
<strong>+     Delays    = sum(ArrDelay, na.rm = TRUE),</strong>
<strong>+     Cancelled = sum(Cancelled),</strong>
<strong>+     Distance  = mean(Distance)</strong>
<strong>+ ), by = date]</strong>
<strong>&gt; str(daily)</strong>
<strong>Classes 'data.table' and 'data.frame':	365 obs. of  5 variables:</strong>
<strong> $ date     : POSIXct, format: "2011-01-01 12:00:00" ...</strong>
<strong> $ N        : int  552 678 702 583 590 660 661 500 602 659 ...</strong>
<strong> $ Delays   : int  5507 7010 4221 4631 2441 3994 2571 1532 ...</strong>
<strong> $ Cancelled: int  4 11 2 2 3 0 2 1 21 38 ...</strong>
<strong> $ Distance : num  827 787 772 755 760 ...</strong>
<strong> - attr(*, ".internal.selfref")=&lt;externalptr&gt;</strong>
</pre></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch12lvl1sec80"/>Visualizing time-series</h1></div></div></div><p>This is in a very <a class="indexterm" id="id868"/>familiar data structure: 365 rows for each day in 2011 and five columns to store the four metrics for the dates stored in the first variable. Let's transform that to a time-series object and plot it right away:</p><div><pre class="programlisting">
<strong>&gt; plot(ts(daily))</strong>
</pre></div><div><img alt="Visualizing time-series" src="img/2028OS_12_01.jpg"/></div><p>It was easy, right? We<a class="indexterm" id="id869"/> have just plotted several independent time-series on a line chart. But what's shown on the first plot? The <em>x</em> axis is indexed from 1 to 365 because <code class="literal">ts</code> did not automatically identify that the first column stores our dates. On the other hand, we find the date transformed to timestamps on the <em>y</em> axis. Shouldn't the points form a linear line?</p><p>This is one of the beauties of data visualization: a simple plot revealed a major issue with our data. It seems we have to sort the data by date:</p><div><pre class="programlisting">
<strong>&gt; setorder(daily, date)</strong>
<strong>&gt; plot(ts(daily))</strong>
</pre></div><div><img alt="Visualizing time-series" src="img/2028OS_12_02.jpg"/></div><p>Much better! Now that the<a class="indexterm" id="id870"/> values are in the right order, we can focus on the actual time-series data one by one at a time. First let's see how the number of flights looked from the first day of 2011 with a daily frequency:</p><div><pre class="programlisting">
<strong>&gt; plot(ts(daily$N, start = 2011, frequency = 365),</strong>
<strong>+      main = 'Number of flights from Houston in 2011')</strong>
</pre></div><div><img alt="Visualizing time-series" src="img/2028OS_12_03.jpg"/></div></div>
<div><div><div><div><h1 class="title"><a id="ch12lvl1sec81"/>Seasonal decomposition</h1></div></div></div><p>Well, it looks like the number of <a class="indexterm" id="id871"/>flights fluctuates a lot on weekdays, which is indeed a dominant characteristic of human-related activities. Let's verify that by identifying and removing the weekly seasonality by decomposing this time-series into the seasonal, trend, and random components with moving averages.</p><p>Although this can be done manually by utilizing the <code class="literal">diff</code> and <code class="literal">lag</code> functions, there's a much more straightforward way to do so with the <code class="literal">decompose</code> function from the <code class="literal">stats</code> package:</p><div><pre class="programlisting">
<strong>&gt; plot(decompose(ts(daily$N, frequency = 7)))</strong>
</pre></div><div><img alt="Seasonal decomposition" src="img/2028OS_12_04.jpg"/></div><p>Removing the spikes in the means of weekly seasonality reveals the overall trend of the number of flights in 2011. As<a class="indexterm" id="id872"/> the <em>x</em> axis shows the number of weeks since January 1 (based on the frequency being 7), the peak interval between 25 and 35 refers to the summertime, and the lowest number of flights happened on the 46th week – probably due to Thanksgiving Day.</p><p>But the weekly seasonality is probably more interesting. Well, it's pretty hard to spot anything on the preceding plot as the very same 7-day repetition can be seen 52 times on the seasonal plot. So, instead, let's extract that data and show it in a table with the appropriate headers:</p><div><pre class="programlisting">
<strong>&gt; setNames(decompose(ts(daily$N, frequency = 7))$figure,</strong>
<strong>+         weekdays(daily$date[1:7]))</strong>
<strong>   Saturday      Sunday      Monday     Tuesday   Wednesday </strong>
<strong>-102.171776   -8.051328   36.595731  -14.928941   -9.483886 </strong>
<strong>   Thursday      Friday </strong>
<strong>  48.335226   49.704974</strong>
</pre></div><p>So the seasonal effects (the preceding numbers representing the relative distance from the average) suggest that the greatest number of flights happened on Monday and the last two weekdays, while there is only a relatively small number of flights on Saturdays.</p><p>Unfortunately, we cannot decompose the yearly seasonal component of this time-series, as we have data only for one year, and we need data for at least two time periods for the given frequency:</p><div><pre class="programlisting">
<strong>&gt; decompose(ts(daily$N, frequency = 365))</strong>
<strong>Error in decompose(ts(daily$N, frequency = 365)) : </strong>
<strong>  time series has no or less than 2 periods</strong>
</pre></div><p>For more advanced <a class="indexterm" id="id873"/>seasonal decomposition, see the <code class="literal">stl</code> function of the <a class="indexterm" id="id874"/>
<code class="literal">stats</code> package, which uses polynomial regression models on the time-series data. The next section will cover some of this background.</p></div>
<div><div><div><div><h1 class="title"><a id="ch12lvl1sec82"/>Holt-Winters filtering</h1></div></div></div><p>We can similarly remove the <a class="indexterm" id="id875"/>seasonal effects of a time-series by Holt-Winters filtering. Setting the <code class="literal">beta</code> parameter of the <code class="literal">HoltWinters</code> function to <code class="literal">FALSE</code> will result in a model with exponential smoothing practically suppressing all the outliers; setting the <code class="literal">gamma</code> argument to <code class="literal">FALSE</code> will result in a non-seasonal model. A quick example:</p><div><pre class="programlisting">
<strong>&gt; nts &lt;- ts(daily$N, frequency = 7)</strong>
<strong>&gt; fit &lt;- HoltWinters(nts, beta = FALSE, gamma = FALSE)</strong>
<strong>&gt; plot(fit)</strong>
</pre></div><div><img alt="Holt-Winters filtering" src="img/2028OS_12_05.jpg"/></div><p>The red line represents the <a class="indexterm" id="id876"/>filtered time-series. We can also fit a double or triple exponential model on the time-series by enabling the <code class="literal">beta</code> and <code class="literal">gamma</code> parameters, resulting in a far better fit:</p><div><pre class="programlisting">
<strong>&gt; fit &lt;- HoltWinters(nts)</strong>
<strong>&gt; plot(fit)</strong>
</pre></div><div><img alt="Holt-Winters filtering" src="img/2028OS_12_06.jpg"/></div><p>As this model provides extremely similar values compared to our original data, it can be used to predict future values as well. For this end, we will use the <a class="indexterm" id="id877"/>
<code class="literal">forecast</code> package. By default, the <code class="literal">forecast</code> function returns a prediction for the forthcoming 2*frequency values:</p><div><pre class="programlisting">
<strong>&gt; library(forecast)</strong>
<strong>&gt; forecast(fit)</strong>
<strong>         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95</strong>
<strong>53.14286       634.0968 595.4360 672.7577 574.9702 693.2235</strong>
<strong>53.28571       673.6352 634.5419 712.7286 613.8471 733.4233</strong>
<strong>53.42857       628.2702 588.7000 667.8404 567.7528 688.7876</strong>
<strong>53.57143       642.5894 602.4969 682.6820 581.2732 703.9057</strong>
<strong>53.71429       678.2900 637.6288 718.9511 616.1041 740.4758</strong>
<strong>53.85714       685.8615 644.5848 727.1383 622.7342 748.9889</strong>
<strong>54.00000       541.2299 499.2901 583.1697 477.0886 605.3712</strong>
<strong>54.14286       641.8039 598.0215 685.5863 574.8445 708.7633</strong>
<strong>54.28571       681.3423 636.8206 725.8639 613.2523 749.4323</strong>
<strong>54.42857       635.9772 590.6691 681.2854 566.6844 705.2701</strong>
<strong>54.57143       650.2965 604.1547 696.4382 579.7288 720.8642</strong>
<strong>54.71429       685.9970 638.9748 733.0192 614.0827 757.9113</strong>
<strong>54.85714       693.5686 645.6194 741.5178 620.2366 766.9005</strong>
<strong>55.00000       548.9369 500.0147 597.8592 474.1169 623.7570</strong>
</pre></div><p>These are estimates for <a class="indexterm" id="id878"/>the first two weeks of 2012, where (besides the exact point predictions) we get the confidence intervals as well. Probably it's more meaningful at this time to visualize these predictions and confidence intervals:</p><div><pre class="programlisting">
<strong>&gt; plot(forecast(HoltWinters(nts), 31))</strong>
</pre></div><div><img alt="Holt-Winters filtering" src="img/2028OS_12_07.jpg"/></div><p>The blue points shows the <a class="indexterm" id="id879"/>estimates for the 31 future time periods and the gray area around that covers the confidence intervals returned by the <code class="literal">forecast</code> function.</p></div>
<div><div><div><div><h1 class="title"><a id="ch12lvl1sec83"/>Autoregressive Integrated Moving Average models</h1></div></div></div><p>We can achieve similar results with <strong>Autoregressive Integrated Moving Average</strong> (<strong>ARIMA</strong>) models. To predict<a class="indexterm" id="id880"/> future values of a time-series, we usually have to <em>stationarize</em> it first, which means that the data has a constant mean, variance, and autocorrelation over time. In the past two sections, we used seasonal decomposition and the Holt-Winters filter to achieve this. Now let's see how the generalized version of the <strong>Autoregressive Moving Average</strong> (<strong>ARMA</strong>) model<a class="indexterm" id="id881"/> can help with this data transformation.</p><p>
<em>ARIMA(p, d, q)</em> actually includes three models with three non-negative integer parameters:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><em>p</em> refers to the autoregressive part of the model</li><li class="listitem" style="list-style-type: disc"><em>d</em> refers to the integrated part</li><li class="listitem" style="list-style-type: disc"><em>q</em> refers to the moving average parts</li></ul></div><p>As ARIMA also includes an integrated (differencing) part over ARMA, it can deal with non-stationary time-series as well, as they naturally become stationary after differencing—in other words, when the <em>d</em> parameter is larger than zero.</p><p>Traditionally, choosing the best ARIMA model for a time-series is required to build multiple models with a variety of parameters and compare model fits. On the other hand, the <code class="literal">forecast</code> package <a class="indexterm" id="id882"/>comes with a very useful function that can select the best fitting ARIMA model for a time-series by running unit root tests and minimizing the<a class="indexterm" id="id883"/> <strong>maximum-likelihood</strong> (<strong>ML</strong>) and the <strong>Akaike Information Criterion</strong> (<strong>AIC</strong>) <a class="indexterm" id="id884"/>of the models:</p><div><pre class="programlisting">
<strong>&gt; auto.arima(nts)</strong>
<strong>Series: ts </strong>
<strong>ARIMA(3,0,0)(2,0,0)[7] with non-zero mean </strong>

<strong>Coefficients:</strong>
<strong>         ar1      ar2     ar3    sar1    sar2  intercept</strong>
<strong>      0.3205  -0.1199  0.3098  0.2221  0.1637   621.8188</strong>
<strong>s.e.  0.0506   0.0538  0.0538  0.0543  0.0540     8.7260</strong>

<strong>sigma^2 estimated as 2626:  log likelihood=-1955.45</strong>
<strong>AIC=3924.9   AICc=3925.21   BIC=3952.2</strong>
</pre></div><p>It seems that the <em>AR(3)</em> model has the highest AIC with <em>AR(2)</em> seasonal effects. But checking the manual of <code class="literal">auto.arima</code> reveals that the information criteria used for the model selection were approximated due to the large number (more than 100) of observations. Re-running the algorithm and disabling approximation returns a different model:</p><div><pre class="programlisting">
<strong>&gt; auto.arima(nts, approximation = FALSE)</strong>
<strong>Series: ts </strong>
<strong>ARIMA(0,0,4)(2,0,0)[7] with non-zero mean </strong>

<strong>Coefficients:</strong>
<strong>         ma1      ma2     ma3     ma4    sar1    sar2  intercept</strong>
<strong>      0.3257  -0.0311  0.2211  0.2364  0.2801  0.1392   621.9295</strong>
<strong>s.e.  0.0531   0.0531  0.0496  0.0617  0.0534  0.0557     7.9371</strong>

<strong>sigma^2 estimated as 2632:  log likelihood=-1955.83</strong>
<strong>AIC=3927.66   AICc=3928.07   BIC=3958.86</strong>
</pre></div><p>Although it seems that <a class="indexterm" id="id885"/>the preceding seasonal ARIMA model fits the data with a high AIC, we might want to build a real ARIMA model by specifying the <em>D</em> argument resulting in an integrated model via the following estimates:</p><div><pre class="programlisting">
<strong>&gt; plot(forecast(auto.arima(nts, D = 1, approximation = FALSE), 31))</strong>
</pre></div><div><img alt="Autoregressive Integrated Moving Average models" src="img/2028OS_12_08.jpg"/></div><p>Although time-series analysis can sometimes be tricky (and finding the optimal model with the appropriate parameters requires a reasonable experience with these statistical methods), the preceding short examples proved that even a basic understanding of the time-series objects and related methods will usually provide some impressive results on the patterns of data <a class="indexterm" id="id886"/>and adequate predictions.</p></div>
<div><div><div><div><h1 class="title"><a id="ch12lvl1sec84"/>Outlier detection</h1></div></div></div><p>Besides forecasting, another <a class="indexterm" id="id887"/>time-series related major task is identifying suspicious or abnormal data in a series of observations that might distort the results of our analysis. One way to do so is to build an ARIMA model and analyze the distance between the predicted and actual values. The <code class="literal">tsoutliers</code> package<a class="indexterm" id="id888"/> provides a very convenient way to do so. Let's build a model on the number of cancelled flights in 2011:</p><div><pre class="programlisting">
<strong>&gt; cts &lt;- ts(daily$Cancelled)</strong>
<strong>&gt; fit &lt;- auto.arima(cts)</strong>
<strong>&gt; auto.arima(cts)</strong>
<strong>Series: ts </strong>
<strong>ARIMA(1,1,2)</strong>

<strong>Coefficients:</strong>
<strong>          ar1      ma1      ma2</strong>
<strong>      -0.2601  -0.1787  -0.7752</strong>
<strong>s.e.   0.0969   0.0746   0.0640</strong>

<strong>sigma^2 estimated as 539.8:  log likelihood=-1662.95</strong>
<strong>AIC=3333.9   AICc=3334.01   BIC=3349.49</strong>
</pre></div><p>So now we can use an <em>ARIMA(1,1,2)</em> model and the <code class="literal">tso</code> function to highlight (and optionally remove) the outliers from our dataset:</p><div><div><h3 class="title"><a id="tip19"/>Tip</h3><p>Please note that the following <code class="literal">tso</code> call can run for several minutes with a full load on a CPU core as it may be performing heavy computations in the background.</p></div></div><div><pre class="programlisting">
<strong>&gt; library(tsoutliers)</strong>
<strong>&gt; outliers &lt;- tso(cts, tsmethod = 'arima',</strong>
<strong>+   args.tsmethod  = list(order = c(1, 1, 2)))</strong>
<strong>&gt; plot(outliers)</strong>
</pre></div><div><img alt="Outlier detection" src="img/2028OS_12_09.jpg"/></div><p>Alternatively, we can run all the preceding steps in one go by automatically calling <code class="literal">auto.arima</code> inside <code class="literal">tso</code> without specifying any extra arguments besides the time-series object:</p><div><pre class="programlisting">
<strong>&gt; plot(tso(ts(daily$Cancelled)))</strong>
</pre></div><p>Anyway, the results show that all<a class="indexterm" id="id889"/> observations with a high number of cancelled flights are outliers and so should be removed from the dataset. Well, considering any day with many cancelled flights as outlier sounds really optimistic! But this is very useful information; it suggests that, for example, forecasting an outlier event is not manageable with the previously discussed methods.</p><p>Traditionally, time-series analysis deals with trends and seasonality of data, and how to <em>stationarize</em> the time-series. If we are interested in deviations from normal events, some other methods need to be used.</p><p>Twitter recently published one of its R packages to detect anomalies in time-series. Now we will use its <code class="literal">AnomalyDetection</code> package<a class="indexterm" id="id890"/> to identify the preceding outliers in a much faster way. As you may have noticed, the <code class="literal">tso</code> function was really slow to run, and it cannot really handle large amount of data – while the <code class="literal">AnomalyDetection</code> package <a class="indexterm" id="id891"/>performs pretty well.</p><p>We can provide the input data as a vector of a <code class="literal">data.frame</code> with the first column storing the timestamps. Unfortunately, the <code class="literal">AnomalyDetectionTs</code> function does not really work well with <code class="literal">data.table</code> objects, so let's revert to the traditional <code class="literal">data.frame</code> class:</p><div><pre class="programlisting">
<strong>&gt; dfc &lt;- as.data.frame(daily[, c('date', 'Cancelled'), with = FALSE])</strong>
</pre></div><p>Now let's load the package and plot<a class="indexterm" id="id892"/> the anomalies identified among the observations:</p><div><pre class="programlisting">
<strong>&gt; library(AnomalyDetection)</strong>
<strong>&gt; AnomalyDetectionTs(dfc, plot = TRUE)$plot</strong>
</pre></div><div><img alt="Outlier detection" src="img/2028OS_12_10.jpg"/></div><p>The results are very similar to the previous plots, but there are two things to note that you might have already noticed. The computation was extremely quick and, on the other hand, this plot includes human-friendly dates instead of some lame indexes on the <em>x</em> axis.</p></div>
<div><div><div><div><h1 class="title"><a id="ch12lvl1sec85"/>More complex time-series objects</h1></div></div></div><p>The main limitation of the <a class="indexterm" id="id893"/>
<code class="literal">ts</code> time-series R object class (besides the aforementioned <em>x</em> axis issue) is that it cannot deal with irregular time-series. To overcome this problem, we have several alternatives in R.</p><p>The <code class="literal">zoo</code> package and its reverse dependent <a class="indexterm" id="id894"/>
<code class="literal">xts</code> packages are <code class="literal">ts</code>-compatible classes with tons of extremely useful methods. For a quick example, let's build a <code class="literal">zoo</code> object from our data, and see how it's represented by the default plot:</p><div><pre class="programlisting">
<strong>&gt; library(zoo)</strong>
<strong>&gt; zd &lt;- zoo(daily[, -1, with = FALSE], daily[[1]])</strong>
<strong>&gt; plot(zd)</strong>
</pre></div><div><img alt="More complex time-series objects" src="img/2028OS_12_11.jpg"/></div><p>As we have defined the <code class="literal">date</code> column to act as the timestamp of the observations, it's not shown here. The <em>x</em> axis has a nice human-friendly date annotation, which is really pleasant after having checked a bunch of integer-annotated plots in the previous pages.</p><p>Of course, <code class="literal">zoo</code> supports most <a class="indexterm" id="id895"/>of the <code class="literal">ts</code> methods, such as <code class="literal">diff</code>, <code class="literal">lag</code> or cumulative sums; these can be very useful for visualizing data velocity:</p><div><pre class="programlisting">
<strong>&gt; plot(cumsum(zd))</strong>
</pre></div><div><img alt="More complex time-series objects" src="img/2028OS_12_12.jpg"/></div><p>Here, the linear line for the <strong>N</strong> variable suggests that we do not have any missing values and our dataset includes exactly one data point per day. On the other hand, the steep elevation of the <strong>Cancelled</strong> line in February highlights that a single day contributed a lot to the overall number of<a class="indexterm" id="id896"/> cancelled flights in 2011.</p></div>
<div><div><div><div><h1 class="title"><a id="ch12lvl1sec86"/>Advanced time-series analysis</h1></div></div></div><p>Unfortunately, this short <a class="indexterm" id="id897"/>chapter cannot provide a more detailed introduction to time-series analysis. To be honest, even two or three times the length of this chapter would not be enough for a decent tutorial, as time-series analysis, forecasting, and anomaly detection are one of the most complex topics of statistical analysis.</p><p>But the good news is that there are plenty of great books on the topics! One of the best resources—and the ultimate free online tutorial on this subject—can be found at <a class="ulink" href="https://www.otexts.org/fpp">https://www.otexts.org/fpp</a>. This is a really practical and detailed online tutorial on forecasting and general time-series analysis, and I heartily recommend it to anyone who would like to build more complex and realizable time-series models in the future.</p></div>
<div><div><div><div><h1 class="title"><a id="ch12lvl1sec87"/>Summary</h1></div></div></div><p>This chapter focused on how to load, visualize, and model time-related data. Although we could not cover all aspects of this challenging topic, we discussed the most widely used smoothing and filtering algorithms, seasonal decompositions, and ARIMA models; we also computed some forecasts and estimates based on these.</p><p>The next chapter is somewhat similar to this one, as we will cover another domain-independent area on another important dimension of datasets: instead of when, we will focus on <em>where</em> the observations were captured.</p></div></body></html>