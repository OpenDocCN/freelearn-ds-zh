["```py\nlibrary(readxl)\n```", "```py\ndf <- read_xlsx(\n  path = \"chapter1/iris_data.xlsx\",\n  sheet = \"iris\"\n)\nhead(df)\n```", "```py\niris_split <- split(df, df$species)\n```", "```py\ndependent_variable <- \"petal_length\"\nindependent_variables <- c(\"petal_width\", \"sepal_length\", \"sepal_width\")\nf_x <- formula(\n  paste(\ndependent_variable,\n\"~\",\npaste(independent_variables, collapse = \" + \")\n)\n)\n```", "```py\nperform_linear_regression <- function(data) {\n  lm_model <- lm(f_x, data = data)\n  return(lm_model)\n}\n```", "```py\nresults <- lapply(iris_split, perform_linear_regression)\n```", "```py\nlapply(results, summary)\n```", "```py\npar(mfrow = c(2,2))\nlapply(results, plot)\npar(mfrow = c(1, 1))\n```", "```py\nlm_models <- lapply(\niris_split,\nfunction(df) lm(f_x, data = df)\n)\n```", "```py\nf_x <- formula(paste(\"petal_width\", \"~\", \"petal_length + sepal_width + sepal_length\"))\n```", "```py\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(tidymodels)\nnested_lm <- df |>\n nest(data = -species) |>\n mutate(split = map(data, ~ initial_split(., prop = 8/10)),\n        train = map(split, ~ training(.)),\n        test = map(split, ~ testing(.)),\n        fit  = map(train, ~ lm(f_x, data = .)),\n        pred = map2(.x = fit, .y = test, ~ predict(object = .x, newdata = .y)))\n```", "```py\nnested_lm |>\n select(species, pred) |>\n unnest(pred)\n```", "```py\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(purrr)\nlibrary(tidymodels)\n# Create a nested linear regression model\nnested_lm <- df |>\n nest(data = -species) |>\n mutate(split = map(data, ~ initial_split(., prop = 8/10)),\n        train = map(split, ~ training(.)),\n        test = map(split, ~ testing(.)),\n        fit  = map(train, ~ lm(f_x, data = .)),\n        pred = map2(.x = fit, .y = test, ~ predict(object = .x, newdata = .y)))\n# Predict the petal width for a new iris flower\nnew_iris <- data.frame(sepal_length = 5.2, sepal_width = 2.7, \n    petal_length = 3.5)\n# Predict the petal width\npredicted_petal_width <- predict(nested_lm[[1]]$fit, \n    newdata = new_iris))\n# Print the predicted petal width\nprint(predicted_petal_width)\n```", "```py\n1.45\n```", "```py\nlibrary(tidyverse)\ndf <- Titanic |>\n       as.data.frame() |>\n       uncount(Freq)\n```", "```py\nset.seed(123)\ntrain_index <- sample(nrow(df), floor(nrow(df) * 0.8), replace = FALSE)\ntrain <- df[train_index, ]\ntest <- df[-train_index, ]\n```", "```py\n    model <- glm(Survived ~ Sex + Age + Class, data = train, family = \"binomial\")\n    ```", "```py\npredictions <- predict(model, newdata = test, type = \"response\")\npred_resp <- ifelse(predictions <= 0.5, \"No\", \"Yes\")\n```", "```py\n    accuracy <- mean(pred_resp == test$Survived)\n    ```", "```py\n    print(accuracy)\n    table(pred_resp, test$Survived)\n    ```", "```py\nlibrary(tidymodels)\nlibrary(healthyR.ai)\n```", "```py\ndf <- Titanic |>\n    as_tibble() |>\n    uncount(n) |>\n    mutate(across(where(is.character), as.factor))\n```", "```py\n# Set seed for reproducibility\nset.seed(123)\n# Split the data into training and test sets\nsplit <- initial_split(df, prop = 0.8)\ntrain <- training(split)\ntest <- testing(split)\n```", "```py\n# Create a recipe for pre-processing\nrecipe <- recipe(Survived ~ Sex + Age + Class, data = train)\n# Specify logistic regression as the model\nlog_reg <- logistic_reg() |> set_engine(\"glm\", family = \"binomial\")\n# Combine the recipe and model into a workflow\nworkflow <- workflow() %>% add_recipe(recipe) %>% add_model(log_reg)\n# Train the logistic regression model\nfit <- fit(workflow, data = train)\n```", "```py\n# Predict on the test set\npredictions <- predict(fit, new_data = test) |> bind_cols(test) |> select(Class:Survived, .pred_class)\n# Better method\npred_fit_tbl <- fit |> augment(new_data = test)\n```", "```py\n# Accuracy metrics for the model to be scored against from the healthyR.ai package\nperf <- hai_default_classification_metric_set()\n# Calculate the accuracy metrics\nperf(pred_fit_tbl, truth = Survived, estimate = .pred_class)\n# Print the confusion matrix\npredictions |> conf_mat(truth = Survived, estimate = .pred_class)\n```", "```py\nroc_curve(\n  pred_fit_tbl, truth = Survived, .pred_Yes,\n  event_level = \"second\"\n) |>\n  autoplot()\n```", "```py\n    # Import necessary libraries\n    import pandas as pd\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from sklearn.model_selection import train_test_split\n    import statsmodels.api as sm\n    from statsmodels.graphics.regressionplots import plot_regress_exog\n    from statsmodels.graphics.gofplots import qqplot\n    ```", "```py\n    # Step 0: Generate sample data and save as Excel file\n    np.random.seed(0)\n    n_samples = 100\n    X = np.random.rand(n_samples, 2)  # Two features\n    y = 2 * X[:, 0] + 3 * X[:, 1] + np.random.randn(n_samples)\n    # Linear relationship with noise\n    # Create a pandas DataFrame\n    data = {'Feature1': X[:, 0], 'Feature2': X[:, 1], 'Target': y}\n    df = pd.DataFrame(data)\n    # Save the data to Excel\n    df.to_excel(\"linear_regression_input.xlsx\")\n    ```", "```py\n    # Step 1: Import Excel data into a pandas DataFrame\n    excel_file = \"linear_regression_input.xlsx\"\n    df = pd.read_excel(excel_file)\n    # Step 2: Explore the data\n    # Use the tools learned in the previous chapter on EDA\n    # Step 3: Data Preparation (if needed)\n    # Use the tools learned in the previous chapter on data cleaning\n    ```", "```py\n    # Step 4: Split data into training and testing sets\n    X = df[['Feature1', 'Feature2']] # Independent variables\n    y = df['Target'] # Dependent variable\n    # Split the data into training and test set using a fixed random seed for reproducibility\n    X_train, X_test, y_train, y_test = train_test_split(X, y, \n        test_size=0.2, random_state=42)\n    # Step 5: Fit the Linear Regression model\n    # Add a constant (intercept) to the independent variables\n    X_train = sm.add_constant(X_train)\n    X_test = sm.add_constant(X_test)\n    # Fit the linear model\n    model = sm.OLS(y_train, X_train).fit()\n    ```", "```py\n    # Step 6: Model Evaluation\n    y_pred = model.predict(X_test)\n    # Print the model summary\n    print(model.summary())\n    ```", "```py\nplt.scatter(X_test['Feature1'], y_test, color='blue', label='Actual')\nplt.scatter(X_test['Feature1'], y_pred, color='red', \n    label='Predicted')\nplt.xlabel('Feature1')\nplt.ylabel('Target')\nplt.title('Linear Regression Prediction')\nplt.legend()\nplt.show()\n```", "```py\n# Set the backend to 'TkAgg' before generating the plots if needed – comment out this line if in WSL or other non-interactive environment\nplt.switch_backend('TkAgg')\n# Residuals\nfig, ax = plt.subplots(figsize=(12, 8))\nplot_regress_exog(model, \"Feature1\", fig=fig)\nplt.show()\n# Q-Q plot:\nqqplot(model.resid, line=\"s\")\nplt.show()\n```", "```py\n    # Import necessary libraries\n    import pandas as pd\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from sklearn.model_selection import train_test_split\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n    ```", "```py\n    # Step 0: Generate sample data\n    np.random.seed(0)\n    n_samples = 100\n    X = np.random.rand(n_samples, 2)  # Two features\n    y = (X[:, 0] + X[:, 1] > 1).astype(int)  # Binary classification based on a condition\n    # Create a pandas DataFrame\n    data = {'Feature1': X[:, 0], 'Feature2': X[:, 1], 'Target': y}\n    df = pd.DataFrame(data)\n    df.to_excel(\"logistic_regression_input.xlsx\")\n    ```", "```py\n    # Step 1: Import Excel data into a pandas DataFrame\n    excel_file = \"logistic_regression_input.xlsx\"\n    df = pd.read_excel(excel_file)\n    # Step 2: Split data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y,\n        test_size=0.2, random_state=42)\n    ```", "```py\n    # Step 3: Create and train the logistic regression model\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n    ```", "```py\n    # Step 4: Visualization\n    # Visualization for binary classification\n    plt.scatter(X_test[y_test == 1][:, 0], \n        X_test[y_test == 1][:, 1], color='blue', \n        label='Class 1 (Actual)')\n    plt.scatter(X_test[y_test == 0][:, 0], \n        X_test[y_test == 0][:, 1], color='red', \n        label='Class 0 (Actual)')\n    plt.xlabel('Feature1')\n    plt.ylabel('Feature2')\n    plt.title('Logistic Regression Prediction')\n    plt.legend()\n    plt.show()\n    ```", "```py\n    # Step 5: Model Evaluation and Interpretation\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    conf_matrix = confusion_matrix(y_test, y_pred)\n    class_report = classification_report(y_test, y_pred)\n    print(\"Accuracy:\", accuracy)\n    print(\"Confusion Matrix:\\n\", conf_matrix)\n    print(\"Classification Report:\\n\", class_report)\n    ```"]