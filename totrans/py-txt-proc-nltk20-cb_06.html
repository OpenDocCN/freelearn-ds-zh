<html><head></head><body><div class="chapter" title="Chapter&#xA0;6.&#xA0;Transforming Chunks and Trees"><div class="titlepage"><div><div><h1 class="title"><a id="ch06"/>Chapter 6. Transforming Chunks and Trees</h1></div></div></div><p>In this chapter, we will cover:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Filtering insignificant words</li><li class="listitem" style="list-style-type: disc">Correcting verb forms</li><li class="listitem" style="list-style-type: disc">Swapping verb phrases</li><li class="listitem" style="list-style-type: disc">Swapping noun cardinals</li><li class="listitem" style="list-style-type: disc">Swapping infinitive phrases</li><li class="listitem" style="list-style-type: disc">Singularizing plural nouns</li><li class="listitem" style="list-style-type: disc">Chaining chunk transformations</li><li class="listitem" style="list-style-type: disc">Converting a chunk tree to text</li><li class="listitem" style="list-style-type: disc">Flattening a deep tree</li><li class="listitem" style="list-style-type: disc">Creating a shallow tree</li><li class="listitem" style="list-style-type: disc">Converting tree nodes</li></ul></div><div class="section" title="Introduction"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec60"/>Introduction</h1></div></div></div><p>Now that you know how to get chunks/phrases from a sentence, what do you do with them? This chapter will show you how to do various transforms on both chunks and trees. The chunk transforms are for grammatical correction and rearranging phrases without loss of meaning. The tree transforms give you ways to modify and flatten deep parse trees.</p><p>The functions detailed in these recipes modify data, as opposed to learning from it. That means it's not safe to apply them indiscriminately. A thorough knowledge of the data you want to transform, along with a few experiments, should help you decide which functions to apply and when.</p><a id="id426" class="indexterm"/><p>Whenever the term <span class="strong"><strong>chunk</strong></span> is used in this chapter, it could refer to an actual chunk extracted by a chunker, or it could simply refer to a short phrase or sentence in the form of a list of tagged words. What's important in this chapter is what you can do with a chunk, not where it came from.</p></div></div>
<div class="section" title="Filtering insignificant words"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec61"/>Filtering insignificant words</h1></div></div></div><a id="id427" class="indexterm"/><a id="id428" class="indexterm"/><p>Many of the most commonly used words are insignificant when it comes to discerning the meaning of a phrase. For example, in the phrase "the movie was terrible", the most <span class="emphasis"><em>significant</em></span> words are "movie" and "terrible", while "the" and "was" are almost useless. You could get the same meaning if you took them out, such as "movie terrible" or "terrible movie". Either way, the sentiment is the same. In this recipe, we'll learn how to remove the insignificant words, and keep the significant ones, by looking at their part-of-speech tags.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec214"/>Getting ready</h2></div></div></div><a id="id429" class="indexterm"/><p>First, we need to decide which part-of-speech tags are significant and which are not. Looking through the <code class="literal">treebank</code> corpus for <code class="literal">stopwords</code>
<a id="id430" class="indexterm"/> yields the following table of insignificant words and tags:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Word</p>
</th><th style="text-align: left" valign="bottom">
<p>Tag</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>a</p>
</td><td style="text-align: left" valign="top">
<p>DT</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>all</p>
</td><td style="text-align: left" valign="top">
<p>PDT</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>an</p>
</td><td style="text-align: left" valign="top">
<p>DT</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>and</p>
</td><td style="text-align: left" valign="top">
<p>CC</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>or</p>
</td><td style="text-align: left" valign="top">
<p>CC</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>that</p>
</td><td style="text-align: left" valign="top">
<p>WDT</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>the</p>
</td><td style="text-align: left" valign="top">
<p>DT</p>
</td></tr></tbody></table></div><p>Other than CC, all the tags end with DT. This means we can filter out insignificant words by looking at the tag's suffix.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec215"/>How to do it...</h2></div></div></div><a id="id431" class="indexterm"/><p>In <code class="literal">transforms.py</code> there is a function called <code class="literal">filter_insignificant()</code>. It takes a single chunk, which should be a list of tagged words, and returns a new chunk without any insignificant tagged words. It defaults to filtering out any tags that end with DT or CC.</p><div class="informalexample"><pre class="programlisting">def filter_insignificant(chunk, tag_suffixes=['DT', 'CC']):
  good = []
  
  for word, tag in chunk:
    ok = True
    
    for suffix in tag_suffixes:
      if tag.endswith(suffix):
        ok = False
        break
    
    if ok:
      good.append((word, tag))
  
  return good</pre></div><a id="id432" class="indexterm"/><a id="id433" class="indexterm"/><p>Now we can use it on the part-of-speech tagged version of "the terrible movie".</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from transforms import filter_insignificant
&gt;&gt;&gt; filter_insignificant([('the', 'DT'), ('terrible', 'JJ'), ('movie', 'NN')])
[('terrible', 'JJ'), ('movie', 'NN')]</pre></div><p>As you can see, the word "the" is eliminated from the chunk.</p></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec216"/>How it works...</h2></div></div></div><a id="id434" class="indexterm"/><p>
<code class="literal">filter_insignificant()</code> iterates over the tagged words in the chunk. For each tag, it checks if that tag ends with any of the <code class="literal">tag_suffixes</code>. If it does, then the tagged word is skipped. However if the tag is ok, then the tagged word is appended to a new good chunk that is returned.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec217"/>There's more...</h2></div></div></div><p>The way <code class="literal">filter_insignificant()</code> is defined, you can pass in your own tag suffixes if DT and CC are not enough, or are incorrect for your case. For example, you might decide that possessive words and pronouns such as "you", "your", "their", and "theirs" are no good but DT and CC words are ok. The tag suffixes would then be PRP and PRP$. Following is an example of this function:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; filter_insignificant([('your', 'PRP$'), ('book', 'NN'), ('is', 'VBZ'), ('great', 'JJ')], tag_suffixes=['PRP', 'PRP$'])
[('book', 'NN'), ('is', 'VBZ'), ('great', 'JJ')]</pre></div><p>Filtering insignificant words can be a good complement to stopword filtering for purposes such as search engine indexing, querying, and text classification.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec218"/>See also</h2></div></div></div><p>This recipe is analogous to the <span class="emphasis"><em>Filtering stopwords in a tokenized sentence</em></span> recipe in <a class="link" href="ch01.html" title="Chapter 1. Tokenizing Text and WordNet Basics">Chapter 1</a>, <span class="emphasis"><em>Tokenizing Text and WordNet Basics</em></span>.</p></div></div>
<div class="section" title="Correcting verb forms"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec62"/>Correcting verb forms</h1></div></div></div><a id="id435" class="indexterm"/><a id="id436" class="indexterm"/><p>It's fairly common to find incorrect verb forms in real-world language. For example, the correct form of "is our children learning?" is "are our children learning?". The verb "is" should only be used with singular nouns, while "are" is for plural nouns, such as "children". We can correct these mistakes by creating verb correction mappings that are used depending on whether there's a plural or singular noun in the chunk.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec219"/>Getting ready</h2></div></div></div><p>We first need to define the verb correction mappings in <code class="literal">transforms.py</code>. We'll create two mappings, one for plural to singular, and another for singular to plural.</p><div class="informalexample"><pre class="programlisting">plural_verb_forms = {
  ('is', 'VBZ'): ('are', 'VBP'),
  ('was', 'VBD'): ('were', 'VBD')
}

singular_verb_forms = {
  ('are', 'VBP'): ('is', 'VBZ'),
  ('were', 'VBD'): ('was', 'VBD')
}</pre></div><p>Each mapping has a tagged verb that maps to another tagged verb. These initial mappings cover the basics of mapping, is to are, was to were, and vice versa.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec220"/>How to do it...</h2></div></div></div><a id="id437" class="indexterm"/><p>In <code class="literal">transforms.py</code> there is a function called <code class="literal">correct_verbs()</code>. Pass it a chunk with incorrect verb forms, and you'll get a corrected chunk back. It uses a helper function <a id="id438" class="indexterm"/>
<code class="literal">first_chunk_index()</code> to search the chunk for the position of the first tagged word where <code class="literal">pred</code> returns <code class="literal">True</code>.</p><div class="informalexample"><pre class="programlisting">def first_chunk_index(chunk, pred, start=0, step=1):
  l = len(chunk)
  end = l if step &gt; 0 else -1
  
  for i in range(start, end, step):
    if pred(chunk[i]):
      return i
  
  return None

def correct_verbs(chunk):
  vbidx = first_chunk_index(chunk, lambda (word, tag): tag.startswith('VB'))
  # if no verb found, do nothing
  if vbidx is None:
    return chunk
  
  verb, vbtag = chunk[vbidx]
  nnpred = lambda (word, tag): tag.startswith('NN')
  # find nearest noun to the right of verb
  nnidx = first_chunk_index(chunk, nnpred, start=vbidx+1)
  # if no noun found to right, look to the left
  if nnidx is None:
    nnidx = first_chunk_index(chunk, nnpred, start=vbidx-1, step=-1)
  # if no noun found, do nothing
  if nnidx is None:
    return chunk
  
  noun, nntag = chunk[nnidx]
  # get correct verb form and insert into chunk
  if nntag.endswith('S'):
    chunk[vbidx] = plural_verb_forms.get((verb, vbtag), (verb, vbtag))
  else:
    chunk[vbidx] = singular_verb_forms.get((verb, vbtag), (verb, vbtag))
  
  return chunk</pre></div><a id="id439" class="indexterm"/><a id="id440" class="indexterm"/><p>When we call it on a part-of-speech tagged "is our children learning" chunk, we get back the correct form, "are our children learning".</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from transforms import correct_verbs
&gt;&gt;&gt; correct_verbs([('is', 'VBZ'), ('our', 'PRP$'), ('children', 'NNS'), ('learning', 'VBG')])
[('are', 'VBP'), ('our', 'PRP$'), ('children', 'NNS'), ('learning', 'VBG')]</pre></div><p>We can also try this with a singular noun and an incorrect plural verb.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; correct_verbs([('our', 'PRP$'), ('child', 'NN'), ('were', 'VBD'), ('learning', 'VBG')])
[('our', 'PRP$'), ('child', 'NN'), ('was', 'VBD'), ('learning', 'VBG')]</pre></div><p>In this case, "were" becomes "was" because "child" is a singular noun.</p></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec221"/>How it works...</h2></div></div></div><a id="id441" class="indexterm"/><p>The <code class="literal">correct_verbs()</code> function starts by looking for a verb in the chunk. If no verb is found, the chunk is returned with no changes. Once a verb is found, we keep the verb, its tag, and its index in the chunk. Then we look on either side of the verb to find the nearest noun, starting on the right, and only looking to the left if no noun is found on the right. If no noun is found at all, the chunk is returned as is. But if a noun is found, then we lookup the correct verb form depending on whether or not the noun is plural.</p><a id="id442" class="indexterm"/><p>Recall from <a class="link" href="ch04.html" title="Chapter 4. Part-of-Speech Tagging">Chapter 4</a>, <span class="emphasis"><em>Part-of-Speech Tagging</em></span>, that plural nouns are tagged with NNS, while singular nouns are tagged with NN. This means we can check the plurality of a noun by seeing if its tag ends with S. Once we get the corrected verb form, it is inserted into the chunk to replace the original verb form.</p><a id="id443" class="indexterm"/><a id="id444" class="indexterm"/><a id="id445" class="indexterm"/><p>To make searching through the chunk easier, we define a function called <code class="literal">first_chunk_index()</code>. It takes a chunk, a <code class="literal">lambda</code> predicate, the starting index, and a step increment. The predicate function is called with each tagged word until it returns <code class="literal">True</code>. If it never returns <code class="literal">True</code>, then <code class="literal">None</code> is returned. The starting index defaults to zero and the step increment to one. As you'll see in upcoming recipes, we can search backwards by overriding <code class="literal">start</code> and setting <code class="literal">step</code> to -1. This small utility function will be a key part of subsequent transform functions.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec222"/>See also</h2></div></div></div><p>The next four recipes all make use of <code class="literal">first_chunk_index()</code> to perform chunk transformations.</p></div></div>
<div class="section" title="Swapping verb phrases"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec63"/>Swapping verb phrases</h1></div></div></div><a id="id446" class="indexterm"/><p>Swapping the words around a verb can eliminate the passive voice from particular phrases. For example, "the book was great" can be transformed into "the great book".</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec223"/>How to do it...</h2></div></div></div><a id="id447" class="indexterm"/><p>In <code class="literal">transforms.py</code> there is a function called <code class="literal">swap_verb_phrase()</code>. It swaps the right-hand side of the chunk with the left-hand side, using the verb as the <span class="emphasis"><em>pivot</em></span> point. It uses <a id="id448" class="indexterm"/>the <code class="literal">first_chunk_index()</code> function defined in the previous recipe to find the verb to pivot around.</p><div class="informalexample"><pre class="programlisting">def swap_verb_phrase(chunk):
  # find location of verb
  vbpred = lambda (word, tag): tag != 'VBG' and tag.startswith('VB') and len(tag) &gt; 2
  vbidx = first_chunk_index(chunk, vbpred)
  
  if vbidx is None:
    return chunk
  
  return chunk[vbidx+1:] + chunk[:vbidx]</pre></div><a id="id449" class="indexterm"/><p>Now we can see how it works on the part-of-speech tagged phrase "the book was great".</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from transforms import swap_verb_phrase
&gt;&gt;&gt; swap_verb_phrase([('the', 'DT'), ('book', 'NN'), ('was', 'VBD'), ('great', 'JJ')])
[('great', 'JJ'), ('the', 'DT'), ('book', 'NN')]</pre></div><p>The result is "great the book". This phrase clearly isn't grammatically correct, so read on to learn how to fix it.</p></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec224"/>How it works...</h2></div></div></div><a id="id450" class="indexterm"/><p>Using <code class="literal">first_chunk_index()</code> from the previous recipe, we start by finding the first matching verb that is not a gerund (a word that ends in "ing") tagged with VBG. Once we've found the verb, we return the chunk with the right side before the left, and remove the verb.</p><p>The reason we don't want to pivot around a gerund is that gerunds are commonly used to describe nouns, and pivoting around one would remove that description. Here's an example where you can see how not pivoting around a gerund is a good thing:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; swap_verb_phrase([('this', 'DT'), ('gripping', 'VBG'), ('book', 'NN'), ('is', 'VBZ'), ('fantastic', 'JJ')])
[('fantastic', 'JJ'), ('this', 'DT'), ('gripping', 'VBG'), ('book', 'NN')]</pre></div><p>If we had pivoted around the gerund, the result would be "book is fantastic this", and we'd lose the gerund "gripping".</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec225"/>There's more...</h2></div></div></div><p>Filtering insignificant words makes the final result more readable. By filtering either before or after <code class="literal">swap_verb_phrase()</code>, we get "fantastic gripping book" instead of "fantastic this gripping book".</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from transforms import swap_verb_phrase, filter_insignificant
&gt;&gt;&gt; swap_verb_phrase(filter_insignificant([('this', 'DT'), ('gripping', 'VBG'), ('book', 'NN'), ('is', 'VBZ'), ('fantastic', 'JJ')]))
[('fantastic', 'JJ'), ('gripping', 'VBG'), ('book', 'NN')]
&gt;&gt;&gt; filter_insignificant(swap_verb_phrase([('this', 'DT'), ('gripping', 'VBG'), ('book', 'NN'), ('is', 'VBZ'), ('fantastic', 'JJ')]))
[('fantastic', 'JJ'), ('gripping', 'VBG'), ('book', 'NN')]</pre></div><p>Either way, we get a shorter grammatical chunk with no loss of meaning.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec226"/>See also</h2></div></div></div><p>The previous recipe defines <code class="literal">first_chunk_index()</code>, which is used to find the verb in the chunk.</p></div></div>
<div class="section" title="Swapping noun cardinals"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec64"/>Swapping noun cardinals</h1></div></div></div><a id="id451" class="indexterm"/><a id="id452" class="indexterm"/><p>In a chunk, a <span class="strong"><strong>cardinal</strong></span> word—tagged as CD—refers to a number, such as "10". These cardinals often occur before or after a noun. For normalization purposes, it can be useful to always put the cardinal before the noun.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec227"/>How to do it...</h2></div></div></div><a id="id453" class="indexterm"/><p>The function <code class="literal">swap_noun_cardinal()</code> is defined in <code class="literal">transforms.py</code>. It swaps any cardinal that occurs immediately after a noun with the noun, so that the cardinal occurs immediately before the noun.</p><div class="informalexample"><pre class="programlisting">def swap_noun_cardinal(chunk):
  cdidx = first_chunk_index(chunk, lambda (word, tag): tag == 'CD')
  # cdidx must be &gt; 0 and there must be a noun immediately before it
  if not cdidx or not chunk[cdidx-1][1].startswith('NN'):
    return chunk
  
  noun, nntag = chunk[cdidx-1]
  chunk[cdidx-1] = chunk[cdidx]
  chunk[cdidx] = noun, nntag
  return chunk</pre></div><p>Let's try it on a date, such as "Dec 10", and another common phrase "the top 10".</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from transforms import swap_noun_cardinal
&gt;&gt;&gt; swap_noun_cardinal([('Dec.', 'NNP'), ('10', 'CD')])
[('10', 'CD'), ('Dec.', 'NNP')]
&gt;&gt;&gt; swap_noun_cardinal([('the', 'DT'), ('top', 'NN'), ('10', 'CD')])
[('the', 'DT'), ('10', 'CD'), ('top', 'NN')]</pre></div><a id="id454" class="indexterm"/><p>The result is that the numbers are now in front of the noun, creating "10 Dec" and "the 10 top".</p></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec228"/>How it works...</h2></div></div></div><a id="id455" class="indexterm"/><p>We start by looking for a CD tag in the chunk. If no CD is found, or if the CD is at the beginning of the chunk, then the chunk is returned as is. There must also be a noun immediately before the CD. If we do find a CD with a noun preceding it, then we swap the noun and cardinal in place.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec229"/>See also</h2></div></div></div><p>The <span class="emphasis"><em>Correcting verb forms</em></span> recipe defines the <code class="literal">first_chunk_index()</code> function, used to find tagged words in a chunk.</p></div></div>
<div class="section" title="Swapping infinitive phrases"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec65"/>Swapping infinitive phrases</h1></div></div></div><a id="id456" class="indexterm"/><a id="id457" class="indexterm"/><p>An infinitive phrase has the form "A of B", such as "book of recipes". These can often be transformed into a new form while retaining the same meaning, such as "recipes book".</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec230"/>How to do it...</h2></div></div></div><a id="id458" class="indexterm"/><p>An infinitive phrase can be found by looking for a word tagged with IN. The function <code class="literal">swap_infinitive_phrase()</code>, defined in <code class="literal">transforms.py</code>, will return a chunk that swaps the portion of the phrase after the IN word with the portion before the IN word.</p><div class="informalexample"><pre class="programlisting">def swap_infinitive_phrase(chunk):
  inpred = lambda (word, tag): tag == 'IN' and word != 'like'
  inidx = first_chunk_index(chunk, inpred)
  
  if inidx is None:
    return chunk
  
  nnpred = lambda (word, tag): tag.startswith('NN')
  nnidx = first_chunk_index(chunk, nnpred, start=inidx, step=-1) or 0
  
  return chunk[:nnidx] + chunk[inidx+1:] + chunk[nnidx:inidx]</pre></div><p>The function can now be used to transform "book of recipes" into "recipes book".</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from transforms import swap_infinitive_phrase
&gt;&gt;&gt; swap_infinitive_phrase([('book', 'NN'), ('of', 'IN'), ('recipes', 'NNS')])
[('recipes', 'NNS'), ('book', 'NN')]</pre></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec231"/>How it works...</h2></div></div></div><a id="id459" class="indexterm"/><p>This function is similar to the <code class="literal">swap_verb_phrase()</code> function described in the <span class="emphasis"><em>Swapping verb phrases</em></span> <a id="id460" class="indexterm"/>
<a id="id461" class="indexterm"/>recipe. The <code class="literal">inpred lambda</code> is passed to <code class="literal">first_chunk_index()</code> to look for a word whose tag is IN. Next, <code class="literal">nnpred</code> is used to find the first noun that occurs before the IN word, so we can insert the portion of the chunk after the IN word between the noun and the beginning of the chunk. A more complicated example should demonstrate this:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; swap_infinitive_phrase([('delicious', 'JJ'), ('book', 'NN'), ('of', 'IN'), ('recipes', 'NNS')])
[('delicious', 'JJ'), ('recipes', 'NNS'), ('book', 'NN')]</pre></div><p>We don't want the result to be "recipes delicious book". Instead, we want to insert "recipes" before the noun "book", but after the adjective "delicious". Hence, the need to find the <code class="literal">nnidx</code> occurring before the <code class="literal">inidx</code>.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec232"/>There's more...</h2></div></div></div><a id="id462" class="indexterm"/><p>You'll notice that the <code class="literal">inpred lambda</code> checks to make sure the word is not "like". That's because "like" phrases must be treated differently, as transforming them the same way will result in an ungrammatical phrase. For example, "tastes like chicken" should not be transformed into "chicken tastes":</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; swap_infinitive_phrase([('tastes', 'VBZ'), ('like', 'IN'), ('chicken', 'NN')])
[('tastes', 'VBZ'), ('like', 'IN'), ('chicken', 'NN')]</pre></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec233"/>See also</h2></div></div></div><p>In the next recipe, we'll learn how to transform "recipes book" into the more normal form "recipe book".</p></div></div>
<div class="section" title="Singularizing plural nouns"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec66"/>Singularizing plural nouns</h1></div></div></div><a id="id463" class="indexterm"/><p>As we saw in the previous recipe, the transformation process can result in phrases such as "recipes book". This is a NNS followed by an NN, when a more proper version of the phrase would be "recipe book", which is an NN followed by another NN. We can do another transform to correct these improper plural nouns.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec234"/>How to do it...</h2></div></div></div><a id="id464" class="indexterm"/><p>
<code class="literal">transforms.py</code> defines a function called <code class="literal">singularize_plural_noun()</code>, which will de-pluralize a plural noun (tagged with NNS) that is followed by another noun.</p><div class="informalexample"><pre class="programlisting">def singularize_plural_noun(chunk):
  nnspred = lambda (word, tag): tag == 'NNS'
  nnsidx = first_chunk_index(chunk, nnspred)
  
  if nnsidx is not None and nnsidx+1 &lt; len(chunk) and chunk[nnsidx+1][1][:2] == 'NN':
    noun, nnstag = chunk[nnsidx]
    chunk[nnsidx] = (noun.rstrip('s'), nnstag.rstrip('S'))
  
  return chunk</pre></div><p>Using it on "recipes book", we get the more correct form, "recipe book".</p><div class="informalexample"><pre class="programlisting">
&gt;&gt;&gt; from transforms import singularize_plural_noun
&gt;&gt;&gt; singularize_plural_noun([('recipes', 'NNS'), ('book', 'NN')])
[('recipe', 'NN'), ('book', 'NN')]</pre></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec235"/>How it works...</h2></div></div></div><a id="id465" class="indexterm"/><a id="id466" class="indexterm"/><a id="id467" class="indexterm"/><p>We start by looking for a plural noun with the tag NNS. If found, and if the next word is a noun (determined by making sure the tag starts with NN), then we de-pluralize the plural noun by removing an "s" from the right side of both the tag and the word.</p><p>The tag is assumed to be capitalized, so an uppercase "S" is removed from the right side of the tag, while a lowercase "s" is removed from the right side of the word.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec236"/>See also</h2></div></div></div><p>The previous recipe shows how a transformation can result in a plural noun followed by a singular noun, though this could also occur naturally in real-world text.</p></div></div>
<div class="section" title="Chaining chunk transformations"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec67"/>Chaining chunk transformations</h1></div></div></div><a id="id468" class="indexterm"/><a id="id469" class="indexterm"/><p>The transform functions defined in the previous recipes can be chained together to normalize chunks. The resulting chunks are often shorter with no loss of meaning.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec237"/>How to do it...</h2></div></div></div><a id="id470" class="indexterm"/><p>In <code class="literal">transforms.py</code> is the function <code class="literal">transform_chunk()</code>. It takes a single chunk and an optional list of transform functions. It calls each transform function on the chunk, one at a time, and returns the final chunk.</p><div class="informalexample"><pre class="programlisting">def transform_chunk(chunk, chain=[filter_insignificant, swap_verb_phrase, swap_infinitive_phrase, singularize_plural_noun], trace=0):
  for f in chain:
    chunk = f(chunk)
    
    if trace:
      print f.__name__, ':', chunk
  
  return chunk</pre></div><p>Using it on the phrase "the book of recipes is delicious", we get "delicious recipe book":</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from transforms import transform_chunk
&gt;&gt;&gt; transform_chunk([('the', 'DT'), ('book', 'NN'), ('of', 'IN'), ('recipes', 'NNS'), ('is', 'VBZ'), ('delicious', 'JJ')])
[('delicious', 'JJ'), ('recipe', 'NN'), ('book', 'NN')]</pre></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec238"/>How it works...</h2></div></div></div><a id="id471" class="indexterm"/><a id="id472" class="indexterm"/><a id="id473" class="indexterm"/><p>The <code class="literal">transform_chunk()</code> function defaults to chaining the following functions in order:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">filter_insignificant()</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">swap_verb_phrase()</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">swap_infinitive_phrase()</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">singularize_plural_noun()</code></li></ul></div><p>Each function transforms the chunk that results from the previous function, starting with the original chunk.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note21"/>Note</h3><p>The order in which you apply transform functions can be significant. Experiment with your own data to determine which transforms are best, and in which order they should be applied.</p></div></div></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec239"/>There's more...</h2></div></div></div><p>You can pass <code class="literal">trace=1</code> into <code class="literal">transform_chunk()</code> to get an output at each step.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from transforms import transform_chunk
&gt;&gt;&gt; transform_chunk([('the', 'DT'), ('book', 'NN'), ('of', 'IN'), ('recipes', 'NNS'), ('is', 'VBZ'), ('delicious', 'JJ')], trace=1)
filter_insignificant : [('book', 'NN'), ('of', 'IN'), ('recipes', 'NNS'), ('is', 'VBZ'), ('delicious', 'JJ')]
swap_verb_phrase : [('delicious', 'JJ'), ('book', 'NN'), ('of', 'IN'), ('recipes', 'NNS')]
swap_infinitive_phrase : [('delicious', 'JJ'), ('recipes', 'NNS'), ('book', 'NN')]
singularize_plural_noun : [('delicious', 'JJ'), ('recipe', 'NN'), ('book', 'NN')]
[('delicious', 'JJ'), ('recipe', 'NN'), ('book', 'NN')]</pre></div><p>This shows you the result of each transform function, which is then passed in to the next transform function until a final chunk is returned.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec240"/>See also</h2></div></div></div><p>The transform functions used were defined in the previous recipes of this chapter.</p></div></div>
<div class="section" title="Converting a chunk tree to text"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec68"/>Converting a chunk tree to text</h1></div></div></div><a id="id474" class="indexterm"/><p>At some point, you may want to convert a <code class="literal">Tree</code> or sub-tree back to a sentence or chunk string. This is mostly straightforward, except when it comes to properly outputting punctuation.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec241"/>How to do it...</h2></div></div></div><p>We'll use the first <code class="literal">Tree</code> of the <code class="literal">treebank_chunk</code> as our example. The obvious first step is to join all the words in the tree with a space.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from nltk.corpus import treebank_chunk
&gt;&gt;&gt; tree = treebank_chunk.chunked_sents()[0]
&gt;&gt;&gt; ' '.join([w for w, t in tree.leaves()])
'Pierre Vinken , 61 years old , will join the board as a nonexecutive director Nov. 29 .'</pre></div><a id="id475" class="indexterm"/><p>As you can see, the punctuation isn't quite right. The commas and period are treated as individual words, and so get the surrounding spaces as well. We can fix this using regular expression substitution. This is implemented in the <code class="literal">chunk_tree_to_sent()</code> function found in <code class="literal">transforms.py</code>.</p><div class="informalexample"><pre class="programlisting">import re
punct_re = re.compile(r'\s([,\.;\?])')

def chunk_tree_to_sent(tree, concat=' '):
  s = concat.join([w for w, t in tree.leaves()])
  return re.sub(punct_re, r'\g&lt;1&gt;', s)</pre></div><p>Using this function results in a much cleaner sentence, with no space before each punctuation mark:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from transforms import chunk_tree_to_sent
&gt;&gt;&gt; chunk_tree_to_sent(tree)
'Pierre Vinken, 61 years old, will join the board as a nonexecutive director Nov. 29.'</pre></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec242"/>How it works...</h2></div></div></div><a id="id476" class="indexterm"/><a id="id477" class="indexterm"/><p>To correct the extra spaces in front of the punctuation, we create a regular expression <code class="literal">punct_re</code> that will match a space followed by any of the known punctuation characters. We have to escape both '.' and '?' with a '\' since they are special characters. The punctuation is surrounded by parenthesis so we can use the matched group for substitution.</p><p>Once we have our regular expression, we define <code class="literal">chunk_tree_to_sent()</code>, whose first step is to join the words by a concatenation character that defaults to a space. Then we can call <code class="literal">re.sub()</code> to replace all the punctuation matches with just the punctuation group. This eliminates the space in front of the punctuation characters, resulting in a more correct string.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec243"/>There's more...</h2></div></div></div><p>We can simplify this function a little by using <code class="literal">nltk.tag.untag()</code> to get words from the tree's leaves, instead of using our own list comprehension.</p><div class="informalexample"><pre class="programlisting">import nltk.tag, re
punct_re = re.compile(r'\s([,\.;\?])')

def chunk_tree_to_sent(tree, concat=' '):
  s = concat.join(nltk.tag.untag(tree.leaves()))
  return re.sub(punct_re, r'\g&lt;1&gt;', s)</pre></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec244"/>See also</h2></div></div></div><p>The <code class="literal">nltk.tag.untag()</code> function was covered at the end of the <span class="emphasis"><em>Default tagging</em></span> recipe in <a class="link" href="ch04.html" title="Chapter 4. Part-of-Speech Tagging">Chapter 4</a>, <span class="emphasis"><em>Part-of-Speech Tagging</em></span>.</p></div></div>
<div class="section" title="Flattening a deep tree"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec69"/>Flattening a deep tree</h1></div></div></div><a id="id478" class="indexterm"/><p>Some of the included corpora contain parsed sentences, which are often deep trees of nested phrases. Unfortunately, these trees are too deep to use for training a chunker, since IOB tag parsing is not designed for nested chunks. To make these trees usable for chunker training, we must flatten them.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec245"/>Getting ready</h2></div></div></div><p>We're going to use the first parsed sentence of the <code class="literal">treebank</code> corpus as our example. Here's a diagram showing how deeply nested this tree is:</p><div class="mediaobject"><img src="graphics/3609_06_01.jpg" alt="Getting ready"/></div><p>You may notice that the part-of-speech tags are part of the tree structure, instead of being included with the word. This will be handled next using the <a id="id479" class="indexterm"/>
<code class="literal">Tree.pos()</code> method, which was designed specifically for combining words with pre-terminal <code class="literal">Tree</code> nodes such as part-of-speech tags.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec246"/>How to do it...</h2></div></div></div><p>In <code class="literal">transforms.py</code> there is a function named <a id="id480" class="indexterm"/>
<code class="literal">flatten_deeptree()</code>. It takes a single <code class="literal">Tree</code> and will return a new <code class="literal">Tree</code> that keeps only the lowest level trees. It uses a helper function <a id="id481" class="indexterm"/>
<code class="literal">flatten_childtrees()</code> to do most of the work.</p><div class="informalexample"><pre class="programlisting">from nltk.tree import Tree

def flatten_childtrees(trees):
  children = []
  
  for t in trees:
    if t.height() &lt; 3:
      children.extend(t.pos())
    elif t.height() == 3:
      children.append(Tree(t.node, t.pos()))
    else:
      children.extend(flatten_childtrees([c for c in t]))
  
  return children

def flatten_deeptree(tree):
  return Tree(tree.node, flatten_childtrees([c for c in tree]))</pre></div><a id="id482" class="indexterm"/><a id="id483" class="indexterm"/><p>We can use it on the first parsed sentence of the <code class="literal">treebank</code> corpus to get a flatter tree:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from nltk.corpus import treebank
&gt;&gt;&gt; from transforms import flatten_deeptree
&gt;&gt;&gt; flatten_deeptree(treebank.parsed_sents()[0])
Tree('S', [Tree('NP', [('Pierre', 'NNP'), ('Vinken', 'NNP')]), (',', ','), Tree('NP', [('61', 'CD'), ('years', 'NNS')]), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), Tree('NP', [('the', 'DT'), ('board', 'NN')]), ('as', 'IN'), Tree('NP', [('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN')]), Tree('NP-TMP', [('Nov.', 'NNP'), ('29', 'CD')]), ('.', '.')])</pre></div><p>The result is a much flatter <code class="literal">Tree</code> that only includes NP phrases. Words that are not part of a NP phrase are separated. This flatter tree is shown as follows:</p><div class="mediaobject"><img src="graphics/3609_06_02.jpg" alt="How to do it..."/></div><p>This <code class="literal">Tree</code> is quite similar to the first chunk <code class="literal">Tree</code> from the <code class="literal">treebank_chunk</code> corpus. The main difference is that the rightmost NP <code class="literal">Tree</code> is separated into two sub-trees in the previous diagram, one of them named NP-TMP.</p><p>The first tree from <code class="literal">treebank_chunk</code> is shown as follows for comparison:</p><div class="mediaobject"><img src="graphics/3609_06_03.jpg" alt="How to do it..."/></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec247"/>How it works...</h2></div></div></div><a id="id484" class="indexterm"/><a id="id485" class="indexterm"/><a id="id486" class="indexterm"/><p>The solution is composed of two functions: <code class="literal">flatten_deeptree()</code>
<a id="id487" class="indexterm"/> returns a new <code class="literal">Tree</code> from the given tree by calling <code class="literal">flatten_childtrees()</code> on each of the given tree's children.</p><a id="id488" class="indexterm"/><p>
<code class="literal">flatten_childtrees()</code> is a recursive function that drills down into the <code class="literal">Tree</code> until it finds child trees whose <code class="literal">height()</code> is equal to or less than three. A <code class="literal">Tree</code> whose <code class="literal">height()</code> is less than three looks like this:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from nltk.tree import Tree
&gt;&gt;&gt; Tree('NNP', ['Pierre']).height()
2</pre></div><div class="mediaobject"><img src="graphics/3609_06_04.jpg" alt="How it works..."/></div><p>These short trees are converted into lists of tuples using the <code class="literal">pos()</code> function.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; Tree('NNP', ['Pierre']).pos()
[('Pierre', 'NNP')]</pre></div><p>Trees whose <code class="literal">height()</code> is equal to three are the lowest level trees that we're interested in keeping. These trees look like this:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; Tree('NP', [Tree('NNP', ['Pierre']), Tree('NNP', ['Vinken'])]).height()
3</pre></div><div class="mediaobject"><img src="graphics/3609_06_05.jpg" alt="How it works..."/></div><p>When we call <code class="literal">pos()</code> on that tree, we get:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; Tree('NP', [Tree('NNP', ['Pierre']), Tree('NNP', ['Vinken'])]).pos()
[('Pierre', 'NNP'), ('Vinken', 'NNP')]</pre></div><p>The recursive nature of <code class="literal">flatten_childtrees()</code> eliminates all trees whose height is greater than three.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec248"/>There's more...</h2></div></div></div><a id="id489" class="indexterm"/><p>Flattening a deep <code class="literal">Tree</code> allows us to call <code class="literal">nltk.chunk.util.tree2conlltags()</code> on the flattened <code class="literal">Tree</code>, a necessary step to train a chunker. If you try to call this function before flattening the <code class="literal">Tree</code>, you get a <code class="literal">ValueError</code> exception.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from nltk.chunk.util import tree2conlltags
&gt;&gt;&gt; tree2conlltags(treebank.parsed_sents()[0])
Traceback (most recent call last):
  File "&lt;stdin&gt;", line 1, in &lt;module&gt;
  File "/usr/local/lib/python2.6/dist-packages/nltk/chunk/util.py", line 417, in tree2conlltags
    raise ValueError, "Tree is too deeply nested to be printed in CoNLL format"
ValueError: Tree is too deeply nested to be printed in CoNLL format</pre></div><p>However, after flattening there's no problem:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; tree2conlltags(flatten_deeptree(treebank.parsed_sents()[0]))
[('Pierre', 'NNP', 'B-NP'), ('Vinken', 'NNP', 'I-NP'), (',', ',', 'O'), ('61', 'CD', 'B-NP'), ('years', 'NNS', 'I-NP'), ('old', 'JJ', 'O'), (',', ',', 'O'), ('will', 'MD', 'O'), ('join', 'VB', 'O'), ('the', 'DT', 'B-NP'), ('board', 'NN', 'I-NP'), ('as', 'IN', 'O'), ('a', 'DT', 'B-NP'), ('nonexecutive', 'JJ', 'I-NP'), ('director', 'NN', 'I-NP'), ('Nov.', 'NNP', 'B-NP-TMP'), ('29', 'CD', 'I-NP-TMP'), ('.', '.', 'O')]</pre></div><p>Being able to flatten trees, opens up the possibility of training a chunker on corpora consisting of deep parse trees.</p><div class="section" title="CESS-ESP and CESS-CAT treebank"><div class="titlepage"><div><div><h3 class="title"><a id="ch06lvl3sec65"/>CESS-ESP and CESS-CAT treebank</h3></div></div></div><a id="id490" class="indexterm"/><a id="id491" class="indexterm"/><p>The <code class="literal">cess_esp</code> and <code class="literal">cess_cat</code> corpora have parsed sentences, but no chunked sentences. In other words, they have deep trees that must be flattened in order to train a chunker. In fact, the trees are so deep that a diagram can't be shown, but the flattening can be demonstrated by showing the <a id="id492" class="indexterm"/>
<code class="literal">height()</code> of the tree before and after flattening.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from nltk.corpus import cess_esp
&gt;&gt;&gt; cess_esp.parsed_sents()[0].height()
22
&gt;&gt;&gt; flatten_deeptree(cess_esp.parsed_sents()[0]).height()
3</pre></div></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec249"/>See also</h2></div></div></div><p>The <span class="emphasis"><em>Training a tagger-based chunker</em></span> recipe in <a class="link" href="ch05.html" title="Chapter 5. Extracting Chunks">Chapter 5</a>, <span class="emphasis"><em>Extracting Chunks</em></span> covers training a chunker using IOB tags.</p></div></div>
<div class="section" title="Creating a shallow tree"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec70"/>Creating a shallow tree</h1></div></div></div><a id="id493" class="indexterm"/><p>In the previous recipe, we flattened a deep <code class="literal">Tree</code> by only keeping the lowest level sub-trees. In this recipe, we'll keep only the highest level sub-trees instead.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec250"/>How to do it...</h2></div></div></div><p>We'll be using the first parsed sentence from the <code class="literal">treebank</code> corpus as our example. Recall from the previous recipe that the sentence <code class="literal">Tree</code> looks like this:</p><div class="mediaobject"><img src="graphics/3609_06_01.jpg" alt="How to do it..."/></div><a id="id494" class="indexterm"/><a id="id495" class="indexterm"/><a id="id496" class="indexterm"/><p>The <code class="literal">shallow_tree()</code> function defined in <code class="literal">transforms.py</code> eliminates all the nested sub-trees, keeping only the top tree nodes.</p><div class="informalexample"><pre class="programlisting">from nltk.tree import Tree

def shallow_tree(tree):
  children = []
  
  for t in tree:
    if t.height() &lt; 3:
      children.extend(t.pos())
    else:
      children.append(Tree(t.node, t.pos()))
  
  return Tree(tree.node, children)</pre></div><p>Using it on the first parsed sentence in <code class="literal">treebank</code> results in a <code class="literal">Tree</code> with only two sub-trees.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from transforms import shallow_tree
&gt;&gt;&gt; shallow_tree(treebank.parsed_sents()[0])
Tree('S', [Tree('NP-SBJ', [('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ',')]), Tree('VP', [('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD')]), ('.', '.')])</pre></div><p>We can visually and programmatically see the difference, as shown in the following diagram and code:</p><div class="mediaobject"><img src="graphics/3609_06_06.jpg" alt="How to do it..."/></div><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; treebank.parsed_sents()[0].height()
7
&gt;&gt;&gt; shallow_tree(treebank.parsed_sents()[0]).height()
3</pre></div><p>As in the previous recipe, the height of the new tree is three so it can be used for training a chunker.</p></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec251"/>How it works...</h2></div></div></div><a id="id497" class="indexterm"/><a id="id498" class="indexterm"/><a id="id499" class="indexterm"/><p>The <code class="literal">shallow_tree()</code> function iterates over each of the top-level sub-trees in order to create new child trees. If the <code class="literal">height()</code> of a sub-tree is less than three, then that sub-tree is replaced by a list of its part-of-speech tagged children. All other sub-trees are replaced by a new <code class="literal">Tree</code> whose children are the part-of-speech tagged leaves. This eliminates all nested sub-trees while retaining the top-level sub-trees.</p><a id="id500" class="indexterm"/><p>This function is an alternative to <code class="literal">flatten_deeptree()</code> from the previous recipe, for when you want to keep the higher level tree nodes and ignore the lower level nodes.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec252"/>See also</h2></div></div></div><p>The previous recipe covers how to flatten a <code class="literal">Tree</code> and keep the lowest level sub-trees, as opposed to keeping the highest level sub-trees.</p></div></div>
<div class="section" title="Converting tree nodes"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec71"/>Converting tree nodes</h1></div></div></div><a id="id501" class="indexterm"/><p>As you've seen in previous recipes, parse trees often have a variety of <code class="literal">Tree</code> node types that are not present in chunk trees. If you want to use the parse trees to train a chunker, then you'll probably want to reduce this variety by converting some of these tree nodes to more common node types.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec253"/>Getting ready</h2></div></div></div><p>First, we have to decide what <code class="literal">Tree</code> nodes need to be converted. Let's take a look at that first <code class="literal">Tree</code> again:</p><div class="mediaobject"><img src="graphics/3609_06_01.jpg" alt="Getting ready"/></div><p>Immediately you can see that there are two alternative NP sub-trees: NP-SBJ and NP-TMP. Let's convert both of those to NP. The mapping will be as follows:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Original Node</p>
</th><th style="text-align: left" valign="bottom">
<p>New Node</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>NP-SBJ</p>
</td><td style="text-align: left" valign="top">
<p>NP</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>NP-TMP</p>
</td><td style="text-align: left" valign="top">
<p>NP</p>
</td></tr></tbody></table></div></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec254"/>How to do it...</h2></div></div></div><a id="id502" class="indexterm"/><a id="id503" class="indexterm"/><a id="id504" class="indexterm"/><p>In <code class="literal">transforms.py</code> there is a function <code class="literal">convert_tree_nodes()</code>. It takes two arguments: the <code class="literal">Tree</code> to convert, and a node conversion <code class="literal">mapping</code>. It returns a new <code class="literal">Tree</code> with all matching nodes replaced based on the values in the <code class="literal">mapping</code>.</p><div class="informalexample"><pre class="programlisting">from nltk.tree import Tree

def convert_tree_nodes(tree, mapping):
  children = []
  
  for t in tree:
    if isinstance(t, Tree):
      children.append(convert_tree_nodes(t, mapping))
    else:
      children.append(t)
  
  node = mapping.get(tree.node, tree.node)
  return Tree(node, children)</pre></div><p>Using the mapping table shown earlier, we can pass it in as a <code class="literal">dict</code> to <code class="literal">convert_tree_nodes()</code> and convert the first parsed sentence from <code class="literal">treebank</code>.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from transforms import convert_tree_nodes
&gt;&gt;&gt; mapping = {'NP-SBJ': 'NP', 'NP-TMP': 'NP'}
&gt;&gt;&gt; convert_tree_nodes(treebank.parsed_sents()[0], mapping)
Tree('S', [Tree('NP', [Tree('NP', [Tree('NNP', ['Pierre']), Tree('NNP', ['Vinken'])]), Tree(',', [',']), Tree('ADJP', [Tree('NP', [Tree('CD', ['61']), Tree('NNS', ['years'])]), Tree('JJ', ['old'])]), Tree(',', [','])]), Tree('VP', [Tree('MD', ['will']), Tree('VP', [Tree('VB', ['join']), Tree('NP', [Tree('DT', ['the']), Tree('NN', ['board'])]), Tree('PP-CLR', [Tree('IN', ['as']), Tree('NP', [Tree('DT', ['a']), Tree('JJ', ['nonexecutive']), Tree('NN', ['director'])])]), Tree('NP', [Tree('NNP', ['Nov.']), Tree('CD', ['29'])])])]), Tree('.', ['.'])])</pre></div><a id="id505" class="indexterm"/><p>In the following diagram, you can see that the NP-* sub-trees have been replaced with NP sub-trees:</p><div class="mediaobject"><img src="graphics/3609_06_07.jpg" alt="How to do it..."/></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec255"/>How it works...</h2></div></div></div><a id="id506" class="indexterm"/><a id="id507" class="indexterm"/><p>
<code class="literal">convert_tree_nodes()</code> recursively converts every child sub-tree using the <code class="literal">mapping</code>. The <code class="literal">Tree</code> is then rebuilt with the converted nodes and children until the entire <code class="literal">Tree</code> has been converted.</p><p>The result is a brand new <code class="literal">Tree</code> instance with new sub-trees whose nodes have been converted.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec256"/>See also</h2></div></div></div><p>The previous two recipes cover different methods of flattening a parse <code class="literal">Tree</code>, both of which can produce sub-trees that may require mapping before using them to train a chunker. Chunker training is covered in the <span class="emphasis"><em>Training a tagger-based chunker</em></span> recipe in <a class="link" href="ch05.html" title="Chapter 5. Extracting Chunks">Chapter 5</a>, <span class="emphasis"><em>Extracting Chunks</em></span>.</p></div></div></body></html>