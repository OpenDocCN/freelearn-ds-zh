- en: Chapter 8. Distributed Processing and Handling Large Datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Distributed tagging with execnet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed chunking with execnet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallel list processing with execnet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing a frequency distribution in Redis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing a conditional frequency distribution in Redis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storing an ordered dictionary in Redis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed word scoring with Redis and execnet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: NLTK is great for in-memory single-processor natural language processing. However,
    there are times when you have a lot of data to process and want to take advantage
    of multiple CPUs, multi-core CPUs, and even multiple computers. Or perhaps you
    want to store frequencies and probabilities in a persistent, shared database so
    multiple processes can access it simultaneously. For the first case, we'll be
    using execnet to do parallel and distributed processing with NLTK. For the second
    case, you'll learn how to use the Redis data structure server/database to store
    frequency distributions and more.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed tagging with execnet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Execnet** is a distributed execution library for python. It allows you to
    create gateways and channels for remote code execution. A **gateway** is a connection
    from the calling process to a remote environment. The remote environment can be
    a local subprocess or an SSH connection to a remote node. A **channel** is created
    from a gateway and handles communication between the channel creator and the remote
    code.'
  prefs: []
  type: TYPE_NORMAL
- en: Since many NLTK processes require 100 percent CPU utilization during computation,
    execnet is an ideal way to distribute that computation for maximum resource usage.
    You can create one gateway per CPU core, and it doesn't matter whether the cores
    are in your local computer or spread across remote machines. In many situations,
    you only need to have the trained objects and data on a single machine, and can
    send the objects and data to the remote nodes as needed.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You'll need to install execnet for this to work. It should be as simple as `sudo
    pip install execnet` or `sudo easy_install execnet`. The current version of execnet,
    as of this writing, is `1.0.8`. The execnet homepage, which has API documentation
    and examples, is at [http://codespeak.net/execnet/](http://codespeak.net/execnet/).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We start by importing the required modules, as well as an additional module
    `remote_tag.py` that will be explained in the next section. We also need to import
    `pickle` so we can serialize the tagger. Execnet does not natively know how to
    deal with complex objects such as a part-of-speech tagger, so we must dump the
    tagger to a string using `pickle.dumps()`. We'll use the default tagger that's
    used by the `nltk.tag.pos_tag()` function, but you could load and dump any pre-trained
    part-of-speech tagger as long as it implements the `TaggerI` interface.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have a serialized tagger, we start execnet by making a gateway with
    `execnet.makegateway()`. The default gateway creates a Python *subprocess*, and
    we can call the `remote_exec()` method with the `remote_tag` module to create
    a `channel`. With an open channel, we send over the serialized tagger and then
    the first tokenized sentence of the `treebank` corpus.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You don't have to do any special serialization of simple types such as lists
    and tuples, since execnet already knows how to handle serializing the built-in
    types.
  prefs: []
  type: TYPE_NORMAL
- en: Now if we call `channel.receive()`, we get back a tagged sentence that is equivalent
    to the first tagged sentence in the `treebank` corpus, so we know the tagging
    worked. We end by exiting the gateway, which closes the channel and kills the
    subprocess.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Visually, the communication process looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/3609OS_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The gateway''s `remote_exec()` method takes a single argument that can be one
    of the following three types:'
  prefs: []
  type: TYPE_NORMAL
- en: A string of code to execute remotely.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The name of a **pure** **function** that will be serialized and executed remotely.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The name of a **pure** **module** whose source will be executed remotely.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We use the third option with the `remote_tag.py` module, which is defined as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: A pure module is a module that is self-contained. It can only access Python
    modules that are available where it executes, and does not have access to any
    variables or states that exist wherever the gateway is initially created. To detect
    that the module is being executed by `execnet`, you can look at the `__name__`
    variable. If it's equal to `'__channelexec__'`, then it is being used to create
    a remote channel. This is similar to doing `if __name__ == '__main__'` to check
    if a module is being executed on the command line.
  prefs: []
  type: TYPE_NORMAL
- en: The first thing we do is call `channel.receive()` to get the serialized `tagger`,
    which we load using `pickle.loads()`. You may notice that `channel` is not imported
    anywhere—that's because it is included in the global namespace of the module.
    Any module that `execnet` executes remotely has access to the `channel` variable
    in order to communicate with the `channel` creator.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have the `tagger`, we iteratively `tag()` each tokenized sentence that
    we receive from the channel. This allows us to tag as many sentences as the sender
    wants to send, as iteration will not stop until the `channel` is closed. What
    we've essentially created is a compute node for part-of-speech tagging that dedicates
    100 percent of its resources to tagging whatever sentences it receives. As long
    as the `channel` remains open, the node is available for processing.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is a simple example that opens a single gateway and channel. But execnet
    can do a lot more, such as opening multiple channels to increase parallel processing,
    as well as opening gateways to remote hosts over SSH to do distributed processing.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple channels
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can create multiple channels, one per gateway, to make the processing more
    parallel. Each gateway creates a new subprocess (or remote interpreter if using
    an SSH gateway) and we use one channel per gateway for communication. Once we've
    created two channels, we can combine them using the `MultiChannel` class, which
    allows us to iterate over the channels, and make a receive queue to receive messages
    from each channel.
  prefs: []
  type: TYPE_NORMAL
- en: After creating each channel and sending the tagger, we cycle through the channels
    to send an even number of sentences to each channel for tagging. Then we collect
    all the responses from the `queue`. A call to `queue.get()` will return a 2-tuple
    of `(channel, message)` in case you need to know which channel the message came
    from.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you don't want to wait forever, you can also pass a `timeout` keyword argument
    with the maximum number of seconds you want to wait, as in `queue.get(timeout=4)`.
    This can be a good way to handle network errors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once all the tagged sentences have been collected, we can exit the gateways.
    Here''s the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Local versus remote gateways
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The default gateway spec is `popen`, which creates a Python subprocess on the
    local machine. This means `execnet.makegateway()` is equivalent to `execnet.makegateway('popen')`.
    If you have passwordless SSH access to a remote machine, then you can create a
    remote gateway using `execnet.makegateway('ssh=remotehost')` where `remotehost`
    should be the hostname of the machine. A SSH gateway spawns a new Python interpreter
    for executing the code remotely. As long as the code you're using for remote execution
    is **pure**, you only need a Python interpreter on the remote machine.
  prefs: []
  type: TYPE_NORMAL
- en: Channels work exactly the same no matter what kind of gateway is used; the only
    difference will be communication time. This means you can mix and match local
    subprocesses with remote interpreters to distribute your computations across many
    machines in a network. There are many more details on gateways in the API documentation
    at [http://codespeak.net/execnet/basics.html](http://codespeak.net/execnet/basics.html).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Part-of-speech tagging and taggers are covered in detail in [Chapter 4](ch04.html
    "Chapter 4. Part-of-Speech Tagging"), *Part-of-Speech Tagging*. In the next recipe,
    we'll use `execnet` to do distributed chunk extraction.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed chunking with execnet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we'll do chunking and tagging over an `execnet` gateway. This
    will be very similar to the tagging in the previous recipe, but we'll be sending
    two objects instead of one, and we will be receiving a `Tree` instead of a list,
    which requires pickling and unpickling for serialization.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As in the previous recipe, you must have `execnet` installed.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The setup code is very similar to the last recipe, and we'll use the same pickled
    `tagger` as well. First we'll pickle the default `chunker` used by `nltk.chunk.ne_chunk()`,
    though any chunker would do. Next, we make a gateway for the `remote_chunk` module,
    get a `channel`, and send the pickled `tagger` and `chunker` over. Then we receive
    back a pickled `Tree`, which we can unpickle and inspect to see the result. Finally,
    we exit the gateway.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The communication this time is slightly different.
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/3609OS_08_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `remote_chunk.py` module is just a little bit more complicated than the
    `remote_tag.py` module from the previous recipe. In addition to receiving a pickled
    `tagger`, it also expects to receive a pickled `chunker` that implements the `ChunkerI`
    interface. Once it has both a `tagger` and a `chunker`, it expects to receive
    any number of tokenized sentences, which it tags and parses into a `Tree`. This
    `tree` is then pickled and sent back over the `channel`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `Tree` must be pickled because it is not a simple built-in type.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Note that the `remote_chunk` module is pure. Its only external dependency is
    the `pickle` (or `cPickle`) module, which is part of the Python standard library.
    It doesn't need to import any NLTK modules in order to use the `tagger` or `chunker`,
    because all the necessary data is pickled and sent over the `channel`. As long
    as you structure your remote code like this, with no external dependencies, you
    only need NLTK to be installed on a single machine—the one that starts the gateway
    and sends the objects over the channel.
  prefs: []
  type: TYPE_NORMAL
- en: Python subprocesses
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you look at your task/system monitor (or `top` in `*nix`) while running the
    `execnet` code, you may notice a few extra python Processes. Every gateway spawns
    a new, self-contained, *shared-nothing* Python interpreter process, which is killed
    when you call the `exit()` method. Unlike with threads, there is no shared memory
    to worry about, and no global interpreter lock to slow things down. All you have
    are separate communicating processes. This is true whether the processes are local
    or remote. Instead of locking and synchronization, all you have to worry about
    is the order in which the messages are sent and received.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The previous recipe explains `execnet` gateways and channels in detail. In the
    next recipe, we'll use `execnet` to process a list in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: Parallel list processing with execnet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe presents a pattern for using `execnet` to process a list in parallel.
    It's a function pattern for mapping each element in the list to a new value, using
    `execnet` to do the mapping in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, we need to decide exactly what we want to do. In this example, we'll
    just double integers, but we could do any pure computation. Following is the module
    `remote_double.py`, which will be executed by `execnet`. It receives a 2-tuple
    of `(i, arg)`, assumes `arg` is a number, and sends back `(i, arg*2)`. The need
    for `i` will be explained in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: To use this module to double every element in a list, we import the `plists`
    module (explained in the next section) and call `plists.map()` with the `remote_double`
    module, and a list of integers to double.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Communication between channels is very simple, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it...](img/3609OS_08_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `map()` function is defined in `plists.py`. It takes a pure module, a list
    of arguments, and an optional list of 2-tuples consisting of `(spec, count)`.
    The default `specs` are `[('popen', 2)]` , which means we'll open two local gateways
    and channels. Once these channels are opened, we put them into an `itertools`
    cycle, which creates an infinite iterator that cycles back to the beginning once
    it hits the end.
  prefs: []
  type: TYPE_NORMAL
- en: Now we can send each argument in `args` to a `channel` for processing, and since
    the channels are cycled, each channel gets an almost even distribution of arguments.
    This is where `i` comes in—we don't know in what order we'll get the results back,
    so `i`, as the index of each `arg` in the list, is passed to the channel and back
    so we can combine the results in the original order. We then wait for results
    with a `MultiChannel` receive queue and insert them into a pre-filled list that's
    the same length as the original `args`. Once we have all the expected results,
    we can exit the gateways and return the results.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can increase the parallelization by modifying the specs, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: However, more parallelization does not necessarily mean faster processing. It
    depends on the available resources, and the more gateways and channels you have
    open, the more overhead is required. Ideally there should be one gateway and channel
    per CPU core.
  prefs: []
  type: TYPE_NORMAL
- en: You can use `plists.map()` with any pure module as long as it receives and sends
    back 2-tuples where `i` is the first element. This pattern is most useful when
    you have a bunch of numbers to crunch, and want to process them as quickly as
    possible.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The previous recipes cover `execnet` features in greater detail.
  prefs: []
  type: TYPE_NORMAL
- en: Storing a frequency distribution in Redis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `nltk.probability.FreqDist` class is used in many classes throughout NLTK
    for storing and managing frequency distributions. It's quite useful, but it's
    all in-memory, and doesn't provide a way to persist the data. A single `FreqDist`
    is also not accessible to multiple processes. We can change all that by building
    a `FreqDist` on top of Redis.
  prefs: []
  type: TYPE_NORMAL
- en: Redis is a **data** **structure** **server** that is one of the more popular
    *NoSQL* databases. Among other things, it provides a network accessible database
    for storing dictionaries (also known as *hash maps*). Building a `FreqDist` interface
    to a Redis hash map will allow us to create a persistent `FreqDist` that is accessible
    to multiple local and remote processes at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most Redis operations are **atomic**, so it's even possible to have multiple
    processes write to the `FreqDist` concurrently.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this and subsequent recipes, we need to install both `Redis` and `redis-py`.
    A quick start install guide for Redis is available at [http://code.google.com/p/redis/wiki/QuickStart](http://code.google.com/p/redis/wiki/QuickStart).
    To use hash maps, you should install at least version `2.0.0` (the latest version
    as of this writing).
  prefs: []
  type: TYPE_NORMAL
- en: The `Redis` Python driver `redis-py` can be installed using `pip install redis`
    or `easy_install redis`. Ensure you install at least version `2.0.0` to use hash
    maps. The `redis-py` homepage is at [http://github.com/andymccurdy/redis-py/](http://github.com/andymccurdy/redis-py/).
  prefs: []
  type: TYPE_NORMAL
- en: Once both are installed and a `redis-server` process is running, you're ready
    to go. Let's assume `redis-server` is running on `localhost` on port `6379` (the
    default host and port).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `FreqDist` class extends the built-in `dict` class, which makes a `FreqDist`
    an enhanced dictionary. The `FreqDist` class provides two additional key methods:
    `inc()` and `N()`. The `inc()` method takes a single `sample` argument for the
    key, along with an optional `count` keyword argument that defaults to `1`, and
    increments the value at `sample` by `count`. `N()` returns the number of sample
    outcomes, which is the sum of all the values in the frequency distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: We can create an API-compatible class on top of Redis by extending a `RedisHashMap`
    (that will be explained in the next section), then implementing the `inc()` and
    `N()` methods. Since the `FreqDist` only stores integers, we also override a few
    other methods to ensure values are always integers. This `RedisHashFreqDist` (defined
    in `redisprob.py`) uses the `hincrby` command for the `inc()` method to increment
    the `sample` value by `count`, and sums all the values in the hash map for the
    `N()` method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We can use this class just like a `FreqDist`. To instantiate it, we must pass
    a `Redis` connection and the `name` of our hash map. The `name` should be a unique
    reference to this particular `FreqDist` so that it doesn't clash with any other
    keys in `Redis`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The name of the hash map and the sample keys will be encoded to replace whitespace
    and `&` characters with `_`. This is because the `Redis` protocol uses these characters
    for communication. It's best if the name and keys don't include whitespace to
    begin with.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Most of the work is done in the `RedisHashMap` class, found in `rediscollections.py`,
    which extends `collections.MutableMapping`, then overrides all methods that require
    Redis-specific commands. Here''s an outline of each method that uses a specific
    `Redis` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '`__len__()`: Uses the `hlen` command to get the number of elements in the hash
    map'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`__contains__()`: Uses the `hexists` command to check if an element exists
    in the hash map'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`__getitem__()`: Uses the `hget` command to get a value from the hash map'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`__setitem__()`: Uses the `hset` command to set a value in the hash map'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`__delitem__()`: Uses the `hdel` command to remove a value from the hash map'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keys()`: Uses the `hkeys` command to get all the keys in the hash map'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`values()`: Uses the `hvals` command to get all the values in the hash map'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`items()`: Uses the `hgetall` command to get a dictionary containing all the
    keys and values in the hash map'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clear()`: Uses the `delete` command to remove the entire hash map from `Redis`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Extending `collections.MutableMapping` provides a number of other `dict` compatible
    methods based on the previous methods, such as `update()` and `setdefault()`,
    so we don't have to implement them ourselves.
  prefs: []
  type: TYPE_NORMAL
- en: The initialization used for the `RedisHashFreqDist` is actually implemented
    here, and requires a `Redis` connection and a name for the hash map. The connection
    and name are both stored internally to use with all the subsequent commands. As
    mentioned before, whitespace is replaced by underscore in the name and all keys,
    for compatibility with the Redis network protocol.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `RedisHashMap` can be used by itself as a persistent key-value dictionary.
    However, while the hash map can support a large number of keys and arbitrary string
    values, its storage structure is more optimal for integer values and smaller numbers
    of keys. However, don't let that stop you from taking full advantage of Redis.
    It's very fast (for a network server) and does its best to efficiently encode
    whatever data you throw at it.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While Redis is quite fast for a network database, it will be significantly slower
    than the in-memory `FreqDist`. There's no way around this, but while you sacrifice
    speed, you gain persistence and the ability to do concurrent processing.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the next recipe, we'll create a conditional frequency distribution based
    on the `Redis` frequency distribution created here.
  prefs: []
  type: TYPE_NORMAL
- en: Storing a conditional frequency distribution in Redis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `nltk.probability.ConditionalFreqDist` class is a container for `FreqDist`
    instances, with one `FreqDist` per condition. It is used to count frequencies
    that are dependent on another condition, such as another word or a class label.
    We used this class in the *Calculating high information words* recipe in [Chapter
    7](ch07.html "Chapter 7. Text Classification"), *Text Classification*. Here, we'll
    create an API-compatible class on top of `Redis` using the `RedisHashFreqDist`
    from the previous recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As in the previous recipe, you'll need to have `Redis` and `redis-py` installed
    with an instance of `redis-server` running.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We define a `RedisConditionalHashFreqDist` class in `redisprob.py` that extends
    `nltk.probability.ConditionalFreqDist` and overrides the `__contains__()` and
    `__getitem__()` methods. We then override `__getitem__()` so we can create an
    instance of `RedisHashFreqDist` instead of a `FreqDist`, and override `__contains__()`
    so we can call `encode_key()` from the `rediscollections` module before checking
    if the `RedisHashFreqDist` exists.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: An instance of this class can be created by passing in a `Redis` connection
    and a *base name*. After that, it works just like a `ConditionalFreqDist`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `RedisConditionalHashFreqDist` uses *name prefixes* to reference `RedisHashFreqDist`
    instances. The name passed in to the `RedisConditionalHashFreqDist` is a *base
    name* that is combined with each condition to create a unique name for each `RedisHashFreqDist`.
    For example, if the *base name* of the `RedisConditionalHashFreqDist` is `'condhash'`,
    and the *condition* is `'cond1'`, then the final name for the `RedisHashFreqDist`
    is `'condhash:cond1'`. This naming pattern is used at initialization to find all
    the existing hash maps using the `keys` command. By searching for all keys matching
    `'condhash:*'`, we can identify all the existing conditions and create an instance
    of `RedisHashFreqDist` for each.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Combining strings with colons is a common naming convention for `Redis` keys
    as a way to define *namespaces*. In our case, each `RedisConditionalHashFreqDist`
    instance defines a single namespace of hash maps.
  prefs: []
  type: TYPE_NORMAL
- en: The `ConditionalFreqDist` class stores an internal dictionary at `self._fdists`
    that is a mapping of `condition` to `FreqDist`. The `RedisConditionalHashFreqDist`
    class still uses `self._fdists`, but the values are instances of `RedisHashFreqDist`
    instead of `FreqDist`. `self._fdists` is created when we call `ConditionalFreqDist.__init__()`,
    and values are initialized as necessary in the `__getitem__()` method.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`RedisConditionalHashFreqDist` also defines a `clear()` method. This is a helper
    method that calls `clear()` on all the internal `RedisHashFreqDist` instances.
    The `clear()` method is not defined in `ConditionalFreqDist`.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The previous recipe covers the `RedisHashFreqDist` in detail. Also see the *Calculating
    high information words* recipe in [Chapter 7](ch07.html "Chapter 7. Text Classification"),
    *Text Classification*, for example usage of a `ConditionalFreqDist`.
  prefs: []
  type: TYPE_NORMAL
- en: Storing an ordered dictionary in Redis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An ordered dictionary is like a normal `dict`, but the keys are ordered by an
    ordering function. In the case of `Redis`, it supports ordered dictionaries whose
    *keys are strings* and whose *values are floating point scores*. This structure
    can come in handy for cases such as calculating information gain (covered in the
    *Calculating high information words* recipe in [Chapter 7](ch07.html "Chapter 7. Text
    Classification"), *Text Classification*) when you want to store all the words
    and scores for later use.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Again, you'll need `Redis` and `redis-py` installed, with an instance of `redis-server`
    running.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `RedisOrderedDict` class in `rediscollections.py` extends `collections.MutableMapping`
    to get a number of `dict` compatible methods for free. Then it implements all
    the key methods that require `Redis` ordered set (also known as **Zset**) commands.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: You can create an instance of `RedisOrderedDict` by passing in a `Redis` connection
    and a unique name.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Much of the code may look similar to the `RedisHashMap`, which is to be expected
    since they both extend `collections.MutableMapping`. The main difference here
    is that `RedisOrderedSet` orders keys by floating point values, and so is not
    suited for arbitrary key-value storage like the `RedisHashMap`. Here''s an outline
    explaining each key method and how it works with `Redis`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`__len__()`: Uses the `zcard` command to get the number of elements in the
    ordered set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`__getitem__()`: Uses the `zscore` command to get the score of a key, and returns
    `0` if the key does not exist.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`__setitem__()`: Uses the `zadd` command to add a key to the ordered set with
    the given score, or updates the score if the key already exists.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`__delitem__()`: Uses the `zrem` command to remove a key from the ordered set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keys()`: Uses the `zrevrange` command to get all the keys in the ordered set,
    sorted by highest score. It takes two optional keyword arguments `start` and `end`
    to more efficiently get a slice of the ordered keys.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`values()`: Extracts all the scores from the `items()` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`items()`: Uses the `zrevrange` command to get the scores of each key in order
    to return a list of 2-tuples ordered by highest score. Like `keys()`, it takes
    `start` and `end` keyword arguments to efficiently get a slice.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clear()`: Uses the `delete` command to remove the entire ordered set from
    `Redis`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The default ordering of items in a `Redis` ordered set is *low-to-high*, so
    that the key with the lowest score comes first. This is the same as Python's default
    list ordering when you call `sort()` or `sorted()`, but it's not what we want
    when it comes to *scoring*. For storing *scores*, we expect items to be sorted
    from *high-to-low*, which is why `keys()` and `items()` use `zrevrange` instead
    of `zrange`.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As mentioned previously, the `keys()` and `items()` methods take optional `start`
    and `end` keyword arguments to get a slice of the results. This makes the `RedisOrderedDict`
    optimal for storing scores, then getting the top N keys. Here''s a simple example
    where we assign three word scores, then get the top two:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Calculating high information words* recipe in [Chapter 7](ch07.html "Chapter 7. Text
    Classification"), *Text Classification,* describes how to calculate information
    gain, which is a good case for storing word scores in a `RedisOrderedDict`. The
    *Storing a frequency distribution in Redis* recipe introduces `Redis` and the
    `RedisHashMap`.'
  prefs: []
  type: TYPE_NORMAL
- en: Distributed word scoring with Redis and execnet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can use `Redis` and `execnet` together to do distributed word scoring. In
    the *Calculating high information words* recipe in [Chapter 7](ch07.html "Chapter 7. Text
    Classification"), *Text Classification*, we calculated the information gain of
    each word in the `movie_reviews` corpus using a `FreqDist` and `ConditionalFreqDist`.
    Now that we have `Redis`, we can do the same thing using a `RedisHashFreqDist`
    and a `RedisConditionalHashFreqDist`, then store the scores in a `RedisOrderedDict`.
    We can use `execnet` to distribute the counting in order to get better performance
    out of `Redis`.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`Redis`, `redis-py`, and `execnet` must be installed, and an instance of `redis-server`
    must be running on `localhost`.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We start by getting a list of `(label, words)` tuples for each label in the
    `movie_reviews` corpus (which only has `pos` and `neg` labels). Then we get the
    `word_scores` using `score_words()` from the `dist_featx` module. `word_scores`
    is an instance of `RedisOrderedDict`, and we can see that the total number of
    words is 39,764\. Using the `keys()` method, we can then get the top 1000 words,
    and inspect the top five just to see what they are. Once we have all we want from
    `word_scores`, we can delete the keys in `Redis` as we no longer need the data.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The `score_words()` function in `dist_featx` can take a while to complete, so
    expect to wait a couple of minutes. The overhead of using `execnet` and `Redis`
    means it will take significantly longer than a non-distributed in-memory version
    of the function.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `dist_featx.py` module contains the `score_words()` function, which does
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Opens gateways and channels, sending initialization data to each.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sends each `(label, words)` tuple over a channel for counting.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sends a `done` message to each channel, waits for a `done` reply back, then
    closes the channels and gateways.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculates the score of each word based on the counts and stores in a `RedisOrderedDict`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In our case of counting words in the `movie_reviews` corpus, calling `score_words()`
    opens two gateways and channels, one for counting the `pos` words, and the other
    for counting the `neg` words. The communication is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works...](img/3609OS_08_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Once the counting is finished, we can score all the words and store the results.
    The code itself is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that this scoring method will only be accurate when there are two labels.
    If there are more than two labels, then word scores for each label should be stored
    in separate `RedisOrderedDict` instances, one per label.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `remote_word_count.py` module looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'You''ll notice this is not a pure module as it requires being able to import
    both `redis` and `redisprob`. The reason is that instances of `RedisHashFreqDist`
    and `RedisConditionalHashFreqDist` cannot be pickled and sent over the `channel`.
    Instead, we send the host name and key names over the channel so we can create
    the instances in the remote module. Once we have the instances, there are two
    kinds of data we can receive over the channel:'
  prefs: []
  type: TYPE_NORMAL
- en: A `done` message, which signals that there is no more data coming in over the
    channel. We reply back with another `done` message, then exit the loop to close
    the channel.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A 2-tuple of `(label, words)`, which we then iterate over to increment counts
    in both the `RedisHashFreqDist` and `RedisConditionalHashFreqDist`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this particular case, it would be faster to compute the scores without using
    `Redis` or `execnet`. However, by using `Redis`, we can store the scores persistently
    for later examination and usage. Being able to inspect all the word counts and
    scores manually is a great way to learn about your data. We can also tweak feature
    extraction without having to re-compute the scores. For example, you could use
    `featx.bag_of_words_in_set()` (found in [Chapter 7](ch07.html "Chapter 7. Text
    Classification"), Text Classification) with the top `N` words from the `RedisOrderedDict`,
    where `N` could be 1,000, 2,000, or whatever number you want. If our data size
    is much greater, the benefits of `execnet` will be much more apparent. Horizontal
    scalability using `execnet` or some other method to distribute computations across
    many nodes becomes more valuable, as the size of the data you need to process
    increases.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Calculating high information words* recipe in [Chapter 7](ch07.html "Chapter 7. Text
    Classification"), *Text Classification* introduces information gain scoring of
    words for feature extraction and classification. The first three recipes of this
    chapter show how to use `execnet`, while the next three recipes describe `RedisHashFreqDist`,
    `RedisConditionalHashFreqDist`, and `RedisOrderedDict` respectively.
  prefs: []
  type: TYPE_NORMAL
