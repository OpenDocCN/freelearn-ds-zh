<html><head></head><body><div class="chapter" title="Chapter&#xA0;4.&#xA0;Using OpenCL Functions"><div class="titlepage"><div><div><h1 class="title"><a id="ch04"/>Chapter 4. Using OpenCL Functions</h1></div></div></div><p>In this chapter, we'll cover the following recipes:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Storing vectors to an array</li><li class="listitem" style="list-style-type: disc">Loading vectors from an array</li><li class="listitem" style="list-style-type: disc">Using geometric functions</li><li class="listitem" style="list-style-type: disc">Using integer functions</li><li class="listitem" style="list-style-type: disc">Using floating-point functions</li><li class="listitem" style="list-style-type: disc">Using trigonometric functions</li><li class="listitem" style="list-style-type: disc">Arithmetic and rounding in OpenCL</li><li class="listitem" style="list-style-type: disc">Using the shuffle function in OpenCL</li><li class="listitem" style="list-style-type: disc">Using the select function in OpenCL</li></ul></div><div class="section" title="Introduction"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec31"/>Introduction</h1></div></div></div><p>In this chapter, we are going to explore how to utilize the common functions provided by OpenCL in your code. The functions we are examining would be mostly mathematical operations applied to the elements, and in particular applied to a vector of elements. Recall that the vectors are OpenCL's primary way to allow multiple elements to be processed on your hardware. As the OpenCL vendor can often produce vectorized hardware instructions to efficiently load and store such elements, try to use them as much as possible.</p><p>In detail, we are going to take a dive into how the following works:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Data load and store functions for vectors</li><li class="listitem" style="list-style-type: disc">Geometric functions</li><li class="listitem" style="list-style-type: disc">Integer functions</li><li class="listitem" style="list-style-type: disc">Floating-point functions</li><li class="listitem" style="list-style-type: disc">Trigonometric functions</li></ul></div><p>Finally, we will present two sections on how the OpenCL's <code class="literal">shuffle</code> and <code class="literal">select</code> functions would work if you choose to use them in your applications.</p></div></div>
<div class="section" title="Storing vectors to an array"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec32"/>Storing vectors to an array</h1></div></div></div><p>In the previous chapters, you caught glimpses of how we use vectors in various ways from a tool to<a id="id300" class="indexterm"/> transport data in an efficient manner to the device and from the<a id="id301" class="indexterm"/> device. We have also learned that OpenCL provides a substantial amount of functions that actually work on vectors. In this section, we will explore how we can store vectors to an array (when we use arrays in this context with a vector, we mean an array that contains scalar values).</p><p>The <code class="literal">vstore&lt;N&gt;</code> functions, where <code class="literal">&lt;N&gt;</code> is <code class="literal">2</code>, <code class="literal">3</code>, <code class="literal">4</code>, <code class="literal">8</code>, and <code class="literal">16</code>, are the primary functions you will use to actually signal the OpenCL that you wish to store the elements in your vector that has to be transported in a parallel fashion to a destination; this is often a scalar array or another vector.</p><p>We should be clear that <code class="literal">gentypeN</code> is not a C-like type alias for a data type, but rather a logical placeholder for the types such as <code class="literal">char</code>, <code class="literal">uchar</code>, <code class="literal">short</code>, <code class="literal">ushort</code>, <code class="literal">int</code>, <code class="literal">uint</code>, <code class="literal">long</code>, <code class="literal">ulong</code>, <code class="literal">float</code>, and <code class="literal">double</code>. The <code class="literal">N</code> stands for whether it is a data structure that aggregates <code class="literal">2</code>, <code class="literal">3</code>, <code class="literal">4</code>, <code class="literal">8</code>, or <code class="literal">16</code> elements. Remember that if you wish to store vectors of the type <code class="literal">double</code>, then you need to ensure that the directive <code class="literal">#pragma OPENCL EXTENSION cl_khr_fp64 : enable</code> is in your code before any <code class="literal">double</code> precision data type is declared in the kernel code.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip16"/>Tip</h3><p>Hence, the <code class="literal">vstoreN</code> API will write <code class="literal">sizeof(gentypeN)</code> bytes given by the data to the address <code class="literal">(p + (offset *N))</code>. The address computed as <code class="literal">(p + (offset * N))</code> must be 8-bit aligned if <code class="literal">gentype</code> is <code class="literal">char</code> or <code class="literal">uchar</code>; 16-bit aligned if <code class="literal">gentype</code> is <code class="literal">short</code> or <code class="literal">ushort</code>; 32-bit aligned if <code class="literal">gentype</code> is <code class="literal">int</code> or <code class="literal">uint</code>; 64-bit aligned if <code class="literal">gentype</code> is <code class="literal">long</code>, <code class="literal">ulong</code> or <code class="literal">double</code>.</p></div></div><p>You should notice that the memory writes can span from the global memory space (<code class="literal">__global</code>) to local (<code class="literal">__local</code>), or even to a work item private memory space (<code class="literal">__private</code>) but never to a constant memory space (<code class="literal">__constant</code> is read-only). Depending on your algorithm, you may need to coordinate the writes to another memory space with memory barriers otherwise known as fences.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip17"/>Tip</h3><p>The reason why you will need memory barriers or fences is that the memory reads and writes, in general, can be out of order, and the main reason for this is that the compiler optimization of the source code re-orders the instructions so that it can take advantage of the hardware.</p></div></div><p>To expand on that idea a little, you might be aware that C++ has a keyword, <code class="literal">volatile</code>, which<a id="id302" class="indexterm"/> is used to mark a variable so that the compiler optimizations generally do not apply optimized load-stores to any use of that variable; and basically any use of such variable typically involves a load-use-store cycle at every use-site also known as sequence points.</p><p>Loop unrolling <a id="id303" class="indexterm"/>is an optimization technique where the compiler attempts to remove branching in the code and hence, emitting any branch predication instructions so that the code executes efficiently. In the loops that you are accustomed to, you often find an expression as follows:</p><div class="informalexample"><pre class="programlisting">for(int i = 0; i  &lt; n; ++i ) { ... }</pre></div><p>What happens<a id="id304" class="indexterm"/> here is that when this code is compiled, you will notice that the<a id="id305" class="indexterm"/> ISA will issue an instruction to compare the value of <code class="literal">i</code> against that of <code class="literal">n</code>, and based on the result of that comparison, perform certain actions. Branching occurs when the executing thread takes a path if the condition is true or another path if the condition is false. Typically, a CPU executes both paths concurrently until it knows with a 100 percent certainty that it should take one of these paths, and the CPU can either dump the other unused path or it needs to backtrack its execution. In either case, you will lose several CPU cycles when this happens. Therefore, the developer can help the compiler and in our case, give a hint to the compiler what the value of <code class="literal">n</code> should be so that the compiler doesn't have to generate code to check for <code class="literal">i &lt; n</code>. Unfortunately, OpenCL 1.2 doesn't support loop unrolling as an extension, but rather the AMD APP SDK and CUDA toolkits provide the following C directives:</p><div class="informalexample"><pre class="programlisting">#pragma unroll &lt;unroll-factor&gt;
#pragma unroll 10
for(int i = 0; i &lt; n; ++i) { ... }</pre></div><p>Without these functions, the OpenCL kernel would potentially issue a memory load-store for each processed element as illustrated by the following diagram:</p><div class="mediaobject"><img src="graphics/4520OT_04_01.jpg" alt="Storing vectors to an array"/></div><p>Let's build a simple <a id="id306" class="indexterm"/>example of how we can use these <code class="literal">vstoreN</code> functions <a id="id307" class="indexterm"/>in a simple example.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec68"/>Getting ready</h2></div></div></div><p>This recipe will show you a code snippet from <code class="literal">Ch4/simple_vector_store/simple_vector_store.cl</code> where a vector of 16 elements is loaded in and subsequently copied by using <code class="literal">vstore16(...)</code>. This API isn't exactly sugar syntax for a loop unrolling of 16 elements, and the reason is the compiler generates instructions that loads a vector of 16 elements from memory; also loop unrolling doesn't exist in OpenCL 1.1 as we know it but, it doesn't hurt to think in terms of that if it helps in understanding the concept behind the <code class="literal">vstoreN</code> APIs.</p></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec69"/>How to do it…</h2></div></div></div><p>The following is the kernel code where we will demonstrate the data transfers:</p><div class="informalexample"><pre class="programlisting">//
// This kernel loads 64-elements using a single thread/work-item
// into its __private memory space and writes it back out
__kernel void wideDataTransfer(__global float* in,__global float* out) {
    size_t id = get_global_id(0);
    size_t offsetA = id ;
    size_t offsetB = (id+1);
    size_t offsetC = (id+2);
    size_t offsetD = (id+3);

    // each work-item loads 64-elements
    float16 A = vload16(offsetA, in);
    float16 B = vload16(offsetB, in);
    float16 C = vload16(offsetC, in);
    float16 D = vload16(offsetD, in);

    vstore16(A, offsetA, out);
    vstore16(B, offsetB, out);
    vstore16(C, offsetC, out);
    vstore16(D, offsetD, out);
}</pre></div><p>To compile it on the <a id="id308" class="indexterm"/>OS X platform, you will have to run a compile <a id="id309" class="indexterm"/>command similar to this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>gcc –std=c99 –Wall –DUNIX –g –DDEBUG –DAPPLE –arch i386 –o VectorStore vector_store.c –framework OpenCL</strong></span>
</pre></div><p>Alternatively, you can type <code class="literal">make</code> in the source directory <code class="literal">Ch4/simple_vector_store/</code>. When that happens, you will have a binary executable named <code class="literal">VectorStore</code>.</p><p>To run the program on OS X, simply execute the program <code class="literal">VectorStore</code> and you should either see the output: <code class="literal">Check passed!</code> or <code class="literal">Check failed!</code> as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Check passed!</strong></span>
</pre></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec70"/>How it works…</h2></div></div></div><p>This code can be understood from the perspective that a large vector exists in the global memory space, and our attempt is to load the vector into a variable in the private memory, that is, each work item has a unique variable named <code class="literal">t</code>; do nothing to it and store it back out to another in-memory array that is present in the global memory space.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip18"/>Tip</h3><p>In case you are curious about how this works, the memory writes are actually coalesced so that the writes are issued in bursts of bytes. The size of this burst is dependent on the hardware's internal architecture. As a concrete example in AMD's ATI GPUs, these memory writes are issued once every 16 writes are known to occur and it is related to the implementation of work items in the GPU. You see that it's very inefficient for the GPU to issue a read or write for every work item. When you combine this with the fact that there could be potentially hundreds of thousands of computing threads active in a clustered GPU solution, you can imagine the complexity is unfathomable if the manufacturers were to implement a logic that allows the developer to manage the programs on a work item/per-thread granularity. Hence graphic card manufacturers have decided that it is more efficient to implement the graphical cards to execute a group of threads in lock-step. ATI calls this group of executing threads a wave-front and NVIDIA calls it a warp. This understanding is critical when you start to develop nontrivial algorithms on your OpenCL device.</p></div></div><p>When you build the<a id="id310" class="indexterm"/> sample application and run it, it doesn't do anything in <a id="id311" class="indexterm"/>particularly special from what we have seen but it is useful to see how the underlying code is generated, and in this example the Intel OpenCL SDK is illustrative.</p><div class="mediaobject"><img src="graphics/4520OT_04_02.jpg" alt="How it works…"/></div><p>The assembly code snippet in particular is that of the resultant translation to <span class="strong"><strong>SSE2/3/4</strong></span><a id="id312" class="indexterm"/> or <span class="strong"><strong>Intel AVX</strong></span> (<span class="strong"><strong>Advanced Vector Extensions</strong></span>)<a id="id313" class="indexterm"/> code.</p></div></div>
<div class="section" title="Loading vectors from an array"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec33"/>Loading vectors from an array</h1></div></div></div><p>The <a id="id314" class="indexterm"/>
<code class="literal">vloadN</code> functions are<a id="id315" class="indexterm"/> typically used to load multiple elements from an in-memory <a id="id316" class="indexterm"/>array to a destination in-memory data structure and are often a vector. Similar to the<a id="id317" class="indexterm"/> <code class="literal">vstoreN</code> functions, the <code class="literal">vloadN</code> functions also load elements from the global (<code class="literal">__global</code>), local (<code class="literal">__local</code>), work item private (<code class="literal">__private</code>), and finally constant memory spaces (<code class="literal">__constant</code>).</p><p>We should be clear that <code class="literal">gentypeN</code> is not a C-like type alias for a data type but rather a logical placeholder for the types: <code class="literal">char</code>, <code class="literal">uchar</code>, <code class="literal">short</code>, <code class="literal">ushort</code>, <code class="literal">int</code>, <code class="literal">uint</code>, <code class="literal">long</code>, <code class="literal">ulong</code>, <code class="literal">float</code>, or <code class="literal">double</code> and the <code class="literal">N</code> stands for whether it's a data structure that aggregates <code class="literal">2</code>, <code class="literal">3</code>, <code class="literal">4</code>, <code class="literal">8</code>, or <code class="literal">16</code> elements. Without this function, the kernel needs to issue potentially multiple memory loads as illustrated by the following diagram:</p><div class="mediaobject"><img src="graphics/4520OT_04_03.jpg" alt="Loading vectors from an array"/></div><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec71"/>Getting ready</h2></div></div></div><p>The following is an excerpt from <code class="literal">Ch4/simple_vector_load/simple_vector_load.cl</code>. We focus our attention to understand how to load vectors of elements from the device memory space for computation within the device, that is, CPU/GPU. But this time round, we use an optimization technique called <a id="id318" class="indexterm"/>
<span class="strong"><strong>prefetching</strong></span> (its warming up the cache when your code is going to make use of the data soon and you want it to be near also known as spatial and temporal locality), and is typically used to assign to local memory space so that all work items can read the data off the <a id="id319" class="indexterm"/>cache without flooding <a id="id320" class="indexterm"/>requests onto the bus.</p></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec72"/>How to do it…</h2></div></div></div><p>The following is the kernel code from which we shall draw our inspiration:</p><div class="informalexample"><pre class="programlisting">__kernel void wideDataTransfer(__global float* in, __global float* out) {
  size_t id = get_group_id(0) * get_local_size(0) +get_local_id(0);
  size_t STRIDE = 16;
  size_t offsetA = id;
  prefetch(in + (id*64), 64);
  barrier(CLK_LOCAL_MEM_FENCE);

  float16 A = vload16(offsetA, in);
  float a[16]; 
  a[0] = A.s0;
  a[1] = A.s1;
  a[2] = A.s2;
  a[3] = A.s3;
  a[4] = A.s4;
  a[5] = A.s5;
  a[6] = A.s6;
  a[7] = A.s7;
  a[8] = A.s8;
  a[9] = A.s9;
  a[10] = A.sa;
  a[11] = A.sb;
  a[12] = A.sc;
  a[13] = A.sd;
  a[14] = A.se;
  a[15] = A.sf;
  for( int i = 0; i &lt; 16; ++i ) {
    out[offsetA*STRIDE+i] = a[i];
  }
}</pre></div><p>To compile it on the OS X platform, you will have to run a compile command similar to this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>gcc –std=c99 –Wall –DUNIX –g –DDEBUG –DAPPLE –arch i386 –o VectorLoad vector_load.c –framework OpenCL</strong></span>
</pre></div><p>Alternatively, you can type <code class="literal">make</code> in the source directory <code class="literal">Ch4/simple_vector_load/</code>. When that happens, you will have a binary executable named <code class="literal">VectorLoad</code>.</p><p>To run the program on OS X, simply execute the program <code class="literal">VectorLoad</code> and you should either see the output: <code class="literal">Check passed!</code> or <code class="literal">Check failed!</code> as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Check passed!</strong></span>
</pre></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec73"/>How it works…</h2></div></div></div><p>The kernel would proceed <a id="id321" class="indexterm"/>to prefetch the <code class="literal">16</code> values of type <code class="literal">float</code> from the <code class="literal">__global</code> <a id="id322" class="indexterm"/>memory space to the global cache via the first work item in the work group, which would ultimately arrive in the work item's <code class="literal">__private</code> memory space via the <code class="literal">vload16</code> API. Once that value is loaded, we can assign individual floats to the array and finally output them to the destination via an explicit write to the <code class="literal">__global</code> memory space of <code class="literal">out</code>. This is one method in which you can conduct memory load from a scalar array that resides in the global memory space.</p><div class="informalexample"><pre class="programlisting">prefetch(in +(id*64), 64);</pre></div><p>The preceding line is an optimization technique used to improve data reuse by making it available before it is required; this prefetch instruction is applied to a work item in a work group and we've chosen the first work item in each work group to carry this out. In algorithms where there is heavy data reuse, the benefits would be more significant than the following example:</p><p>Another thing you may have noticed is that we didn't write the following code:</p><div class="informalexample"><pre class="programlisting">out[offset*STRIDE + i] = A; // 'A' is a vector of 16 floats</pre></div><p>The reason why we did not do this is because OpenCL forbids the implicit/explicit conversion of a vector type to a scalar.</p><div class="mediaobject"><img src="graphics/4520OT_04_04.jpg" alt="How it works…"/></div><p>One interesting thing <a id="id323" class="indexterm"/>that is worth pointing out other than the generated SSE instructions <a id="id324" class="indexterm"/>is the fact that multiple hardware prefetch instructions are generated, even though the code only mentions one prefetch instruction. This is the sort of façade that allows OpenCL vendors to implement the functionality based on an open standard, while still allowing the vendors to hide the actual implementation details from the developer.</p></div></div>
<div class="section" title="Using geometric functions"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec34"/>Using geometric functions</h1></div></div></div><p>The geometric functions<a id="id325" class="indexterm"/> are used by the programmers to perform common <a id="id326" class="indexterm"/>computation on vectors, for example, cross <a id="id327" class="indexterm"/>or dot products, normalizing a vector, and length of a vector. To recap a little about vector cross and dot products, remember that a vector in the mathematical sense represents a quantity that has both direction and magnitude, and these vectors are used extensively in computer graphics.</p><p>Quite often, we need to compute the distance (in degrees or radians) between two vectors and to do this, we need to compute the dot product, which is defined as:</p><div class="mediaobject"><img src="graphics/4520OT_04_05.jpg" alt="Using geometric functions"/></div><p>It follows that if <span class="emphasis"><em>a</em></span> is perpendicular to <span class="emphasis"><em>b</em></span> then it must be that <span class="emphasis"><em>a . b = 0</em></span>. The dot product is also used to compute the matrix-vector multiplication which solves a class of problems known as<a id="id328" class="indexterm"/> <span class="strong"><strong>linear systems</strong></span>. Cross products of two 3D vectors will produce a vector that is perpendicular to both of them and can be defined as:</p><div class="mediaobject"><img src="graphics/4520OT_04_06.jpg" alt="Using geometric functions"/></div><p>The difference <a id="id329" class="indexterm"/>between these products is the fact that the dot <a id="id330" class="indexterm"/>product produces a scalar value while the cross product produces a vector value.</p><p>The following is a list of OpenCL's geometric functions:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Function</p>
</th><th style="text-align: left" valign="bottom">
<p>Description</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p><code class="literal">float4 cross(float4 m, float4 n)</code></p>
<p><code class="literal">float3 cross(float3 m, float3 n)</code></p>
</td><td style="text-align: left" valign="top">
<p>Returns the cross product of <code class="literal">m.xyz</code> and <code class="literal">n.xyz</code> and the <code class="literal">w</code> component in the result vector is always zero</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">float dot(floatn m, floatn n)</code></p>
</td><td style="text-align: left" valign="top">
<p>Returns the dot product of two vectors</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">float distance(floatn m, floatn n)</code></p>
</td><td style="text-align: left" valign="top">
<p>Returns the distance between <code class="literal">m</code> and <code class="literal">n</code>. This is computed as:<code class="literal">length(m – n)</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">float length(floatn p)</code></p>
</td><td style="text-align: left" valign="top">
<p>Return the length of the vector <code class="literal">p</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">floatn normalize(floatn p)</code></p>
</td><td style="text-align: left" valign="top">
<p>Returns a vector in the same direction as <code class="literal">p</code> but with a length of <code class="literal">1</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">float fast_distance(floatn p0, floatn p1)</code></p>
</td><td style="text-align: left" valign="top">
<p>Returns <code class="literal">fast_length(p0 – p1)</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">float fast_length(floatn p)</code></p>
</td><td style="text-align: left" valign="top">
<p>Returns the length of vector <code class="literal">p</code> computed as:</p>
<p><code class="literal">half_sqrt()</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">floatn fast_normalize(floatn p)</code></p>
</td><td style="text-align: left" valign="top">
<p>Returns a vector in the same direction as <code class="literal">p</code> but with a length of <code class="literal">1</code>. <code class="literal">fast_normalize</code> is computed as:</p>
<p><code class="literal">p * half_sqrt()</code></p>
</td></tr></tbody></table></div><p>You should be aware that these functions are implemented in OpenCL using the <span class="emphasis"><em>round to nearest even</em></span> rounding mode also known as<a id="id331" class="indexterm"/> <span class="strong"><strong>rte-mode</strong></span>.</p><p>Next, let's take a look at an example that utilizes some of these functions.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec74"/>Getting ready</h2></div></div></div><p>The code <a id="id332" class="indexterm"/>snippet in <code class="literal">Ch4/simple_dot_product/matvecmult.cl</code> illustrates how to compute the dot product between a 2D vector and a <a id="id333" class="indexterm"/>matrix and write back the result of that computation to the output array. When you are starting out with OpenCL, there might be two probable ways in which you will write this functionality, and I think it is instructive to discover what the differences are; however we only show the relevant code snippet that demonstrates the dot API.</p></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec75"/>How to do it…</h2></div></div></div><p>The following is the simplest implementation of the matrix dot product operation:</p><div class="informalexample"><pre class="programlisting">__kernel void MatVecMultUsingDotFn(__global float4* matrix,__global float4* vector, __global float* result) {
    int i = get_global_id(0);
    result[i] = dot(matrix[i], vector[0]);
}</pre></div><p>To compile this on the OS X platform, you will have to run a compile command similar to this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>gcc –std=c99 –Wall –DUNIX –g –DDEBUG –DAPPLE –arch i386 –o MatVecMult matvecmult.c –framework OpenCL</strong></span>
</pre></div><p>Alternatively, you can type <code class="literal">make</code> in the source directory <code class="literal">Ch4/simple_dot_product/</code>. When that happens, you will have a binary executable named <code class="literal">MatVecMult</code>.</p><p>To run the program on OS X, simply execute the program <code class="literal">MatVecMult</code> and you should either see the output: <code class="literal">Check passed!</code> or <code class="literal">Check failed!</code> as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Check passed!</strong></span>
</pre></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec76"/>How it works…</h2></div></div></div><p>The previous code snippet is probably the simplest you will want to write to implement the matrix dot product operation. The kernel actually reads a vector of <code class="literal">4</code> floats from the <code class="literal">__global</code> memory spaces of both inputs, computes the dot product between them, and writes it back out to <code class="literal">__global</code> memory space of the destination. Previously, we mentioned that there might be another way to write this. Yes, there is and the relevant code is shown as follows:</p><div class="informalexample"><pre class="programlisting">__kernel void MatVecMult(const __global float* M,const __global float* V, uint width, uint height,__global float* W) {
    // Row index
    uint y = get_global_id(0);
    if (y &lt; height) {
       // Row pointer
       const __global float* row = M + y * width;
       // Compute dot product
       float dotProduct = 0;
      for (int x = 0; x &lt; width; ++x)
        dotProduct += row[x] * V[x];
    // Write result to global memory
    W[y] = dotProduct;
    }
}</pre></div><p>When you <a id="id334" class="indexterm"/>compare this implementation without using the dot API, you<a id="id335" class="indexterm"/> will discover that you not only need to type more but also you will have increased the number of work item variables which happens to be in the <code class="literal">__private</code> memory space; often you don't want to do this because it hinders the code readability, and also quite importantly scalability because too many registers are consumed.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note14"/>Note</h3><p>In OpenCL implementations, they would need to manage the available resources on the device, which could be available memory or available compute units. One such resource is the register file that contains a fixed number of general-purpose registers that the device has for executing one or many kernels. During the compilation of the OpenCL kernel, it will be determined how many registers are needed by each kernel for execution. An example would be where we assume that a kernel is developed that uses 10 variables in the <code class="literal">__private</code> memory space and the register file is <code class="literal">65536</code>, and that would imply that we can launch 65536 / 10 = 6553 work items to run our kernel. If you rewrite your kernel in such a way that uses more data sharing through the <code class="literal">__local</code> memory spaces, then you can free more registers and you can scale your kernel better.</p></div></div></div></div>
<div class="section" title="Using integer functions"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec35"/>Using integer functions</h1></div></div></div><p>The integer functions in <a id="id336" class="indexterm"/>OpenCL primarily provides useful ways in which you can use them to perform the usual mathematical calculations such as obtaining an absolute value, halving a value, locating the minimum or maximum of three values, cyclic shift of a number, and specialized form of multiplication which is designed to work for a certain class of problems. Many of the functions that we have mentioned such as <code class="literal">min</code> and <code class="literal">max</code> do not perform the comparisons in an atomic fashion, but if you do like to ensure that, then a class of atomic functions can be used instead and we'll examine them later.</p><p>A class of integer<a id="id337" class="indexterm"/> functions is<a id="id338" class="indexterm"/> the atomic functions, which allows the developer to swap values (single-precision floating-point values too) in an atomic fashion, and some of these functions implements <span class="strong"><strong>CAS</strong></span> (<span class="strong"><strong>Compare-And-Swap</strong></span>)<a id="id339" class="indexterm"/> semantics. Typically, you may want to ensure some sort of atomicity to certain operations because without that, you will encounter race conditions.</p><div class="mediaobject"><img src="graphics/4520OT_04_07.jpg" alt="Using integer functions"/></div><p>The atomic functions typically take in two inputs (they have to be of integral types, only <code class="literal">atomic_xchg</code> supports single-precision floating-point types), where the first argument is a pointer to a memory location in the global (<code class="literal">__global</code>) and local (<code class="literal">__local</code>) memory spaces ,and they are typically annotated with the <code class="literal">volatile</code> keyword, which prevents the compiler from optimizing the instructions related to the use of the variable; this is important as the reads and writes could be out of order and could affect the correctness of the program. The following is an illustration of a mental model of how atomic operations serialize the access to a piece of shared data:</p><div class="mediaobject"><img src="graphics/4520OT_04_08.jpg" alt="Using integer functions"/></div><p>The <a id="id340" class="indexterm"/>following<a id="id341" class="indexterm"/> example, <code class="literal">atomic_add</code>, has two versions which work on signed or unsigned values:</p><div class="informalexample"><pre class="programlisting">int atomic_add(volatile __global int*p, int val)
unsigned int atomic_add(volatile __global uint*p, uint val)</pre></div><p>Another observation you need to be aware of is the fact that just because you can apply atomicity to assert the correctness of certain values, it does not necessarily imply program correctness.</p><p>The reason why this is the case is due to the manner in which work items are implemented as we mentioned earlier in this chapter, that NVIDIA and ATI executes work items in groups known as work groups and each work group would contain multiple chunks of executing threads, otherwise, known as <span class="strong"><strong>warp</strong></span><a id="id342" class="indexterm"/> (32 threads) and <a id="id343" class="indexterm"/>
<span class="strong"><strong>wavefront</strong></span> (64 threads) respectively. Hence when a work group executes on a kernel, all the work items in that group are executing in lock-step and normally this isn't a problem. The problem arises when the work group is large enough to contain more than one warp/wavefront; then you have a situation where one warp/wavefront executes slower than another and this can be a big issue.</p><p>The real issue is that the memory ordering cannot be enforced across all compliant OpenCL devices; so the only way to tell the kernel that we like the loads and stores to be coordinated is by putting a memory barrier at certain points in your program. When such a barrier is present, the compiler will generate the instructions that will make sure all the loads-stores to the global/local memory space prior to the barrier is done for all the executing work items before executing any instructions that come after the barrier, which will guarantee that the updated data is seen; or in compiler lingo: memory loads and stores will be committed to the memory before any loads and stores follows the barrier/fence.</p><p>These APIs provide the developer with a much better level of control when it comes to ordering both reads and writes, reads only, or writes only. The argument flags, can take a combination of <code class="literal">CLK_LOCAL_MEM_FENCE</code> and/or <code class="literal">CLK_GLOBAL_MEM_FENCE</code>.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec77"/>Getting ready</h2></div></div></div><p>The recipe will show <a id="id344" class="indexterm"/>you the code snippet in <code class="literal">Ch4/par_min/par_min.cl</code> for finding<a id="id345" class="indexterm"/> the minimum value in a large array in the device, that is, GPU or CPU memory space. This example combines a few concepts such as using the OpenCL's atomic directives to enable atomic functions and memory barriers to coordinate memory loads and stores.</p></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec78"/>How to do it…</h2></div></div></div><p>The following code demonstrates how you might want to find the minimum number in a large container of integers:</p><div class="informalexample"><pre class="programlisting">#pragma OPENCL EXTENSION cl_khr_local_int32_extended_atomics : enable
#pragma OPENCL EXTENSION cl_khr_global_int32_extended_atomics : enable
__kernel void par_min(__global uint4* src,__global uint * globalMin, __local  uint * localMin,int numOfItems) {
    uint count = ( numOfItems / 4) / get_global_size(0);
    uint index = get_global_id(0) * count;
    uint stride = 1;
    uint partialMin = (uint) -1;
    for(int i = 0; i &lt; count; ++i,index += stride) {
      partialMin = min(partialMin, src[index].x);
      partialMin = min(partialMin, src[index].y);
      partialMin = min(partialMin, src[index].z);
      partialMin = min(partialMin, src[index].w);
    }
    if(get_local_id(0) == 0) localMin[0] = (uint) -1;
      barrier(CLK_LOCAL_MEM_FENCE);
    atomic_min(localMin, partialMin);
    barrier(CLK_LOCAL_MEM_FENCE);
    if (get_local_id(0) == 0)
      globalMin[ get_group_id[0] ] = localMin[0];
}
__kernel void reduce(__global uint4* src,__global uint * globalMin) {
    atom_min(globalMin, globalMin[get_global_id(0)]);
}</pre></div><p>To compile it on the OS X platform, you will have to run a compile command similar to this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>gcc –std=c99 –Wall –DUNIX –g –DDEBUG –DAPPLE –arch i386 –o ParallelMin par_min.c –framework OpenCL</strong></span>
</pre></div><p>Alternatively, you can type <code class="literal">make</code> in the source directory <code class="literal">Ch4/par_min/</code>. When that happens, you will have a binary executable named <code class="literal">ParallelMin</code>.</p><p>To run the program on OS X, simply execute the program <code class="literal">ParallelMin</code> and you should either see the output: <code class="literal">Check passed!</code> or <code class="literal">Check failed!</code> as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Check passed!</strong></span>
</pre></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec79"/>How it works…</h2></div></div></div><p>The way this <a id="id346" class="indexterm"/>works is that a work item walks through the source <a id="id347" class="indexterm"/>buffer and attempts to locate the minimum value in parallel, and when the kernel is running on the CPU or GPU, the source buffer is chopped evenly between those threads and each thread would walk through the buffer that's assigned to them in <code class="literal">__global memory</code> and reduces all values into a minimum value in the <code class="literal">__private</code> memory.</p><p>Subsequently, all threads will reduce the minimum values in their <code class="literal">__private</code> memories to <code class="literal">__local</code> memory via an atomic operation and this reduced value is flushed to the <code class="literal">_</code>
<code class="literal">_global</code> memory.</p><p>Once the work groups have completed the execution, the second kernel, that is, <code class="literal">reduce</code> will reduce all the work group values into a single value in the <code class="literal">__global</code> memory using an atomic operation.</p></div></div>
<div class="section" title="Using floating-point functions"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec36"/>Using floating-point functions</h1></div></div></div><p>So far, you have seen a<a id="id348" class="indexterm"/> couple of functions that takes argument as <a id="id349" class="indexterm"/>input or output single-precision or double-precision floating-point values. Given a floating-point value <span class="emphasis"><em>x</em></span>, the OpenCL floating-point functions provide you with the capability to extract the mantissa and exponent<a id="id350" class="indexterm"/> from <span class="emphasis"><em>x</em></span> via <code class="literal">frexp()</code>, decompose <span class="emphasis"><em>x</em></span> via <code class="literal">modf()</code>, compute<a id="id351" class="indexterm"/> the next largest/smallest single-precision floating-point value via<a id="id352" class="indexterm"/> <code class="literal">nextafter()</code>, and others. Considering that there are so many useful floating-point functions, there are two functions which are important to understand because it's very common in OpenCL code. They are the <code class="literal">mad()</code> <a id="id353" class="indexterm"/>and<a id="id354" class="indexterm"/> <code class="literal">fma()</code> functions which is Multiply-Add and Fused Multiply-Add instruction respectively.</p><p>The <span class="strong"><strong>Multiply-Add</strong></span> (<span class="strong"><strong>MAD</strong></span>) instruction<a id="id355" class="indexterm"/> performs a floating-point multiplication followed by a floating-point addition, but whether the product and its intermediary products are rounded is undefined. The <span class="strong"><strong>Fused Multiply-Add</strong></span> (<span class="strong"><strong>FMA</strong></span>) instruction<a id="id356" class="indexterm"/> only rounds the product and none of its intermediary products. The implementations typically trade off the precision against the speed of the operations.</p><p>We probably<a id="id357" class="indexterm"/> shouldn't dive into academic studies of this nature; however in<a id="id358" class="indexterm"/> times like this, we thought it might be helpful to point out how academia in many situations can help us to make an informed decision. Having said that, a particular study by Delft University of Technology entitled <span class="emphasis"><em>A Comprehensive Performance Comparison of CUDA</em></span> and OpenCL link <a class="ulink" href="http://www.pds.ewi.tudelft.nl/pubs/papers/icpp2011a.pdf">http://www.pds.ewi.tudelft.nl/pubs/papers/icpp2011a.pdf</a>, suggests that FMA has a higher instruction count as compared to MAD implementations, which might lead us to the conclusion that MAD should run faster than FMA. We can guess approximately how much faster by taking a simple ratio between both instruction counts, which we should point out is a really simplistic view since we should not dispense away the fact that compiler vendors play a big role with their optimizing compilers, and to highlight that NVIDIA conducted a study entitled <span class="emphasis"><em>Precision &amp; Performance: Floating Point and IEEE 754 compliance for NVIDIA GPUs</em></span>, which can be read at: <a class="ulink" href="http://developer.download.nvidia.com/assets/cuda/files/NVIDIA-CUDA-Floating-Point.pdf">http://developer.download.nvidia.com/assets/cuda/files/NVIDIA-CUDA-Floating-Point.pdf</a>. The study suggests that FMA can offer performance in addition to precision, and NVIDIA is at least one company that we are aware of who is replacing MAD with FMA in their GPU chips.</p><p>Following the subject of multiplication, you should be aware that there are instructions for the multiplication of integers instead of floats; examples of those are <code class="literal">mad_hi</code>, <code class="literal">mad_sat</code>, and <code class="literal">mad24</code>, and these functions provide the developer with the fine grain control of effecting a more efficient computation and how it can be realized using these optimized versions. For example, <code class="literal">mad24</code> only operates on the lower 24-bits of a 32-bit integer because the expected value is in the range of [-223, 223 -1] when operating signed integers or [0, 224 -1] for unsigned integers.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec80"/>Getting ready</h2></div></div></div><p>The code snippet in <code class="literal">Ch4/simple_fma_vs_mad/fma_mad_cmp.cl</code> demonstrates how we can test the performance between the MAD and FMA instructions, if you so wish, to accomplish the computation. However, what we are going to demonstrate is to simply run each one of the kernels in turn, and we can check that the results are the same in both computations.</p></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec81"/>How to do it…</h2></div></div></div><p>The following code demonstrates how to use the MAD and FMA functions in OpenCL:</p><div class="informalexample"><pre class="programlisting">__kernel void mad_test(__global float* a, __global float* b, __global float* c, __global float* result) {
  float temp = mad(a, b, c);
  result[get_global_id(0)] = temp;
}
__kernel void fma_test(__global float* a, __global float* b,__global float* c, __global float* result) {
  float temp = fma(a, b, c);
  result[get_global_id(0)] = temp;
}</pre></div><p>To compile it on the OS X platform, you will have to run a compile command similar to this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>gcc –std=c99 –Wall –DUNIX –g –DDEBUG –DAPPLE –arch i386 –o FmaMadCmp fma_mad_cmp.c –framework OpenCL</strong></span>
</pre></div><p>Alternatively, you can type <code class="literal">make</code> in the source directory <code class="literal">Ch4/simple_fma_vs_mad/</code>. When that happens, you will have a binary executable named <code class="literal">FmaMadCmp</code>.</p><p>To run the program on OS X, simply execute the program <code class="literal">FmaMadCmp</code> and you should either see the output: <code class="literal">Check passed!</code> or <code class="literal">Check failed!</code> as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Check passed!</strong></span>
</pre></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec82"/>How it works…</h2></div></div></div><p>The driver code <a id="id359" class="indexterm"/>uses single-precision floating-point values to <a id="id360" class="indexterm"/>compute the value of the equation by running the two kernels in turn on the GPU/CPU. Each kernel would load the values from the <code class="literal">__global</code> memory space to the work item/thread's <code class="literal">__private</code> memory space. The difference between both kernels is that one uses the FMA instruction while the other uses the MAD instruction. The method that is used to detect whether FMA instruction support is available on the device of choice is to detect whether <code class="literal">CP_FP_FMA</code> is returned after a call to <code class="literal">clGetDeviceInfo</code> passing in any of the following parameters: <code class="literal">CL_DEVICE_SINGLE_FP_CONFIG</code>, <code class="literal">CL_DEVICE_DOUBLE_FP_CONFIG</code>, and <code class="literal">CL_DEVICE_HALF_FP_CONFIG</code>. We use the flag <code class="literal">CP_FP_FMA</code> and <code class="literal">FP_FAST_FMA</code> to load the <code class="literal">fma</code> functions on our platform by including the header file <code class="literal">#include &lt;math.h&gt;</code>.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note15"/>Note</h3><p>The C-macro <code class="literal">FP_FAST_FMA</code>, if defined is set to the constant of 1 to indicate that the <code class="literal">fma()</code> generally executes about as fast, or faster than, a multiple and an addition of double operands. If this macro is undefined, then it implies that your hardware doesn't support it.In the GNU GCC compiler suite, the macro you want to detect is <code class="literal">__FP_FAST_FMA</code>, which links to the <code class="literal">FP_FAST_FMA</code> if defined or passing <code class="literal">–mfused-madd</code> to the GCC compiler (on by default, autogenerate the FMA instructions if ISA supports).</p></div></div></div></div>
<div class="section" title="Using trigonometric functions"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec37"/>Using trigonometric functions</h1></div></div></div><p>The<a id="id361" class="indexterm"/> trigonometric functions<a id="id362" class="indexterm"/> are very useful if you were in the computer graphics industry ,or you are writing a simulation program for weather forecasts, continued fractions, and so on. OpenCL provides the usual suspects when it comes to the trigonometry support with <code class="literal">cos</code>, <code class="literal">acos</code>, <code class="literal">sin</code>, <code class="literal">asin</code>, <code class="literal">tan</code>, <code class="literal">atan</code>, <code class="literal">atanh</code> (hyperbolic arc tangent), <code class="literal">sinh</code> (hyperbolic sine), and so on.</p><p>In this section, we will <a id="id363" class="indexterm"/>take a look at the popular trigonometric identity function:</p><div class="informalexample"><pre class="programlisting">
<span class="emphasis"><em>sin2 + cos2 = 1</em></span>
</pre></div><p>From the Pythagoras's theorem, we understood that a right-angled triangle with sides <span class="emphasis"><em>a</em></span>,<span class="emphasis"><em>b</em></span>,<span class="emphasis"><em>c</em></span> and angle <span class="emphasis"><em>t</em></span> at the vertex where <span class="emphasis"><em>a</em></span> and <span class="emphasis"><em>c</em></span> meet, <span class="emphasis"><em>cos(t)</em></span> is by definition <span class="emphasis"><em>a</em></span>/<span class="emphasis"><em>c</em></span>, <span class="emphasis"><em>sin(t</em></span>
<code class="literal">)</code> is by definition <span class="emphasis"><em>b</em></span>/<span class="emphasis"><em>c</em></span>, and so <span class="emphasis"><em>cos2(t) + sin2(t) = (a/c)2 + (b/c)2</em></span> when combined with the fact that <span class="emphasis"><em>a2 + b2 = c2</em></span> hence <span class="emphasis"><em>cos2(t) + sin2(t) = 1</em></span>.</p><p>Having armed ourselves with this knowledge, there are many interesting problems you can solve with this identity but for the sake of illustration let's suppose that we want to find the number of unit circles.</p><p>Unit circles are another way of looking at the identity we just talked about. A contrived example of this would be to determine which values would be valid unit circles from the given two arrays of supposedly values in degrees.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec83"/>Getting ready</h2></div></div></div><p>The code snippet in <code class="literal">Ch4/simple_trigonometry/simple_trigo.cl</code> demonstrates the OpenCL kernel that is used to compute which values from the two data sources can correctly form a unit circle.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note16"/>Note</h3><p>If you recall from basic trigonometry lessons you took, when you add the result of <span class="emphasis"><em>sin(x) + cos(x)</em></span> where <span class="emphasis"><em>x</em></span> is drawn from either positive or negative numbers, it will produce two distinct straight line functions <span class="emphasis"><em>y = 1</em></span> and <span class="emphasis"><em>y = -1</em></span> and when you square the results of <span class="emphasis"><em>sin(x)</em></span> and <span class="emphasis"><em>cos(x)</em></span>, the result of <span class="emphasis"><em>cos2(t) + sin2(t) = 1</em></span> is obvious. See the following diagrams for illustration:</p></div></div><div class="mediaobject"><img src="graphics/4520OT_04_09.jpg" alt="Getting ready"/></div><p>The preceding <a id="id364" class="indexterm"/>diagram and the following diagram <a id="id365" class="indexterm"/>reflect the graphs of <span class="emphasis"><em>sin(x)</em></span> and <span class="emphasis"><em>cos(x)</em></span> respectively:</p><div class="mediaobject"><img src="graphics/4520OT_04_10.jpg" alt="Getting ready"/></div><p>The following diagram illustrates how superimposing the previous two graphs would give a straight line that is represented by the equation:</p><div class="mediaobject"><img src="graphics/4520OT_04_11.jpg" alt="Getting ready"/></div></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec84"/>How to do it…</h2></div></div></div><p>The following<a id="id366" class="indexterm"/> code snippet shows you the kernel code that will determine<a id="id367" class="indexterm"/> unit circles:</p><div class="informalexample"><pre class="programlisting">__kernel void find_unit_circles(__global float16* a,__global float16* b, __global float16* result) {
    uint id = get_global_id(0);
    float16 x = a[id];
    float16 y = b[id];
    float16 tresult = sin(x) * sin(x) + cos(y) * cos(y);
    result[id] = tresult;
}</pre></div><p>To compile it on the OS X platform, you will have to run a compile command similar to this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>gcc –std=c99 –Wall –DUNIX –g –DDEBUG –DAPPLE –arch i386 –o SimpleTrigo simple_trigo.c –framework OpenCL</strong></span>
</pre></div><p>Alternatively, you can type <code class="literal">make</code> in the source directory <code class="literal">Ch4/simple_trigonometry/</code>. When that happens, you will have a binary executable named <code class="literal">SimpleTrigo</code>.</p><p>To run the program <a id="id368" class="indexterm"/>on OS X, simply execute the program <code class="literal">SimpleTrigo</code> and<a id="id369" class="indexterm"/> you should either see the output shown as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Find Unit Circle:</strong></span>
<span class="strong"><strong>Unit circle with x=1, y=1</strong></span>
</pre></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec85"/>How it works…</h2></div></div></div><p>The driver program conducts its usual operations of loading the two data sources by filling it up with values. Then the data sources is registered on the device command queue along with the kernel program objects that are ready for execution.</p><p>During the execution of the kernel, the data sources are loaded into the device via a single-precision floating-point 16-element vector. As highlighted in previous chapters, this takes advantage of the device's vectorized hardware. The in-memory vectors are passed into the sine and cosine functions which comes in two versions where one takes a scalar value and second takes a vector value, and we flush the result out to global memory once we are done; and you will notice that the multiplication/addition operator actually does component-wise multiplication and addition.</p></div></div>
<div class="section" title="Arithmetic and rounding in OpenCL"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec38"/>Arithmetic and rounding in OpenCL</h1></div></div></div><p>Rounding <a id="id370" class="indexterm"/>is an important topic in OpenCL and we have not really dived into it yet but that's about to change. OpenCL 1.1 supports four rounding modes: round to nearest (even number), round to zero, round to positive infinity, and round to negative infinity. The only round mode required by OpenCL 1.1 compliant devices is the round to nearest even.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note17"/>Note</h3><p>If the result is intermediate between two representable values, the even representation is chosen. Even, here, means that the lowest bit is zero.</p></div></div><p>You should be<a id="id371" class="indexterm"/> aware that these are applicable to single-precision <a id="id372" class="indexterm"/>floating-point values supported in OpenCL 1.1; we have to<a id="id373" class="indexterm"/> check with the vendors who provide functions that operate <a id="id374" class="indexterm"/>on double-precision floating-point values, though the author suspects that they should comply at least to support the round to nearest even mode.</p><p>Another point is that, you cannot programmatically configure your kernels to inherit/change the rounding mode used by your calling environment, which most likely is where your program executes on the CPU. In GCC at least, you can actually use the inline assembly directives, for example, <code class="literal">asm("assembly code inside quotes")</code> to change the rounding mode in your program by inserting appropriate hardware instructions to your program. The <a id="id375" class="indexterm"/>next section attempts to demonstrate how this can be done by using the <a id="id376" class="indexterm"/>regular <a id="id377" class="indexterm"/>C programming with<a id="id378" class="indexterm"/> a little help from GCC.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note18"/>Note</h3><p>In the Intel 64 and IA-32 architectures, the rounding mode is controlled by a 2-bit <span class="strong"><strong>rounding control</strong></span> (<span class="strong"><strong>RC</strong></span>) field<a id="id379" class="indexterm"/>, and the implementation is hidden in two hardware registers: <span class="strong"><strong>x87 FPU</strong></span> control register<a id="id380" class="indexterm"/> and <a id="id381" class="indexterm"/>
<span class="strong"><strong>MXCSR</strong></span> register. These two registers have the RC field and the RC in the x87 FPU control register is used by the CPU when computations are performed in the x87 FPU, while the RC field in the MXCSR is used to control rounding for <span class="strong"><strong>SIMD</strong></span> floating-point<a id="id382" class="indexterm"/> computations performed with the <a id="id383" class="indexterm"/>
<span class="strong"><strong>SSE/SSE2</strong></span> instructions.</p></div></div><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec86"/>Getting ready</h2></div></div></div><p>In the code snippet found in <code class="literal">Ch4/simple_rounding/simple_rounding.cl</code>, we demonstrate how <span class="emphasis"><em>round to nearest even</em></span> mode is the default mode in the built-in functions provided by OpenCL 1.1. The example proceeds to demonstrate how a particular built-in function and remainder, will use the default rounding mode to store the result of a floating-point computation. The next couple of operations is to demonstrate the usage of the following OpenCL built-in functions such as <code class="literal">rint</code>, <code class="literal">round</code>, <code class="literal">ceil</code>, <code class="literal">floor</code>, and <code class="literal">trunc</code>.</p></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec87"/>How to do it…</h2></div></div></div><p>The following code snippet examines the various rounding modes:</p><div class="informalexample"><pre class="programlisting">__kernel void rounding_demo(__global float *mod_input, __global float *mod_output, __global float4 *round_input,__global float4 *round_output) {
    mod_output[1] = remainder(mod_input[0], mod_input[1]);
    round_output[0] = rint(*round_input);
    round_output[1] = round(*round_input);
    round_output[2] = ceil(*round_input);
    round_output[3] = floor(*round_input);
    round_output[4] = trunc(*round_input);
}</pre></div><p>To compile it on the OS X platform, you will have to run a compile command similar to this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>gcc –std=c99 –Wall –DUNIX –g –DDEBUG –DAPPLE –arch i386 –o SimpleRounding simple_rounding.c –framework OpenCL</strong></span>
</pre></div><p>Alternatively, you can type <code class="literal">make</code> in the source directory <code class="literal">Ch4/simple_rounding/</code>. When that happens, you will have a binary executable named <code class="literal">SimpleRounding</code>.</p><p>To run the program on OS X, simply execute the program <code class="literal">SimpleRounding</code> and you should either see the output shown as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Input: -4.5f, -1.5f, 1.5f, 4.5f</strong></span>
<span class="strong"><strong>Rint:</strong></span>
<span class="strong"><strong>Round:</strong></span>
<span class="strong"><strong>Ceil:</strong></span>
<span class="strong"><strong>Floor:</strong></span>
<span class="strong"><strong>Trunc:</strong></span>
</pre></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec88"/>How it works…</h2></div></div></div><p>As before, the in-memory<a id="id384" class="indexterm"/> data structures on the host are initialized with <a id="id385" class="indexterm"/>values and they are issued to the device once the device's <a id="id386" class="indexterm"/>command queue is created; once that's done the <a id="id387" class="indexterm"/>kernel is sent off to the command queue for execution. The results is subsequently read back from the device and displayed on the console.</p><p>In order to understand how these functions work, is important that we study their behavior by first probing their method signatures, and subsequently analyzing the results of executing the program to gain insights into how the results came to be.</p></div><div class="section" title="There's more…"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec89"/>There's more…</h2></div></div></div><p>OpenCL 1.2 brings a wealth of mathematical functions to arm the developer and four of the common ones are computing the floor and ceiling, round-to-integral, truncation, and rounding floating-point values. The floor's method signature is:</p><div class="informalexample"><pre class="programlisting">gentype floor(gentype x);
// gentype can be float,float2,float3,float4,float8,float16</pre></div><p>This function rounds to the integral value using the <span class="emphasis"><em>round to negative infinity</em></span> rounding mode. First of all, your OpenCL device needs to support this mode of rounding, and you can determine this by checking the existence of the value <code class="literal">CL_FP_ROUND_TO_INF</code> when you pass in <code class="literal">CL_DEVICE_DOUBLE_FP_CONFIG</code> to <code class="literal">clGetDeviceInfo(device_id, ...)</code>.</p><p>The next method, ceil's signature is:</p><div class="informalexample"><pre class="programlisting">gentype ceil(gentype x);
// gentype can be float,float2,float3,float4,float8,float16</pre></div><p>This function rounds to the integral value using the <span class="emphasis"><em>round to positive infinity</em></span> rounding mode.</p><p>Be aware that when a value between <code class="literal">-1</code> and <code class="literal">0</code> is passed to <code class="literal">ceil</code>, then the result is automatically <code class="literal">-0</code>.</p><p>The method for rounding to the integral value has a signature like this:</p><div class="informalexample"><pre class="programlisting">gentype rint(gentype x);
// gentype can be float,float2,float3,float4,float8,float16</pre></div><p>This function<a id="id388" class="indexterm"/> rounds to the integral value using the <span class="emphasis"><em>round to nearest even</em></span> rounding <a id="id389" class="indexterm"/>mode.</p><p>Be aware that when a value<a id="id390" class="indexterm"/> between <code class="literal">-0.5</code> and <code class="literal">0</code> is passed to <code class="literal">rint</code>, then the result is<a id="id391" class="indexterm"/> automatically <code class="literal">-0</code>.</p><p>The truncation function<a id="id392" class="indexterm"/> is very <a id="id393" class="indexterm"/>useful when precision is not high on your priority list and its method signature is:</p><div class="informalexample"><pre class="programlisting">gentype trunc(gentype x);
// gentype can be float,float2,float3,float4,float8,float16</pre></div><p>This function rounds to the integral value using the <span class="emphasis"><em>round to zero</em></span> rounding mode.</p><p>The rounding method signature is:</p><div class="informalexample"><pre class="programlisting">gentype round(gentype x);
// gentype can be float,float2,float3,float4,float8,float16</pre></div><p>This function returns the integral value nearest to <span class="emphasis"><em>x</em></span> rounding halfway cases away from zero, regardless of the current rounding direction. The full list of available functions can be found in the <span class="emphasis"><em>Section 6.12.2</em></span> of the OpenCL 1.2 specification.</p><p>When you run the program, you should get the following result:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Input: -4.5, 1.5, 1.5, 4.5</strong></span>
<span class="strong"><strong>Rint:  -4.0, -2.0, 2.0, 4.0</strong></span>
<span class="strong"><strong>Round: -5.0, -2.0, 2.0, 5.0</strong></span>
<span class="strong"><strong>Ceil:  -4.0, -1.0, 2.0, 5.0</strong></span>
<span class="strong"><strong>Floor: -5.0, -2.0, 1.0, 4.0</strong></span>
<span class="strong"><strong>Trunc: -4.0, -1.0, 1.0, 4.0</strong></span>
</pre></div></div></div>
<div class="section" title="Using the shuffle function in OpenCL"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec39"/>Using the shuffle function in OpenCL</h1></div></div></div><p>The <code class="literal">shuffle</code> and <code class="literal">shuffle2</code> functions were introduced in OpenCL 1.1 to construct a permutation of elements from their inputs (which are either one vector or two vectors), and returns a vector of the <a id="id394" class="indexterm"/>same type as its input; the number of elements in the returned vector is determined by the<a id="id395" class="indexterm"/> argument, <code class="literal">mask</code>, that is passed to it. Let's take a look at its method signature:</p><div class="informalexample"><pre class="programlisting">gentypeN shuffle(gentypeM x, ugentypeN mask);
gentypeN shuffle(gentypeM x, gentypeM y, ugentypeN mask);</pre></div><p>The <code class="literal">N</code> and <code class="literal">M</code> used in the signatures represents the length of the returned and input vectors and can take values from {<code class="literal">2</code>,<code class="literal">3</code>,<code class="literal">4</code>,<code class="literal">8</code>,<code class="literal">16</code>}. The <code class="literal">ugentype</code> represents an unsigned type, <code class="literal">gentype</code> represents the integral types in OpenCL, and floating-point types (that is, half, single, or double-precision) too; and if you choose to use the floating-point types then recall the extensions <code class="literal">cl_khr_fp16</code> or <code class="literal">cl_khr_fp64</code>.</p><p>Here's an example of how it works:</p><div class="informalexample"><pre class="programlisting">uint4 mask = {0,2,4,6};
uint4 elements = {0,1,2,3,4,5,6};
uint4 result = shuffle(elements, mask);
// result = {0,2,4,6};</pre></div><p>Let's take a look at a simple implementation where we draw our inspiration from the popular <a id="id396" class="indexterm"/>
<span class="strong"><strong>Fisher-Yates Shuffle</strong></span>(<span class="strong"><strong>FYS</strong></span>) algorithm. This FYS algorithm generates a random permutation of a finite set and the basic process is similar to randomly picking a numbered ticket from a container, or cards from a deck, one after another until none is left in the container/deck. One of the nicest properties of this algorithm is that it is guaranteed to produce an unbiased result. Our example would focus on how shuffling would work, since what it essentially does is to select a particular element based on a mask that's supposed to be randomly generated.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec90"/>Getting ready</h2></div></div></div><p>The code snippet in <code class="literal">Ch4/simple_shuffle/simple_shuffle.cl</code> pretty much captured most of the ideas we are trying to illustrate. The idea is simple, we want to generate a mask and use the mask to generate permutations of the output array. We are not going to use a pseudo random number generator like the Mersenne twister, but rather rely on C's <code class="literal">stdlib.h</code> function, a <a id="id397" class="indexterm"/>random function with a valid seed from which we generate a bunch of random numbers where each number cannot exceed the maximum size of the array of the output array, that is, <code class="literal">15</code>.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note19"/>Note</h3><p>The <code class="literal">rand()</code> function<a id="id398" class="indexterm"/> in <code class="literal">stdlib.h</code> is not really favored because it generates a less random sequence<a id="id399" class="indexterm"/> than <code class="literal">random()</code>, because the lower dozen bits generated by <code class="literal">rand()</code> go through a cyclic pattern.</p></div></div></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec91"/>How to do it…</h2></div></div></div><p>Before we begin the shuffling, <a id="id400" class="indexterm"/>we need to seed the RNG prior, and we can do that <a id="id401" class="indexterm"/>via a simple API call to <code class="literal">srandom()</code> passing the seed. The next step is to run our kernel a number of times and we achieve this by enclosing the kernel execution in a loop. The following code snippet from the host code in <code class="literal">Ch4/simple_shuffle/simple_shuffle.c</code> shows this:</p><div class="informalexample"><pre class="programlisting">#define ITERATIONS 6
#define DATA_SIZE 1024
srandom(41L);
  for(int iter = 0; iter &lt; ITERATIONS; ++iter) {
    for(int i = 0; i &lt; DATA_SIZE; ++i) {
      mask[i] = random() % DATA_SIZE;
      // kernel is invoked
    }// end of inner-for-loop
   }//end of out-for-loop</pre></div><p>The following kernel code transports the inputs via <span class="emphasis"><em>a</em></span> and <span class="emphasis"><em>b</em></span> and their combined element size is <code class="literal">16</code>, the mask is being transported on the constant memory space (that is, read-only).</p><div class="informalexample"><pre class="programlisting">__kernel void simple_shuffle(__global float8* a,__global float8* b, __constant uint16 mask,__global float16* result) {
    uint id = get_global_id(0);
    float8 in1 = a[id];
    float8 in2 = b[id];
    result[id] = shuffle2(in1, in2, mask);
}</pre></div><p>To compile it on the OS X platform, you will have to run a compile command similar to this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>gcc –std=c99 –Wall –DUNIX –g –DDEBUG –DAPPLE –arch i386 –o SimpleShuffle simple_shuffle.c –framework OpenCL</strong></span>
</pre></div><p>Alternatively, you can type <code class="literal">make</code> in the source directory <code class="literal">Ch4/simple_shuffle/.</code> When that happens, you will have a binary executable named <code class="literal">SimpleShuffle</code>.</p><p>To run the program on OS X, simply execute the program <code class="literal">SimpleShuffle</code> and you should see the output shown as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Shuffle: -4.5f, -1.5f, 1.5f, 4.5f</strong></span>
</pre></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec92"/>How it works…</h2></div></div></div><p>The following <a id="id402" class="indexterm"/>diagram suggests that each executing kernel works through a <a id="id403" class="indexterm"/>portion of the source array, which contains of <span class="emphasis"><em>k</em></span> elements by fetching the data from the <code class="literal">__global</code> memory space to the <code class="literal">__private</code> memory space. The next operation is to run the shuffling using a vector of random numbers, which we have pregenerated on the host and for each partitioned data block, the kernel will produce a resultant array; and once that's done the kernel flushes out the data to the <code class="literal">__global</code> memory space. The following diagram illustrates the idea where the resultant array consists of a permutated array made from its individual constituents which are themselves permutations:</p><div class="mediaobject"><img src="graphics/4520OT_04_12.jpg" alt="How it works…"/></div></div></div>
<div class="section" title="Using the select function in OpenCL"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec40"/>Using the select function in OpenCL</h1></div></div></div><p>The <code class="literal">select</code> function is<a id="id404" class="indexterm"/> first of all similar to the <code class="literal">shuffle</code> and <code class="literal">shuffle2</code> functions we have seen in the previous section and is also known as the <a id="id405" class="indexterm"/>
<span class="strong"><strong>ternary selection</strong></span>, and it is a member of the relational functions in OpenCL, which is commonly found in the C++ and Java programming languages; but there is a significant difference and that is the <code class="literal">select</code> function and its variant <code class="literal">bitselect</code> works not only with single-precision or double-precision floating types, but also <a id="id406" class="indexterm"/>vectors of single-precision or double-precision floating-point values. Here's what it looks like:</p><div class="informalexample"><pre class="programlisting">(predicate_is_true? eval_expr_if_true : eval_expr_if_false)</pre></div><p>Hence, when the predicate is evaluated to be true the expression on the left-hand side of the colon will be evaluated; otherwise the expression on the right-hand side of the colon is evaluated and in both evaluations, a result is returned.</p><p>Using an example in OpenCL, the conditional statement as follows:</p><div class="informalexample"><pre class="programlisting">if (x == 1) r = 0.5;
if (x == 2) r = 1.0;</pre></div><p>can be rewritten using the <code class="literal">select()</code> function as:</p><div class="informalexample"><pre class="programlisting">r = select(r, 0.5, isequal(x, 1));
r = select(r, 1.0, isequal(x, 2));</pre></div><p>And for such a transformation to be correct, the original <code class="literal">if</code> statement cannot contain any code that calls to I/O.</p><p>The main advantage <code class="literal">select</code>/<code class="literal">bitselect</code> offers is that vendors can choose to eradicate branching and branch predication from its implementation, which means that the resultant program is likely to be more efficient. What this means is that these two functions act as a façade so that vendors such as AMD could implement the actual functionality using the ISA of SSE2 <code class="literal">__mm_cmpeq_pd</code>, and <code class="literal">__mm_cmpneq_pd</code> ; similarly, Intel could choose from the ISA of Intel AVX such as <code class="literal">__mm_cmp_pd</code>, <code class="literal">__mm256_cmp_pd</code>, or from SSE2 to implement the functionality of <code class="literal">select</code> or <code class="literal">bitselect</code>.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec93"/>Getting ready</h2></div></div></div><p>The following example demonstrates how we can use the function, <code class="literal">select</code>. The function demonstrates the convenience that it offers since it operates on the abstraction of applying a function to several data values, which happens to be in a vector. The code snippet in <code class="literal">Ch4/simple_select_filter/select_filter.cl</code> attempts to conduct a selection by picking the elements from each list in turn to establish the result, which in this example happens to be a vector.</p></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec94"/>How to do it…</h2></div></div></div><p>The following snippet demonstrates how to do use the <code class="literal">select</code> function in OpenCL:</p><div class="informalexample"><pre class="programlisting">__kernel void filter_by_selection(__global float8* a,__global float8* b, __global float8* result) {
    uint8 mask = (uint8)(0,-1,0,-1,0,-1,0,-1);
    uint id = get_global_id(0);
    float8 in1 = a[id];
    float8 in2 = b[id];
    result[id] = select(in1, in2, mask);
}</pre></div><p>To compile it on <a id="id407" class="indexterm"/>the OS X platform, you will have to run a compile<a id="id408" class="indexterm"/> command similar to this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>gcc –std=c99 –Wall –DUNIX –g –DDEBUG –DAPPLE –arch i386 –o SelectFilter simple_select.c –framework OpenCL</strong></span>
</pre></div><p>Alternatively, you can type <code class="literal">make</code> in the source directory <code class="literal">Ch4/simple_select/</code>. When that happens, you will have a binary executable named <code class="literal">SelectFilter</code>.</p><p>To run the program on OS X, simply execute the program <code class="literal">SelectFilter</code> and you should either see the output shown as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>select: -4.5f, -1.5f, 1.5f, 4.5f</strong></span>
</pre></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec95"/>How it works…</h2></div></div></div><p>The program proceeds to establish a context to the OpenCL compliant device through the APIs <code class="literal">clGetPlatformIDs</code> and <code class="literal">clGetDeviceIDs</code>. Once that is established, we go about creating our in-memory data structures and prepare it for submission to the device's command queue.</p><p>The in-memory data structures on the host are small arrays, which we can submit to the device for consumption by sending it across the system bus to hydrate the structures in the device memory. They stay in the device memory as local variables represented by variables <code class="literal">in1</code> and <code class="literal">in2</code>.</p><p>Once the data is inflated in the device's memory, the algorithm in <code class="literal">select_filter.cl</code> will proceed to select each element in turn by conducting a bit comparison where the most significant bit is checked; if the MSB is equal to <code class="literal">1</code> the corresponding value from <span class="strong"><strong>Buffer B</strong></span> is returned; otherwise the corresponding position from <a id="id409" class="indexterm"/>
<span class="strong"><strong>Buffer A</strong></span> is returned. Recall from computer science that -1, that is, unary minus 1, works out to be <code class="literal">0xffff</code> in 2's complement notation and hence the MSB of that value would most definitely be equal to <code class="literal">1</code>.</p><p>The following diagram illustrates this selection process. As before, once the selection process is completed, it is <a id="id410" class="indexterm"/>flushed <a id="id411" class="indexterm"/>out to the results vector, result.</p><div class="mediaobject"><img src="graphics/4520OT_04_13.jpg" alt="How it works…"/></div></div></div></body></html>