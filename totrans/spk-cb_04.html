<html><head></head><body><div class="chapter" title="Chapter&#xA0;4.&#xA0;Spark SQL"><div class="titlepage"><div><div><h1 class="title"><a id="ch04"/>Chapter 4. Spark SQL</h1></div></div></div><p>Spark SQL<a id="id202" class="indexterm"/> is a Spark module for processing a structured data. This chapter is divided into the following recipes:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Understanding the Catalyst optimizer</li><li class="listitem" style="list-style-type: disc">Creating HiveContext</li><li class="listitem" style="list-style-type: disc">Inferring schema using case classes</li><li class="listitem" style="list-style-type: disc">Programmatically specifying the schema</li><li class="listitem" style="list-style-type: disc">Loading and saving data using the Parquet format</li><li class="listitem" style="list-style-type: disc">Loading and saving data using the JSON format</li><li class="listitem" style="list-style-type: disc">Loading and saving data from relational databases</li><li class="listitem" style="list-style-type: disc">Loading and saving data from an arbitrary source</li></ul></div><div class="section" title="Introduction"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec30"/>Introduction</h1></div></div></div><p>Spark can process data from various data sources such as HDFS, Cassandra, HBase, and relational databases, including HDFS. Big data frameworks (unlike relational database systems) do not enforce schema while writing. HDFS is a perfect example where any arbitrary file is welcome during the write phase. Reading data is a different story, however. You need to give some structure to even completely unstructured data to make sense out of it. With this structured data, SQL comes very handy when it comes to analysis.</p><p>Spark SQL is a relatively new component in Spark ecosystem, introduced in Spark 1.0 for the first time. It incorporates a project named Shark, which was an attempt to make Hive run on Spark.</p><p>Hive is essentially a relational abstraction, which converts SQL queries to MapReduce jobs.</p><div class="mediaobject"><img src="graphics/3056_04_01.jpg" alt="Introduction"/></div><p>Shark replaced<a id="id203" class="indexterm"/> the MapReduce part with Spark while retaining most of the code base.</p><div class="mediaobject"><img src="graphics/3056_04_02.jpg" alt="Introduction"/></div><p>Initially, it worked fine, but very soon, Spark developers hit roadblocks and could not optimize it any further. Finally, they decided to write the SQL Engine from scratch and that gave birth to Spark SQL.</p><div class="mediaobject"><img src="graphics/3056_04_03.jpg" alt="Introduction"/></div><p>Spark SQL<a id="id204" class="indexterm"/> took care of all the performance challenges, but it had to provide compatibility with Hive and for that reason, a new wrapper context, <code class="literal">HiveContext</code>, was created on top of <code class="literal">SQLContext</code>.</p><p>Spark SQL supports accessing data using standard SQL queries and HiveQL, a SQL-like query language that Hive uses. In this chapter, we will explore different features of Spark SQL. It supports a subset of HiveQL as well as a subset of SQL 92. It runs SQL/HiveQL queries alongside, or replacing the existing Hive deployments.</p><p>Running SQL is only a part of the reason for the creation of Spark SQL. One big reason is that it helps to create and run Spark programs faster. It lets developers write less code, program read less data, and let the catalyst optimizer do all the heavy lifting.</p><p>Spark SQL uses a programming <a id="id205" class="indexterm"/>abstraction called <span class="strong"><strong>DataFrame</strong></span>. It is a distributed collection of data organized in named columns. DataFrame is equivalent to a database table, but provides much finer level of optimization. The DataFrame API also ensures that Spark's performance is consistent across different language bindings.</p><p>Let's contrast DataFrames with RDDs. An RDD is an opaque collection of objects with no idea about the format of the underlying data. In contrast, DataFrames have schema associated with them. You can also look at DataFrames as RDDs with schema added to them. In fact, until Spark 1.2, there was an artifact called <span class="strong"><strong>SchemaRDD</strong></span>, which has now evolved into<a id="id206" class="indexterm"/> DataFrame. They provide much richer functionality than SchemaRDDs.</p><p>This extra information about schema makes possible to do a lot of optimizations, which were not otherwise possible.</p><p>DataFrames also transparently load from various data sources, such as Hive tables, Parquet files, JSON files, and external databases using JDBC. DataFrames can be viewed as RDDs of row objects, allowing users to call the procedural Spark APIs such as map.</p><p>The DataFrame API is available in Scala, Java, Python, and also R starting Spark 1.4.</p><p>Users can perform<a id="id207" class="indexterm"/> relational operations on DataFrames using a <span class="strong"><strong>domain-specific language</strong></span> (<span class="strong"><strong>DSL</strong></span>). DataFrames support all the common relational operators and they all take expression objects in a limited DSL that lets Spark capture the structure of the expression.</p><p>We will <a id="id208" class="indexterm"/>start with the entry point into Spark SQL, that is, SQLContext. We will also cover HiveContext that is a wrapper around SQLContext to support Hive functionality. Please note that HiveContext is more battle-tested and provides a richer functionality, so it is strongly recommended to use it even if you do not plan to connect to Hive. Slowly, SQLContext will come to the same level of functionality as HiveContext is.</p><p>There are two ways to associate schema with RDDs to create DataFrames. The easy way is to leverage Scala case classes, which we are going to cover first. Spark uses Java reflection to deduce schema from case classes. There is also a way to programmatically specify schema for advanced needs, which we will cover next.</p><p>Spark SQL provides an easy way to both load and save the Parquet files, which will also be covered. Lastly, we will cover loading from and saving data to JSON.</p></div></div>
<div class="section" title="Understanding the Catalyst optimizer"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec31"/>Understanding the Catalyst optimizer</h1></div></div></div><p>Most of<a id="id209" class="indexterm"/> the power of Spark SQL comes due to Catalyst optimizer, so it makes sense to spend some time understanding it.</p><div class="mediaobject"><img src="graphics/3056_04_04.jpg" alt="Understanding the Catalyst optimizer"/></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec45"/>How it works…</h2></div></div></div><p>Catalyst optimizer primarily leverages functional programming constructs of Scala such as pattern matching. It offers a general framework for transforming trees, which we use to perform analysis, optimization, planning, and runtime code generation.</p><p>Catalyst optimizer<a id="id210" class="indexterm"/> has two primary goals:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Make adding new optimization techniques easy</li><li class="listitem" style="list-style-type: disc">Enable external developers to extend the optimizer</li></ul></div><p>Spark SQL uses Catalyst's transformation framework in four phases:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Analyzing a logical plan to resolve references</li><li class="listitem" style="list-style-type: disc">Logical plan optimization</li><li class="listitem" style="list-style-type: disc">Physical planning</li><li class="listitem" style="list-style-type: disc">Code generation to compile the parts of the query to Java bytecode</li></ul></div><div class="section" title="Analysis"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec02"/>Analysis</h3></div></div></div><p>The analysis phase<a id="id211" class="indexterm"/> involved looking at a SQL query or a DataFrame, creating a logical plan out of it, which is still unresolved (the columns referred may not exist or may be of wrong datatype) and then resolving this plan using the Catalog object (which connects to the physical data source), and creating a logical plan, as shown in the following diagram:</p><div class="mediaobject"><img src="graphics/3056_04_05.jpg" alt="Analysis"/></div></div><div class="section" title="Logical plan optimization"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec03"/>Logical plan optimization</h3></div></div></div><p>The <a id="id212" class="indexterm"/>logical plan optimization phase applies standard rule-based optimization to the logical plan. These include constant folding, predicate pushdown, projection pruning, null propagation, Boolean expression simplification, and other rules.</p><p>I would like to draw special attention to predicate the pushdown rule here. The concept is simple; if you issue a query in one place to run against the massive data, which is another place, it can lead to a lot of unnecessary data moving across the network.</p><p>If we can push down the part of the query to where the data is stored, and thus filter out unnecessary data, it reduces network traffic significantly.</p><div class="mediaobject"><img src="graphics/3056_04_06.jpg" alt="Logical plan optimization"/></div></div><div class="section" title="Physical planning"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec04"/>Physical planning</h3></div></div></div><p>In the <a id="id213" class="indexterm"/>physical planning phase, Spark SQL takes a logical plan and generates one or more physical plans. It then measures the cost of each physical plan and generates one physical plan based on that.</p><div class="mediaobject"><img src="graphics/3056_04_07.jpg" alt="Physical planning"/></div></div><div class="section" title="Code generation"><div class="titlepage"><div><div><h3 class="title"><a id="ch04lvl3sec05"/>Code generation</h3></div></div></div><p>The<a id="id214" class="indexterm"/> final phase of query optimization involves generating Java bytecode to run on each machine. It uses a special Scala feature <a id="id215" class="indexterm"/>called <span class="strong"><strong>Quasi quotes</strong></span> to accomplish that.</p></div></div></div>
<div class="section" title="Creating HiveContext"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec32"/>Creating HiveContext</h1></div></div></div><p><code class="literal">SQLContext</code> and its descendant <code class="literal">HiveContext</code> are the two entry points into the world of Spark SQL. <code class="literal">HiveContext</code> provides<a id="id216" class="indexterm"/> a superset of functionality<a id="id217" class="indexterm"/> provided by SQLContext. The additional features are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">More complete and battle-tested HiveQL parser</li><li class="listitem" style="list-style-type: disc">Access to Hive UDFs</li><li class="listitem" style="list-style-type: disc">Ability to read data from Hive tables</li></ul></div><p>From <a id="id218" class="indexterm"/>Spark 1.3 onwards, the Spark shell comes loaded with sqlContext (which is an instance of <code class="literal">HiveContext</code> not <code class="literal">SQLContext</code>). If you are creating <code class="literal">SQLContext</code> in Scala code, it can be created using <code class="literal">SparkContext</code>, as follows:</p><div class="informalexample"><pre class="programlisting">val sc: SparkContext
val sqlContext = new org.apache.spark.sql.SQLContext(sc)</pre></div><p>In this recipe, we will cover how to create instance of <code class="literal">HiveContext</code>, and then access Hive functionality through Spark SQL.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec46"/>Getting ready</h2></div></div></div><p>To enable Hive functionality, make sure that you have Hive enabled (-Phive) assembly JAR is available on all worker nodes; also, copy <code class="literal">hive-site.xml</code> into the <code class="literal">conf</code> directory of the Spark installation. It is important that Spark has access to <code class="literal">hive-site.xml</code>; otherwise, it <a id="id219" class="indexterm"/>will create its own Hive metastore and will not connect to your existing Hive warehouse.</p><p>By default, all the tables created by Spark SQL are Hive-managed tables, that is, Hive has complete control on life cycle of a table, including deleting it if table metadata is dropped using the <code class="literal">drop table</code> command. This holds true only for persistent tables. Spark SQL also has mechanism to create temporary tables out of DataFrames for ease of writing queries, and they are not managed by Hive.</p><p>Please note that Spark 1.4 supports Hive versions 0.13.1. You can specify a version of Hive you would like to build against using the <code class="literal">-Phive-&lt;version&gt; build</code> option while building with Maven. For example, to build with 0.12.0, you can use <code class="literal">-Phive-0.12.0</code>.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec47"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Start the Spark shell and give it some extra memory:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell --driver-memory 1G</strong></span>
</pre></div></li><li class="listitem">Create an instance of <code class="literal">HiveContext</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val hc = new org.apache.spark.sql.hive.HiveContext(sc)</strong></span>
</pre></div></li><li class="listitem">Create a Hive table <code class="literal">Person</code> with <code class="literal">first_name</code>, <code class="literal">last_name</code>, and <code class="literal">age</code> as columns:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt;  hc.sql("create table if not exists person(first_name string, last_name string, age int) row format delimited fields terminated by ','")</strong></span>
</pre></div></li><li class="listitem">Open another shell and create the <code class="literal">person</code> data in a local file:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ mkdir person</strong></span>
<span class="strong"><strong>$ echo "Barack,Obama,53" &gt;&gt; person/person.txt</strong></span>
<span class="strong"><strong>$ echo "George,Bush,68" &gt;&gt; person/person.txt</strong></span>
<span class="strong"><strong>$ echo "Bill,Clinton,68" &gt;&gt; person/person.txt</strong></span>
</pre></div></li><li class="listitem">Load the data in the <code class="literal">person</code> table:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; hc.sql("load data local inpath \"/home/hduser/person\" into table person")</strong></span>
</pre></div></li><li class="listitem">Alternatively, load that data in the <code class="literal">person</code> table from HDFS:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; hc.sql("load data inpath \"/user/hduser/person\" into table person")</strong></span>
</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note11"/>Note</h3><p>Please note that using <code class="literal">load data inpath</code> moves the data from another HDFS location to the Hive's <code class="literal">warehouse</code> directory, which is, by default, <code class="literal">/user/hive/warehouse</code>. You can also specify fully qualified path such as <code class="literal">hdfs://localhost:9000/user/hduser/person</code>.</p></div></div></li><li class="listitem">Select the<a id="id220" class="indexterm"/> person data using HiveQL:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val persons = hc.sql("from person select first_name,last_name,age")</strong></span>
<span class="strong"><strong>scala&gt; persons.collect.foreach(println)</strong></span>
</pre></div></li><li class="listitem">Create a new table from the output of a <code class="literal">select</code> query:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; hc.sql("create table person2 as select first_name, last_name from person;")</strong></span>
</pre></div></li><li class="listitem">You can also copy directly from one table to another:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; hc.sql("create table person2 like person location '/user/hive/warehouse/person'")</strong></span>
</pre></div></li><li class="listitem">Create two tables <code class="literal">people_by_last_name</code> and <code class="literal">people_by_age</code> to keep counts:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; hc.sql("create table people_by_last_name(last_name string,count int)")</strong></span>
<span class="strong"><strong>scala&gt; hc.sql("create table people_by_age(age int,count int)")</strong></span>
</pre></div></li><li class="listitem">You can also insert records into multiple tables using a HiveQL query:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; hc.sql("""from person</strong></span>
<span class="strong"><strong>  insert overwrite table people_by_last_name</strong></span>
<span class="strong"><strong>    select last_name, count(distinct first_name)</strong></span>
<span class="strong"><strong>    group by last_name</strong></span>
<span class="strong"><strong>insert overwrite table people_by_age</strong></span>
<span class="strong"><strong>    select age, count(distinct first_name)</strong></span>
<span class="strong"><strong>    group by age; """)</strong></span>
</pre></div></li></ol></div></div></div>
<div class="section" title="Inferring schema using case classes"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec33"/>Inferring schema using case classes</h1></div></div></div><p>Case classes <a id="id221" class="indexterm"/>are special classes in Scala that provide you <a id="id222" class="indexterm"/>with the boiler plate implementation of the constructor, getters (accessors), equals and hashCode, and implement <code class="literal">Serializable</code>. Case classes work really well to encapsulate data as objects. Readers, familiar <a id="id223" class="indexterm"/>with Java, can relate it to <span class="strong"><strong>plain old Java objects</strong></span> (<span class="strong"><strong>POJOs</strong></span>) or Java bean.</p><p>The beauty of case classes is that all that grunt work, which is required in Java, can be done with case classes in a single line of code. Spark uses reflection on case classes to infer schema.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec48"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Start <a id="id224" class="indexterm"/>the Spark shell and give it some extra memory:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell --driver-memory 1G</strong></span>
</pre></div></li><li class="listitem">Import<a id="id225" class="indexterm"/> for the implicit conversions:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import sqlContext.implicits._</strong></span>
</pre></div></li><li class="listitem">Create a <code class="literal">Person</code> case class:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; case class Person(first_name:String,last_name:String,age:Int)</strong></span>
</pre></div></li><li class="listitem">In another shell, create some sample data to be put in HDFS:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ mkdir person</strong></span>
<span class="strong"><strong>$ echo "Barack,Obama,53" &gt;&gt; person/person.txt</strong></span>
<span class="strong"><strong>$ echo "George,Bush,68" &gt;&gt; person/person.txt</strong></span>
<span class="strong"><strong>$ echo "Bill,Clinton,68" &gt;&gt; person/person.txt</strong></span>
<span class="strong"><strong>$ hdfs dfs -put person person</strong></span>
</pre></div></li><li class="listitem">Load the <code class="literal">person</code> directory as an RDD:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val p = sc.textFile("hdfs://localhost:9000/user/hduser/person")</strong></span>
</pre></div></li><li class="listitem">Split each line into an array of string, based on a comma, as a delimiter:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>val pmap = p.map( line =&gt; line.split(","))</strong></span>
</pre></div></li><li class="listitem">Convert the RDD of Array[String] into the RDD of <code class="literal">Person</code> case objects:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val personRDD = pmap.map( p =&gt; Person(p(0),p(1),p(2).toInt))</strong></span>
</pre></div></li><li class="listitem">Convert the <code class="literal">personRDD</code> into the <code class="literal">personDF</code> DataFrame:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val personDF = personRDD.toDF</strong></span>
</pre></div></li><li class="listitem">Register the <code class="literal">personDF</code> as a table:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; personDF.registerTempTable("person")</strong></span>
</pre></div></li><li class="listitem">Run a SQL query against it:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val people = sql("select * from person")</strong></span>
</pre></div></li><li class="listitem">Get <a id="id226" class="indexterm"/>the <a id="id227" class="indexterm"/>output values from <code class="literal">persons</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; people.collect.foreach(println)</strong></span>
</pre></div></li></ol></div></div></div>
<div class="section" title="Programmatically specifying the schema"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec34"/>Programmatically specifying the schema</h1></div></div></div><p>There are few <a id="id228" class="indexterm"/>cases where case classes might not work; one of these cases is that the case classes cannot take more than 22 fields. Another case can be that you do not know about schema beforehand. In this approach, the data is loaded as an RDD of the <code class="literal">Row</code> objects. Schema is created separately using the <code class="literal">StructType</code> and <code class="literal">StructField</code> objects, which represent a table and a field respectively. Schema is applied to the <code class="literal">Row</code> RDD to create a DataFrame.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec49"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Start the Spark shell and give it some extra memory:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell --driver-memory 1G</strong></span>
</pre></div></li><li class="listitem">Import for the implicit conversion:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import sqlContext.implicit._</strong></span>
</pre></div></li><li class="listitem">Import the Spark SQL datatypes and <code class="literal">Row</code> objects:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.sql._</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.sql.types._</strong></span>
</pre></div></li><li class="listitem">In another shell, create some sample data to be put in HDFS:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ mkdir person</strong></span>
<span class="strong"><strong>$ echo "Barack,Obama,53" &gt;&gt; person/person.txt</strong></span>
<span class="strong"><strong>$ echo "George,Bush,68" &gt;&gt; person/person.txt</strong></span>
<span class="strong"><strong>$ echo "Bill,Clinton,68" &gt;&gt; person/person.txt</strong></span>
<span class="strong"><strong>$ hdfs dfs -put person person</strong></span>
</pre></div></li><li class="listitem">Load the <code class="literal">person</code> data in an RDD:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val p = sc.textFile("hdfs://localhost:9000/user/hduser/person")</strong></span>
</pre></div></li><li class="listitem">Split each line into an array of string, based on a comma, as a delimiter:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val pmap = p.map( line =&gt; line.split(","))</strong></span>
</pre></div></li><li class="listitem">Convert the RDD of array[string] to the RDD of the <code class="literal">Row</code> objects:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val personData = pmap.map( p =&gt; Row(p(0),p(1),p(2).toInt))</strong></span>
</pre></div></li><li class="listitem">Create schema using the <code class="literal">StructType</code> and <code class="literal">StructField</code> objects. The <code class="literal">StructField</code> object takes parameters in the form of param name, param type, and nullability:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val schema = StructType(</strong></span>
<span class="strong"><strong>    Array(StructField("first_name",StringType,true),</strong></span>
<span class="strong"><strong>StructField("last_name",StringType,true),</strong></span>
<span class="strong"><strong>StructField("age",IntegerType,true)</strong></span>
<span class="strong"><strong>))</strong></span>
</pre></div></li><li class="listitem">Apply<a id="id229" class="indexterm"/> schema to create the <code class="literal">personDF</code> DataFrame:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val personDF = sqlContext.createDataFrame(personData,schema)</strong></span>
</pre></div></li><li class="listitem">Register the <code class="literal">personDF</code> as a table:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; personDF.registerTempTable("person")</strong></span>
</pre></div></li><li class="listitem">Run a SQL query against it:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val persons = sql("select * from person")</strong></span>
</pre></div></li><li class="listitem">Get the output values from <code class="literal">persons</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; persons.collect.foreach(println)</strong></span>
</pre></div></li></ol></div><p>In this recipe, we learned how to create a DataFrame by programmatically specifying schema.</p></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec50"/>How it works…</h2></div></div></div><p>A <code class="literal">StructType</code> object defines the schema. You can consider it equivalent to a table or a row in the relational world. <code class="literal">StructType</code> takes in an array of the <code class="literal">StructField</code> objects, as in the following signature:</p><div class="informalexample"><pre class="programlisting">StructType(fields: Array[StructField])</pre></div><p>A <code class="literal">StructField</code> object has the following signature:</p><div class="informalexample"><pre class="programlisting">StructField(name: String, dataType: DataType, nullable: Boolean = true, metadata: Metadata = Metadata.empty)</pre></div><p>Here is some more information on the parameters used:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">name</code>: This represents the name of the field.</li><li class="listitem" style="list-style-type: disc"><code class="literal">dataType</code>: This shows the datatype of this field.<p>The following datatypes are allowed:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><tbody><tr><td style="text-align: left" valign="top">
<p><code class="literal">IntegerType</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">FloatType</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">BooleanType</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">ShortType</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">LongType</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">ByteType</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">DoubleType</code></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">StringType</code></p>
</td></tr></tbody></table></div></li><li class="listitem" style="list-style-type: disc"><code class="literal">nullable</code>: This <a id="id230" class="indexterm"/>shows whether this field can be null.</li><li class="listitem" style="list-style-type: disc"><code class="literal">metadata</code>: This shows the metadata of this field. Metadata is a wrapper over <code class="literal">Map[String,Any]</code> so that it can contain any arbitrary metadata.</li></ul></div></div></div>
<div class="section" title="Loading and saving data using the Parquet format"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec35"/>Loading and saving data using the Parquet format</h1></div></div></div><p>Apache Parquet is a columnar data storage format, specifically designed for big data storage and processing. Parquet is based on record shredding and assembly algorithm in the Google Dremel paper. In Parquet, data in a single column is stored contiguously.</p><p>The<a id="id231" class="indexterm"/> columnar format gives Parquet some unique benefits. For<a id="id232" class="indexterm"/> example, if you have a table with 100 columns<a id="id233" class="indexterm"/> and you mostly access 10 columns, in a row-based format <a id="id234" class="indexterm"/>you will have to load all 100 columns, as granularity level is at row level. But, in Parquet, you will only load 10 columns. Another benefit is that since all the data in a given column is of the same datatype (by definition), compression is much more efficient.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec51"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Open the terminal and create the <code class="literal">person</code> data in a local file:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ mkdir person</strong></span>
<span class="strong"><strong>$ echo "Barack,Obama,53" &gt;&gt; person/person.txt</strong></span>
<span class="strong"><strong>$ echo "George,Bush,68" &gt;&gt; person/person.txt</strong></span>
<span class="strong"><strong>$ echo "Bill,Clinton,68" &gt;&gt; person/person.txt</strong></span>
</pre></div></li><li class="listitem">Upload the <code class="literal">person</code> directory to HDFS:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hdfs dfs -put person /user/hduser/person</strong></span>
</pre></div></li><li class="listitem">Start the Spark shell and give it some extra memory:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell --driver-memory 1G</strong></span>
</pre></div></li><li class="listitem">Import for the implicit conversion:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import sqlContext.implicits._</strong></span>
</pre></div></li><li class="listitem">Create a case class for <code class="literal">Person</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; case class Person(firstName: String, lastName: String, age:Int)</strong></span>
</pre></div></li><li class="listitem">Load the <code class="literal">person</code> directory from HDFS and map it to the <code class="literal">Person</code> case class:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val personRDD = sc.textFile("hdfs://localhost:9000/user/hduser/person").map(_.split("\t")).map(p =&gt; Person(p(0),p(1),p(2).toInt))</strong></span>
</pre></div></li><li class="listitem">Convert the <code class="literal">personRDD</code> into the <code class="literal">person</code> DataFrame:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val person = personRDD.toDF</strong></span>
</pre></div></li><li class="listitem">Register <a id="id235" class="indexterm"/>the <code class="literal">person</code> DataFrame as a temp <a id="id236" class="indexterm"/>table so that SQL queries can be run <a id="id237" class="indexterm"/>against it. Please note that the DataFrame <a id="id238" class="indexterm"/>name does not have to be the same as the table name.<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; person.registerTempTable("person")</strong></span>
</pre></div></li><li class="listitem">Select all the person with age over 60 years:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val sixtyPlus = sql("select * from person where age &gt; 60")</strong></span>
</pre></div></li><li class="listitem">Print values:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; sixtyPlus.collect.foreach(println)</strong></span>
</pre></div></li><li class="listitem">Let's save this <code class="literal">sixtyPlus</code> RDD in the Parquet format:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; sixtyPlus.saveAsParquetFile("hdfs://localhost:9000/user/hduser/sp.parquet")</strong></span>
</pre></div></li><li class="listitem">The previous step created a directory called <code class="literal">sp.parquet</code> in the HDFS root. You can run the <code class="literal">hdfs dfs -ls</code> command in another shell to make sure that it's created:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hdfs dfs -ls sp.parquet</strong></span>
</pre></div></li><li class="listitem">Load contents of the Parquet files in the Spark shell:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val parquetDF = sqlContext.load("hdfs://localhost:9000/user/hduser/sp.parquet")</strong></span>
</pre></div></li><li class="listitem">Register the loaded <code class="literal">parquet</code> DF as a <code class="literal">temp</code> table:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; </strong></span>
<span class="strong"><strong>parquetDF</strong></span>
<span class="strong"><strong>.registerTempTable("sixty_plus")</strong></span>
</pre></div></li><li class="listitem">Run a query against the preceding <code class="literal">temp</code> table:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; sql("select * from sixty_plus")</strong></span>
</pre></div></li></ol></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec52"/>How it works…</h2></div></div></div><p>Let's spend some time understanding the Parquet format deeper. The following is sample data represented in the table format:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>First_Name</p>
</th><th style="text-align: left" valign="bottom">
<p>Last_Name</p>
</th><th style="text-align: left" valign="bottom">
<p>Age</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Barack</p>
</td><td style="text-align: left" valign="top">
<p>Obama</p>
</td><td style="text-align: left" valign="top">
<p>53</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>George</p>
</td><td style="text-align: left" valign="top">
<p>Bush</p>
</td><td style="text-align: left" valign="top">
<p>68</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Bill</p>
</td><td style="text-align: left" valign="top">
<p>Clinton</p>
</td><td style="text-align: left" valign="top">
<p>68</p>
</td></tr></tbody></table></div><p>In the row format, the data will be stored like this:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><tbody><tr><td style="text-align: left" valign="top">
<p>Barack</p>
</td><td style="text-align: left" valign="top">
<p>Obama</p>
</td><td style="text-align: left" valign="top">
<p>53</p>
</td><td style="text-align: left" valign="top">
<p>George</p>
</td><td style="text-align: left" valign="top">
<p>Bush</p>
</td><td style="text-align: left" valign="top">
<p>68</p>
</td><td style="text-align: left" valign="top">
<p>Bill</p>
</td><td style="text-align: left" valign="top">
<p>Clinton</p>
</td><td style="text-align: left" valign="top">
<p>68</p>
</td></tr></tbody></table></div><p>In the columnar layout, the data will be stored like this:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><tbody><tr><td style="text-align: left" valign="top">
<p>Row group =&gt;</p>
</td><td style="text-align: left" valign="top">
<p>Barack</p>
</td><td style="text-align: left" valign="top">
<p>George</p>
</td><td style="text-align: left" valign="top">
<p>Bill</p>
</td><td style="text-align: left" valign="top">
<p>Obama</p>
</td><td style="text-align: left" valign="top">
<p>Bush</p>
</td><td style="text-align: left" valign="top">
<p>Clinton</p>
</td><td style="text-align: left" valign="top">
<p>53</p>
</td><td style="text-align: left" valign="top">
<p>68</p>
</td><td style="text-align: left" valign="top">
<p>68</p>
</td></tr><tr><td style="text-align: left" valign="top"> </td><td colspan="3" style="text-align: center" valign="top">
<p>Column chunk</p>
</td><td colspan="3" style="text-align: center" valign="top">
<p>Column chunk</p>
</td><td colspan="3" style="text-align: center" valign="top">
<p>Column chunk</p>
</td></tr></tbody></table></div><p>Here's a<a id="id239" class="indexterm"/> brief description about the different parts:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Row group</strong></span>: This<a id="id240" class="indexterm"/> shows the horizontal partitioning of data into rows. A row group consists of column chunks.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Column chunk</strong></span>: A column chunk has data for a given column in a row group. A <a id="id241" class="indexterm"/>column chunk is always physically <a id="id242" class="indexterm"/>contiguous. A row group has only one column chunk per column.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Page</strong></span>: A column chunk is divided into pages. A page is a unit of storage and cannot be further divided. Pages are written back to back in column chunk. The data for a page can be compressed.</li></ul></div><p>If there is already data in a Hive table, say, the <code class="literal">person</code> table, you can directly save it in the Parquet format by performing the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Create a table named <code class="literal">person_parquet</code> with schema, the same as <code class="literal">person</code>, but in the Parquet storage format (for Hive 0.13 onwards):<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>hive&gt; create table person_parquet like person stored as parquet</strong></span>
</pre></div></li><li class="listitem">Insert data in the <code class="literal">person_parquet</code> table by importing it from the <code class="literal">person</code> table:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>hive&gt; insert overwrite table person_parquet select * from person;</strong></span>
</pre></div></li></ol></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip02"/>Tip</h3><p>Sometimes, data imported from other sources, such as Impala, saves string as binary. To convert it to string while reading, set the following property in <code class="literal">SparkConf</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; sqlContext.setConf("spark.sql.parquet.binaryAsString","true")</strong></span>
</pre></div></div></div></div><div class="section" title="There's more…"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec53"/>There's more…</h2></div></div></div><p>If you <a id="id243" class="indexterm"/>are using Spark 1.4 or later, there is a new interface <a id="id244" class="indexterm"/>both to write to and read from Parquet. To write the<a id="id245" class="indexterm"/> data to Parquet (step 11 rewritten), let's save<a id="id246" class="indexterm"/> this <code class="literal">sixtyPlus</code> RDD to the Parquet format (RDD implicitly gets converted to DataFrame):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt;sixtyPlus.write.parquet("hdfs://localhost:9000/user/hduser/sp.parquet")</strong></span>
</pre></div><p>To read from Parquet (step 13 rewritten; the result is DataFrame), load the contents of the Parquet files in the Spark shell:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt;val parquetDF = sqlContext.read.parquet("hdfs://localhost:9000/user/hduser/sp.parquet")</strong></span>
</pre></div></div></div>
<div class="section" title="Loading and saving data using the JSON format"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec36"/>Loading and saving data using the JSON format</h1></div></div></div><p>JSON is <a id="id247" class="indexterm"/>a lightweight data-interchange format. It is based on a <a id="id248" class="indexterm"/>subset of the JavaScript programming language. JSON's popularity is directly related to XML getting unpopular. XML was a great solution <a id="id249" class="indexterm"/>to provide a structure to the data in a plain text<a id="id250" class="indexterm"/> format. With time, XML documents became more and more heavy and the overhead was not worth it.</p><p>JSON solved this problem by providing structure with minimal overhead. Some people call JSON <span class="strong"><strong>fat-free XML</strong></span>.</p><p>The JSON <a id="id251" class="indexterm"/>syntax follows these rules:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Data is in the form of key-value pairs:<div class="informalexample"><pre class="programlisting">"firstName" : "Bill"</pre></div></li><li class="listitem" style="list-style-type: disc">There are four datatypes in JSON:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">String ("firstName" : "Barack")</li><li class="listitem" style="list-style-type: disc">Number ("age" : 53)</li><li class="listitem" style="list-style-type: disc">Boolean ("alive": true)</li><li class="listitem" style="list-style-type: disc">null ("manager" : null)</li></ul></div></li><li class="listitem" style="list-style-type: disc">Data is delimited by commas</li><li class="listitem" style="list-style-type: disc">Curly braces {} represents an object:<div class="informalexample"><pre class="programlisting">{ "firstName" : "Bill", "lastName": "Clinton", "age": 68 }</pre></div></li><li class="listitem" style="list-style-type: disc">Square brackets [] represent an array:<div class="informalexample"><pre class="programlisting">[{ "firstName" : "Bill", "lastName": "Clinton", "age": 68 },{"firstName": "Barack","lastName": "Obama", "age": 43}]</pre></div></li></ul></div><p>In this recipe, we will explore how to save and load it in the JSON format.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec54"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Open<a id="id252" class="indexterm"/> the terminal and create the <code class="literal">person</code> data<a id="id253" class="indexterm"/> in the JSON format:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ mkdir jsondata</strong></span>
<span class="strong"><strong>$ vi jsondata/person.json</strong></span>
<span class="strong"><strong>{"first_name" : "Barack", "last_name" : "Obama", "age" : 53}</strong></span>
<span class="strong"><strong>{"first_name" : "George", "last_name" : "Bush", "age" : 68 }</strong></span>
<span class="strong"><strong>{"first_name" : "Bill", "last_name" : "Clinton", "age" : 68 }</strong></span>
</pre></div></li><li class="listitem">Upload<a id="id254" class="indexterm"/> the <code class="literal">jsondata</code> directory to HDFS:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hdfs dfs -put jsondata /user/hduser/jsondata</strong></span>
</pre></div></li><li class="listitem">Start the <a id="id255" class="indexterm"/>Spark shell and give it some extra memory:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell --driver-memory 1G</strong></span>
</pre></div></li><li class="listitem">Create an instance of <code class="literal">SQLContext</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val sqlContext = new org.apache.spark.sql.SQLContext(sc)</strong></span>
</pre></div></li><li class="listitem">Import for the implicit conversion:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import sqlContext.implicits._</strong></span>
</pre></div></li><li class="listitem">Load the <code class="literal">jsondata</code> directory from HDFS:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val person = sqlContext.jsonFile("hdfs://localhost:9000/user/hduser/jsondata")</strong></span>
</pre></div></li><li class="listitem">Register the <code class="literal">person</code> DF as a <code class="literal">temp</code> table so that the SQL queries can be run against it:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; person.registerTempTable("person")</strong></span>
</pre></div></li><li class="listitem">Select all the persons with age over 60 years:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val sixtyPlus = sql("select * from person where age &gt; 60")</strong></span>
</pre></div></li><li class="listitem">Print values:<div class="informalexample"><pre class="programlisting">scala&gt; sixtyPlus.collect.foreach(println)</pre></div></li><li class="listitem">Let's save this <code class="literal">sixtyPlus</code> DF in the JSON format<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; sixtyPlus.toJSON.saveAsTextFile("hdfs://localhost:9000/user/hduser/sp")</strong></span>
</pre></div></li><li class="listitem">Last step created a directory called <code class="literal">sp</code> in the HDFS root. You can run the <code class="literal">hdfs dfs -ls</code> command in another shell to make sure it's created:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hdfs dfs -ls sp</strong></span>
</pre></div></li></ol></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec55"/>How it works…</h2></div></div></div><p>The <code class="literal">sc.jsonFile</code> internally uses <code class="literal">TextInputFormat</code>, which processes one line at a time. Therefore, one JSON record cannot be on multiple lines. It would be a valid JSON format if <a id="id256" class="indexterm"/>you use multiple lines, but it will not work with <a id="id257" class="indexterm"/>Spark and will throw an exception.</p><p>It is allowed<a id="id258" class="indexterm"/> to have more than one object in a line. For example, you<a id="id259" class="indexterm"/> can have the information of two persons in one line as an array, as follows:</p><div class="informalexample"><pre class="programlisting">[{"firstName":"Barack", "lastName":"Obama"},{"firstName":"Bill", "lastName":"Clinton"}]</pre></div><p>This recipe concludes saving and loading data in the JSON format using Spark.</p></div><div class="section" title="There's more…"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec56"/>There's more…</h2></div></div></div><p>If you are using Spark Version 1.4 or later, <code class="literal">SqlContext</code> provides an easier interface to load the <code class="literal">jsondata</code> directory from HDFS:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val person = sqlContext.read.json ("hdfs://localhost:9000/user/hduser/jsondata")</strong></span>
</pre></div><p>The <code class="literal">sqlContext.jsonFile</code> is deprecated in version 1.4, and <code class="literal">sqlContext.read.json</code> is the recommend approach.</p></div></div>
<div class="section" title="Loading and saving data from relational databases"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec37"/>Loading and saving data from relational databases</h1></div></div></div><p>In the<a id="id260" class="indexterm"/> previous chapter, we learned how to load data<a id="id261" class="indexterm"/> from a relational data into an RDD <a id="id262" class="indexterm"/>using JdbcRDD. Spark 1.4 has support to load data <a id="id263" class="indexterm"/>directly into Dataframe from a JDBC resource. This recipe will explore how to do it.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec57"/>Getting ready</h2></div></div></div><p>Please make sure that JDBC driver JAR is visible on the client node and all the slaves nodes on which executor will run.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec58"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Create a<a id="id264" class="indexterm"/> table named <code class="literal">person</code> in MySQL <a id="id265" class="indexterm"/>using the following DDL:<div class="informalexample"><pre class="programlisting">CREATE TABLE 'person' (
  'person_id' int(11) NOT NULL AUTO_INCREMENT,
  'first_name' varchar(30) DEFAULT NULL,
  'last_name' varchar(30) DEFAULT NULL,
  'gender' char(1) DEFAULT NULL,
  'age' tinyint(4) DEFAULT NULL,
  PRIMARY KEY ('person_id')
)</pre></div></li><li class="listitem">Insert <a id="id266" class="indexterm"/>some data:<div class="informalexample"><pre class="programlisting">Insert into person values('Barack','Obama','M',53);
Insert into person values('Bill','Clinton','M',71);
Insert into person values('Hillary','Clinton','F',68);
Insert into person values('Bill','Gates','M',69);
Insert into person values('Michelle','Obama','F',51);</pre></div></li><li class="listitem">Download <code class="literal">mysql-connector-java-x.x.xx-bin.jar</code> from <a class="ulink" href="http://dev.mysql.com/downloads/connector/j/">http://dev.mysql.com/downloads/connector/j/</a>.</li><li class="listitem">Make <a id="id267" class="indexterm"/>MySQL driver available to the Spark shell and launch it:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell --driver-class-path/path-to-mysql-jar/mysql-connector-java-5.1.34-bin.jar</strong></span>
</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note12"/>Note</h3><p>Please note that <code class="literal">path-to-mysql-jar</code> is not the actual path name. You need to use your path name.</p></div></div></li><li class="listitem">Construct <a id="id268" class="indexterm"/>a JDBC URL:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val url="jdbc:mysql://localhost:3306/hadoopdb"</strong></span>
</pre></div></li><li class="listitem">Create a connection properties object with username and password:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val prop = new java.util.Properties</strong></span>
<span class="strong"><strong>scala&gt; prop.setProperty("user","hduser")</strong></span>
<span class="strong"><strong>scala&gt; prop.setProperty("password","********")</strong></span>
</pre></div></li><li class="listitem">Load DataFrame with JDBC data source (url, table name, properties):<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>     scala&gt; val people = sqlContext.read.jdbc(url,"person",prop)</strong></span>
</pre></div></li><li class="listitem">Show the results in a nice tabular format by executing the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; people.show</strong></span>
</pre></div></li><li class="listitem">This has loaded the whole table. What if I only would like to load males (url, table name, predicates, properties)? To do this, run the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val males = sqlContext.read.jdbc(url,"person",Array("gender='M'"),prop)</strong></span>
<span class="strong"><strong>scala&gt; males.show</strong></span>
</pre></div></li><li class="listitem">Show only first names by executing the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val first_names = people.select("first_name")</strong></span>
<span class="strong"><strong>scala&gt; first_names.show</strong></span>
</pre></div></li><li class="listitem">Show only people below age 60 by executing the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val below60 = people.filter(people("age") &lt; 60)</strong></span>
<span class="strong"><strong>scala&gt; below60.show</strong></span>
</pre></div></li><li class="listitem">Group <a id="id269" class="indexterm"/>people by gender as follows:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val grouped = people.groupBy("gender")</strong></span>
</pre></div></li><li class="listitem">Find the<a id="id270" class="indexterm"/> number of males and females by<a id="id271" class="indexterm"/> executing the following <a id="id272" class="indexterm"/>command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val gender_count = grouped.count</strong></span>
<span class="strong"><strong>scala&gt; gender_count.show</strong></span>
</pre></div></li><li class="listitem">Find the average age of males and females by executing the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val avg_age = grouped.avg("age")</strong></span>
<span class="strong"><strong>scala&gt; avg_age.show</strong></span>
</pre></div></li><li class="listitem">Now if you'd like to save this <code class="literal">avg_age</code> data to a new table, run the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; gender_count.write.jdbc(url,"gender_count",prop)</strong></span>
</pre></div></li><li class="listitem">Save the people DataFrame in the Parquet format:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; people.write.parquet("people.parquet")</strong></span>
</pre></div></li><li class="listitem">Save the people DataFrame in the JSON format:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; people.write.json("people.json")</strong></span>
</pre></div></li></ol></div></div></div>
<div class="section" title="Loading and saving data from an arbitrary source"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec38"/>Loading and saving data from an arbitrary source</h1></div></div></div><p>So far, we<a id="id273" class="indexterm"/> have covered three data sources that are inbuilt <a id="id274" class="indexterm"/>with DataFrames—<code class="literal">parquet</code> (default), <code class="literal">json</code>, and <code class="literal">jdbc</code>. Dataframes<a id="id275" class="indexterm"/> are not limited to these three<a id="id276" class="indexterm"/> and can load and save to any arbitrary data source by specifying the format manually.</p><p>In this recipe, we will cover loading and saving data from arbitrary sources.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec59"/>How to do it...</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Start the Spark shell and give it some extra memory:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell --driver-memory 1G</strong></span>
</pre></div></li><li class="listitem">Load <a id="id277" class="indexterm"/>the data from Parquet; since <code class="literal">parquet</code> is<a id="id278" class="indexterm"/> the default data source, you do not have to specify it:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val people = sqlContext.read.load("hdfs://localhost:9000/user/hduser/people.parquet") </strong></span>
</pre></div></li><li class="listitem">Load the <a id="id279" class="indexterm"/>data from Parquet by manually <a id="id280" class="indexterm"/>specifying the format:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val people = sqlContext.read.format("org.apache.spark.sql.parquet").load("hdfs://localhost:9000/user/hduser/people.parquet") </strong></span>
</pre></div></li><li class="listitem">For inbuilt datatypes (<code class="literal">parquet</code>,<code class="literal">json</code>, and <code class="literal">jdbc</code>), you do not have to specify the full format name, only specifying <code class="literal">"parquet"</code>, <code class="literal">"json"</code>, or <code class="literal">"jdbc"</code> works:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val people = sqlContext.read.format("parquet").load("hdfs://localhost:9000/user/hduser/people.parquet") </strong></span>
</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note13"/>Note</h3><p>When writing data, there are four save modes: <code class="literal">append</code>, <code class="literal">overwrite</code>, <code class="literal">errorIfExists</code>, and <code class="literal">ignore</code>. The <code class="literal">append</code> mode adds data to data source, <code class="literal">overwrite</code> overwrites it, <code class="literal">errorIfExists</code> throws an exception that data already exists, and <code class="literal">ignore</code> does nothing when data already exists.</p></div></div></li><li class="listitem">Save people as JSON in the <code class="literal">append</code> mode:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val people = people.write.format("json").mode("append").save ("hdfs://localhost:9000/user/hduser/people.json") </strong></span>
</pre></div></li></ol></div></div><div class="section" title="There's more…"><div class="titlepage"><div><div><h2 class="title"><a id="ch04lvl2sec60"/>There's more…</h2></div></div></div><p>The <a id="id281" class="indexterm"/>Spark SQL's data source API saves to a variety of data sources. To find more information, visit <a class="ulink" href="http://spark-packages.org/">http://spark-packages.org/</a>.</p></div></div></body></html>