- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Reading CSV and JSON Files and Solving Problems
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 读取 CSV 和 JSON 文件并解决问题
- en: When working with data, we come across several different types of data, such
    as structured, semi-structured, and non-structured, and some specifics from other
    systems’ outputs. Yet two widespread file types are ingested, **comma-separated
    values** (**CSV**) and **JavaScript Object Notation** (**JSON**). There are many
    applications for these two files, which are widely used for data ingestion due
    to their versatility.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理数据时，我们会遇到多种不同的数据类型，例如结构化、半结构化和非结构化数据，以及来自其他系统输出的某些具体信息。然而，两种广泛使用的文件类型是**逗号分隔值**（**CSV**）和**JavaScript
    对象表示法**（**JSON**）。这两种文件有许多应用，由于它们的通用性，它们被广泛用于数据导入。
- en: In this chapter, you will learn more about these file formats and how to ingest
    them using Python and PySpark, apply the best practices, and solve ingestion and
    transformation-related problems.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将了解这些文件格式以及如何使用 Python 和 PySpark 导入它们，应用最佳实践，并解决导入和转换相关的问题。
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下食谱：
- en: Reading a CSV ﬁle
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 读取 CSV 文件
- en: Reading a JSON ﬁle
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 读取 JSON 文件
- en: Creating a SparkSession for PySpark
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建 PySpark 的 SparkSession
- en: Using PySpark to read CSV ﬁles
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 PySpark 读取 CSV 文件
- en: Using PySpark to read JSON ﬁles
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 PySpark 读取 JSON 文件
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'You can find the code for this chapter in this **GitHub** repository: [https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这个 **GitHub** 仓库中找到本章的代码：[https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook)。
- en: 'Using **Jupyter Notebook** is not mandatory but can help you see how the code
    works interactively. Since we will execute Python and PySpark code, it can help
    us understand the scripts better. Once you have installed it, you can execute
    Jupyter using the following command:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 **Jupyter Notebook** 不是强制性的，但它可以帮助你交互式地查看代码的工作方式。由于我们将执行 Python 和 PySpark
    代码，它可以帮助我们更好地理解脚本。一旦安装，你可以使用以下命令执行 Jupyter：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: It is recommended to create a separate folder to store the Python files or notebooks
    we will create in this chapter; however, feel free to organize them however suits
    you best.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 建议创建一个单独的文件夹来存储本章中我们将创建的 Python 文件或笔记本；然而，请随意按照最适合你的方式组织它们。
- en: Reading a CSV ﬁle
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 读取 CSV 文件
- en: A **CSV** file is a plain text file where commas separate each data point, and
    each line represents a new record. It is widely used in many areas, such as finance,
    marketing, and sales, to store data. Software such as **Microsoft Excel** and
    **LibreOffice**, and even online solutions such as **Google Spreadsheets**, provide
    reading and writing operations for this file. Visually it resembles a structured
    table, which greatly enhances the file’s usability.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**CSV** 文件是一种纯文本文件，其中逗号分隔每个数据点，每行代表一条新记录。它在许多领域得到广泛应用，如金融、营销和销售，用于存储数据。例如 **Microsoft
    Excel** 和 **LibreOffice** 以及在线解决方案 **Google Spreadsheets** 等软件都提供了对此文件的读写操作。从视觉上看，它类似于结构化表格，这大大增强了文件的可用性。'
- en: Getting ready
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'You can download the CSV dataset for this from **Kaggle**. Use this link to
    download the file: [https://www.kaggle.com/datasets/jfreyberg/spotify-chart-data](https://www.kaggle.com/datasets/jfreyberg/spotify-chart-data).
    We are going to use the same Spotify dataset as in [*Chapter 2*](B19453_02.xhtml#_idTextAnchor064).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从 **Kaggle** 下载这个 CSV 数据集。使用此链接下载文件：[https://www.kaggle.com/datasets/jfreyberg/spotify-chart-data](https://www.kaggle.com/datasets/jfreyberg/spotify-chart-data)。我们将使用与
    [*第 2 章*](B19453_02.xhtml#_idTextAnchor064) 中相同的 Spotify 数据集。
- en: Note
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Since Kaggle is a dynamic platform, the filename might change occasionally.
    After downloading it, I named the file `spotify_data.csv`.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Kaggle 是一个动态平台，文件名可能会偶尔更改。下载后，我将文件命名为 `spotify_data.csv`。
- en: For this recipe, we will use only Python and Jupyter Notebook to execute the
    code and create a more friendly visualization.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个食谱，我们将仅使用 Python 和 Jupyter Notebook 来执行代码并创建一个更友好的可视化。
- en: How to do it…
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'Follow these steps to try this recipe:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤尝试这个食谱：
- en: 'We begin by reading the CSV file:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先读取 CSV 文件：
- en: '[PRE1]'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then, we print the column names as follows:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们按以下方式打印列名：
- en: '[PRE2]'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then, we print the first ten columns:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们打印前十个列：
- en: '[PRE3]'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This is what the output looks like:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是输出看起来像什么：
- en: '![Figure 4.1 – First five rows of the spotify_data.csv file](img/Figure_4.1_B19453.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.1 – spotify_data.csv 文件的前五行](img/Figure_4.1_B19453.jpg)'
- en: Figure 4.1 – First five rows of the spotify_data.csv file
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1 – spotify_data.csv 文件的前五行
- en: How it works…
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'Have a look at the following code:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 看看下面的代码：
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In the first step of the *How to do it…* section, we imported the built-in
    library and specified the name of our file. Since it was at the same directory
    level as our Python script, there was no need to include the full path. Then we
    declared two lists: one to store our column names (or the first line of our CSV)
    and the other to store our rows.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在“如何做...”部分的第一个步骤中，我们导入了内置库并指定了我们的文件名。由于它与我们的Python脚本位于同一目录级别，因此不需要包含完整路径。然后我们声明了两个列表：一个用于存储我们的列名（或CSV的第一行），另一个用于存储我们的行。
- en: Then we proceeded with the `with open` statement. Behind the scenes, the `with`
    statement creates a context manager that simplifies opening and closing file handlers.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用了`with open`语句。在幕后，`with`语句创建了一个上下文管理器，它简化了文件处理器的打开和关闭。
- en: '`(filename, ''r'')` indicates we want to use `filename` and only read it (`''r''`).
    After that, we read the file and store the first line in our `columns` list using
    the `next()` method, which returns the following item from the iterator. For the
    rest of the records (or rows), we used the `for` iteration to store them in a
    list.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`(filename, ''r'')`表示我们想要使用`filename`并且只读取它（`''r''`）。之后，我们读取文件，并使用`next()`方法将第一行存储在我们的`columns`列表中，该方法从迭代器返回下一个项目。对于其余的记录（或行），我们使用了`for`迭代来将它们存储在一个列表中。'
- en: Since both the declared variables are lists, we can easily read them using the
    `for` iterator.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 由于声明的变量都是列表，我们可以很容易地使用`for`迭代器读取它们。
- en: There’s more…
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容...
- en: For this recipe, we used the built-in CSV library of Python and created a simple
    structure for handling the columns and rows of our CSV file; however, there is
    a more straightforward way to do it using pandas.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个菜谱，我们使用了Python内置的CSV库，并为处理CSV文件的列和行创建了一个简单的结构；然而，使用pandas有一个更直接的方法来做这件事。
- en: pandas is a Python library that was built to analyze and manipulate data by
    converting it into structures called **DataFrames**. Don’t worry if this is a
    new concept for you; we will cover it in the following recipes and chapters.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: pandas是一个Python库，它被构建用来通过将其转换为称为**DataFrame**的结构来分析和操作数据。如果您对此概念感到陌生，请不要担心；我们将在接下来的菜谱和章节中介绍它。
- en: 'Let’s see an example of using pandas to read a CSV file:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看使用pandas读取CSV文件的一个例子：
- en: 'We need to install pandas. To do this, use the following command:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要安装pandas。为此，请使用以下命令：
- en: '[PRE5]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Remember to use the `pip` command that is associated with your Python version.
    For some readers, it might be best to use `pip3`. You can verify the version of
    `pip` and the associated Python version using the `pip –version` command in the
    CLI.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住使用与您的Python版本关联的`pip`命令。对于一些读者来说，最好使用`pip3`。您可以使用CLI中的`pip –version`命令验证`pip`的版本和关联的Python版本。
- en: 'Then, we read the CSV file:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们读取CSV文件：
- en: '[PRE6]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'You should have the following output:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该得到以下输出：
- en: '![Figure 4.2 – spotify_df DataFrame first five lines](img/Figure_4.2_B19453.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图4.2 – spotify_df DataFrame前五行](img/Figure_4.2_B19453.jpg)'
- en: Figure 4.2 – spotify_df DataFrame first five lines
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2 – spotify_df DataFrame前五行
- en: Note
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Due to its specific rendering capabilities, this *friendly* visualization can
    only be seen when using Jupyter Notebook. The output is entirely different if
    you execute the `.head()` method on the command line or in your code.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其特定的渲染能力，这种**友好**的可视化只能在Jupyter Notebook中使用时看到。如果您在命令行或代码中执行`.head()`方法，输出将完全不同。
- en: See also
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'There are other ways to install pandas, and you can explore one here: [https://pandas.pydata.org/docs/getting_started/install.xhtml](https://pandas.pydata.org/docs/getting_started/install.xhtml).'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 安装pandas还有其他方法，您可以在以下链接中探索一种方法：[https://pandas.pydata.org/docs/getting_started/install.xhtml](https://pandas.pydata.org/docs/getting_started/install.xhtml)。
- en: Reading a JSON ﬁle
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 读取JSON文件
- en: '**JavaScript Object Notation** (**JSON**) is a semi-structured data format.
    Some articles also define JSON as an unstructured data format, but the truth is
    this format can be used for multiple purposes.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**JavaScript对象表示法**（**JSON**）是一种半结构化数据格式。一些文章也将JSON定义为非结构化数据格式，但事实是这种格式可以用于多种目的。'
- en: JSON structure uses nested objects and arrays and, due to its flexibility, many
    applications and APIs use it to export or share data. That is why describing this
    file format in this chapter is essential.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: JSON结构使用嵌套的对象和数组，由于其灵活性，许多应用程序和API都使用它来导出或共享数据。这就是为什么在本章中描述这种文件格式是至关重要的。
- en: This recipe will explore how to read a JSON file using a built-in Python library
    and explain how the process works.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 本菜谱将探讨如何使用内置的Python库读取JSON文件，并解释这个过程是如何工作的。
- en: Note
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: JSON is an alternative to XML files, which are very verbose and require more
    coding to manipulate their data.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: JSON 是 XML 文件的替代品，XML 文件非常冗长，并且需要更多的代码来操作其数据。
- en: Getting ready
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: This recipe is going to use the GitHub Events JSON data, which can be found
    in the GitHub repository of this book at [https://github.com/jdorfman/awesome-json-datasets](https://github.com/jdorfman/awesome-json-datasets)
    with other free JSON data.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这个食谱将要使用 GitHub 事件 JSON 数据，这些数据可以在本书的 GitHub 仓库中找到，网址为 [https://github.com/jdorfman/awesome-json-datasets](https://github.com/jdorfman/awesome-json-datasets)，以及其他免费
    JSON 数据。
- en: To retrieve the data, click on `.``json` file.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 要检索数据，点击 `.json` 文件。
- en: How to do it…
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'Follow these steps to complete this recipe:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤完成这个食谱：
- en: 'Let’s start by reading the file:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们先从读取文件开始：
- en: '[PRE7]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, let’s get the data by making the lines interact with our JSON file:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们通过让这些行与我们的 JSON 文件交互来获取数据：
- en: '[PRE8]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output looks as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '[PRE9]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: How it works…
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'Have a look at the following code:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 看看以下代码：
- en: '[PRE10]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As for the CSV file, Python also has a built-in library for JSON files, and
    we start our script by importing it. Then, we define a variable to refer to our
    filename.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 CSV 文件，Python 也有一个内置的 JSON 文件库，我们通过导入它来开始我们的脚本。然后，我们定义一个变量来引用我们的文件名。
- en: 'Have a look at the following code:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 看看以下代码：
- en: '[PRE11]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Using the `open()` function, we open the JSON file. The `json.loads()` statement
    can’t open JSON files. To do that, we used `f.read()`, which will return the file’s
    content as it is, which is then passed as an argument to the first statement.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `open()` 函数，我们打开 JSON 文件。`json.loads()` 语句不能打开 JSON 文件。为了做到这一点，我们使用了 `f.read()`，它将返回文件的内容，然后将其作为参数传递给第一个语句。
- en: However, there is a trick here. Unlike in the CSV file, we don’t have one single
    line with the names of the columns. Instead, each data record has its own key
    representing the data. Since a JSON file is very similar to a Python dictionary,
    we need to iterate over each record to get all the `id` values in the file.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这里有一个技巧。与 CSV 文件不同，我们并没有一行单独的列名。相反，每个数据记录都有自己的键来表示数据。由于 JSON 文件与 Python 字典非常相似，我们需要遍历每个记录来获取文件中的所有
    `id` 值。
- en: 'To simplify the process a bit, we have created the following one-line `for`
    loop inside a list:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化这个过程，我们在列表中创建了一个以下单行 `for` 循环：
- en: '[PRE12]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: There’s more…
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Even though using Python’s built-in JSON library seemed very simple, manipulating
    the data better or filtering by one line, in this case, can create unnecessary
    complexity. We can use **pandas** once more to simplify the process.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管使用 Python 的内置 JSON 库看起来非常简单，但更好地操作数据或通过一行进行过滤，在这种情况下，可能会创建不必要的复杂性。我们可以再次使用
    **pandas** 来简化这个过程。
- en: 'Let’s read JSON with pandas. Check out the following code:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用 pandas 读取 JSON。看看以下代码：
- en: '[PRE13]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output looks as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '![Figure 4.3 – github_events DataFrame first five lines](img/Figure_4.3_B19453.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.3 – github_events DataFrame 前五行](img/Figure_4.3_B19453.jpg)'
- en: Figure 4.3 – github_events DataFrame first five lines
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3 – github_events DataFrame 前五行
- en: 'Then, let’s get the `id` list:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，让我们获取 `id` 列表：
- en: '[PRE14]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output looks as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '[PRE15]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Like what we did before to read the CSV file, reading a JSON file with pandas
    was very simple and saved a lot of time and code to get the same information we
    needed. While in the first example, we needed to iterate over the JSON object
    list, pandas natively understands it as a DataFrame. Since every column behaves
    as a one-dimensional array, we can quickly get the values by passing the name
    of the column (or key) to the DataFrame name.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们之前读取 CSV 文件一样，使用 pandas 读取 JSON 文件非常简单，节省了大量时间和代码来获取我们所需的信息。在第一个例子中，我们需要遍历
    JSON 对象列表，而 pandas 本地将其理解为一个 DataFrame。由于每个列都表现得像一个一维数组，我们可以通过传递列名（或键）到 DataFrame
    名称来快速获取值。
- en: As with any other library, pandas has limitations, such as reading multiple
    files simultaneously, parallelism, or reading large datasets. Having this in mind
    can prevent problems and help us use this library optimally.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 就像任何其他库一样，pandas 有其局限性，例如同时读取多个文件、并行处理或读取大型数据集。考虑到这一点可以防止问题发生，并帮助我们最优地使用这个库。
- en: Why DataFrames?
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为什么使用 DataFrame？
- en: DataFrames are bi-dimensional and size-mutable tabular structures and visually
    resemble a structured table. Due to their versatility, they are widely used in
    libraries such as pandas (as we saw previously).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame 是二维和大小可变的表格结构，在视觉上类似于结构化表格。由于它们的通用性，它们在如 pandas（如我们之前所见）等库中被广泛使用。
- en: '**PySpark** is no different. **Spark** uses DataFrames as distributed data
    collections and can *parallelize* its tasks to process them through other cores
    or nodes. We will cover this in more depth in [*Chapter 7*](B19453_07.xhtml#_idTextAnchor227).'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**PySpark** 与此不同。**Spark** 使用 DataFrame 作为分布式数据集合，并且可以 *并行化* 其任务，通过其他核心或节点来处理。我们将在
    [*第七章*](B19453_07.xhtml#_idTextAnchor227) 中更深入地介绍这一点。'
- en: See also
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'To find out more about the files supported by the pandas library, follow this
    link: [https://pandas.pydata.org/pandas-docs/stable/user_guide/io.xhtml](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.xhtml).'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于 pandas 库支持的文件的信息，请点击以下链接：[https://pandas.pydata.org/pandas-docs/stable/user_guide/io.xhtml](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.xhtml)。
- en: Creating a SparkSession for PySpark
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为 PySpark 创建 SparkSession
- en: Previously introduced in [*Chapter 1*](B19453_01.xhtml#_idTextAnchor022), **PySpark**
    is a Spark library that was designed to work with Python. PySpark uses a Python
    API to write **Spark** functionalities such as data manipulation, processing (batch
    or real-time), and machine learning.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*第一章*](B19453_01.xhtml#_idTextAnchor022) 中之前介绍过的，**PySpark** 是一个为与 Python
    一起工作而设计的 Spark 库。PySpark 使用 Python API 来编写 **Spark** 功能，如数据处理、处理（批量或实时）和机器学习。
- en: However, before ingesting or processing data using PySpark, we must initialize
    a SparkSession. This recipe will teach us how to create a SparkSession using PySpark
    and explain its importance.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在用 PySpark 导入或处理数据之前，我们必须初始化一个 SparkSession。本食谱将教会我们如何使用 PySpark 创建 SparkSession，并解释其重要性。
- en: Getting ready
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'We first need to ensure we have the correct PySpark version. We installed PySpark
    in [*Chapter 1*](B19453_01.xhtml#_idTextAnchor022); however, checking if we are
    using the correct version is always good. Run the following command:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要确保我们拥有正确的 PySpark 版本。我们在 [*第一章*](B19453_01.xhtml#_idTextAnchor022) 中安装了
    PySpark；然而，检查我们是否使用正确的版本总是好的。运行以下命令：
- en: '[PRE16]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'You should see the following output:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下输出：
- en: '[PRE17]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Next, we choose a code editor that can be any code editor that you want. I will
    use Jupyter due to the interactive interface.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们选择一个代码编辑器，可以是任何你想要的代码编辑器。我将使用 Jupyter，因为它具有交互式界面。
- en: How to do it…
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Let’s see how to create a SparkSession:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何创建 SparkSession：
- en: 'We first create `SparkSession` as follows:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先创建 `SparkSession` 如下：
- en: '[PRE18]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'And these are the warnings received:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这是收到的警告：
- en: '[PRE19]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We are not going to have an output when executing the code. Also, don’t worry
    about the `WARN` messages; they will not affect our work.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 执行代码时，我们不会得到任何输出。另外，不要担心 `WARN` 消息；它们不会影响我们的工作。
- en: 'Then we get the Spark UI. To do this, in your Jupyter cell, type and execute
    the name of your instantiated as follows:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们获取 Spark UI。为此，在你的 Jupyter 单元中，按照以下方式输入并执行实例名称：
- en: '[PRE20]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'You should see the following output:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下输出：
- en: '![Figure 4.4 – Output of execution of instance](img/Figure_4.4_B19453.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.4 – 实例执行输出](img/Figure_4.4_B19453.jpg)'
- en: Figure 4.4 – Output of execution of instance
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4 – 实例执行输出
- en: 'Next, we access SparkUI in the browser. To do this, we click on the **Spark
    UI** hyperlink. It will open a new tab in your browser, showing a graph with **Executors**
    and **Jobs**, and other helpful information:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们在浏览器中访问 SparkUI。为此，我们点击 **Spark UI** 超链接。它将在你的浏览器中打开一个新标签页，显示一个包含 **Executors**
    和 **Jobs** 以及其他有用信息的图表：
- en: '![Figure 4.5 – Spark UI home page with an Event Timeline graph](img/Figure_4.5_B19453.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.5 – 带有事件时间线图的 Spark UI 主页](img/Figure_4.5_B19453.jpg)'
- en: Figure 4.5 – Spark UI home page with an Event Timeline graph
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.5 – 带有事件时间线图的 Spark UI 主页
- en: Since we still haven’t executed any processes, the graph is empty. Don’t worry;
    we will see it in action in the following recipes.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们还没有执行任何过程，图表是空的。不要担心；我们将在接下来的食谱中看到它的实际应用。
- en: How it works…
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: As we saw at the beginning of this chapter, a is a fundamental part of starting
    our Spark jobs. It sets all the required configurations for **Spark’s YARN** (short
    for **Yet Another Resource Manager**) to allocate the memory, cores, and paths
    to write temporary and final outputs, among other things.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本章开头所看到的，SparkSession 是启动我们的 Spark 作业的基本部分。它为 **Spark 的 YARN**（即 **Yet
    Another Resource Manager**）设置所有必需的配置，以便分配内存、核心、写入临时和最终输出的路径等。
- en: 'With this in mind, let’s visit each step of our code:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在此基础上，让我们查看代码的每个步骤：
- en: '[PRE21]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'To execute operations such as creating DataFrames, we need to use an instance
    of . The `spark` variable will be used to access DataFrames and other procedures.
    Now, take a look at this:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行创建 DataFrame 等操作，我们需要使用一个实例。`spark` 变量将用于访问 DataFrame 和其他程序。现在，看看这个：
- en: '[PRE22]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The `.master()` method indicates which type of distributed processing we have.
    Since this is our local machine used only for educational purposes, we defined
    it as `"local[1]"`, where the integer value needs to be greater than `0`, since
    it represents the number of partitions. The `.appName()` method defines the name
    of our application session.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '`.master()`方法指示我们使用了哪种类型的分布式处理。由于这是我们仅用于教育目的的本地机器，我们将其定义为`"local[1]"`，其中整数值需要大于`0`，因为它表示分区数。`.appName()`方法定义了我们的应用程序会话名称。'
- en: 'After declaring the name of our application, we can set the `.``config()` methods:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在声明我们的应用程序名称后，我们可以设置`.config()`方法：
- en: '[PRE23]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Here, we defined two types of configuration: memory allocation and core allocation.
    `spark.executor.memory` tells YARN how much memory each executor can allocate
    to process data; `g` represents the size unit of it.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们定义了两种类型的配置：内存分配和核心分配。`spark.executor.memory`告诉YARN每个执行器可以分配多少内存来处理数据；`g`代表其大小单位。
- en: '`spark.executor.cores` defines the number of executors used by YARN. By default,
    `1` is used. Next, `spark.cores.max` sets how many cores YARN can scale.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '`spark.executor.cores`定义了YARN使用的执行器数量。默认情况下，使用`1`。接下来，`spark.cores.max`设置YARN可以扩展的核心数量。'
- en: Last, `.enableHiveSupport()` enables the support of Hive queries.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`.enableHiveSupport()`启用了对Hive查询的支持。
- en: 'Finally, let’s look at this:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们看看这个：
- en: '[PRE24]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '`.getOrCreate()` is simple as its name. If there is a session with this name,
    it will retrieve its configurations of it. Otherwise, it will create a new one.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '`.getOrCreate()`就像它的名字一样简单。如果有这个名称的会话，它将检索其配置。如果没有，它将创建一个新的。'
- en: There’s more…
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Looking at the Spark documentation page, you will see that most configurations
    are not required to start our jobs or ingestions. However, it is essential to
    remember that we are handling a scalable framework that was created to allocate
    resources in a single machine or in clusters to process vast amounts of data.
    With no limit set to a SparkSession, YARN will allocate every resource it needs
    to process or ingest data, and this can result in server downtime or even freeze
    your entire local machine.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 查看Spark文档页面，你会看到大多数配置不是启动我们的作业或数据摄取所必需的。然而，我们必须记住，我们正在处理一个可扩展的框架，该框架旨在在单个机器或集群中分配资源以处理大量数据。如果没有为SparkSession设置限制，YARN将分配它处理或摄取数据所需的所有资源，这可能导致服务器停机，甚至冻结整个本地机器。
- en: In a real-world scenario, a Kubernetes cluster is typically used to ingest or
    process shared data with other applications or users who do the same thing as
    you or your team. Physical memory and computational resources tend to be limited,
    so it is always an excellent practice to set the configurations, even in small
    projects that use serverless cloud solutions.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界的场景中，Kubernetes集群通常用于与其他应用程序或用户摄取或处理共享数据，这些用户或团队与你或你的团队做同样的事情。物理内存和计算资源往往有限，因此始终设置配置是一个非常好的做法，即使在小型项目中使用无服务器云解决方案也是如此。
- en: Getting all configurations
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 获取所有配置
- en: 'It is also possible to retrieve the current configurations used for this application
    session using the following code:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码也可以检索此应用程序会话当前使用的配置：
- en: '[PRE25]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This is the result we get:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的结果：
- en: '![Figure 4.6 – configurations set for the recipe](img/Figure_4.6_B19453.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![图4.6 – 为配方设置的配置](img/Figure_4.6_B19453.jpg)'
- en: Figure 4.6 – configurations set for the recipe
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6 – 为配方设置的配置
- en: See also
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'See all configurations: [https://spark.apache.org/docs/latest/configuration.xhtml](https://spark.apache.org/docs/latest/configuration.xhtml)'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看所有配置：[https://spark.apache.org/docs/latest/configuration.xhtml](https://spark.apache.org/docs/latest/configuration.xhtml)
- en: 'In early versions of Spark, SparkContext was the starting point for working
    with data and was replaced by in the recent versions. You can read more about
    it here: [https://sparkbyexamples.com/spark/-vs-sparkcontext/](https://sparkbyexamples.com/spark/-vs-sparkcontext/).'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Spark的早期版本中，SparkContext是处理数据时的起点，在最新版本中被取代。你可以在这里了解更多信息：[https://sparkbyexamples.com/spark/-vs-sparkcontext/](https://sparkbyexamples.com/spark/-vs-sparkcontext/)。
- en: Using PySpark to read CSV ﬁles
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PySpark读取CSV文件
- en: As expected, PySpark provides native support for reading and writing CSV files.
    It also allows data engineers to pass diverse kinds of setups in case the CSV
    has a different type of delimiter, special encoding, and so on.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，PySpark为读取和写入CSV文件提供了原生支持。它还允许数据工程师在CSV具有不同类型的分隔符、特殊编码等情况时传递各种设置。
- en: In this recipe, we are going to cover how to read CSV files using PySpark using
    the most common configurations, and we will explain why they are needed.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将介绍如何使用 PySpark 读取 CSV 文件，使用最常用的配置，并解释为什么需要它们。
- en: Getting ready
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'You can download the CSV dataset for this recipe from Kaggle: [https://www.kaggle.com/datasets/jfreyberg/spotify-chart-data](https://www.kaggle.com/datasets/jfreyberg/spotify-chart-data).
    We are going to use the same Spotify dataset as in [*Chapter 2*](B19453_02.xhtml#_idTextAnchor064).'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从 Kaggle 下载此菜谱的 CSV 数据集：[https://www.kaggle.com/datasets/jfreyberg/spotify-chart-data](https://www.kaggle.com/datasets/jfreyberg/spotify-chart-data)。我们将使用与
    [*第 2 章*](B19453_02.xhtml#_idTextAnchor064) 中相同的 Spotify 数据集。
- en: As in the *Creating a SparkSession for PySpark* recipe, make sure PySpark is
    installed and running with the latest stable version. Also, using Jupyter Notebook
    is optional.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在 *为 PySpark 创建 SparkSession* 菜谱中一样，请确保 PySpark 已安装并运行最新稳定版本。此外，使用 Jupyter
    Notebook 是可选的。
- en: How to do it…
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点…
- en: 'Let’s get started:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧：
- en: 'We first import and create a SparkSession :'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先导入并创建一个 SparkSession：
- en: '[PRE26]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Then, we read the CSV file:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们读取 CSV 文件：
- en: '[PRE27]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Then, we show the data
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们展示数据
- en: '[PRE28]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This is the result we get:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的结果：
- en: '![Figure 4.7 – spotify_data.csv DataFrame vision using Spark](img/Figure_4.7_B19453.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.7 – 使用 Spark 查看 spotify_data.csv DataFrame](img/Figure_4.7_B19453.jpg)'
- en: Figure 4.7 – spotify_data.csv DataFrame vision using Spark
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7 – 使用 Spark 查看 spotify_data.csv DataFrame
- en: How it works…
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: 'Simple as it is, we need to understand how reading a file with Spark works
    to make sure it always executes properly:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然很简单，但我们需要了解 Spark 读取文件的方式，以确保它始终正确执行：
- en: '[PRE29]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: We attributed a variable to our code statement. This is a good practice when
    initializing any file reading because it allows us to control the version of the
    file and, if a change needs to be made, we attribute it to another variable. The
    name of the variable is also intentional since we are creating our first DataFrame.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将一个变量分配给我们的代码语句。在初始化任何文件读取时，这是一个好的做法，因为它允许我们控制文件的版本，如果需要做出更改，我们可以将其分配给另一个变量。变量的名称也是故意的，因为我们正在创建我们的第一个
    DataFrame。
- en: 'Using the `.option()` method allows us to tell PySpark which type of configuration
    we want to pass. In this case, we set `header` to `True`, which makes PySpark
    set the first row of the CSV file as the column names. If we didn’t pass the required
    configuration, the DataFrame would look like this:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `.option()` 方法允许我们告诉 PySpark 我们想要传递哪种类型的配置。在这种情况下，我们将 `header` 设置为 `True`，这使得
    PySpark 将 CSV 文件的第一个行设置为列名。如果我们没有传递所需的配置，DataFrame 将看起来像这样：
- en: '![Figure 4.8 – spotify_data.csv DataFrame without column names](img/Figure_4.8_B19453.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.8 – 没有列名的 spotify_data.csv DataFrame](img/Figure_4.8_B19453.jpg)'
- en: Figure 4.8 – spotify_data.csv DataFrame without column names
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.8 – 没有列名的 spotify_data.csv DataFrame
- en: There’s more…
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容…
- en: 'The content of the files can differ, but some setups are welcome when handling
    CSV files in PySpark. Here, please note that we are going to change the method
    to `.options()`:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 文件的内容可能不同，但在 PySpark 处理 CSV 文件时，一些设置是受欢迎的。在这里，请注意，我们将更改方法为 `.options()`：
- en: '[PRE30]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '`header`, `sep`, and `inferSchema` are the most commonly used setups when reading
    a CSV file. Although CSV stands for `sep` (which stands for separator) declared.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '`header`、`sep` 和 `inferSchema` 是读取 CSV 文件时最常用的设置。尽管 CSV 代表 `sep`（代表分隔符）声明。'
- en: 'Let’s see an example of an error when reading a CSV that uses a pipe to separate
    the strings and passes the wrong separator:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看读取使用管道分隔字符串并传递错误分隔符的 CSV 文件时出现的错误示例：
- en: '[PRE31]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This is how the output looks:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是输出看起来像这样：
- en: '![Figure 4.9 – CSV DataFrame reading without a proper sep definition](img/Figure_4.9_B19453.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.9 – 在没有正确设置 sep 定义的情况下读取 CSV DataFrame](img/Figure_4.9_B19453.jpg)'
- en: Figure 4.9 – CSV DataFrame reading without a proper sep definition
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.9 – 在没有正确设置 sep 定义的情况下读取 CSV DataFrame
- en: 'As you can see, it creates only one column with all the information. But if
    we pass `sep=''|''`, it will return correctly:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，它只创建了一个包含所有信息的列。但如果我们传递 `sep='|'`，它将正确返回：
- en: '![Figure 4.10 – CSV DataFrame with the sep definition set to pipe](img/Figure_4.10_B19453.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.10 – 将 sep 定义设置为管道的 CSV DataFrame](img/Figure_4.10_B19453.jpg)'
- en: Figure 4.10 – CSV DataFrame with the sep definition set to pipe
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.10 – 将 sep 定义设置为管道的 CSV DataFrame
- en: Other common .options() configurations
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 其他常见的 `.options()` 配置
- en: 'There are other complex situations that, if not corrected during ingestion,
    can result in some problems with the other ETL steps. Here, I am using the `listing.csv`
    dataset, which can be found here at [http://data.insideairbnb.com/the-netherlands/north-holland/amsterdam/2022-03-08/visualisations/listings.csv](http://data.insideairbnb.com/the-netherlands/north-holland/amsterdam/2022-03-08/visualisations/listings.csv):'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据导入过程中，还有其他复杂情况，如果不进行纠正，可能会导致其他 ETL 步骤出现一些问题。这里，我使用的是 `listing.csv` 数据集，可以在以下链接找到：[http://data.insideairbnb.com/the-netherlands/north-holland/amsterdam/2022-03-08/visualisations/listings.csv](http://data.insideairbnb.com/the-netherlands/north-holland/amsterdam/2022-03-08/visualisations/listings.csv)：
- en: 'We first read our common configurations:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先读取我们的通用配置：
- en: '[PRE32]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '![Figure 4.11 – listing.csv DataFrame](img/Figure_4.11_B19453.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.11 – listing.csv DataFrame](img/Figure_4.11_B19453.jpg)'
- en: Figure 4.11 – listing.csv DataFrame
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.11 – listing.csv DataFrame
- en: At first glance, all is normal. However, what happens if I try to execute a
    simple group by using `room_type`?
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 初看一切正常。然而，如果我尝试使用 `room_type` 执行一个简单的分组操作会发生什么呢？
- en: '[PRE33]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This is the output we get:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的输出：
- en: '![Figure 4.12 – Grouping df_broken by room_type](img/Figure_4.12_B19453.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.12 – 按房间类型分组 df_broken](img/Figure_4.12_B19453.jpg)'
- en: Figure 4.12 – Grouping df_broken by room_type
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.12 – 按房间类型分组 df_broken
- en: Ignoring `group by` for now, this happens because the file has plenty of escaped
    quotes and line breaks.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 目前忽略 `group by`，这是因为文件中有大量的转义引号和换行符。
- en: 'Now, let’s set the correct `.options()`:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们设置正确的 `.options()`：
- en: '[PRE34]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'This is the result:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这是结果：
- en: '![Figure 4.13 – Grouping by room_type using the right options() settings](img/Figure_4.13_B19453.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.13 – 使用正确的 options() 设置按房间类型分组](img/Figure_4.13_B19453.jpg)'
- en: Figure 4.13 – Grouping by room_type using the right options() settings
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.13 – 使用正确的 options() 设置按房间类型分组
- en: See also
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'You can see more PySpark `.options()` in the official documentation: [https://spark.apache.org/docs/latest/sql-data-sources-csv.xhtml](https://spark.apache.org/docs/latest/sql-data-sources-csv.xhtml).'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在官方文档中查看更多 PySpark `.options()`：[https://spark.apache.org/docs/latest/sql-data-sources-csv.xhtml](https://spark.apache.org/docs/latest/sql-data-sources-csv.xhtml)。
- en: Using PySpark to read JSON ﬁles
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 PySpark 读取 JSON 文件
- en: In the *Reading a JSON file* recipe, we saw that JSON files are widely used
    to transport and share data between applications, and we saw how to read a JSON
    file using simple Python code.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在“读取 JSON 文件”的菜谱中，我们了解到 JSON 文件广泛用于在应用程序之间传输和共享数据，我们也看到了如何使用简单的 Python 代码读取
    JSON 文件。
- en: However, with the increase in data size and sharing, using only Python to process
    a high volume of data can lead to performance or resilience issues. That’s why,
    for this type of scenario, it is highly recommended to use PySpark to read and
    process JSON files. As you might expect, PySpark comes with a straightforward
    reading solution.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，随着数据量和共享的增加，仅使用 Python 处理大量数据可能会导致性能或弹性问题。这就是为什么，对于这种场景，强烈建议使用 PySpark 读取和处理
    JSON 文件。正如你所预期的，PySpark 提供了一个直接的读取解决方案。
- en: In this recipe, we will cover how to read a JSON file with PySpark, the common
    associated issues, and how to solve them.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将介绍如何使用 PySpark 读取 JSON 文件，以及常见的相关问题和解决方法。
- en: Getting ready
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: As in the previous recipe, *Reading a JSON file*, we are going to use the `GitHub
    Events` JSON file. Also, the use of Jupyter Notebook is optional.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的菜谱“读取 JSON 文件”一样，我们将使用 `GitHub Events` JSON 文件。此外，使用 Jupyter Notebook 是可选的。
- en: How to do it…
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Here are the steps for this recipe:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是这个菜谱的步骤：
- en: 'We first create the SparkSession:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先创建 SparkSession：
- en: '[PRE35]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Then, we read the JSON file:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们读取 JSON 文件：
- en: '[PRE36]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Then, we show the data:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们展示数据：
- en: '[PRE37]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'This is how the output appears:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出结果：
- en: '![Figure 4.14 – df_json DataFrame](img/Figure_4.14_B19453.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.14 – df_json DataFrame](img/Figure_4.14_B19453.jpg)'
- en: Figure 4.14 – df_json DataFrame
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.14 – df_json DataFrame
- en: How it works…
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: Similar to CSV files, reading a JSON file using PySpark is very simple, requiring
    only one line of code. Like pandas, it ignores the brackets in the file and creates
    a table-structured DataFrame, even though we are handling a semi-structured data
    file. However, the great magic is in`.option("multiline", "true")`.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 与 CSV 文件类似，使用 PySpark 读取 JSON 文件非常简单，只需要一行代码。就像 pandas 一样，它忽略了文件中的括号，并创建了一个表格结构的
    DataFrame，尽管我们正在处理一个半结构化数据文件。然而，真正的魔法在于 `.option("multiline", "true")`。
- en: 'If you remember our JSON structure for this file, it is something like this:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你记得这个文件的 JSON 结构，它可能像这样：
- en: '[PRE38]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'It is a multi-lined JSON since it has objects inside objects. The `.option()`
    setup passed when reading the file guarantees PySpark will read it as it should,
    and if we don’t pass this argument, an error like this will appear:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它包含对象中的对象，这是一个多行 JSON。在读取文件时传递的 `.option()` 设置确保 PySpark 会按预期读取它，如果我们不传递此参数，将出现如下错误：
- en: '![Figure 4.15 – This error is shown when .options() is not well defined for
    a multi-lined JSON file](img/Figure_4.15_B19453.jpg)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.15 – 当多行 JSON 文件中的 .options() 未正确定义时显示此错误](img/Figure_4.15_B19453.jpg)'
- en: Figure 4.15 – This error is shown when .options() is not well defined for a
    multi-lined JSON file
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.15 – 当多行 JSON 文件中的 .options() 未正确定义时显示此错误
- en: PySpark understands it as a corrupted file.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark 会将其视为损坏的文件。
- en: There’s more…
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: A very widely used configuration for reading JSON files is `dropFieldIfAllNull`.
    When set to `true`, if there is an empty array, it will drop it from the schema.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 读取 JSON 文件的一个非常广泛使用的配置是 `dropFieldIfAllNull`。当设置为 `true` 时，如果存在空数组，它将从模式中删除。
- en: Unstructured and semi-structured data are valuable due to their elasticity.
    Sometimes, applications can change their output and some fields become deprecated.
    To avoid changing the ingest script (especially if these changes can be frequent),
    `dropFieldIfAllNull` removes them from the DataFrame.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 非结构化和半结构化数据因其弹性而具有价值。有时，应用程序可以更改其输出，某些字段可能已过时。为了避免更改摄入脚本（尤其是如果这些更改可能很频繁），`dropFieldIfAllNull`
    会将其从 DataFrame 中删除。
- en: See also
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'To find out more about PySpark `.options()`, refer to the official documentation:
    [https://spark.apache.org/docs/latest/sql-data-sources-json.xhtml](https://spark.apache.org/docs/latest/sql-data-sources-json.xhtml)'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要了解更多关于 PySpark `.options()` 的信息，请参阅官方文档：[https://spark.apache.org/docs/latest/sql-data-sources-json.xhtml](https://spark.apache.org/docs/latest/sql-data-sources-json.xhtml)
- en: Further reading
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '[https://docs.fileformat.com/spreadsheet/csv/](https://docs.fileformat.com/spreadsheet/csv/)'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://docs.fileformat.com/spreadsheet/csv/](https://docs.fileformat.com/spreadsheet/csv/)'
- en: '[https://codefather.tech/blog/python-with-open/](https://codefather.tech/blog/python-with-open/)'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://codefather.tech/blog/python-with-open/](https://codefather.tech/blog/python-with-open/)'
- en: '[https://www.programiz.com/python-programming/methods/built-in/next](https://www.programiz.com/python-programming/methods/built-in/next)'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.programiz.com/python-programming/methods/built-in/next](https://www.programiz.com/python-programming/methods/built-in/next)'
- en: '[https://spark.apache.org/docs/latest/sql-data-sources-csv.xhtml](https://spark.apache.org/docs/latest/sql-data-sources-csv.xhtml)'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://spark.apache.org/docs/latest/sql-data-sources-csv.xhtml](https://spark.apache.org/docs/latest/sql-data-sources-csv.xhtml)'
- en: '[https://sparkbyexamples.com/pyspark/pyspark-read-json-file-into-dataframe/](https://sparkbyexamples.com/pyspark/pyspark-read-json-file-into-dataframe/)'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://sparkbyexamples.com/pyspark/pyspark-read-json-file-into-dataframe/](https://sparkbyexamples.com/pyspark/pyspark-read-json-file-into-dataframe/)'
