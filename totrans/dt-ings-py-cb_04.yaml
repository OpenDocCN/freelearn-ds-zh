- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reading CSV and JSON Files and Solving Problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When working with data, we come across several different types of data, such
    as structured, semi-structured, and non-structured, and some specifics from other
    systems’ outputs. Yet two widespread file types are ingested, **comma-separated
    values** (**CSV**) and **JavaScript Object Notation** (**JSON**). There are many
    applications for these two files, which are widely used for data ingestion due
    to their versatility.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will learn more about these file formats and how to ingest
    them using Python and PySpark, apply the best practices, and solve ingestion and
    transformation-related problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Reading a CSV ﬁle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reading a JSON ﬁle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a SparkSession for PySpark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using PySpark to read CSV ﬁles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using PySpark to read JSON ﬁles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can find the code for this chapter in this **GitHub** repository: [https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using **Jupyter Notebook** is not mandatory but can help you see how the code
    works interactively. Since we will execute Python and PySpark code, it can help
    us understand the scripts better. Once you have installed it, you can execute
    Jupyter using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: It is recommended to create a separate folder to store the Python files or notebooks
    we will create in this chapter; however, feel free to organize them however suits
    you best.
  prefs: []
  type: TYPE_NORMAL
- en: Reading a CSV ﬁle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **CSV** file is a plain text file where commas separate each data point, and
    each line represents a new record. It is widely used in many areas, such as finance,
    marketing, and sales, to store data. Software such as **Microsoft Excel** and
    **LibreOffice**, and even online solutions such as **Google Spreadsheets**, provide
    reading and writing operations for this file. Visually it resembles a structured
    table, which greatly enhances the file’s usability.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can download the CSV dataset for this from **Kaggle**. Use this link to
    download the file: [https://www.kaggle.com/datasets/jfreyberg/spotify-chart-data](https://www.kaggle.com/datasets/jfreyberg/spotify-chart-data).
    We are going to use the same Spotify dataset as in [*Chapter 2*](B19453_02.xhtml#_idTextAnchor064).'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Since Kaggle is a dynamic platform, the filename might change occasionally.
    After downloading it, I named the file `spotify_data.csv`.
  prefs: []
  type: TYPE_NORMAL
- en: For this recipe, we will use only Python and Jupyter Notebook to execute the
    code and create a more friendly visualization.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to try this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin by reading the CSV file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we print the column names as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we print the first ten columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This is what the output looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – First five rows of the spotify_data.csv file](img/Figure_4.1_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – First five rows of the spotify_data.csv file
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Have a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In the first step of the *How to do it…* section, we imported the built-in
    library and specified the name of our file. Since it was at the same directory
    level as our Python script, there was no need to include the full path. Then we
    declared two lists: one to store our column names (or the first line of our CSV)
    and the other to store our rows.'
  prefs: []
  type: TYPE_NORMAL
- en: Then we proceeded with the `with open` statement. Behind the scenes, the `with`
    statement creates a context manager that simplifies opening and closing file handlers.
  prefs: []
  type: TYPE_NORMAL
- en: '`(filename, ''r'')` indicates we want to use `filename` and only read it (`''r''`).
    After that, we read the file and store the first line in our `columns` list using
    the `next()` method, which returns the following item from the iterator. For the
    rest of the records (or rows), we used the `for` iteration to store them in a
    list.'
  prefs: []
  type: TYPE_NORMAL
- en: Since both the declared variables are lists, we can easily read them using the
    `for` iterator.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this recipe, we used the built-in CSV library of Python and created a simple
    structure for handling the columns and rows of our CSV file; however, there is
    a more straightforward way to do it using pandas.
  prefs: []
  type: TYPE_NORMAL
- en: pandas is a Python library that was built to analyze and manipulate data by
    converting it into structures called **DataFrames**. Don’t worry if this is a
    new concept for you; we will cover it in the following recipes and chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see an example of using pandas to read a CSV file:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to install pandas. To do this, use the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Remember to use the `pip` command that is associated with your Python version.
    For some readers, it might be best to use `pip3`. You can verify the version of
    `pip` and the associated Python version using the `pip –version` command in the
    CLI.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we read the CSV file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should have the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – spotify_df DataFrame first five lines](img/Figure_4.2_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 – spotify_df DataFrame first five lines
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Due to its specific rendering capabilities, this *friendly* visualization can
    only be seen when using Jupyter Notebook. The output is entirely different if
    you execute the `.head()` method on the command line or in your code.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are other ways to install pandas, and you can explore one here: [https://pandas.pydata.org/docs/getting_started/install.xhtml](https://pandas.pydata.org/docs/getting_started/install.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: Reading a JSON ﬁle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**JavaScript Object Notation** (**JSON**) is a semi-structured data format.
    Some articles also define JSON as an unstructured data format, but the truth is
    this format can be used for multiple purposes.'
  prefs: []
  type: TYPE_NORMAL
- en: JSON structure uses nested objects and arrays and, due to its flexibility, many
    applications and APIs use it to export or share data. That is why describing this
    file format in this chapter is essential.
  prefs: []
  type: TYPE_NORMAL
- en: This recipe will explore how to read a JSON file using a built-in Python library
    and explain how the process works.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: JSON is an alternative to XML files, which are very verbose and require more
    coding to manipulate their data.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe is going to use the GitHub Events JSON data, which can be found
    in the GitHub repository of this book at [https://github.com/jdorfman/awesome-json-datasets](https://github.com/jdorfman/awesome-json-datasets)
    with other free JSON data.
  prefs: []
  type: TYPE_NORMAL
- en: To retrieve the data, click on `.``json` file.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to complete this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by reading the file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s get the data by making the lines interact with our JSON file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Have a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: As for the CSV file, Python also has a built-in library for JSON files, and
    we start our script by importing it. Then, we define a variable to refer to our
    filename.
  prefs: []
  type: TYPE_NORMAL
- en: 'Have a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Using the `open()` function, we open the JSON file. The `json.loads()` statement
    can’t open JSON files. To do that, we used `f.read()`, which will return the file’s
    content as it is, which is then passed as an argument to the first statement.
  prefs: []
  type: TYPE_NORMAL
- en: However, there is a trick here. Unlike in the CSV file, we don’t have one single
    line with the names of the columns. Instead, each data record has its own key
    representing the data. Since a JSON file is very similar to a Python dictionary,
    we need to iterate over each record to get all the `id` values in the file.
  prefs: []
  type: TYPE_NORMAL
- en: 'To simplify the process a bit, we have created the following one-line `for`
    loop inside a list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Even though using Python’s built-in JSON library seemed very simple, manipulating
    the data better or filtering by one line, in this case, can create unnecessary
    complexity. We can use **pandas** once more to simplify the process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s read JSON with pandas. Check out the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3 – github_events DataFrame first five lines](img/Figure_4.3_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – github_events DataFrame first five lines
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, let’s get the `id` list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The output looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Like what we did before to read the CSV file, reading a JSON file with pandas
    was very simple and saved a lot of time and code to get the same information we
    needed. While in the first example, we needed to iterate over the JSON object
    list, pandas natively understands it as a DataFrame. Since every column behaves
    as a one-dimensional array, we can quickly get the values by passing the name
    of the column (or key) to the DataFrame name.
  prefs: []
  type: TYPE_NORMAL
- en: As with any other library, pandas has limitations, such as reading multiple
    files simultaneously, parallelism, or reading large datasets. Having this in mind
    can prevent problems and help us use this library optimally.
  prefs: []
  type: TYPE_NORMAL
- en: Why DataFrames?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: DataFrames are bi-dimensional and size-mutable tabular structures and visually
    resemble a structured table. Due to their versatility, they are widely used in
    libraries such as pandas (as we saw previously).
  prefs: []
  type: TYPE_NORMAL
- en: '**PySpark** is no different. **Spark** uses DataFrames as distributed data
    collections and can *parallelize* its tasks to process them through other cores
    or nodes. We will cover this in more depth in [*Chapter 7*](B19453_07.xhtml#_idTextAnchor227).'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To find out more about the files supported by the pandas library, follow this
    link: [https://pandas.pydata.org/pandas-docs/stable/user_guide/io.xhtml](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a SparkSession for PySpark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Previously introduced in [*Chapter 1*](B19453_01.xhtml#_idTextAnchor022), **PySpark**
    is a Spark library that was designed to work with Python. PySpark uses a Python
    API to write **Spark** functionalities such as data manipulation, processing (batch
    or real-time), and machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: However, before ingesting or processing data using PySpark, we must initialize
    a SparkSession. This recipe will teach us how to create a SparkSession using PySpark
    and explain its importance.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We first need to ensure we have the correct PySpark version. We installed PySpark
    in [*Chapter 1*](B19453_01.xhtml#_idTextAnchor022); however, checking if we are
    using the correct version is always good. Run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Next, we choose a code editor that can be any code editor that you want. I will
    use Jupyter due to the interactive interface.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s see how to create a SparkSession:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first create `SparkSession` as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And these are the warnings received:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We are not going to have an output when executing the code. Also, don’t worry
    about the `WARN` messages; they will not affect our work.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we get the Spark UI. To do this, in your Jupyter cell, type and execute
    the name of your instantiated as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – Output of execution of instance](img/Figure_4.4_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – Output of execution of instance
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we access SparkUI in the browser. To do this, we click on the **Spark
    UI** hyperlink. It will open a new tab in your browser, showing a graph with **Executors**
    and **Jobs**, and other helpful information:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.5 – Spark UI home page with an Event Timeline graph](img/Figure_4.5_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 – Spark UI home page with an Event Timeline graph
  prefs: []
  type: TYPE_NORMAL
- en: Since we still haven’t executed any processes, the graph is empty. Don’t worry;
    we will see it in action in the following recipes.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we saw at the beginning of this chapter, a is a fundamental part of starting
    our Spark jobs. It sets all the required configurations for **Spark’s YARN** (short
    for **Yet Another Resource Manager**) to allocate the memory, cores, and paths
    to write temporary and final outputs, among other things.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this in mind, let’s visit each step of our code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'To execute operations such as creating DataFrames, we need to use an instance
    of . The `spark` variable will be used to access DataFrames and other procedures.
    Now, take a look at this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The `.master()` method indicates which type of distributed processing we have.
    Since this is our local machine used only for educational purposes, we defined
    it as `"local[1]"`, where the integer value needs to be greater than `0`, since
    it represents the number of partitions. The `.appName()` method defines the name
    of our application session.
  prefs: []
  type: TYPE_NORMAL
- en: 'After declaring the name of our application, we can set the `.``config()` methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we defined two types of configuration: memory allocation and core allocation.
    `spark.executor.memory` tells YARN how much memory each executor can allocate
    to process data; `g` represents the size unit of it.'
  prefs: []
  type: TYPE_NORMAL
- en: '`spark.executor.cores` defines the number of executors used by YARN. By default,
    `1` is used. Next, `spark.cores.max` sets how many cores YARN can scale.'
  prefs: []
  type: TYPE_NORMAL
- en: Last, `.enableHiveSupport()` enables the support of Hive queries.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let’s look at this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '`.getOrCreate()` is simple as its name. If there is a session with this name,
    it will retrieve its configurations of it. Otherwise, it will create a new one.'
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Looking at the Spark documentation page, you will see that most configurations
    are not required to start our jobs or ingestions. However, it is essential to
    remember that we are handling a scalable framework that was created to allocate
    resources in a single machine or in clusters to process vast amounts of data.
    With no limit set to a SparkSession, YARN will allocate every resource it needs
    to process or ingest data, and this can result in server downtime or even freeze
    your entire local machine.
  prefs: []
  type: TYPE_NORMAL
- en: In a real-world scenario, a Kubernetes cluster is typically used to ingest or
    process shared data with other applications or users who do the same thing as
    you or your team. Physical memory and computational resources tend to be limited,
    so it is always an excellent practice to set the configurations, even in small
    projects that use serverless cloud solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Getting all configurations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It is also possible to retrieve the current configurations used for this application
    session using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the result we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6 – configurations set for the recipe](img/Figure_4.6_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 – configurations set for the recipe
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'See all configurations: [https://spark.apache.org/docs/latest/configuration.xhtml](https://spark.apache.org/docs/latest/configuration.xhtml)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In early versions of Spark, SparkContext was the starting point for working
    with data and was replaced by in the recent versions. You can read more about
    it here: [https://sparkbyexamples.com/spark/-vs-sparkcontext/](https://sparkbyexamples.com/spark/-vs-sparkcontext/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using PySpark to read CSV ﬁles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As expected, PySpark provides native support for reading and writing CSV files.
    It also allows data engineers to pass diverse kinds of setups in case the CSV
    has a different type of delimiter, special encoding, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we are going to cover how to read CSV files using PySpark using
    the most common configurations, and we will explain why they are needed.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can download the CSV dataset for this recipe from Kaggle: [https://www.kaggle.com/datasets/jfreyberg/spotify-chart-data](https://www.kaggle.com/datasets/jfreyberg/spotify-chart-data).
    We are going to use the same Spotify dataset as in [*Chapter 2*](B19453_02.xhtml#_idTextAnchor064).'
  prefs: []
  type: TYPE_NORMAL
- en: As in the *Creating a SparkSession for PySpark* recipe, make sure PySpark is
    installed and running with the latest stable version. Also, using Jupyter Notebook
    is optional.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first import and create a SparkSession :'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we read the CSV file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Then, we show the data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This is the result we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7 – spotify_data.csv DataFrame vision using Spark](img/Figure_4.7_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 – spotify_data.csv DataFrame vision using Spark
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Simple as it is, we need to understand how reading a file with Spark works
    to make sure it always executes properly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: We attributed a variable to our code statement. This is a good practice when
    initializing any file reading because it allows us to control the version of the
    file and, if a change needs to be made, we attribute it to another variable. The
    name of the variable is also intentional since we are creating our first DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the `.option()` method allows us to tell PySpark which type of configuration
    we want to pass. In this case, we set `header` to `True`, which makes PySpark
    set the first row of the CSV file as the column names. If we didn’t pass the required
    configuration, the DataFrame would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8 – spotify_data.csv DataFrame without column names](img/Figure_4.8_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 – spotify_data.csv DataFrame without column names
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The content of the files can differ, but some setups are welcome when handling
    CSV files in PySpark. Here, please note that we are going to change the method
    to `.options()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '`header`, `sep`, and `inferSchema` are the most commonly used setups when reading
    a CSV file. Although CSV stands for `sep` (which stands for separator) declared.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see an example of an error when reading a CSV that uses a pipe to separate
    the strings and passes the wrong separator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'This is how the output looks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9 – CSV DataFrame reading without a proper sep definition](img/Figure_4.9_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 – CSV DataFrame reading without a proper sep definition
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, it creates only one column with all the information. But if
    we pass `sep=''|''`, it will return correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.10 – CSV DataFrame with the sep definition set to pipe](img/Figure_4.10_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 – CSV DataFrame with the sep definition set to pipe
  prefs: []
  type: TYPE_NORMAL
- en: Other common .options() configurations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are other complex situations that, if not corrected during ingestion,
    can result in some problems with the other ETL steps. Here, I am using the `listing.csv`
    dataset, which can be found here at [http://data.insideairbnb.com/the-netherlands/north-holland/amsterdam/2022-03-08/visualisations/listings.csv](http://data.insideairbnb.com/the-netherlands/north-holland/amsterdam/2022-03-08/visualisations/listings.csv):'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first read our common configurations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 4.11 – listing.csv DataFrame](img/Figure_4.11_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.11 – listing.csv DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: At first glance, all is normal. However, what happens if I try to execute a
    simple group by using `room_type`?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the output we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.12 – Grouping df_broken by room_type](img/Figure_4.12_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.12 – Grouping df_broken by room_type
  prefs: []
  type: TYPE_NORMAL
- en: Ignoring `group by` for now, this happens because the file has plenty of escaped
    quotes and line breaks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s set the correct `.options()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.13 – Grouping by room_type using the right options() settings](img/Figure_4.13_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.13 – Grouping by room_type using the right options() settings
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can see more PySpark `.options()` in the official documentation: [https://spark.apache.org/docs/latest/sql-data-sources-csv.xhtml](https://spark.apache.org/docs/latest/sql-data-sources-csv.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: Using PySpark to read JSON ﬁles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the *Reading a JSON file* recipe, we saw that JSON files are widely used
    to transport and share data between applications, and we saw how to read a JSON
    file using simple Python code.
  prefs: []
  type: TYPE_NORMAL
- en: However, with the increase in data size and sharing, using only Python to process
    a high volume of data can lead to performance or resilience issues. That’s why,
    for this type of scenario, it is highly recommended to use PySpark to read and
    process JSON files. As you might expect, PySpark comes with a straightforward
    reading solution.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will cover how to read a JSON file with PySpark, the common
    associated issues, and how to solve them.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As in the previous recipe, *Reading a JSON file*, we are going to use the `GitHub
    Events` JSON file. Also, the use of Jupyter Notebook is optional.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are the steps for this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first create the SparkSession:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we read the JSON file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we show the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This is how the output appears:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.14 – df_json DataFrame](img/Figure_4.14_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.14 – df_json DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to CSV files, reading a JSON file using PySpark is very simple, requiring
    only one line of code. Like pandas, it ignores the brackets in the file and creates
    a table-structured DataFrame, even though we are handling a semi-structured data
    file. However, the great magic is in`.option("multiline", "true")`.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you remember our JSON structure for this file, it is something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'It is a multi-lined JSON since it has objects inside objects. The `.option()`
    setup passed when reading the file guarantees PySpark will read it as it should,
    and if we don’t pass this argument, an error like this will appear:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.15 – This error is shown when .options() is not well defined for
    a multi-lined JSON file](img/Figure_4.15_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.15 – This error is shown when .options() is not well defined for a
    multi-lined JSON file
  prefs: []
  type: TYPE_NORMAL
- en: PySpark understands it as a corrupted file.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A very widely used configuration for reading JSON files is `dropFieldIfAllNull`.
    When set to `true`, if there is an empty array, it will drop it from the schema.
  prefs: []
  type: TYPE_NORMAL
- en: Unstructured and semi-structured data are valuable due to their elasticity.
    Sometimes, applications can change their output and some fields become deprecated.
    To avoid changing the ingest script (especially if these changes can be frequent),
    `dropFieldIfAllNull` removes them from the DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To find out more about PySpark `.options()`, refer to the official documentation:
    [https://spark.apache.org/docs/latest/sql-data-sources-json.xhtml](https://spark.apache.org/docs/latest/sql-data-sources-json.xhtml)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[https://docs.fileformat.com/spreadsheet/csv/](https://docs.fileformat.com/spreadsheet/csv/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://codefather.tech/blog/python-with-open/](https://codefather.tech/blog/python-with-open/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.programiz.com/python-programming/methods/built-in/next](https://www.programiz.com/python-programming/methods/built-in/next)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://spark.apache.org/docs/latest/sql-data-sources-csv.xhtml](https://spark.apache.org/docs/latest/sql-data-sources-csv.xhtml)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://sparkbyexamples.com/pyspark/pyspark-read-json-file-into-dataframe/](https://sparkbyexamples.com/pyspark/pyspark-read-json-file-into-dataframe/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
