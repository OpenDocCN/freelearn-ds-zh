- en: Chapter 8. Sharing Models with Prediction Services
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章. 使用预测服务共享模型
- en: 'Thus far, we have examined how to build a variety of models with data sources
    ranging from standard ''tabular'' data to text and images. However, this only
    accomplishes part of our goal in business analysis: we can generate predictions
    from a dataset, but we cannot easily share the results with colleagues or with
    other software systems within a company. We also cannot easily replicate the results
    as new data becomes available without manually re-running the sorts of analyses
    discussed in previous chapters or scale it to larger datasets over time. We will
    also have difficulty to use our models in a public setting, such as a company''s
    website, without revealing the details of the analysis through the model parameters
    exposed in our code.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经探讨了如何使用从标准“表格”数据到文本和图像的各种数据源构建各种模型。然而，这仅仅完成了我们在商业分析中的部分目标：我们可以从数据集中生成预测，但无法轻松与同事或其他公司内部的软件系统共享结果。我们也无法在新的数据可用时轻松复制结果，而无需手动重新运行前几章讨论的分析，或者随着时间的推移将其扩展到更大的数据集。在没有通过我们代码中公开的分析模型参数揭示分析细节的情况下，我们还将难以在公共环境中（如公司的网站）使用我们的模型。
- en: To overcome these challenges, the following chapter will describe how to build
    'prediction services', web applications that encapsulate and automate the core
    components of data transformation, model fitting, and scoring of new observations
    that we have discussed in the context of predicative algorithms in prior sections.
    By packaging our analysis into a web application, we can both easily scale the
    modeling system and change implementations in the underlying algorithm, all the
    while making such changes invisible to the consumer (whether a human or other
    software system), who interacts with our predictive models by making requests
    to our application through web URLs and a standard REST **application programmer
    interface** (**API**). It also allows initialization and updates to the analysis
    to be automated through calls to the service, making the predictive modeling task
    consistent and replicable. Finally, by carefully parameterizing many of the steps,
    we can use the same service to interact with interchangeable data sources computation
    frameworks.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这些挑战，下一章将描述如何构建“预测服务”，这些是封装并自动化数据转换、模型拟合以及新观测评分等核心组件的Web应用程序，这些内容我们在前几节讨论预测算法时已经讨论过。通过将我们的分析打包成Web应用程序，我们不仅可以轻松扩展建模系统，还可以更改底层算法的实现，同时使这些更改对消费者（无论是人类还是其他软件系统）不可见，消费者通过向我们的应用程序通过Web
    URL和标准的REST **应用程序编程接口**（**API**）发送请求来与我们的预测模型交互。这还允许通过调用服务来自动化分析初始化和更新，使预测建模任务保持一致并可重复。最后，通过仔细参数化许多步骤，我们可以使用同一服务与可互换的数据源计算框架交互。
- en: 'In essences, building a prediction service involves linking several of the
    components we have already discussed, such as data transformation and predictive
    modeling, with a set of new components that we will discuss in this chapter for
    the first time. To this end, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，构建预测服务涉及将我们已讨论的几个组件（如数据转换和预测建模）与本章首次讨论的一系列新组件相连接。为此，我们将涵盖以下主题：
- en: How to instrument a basic web application and server using the Cherrypy and
    Flask frameworks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用Cherrypy和Flask框架对基本Web应用程序和服务器进行监控
- en: How to automate a generic modeling framework using a RESTful API
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用RESTful API自动化通用建模框架
- en: Scaling our system using a Spark computation framework
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark计算框架扩展我们的系统
- en: Storing the results of our predictive model in database systems for reporting
    applications we will discuss in [Chapter 9](ch09.html "Chapter 9. Reporting and
    Testing – Iterating on Analytic Systems"), *Reporting and Testing – Iterating
    on Analytic Systems*
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将我们的预测模型结果存储在数据库系统中，以便在[第9章](ch09.html "第9章。报告和测试 – 在分析系统中迭代")中讨论的报告应用程序中，*报告和测试
    – 在分析系统中迭代*
- en: The architecture of a prediction service
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测服务的架构
- en: Now with a clear goal in mind—to share and scale the results of our predictive
    modeling using a web application—what are the components required to accomplish
    this objective?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有了明确的目标——通过Web应用程序共享和扩展我们的预测建模结果——要实现这一目标需要哪些组件？
- en: 'The first is the *client*: this could be either a web browser or simply a user
    entering a `curl` command in the terminal (see Aside). In either case, the client
    sends requests using **hypertext transfer protocol** (**HTTP**), a standard transport
    convention to retrieve or transmit information over a network (Berners-Lee, Tim,
    Roy Fielding, and Henrik Frystyk. *Hypertext transfer protocol--HTTP/1.0*. No.
    RFC 1945\. 1996). An important feature of the HTTP standard is that the client
    and server do not have to ''know'' anything about how the other is implemented
    (for example, which programming language is used to write these components) because
    the message will remain consistent between them regardless by virtue of following
    the HTTP standard.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个是*客户端*：这可以是网络浏览器，或者简单地是用户在终端中输入`curl`命令（见旁注）。在两种情况下，客户端都使用**超文本传输协议**（**HTTP**）发送请求，这是一种标准传输约定，用于在网络中检索或传输信息（Berners-Lee,
    Tim, Roy Fielding, and Henrik Frystyk. *Hypertext transfer protocol--HTTP/1.0*.
    No. RFC 1945\. 1996）。HTTP标准的一个重要特性是客户端和服务器不必“知道”任何关于对方实现的信息（例如，用于编写这些组件的编程语言），因为只要遵循HTTP标准，消息在他们之间就会保持一致。
- en: 'The next component is the *server*, which receives HTTP requests from a client
    and forwards them to the application. You can think of it as the gateway for the
    requests from the client to our actual predictive modeling application. In Python,
    web servers and applications each conform to the **Web Server Gateway Interface**
    (**WSGI**), which specifies how the server and application should communicate.
    Like the HTTP requests between client and server, this standard allows the server
    and application to be modular as long as both consistently implement the interface.
    In fact, there could even be intervening middleware between the server and application
    that further modifies communication between the two: as long as the format of
    this communication remains constant, the details of each side of the interface
    are flexible. While we will use the Cherrypy library to build a server for our
    application, other common choices are Apache Tomcat and Nginx, both written in
    the Java programming language.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个组件是*服务器*，它从客户端接收HTTP请求并将它们转发到应用程序。你可以将其视为客户端请求通往我们实际预测建模应用的网关。在Python中，Web服务器和应用程序都遵循**Web服务器网关接口**（**WSGI**），它指定了服务器和应用程序应该如何通信。就像客户端和服务器之间的HTTP请求一样，这个标准允许服务器和应用程序只要两者一致实现接口就可以模块化。实际上，服务器和应用程序之间甚至可能存在中间件来进一步修改两者之间的通信：只要这种通信的格式保持一致，接口两边的细节就是灵活的。虽然我们将使用Cherrypy库为我们构建服务器，但其他常见的选择是Apache
    Tomcat和Nginx，它们都是用Java编程语言编写的。
- en: After the client request has been received and forwarded by the server, the
    application performs operations in response to the requests, and returns a value
    indicating the success or failure of the task. These requests could, for example,
    obtain for the predicted score for a particular user, update to the training dataset,
    or perform a round of model training.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在客户端请求被服务器接收并转发之后，应用程序会根据请求执行操作，并返回一个值来指示任务的执行成功或失败。例如，这些请求可以获取特定用户的预测分数，更新训练数据集，或者进行一轮模型训练。
- en: Tip
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: '**Aside: The curl command**'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**旁注：curl命令**'
- en: 'As part of testing our prediction service, it is useful to have a way to quickly
    issue commands to the server and observe the response we receive back. While we
    could do some of this interactively using the address bar of a web browser, it
    is not easy to script browser activities in cases where we want to run a number
    of tests or replicate a particular command. The `curl` command, found in most
    Linux command line terminals, is very useful for this purpose: the same requests
    (in terms of a URL) can be issued to the prediction service using the `curl` command
    as would be given in the browser, and this call can be automated using shell scripting.
    The `curl` application can be installed from [https://curl.haxx.se/](https://curl.haxx.se/).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 作为测试我们的预测服务的一部分，有一个快速向服务器发送命令并观察我们收到的响应的方法是非常有用的。虽然我们可以通过使用网络浏览器的地址栏进行一些交互式操作，但在我们需要运行多个测试或复制特定命令的情况下，脚本浏览器活动并不容易。大多数Linux命令行终端中都可以找到的`curl`命令对于这个目的非常有用：可以使用`curl`命令向预测服务发出与在浏览器中给出的相同的请求（就URL而言），并且可以使用shell脚本自动化这个调用。`curl`应用程序可以从[https://curl.haxx.se/](https://curl.haxx.se/)安装。
- en: 'The web application relies upon server-side code to perform commands in response
    to requests from the web server. In our example, this server-side code is divided
    into several components: the first is a generic interface for the modeling logic,
    which specifies a standard way to construct predictive models, train them with
    an input dataset, and score incoming data. The second is an implementation of
    this framework using the logistic regression algorithm from [Chapter 5](ch05.html
    "Chapter 5. Putting Data in its Place – Classification Methods and Analysis"),
    *Putting Data in its Place – Classification Methods and Analysis*. This code relies
    upon executing Spark jobs, which could be carried out either locally (on the same
    machine as the web application) or remotely (on a separate cluster).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Web应用程序依赖于服务器端代码来执行响应于Web服务器请求的命令。在我们的例子中，这个服务器端代码被分为几个组件：第一个是为建模逻辑提供的通用接口，它指定了构建预测模型、使用输入数据集对其进行训练以及评分传入数据的标准方式。第二个是使用[第5章](ch05.html
    "第5章。将数据放在合适的位置 – 分类方法和分析")中的逻辑回归算法（*Putting Data in its Place – Classification
    Methods and Analysis*）实现此框架。此代码依赖于执行Spark作业，这些作业可以在本地（在Web应用程序所在的同一台机器上）或远程（在单独的集群上）执行。
- en: The final piece of this chain is database systems that can persist information
    used by the prediction service This database could be as simple as file system
    on the same machine as the web server or as complex as a distributed database
    software. In our example we will use both Redis (a simple key-value store) and
    MongoDB (a NoSQL database) to store the data used in modeling, transient information
    about our application, and the model results themselves.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这个链的最后一部分是数据库系统，它可以持久化预测服务使用的信息。这个数据库可能只是与Web服务器在同一台机器上的文件系统，也可能像分布式数据库软件一样复杂。在我们的例子中，我们将使用Redis（一个简单的键值存储）和MongoDB（一个NoSQL数据库）来存储用于建模的数据、关于我们应用程序的瞬态信息以及模型结果本身。
- en: 'As we have emphasized, an important feature of these three components is that
    they are largely independent: because the WGSI standard defines how the web server
    and application communicate, we could change server and predictive model implementation,
    and as long as the commands used in the web application are the same, the code
    will still work since these commands are formatted in a consistent way.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前强调的，这三个组件的一个重要特征是它们在很大程度上是独立的：因为WGSI标准定义了Web服务器和应用程序之间的通信方式，所以我们可以更改服务器和预测模型实现，只要在Web应用程序中使用的命令相同，代码仍然可以工作，因为这些命令是以一致的方式格式化的。
- en: Now that we have covered the basic components of a prediction service and how
    they communicate with one another, let us examine each in greater detail.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了预测服务的基本组件以及它们如何相互通信，让我们更详细地考察每一个。
- en: Clients and making requests
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 客户端和发出请求
- en: 'When a client issues requests to the server and the downstream application,
    we might potentially have a major design problem: how do we know in advance what
    kind of requests we might receive? If we had to re-implement a new set of standard
    requests every time we developed a web application, it would be difficult to reuse
    code and write generic services that other programs could call, since their requests
    would potentially have to change for every web application a client might interact
    with.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当客户端向服务器和下游应用程序发出请求时，我们可能会遇到一个主要的设计问题：我们如何事先知道我们可能会收到什么类型的请求？如果我们每次开发Web应用程序时都必须重新实现一组新的标准请求，那么将很难重用代码并编写其他程序可以调用的通用服务，因为它们的请求可能会随着客户端可能与之交互的每个Web应用程序而变化。
- en: 'This is the problem solved by the HTTP standard, which describes a standard
    language and format in which requests are sent between servers and clients, allowing
    us to rely upon a common command syntax, which could be consumed by many different
    applications. While we could, in theory, issue some of these commands to our prediction
    service by pasting a URL into the address bar of our browser (such as GET, described
    below), this will only cover a subset of the kinds of requests we want to issue.
    The standard sorts of requests we typically implement in a web application are:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是HTTP标准解决的问题，它描述了一种标准语言和格式，用于服务器和客户端之间发送请求，使我们能够依赖于一个通用的命令语法，该语法可以被许多不同的应用程序消费。虽然从理论上讲，我们可以通过将URL粘贴到浏览器的地址栏中（例如GET，下面将描述）向我们的预测服务发出一些这些命令，但这只会覆盖我们想要发出的请求类型的一个子集。我们在Web应用程序中通常实现的请求类型包括：
- en: The GET requests
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GET请求
- en: 'The `GET` requests only retrieve information, which will then be rendered in
    our web browser depending upon the kind of response. We could receive back an
    actual `html` page, or simply a piece of text. In order to specify what information
    we want to receive, a GET request will include variables in the URL in the form
    `url?key1=value1&key2=value2`. URL is the web address given to the prediction
    service, which in our example will just be the local machine, but could also be
    any valid IP address or URL. This URL is separated by a question mark (**?**)
    from the (key, value) pairs that define the parameters of our request for information.
    Multiple parameters may be specified: for example, we could indicate a pair of
    parameters for a user and item dataset using the string `userid=12894&itemid=93819`,
    with each key, value pair separated by the ampersand symbol (`&`).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '`GET`请求只检索信息，这些信息将根据响应类型在我们的Web浏览器中渲染。我们可能会收到一个实际的`html`页面，或者只是一段文本。为了指定我们想要接收的信息，GET请求将在URL中包含变量，形式为`url?key1=value1&key2=value2`。URL是提供给预测服务的Web地址，在我们的例子中将是本地机器，但也可能是任何有效的IP地址或URL。这个URL通过一个问号（**?**）与定义我们信息请求参数的（键，值）对分开。可以指定多个参数：例如，我们可以使用字符串`userid=12894&itemid=93819`来表示用户和项目数据集的一对参数，每个键值对由与符号（`&`）分隔。'
- en: 'We can directly issue a `GET` request by pasting the URL format described previously
    into the address bar of a browser or by issuing a `curl` command to the same address
    by typing the following into a terminal:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直接通过将之前描述的URL格式粘贴到浏览器的地址栏中，或者通过在终端中输入以下命令向同一地址发出一个`curl`命令来直接发出一个`GET`请求：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can also use the Python requests library ([http://docs.python-requests.org/en/master/](http://docs.python-requests.org/en/master/)),
    which allows us to not worry about the details of formatting the URL. Using this
    library, the same GET request is called in the following way:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用Python requests库（[http://docs.python-requests.org/en/master/](http://docs.python-requests.org/en/master/)），它允许我们不必担心URL格式化的细节。使用这个库，相同的GET请求可以通过以下方式调用：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here, `params` is a dictionary of key-value pairs that we would have passed
    in the URL. The requests library performs this formatting for us, as we can see
    by printing the resulting URL:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`params`是我们本应传递到URL中的键值对字典。requests库为我们执行了这种格式化，正如我们可以通过打印生成的URL所看到的那样：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Once we have issued the request, we can check the result using either of the
    following two commands:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们发出了请求，我们可以使用以下两个命令中的任何一个来检查结果：
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can also check the status code of the response to see if there was an error
    or not (see aside on standard response codes):'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以检查响应的状态码，以查看是否发生了错误（参见关于标准响应代码的附录）：
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Tip
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: '**Aside: HTTP Status Codes**'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**附录：HTTP状态码**'
- en: 'When we issue a request to a web application using the methods discussed in
    this chapter, one way to check the success of the request is to examine the response
    code, which gives a standard number corresponding to the response of the web application
    to the request. You may have even seen these codes before without realizing it,
    such as the 404 error that is returned when a webpage cannot be displayed in your
    browser. The standard codes to be aware of are:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用本章讨论的方法向Web应用程序发出请求时，检查请求成功的一种方法是通过检查响应代码，它给出一个与Web应用程序对请求的响应相对应的标准数字。你可能甚至在没有意识到的情况下见过这些代码，例如当网页在你的浏览器中无法显示时返回的404错误。需要注意的标准代码如下：
- en: '200: success, we usually check this value to make sure we received a correct
    response.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 200：成功，我们通常检查这个值以确保我们收到了正确的响应。
- en: '404: not found, indicating that the web application could not find the resource
    we requested.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 404：未找到，表示Web应用程序找不到我们请求的资源。
- en: '500: server error, which we will often receive if the code run by our web application
    runs into problems.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 500：服务器错误，如果我们运行的Web应用程序代码遇到问题，我们通常会收到这个错误。
- en: For a more comprehensive list please, see (Nottingham, Mark, and Roy Fielding.
    "Additional HTTP Status Codes." (2012); Berners-Lee, Tim, Roy Fielding, and Henrik
    Frystyk. Hypertext transfer protocol--HTTP/1.0\. No. RFC 1945\. 1996).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更全面的列表，请参阅（Nottingham, Mark，和Roy Fielding. "Additional HTTP Status Codes."
    (2012)；Berners-Lee, Tim，Roy Fielding，和Henrik Frystyk. Hypertext transfer protocol--HTTP/1.0\.
    No. RFC 1945\. 1996）。
- en: The POST request
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: POST请求
- en: 'Unlike the GET command, the POST request does not use data contained in the
    URL, but rather transmits information separate from the URL. If you have ever
    entered your credit card information in an online store, this information is probably
    transmitted using a POST request, which is fortunate since it then remains hidden.
    However, the fact that the information for the request is not contained in the
    URL means that we cannot simply paste the request into the address bar of our
    web browser: we would need a form on the webpage that issues the POST request
    or make the request programmatically ourselves. Without an actual form on a webpage,
    we can use a `curl` command to issue a POST request using the following syntax:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 与 GET 命令不同，POST 请求不使用 URL 中包含的数据，而是传输与 URL 分离的信息。如果您曾在在线商店输入过信用卡信息，那么这些信息可能就是通过
    POST 请求传输的，这是幸运的，因为这样信息就保持隐藏了。然而，由于请求信息不包含在 URL 中，我们不能简单地将其粘贴到 Web 浏览器的地址栏中：我们需要网页上发出
    POST 请求的表单，或者我们自己编程发出请求。如果没有实际的表单在网页上，我们可以使用 `curl` 命令，使用以下语法发出 POST 请求：
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can also use the Python requests library:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用 Python 的 requests 库：
- en: '[PRE6]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In the preceding code, `data` is a Python dictionary of information that the
    web application can access in fulfilling the POST request.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，`data` 是一个 Python 字典，包含网络应用程序在满足 POST 请求时可以访问的信息。
- en: The HEAD request
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: HEAD 请求
- en: 'Like the GET request, HEAD retrieves information, but instead of the body of
    the response (such as a webpage or JSON), it only retrieves metadata about the
    response (such as the encoding). We can issue a HEAD request using the following:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 与 GET 请求类似，HEAD 请求检索信息，但它只检索响应的元数据（例如编码），而不是响应的主体（例如网页或 JSON）。我们可以使用以下方式发出 HEAD
    请求：
- en: '[PRE7]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Note that we have added the `–i` flag to this request; normally, the `curl`
    command will not print header information without this option. Using the Python
    requests library we would use the command:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们已向此请求添加了 `–i` 标志；通常，没有此选项，`curl` 命令不会打印头部信息。使用 Python 的 requests 库，我们将使用以下命令：
- en: '[PRE8]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The PUT request
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PUT 请求
- en: 'In cases where our web application has access to a database system, we issue
    PUT commands in order to store new information. Using `curl`, we make this request
    using the following:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的网络应用程序可以访问数据库系统的情况下，我们发出 PUT 命令以存储新信息。使用 `curl`，我们使用以下方式发出此请求：
- en: '[PRE9]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can also make this request using the requests library:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用 requests 库发出此请求：
- en: '[PRE10]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Here, data is a dictionary of the arguments we wish to place in the applications
    storage system.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，数据是我们希望放置在应用程序存储系统中的参数的字典。
- en: The DELETE request
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DELETE 请求
- en: 'The opposite of the PUT command, DELETE requests are issued to remove a piece
    of data from the application''s storage system. The curl command is as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 与 PUT 命令相反，DELETE 请求用于从应用程序的存储系统中删除数据。curl 命令如下：
- en: '[PRE11]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'While the same request using the requests library is as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 而使用 requests 库执行相同请求的方式如下：
- en: '[PRE12]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Here, data is a dictionary of the arguments we wish to remove from the applications
    storage system.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，数据是我们希望从应用程序存储系统中删除的参数的字典。
- en: While there are other requests types available, we will not cover them in this
    discussion; for more details please see (Berners-Lee, Tim, Roy Fielding, and Henrik
    Frystyk. Hypertext transfer protocol--HTTP/1.0\. No. RFC 1945\. 1996). Note that
    since we can issue these requests using the Python request library, we can actually
    test our web application in the Python notebooks we have been using in the exercises
    in this volume.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管还有其他请求类型可用，但在此讨论中我们将不涉及它们；更多详情请参阅（Berners-Lee, Tim, Roy Fielding, and Henrik
    Frystyk. 超文本传输协议--HTTP/1.0\. No. RFC 1945\. 1996）。请注意，由于我们可以使用 Python 的 request
    库发出这些请求，我们实际上可以在本卷练习中使用的 Python 笔记本中测试我们的 Web 应用程序。
- en: For our purposes, the client will be the Jupyter notebook itself or the command
    line of the terminal; however, we could imagine other cases where the client is
    actually another web application that issues these commands and acts on the response.
    Again, since the server only needs to guarantee a particular message format rather
    than the details of the sender, either option is interchangeable.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的目的，客户端将是 Jupyter 笔记本本身或终端的命令行；然而，我们可以想象其他情况，其中客户端实际上是另一个发出这些命令并对响应采取行动的
    Web 应用程序。再次强调，由于服务器只需要保证特定的消息格式而不是发送者的详细信息，两种选项可以互换。
- en: Now that we know how to issue HTTP requests to our service, let us look at the
    server.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了如何向我们的服务发出 HTTP 请求，让我们来看看服务器。
- en: Server – the web traffic controller
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 服务器 – 网络流量控制器
- en: To run our prediction service, we need to communicate with external systems
    to receive requests to train a model, score new data, evaluate existing performance,
    or provide model parameter information. The web server performs this function,
    accepting incoming HTTP requests and forwarding them on to our web application
    either directly or through whatever middleware may be used.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行我们的预测服务，我们需要与外部系统通信以接收训练模型、评分新数据、评估现有性能或提供模型参数信息的请求。Web服务器执行这个功能，接受传入的HTTP请求，并将它们直接或通过可能使用的任何中间件转发到我们的Web应用程序。
- en: Though we could have made many different choices of server in illustrating this
    example, we have chosen the CherryPy library because unlike other popular servers
    such as Apache Tomcat or Nginx, it is written in Python (allowing us to demonstrate
    its functionality inside a notebook) and is scalable, processing many requests
    in only a few milliseconds ([http://www.aminus.org/blogs/index.php/2006/12/23/cherrypy_3_has_fastest_wsgi_server_yet](http://www.aminus.org/blogs/index.php/2006/12/23/cherrypy_3_has_fastest_wsgi_server_yet).).
    The server is attached to a particular port, or endpoint (this is usually given
    in the format `url:port`), to which we direct requests that are then forwarded
    to the web application. The use of ports means that we could in theory have multiple
    servers on a given URL, each listening to requests on a different endpoint.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们可以在展示这个示例时选择许多不同的服务器，但我们选择了CherryPy库，因为它与Apache Tomcat或Nginx等其他流行的服务器不同，它是用Python编写的（允许我们在笔记本中演示其功能），并且是可扩展的，只需几毫秒就能处理许多请求（[http://www.aminus.org/blogs/index.php/2006/12/23/cherrypy_3_has_fastest_wsgi_server_yet](http://www.aminus.org/blogs/index.php/2006/12/23/cherrypy_3_has_fastest_wsgi_server_yet)）。服务器连接到特定的端口或端点（这通常以`url:port`的格式给出），我们将请求指向该端点，然后请求被转发到Web应用程序。端口的用途意味着在理论上我们可以在给定的URL上有多个服务器，每个服务器监听不同的端点。
- en: 'As we discussed previously, the server uses the WGSI specification to communicate
    with the application itself. In concrete terms, the server has a function known
    as a callable (for example, any object with a `__call__` method) that is executed
    every time it receives a request, whose result is handed off to the application.
    In our example in this chapter, the WGSI is already implemented by CherryPy, and
    we will simply illustrate how it does so. Complete documentation of the interface
    is available at ([https://www.python.org/dev/peps/pep-0333/](https://www.python.org/dev/peps/pep-0333/)).
    In a way, the WGSI solves the same problem as HTTP in the communication between
    servers and applications: it provides a common way in which the two systems exchange
    information, allowing us to swap the components or event place intervening components
    without altering the fundamental way in which information is transferred.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前讨论的，服务器使用WGSI规范与应用程序本身进行通信。具体来说，服务器有一个名为可调用（callable）的功能（例如，任何具有`__call__`方法的对象），每次接收到请求时都会执行这个功能，并将结果传递给应用程序。在本章的示例中，WGSI已经被CherryPy实现，我们将简单地说明它是如何做到这一点的。该接口的完整文档可在[https://www.python.org/dev/peps/pep-0333/](https://www.python.org/dev/peps/pep-0333/)找到。从某种意义上说，WGSI解决了服务器与应用程序之间通信中与HTTP相同的问题：它提供了一个两个系统交换信息的通用方式，使我们能够在不改变信息传输基本方式的情况下交换组件或放置中间组件。
- en: In cases where we might wish to scale the application to a larger load, we could
    imagine middleware such as a load balancer between the server and the application.
    The middleware would receive the callable output and pass it along to the web
    application. In the case of a load balancer, this could potentially redistribute
    requests to many separate instances of the same predictive service, allowing us
    to scale the service horizontally (see Aside). Each of these services would then
    return their response to the server before it is sent back to the client.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们可能希望将应用程序扩展到更大负载的情况下，我们可以想象在服务器和应用程序之间有一个中间件，比如负载均衡器。这个中间件会接收可调用输出并将其传递给Web应用程序。在负载均衡器的情况下，这可能会将请求重新分配到许多相同的预测服务的单独实例，使我们能够水平扩展服务（见旁注）。然后，这些服务中的每一个都会在将响应发送回客户端之前将其发送回服务器。
- en: Tip
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**提示**'
- en: '**Aside: horizontal and vertical scaling**'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**旁注：水平和垂直扩展**'
- en: As the volume of data or computational complexity of our prediction services
    increases, we have two primary ways to increase the performance of the service.
    The first, known as horizontal scaling, might involve adding more instances of
    our application. Separately, we might also increase the number of resources in
    our underlying computing layer, such as Spark. In contrast, vertical scaling involves
    improving the existing resources by adding more RAM, CPU, or disk space. While
    horizontal scaling is more easily implemented using software alone, the right
    solution for such resources constraints will depend on the problem domain and
    organizational budget.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们的预测服务数据量或计算复杂度的增加，我们有两种主要方式来提高服务的性能。第一种，称为水平扩展，可能涉及添加我们应用程序的更多实例。另外，我们也可能增加我们底层计算层中的资源数量，例如Spark。相比之下，垂直扩展涉及通过添加更多RAM、CPU或磁盘空间来改进现有资源。虽然仅使用软件就可以更容易地实现水平扩展，但针对此类资源约束的正确解决方案将取决于问题领域和组织预算。
- en: Application – the engine of the predictive services
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用程序 – 预测服务的引擎
- en: Once a request has made its way from the client to the application, we need
    to provide the logic that will execute these commands and return a response to
    the server and subsequently client upstream. To do so, we must attach a function
    to the particular endpoint and requests we anticipate receiving.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦请求从客户端传递到应用程序，我们需要提供执行这些命令并返回响应给服务器以及随后客户端的后续逻辑。为此，我们必须将一个函数附加到我们预期接收的特定端点和请求上。
- en: 'In this chapter, we will be using the Flask framework to develop our web application
    ([http://flask.pocoo.org/](http://flask.pocoo.org/)). While Flask can also support
    template generation of HTML pages, in this chapter we will be using it purely
    to implement various requests to the underlying predictive algorithm code through
    URL endpoints corresponding to the HTTP requests discussed previously. Implementing
    these endpoints allows a consistent interface through which many other software
    systems could interact with our application—they just need to point to the appropriate
    web address and process the response returned from our service. In case you are
    concerned we will not generate any actual ''webpages'' in our application, do
    not be worried: we will use the same Flask framework in [Chapter 9](ch09.html
    "Chapter 9. Reporting and Testing – Iterating on Analytic Systems"), *Reporting
    and Testing – Iterating on Analytic Systems*, to develop a dashboard system based
    on the data we will generate through the predictive modeling service in this chapter.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用Flask框架来开发我们的Web应用程序([http://flask.pocoo.org/](http://flask.pocoo.org/))。虽然Flask也可以支持HTML页面的模板生成，但在本章中，我们将仅使用它通过对应于之前讨论的HTTP请求的URL端点来实现对底层预测算法代码的各种请求。实现这些端点允许通过一个一致的接口，许多其他软件系统可以与我们的应用程序交互——他们只需要指向适当的Web地址并处理从我们的服务返回的响应。如果你担心我们不会在我们的应用程序中生成任何实际的“网页”，请不要担心：我们将在[第9章](ch09.html
    "第9章。报告和测试 – 在分析系统中迭代")，*报告和测试 – 在分析系统中迭代*中使用相同的Flask框架，来开发一个基于我们将在本章通过预测建模服务生成数据的仪表板系统。
- en: In writing the logic for our predictive modeling application, it is important
    to keep in mind that the functions that are called in response to client requests
    can themselves be interfaces specifying a generic, modular service. While we could
    directly implement a particular machine learning algorithm in the code for the
    web application itself, we have chosen to abstract this design, with the web application
    instead making a generic call to construct a model with some parameters, train,
    and score using an algorithm, regardless of the data or particular model used
    in the application. This allows us to reuse the web application code with many
    different algorithms while also affording the flexibility to implement these algorithms
    in different ways over time. It also forces us to determine a consistent set of
    operations for our algorithms since the web application will only interact with
    them through this abstraction layer.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写我们的预测建模应用程序的逻辑时，重要的是要记住，响应客户端请求而调用的函数本身可以是指定通用、模块化服务的接口。虽然我们可以在网络应用程序的代码中直接实现特定的机器学习算法，但我们选择抽象这种设计，让网络应用程序通过一些参数构建模型、训练和评分的通用调用，而不论应用程序中使用的数据或特定模型。这使我们能够在许多不同的算法上重用网络应用程序代码，同时也提供了随着时间的推移以不同方式实现这些算法的灵活性。这也迫使我们确定算法的一致操作集，因为网络应用程序将通过这个抽象层与它们交互。
- en: Finally, we have the algorithm itself, which is called by the web application
    code. This program needs to implement functions, such as training a model and
    scoring records using a set of data, specified in the web application. The details
    can change substantially over time without need to modify the web application,
    allowing us to flexibly develop new models or experiment with different libraries.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有算法本身，这是由网络应用程序代码调用的。这个程序需要实现函数，例如使用一组数据训练模型和评分记录，这些函数在网络应用程序中指定。随着时间的推移，这些细节可能会发生重大变化，而无需修改网络应用程序，这使得我们能够灵活地开发新的模型或尝试不同的库。
- en: Persisting information with database systems
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用数据库系统持久化信息
- en: Our prediction service will use data in a number of ways. When we start the
    service, we have standard configurations we would like to retrieve (for example,
    the model parameters), and we might also like to log records of the requests that
    the application responds to for debugging purposes. As we score data or prepare
    trained models, we would ideally like to store these somewhere in case the prediction
    service needs to be restarted. Finally, as we will discuss in more detail, a database
    can allow us to keep track of application state (such as which tasks are in progress).
    For all these uses, a number of database systems can be applied.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的预测服务将以多种方式使用数据。当我们启动服务时，我们希望检索标准配置（例如，模型参数），并且我们也可能希望记录应用程序响应的请求记录以供调试目的。在我们评分数据或准备训练模型时，我们理想情况下希望将这些数据存储在某个地方，以防预测服务需要重新启动。最后，正如我们将更详细讨论的，数据库可以让我们跟踪应用程序状态（例如，哪些任务正在进行）。对于所有这些用途，可以应用多种数据库系统。
- en: 'Databases are generally categorized into two groups: relational and non-relational.
    Relational databases are probably familiar to you, as they are used in most business
    data warehouses. Data is stored in the form of tables, often with facts (such
    as purchases or search events) containing columns (such as user account IDs or
    an item identifier) that may be joined to dimensional tables (containing information
    on an item or user) or relational information (such as a hierarchy of items IDs
    that define the contents of an online store). In a web application, a relational
    system can be used behind the scenes to retrieve information (for example, in
    response to a GET request for user information), to insert new information, or
    delete rows from the database. Because the data in a relational system is stored
    in tables, it needs to follow a common series of columns, and these sorts of systems
    are not designed with nested structures such as JSON in mind. If we know there
    are columns we will frequently query (such as an item ID), we can design indices
    on the tables in these systems that speed up retrieval. Some common popular (and
    open source) relational systems are MySQL, PostGreSQL, and SQLite.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库通常分为两组：关系型和非关系型。关系型数据库可能对你来说很熟悉，因为它们被用于大多数商业数据仓库。数据以表格形式存储，通常包含事实（如购买或搜索事件），这些事实包含列（如用户账户ID或项目标识符），这些列可以与维度表（包含有关项目或用户的信息）或关系信息（如定义在线商店内容的物品ID层次结构）相关联。在Web应用程序中，关系型系统可以在幕后用于检索信息（例如，响应用户信息的GET请求），插入新信息或从数据库中删除行。由于关系型系统中的数据存储在表中，它需要遵循一系列常见的列，并且这些系统并不是针对嵌套结构（如JSON）设计的。如果我们知道将经常查询的列（如项目ID），我们可以在这些系统中的表上设计索引，以加快检索速度。一些常见的流行（和开源）关系型系统包括MySQL、PostgreSQL和SQLite。
- en: Non-relational databases, also known as 'NoSQL', follow a very different data
    model. Instead of being formed of tables with multiple columns, these systems
    are designed as with alternative layouts such as key-value stores, where a row
    of information (such as a customer account) has a key (such as an item index)
    and an arbitrary amount of information in the value field. For example, the value
    could be a single item or a nested series of other key-values. This flexibility
    means that NoSQL databases can store information with diverse schema even in the
    same table, since the fields in the value do not need to be specifically defined.
    Some of these applications allow us to create indices on particular fields within
    the value, just as for relational systems. In addition to key-value databases
    (such as Redis) and document stores (such as MongoDB), NoSQL systems also include
    columnar stores where data are co-located in files based primarily on column chunks
    rather than rows (examples include Cassandra and Druid), and graph databases such
    as Neo4j which are optimized for data composed of nodes and edges (such as what
    we studied in the context of spectral clustering in [Chapter 3](ch03.html "Chapter 3. Finding
    Patterns in the Noise – Clustering and Unsupervised Learning"), *Finding Patterns
    in the Noise – Clustering and Unsupervised Learning*). We will use MongoDB and
    Redis in our example in this chapter.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 非关系型数据库，也称为'NoSQL'，遵循一个非常不同的数据模型。这些系统不是由多列的表组成，而是设计为具有替代布局，例如键值存储，其中一行信息（如客户账户）有一个键（如项目索引）和值字段中的任意数量的信息。例如，值可以是单个项目或嵌套的其他键值序列。这种灵活性意味着NoSQL数据库可以在同一表中存储具有不同模式的信息，因为值字段中的字段不需要具体定义。其中一些应用程序允许我们在值中的特定字段上创建索引，就像关系型系统一样。除了键值数据库（如Redis）和文档存储（如MongoDB）之外，NoSQL系统还包括列存储，其中数据主要基于列块而不是行（例如Cassandra和Druid），以及图数据库，如Neo4j，这些数据库针对由节点和边组成的数据进行了优化（例如我们在[第3章](ch03.html
    "第3章。在噪声中寻找模式 - 聚类和无监督学习")中研究的谱聚类上下文），*在噪声中寻找模式 - 聚类和无监督学习*。在本章的示例中，我们将使用MongoDB和Redis。
- en: In addition to storing data with flexible schema, such as the nested JSON strings
    we might encounter in REST API calls, key-value stores can server another function
    in a web application by allowing us to persist the state of a task. For quickly
    answered requests such as a GET class for information, this is not necessary.
    However, prediction services might frequently have long-running tasks that are
    launched by a POST request and take time to compute a response. Even if the task
    is not complete though, we want to return an immediate response to the client
    that initiated the task. Otherwise, the client will stall waiting for the server
    to complete, and this can potentially affect performance of the client and is
    very much against the philosophy of decoupling the components of the system described
    previously. Instead, we want to return a task identifier to the client immediately,
    which will allow the client to poll the service to check on the progress of the
    task and retrieve the result when it is available. We can store the state of a
    task using a key-value database and provide both update methods to allow us to
    provide information on intermediate progress by editing the task records and GET
    methods to allow clients to retrieve the current status of the task. In our example,
    we will be using Redis as the backend to store task results for long-running applications,
    and also as the message queue by which tasks can communicate, a role known as
    a "broker".
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 除了存储具有灵活模式的数据，如我们可能在REST API调用中遇到的嵌套JSON字符串外，键值存储还可以通过允许我们持久化任务的状태在Web应用程序中发挥另一个功能。对于快速响应的请求，如获取信息的GET类，这并不是必需的。然而，预测服务可能经常有长时间运行的任务，这些任务是通过POST请求启动的，需要时间来计算响应。即使任务尚未完成，我们也希望立即向启动任务的客户端返回一个响应。否则，客户端将等待服务器完成而停滞，这可能会影响客户端的性能，并且与之前描述的系统组件解耦的哲学非常不符。相反，我们希望立即向客户端返回一个任务标识符，这将允许客户端轮询服务以检查任务的进度，并在结果可用时检索它。我们可以使用键值数据库存储任务的状태，并提供更新方法，允许我们通过编辑任务记录提供中间进度信息，以及GET方法，允许客户端检索任务的当前状态。在我们的示例中，我们将使用Redis作为后端存储长时间运行应用程序的任务结果，并且作为任务可以通过其通信的消息队列，这个角色被称为“经纪人”。
- en: Now that we have covered the basic structure of our prediction service, let
    us examine a concrete example that ties together many of the patterns we have
    developed in predictive modeling tasks over the previous sections.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了预测服务的基本结构，让我们考察一个具体的例子，这个例子将结合我们在前几节中开发的许多预测建模任务的模式。
- en: Case study – logistic regression service
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究 – 逻辑回归服务
- en: As an illustration of the architecture covered previously, let us look at an
    example of a prediction service that implements a logistic regression model. The
    model is both trained and scores new data using information passed through URLs
    (either through the web browser or invoking curl on the command line), and illustrates
    how these components fit together. We will also examine how we can interactively
    test these components using the same IPython notebooks as before, while also allowing
    us to seamlessly deploying the resulting code in an independent application.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 作为之前提到的架构的示例，让我们来看一个实现逻辑回归模型的预测服务示例。该模型既用于训练数据，也用于使用通过URL传递的信息（无论是通过网页浏览器还是通过命令行调用curl）对新数据进行评分，并展示了这些组件是如何协同工作的。我们还将检查如何使用之前相同的IPython笔记本交互式测试这些组件，同时允许我们将生成的代码无缝部署到独立的应用程序中。
- en: Our first task is to set up the databases used to store the information used
    in modeling, as well as the result and model parameters.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的首要任务是设置用于存储建模中使用的信息的数据库，以及结果和模型参数。
- en: Setting up the database
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置数据库
- en: 'As a first step in our application, we will set up the database to store our
    training data and models, and scores obtained for new data. The examples for this
    exercise consist of data from a marketing campaign, where the objective was to
    convince customers to subscribe for a term deposit (Moro, Sérgio, Paulo Cortez,
    and Paulo Rita. "A data-driven approach to predict the success of bank telemarketing."Decision
    Support Systems 62 (2014): 22-31). Thus, the objective with this data is to predict
    based on a customer''s feature variables whether they are likely to pay for this
    service. The data is contained in the `bank-full.csv` file, which we need to load
    into MongoDB ([https://www.mongodb.org/](https://www.mongodb.org/)).'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '在我们的应用程序的第一步中，我们将设置数据库以存储我们的训练数据和模型，以及为新数据获得的分数。这个练习的示例包括来自营销活动的数据，其目标是说服客户订阅定期存款（Moro,
    Sérgio, Paulo Cortez, 和 Paulo Rita. "A data-driven approach to predict the success
    of bank telemarketing."Decision Support Systems 62 (2014): 22-31）。因此，使用这些数据的目的是根据客户的特征变量预测他们是否可能为此服务付费。数据包含在`bank-full.csv`文件中，我们需要将其加载到MongoDB中([https://www.mongodb.org/](https://www.mongodb.org/))。'
- en: 'After installing MongoDB for your system, you can test the database by running
    the following command in your terminal:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的系统上安装MongoDB后，您可以通过在终端中运行以下命令来测试数据库：
- en: '[PRE13]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The preceding command should start the database. Now, to import our training
    data, we can use the following command in a separate terminal window:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令应该启动数据库。现在，为了导入我们的训练数据，我们可以在另一个终端窗口中使用以下命令：
- en: '[PRE14]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This will allow us to import the data into a database called ''datasets'',
    in a collection called bank. We can test if the data has been successfully loaded
    by opening a mongo client in the terminal:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使我们能够将数据导入一个名为'datasets'的数据库中，在名为bank的集合中。我们可以通过在终端中打开一个mongo客户端来测试数据是否已成功加载：
- en: '[PRE15]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'If we run the following command, we should be able to see our dataset listed
    under the datasets database:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们运行以下命令，我们应该能够在datasets数据库下看到我们的数据集列表：
- en: '[PRE16]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We can verify that the data has been correctly parsed by examining one record:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过检查一条记录来验证数据是否被正确解析：
- en: '[PRE17]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Note
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The code here is inspired by examples in [https://github.com/jadianes/spark-movie-lens](https://github.com/jadianes/spark-movie-lens)
    and [http://fgimian.github.io/blog/2012/12/08/setting-up-a-rock-solid-python-development-web-server](http://fgimian.github.io/blog/2012/12/08/setting-up-a-rock-solid-python-development-web-server).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的代码灵感来源于[https://github.com/jadianes/spark-movie-lens](https://github.com/jadianes/spark-movie-lens)和[http://fgimian.github.io/blog/2012/12/08/setting-up-a-rock-solid-python-development-web-server](http://fgimian.github.io/blog/2012/12/08/setting-up-a-rock-solid-python-development-web-server)中的示例。
- en: 'You can see that record appears like a Python dictionary. To retrieve elements
    with particular values, we can use findOne with key:values set to the filters
    we want to apply:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到记录看起来像Python字典。为了检索具有特定值的元素，我们可以使用带有key:values设置为要应用的过滤器的findOne：
- en: '[PRE18]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now that we have the data loaded, we can interact with it through Python using
    the pymongo client. We initialize a client with access to the database we just
    created using the following:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经加载数据，我们可以通过使用pymongo客户端通过Python与之交互。我们使用以下方式初始化一个客户端，以访问我们刚刚创建的数据库：
- en: '[PRE19]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Note that the `mongod` command still needs to be running in a separate terminal
    window for you to access the database through Python. The customers object will
    then contain each customer's records. While for the current example we will primarily
    access MongoDB using the SparkConnector, the commands above will be useful in
    [Chapter 9](ch09.html "Chapter 9. Reporting and Testing – Iterating on Analytic
    Systems"), *Reporting and Testing – Iterating on Analytic Systems* when we analyze
    the output of our model. Indeed, the MongoDB database allows us to store information
    used by our model service, but also can be a source of shared information for
    the reporting service we will build in [Chapter 9](ch09.html "Chapter 9. Reporting
    and Testing – Iterating on Analytic Systems"), *Reporting and Testing – Iterating
    on Analytic Systems*, by visualizing the results of our modeling.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`mongod`命令仍然需要在单独的终端窗口中运行，以便您可以通过Python访问数据库。客户对象将包含每个客户的记录。而对于当前示例，我们将主要使用SparkConnector通过MongoDB进行分析，上述命令将在[第9章](ch09.html
    "第9章。报告和测试 – 在分析系统中迭代")，*报告和测试 – 在分析系统中迭代*时有用，当我们分析模型输出时。实际上，MongoDB数据库允许我们存储模型服务使用的信息，也可以成为我们将要在[第9章](ch09.html
    "第9章。报告和测试 – 在分析系统中迭代")，*报告和测试 – 在分析系统中迭代*中构建的报告服务共享信息的来源，通过可视化我们的建模结果。
- en: 'As we mentioned previously, we will also use the Redis ([http://redis.io/](http://redis.io/))
    key-value store to log the intermediate state of long-running tasks, and also
    to store the serialized output from training models in Spark. After installing
    Redis DB on your system, you should be able to start the server by typing the
    following command in the terminal:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们还将使用Redis ([http://redis.io/](http://redis.io/)) 键值存储来记录长时间运行任务的中间状态，以及存储在Spark中训练模型的序列化输出。在您的系统上安装Redis数据库后，您应该在终端中键入以下命令来启动服务器：
- en: '[PRE20]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Which, if successful, should give and output like the following:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果成功，应该会得到以下输出：
- en: '![Setting up the database](img/B04881_08_01.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![设置数据库](img/B04881_08_01.jpg)'
- en: 'The Python interface for Redis in the redis-py package (which, like many of
    the libraries we have seen in prior chapters, may be installed using `pip` or
    `easy_install`) is comparable to MongoDB. If we wanted to retrieve a record from
    our redis database, we could the following commands to start a client and issue
    a query or store data:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: redis-py包中的Redis Python接口（类似于我们在前几章中看到的许多库，可以使用`pip`或`easy_install`安装）与MongoDB相当。如果我们想从我们的redis数据库中检索记录，我们可以使用以下命令启动客户端并发出查询或存储数据：
- en: '[PRE21]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: When we start a new client using 'StrictRedis', we specify the port the redis-server
    is listening on (default of 6379) and the database identifier. By issuing get
    and set commands, we can respectively retrieve prior results or update the database
    with new information. As with the Python mongo client, we will need to have the
    redis-server command running in a separate command line window to allow us to
    issue commands to the database in Python.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用 'StrictRedis' 启动新客户端时，我们指定redis-server监听的端口（默认为6379）和数据库标识符。通过发出get和set命令，我们可以分别检索先前结果或更新数据库中的新信息。与Python
    mongo客户端一样，我们需要在单独的命令行窗口中运行redis-server命令，以便我们可以在Python中向数据库发出命令。
- en: Now that we have our databases set up, let us look at the server that will manage
    requests for the applications using this data.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置了数据库，让我们看看将管理使用这些数据的请求的服务器。
- en: The web server
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络服务器
- en: 'As described previously, the web server receives requests and forwards them
    to the web application. For our example, we start the server using the main function:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，网络服务器接收请求并将它们转发到网络应用程序。对于我们的示例，我们使用main函数启动服务器：
- en: '[PRE22]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'There are three steps: we read the parameters for this service (here, just
    the name of the algorithm used), which is passed as command line argument, create
    the web application (using the same parameter file passed in during creation in
    the constructor), and then start the server. As you can see, the algorithm run
    by the prediction service is specified using a string argument. Later we will
    examine how this allows us to write a generic prediction service class, rather
    than a specific web application for each new algorithm we might use. When we start
    the server; it is registered on localhost on port 5000, as you can see by examining
    the body of the `run_server` function:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 有三个步骤：我们读取此服务的参数（在这里，只是使用的算法名称），它作为命令行参数传递，创建网络应用程序（使用在构造函数中创建时传递的相同参数文件），然后启动服务器。如您所见，预测服务运行的算法使用字符串参数指定。稍后我们将检查这如何允许我们编写一个通用的预测服务类，而不是为每个可能使用的新算法编写特定的网络应用程序。当我们启动服务器时；它在localhost的5000端口上注册，如您通过检查`run_server`函数的主体所见：
- en: '[PRE23]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: There are a few key things happening in this function. Firstly, we see middleware
    in action since the TransLogger class from the paste library passes requests between
    the server and the application. The TransLogger object then represents a valid
    WGSI application since it has a callable (the application). We use the `tree.graft`
    command to attach the application (the model service itself) so that the object
    is called by the CherryPy modelserver whenever it receives an HTTP request.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个函数中发生了一些关键的事情。首先，我们看到中间件的作用，因为来自paste库的TransLogger类在服务器和应用程序之间传递请求。然后，TransLogger对象代表一个有效的WGSI应用程序，因为它有一个可调用的对象（即应用程序）。我们使用`tree.graft`命令附加应用程序（即模型服务本身），这样当CherryPy模型服务器接收到HTTP请求时，就会调用该对象。
- en: When we start the cherrypy server, we provide a few configurations. The enable.autoreload.on
    parameter controls whether the application will refresh when we change the source
    files it is pointing to, in this case our Flask application. Log.screen directs
    the output of error and access message to the stdout, which is useful when we
    are still debugging. Finally, the last two settings specify the URL and endpoint
    where we will send requests to the application.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们启动cherrypy服务器时，我们会提供一些配置。`enable.autoreload.on`参数控制当更改应用程序指向的源文件时，应用程序是否会刷新，在这种情况下是我们的Flask应用程序。`Log.screen`将错误和访问消息的输出定向到stdout，这在调试时很有用。最后，最后两个设置指定了我们将向应用程序发送请求的URL和端点。
- en: 'Once we start the application, we also set it to block, which means it must
    finish processing one request before considering another. If we want to tune performance,
    we could remove this configuration, which would allow the application to receive
    multiple requests without waiting for the first to finish. The URL for this server
    is thus accessed by `http://0.0.0.0:5000` once it is running—this is the address
    where we will send our various commands to the prediction service. To start the
    server, type the following in the command line:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们启动了应用程序，我们也会将其设置为阻塞模式，这意味着它必须完成处理一个请求之后才会考虑另一个请求。如果我们想调整性能，我们可以移除这个配置，这样应用程序就可以在等待第一个请求完成之前接收多个请求。因此，一旦服务器启动，可以通过`http://0.0.0.0:5000`访问这个服务器的URL——这是我们向预测服务发送各种命令的地址。要启动服务器，请在命令行中输入以下内容：
- en: '[PRE24]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The `parameters.json` file could contain parameters for the `modelservice`
    application that will be used when starting the modeling application, but for
    now we actually place nothing in this file. If successful, you should see the
    following output in the terminal:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '`parameters.json`文件可能包含在启动建模应用程序时将使用的`modelservice`应用程序的参数，但到目前为止，我们实际上在这个文件中放置了空的内容。如果成功，你应该在终端中看到以下输出：'
- en: '![The web server](img/B04881_08_02.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![网络服务器](img/B04881_08_02.jpg)'
- en: As we issue `curl` commands to the server, we will see the responses displayed
    in this output as well.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们向服务器发出`curl`命令时，我们将在输出中看到相应的响应。
- en: The web application
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络应用程序
- en: Now that we have started the server and can begin receiving commands from the
    client, let us look at the commands that will be executed by our application,
    such as HTTP requests issued through the Python notebook or curl commands. The
    code that is executed when we send requests to the `CherryPy` server is contained
    in the `modelservice.py` file.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经启动了服务器，并且可以开始接收来自客户端的命令，让我们看看我们的应用程序将要执行的命令，例如通过Python笔记本或curl命令发出的HTTP请求。当我们向`CherryPy`服务器发送请求时执行的代码包含在`modelservice.py`文件中。
- en: 'The constructor for the application, called by the `CherryPy` server when we
    started it, returns an app object specified using the Flask framework:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们启动应用程序时，`CherryPy`服务器会调用构造函数，该构造函数返回一个使用Flask框架指定的app对象：
- en: '[PRE25]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In addition to creating the Flask object app, we also generate a celery object.
    What is this celery object? As mentioned previously, we do not want to have our
    clients wait on long-running tasks to respond, as this would cause the client
    applications to potentially hang or timeout. Thus, our application needs to be
    non-blocking and return an immediate value for a long-running task, which is an
    ID that allows us to access the progress and results of the task through a REST
    API. We want to run the long-running task in a secondary process and have it report
    back the results or intermediate state as they become available. For our application,
    we will be using the `Celery` library ([http://www.celeryproject.org/](http://www.celeryproject.org/)),
    an asynchronous task queuing system that is ideal for this sort of application.
    Celery consists of a client that submits jobs to a queue, and worker tasks, which
    read from this queue, perform work, and return the results to the client. The
    client and workers communicate via a messaging queue, such as the Redis key-value
    store we mentioned previously, and results are also persisted to this database.
    The arguments `CELERY_BROKER_URL` and `CELERY_RESULT_BACKEND` are used to specify,
    respectively, where the worker tasks retrieve information on scheduled tasks,
    and where we can look up information on the status of currently running tasks.
    In our example, both functions are served by Redis, but we could substitute other
    systems, such as the message queue system RabbitMQ ([https://www.rabbitmq.com/](https://www.rabbitmq.com/)).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 除了创建 Flask 对象 app，我们还生成一个 celery 对象。这个 celery 对象是什么？如前所述，我们不希望我们的客户端在等待长时间运行的任务响应，因为这可能会导致客户端应用程序挂起或超时。因此，我们的应用程序需要是非阻塞的，并且对于长时间运行的任务，立即返回一个
    ID，这个 ID 允许我们通过 REST API 访问任务的进度和结果。我们希望在辅助进程中运行长时间运行的任务，并在结果或中间状态可用时报告它们。对于我们的应用程序，我们将使用
    `Celery` 库（[http://www.celeryproject.org/](http://www.celeryproject.org/)），这是一个异步任务队列系统，非常适合此类应用程序。Celery
    由提交作业到队列的客户机和读取此队列、执行工作并将结果返回给客户的工人任务组成。客户端和工人通过消息队列进行通信，例如我们之前提到的 Redis 键值存储，结果也持久化到这个数据库中。`CELERY_BROKER_URL`
    和 `CELERY_RESULT_BACKEND` 参数分别用于指定工人任务检索计划任务信息的位置，以及我们可以查找当前运行任务状态信息的位置。在我们的示例中，这两个功能都由
    Redis 提供，但我们可以用其他系统替换，例如消息队列系统 RabbitMQ（[https://www.rabbitmq.com/](https://www.rabbitmq.com/)）。
- en: 'In order for us to issue HTTP requests to the Celery worker tasks, we need
    to make sure that redis is already running, and then start the Celery workers
    using the following command:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让我们向 Celery 工人任务发出 HTTP 请求，我们需要确保 redis 已经运行，然后使用以下命令启动 Celery 工人：
- en: '[PRE26]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This starts celery worker processes with access to the commands specified in
    `modelservice.py` which we will cover below. If successful, you will see the following
    in your terminal.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这将启动具有对 `modelservice.py` 中指定命令的访问权限的芹菜工作进程，我们将在下面进行介绍。如果成功，你将在你的终端中看到以下内容。
- en: '![The web application](img/B04881_08_03.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![网络应用程序](img/B04881_08_03.jpg)'
- en: As we later send requests to the service which are passed off to the Celery
    workers, information (such as Spark outputs) will be printed in this window as
    well.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们稍后向服务发送请求，这些请求被传递给 Celery 工人时，信息（如 Spark 输出）也将在此窗口中打印。
- en: The flow of a prediction service – training a model
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预测服务的流程 - 训练模型
- en: So now that we have the Celery process running along with the Flask application,
    how can we define the functions executed by the workers in response to our HTTP
    requests? How can we specify the URLs to which we will issue curl commands? We
    will illustrate the flow of events by showing how a call to the training function
    will kick off a series of Spark jobs to perform cross validation and store a LogisticRegression
    model.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在我们已经启动了 Celery 进程以及 Flask 应用程序，我们如何定义工人执行的函数以响应我们的 HTTP 请求？我们如何指定我们将发出
    curl 命令的 URL？我们将通过展示对训练函数的调用如何启动一系列 Spark 作业以执行交叉验证并存储逻辑回归模型来展示事件流程。
- en: 'We start by issuing a curl command to the `train` function with the following
    command:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先向 `train` 函数发出 curl 命令，如下所示：
- en: '[PRE27]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We could have similarly used the Python requests library to transmit the information
    in `job.json` to the model training task. The `job.json` file contains all the
    parameters we might need to use in the various stages of parsing the data and
    training the model, as we will see as we walk through the flow of this request
    through our application. When this command is received by the CherryPy modelserver,
    it is forwarded to the Flask app defined in `modelservice.py`. How can we make
    the Flask application respond to this request? It is as easy as providing a decorator
    specifying a function to run in response to requests to this URL:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们同样可以使用 Python 的 requests 库将 `job.json` 中的信息传输给模型训练任务。`job.json` 文件包含了我们在解析数据和训练模型各个阶段可能需要使用的所有参数，正如我们在遍历这个请求流程时将会看到的。当这个命令被
    CherryPy 模型服务器接收时，它会被转发到在 `modelservice.py` 中定义的 Flask 应用。我们如何让 Flask 应用响应这个请求呢？这就像提供一个装饰器，指定一个在接收到这个
    URL 的请求时运行的函数一样简单：
- en: '[PRE28]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The `@app.route` decorator indicates that the Flask object app listens for POST
    commands to a URL given as an argument to route. In responses, it extracts the
    dictionary of parameters from the POST request and passes them to a `train_task`,
    which will be run on a Celery worker process through the `apply_async` function.
    We then immediately return a task identifier associated with this task, which
    we can use to check the status or, as we will see, identify the output of the
    resulting model.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '`@app.route` 装饰器表示 Flask 对象 app 监听一个作为 route 参数提供的 URL 的 POST 命令。在响应中，它从 POST
    请求中提取参数字典，并将其传递给 `train_task`，该任务将通过 `apply_async` 函数在 Celery 工作进程上运行。然后我们立即返回与这个任务关联的任务标识符，我们可以使用它来检查状态，或者，正如我们将看到的，识别结果的模型输出。'
- en: 'How do we specify the Celery task `train_task`? Similarly, we provide a decorator
    indicating that this function will be run on a worker process:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何指定 Celery 任务 `train_task`？同样，我们提供一个装饰器，表示这个函数将在工作进程中运行：
- en: '[PRE29]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'There are a few important details here. First, along with annotating the function
    with `@celery.task`, we provide the argument `bind=True`. This ensures that the
    function has a ''self'' argument. Why would we need a self argument? In our example,
    we attach a `MessangeHandler` object to the training task using a reference to
    the function (self), allowing us to inject updates on the status of the task as
    it proceeds, and also retrieve the identifier for the task which was returned
    after we issued the POST request. The `MessageHandler` class is relatively simple
    and defined as follows in the `messagehandler.py` file in the code examples for
    this chapter:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有几个重要的细节。首先，除了使用 `@celery.task` 装饰函数外，我们还提供了 `bind=True` 参数。这确保了函数有一个 `self`
    参数。我们为什么需要一个 `self` 参数呢？在我们的例子中，我们使用函数的引用（self）将 `MessageHandler` 对象附加到训练任务上，这样我们就可以在任务进行过程中注入状态更新，并检索在发出
    POST 请求后返回的任务标识符。`MessageHandler` 类相对简单，如下所示，定义在代码示例这一章的 `messagehandler.py` 文件中：
- en: '[PRE30]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: When we construct the `MessageHandler` object, we retrieve the ID associated
    with the tasks from the `request.id` field. If we had not used the `bind=True`
    argument above, we would not be able to access this field, since we would not
    have a reference (self) to the task object to pass to the `MessageHandler`. This
    is also needed for the `update` function, which allows us to inject status updates
    about the progress of the task using the reference to the train task above. Finally,
    if we need to access the training task identifier anywhere else in our application,
    we can do so using `get_id`.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们构建 `MessageHandler` 对象时，我们从 `request.id` 字段检索与任务关联的 ID。如果我们没有在上面的 `bind=True`
    参数中使用，我们就无法访问这个字段，因为我们没有任务对象的引用（self）来传递给 `MessageHandler`。这也需要 `update` 函数，它允许我们使用上面的训练任务引用注入任务进度的状态更新。最后，如果我们需要在应用程序的其他地方访问训练任务标识符，我们可以使用
    `get_id` 来实现。
- en: 'How could we access the tasks status modified by update? If you recall, when
    we initialized the Celery application, we provided the Redis database as a storage
    location for task status information. Using the identifier returned in response
    to our POST request, we could use a GET method to look up the status of this task,
    which we specify through another Flask app endpoint:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何访问更新后修改的任务状态？如果你还记得，当我们初始化 Celery 应用时，我们提供了 Redis 数据库作为任务状态信息的存储位置。使用我们
    POST 请求返回的标识符，我们可以使用 GET 方法来查找这个任务的状态，我们通过另一个 Flask 应用端点来指定这个状态：
- en: '[PRE31]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Thus, using a `curl` command, we could issue a GET to obtain the status of our
    training task, either printing it to the console or, if we made this application
    more complex, using it to generate a dashboard of job states in a pipeline or
    system.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用`curl`命令，我们可以发出一个GET请求来获取我们训练任务的状况，要么将其打印到控制台，要么如果我们使这个应用程序更复杂，可以使用它来生成一个工作状态仪表板，用于管道或系统。
- en: 'Now that we have a way to inject updates about the status of our tasks, let
    us return to the `train_task` definition. In addition to creating the `MessageHandler`
    for this task, we also generate a `SparkConfiguration` and initialize a model
    object. The SparkConfiguration will probably look familiar from some of the examples
    in previous chapters, and is returned from the following function:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经有了注入关于任务状态更新的方法，让我们回到`train_task`定义。除了为这个任务创建`MessageHandler`之外，我们还生成一个`SparkConfiguration`并初始化一个模型对象。`SparkConfiguration`可能看起来与之前章节中的一些示例相似，并且由以下函数返回：
- en: '[PRE32]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Note
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'Note that the arguments to the `SparkConfiguration` are used by the Spark mongo
    connector. This connector is an external dependency that needs to be downloaded
    and added at runtime to the system path of our Spark application, which can be
    accomplished by adding the following to your system parameters (assuming a Linux
    command line environment):'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`SparkConfiguration`的参数由Spark mongo连接器使用。这个连接器是一个外部依赖项，需要在运行时下载并添加到我们的Spark应用程序的系统路径中，这可以通过向您的系统参数添加以下内容来完成（假设Linux命令行环境）：
- en: '[PRE33]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Here we set the application name by which we will identify the train task in
    the Spark UI on port 4040, and allow multiple contexts through `"spark.driver.allowMultipleContexts"`
    such that several Spark applications could be potentially run in parallel. Finally,
    we provide the `mongodb` input and output locations where Spark will read the
    data for training and store scored results. Note that these are both given as
    defaults, but could be changed by modifying parameters in the `job.json` file,
    allowing our application to operate on different inputs and store to different
    output locations by only changing the arguments to the POST request.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们设置了应用程序名称，我们将通过该名称在Spark UI的4040端口上识别训练任务，并允许通过`"spark.driver.allowMultipleContexts"`使用多个上下文，这样几个Spark应用程序可以并行运行。最后，我们提供了`mongodb`输入和输出位置，Spark将从中读取训练数据并将评分结果存储在这些位置。请注意，这些默认值可以更改，只需修改`job.json`文件中的参数即可，这样我们的应用程序可以通过仅更改POST请求的参数来在不同的输入上运行并将数据存储到不同的输出位置。
- en: 'Now that we have the configuration to pass to the Spark job, let us look at
    the model object which will receive these parameters. We construct it as a global
    object at the beginning of the `modelservice` file in the line:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了传递给Spark作业的配置，让我们看看将接收这些参数的模型对象。我们在`modelservice`文件的开始处构建它，如下所示：
- en: '[PRE34]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'If you examine the definition of the `ModelFactory` class in the `modelfactory.py`
    file supplied with the code example for this chapter, you see can see that it
    provides a generic interface for wrapping the training and prediction functions
    of different machine learning algorithms:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您检查随代码示例提供的`modelfactory.py`文件中`ModelFactory`类的定义，您会看到它为不同机器学习算法的训练和预测函数提供了一个通用接口：
- en: '[PRE35]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: As you can see, nowhere in this class do we specify the particular implementation
    of train or prediction tasks. Rather, we create an object with an internal member
    (`self_model`) that we can set using `set_model`, by dynamically retrieving code
    associated with a particular algorithm using `importlib`. The `"name"` argument
    also comes from `job.json`, meaning we could load different algorithms in our
    application and run training tasks simply by changing the parameters of our POST
    request. In this example, we specify the model as `LogisticRegressionWrapper`,
    which will cause this model (and the class of the same name) to be loaded and
    inserted into the `self_model` of the `ModelFactory` when we call `train_task`.
    ModelFactory also has a generic method for loading an existing model, `get_model`,
    which takes as input a task ID such as the one generated in response to our train
    request and sets `self_model` to be a previously trained model object which is
    retrieved using this task ID as a reference. In addition, this class has methods
    for predict (to give the predicted response for a single row of data) or `predict_all`
    (to perform bulk scoring using Spark).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，在这个类中，我们并没有指定训练或预测任务的特定实现。相反，我们创建了一个具有内部成员（`self_model`）的对象，我们可以通过 `set_model`
    使用它来设置，通过使用 `importlib` 动态检索与特定算法相关的代码。`"name"` 参数也来自 `job.json`，这意味着我们可以在应用程序中加载不同的算法并运行训练任务，只需更改我们的
    POST 请求的参数即可。在这个例子中，我们将模型指定为 `LogisticRegressionWrapper`，这将导致在调用 `train_task`
    时，此模型（以及同名的类）被加载并插入到 `ModelFactory` 的 `self_model` 中。ModelFactory 还有一个用于加载现有模型的通用方法
    `get_model`，它接受一个任务 ID 作为输入，例如响应我们的训练请求生成的任务 ID，并将 `self_model` 设置为使用此任务 ID 作为参考检索到的先前训练的模型对象。此外，这个类还有用于预测（为单行数据提供预测响应）或
    `predict_all`（使用 Spark 执行批量评分）的方法。
- en: To recap, now we see that in response to our POST request, the CherryPy server
    hands off the information in `data.json` to the `train` function of our Flask
    service, which starts a background process on a Celery worker. This worker process
    sets the generic model object of our Flask app to a Logistic Regression, creates
    a Spark configuration to run the training task, and returns a task ID that we
    can use to monitor the progress of the model training. In the final step in the
    journey of this POST request, let us see how the Logistic Regression model implements
    the training task.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下，现在我们看到，在响应我们的 POST 请求时，CherryPy 服务器将 `data.json` 中的信息传递给我们的 Flask 服务中的
    `train` 函数，该函数在 Celery 工作器上启动一个后台进程。这个工作进程将我们的 Flask 应用程序的通用模型对象设置为逻辑回归，创建一个 Spark
    配置来运行训练任务，并返回一个任务 ID，我们可以用它来监控模型训练的进度。在这次 POST 请求的最终步骤中，让我们看看逻辑回归模型是如何实现训练任务的。
- en: 'In the `LogisticRegressionWrapper.py` file, you can see the specifications
    of the train task:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `LogisticRegressionWrapper.py` 文件中，你可以看到训练任务的规格：
- en: '[PRE36]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'First of all, we start a SparkContext using the parameters we defined in the
    SparkConfiguration we passed to this function. The parameters in our `job.json`
    file also include the algorithm parameters, which we parse. We then read the input
    data which we specified in the SparkConfiguration in a distributed fashion from
    mongodb into a Spark DataFrame, using a lambda function to parse the input. The
    parsing logic is defined in `dataparser.py`, in the `parse_line` function of the
    `DataParser` class:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用传递给此函数的 SparkConfiguration 中定义的参数启动 SparkContext。我们的 `job.json` 文件中的参数还包括算法参数，我们解析这些参数。然后，我们以分布式方式从
    mongodb 读取我们在 SparkConfiguration 中指定的输入数据，将其读取到 Spark DataFrame 中，使用 lambda 函数解析输入。解析逻辑在
    `dataparser.py` 文件中的 `DataParser` 类的 `parse_line` 函数中定义：
- en: '[PRE37]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The `DataParser` class takes as input a parameters dictionary containing the
    schema of the data that—once again—we specified in our `job.json` data we included
    in our POST request. This information is stored in the `self._schema` property
    of the parser. Using this information, the parse_line function extracts the label
    (the response column) and encodes it as a numeric value if necessary. Similarly,
    the features of each record are parsed and, if necessary, one-hot encoded using
    information in the POST request. If the data is to be used in training (`train=True`),
    the parser returns the label and a vector of features. Otherwise, it just returns
    the features to be used in scoring new records. In either case, the features are
    encoded as a dense Vector from the Spark ml library (which is required for the
    logistic regression algorithm), and the row is returned as a Row object to be
    compatible with the Spark DataFrame needed for the training code. Because the
    fields we use as features are specified in our `job.json` data, we could train
    models using different columns from the same dataset without changing the underlying
    code.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '`DataParser`类接受一个包含数据模式的参数字典作为输入，该模式——再次强调——我们在我们的`job.json`数据中指定了，这是我们包含在POST请求中的。这些信息存储在解析器的`self._schema`属性中。使用这些信息，parse_line函数提取标签（响应列）并在必要时将其编码为数值。同样，解析每个记录的特征，并在必要时使用POST请求中的信息进行独热编码。如果数据要用于训练（`train=True`），解析器返回标签和特征向量。否则，它只返回用于评分新记录的特征。在任何情况下，特征都编码为来自Spark
    ml库的密集向量（这对于逻辑回归算法是必需的），并将行作为Row对象返回，以与用于训练代码的Spark DataFrame兼容。因为我们在`job.json`数据中指定的字段用作特征，所以我们可以使用相同的数据集的不同列来训练模型，而无需更改底层代码。'
- en: 'Once the data is parsed, we construct a Spark Pipeline object to handle the
    stages of the model training. In our example, the only step is the model training
    itself, but we could potentially have transformations like the Vectorizers we
    examined in [Chapter 6](ch06.html "Chapter 6. Words and Pixels – Working with
    Unstructured Data"), *Words and Pixels – Working with Unstructured Data* in the
    context of text data as part of such as pipeline. We then create a ParamGrid to
    perform a grid search of the regularization parameter of our model, and pass it
    to a CrossValidator, which will peform n-fold validation to determine the best
    model. Once we have fit this model, we retrieve the optimal model from the CrossValidator
    results and determine the number of features and classes used in the model. Finally,
    we open a connection to the Redis database and store the parameters of this model
    after serializing it with the function:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据被解析，我们构建一个Spark Pipeline对象来处理模型训练的阶段。在我们的示例中，唯一的步骤就是模型训练本身，但我们可能具有像我们在[第6章](ch06.html
    "第6章。文字和像素 - 处理非结构化数据")中检查的Vectorizers这样的转换，作为文本数据处理的管道的一部分。然后我们创建一个ParamGrid来执行模型正则化参数的网格搜索，并将其传递给CrossValidator，它将执行n折验证以确定最佳模型。一旦我们拟合了这个模型，我们就从CrossValidator的结果中检索最佳模型，并确定模型中使用的特征和类的数量。最后，我们通过使用函数序列化该模型后，打开与Redis数据库的连接并存储其参数：
- en: '[PRE38]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Notice that we use the MessageHandler attached to this task to retrieve the
    task ID, which is used as the key to store the serialized model in Redis. Also,
    though we store the result in the same Redis instance listening on port 6379 that
    is used by Celery to queue tasks and update the status of background tasks, we
    save to db 1 instead of the default 0 to separate the information.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们使用附加到该任务的MessageHandler来检索任务ID，该ID用作在Redis中存储序列化模型的键。此外，尽管我们将结果存储在由Celery用于队列任务和更新后台任务状态的同一Redis实例（监听端口6379）中，但我们将其保存到db
    1而不是默认的0，以分离信息。
- en: By tracing through the steps above, you should now be able to see how a POST
    request can be translated into a series of commands that parse data, perform cross-validated
    grid-search to train a model, and then serialize that model for later use. You
    should also appreciate how the parameterizations at each layer allow us to modify
    the behavior of this training task purely by modifying the contents of the POST
    request, and how the modularity of the application will make it easy to extend
    to other models. We also have utilized Spark, which will allow us to easily scale
    our calculations to larger datasets over time.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 通过追踪上述步骤，你现在应该能够看到如何将 POST 请求转换成一系列命令，这些命令解析数据，执行交叉验证网格搜索来训练模型，然后将该模型序列化以供以后使用。你也应该欣赏到每一层的参数化如何使我们能够仅通过修改
    POST 请求的内容来修改训练任务的行为，以及应用程序的模块化如何使其易于扩展到其他模型。我们还使用了 Spark，这将允许我们随着时间的推移轻松地将我们的计算扩展到更大的数据集。
- en: Now that we have illustrated the logical flow of data in our prediction service,
    let us finish by examining the prediction functions, whose output we will use
    in [Chapter 9](ch09.html "Chapter 9. Reporting and Testing – Iterating on Analytic
    Systems"), *Reporting and Testing – Iterating on Analytic Systems*.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经说明了我们预测服务中的数据逻辑流程，让我们通过检查预测函数来结束，这些函数的输出将用于[第9章](ch09.html "第9章。报告和测试
    – 在分析系统中迭代")，*报告和测试 – 在分析系统中迭代*。
- en: On-demand and bulk prediction
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 按需和批量预测
- en: 'Now that we have a trained model saved in our system, how can we utilize it
    to score new data? Our Flask app has two endpoints for this service. In the first,
    we make a POST request giving a row of data as a json, along with a model ID,
    and ask for a score from the logistic regression model:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经在系统中保存了一个训练好的模型，我们如何利用它来评分新数据？我们的 Flask 应用程序为此服务提供了两个端点。在第一个端点，我们发送一个
    POST 请求，其中包含一行数据作为 json，以及一个模型 ID，并请求从逻辑回归模型中获取一个评分：
- en: '[PRE39]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: This time, instead of calling the `set_model` method of ModelFactory, we use
    `get_model` to load a previously trained model, then use it to predict the label
    of the input record and return the value. In the case of Logistic Regression,
    this will be a 0 or 1 value. While we do not provide a user interface in this
    example, we could imagine a simple form in which the user specifies a number of
    features of a record and submits them through a POST request, receiving back a
    prediction in realtime.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们不是调用 `ModelFactory` 的 `set_model` 方法，而是使用 `get_model` 加载一个先前训练好的模型，然后使用它来预测输入记录的标签并返回值。在逻辑回归的情况下，这将是一个
    0 或 1 的值。虽然在这个例子中我们没有提供用户界面，但我们可以想象一个简单的表单，用户在其中指定记录的几个特征，并通过 POST 请求提交它们，实时收到预测结果。
- en: 'Looking at the implementation of `get_model` in LogisticRegressionWrapper,
    we see that we can retrieve and de-serialize the model we generated in the train
    task, and assign it to the `self._model` member of ModelFactory:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在观察 `LogisticRegressionWrapper` 中 `get_model` 的实现时，我们发现可以检索并反序列化我们在训练任务中生成的模型，并将其分配给
    `ModelFactory` 的 `self._model` 成员：
- en: '[PRE40]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Subsequently, when we score a new record, we call the `predict` function to
    parse this record and use the de-serialized model to generate a prediction:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，当我们评分一个新记录时，我们调用 `predict` 函数来解析这个记录，并使用反序列化的模型生成一个预测：
- en: '[PRE41]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'This sort of functionality will be useful for interactive applications, such
    as a human user submitting a few records of interest to obtain predictions, or
    for real time applications in which we might receive streaming input and provide
    predictions for immediate use. Note that thought we do not use Spark in this particular
    instance, we still have a nice opportunity for horizontal scaling. Once we have
    trained the model, we could de-serialize the resulting parameters in several copies
    of the modelservice, which will allow use to potentially avoid timeouts if we
    receive many requests. However, in cases where the volume of predictions required
    is large and the necessary latency is *not* realtime, it may be more effective
    to utilize Spark to perform bulk-scoring of records in our database. We implement
    this bulk-scoring capability using a Celery task in a manner similar to the `train_task`,
    specifying a `predictall` endpoint in the Flask app:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这种功能对于交互式应用程序非常有用，例如，人类用户提交一些感兴趣的记录以获取预测，或者对于实时应用程序，我们可能会接收流输入并提供即时使用的预测。请注意，尽管在这个特定实例中我们没有使用Spark，但我们仍然有很好的横向扩展机会。一旦我们训练了模型，我们就可以在多个modelservice副本中反序列化结果参数，这样我们就可以在收到许多请求时避免超时。然而，在需要大量预测且必要的延迟不是实时的案例中，利用Spark来执行数据库中记录的批量评分可能更有效。我们通过在Flask应用程序中指定`predictall`端点的方式，使用Celery任务实现这种批量评分功能，类似于`train_task`：
- en: '[PRE42]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The associated Celery task is show below:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 相关的Celery任务如下所示：
- en: '[PRE43]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Again, we create a SparkConfiguration and MessageHandler, and like the predict
    method, we use a prior model ID specified in `job.json` to load a previous train
    model. We then call the `predict_all` method of this model to start a bulk scoring
    routine that will generate predictions for a large collection of data, and store
    the resulting in the `mongodb` collection specified by the output location parameter
    of the SparkConfiguration. For the `LogisticRegressionWrapper`, the `predict_all`
    method is shown below:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们创建一个SparkConfiguration和一个MessageHandler，就像predict方法一样，我们使用`job.json`中指定的先前模型ID来加载一个之前的训练模型。然后我们调用该模型的`predict_all`方法来启动一个批量评分流程，该流程将为大量数据生成预测，并将结果存储在SparkConfiguration输出位置参数指定的`mongodb`集合中。对于`LogisticRegressionWrapper`，`predict_all`方法如下所示：
- en: '[PRE44]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'As with the training task, we start a SparkContext using the SparkConfiguration
    we defined in the Celery task, and load the input from mongodb using the Spark
    connector. Instead of simply parsing the data, we score the parsed records using
    the de-serialized model we loaded using the `get_model` command, and pass both
    it and the original record into a new Row object, which now has two columns: the
    score and the input. We then save this data back to mongodb.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 与训练任务一样，我们使用在Celery任务中定义的SparkConfiguration启动一个SparkContext，并使用Spark连接器从mongodb加载数据。我们不仅解析数据，还使用`get_model`命令加载的反序列化模型对解析的记录进行评分，并将这两个以及原始记录都传递给一个新的Row对象，该对象现在有两个列：评分和输入。然后我们将这些数据保存回mongodb。
- en: If you open the mongo client and examine the `bankResults` collection, you can
    verify that it now contains the bulk-scored input data. We will utilize these
    results in [Chapter 9](ch09.html "Chapter 9. Reporting and Testing – Iterating
    on Analytic Systems"), *Reporting and Testing – Iterating on Analytic Systems*
    where we will expose these scores in a reporting application to visualize the
    ongoing performance of our model and diagnose potential issues in model performance.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你打开mongo客户端并检查`bankResults`集合，你可以验证它现在包含批量评分的输入数据。我们将在[第9章](ch09.html "第9章。报告和测试
    – 在分析系统中迭代") *报告和测试 – 在分析系统中迭代*中使用这些结果，我们将这些评分暴露在报告应用程序中以可视化我们模型的持续性能并诊断模型性能中的潜在问题。
- en: Summary
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we described the three components of a basic prediction service:
    a client, the server, and the web application. We discussed how this design allows
    us to share the results of predictive modelling with other users or software systems,
    and scale our modeling horizontally and modularly to meet the demands of various
    use cases. Our code examples illustrate how to create a prediction service with
    generic model and data parsing functions that can be reused as we try different
    algorithms for a particular business use case. By utilizing background tasks through
    Celery worker threads and distributed training and scoring on Spark, we showed
    how to potentially scale this application to large datasets while providing intermediate
    feedback to the client on task status. We also showed how an on-demand prediction
    utility could be used to generate real-time scores for streams of data through
    a REST API.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们描述了基本预测服务的三个组成部分：客户端、服务器和Web应用。我们讨论了这种设计如何使我们能够与其他用户或软件系统共享预测建模的结果，以及如何将我们的建模水平化和模块化以适应各种用例的需求。我们的代码示例说明了如何创建一个具有通用模型和数据解析功能的预测服务，这些功能可以在我们尝试特定业务用例的不同算法时重复使用。通过利用Celery工作线程中的后台任务以及在Spark上进行分布式训练和评分，我们展示了如何有可能将此应用程序扩展到大型数据集，同时向客户端提供任务状态的中间反馈。我们还展示了如何使用按需预测工具通过REST
    API生成数据流的实时评分。
- en: Using this prediction service framework, in the next chapter we will extend
    this application to provide ongoing monitoring and reporting about the performance
    and health of our predictive models.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个预测服务框架，在下一章中，我们将扩展这个应用以提供对我们预测模型性能和健康状况的持续监控和报告。
