<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer323">
<h1 class="chapter-number" id="_idParaDest-395"><a id="_idTextAnchor402"/>11</h1>
<h1 id="_idParaDest-396"><a id="_idTextAnchor403"/>Automating Your Data Ingestion Pipelines</h1>
<p>Data sources are frequently updated, and this requires us to update our data lake. However, with multiple sources or projects, it becomes impossible to trigger data pipelines manually. Data pipeline automation makes ingesting and processing data mechanical, obviating the human actions to trigger it. The importance of automation configuration lies in the ability to streamline data flow and improve data quality, reducing errors <span class="No-Break">and inconsistency.</span></p>
<p>In this chapter, we will cover how to automate the data ingestion pipelines in Airflow, along with two essential topics in data engineering, data replication and historical data ingestion, as well as <span class="No-Break">best practices.</span></p>
<p>In this chapter, we will cover the <span class="No-Break">following recipes:</span></p>
<ul>
<li>Scheduling <span class="No-Break">daily ingestions</span></li>
<li>Scheduling historical <span class="No-Break">data ingestion</span></li>
<li>Scheduling <span class="No-Break">data replication</span></li>
<li>Setting up the <span class="No-Break"><strong class="source-inline">schedule_interval</strong></span><span class="No-Break"> parameter</span></li>
<li>Solving <span class="No-Break">scheduling errors</span></li>
</ul>
<h1 id="_idParaDest-397"><a id="_idTextAnchor404"/>Technical requirements</h1>
<p>You can find the code from this chapter in the GitHub repository <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_11"><span class="No-Break">https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_11</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-398"><a id="_idTextAnchor405"/>Installing and running Airflow</h1>
<p>This chapter requires that<a id="_idIndexMarker768"/> Airflow is installed on your local machine. You can install it directly on your <strong class="bold">Operating System</strong> (<strong class="bold">OS</strong>) or <a id="_idIndexMarker769"/>use a Docker image. For more information, refer to the <em class="italic">Configuring Docker for Airflow </em>recipe in <a href="B19453_01.xhtml#_idTextAnchor022"><span class="No-Break"><em class="italic">Chapter 1</em></span></a><span class="No-Break">.</span></p>
<p>After<a id="_idIndexMarker770"/> following the steps described in <a href="B19453_01.xhtml#_idTextAnchor022"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>, ensure your Airflow instance runs correctly. You can do that by checking the Airflow UI <span class="No-Break">at </span><span class="No-Break"><strong class="source-inline">http://localhost:8080</strong></span><span class="No-Break">.</span></p>
<p>If you are using a Docker container (as I am) to host your Airflow application, you can check its status in the terminal with the <span class="No-Break">following command:</span></p>
<pre class="source-code">
$ docker ps</pre>
<p>Here is the <a id="_idIndexMarker771"/>status of <span class="No-Break">the container:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer302">
<img alt="Figure 11.1 –  Airflow containers running" height="131" src="image/Figure_11.01_B19453.jpg" width="1427"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.1 – Airflow containers running</p>
<p>Or you can check the container <a id="_idIndexMarker772"/>status on <span class="No-Break"><strong class="bold">Docker Desktop</strong></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer303">
<img alt="Figure 11.2 – Docker Desktop showing Airflow running containers" height="455" src="image/Figure_11.02_B19453.jpg" width="1111"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.2 – Docker Desktop showing Airflow running containers</p>
<h1 id="_idParaDest-399"><a id="_idTextAnchor406"/>Scheduling daily ingestions</h1>
<p>Data constantly changes in our <a id="_idIndexMarker773"/>dynamic world, with new information being added every day and even every second. Therefore, it is crucial to regularly update our data lake to reflect the latest scenarios <span class="No-Break">and information.</span></p>
<p>Managing multiple projects or pipelines concurrently and manually triggering them while integrating new data from various sources can be daunting. To alleviate this issue, we can rely on schedulers, and Airflow provides a straightforward solution for <span class="No-Break">this purpose.</span></p>
<p>In this recipe, we will create a<a id="_idIndexMarker774"/> simple <strong class="bold">Directed Acyclic Graph</strong> (<strong class="bold">DAG</strong>) in Airflow and explore how to use its parameters to schedule a pipeline to <span class="No-Break">run daily.</span></p>
<h2 id="_idParaDest-400"><a id="_idTextAnchor407"/>Getting ready</h2>
<p>Please refer to the <em class="italic">Technical requirements</em> section for this recipe since we will handle it with the same technology <span class="No-Break">mentioned here.</span></p>
<p>In this exercise, we will create a simple DAG. The structure of your Airflow folder should look like <span class="No-Break">the following:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer304">
<img alt="Figure 11.3 – daily_ingestion_dag DAG folder structure" height="247" src="image/Figure_11.03_B19453.jpg" width="320"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.3 – daily_ingestion_dag DAG folder structure</p>
<p>All code in this recipe will be placed inside <strong class="source-inline">daily_ingestion_dag.py</strong>. Ensure you have created the file by following the folder structure in <span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">.</span></p>
<h2 id="_idParaDest-401"><a id="_idTextAnchor408"/>How to do it…</h2>
<p>These <a id="_idIndexMarker775"/>are the steps for <span class="No-Break">this recipe:</span></p>
<ol>
<li>Let’s start by importing the <span class="No-Break">required libraries:</span><pre class="source-code">
from airflow import DAG
from airflow.operators.bash import BashOperator
from datetime import datetime, timedelta</pre></li>
<li>Now, we will define <strong class="source-inline">default_args</strong> for our DAG. For the <strong class="source-inline">start_date</strong> parameter, insert today’s date or a few days before you are doing this exercise. For <strong class="source-inline">end_date</strong>, insert a date a few days ahead of today’s date. In the end, it should look like <span class="No-Break">the following:</span><pre class="source-code">
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email': ['airflow@example.com'],
    'email_on_failure': True,
    'email_on_retry': True,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    'start_date': datetime(2023, 4, 12),
    'end_date': datetime(2023, 4, 30),
    'schedule_interval': '@daily
}</pre></li>
<li>Then, we will define our DAG and the tasks inside it. Since we want to focus on how to schedule daily ingestion, our tasks will each be a <strong class="source-inline">BashOperator</strong> since they can execute Bash commands with simplicity, as you can <span class="No-Break">see here:</span><pre class="source-code">
with DAG(
    'daily_ingestion_dag',
    default_args=default_args,
    description='A simple ETL job using Bash commands',
) as dag:
    t1 = BashOperator(
                task_id="t1",
                bash_command="echo 'This is task no1 '",
            )
    t2 = BashOperator(
                task_id="t2",
                bash_command="echo 'This is task no2 '",
            )
t1 &gt;&gt; t2</pre></li>
<li>With the <a id="_idIndexMarker776"/>DAG written, let’s enable it on the Airflow UI, and the DAG should run immediately. After running, the DAG will have a <strong class="source-inline">SUCCESS</strong> status, <span class="No-Break">as follows:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer305">
<img alt="Figure 11.4 –  daily_ingestion_dag DAG in the Airflow UI" height="502" src="image/Figure_11.04_B19453.jpg" width="1620"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.4 – daily_ingestion_dag DAG in the Airflow UI</p>
<p>If we <a id="_idIndexMarker777"/>check the logs, it will show the <strong class="source-inline">echo</strong> command output, similar to <span class="No-Break">the following:</span></p>
<pre class="source-code">
<strong class="bold">[2023-04-12, 19:54:38 UTC] [ 1686 - airflow.hooks.subprocess.SubprocessHook ] {subprocess.py:74} INFO - Running command: ['bash', '-c', "echo 'This is task no2 '"]</strong>
<strong class="bold">[2023-04-12, 19:54:38 UTC] [ 1686 - airflow.hooks.subprocess.SubprocessHook ] {subprocess.py:85} INFO - Output:</strong>
<strong class="bold">[2023-04-12, 19:54:38 UTC] [ 1686 - airflow.hooks.subprocess.SubprocessHook ] {subprocess.py:92} INFO - This is task no2</strong></pre>
<ol>
<li value="5">Now, we need to ensure the DAG will run daily. To confirm this, select the <strong class="bold">Calendar</strong> option on your DAG page. You will see something similar <span class="No-Break">to this:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer306">
<img alt="Figure 11.5 – DAG’s Calendar visualization in the Airflow UI" height="308" src="image/Figure_11.05_B19453.jpg" width="1178"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.5 – DAG’s Calendar visualization in the Airflow UI</p>
<p>As you<a id="_idIndexMarker778"/> can see, the execution is depicted in the shaded region to the left, indicating a successful outcome (<strong class="bold">SUCCESS</strong>) relative to the current date. The following days, until <strong class="source-inline">end_date</strong>, are marked with a dot inside, indicating the job will run every day for the next <span class="No-Break">few days.</span></p>
<p class="callout-heading">Note</p>
<p class="callout"><span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.5</em> shows some days when the job was executed successfully. This is shown to users how the same calendar behaves on <span class="No-Break">previous executions.</span></p>
<h2 id="_idParaDest-402"><a id="_idTextAnchor409"/>How it works…</h2>
<p>Airflow’s scheduler is mainly defined by three parameters: <strong class="source-inline">start_date</strong>, <strong class="source-inline">end_date</strong>, and <strong class="source-inline">schedule_interval</strong>. These three parameters define the beginning and end of the job and the interval <span class="No-Break">between executions.</span></p>
<p>Let’s take a look <span class="No-Break">at </span><span class="No-Break"><strong class="source-inline">default_args</strong></span><span class="No-Break">:</span></p>
<pre class="source-code">
default_args = {
    'owner': 'airflow',
    ...
    'start_date': datetime(2023, 4, 12),
    'end_date': datetime(2023, 4, 30),
    'schedule_interval': '@daily
}</pre>
<p>Since I am writing this exercise on April 12, 2023, I set my <strong class="source-inline">start_date</strong> parameter to the same day. This will make the job retrieve information relating to April 12, and if I put it a few days before the current date, Airflow will retrieve the earlier date. Don’t worry about it now; we will cover more about this in the <em class="italic">Scheduling historical data </em><span class="No-Break"><em class="italic">ingestion recipe</em></span><span class="No-Break">.</span></p>
<p>The key here is the <strong class="source-inline">schedule_interval</strong> parameter. As the name suggests, this parameter will define the periodicity or the interval of each execution, and, as you can observe, it was simply set using the <strong class="source-inline">@</strong><span class="No-Break"><strong class="source-inline">daily</strong></span><span class="No-Break"> value.</span></p>
<p>The <strong class="bold">Calendar</strong> option on the DAG UI page is an excellent feature of Airflow 2.2 onward. This functionality allows the developers to see the next execution days for the DAG, preventing<a id="_idIndexMarker779"/> <span class="No-Break">some confusion.</span></p>
<h2 id="_idParaDest-403"><a id="_idTextAnchor410"/>There's more…</h2>
<p>The DAG parameters are not limited to the ones we have seen in this recipe. Many others are available to make the data pipeline even more automated and intelligent. Let’s take a look at the <span class="No-Break">following code:</span></p>
<pre class="source-code">
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email': ['airflow@example.com'],
    'email_on_failure': True,
    'email_on_retry': True,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    'start_date': datetime(2023, 4, 12),
    'end_date': datetime(2023, 4, 30),
    'schedule_interval': '@daily,
    'queue': 'bash_queue',
    'pool': 'backfill',
    'priority_weight': 10
}</pre>
<p>There are three additional parameters here: <strong class="source-inline">queue</strong>, <strong class="source-inline">pool</strong>, and <strong class="source-inline">priority_weight</strong>. As we saw in <a href="B19453_09.xhtml#_idTextAnchor319"><span class="No-Break"><em class="italic">Chapter 9</em></span></a> and <a href="B19453_10.xhtml#_idTextAnchor364"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, the Airflow architecture includes a queue (usually executed by <strong class="bold">Celery</strong>) to<a id="_idIndexMarker780"/> create an order of execution when we have parallel jobs running simultaneously. The <strong class="source-inline">pool</strong> parameter limits the number of simultaneous jobs. Finally, <strong class="source-inline">priority_weight</strong>, as the name suggests, defines the priority of a DAG over <span class="No-Break">other DAGs.</span></p>
<p>You can read more about these parameters in the Airflow<a id="_idIndexMarker781"/> official <span class="No-Break">documentation here:</span></p>
<p><a href="https://airflow.apache.org/docs/apache-airflow/1.10.2/tutorial.xhtml"><span class="No-Break">https://airflow.apache.org/docs/apache-airflow/1.10.2/tutorial.xhtml</span></a></p>
<h2 id="_idParaDest-404"><a id="_idTextAnchor411"/>See also</h2>
<p>You can read more about scheduling with crontab<a id="_idIndexMarker782"/> also <span class="No-Break">at </span><a href="https://crontab.guru/"><span class="No-Break">https://crontab.guru/</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-405"><a id="_idTextAnchor412"/>Scheduling historical data ingestion</h1>
<p>Historical data is<a id="_idIndexMarker783"/> vital for data-driven decisions, providing valuable insights and supporting decision-making processes. It can also refer to data that has been accumulated over a period of time. For example, a sales company can use historical data from previous marketing campaigns to see how they have influenced the sales of a specific product over <span class="No-Break">the years.</span></p>
<p>This exercise will show how to create a scheduler in Airflow to ingest historical data using the best practices and common concerns related to <span class="No-Break">this process.</span></p>
<h2 id="_idParaDest-406"><a id="_idTextAnchor413"/>Getting ready</h2>
<p>Please refer to the <em class="italic">Technical requirements</em> section for this recipe since we will handle it with the same technology <span class="No-Break">mentioned here.</span></p>
<p>In this exercise, we will create a simple DAG inside our DAGs folder. The structure of your Airflow folder should look like <span class="No-Break">the following:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer307">
<img alt="Figure 11.6 – historical_data_dag folder structure in your local Airflow directory" height="242" src="image/Figure_11.06_B19453.jpg" width="321"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.6 – historical_data_dag folder structure in your local Airflow directory</p>
<h2 id="_idParaDest-407"><a id="_idTextAnchor414"/>How to do it…</h2>
<p>Here are the<a id="_idIndexMarker784"/> steps for <span class="No-Break">this recipe:</span></p>
<ol>
<li>Let’s start by importing <span class="No-Break">our libraries:</span><pre class="source-code">
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime, timedelta</pre></li>
<li>Now, let’s define <strong class="source-inline">default_args</strong>. Since we wish to process old data, I will set <strong class="source-inline">datetime</strong> for <strong class="source-inline">start_date</strong> before the current day, and <strong class="source-inline">end_date</strong> will be near the current day. See the <span class="No-Break">following code:</span><pre class="source-code">
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email': ['airflow@example.com'],
    'email_on_failure': True,
    'email_on_retry': True,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    'start_date': datetime(2023, 4, 2),
    'end_date': datetime(2023, 4, 10),
    'schedule_interval': '@daily'
}</pre></li>
<li>Then, we will create a simple function to print the date Airflow used to execute the pipeline. You can see <span class="No-Break">it here:</span><pre class="source-code">
def my_task(execution_date=None):
    print(f"execution_date:{execution_date}")</pre></li>
<li>Finally, we<a id="_idIndexMarker785"/> will declare our DAG parameters and a <strong class="source-inline">PythonOperator</strong> task to execute it, as you can <span class="No-Break">see here:</span><pre class="source-code">
with DAG(
    'historical_data_dag',
    default_args=default_args,
    description='A simple ETL job using Python commands to retrieve historical data',
) as dag:
    p1 = PythonOperator(
                task_id="p1",
                python_callable=my_task,
        )
p1</pre></li>
<li>Heading to the Airflow UI, let’s proceed with the usual steps to enable the DAG and see its execution. On the <strong class="source-inline">historical_data_dag</strong> page, you should see something similar to the <span class="No-Break">following screenshot:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer308">
<img alt="Figure 11.7 – historical_data_dag DAG in the Airflow UI" height="527" src="image/Figure_11.07_B19453.jpg" width="1619"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.7 – historical_data_dag DAG in the Airflow UI</p>
<p>As you can see, the task ran <span class="No-Break">with success.</span></p>
<ol>
<li value="6">Now, let’s check <a id="_idIndexMarker786"/>our <strong class="source-inline">logs</strong> folder. If we select the folder with the same name as the DAG we created (<strong class="source-inline">historical_data_dag</strong>), we will observe <strong class="source-inline">run_id </strong>instances on different days, beginning on April 2 and finishing on <span class="No-Break">April 10:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer309">
<img alt="Figure 11.8 – Airflow logs folder showing retroactive ingestion" height="290" src="image/Figure_11.08_B19453.jpg" width="467"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.8 – Airflow logs folder showing retroactive ingestion</p>
<ol>
<li value="7">Let’s open the<a id="_idIndexMarker787"/> first <strong class="source-inline">run_id</strong> folder to explore the log for <span class="No-Break">that run:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer310">
<img alt="Figure 11.9 – DAG log for April 2, 2023" height="56" src="image/Figure_11.09_B19453.jpg" width="451"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.9 – DAG log for April 2, 2023</p>
<p>The log tells us the <strong class="source-inline">execution_date</strong> parameter, which is the same as the <span class="No-Break"><strong class="source-inline">start_date</strong></span><span class="No-Break"> parameter.</span></p>
<p>Here is a closer look at <span class="No-Break">the logs:</span></p>
<pre class="source-code">
<strong class="bold">[2023-04-12 20:10:25,205] [ ... ] {logging_mixin.py:115} INFO - execution_date:2023-04-02T00:00:00+00:00</strong>
<strong class="bold">[2023-04-12 20:10:25,205] [ ... ] {python.py:173} INFO - Done. Returned value was: None</strong></pre>
<p>We will observe the same pattern for the <strong class="source-inline">run_id</strong> for <span class="No-Break">April 3:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer311">
<img alt="Figure 11.10 – DAG log for April 3, 2023" height="51" src="image/Figure_11.10_B19453.jpg" width="446"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.10 – DAG log for April 3, 2023</p>
<p>Here is a closer look at the <span class="No-Break">log output:</span></p>
<pre class="source-code">
<strong class="bold">2023-04-12 20:10:25,276] [ ... ] {logging_mixin.py:115} INFO - execution_date:2023-04-03T00:00:00+00:00</strong>
<strong class="bold">[2023-04-12 20:10:25,276] [...] {python.py:173} INFO - Done. Returned value was: None</strong></pre>
<p>The <strong class="source-inline">execution_date</strong> also refers to <span class="No-Break">April 3.</span></p>
<p>This shows us that Airflow has used the interval declared on <strong class="source-inline">start_date</strong> and <strong class="source-inline">end_date</strong> to run <span class="No-Break">the task!</span></p>
<p>Now, let’s proceed to understand how the <span class="No-Break">scheduler works.</span></p>
<h2 id="_idParaDest-408"><a id="_idTextAnchor415"/>How it works…</h2>
<p>As we saw, scheduling<a id="_idIndexMarker788"/> and retrieving historical data with Airflow is straightforward, and the key parameters were <strong class="source-inline">start_date</strong>, <strong class="source-inline">end_date</strong>, and <strong class="source-inline">schedule_interval</strong>. Let’s discuss them in a little <span class="No-Break">more detail:</span></p>
<ul>
<li>The <strong class="source-inline">start_date</strong> parameter defines the first date Airflow will look at when the pipeline is triggered. In our case, it was <span class="No-Break">April 2.</span></li>
<li>Next is <strong class="source-inline">end_date</strong>. Usually, this is not a mandatory parameter, even for recurrent ingests. However, the purpose of using it was to show that we can set a date as a limit to stop <span class="No-Break">the ingestion.</span></li>
<li>Finally, <strong class="source-inline">schedule_interval</strong> dictates the interval between two dates. We used a daily interval in this exercise, but we could also use <strong class="source-inline">crontab</strong> if we needed more granular historical ingestion. We will explore this in more detail in the <em class="italic">Setting up the schedule_interval </em><span class="No-Break"><em class="italic">parameter</em></span><span class="No-Break"> recipe.</span></li>
</ul>
<p>With this information, it is easier to understand the logs we got <span class="No-Break">from Airflow:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer312">
<img alt="Figure 11.11 – Airflow logs folder showing historic ingestion" height="290" src="image/Figure_11.11_B19453.jpg" width="469"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.11 – Airflow logs folder showing historic ingestion</p>
<p>Each folder represents one historical ingestion that occurred at a daily interval. Since we did not define a more granular date-time specification, the folder name uses the time that the job was triggered. This information is not included in <span class="No-Break">the logs.</span></p>
<p>To show what date Airflow was using behind the scenes, we created a <span class="No-Break">simple function:</span></p>
<pre class="source-code">
def my_task(execution_date=None):
    print(f"execution_date:{execution_date}")</pre>
<p>The only purpose of the function is to show the execution date of the task. The <strong class="source-inline">execution_date</strong> parameter is an internal parameter that displays when a task is executed and can be used by operators or other functions to execute something based on <span class="No-Break">a date.</span></p>
<p>For example, let’s say we need to retrieve historical data stored as a partition. We can use <strong class="source-inline">execution_date</strong> to pass the date-time information to a Spark function, which will read and retrieve data from that partition with the same <span class="No-Break">date information.</span></p>
<p>As you can see, retrieving historical data/information in Airflow requires a few configurations. A good practice is to have a separate and dedicated DAG for historical data processing so that current data ingestion is not impaired. Also, if it is necessary to reprocess data, we<a id="_idIndexMarker789"/> can do it with a few <span class="No-Break">parameter changes.</span></p>
<h2 id="_idParaDest-409"><a id="_idTextAnchor416"/>There's more…</h2>
<p>Inside the technique of ingesting historical data using Airflow are two important concepts: <em class="italic">catchup</em> <span class="No-Break">and </span><span class="No-Break"><em class="italic">backfill</em></span><span class="No-Break">.</span></p>
<p>Scheduling and running DAGs for past periods that may have been missed for various reasons is commonly known as <strong class="bold">catchup</strong> in <a id="_idIndexMarker790"/>Apache Airflow. This mechanism allows the system to execute DAGs retrospectively by following their pre-specified <strong class="source-inline">schedule_interval</strong>. By default, this feature is enabled in Airflow. Therefore, if a paused or uncreated DAG’s <strong class="source-inline">start_date</strong> lies in the past, it will be automatically scheduled and executed for missed time intervals. The following diagram <span class="No-Break">illustrates this:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer313">
<img alt="Figure 11.12 – Airflow catchup timeline. Source: https://medium.com/nerd-for-tech/airflow-catchup-backfill-demystified-355def1b6f92" height="372" src="image/Figure_11.12_B19453.jpg" width="643"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.12 – Airflow catchup timeline. Source: https://medium.com/nerd-for-tech/airflow-catchup-backfill-demystified-355def1b6f92</p>
<p>On the other hand, Airflow’s <em class="italic">backfill</em> functionality<a id="_idIndexMarker791"/> allows you to execute DAGs retroactively, along with their associated tasks for past periods that may have been missed due to the DAG being paused, not yet created, or for any other reason. Backfilling in Airflow is a powerful feature that helps you to fill the gaps and catch up on data processing or workflow execution that may have been missed in <span class="No-Break">the past.</span></p>
<p>You can read more about it on <em class="italic">Amit Singh Rathore’s</em> blog page <span class="No-Break">here: </span><a href="https://medium.com/nerd-for-tech/airflow-catchup-backfill-demystified-355def1b6f92"><span class="No-Break">https://medium.com/nerd-for-tech/airflow-catchup-backfill-demystified-355def1b6f92</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-410"><a id="_idTextAnchor417"/>Scheduling data replication</h1>
<p>In the first chapter of this<a id="_idIndexMarker792"/> book, we covered what data replication is and why it’s important. We saw how vital this process is in the prevention of data loss and in promoting recovery <span class="No-Break">from disasters.</span></p>
<p>Now, it is time to learn how to create an optimized schedule window to make data replication happen. In this recipe, we will create a diagram to help us decide the best moment to replicate <span class="No-Break">our data.</span></p>
<h2 id="_idParaDest-411"><a id="_idTextAnchor418"/>Getting ready</h2>
<p>This exercise does not require technical preparation. However, to make it closer to a real scenario, let’s imagine we need to decide the best way to ensure the data from a hospital is being <span class="No-Break">adequately replicated.</span></p>
<p>We will have two pipelines: one holding patient information and another with financial information. The first pipeline collects information from a patient database and synthesizes it into readable reports used by the medical team. The second will feed an internal dashboard used by the <span class="No-Break">hospital executives.</span></p>
<p>Due to infrastructure limitations, the operations team has only one requirement: only one pipeline can have its data <span class="No-Break">replicated quickly.</span></p>
<h2 id="_idParaDest-412"><a id="_idTextAnchor419"/>How to do it…</h2>
<p>Here are the <a id="_idIndexMarker793"/>steps for <span class="No-Break">this recipe:</span></p>
<ol>
<li><strong class="bold">Identify the targets to replicate</strong>: As described in the <em class="italic">Getting ready</em> section, we have identified the target data, which are the pipeline holding patient information, and the pipeline with financial data to feed <span class="No-Break">a dashboard.</span></li>
</ol>
<p>However, suppose this information is not coming promptly from stakeholders or other relevant people. In that case, we must always start by identifying our project’s most critical tables <span class="No-Break">or databases.</span></p>
<ol>
<li value="2"><strong class="bold">Replication periodicity</strong>: We must define the replication schedule based on our data’s criticality or relevance. Let’s take a look at the <span class="No-Break">following diagram:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer314">
<img alt="Figure 11.13 – Periodicity of data replication" height="494" src="image/Figure_11.13_B19453.jpg" width="954"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.13 – Periodicity of data replication</p>
<p>As we can see, the<a id="_idIndexMarker794"/> more critical the data is, the more frequently the replication is recommended. In our scenario, the patient reports would fit better with 30 minutes to 3 hours after the ingestion, while the financial data can be replicated until 24 hours <span class="No-Break">have passed.</span></p>
<ol>
<li value="3"><strong class="bold">Set a schedule window for replication</strong>: Now, we need to create a schedule window to replicate the data. This replication decision needs to take into consideration two important factors, as you can see in the <span class="No-Break">following diagram:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer315">
<img alt="Figure 11.14 – Replication window" height="318" src="image/Figure_11.14_B19453.jpg" width="698"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.14 – Replication window</p>
<p>Based on the two pipelines (and remembering we need to prioritize one), the suggestion would be to replicate the financial data every day after business working hours, while the patient data can be done at the same time as new <span class="No-Break">information arrives.</span></p>
<p>Don’t worry if this <a id="_idIndexMarker795"/>seems a bit confusing. Let’s explore the details in the <em class="italic">How it </em><span class="No-Break"><em class="italic">works</em></span><span class="No-Break"> section.</span></p>
<h2 id="_idParaDest-413"><a id="_idTextAnchor420"/>How it works…</h2>
<p>Data replication is a vital process that ensures data availability and disaster recovery. Its concept is older than the current ETL process and has been used for many years in on-premises databases. Our advantage today is that we can carry out this process at any moment. In contrast, replication had a strict schedule window a few years ago due to <span class="No-Break">hardware limitations.</span></p>
<p>In our example, we handled two pipelines that had distinct severity levels. The idea behind this is to teach attentive eyes to decide when doing <span class="No-Break">each replication.</span></p>
<p>The first pipeline, which is the patient reports pipeline, handles sensitive data such as personal information and medical history. It also may be helpful for doctors and other health workers to help <span class="No-Break">a patient.</span></p>
<p>Based on this, the best approach is to replicate this data within a few minutes or hours of it being processed, allowing high availability <span class="No-Break">and redundancy.</span></p>
<p>At first look, the financial data seems to be very critical and demands fast replication; we need to remember this pipeline feeds data to a dashboard, and therefore, an analyst can use the raw data to <span class="No-Break">generate reports.</span></p>
<p>The decision to schedule data replication must consider other factors besides the data involved. It is also essential to understand who is interested in or needs to access the data and how it <a id="_idIndexMarker796"/>impacts the project, area, or business when it <span class="No-Break">becomes unavailable.</span></p>
<h2 id="_idParaDest-414"><a id="_idTextAnchor421"/>There's more…</h2>
<p>This recipe covered a simple example of setting a scheduling agenda for data replication. We also covered in <em class="italic">step 3</em> the two main points to have in mind when doing so. Nevertheless, many other factors can influence the scheduler’s performance and execution. A few examples are <span class="No-Break">as follows:</span></p>
<ul>
<li>Where Airflow (or a similar platform) is hosted on <span class="No-Break">a server</span></li>
<li>The <span class="No-Break">CPU capacity</span></li>
<li>The number <span class="No-Break">of schedulers</span></li>
<li><span class="No-Break">Networking throughput</span></li>
</ul>
<p>If you want to know more about it, you can find a complete list of these factors in the Airflow <span class="No-Break">documentation: </span><a href="https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/scheduler.xhtml#what-impacts-scheduler-s-performance"><span class="No-Break">https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/scheduler.xhtml#what-impacts-scheduler-s-performance</span></a><span class="No-Break">.</span></p>
<p>The great thing about this documentation is that many points also apply to other data pipeline processors and can serve as <span class="No-Break">a guide.</span></p>
<h1 id="_idParaDest-415"><a id="_idTextAnchor422"/>Setting up the schedule_interval parameter</h1>
<p>One of the most widely used <a id="_idIndexMarker797"/>parameters in Airflow DAG scheduler configuration is <strong class="source-inline">schedule_interval</strong>. Together with <strong class="source-inline">start_date</strong>, it creates a dynamic and continuous trigger for the pipeline. However, there are some small details we need to pay attention to when <span class="No-Break">setting </span><span class="No-Break"><strong class="source-inline">schedule_interval</strong></span><span class="No-Break">.</span></p>
<p>This recipe will cover different forms to set up the <strong class="source-inline">schedule_interval</strong> parameter. We will also explore a practical example to see how the scheduling window works in Airflow, making it more straightforward to manage <span class="No-Break">pipeline executions.</span></p>
<h2 id="_idParaDest-416"><a id="_idTextAnchor423"/>Getting ready</h2>
<p>While this exercise does not require any technical preparation, it is recommended to take notes about when the pipeline is supposed to start and the interval between <span class="No-Break">each trigger.</span></p>
<h2 id="_idParaDest-417"><a id="_idTextAnchor424"/>How to do it…</h2>
<p>Here, we will show only the <strong class="source-inline">default_args</strong> dictionary to avoid code redundancy. However, you can always check out the complete code in the GitHub <span class="No-Break">repository: </span><a href="https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_11/settingup_schedule_interval"><span class="No-Break">https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_11/settingup_schedule_interval</span></a><span class="No-Break">.</span></p>
<p>Let’s see how we can <span class="No-Break">declare </span><span class="No-Break"><strong class="source-inline">schedule_interval</strong></span><span class="No-Break">:</span></p>
<ul>
<li><strong class="bold">Using friendly names</strong>: A <a id="_idIndexMarker798"/>common way to declare the <strong class="source-inline">schedule_interval</strong> value is by using accessible names such as <strong class="source-inline">@daily</strong>, <strong class="source-inline">@hourly</strong>, or <strong class="source-inline">@weekly</strong>. See what it looks like in the <span class="No-Break">following code:</span><pre class="source-code">
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    'start_date': datetime(2023, 4, 2),
    'schedule_interval': '@daily'
}</pre></li>
<li><strong class="bold">Using crontab notation:</strong> Airflow <a id="_idIndexMarker799"/>also supports defining <strong class="source-inline">schedule_interval</strong> using <a id="_idIndexMarker800"/><span class="No-Break">crontab notation:</span><pre class="source-code">
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    'start_date': datetime(2023, 4, 2),
    'schedule_interval': '0 22 * * 1-5'
}</pre></li>
</ul>
<p>In this case, we set the scheduler to start every weekday, from Monday to Friday, at 10 pm (or <span class="No-Break">22:00 hours).</span></p>
<ul>
<li><strong class="bold">Using Python’s timedelta function</strong>: Finally, another<a id="_idIndexMarker801"/> common way to set <strong class="source-inline">schedule_interval</strong> is by using the <strong class="source-inline">timedelta</strong> method. In the following code, we can set the pipeline to trigger with an interval of one day between <span class="No-Break">each execution:</span><pre class="source-code">
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    'start_date': datetime(2023, 4, 2),
    'schedule_interval': timedelta(days=1)
}</pre></li>
</ul>
<h2 id="_idParaDest-418"><a id="_idTextAnchor425"/>How it works…</h2>
<p>The <strong class="source-inline">schedule_interval</strong> parameter is <a id="_idIndexMarker802"/>an essential aspect of scheduling DAGs in Airflow and provides a flexible way to define how frequently your workflows should be executed. We can think of it as the core of the <span class="No-Break">Airflow scheduler.</span></p>
<p>The goal of this recipe was to show the different ways to set <strong class="source-inline">schedule_interval</strong> and when to use each of them. Let’s explore them in <span class="No-Break">more depth:</span></p>
<ul>
<li><strong class="bold">Friendly names:</strong> As the name suggests, this notation uses user-friendly labels or aliases. It provides an easy and<a id="_idIndexMarker803"/> convenient way to specify the exact time and date for scheduled tasks to run. It can be an easy and simple solution if you don’t have a specific date-time to run <span class="No-Break">the scheduler.</span></li>
<li><strong class="bold">Crontab notation:</strong> Crontabs have long been widely used across applications and systems. Crontab notation<a id="_idIndexMarker804"/> consists of five fields, representing the minute, hour, day of the month, month, and day of the week. It is a great choice when handling complex schedules, for example, executing the trigger at 1 p.m. on Mondays and Fridays, or <span class="No-Break">other combinations.</span></li>
<li><strong class="bold">timedelta function</strong>: This<a id="_idIndexMarker805"/> Pythonic technique is commonly used by Airflow users to set the schedule of the DAGs. Using a simple declaration, we can set whether the DAG will run with the interval of one day (as we saw in <em class="italic">step 3</em>) or every five minutes (<strong class="source-inline">timedelta(minutes=5)</strong>). It is also a user-friendly notation but with more <span class="No-Break">granular </span><span class="No-Break"><a id="_idIndexMarker806"/></span><span class="No-Break">power.</span></li>
</ul>
<p class="callout-heading">Note</p>
<p class="callout">Although we have seen three ways to set the <strong class="source-inline">schedule_interval</strong> here, remember Airflow is not a streaming solution, and having multiple DAGs running with a small interval between them can overload the server. Consider using a streaming tool if you or your team needs to schedule ingestions every 10-30 minutes, <span class="No-Break">or less.</span></p>
<h2 id="_idParaDest-419"><a id="_idTextAnchor426"/>See also</h2>
<p><em class="italic">TowardsDataScience</em> has a fantastic blog post about <a id="_idIndexMarker807"/>how <strong class="source-inline">schedule_interval</strong> works behind the scenes. You can find it <span class="No-Break">here: </span><a href="https://towardsdatascience.com/airflow-schedule-interval-101-bbdda31cc463"><span class="No-Break">https://towardsdatascience.com/airflow-schedule-interval-101-bbdda31cc463</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-420"><a id="_idTextAnchor427"/>Solving scheduling errors</h1>
<p>At this point, you may have already<a id="_idIndexMarker808"/> experienced some issues with scheduling pipelines not being triggered as expected. If not, don’t worry; it will happen sometime and is totally normal. With several pipelines running in parallel, in different windows, or attached to different timezones, it is expected to be entangled with one <span class="No-Break">or another.</span></p>
<p>To avoid this entanglement, in this exercise, we will create a diagram to assist in the debugging process, identify the possible causes of a scheduler not working correctly in Airflow, and see how to <span class="No-Break">solve it.</span></p>
<h2 id="_idParaDest-421"><a id="_idTextAnchor428"/>Getting ready</h2>
<p>This recipe does not require any technical preparation. Nevertheless, taking notes and writing down the steps we will follow here can be helpful. Writing when learning something new can help to fix the knowledge in our minds, making it easier to <span class="No-Break">remember later.</span></p>
<p>Back to our exercise; scheduler errors in Airflow typically give the DAG status <strong class="source-inline">None</strong>, as <span class="No-Break">shown here:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer316">
<img alt="Figure 11.15 – DAG in the Airflow UI with an error in the scheduler" height="410" src="image/Figure_11.15_B19453.jpg" width="1603"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.15 – DAG in the Airflow UI with an error in the scheduler</p>
<p>We will now find out how to fix the error and make the job <span class="No-Break">run again.</span></p>
<h2 id="_idParaDest-422"><a id="_idTextAnchor429"/>How to do it…</h2>
<p>Let’s try to<a id="_idIndexMarker809"/> identify what could be the cause of the error in our scheduler. Don’t worry about understanding why we used the approaches that we have. We will cover them in detail in <em class="italic">How </em><span class="No-Break"><em class="italic">it works</em></span><span class="No-Break">:</span></p>
<ol>
<li>We can first check whether <strong class="source-inline">start_date</strong> has been set to <strong class="source-inline">datetime.now()</strong>. If this is the case, the best approach here is to change this parameter value to a specific date, as you can <span class="No-Break">see here:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer317">
<img alt="Figure 11.16 – Error caused by start_date parameter" height="192" src="image/Figure_11.16_B19453.jpg" width="325"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.16 – Error caused by start_date parameter</p>
<p>The code will look <span class="No-Break">like this:</span></p>
<pre class="source-code">
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    'start_date': datetime(2023, 4, 2),
    'schedule_interval': '@daily'
}</pre>
<ol>
<li value="2">Now we can verify <a id="_idIndexMarker810"/>whether <strong class="source-inline">schedule_interval</strong> is aligned with the <strong class="source-inline">start_date</strong> parameter. In the following diagram, you can see three possibilities to fix <span class="No-Break">the issue:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer318">
<img alt="Figure 11.17 – Error caused by the start_date and schedule_interval parameters" height="209" src="image/Figure_11.17_B19453.jpg" width="910"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.17 – Error caused by the start_date and schedule_interval parameters</p>
<p>You can prevent this error by using crontab notation in <strong class="source-inline">schedule_interval</strong>, <span class="No-Break">as follows:</span></p>
<pre class="source-code">
schedule_interval='0 2 * * *'</pre>
<p>If you are facing problems with the timezone, you can define which timezone Airflow will trigger the job in by using the <span class="No-Break"><strong class="source-inline">pendulum</strong></span><span class="No-Break"> library:</span></p>
<pre class="source-code">
pendulum.now("Europe/Paris")</pre>
<ol>
<li value="3">Finally, another standard error scenario is when <strong class="source-inline">schedule_interval</strong> changes after the DAG has been running for some time. In this case, the solution usually is to recreate <span class="No-Break">the DAG:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer319">
<img alt="Figure 11.18 – Error caused by a change in schedule_interval" height="213" src="image/Figure_11.18_B19453.jpg" width="339"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.18 – Error caused by a change in schedule_interval</p>
<p>At the end of these steps, we will end up with a debug diagram similar <span class="No-Break">to this:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer320">
<img alt="Figure 11.19 – Diagram to assist in identifying an error caused in the Airflow scheduler" height="537" src="image/Figure_11.19_B19453.jpg" width="1180"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.19 – Diagram to assist in identifying an error caused in the Airflow scheduler</p>
<h2 id="_idParaDest-423"><a id="_idTextAnchor430"/>How it works…</h2>
<p>As you can see, the goal of<a id="_idIndexMarker811"/> this recipe was to show three different scenarios in which it is common to observe errors related to the scheduler. Errors in the scheduler normally lead to a DAG status as <strong class="source-inline">None</strong>, as we saw in the <em class="italic">Getting ready</em> section. However, having a trigger that does not behave as expected is also considered an error. Now, let’s explore the three addressed scenarios and <span class="No-Break">their solutions.</span></p>
<p>The first scenario usually occurs when we want to use the current date for <strong class="source-inline">start_date</strong>. Although it seems like a good idea to use the <strong class="source-inline">datetime.now()</strong> function to represent the current date-time, Airflow will not interpret it as we do. The <strong class="source-inline">datetime.now()</strong> function will create what we call a <em class="italic">dynamic scheduler</em>, and the trigger will never be executed. It happens because the execution schedule uses <strong class="source-inline">start_date</strong> and <strong class="source-inline">schedule_interval</strong> to know when to execute the trigger, as you can <span class="No-Break">see here:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer321">
<img alt="Figure 11.20 – Airflow execution scheduler equation" height="47" src="image/Figure_11.20_B19453.jpg" width="643"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.20 – Airflow execution scheduler equation</p>
<p>If we use <strong class="source-inline">datetime.now()</strong>, it moves<a id="_idIndexMarker812"/> along with time and will never be triggered. We recommend using a static schedule definition, as we saw in <span class="No-Break"><em class="italic">step 1</em></span><span class="No-Break">.</span></p>
<p>A typical error is when <strong class="source-inline">start_date</strong> and <strong class="source-inline">schedule_interval</strong> are not aligned. Based on the explanation of <span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.20</em>, we can already imagine why aligning these two parameters and preventing overlapping are so important. As addressed in <em class="italic">step 2</em>, a good way to prevent this is by using crontab notation to <span class="No-Break">set </span><span class="No-Break"><strong class="source-inline">schedule_interval</strong></span><span class="No-Break">.</span></p>
<p>A vital topic is the timezones involved in the process. If you look closely at the top of the Airflow UI, you will see a clock and its associated timezone, <span class="No-Break">as follows:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer322">
<img alt="Figure 11.21 – Airflow UI clock with the timezone displayed" height="58" src="image/Figure_11.21_B19453.jpg" width="820"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.21 – Airflow UI clock with the timezone displayed</p>
<p>This indicates that the Airflow server is running in the UTC timezone, and all DAGs and tasks will be executed using the same logic. If you are working in a different timezone and want to ensure it will run according to your timezone, you can use the <strong class="source-inline">pendulum</strong> library, as you can <span class="No-Break">see here:</span></p>
<pre class="source-code">
schedule_interval = pendulum.now("Europe/Paris")</pre>
<p><strong class="source-inline">pendulum</strong> is a third-party Python library that provides easy date-time manipulations using the built-in <strong class="source-inline">datetime</strong> Python package. You can find out more about it in the <strong class="source-inline">pendulum</strong> official <a id="_idIndexMarker813"/><span class="No-Break">documentation: </span><a href="https://pendulum.eustace.io/"><span class="No-Break">https://pendulum.eustace.io/</span></a><span class="No-Break">.</span></p>
<p>Finally, the last scenario has a straightforward solution: recreate the DAG if <strong class="source-inline">schedule_interval</strong> changes after some executions. Although this error may not always occur, it is a good practice<a id="_idIndexMarker814"/> to recreate the DAG to prevent <span class="No-Break">further problems.</span></p>
<h2 id="_idParaDest-424"><a id="_idTextAnchor431"/>There’s more…</h2>
<p>We have provided in this recipe some examples of what you can check if the scheduler is not working, but other common errors in Airflow can happen. You can find out more about this on <em class="italic">Astronomer’s</em> blog page <span class="No-Break">here: </span><a href="https://www.astronomer.io/blog/7-common-errors-to-check-when-debugging-airflow-dag/"><span class="No-Break">https://www.astronomer.io/blog/7-common-errors-to-check-when-debugging-airflow-dag/</span></a><span class="No-Break">.</span></p>
<p>In the blog, you can find other scenarios where Airflow throws a silent error (or an error without an explicit error message) and how to <span class="No-Break">solve them.</span></p>
<h1 id="_idParaDest-425"><a id="_idTextAnchor432"/>Further reading</h1>
<ul>
<li><a href="https://airflow.apache.org/docs/apache-airflow/stable/faq.xhtml#what-s-the-deal-with-start-date"><span class="No-Break">https://airflow.apache.org/docs/apache-airflow/stable/faq.xhtml#what-s-the-deal-with-start-date</span></a></li>
<li><a href="https://se.devoteam.com/expert-view/why-my-scheduled-dag-does-not-runapache-airflow-dynamic-start-date-for-equally-unequally-spaced-interval/"><span class="No-Break">https://se.devoteam.com/expert-view/why-my-scheduled-dag-does-not-runapache-airflow-dynamic-start-date-for-equally-unequally-spaced-interval/</span></a></li>
<li><a href="https://stackoverflow.com/questions/66098050/airflow-dag-not-triggered-at-schedule-time"><span class="No-Break">https://stackoverflow.com/questions/66098050/airflow-dag-not-triggered-at-schedule-time</span></a></li>
</ul>
</div>
</div></body></html>