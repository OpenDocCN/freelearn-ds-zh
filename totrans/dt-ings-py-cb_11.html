<html><head></head><body>
<div><div><h1 class="chapter-number" id="_idParaDest-395"><a id="_idTextAnchor402"/>11</h1>
<h1 id="_idParaDest-396"><a id="_idTextAnchor403"/>Automating Your Data Ingestion Pipelines</h1>
<p>Data sources are frequently updated, and this requires us to update our data lake. However, with multiple sources or projects, it becomes impossible to trigger data pipelines manually. Data pipeline automation makes ingesting and processing data mechanical, obviating the human actions to trigger it. The importance of automation configuration lies in the ability to streamline data flow and improve data quality, reducing errors and inconsistency.</p>
<p>In this chapter, we will cover how to automate the data ingestion pipelines in Airflow, along with two essential topics in data engineering, data replication and historical data ingestion, as well as best practices.</p>
<p>In this chapter, we will cover the following recipes:</p>
<ul>
<li>Scheduling daily ingestions</li>
<li>Scheduling historical data ingestion</li>
<li>Scheduling data replication</li>
<li>Setting up the <code>schedule_interval</code> parameter</li>
<li>Solving scheduling errors</li>
</ul>
<h1 id="_idParaDest-397"><a id="_idTextAnchor404"/>Technical requirements</h1>
<p>You can find the code from this chapter in the GitHub repository at <a href="https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_11">https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_11</a>.</p>
<h1 id="_idParaDest-398"><a id="_idTextAnchor405"/>Installing and running Airflow</h1>
<p>This chapter requires that<a id="_idIndexMarker768"/> Airflow is installed on your local machine. You can install it directly on your <strong class="bold">Operating System</strong> (<strong class="bold">OS</strong>) or <a id="_idIndexMarker769"/>use a Docker image. For more information, refer to the <em class="italic">Configuring Docker for Airflow </em>recipe in <a href="B19453_01.xhtml#_idTextAnchor022"><em class="italic">Chapter 1</em></a>.</p>
<p>After<a id="_idIndexMarker770"/> following the steps described in <a href="B19453_01.xhtml#_idTextAnchor022"><em class="italic">Chapter 1</em></a>, ensure your Airflow instance runs correctly. You can do that by checking the Airflow UI at <code>http://localhost:8080</code>.</p>
<p>If you are using a Docker container (as I am) to host your Airflow application, you can check its status in the terminal with the following command:</p>
<pre class="source-code">
$ docker ps</pre>
<p>Here is the <a id="_idIndexMarker771"/>status of the container:</p>
<div><div><img alt="Figure 11.1 –  Airflow containers running" height="131" src="img/Figure_11.01_B19453.jpg" width="1427"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.1 – Airflow containers running</p>
<p>Or you can check the container <a id="_idIndexMarker772"/>status on <strong class="bold">Docker Desktop</strong>:</p>
<div><div><img alt="Figure 11.2 – Docker Desktop showing Airflow running containers" height="455" src="img/Figure_11.02_B19453.jpg" width="1111"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.2 – Docker Desktop showing Airflow running containers</p>
<h1 id="_idParaDest-399"><a id="_idTextAnchor406"/>Scheduling daily ingestions</h1>
<p>Data constantly changes in our <a id="_idIndexMarker773"/>dynamic world, with new information being added every day and even every second. Therefore, it is crucial to regularly update our data lake to reflect the latest scenarios and information.</p>
<p>Managing multiple projects or pipelines concurrently and manually triggering them while integrating new data from various sources can be daunting. To alleviate this issue, we can rely on schedulers, and Airflow provides a straightforward solution for this purpose.</p>
<p>In this recipe, we will create a<a id="_idIndexMarker774"/> simple <strong class="bold">Directed Acyclic Graph</strong> (<strong class="bold">DAG</strong>) in Airflow and explore how to use its parameters to schedule a pipeline to run daily.</p>
<h2 id="_idParaDest-400"><a id="_idTextAnchor407"/>Getting ready</h2>
<p>Please refer to the <em class="italic">Technical requirements</em> section for this recipe since we will handle it with the same technology mentioned here.</p>
<p>In this exercise, we will create a simple DAG. The structure of your Airflow folder should look like the following:</p>
<div><div><img alt="Figure 11.3 – daily_ingestion_dag DAG folder structure" height="247" src="img/Figure_11.03_B19453.jpg" width="320"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.3 – daily_ingestion_dag DAG folder structure</p>
<p>All code in this recipe will be placed inside <code>daily_ingestion_dag.py</code>. Ensure you have created the file by following the folder structure in <em class="italic">Figure 11</em><em class="italic">.3</em>.</p>
<h2 id="_idParaDest-401"><a id="_idTextAnchor408"/>How to do it…</h2>
<p>These <a id="_idIndexMarker775"/>are the steps for this recipe:</p>
<ol>
<li>Let’s start by importing the required libraries:<pre class="source-code">
from airflow import DAG
from airflow.operators.bash import BashOperator
from datetime import datetime, timedelta</pre></li>
<li>Now, we will define <code>default_args</code> for our DAG. For the <code>start_date</code> parameter, insert today’s date or a few days before you are doing this exercise. For <code>end_date</code>, insert a date a few days ahead of today’s date. In the end, it should look like the following:<pre class="source-code">
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email': ['airflow@example.com'],
    'email_on_failure': True,
    'email_on_retry': True,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    'start_date': datetime(2023, 4, 12),
    'end_date': datetime(2023, 4, 30),
    'schedule_interval': '@daily
}</pre></li>
<li>Then, we will define our DAG and the tasks inside it. Since we want to focus on how to schedule daily ingestion, our tasks will each be a <code>BashOperator</code> since they can execute Bash commands with simplicity, as you can see here:<pre class="source-code">
with DAG(
    'daily_ingestion_dag',
    default_args=default_args,
    description='A simple ETL job using Bash commands',
) as dag:
    t1 = BashOperator(
                task_id="t1",
                bash_command="echo 'This is task no1 '",
            )
    t2 = BashOperator(
                task_id="t2",
                bash_command="echo 'This is task no2 '",
            )
t1 &gt;&gt; t2</pre></li>
<li>With the <a id="_idIndexMarker776"/>DAG written, let’s enable it on the Airflow UI, and the DAG should run immediately. After running, the DAG will have a <code>SUCCESS</code> status, as follows:</li>
</ol>
<div><div><img alt="Figure 11.4 –  daily_ingestion_dag DAG in the Airflow UI" height="502" src="img/Figure_11.04_B19453.jpg" width="1620"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.4 – daily_ingestion_dag DAG in the Airflow UI</p>
<p>If we <a id="_idIndexMarker777"/>check the logs, it will show the <code>echo</code> command output, similar to the following:</p>
<pre class="source-code">
<strong class="bold">[2023-04-12, 19:54:38 UTC] [ 1686 - airflow.hooks.subprocess.SubprocessHook ] {subprocess.py:74} INFO - Running command: ['bash', '-c', "echo 'This is task no2 '"]</strong>
<strong class="bold">[2023-04-12, 19:54:38 UTC] [ 1686 - airflow.hooks.subprocess.SubprocessHook ] {subprocess.py:85} INFO - Output:</strong>
<strong class="bold">[2023-04-12, 19:54:38 UTC] [ 1686 - airflow.hooks.subprocess.SubprocessHook ] {subprocess.py:92} INFO - This is task no2</strong></pre>
<ol>
<li value="5">Now, we need to ensure the DAG will run daily. To confirm this, select the <strong class="bold">Calendar</strong> option on your DAG page. You will see something similar to this:</li>
</ol>
<div><div><img alt="Figure 11.5 – DAG’s Calendar visualization in the Airflow UI" height="308" src="img/Figure_11.05_B19453.jpg" width="1178"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.5 – DAG’s Calendar visualization in the Airflow UI</p>
<p>As you<a id="_idIndexMarker778"/> can see, the execution is depicted in the shaded region to the left, indicating a successful outcome (<code>end_date</code>, are marked with a dot inside, indicating the job will run every day for the next few days.</p>
<p class="callout-heading">Note</p>
<p class="callout"><em class="italic">Figure 11</em><em class="italic">.5</em> shows some days when the job was executed successfully. This is shown to users how the same calendar behaves on previous executions.</p>
<h2 id="_idParaDest-402"><a id="_idTextAnchor409"/>How it works…</h2>
<p>Airflow’s scheduler is mainly defined by three parameters: <code>start_date</code>, <code>end_date</code>, and <code>schedule_interval</code>. These three parameters define the beginning and end of the job and the interval between executions.</p>
<p>Let’s take a look at <code>default_args</code>:</p>
<pre class="source-code">
default_args = {
    'owner': 'airflow',
    ...
    'start_date': datetime(2023, 4, 12),
    'end_date': datetime(2023, 4, 30),
    'schedule_interval': '@daily
}</pre>
<p>Since I am writing this exercise on April 12, 2023, I set my <code>start_date</code> parameter to the same day. This will make the job retrieve information relating to April 12, and if I put it a few days before the current date, Airflow will retrieve the earlier date. Don’t worry about it now; we will cover more about this in the <em class="italic">Scheduling historical data </em><em class="italic">ingestion recipe</em>.</p>
<p>The key here is the <code>schedule_interval</code> parameter. As the name suggests, this parameter will define the periodicity or the interval of each execution, and, as you can observe, it was simply set using the <code>@</code><code>daily</code> value.</p>
<p>The <strong class="bold">Calendar</strong> option on the DAG UI page is an excellent feature of Airflow 2.2 onward. This functionality allows the developers to see the next execution days for the DAG, preventing<a id="_idIndexMarker779"/> some confusion.</p>
<h2 id="_idParaDest-403"><a id="_idTextAnchor410"/>There's more…</h2>
<p>The DAG parameters are not limited to the ones we have seen in this recipe. Many others are available to make the data pipeline even more automated and intelligent. Let’s take a look at the following code:</p>
<pre class="source-code">
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email': ['airflow@example.com'],
    'email_on_failure': True,
    'email_on_retry': True,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    'start_date': datetime(2023, 4, 12),
    'end_date': datetime(2023, 4, 30),
    'schedule_interval': '@daily,
    'queue': 'bash_queue',
    'pool': 'backfill',
    'priority_weight': 10
}</pre>
<p>There are three additional parameters here: <code>queue</code>, <code>pool</code>, and <code>priority_weight</code>. As we saw in <a href="B19453_09.xhtml#_idTextAnchor319"><em class="italic">Chapter 9</em></a> and <a href="B19453_10.xhtml#_idTextAnchor364"><em class="italic">Chapter 10</em></a>, the Airflow architecture includes a queue (usually executed by <code>pool</code> parameter limits the number of simultaneous jobs. Finally, <code>priority_weight</code>, as the name suggests, defines the priority of a DAG over other DAGs.</p>
<p>You can read more about these parameters in the Airflow<a id="_idIndexMarker781"/> official documentation here:</p>
<p><a href="https://airflow.apache.org/docs/apache-airflow/1.10.2/tutorial.xhtml">https://airflow.apache.org/docs/apache-airflow/1.10.2/tutorial.xhtml</a></p>
<h2 id="_idParaDest-404"><a id="_idTextAnchor411"/>See also</h2>
<p>You can read more about scheduling with crontab<a id="_idIndexMarker782"/> also at <a href="https://crontab.guru/">https://crontab.guru/</a>.</p>
<h1 id="_idParaDest-405"><a id="_idTextAnchor412"/>Scheduling historical data ingestion</h1>
<p>Historical data is<a id="_idIndexMarker783"/> vital for data-driven decisions, providing valuable insights and supporting decision-making processes. It can also refer to data that has been accumulated over a period of time. For example, a sales company can use historical data from previous marketing campaigns to see how they have influenced the sales of a specific product over the years.</p>
<p>This exercise will show how to create a scheduler in Airflow to ingest historical data using the best practices and common concerns related to this process.</p>
<h2 id="_idParaDest-406"><a id="_idTextAnchor413"/>Getting ready</h2>
<p>Please refer to the <em class="italic">Technical requirements</em> section for this recipe since we will handle it with the same technology mentioned here.</p>
<p>In this exercise, we will create a simple DAG inside our DAGs folder. The structure of your Airflow folder should look like the following:</p>
<div><div><img alt="Figure 11.6 – historical_data_dag folder structure in your local Airflow directory" height="242" src="img/Figure_11.06_B19453.jpg" width="321"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.6 – historical_data_dag folder structure in your local Airflow directory</p>
<h2 id="_idParaDest-407"><a id="_idTextAnchor414"/>How to do it…</h2>
<p>Here are the<a id="_idIndexMarker784"/> steps for this recipe:</p>
<ol>
<li>Let’s start by importing our libraries:<pre class="source-code">
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime, timedelta</pre></li>
<li>Now, let’s define <code>default_args</code>. Since we wish to process old data, I will set <code>datetime</code> for <code>start_date</code> before the current day, and <code>end_date</code> will be near the current day. See the following code:<pre class="source-code">
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email': ['airflow@example.com'],
    'email_on_failure': True,
    'email_on_retry': True,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    'start_date': datetime(2023, 4, 2),
    'end_date': datetime(2023, 4, 10),
    'schedule_interval': '@daily'
}</pre></li>
<li>Then, we will create a simple function to print the date Airflow used to execute the pipeline. You can see it here:<pre class="source-code">
def my_task(execution_date=None):
    print(f"execution_date:{execution_date}")</pre></li>
<li>Finally, we<a id="_idIndexMarker785"/> will declare our DAG parameters and a <code>PythonOperator</code> task to execute it, as you can see here:<pre class="source-code">
with DAG(
    'historical_data_dag',
    default_args=default_args,
    description='A simple ETL job using Python commands to retrieve historical data',
) as dag:
    p1 = PythonOperator(
                task_id="p1",
                python_callable=my_task,
        )
p1</pre></li>
<li>Heading to the Airflow UI, let’s proceed with the usual steps to enable the DAG and see its execution. On the <code>historical_data_dag</code> page, you should see something similar to the following screenshot:</li>
</ol>
<div><div><img alt="Figure 11.7 – historical_data_dag DAG in the Airflow UI" height="527" src="img/Figure_11.07_B19453.jpg" width="1619"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.7 – historical_data_dag DAG in the Airflow UI</p>
<p>As you can see, the task ran with success.</p>
<ol>
<li value="6">Now, let’s check <a id="_idIndexMarker786"/>our <code>logs</code> folder. If we select the folder with the same name as the DAG we created (<code>historical_data_dag</code>), we will observe <code>run_id </code>instances on different days, beginning on April 2 and finishing on April 10:</li>
</ol>
<div><div><img alt="Figure 11.8 – Airflow logs folder showing retroactive ingestion" height="290" src="img/Figure_11.08_B19453.jpg" width="467"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.8 – Airflow logs folder showing retroactive ingestion</p>
<ol>
<li value="7">Let’s open the<a id="_idIndexMarker787"/> first <code>run_id</code> folder to explore the log for that run:</li>
</ol>
<div><div><img alt="Figure 11.9 – DAG log for April 2, 2023" height="56" src="img/Figure_11.09_B19453.jpg" width="451"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.9 – DAG log for April 2, 2023</p>
<p>The log tells us the <code>execution_date</code> parameter, which is the same as the <code>start_date</code> parameter.</p>
<p>Here is a closer look at the logs:</p>
<pre class="source-code">
<strong class="bold">[2023-04-12 20:10:25,205] [ ... ] {logging_mixin.py:115} INFO - execution_date:2023-04-02T00:00:00+00:00</strong>
<strong class="bold">[2023-04-12 20:10:25,205] [ ... ] {python.py:173} INFO - Done. Returned value was: None</strong></pre>
<p>We will observe the same pattern for the <code>run_id</code> for April 3:</p>
<div><div><img alt="Figure 11.10 – DAG log for April 3, 2023" height="51" src="img/Figure_11.10_B19453.jpg" width="446"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.10 – DAG log for April 3, 2023</p>
<p>Here is a closer look at the log output:</p>
<pre class="source-code">
<strong class="bold">2023-04-12 20:10:25,276] [ ... ] {logging_mixin.py:115} INFO - execution_date:2023-04-03T00:00:00+00:00</strong>
<strong class="bold">[2023-04-12 20:10:25,276] [...] {python.py:173} INFO - Done. Returned value was: None</strong></pre>
<p>The <code>execution_date</code> also refers to April 3.</p>
<p>This shows us that Airflow has used the interval declared on <code>start_date</code> and <code>end_date</code> to run the task!</p>
<p>Now, let’s proceed to understand how the scheduler works.</p>
<h2 id="_idParaDest-408"><a id="_idTextAnchor415"/>How it works…</h2>
<p>As we saw, scheduling<a id="_idIndexMarker788"/> and retrieving historical data with Airflow is straightforward, and the key parameters were <code>start_date</code>, <code>end_date</code>, and <code>schedule_interval</code>. Let’s discuss them in a little more detail:</p>
<ul>
<li>The <code>start_date</code> parameter defines the first date Airflow will look at when the pipeline is triggered. In our case, it was April 2.</li>
<li>Next is <code>end_date</code>. Usually, this is not a mandatory parameter, even for recurrent ingests. However, the purpose of using it was to show that we can set a date as a limit to stop the ingestion.</li>
<li>Finally, <code>schedule_interval</code> dictates the interval between two dates. We used a daily interval in this exercise, but we could also use <code>crontab</code> if we needed more granular historical ingestion. We will explore this in more detail in the <em class="italic">Setting up the schedule_interval </em><em class="italic">parameter</em> recipe.</li>
</ul>
<p>With this information, it is easier to understand the logs we got from Airflow:</p>
<div><div><img alt="Figure 11.11 – Airflow logs folder showing historic ingestion" height="290" src="img/Figure_11.11_B19453.jpg" width="469"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.11 – Airflow logs folder showing historic ingestion</p>
<p>Each folder represents one historical ingestion that occurred at a daily interval. Since we did not define a more granular date-time specification, the folder name uses the time that the job was triggered. This information is not included in the logs.</p>
<p>To show what date Airflow was using behind the scenes, we created a simple function:</p>
<pre class="source-code">
def my_task(execution_date=None):
    print(f"execution_date:{execution_date}")</pre>
<p>The only purpose of the function is to show the execution date of the task. The <code>execution_date</code> parameter is an internal parameter that displays when a task is executed and can be used by operators or other functions to execute something based on a date.</p>
<p>For example, let’s say we need to retrieve historical data stored as a partition. We can use <code>execution_date</code> to pass the date-time information to a Spark function, which will read and retrieve data from that partition with the same date information.</p>
<p>As you can see, retrieving historical data/information in Airflow requires a few configurations. A good practice is to have a separate and dedicated DAG for historical data processing so that current data ingestion is not impaired. Also, if it is necessary to reprocess data, we<a id="_idIndexMarker789"/> can do it with a few parameter changes.</p>
<h2 id="_idParaDest-409"><a id="_idTextAnchor416"/>There's more…</h2>
<p>Inside the technique of ingesting historical data using Airflow are two important concepts: <em class="italic">catchup</em> and <em class="italic">backfill</em>.</p>
<p>Scheduling and running DAGs for past periods that may have been missed for various reasons is commonly known as <code>schedule_interval</code>. By default, this feature is enabled in Airflow. Therefore, if a paused or uncreated DAG’s <code>start_date</code> lies in the past, it will be automatically scheduled and executed for missed time intervals. The following diagram illustrates this:</p>
<div><div><img alt="Figure 11.12 – Airflow catchup timeline. Source: https://medium.com/nerd-for-tech/airflow-catchup-backfill-demystified-355def1b6f92" height="372" src="img/Figure_11.12_B19453.jpg" width="643"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.12 – Airflow catchup timeline. Source: https://medium.com/nerd-for-tech/airflow-catchup-backfill-demystified-355def1b6f92</p>
<p>On the other hand, Airflow’s <em class="italic">backfill</em> functionality<a id="_idIndexMarker791"/> allows you to execute DAGs retroactively, along with their associated tasks for past periods that may have been missed due to the DAG being paused, not yet created, or for any other reason. Backfilling in Airflow is a powerful feature that helps you to fill the gaps and catch up on data processing or workflow execution that may have been missed in the past.</p>
<p>You can read more about it on <em class="italic">Amit Singh Rathore’s</em> blog page here: <a href="https://medium.com/nerd-for-tech/airflow-catchup-backfill-demystified-355def1b6f92">https://medium.com/nerd-for-tech/airflow-catchup-backfill-demystified-355def1b6f92</a>.</p>
<h1 id="_idParaDest-410"><a id="_idTextAnchor417"/>Scheduling data replication</h1>
<p>In the first chapter of this<a id="_idIndexMarker792"/> book, we covered what data replication is and why it’s important. We saw how vital this process is in the prevention of data loss and in promoting recovery from disasters.</p>
<p>Now, it is time to learn how to create an optimized schedule window to make data replication happen. In this recipe, we will create a diagram to help us decide the best moment to replicate our data.</p>
<h2 id="_idParaDest-411"><a id="_idTextAnchor418"/>Getting ready</h2>
<p>This exercise does not require technical preparation. However, to make it closer to a real scenario, let’s imagine we need to decide the best way to ensure the data from a hospital is being adequately replicated.</p>
<p>We will have two pipelines: one holding patient information and another with financial information. The first pipeline collects information from a patient database and synthesizes it into readable reports used by the medical team. The second will feed an internal dashboard used by the hospital executives.</p>
<p>Due to infrastructure limitations, the operations team has only one requirement: only one pipeline can have its data replicated quickly.</p>
<h2 id="_idParaDest-412"><a id="_idTextAnchor419"/>How to do it…</h2>
<p>Here are the <a id="_idIndexMarker793"/>steps for this recipe:</p>
<ol>
<li><strong class="bold">Identify the targets to replicate</strong>: As described in the <em class="italic">Getting ready</em> section, we have identified the target data, which are the pipeline holding patient information, and the pipeline with financial data to feed a dashboard.</li>
</ol>
<p>However, suppose this information is not coming promptly from stakeholders or other relevant people. In that case, we must always start by identifying our project’s most critical tables or databases.</p>
<ol>
<li value="2"><strong class="bold">Replication periodicity</strong>: We must define the replication schedule based on our data’s criticality or relevance. Let’s take a look at the following diagram:</li>
</ol>
<div><div><img alt="Figure 11.13 – Periodicity of data replication" height="494" src="img/Figure_11.13_B19453.jpg" width="954"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.13 – Periodicity of data replication</p>
<p>As we can see, the<a id="_idIndexMarker794"/> more critical the data is, the more frequently the replication is recommended. In our scenario, the patient reports would fit better with 30 minutes to 3 hours after the ingestion, while the financial data can be replicated until 24 hours have passed.</p>
<ol>
<li value="3"><strong class="bold">Set a schedule window for replication</strong>: Now, we need to create a schedule window to replicate the data. This replication decision needs to take into consideration two important factors, as you can see in the following diagram:</li>
</ol>
<div><div><img alt="Figure 11.14 – Replication window" height="318" src="img/Figure_11.14_B19453.jpg" width="698"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.14 – Replication window</p>
<p>Based on the two pipelines (and remembering we need to prioritize one), the suggestion would be to replicate the financial data every day after business working hours, while the patient data can be done at the same time as new information arrives.</p>
<p>Don’t worry if this <a id="_idIndexMarker795"/>seems a bit confusing. Let’s explore the details in the <em class="italic">How it </em><em class="italic">works</em> section.</p>
<h2 id="_idParaDest-413"><a id="_idTextAnchor420"/>How it works…</h2>
<p>Data replication is a vital process that ensures data availability and disaster recovery. Its concept is older than the current ETL process and has been used for many years in on-premises databases. Our advantage today is that we can carry out this process at any moment. In contrast, replication had a strict schedule window a few years ago due to hardware limitations.</p>
<p>In our example, we handled two pipelines that had distinct severity levels. The idea behind this is to teach attentive eyes to decide when doing each replication.</p>
<p>The first pipeline, which is the patient reports pipeline, handles sensitive data such as personal information and medical history. It also may be helpful for doctors and other health workers to help a patient.</p>
<p>Based on this, the best approach is to replicate this data within a few minutes or hours of it being processed, allowing high availability and redundancy.</p>
<p>At first look, the financial data seems to be very critical and demands fast replication; we need to remember this pipeline feeds data to a dashboard, and therefore, an analyst can use the raw data to generate reports.</p>
<p>The decision to schedule data replication must consider other factors besides the data involved. It is also essential to understand who is interested in or needs to access the data and how it <a id="_idIndexMarker796"/>impacts the project, area, or business when it becomes unavailable.</p>
<h2 id="_idParaDest-414"><a id="_idTextAnchor421"/>There's more…</h2>
<p>This recipe covered a simple example of setting a scheduling agenda for data replication. We also covered in <em class="italic">step 3</em> the two main points to have in mind when doing so. Nevertheless, many other factors can influence the scheduler’s performance and execution. A few examples are as follows:</p>
<ul>
<li>Where Airflow (or a similar platform) is hosted on a server</li>
<li>The CPU capacity</li>
<li>The number of schedulers</li>
<li>Networking throughput</li>
</ul>
<p>If you want to know more about it, you can find a complete list of these factors in the Airflow documentation: <a href="https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/scheduler.xhtml#what-impacts-scheduler-s-performance">https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/scheduler.xhtml#what-impacts-scheduler-s-performance</a>.</p>
<p>The great thing about this documentation is that many points also apply to other data pipeline processors and can serve as a guide.</p>
<h1 id="_idParaDest-415"><a id="_idTextAnchor422"/>Setting up the schedule_interval parameter</h1>
<p>One of the most widely used <a id="_idIndexMarker797"/>parameters in Airflow DAG scheduler configuration is <code>schedule_interval</code>. Together with <code>start_date</code>, it creates a dynamic and continuous trigger for the pipeline. However, there are some small details we need to pay attention to when setting <code>schedule_interval</code>.</p>
<p>This recipe will cover different forms to set up the <code>schedule_interval</code> parameter. We will also explore a practical example to see how the scheduling window works in Airflow, making it more straightforward to manage pipeline executions.</p>
<h2 id="_idParaDest-416"><a id="_idTextAnchor423"/>Getting ready</h2>
<p>While this exercise does not require any technical preparation, it is recommended to take notes about when the pipeline is supposed to start and the interval between each trigger.</p>
<h2 id="_idParaDest-417"><a id="_idTextAnchor424"/>How to do it…</h2>
<p>Here, we will show only the <code>default_args</code> dictionary to avoid code redundancy. However, you can always check out the complete code in the GitHub repository: <a href="https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_11/settingup_schedule_interval">https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_11/settingup_schedule_interval</a>.</p>
<p>Let’s see how we can declare <code>schedule_interval</code>:</p>
<ul>
<li><code>schedule_interval</code> value is by using accessible names such as <code>@daily</code>, <code>@hourly</code>, or <code>@weekly</code>. See what it looks like in the following code:<pre class="source-code">
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    'start_date': datetime(2023, 4, 2),
    'schedule_interval': '@daily'
}</pre></li>
<li><code>schedule_interval</code> using <a id="_idIndexMarker800"/>crontab notation:<pre class="source-code">
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    'start_date': datetime(2023, 4, 2),
    'schedule_interval': '0 22 * * 1-5'
}</pre></li>
</ul>
<p>In this case, we set the scheduler to start every weekday, from Monday to Friday, at 10 pm (or 22:00 hours).</p>
<ul>
<li><code>schedule_interval</code> is by using the <code>timedelta</code> method. In the following code, we can set the pipeline to trigger with an interval of one day between each execution:<pre class="source-code">
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    'start_date': datetime(2023, 4, 2),
    'schedule_interval': timedelta(days=1)
}</pre></li>
</ul>
<h2 id="_idParaDest-418"><a id="_idTextAnchor425"/>How it works…</h2>
<p>The <code>schedule_interval</code> parameter is <a id="_idIndexMarker802"/>an essential aspect of scheduling DAGs in Airflow and provides a flexible way to define how frequently your workflows should be executed. We can think of it as the core of the Airflow scheduler.</p>
<p>The goal of this recipe was to show the different ways to set <code>schedule_interval</code> and when to use each of them. Let’s explore them in more depth:</p>
<ul>
<li><strong class="bold">Friendly names:</strong> As the name suggests, this notation uses user-friendly labels or aliases. It provides an easy and<a id="_idIndexMarker803"/> convenient way to specify the exact time and date for scheduled tasks to run. It can be an easy and simple solution if you don’t have a specific date-time to run the scheduler.</li>
<li><strong class="bold">Crontab notation:</strong> Crontabs have long been widely used across applications and systems. Crontab notation<a id="_idIndexMarker804"/> consists of five fields, representing the minute, hour, day of the month, month, and day of the week. It is a great choice when handling complex schedules, for example, executing the trigger at 1 p.m. on Mondays and Fridays, or other combinations.</li>
<li><code>timedelta(minutes=5)</code>). It is also a user-friendly notation but with more granular <a id="_idIndexMarker806"/>power.</li>
</ul>
<p class="callout-heading">Note</p>
<p class="callout">Although we have seen three ways to set the <code>schedule_interval</code> here, remember Airflow is not a streaming solution, and having multiple DAGs running with a small interval between them can overload the server. Consider using a streaming tool if you or your team needs to schedule ingestions every 10-30 minutes, or less.</p>
<h2 id="_idParaDest-419"><a id="_idTextAnchor426"/>See also</h2>
<p><em class="italic">TowardsDataScience</em> has a fantastic blog post about <a id="_idIndexMarker807"/>how <code>schedule_interval</code> works behind the scenes. You can find it here: <a href="https://towardsdatascience.com/airflow-schedule-interval-101-bbdda31cc463">https://towardsdatascience.com/airflow-schedule-interval-101-bbdda31cc463</a>.</p>
<h1 id="_idParaDest-420"><a id="_idTextAnchor427"/>Solving scheduling errors</h1>
<p>At this point, you may have already<a id="_idIndexMarker808"/> experienced some issues with scheduling pipelines not being triggered as expected. If not, don’t worry; it will happen sometime and is totally normal. With several pipelines running in parallel, in different windows, or attached to different timezones, it is expected to be entangled with one or another.</p>
<p>To avoid this entanglement, in this exercise, we will create a diagram to assist in the debugging process, identify the possible causes of a scheduler not working correctly in Airflow, and see how to solve it.</p>
<h2 id="_idParaDest-421"><a id="_idTextAnchor428"/>Getting ready</h2>
<p>This recipe does not require any technical preparation. Nevertheless, taking notes and writing down the steps we will follow here can be helpful. Writing when learning something new can help to fix the knowledge in our minds, making it easier to remember later.</p>
<p>Back to our exercise; scheduler errors in Airflow typically give the DAG status <code>None</code>, as shown here:</p>
<div><div><img alt="Figure 11.15 – DAG in the Airflow UI with an error in the scheduler" height="410" src="img/Figure_11.15_B19453.jpg" width="1603"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.15 – DAG in the Airflow UI with an error in the scheduler</p>
<p>We will now find out how to fix the error and make the job run again.</p>
<h2 id="_idParaDest-422"><a id="_idTextAnchor429"/>How to do it…</h2>
<p>Let’s try to<a id="_idIndexMarker809"/> identify what could be the cause of the error in our scheduler. Don’t worry about understanding why we used the approaches that we have. We will cover them in detail in <em class="italic">How </em><em class="italic">it works</em>:</p>
<ol>
<li>We can first check whether <code>start_date</code> has been set to <code>datetime.now()</code>. If this is the case, the best approach here is to change this parameter value to a specific date, as you can see here:</li>
</ol>
<div><div><img alt="Figure 11.16 – Error caused by start_date parameter" height="192" src="img/Figure_11.16_B19453.jpg" width="325"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.16 – Error caused by start_date parameter</p>
<p>The code will look like this:</p>
<pre class="source-code">
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    'start_date': datetime(2023, 4, 2),
    'schedule_interval': '@daily'
}</pre>
<ol>
<li value="2">Now we can verify <a id="_idIndexMarker810"/>whether <code>schedule_interval</code> is aligned with the <code>start_date</code> parameter. In the following diagram, you can see three possibilities to fix the issue:</li>
</ol>
<div><div><img alt="Figure 11.17 – Error caused by the start_date and schedule_interval parameters" height="209" src="img/Figure_11.17_B19453.jpg" width="910"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.17 – Error caused by the start_date and schedule_interval parameters</p>
<p>You can prevent this error by using crontab notation in <code>schedule_interval</code>, as follows:</p>
<pre class="source-code">
schedule_interval='0 2 * * *'</pre>
<p>If you are facing problems with the timezone, you can define which timezone Airflow will trigger the job in by using the <code>pendulum</code> library:</p>
<pre class="source-code">
pendulum.now("Europe/Paris")</pre>
<ol>
<li value="3">Finally, another standard error scenario is when <code>schedule_interval</code> changes after the DAG has been running for some time. In this case, the solution usually is to recreate the DAG:</li>
</ol>
<div><div><img alt="Figure 11.18 – Error caused by a change in schedule_interval" height="213" src="img/Figure_11.18_B19453.jpg" width="339"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.18 – Error caused by a change in schedule_interval</p>
<p>At the end of these steps, we will end up with a debug diagram similar to this:</p>
<div><div><img alt="Figure 11.19 – Diagram to assist in identifying an error caused in the Airflow scheduler" height="537" src="img/Figure_11.19_B19453.jpg" width="1180"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.19 – Diagram to assist in identifying an error caused in the Airflow scheduler</p>
<h2 id="_idParaDest-423"><a id="_idTextAnchor430"/>How it works…</h2>
<p>As you can see, the goal of<a id="_idIndexMarker811"/> this recipe was to show three different scenarios in which it is common to observe errors related to the scheduler. Errors in the scheduler normally lead to a DAG status as <code>None</code>, as we saw in the <em class="italic">Getting ready</em> section. However, having a trigger that does not behave as expected is also considered an error. Now, let’s explore the three addressed scenarios and their solutions.</p>
<p>The first scenario usually occurs when we want to use the current date for <code>start_date</code>. Although it seems like a good idea to use the <code>datetime.now()</code> function to represent the current date-time, Airflow will not interpret it as we do. The <code>datetime.now()</code> function will create what we call a <em class="italic">dynamic scheduler</em>, and the trigger will never be executed. It happens because the execution schedule uses <code>start_date</code> and <code>schedule_interval</code> to know when to execute the trigger, as you can see here:</p>
<div><div><img alt="Figure 11.20 – Airflow execution scheduler equation" height="47" src="img/Figure_11.20_B19453.jpg" width="643"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.20 – Airflow execution scheduler equation</p>
<p>If we use <code>datetime.now()</code>, it moves<a id="_idIndexMarker812"/> along with time and will never be triggered. We recommend using a static schedule definition, as we saw in <em class="italic">step 1</em>.</p>
<p>A typical error is when <code>start_date</code> and <code>schedule_interval</code> are not aligned. Based on the explanation of <em class="italic">Figure 11</em><em class="italic">.20</em>, we can already imagine why aligning these two parameters and preventing overlapping are so important. As addressed in <em class="italic">step 2</em>, a good way to prevent this is by using crontab notation to set <code>schedule_interval</code>.</p>
<p>A vital topic is the timezones involved in the process. If you look closely at the top of the Airflow UI, you will see a clock and its associated timezone, as follows:</p>
<div><div><img alt="Figure 11.21 – Airflow UI clock with the timezone displayed" height="58" src="img/Figure_11.21_B19453.jpg" width="820"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.21 – Airflow UI clock with the timezone displayed</p>
<p>This indicates that the Airflow server is running in the UTC timezone, and all DAGs and tasks will be executed using the same logic. If you are working in a different timezone and want to ensure it will run according to your timezone, you can use the <code>pendulum</code> library, as you can see here:</p>
<pre class="source-code">
schedule_interval = pendulum.now("Europe/Paris")</pre>
<p><code>pendulum</code> is a third-party Python library that provides easy date-time manipulations using the built-in <code>datetime</code> Python package. You can find out more about it in the <code>pendulum</code> official <a id="_idIndexMarker813"/>documentation: <a href="https://pendulum.eustace.io/">https://pendulum.eustace.io/</a>.</p>
<p>Finally, the last scenario has a straightforward solution: recreate the DAG if <code>schedule_interval</code> changes after some executions. Although this error may not always occur, it is a good practice<a id="_idIndexMarker814"/> to recreate the DAG to prevent further problems.</p>
<h2 id="_idParaDest-424"><a id="_idTextAnchor431"/>There’s more…</h2>
<p>We have provided in this recipe some examples of what you can check if the scheduler is not working, but other common errors in Airflow can happen. You can find out more about this on <em class="italic">Astronomer’s</em> blog page here: <a href="https://www.astronomer.io/blog/7-common-errors-to-check-when-debugging-airflow-dag/">https://www.astronomer.io/blog/7-common-errors-to-check-when-debugging-airflow-dag/</a>.</p>
<p>In the blog, you can find other scenarios where Airflow throws a silent error (or an error without an explicit error message) and how to solve them.</p>
<h1 id="_idParaDest-425"><a id="_idTextAnchor432"/>Further reading</h1>
<ul>
<li><a href="https://airflow.apache.org/docs/apache-airflow/stable/faq.xhtml#what-s-the-deal-with-start-date">https://airflow.apache.org/docs/apache-airflow/stable/faq.xhtml#what-s-the-deal-with-start-date</a></li>
<li><a href="https://se.devoteam.com/expert-view/why-my-scheduled-dag-does-not-runapache-airflow-dynamic-start-date-for-equally-unequally-spaced-interval/">https://se.devoteam.com/expert-view/why-my-scheduled-dag-does-not-runapache-airflow-dynamic-start-date-for-equally-unequally-spaced-interval/</a></li>
<li><a href="https://stackoverflow.com/questions/66098050/airflow-dag-not-triggered-at-schedule-time">https://stackoverflow.com/questions/66098050/airflow-dag-not-triggered-at-schedule-time</a></li>
</ul>
</div>
</div></body></html>