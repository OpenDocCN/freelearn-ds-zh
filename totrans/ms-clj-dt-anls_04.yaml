- en: Chapter 4. Classifying UFO Sightings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we're going to look at a dataset of UFO sightings. Sometimes,
    data analysis begins with a specific question or problem. Sometimes, however,
    it's more nebulous and vague. We'll engage with this UFO sighting dataset, and
    along the way, we'll learn more about data exploration, data visualization, and
    topic modeling before we dive into Naïve Bayesian classification.
  prefs: []
  type: TYPE_NORMAL
- en: This dataset was collected by the **National UFO Reporting Center** (**NUFORC)**,
    and is available at [http://www.nuforc.org/](http://www.nuforc.org/). They have
    included dates, rough locations, shapes, and descriptions of the sightings. We'll
    download and pull in this dataset. We'll see how to extract more structured data
    from messy, free-form text. And from there, we'll see how to visualize, analyze,
    and gain insights into our data.
  prefs: []
  type: TYPE_NORMAL
- en: In the process, we'll discover when is the best time to look for UFOs. We'll
    also learn what their important characteristics are. And we'll learn how to tell
    a description of a possible hoax sighting from one that may be real. In the end,
    hopefully, we'll be better prepared for seeing one of these ourselves. After all,
    we'll know when to look and for what to look.
  prefs: []
  type: TYPE_NORMAL
- en: Getting the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this chapter, actually acquiring the data will be relatively easy. In other
    chapters, this step involves screen scraping, SPARQL, or other data extraction,
    munging, and cleaning techniques. For this dataset, we''ll just download it from
    Infochimps ([http://www.infochimps.com/](http://www.infochimps.com/)). Infochimps
    is a company (and their website) devoted to Big Data and doing more with data
    analysis. They provide a collection of datasets that are online and freely available.
    To download this specific dataset, browse to [http://www.infochimps.com/datasets/60000-documented-ufo-sightings-with-text-descriptions-and-metada](http://www.infochimps.com/datasets/60000-documented-ufo-sightings-with-text-descriptions-and-metada)
    and download the data from the link there, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting the data](img/4139OS_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The data is in a ZIP-compressed file. This expands the files into the `chimps_16154-2010-10-20_14-33-35`
    directory. This contains a file that lists metadata for the dataset as well as
    the data itself in several different formats. For the purposes of this chapter,
    we'll use the **tab separated values** (**TSV**) file. It's similar to a **comma
    separated values** (**CSV**) file, but it uses the tab character as a delimiter
    instead of a comma. This works nicely, because the tab character is used less
    often in text files in general, so it's often possible to use this data format
    without escaping many, if any, fields.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we open the `16154.yaml` file, we''ll see metadata and other information
    about the dataset. And we learn that the fields in the dataset are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sighted_at`: The date (as YYYYMMDD) the sighting happened'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reported_at`: The date the sighting was reported to NUFORC'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`location`: The city and state the event happened in'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`shape`: The shape of the object'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`duration`: The duration the event lasted'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`description`: A longer description of the sighting as a raw text string'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can get a better feel for this data by examining a row from the downloaded
    file. The following table represents what the fields contain for that record:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Field | Value |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `sighted_at` | 19950202 |'
  prefs: []
  type: TYPE_TB
- en: '| `reported_at` | 19950203 |'
  prefs: []
  type: TYPE_TB
- en: '| `location` | Denmark, WI |'
  prefs: []
  type: TYPE_TB
- en: '| `shape` | Cone |'
  prefs: []
  type: TYPE_TB
- en: '| `duration` | 75 min |'
  prefs: []
  type: TYPE_TB
- en: '| `description` | Caller, and apparently several other people, witnessed multiple
    strange craft streaking through the night sky in the vicinity of Denmark and Mirabel,
    WI. Craft were seen to streak overhead, as well as to descend vertically, as fast
    as a meteorite, then stop suddenly just above the ground. During the last 30 minutes
    of the sighting, aircraft, which appeared to be US military craft, were seen either
    pursuing, or chaperoning, the strange craft. The objects were cone shaped, with
    a red nose and a green tail (sic). |'
  prefs: []
  type: TYPE_TB
- en: Browsing through other rows, you will observe that some important fields—shape
    and duration—may be missing data. The description has XML entities and abbreviations
    such as *w/* and *repts*.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see what we can do with that.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we go further, let''s look at the following Leiningen 2 ([http://leiningen.org/](http://leiningen.org/))
    `project.clj` file that we''ll use for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code shows that over the course of this chapter, we'll parse time
    with the `clj-time` library ([https://github.com/clj-time/clj-time](https://github.com/clj-time/clj-time)).
    This provides a rich, robust date and time library. We'll also use ClojureScript
    ([https://github.com/clojure/clojurescript](https://github.com/clojure/clojurescript))
    for the visualizations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our first step in working with this data is to load it from the data file.
    To facilitate this, we''ll read it into a record type that we''ll define just
    to store the UFO sightings. We''ll work with the `model.clj` file placed at `src/ufo_data/`.
    The following is a namespace declaration with the imports and requirements that
    we''ll use in this module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we''ll define the record. It simply lists the same fields that we walked
    through earlier. We also include a few new fields. We''ll use these to parse the
    year, month, and season from the `reported_at` field as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, when we take a row from the TSV file, we''ll need to parse it into one
    of these structures. Because each line of input only has six fields, we''ll make
    sure that it''s padded out to nine fields. We''ll also verify that there are exactly
    six input fields. If there are more or less, we''ll take steps to either further
    pad the fields or to join some of the fields, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Some of the fields (the most important ones, actually) are dates, and we''ll
    want to parse them into valid date objects. To do this, we''ll use the excellent
    `clj-time` library ([https://github.com/clj-time/clj-time](https://github.com/clj-time/clj-time)).
    This provides a more "Clojuresque" interface for the Joda time library ([http://joda-time.sourceforge.net/](http://joda-time.sourceforge.net/)).
    The code that does this takes a custom date format and attempts to parse the dates.
    If any fail, we just fall back on using `nil`. Look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the following function to coerce the raw string date fields into the
    more useful date objects that Joda time provides:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'That''s all that we need to load the data. Now we can write the function that
    will actually take care of reading the data from the file on disk into a sequence
    of records, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Now that we can read in the data, we can start picking it apart and learn about
    the data that we have.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with messy data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first thing that we need to deal with is qualitative data from the `shape`
    and `description` fields.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `shape` field seems like a likely place to start. Let''s see how many items
    have good data for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'So 4 percent of the data does not have the `shape` field set to meaningful
    data. Let''s see what the most popular values for that field are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Interesting! The most frequent shape isn't a shape at all. The values `other`
    and `unknown` also rank pretty high. We can use the `shape` field, but we need
    to keep these things in mind.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing UFO data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll spend a good bit of time visualizing the data, and we''ll use the same
    system that we have in the previous chapters: a bit of HTML, a splash of CSS,
    and a lot of JavaScript, which we''ll generate from ClojureScript.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ve already taken care of the configuration for using ClojureScript in the
    `project.clj` file that I mentioned earlier. The rest of it involves a couple
    of more parts:'
  prefs: []
  type: TYPE_NORMAL
- en: The code to generate the JSON data for the graph. This will be in the `src/ufo_data/analysis.clj`
    file. We'll write this code first.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An HTML page that loads the JavaScript libraries that we'll use—jQuery ([https://jquery.org/](https://jquery.org/))
    and D3 ([http://d3js.org/](http://d3js.org/))—and creates a `div` container in
    which to put the graph itself.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The source code for the graph. This will include a namespace for utilities in
    `src-cljs/ufo-data/utils.cljs` and the main namespace at `src-cljs/ufo-data/viz.cljs`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With these prerequisites in place, we can start creating the graph of the frequencies
    of the different shapes.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to make sure we have what we need for this namespace. This will
    be in the `src/ufo_data/analysis.clj` file. The following code gives the `ns`
    declaration. Most of these dependencies won''t be needed immediately, but we will
    use them at some point in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we''ll define a rather long function that takes the input data. It will
    pull out the `shape` field, remove blanks, break it into words, and count their
    frequencies. A few of the functions that this function uses aren''t listed here,
    but they''re available in the code download for this chapter. Then, the following
    function will remove any shapes that don''t occur at least once, reverse-sort
    them by their frequencies, and finally turn them into map structures in a vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then use the `clojure.data.json` package ([https://github.com/clojure/data.json](https://github.com/clojure/data.json))
    to save it to disk. I saved it to `www/term-freqs.json`. The following is a small
    sample of the first two records:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we need a web page in which to draw the graph. I downloaded a template
    from the HTML 5 Boilerplate project ([http://html5boilerplate.com/](http://html5boilerplate.com/))
    and saved it as `www/term-freqs.html`. I removed almost everything inside the
    `body` tag. I left only the following `div` tag and a string of `script` tags:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This takes care of the HTML page, so we can move on to the ClojureScript that
    will create the graph.
  prefs: []
  type: TYPE_NORMAL
- en: All of the ClojureScript files for this chapter will be in the `src-cljs` directory.
    Under this directory is a tree of Clojure namespaces, similar to how the code
    in `src` is organized for Clojure. Most of the ClojureScript for this chapter
    will be in the `src-cljs/ufo-data/viz.cljs` file. There are a number of utility
    functions in another namespace, but those are primarily boilerplate, and you can
    find them in the code download for this chapter. The following function loads
    the data and creates the graph. We'll walk through it step-by-step.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The part of the function before the highlighting sets up the axes, the scales,
    and the parent SVG element. Then, we load the data from the server. Once it's
    loaded, we set the domains on the axes and draw the axes themselves.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main part of the function is highlighted. This creates the bars in the
    SVG element. All these tasks take place in the following manner:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(selectAll ".bar") (data data)`: This command selects all elements with the
    `bar` class. Currently, there aren''t any elements to select because we haven''t
    created any, but that''s all right. Then it joins those elements with the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`(enter)`: This command starts processing any data rows that don''t have previously
    created `.bar` elements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`(append "rect")`: For each row of data with no `.bar` elements, this command
    appends a `rect` tag to the element.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`(attr "id" #(str "id" (get-shape %))) (attr "class" "bar")`: This line of
    code adds the `ID` and `class` attributes to the rectangle.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`(attr "x" (comp x get-shape)) (attr "y" (comp y get-count))`: This line of
    code populates the *x* and *y* attributes with values from each data row, projected
    onto the graph''s pixel grid.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`(attr "width" (.rangeBand x)) (attr "height" #(- u/height (y (get-count %)))))`:
    This line of code finally sets the height and width for each rectangle.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These commands together create the graph. There''s a little bit of CSS involved,
    also. Refer to the code download for all the details. But in the end, the graph
    looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualizing UFO data](img/4139OS_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This set of files acts as a framework for all of the visualizations and charts
    that we'll see in this chapter. Although bar charts are simple, once in place,
    this framework can be used for much more complex and sophisticated types of graphs.
  prefs: []
  type: TYPE_NORMAL
- en: 'This graph shows us more clearly what the quick frequency dump at the REPL
    also showed us: most of the people listed the shape as *light*. More than twice
    as many people listed the shape of *light* as listed the runner-up, *triangle*.
    In fact, almost one in five observations listed that as the shape.'
  prefs: []
  type: TYPE_NORMAL
- en: Now let's try to get a feel for some other facts about this data.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, when have UFOs been observed? To find this out, we have to group the
    observations by the year from the `sighted-at` field. We group the items under
    each year, and then we save that to graph it. The following are the functions
    in `ufo-data.analysis` that will take care of getting the right data for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we''ve created the graph from this data, the following is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualizing UFO data](img/4139OS_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This graph suggests that the number of observations in the dataset increased
    dramatically in the mid-1990s, and that they have continued to increase. NUFORC,
    the organization that collects the data, was established in 1974\. I was unable
    to discover when they began collecting data online, but the increased widespread
    use of the Internet could also be a factor in the increase in reported sightings.
    Also, wider cultural trends, such as the popularity of X-Files, may have contributed
    to a greater awareness of UFOs during this time period.
  prefs: []
  type: TYPE_NORMAL
- en: As we continue to get to know our data, another interesting distribution is
    looking at the number of sightings each month. The process for getting this data
    is very similar to the process for getting the number of sightings by year, so
    we won't go into that now.
  prefs: []
  type: TYPE_NORMAL
- en: '![Visualizing UFO data](img/4139OS_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The preceding graph shows that the summer, starting in June, is a good time
    to see a UFO. One explanation for this is that during these months, people are
    outside more in the evenings.
  prefs: []
  type: TYPE_NORMAL
- en: Description
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While the *shape* field is important, the *description* has more information.
    Let's see what we can do with it.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s examine a few and see what some of them look like. The following
    example is one that I selected randomly:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Large boomerang shaped invisible object blocked starlight while flying across
    sky. I have a sketch and noted the year was 1999, but did not write down the day.
    The sighting took place in the late evening when it was completely dark and the
    sky was clear and full of stars. Out of the corner of my eye, I noticed movement
    in the sky from the north moving to the south. When I looked closer, however,
    it wasn&rsquo;t an object that I was seeing move, rather it was the disappearance
    and reappearance of stars behind an object. The object itself was black or invisible
    with no lights. Given the area of stars that were blocked out, I would say the
    object was five times larger than a jet. It was completely silent. It was shaped
    like a boomerang only a little more rounded in front rather than triangle and
    a slightly sharper points on the &ldquo;wing&rdquo; tips. Since the object was
    invisible, I can only suggest the shape based on the black area absent of stars
    like a silhouette as it moved across the sky. If the object was indeed five times
    the size of a jet and flying at about the attitude of a jet, then it was moving
    much faster than a jet. I blinked a couple times, looked away and looked back,
    and then followed the object across the remainder of the horizon until it was
    out of sight. In all it took about 8-10 seconds to span the sky and flew at the
    same altitude the whole time. Given the triangular shape, I suppose it could have
    been a low-flying Stealth Bomber that just appeared much larger if flying low.
    But is a Stealth completely silent? Also, Stealth Bombers have three triangles
    pointing backwards from the mid section. The object I saw did not seem to have
    any mid section as such.((NUFORC Note: Witness indicates that date of incident
    is approximate. PD))*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'So we can see that some examples are fairly long, and they may have characters
    encoded as HTML/XML entities (`&ldquo;` and `&rdquo;` in this example). And this
    quote is relatively clean: some have two or more words jammed together with just
    punctuation—often several periods—stuck between the words.'
  prefs: []
  type: TYPE_NORMAL
- en: In order to deal with this data, we'll need to clean it up some and break the
    words out, or tokenize it. You can see the details of this in the code download,
    most of which is just pasting together a lot of string manipulation methods, but
    it's helpful to remind ourselves with what we're working and how we need to deal
    with it. I also filtered on a standard English stop-words list, which I augmented
    by adding a few words that are specific to the *description* fields, such as *PD*
    and *NUFORC*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see what the most frequent words are in the description fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This seems more like what we'd expect. The most frequent word is *object*, which
    seems appropriate for a corpus made up of people talking about things that they
    can't identify. The next two words are *light* and *lights*, which would be expected,
    especially since *light* is the most common item in the *shape* field.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s graph these terms too. We won''t be able to see the details of the words''
    frequencies but it will give us a better feel for their distribution. There are
    enough tokens; however, we''ll only look at the 75 most frequent ones in the following
    graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Description](img/4139OS_04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The distribution of these words seems very similar. In fact, it very roughly
    conforms to Zipf's law, which predicts the power-law distribution of many types
    of physical and social data, including language frequencies.
  prefs: []
  type: TYPE_NORMAL
- en: Topic modeling descriptions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another way to gain a better understanding of the descriptions is to use topic
    modeling. We learned about this text mining and machine learning algorithm in
    [Chapter 3](ch03.html "Chapter 3. Topic Modeling – Changing Concerns in the State
    of the Union Addresses"), *Topic Modeling – Changing Concerns in the State of
    the Union Addresses*. In this case, we'll see if we can use it to create topics
    over these descriptions and to pull out the differences, trends, and patterns
    from this set of texts.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we''ll create a new namespace to handle our topic modeling. We''ll use
    the `src/ufo_data/tm.clj` file. The following is the namespace declaration for
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The process for generating the topic model is very similar to the process that
    we used in [Chapter 3](ch03.html "Chapter 3. Topic Modeling – Changing Concerns
    in the State of the Union Addresses"), *Topic Modeling – Changing Concerns in
    the State of the Union Addresses*. The first change that we need to make is that
    we'll load the instances from the in-memory data that we read earlier in this
    chapter. We'll create a function that pushes an input collection into an array
    and uses `ArrayIterator` to then feed that array into the processing pipeline.
    The function to train the data is the same as it was in the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll look at more functions that help us introspect on the
    trained model, the instances, and the probabilities and keywords that are important
    to each topic. The first function returns the words that apply to a topic and
    their weights. We get the feature vectors from the model, and the words themselves
    from the instance list as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The other reporting function that we''ll use ranks the instances by their probabilities
    for each topic. We can use this to look at the documents that are most likely
    to apply to any particular topic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We can use these functions—as well as a few others based on these—from the REPL
    to explore our data.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, when deciding how many topics to use, we'll want to use some kind
    of objective metric to find a good definition of the sets. However, for exploring
    in an off-the-cuff way, we'll use something more subjective. First, after playing
    around with the number of topics, I chose to use a topic count of twelve. Since
    all of these are really about just one thing, UFO sightings, I didn't expect there
    to be too many meaningful topics, even at a fairly detailed, narrow level. At
    twelve topics, there still seemed to be some vague, less helpful topics, but the
    more interesting topics that I'd seen before were still there. When I attempted
    fewer topics, some of those interesting topics disappeared.
  prefs: []
  type: TYPE_NORMAL
- en: 'So to get started, let''s see the topics and the top 10 words for each. Remember
    that the topic descriptions here aren''t generated by the computer. I came up
    with them after looking at the top words and the top few descriptions for those
    topics. Some of these are not obvious, given the small sample of terms included
    here. However, diving further into the topic terms, the documents themselves gave
    these categorizations. In some cases, I''ve included notes in parentheses as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Remembering childhood experiences**: back time house craft car looked years
    remember road home'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lots of NUFORC notes, thanks to other organizations or local chapters**:
    report witness nuforc note ufo sighting pd date reported object'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bright, silent objects in the sky**: light sky bright lights white star object
    red moving looked'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Visual descriptions**: lights sky light time night red minutes objects back
    bright (this one doesn''t have a clear topic as it''s commonly defined)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**White, red, and reddish-orange lights**: light sky lights looked bright moving
    object back red white'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Very fast, bright objects in the sky, compared to airplanes and meteors**:
    lights sky object aircraft light west north appeared flying south'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NUFORC notes. "Witness elects to remain totally anonymous"**: nuforc note
    pd witness date sky light anonymous remain approximate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vague**: ufo camera air object picture time pictures photo photos day (again,
    the subject of this topic isn''t clear)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Objects in the sky, no lights, or not mentioned**: object driving road car
    lights shaped craft looked side feet'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Abductions, visitations, fear. Close encounters of the fourth kind**: time
    night back looked light house thing window lights sound'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sightings. Moving in different directions**: lights object craft light flying
    white north south east moving'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Technical descriptions**: object sky light moving objects appeared bright
    time high north'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Several of these topics, for instance, the third, fifth, sixth, and ninth bullet,
    seem to be pretty generic descriptions of sightings. They describe lots of moving
    lights in the sky.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other topics are more interesting. Topic one contained a number of descriptions
    written by people looking back at their childhood or college years. For instance,
    in the following paragraph, someone describes having a close encounter when they
    were about six years old. There are a number of spelling mistakes, and part of
    the reason I''ve kept it in is to illustrate just how messy this data can be:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Blus light, isolated road, possible missing timeI was six years old at the
    time, and even now, if I concentrate, I can recall what happened. My mother, her
    best friend, and myself were driving on a section of road called "Grange Road."
    Today, there are a lot of houses, but at the time, it was all farmland with maybe
    one or two houses. It was just after midnight, and I remember waking up. I was
    alseep in the back seat, and I woke up feeling very frightened. I sat up, and
    my mother and her friend were obviously worried. The car we were in was cutting
    in-and-out, and finally died. As soon as the car stopped, we all saw a blue light
    directly ahead, maybe about 20 feet off of the ground, and about a football field
    legnth away. It glided towards us, made no noise, and as it got to within 15 feet,
    it stopped in midair, hoovering. My mom grabbed me from the backseat and held
    on, and her friend was crying. I was crying, too, because whatever it was, it
    was making us all upset. After about five minutes, I don''t recall what happened,
    because for whatever reason, I fell alseep. Weird, I know, but I swear it happened.
    I woke up sometime later, and we three were sitting there, shocked, and the light
    was gone. My mom and her friend - to this day - swear they had missing time, about
    10 minutes worth. I hope this helps...((NUFORC Note: Witness indicates that date
    of sighting is approximate. PD))*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: And some topics are puzzling, number eight, for instance. The top 10 documents
    for it had nothing obvious that appeared to make them a coherent subject. There
    may be something about some of the subtler vocabulary selection that was getting
    identified, but it wasn't readily apparent.
  prefs: []
  type: TYPE_NORMAL
- en: Hoaxes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the most interesting finds in this was topic seven. This topic was focused
    on annotations added to the descriptions for which the witnesses wished to remain
    anonymous. But its most likely document was the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Round, lighted object over Shelby, NC, hovered then zoomed away. It was my
    birthday party and me and my friends were walking around the block about 21:30\.
    I just happened to look up and I saw a circular object with white and bright blue
    lights all over the bottom of it. It hovered in place for about 8 seconds then
    shot off faster than anything I have ever seen.((NUFORC Note: Witness elects to
    remain totally anonymous; provides no contact information. Possible hoax?? PD))((NUFORC
    Note: Source of report indicates that the date of the sighting is approximate.
    PD))*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: What caught my attention was the note "Possible hoax??" Several other descriptions
    in this topic had similar notes, often including the word *hoax*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finding this raised an interesting possibility: could we train a classifier
    to recognize possible hoaxes? My initial reaction was to be skeptical. But I still
    thought it would be an interesting experiment.'
  prefs: []
  type: TYPE_NORMAL
- en: Eventually, we'll want to load this data and process it with MALLET ([http://mallet.cs.umass.edu/](http://mallet.cs.umass.edu/)).
    MALLET works a little easier with data that's kept in a particular directory format.
    The template for this is `base-directory/tag/data-file.txt`. In fact, we'll include
    a directory above these, and for `base-directory`, we'll define a directory for
    training data and one for test data.
  prefs: []
  type: TYPE_NORMAL
- en: The training group is used to train the classifier, and the test group is used
    to evaluate the classifier after it's been trained in order to determine how successful
    it is. Having two different groups for these tasks helps to find whether the classifier
    is over-fitting, that is, whether it has learned the training group so well that
    it performs poorly on new data.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So before we get started, we''ll preprocess the data to put it into a directory
    structure such as `src/ufo_data/`. All the code for this will go into the `model.clj`
    file. The namespace declaration for this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, to process this dataset into a form that MALLET can deal with easily,
    we''re going to put it through the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Read the data into a sequence of data records.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split out the NUFORC comments.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Categorize the documents based on the comments.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Partition them into directories based on the categories.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Divide them into training and test sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's see how we'll put these together.
  prefs: []
  type: TYPE_NORMAL
- en: Reading the data into a sequence of data records
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The data in the downloaded file has a number of problems with values that can't
    be escaped properly. I've cleaned this up and made a new data file, available
    at [http://www.ericrochester.com/clj-data-master/data/ufo.json](http://www.ericrochester.com/clj-data-master/data/ufo.json).
    I've saved this into my `data` directory and bound that path to the name `*data-file*`.
    You can find this and a few other definitions in the code download for this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'But primarily, I''d like to focus on the data record for a minute. This just
    contains the fields from the JSON objects being read in. The following definition
    will serve as documentation of our data and make working with the rows a little
    easier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The data as we read it in from the JSON file won''t be quite right, however.
    We''ll still need to convert date strings into data objects. We''ll do that with
    `read-date`, which parses a single date string, and with `coerce-fields`, which
    coordinates the calling of `read-date` on the appropriate fields in `UfoSighting`,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can use these functions to read and parse each line of the input data
    file. As shown in the following code, each line is a separate JSON object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can use these on the REPL to load the data file. As shown in the following
    code, in this session, `model` is bound to `ufo-data.model`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Looks good. We're ready to start processing the descriptions further.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting the NUFORC comments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Many of the descriptions contain comments by NUFORC ([http://www.nuforc.org/](http://www.nuforc.org/)).
    These contain editorial remarks – some of them about the authenticity of the report.
    The following is a sample description with NUFORC commentary:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Telephoned Report:Husband and wife were awakened by a very bright light outside
    their house in Rio Vista area of McCall. It was so bright, it was &quot;like being
    inside a football stadium.&quot; No sound. Ground was covered with snow at the
    time. It lasted for 10 seconds.((NUFORC Note: We spoke with the husband and wife,
    and found them to be quite credible and convincing in their description of what
    they allegedly had seen. Both have responsible jobs. PD))*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'This is a standard format for these comments: They''re enclosed in double parentheses
    and begin with "NUFORC." We can leverage this information, and a regular expression,
    to pull all the notes out of the document.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, we''ll go a little deeper into the Java regular expression API
    than Clojure has utility functions defined to do. Let''s see what we need to do,
    and then we can take it apart after the following code listing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: So first we create a regular expression that picks out text enclosed in double
    parentheses. We also create `java.lang.StringBuffer`. We'll use this to accumulate
    the description of the UFO sighting, with the NUFORC comments stripped out.
  prefs: []
  type: TYPE_NORMAL
- en: The body of the function is a loop that has a single parameter, a vector named
    `accum`. This will accumulate the NUFORC comments.
  prefs: []
  type: TYPE_NORMAL
- en: Inside the loop, every time the regular expression finds a match, we extract
    the NUFORC comment out of the original string and replace the match with an empty
    string in `StringBuffer`. Finally, when there are no more matches on the regular
    expression, we append the rest of the string onto `StringBuffer`, and we can retrieve
    its contents and the comments, joined together.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see what happens when we strip the NUFORC comments from the description
    quoted earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: So we can see that the first item in the pair returned by `split-nuforc` contains
    the description by itself, and the second item is the comments.
  prefs: []
  type: TYPE_NORMAL
- en: Now we can use the comments to categorize the descriptions in the first part.
    And we'll use that to figure out where to save the cleaned-up descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: Categorizing the documents based on the comments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Categorizing the documents is relatively easy. We''ll use a `tokenize` function,
    which can be found in the code download for this chapter, in the namespace `ufo-data.text`
    (which is aliased to `t` in the code). We can convert the words in the comment
    to a set of tokens and then look for the word `"`*hoax*`"`. If found, we''ll categorize
    it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'When called with the tokens of a comment, it returns the category of the description
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Of course, this is very rough, but it should be all right for this experiment.
  prefs: []
  type: TYPE_NORMAL
- en: Partitioning the documents into directories based on the categories
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that they're in categories, we can use those categories to save the descriptions
    into files. Each description will be in its own file.
  prefs: []
  type: TYPE_NORMAL
- en: Initially, we'll put all of the files into one pair of directories. In the next
    step, we'll divide them further into test and training sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first function for this section will take a base directory, a number, and
    the document pair, as returned by `ufo-data.model/split-nuforc`. From there, it
    will save the text to a file and return the file''s category and filename, as
    shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The next function, `make-dirtree-sighting`, will do a lot of the work. It will
    take an instance of `UfoSighting` and will split out the NUFORC commentary, tokenize
    both parts, get the category, and use it to save the filename, as shown in the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This will handle saving each file individually into one pair of directories:
    one for hoaxes and one for non-hoaxes. We''ll want to process all of the UFO sightings,
    however, and we''ll want to divide the two sets of documents into a test set and
    a training set. We''ll do all of this in the next section.'
  prefs: []
  type: TYPE_NORMAL
- en: Dividing them into training and test sets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now, we can divide the data that we have into a training set and a test set.
    We''ll need the following two utility functions to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll need to create subdirectories for the categories several times. Let''s
    put that into the following function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We'll also need to divide a collection into two groups by ratio, as shown in
    the following code. That is, one subgroup will be 80 percent of the original and
    the other subgroup will be 20 percent of the original.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, the function to move a collection of files into a stage''s subdirectory
    (testing or training) will be `mv-stage`. The collection of files is generated
    by `save-document`, so it''s a collection of maps, each containing the category
    and filename of the file, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'To control this whole process, we''ll use `make-dirtree`. This will take a
    collection of instances of `UfoSighting` and process them into separate text files.
    All of the files will be in the `basedir` directory, and then they''ll be divided
    into a training set and a test set. These will be put into sibling directories
    under `basedir` as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s use this to divide out sightings data into groups and save them
    into the `bayes-data` directory as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: We have the data now, and it's in a shape that MALLET can use. Let's look at
    how we're going to leverage that library for Naïve Bayesian classification.
  prefs: []
  type: TYPE_NORMAL
- en: Classifying the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bayesian inference can seem off-putting at first, but at its most basic level,
    it's how we tend to deal with the world. We start out with an idea of how likely
    something is, and then we update that expectation as we receive more information.
    In this case, depending on our background, training, history, and tendencies,
    we may think that all UFO reports are hoaxes or that most of them are. We may
    think that few UFO reports are hoaxes, or we may be completely undecided and assume
    that about half of them are hoaxes and half are true. But as we hear reports that
    we know the truth of, we change our opinions and expectations of the other reports.
    We may notice patterns, too. Hoaxes may talk about green men, while true reports
    may talk about grays. So you may also further refine your intuition based on that.
    Now, when you see a report that talks about little green men, you're more likely
    to think it's a hoax than when you see a report that talks about little gray men.
  prefs: []
  type: TYPE_NORMAL
- en: You may also notice that triangular UFOs are considered hoaxes, while circular
    UFOs are not. Now, when you read another document, this observation then further
    influences your beliefs about whether that document is a hoax or not.
  prefs: []
  type: TYPE_NORMAL
- en: In Bayesian terms, our original expectation that a document is a hoax or not
    is called the **prior or assumed probability**, and its notation is *P(H)*, where
    *H* is the probability that the document is considered a hoax. The updated expectation
    after seeing the color of the aliens in the description, *C*, is called the **conditional
    probability**, and its notation is *P(C|H)*, which is read as *the probability
    of C given H*. In this case, it's the probability distribution over the alien's
    color, given that the document is a hoax.
  prefs: []
  type: TYPE_NORMAL
- en: Bayes' theorem is a way of swapping the conditions for a set of conditional
    probabilities. That is, we can now find *P(H|C)*, or the probability distribution
    over the document's being a hoax, given that the alien is green or gray.
  prefs: []
  type: TYPE_NORMAL
- en: 'The formula to do this is pretty simple. To compute the probability that the
    document is a hoax, given the aliens'' color, consider the following conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: The probability of the aliens' color, given that the document is a hoax or not
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The probability that the document is a hoax
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The probability of the aliens' color.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For Naïve Bayesian classification, we make an important assumption: we assume
    that the features in a document are independent. This means that the probability
    that whether aliens are green or gray in a document is independent of whether
    the UFO is a disk or a triangle.'
  prefs: []
  type: TYPE_NORMAL
- en: In spite of this assumption, Naïve Bayesian classifiers often work well in the
    real world. We can train them easily and quickly, and they classify new data quickly
    and often perform well enough to be useful.
  prefs: []
  type: TYPE_NORMAL
- en: So with that understanding, let's look at how MALLET handles Naïve Bayesian
    classification.
  prefs: []
  type: TYPE_NORMAL
- en: Coding the classifier interface
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before we begin the next part of this chapter, it''s probably a good time to
    start a new namespace for the following code to live in. Let''s put it into the
    `src/ufo_data/bayes.clj` file. The `ns` declaration is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: With the preceding code in place, let's see what we need to do.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the Pipe and InstanceList
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: MALLET processes all input through `Pipe`. Pipes represent a series of transformations
    over the text. When you're working with a classifier, the data that's used for
    training, testing, and later for classifying new documents, all need to be put
    through the same pipe of processes. Also, all of them must use the same set of
    features and labels. MALLET calls these *alphabets*.
  prefs: []
  type: TYPE_NORMAL
- en: Each data document, at whatever stage of processing, is stored in an `Instance`
    object, and corpora of these are kept in `InstanceList`. `Pipe` objects are associated
    with `InstanceList` objects. This makes sure that all `Instance` objects in a
    collection are processed consistently.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to keep things straight, we''ll define `make-pipe-list`. This will
    create the `Pipe` object as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'This processing pipeline performs the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Target2Label` takes the category from the directory path and assigns it to
    the `Instance` object''s label. Labels are the categories or classes used for
    classification.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`SaveDataInSource` takes the path name, which is currently in the data property,
    and puts it into the `Instance` object''s source property.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Input2CharSequence` reads the data from the filename and replaces it with
    the file''s contents.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`CharSequence2TokenSequence` tokenizes the file''s contents.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`TokenSequenceLowercase` converts all uppercase characters in the tokens to
    lowercase.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`TokenSequenceRemoveStoplist` removes common English words so that the classifier
    can focus on content words.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`TokenSequence2FeatureSequence` categorizes the tokens as sequences. Each unique
    word is assigned a unique integer identifier.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`FeatureSequence2AugmentableFeatureVector` converts the sequence of tokens
    into a vector. The token''s feature identifier is that token''s index in the feature
    vector.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: MALLET's classifier expects feature vectors as input, so this is the appropriate
    pipeline to use.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we need to take an input directory, generate `Instance` objects from it,
    and associate their processing with a pipeline. In the following code, we''ll
    use the `add-input-directory` function to do all of that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The regular expression in the last line takes the name of the file's directory
    and uses that as the `Instance` object's classification. We can use these two
    functions to handle the loading and processing of the inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Training
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Training is pretty simple. We create an instance of `NaiveBayesTrainer`. Its
    `train` method returns an instance of `NaiveBayes`, which is the classifier. We''ll
    wrap this in the following function to make it slightly easier to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Wrapping it in this way provides a Clojure-native way of dealing with this library.
    It also keeps users of our module from needing to import `NaiveBayesTrainer` and
    the other classes from MALLET directly.
  prefs: []
  type: TYPE_NORMAL
- en: Classifying
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Just like training, classifying is also easy. The classifier returned by the
    `train` function just defers to the `classify` method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code will return an instance of type `cc.mallet.classify.Classification`.
    This returns not only the best label and the probabilities associated with it,
    but also the probabilities of the other labels and the classifier and document
    instance involved.
  prefs: []
  type: TYPE_NORMAL
- en: Validating
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We can now train a classifier and run it on new documents. We'd like to be able
    to test it as well, by comparing our expectations from preclassified documents
    with how the classifier actually performs.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the lowest level, we''ll want to compare the expected classification with
    the actual classification and keep a count of each pairing of these values. We
    can do that with `validate1`. This gets the expected and actual labels, and it
    creates a vector pair of them. The `confusion-matrix` function then gets the frequency
    of those pairs as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: A confusion matrix is a table with the counts of the correctly classified instances
    (expected and actual match), the false positives (expected is to not classify,
    but the actual is to classify it), and the false negatives (expected is to classify
    the instance, but the actual is to not classify it). This provides an easy-to-comprehend
    overview of the performance of a classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Tying it all together
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the following code, we'll create a `bayes` function that creates, trains,
    and tests a classifier on a directory of data. It will take the hash map of information
    returned by `validate` and add the classifier and the `Pipe` object to it. Having
    the pipe object available later will be necessary to run the classifier on more
    data in the future.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have all the pieces in place, let's see how to run the classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Running the classifier and examining the results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For this section, I've loaded the `ufo-data.bayes` namespace into the REPL and
    aliased it with the name `bayes`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can pass to the `bayes` function the test and training directories that
    we created from the sightings as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s put this into a more traditional form for this information. The expected
    values have their labels across the top of the table. The actual values have theirs
    down the side. Look at the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | Hoax | Non-hoax |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Hoax | 0 | 31 |'
  prefs: []
  type: TYPE_TB
- en: '| Non-hoax | 83 | 12100 |'
  prefs: []
  type: TYPE_TB
- en: Well, that seems pretty useless. Evidently, my previous skepticism was warranted.
    The classifier managed to identify no hoaxes correctly, and it incorrectly identified
    31 non-hoaxes as hoaxes (false positives).
  prefs: []
  type: TYPE_NORMAL
- en: 'But that''s not all that we can learn about this. Instances of `NaiveBayes`
    also include a way to print out the top-weighted words for each category. Let''s
    see what the top 10 words for each classification are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: So the terms are in slightly different order, but the vocabulary describing
    hoaxes and non-hoaxes is almost identical. Both mention *object*, *light*, *lights*,
    *sky*, and *looked*. So, based on the features we've selected here (single-word
    tokens), it's not surprising that we didn't get good results.
  prefs: []
  type: TYPE_NORMAL
- en: However, the primary thing that we can learn is that hoaxes are considered to
    be extremely rare, and the decision that a sighting is a hoax or not is often
    based on external data. Consider the sighting quoted earlier. To support the judgment
    that the sighting is not a hoax, the commenter mentions that they have a stable
    job, even though that's not mentioned in the description itself.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This has been a wandering and hopefully fun trip through the UFO sightings dataset.
    We've learned something about the language used in describing close encounters,
    and we've learned about how to use visualizations, exploratory data analysis,
    and Naïve Bayesian classification to learn more about the data.
  prefs: []
  type: TYPE_NORMAL
- en: But the primary impression I have of this is the feedback analysis, visualization,
    and exploration. The visualization led us to topic modeling, and something we
    discovered there led us to Bayesian classification. This is typical of data analysis,
    where one thing we learn informs and motivates the next stage in the analysis.
    Each answer can raise further questions and drive us back into the data.
  prefs: []
  type: TYPE_NORMAL
