["```py\nresults = spark.sql(\"SELECT * FROM tableName\")\n```", "```py\nsalary_data_with_id = [(1, \"John\", \"Field-eng\", 3500, 40), \\\n    (2, \"Robert\", \"Sales\", 4000, 38), \\\n    (3, \"Maria\", \"Finance\", 3500, 28), \\\n    (4, \"Michael\", \"Sales\", 3000, 20), \\\n    (5, \"Kelly\", \"Finance\", 3500, 35), \\\n    (6, \"Kate\", \"Finance\", 3000, 45), \\\n    (7, \"Martin\", \"Finance\", 3500, 26), \\\n    (8, \"Kiran\", \"Sales\", 2200, 35), \\\n  ]\ncolumns= [\"ID\", \"Employee\", \"Department\", \"Salary\", \"Age\"]\nsalary_data_with_id = spark.createDataFrame(data = salary_data_with_id, schema = columns)\nsalary_data_with_id.show()\n```", "```py\n+---+--------+----------+------+---+\n| ID|Employee|Department|Salary|Age|\n+---+--------+----------+------+---+\n|  1|    John| Field-eng|  3500| 40|\n|  2|  Robert|     Sales|  4000| 38|\n|  3|   Maria|   Finance|  3500| 28|\n|  4| Michael|     Sales|  3000| 20|\n|  5|   Kelly|   Finance|  3500| 35|\n|  6|    Kate|   Finance|  3000| 45|\n|  7|  Martin|   Finance|  3500| 26|\n|  8|   Kiran|     Sales|  2200| 35|\n+---+--------+----------+------+---+\n```", "```py\nsalary_data_with_id.write.format(\"csv\").mode(\"overwrite\").option(\"header\", \"true\").save(\"salary_data.csv\")\n```", "```py\n+---+--------+----------+------+---+\n| ID|Employee|Department|Salary|Age|\n+---+--------+----------+------+---+\n|  1|    John| Field-eng|  3500| 40|\n|  2|  Robert|     Sales|  4000| 38|\n|  3|   Maria|   Finance|  3500| 28|\n|  4| Michael|     Sales|  3000| 20|\n|  5|   Kelly|   Finance|  3500| 35|\n|  6|    Kate|   Finance|  3000| 45|\n|  7|  Martin|   Finance|  3500| 26|\n|  8|   Kiran|     Sales|  2200| 35|\n+---+--------+----------+------+---+\n```", "```py\n# Perform transformations on the loaded data\nprocessed_data = csv_data.filter(csv_data[\"Salary\"] > 3000)\n# Save the processed data as a table\nprocessed_data.createOrReplaceTempView(\"high_salary_employees\")\n# Perform SQL queries on the saved table\nresults = spark.sql(\"SELECT * FROM high_salary_employees \")\nresults.show()\n```", "```py\n+---+--------+----------+------+---+\n| ID|Employee|Department|Salary|Age|\n+---+--------+----------+------+---+\n|  1|    John| Field-eng|  3500| 40|\n|  2|  Robert|     Sales|  4000| 38|\n|  3|   Maria|   Finance|  3500| 28|\n|  5|   Kelly|   Finance|  3500| 35|\n|  7|  Martin|   Finance|  3500| 26|\n+---+--------+----------+------+---+\n```", "```py\n# Save the processed data as a view\nsalary_data_with_id.createOrReplaceTempView(\"employees\")\n#Apply filtering on data\nfiltered_data = spark.sql(\"SELECT Employee, Department, Salary, Age FROM employees WHERE age > 30\")\n# Display the results\nfiltered_data.show()\n```", "```py\n+--------+----------+------+---+\n|Employee|Department|Salary|Age|\n+--------+----------+------+---+\n|    John| Field-eng|  3500| 40|\n|  Robert|     Sales|  4000| 38|\n|   Kelly|   Finance|  3500| 35|\n|    Kate|   Finance|  3000| 45|\n|   Kiran|     Sales|  2200| 35|\n+--------+----------+------+---+\n```", "```py\n# Perform an aggregation to calculate the average salary\naverage_salary = spark.sql(\"SELECT AVG(Salary) AS average_salary FROM employees\")\n# Display the average salary\naverage_salary.show()\n```", "```py\n+--------------+\n|average_salary|\n+--------------+\n|        3275.0|\n+--------------+\n```", "```py\n# Sort the data based on the salary column in descending order\nsorted_data = spark.sql(\"SELECT * FROM employees ORDER BY Salary DESC\")\n# Display the sorted data\nsorted_data.show()\n```", "```py\n+---+--------+----------+------+---+\n| ID|Employee|Department|Salary|Age|\n+---+--------+----------+------+---+\n|  2|  Robert|     Sales|  4000| 38|\n|  1|    John| Field-eng|  3500| 40|\n|  5|   Kelly|   Finance|  3500| 35|\n|  3|   Maria|   Finance|  3500| 28|\n|  7|  Martin|   Finance|  3500| 26|\n|  6|    Kate|   Finance|  3000| 45|\n|  4| Michael|     Sales|  3000| 20|\n|  8|   Kiran|     Sales|  2200| 35|\n+---+--------+----------+------+---+\n```", "```py\n# Sort the data based on the salary column in descending order\nfiltered_data = spark.sql(\"SELECT Employee, Department, Salary, Age FROM employees WHERE age > 30 AND Salary > 3000 ORDER BY Salary DESC\")\n# Display the results\nfiltered_data.show()\n```", "```py\n+--------+----------+------+---+\n|Employee|Department|Salary|Age|\n+--------+----------+------+---+\n|  Robert|     Sales|  4000| 38|\n|   Kelly|   Finance|  3500| 35|\n|    John| Field-eng|  3500| 40|\n+--------+----------+------+---+\n```", "```py\n# Group the data based on the Department column and take average salary for each department\ngrouped_data = spark.sql(\"SELECT Department, avg(Salary) FROM employees GROUP BY Department\")\n# Display the results\ngrouped_data.show()\n```", "```py\n+----------+------------------+\n|Department|       avg(Salary)|\n+----------+------------------+\n| Field-eng|            3500.0|\n|     Sales|3066.6666666666665|\n|   Finance|            3375.0|\n+----------+------------------+\n```", "```py\n# Perform grouping and multiple aggregations\naggregated_data = spark.sql(\"SELECT Department, sum(Salary) AS total_salary, max(Salary) AS max_salary FROM employees GROUP BY Department\")\n# Display the results\naggregated_data.show()\n```", "```py\n+----------+-----------+-----------+\n|Department|sum(Salary)|max(Salary)|\n+----------+-----------+-----------+\n| Field-eng|       3500|       3500|\n|     Sales|       9200|       4000|\n|   Finance|      13500|       3500|\n+----------+-----------+-----------+\n```", "```py\nfunction().over(Window.partitionBy(\"column1\", \"column2\").orderBy(\"column3\").rowsBetween(start, end))\n```", "```py\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import col, sum\n# Define the window specification\nwindow_spec = Window.partitionBy(\"Department\").orderBy(\"Age\")\n# Calculate the cumulative sum using window function\ndf_with_cumulative_sum = salary_data_with_id.withColumn(\"cumulative_sum\", sum(col(\"Salary\")).over(window_spec))\n# Display the result\ndf_with_cumulative_sum.show()\n```", "```py\n+---+--------+----------+------+---+--------------+\n| ID|Employee|Department|Salary|Age|cumulative_sum|\n+---+--------+----------+------+---+--------------+\n|  1|    John| Field-eng|  3500| 40|          3500|\n|  7|  Martin|   Finance|  3500| 26|          3500|\n|  3|   Maria|   Finance|  3500| 28|          7000|\n|  5|   Kelly|   Finance|  3500| 35|         10500|\n|  6|    Kate|   Finance|  3000| 45|         13500|\n|  4| Michael|     Sales|  3000| 20|          3000|\n|  8|   Kiran|     Sales|  2200| 35|          5200|\n|  2|  Robert|     Sales|  4000| 38|          9200|\n+---+--------+----------+------+---+--------------+\n```", "```py\nfrom pyspark.sql.functions import udf\nudf_name = udf(lambda_function, return_type)\n```", "```py\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import StringType\n# Define a UDF to capitalize a string\ncapitalize_udf = udf(lambda x: x.upper(), StringType())\n# Apply the UDF to a column\ndf_with_capitalized_names = salary_data_with_id.withColumn(\"capitalized_name\", capitalize_udf(\"Employee\"))\n# Display the result\ndf_with_capitalized_names.show()\n```", "```py\n+---+--------+----------+------+---+----------------+\n| ID|Employee|Department|Salary|Age|capitalized_name|\n+---+--------+----------+------+---+----------------+\n|  1|    John| Field-eng|  3500| 40|            JOHN|\n|  2|  Robert|     Sales|  4000| 38|          ROBERT|\n|  3|   Maria|   Finance|  3500| 28|           MARIA|\n|  4| Michael|     Sales|  3000| 20|         MICHAEL|\n|  5|   Kelly|   Finance|  3500| 35|           KELLY|\n|  6|    Kate|   Finance|  3000| 45|            KATE|\n|  7|  Martin|   Finance|  3500| 26|          MARTIN|\n|  8|   Kiran|     Sales|  2200| 35|           KIRAN|\n+---+--------+----------+------+---+----------------+\n```", "```py\nimport pandas as pd\nfrom pyspark.sql.functions import pandas_udf\n@pandas_udf('long')\ndef pandas_plus_one(series: pd.Series) -> pd.Series:\n    # Simply plus one by using pandas Series.\n    return series + 1\nsalary_data_with_id.select(pandas_plus_one(salary_data_with_id.Salary)).show()\n```", "```py\n+-----------------------+\n|pandas_plus_one(Salary)|\n+-----------------------+\n|                   3501|\n|                   4001|\n|                   3501|\n|                   3001|\n|                   3501|\n|                   3001|\n|                   3501|\n|                   2201|\n+-----------------------+\n```", "```py\n@pandas_udf(\"integer\")\ndef add_one(s: pd.Series) -> pd.Series:\n    return s + 1\nspark.udf.register(\"add_one\", add_one)\nspark.sql(\"SELECT add_one(Salary) FROM employees\").show()\n```", "```py\n+---------------+\n|add_one(Salary)|\n+---------------+\n|           3501|\n|           4001|\n|           3501|\n|           3001|\n|           3501|\n|           3001|\n|           3501|\n|           2201|\n+---------------+\n```"]