<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Where to Go from Here</h1>
                </header>
            
            <article>
                
<p class="mce-root">This book has been a journey, much like a daring mountain hike… but now, at last, we have arrived at the end of our trek. We now stand upon the summit of mount introductory-GPU-programming, and we stand proud as we gaze back upon our native village of serial-programming-ville and smile as we think about the quaint naivity of our old one-dimensional programming traditions, where we considered <em>forking</em> a process in Unix to be our entire understanding of the notion of <em>parallel programming</em>. We have braved many pitfalls and dangers to arrive at this point, and we may have even made such mishaps as installing a broken NVIDIA<span> </span>driver module in Linux, or maybe downloading the wrong Visual Studio version over a slow 100k connection while visiting our parents for vacation. But these setbacks were only temporary, leaving wounds that developed into calluses that made us even stronger against the forces of (GPU) nature.</p>
<p class="mce-root">But, in the corner of our eye, we can see two wooden signs a few meters away from where we are standing; we avert our gaze from the little village of our past and now take a look at them. The first has an arrow pointing in the direction from which we are currently faced, with only one word on it—PAST. The other is pointing in the opposite direction, also with only one word—FUTURE. We turn around in the direction pointing to FUTURE, and we see a large glimmering metropolis strewn out before us to the horizon, beckoning us. Now that we have finally caught our breath, we can start walking into the future…</p>
<p class="mce-root">In this chapter, we will go over some of the options that you now have so that you can continue your education and career in fields related to GPU programming. Whether you are trying to build a career, a hobbyist doing this for fun, an engineering student studying GPUs for a class, a programmer or engineer trying to enhance your technical background, or an academic scientist trying to apply GPUs to a research project, there are many, many options that you now have at this point. Much like our metaphorical metropolis, it is easy to get lost, and it is difficult to determine where we should go. We hope to provide something akin to a brief tour guide in this final chapter, providing you with some of the options for where you can go next.</p>
<p>We will now take a look at the following paths in this chapter:</p>
<ul>
<li>Advanced CUDA and GPGPU programming</li>
<li>Graphics</li>
<li>Machine learning and computer vision</li>
<li>Blockchain technology</li>
</ul>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Furthering your knowledge of CUDA and GPGPU programming</h1>
                </header>
            
            <article>
                
<p class="mce-root">The first option you have is, of course, to learn more about CUDA and <strong>General-Purpose GPU</strong> <span>(<strong>GPGPU</strong>) </span>programming in particular. In this case, you have probably already found a good application of this and want to write even more advanced or optimized CUDA code. You may find it interesting for its own sake, or perhaps you want to get a job as a CUDA/GPU programmer. <span><span>With a strong GPU programming foundation in place (which was provided by this book), we will now look at some of the advanced topics in this field that we are now prepared to learn about.</span></span></p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Multi-GPU systems</h1>
                </header>
            
            <article>
                
<p class="mce-root">The first major topic that comes to mind would be to learn how to program systems with more than one GPU installed. Many professional workstations and servers contain several GPUs that have been installed with the intention of processing far more data that requires not one, but several top-of-the-line GPUs. To this end, there exists a subfield called Multi-GPU programming. Much of the work is focused on load balancing, which is the art of using each GPU at its peak capacity, ensuring that no GPU gets saturated with too much work while the other goes without being fully utilized. Another topic here is Inter-GPU Communication, which is generally concerned about the issue of one GPU directly copying memory arrays to or from another using CUDA's GPUDirect <strong>peer-to-peer</strong> (<strong>P2P</strong>) memory access.</p>
<div class="mce-root packt_infobox">NVIDIA<span> </span>provides a brief introduction to Multi-GPU programming here: <a href="https://www.nvidia.com/docs/IO/116711/sc11-multi-gpu.pdf">https://www.nvidia.com/docs/IO/116711/sc11-multi-gpu.pdf</a>.</div>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Cluster computing and MPI</h1>
                </header>
            
            <article>
                
<p class="mce-root">Another topic is cluster computing, that is, writing programs that make collective use of a multitude of servers containing GPUs. These are the <em>server farms</em> that populate the data-processing facilities of well-known internet companies such as Facebook and Google, as well as the scientific supercomputing facilities used by governments and militaries. Clusters are generally programmed with a programming paradigm called <strong>message-passing interface</strong> (<strong>MPI</strong>), which is an interface used with languages such as C++ or Fortran that allows you to program many computers that are connected to the same network.</p>
<div class="mce-root packt_infobox">More information about using CUDA with MPI is available here: <a href="https://devblogs.nvidia.com/introduction-cuda-aware-mpi/">https://devblogs.nvidia.com/introduction-cuda-aware-mpi/</a>.</div>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">OpenCL and PyOpenCL</h1>
                </header>
            
            <article>
                
<p class="mce-root">CUDA isn't the only language that can be used to program a GPU. CUDA's most major competitor is called Open Computing Language, or OpenCL. Where CUDA is a closed and proprietary system that will work exclusively on only NVIDIA<span> </span>hardware, OpenCL is an open standard that's developed and supported by the nonprofit Khronos Group. OpenCL can be used to program not only an NVIDIA GPU, but also AMD Radeon GPUs and even Intel HD GPUs—most major technology companies have committed to supporting OpenCL in their products. Additionally, the author of PyCUDA, Professor Andreas Kloeckner of UIUC, has written another excellent (and free) Python library called PyOpenCL, which provides an equally user-friendly interface to OpenCL, with nearly the same syntax and notions as PyCUDA.</p>
<div class="mce-root packt_infobox">Information on OpenCL is provided by NVIDIA<span> </span>here: <a href="https://developer.nvidia.com/opencl">https://developer.nvidia.com/opencl</a>.<br/>
<br/>
Information on the free PyOpenCL library is available from Andreas Kloeckner’s website here:<br/>
<a href="https://mathema.tician.de/software/pyopencl/">https://mathema.tician.de/software/pyopencl/</a>.</div>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Graphics</h1>
                </header>
            
            <article>
                
<p class="mce-root">Obviously, the <strong>G</strong> in GPU stands for <strong>graphics</strong>, which we really haven't seen much of in this book. Even though machine learning applications are now NVIDIA's bread and butter, it all started with rendering nice-looking graphics. We will provide some resources to get you started here, whether you want to develop video game engines, render CGI movies, or develop CAD software. CUDA can actually be used hand in hand with graphics applications, and is actually used in professional software such as Adobe's Photoshop and After Effects, as well as in many recent video games such as the <em>Mafia</em> and <em>Just Cause</em> series. We will briefly cover some of the major APIs you might consider starting with here.</p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">OpenGL</h1>
                </header>
            
            <article>
                
<p>The Open Graphics Language, or OpenGL, is an industry open standard that has existed since the early 90's. While in some ways it is showing its age, it is a stable API that enjoys widespread support, and if you write a program that makes use of this, it is pretty much guaranteed to work on any relatively modern GPU in existence. The CUDA samples folder actually contains many examples of how OpenGL can interface with CUDA (particularly in the <kbd>2_Graphics</kbd> subdirectory), so interested readers may consider going over these examples. (The default location is <kbd>C:\ProgramData\NVIDIA Corporation\CUDA Samples</kbd> in Windows, and <kbd>/usr/local/cuda/samples</kbd> in Linux.)</p>
<div class="packt_infobox">Information about OpenGL is available directly from NVIDIA<span> </span>here: <a href="https://developer.nvidia.com/opengl">https://developer.nvidia.com/opengl</a>.<br/>
<br/>
PyCUDA also provides an interface for the NVIDIA<span> </span>OpenGL driver. Information is available here: <a href="https://documen.tician.de/pycuda/gl.html">https://documen.tician.de/pycuda/gl.html</a>.</div>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">DirectX 12</h1>
                </header>
            
            <article>
                
<p>DirectX 12 is the latest iteration of Microsoft's well-known and well-supported graphics API. While this is proprietary for Windows PCs and Microsoft Xbox game consoles, these systems obviously have a wide install base of hundreds of millions of users. Furthermore, a variety of GPUs are supported on Windows PCs besides NVIDIA<span> </span>cards, and the Visual Studio IDE provides a great ease of use. DirectX 12 actually supports low-level GPGPU programming-type concepts and can utilize multiple GPUs.</p>
<div class="packt_infobox">Microsoft's DirectX 12 Programming Guide is available here: <a href="https://docs.microsoft.com/en-us/windows/desktop/direct3d12/directx-12-programming-guide">https://docs.microsoft.com/en-us/windows/desktop/direct3d12/directx-12-programming-guide</a>.</div>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Vulkan</h1>
                </header>
            
            <article>
                
<p>Vulkan can be thought of as the open equivalent of DirectX 12, which was developed by the Khronos Group as the <em>next-gen</em> successor of OpenGL. Along with Windows, Vulkan is also supported on macOS and Linux, as well as on the Sony PlayStation 4, Nintendo Switch, and Xbox One consoles. Vulkan has many of the same features as DirectX 12, such as quasi-GPGPU programming. Vulkan is providing some serious competition to DirectX 12, with video games such as the 2016 <em>DOOM</em> remake.</p>
<div class="packt_infobox">The <em>Beginner's Guide to Vulkan</em> is available from the Khronos Group here: <a href="https://www.khronos.org/blog/beginners-guide-to-vulkan">https://www.khronos.org/blog/beginners-guide-to-vulkan</a>.</div>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Machine learning and computer vision</h1>
                </header>
            
            <article>
                
<p>Of course, the elephant in the room of this chapter is machine learning and its fraternal twin computer vision. It goes without saying that machine learning (particularly the subfields of deep neural networks and convolutional neural networks) is what is keeping a roof over NVIDIA<span> </span>CEO Jensen Huang's head these days. (Okay, we admit that was the understatement of the decade...) If you need a reminder as to why GPUs are so applicable and useful in this field, please take another look at <a href="3562f1e0-a53d-470f-9b4d-94fa41b1b2fa.xhtml">Chapter 9</a>, <em>Implementation of a Deep Neural Network</em>. A large number of parallel computations and mathematical operations, as well as the user-friendly mathematical libraries, have made NVIDIA<span> </span>GPUs the hardware backbone of the machine learning industry.</p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The basics</h1>
                </header>
            
            <article>
                
<p>While you now know many of the intricacies of low-level GPU programming, you won't be able to apply this knowledge to machine learning<span> immediately</span>. If you don't have the basic skills in this field, like how to do a basic statistical analysis of a dataset, you really should stop and familiarize yourself with them. Stanford Professor Andrew Ng, the founder of Google Brain, provides many materials that are available for free on the web and on YouTube. Professor Ng's work is generally considered to be the gold standard of educational material on machine learning.</p>
<div class="packt_infobox">Professor Ng provides a free introductory machine learning class on the web here: <a href="http://www.ml-class.org">http://www.ml-class.org</a>.</div>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">cuDNN</h1>
                </header>
            
            <article>
                
<p>NVIDIA<span> </span>provides an optimized GPU library for deep neural network primitives called cuDNN. These primitives include operations such as forward propagation, convolutions, back propagation, activation functions (such as sigmoid, ReLU, and tanh), and gradient descent. cuDNN is what most of the mainstream deep neural network frameworks such as Tensorflow use as a backend for NVIDIA GPUs. This is provided for free by NVIDIA<span> </span>, but has to be downloaded separately from the CUDA Toolkit.</p>
<div class="packt_infobox">More information on cuDNN is available here: <a href="https://developer.nvidia.com/cudnn">https://developer.nvidia.com/cudnn</a>.</div>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Tensorflow and Keras</h1>
                </header>
            
            <article>
                
<p>Tensorflow is, of course, Google's well-known neural network framework. This is a free and open source framework that is usable with Python and C++, and has been available to the general public since 2015.</p>
<div class="packt_infobox">Tutorials on Tensorflow are available from Google here: <a href="https://www.tensorflow.org/tutorials/">https://www.tensorflow.org/tutorials/</a>.</div>
<p>Keras is a higher level library that provides a more <em>user-friendly</em> interface to Tensorflow, which was originally written by Google Brain's Francois Chollet. Readers may actually consider starting with Keras before moving on to Tensorflow.</p>
<div class="packt_infobox">Information on Keras is available here: <a href="https://keras.io/">https://keras.io/</a>.</div>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Chainer</h1>
                </header>
            
            <article>
                
<p>Chainer is another neural network API that was developed by Seiya Tokui, who is currently a PhD student at the University of Tokyo in Japan. While it is less mainstream than Tensorflow, it is very well-respected due to its incredible speed and efficiency. Moreover, readers may find Chainer of particular interest, since this was originally developed using PyCUDA. (This was later switched to CuPy, which is a PyCUDA branch that was developed to provide an interface that is more similar to NumPy.)</p>
<div class="packt_infobox">Information on Chainer is available here: <a href="https://chainer.org/">https://chainer.org/</a>.</div>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">OpenCV</h1>
                </header>
            
            <article>
                
<p>The Open Source computer vision Library (OpenCV) has been around since 2001. This library provides many of the tools from classical computer vision and image processing, which are still extremely useful in this age of the deep neural network. Most of the algorithms in OpenCV have been ported to CUDA in recent years, and it interfaces very easily with PyCUDA.</p>
<div class="packt_infobox">Information on OpenCV is here: <a href="https://opencv.org/">https://opencv.org/</a>.</div>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Blockchain technology</h1>
                </header>
            
            <article>
                
<p>Last, but certainly not least, is <strong>blockchain technology</strong>. This is the underlying cryptographic technology that powers cryptocurrencies such as Bitcoin and Ethereum. This is, of course, a very new field, which was first described by Bitcoin's mysterious creator, Satoshi Nakamoto, in a white paper published in 2008. GPUs were applied to this field almost immediately after its invention—generating a unit of currency comes down to brute-force cracking a cryptographic puzzle, and a GPU can attempt to brute-force crack more combinations in parallel than any other piece of hardware available to the general public today. This process is known as <strong>mining</strong>.</p>
<div class="packt_infobox">Those who are interested in blockchain technology are suggested to read Satoshi Nakamoto's original white paper on Bitcoin, which is available here: <a href="https://bitcoin.org/bitcoin.pdf">https://bitcoin.org/bitcoin.pdf</a>.<br/>
<br/>
GUIMiner, an open source, CUDA-based Bitcoin miner, is available here: <a href="https://guiminer.org/">https://guiminer.org/</a>.</div>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we went over some of the options and paths for those that are interested in furthering their background in GPU programming, which is beyond the scope of this book. The first path we covered was expanding your background in pure CUDA and GPGPU programming—some of the things you can learn about that weren't covered in this book include programming systems with multiple GPUs and networked clusters. We also looked at some of the parallel programming languages/APIs besides CUDA, such as MPI and OpenCL. Next, we discussed some of the well-known APIs available to those who are interested in applying GPUs to rendering graphics, such as Vulkan and DirectX 12. We then looked at machine learning and went into some of the basic backgrounds that you should have as well as some of the major frameworks available for developing deep neural networks. Finally, we ended by taking a brief look at blockchain technology and GPU-based cryptocurrency mining.</p>
<p>As the author, I would like to say <em>thank you</em> to everyone who has pushed through this book and made it here, to the end. GPU programming is one of the trickiest subfields of programming that I have encountered, and I hope my text has helped you come to grips with the essentials. As the reader, you should now feel free to indulge in a slice of the richest, most calorie-laden slice of chocolate cake you can find—just know that you've <em>earned</em> it. (But only one slice!)</p>


            </article>

            
        </section>
    </div></div>
<div id="book-content"><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>Use Google or some other search engine to find at least one application of GPU programming that is not featured in this chapter.</li>
<li>Try to find at least one programming language or API that can be used to program a GPU that is not featured in this chapter.</li>
<li>Look up Google's new Tensor Processing Unit (TPU) chips. How do these differ from GPUs?</li>
<li>Do you think it is a better idea to connect computers together into a cluster using Wi-Fi or wired Ethernet cables?</li>
</ol>


            </article>

            
        </section>
    </div></div></body></html>