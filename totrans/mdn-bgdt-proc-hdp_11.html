<html><head></head><body>
        

                            
                    <h1 class="header-title">Production Hadoop Cluster Deployment</h1>
                
            
            
                
<p class="mce-root"/>
<p>Hadoop itself started with a strong core and File System designed to handle the big data challenges. Later, many applications were developed on top of this, creating a big ecosystem of applications that play nicely with each other. As the number of applications started increasing, the challenges to create and manage the Hadoop environment increased as well.</p>
<p>In this chapter, we will look at the following:</p>
<ul>
<li style="font-weight: 400">Apache Ambari</li>
<li>A Hadoop cluster with Ambari</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Apache Ambari architecture</h1>
                
            
            
                
<p>Apache Ambari follows a master/slave architecture where the master node instructs the slave nodes to perform certain actions and report back the state of every action. The master node is responsible for keeping track of the state of the infrastructure. In order to do this, the master node uses a database server, which can be configured during setup time.</p>
<p>In order to have a better understanding of how Ambari works, let's take a look at the high level architecture of Ambari, in the following diagram:</p>
<div><img src="img/c9883b7a-2f6f-47b2-a18e-6e90f41f2f76.png" style="width:25.42em;height:32.00em;"/></div>
<p>At the core, we have the following applications:</p>
<ul>
<li style="font-weight: 400">Ambari server</li>
<li style="font-weight: 400">Ambari agent</li>
<li style="font-weight: 400">Ambari web UI</li>
<li style="font-weight: 400">Database</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">The Ambari server</h1>
                
            
            
                
<p>The Ambari server (<kbd>ambari-server</kbd>) is a shell script which is the entry point for all administrative activities on the master server. This script internally uses Python code, <kbd>ambari-server.py,</kbd> and routes all the requests to it.</p>
<p>The Ambari server has the following entry points which are available when passed different parameters to the <kbd>ambari-server</kbd> program:</p>
<ul>
<li style="font-weight: 400">Daemon management</li>
<li style="font-weight: 400">Software upgrade</li>
<li style="font-weight: 400">Software setup</li>
<li style="font-weight: 400">LDAP/PAM/Kerberos management</li>
<li style="font-weight: 400">Ambari backup and restore</li>
<li style="font-weight: 400">Miscellaneous options</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Daemon management</h1>
                
            
            
                
<p>The daemon management mode is activated when the script is invoked with <kbd>start</kbd>, <kbd>stop</kbd>, <kbd>reset</kbd>, <kbd>restart</kbd> arguments from the command line.</p>
<p>For example, if we want to start the Ambari background server, we can run the following command:</p>
<pre>Example: ambari-server start</pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Software upgrade</h1>
                
            
            
                
<p>Once Ambari is installed, we can use this mode to upgrade the Ambari server itself. This is triggered when we call the <kbd>ambari-server</kbd> program with the <kbd>upgrade</kbd> flag. In case we want to upgrade the entire stack of Ambari, we can pass the <kbd>upgradestack</kbd> flag:</p>
<pre>Example: ambari-server upgrade</pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Software setup</h1>
                
            
            
                
<p>Once Ambari is downloaded from the internet (or installed via YUM and APT), we need to do a preliminary setup of the software. This mode can be triggered when we pass the <kbd>setup</kbd> flag to the program. This mode will ask us several questions that we need to answer. Unless we finish this step, Ambari cannot be used for any kind of management of our servers:</p>
<pre>Example: ambari-server setup</pre>


            

            
        
    

        

                            
                    <h1 class="header-title">LDAP/PAM/Kerberos management</h1>
                
            
            
                
<p><strong>T</strong>he <strong>Lightweight Directory Access Protocol</strong> (<strong>LDAP</strong>) is used for identity management in enterprises. In order to use LDAP-based authentication, we need to use the following flags: <kbd>setup-ldap</kbd> (for setting up <kbd>ldap</kbd> properties with <kbd>ambari</kbd>) and <kbd>sync-ldap</kbd> (to perform a synchronization of the data from the <kbd>ldap</kbd> server):</p>
<pre>Example: ambari-server setup-ldap<br/>Example: ambari-server sync-ldap</pre>
<p><strong>Pluggable Authentication Module </strong>(<strong>PAM</strong>) is at the core of the authentication and authorization in any UNIX or Linux operating systems. If we want to leverage the PAM-based access for Ambari then we need to run it with the <kbd>setup-pam</kbd> option. If we then want to move from LDAP to PAM-based authentication, we need to run it with <kbd>migrate-ldap-pam</kbd>:</p>
<pre>Example: ambari-server setup-pam<br/>Example: ambari-server migrate-ldap-pam</pre>
<p><strong>Kerberos</strong> is another advanced authentication and authorization mechanism which is very helpful in networked environments. This simplifies <strong>Authenticity, Authorisation and Auditing</strong> (<strong>AAA</strong>) on large-scale servers. If we want to use Kerberos for Ambari, we can use the <kbd>setup-kerberos</kbd> flag:</p>
<pre>Example: ambari-server setup-kerberos</pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Ambari backup and restore</h1>
                
            
            
                
<p>If we want to take a snapshot of the current installation of Ambari (excluding the database), we can enter this mode. This supports both backup and restore methods invoked via the <kbd>backup</kbd> and <kbd>restore</kbd> flags:</p>
<pre>Example: ambari-server backup<br/>Example: ambari-server restore</pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Miscellaneous options</h1>
                
            
            
                
<p>In addition to these options, there are other options that are available with the Ambari server program which you can invoke with the <kbd>-h</kbd> (help) flag.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Ambari Agent</h1>
                
            
            
                
<p>Ambari Agent is a program which runs on all the nodes that we want to be managed with Ambari. This program periodically heartbeats to the master node. Using this agent, <kbd>ambari-server</kbd> executes many of the tasks on the servers.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Ambari web interface</h1>
                
            
            
                
<p>This is one of the powerful features of the Ambari application. This web application is exposed by the Ambari server program that is running on the master host; we can access this application on port <kbd>8080</kbd> and it is protected by authentication.</p>
<p>Once we log in to this web portal, we can control and view all aspects of our Hadoop clusters.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Database</h1>
                
            
            
                
<p>Ambari supports multiple RDBMS to keep track of the state of the entire Hadoop infrastructure. During the setup of the Ambari server for the first time, we can choose the database we want to use.</p>
<p>At the time of writing, Ambari supports the following databases:</p>
<ul>
<li style="font-weight: 400">PostgreSQL</li>
<li style="font-weight: 400">Oracle</li>
<li style="font-weight: 400">MySQL or MariaDB</li>
<li style="font-weight: 400">Embedded PostgreSQL</li>
<li style="font-weight: 400">Microsoft SQL Server</li>
<li style="font-weight: 400">SQL Anywhere</li>
<li style="font-weight: 400">Berkeley DB</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Setting up a Hadoop cluster with Ambari</h1>
                
            
            
                
<p>In this section, we will learn how to set up a brand new Hadoop cluster from scratch using Ambari. In order to do this, we are going to need four servers – one server for running the Ambari server and three other nodes for running the Hadoop components.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Server configurations</h1>
                
            
            
                
<p>The following table displays the configurations of the servers we are using as part of this exercise:</p>
<div><table>
<tbody>
<tr>
<td>
<p><strong>Server Type</strong></p>
</td>
<td>
<p><strong>Name</strong></p>
</td>
<td>
<p><strong>CPU</strong></p>
</td>
<td>
<p><strong>RAM</strong></p>
</td>
<td>
<p><strong>DISK</strong></p>
</td>
</tr>
<tr>
<td>
<p>Ambari Server node</p>
</td>
<td>
<p>master</p>
</td>
<td>
<p>1</p>
</td>
<td>
<p>3.7 GB</p>
</td>
<td>
<p>100 GB</p>
</td>
</tr>
<tr>
<td>
<p>Hadoop node 1</p>
</td>
<td>
<p>node-1</p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>13 GB</p>
</td>
<td>
<p>250 GB</p>
</td>
</tr>
<tr>
<td>
<p>Hadoop node 2</p>
</td>
<td>
<p>node-2</p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>13 GB</p>
</td>
<td>
<p>250 GB</p>
</td>
</tr>
<tr>
<td>
<p>Hadoop node 3</p>
</td>
<td>
<p>node-3</p>
</td>
<td>
<p>2</p>
</td>
<td>
<p>13 GB</p>
</td>
<td>
<p>250 GB</p>
</td>
</tr>
</tbody>
</table>
</div>
<p> </p>
<p>Since this is a sample setup, we are good with this configuration. For real-world scenarios, please choose the configuration according to your requirements.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Preparing the server </h1>
                
            
            
                
<p>This section and all further sections assume that you have a working internet connection on all the servers and are safely firewalled to prevent any intrusions.</p>
<p>All the servers are running the CentOS 7 operating system, as it's a system that uses RPM/YUM for package management. Don't get confused when you see <kbd>yum</kbd> in the following sections.</p>
<p>Before we go ahead and start using the servers, we need to run basic utility programs which help us troubleshoot various issues with the servers. They are installed as part of the next command. Don't worry if you are not sure what they are. Except for <kbd>mysql-connector-java</kbd> and <kbd>wget</kbd>, all other utilities are not mandatory:</p>
<pre>sudo yum install mysql-connector-java wget iftop iotop smartctl -y</pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Installing the Ambari server </h1>
                
            
            
                
<p>The first step in creating the Hadoop cluster is to get our Ambari server application up and running. So, log in to the master node with SSH and perform the following steps in order:</p>
<ol>
<li>Download the Ambari YUM repository for CentOS 7 with this command:</li>
</ol>
<pre style="padding-left: 60px">[user@master ~]$ wget http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.6.1.5/ambari.repo<br/><br/></pre>
<ol start="2">
<li>After this step, we need to move the <kbd>ambari.repo</kbd> file to the <kbd>/etc/yum.repos.d</kbd> directory using this command:</li>
</ol>
<pre style="padding-left: 60px">[user@master ~]$ sudo mv ambari.repo /etc/yum.repos.d</pre>
<ol start="3">
<li>The next step is to install the <kbd>ambari-server</kbd> package with the help of this command:</li>
</ol>
<pre style="padding-left: 60px">[user@master ~]$ sudo yum install ambari-server -y</pre>
<ol start="4">
<li>We are going to use a MySQL server for our Ambari server. So, let's install the required packages as well:</li>
</ol>
<pre style="padding-left: 60px">[user@master ~]$ sudo yum install mariadb-server -y</pre>
<ol start="5">
<li>Let's configure the MySQL server (or MariaDB) before we touch the Ambari setup process. This is done with the following commands:</li>
</ol>
<pre style="padding-left: 60px">[user@master ~]$ sudo service mariadb start<br/>Redirecting to /bin/systemctl start mariadb.service</pre>
<ol start="6">
<li>Then, create a database called <kbd>ambari</kbd> and a user called <kbd>ambari</kbd> with the password, <kbd>ambari,</kbd> so that the Ambari server configuration is easy to set up in the following steps. This can be done with these SQL queries:</li>
</ol>
<pre style="padding-left: 60px">CREATE DATABASE ambari;<br/>GRANT ALL PRIVILEGES ON ambari.* to ambari@localhost identified by 'ambari';<br/>GRANT ALL PRIVILEGES ON ambari.* to ambari@'%' identified by 'ambari';<br/>FLUSH PRIVILEGES;</pre>
<ol start="7">
<li>Store these four lines into a text file called <kbd>ambari.sql</kbd> and execute with the following command:</li>
</ol>
<pre style="padding-left: 60px">[user@master ~] mysql -uroot &lt; ambari.sql</pre>
<ol start="8">
<li>This will create a database, users and give necessary privileges.</li>
</ol>
<p>Please use a strong password for production setup, otherwise your system will be vulnerable to any attacks.</p>
<p style="padding-left: 60px">Now that we have done the groundwork, let's run the Ambari server setup. Note that we are required to answer a few questions that are highlighted as follows:</p>
<pre style="padding-left: 60px">[user@master ~]$ sudo ambari-server setup<br/>Using python /usr/bin/python<br/>Setup ambari-server<br/>Checking SELinux...<br/>SELinux status is 'enabled'<br/>SELinux mode is 'enforcing'<br/>Temporarily disabling SELinux<br/>WARNING: SELinux is set to 'permissive' mode and temporarily disabled.<br/>OK to continue [y/n] (y)? <strong>&lt;ENTER&gt;</strong><br/>Customize user account for ambari-server daemon [y/n] (n)? <strong>&lt;ENTER&gt;</strong><br/>Adjusting ambari-server permissions and ownership...<br/>Checking firewall status...<br/>WARNING: iptables is running. Confirm the necessary Ambari ports are accessible. Refer to the Ambari documentation for more details on ports.<br/>OK to continue [y/n] (y)? <strong>&lt;ENTER&gt;</strong><br/>Checking JDK...<br/>[1] Oracle JDK 1.8 + Java Cryptography Extension (JCE) Policy Files 8<br/>[2] Oracle JDK 1.7 + Java Cryptography Extension (JCE) Policy Files 7<br/>[3] Custom JDK<br/>==============================================================================<br/>Enter choice (1): <strong>&lt;ENTER&gt;</strong><br/>To download the Oracle JDK and the Java Cryptography Extension (JCE) Policy Files you must accept the license terms found at http://www.oracle.com/technetwork/java/javase/terms/license/index.html and not accepting will cancel the Ambari Server setup and you must install the JDK and JCE files manually.<br/>Do you accept the Oracle Binary Code License Agreement [y/n] (y)? <strong>&lt;ENTER&gt;</strong><br/>Downloading JDK from http://public-repo-1.hortonworks.com/ARTIFACTS/jdk-8u112-linux-x64.tar.gz to /var/lib/ambari-server/resources/jdk-8u112-linux-x64.tar.gz<br/>jdk-8u112-linux-x64.tar.gz... 100% (174.7 MB of 174.7 MB)<br/>Successfully downloaded JDK distribution to /var/lib/ambari-server/resources/jdk-8u112-linux-x64.tar.gz<br/>Installing JDK to /usr/jdk64/<br/>Successfully installed JDK to /usr/jdk64/<br/>Downloading JCE Policy archive from http://public-repo-1.hortonworks.com/ARTIFACTS/jce_policy-8.zip to /var/lib/ambari-server/resources/jce_policy-8.zip<br/><br/>Successfully downloaded JCE Policy archive to /var/lib/ambari-server/resources/jce_policy-8.zip<br/>Installing JCE policy...<br/>Checking GPL software agreement...<br/>GPL License for LZO: https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html<br/>Enable Ambari Server to download and install GPL Licensed LZO packages [y/n] (n)? <strong>y &lt;ENTER&gt;</strong><br/>Completing setup...<br/>Configuring database...<br/>Enter advanced database configuration [y/n] (n)? <strong>y &lt;ENTER&gt;</strong><br/>Configuring database...<br/>==============================================================================<br/>Choose one of the following options:<br/>[1] - PostgreSQL (Embedded)<br/>[2] - Oracle<br/>[3] - MySQL / MariaDB<br/>[4] - PostgreSQL<br/>[5] - Microsoft SQL Server (Tech Preview)<br/>[6] - SQL Anywhere<br/>[7] - BDB<br/>==============================================================================<br/>Enter choice (1): <strong>3 &lt;ENTER&gt;</strong><br/>Hostname (localhost): <br/>Port (3306): <br/>Database name (ambari): <br/>Username (ambari): <br/>Enter Database Password (bigdata): <strong>ambari &lt;ENTER&gt;</strong><br/>Re-enter password: <strong>ambari &lt;ENTER&gt;</strong><br/>Configuring ambari database...<br/>Configuring remote database connection properties...<br/>WARNING: Before starting Ambari Server, you must run the following DDL against the database to create the schema: /var/lib/ambari-server/resources/Ambari-DDL-MySQL-CREATE.sql<br/>Proceed with configuring remote database connection properties [y/n] (y)? <strong>&lt;ENTER&gt;</strong><br/>Extracting system views...<br/>ambari-admin-2.6.1.5.3.jar<br/>...........<br/>Adjusting ambari-server permissions and ownership...<br/>Ambari Server 'setup' completed successfully.</pre>
<ol start="9">
<li>Once the setup is complete, we need to create the tables in the Ambari database by using the previous file that is generated during the setup process. This can be done with this command:</li>
</ol>
<pre style="padding-left: 60px">[user@master ~] mysql -u ambari -pambari ambari &lt; /var/lib/ambari-server/resources/Ambari-DDL-MySQL-CREATE.sql</pre>
<ol start="10">
<li>The next step is for us to start the <kbd>ambari-server</kbd> daemon. This will start the web interface that we will use in the following steps to create the Hadoop cluster:</li>
</ol>
<pre style="padding-left: 60px">[user@master ~]$ sudo ambari-server start<br/>Using python /usr/bin/python<br/>Starting ambari-server<br/>Ambari Server running with administrator privileges.<br/>Organizing resource files at /var/lib/ambari-server/resources...<br/>Ambari database consistency check started...<br/>Server PID at: /var/run/ambari-server/ambari-server.pid<br/>Server out at: /var/log/ambari-server/ambari-server.out<br/>Server log at: /var/log/ambari-server/ambari-server.log<br/>Waiting for server start...............................<br/>Server started listening on 8080<br/>DB configs consistency check: no errors and warnings were found.<br/>Ambari Server 'start' completed successfully.</pre>
<ol start="11">
<li>Once the server setup is complete, configure the JDBC driver (which is helpful for all the other nodes as well):</li>
</ol>
<pre style="padding-left: 60px">[user@master ~] sudo ambari-server setup --jdbc-db=mysql --jdbc-driver=/usr/share/java/mysql-connector-java.jar</pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Preparing the Hadoop cluster</h1>
                
            
            
                
<p>There are a few more steps that we need to do before we go ahead and create the Hadoop cluster.</p>
<p>Since we have the Ambari server up and running, let's generate an RSA key pair that we can use for communication between the Ambari server and the Ambari agent nodes.</p>
<p>This key pair lets the Ambari server node log in to all the Hadoop nodes and perform the installation in an automated way.</p>
<p>This step is optional if you have already done this as part of procuring the servers and infrastructure:</p>
<pre>[user@master ~]$ ssh-keygen -t rsa<br/>Generating public/private rsa key pair.<br/>Enter file in which to save the key (/home/user/.ssh/id_rsa): <br/>Enter passphrase (empty for no passphrase): <strong>&lt;ENTER&gt;</strong><br/>Enter same passphrase again: <strong>&lt;ENTER&gt;</strong><br/>Your identification has been saved in /home/user/.ssh/id_rsa.<br/>Your public key has been saved in /home/user/.ssh/id_rsa.pub.<br/>The key fingerprint is:<br/>SHA256:JWBbGdAnRHM0JFj35iSAcQk+rC0MhyHlrFawr+d2cZ0 user@master<br/>The key's randomart image is:<br/>+---[RSA 2048]----+<br/>|.oo   *@@**    |<br/>| +oo +o==*.o     |<br/>| .=.. = .oo +    |<br/>| .o+ o . o =     |<br/>|.. .+ . S . .    |<br/>|. .  o . E      |<br/>| . .  o    |<br/>|  o. .           |<br/>|  ...            |<br/>+----[SHA256]-----+</pre>
<p>This will generate two files inside the <kbd>/home/user/.ssh</kbd> directory:</p>
<ul>
<li><kbd>~/.ssh/id_rsa</kbd>: This is the private key file which has to be kept in a secret place</li>
<li><kbd>~/.ssh/id_rsa.pub</kbd>: This is the public key file which allows any SSH login using the private key file</li>
</ul>
<p>The contents of this <kbd>id_rsa.pub</kbd> file should be put in <kbd>~/.ssh/authorized_keys</kbd> on all the Hadoop nodes. In this case, they are node servers (1–3).</p>
<p>This step of propagating all the public SSH keys can be done during the server provisioning itself, so a manual step is avoided every time we acquire new servers.</p>
<p>Now, we will do all the work with only the Ambari web interface.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Creating the Hadoop cluster </h1>
                
            
            
                
<p>In this section, we will build a Hadoop cluster using the Ambari web interface. This section assumes the following things:</p>
<ul>
<li style="font-weight: 400">The nodes (1–3) are reachable over SSH from the master server</li>
<li style="font-weight: 400">Admin can log in to the nodes (1–3) using the <kbd>id-rsa</kbd> private key from the master server</li>
<li style="font-weight: 400">A UNIX user can run <kbd>sudo</kbd> and perform all administrative actions on the node (1–3) servers</li>
<li style="font-weight: 400">The Ambari server setup is complete</li>
<li style="font-weight: 400">The Ambari web interface is accessible to the browser without any firewall restrictions</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Ambari web interface</h1>
                
            
            
                
<p>Let's open a web browser and connect to the Ambari server web interface using <kbd>http://&lt;server-ip&gt;:8080</kbd>. We are presented with a login screen like this. Please enter <kbd>admin</kbd> as the username and <kbd>admin</kbd> as the password to continue:</p>
<div><img src="img/342804dc-4adf-4c32-8c58-c81024b86a4f.png" style="width:49.00em;height:25.50em;"/></div>
<p>Once the login is successful, we are taken to the home page.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The Ambari home page</h1>
                
            
            
                
<p>This is the main page where there are multiple options on the UI. Since this is a brand new installation, there is no cluster data available yet.</p>
<p>Let's take a look at the home page with this screenshot:</p>
<div><img src="img/e406237d-ad0e-4dba-8fc2-d82db7913b57.png" style="width:68.25em;height:40.00em;"/></div>
<p>From this place, we can carry out the following activities:</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Creating a cluster</h1>
                
            
            
                
<p>As you may have guessed, this section is used to launch a wizard that will help us create a Hadoop cluster from the browser.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Managing users and groups</h1>
                
            
            
                
<p>This section is helpful to manage users and groups that can use and manage the Ambari web application.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Deploying views</h1>
                
            
            
                
<p>This interface is helpful in creating views for different types of users and what actions they can perform via the Ambari web interface.</p>
<p>Since our objective is to create a new Hadoop cluster, we will click on the Launch Install Wizard button and start the process of creating a Hadoop cluster.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The cluster install wizard</h1>
                
            
            
                
<p>Hadoop cluster creation is broken down into multiple steps. We will go through all these steps in the following sections. First, we are presented with a screen where we need to name our Hadoop cluster.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Naming your cluster</h1>
                
            
            
                
<p>I have chosen <kbd>packt</kbd> as the Hadoop cluster name. Click Next when the Hadoop name is entered in the screen. The screen looks like this:</p>
<div><img src="img/6cae5fdd-9dbd-49e1-b16e-793bf355b979.png"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title">Selecting the Hadoop version </h1>
                
            
            
                
<p>Once we name the Hadoop cluster, we are presented with a screen to select the version of Hadoop we want to run.</p>
<p>At the time of writing, Ambari supports the following Hadoop versions:</p>
<ul>
<li style="font-weight: 400">Hadoop 2.3</li>
<li style="font-weight: 400">Hadoop 2.4</li>
<li style="font-weight: 400">Hadoop 2.5</li>
<li style="font-weight: 400">Hadoop 2.6 (upto 2.6.3.0)</li>
</ul>
<p>You can choose any version for the installation. I have selected the default option which is version 2.6.3.0, which can be seen in this screenshot:</p>
<div><img src="img/bf80801e-ac87-404a-b773-b28e24e41787.png"/></div>
<p>Click Next at the bottom of the screen to continue to the next step.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Selecting a server </h1>
                
            
            
                
<p>The next logical step is to select the list of servers on which we are going to install the Hadoop-2.6.3.0 version. If you remember the original table, we named our node servers (1–3). We will enter those in the UI.</p>
<p>Since the installation is going to be completely automated, we also need to provide the RSA private key that we generated in the previous section in the UI. This will make sure that the master node can log in to the servers without any password over SSH.</p>
<p>Also, we need to provide a UNIX username that's already been created on all the node (1–3) servers that can also accept RSA key for authentication.</p>
<p>Add <kbd>id_rsa.pub</kbd> to <kbd>~/.ssh/authorized_keys</kbd> on the node (1–3) servers.</p>
<p>Please keep in mind that these hostnames should have proper entries in the <strong>DNS</strong> (<strong>Domain Name System</strong>) Servers otherwise the installation won't be able to proceed from this step.</p>
<p>The names that I have given can be seen in this following screenshot:</p>
<div><img src="img/8f8538ec-bcee-4508-b32b-b6056f188dca.png"/></div>
<p>After the data is entered, click on Register and Confirm.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Setting up the node</h1>
                
            
            
                
<p>In this step, the Ambari agent is automatically installed on the given nodes, provided the details are accurate. Success confirmation looks like this:</p>
<div><img src="img/d657b897-b614-45c0-8735-e66399aac561.png"/></div>
<p>If we want to remove any nodes, this is the screen in which we can do it. Click Next when we are ready to go to the next step.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Selecting services</h1>
                
            
            
                
<p>Now, we need to select the list of applications/services that we want to install on the three servers we have selected.</p>
<p>At the time of writing, Ambari supports the following services:</p>
<div><table>
<tbody>
<tr>
<td>
<p><strong>Application/Service</strong></p>
</td>
<td>
<p><strong>Application Description</strong></p>
</td>
</tr>
<tr>
<td>
<p>HDFS</p>
</td>
<td>
<p>Hadoop Distributed File System</p>
</td>
</tr>
<tr>
<td>
<p>YARN + MapReduce2</p>
</td>
<td>
<p>Next generation Map Reduce framework</p>
</td>
</tr>
<tr>
<td>
<p>Tez</p>
</td>
<td>
<p>Hadoop query processing framework built on top of YARN</p>
</td>
</tr>
<tr>
<td>
<p>Hive</p>
</td>
<td>
<p>Data warehouse system for ad hoc queries</p>
</td>
</tr>
<tr>
<td>
<p>HBase</p>
</td>
<td>
<p>Non-relational distributed database</p>
</td>
</tr>
<tr>
<td>
<p>Pig</p>
</td>
<td>
<p>Scripting platform to analyze datasets in HDFS</p>
</td>
</tr>
<tr>
<td>
<p>Sqoop</p>
</td>
<td>
<p>Tool to transfer data between Hadoop and RDBMS</p>
</td>
</tr>
<tr>
<td>
<p>Oozie</p>
</td>
<td>
<p>Workflow co-ordination for Hadoop jobs with a web UI</p>
</td>
</tr>
<tr>
<td>
<p>ZooKeeper</p>
</td>
<td>
<p>Distributed system coordination providing service</p>
</td>
</tr>
<tr>
<td>
<p>Falcon</p>
</td>
<td>
<p>Data processing and management platform</p>
</td>
</tr>
<tr>
<td>
<p>Storm</p>
</td>
<td>
<p>Stream processing framework</p>
</td>
</tr>
<tr>
<td>
<p>Flume</p>
</td>
<td>
<p>Distributed system to collect, aggregate, and move streaming data to HDFS</p>
</td>
</tr>
<tr>
<td>
<p>Accumulo</p>
</td>
<td>
<p>Distributed key/value store</p>
</td>
</tr>
<tr>
<td>
<p>Ambari Infra</p>
</td>
<td>
<p>Shared service used by Amari components</p>
</td>
</tr>
<tr>
<td>
<p>Ambari Metrics</p>
</td>
<td>
<p>Grafana-based system for metric collection and storage</p>
</td>
</tr>
<tr>
<td>
<p>Atlas</p>
</td>
<td>
<p>Metadata and governance platform</p>
</td>
</tr>
<tr>
<td>
<p>Kafka</p>
</td>
<td>
<p>Distributed streaming platform</p>
</td>
</tr>
<tr>
<td>
<p>Knox</p>
</td>
<td>
<p>Single-point authentication provider for all Hadoop components</p>
</td>
</tr>
<tr>
<td>
<p>Log Search</p>
</td>
<td>
<p>Ambari-managed services log aggregator and viewer</p>
</td>
</tr>
<tr>
<td>
<p>Ranger</p>
</td>
<td>
<p>Hadoop data security application</p>
</td>
</tr>
<tr>
<td>
<p>Ranger KMS</p>
</td>
<td>
<p>Key management server</p>
</td>
</tr>
<tr>
<td>
<p>SmartSense</p>
</td>
<td>
<p>Hortonworks Smart Sense tool to diagnose applications</p>
</td>
</tr>
<tr>
<td>
<p>Spark</p>
</td>
<td>
<p>Large-scale data processing framework</p>
</td>
</tr>
<tr>
<td>
<p>Zeppelin Notebook</p>
</td>
<td>
<p>Web-based notebook for data analytics</p>
</td>
</tr>
<tr>
<td>
<p>Druid</p>
</td>
<td>
<p>Column-oriented data store</p>
</td>
</tr>
<tr>
<td>
<p>Mahout</p>
</td>
<td>
<p>Machine learning algorithms</p>
</td>
</tr>
<tr>
<td>
<p>Slider</p>
</td>
<td>
<p>Framework to monitor applications on YARN</p>
</td>
</tr>
<tr>
<td>
<p>Superset</p>
</td>
<td>
<p>Browser-based data exploration platform for RDBMS and Druid</p>
</td>
</tr>
</tbody>
</table>
</div>
<p class="mce-root CDPAlignLeft CDPAlign">As part of the current step, we have selected only HDFS and its dependencies. The screen is shown as follows:</p>
<div><img src="img/3d5ef787-a2a7-47b3-a56a-4f0726dc3793.png"/></div>
<p>Once you have made your choices, click the Next button at the bottom of the UI.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Service placement on nodes</h1>
                
            
            
                
<p>In this step, we are shown the automatic selection of services on the three nodes we have selected for installation. If we want to customize the placement of the services on the nodes, we can do so. The placement looks like this:</p>
<div><img src="img/fd7ee964-d0a2-40e5-a058-5d31a73a6c9d.png" style="width:50.58em;height:30.00em;"/></div>
<p>Click Next when the changes look good.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Selecting slave and client nodes </h1>
                
            
            
                
<p>Some applications support slaves and client utilities. In this screen, we need to select the nodes on which we want these applications to be installed. If you are unsure, click Next. The screen looks like this:</p>
<div><img src="img/67a9dcf8-1d46-4bc4-9907-db153f82d489.png" style="width:48.75em;height:23.58em;"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title">Customizing services</h1>
                
            
            
                
<p>Even though Ambari automatically selects most of the properties and linkage between the applications, it provides us with some flexibility to choose values for some of the features, such as:</p>
<ul>
<li style="font-weight: 400">Databases</li>
<li style="font-weight: 400">Usernames</li>
<li style="font-weight: 400">Passwords</li>
</ul>
<p>And other properties that help the applications run smoothly. These are highlighted in the current screen in red.</p>
<p>In order to customize these, we need to go to the tab with the highlighted properties and choose the values according to our need. The screen looks like this:</p>
<div><img src="img/0736a2bc-163a-4774-b4c1-347e8e2afea7.png" style="width:55.25em;height:30.50em;"/></div>
<p>After all the service properties are configured correctly, we will not see anything in red in the UI and can click the Next button at the bottom of the page.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Reviewing the services</h1>
                
            
            
                
<p>In this step, we are shown a summary of the changes we have made so far. We are given an option to print the changes so that we will not forget them (don't worry, all these are available on the UI later). For now we can click Deploy. This is when the actual changes will be made to the nodes.</p>
<p>No changes will be made to the servers if we cancel this process. The current state of the wizard looks like this now:</p>
<div><img src="img/a89754af-2fc1-4e5c-8b69-64cde207f49e.png" style="width:54.00em;height:32.17em;"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title">Installing the services on the nodes</h1>
                
            
            
                
<p>After we've clicked Deploy in the previous step, a deployment plan is generated by the Ambari server and applications will be deployed on all the nodes in parallel, using the Ambari agents running on all the nodes.</p>
<p>We are shown the progress of what is being deployed in real time in this step.</p>
<p>Once all the components have been installed, they will be automatically started and we can see the successful completion in this screen:</p>
<div><img src="img/a3026110-8cdf-4996-a162-cb94ea509f42.png" style="width:46.00em;height:22.50em;"/></div>
<p>Click Next when everything is done successfully. In the case of any failures, we are shown what has failed and will be given an option to retry the installation. If there are any failures, we need to dig into the errors and fix the underlying problems.</p>
<p>If you have followed the instructions given at the beginning of the section you should have everything running smoothly.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Installation summary</h1>
                
            
            
                
<p>In this step, we are shown the summary of what has been installed. The screen looks like this:</p>
<div><img src="img/324eceec-02e5-4514-b822-a2fb762a9caf.png" style="width:63.58em;height:27.33em;"/></div>
<p>Click on the Complete button which marks the end of the Hadoop cluster setup. Next, we will be taken to the cluster dashboard.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The cluster dashboard</h1>
                
            
            
                
<p>This is the home page of the Hadoop cluster we have just created where we can see the list of all the services that have been installed and the health sensors.</p>
<p>We can manage all aspects of the Hadoop cluster in this interface. Feel free to explore the interface and play with it to understand more:</p>
<div><img src="img/653fc1b7-f43b-4cff-8613-447f54d147bb.png" style="width:68.00em;height:40.58em;"/></div>
<p>This marks the end of the Hadoop cluster creation with Ambari.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Hadoop clusters</h1>
                
            
            
                
<p>So far, we have seen how to create a single Hadoop cluster using Ambari. But, is there ever a requirement for multiple Hadoop clusters?</p>
<p>The answer depends on the business requirements. There are trade-offs for both single versus multiple Hadoop clusters.</p>
<p>Before we jump into the advantages and disadvantages of both of these, let's see in what scenarios we might use either.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">A single cluster for the entire business</h1>
                
            
            
                
<p>This is the most straightforward approach and every business starts with one cluster, at least. As the diversity of the business increases, organizations tend to choose one cluster per department, or business unit.</p>
<p>The following are some of the advantages:</p>
<ul>
<li style="font-weight: 400"><strong>Ease of operability</strong>: Since there is only one Hadoop cluster, managing it is very easy and the team sizes will also be optimal when administering it.</li>
<li style="font-weight: 400"><strong>One-stop shop</strong>: Since all the company data is in a single place, it's very easy to come up with innovative ways to use and generate analytics on top of the data.</li>
<li style="font-weight: 400"><strong>Integration cost</strong>: Teams and departments within the enterprise can integrate with this single system very easily. They have less complex configurations to deal with when managing their applications.</li>
<li style="font-weight: 400"><strong>Cost to serve</strong>: Enterprises can have a better understanding of their entire big data usage and can also plan, in a less stringent way, on scaling their system.</li>
</ul>
<p>Some disadvantages of employing this approach are as follows:</p>
<ul>
<li style="font-weight: 400"><strong>Scale becomes a challenge</strong>: Even though Hadoop can be run on hundreds and thousands of servers, it becomes a challenge to manage such big clusters, particularly during upgrades and other changes.</li>
<li style="font-weight: 400"><strong>Single point of failure</strong>: Hadoop internally has replication built-in to it in the HDFS File System. When more nodes fail, the chances are that there is loss of data and it's hard to recover from that.</li>
<li style="font-weight: 400"><strong>Governance is a challenge</strong>: As the scale of data, applications, and users increase, it is a challenge to keep track of the data without proper planning and implementation in place.
<ul>
<li style="font-weight: 400"><strong>Security and confidential data management</strong>: Enterprises deal with a variety of data that varies from highly sensitive to transient data. When all sorts of data is put in a big-data solution, we have to employ very strong authentication and authorization rules so that the data is visible only to the right audience.</li>
</ul>
</li>
</ul>
<p>With these thoughts, let's take a look at the other possibility of having Hadoop clusters in an enterprise.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Multiple Hadoop clusters</h1>
                
            
            
                
<p>Even though having a single Hadoop cluster is easier to maintain within an organization, sometimes its important to have multiple Hadoop clusters to keep the business running smoothly and reduce dependency on a single point of failure system.</p>
<p>These multiple Hadoop clusters can be used for several reasons:</p>
<ul>
<li style="font-weight: 400">Redundancy</li>
<li style="font-weight: 400">Cold backup</li>
<li style="font-weight: 400">High availability</li>
<li style="font-weight: 400">Business continuity</li>
<li style="font-weight: 400">Application environments</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Redundancy</h1>
                
            
            
                
<p>When we think of redundant Hadoop clusters, we should think about how much redundancy we can keep. As we already know, the <strong>Hadoop Distributed File System</strong> (<strong>HDFS</strong>) has internal data redundancy built in to it.</p>
<p>Given that a Hadoop cluster has lot of ecosystem built around it (services such as YARN, Kafka, and so on), we should think and plan carefully about whether to have the entire ecosystem made redundant or make only the data redundant by keeping it in a different cluster.</p>
<p>It's easier to make the HDFS portion of the Hadoop redundant as there are tools to copy the data from one HDFS to another HDFS.</p>
<p>Let's take a look at possible ways to achieve this via this diagram:</p>
<div><img src="img/9ac21035-486c-41d0-903a-c7b9e1a0b41a.png" style="width:27.50em;height:38.92em;"/></div>
<p>As we can see here, the main Hadoop cluster runs a full stack of all its applications, and data is supplied to it via multiple sources.</p>
<p>We have defined two types of redundant clusters:</p>


            

            
        
    

        

                            
                    <h1 class="header-title">A fully redundant Hadoop cluster</h1>
                
            
            
                
<p>This cluster runs the exact set of applications as the primary cluster and the data is copied periodically from the main Hadoop cluster. Since this is a one-way copy from the main cluster to the second cluster, we can be 100% sure that the main cluster isn't impacted when we make any changes to this fully redundant cluster.</p>
<p>One important thing to understand is that we are running all other instances of applications in this cluster. Since every application maintains its state in its own predefined location, the application states are not replicated from the main Hadoop cluster to this cluster, which means that the jobs that were created in the main Hadoop cluster are not visible in this cluster. The same applies to the Kafka topics, zookeeper nodes, and many more.</p>
<p>This type of cluster is helpful for running different environments such as QA, Staging, and so on.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">A data redundant Hadoop cluster</h1>
                
            
            
                
<p>In this type of cluster setup, we create a new Hadoop cluster and copy the data from the main cluster, like in the previous case; but here we are not worried about the other applications that are run in this cluster.</p>
<p>This type of setup is good for:</p>
<ul>
<li style="font-weight: 400">Having data backup for Hadoop in a different geography</li>
<li style="font-weight: 400">Sharing big data with other enterprises/organizations</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Cold backup</h1>
                
            
            
                
<p>Cold backup is important for enterprises as the data gets older. Even though Hadoop is designed to store unlimited amounts of data, it's not always necessary to keep all the data available for processing.</p>
<p>It is sometimes necessary to preserve the data for auditing purposes and also for historical reasons. In such cases, we can create a dedicated Hadoop cluster with only the HDFS (File System) component and periodically sync all the data into this cluster.</p>
<p>The design for this system is similar to the data redundant Hadoop cluster.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">High availability</h1>
                
            
            
                
<p>Even though Hadoop has multiple components within the architecture, not all the components are highly available due to the internal design.</p>
<p>The core component of Hadoop is its distributed, fault-tolerant, filesystem HDFS. HDS has multiple components one of them is the NameNode which is the registry of where the files are located in the HDFS. In the earlier versions of HDS NameNode was Single point of Failure, In the recent versions Secondary NameNode has been added to assist with high availability requirements for Hadoop Cluster.</p>
<p>In order to make every component of the Hadoop ecosystem a highly available system, we need to add multiple redundant nodes (they come with their own cost) which work together as a cluster.</p>
<p>One more thing to note is that high availability with Hadoop is possible within a single geographical region, as the locality of the data with applications is one of the key things with Hadoop. The moment we have multiple data centers in play we need to think alternatively to achieve high availability across the data centers.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Business continuity</h1>
                
            
            
                
<p>This is part of <strong>Business Continuity Planning </strong>(<strong>BCP</strong>) where natural disasters can bring an end to the Hadoop system, if not planned correctly.</p>
<p>Here, the strategy would be to use multiple geographical regions as providers to run the big data systems. When we talk about multiple data centers, the obvious challenge is the network and the cost associated with managing both systems. One of the biggest challenges is how to keep multiple regions in sync.</p>
<p>One possible solution is to build a fully redundant Hadoop cluster in other geographical regions and keep the data in sync, periodically. In the case of any disaster/breakdown of one region, our businesses won't come to halt as we can smoothly run our operations.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Application environments</h1>
                
            
            
                
<p>Many businesses internally follow different ways of releasing their software to production. As part of this, they follow several continuous integration methodologies, in order to have better control over the stability of the Hadoop environments. It's good to build multiple smaller Hadoop clusters with X% of the data from the main production environment and run all the applications here.</p>
<p>Applications can build their integration tests on these dedicated environments (QA, Staging, and so on) and can release their software to production once everything is good.</p>
<p>One practice that I have come across is that organizations tend to directly ship the code to production and end up facing outage of their applications because of an untested workflow or bug. It's good practice to have dedicated Hadoop application environments to test the software thoroughly and achieve higher uptime and happier customers.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Hadoop data copy</h1>
                
            
            
                
<p>We have seen in the previous sections that, having highly available data is very important for a business to succeed and stay up to date with its competition.</p>
<p>In this section, we will explore the possible ways to achieve highly available data setup.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">HDFS data copy</h1>
                
            
            
                
<p>Hadoop uses HDFS as its core to store the files. HDFS is rack aware and is intelligent enough to reduce the network data transfer when applications are run on the data nodes.</p>
<p>One of the preferred ways of data copying in an HDFS environment is to use the DistCp. The official documentation for this is available at the following URL <a href="http://hadoop.apache.org/docs/r1.2.1/distcp.html">http://hadoop.apache.org/docs/r1.2.1/distcp.html</a>.<a href="http://hadoop.apache.org/docs/r1.2.1/distcp.html"/></p>
<p>We will see a few examples of copying data from one Hadoop cluster to another Hadoop cluster. But before that, let's look at how the data is laid out:</p>
<div><img src="img/5a884670-9141-4abc-890e-e045b5280301.png" style="width:21.50em;height:19.33em;"/></div>
<p>In order to copy the data from the production Hadoop cluster to the backup Hadoop cluster, we can use <kbd>distcp</kbd>. Let's see how to do it:</p>
<pre><strong>hadoop distcp hdfs://NameNode1:8020/projects hdfs://NameNode2:8020/projects</strong><br/><strong>hadoop distcp hdfs://NameNode1:8020/users hdfs://NameNode2:8020/users</strong><br/><strong>hadoop distcp hdfs://NameNode1:8020/streams hdfs://NameNode2:8020/streams</strong><br/><strong>hadoop distcp hdfs://NameNode1:8020/marketing hdfs://NameNode2:8020/marketing</strong><br/><strong>hadoop distcp hdfs://NameNode1:8020/sales hdfs://NameNode2:8020/sales</strong></pre>
<p>When we run the <kbd>distcp</kbd> command, a MapReduce job is created to automatically find out the list of files and then copy them to the destination.</p>
<p>The full command syntax looks like this:</p>
<pre>Distcp [OPTIONS] &lt;source path …&gt; &lt;destination path&gt;</pre>
<ul>
<li><kbd>OPTIONS</kbd>: These are the multiple options the command takes which control the behavior of the execution.</li>
<li><kbd>source path</kbd>: A source path can be any valid File System URI that's supported by Hadoop. DistCp supports taking multiple source paths in one go.</li>
<li><kbd>destination path</kbd>: This is a single path where all the source paths need to be copied.</li>
</ul>
<p>Let's take a closer look at a few of the important options:</p>
<div><table>
<tbody>
<tr>
<td>
<p><strong>Flag/Option</strong></p>
</td>
<td>
<p><strong>Description</strong></p>
</td>
</tr>
<tr>
<td>
<p><kbd>append</kbd></p>
</td>
<td>
<p>Incrementally writes the data to the destination files if they already exist (only <kbd>append</kbd> is performed, no block level check is performed to do incremental copy).</p>
</td>
</tr>
<tr>
<td>
<p><kbd>async</kbd></p>
</td>
<td>
<p>Performs the copy in a non-blocking way.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>atomic</kbd></p>
</td>
<td>
<p>Perform all the file copy or aborts even if one fails.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>Tmp &lt;path&gt;</kbd></p>
</td>
<td>
<p>Path to be used for atomic commit.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>delete</kbd></p>
</td>
<td>
<p>Deletes the files from the destination if they are not present in the source tree.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>Bandwidth &lt;arg&gt;</kbd></p>
</td>
<td>
<p>Limits how much network bandwidth to be used during the copy process.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>f &lt;file-path&gt;</kbd></p>
</td>
<td>
<p>Filename consisting of a list of all paths which need to be copied.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>i</kbd></p>
</td>
<td>
<p>Ignores any errors during file copy.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>Log &lt;file-path&gt;</kbd></p>
</td>
<td>
<p>Location where the execution log is saved.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>M &lt;number&gt;</kbd></p>
</td>
<td>
<p>Maximum number of concurrent maps to use for copying.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>overwrite</kbd></p>
</td>
<td>
<p>Overwrites the files even if they exist on destination.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>update</kbd></p>
</td>
<td>
<p>Copies only the missing files and directories.</p>
</td>
</tr>
<tr>
<td>
<p><kbd>skipcrccheck</kbd></p>
</td>
<td>
<p>If passed, CRC checks are skipped during transfer.</p>
</td>
</tr>
</tbody>
</table>
</div>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>In this chapter, we learned about Apache Ambari and studied its architecture in detail. We then understood how to prepare and create our own Hadoop cluster with Ambari. In order to do this, we also looked into configuring the Ambari server as per the requirement before preparing our cluster. We also learned about single and multiple Hadoop clusters and how they can be used, based on the business requirement.</p>
<p> </p>


            

            
        
    </body></html>