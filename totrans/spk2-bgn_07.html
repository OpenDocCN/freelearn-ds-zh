<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch07"/>Chapter 7.  Spark Machine Learning </h1></div></div></div><p>Calculations based on formulas or algorithms have been used commonly since ancient times to find the output for a given input. But without knowing the formulas or algorithms, computer scientists and mathematicians devised methods to generate formulas or algorithms based on an existing input/output dataset and predicted the output of a new input data based on the generated formulas or algorithms. Generally, this process of learning from a dataset and doing predictions based on the learning is known as machine learning. Machine learning originates from the study of artificial intelligence in computer science.</p><p>Practical machine learning has numerous applications that are being consumed by laymen on a daily basis. YouTube users now get suggestions for the next items to be played in the playlist based on the video they are currently viewing. Popular movie rating sites give ratings and recommendations based on the user preferences of movie genres. Social media web sites such as Facebook suggest a list of names of the users' friends for easy tagging of pictures. What Facebook is doing here is classifying pictures by the names that are already available in the existing albums and checking whether the newly added picture has any similarities with the existing ones. If it finds a similarity, it suggests the name. The applications of this kind of picture identification are manifold. The way all these applications work is based on the huge amount of input/output datasets that have already been collected and the learning that has been done based on those datasets. When a new input dataset arrives, a prediction is made by making use of the learning that the computer or machine has already done.</p><p>We will cover the following topics in this chapter:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Using Spark for machine learning</li><li class="listitem" style="list-style-type: disc">Model persistence</li><li class="listitem" style="list-style-type: disc">Spam filtering</li><li class="listitem" style="list-style-type: disc">Feature algorithms</li><li class="listitem" style="list-style-type: disc">Finding synonyms</li></ul></div><div><div><div><div><h1 class="title"><a id="ch07lvl1sec55"/>Understanding machine learning</h1></div></div></div><p>In traditional computing, input data is fed to a program to generate output. But in machine learning, input data and output data are fed to a machine learning algorithm to generate a function or program that can be used to predict the output of an input according to the learning done on the input/output dataset fed to the machine learning algorithm.</p><p>The data available in the wild may be classified into groups, it may form clusters, or it may fit into certain relationships. These are different kinds of machine learning problem. For example, if there is a databank of pre-owned car sale prices with its associated attributes or features, it is possible to predict the price of a car just by knowing the associated attributes or features. Regression algorithms are used to solve these kinds of problem. If there is a databank of spam and non-spam e-mails, then when a new e-mail comes, it is possible to predict whether the new e-mail is spam or non-spam. Classification algorithms are used to solve these kinds of problem.</p><p>These are just a few machine learning algorithm types. But in general, when using a bank of data, if it is necessary to apply a machine learning algorithm and use that model to make predictions, then the data should be divided into features and outputs. For example, in the case of the car price prediction problem, price is the output, and here are some of the possible features of the data:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Car make</li><li class="listitem" style="list-style-type: disc">Car model</li><li class="listitem" style="list-style-type: disc">Year of manufacture</li><li class="listitem" style="list-style-type: disc">Mileage</li><li class="listitem" style="list-style-type: disc">Fuel type</li><li class="listitem" style="list-style-type: disc">Gearbox type</li></ul></div><p>So whichever machine learning algorithm is being used, there will be a set of features and one or more outputs.</p><div><div><h3 class="title"><a id="note53"/>Note</h3><p>Many books and publications use the term <em>label</em> for output. In other words, <em>features</em> are the input and <em>label</em> is the output.</p></div></div><p>
<em>Figure 1</em> depicts the way a machine learning algorithm works on the underlying data to enable predictions.</p><p>
</p><div><img alt="Understanding machine learning" src="img/image_07_001.jpg"/><div><p>Figure 1</p></div></div><p>
</p><p>Data comes in various shapes and forms. Depending on the machine learning algorithm used, the training data has to be pre-processed to have the features and labels in the right format to be fed to the machine learning algorithm. That in turn generates the appropriate hypothesis function, which takes the features as the input and produces the predicted label.</p><div><div><h3 class="title"><a id="tip54"/>Tip</h3><p>
</p><p>The dictionary definition of the word hypothesis is a supposition or proposed explanation made on the basis of limited evidence as a starting point for further investigation. Here, the function or program that is generated by the machine learning algorithm is based on the limited evidence that is the training data fed to the machine learning algorithm, and hence it is widely known as hypothesis function.</p><p>
</p><p>In other words, this hypothesis function is not a definitive function that produces consistent results all the time with all types of input data. It is rather a function based on the training data. When a new piece of data is added to the training dataset, re-learning is required, and at that time even the hypothesis function generated will change accordingly.</p><p>
</p></div></div><p>In reality, the flow given in <em>Figure 1</em> is not as simple as it seems. Once the model is trained, a lot of testing has to be done on the model to test predictions with known labels. The chain of train and test processes is an iterative process, and in each iteration the parameters of the algorithm are tweaked to make the prediction quality better. Once an acceptable test result is produced by the model, the model can be moved to production for doing the live prediction needs. Spark comes with a machine learning librarythat is rich with capabilities to make practical machine learning a reality.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec56"/>Why Spark for machine learning?</h1></div></div></div><p>The previous chapters covered various data processing functionalities of Spark in detail. Spark's machine learning library uses many Spark core functionalities as well as Spark libraries such as Spark SQL. The Spark machine learning library makes machine learning application development easy by combining data processing and machine learning algorithm implementations in a unified framework with the ability to do data processing on a cluster of nodes, combined with ability to read and write data to a variety of data formats. </p><p>Spark comes with two flavors of the machine learning library. They are <code class="literal">spark.mllib</code> and <code class="literal">spark.ml</code>. The first one is developed on top of Spark's RDD abstraction, and the second one is developed on top of Spark's DataFrame abstraction. It is recommended to use the spark.ml library for any future machine learning application developments. </p><p>This chapter is going to focus only on the spark.ml machine learning library. The following list explains terminology and concepts that are used again and again in this chapter: </p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Estimator</strong>: This is an algorithm that works on top of a Spark DataFrame containing features and labels. It trains on the data provided in the Spark DataFrame and creates a model. This model is used to do future predictions.</li><li class="listitem" style="list-style-type: disc"><strong>Transformer</strong>: This converts a Spark DataFrame containing features and transforms it to another Spark DataFrame containing predictions. The model created by an Estimator is a Transformer. </li><li class="listitem" style="list-style-type: disc"><strong>Parameter</strong>: This is to be used by the Estimators and Transformers. Often, this is specific to the machine learning algorithm. Spark machine learning library comes with a uniform API for specifying the right parameters to the algorithms.</li><li class="listitem" style="list-style-type: disc"><strong>Pipeline</strong>: This is a chain of Estimators and Transformers working together forming a machine learning workflow.</li></ul></div><p>All these new terms are slightly difficult to understand in a theoretical perspective but if an example is given, the concepts will become much clearer. </p></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec57"/>Wine quality prediction</h1></div></div></div><p>The University of California Irvine Machine Learning Repository (<a class="ulink" href="http://archive.ics.uci.edu/ml/index.html">http://archive.ics.uci.edu/ml/index.html</a>) provides a lot of datasets as a service to those who are interested in learning about machine learning. The Wine Quality Dataset (<a class="ulink" href="http://archive.ics.uci.edu/ml/datasets/Wine+Quality">http://archive.ics.uci.edu/ml/datasets/Wine+Quality</a>) is being used here to demonstrate some machine learning applications. It contains two datasets with various features of white and red wines from Portugal. </p><div><div><h3 class="title"><a id="note55"/>Note</h3><p>The Wine Quality Dataset download link lets you download the datasets for red wine and white wine as two separate CSV files. Once those files are downloaded, edit the two datasets to remove the first header line containing the column names. This is to let the programs parse the numerical data without errors. Detailed error handling and excluding the header record are avoided on purpose to focus on the machine learning functionality.</p></div></div><p>The dataset containing various features of red wine is used in this wine quality prediction use case. The following are the features of the dataset:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Fixed acidity </li><li class="listitem" style="list-style-type: disc">Volatile acidity </li><li class="listitem" style="list-style-type: disc">Citric acid </li><li class="listitem" style="list-style-type: disc">Residual sugar </li><li class="listitem" style="list-style-type: disc">Chlorides </li><li class="listitem" style="list-style-type: disc">Free sulfur dioxide </li><li class="listitem" style="list-style-type: disc">Total sulfur dioxide </li><li class="listitem" style="list-style-type: disc">Density </li><li class="listitem" style="list-style-type: disc">pH </li><li class="listitem" style="list-style-type: disc">Sulphates </li><li class="listitem" style="list-style-type: disc">Alcohol </li></ul></div><p>Based on these features, the quality (score between 0 and 10) is determined. Here, quality is the label of this dataset. Using this dataset, a model is going to be trained and then, using the trained model, testing is done and predictions are made. This is a regression problem. The Linear Regression algorithm is used to train the model. The Linear Regression algorithm generates a linear hypothesis function. In mathematical terms, a linear function is a polynomial of degree one or less. In this machine learning application use case, it deals with modeling the relationship between a dependent variable (wine quality) and a set of independent variables (the features of the wine).</p><p>At the Scala REPL prompt, try the following statements:</p><pre class="programlisting">
<strong>scala&gt; import org.apache.spark.ml.regression.LinearRegression
      import org.apache.spark.ml.regression.LinearRegression
    
	scala&gt; import org.apache.spark.ml.param.ParamMap
	
      import org.apache.spark.ml.param.ParamMap
    
	scala&gt; import org.apache.spark.ml.linalg.{Vector, Vectors}
	
      import org.apache.spark.ml.linalg.{Vector, Vectors}
    
	scala&gt; import org.apache.spark.sql.Row
	
      import org.apache.spark.sql.Row
    
	scala&gt; // TODO - Change this directory to the right location where the data
    is stored
	scala&gt; val dataDir = "/Users/RajT/Downloads/wine-quality/"
	
      dataDir: String = /Users/RajT/Downloads/wine-quality/
    
	scala&gt; // Define the case class that holds the wine data
	scala&gt; case class Wine(FixedAcidity: Double, VolatileAcidity: Double, CitricAcid: Double, ResidualSugar: Double, Chlorides: Double, FreeSulfurDioxide: Double, TotalSulfurDioxide: Double, Density: Double, PH: Double, Sulphates: Double, Alcohol: Double, Quality: Double)
	
      defined class Wine
    
	scala&gt; // Create the the RDD by reading the wine data from the disk 
	scala&gt; //TODO - The wine data has to be downloaded to the appropriate working directory in the system where this is being run and the following line of code should use that path
	scala&gt; val wineDataRDD = sc.textFile(dataDir + "winequality-red.csv").map(_.split(";")).map(w =&gt; Wine(w(0).toDouble, w(1).toDouble, w(2).toDouble, w(3).toDouble, w(4).toDouble, w(5).toDouble, w(6).toDouble, w(7).toDouble, w(8).toDouble, w(9).toDouble, w(10).toDouble, w(11).toDouble))
	
      wineDataRDD: org.apache.spark.rdd.RDD[Wine] = MapPartitionsRDD[3] at map at &lt;console&gt;:32
    
	scala&gt; // Create the data frame containing the training data having two columns. 1) The actual output or label of the data 2) The vector containing the features
	scala&gt; //Vector is a data type with 0 based indices and double-typed values. In that there are two types namely dense and sparse.
	scala&gt; //A dense vector is backed by a double array representing its entry values 
	scala&gt; //A sparse vector is backed by two parallel arrays: indices and values
	scala&gt; val trainingDF = wineDataRDD.map(w =&gt; (w.Quality, Vectors.dense(w.FixedAcidity, w.VolatileAcidity, w.CitricAcid, w.ResidualSugar, w.Chlorides, w.FreeSulfurDioxide, w.TotalSulfurDioxide, w.Density, w.PH, w.Sulphates, w.Alcohol))).toDF("label", "features")
	
      trainingDF: org.apache.spark.sql.DataFrame = [label: double, features: vector]
    scala&gt; trainingDF.show()
	
      +-----+--------------------+
    
      |label|            features|
    
      +-----+--------------------+
    
      |  5.0|[7.4,0.7,0.0,1.9,...|
    
      |  5.0|[7.8,0.88,0.0,2.6...|
    
      |  5.0|[7.8,0.76,0.04,2....|
    
      |  6.0|[11.2,0.28,0.56,1...|
    
      |  5.0|[7.4,0.7,0.0,1.9,...|
    
      |  5.0|[7.4,0.66,0.0,1.8...|
    
      |  5.0|[7.9,0.6,0.06,1.6...|
    
      |  7.0|[7.3,0.65,0.0,1.2...|
    
      |  7.0|[7.8,0.58,0.02,2....|
    
      |  5.0|[7.5,0.5,0.36,6.1...|
    
      |  5.0|[6.7,0.58,0.08,1....|
    
      |  5.0|[7.5,0.5,0.36,6.1...|
    
      |  5.0|[5.6,0.615,0.0,1....|
    
      |  5.0|[7.8,0.61,0.29,1....|
    
      |  5.0|[8.9,0.62,0.18,3....|
    
      |  5.0|[8.9,0.62,0.19,3....|
    
      |  7.0|[8.5,0.28,0.56,1....|
    
      |  5.0|[8.1,0.56,0.28,1....|
    
      |  4.0|[7.4,0.59,0.08,4....|
    
      |  6.0|[7.9,0.32,0.51,1....|
    
      +-----+--------------------+
    
      only showing top 20 rows
    scala&gt; // Create the object of the algorithm which is the Linear Regression
	scala&gt; val lr = new LinearRegression()
      lr: org.apache.spark.ml.regression.LinearRegression = linReg_f810f0c1617b
    scala&gt; // Linear regression parameter to make lr.fit() use at most 10 iterations
	scala&gt; lr.setMaxIter(10)
      res1: lr.type = linReg_f810f0c1617b
    scala&gt; // Create a trained model by fitting the parameters using the training data
	scala&gt; val model = lr.fit(trainingDF)
      model: org.apache.spark.ml.regression.LinearRegressionModel = linReg_f810f0c1617b
    scala&gt; // Once the model is prepared, to test the model, prepare the test data containing the labels and feature vectors
	scala&gt; val testDF = spark.createDataFrame(Seq((5.0, Vectors.dense(7.4, 0.7, 0.0, 1.9, 0.076, 25.0, 67.0, 0.9968, 3.2, 0.68,9.8)),(5.0, Vectors.dense(7.8, 0.88, 0.0, 2.6, 0.098, 11.0, 34.0, 0.9978, 3.51, 0.56, 9.4)),(7.0, Vectors.dense(7.3, 0.65, 0.0, 1.2, 0.065, 15.0, 18.0, 0.9968, 3.36, 0.57, 9.5)))).toDF("label", "features")
      testDF: org.apache.spark.sql.DataFrame = [label: double, features: vector]
    scala&gt; testDF.show()
      +-----+--------------------+
    
      |label|            features|
    
      +-----+--------------------+
    
      |  5.0|[7.4,0.7,0.0,1.9,...|
    
      |  5.0|[7.8,0.88,0.0,2.6...|
    
      |  7.0|[7.3,0.65,0.0,1.2...|
    
      +-----+--------------------+
    scala&gt; testDF.createOrReplaceTempView("test")scala&gt; // Do the transformation of the test data using the model and predict the output values or lables. This is to compare the predicted value and the actual label value
	scala&gt; val tested = model.transform(testDF).select("features", "label", "prediction")
      tested: org.apache.spark.sql.DataFrame = [features: vector, label: double ... 1 more field]
    scala&gt; tested.show()
      +--------------------+-----+-----------------+
    
      |            features|label|       prediction|
    
      +--------------------+-----+-----------------+
    
      |[7.4,0.7,0.0,1.9,...|  5.0|5.352730835898477|
    
      |[7.8,0.88,0.0,2.6...|  5.0|4.817999362011964|
    
      |[7.3,0.65,0.0,1.2...|  7.0|5.280106355653388|
    
      +--------------------+-----+-----------------+
    scala&gt; // Prepare a dataset without the output/lables to predict the output using the trained model
	scala&gt; val predictDF = spark.sql("SELECT features FROM test")predictDF: org.apache.spark.sql.DataFrame = [features: vector]
	scala&gt; predictDF.show()
      +--------------------+
    
      |            features|
    
      +--------------------+
    
      |[7.4,0.7,0.0,1.9,...|
    
      |[7.8,0.88,0.0,2.6...|
    
      |[7.3,0.65,0.0,1.2...|
    
      +--------------------+
    scala&gt; // Do the transformation with the predict dataset and display the predictions
	scala&gt; val predicted = model.transform(predictDF).select("features", "prediction")
      predicted: org.apache.spark.sql.DataFrame = [features: vector, prediction: double]
    scala&gt; predicted.show()
      +--------------------+-----------------+
    
      |            features|       prediction|
    
      +--------------------+-----------------+
    
      |[7.4,0.7,0.0,1.9,...|5.352730835898477|
    
      |[7.8,0.88,0.0,2.6...|4.817999362011964|
    
      |[7.3,0.65,0.0,1.2...|5.280106355653388|
    
      +--------------------+-----------------+
    scala&gt; //IMPORTANT - To continue with the model persistence coming in the next section, keep this session on.</strong>
</pre><p>The preceding code does a lot of things. It performs the following chain of activities in a pipeline:</p><div><ol class="orderedlist arabic"><li class="listitem">It reads the wine data from the data file to form a training DataFrame.</li><li class="listitem">Then it creates a <code class="literal">LinearRegression</code> object and sets the parameters.</li><li class="listitem">It fits the model with the training data and this completes the estimator pipeline.</li><li class="listitem">It creates a DataFrame containing test data. Typically, the test data will have both features and labels. This is to make sure that the model is right and used for comparing the predicted label and actual label.</li><li class="listitem">Using the model created, it does a transformation with the test data, and from the DataFrame produced, extracts the features, input labels, and predictions. Note that while doing the transformation using the model, the labels are not required. In other words, the labels will not be used at all.</li><li class="listitem">Using the model created, it does a transformation with the prediction data and from the DataFrame produced, extracts the features and predictions. Note that while doing the transformation using the model, the labels are not used. In other words, the labels are not used while doing the predictions. This completes a transformer pipeline.</li></ol></div><div><div><h3 class="title"><a id="tip56"/>Tip</h3><p>The pipelines in the preceding code snippet are single stage pipelines, and for this reason there is no need to use the Pipeline object. Multiple stage pipelines are going to be discussed in the following sections.</p></div></div><p>The fitting/testing phases are repeated iteratively in real-world use cases until the model is giving the desired results when doing predictions. Figure 2 elucidates the pipeline concept that is demonstrated through the code:</p><p>
</p><div><img alt="Wine quality prediction" src="img/image_07_002.jpg"/></div><p>
</p><p>Figure 2</p><p>The following code demonstrates the same use case using Python. At the Python REPL prompt, try the following statements: </p><pre class="programlisting">
<strong>
	&gt;&gt;&gt; from pyspark.ml.linalg import Vectors
	&gt;&gt;&gt; from pyspark.ml.regression import LinearRegression
	&gt;&gt;&gt; from pyspark.ml.param import Param, Params
	&gt;&gt;&gt; from pyspark.sql import Row
	&gt;&gt;&gt; # TODO - Change this directory to the right location where the data is stored
	&gt;&gt;&gt; dataDir = "/Users/RajT/Downloads/wine-quality/"
	&gt;&gt;&gt; # Create the the RDD by reading the wine data from the disk 
	&gt;&gt;&gt; lines = sc.textFile(dataDir + "winequality-red.csv")
	&gt;&gt;&gt; splitLines = lines.map(lambda l: l.split(";"))
	&gt;&gt;&gt; # Vector is a data type with 0 based indices and double-typed values. In that there are two types namely dense and sparse.
	&gt;&gt;&gt; # A dense vector is backed by a double array representing its entry values
	&gt;&gt;&gt; # A sparse vector is backed by two parallel arrays: indices and values
	&gt;&gt;&gt; wineDataRDD = splitLines.map(lambda p: (float(p[11]), Vectors.dense([float(p[0]), float(p[1]), float(p[2]), float(p[3]), float(p[4]), float(p[5]), float(p[6]), float(p[7]), float(p[8]), float(p[9]), float(p[10])])))
	&gt;&gt;&gt; # Create the data frame containing the training data having two columns. 1) The actula output or label of the data 2) The vector containing the features
	&gt;&gt;&gt; trainingDF = spark.createDataFrame(wineDataRDD, ['label', 'features'])
	&gt;&gt;&gt; trainingDF.show()
	
      +-----+--------------------+
    
      |label|            features|
    
      +-----+--------------------+
    
      |  5.0|[7.4,0.7,0.0,1.9,...|
    
      |  5.0|[7.8,0.88,0.0,2.6...|
    
      |  5.0|[7.8,0.76,0.04,2....|
    
      |  6.0|[11.2,0.28,0.56,1...|
    
      |  5.0|[7.4,0.7,0.0,1.9,...|
    
      |  5.0|[7.4,0.66,0.0,1.8...|
    
      |  5.0|[7.9,0.6,0.06,1.6...|
    
      |  7.0|[7.3,0.65,0.0,1.2...|
    
      |  7.0|[7.8,0.58,0.02,2....|
    
      |  5.0|[7.5,0.5,0.36,6.1...|
    
      |  5.0|[6.7,0.58,0.08,1....|
    
      |  5.0|[7.5,0.5,0.36,6.1...|
    
      |  5.0|[5.6,0.615,0.0,1....|
    
      |  5.0|[7.8,0.61,0.29,1....|
    
      |  5.0|[8.9,0.62,0.18,3....|
    
      |  5.0|[8.9,0.62,0.19,3....|
    
      |  7.0|[8.5,0.28,0.56,1....|
    
      |  5.0|[8.1,0.56,0.28,1....|
    
      |  4.0|[7.4,0.59,0.08,4....|
    
      |  6.0|[7.9,0.32,0.51,1....|
    
      +-----+--------------------+
    
      only showing top 20 rows
    
	&gt;&gt;&gt; # Create the object of the algorithm which is the Linear Regression with the parameters
	&gt;&gt;&gt; # Linear regression parameter to make lr.fit() use at most 10 iterations
	&gt;&gt;&gt; lr = LinearRegression(maxIter=10)
	&gt;&gt;&gt; # Create a trained model by fitting the parameters using the training data
	&gt;&gt;&gt; model = lr.fit(trainingDF)
	&gt;&gt;&gt; # Once the model is prepared, to test the model, prepare the test data containing the labels and feature vectors 
	&gt;&gt;&gt; testDF = spark.createDataFrame([(5.0, Vectors.dense([7.4, 0.7, 0.0, 1.9, 0.076, 25.0, 67.0, 0.9968, 3.2, 0.68,9.8])),(5.0,Vectors.dense([7.8, 0.88, 0.0, 2.6, 0.098, 11.0, 34.0, 0.9978, 3.51, 0.56, 9.4])),(7.0, Vectors.dense([7.3, 0.65, 0.0, 1.2, 0.065, 15.0, 18.0, 0.9968, 3.36, 0.57, 9.5]))], ["label", "features"])
	&gt;&gt;&gt; testDF.createOrReplaceTempView("test")
	&gt;&gt;&gt; testDF.show()
	
      +-----+--------------------+
    
      |label|            features|
    
      +-----+--------------------+
    
      |  5.0|[7.4,0.7,0.0,1.9,...|
    
      |  5.0|[7.8,0.88,0.0,2.6...|
    
      |  7.0|[7.3,0.65,0.0,1.2...|
    
      +-----+--------------------+
    &gt;&gt;&gt; # Do the transformation of the test data using the model and predict the output values or lables. This is to compare the predicted value and the actual label value
	&gt;&gt;&gt; testTransform = model.transform(testDF)
	&gt;&gt;&gt; tested = testTransform.select("features", "label", "prediction")
	&gt;&gt;&gt; tested.show()
	
      +--------------------+-----+-----------------+
    
      |            features|label|       prediction|
    
      +--------------------+-----+-----------------+
    
      |[7.4,0.7,0.0,1.9,...|  5.0|5.352730835898477|
    
      |[7.8,0.88,0.0,2.6...|  5.0|4.817999362011964|
    
      |[7.3,0.65,0.0,1.2...|  7.0|5.280106355653388|
    
      +--------------------+-----+-----------------+
    
	&gt;&gt;&gt; # Prepare a dataset without the output/lables to predict the output using the trained model
	&gt;&gt;&gt; predictDF = spark.sql("SELECT features FROM test")
	&gt;&gt;&gt; predictDF.show()
	
      +--------------------+
    
      |            features|
    
      +--------------------+
    
      |[7.4,0.7,0.0,1.9,...|
    
      |[7.8,0.88,0.0,2.6...|
    
      |[7.3,0.65,0.0,1.2...|
    
      +--------------------+
    
	&gt;&gt;&gt; # Do the transformation with the predict dataset and display the predictions
	&gt;&gt;&gt; predictTransform = model.transform(predictDF)
	&gt;&gt;&gt; predicted = predictTransform.select("features", "prediction")
	&gt;&gt;&gt; predicted.show()
	
      +--------------------+-----------------+
    
      |            features|       prediction|
    
      +--------------------+-----------------+
    
      |[7.4,0.7,0.0,1.9,...|5.352730835898477|
    
      |[7.8,0.88,0.0,2.6...|4.817999362011964|
    
      |[7.3,0.65,0.0,1.2...|5.280106355653388|
    
      +--------------------+-----------------+
    
	&gt;&gt;&gt; #IMPORTANT - To continue with the model persistence coming in the next section, keep this session on.</strong>
</pre><p>As mentioned earlier, linear regression is statistical model and an approach for modeling the relationship between two types of variable. One is an independent variable, and the other is a dependent variable. The dependent variable is computed from the independent variables. In many cases, if there is only one independent variable, then the regression will be a simple linear regression. But in reality, in practical real-world use cases, there will be multitude of independent variables, just like in the wine dataset. This falls into the case of multiple linear regressions. This should not be confused with multivariate linear regression. In multivariate regression, multiple and correlated dependent variables are predicted.</p><p>In the use case that is being discussed here, the prediction is only done for one variable, which is the quality of the wine and hence it is a multiple linear regression and not a multivariate linear regression problem. Some schools even use multiple linear regression as univariate linear regression. In other words, irrespective of the number of independent variables, if there is only one dependent variable, it is termed as univariate linear regression.</p></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec58"/>Model persistence</h1></div></div></div><p>Spark 2.0 comes with the ability to save and load machine learning models across programming languages with ease. In other words, you can create a machine learning model in Scala and load it in Python. This allows us to create a model in one system, save it, copy it, and use it in other systems. Continuing with the same Scala REPL prompt, try the following statements:</p><pre class="programlisting">
<strong>
	scala&gt; // Assuming that the model definition line "val model = 
    lr.fit(trainingDF)" is still in context
	scala&gt; import org.apache.spark.ml.regression.LinearRegressionModel
	
      import org.apache.spark.ml.regression.LinearRegressionModel
    
	scala&gt; model.save("wineLRModelPath")
	scala&gt; val newModel = LinearRegressionModel.load("wineLRModelPath")
	
      newModel: org.apache.spark.ml.regression.LinearRegressionModel = 
      linReg_6a880215ab96
    
	</strong>
</pre><p>Now the loaded model can be used for testing or prediction, just like the original model. Continuing with the same Python REPL prompt, try the following statements to load the model saved using the Scala program:</p><pre class="programlisting">
<strong>
	&gt;&gt;&gt; from pyspark.ml.regression import LinearRegressionModel
	&gt;&gt;&gt; newModel = LinearRegressionModel.load("wineLRModelPath")
	&gt;&gt;&gt; newPredictTransform = newModel.transform(predictDF) 
	&gt;&gt;&gt; newPredicted = newPredictTransform.select("features", "prediction")
	&gt;&gt;&gt; newPredicted.show()
	
      +--------------------+-----------------+
    
      |            features|       prediction|
    
      +--------------------+-----------------+
    
      |[7.4,0.7,0.0,1.9,...|5.352730835898477|
    
      |[7.8,0.88,0.0,2.6...|4.817999362011964|
    
      |[7.3,0.65,0.0,1.2...|5.280106355653388|
    
      +--------------------+-----------------+
    </strong>
</pre></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec59"/>Wine classification</h1></div></div></div><p>The dataset containing various features of white wine is used in this wine quality classification use case. The following are the features of the dataset:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Fixed acidity </li><li class="listitem" style="list-style-type: disc">Volatile acidity </li><li class="listitem" style="list-style-type: disc">Citric acid </li><li class="listitem" style="list-style-type: disc">Residual sugar </li><li class="listitem" style="list-style-type: disc">Chlorides </li><li class="listitem" style="list-style-type: disc">Free sulfur dioxide </li><li class="listitem" style="list-style-type: disc">Total sulfur dioxide </li><li class="listitem" style="list-style-type: disc">Density </li><li class="listitem" style="list-style-type: disc">pH </li><li class="listitem" style="list-style-type: disc">Sulphates </li><li class="listitem" style="list-style-type: disc">Alcohol </li></ul></div><p>Based on these features, the quality (score between 0 and 10) is determined. If the quality is less than 7, then it is classified as bad and a value of 0 is assigned to the label. If the quality is 7 or above, then it is classified as good and a value of 1 is assigned to the label. In other words, the classification value is the label of this dataset. Using this dataset, a model is going to be trained and then using the trained model, testing is done and predictions are made. This is a classification problem. The Logistic Regression algorithm is used to train the model. In this machine learning application use case, it deals with modeling the relationship between a dependent variable (wine quality) and a set of independent variables (the features of the wine). At the Scala REPL prompt, try the following statements:</p><pre class="programlisting">
	<strong>
	scala&gt; import org.apache.spark.ml.classification.LogisticRegression
	
      import org.apache.spark.ml.classification.LogisticRegression
    
	scala&gt; import org.apache.spark.ml.param.ParamMap
	
      import org.apache.spark.ml.param.ParamMap
    
	scala&gt; import org.apache.spark.ml.linalg.{Vector, Vectors}
	
      import org.apache.spark.ml.linalg.{Vector, Vectors}
    scala&gt; import org.apache.spark.sql.Row
	
      import org.apache.spark.sql.Row
    
	scala&gt; // TODO - Change this directory to the right location where the data is stored
	scala&gt; val dataDir = "/Users/RajT/Downloads/wine-quality/"
	
      dataDir: String = /Users/RajT/Downloads/wine-quality/
    
	scala&gt; // Define the case class that holds the wine data
	scala&gt; case class Wine(FixedAcidity: Double, VolatileAcidity: Double, CitricAcid: Double, ResidualSugar: Double, Chlorides: Double, FreeSulfurDioxide: Double, TotalSulfurDioxide: Double, Density: Double, PH: Double, Sulphates: Double, Alcohol: Double, Quality: Double)
	
      defined class Wine
    
	scala&gt; // Create the the RDD by reading the wine data from the disk 
	scala&gt; val wineDataRDD = sc.textFile(dataDir + "winequality-white.csv").map(_.split(";")).map(w =&gt; Wine(w(0).toDouble, w(1).toDouble, w(2).toDouble, w(3).toDouble, w(4).toDouble, w(5).toDouble, w(6).toDouble, w(7).toDouble, w(8).toDouble, w(9).toDouble, w(10).toDouble, w(11).toDouble))
	
      wineDataRDD: org.apache.spark.rdd.RDD[Wine] = MapPartitionsRDD[35] at map at &lt;console&gt;:36
    
	scala&gt; // Create the data frame containing the training data having two columns. 1) The actula output or label of the data 2) The vector containing the features
	scala&gt; val trainingDF = wineDataRDD.map(w =&gt; (if(w.Quality &lt; 7) 0D else 1D, Vectors.dense(w.FixedAcidity, w.VolatileAcidity, w.CitricAcid, w.ResidualSugar, w.Chlorides, w.FreeSulfurDioxide, w.TotalSulfurDioxide, w.Density, w.PH, w.Sulphates, w.Alcohol))).toDF("label", "features")
	
      trainingDF: org.apache.spark.sql.DataFrame = [label: double, features: vector]
    
	scala&gt; trainingDF.show()
	
      +-----+--------------------+
    
      |label|            features|
    
      +-----+--------------------+
    
      |  0.0|[7.0,0.27,0.36,20...|
    
      |  0.0|[6.3,0.3,0.34,1.6...|
    
      |  0.0|[8.1,0.28,0.4,6.9...|
    
      |  0.0|[7.2,0.23,0.32,8....|
    
      |  0.0|[7.2,0.23,0.32,8....|
    
      |  0.0|[8.1,0.28,0.4,6.9...|
    
      |  0.0|[6.2,0.32,0.16,7....|
    
      |  0.0|[7.0,0.27,0.36,20...|
    
      |  0.0|[6.3,0.3,0.34,1.6...|
    
      |  0.0|[8.1,0.22,0.43,1....|
    
      |  0.0|[8.1,0.27,0.41,1....|
    
      |  0.0|[8.6,0.23,0.4,4.2...|
    
      |  0.0|[7.9,0.18,0.37,1....|
    
      |  1.0|[6.6,0.16,0.4,1.5...|
    
      |  0.0|[8.3,0.42,0.62,19...|
    
      |  1.0|[6.6,0.17,0.38,1....|
    
      |  0.0|[6.3,0.48,0.04,1....|
    
      |  1.0|[6.2,0.66,0.48,1....|
    
      |  0.0|[7.4,0.34,0.42,1....|
    
      |  0.0|[6.5,0.31,0.14,7....|
    
      +-----+--------------------+
    
      only showing top 20 rows
    
	scala&gt; // Create the object of the algorithm which is the Logistic Regression
	scala&gt; val lr = new LogisticRegression()
	
      lr: org.apache.spark.ml.classification.LogisticRegression = logreg_a7e219daf3e1
    
	scala&gt; // LogisticRegression parameter to make lr.fit() use at most 10 iterations and the regularization parameter.
	scala&gt; // When a higher degree polynomial used by the algorithm to fit a set of points in a linear regression model, to prevent overfitting, regularization is used and this parameter is just for that
	scala&gt; lr.setMaxIter(10).setRegParam(0.01)
	
      res8: lr.type = logreg_a7e219daf3e1
    
	scala&gt; // Create a trained model by fitting the parameters using the training data
	scala&gt; val model = lr.fit(trainingDF)
	
      model: org.apache.spark.ml.classification.LogisticRegressionModel = logreg_a7e219daf3e1
    
	scala&gt; // Once the model is prepared, to test the model, prepare the test data containing the labels and feature vectors
	scala&gt; val testDF = spark.createDataFrame(Seq((1.0, Vectors.dense(6.1,0.32,0.24,1.5,0.036,43,140,0.9894,3.36,0.64,10.7)),(0.0, Vectors.dense(5.2,0.44,0.04,1.4,0.036,38,124,0.9898,3.29,0.42,12.4)),(0.0, Vectors.dense(7.2,0.32,0.47,5.1,0.044,19,65,0.9951,3.38,0.36,9)),(0.0,Vectors.dense(6.4,0.595,0.14,5.2,0.058,15,97,0.991,3.03,0.41,12.6)))).toDF("label", "features")
	
      testDF: org.apache.spark.sql.DataFrame = [label: double, features: vector]
    
	scala&gt; testDF.show()
	
      +-----+--------------------+
    
      |label|            features|
    
      +-----+--------------------+
    
      |  1.0|[6.1,0.32,0.24,1....|
    
      |  0.0|[5.2,0.44,0.04,1....|
    
      |  0.0|[7.2,0.32,0.47,5....|
    
      |  0.0|[6.4,0.595,0.14,5...|
    
      +-----+--------------------+
    scala&gt; testDF.createOrReplaceTempView("test")
	scala&gt; // Do the transformation of the test data using the model and predict the output values or labels. This is to compare the predicted value and the actual label value
	scala&gt; val tested = model.transform(testDF).select("features", "label", "prediction")
      tested: org.apache.spark.sql.DataFrame = [features: vector, label: double ... 1 more field]
    
	scala&gt; tested.show()
      +--------------------+-----+----------+
    
      |            features|label|prediction|
    
      +--------------------+-----+----------+
    
      |[6.1,0.32,0.24,1....|  1.0|       0.0|
    
      |[5.2,0.44,0.04,1....|  0.0|       0.0|
    
      |[7.2,0.32,0.47,5....|  0.0|       0.0|
    
      |[6.4,0.595,0.14,5...|  0.0|       0.0|
    
      +--------------------+-----+----------+
    
	scala&gt; // Prepare a dataset without the output/lables to predict the output using the trained model
	scala&gt; val predictDF = spark.sql("SELECT features FROM test")
	
      predictDF: org.apache.spark.sql.DataFrame = [features: vector]
    
	scala&gt; predictDF.show()
	
      +--------------------+
    
      |            features|
    
      +--------------------+
    
      |[6.1,0.32,0.24,1....|
    
      |[5.2,0.44,0.04,1....|
    
      |[7.2,0.32,0.47,5....|
    
      |[6.4,0.595,0.14,5...|
    
      +--------------------+
    
	scala&gt; // Do the transformation with the predict dataset and display the predictions
	scala&gt; val predicted = model.transform(predictDF).select("features", "prediction")
	
      predicted: org.apache.spark.sql.DataFrame = [features: vector, prediction: double]
    
	scala&gt; predicted.show()
	
      +--------------------+----------+
    
      |            features|prediction|
    
      +--------------------+----------+
    
      |[6.1,0.32,0.24,1....|       0.0|
    
      |[5.2,0.44,0.04,1....|       0.0|
    
      |[7.2,0.32,0.47,5....|       0.0|
    
      |[6.4,0.595,0.14,5...|       0.0|
    
      +--------------------+----------+
	  </strong>
</pre><p>The preceding code snippet works exactly like a linear regression use case, except for the model used here. The model used here is Logistic Regression and its label takes only two values, 0 and 1. Creating the model, testing the model, and then the predictions are all similar here. In other words, the pipelines look very similar.</p><p>The following code demonstrates the same use case using Python. At the Python REPL prompt, try the following statements:</p><pre class="programlisting">
<strong>
	  &gt;&gt;&gt; from pyspark.ml.linalg import Vectors
	  &gt;&gt;&gt; from pyspark.ml.classification import LogisticRegression
	  &gt;&gt;&gt; from pyspark.ml.param import Param, Params
	  &gt;&gt;&gt; from pyspark.sql import Row
	  &gt;&gt;&gt; # TODO - Change this directory to the right location where the data is stored
	  &gt;&gt;&gt; dataDir = "/Users/RajT/Downloads/wine-quality/"
	  &gt;&gt;&gt; # Create the the RDD by reading the wine data from the disk 
	  &gt;&gt;&gt; lines = sc.textFile(dataDir + "winequality-white.csv")
	  &gt;&gt;&gt; splitLines = lines.map(lambda l: l.split(";"))
	  &gt;&gt;&gt; wineDataRDD = splitLines.map(lambda p: (float(0) if (float(p[11]) &lt; 7) else float(1), Vectors.dense([float(p[0]), float(p[1]), float(p[2]), float(p[3]), float(p[4]), float(p[5]), float(p[6]), float(p[7]), float(p[8]), float(p[9]), float(p[10])])))
	  &gt;&gt;&gt; # Create the data frame containing the training data having two columns. 1) The actula output or label of the data 2) The vector containing the features
	  &gt;&gt;&gt; trainingDF = spark.createDataFrame(wineDataRDD, ['label', 'features'])
	  &gt;&gt;&gt; trainingDF.show()
	  +-----+--------------------+
	  |label|            features|
	  
      +-----+--------------------+
    
      |  0.0|[7.0,0.27,0.36,20...|
    
      |  0.0|[6.3,0.3,0.34,1.6...|
    
      |  0.0|[8.1,0.28,0.4,6.9...|
    
      |  0.0|[7.2,0.23,0.32,8....|
    
      |  0.0|[7.2,0.23,0.32,8....|
    
      |  0.0|[8.1,0.28,0.4,6.9...|
    
      |  0.0|[6.2,0.32,0.16,7....|
    
      |  0.0|[7.0,0.27,0.36,20...|
    
      |  0.0|[6.3,0.3,0.34,1.6...|
    
      |  0.0|[8.1,0.22,0.43,1....|
    
      |  0.0|[8.1,0.27,0.41,1....|
    
      |  0.0|[8.6,0.23,0.4,4.2...|
    
      |  0.0|[7.9,0.18,0.37,1....|
    
      |  1.0|[6.6,0.16,0.4,1.5...|
    
      |  0.0|[8.3,0.42,0.62,19...|
    
      |  1.0|[6.6,0.17,0.38,1....|
    
      |  0.0|[6.3,0.48,0.04,1....|
    
      |  1.0|[6.2,0.66,0.48,1....|
    
      |  0.0|[7.4,0.34,0.42,1....|
    
      |  0.0|[6.5,0.31,0.14,7....|
    
      +-----+--------------------+
    
      only showing top 20 rows
    
	&gt;&gt;&gt; # Create the object of the algorithm which is the Logistic Regression with the parameters
	&gt;&gt;&gt; # LogisticRegression parameter to make lr.fit() use at most 10 iterations and the regularization parameter.
	&gt;&gt;&gt; # When a higher degree polynomial used by the algorithm to fit a set of points in a linear regression model, to prevent overfitting, regularization is used and this parameter is just for that
	&gt;&gt;&gt; lr = LogisticRegression(maxIter=10, regParam=0.01)
	&gt;&gt;&gt; # Create a trained model by fitting the parameters using the training data&gt;&gt;&gt; model = lr.fit(trainingDF)
	&gt;&gt;&gt; # Once the model is prepared, to test the model, prepare the test data containing the labels and feature vectors
	&gt;&gt;&gt; testDF = spark.createDataFrame([(1.0, Vectors.dense([6.1,0.32,0.24,1.5,0.036,43,140,0.9894,3.36,0.64,10.7])),(0.0, Vectors.dense([5.2,0.44,0.04,1.4,0.036,38,124,0.9898,3.29,0.42,12.4])),(0.0, Vectors.dense([7.2,0.32,0.47,5.1,0.044,19,65,0.9951,3.38,0.36,9])),(0.0, Vectors.dense([6.4,0.595,0.14,5.2,0.058,15,97,0.991,3.03,0.41,12.6]))], ["label", "features"])
	&gt;&gt;&gt; testDF.createOrReplaceTempView("test")
	&gt;&gt;&gt; testDF.show()
	
      +-----+--------------------+
    
      |label|            features|
    
      +-----+--------------------+
    
      |  1.0|[6.1,0.32,0.24,1....|
    
      |  0.0|[5.2,0.44,0.04,1....|
    
      |  0.0|[7.2,0.32,0.47,5....|
    
      |  0.0|[6.4,0.595,0.14,5...|
    
      +-----+--------------------+
    
	&gt;&gt;&gt; # Do the transformation of the test data using the model and predict the output values or lables. This is to compare the predicted value and the actual label value
	&gt;&gt;&gt; testTransform = model.transform(testDF)
	&gt;&gt;&gt; tested = testTransform.select("features", "label", "prediction")
	&gt;&gt;&gt; tested.show()
	
      +--------------------+-----+----------+
    
	
      |            features|label|prediction|
    
	
      +--------------------+-----+----------+
    
      |[6.1,0.32,0.24,1....|  1.0|       0.0|
    
      |[5.2,0.44,0.04,1....|  0.0|       0.0|
    
      |[7.2,0.32,0.47,5....|  0.0|       0.0|
    
      |[6.4,0.595,0.14,5...|  0.0|       0.0|
    
      +--------------------+-----+----------+
    
	&gt;&gt;&gt; # Prepare a dataset without the output/lables to predict the output using the trained model
	&gt;&gt;&gt; predictDF = spark.sql("SELECT features FROM test")
	&gt;&gt;&gt; predictDF.show()
	
      +--------------------+
    
      |            features|
    
      +--------------------+
    
      |[6.1,0.32,0.24,1....|
    
      |[5.2,0.44,0.04,1....|
    
      |[7.2,0.32,0.47,5....|
    
      |[6.4,0.595,0.14,5...|
    
      +--------------------+
    
	&gt;&gt;&gt; # Do the transformation with the predict dataset and display the predictions
	&gt;&gt;&gt; predictTransform = model.transform(predictDF)
	&gt;&gt;&gt; predicted = testTransform.select("features", "prediction")
	&gt;&gt;&gt; predicted.show()
      +--------------------+----------+
    
      |            features|prediction|
    
      +--------------------+----------+
    
      |[6.1,0.32,0.24,1....|       0.0|
    
      |[5.2,0.44,0.04,1....|       0.0|
    
      |[7.2,0.32,0.47,5....|       0.0|
    
      |[6.4,0.595,0.14,5...|       0.0|
    
      +--------------------+----------+</strong>
</pre><p>Logistic regression is very similar to linear regression. The major difference in Logistic regression is that its dependent variable is a categorical variable. In other words, the dependent variable takes only a selected set of values. In this use case the values are 0 or 1. The value 0 means that the wine quality is bad and the value 1 means that the wine quality is good. To be more precise, here, the dependent variable used is a binary dependent variable.</p><p>The use cases covered so far have only a handful of features. But in real-world use cases the number of features is going to be really huge, especially in machine learning use cases where lots of text processing is done. The next section is going to discuss one such use case.</p></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec60"/>Spam filtering</h1></div></div></div><p>Spam filtering is a very common use case that is used in many applications. It is ubiquitous in e-mail applications. It is one of the most widely used classification problems. In a typical mail server, a huge number of e-mails are processed. The spam filtering is done on the e-mails received before they are delivered to the recipient's mailboxes. For any machine learning algorithm, a model has to be trained before making a prediction. To train the model, training data is required. How is training data collected? A trivial way is that the users themselves mark some of the e-mails received as spam. Use all the e-mails in the mail server as training data and keep refreshing the model on a regular basis. This includes spam and non-spam e-mails. When the model has a good sample of both kinds of e-mail, the prediction is going to be good.</p><p>The spam filtering use case covered here is not a full-blown production-ready application, but it gives a good insight into how one can be built. Here, instead of using the entire text of an e-mail, only one line is used for simplicity. If this is to be extended to process real e-mails, instead of the single strings, read the contents of a full e-mail to one string and proceed as per the logic given in this application.</p><p>Unlike the numeric features that are covered in the earlier use cases of this chapter, the input here is pure text and selecting features is not as easy as those use cases. The lines are split into words to have a bag of words and the words are chosen as features. Since it is easy to process numerical features, these words are transformed to hashed term frequency vectors. In other words, the series of words or terms in the lines are converted to their term frequencies using a hashing method. So even in small-scale text processing use cases, there will be thousands of features. That is why they need to be hashed for easy comparison. </p><p>As discussed earlier, in typical machine learning applications, the input data needs to undergo lots of pre-processing to get it in the right form of features and labels in order to build the model. This typically forms a pipeline of transformations and estimations. In this use case, the incoming lines are split into words, and those words are transformed using HashingTF algorithm and then a LogisticRegression model is trained before doing the prediction. This is done using the Pipeline abstraction in the Spark machine learning library. At the Scala REPL prompt, try the following statements:</p><pre class="programlisting">
<strong>
	  scala&gt; import org.apache.spark.ml.classification.LogisticRegression
	  
      import org.apache.spark.ml.classification.LogisticRegression
    
	scala&gt; import org.apache.spark.ml.param.ParamMap
	
      import org.apache.spark.ml.param.ParamMap
    
	scala&gt; import org.apache.spark.ml.linalg.{Vector, Vectors}
	
      import org.apache.spark.ml.linalg.{Vector, Vectors}
    
	scala&gt; import org.apache.spark.sql.Row
	
      import org.apache.spark.sql.Row
    
	scala&gt; import org.apache.spark.ml.Pipeline
	
      import org.apache.spark.ml.Pipeline
    
	scala&gt; import org.apache.spark.ml.feature.{HashingTF, Tokenizer, RegexTokenizer, Word2Vec, StopWordsRemover}
	
      import org.apache.spark.ml.feature.{HashingTF, Tokenizer, RegexTokenizer, Word2Vec, StopWordsRemover}
    
	scala&gt; // Prepare training documents from a list of messages from emails used to filter them as spam or not spam
	scala&gt; // If the original message is a spam then the label is 1 and if the message is genuine then the label is 0
	scala&gt; val training = spark.createDataFrame(Seq(("you@example.com", "hope you are well", 0.0),("raj@example.com", "nice to hear from you", 0.0),("thomas@example.com", "happy holidays", 0.0),("mark@example.com", "see you tomorrow", 0.0),("xyz@example.com", "save money", 1.0),("top10@example.com", "low interest rate", 1.0),("marketing@example.com", "cheap loan", 1.0))).toDF("email", "message", "label")
	
      training: org.apache.spark.sql.DataFrame = [email: string, message: string ... 1 more field]
    
	scala&gt; training.show()
	
      +--------------------+--------------------+-----+
    
      |               email|             message|label|
    
      +--------------------+--------------------+-----+
    
      |     you@example.com|   hope you are well|  0.0|
    
      |     raj@example.com|nice to hear from...|  0.0|
    
      |  thomas@example.com|      happy holidays|  0.0|
    
      |    mark@example.com|    see you tomorrow|  0.0|
    
      |     xyz@example.com|          save money|  1.0|
    
      |   top10@example.com|   low interest rate|  1.0|
    
      |marketing@example...|          cheap loan|  1.0|
    
      +--------------------+--------------------+-----+
    
	scala&gt;  // Configure an Spark machine learning pipeline, consisting of three stages: tokenizer, hashingTF, and lr.
	scala&gt; val tokenizer = new Tokenizer().setInputCol("message").setOutputCol("words")
	
      tokenizer: org.apache.spark.ml.feature.Tokenizer = tok_166809bf629c
    
	scala&gt; val hashingTF = new HashingTF().setNumFeatures(1000).setInputCol("words").setOutputCol("features")
	
      hashingTF: org.apache.spark.ml.feature.HashingTF = hashingTF_e43616e13d19
    
	scala&gt; // LogisticRegression parameter to make lr.fit() use at most 10 iterations and the regularization parameter.
	scala&gt; // When a higher degree polynomial used by the algorithm to fit a set of points in a linear regression model, to prevent overfitting, regularization is used and this parameter is just for that
	scala&gt; val lr = new LogisticRegression().setMaxIter(10).setRegParam(0.01)
	
      lr: org.apache.spark.ml.classification.LogisticRegression = logreg_ef3042fc75a3
    
	scala&gt; val pipeline = new Pipeline().setStages(Array(tokenizer, hashingTF, lr))
	
      pipeline: org.apache.spark.ml.Pipeline = pipeline_658b5edef0f2
    
	scala&gt; // Fit the pipeline to train the model to study the messages
	scala&gt; val model = pipeline.fit(training)
	
      model: org.apache.spark.ml.PipelineModel = pipeline_658b5edef0f2
    
	scala&gt; // Prepare messages for prediction, which are not categorized and leaving upto the algorithm to predict
	scala&gt; val test = spark.createDataFrame(Seq(("you@example.com", "how are you"),("jain@example.com", "hope doing well"),("caren@example.com", "want some money"),("zhou@example.com", "secure loan"),("ted@example.com","need loan"))).toDF("email", "message")
	
      test: org.apache.spark.sql.DataFrame = [email: string, message: string]
    
	scala&gt; test.show()
	
      +-----------------+---------------+
    
      |            email|        message|
    
      +-----------------+---------------+
    
      |  you@example.com|    how are you|
    
      | jain@example.com|hope doing well|
    
      |caren@example.com|want some money|
    
      | zhou@example.com|    secure loan|
    
      |  ted@example.com|      need loan|
    
      +-----------------+---------------+
    
	scala&gt; // Make predictions on the new messages
	scala&gt; val prediction = model.transform(test).select("email", "message", "prediction")
	
      prediction: org.apache.spark.sql.DataFrame = [email: string, message: string ... 1 more field]
    
	scala&gt; prediction.show()
	
      +-----------------+---------------+----------+
    
      |            email|        message|prediction|
    
      +-----------------+---------------+----------+
    
      |  you@example.com|    how are you|       0.0|
    
      | jain@example.com|hope doing well|       0.0|
    
      |caren@example.com|want some money|       1.0|
    
      | zhou@example.com|    secure loan|       1.0|
    
      |  ted@example.com|      need loan|       1.0|
    
      +-----------------+---------------+----------+    
	  </strong>
</pre><p>The preceding code snippet does the typical chain of activities: preparing training data, creating the model using the Pipeline abstraction, and then predicting using the test data. It doesn't reveal how the features are created and processed. In an application development perspective, Spark machine learning library does the heavy lifting and does everything under the hood using the Pipeline abstraction. If the Pipeline methodology is not used, then tokenisation and then hashing are to be done as a separate DataFrame transformation. The following code snippet executed as the continuation of the preceding commands will give an insight into how that can be done as simple transformations to see the features using the naked eyes:</p><pre class="programlisting">
<strong>
	  scala&gt; val wordsDF = tokenizer.transform(training)
	  
      wordsDF: org.apache.spark.sql.DataFrame = [email: string, message: string ... 2 more fields]
    
	scala&gt; wordsDF.createOrReplaceTempView("word")
	scala&gt; val selectedFieldstDF = spark.sql("SELECT message, words FROM word")
	
      selectedFieldstDF: org.apache.spark.sql.DataFrame = [message: string, words: array&lt;string&gt;]
    
	scala&gt; selectedFieldstDF.show()
	
      +--------------------+--------------------+
    
      |             message|               words|
    
      +--------------------+--------------------+
    
      |   hope you are well|[hope, you, are, ...|
    
      |nice to hear from...|[nice, to, hear, ...|
    
      |      happy holidays|   [happy, holidays]|
    
      |    see you tomorrow|[see, you, tomorrow]|
    
      |          save money|       [save, money]|
    
      |   low interest rate|[low, interest, r...|
    
      |          cheap loan|       [cheap, loan]|
    
      +--------------------+--------------------+
    scala&gt; val featurizedDF = hashingTF.transform(wordsDF)
	
      featurizedDF: org.apache.spark.sql.DataFrame = [email: string, message: string ... 3 more fields]
    
	scala&gt; featurizedDF.createOrReplaceTempView("featurized")
	scala&gt; val selectedFeaturizedFieldstDF = spark.sql("SELECT words, features FROM featurized")
	
      selectedFeaturizedFieldstDF: org.apache.spark.sql.DataFrame = [words: array&lt;string&gt;, features: vector]
    
	scala&gt; selectedFeaturizedFieldstDF.show()
	
      +--------------------+--------------------+
    
      |               words|            features|
    
      +--------------------+--------------------+
    
      |[hope, you, are, ...|(1000,[0,138,157,...|
    
      |[nice, to, hear, ...|(1000,[370,388,42...|
    
      |   [happy, holidays]|(1000,[141,457],[...|
    
      |[see, you, tomorrow]|(1000,[25,425,515...|
    
      |       [save, money]|(1000,[242,520],[...|
    
      |[low, interest, r...|(1000,[70,253,618...|
    
      |       [cheap, loan]|(1000,[410,666],[...|
    </strong>
	<strong>
      +--------------------+--------------------+ </strong>
	  </pre><p>The same use case implemented in Python is as follows. At the Python REPL prompt, try the following statements:</p><pre class="programlisting">
	  <strong>
	  &gt;&gt;&gt; from pyspark.ml import Pipeline
	  &gt;&gt;&gt; from pyspark.ml.classification import LogisticRegression
	  &gt;&gt;&gt; from pyspark.ml.feature import HashingTF, Tokenizer
	  &gt;&gt;&gt; from pyspark.sql import Row
	  &gt;&gt;&gt; # Prepare training documents from a list of messages from emails used to filter them as spam or not spam
	  &gt;&gt;&gt; # If the original message is a spam then the label is 1 and if the message is genuine then the label is 0
	  &gt;&gt;&gt; LabeledDocument = Row("email", "message", "label")
	  &gt;&gt;&gt; training = spark.createDataFrame([("you@example.com", "hope you are well", 0.0),("raj@example.com", "nice to hear from you", 0.0),("thomas@example.com", "happy holidays", 0.0),("mark@example.com", "see you tomorrow", 0.0),("xyz@example.com", "save money", 1.0),("top10@example.com", "low interest rate", 1.0),("marketing@example.com", "cheap loan", 1.0)], ["email", "message", "label"])
	  &gt;&gt;&gt; training.show()
	  
      +--------------------+--------------------+-----+
    
      |               email|             message|label|
    
      +--------------------+--------------------+-----+
    
      |     you@example.com|   hope you are well|  0.0|
    
      |     raj@example.com|nice to hear from...|  0.0|
    
      |  thomas@example.com|      happy holidays|  0.0|
    
      |    mark@example.com|    see you tomorrow|  0.0|
    
      |     xyz@example.com|          save money|  1.0|
    
      |   top10@example.com|   low interest rate|  1.0|
    
      |marketing@example...|          cheap loan|  1.0|
    
      +--------------------+--------------------+-----+
    
	&gt;&gt;&gt; # Configure an Spark machin learning pipeline, consisting of three stages: tokenizer, hashingTF, and lr.
	&gt;&gt;&gt; tokenizer = Tokenizer(inputCol="message", outputCol="words")
	&gt;&gt;&gt; hashingTF = HashingTF(inputCol="words", outputCol="features")
	&gt;&gt;&gt; # LogisticRegression parameter to make lr.fit() use at most 10 iterations and the regularization parameter.
	&gt;&gt;&gt; # When a higher degree polynomial used by the algorithm to fit a set of points in a linear regression model, to prevent overfitting, regularization is used and this parameter is just for that
	&gt;&gt;&gt; lr = LogisticRegression(maxIter=10, regParam=0.01)
	&gt;&gt;&gt; pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])
	&gt;&gt;&gt; # Fit the pipeline to train the model to study the messages
	&gt;&gt;&gt; model = pipeline.fit(training)
	&gt;&gt;&gt; # Prepare messages for prediction, which are not categorized and leaving upto the algorithm to predict
	&gt;&gt;&gt; test = spark.createDataFrame([("you@example.com", "how are you"),("jain@example.com", "hope doing well"),("caren@example.com", "want some money"),("zhou@example.com", "secure loan"),("ted@example.com","need loan")], ["email", "message"])
	&gt;&gt;&gt; test.show()
	
      +-----------------+---------------+
    
      |            email|        message|
    
      +-----------------+---------------+
    
      |  you@example.com|    how are you|
    
      | jain@example.com|hope doing well|
    
      |caren@example.com|want some money|
    
      | zhou@example.com|    secure loan|
    
      |  ted@example.com|      need loan|
    
      +-----------------+---------------+
    
	&gt;&gt;&gt; # Make predictions on the new messages
	&gt;&gt;&gt; prediction = model.transform(test).select("email", "message", "prediction")
	&gt;&gt;&gt; prediction.show()
	
      +-----------------+---------------+----------+
    
      |            email|        message|prediction|
    
      +-----------------+---------------+----------+
    
      |  you@example.com|    how are you|       0.0|
    
      | jain@example.com|hope doing well|       0.0|
    
      |caren@example.com|want some money|       1.0|
    
      | zhou@example.com|    secure loan|       1.0|
    
      |  ted@example.com|      need loan|       1.0|    
	  
      +-----------------+---------------+----------+
	  </strong>
</pre><p>As discussed in earlier, the transformations abstracted by the Pipeline is elucidated as follows using Python explicitly. The following code snippet executed as the continuation of the preceding commands will give an insight into how that can be done as simple transformations to see the features using the naked eyes:</p><pre class="programlisting">
	  <strong>&gt;&gt;&gt; wordsDF = tokenizer.transform(training)
	  &gt;&gt;&gt; wordsDF.createOrReplaceTempView("word")
	  &gt;&gt;&gt; selectedFieldstDF = spark.sql("SELECT message, words FROM word")
	  &gt;&gt;&gt; selectedFieldstDF.show()
	  
      +--------------------+--------------------+
    
	
      |             message|               words|
    
	
      +--------------------+--------------------+
    
      |   hope you are well|[hope, you, are, ...|
    
      |nice to hear from...|[nice, to, hear, ...|
    
      |      happy holidays|   [happy, holidays]|
    
      |    see you tomorrow|[see, you, tomorrow]|
    
      |          save money|       [save, money]|
    
      |   low interest rate|[low, interest, r...|
    
      |          cheap loan|       [cheap, loan]|
    
      +--------------------+--------------------+
    
	&gt;&gt;&gt; featurizedDF = hashingTF.transform(wordsDF)
	&gt;&gt;&gt; featurizedDF.createOrReplaceTempView("featurized")
	&gt;&gt;&gt; selectedFeaturizedFieldstDF = spark.sql("SELECT words, features FROM featurized")
	&gt;&gt;&gt; selectedFeaturizedFieldstDF.show()
	
      +--------------------+--------------------+
    
	
      |               words|            features|
    
	
      +--------------------+--------------------+
    
	
      |[hope, you, are, ...|(262144,[128160,1...|
    
      |[nice, to, hear, ...|(262144,[22346,10...|
    
      |   [happy, holidays]|(262144,[86293,23...|
    
      |[see, you, tomorrow]|(262144,[29129,21...|
    
      |       [save, money]|(262144,[199496,2...|
    
      |[low, interest, r...|(262144,[68685,13...|
    
      |       [cheap, loan]|(262144,[12946,16...|
    
      +--------------------+--------------------+</strong>
</pre><p>Based on the insight provided in the preceding use case, lots of text processing machine learning applications can be developed by abstracting away lots of transformations using the Spark machine learning library Pipelines.</p><div><div><h3 class="title"><a id="tip57"/>Tip</h3><p>Just like the way the machine learning models are persisted to the media, all of the Spark machine learning library Pipelines can also be persisted to the media and reloaded by other programs.</p></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec61"/>Feature algorithms</h1></div></div></div><p>In real-world use cases, it is not very easy to get the raw data in the appropriate form of features and labels in order to train the model. Doing lots of pre-processing is very common. Unlike other data processing paradigms, Spark in conjunction with the Spark machine learning library provides a comprehensive set of tools and algorithms for this purpose. This pre-processing algorithms can be put into three categories:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Feature extraction</li><li class="listitem" style="list-style-type: disc">Feature transformation</li><li class="listitem" style="list-style-type: disc">Feature selection</li></ul></div><p>The process of extracting the features from the raw data is feature extraction. The HashingTF that was used in the preceding use case is a good example of an algorithm that converts terms of text data to feature vectors. The process of transforming features into different formats is feature transformation. The process of selecting a subset of features from a super set is feature selection. Covering all these is beyond the scope of this chapter, but the next section is going to discuss an Estimator, which is an algorithm that is used to extract features, that is used to find synonyms of words in documents,. These are not the word's actual synonyms, but the words that are related to a given word in a context.</p></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec62"/>Finding synonyms</h1></div></div></div><p>A synonym is a word or phrase that has exactly the same meaning or very close meaning to another word. In a purely literature perspective this explanation is correct, but in a much wider perspective, in a given context, some of the words will have a very close relationship, and that is also called synonymous in this context. For example, Roger Federer is <em>synonymous</em> with Tennis. Finding this kind of synonym in context is a very common requirement in entity recognition, machine translation, and so on. The <strong>Word2Vec</strong> algorithm computes a distributed vector representation of words from the words of a given document or collection of words. If this vector space is taken, the words that have similarity or synonymity will be close to each other.</p><p>The University of California Irvine Machine Learning Repository (<a class="ulink" href="http://archive.ics.uci.edu/ml/index.html">http://archive.ics.uci.edu/ml/index.html</a>) provides a lot of datasets as a service to those who are interested to learn machine learning. The Twenty Newsgroups Dataset (<a class="ulink" href="http://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups">http://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups</a>) is being used here to find synonyms of words in context. It contains a dataset consists of 20,000 messages taken from 20 newsgroups.</p><div><div><h3 class="title"><a id="note58"/>Note</h3><p>The Twenty Newsgroups Dataset download link lets you download the dataset discussed here. The file <code class="literal">20_newsgroups.tar.gz</code> is to be downloaded and unzipped. The data directory used in the following code snippets should point to the directory where the data is available in unzipped form. If the Spark Driver is giving out of memory error because of the huge size of the data, remove some of the newsgroups data that is of not interest and experiment with a subset of the data. Here, to train the model, only the following news group data is used: talk.politics.guns, talk.politics.mideast, talk.politics.misc, and talk.religion.misc.</p></div></div><p>At the Scala REPL prompt, try the following statements:</p><pre class="programlisting">
	  <strong>
	  scala&gt; import org.apache.spark.ml.feature.{HashingTF, Tokenizer, RegexTokenizer, Word2Vec, StopWordsRemover}
	  
      import org.apache.spark.ml.feature.{HashingTF, Tokenizer, RegexTokenizer, Word2Vec, StopWordsRemover}
    
	scala&gt; // TODO - Change this directory to the right location where the data is stored
	scala&gt; val dataDir = "/Users/RajT/Downloads/20_newsgroups/*"
	
      dataDir: String = /Users/RajT/Downloads/20_newsgroups/*
    
	scala&gt; //Read the entire text into a DataFrame
	scala&gt; // Only the following directories under the data directory has benn considered for running this program talk.politics.guns, talk.politics.mideast, talk.politics.misc, talk.religion.misc. All other directories have been removed before running this program. There is no harm in retaining all the data. The only difference will be in the output.
	scala&gt;  val textDF = sc.wholeTextFiles(dataDir).map{case(file, text) =&gt; text}.map(Tuple1.apply).toDF("sentence")
	
      textDF: org.apache.spark.sql.DataFrame = [sentence: string]
    
	scala&gt;  // Tokenize the sentences to words
	scala&gt;  val regexTokenizer = new RegexTokenizer().setInputCol("sentence").setOutputCol("words").setPattern("\\w+").setGaps(false)
	
      regexTokenizer: org.apache.spark.ml.feature.RegexTokenizer = regexTok_ba7ce8ec2333
    
	scala&gt; val tokenizedDF = regexTokenizer.transform(textDF)
	
      tokenizedDF: org.apache.spark.sql.DataFrame = [sentence: string, words: array&lt;string&gt;]
    
	scala&gt;  // Remove the stop words such as a, an the, I etc which doesn't have any specific relevance to the synonyms
	scala&gt; val remover = new StopWordsRemover().setInputCol("words").setOutputCol("filtered")
	
      remover: org.apache.spark.ml.feature.StopWordsRemover = stopWords_775db995b8e8
    
	scala&gt; //Remove the stop words from the text
	scala&gt; val filteredDF = remover.transform(tokenizedDF)
	
      filteredDF: org.apache.spark.sql.DataFrame = [sentence: string, words: array&lt;string&gt; ... 1 more field]
    
	scala&gt; //Prepare the Estimator
	scala&gt; //It sets the vector size, and the method setMinCount sets the minimum number of times a token must appear to be included in the word2vec model's vocabulary.
	scala&gt; val word2Vec = new Word2Vec().setInputCol("filtered").setOutputCol("result").setVectorSize(3).setMinCount(0)
	
      word2Vec: org.apache.spark.ml.feature.Word2Vec = w2v_bb03091c4439
    
	scala&gt; //Train the model
	scala&gt; val model = word2Vec.fit(filteredDF)
	
      model: org.apache.spark.ml.feature.Word2VecModel = w2v_bb03091c4439   
    
	scala&gt; //Find 10 synonyms of a given word
	scala&gt; val synonyms1 = model.findSynonyms("gun", 10)
	
      synonyms1: org.apache.spark.sql.DataFrame = [word: string, similarity: double]
    
	scala&gt; synonyms1.show()
	
      +---------+------------------+
    
      |     word|        similarity|
    
      +---------+------------------+
    
      |      twa|0.9999976163843671|
    
      |cigarette|0.9999943935045497|
    
      |    sorts|0.9999885527530025|
    
      |       jj|0.9999827967650881|
    
      |presently|0.9999792188771406|
    
      |    laden|0.9999775888361028|
    
      |   notion|0.9999775296680583|
    
      | settlers|0.9999746245431419|
    
      |motivated|0.9999694932468436|
    
      |qualified|0.9999678135106314|
    
      +---------+------------------+
    
	scala&gt; //Find 10 synonyms of a different word
	scala&gt; val synonyms2 = model.findSynonyms("crime", 10)
	
      synonyms2: org.apache.spark.sql.DataFrame = [word: string, similarity: double]
    
	scala&gt; synonyms2.show()
	
      +-----------+------------------+
    
	
      |       word|        similarity|
    
      +-----------+------------------+
    
      | abominable|0.9999997331058447|
    
      |authorities|0.9999946968941679|
    
      |cooperation|0.9999892536435327|
    
      |  mortazavi| 0.999986396931714|
    
      |herzegovina|0.9999861828226779|
    
      |  important|0.9999853354260315|
    
      |      1950s|0.9999832312575262|
    
      |    analogy|0.9999828272311249|
    
      |       bits|0.9999820987679822|
    
      |technically|0.9999808208936487|
    
      +-----------+------------------+</strong>
</pre><p>The preceding code snippet is loaded with a lot of functionality. The dataset is read from the filesystem into a DataFrame as one sentence of text from a given file. Then tokenisation is done to convert the sentences into words using regular expressions and removing the gaps. Then, from those words, the stop words are removed so that we only have relevant words. Finally, using the <strong>Word2Vec</strong> estimator, a model is trained with the data prepared. From the trained model, synonyms are determined.</p><p>The following code demonstrates the same use case using Python. At the Python REPL prompt, try the following statements:</p><pre class="programlisting">
<strong>
	  &gt;&gt;&gt; from pyspark.ml.feature import Word2Vec
	  &gt;&gt;&gt; from pyspark.ml.feature import RegexTokenizer
	  &gt;&gt;&gt; from pyspark.sql import Row
	  &gt;&gt;&gt; # TODO - Change this directory to the right location where the data is stored
	  &gt;&gt;&gt; dataDir = "/Users/RajT/Downloads/20_newsgroups/*"
	  &gt;&gt;&gt; # Read the entire text into a DataFrame. Only the following directories under the data directory has benn considered for running this program talk.politics.guns, talk.politics.mideast, talk.politics.misc, talk.religion.misc. All other directories have been removed before running this program. There is no harm in retaining all the data. The only difference will be in the output.
	  &gt;&gt;&gt; textRDD = sc.wholeTextFiles(dataDir).map(lambda recs: Row(sentence=recs[1]))
	  &gt;&gt;&gt; textDF = spark.createDataFrame(textRDD)
	  &gt;&gt;&gt; # Tokenize the sentences to words
	  &gt;&gt;&gt; regexTokenizer = RegexTokenizer(inputCol="sentence", outputCol="words", gaps=False, pattern="\\w+")
	  &gt;&gt;&gt; tokenizedDF = regexTokenizer.transform(textDF)
	  &gt;&gt;&gt; # Prepare the Estimator
	  &gt;&gt;&gt; # It sets the vector size, and the parameter minCount sets the minimum number of times a token must appear to be included in the word2vec model's vocabulary.
	  &gt;&gt;&gt; word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol="words", outputCol="result")
	  &gt;&gt;&gt; # Train the model
	  &gt;&gt;&gt; model = word2Vec.fit(tokenizedDF)
	  &gt;&gt;&gt; # Find 10 synonyms of a given word
	  &gt;&gt;&gt; synonyms1 = model.findSynonyms("gun", 10)
	  &gt;&gt;&gt; synonyms1.show()
	  
      +---------+------------------+
    
      |     word|        similarity|
    
      +---------+------------------+
    
      | strapped|0.9999918504219028|
    
      |    bingo|0.9999909957939888|
    
      |collected|0.9999907658056393|
    
      |  kingdom|0.9999896797527402|
    
      | presumed|0.9999806586578037|
    
      | patients|0.9999778970248504|
    
      |    azats|0.9999718388241235|
    
      |  opening| 0.999969723774294|
    
      |  holdout|0.9999685636131942|
    
      | contrast|0.9999677676714386|
    
      +---------+------------------+
    
	&gt;&gt;&gt; # Find 10 synonyms of a different word
	&gt;&gt;&gt; synonyms2 = model.findSynonyms("crime", 10)
	&gt;&gt;&gt; synonyms2.show()
	
      +-----------+------------------+
    
      |       word|        similarity|
    
      +-----------+------------------+
    
      |   peaceful|0.9999983523475047|
    
      |  democracy|0.9999964568156694|
    
      |      areas| 0.999994036518118|
    
      |  miniscule|0.9999920828755365|
    
      |       lame|0.9999877327660102|
    
      |    strikes|0.9999877253180771|
    
      |terminology|0.9999839393584438|
    
      |      wrath|0.9999829348358952|
    
      |    divided| 0.999982619125983|
    
      |    hillary|0.9999795817857984|
    
      +-----------+------------------+
</strong>
</pre><p>The major difference between the Scala implementation and Python implementation is that in the Python implementation, the stop words have not been removed. That is because that functionality is not available in Python API of the Spark Machine Library. Because of this difference, the list of synonyms generated by Scala program and Python program are different.</p></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec63"/>References</h1></div></div></div><p>For more information refer the following links:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><a class="ulink" href="http://archive.ics.uci.edu/ml/index.html">http://archive.ics.uci.edu/ml/index.html</a></li><li class="listitem" style="list-style-type: disc"><a class="ulink" href="http://archive.ics.uci.edu/ml/datasets/Wine+Quality">http://archive.ics.uci.edu/ml/datasets/Wine+Quality</a></li><li class="listitem" style="list-style-type: disc"><a class="ulink" href="http://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups">http://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups</a></li></ul></div></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec64"/>Summary</h1></div></div></div><p>Spark provides a very powerful core data processing framework and the Spark machine learning library makes use of all the core features of Spark and Spark libraries such as Spark SQL, in addition to its rich set of machine learning algorithms. This chapter covered some of the very common prediction use cases and classification use cases with Scala and Python implementations using the Spark machine learning library with a few lines of code. These wine quality prediction, wine classification, spam filter, and synonym finder machine learning use cases have great potential to be developed into full-blown real-world use cases. Spark 2.0 brings flexibility to model creation, pipeline creation, and their usage in different programs written in a different languages by enabling the model and pipeline persistence.</p><p>Pair-wise relationships are very common in real-world use cases. Backed by a strong mathematical theoretical base, computer scientists have developed many data structures and the algorithms that are going with it falling under the subject of Graph Theory. These data structures and algorithms have huge applicability in applications such as social networking websites, scheduling problems, and many other applications. Graph processing is very computationally intensive and distributed data processing paradigms such as Spark are ideal for doing such computations. The Spark GraphX library built on top of Spark is a collection of graph processing APIs. The next chapter is going to take a look at Spark GraphX.</p></div></body></html>