- en: Chapter 2. Getting Data from the Web
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It happens pretty often that we want to use data in a project that is not yet
    available in our databases or on our disks, but can be found on the Internet.
    In such situations, one option might be to get the IT department or a data engineer
    at our company to extend our data warehouse to scrape, process, and load the data
    into our database as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting Data from the Web](img/2028OS_02_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'On the other hand, if we have no *ETL* system (*to Extract, Transform, and
    Load data*) or simply just cannot wait a few weeks for the IT department to implement
    our request, we are on our own. This is pretty standard for the data scientist,
    as most of the time we are developing prototypes that can be later transformed
    into products by software developers. To this end, a variety of skills are required
    in the daily round, including the following topics that we will cover in this
    chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Downloading data programmatically from the Web
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing XML and JSON formats
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scraping and parsing data from raw HTML sources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interacting with APIs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Although being a *data scientist* was referred to as the sexiest job of the
    21st century (Source: [https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century/](https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century/)),
    most data science tasks have nothing to do with data analysis. Worse, sometimes
    the job seems to be boring, or the daily routine requires just basic IT skills
    and no machine learning at all. Hence, I prefer to call this role a *data hacker*
    instead of *data scientist*, which also means that we often have to get our hands
    dirty.'
  prefs: []
  type: TYPE_NORMAL
- en: For instance, scraping and scrubbing data is the least sexy part of the analysis
    process for sure, but it's one of the most important steps; it is also said, that
    around 80 percent of data analysis is spent cleaning data. There is no sense in
    running the most advanced machine learning algorithm on junk data, so be sure
    to take your time to get useful and tidy data from your sources.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This chapter will also depend on extensive usage of Internet browser debugging
    tools with some R packages. These include Chrome `DevTools` or `FireBug` in Firefox.
    Although the steps to use these tools will be straightforward and also shown on
    screenshots, it's definitely worth mastering these tools for future usage; therefore,
    I suggest checking out a few tutorials on these tools if you are into fetching
    data from online sources. Some starting points are listed in the *References*
    section of the *Appendix* at the end of the book.
  prefs: []
  type: TYPE_NORMAL
- en: For a quick overview and a collection of relevant R packages for scraping data
    from the Web and to interact with Web services, see the *Web Technologies and
    Services CRAN Task View* at [http://cran.r-project.org/web/views/WebTechnologies.html](http://cran.r-project.org/web/views/WebTechnologies.html).
  prefs: []
  type: TYPE_NORMAL
- en: Loading datasets from the Internet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The most obvious task is to download datasets from the Web and load those into
    our R session in two manual steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Save the datasets to disk.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Read those with standard functions, such as `read.table` or for example `foreign::read.spss`,
    to import `sav` files.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'But we can often save some time by skipping the first step and loading the
    flat text data files directly from the URL. The following example fetches a comma-separated
    file from the **Americas Open Geocode** (**AOG**) database at [http://opengeocode.org](http://opengeocode.org),
    which contains the government, national statistics, geological information, and
    post office websites for the countries of the world:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we passed a hyperlink to the `file` argument of `read.table`,
    which actually downloaded the text file before processing. The `url` function,
    used by `read.table` in the background, supports HTTP and FTP protocols, and can
    also handle proxies, but it has its own limitations. For example `url` does not
    support **Hypertext Transfer Protocol Secure** (**HTTPS**) except for a few exceptions
    on Windows, which is often a must to access Web services that handle sensitive
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: HTTPS is not a separate protocol alongside HTTP, but instead HTTP over an encrypted
    SSL/TLS connection. While HTTP is considered to be insecure due to the unencrypted
    packets travelling between the client and server, HTTPS does not let third-parties
    discover sensitive information with the help of signed and trusted certificates.
  prefs: []
  type: TYPE_NORMAL
- en: 'In such situations, it''s wise, and used to be the only reasonable option,
    to install and use the `RCurl` package, which is an R client interface to curl:
    [http://curl.haxx.se](http://curl.haxx.se). Curl supports a wide variety of protocols
    and URI schemes and handles cookies, authentication, redirects, timeouts, and
    even more.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, let's check the U.S. Government's open data catalog at [http://catalog.data.gov/dataset](http://catalog.data.gov/dataset).
    Although the general site can be accessed without SSL, most of the generated download
    URLs follow the HTTPS URI scheme. In the following example, we will fetch the
    **Comma Separated Values** (**CSV**) file of the Consumer Complaint Database from
    the Consumer Financial Protection Bureau, which can be accessed at [http://catalog.data.gov/dataset/consumer-complaint-database](http://catalog.data.gov/dataset/consumer-complaint-database).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This CSV file contains metadata on around a quarter of a million of complaints
    about financial products and services since 2011\. Please note that the file is
    around 35-40 megabytes, so downloading it might take some time, and you would
    probably not want to reproduce the following example on mobile or limited Internet.
    If the `getURL` function fails with a certificate error (this might happen on
    Windows), please provide the path of the certificate manually by `options(RCurlOptions
    = list(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl")))` or
    try the more recently published `curl` package by Jeroen Ooms or `httr` (`RCurl`
    front-end) by Hadley Wickham—see later.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see the distribution of these complaints by product type after fetching
    and loading the CSV file directly from R:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Although it's nice to know that most complaints were received about mortgages,
    the point here was to use curl to download the CSV file with a HTTPS URI and then
    pass the content to the `read.csv` function (or any other parser we discussed
    in the previous chapter) as text.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Besides `GET` requests, you can easily interact with RESTful API endpoints via
    `POST`, `DELETE`, or `PUT` requests as well by using the `postForm` function from
    the `RCurl` package or the `httpDELETE`, `httpPUT`, or `httpHEAD` functions— see
    details about the `httr` package later.
  prefs: []
  type: TYPE_NORMAL
- en: Curl can also help to download data from a secured site that requires authorization.
    The easiest way to do so is to login to the homepage in a browser, save the cookie
    to a text file, and then pass the path of that to `cookiefile` in `getCurlHandle`.
    You can also specify `useragent` among other options. Please see [http://www.omegahat.org/RCurl/RCurlJSS.pdf](http://www.omegahat.org/RCurl/RCurlJSS.pdf)
    for more details and an overall (and very useful) overview on the most important
    RCurl features.
  prefs: []
  type: TYPE_NORMAL
- en: Although curl is extremely powerful, the syntax and the numerous options with
    the technical details might be way too complex for those without a decent IT background.
    The `httr` package is a simplified wrapper around `RCurl` with some sane defaults
    and much simpler configuration options for common operations and everyday actions.
  prefs: []
  type: TYPE_NORMAL
- en: For example, cookies are handled automatically by sharing the same connection
    across all requests to the same website; error handling is much improved, which
    means easier debugging if something goes wrong; the package comes with various
    helper functions to, for instance, set headers, use proxies, and easily issue
    `GET`, `POST`, `PUT`, `DELETE`, and other methods. Even more, it also handles
    authentication in a much more user-friendly way—along with OAuth support.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: OAuth is the open standard for authorization with the help of intermediary service
    providers. This simply means that the user does not have to share actual credentials,
    but can rather delegate rights to access some of the stored information at the
    service providers. For example, one can authorize Google to share the real name,
    e-mail address, and so on with a third-party without disclosing any other sensitive
    information or any need for passwords. Most generally, OAuth is used for password-less
    login to various Web services and APIs. For more information, please see the [Chapter
    14](ch14.html "Chapter 14. Analyzing the R Community"), *Analyzing the R Community*,
    where we will use OAuth with Twitter to authorize the R session for fetching data.
  prefs: []
  type: TYPE_NORMAL
- en: But what if the data is not available to be downloaded as CSV files?
  prefs: []
  type: TYPE_NORMAL
- en: Other popular online data formats
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Structured data is often available in XML or JSON formats on the Web. The high
    popularity of these two formats is due to the fact that both are human-readable,
    easy to handle from a programmatic point of view, and can manage any type of hierarchical
    data structure, not just a simple tabular design, as CSV files are.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: JSON is originally derived from *JavaScript Object Notation*, which recently
    became one of the top, most-used standards for human-readable data exchange format.
    JSON is considered to be a low-overhead alternative to XML with attribute-value
    pairs, although it also supports a wide variety of object types such as number,
    string, boolean, ordered lists, and associative arrays. JSON is highly used in
    Web applications, services, and APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, R also supports loading (and saving) data in JSON. Let's demonstrate
    that by fetching some data from the previous example via the Socrata API (more
    on that later in the *R packages to interact with data source APIs* section of
    this chapter), provided by the Consumer Financial Protection Bureau. The full
    documentation of the API is available at [http://www.consumerfinance.gov/complaintdatabase/technical-documentation](http://www.consumerfinance.gov/complaintdatabase/technical-documentation).
  prefs: []
  type: TYPE_NORMAL
- en: 'The endpoint of the API is a URL where we can query the background database
    without authentication is [http://data.consumerfinance.gov/api/views](http://data.consumerfinance.gov/api/views).
    To get an overall picture on the structure of the data, the following is the returned
    JSON list opened in a browser:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Other popular online data formats](img/2028OS_02_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As JSON is extremely easy to read, it''s often very helpful to skim through
    the structure manually before parsing. Now let''s load that tree list into R with
    the `rjson` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Well, it does not seem to be the same data we have seen before in the comma-separated
    values file! After a closer look at the documentation, it''s clear that the endpoint
    of the API returns metadata on the available views instead of the raw tabular
    data that we saw in the CSV file. So let''s see the view with the ID of `25ei-6bcr`
    now for the first five rows by opening the related URL in a browser:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Other popular online data formats](img/2028OS_02_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The structure of the resulting JSON list has changed for sure. Now let''s read
    that hierarchical list into R:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We managed to fetch the data along with some further meta-information on the
    view, columns, and so on, which is not something that we are interested in at
    the moment. As `fromJSON` returned a `list` object, we can simply drop the metadata
    and work with the `data` rows from now on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This is still a `list`, which we usually want to transform into a `data.frame`
    instead. So we have `list` with five elements, each holding 19 nested children.
    Please note that one of those, the 13th sub element, is list again with 5-5 vectors.
    This means that transforming the tree list into tabular format is not straightforward,
    even less so when we realize that one of those vectors holds multiple values in
    an unprocessed JSON format. So, for the sake of simplicity and proof of a concept
    demo, let''s simply ditch the location-related values now and transform all other
    values to `data.frame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: So we applied a simple function that drops location information from each element
    of the list (by removing the 13th element of each *x*), automatically simplified
    to `matrix` (by using `sapply` instead of `lapply` to iterate though each element
    of the list), transposed it (via `t`), and then coerced the resulting object to
    `data.frame`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, we can also use some helper functions instead of manually tweaking all
    the list elements, as earlier. The `plyr` package (please find more details in
    [Chapter 3](ch03.html "Chapter 3. Filtering and Summarizing Data"), *Filtering
    and Summarizing Data* and [Chapter 4](ch04.html "Chapter 4. Restructuring Data"),
    *Restructuring Data*) includes some extremely useful functions to split and combine
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'It looks a lot more familiar now, although we miss the variable names, and
    all values were converted to character vectors or factors—even the dates that
    were stored as UNIX timestamps. We can easily fix these problems with the help
    of the provided metadata (`res$meta`): for example, let''s set the variable names
    by extracting (via the `[` operator) the name field of all columns except for
    the dropped (13th) location data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: One might also identify the object classes with the help of the provided metadata.
    For example, the `renderTypeName` field would be a good start to check, and using
    `as.numeric` for number and `as.POSIXct` for all `calendar_date` fields would
    resolve most of the preceding issues.
  prefs: []
  type: TYPE_NORMAL
- en: Well, did you ever hear that around 80 percent of data analysis is spent on
    data preparation?
  prefs: []
  type: TYPE_NORMAL
- en: Parsing and restructuring JSON and XML to `data.frame` can take a long time,
    especially when you are dealing with hierarchical lists primarily. The `jsonlite`
    package tries to overcome this issue by transforming R objects into a conventional
    JSON data structure and vice-versa instead of raw conversion. This means from
    a practical point of view that `jsonlite::fromJSON` will result in `data.frame`
    instead of raw list if possible, and it makes the interchange data format even
    more seamless. Unfortunately, we cannot always transform lists to a tabular format;
    in such cases, the list transformations can be speeded up by for example the `rlist`
    package. Please find more details on list manipulations in [Chapter 14](ch14.html
    "Chapter 14. Analyzing the R Community"), *Analyzing the R Community*.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Extensible Markup Language** (**XML**) was originally developed by the World
    Wide Web Consortium in 1996 to store documents in a both human-readable and machine-readable
    format. This popular syntax is used in for example the Microsoft Office Open XML
    and Open/LibreOffice OpenDocument file formats, in RSS feeds, and in various configuration
    files. As the format is also highly used for the interchange of data over the
    Internet, data is often available in XML as the only option—especially with some
    older APIs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us also see how we can handle another popular online data interchange format
    besides JSON. The XML API can be used in a similar way, but we must define the
    desired output format in the endpoint URL: [http://data.consumerfinance.gov/api/views.xml](http://data.consumerfinance.gov/api/views.xml),
    as you should be able to see in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Other popular online data formats](img/2028OS_02_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'It seems that the XML output of the API differs from what we have seen in the
    JSON format, and it simply includes the rows that we are interested in. This way,
    we can simply parse the XML document and extract the rows from the response then
    transform them to `data.frame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Although we could manually set the desired classes of the variables in the
    `colClasses` argument passed to `xmlToDataFrame`, just like in `read.tables` we
    can also fix this issue afterwards with a quick `helper` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: So we tried to guess if a column includes only numbers, and convert those to
    `numeric` if our helper function returns `TRUE`. Please note that we first convert
    the `factor` to `character` before transforming to number, as a direct conversion
    from `factor` to `numeric` would return the `factor` order instead of the real
    value. One might also try to resolve this issue with the `type.convert` function,
    which is used by default in `read.table`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To test similar APIs and JSON or XML resources, you may find it interesting
    to check out the API of Twitter, GitHub, or probably any other online service
    provider. On the other hand, there is also another open-source service based on
    R that can return XML, JSON, or CSV files from any R code. Please find more details
    at [http://www.opencpu.org](http://www.opencpu.org).
  prefs: []
  type: TYPE_NORMAL
- en: So now we can process structured data from various kinds of downloadable data
    formats but, as there are still some other data source options to master, I promise
    you it's worth it to keep reading.
  prefs: []
  type: TYPE_NORMAL
- en: Reading data from HTML tables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: According to the traditional document formats on the World Wide Web, most texts
    and data are served in HTML pages. We can often find interesting pieces of information
    in for example HTML tables, from which it's pretty easy to copy and paste data
    into an Excel spreadsheet, save that to disk, and load it to R afterwards. But
    it takes time, it's boring, and can be automated anyway.
  prefs: []
  type: TYPE_NORMAL
- en: 'Such HTML tables can be easily generated with the help of the aforementioned
    API of the Customer Compliant Database. If we do not set the required output format
    for which we used XML or JSON earlier, then the browser returns a HTML table instead,
    as you should be able to see in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Reading data from HTML tables](img/2028OS_02_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Well, in the R console it''s a bit more complicated as the browser sends some
    non-default HTTP headers while using curl, so the preceding URL would simply return
    a JSON list. To get HTML, let the server know that we expect HTML output. To do
    so, simply set the appropriate HTTP header of the query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The `XML` package provides an extremely easy way to parse all the HTML tables
    from a document or specific nodes with the help of the `readHTMLTable` function,
    which returns a `list` of `data.frames` by default:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'To get only the first table on the page, we can filter `res` afterwards or
    pass the `which` argument to `readHTMLTable`. The following two R expressions
    have the very same results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Reading tabular data from static Web pages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Okay, so far we have seen a bunch of variations on the same theme, but what
    if we do not find a downloadable dataset in any popular data format? For example,
    one might be interested in the available R packages hosted at CRAN, whose list
    is available at [http://cran.r-project.org/web/packages/available_packages_by_name.html](http://cran.r-project.org/web/packages/available_packages_by_name.html).
    How do we scrape that? No need to call `RCurl` or to specify custom headers, still
    less do we have to download the file first; it''s enough to pass the URL to `readHTMLTable`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: So `readHTMLTable` can directly fetch HTML pages, then it extracts all the HTML
    tables to `data.frame` R objects, and returns a `list` of those. In the preceding
    example, we got a `list` of only one `data.frame` with all the package names and
    descriptions as columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, this amount of textual information is not really informative with the
    `str` function. For a quick example of processing and visualizing this type of
    raw data, and to present the plethora of available features by means of R packages
    at CRAN, now we can create a word cloud of the package descriptions with some
    nifty functions from the `wordcloud` and the `tm` packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This short command results in the following screenshot, which shows the most
    frequent words found in the R package descriptions. The position of the words
    has no special meaning, but the larger the font size, the higher the frequency.
    Please see the technical description of the plot following the screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Reading tabular data from static Web pages](img/2028OS_02_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: So we simply passed all the strings from the second column of the first `list`
    element to the `wordcloud` function, which automatically runs a few text-mining
    scripts from the `tm` package on the text. You can find more details on this topic
    in [Chapter 7](ch07.html "Chapter 7. Unstructured Data"), *Unstructured Data*.
    Then, it renders the words with a relative size weighted by the number of occurrences
    in the package descriptions. It seems that R packages are indeed primarily targeted
    at building models and applying multivariate tests on data.
  prefs: []
  type: TYPE_NORMAL
- en: Scraping data from other online sources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Although the `readHTMLTable` function is very useful, sometimes the data is
    not structured in tables, but rather it''s available only as HTML lists. Let''s
    demonstrate such a data format by checking all the R packages listed in the relevant
    CRAN Task View at [http://cran.r-project.org/web/views/WebTechnologies.html](http://cran.r-project.org/web/views/WebTechnologies.html),
    as you can see in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Scraping data from other online sources](img/2028OS_02_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'So we see a HTML list of the package names along with a URL pointing to the
    CRAN, or in some cases to the GitHub repositories. To proceed, first we have to
    get acquainted a bit with the HTML sources to see how we can parse them. You can
    do that easily either in Chrome or Firefox: just right-click on the **CRAN** packages
    heading at the top of the list, and choose **Inspect Element**, as you can see
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Scraping data from other online sources](img/2028OS_02_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: So we have the list of related R packages in an `ul` (unordered list) HTML tag,
    just after the `h3` (level 3 heading) tag holding the `CRAN packages` string.
  prefs: []
  type: TYPE_NORMAL
- en: 'In short:'
  prefs: []
  type: TYPE_NORMAL
- en: We have to parse this HTML file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Look for the third-level heading holding the search term
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get all the list elements from the subsequent unordered HTML list
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This can be done by, for example, the XML Path Language, which has a special
    syntax to select nodes in XML/HTML documents via queries.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For more details and R-driven examples, see *Chapter 4*, *XPath, XPointer, and
    XInclude* of the book *XML and Web Technologies for Data Sciences with R* written
    by Deborah Nolan and Duncan Temple Lang in the Use R! series from Springer. Please
    see more references in the *Appendix* at the end of the book.
  prefs: []
  type: TYPE_NORMAL
- en: 'XPath can be rather ugly and complex at first glance. For example, the preceding
    list can be described with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Let me elaborate a bit on this:'
  prefs: []
  type: TYPE_NORMAL
- en: We are looking for a `h3` tag which has `CRAN packages` as its text, so we are
    searching for a specific node in the whole document with these attributes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then the `following-siblings` expression stands for all the subsequent nodes
    at the same hierarchy level as the chosen `h3` tag.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Filter to find only `ul` HTML tags.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As we have several of those, we select only the first of the further siblings
    with the index `(1)` between the brackets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then we simply select all `li` tags (the list elements) inside that.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s try it in R:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'And we have the character vector of the related 118 R packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'XPath is really powerful for selecting and searching for nodes in HTML documents,
    so is `xpathApply`. The latter is the R wrapper around most of the XPath functionality
    in `libxml`, which makes the process rather quick and efficient. One might rather
    use the `xpathSApply` instead, which tries to simplify the returned list of elements,
    just like `sapply` does compared to the `lapply` function. So we can also update
    our previous code to save the `unlist` call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The attentive reader must have noticed that the returned list was a simple character
    vector, while the original HTML list also included the URLs of the aforementioned
    packages. Where and why did those vanish?
  prefs: []
  type: TYPE_NORMAL
- en: We can blame `xmlValue` for this result, which we called instead of the default
    `NULL` as the evaluating function to extract the nodes from the original document
    at the `xpathSApply` call. This function simply extracts the raw text content
    of each leaf node without any children, which explains this behavior. What if
    we are rather interested in the package URLs?
  prefs: []
  type: TYPE_NORMAL
- en: 'Calling `xpathSapply` without a specified fun returns all the raw child nodes,
    which is of no direct help, and we shouldn''t try to apply some regular expressions
    on those. The help page of `xmlValue` can point us to some similar functions that
    can be very handy with such tasks. Here we definitely want to use `xmlAttrs`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Please note that an updated path was used here, where now we selected all the
    `a` tags instead of the `li` parents. And, instead of the previously introduced
    `xmlValue`, now we called `xmlAttrs` with the `'href'` extra argument. This simply
    extracts all the `href` arguments of all the related `a` nodes.
  prefs: []
  type: TYPE_NORMAL
- en: With these primitives, you will be able to fetch any publicly available data
    from online sources, although sometimes the implementation can end up being rather
    complex.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: On the other hand, please be sure to always consult the terms and conditions
    and other legal documents of all potential data sources, as fetching data is often
    prohibited by the copyright owner.
  prefs: []
  type: TYPE_NORMAL
- en: Beside the legal issues, it's also wise to think of fetching and crawling data
    from the technical point of view of the service provider. If you start to send
    a plethora of queries to a server without consulting with their administrators
    beforehand, this action might be construed as a network attack and/or might result
    in an unwanted load on the servers. To keep it simple, always use a sane delay
    between your queries. This should be for example, a 2-second pause between queries
    at a minimum, but it's better to check the *Crawl-delay* directive set in the
    site's *robot.txt*, which can be found in the root path if available. This file
    also contains other directives if crawling is allowed or limited. Most of the
    data provider sites also have some technical documentation on data crawling; please
    be sure to search for Rate limits and throttling.
  prefs: []
  type: TYPE_NORMAL
- en: And sometimes we are just simply lucky in that someone else has already written
    the tricky XPath selectors or other interfaces, so we can load data from Web services
    and homepages with the help of native R packages.
  prefs: []
  type: TYPE_NORMAL
- en: R packages to interact with data source APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although it's great that we can read HTML tables, CSV files and JSON and XML
    data, and even parse raw HTML documents to store some parts of those in a dataset,
    there is no sense in spending too much time developing custom tools until we have
    no other option. First, always start with a quick look on the Web Technologies
    and Services CRAN Task View; also search R-bloggers, StackOverflow, and GitHub
    for any possible solution before getting your hands dirty with custom XPath selectors
    and JSON list magic.
  prefs: []
  type: TYPE_NORMAL
- en: Socrata Open Data API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s do this for our previous examples by searching for Socrata, the Open
    Data Application Program Interface of the Consumer Financial Protection Bureau.
    Yes, there is a package for that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: As a matter of fact, the `RSocrata` package uses the same JSON sources (or CSV
    files), as we did before. Please note the warning message, which says that `RSocrata`
    depends on another JSON parser R package rather than the one we used, so some
    function names are conflicting. It's probably wise to `detach('package:rjson')`
    before automatically loading the `RJSONIO` package.
  prefs: []
  type: TYPE_NORMAL
- en: 'Loading the Customer Complaint Database by the given URL is pretty easy with
    `RSocrata`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We got `numeric` values for numbers, and the dates are also automatically processed
    to `POSIXlt`!
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, the Web Technologies and Services CRAN Task View contains more than
    a hundred R packages to interact with data sources on the Web in natural sciences
    such as ecology, genetics, chemistry, weather, finance, economics, and marketing,
    but we can also find R packages to fetch texts, bibliography resources, Web analytics,
    news, and map and social media data besides some other topics. Due to page limitations,
    here we will only focus on the most-used packages.
  prefs: []
  type: TYPE_NORMAL
- en: Finance APIs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Yahoo! and Google Finance are pretty standard free data sources for all those
    working in the industry. Fetching for example stock, metal, or foreign exchange
    prices is extremely easy with the `quantmod` package and the aforementioned service
    providers. For example, let us see the most recent stock prices for Agilent Technologies
    with the `A` ticker symbol:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: By default, `getSymbols` assigns the fetched results to the `parent.frame` (usually
    the global) environment with the name of the symbols, while specifying `NULL`
    as the desired environment simply returns the fetched results as an `xts` time-series
    object, as seen earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Foreign exchange rates can be fetched just as easily:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The returned string of `getSymbols` refers to the R variable in which the data
    was saved inside `.GlobalEnv`. To see all the available data sources, let''s query
    the related S3 methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'So besides some offline data sources, we can query Google, Yahoo!, and OANDA
    for recent financial information. To see the full list of available symbols, the
    already loaded `TTR` package might help:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Find more information on how to handle and analyze similar datasets in [Chapter
    12](ch12.html "Chapter 12. Analyzing Time-series"), *Analyzing Time-series*.
  prefs: []
  type: TYPE_NORMAL
- en: Fetching time series with Quandl
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Quandl provides access to millions of similar time-series data in a standard
    format, via a custom API, from around 500 data sources. In R, the `Quandl` package
    provides easy access to all these open data in various industries all around the
    world. Let us see for example the dividends paid by Agilent Technologies published
    by the U.S. Securities and Exchange Commission. To do so, simply search for "Agilent
    Technologies" at the [http://www.quandl.com](http://www.quandl.com) homepage,
    and provide the code of the desired data from the search results to the `Quandl`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the API is rather limited without a valid authentication token,
    which can be redeemed at the `Quandl` homepage for free. To set your token, simply
    pass that to the `Quandl.auth` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'This package also lets you:'
  prefs: []
  type: TYPE_NORMAL
- en: Fetch filtered data by time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform some transformations of the data on the server side—such as cumulative
    sums and the first differential
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sort the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define the desired class of the returning object—such as `ts`, `zoo`, and `xts`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Download some meta-information on the data source
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The latter is saved as `attributes` of the returning R object. So, for example,
    to see the frequency of the queried dataset, call:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Google documents and analytics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You might however be more interested in loading your own or custom data from
    Google Docs, to which end the `RGoogleDocs` package is a great help and is available
    for download at the [http://www.omegahat.org/](http://www.omegahat.org/) homepage.
    It provides authenticated access to Google spreadsheets with both read and write
    access.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, this package is rather outdated and uses some deprecated API
    functions, so you might be better trying some newer alternatives, such as the
    recently released `googlesheets` package, which can manage Google Spreadsheets
    (but not other documents) from R.
  prefs: []
  type: TYPE_NORMAL
- en: Similar packages are also available to interact with Google Analytics or Google
    Adwords for all those, who would like to analyze page visits or ad performance
    in R.
  prefs: []
  type: TYPE_NORMAL
- en: Online search trends
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: On the other hand, we interact with APIs to download public data. Google also
    provides access to some public data of the World Bank, IMF, US Census Bureau,
    and so on at [http://www.google.com/publicdata/directory](http://www.google.com/publicdata/directory)
    and also some of their own internal data in the form of search trends at [http://google.com/trends](http://google.com/trends).
  prefs: []
  type: TYPE_NORMAL
- en: 'The latter can be queried extremely easily with the `GTrendsR` package, which
    is not yet available on CRAN, but we can at least practice how to install R packages
    from other sources. The `GTrendR` code repository can be found on `BitBucket`,
    from where it''s really convenient to install it with the `devtools` package:'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To make sure you install the same version of `GTrensR` as used in the following,
    you can specify the `branch`, `commit`, or other reference in the `ref` argument
    of the `install_bitbucket` (or `install_github`) function. Please see the *References*
    section in the *Appendix* at the end of the book for the commit hash.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'So installing R packages from BitBucket or GitHub is as easy as providing the
    name of the code repository and author''s username and allowing `devtools` to
    do the rest: downloading the sources and compiling them.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Windows users should install `Rtools` prior to compiling packages from the
    source: [http://cran.r-project.org/bin/windows/Rtools/](http://cran.r-project.org/bin/windows/Rtools/).
    We also enabled the quiet mode, to suppress compilation logs and the boring details.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After the package has been installed, we can load it in the traditional way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we have to authenticate with a valid Google username and password before
    being able to query the Google Trends database. Our search term will be "how to
    install R":'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Please make sure you provide a valid username and password; otherwise the following
    query will fail.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The returned dataset includes weekly metrics on the relative amount of search
    queries on R installation. The data shows that the highest activity was recorded
    in the middle of July, while only around 75 percent of those search queries were
    triggered at the beginning of the next month. So Google do not publish raw search
    query statistics, but rather comparative studies can be done with different search
    terms and time periods.
  prefs: []
  type: TYPE_NORMAL
- en: Historical weather data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are also various packages providing access to data sources for all R
    users in Earth Science. For example, the `RNCEP` package can download historical
    weather data from the National Centers for Environmental Prediction for more than
    one hundred years in six hourly resolutions. The `weatherData` package provides
    direct access to [http://wunderground.com](http://wunderground.com). For a quick
    example, let us download the daily temperature averages for the last seven days
    in London:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Please note that an unimportant part of the preceding output was suppressed,
    but what happened here is quite straightforward: the package fetched the specified
    URL, which is a CSV file by the way, then parsed that with some additional information.
    Setting `opt_detailed` to `TRUE` would also return intraday data with a 30-minute
    resolution.'
  prefs: []
  type: TYPE_NORMAL
- en: Other online data sources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Of course, this short chapter cannot provide an overview of querying all the
    available online data sources and R implementations, but please consult the Web
    Technologies and Services CRAN Task View, R-bloggers, StackOverflow, and the resources
    in the *References* chapter at the end of the book to look for any already existing
    R packages or helper functions before creating your own crawler R scripts.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter focused on how to fetch and process data directly from the Web,
    including some problems with downloading files, processing XML and JSON formats,
    parsing HTML tables, applying XPath selectors to extract data from HTML pages,
    and interacting with RESTful APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Although some examples in this chapter might appear to have been an idle struggle
    with the Socrata API, it turned out that the `RSocrata` package provides production-ready
    access to all those data. However, please bear in mind that you will face some
    situations without ready-made R packages; thus, as a data hacker, you will have
    to get your hands dirty with all the JSON, HTML and XML sources.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discover how to filter and aggregate the already
    acquired and loaded data with the top, most-used methods for reshaping and restructuring
    data.
  prefs: []
  type: TYPE_NORMAL
