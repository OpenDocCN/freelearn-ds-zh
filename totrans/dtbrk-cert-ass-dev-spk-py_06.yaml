- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SQL Queries in Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will explore the vast capabilities of Spark SQL for structured
    data processing. We will dive into loading and manipulating data, executing SQL
    queries, performing advanced analytics, and integrating Spark SQL with external
    systems. By the end of this chapter, you will have a solid understanding of Spark
    SQL’s features and be equipped with the knowledge to leverage its power in your
    data processing tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What is Spark SQL?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting Started with Spark SQL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advanced Spark SQL operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is Spark SQL?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark SQL is a powerful module within the Apache Spark ecosystem that allows
    for the efficient processing and analysis of structured data. It provides a higher-level
    interface for working with structured data compared to the traditional RDD-based
    API of Apache Spark. Spark SQL combines the benefits of both relational and procedural
    processing, enabling users to seamlessly integrate SQL queries with complex analytics.
    By leveraging Spark’s distributed computing capabilities, Spark SQL enables scalable
    and high-performance data processing.
  prefs: []
  type: TYPE_NORMAL
- en: It provides a programming interface to work with structured data using SQL queries,
    DataFrame API, and Datasets API.
  prefs: []
  type: TYPE_NORMAL
- en: It allows users to query data using SQL-like syntax and provides a powerful
    engine for executing SQL queries on large datasets. Spark SQL also supports reading
    and writing data from various structured sources such as Hive tables, Parquet
    files, and JDBC databases.
  prefs: []
  type: TYPE_NORMAL
- en: Advantages of Spark SQL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Spark SQL offers several key advantages that make it a popular choice for structured
    data processing:'
  prefs: []
  type: TYPE_NORMAL
- en: Unified data processing with Spark SQL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With Spark SQL, users can process both structured and unstructured data using
    a single engine. This means that users can use the same programming interface
    to query data stored in different formats such as JSON, CSV, and Parquet.
  prefs: []
  type: TYPE_NORMAL
- en: Users can seamlessly switch between SQL queries, DataFrame transformations,
    and Spark’s machine learning APIs. This unified data processing approach allows
    for the easier integration of different data processing tasks within a single
    application, reducing development complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Performance and scalability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Spark SQL leverages the distributed computing capabilities of Apache Spark,
    enabling the processing of large-scale datasets across a cluster of machines.
    It utilizes advanced query optimization techniques, such a the Catalyst optimizer
    (discussed in detail in [*Chapter 5*](B19176_05.xhtml#_idTextAnchor115)), to optimize
    and accelerate query execution. Additionally, Spark SQL supports data partitioning
    and caching mechanisms, further enhancing performance and scalability.
  prefs: []
  type: TYPE_NORMAL
- en: Spark SQL uses an optimized execution engine that can process queries much faster
    than traditional SQL engines. It achieves this by using in-memory caching and
    optimized query execution plans.
  prefs: []
  type: TYPE_NORMAL
- en: Spark SQL is designed to scale horizontally across a cluster of machines. It
    can handle large datasets by partitioning them across multiple machines and processing
    them in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: Seamless integration with existing infrastructure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Spark SQL integrates seamlessly with existing Apache Spark infrastructure and
    tools. It provides interoperability with other Spark components, such as Spark
    Streaming for real-time data processing and Spark MLlib for machine learning tasks.
    Furthermore, Spark SQL integrates with popular storage systems and data formats,
    including Parquet, Avro, ORC, and Hive, making it compatible with a wide range
    of data sources.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced analytics capabilities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Spark SQL extends traditional SQL capabilities by using advanced analytics features.
    It supports window functions, which enable users to perform complex analytical
    operations, such as ranking, aggregation over sliding windows, and cumulative
    aggregations. The integration with machine learning libraries in Spark allows
    for the seamless integration of predictive analytics and data science workflows.
  prefs: []
  type: TYPE_NORMAL
- en: Ease of use
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Spark SQL provides a simple programming interface that allows users to query
    data using SQL-like syntax. This makes it easy for users who are familiar with
    SQL to get started with Spark SQL.
  prefs: []
  type: TYPE_NORMAL
- en: Integration with Apache Spark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Spark SQL is an integral part of the Apache Spark framework and works seamlessly
    with other Spark components. It leverages Spark’s core functionalities, such as
    fault tolerance, data parallelism, and distributed computing, to provide scalable
    and efficient data processing. Spark SQL can read data from a variety of sources,
    including distributed file systems (such as HDFS), object stores (like Amazon
    S3), and relational databases (via JDBC). It also integrates with external systems
    such as Hive, allowing users to leverage existing Hive metadata and queries.
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s take a look at some basic constructs of Spark SQL.
  prefs: []
  type: TYPE_NORMAL
- en: Key concepts – DataFrames and datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Spark SQL introduces two fundamental abstractions for working with structured
    data: DataFrames and Datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: DataFrames
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: DataFrames represent distributed collections of data organized into named columns.
    They provide a higher-level interface for working with structured data and offer
    rich APIs for data manipulation, filtering, aggregation, and querying. DataFrames
    are immutable and lazily evaluated, enabling optimized execution plans through
    Spark’s Catalyst optimizer. They can be created from various data sources, including
    structured files (CSV, JSON, and Parquet), Hive tables, and existing RDDs.
  prefs: []
  type: TYPE_NORMAL
- en: Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Datasets are an extension of DataFrames and provide a type-safe, object-oriented
    programming interface. Datasets combine the benefits of Spark’s RDDs (strong typing
    and user-defined functions) with the performance optimizations of DataFrames.
    Datasets enable compile-time type checking and can be seamlessly converted to
    DataFrames, allowing for flexible and efficient data processing.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know what DataFrames and Datasets are, we’ll see how to apply different
    Spark SQL operations to these constructs in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with Spark SQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To get started with Spark SQL operations, we would first need to load data into
    a DataFrame. We’ll see how to do that next. Then, we will see how we can switch
    between PySpark and Spark SQL data and apply different transformations to it.
  prefs: []
  type: TYPE_NORMAL
- en: Loading and saving data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will explore various techniques for loading data into Spark
    SQL from different sources and saving this as a table. We will delve into Python
    code examples that demonstrate how to effectively load data into Spark SQL, perform
    the necessary transformations, and save the processed data as a table for further
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Executing SQL queries in Spark SQL allows us to leverage the familiar SQL syntax
    and take advantage of its expressive power. Let’s take a look at the syntax and
    an example of executing an SQL query using Spark SQL:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To execute an SQL query in Spark SQL, we use the `spark.sql()` method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The `spark.sql()` method is used to execute SQL queries in Spark SQL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inside the method, we provide the SQL query as a string argument. In this example,
    we select all columns from the `tableName` table
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The results of the query are stored in the `results` variable, which can be
    further processed or displayed as desired
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To start with code examples in this chapter, we will use the DataFrame we created
    in [*Chapter 4*](B19176_04.xhtml#_idTextAnchor071):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: I have added an `Age` column in this DataFrame for further processing.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, in [*Chapter 4*](B19176_04.xhtml#_idTextAnchor071), we saved this
    DataFrame as a CSV file. The code snippet we used to read CSV files can be seen
    in the following code.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you might recall, we write CSV files with Spark using this line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the DataFrame, we can use SQL operations on it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code snippet shows how to perform a transformation on the loaded
    data. In this case, we filter the data to only include rows where the `Salary`
    column is greater than 3,000.
  prefs: []
  type: TYPE_NORMAL
- en: By using the `filter()` function, we can apply specific conditions to select
    the desired subset of data.
  prefs: []
  type: TYPE_NORMAL
- en: The transformed data will be stored in the results variable and ready for further
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Saving transformed data as a view
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once we have performed the necessary transformations, it is often useful to
    save the processed data as a view for easier access and future analysis. Let’s
    see how we can accomplish this in Spark SQL:'
  prefs: []
  type: TYPE_NORMAL
- en: The `createOrReplaceTempView()` method allows us to save the processed data
    as a view in Spark SQL. We provide a name for the view, in this case, `high_salary_employees`.
  prefs: []
  type: TYPE_NORMAL
- en: By giving the table a meaningful name, we can easily refer to it in subsequent
    operations and queries. The saved table acts as a structured representation of
    the processed data, facilitating further analysis and exploration.
  prefs: []
  type: TYPE_NORMAL
- en: With the transformed data saved as a table, we can leverage the power of SQL
    queries to gain insights and extract valuable information.
  prefs: []
  type: TYPE_NORMAL
- en: By using the `spark.sql()` method, we can execute SQL queries on the saved view
    `high_salary_employees`.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding example, we perform a simple query to select all columns from
    the view based on a filter condition.
  prefs: []
  type: TYPE_NORMAL
- en: The `show()` function displays the results of the SQL query, allowing us to
    examine the desired information extracted from the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Another method to create a view in Spark SQL is `createTempView()`. The difference
    between this method and `createOrReplaceTempView()` method is that `createTempView()`
    would only try to create a view. If that view name already exists in a catalog,
    then it would throw a `TempTableAlreadyExistsException` exception.
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing Spark SQL to filter and select data based on specific criteria
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will explore the syntax and practical examples of executing
    SQL queries and applying transformations using Spark SQL.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s consider a practical example where we execute an SQL query to filter
    and select specific data from a table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we create a temp view with the names of `employees` and execute
    an SQL query using Spark SQL to filter and select specific columns from the `employees`
    table.
  prefs: []
  type: TYPE_NORMAL
- en: The query selects the `employee`, `department`, `salary`, and `age` columns
    from the table where `age` is greater than 30\. The results of the query are stored
    in the `filtered_data` variable.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we call the `show()` method to display the filtered data.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring sorting and aggregation operations using Spark SQL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Spark SQL provides a rich set of transformation functions that can be applied
    to manipulate and transform data. Let’s explore some of the practical examples
    of transformations in Spark SQL:'
  prefs: []
  type: TYPE_NORMAL
- en: Aggregation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this example, we perform an aggregation operation using Spark SQL to calculate
    the average salary from the `employees` table.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The `AVG()` function calculates the average of the `salary` column. We alias
    the result as `average_salary` using the AS keyword.
  prefs: []
  type: TYPE_NORMAL
- en: 'The results are stored in the `average_salary` variable and displayed using
    the `show()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: Sorting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this example, we apply a sorting transformation to the `employees` table
    using Spark SQL.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The `ORDER BY` clause is used to specify the sorting criteria, in this case,
    the `salary` column in descending order.
  prefs: []
  type: TYPE_NORMAL
- en: The sorted data are stored in the `sorted_data` variable and displayed using
    the `show()` method.
  prefs: []
  type: TYPE_NORMAL
- en: Combining aggregations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can also combine different aggregations in one SQL command, such as in the
    following code example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we combine different transformations to the `employees` table
    using Spark SQL. First, we select those employees whose age is greater than 30
    and who have a salary greater than 3,000\. The `ORDER BY` clause is used to specify
    the sorting criteria; in this case, the `salary` column in descending order.
  prefs: []
  type: TYPE_NORMAL
- en: The resulting data are stored in the “`filtered_data`” variable and displayed
    using the `show()` method.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we explored the process of executing SQL queries and applying
    transformations using Spark SQL. We learned about the syntax for executing SQL
    queries and demonstrated practical examples of executing queries, filtering data,
    performing aggregations, and sorting data. By leveraging the expressive power
    of SQL and the flexibility of Spark SQL, you can efficiently analyze and manipulate
    structured data for a wide range of data analysis tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Grouping and aggregating data – grouping data based on specific columns and
    performing aggregate functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In Spark SQL, grouping and aggregating data are common operations that are performed
    to gain insights and summarize information from large datasets. This section will
    explore how to group data based on specific columns and perform various aggregate
    functions using Spark SQL. We will walk through code examples that demonstrate
    the capabilities of Spark SQL in this regard.
  prefs: []
  type: TYPE_NORMAL
- en: Grouping data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When we want to group data based on specific columns, we can utilize the `GROUP
    BY` clause in SQL queries. Let’s consider an example where we have a DataFrame
    of employees with the columns `department` and `salary`. We want to calculate
    the average salary for each department:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we group the data based on different transformations to the
    `employees` table using Spark SQL. First, we group employees based on the `Department`
    column. We take the average salary of each department from the `employees` table.
  prefs: []
  type: TYPE_NORMAL
- en: The resulting data are stored in the `grouped_data` variable and displayed using
    the `show()` method.
  prefs: []
  type: TYPE_NORMAL
- en: Aggregating data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Spark SQL provides a wide range of aggregate functions to calculate summary
    statistics on grouped data. Let’s consider another example where we want to calculate
    the total salary and the maximum salary for each department:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we combine and group the data based on different transformations
    to the `employees` table using Spark SQL. First, we group the employees based
    on `Department` column. We take the total salary and maximum salary of each department
    from the employees table. We also use an alias for these aggregated columns.
  prefs: []
  type: TYPE_NORMAL
- en: The resulting data are stored in the `aggregated_data` variable and displayed
    using the `show()` method.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have explored the capabilities of Spark SQL in grouping
    and aggregating data. We have seen examples of how to group data based on specific
    columns and perform various aggregate functions. Spark SQL provides a wide range
    of aggregate functions and allows for the creation of custom aggregate functions
    to suit specific requirements. With these capabilities, you can efficiently summarize
    and gain insights from large datasets using Spark SQL.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will look at advanced Spark SQL functions for complex
    data manipulation operations.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced Spark SQL operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's explore the key capabilities of Apache Spark's advanced operations.
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging window functions to perform advanced analytical operations on DataFrames
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will explore the powerful capabilities of window functions
    in Spark SQL for performing advanced analytical operations on DataFrames. Window
    functions provide a way to perform calculations across a set of rows within a
    partition, allowing us to derive insights and perform complex computations efficiently.
    In this section, we will dive into the topic of window functions and showcase
    code examples that demonstrate their usage in Spark SQL queries.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding window functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Window functions in Spark SQL enable advanced analytical operations by dividing
    a dataset into groups or partitions based on specified criteria. These functions
    operate on a sliding window of rows within each partition, performing calculations
    or aggregations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The general syntax for using window functions in Spark SQL is as follows:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The `function()` represents the window function that you want to apply, such
    as `sum`, `avg`, `row_number`, or custom-defined functions. The `over()` clause
    defines the window to which the function is applied. `Window.partitionBy()` specifies
    the columns used to divide the dataset into partitions. It also determines the
    order of rows within each partition. `rowsBetween(start, end)` specifies the range
    of rows included in the window. It can be unbounded or defined relative to the
    current row.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating cumulative sum using window functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s explore a practical example that demonstrates the usage of window functions
    to calculate a cumulative sum of a column in a DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, we start by importing the necessary libraries. We use the
    same DataFrame as our previous examples: `salary_data_with_id`.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we define a window specification using `Window.partitionBy("Department").orderBy("Age")`,
    which partitions the data according to the `Department` column and orders the
    rows within each partition according to the `Age` column.
  prefs: []
  type: TYPE_NORMAL
- en: We then use the `sum()` function as a window function, applied over the defined
    window specification, to calculate the cumulative sum of the `Salary` column.
    The result is stored in a new column called `cumulative_sum`.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we call the `show()` method to display the DataFrame with the added
    cumulative sum column. By leveraging window functions, we can efficiently calculate
    cumulative sums, running totals, rolling averages, and other complex analytical
    calculations over defined windows in Spark SQL.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we explored the powerful capabilities of window functions in
    Spark SQL for advanced analytics. We discussed the syntax and usage of window
    functions, allowing us to perform complex calculations and aggregations within
    defined partitions and windows. By incorporating window functions into Spark SQL
    queries, you can derive valuable insights and gain a deeper understanding of your
    data for advanced analytical operations.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will explore Spark user-defined functions.
  prefs: []
  type: TYPE_NORMAL
- en: User-defined functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will delve into the topic of **user-defined functions**
    (**UDFs**) in Spark SQL. UDFs allow us to extend the functionality of Spark SQL
    by defining our custom functions that can be applied to DataFrames or SQL queries.
    In this section, we will explore the concept of UDFs and provide code examples
    to demonstrate their usage and benefits in Spark SQL.
  prefs: []
  type: TYPE_NORMAL
- en: UDFs in Spark SQL enable us to create custom functions to perform transformations
    or computations on columns in a DataFrame or in SQL queries. UDFs are particularly
    useful when Spark’s built-in functions do not meet our specific requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'To define a UDF in Spark SQL, we use the `udf()` function from the `pyspark.sql.functions`
    module. The general syntax is as follows:'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: First, we import the `udf()` function from the `pyspark.sql.functions` module.
    Next, we define the UDF by providing a lambda function or a regular Python function
    as the `lambda_function` argument. This function encapsulates the custom logic
    we want to apply.
  prefs: []
  type: TYPE_NORMAL
- en: We also specify `return_type` for the UDF, which represents the data type that
    the UDF will return.
  prefs: []
  type: TYPE_NORMAL
- en: Applying a UDF to a DataFrame
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s explore a practical example that demonstrates the usage of UDFs in Spark
    SQL by applying a custom function to a DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we start by defining a UDF called `capitalize_udf` using the
    `udf()` function. It applies a lambda function that changes the input string to
    upper case. We use the `withColumn()` method to apply the UDF `capitalize_udf`
    to the `name` column, creating a new column called `capitalized_name` in the resulting
    DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we call the `show()` method to display the DataFrame with the transformed
    column.
  prefs: []
  type: TYPE_NORMAL
- en: UDFs allow us to apply custom logic and transformations to columns in DataFrames,
    enabling us to handle complex computations, perform string manipulations, or apply
    domain-specific operations that are not available in Spark’s built-in functions.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we explored the concept of UDFs in Spark SQL. We discussed
    the syntax for defining UDFs and demonstrated their usage through a code example.
    UDFs provide a powerful mechanism to extend Spark SQL’s functionality by allowing
    us to apply custom transformations and computations to DataFrames or SQL queries.
    By incorporating UDFs into your Spark SQL workflows, you can handle complex data
    operations and tailor your data processing pipelines to meet specific requirements
    or domain-specific needs.
  prefs: []
  type: TYPE_NORMAL
- en: Applying a function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'PySpark also supports various UDFs and APIs to allow users to execute native
    Python functions. For instance, the following example allows users to directly
    use the APIs in a pandas series within the Python native function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we start by defining a pandas UDF called `pandas_plus_one`
    using the `@pandas_udf()` function. We define this function so that it adds 1
    to a pandas series. We use the already created DataFrame named `salary_data_with_id`
    and call the pandas UDF to apply this function to the `salary` column of the DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we call the `show()` method in the same statement, to display the DataFrame
    with the transformed column.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, UDFs can be registered and invoked in SQL out of the box. The
    following is an example of how we can achieve this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we start by defining a pandas UDF called `add_one` by using
    the `@pandas_udf()` function. We define this function so that it adds 1 to a pandas
    series. We then register this UDF for use in SQL functions. We use the already
    created employees table and call the pandas UDF to apply this function to the
    `salary` column of the table.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we call the `show()` method in the same statement to display the results.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we explored the powerful capabilities of UDFs and how we can
    use them in calculating aggregations.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will explore pivot and unpivot functions.
  prefs: []
  type: TYPE_NORMAL
- en: Working with complex data types – pivot and unpivot
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pivot and unpivot operations are used to transform data from a row-based format
    to a column-based format and vice versa. In Spark SQL, these operations can be
    performed using the pivot and unpivot functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The pivot function is used to transform rows into columns. It takes three arguments:
    the column to use as the new column headers, the column to use as the new row
    headers and the columns to use as the values in the new table. The resulting table
    will have one row for each unique value in the row header column, and one column
    for each unique value in the column header column.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The unpivot function is used to transform columns into rows. It takes two arguments:
    the columns to use as the new row headers, and the column to use as the values
    in the new table. The resulting table will have one row for each unique combination
    of values in the row header columns and one column for the values.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Some use cases for pivot and unpivot operations include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Converting data from a wide format to a long format or vice versa
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aggregating data by multiple dimensions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating summary tables or reports
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing data for visualization or analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall, pivot and unpivot operations are useful tools for transforming data
    in Spark SQL.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the process of transforming and analyzing data
    in Spark SQL. We learned how to filter and manipulate loaded data, save the transformed
    data as a table, and execute SQL queries to extract meaningful insights. By following
    the Python code examples provided, you can apply these techniques to your own
    datasets, unlocking the potential of Spark SQL for data analysis and exploration.
  prefs: []
  type: TYPE_NORMAL
- en: After covering those topics, we explored the powerful capabilities of window
    functions in Spark SQL for advanced analytics. We discussed the syntax and usage
    of window functions, allowing us to perform complex calculations and aggregations
    within defined partitions and windows. By incorporating window functions into
    Spark SQL queries, you can derive valuable insights and gain a deeper understanding
    of your data for advanced analytical operations.
  prefs: []
  type: TYPE_NORMAL
- en: We then discussed some ways to use UDFs in Spark and how they can be useful
    in complex aggregations over multiple rows and columns of a DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we covered some of the ways to use pivot and unpivot in Spark SQL.
  prefs: []
  type: TYPE_NORMAL
- en: Sample questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Question 1**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following code snippets creates a view in Spark SQL that will replace
    existing views if already present?
  prefs: []
  type: TYPE_NORMAL
- en: '`dataFrame.createOrReplaceTempView()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`dataFrame.createTempView()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`dataFrame.createTableView()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`dataFrame.createOrReplaceTableView()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`dataDF.write.path(filePath)`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Question 2**:'
  prefs: []
  type: TYPE_NORMAL
- en: Which function do we use to join two DataFrames together?
  prefs: []
  type: TYPE_NORMAL
- en: '`DataFrame.filter()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`DataFrame.distinct()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`DataFrame.intersect()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`DataFrame.join()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`DataFrame.count()`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Answers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: D
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Part 4: Spark Applications'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this part, we will cover Spark’s Structured Streaming, focusing on real-time
    data processing with concepts such as event time processing, watermarking, triggers,
    and output modes. Practical examples will illustrate building and deploying streaming
    applications using Structured Streaming. Additionally, we will delve into Spark
    ML, Spark’s machine learning library, exploring supervised and unsupervised techniques,
    model building, evaluation, and hyperparameter tuning across various algorithms.
    Practical examples will demonstrate Spark ML's application in real-world machine
    learning tasks, crucial in contemporary data science. While not included in the
    Spark certification exam, understanding these concepts is essential in modern
    data engineering.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part has the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 7*](B19176_07.xhtml#_idTextAnchor183), *Structured Streaming in Spark*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B19176_08.xhtml#_idTextAnchor220), *Machine Learning with Spark
    ML*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
