- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: SQL Queries in Spark
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark SQL中的SQL查询
- en: In this chapter, we will explore the vast capabilities of Spark SQL for structured
    data processing. We will dive into loading and manipulating data, executing SQL
    queries, performing advanced analytics, and integrating Spark SQL with external
    systems. By the end of this chapter, you will have a solid understanding of Spark
    SQL’s features and be equipped with the knowledge to leverage its power in your
    data processing tasks.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探索Spark SQL在结构化数据处理方面的广泛功能。我们将深入了解加载数据、操作数据、执行SQL查询、执行高级分析和将Spark SQL与外部系统集成。到本章结束时，您将深入了解Spark
    SQL的功能，并具备利用其在数据处理任务中发挥其强大功能的知识。
- en: 'We will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖以下主题：
- en: What is Spark SQL?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是Spark SQL？
- en: Getting Started with Spark SQL
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark SQL入门
- en: Advanced Spark SQL operations
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高级Spark SQL操作
- en: What is Spark SQL?
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是Spark SQL？
- en: Spark SQL is a powerful module within the Apache Spark ecosystem that allows
    for the efficient processing and analysis of structured data. It provides a higher-level
    interface for working with structured data compared to the traditional RDD-based
    API of Apache Spark. Spark SQL combines the benefits of both relational and procedural
    processing, enabling users to seamlessly integrate SQL queries with complex analytics.
    By leveraging Spark’s distributed computing capabilities, Spark SQL enables scalable
    and high-performance data processing.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL是Apache Spark生态系统中的一个强大模块，它允许高效地处理和分析结构化数据。它提供了一个比Apache Spark传统的基于RDD的API更高级的接口来处理结构化数据。Spark
    SQL结合了关系和过程处理的优势，使用户能够无缝地将SQL查询与复杂分析集成。通过利用Spark的分布式计算能力，Spark SQL实现了可扩展和高效的数据处理。
- en: It provides a programming interface to work with structured data using SQL queries,
    DataFrame API, and Datasets API.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 它提供了一个编程接口，使用SQL查询、DataFrame API和Dataset API来处理结构化数据。
- en: It allows users to query data using SQL-like syntax and provides a powerful
    engine for executing SQL queries on large datasets. Spark SQL also supports reading
    and writing data from various structured sources such as Hive tables, Parquet
    files, and JDBC databases.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 它允许用户使用类似SQL的语法查询数据，并为在大型数据集上执行SQL查询提供强大的引擎。Spark SQL还支持从各种结构化数据源（如Hive表、Parquet文件和JDBC数据库）读取和写入数据。
- en: Advantages of Spark SQL
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark SQL的优势
- en: 'Spark SQL offers several key advantages that make it a popular choice for structured
    data processing:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL提供了几个关键优势，使其成为结构化数据处理的热门选择：
- en: Unified data processing with Spark SQL
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Spark SQL进行统一数据处理
- en: With Spark SQL, users can process both structured and unstructured data using
    a single engine. This means that users can use the same programming interface
    to query data stored in different formats such as JSON, CSV, and Parquet.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Spark SQL，用户可以使用单个引擎处理结构化和非结构化数据。这意味着用户可以使用相同的编程接口查询存储在不同格式（如JSON、CSV和Parquet）中的数据。
- en: Users can seamlessly switch between SQL queries, DataFrame transformations,
    and Spark’s machine learning APIs. This unified data processing approach allows
    for the easier integration of different data processing tasks within a single
    application, reducing development complexity.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 用户可以在SQL查询、DataFrame转换和Spark的机器学习API之间无缝切换。这种统一的数据处理方法使得在单个应用程序中集成不同的数据处理任务变得更加容易，从而降低了开发复杂性。
- en: Performance and scalability
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 性能和可扩展性
- en: Spark SQL leverages the distributed computing capabilities of Apache Spark,
    enabling the processing of large-scale datasets across a cluster of machines.
    It utilizes advanced query optimization techniques, such a the Catalyst optimizer
    (discussed in detail in [*Chapter 5*](B19176_05.xhtml#_idTextAnchor115)), to optimize
    and accelerate query execution. Additionally, Spark SQL supports data partitioning
    and caching mechanisms, further enhancing performance and scalability.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL利用Apache Spark的分布式计算能力，在机器集群上处理大规模数据集。它使用高级查询优化技术，如Catalyst优化器（在第5章中详细讨论），来优化和加速查询执行。此外，Spark
    SQL支持数据分区和缓存机制，进一步提高了性能和可扩展性。
- en: Spark SQL uses an optimized execution engine that can process queries much faster
    than traditional SQL engines. It achieves this by using in-memory caching and
    optimized query execution plans.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL使用优化的执行引擎，可以比传统的SQL引擎更快地处理查询。它通过使用内存缓存和优化的查询执行计划来实现这一点。
- en: Spark SQL is designed to scale horizontally across a cluster of machines. It
    can handle large datasets by partitioning them across multiple machines and processing
    them in parallel.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL 被设计为可以在机器集群上水平扩展。它可以通过将数据集分区到多台机器上并并行处理来处理大型数据集。
- en: Seamless integration with existing infrastructure
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 与现有基础设施的无缝集成
- en: Spark SQL integrates seamlessly with existing Apache Spark infrastructure and
    tools. It provides interoperability with other Spark components, such as Spark
    Streaming for real-time data processing and Spark MLlib for machine learning tasks.
    Furthermore, Spark SQL integrates with popular storage systems and data formats,
    including Parquet, Avro, ORC, and Hive, making it compatible with a wide range
    of data sources.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL 与现有的 Apache Spark 基础设施和工具无缝集成。它与其他 Spark 组件（如 Spark Streaming 用于实时数据处理和
    Spark MLlib 用于机器学习任务）提供互操作性。此外，Spark SQL 与流行的存储系统和数据格式（包括 Parquet、Avro、ORC 和 Hive）集成，使其与广泛的数据源兼容。
- en: Advanced analytics capabilities
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高级分析功能
- en: Spark SQL extends traditional SQL capabilities by using advanced analytics features.
    It supports window functions, which enable users to perform complex analytical
    operations, such as ranking, aggregation over sliding windows, and cumulative
    aggregations. The integration with machine learning libraries in Spark allows
    for the seamless integration of predictive analytics and data science workflows.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL 通过使用高级分析功能扩展了传统的 SQL 功能。它支持窗口函数，使用户能够执行复杂的分析操作，例如排名、滑动窗口上的聚合和累积聚合。与
    Spark 中的机器学习库的集成允许预测分析和数据科学工作流程的无缝集成。
- en: Ease of use
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 易用性
- en: Spark SQL provides a simple programming interface that allows users to query
    data using SQL-like syntax. This makes it easy for users who are familiar with
    SQL to get started with Spark SQL.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL 提供了一个简单的编程接口，允许用户使用类似 SQL 的语法查询数据。这使得熟悉 SQL 的用户能够轻松开始使用 Spark SQL。
- en: Integration with Apache Spark
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与 Apache Spark 的集成
- en: Spark SQL is an integral part of the Apache Spark framework and works seamlessly
    with other Spark components. It leverages Spark’s core functionalities, such as
    fault tolerance, data parallelism, and distributed computing, to provide scalable
    and efficient data processing. Spark SQL can read data from a variety of sources,
    including distributed file systems (such as HDFS), object stores (like Amazon
    S3), and relational databases (via JDBC). It also integrates with external systems
    such as Hive, allowing users to leverage existing Hive metadata and queries.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL 是 Apache Spark 框架的组成部分，与 Spark 的其他组件无缝协作。它利用 Spark 的核心功能，如容错、数据并行和分布式计算，以提供可扩展和高效的数据处理。Spark
    SQL 可以从各种来源读取数据，包括分布式文件系统（如 HDFS）、对象存储（如 Amazon S3）和关系数据库（通过 JDBC）。它还与外部系统（如 Hive）集成，使用户能够利用现有的
    Hive 元数据和查询。
- en: Now let’s take a look at some basic constructs of Spark SQL.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看 Spark SQL 的一些基本结构。
- en: Key concepts – DataFrames and datasets
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关键概念 – DataFrame 和 Dataset
- en: 'Spark SQL introduces two fundamental abstractions for working with structured
    data: DataFrames and Datasets.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL 引入了两个基本抽象，用于处理结构化数据：DataFrame 和 Dataset。
- en: DataFrames
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DataFrame
- en: DataFrames represent distributed collections of data organized into named columns.
    They provide a higher-level interface for working with structured data and offer
    rich APIs for data manipulation, filtering, aggregation, and querying. DataFrames
    are immutable and lazily evaluated, enabling optimized execution plans through
    Spark’s Catalyst optimizer. They can be created from various data sources, including
    structured files (CSV, JSON, and Parquet), Hive tables, and existing RDDs.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame 表示组织成命名列的分布式数据集合。它们提供了高级接口，用于处理结构化数据，并提供了丰富的 API 用于数据操作、过滤、聚合和查询。DataFrame
    是不可变的，并且是惰性评估的，通过 Spark 的 Catalyst 优化器实现优化执行计划。它们可以从各种数据源创建，包括结构化文件（CSV、JSON 和
    Parquet）、Hive 表和现有的 RDD。
- en: Datasets
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Dataset
- en: Datasets are an extension of DataFrames and provide a type-safe, object-oriented
    programming interface. Datasets combine the benefits of Spark’s RDDs (strong typing
    and user-defined functions) with the performance optimizations of DataFrames.
    Datasets enable compile-time type checking and can be seamlessly converted to
    DataFrames, allowing for flexible and efficient data processing.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Dataset 是 DataFrame 的扩展，提供了一种类型安全、面向对象的编程接口。Dataset 结合了 Spark 的 RDD（强类型和用户定义函数）的优点与
    DataFrame 的性能优化。Dataset 允许编译时类型检查，并且可以无缝转换为 DataFrame，从而实现灵活高效的数据处理。
- en: Now that we know what DataFrames and Datasets are, we’ll see how to apply different
    Spark SQL operations to these constructs in the next section.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经知道了 DataFrame 和 Dataset 是什么，我们将在下一节中看到如何将这些结构应用于不同的 Spark SQL 操作。
- en: Getting started with Spark SQL
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用 Spark SQL
- en: To get started with Spark SQL operations, we would first need to load data into
    a DataFrame. We’ll see how to do that next. Then, we will see how we can switch
    between PySpark and Spark SQL data and apply different transformations to it.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始使用 Spark SQL 操作，我们首先需要将数据加载到 DataFrame 中。我们将在下一节中看到如何做到这一点。然后，我们将看到如何在不同数据之间切换
    PySpark 和 Spark SQL，并对它应用不同的转换。
- en: Loading and saving data
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载数据和保存数据
- en: In this section, we will explore various techniques for loading data into Spark
    SQL from different sources and saving this as a table. We will delve into Python
    code examples that demonstrate how to effectively load data into Spark SQL, perform
    the necessary transformations, and save the processed data as a table for further
    analysis.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探索从不同来源将数据加载到 Spark SQL 中并保存为表的各种技术。我们将深入研究 Python 代码示例，演示如何有效地将数据加载到
    Spark SQL 中，执行必要的转换，并将处理后的数据保存为表以供进一步分析。
- en: 'Executing SQL queries in Spark SQL allows us to leverage the familiar SQL syntax
    and take advantage of its expressive power. Let’s take a look at the syntax and
    an example of executing an SQL query using Spark SQL:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark SQL 中执行 SQL 查询使我们能够利用熟悉的 SQL 语法并利用其表达力。让我们看看执行 SQL 查询的语法和示例：
- en: 'To execute an SQL query in Spark SQL, we use the `spark.sql()` method as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark SQL 中执行 SQL 查询，我们使用 `spark.sql()` 方法，如下所示：
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The `spark.sql()` method is used to execute SQL queries in Spark SQL
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spark.sql()` 方法用于在 Spark SQL 中执行 SQL 查询'
- en: Inside the method, we provide the SQL query as a string argument. In this example,
    we select all columns from the `tableName` table
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在方法内部，我们提供 SQL 查询作为字符串参数。在这个例子中，我们从 `tableName` 表中选择所有列。
- en: The results of the query are stored in the `results` variable, which can be
    further processed or displayed as desired
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询的结果存储在 `results` 变量中，可以进一步处理或按需显示。
- en: 'To start with code examples in this chapter, we will use the DataFrame we created
    in [*Chapter 4*](B19176_04.xhtml#_idTextAnchor071):'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始本章的代码示例，我们将使用在 [*第 4 章*](B19176_04.xhtml#_idTextAnchor071) 中创建的 DataFrame。
- en: '[PRE1]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The output will be the following:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: I have added an `Age` column in this DataFrame for further processing.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这个 DataFrame 中添加了一个 `Age` 列以进行进一步处理。
- en: Remember, in [*Chapter 4*](B19176_04.xhtml#_idTextAnchor071), we saved this
    DataFrame as a CSV file. The code snippet we used to read CSV files can be seen
    in the following code.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，在 [*第 4 章*](B19176_04.xhtml#_idTextAnchor071) 中，我们将这个 DataFrame 保存为 CSV 文件。我们用于读取
    CSV 文件的代码片段可以在以下代码中看到。
- en: 'As you might recall, we write CSV files with Spark using this line of code:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所回忆的，我们使用以下代码行用 Spark 编写 CSV 文件：
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The output will be the following:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now that we have the DataFrame, we can use SQL operations on it:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了 DataFrame，我们可以使用 SQL 操作来处理它：
- en: '[PRE5]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output will be the following:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The preceding code snippet shows how to perform a transformation on the loaded
    data. In this case, we filter the data to only include rows where the `Salary`
    column is greater than 3,000.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码片段展示了如何对加载的数据执行转换。在这种情况下，我们过滤数据，只包括 `Salary` 列大于 3,000 的行。
- en: By using the `filter()` function, we can apply specific conditions to select
    the desired subset of data.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用 `filter()` 函数，我们可以应用特定条件来选择所需的数据子集。
- en: The transformed data will be stored in the results variable and ready for further
    analysis.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 转换后的数据将存储在 `results` 变量中，并准备好进行进一步分析。
- en: Saving transformed data as a view
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将转换后的数据保存为视图
- en: 'Once we have performed the necessary transformations, it is often useful to
    save the processed data as a view for easier access and future analysis. Let’s
    see how we can accomplish this in Spark SQL:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们完成了必要的转换，通常将处理后的数据保存为视图以便于访问和未来的分析是有用的。让我们看看如何在 Spark SQL 中实现这一点：
- en: The `createOrReplaceTempView()` method allows us to save the processed data
    as a view in Spark SQL. We provide a name for the view, in this case, `high_salary_employees`.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`createOrReplaceTempView()` 方法允许我们将处理后的数据作为 Spark SQL 中的视图保存。我们为视图提供名称，在本例中为
    `high_salary_employees`。'
- en: By giving the table a meaningful name, we can easily refer to it in subsequent
    operations and queries. The saved table acts as a structured representation of
    the processed data, facilitating further analysis and exploration.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 通过给表赋予一个有意义的名称，我们可以在后续的操作和查询中轻松引用它。保存的表作为处理数据的结构化表示，便于进一步分析和探索。
- en: With the transformed data saved as a table, we can leverage the power of SQL
    queries to gain insights and extract valuable information.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 将转换后的数据保存为表格后，我们可以利用SQL查询的功能来获取洞察力并提取有价值的信息。
- en: By using the `spark.sql()` method, we can execute SQL queries on the saved view
    `high_salary_employees`.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用`spark.sql()`方法，我们可以对保存的视图`high_salary_employees`执行SQL查询。
- en: In the preceding example, we perform a simple query to select all columns from
    the view based on a filter condition.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们执行了一个简单的查询，根据过滤条件从视图中选择所有列。
- en: The `show()` function displays the results of the SQL query, allowing us to
    examine the desired information extracted from the dataset.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`show()`函数显示了SQL查询的结果，使我们能够检查从数据集中提取的所需信息。'
- en: Another method to create a view in Spark SQL is `createTempView()`. The difference
    between this method and `createOrReplaceTempView()` method is that `createTempView()`
    would only try to create a view. If that view name already exists in a catalog,
    then it would throw a `TempTableAlreadyExistsException` exception.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark SQL中创建视图的另一种方法是`createTempView()`。与`createOrReplaceTempView()`方法相比，`createTempView()`只会尝试创建一个视图。如果该视图名称已存在于目录中，则会抛出`TempTableAlreadyExistsException`异常。
- en: Utilizing Spark SQL to filter and select data based on specific criteria
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用Spark SQL根据特定标准过滤和选择数据
- en: In this section, we will explore the syntax and practical examples of executing
    SQL queries and applying transformations using Spark SQL.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨执行SQL查询和应用转换的语法和实际示例。
- en: 'Let’s consider a practical example where we execute an SQL query to filter
    and select specific data from a table:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个实际示例，其中我们执行一个SQL查询来过滤和选择表中的特定数据：
- en: '[PRE7]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output will be the following:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE8]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In this example, we create a temp view with the names of `employees` and execute
    an SQL query using Spark SQL to filter and select specific columns from the `employees`
    table.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们创建一个临时视图，名为`employees`，并使用Spark SQL执行SQL查询以过滤和选择`employees`表中的特定列。
- en: The query selects the `employee`, `department`, `salary`, and `age` columns
    from the table where `age` is greater than 30\. The results of the query are stored
    in the `filtered_data` variable.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 查询选择了`employee`、`department`、`salary`和`age`列，其中`age`大于30。查询的结果存储在`filtered_data`变量中。
- en: Finally, we call the `show()` method to display the filtered data.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们调用`show()`方法来显示过滤后的数据。
- en: Exploring sorting and aggregation operations using Spark SQL
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索使用Spark SQL进行排序和聚合操作
- en: 'Spark SQL provides a rich set of transformation functions that can be applied
    to manipulate and transform data. Let’s explore some of the practical examples
    of transformations in Spark SQL:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL提供了一组丰富的转换函数，可以应用于操作和转换数据。让我们探索一些Spark SQL中转换的实际示例：
- en: Aggregation
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 聚合
- en: In this example, we perform an aggregation operation using Spark SQL to calculate
    the average salary from the `employees` table.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们使用Spark SQL执行聚合操作，从`employees`表中计算平均工资。
- en: '[PRE9]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output will be the following:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE10]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The `AVG()` function calculates the average of the `salary` column. We alias
    the result as `average_salary` using the AS keyword.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '`AVG()`函数计算`salary`列的平均值。我们使用AS关键字将结果别名为`average_salary`。'
- en: 'The results are stored in the `average_salary` variable and displayed using
    the `show()` method:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 结果存储在`average_salary`变量中，并使用`show()`方法显示：
- en: Sorting
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 排序
- en: In this example, we apply a sorting transformation to the `employees` table
    using Spark SQL.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们使用Spark SQL对`employees`表应用排序转换。
- en: '[PRE11]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output will be the following:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE12]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The `ORDER BY` clause is used to specify the sorting criteria, in this case,
    the `salary` column in descending order.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`ORDER BY`子句来指定排序标准，在本例中是对`salary`列进行降序排序。
- en: The sorted data are stored in the `sorted_data` variable and displayed using
    the `show()` method.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 排序后的数据存储在`sorted_data`变量中，并使用`show()`方法进行显示。
- en: Combining aggregations
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结合聚合
- en: 'We can also combine different aggregations in one SQL command, such as in the
    following code example:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以在一个SQL命令中结合不同的聚合操作，如下面的代码示例所示：
- en: '[PRE13]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output will be the following:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE14]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In this example, we combine different transformations to the `employees` table
    using Spark SQL. First, we select those employees whose age is greater than 30
    and who have a salary greater than 3,000\. The `ORDER BY` clause is used to specify
    the sorting criteria; in this case, the `salary` column in descending order.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用 Spark SQL 对 `employees` 表进行不同的转换。首先，我们选择那些年龄大于 30 岁且工资大于 3,000 的员工。`ORDER
    BY` 子句用于指定排序标准；在这种情况下，按 `salary` 列降序排序。
- en: The resulting data are stored in the “`filtered_data`” variable and displayed
    using the `show()` method.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 结果数据存储在“`filtered_data`”变量中，并使用 `show()` 方法显示。
- en: In this section, we explored the process of executing SQL queries and applying
    transformations using Spark SQL. We learned about the syntax for executing SQL
    queries and demonstrated practical examples of executing queries, filtering data,
    performing aggregations, and sorting data. By leveraging the expressive power
    of SQL and the flexibility of Spark SQL, you can efficiently analyze and manipulate
    structured data for a wide range of data analysis tasks.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了使用 Spark SQL 执行 SQL 查询和应用转换的过程。我们学习了执行 SQL 查询的语法，并展示了执行查询、过滤数据、执行聚合和排序数据的实际示例。通过利用
    SQL 的表达能力和 Spark SQL 的灵活性，您可以高效地分析和操作结构化数据，以完成各种数据分析任务。
- en: Grouping and aggregating data – grouping data based on specific columns and
    performing aggregate functions
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 根据特定列进行分组和聚合数据 – 基于特定列进行分组并执行聚合函数
- en: In Spark SQL, grouping and aggregating data are common operations that are performed
    to gain insights and summarize information from large datasets. This section will
    explore how to group data based on specific columns and perform various aggregate
    functions using Spark SQL. We will walk through code examples that demonstrate
    the capabilities of Spark SQL in this regard.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark SQL 中，分组和聚合数据是常见的操作，用于从大型数据集中获取洞察和总结信息。本节将探讨如何使用 Spark SQL 根据特定列分组数据并执行各种聚合函数。我们将通过代码示例演示
    Spark SQL 在此方面的功能。
- en: Grouping data
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据分组
- en: 'When we want to group data based on specific columns, we can utilize the `GROUP
    BY` clause in SQL queries. Let’s consider an example where we have a DataFrame
    of employees with the columns `department` and `salary`. We want to calculate
    the average salary for each department:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们想要根据特定列对数据进行分组时，可以利用 SQL 查询中的 `GROUP BY` 子句。让我们考虑一个例子，其中我们有一个包含 `department`
    和 `salary` 列的员工 DataFrame。我们想要计算每个部门的平均工资：
- en: '[PRE15]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output will be the following:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE16]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In this example, we group the data based on different transformations to the
    `employees` table using Spark SQL. First, we group employees based on the `Department`
    column. We take the average salary of each department from the `employees` table.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用 Spark SQL 对 `employees` 表的不同转换进行数据分组。首先，我们根据 `Department` 列对员工进行分组。我们从
    `employees` 表中获取每个部门的平均工资。
- en: The resulting data are stored in the `grouped_data` variable and displayed using
    the `show()` method.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 结果数据存储在 `grouped_data` 变量中，并使用 `show()` 方法显示。
- en: Aggregating data
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据聚合
- en: 'Spark SQL provides a wide range of aggregate functions to calculate summary
    statistics on grouped data. Let’s consider another example where we want to calculate
    the total salary and the maximum salary for each department:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL 提供了广泛的聚合函数，用于对分组数据进行汇总统计。让我们考虑另一个例子，其中我们想要计算每个部门的总工资和最高工资：
- en: '[PRE17]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output will be the following:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE18]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In this example, we combine and group the data based on different transformations
    to the `employees` table using Spark SQL. First, we group the employees based
    on `Department` column. We take the total salary and maximum salary of each department
    from the employees table. We also use an alias for these aggregated columns.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用 Spark SQL 对 `employees` 表的不同转换进行合并和分组。首先，我们根据 `Department` 列对员工进行分组。我们从员工表中获取每个部门的总工资和最高工资。我们还为这些聚合列使用了别名。
- en: The resulting data are stored in the `aggregated_data` variable and displayed
    using the `show()` method.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 结果数据存储在 `aggregated_data` 变量中，并使用 `show()` 方法显示。
- en: In this section, we have explored the capabilities of Spark SQL in grouping
    and aggregating data. We have seen examples of how to group data based on specific
    columns and perform various aggregate functions. Spark SQL provides a wide range
    of aggregate functions and allows for the creation of custom aggregate functions
    to suit specific requirements. With these capabilities, you can efficiently summarize
    and gain insights from large datasets using Spark SQL.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了 Spark SQL 在数据分组和聚合方面的功能。我们看到了如何根据特定列分组数据并执行各种聚合函数的示例。Spark SQL 提供了广泛的聚合函数，并允许创建自定义聚合函数以满足特定需求。利用这些功能，您可以使用
    Spark SQL 高效地总结和从大量数据集中获得洞察。
- en: In the next section, we will look at advanced Spark SQL functions for complex
    data manipulation operations.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨用于复杂数据操作的高级 Spark SQL 函数。
- en: Advanced Spark SQL operations
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级 Spark SQL 操作
- en: Let's explore the key capabilities of Apache Spark's advanced operations.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索 Apache Spark 高级操作的关键功能。
- en: Leveraging window functions to perform advanced analytical operations on DataFrames
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用窗口函数在 DataFrame 上执行高级分析操作
- en: In this section, we will explore the powerful capabilities of window functions
    in Spark SQL for performing advanced analytical operations on DataFrames. Window
    functions provide a way to perform calculations across a set of rows within a
    partition, allowing us to derive insights and perform complex computations efficiently.
    In this section, we will dive into the topic of window functions and showcase
    code examples that demonstrate their usage in Spark SQL queries.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨 Spark SQL 中窗口函数的强大功能，用于在 DataFrame 上执行高级分析操作。窗口函数提供了一种在分区内对一组行进行计算的方法，使我们能够获得洞察并有效地执行复杂计算。在本节中，我们将深入研究窗口函数的主题，并通过展示
    Spark SQL 查询中其使用的代码示例来展示其用法。
- en: Understanding window functions
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解窗口函数
- en: Window functions in Spark SQL enable advanced analytical operations by dividing
    a dataset into groups or partitions based on specified criteria. These functions
    operate on a sliding window of rows within each partition, performing calculations
    or aggregations.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL 中的窗口函数通过根据指定标准将数据集划分为组或分区，从而实现高级分析操作。这些函数在每个分区内部对滑动窗口中的行进行计算或聚合。
- en: 'The general syntax for using window functions in Spark SQL is as follows:'
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 Spark SQL 中使用窗口函数的一般语法如下：
- en: '[PRE19]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The `function()` represents the window function that you want to apply, such
    as `sum`, `avg`, `row_number`, or custom-defined functions. The `over()` clause
    defines the window to which the function is applied. `Window.partitionBy()` specifies
    the columns used to divide the dataset into partitions. It also determines the
    order of rows within each partition. `rowsBetween(start, end)` specifies the range
    of rows included in the window. It can be unbounded or defined relative to the
    current row.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '`function()` 代表您想要应用的窗口函数，例如 `sum`、`avg`、`row_number` 或自定义定义的函数。`over()` 子句定义了函数应用的窗口。`Window.partitionBy()`
    指定用于将数据集划分为分区的列。它还确定了每个分区内部行的顺序。`rowsBetween(start, end)` 指定窗口中包含的行范围。它可以是不限定的或相对于当前行的相对定义。'
- en: Calculating cumulative sum using window functions
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用窗口函数计算累积和
- en: 'Let’s explore a practical example that demonstrates the usage of window functions
    to calculate a cumulative sum of a column in a DataFrame:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个实际示例来探索窗口函数的使用，以计算 DataFrame 中某列的累积和：
- en: '[PRE20]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The output will be the following:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE21]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'In this example, we start by importing the necessary libraries. We use the
    same DataFrame as our previous examples: `salary_data_with_id`.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们首先导入必要的库。我们使用与之前示例相同的 DataFrame：`salary_data_with_id`。
- en: Next, we define a window specification using `Window.partitionBy("Department").orderBy("Age")`,
    which partitions the data according to the `Department` column and orders the
    rows within each partition according to the `Age` column.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用 `Window.partitionBy("Department").orderBy("Age")` 定义窗口规范，根据 `Department`
    列对数据进行分区，并在每个分区内部根据 `Age` 列对行进行排序。
- en: We then use the `sum()` function as a window function, applied over the defined
    window specification, to calculate the cumulative sum of the `Salary` column.
    The result is stored in a new column called `cumulative_sum`.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将 `sum()` 函数用作窗口函数，在定义的窗口规范上应用，以计算 `Salary` 列的累积和。结果存储在一个名为 `cumulative_sum`
    的新列中。
- en: Finally, we call the `show()` method to display the DataFrame with the added
    cumulative sum column. By leveraging window functions, we can efficiently calculate
    cumulative sums, running totals, rolling averages, and other complex analytical
    calculations over defined windows in Spark SQL.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们调用 `show()` 方法来显示具有附加累积和列的 DataFrame。通过利用窗口函数，我们可以在 Spark SQL 中高效地计算累积和、滚动总和、滚动平均值以及其他复杂分析计算。
- en: In this section, we explored the powerful capabilities of window functions in
    Spark SQL for advanced analytics. We discussed the syntax and usage of window
    functions, allowing us to perform complex calculations and aggregations within
    defined partitions and windows. By incorporating window functions into Spark SQL
    queries, you can derive valuable insights and gain a deeper understanding of your
    data for advanced analytical operations.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了 Spark SQL 中窗口函数的强大功能，用于高级分析。我们讨论了窗口函数的语法和用法，使我们能够在定义的分区和窗口内执行复杂计算和聚合。通过将窗口函数集成到
    Spark SQL 查询中，您可以获得有价值的见解，并深入了解您的数据，以便进行高级分析操作。
- en: In the next section, we will explore Spark user-defined functions.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨 Spark 用户定义函数。
- en: User-defined functions
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用户定义函数
- en: In this section, we will delve into the topic of **user-defined functions**
    (**UDFs**) in Spark SQL. UDFs allow us to extend the functionality of Spark SQL
    by defining our custom functions that can be applied to DataFrames or SQL queries.
    In this section, we will explore the concept of UDFs and provide code examples
    to demonstrate their usage and benefits in Spark SQL.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将深入探讨 Spark SQL 中的 **用户定义函数**（**UDFs**）主题。UDFs 允许我们通过定义自己的函数来扩展 Spark
    SQL 的功能，这些函数可以应用于 DataFrame 或 SQL 查询。在本节中，我们将探讨 UDF 的概念，并提供代码示例以展示它们在 Spark SQL
    中的使用和优势。
- en: UDFs in Spark SQL enable us to create custom functions to perform transformations
    or computations on columns in a DataFrame or in SQL queries. UDFs are particularly
    useful when Spark’s built-in functions do not meet our specific requirements.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL 中的 UDF 允许我们创建自定义函数，以在 DataFrame 或 SQL 查询中对列进行转换或计算。当 Spark 的内置函数不能满足我们的特定要求时，UDF
    特别有用。
- en: 'To define a UDF in Spark SQL, we use the `udf()` function from the `pyspark.sql.functions`
    module. The general syntax is as follows:'
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 要在 Spark SQL 中定义 UDF，我们使用 `pyspark.sql.functions` 模块中的 `udf()` 函数。其一般语法如下：
- en: '[PRE22]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: First, we import the `udf()` function from the `pyspark.sql.functions` module.
    Next, we define the UDF by providing a lambda function or a regular Python function
    as the `lambda_function` argument. This function encapsulates the custom logic
    we want to apply.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们从 `pyspark.sql.functions` 模块导入 `udf()` 函数。接下来，我们通过提供 lambda 函数或常规 Python
    函数作为 `lambda_function` 参数来定义 UDF。此函数封装了我们想要应用的自定义逻辑。
- en: We also specify `return_type` for the UDF, which represents the data type that
    the UDF will return.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还指定了 UDF 的 `return_type`，它表示 UDF 将返回的数据类型。
- en: Applying a UDF to a DataFrame
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将 UDF 应用到 DataFrame
- en: 'Let’s explore a practical example that demonstrates the usage of UDFs in Spark
    SQL by applying a custom function to a DataFrame:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个实际示例来探索 UDF 在 Spark SQL 中的使用，该示例通过将自定义函数应用于 DataFrame 来演示：
- en: '[PRE23]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The output will be the following:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将是以下内容：
- en: '[PRE24]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In this example, we start by defining a UDF called `capitalize_udf` using the
    `udf()` function. It applies a lambda function that changes the input string to
    upper case. We use the `withColumn()` method to apply the UDF `capitalize_udf`
    to the `name` column, creating a new column called `capitalized_name` in the resulting
    DataFrame.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们首先使用 `udf()` 函数定义一个名为 `capitalize_udf` 的 UDF。它应用一个 lambda 函数，将输入字符串转换为大写。我们使用
    `withColumn()` 方法将 UDF `capitalize_udf` 应用到 `name` 列，在结果 DataFrame 中创建一个名为 `capitalized_name`
    的新列。
- en: Finally, we call the `show()` method to display the DataFrame with the transformed
    column.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们调用 `show()` 方法来显示具有转换列的 DataFrame。
- en: UDFs allow us to apply custom logic and transformations to columns in DataFrames,
    enabling us to handle complex computations, perform string manipulations, or apply
    domain-specific operations that are not available in Spark’s built-in functions.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: UDFs 允许我们对 DataFrame 中的列应用自定义逻辑和转换，使我们能够处理复杂计算、执行字符串操作或应用在 Spark 内置函数中不可用的特定领域操作。
- en: In this section, we explored the concept of UDFs in Spark SQL. We discussed
    the syntax for defining UDFs and demonstrated their usage through a code example.
    UDFs provide a powerful mechanism to extend Spark SQL’s functionality by allowing
    us to apply custom transformations and computations to DataFrames or SQL queries.
    By incorporating UDFs into your Spark SQL workflows, you can handle complex data
    operations and tailor your data processing pipelines to meet specific requirements
    or domain-specific needs.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了 Spark SQL 中 UDFs 的概念。我们讨论了定义 UDFs 的语法，并通过代码示例演示了其用法。UDFs 通过允许我们对
    DataFrame 或 SQL 查询应用自定义转换和计算，提供了一个强大的机制来扩展 Spark SQL 的功能。通过将 UDFs 纳入您的 Spark SQL
    工作流程，您可以处理复杂的数据操作，并定制数据处理管道以满足特定要求或特定领域的需求。
- en: Applying a function
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应用函数
- en: 'PySpark also supports various UDFs and APIs to allow users to execute native
    Python functions. For instance, the following example allows users to directly
    use the APIs in a pandas series within the Python native function:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark 也支持各种 UDFs 和 API，允许用户在 Python 原生函数中直接使用这些 API。例如，以下示例允许用户在 Python 原生函数中直接使用
    pandas 序列中的 API：
- en: '[PRE25]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The output will be the following:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE26]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: In this example, we start by defining a pandas UDF called `pandas_plus_one`
    using the `@pandas_udf()` function. We define this function so that it adds 1
    to a pandas series. We use the already created DataFrame named `salary_data_with_id`
    and call the pandas UDF to apply this function to the `salary` column of the DataFrame.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们首先使用 `@pandas_udf()` 函数定义一个名为 `pandas_plus_one` 的 pandas UDF。我们定义此函数以便将其添加到
    pandas 序列中。我们使用已创建的名为 `salary_data_with_id` 的 DataFrame，并调用 pandas UDF 将此函数应用于
    DataFrame 的 `salary` 列。
- en: Finally, we call the `show()` method in the same statement, to display the DataFrame
    with the transformed column.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在同一语句中调用 `show()` 方法，以显示转换后的列的 DataFrame。
- en: 'In addition, UDFs can be registered and invoked in SQL out of the box. The
    following is an example of how we can achieve this:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，UDFs 可以直接在 SQL 中注册和调用。以下是一个示例，说明我们如何实现这一点：
- en: '[PRE27]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The output will be the following:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '[PRE28]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: In this example, we start by defining a pandas UDF called `add_one` by using
    the `@pandas_udf()` function. We define this function so that it adds 1 to a pandas
    series. We then register this UDF for use in SQL functions. We use the already
    created employees table and call the pandas UDF to apply this function to the
    `salary` column of the table.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们首先使用 `@pandas_udf()` 函数定义一个名为 `add_one` 的 pandas UDF。我们定义此函数以便将其添加到 pandas
    序列中。然后，我们将此 UDF 注册以用于 SQL 函数。我们使用已创建的员工表，并调用 pandas UDF 将此函数应用于表的 `salary` 列。
- en: Finally, we call the `show()` method in the same statement to display the results.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在同一语句中调用 `show()` 方法以显示结果。
- en: In this section, we explored the powerful capabilities of UDFs and how we can
    use them in calculating aggregations.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了 UDFs 的强大功能以及我们如何在使用聚合计算中使用它们。
- en: In the next section, we will explore pivot and unpivot functions.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨旋转和逆旋转函数。
- en: Working with complex data types – pivot and unpivot
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理复杂数据类型 – 旋转和逆旋转
- en: Pivot and unpivot operations are used to transform data from a row-based format
    to a column-based format and vice versa. In Spark SQL, these operations can be
    performed using the pivot and unpivot functions.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 旋转和逆旋转操作用于将数据从基于行的格式转换为基于列的格式，反之亦然。在 Spark SQL 中，可以使用旋转和逆旋转函数执行这些操作。
- en: 'The pivot function is used to transform rows into columns. It takes three arguments:
    the column to use as the new column headers, the column to use as the new row
    headers and the columns to use as the values in the new table. The resulting table
    will have one row for each unique value in the row header column, and one column
    for each unique value in the column header column.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 旋转函数用于将行转换为列。它接受三个参数：用作新列标题的列，用作新行标题的列，以及用作新表中值的列。结果表将具有行标题列中每个唯一值的一行，以及列标题列中每个唯一值的一列。
- en: 'The unpivot function is used to transform columns into rows. It takes two arguments:
    the columns to use as the new row headers, and the column to use as the values
    in the new table. The resulting table will have one row for each unique combination
    of values in the row header columns and one column for the values.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 逆旋转函数用于将列转换为行。它接受两个参数：用作新行标题的列，以及用作新表中值的列。结果表将具有每个行标题列中值的唯一组合的一行，以及每个值的一列。
- en: 'Some use cases for pivot and unpivot operations include the following:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: pivot 和 unpivot 操作的一些用例包括以下内容：
- en: Converting data from a wide format to a long format or vice versa
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据从宽格式转换为长格式或反之亦然
- en: Aggregating data by multiple dimensions
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过多个维度聚合数据
- en: Creating summary tables or reports
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建汇总表或报告
- en: Preparing data for visualization or analysis
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备数据以进行可视化或分析
- en: Overall, pivot and unpivot operations are useful tools for transforming data
    in Spark SQL.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，pivot 和 unpivot 操作是 Spark SQL 中转换数据的实用工具。
- en: Summary
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored the process of transforming and analyzing data
    in Spark SQL. We learned how to filter and manipulate loaded data, save the transformed
    data as a table, and execute SQL queries to extract meaningful insights. By following
    the Python code examples provided, you can apply these techniques to your own
    datasets, unlocking the potential of Spark SQL for data analysis and exploration.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了在 Spark SQL 中转换和分析数据的过程。我们学习了如何过滤和操作加载的数据，将转换后的数据保存为表，并执行 SQL 查询以提取有意义的见解。通过遵循提供的
    Python 代码示例，你可以将这些技术应用到自己的数据集中，释放 Spark SQL 在数据分析和解探中的潜力。
- en: After covering those topics, we explored the powerful capabilities of window
    functions in Spark SQL for advanced analytics. We discussed the syntax and usage
    of window functions, allowing us to perform complex calculations and aggregations
    within defined partitions and windows. By incorporating window functions into
    Spark SQL queries, you can derive valuable insights and gain a deeper understanding
    of your data for advanced analytical operations.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍那些主题之后，我们探讨了 Spark SQL 中窗口函数的强大功能，用于高级分析。我们讨论了窗口函数的语法和用法，使我们能够在定义的分区和窗口内执行复杂的计算和聚合。通过将窗口函数纳入
    Spark SQL 查询，你可以获得有价值的见解，并更深入地理解你的数据，以便进行高级分析操作。
- en: We then discussed some ways to use UDFs in Spark and how they can be useful
    in complex aggregations over multiple rows and columns of a DataFrame.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们讨论了一些在 Spark 中使用 UDF 的方法以及它们如何在 DataFrame 的多行和多列的复杂聚合中变得有用。
- en: Finally, we covered some of the ways to use pivot and unpivot in Spark SQL.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们介绍了一些在 Spark SQL 中使用 pivot 和 unpivot 的方法。
- en: Sample questions
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 样题
- en: '**Question 1**:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题1**：'
- en: Which of the following code snippets creates a view in Spark SQL that will replace
    existing views if already present?
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个代码片段会在 Spark SQL 中创建一个视图，如果已经存在则替换现有视图？
- en: '`dataFrame.createOrReplaceTempView()`'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`dataframe.createOrReplaceTempView()`'
- en: '`dataFrame.createTempView()`'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`dataframe.createTempView()`'
- en: '`dataFrame.createTableView()`'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`dataFrame.createTableView()`'
- en: '`dataFrame.createOrReplaceTableView()`'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`dataFrame.createOrReplaceTableView()`'
- en: '`dataDF.write.path(filePath)`'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`dataDF.write.path(filePath)`'
- en: '**Question 2**:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题2**：'
- en: Which function do we use to join two DataFrames together?
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用哪个函数将两个 DataFrame 合并在一起？
- en: '`DataFrame.filter()`'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`DataFrame.filter()`'
- en: '`DataFrame.distinct()`'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`DataFrame.distinct()`'
- en: '`DataFrame.intersect()`'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`DataFrame.intersect()`'
- en: '`DataFrame.join()`'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`DataFrame.join()`'
- en: '`DataFrame.count()`'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`DataFrame.count()`'
- en: Answers
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 答案
- en: A
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A
- en: D
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: D
- en: 'Part 4: Spark Applications'
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四部分：Spark 应用程序
- en: In this part, we will cover Spark’s Structured Streaming, focusing on real-time
    data processing with concepts such as event time processing, watermarking, triggers,
    and output modes. Practical examples will illustrate building and deploying streaming
    applications using Structured Streaming. Additionally, we will delve into Spark
    ML, Spark’s machine learning library, exploring supervised and unsupervised techniques,
    model building, evaluation, and hyperparameter tuning across various algorithms.
    Practical examples will demonstrate Spark ML's application in real-world machine
    learning tasks, crucial in contemporary data science. While not included in the
    Spark certification exam, understanding these concepts is essential in modern
    data engineering.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在这部分，我们将介绍 Spark 的结构化流，重点关注使用事件时间处理、水印、触发器和输出模式等概念进行实时数据处理。实际示例将说明如何使用结构化流构建和部署流应用程序。此外，我们还将深入研究
    Spark ML，Spark 的机器学习库，探索监督和非监督技术，模型构建、评估以及跨各种算法的超参数调整。实际示例将展示 Spark ML 在现实世界机器学习任务中的应用，这对于当代数据科学至关重要。虽然这些内容不包括在
    Spark 认证考试中，但理解这些概念对于现代数据工程至关重要。
- en: 'This part has the following chapters:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包含以下章节：
- en: '[*Chapter 7*](B19176_07.xhtml#_idTextAnchor183), *Structured Streaming in Spark*'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第7章*](B19176_07.xhtml#_idTextAnchor183)，*Spark中的结构化流*'
- en: '[*Chapter 8*](B19176_08.xhtml#_idTextAnchor220), *Machine Learning with Spark
    ML*'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第8章*](B19176_08.xhtml#_idTextAnchor220)，*使用 Spark ML 进行机器学习*'
