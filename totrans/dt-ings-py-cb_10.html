<html><head></head><body>
<div><div><h1 class="chapter-number" id="_idParaDest-357"><a id="_idTextAnchor364"/>10</h1>
<h1 id="_idParaDest-358"><a id="_idTextAnchor365"/>Logging and Monitoring Your Data Ingest in Airﬂow</h1>
<p>We already know how vital logging and monitoring are to manage applications and systems, and Airflow is no different. In fact, <strong class="bold">Apache Airflow</strong> already has built-in modules to create logs and export them. But what about improving them?</p>
<p>In the previous chapter, <em class="italic">Putting Everything Together with Airﬂow</em>, we covered the fundamental aspects of Airflow, how to start our data ingestion, and how to orchestrate a pipeline and use the best data development practices. Now, let’s put into practice the best techniques to enhance logging and monitor Airflow pipelines.</p>
<p>In this chapter, you will learn the following recipes:</p>
<ul>
<li>Creating basic logs in Airﬂow</li>
<li>Storing log files in a remote location</li>
<li>Configuring logs in <code>airflow.cfg</code></li>
<li>Designing advanced monitoring</li>
<li>Using notiﬁcation operators</li>
<li>Using SQL operators for data quality</li>
</ul>
<h1 id="_idParaDest-359"><a id="_idTextAnchor366"/>Technical requirements</h1>
<p>You can find the code for this chapter in the GitHub repository here: <a href="https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook">https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook</a>.</p>
<h2 id="_idParaDest-360"><a id="_idTextAnchor367"/>Installing and running Airflow</h2>
<p>This <a id="_idIndexMarker665"/>chapter requires that Airflow is installed on your local machine. You can install it directly on your <strong class="bold">operating system</strong> (<strong class="bold">OS</strong>) or by using a Docker image. For more information, refer to the <em class="italic">Configuring Docker for Airflow</em> recipe in <a href="B19453_01.xhtml#_idTextAnchor022"><em class="italic">Chapter 1</em></a>.</p>
<p>After following the steps described in <a href="B19453_01.xhtml#_idTextAnchor022"><em class="italic">Chapter 1</em></a>, ensure your Airflow runs correctly. You can do that by checking the Airflow UI here: <code>http://localhost:8080</code>.</p>
<p>If you are using a <a id="_idIndexMarker666"/>Docker container (as I am) to host your Airflow application, you can check its status on the terminal by running the following command:</p>
<pre class="source-code">
$ docker ps</pre>
<p>You can see the command running here:</p>
<div><div><img alt="Figure 10.1 – Airflow containers running" height="131" src="img/Figure_9.01_B19453.jpg" width="1407"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.1 – Airflow containers running</p>
<p>For Docker, check the container status<a id="_idIndexMarker667"/> on <strong class="bold">Docker Desktop</strong>, as shown in the following screenshot:</p>
<div><div><img alt="Figure 10.2 – The Docker Desktop version of Airflow containers running" height="455" src="img/Figure_10.02_B19453.jpg" width="1009"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.2 – The Docker Desktop version of Airflow containers running</p>
<h3>Airflow environment variables in docker-compose</h3>
<p>This <a id="_idIndexMarker668"/>section is aimed at users with Airflow <a id="_idIndexMarker669"/>running in a Docker container. If you install it directly on your machine, you can skip it.</p>
<p>We need to configure or change Airflow environment variables to complete most of the recipes in this chapter. This kind of configuration is supposed to be done by editing the <code>airflow.cfg</code> file. However, this can be tricky if you opt to run your Airflow application using <code>docker-compose</code>.</p>
<p>Ideally, we <a id="_idIndexMarker670"/>should be able to access <a id="_idIndexMarker671"/>the <code>airflow.cfg</code> file by mounting a volume in <code>docker-compose.yaml</code>, as follows:</p>
<div><div><img alt="Figure 10.3 – docker-compose.yaml volumes" height="139" src="img/Figure_10.03_B19453.jpg" width="470"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.3 – docker-compose.yaml volumes</p>
<p>Nevertheless, instead of reflecting the file in the local machine, it creates a directory named <code>airflow.cfg</code>. It is a bug known by the community (see <a href="https://github.com/puckel/docker-airflow/issues/571">https://github.com/puckel/docker-airflow/issues/571</a>) with no resolution.</p>
<p>To work around it, we will set all the <code>airflow.cfg</code> configurations in <code>docker-compose.yaml</code> using the environment variables, as shown in the following example:</p>
<pre class="source-code">
# Remote logging configuration
AIRFLOW__LOGGING__REMOTE_LOGGING: "True"</pre>
<p>For users who install and run Airflow directly on their local machine, you can proceed by following the steps that instruct you how to edit the <code>airflow.cfg</code> file.</p>
<h1 id="_idParaDest-361"><a id="_idTextAnchor368"/>Creating basic logs in Airﬂow</h1>
<p>The<a id="_idIndexMarker672"/> internal Airflow logging library is based on the Python built-in logs, which provide flexible and configurable forms to capture and store log messages using different components <a id="_idIndexMarker673"/>of <strong class="bold">directed acyclic graphs</strong> (<strong class="bold">DAGs</strong>). Let’s start this chapter by covering the basic concepts of how Airflow logs work. This knowledge will allow us to apply more advanced concepts and create mature data ingestion pipelines in real-life projects.</p>
<p>In this recipe, we will create a simple DAG to generate logs based on the default configurations of Airflow. We will also understand how Airflow internally sets the logging architecture.</p>
<h2 id="_idParaDest-362"><a id="_idTextAnchor369"/>Getting ready</h2>
<p>Refer<a id="_idIndexMarker674"/> to the <em class="italic">Technical requirements</em> section for this recipe, since we will <a id="_idIndexMarker675"/>handle it with the same technology.</p>
<p>Since we will create a new DAG, let’s create a folder under the <code>dag/</code> directory called <code>basic_logging</code> and a file inside it called <code>basic_logging_dag.py</code> to insert our script. By the end, your folder structure should look like the following:</p>
<div><div><img alt="Figure 10.4 – An Airflow directory with a basic_logging DAG structure" height="273" src="img/Figure_10.04_B19453.jpg" width="305"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.4 – An Airflow directory with a basic_logging DAG structure</p>
<h2 id="_idParaDest-363"><a id="_idTextAnchor370"/>How to do it…</h2>
<p>The goal is to understand how to create logs in Airflow properly so that the DAG script will be pretty straightforward:</p>
<ol>
<li>Let’s start by importing the Airflow and Python libraries:<pre class="source-code">
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime, timedelta
import logging</pre></li>
<li>Then, let’s get the log configuration we want to use:<pre class="source-code">
# Defining the log configuration
logger = logging.getLogger("airflow.task")</pre></li>
<li>Now, let’s <a id="_idIndexMarker676"/>define <code>default_args</code> and the DAG object <a id="_idIndexMarker677"/>which Airflow can create:<pre class="source-code">
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2023, 4, 1),
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}
dag = DAG(
    'basic_logging_dag',
    default_args=default_args,
    description='A simple ETL job using Python and Airflow',
    schedule_interval=timedelta(days=1),
)</pre></li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">Unlike in <a href="B19453_09.xhtml#_idTextAnchor319"><em class="italic">Chapter 9</em></a>, here we will define which tasks belong to this DAG by assigning them to the operator instantiation in <em class="italic">step 5</em> of this recipe.</p>
<ol>
<li value="4">Now, let’s create three example functions only to return log messages. The functions will be named after the ETL steps, as you can see here:<pre class="source-code">
def extract_data():
    logger.info("Let's extract data")
    pass
def transform_data():
    logger.info("Then transform data")
    pass
def load_data():
    logger.info("Finally load data")
    logger.error("Oh, where is the data?")
    pass</pre></li>
</ol>
<p>Feel free to insert more log levels if you want to.</p>
<ol>
<li value="5">For<a id="_idIndexMarker678"/> each function, we will set a task using <code>PythonOperator</code> and the <a id="_idIndexMarker679"/>execution order:<pre class="source-code">
extract_task = PythonOperator(
    task_id='extract_data',
    python_callable=extract_data,
    dag=dag,
)
transform_task = PythonOperator(
    task_id='transform_data',
    python_callable=transform_data,
    dag=dag,
)
load_task = PythonOperator(
    task_id='load_data',
    python_callable=load_data,
    dag=dag,
)
extract_task &gt;&gt; transform_task &gt;&gt; load_task</pre></li>
</ol>
<p>You <a id="_idIndexMarker680"/>can see that we referred the DAG to each task by assigning the <code>dag</code> object (defined in <em class="italic">step 4</em>) to a <code>dag</code> parameter.</p>
<p>Save the<a id="_idIndexMarker681"/> file and go to the Airflow UI.</p>
<ol>
<li value="6">In the Airflow UI, look for the <strong class="bold">basic_logging_dag</strong> DAG and enable it by clicking the toggle button. The job will start right away, and if you check the <strong class="bold">Graph</strong> vision of the DAG, you should see something similar to the following screenshot:</li>
</ol>
<div><div><img alt="Figure 10.5 – The DAG Graph view showing the successful state of the tasks" height="566" src="img/Figure_10.05_B19453.jpg" width="1473"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.5 – The DAG Graph view showing the successful state of the tasks</p>
<p>It means the pipeline ran successfully!</p>
<ol>
<li value="7">Let’s check the <code>logs/</code> directory on our local machine. This directory is at the same level as the <code>DAGs</code> folder, where we put our scripts.</li>
<li>You can see more folders inside if you open the <code>logs/</code> folder. Look for the one beginning with <code>dag_id= basic_logging</code> and open it.</li>
</ol>
<div><div><img alt="Figure 10.6 – The Airflow logs folder for the basic_logging DAG and its tasks" height="132" src="img/Figure_10.06_B19453.jpg" width="499"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.6 – The Airflow logs folder for the basic_logging DAG and its tasks</p>
<ol>
<li value="9">Now, select <a id="_idIndexMarker682"/>the folder named <code>task_id=transform_data</code> and open the log file inside. You should see something like the <a id="_idIndexMarker683"/>following screenshot:</li>
</ol>
<div><div><img alt="Figure 10.7 – Log messages for the transform_data task" height="52" src="img/Figure_10.07_B19453.jpg" width="767"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.7 – Log messages for the transform_data task</p>
<p>As you can see, the logs were printed on the output and even colored accordingly with the log level, where <strong class="bold">INFO</strong> is in green and <strong class="bold">ERROR</strong> is in red.</p>
<h2 id="_idParaDest-364"><a id="_idTextAnchor371"/>How it works…</h2>
<p>This exercise was straightforward, but what if I told you that many developers struggle to understand how Airflow creates its logs? It often happens for two reasons – developers are used to inserting <code>print()</code> functions instead of logging methods and only check the records in the Airflow UI.</p>
<p>Depending on the Airflow configuration, it will not show <code>print()</code> messages on the UI, and messages used to debug or find where the code ran can be lost. Also, the Airflow UI has a limit on the number of record lines to show, and Spark error messages can be easily omitted in this case.</p>
<p>That’s why it is vital to understand that, by default, Airflow stores all its logs under a <code>logs/</code> directory, even organizing it by <code>dag_id</code>, <code>run_id</code>, and each task separately, as we saw in <em class="italic">step 7</em>. This folder structure can also be changed or improved depending on your needs, and all you need to do is alter the <code>log_filename_template</code> variable in <code>airflow.cfg</code>. The following is how it is set by default:</p>
<pre class="source-code">
# Formatting for how airflow generates file names/paths for each task run.
log_filename_template = dag_id={{ ti.dag_id }}/run_id={{ ti.run_id }}/task_id={{ ti.task_id }}/{%% if ti.map_index &gt;= 0 %%}map_index={{ ti.map_index }}/{%% endif %%}attempt={{ try_number }}.log</pre>
<p>Now, looking<a id="_idIndexMarker684"/> inside the log file, you can see that it is the same as what is on the UI, as shown in the following screenshot:</p>
<div><div><img alt="Figure 10.8 – A complete log message stored in a log file found in the local Airflow log folder" height="611" src="img/Figure_10.08_B19453.jpg" width="1544"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.8 – A complete log message stored in a log file found in the local Airflow log folder</p>
<p>In the first lines, it <a id="_idIndexMarker685"/>is possible to see the internal calls Airflow makes to start a task, and even the specific function names, such as <code>taskinstance.py</code> or <code>standard_task_runner.py</code>. Those are all internal scripts. Then, we can see our log messages below in the file.</p>
<p>If you look closely, you can see that the format for our logs is similar to the Airflow core. It happens for two reasons:</p>
<ul>
<li>At the beginning of our code, we used the <code>getLogger()</code> method to retrieve the configuration used by the <code>airflow.task</code> module, as you can see here:<pre class="source-code">
logger = logging.getLogger("airflow.task")</pre></li>
<li><code>airflow.task</code> uses the Airflow default configuration to format all logs, which can also be found inside the <code>airflow.cfg</code> file. Don’t worry about this now; we will cover it later in the <em class="italic">Configuring logs in </em><em class="italic">airflow.cfg</em> recipe.</li>
</ul>
<p>After <a id="_idIndexMarker686"/>defining the <code>logger</code> variable and setting the logging class configurations, the rest of the script is straightforward.</p>
<h2 id="_idParaDest-365"><a id="_idTextAnchor372"/>See also</h2>
<p>You can read more details about Airflow logs on<a id="_idIndexMarker687"/> the Astronomer page here: <a href="https://docs.astronomer.io/learn/logging">https://docs.astronomer.io/learn/logging</a>.</p>
<h1 id="_idParaDest-366"><a id="_idTextAnchor373"/>Storing log files in a remote location</h1>
<p>By default, Airflow<a id="_idIndexMarker688"/> stores and organizes its logs in a local folder with easy access for developers, which facilitates the debugging process when something does not go as expected. However, working with larger projects or teams makes giving everyone access to an Airflow instance or server almost impracticable. Besides looking at the DAG console output, there are other ways to allow access to the logging folder without granting access to Airflow’s server.</p>
<p>One of the most straightforward solutions is to export logs to external storage, such as S3 or <strong class="bold">Google Cloud Storage</strong>. The<a id="_idIndexMarker689"/> good news is that Airflow already has native support to export records to cloud resources.</p>
<p>In this recipe, we will set a configuration in our <code>airflow.cfg</code> file that allows the use of the remote logging feature and test it using an example DAG.</p>
<h2 id="_idParaDest-367"><a id="_idTextAnchor374"/>Getting ready</h2>
<p>Refer to the <em class="italic">Technical requirements </em>section for this recipe.</p>
<h3>AWS S3</h3>
<p>To complete this exercise, it is necessary to <a id="_idIndexMarker690"/>create an <strong class="bold">AWS S3 </strong>bucket. Here are the steps required to accomplish it:</p>
<ol>
<li>Create an AWS account by following the steps here: <a href="https://docs.aws.amazon.com/accounts/latest/reference/manage-acct-creating.xhtml">https://docs.aws.amazon.com/accounts/latest/reference/manage-acct-creating.xhtml</a></li>
<li>Then, proceed to create an S3 bucket, guided by the AWS documentation here: <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/creating-bucket.xhtml">https://docs.aws.amazon.com/AmazonS3/latest/userguide/creating-bucket.xhtml</a></li>
</ol>
<p>In my <a id="_idIndexMarker691"/>case, I will create an S3 bucket called <code>airflow-cookbook</code> for use in this recipe, as you can see in the following screenshot:</p>
<div><div><img alt="Figure 10.9 – The AWS S3 Create bucket page" height="675" src="img/Figure_10.09_B19453.jpg" width="1027"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.9 – The AWS S3 Create bucket page</p>
<h3>Airflow DAG code</h3>
<p>To avoid<a id="_idIndexMarker692"/> redundancy and focus on the goal of this recipe, which is to configure remote logging in Airflow, we will use the same DAG as the <em class="italic">Creating basic logs in Airﬂow</em> recipe. However, feel free to create another DAG with a different name but the same code.</p>
<h2 id="_idParaDest-368"><a id="_idTextAnchor375"/>How to do it…</h2>
<p>Here are the steps to perform this recipe:</p>
<ol>
<li>First, let’s create a programmatic user in our AWS account. Airflow will use this user to authenticate<a id="_idIndexMarker693"/> on AWS and will be able to write the logs. On your AWS console, select <strong class="bold">IAM services</strong>, and you will be redirected to a page similar to this:</li>
</ol>
<div><div><img alt="Figure 10.10 – The AWS IAM main page" height="795" src="img/Figure_10.10_B19453.jpg" width="1295"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.10 – The AWS IAM main page</p>
<ol>
<li value="2">Since this is a test account with a strict purpose, I will ignore the alerts on the IAM dashboard.</li>
<li>Then, select <strong class="bold">Users</strong> and <strong class="bold">Add users</strong>, as shown here:</li>
</ol>
<div><div><img alt="Figure 10.11 – The AWS IAM Users page" height="293" src="img/Figure_10.11_B19453.jpg" width="1221"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.11 – The AWS IAM Users page</p>
<p>On the <strong class="bold">Create user</strong> page, insert a username that is easy to remember, as shown here:</p>
<div><div><img alt="Figure 10.12 – The AWS IAM new user details" height="697" src="img/Figure_10.12_B19453.jpg" width="1268"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.12 – The AWS IAM new user details</p>
<p>Leave the <a id="_idIndexMarker694"/>checkbox unmarked and select <strong class="bold">Next</strong> to add the access policies.</p>
<ol>
<li value="4">On the <strong class="bold">Set permissions</strong> page, select <strong class="bold">Attach policies directly</strong> and then look for <strong class="bold">AmazonS3FullAccess</strong> in the <strong class="bold">Permission </strong><strong class="bold">policies</strong> checkbox:</li>
</ol>
<div><div><img alt="Figure 10.13 – AWS IAM set permissions for user creation" height="901" src="img/Figure_10.13_B19453.jpg" width="1196"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.13 – AWS IAM set permissions for user creation</p>
<p>Since this is a testing exercise, we can use full access to the S3 resource. However, remember to attach specific policies to access the resources in a production environment.</p>
<p>Select <strong class="bold">Next</strong> and then click on the <strong class="bold">Create </strong><strong class="bold">user</strong> button.</p>
<ol>
<li value="5">Now, retrieve <a id="_idIndexMarker695"/>the access key by selecting the user you created, go to <strong class="bold">Security credentials</strong>, and scroll down until you see the <strong class="bold">Access keys</strong> box. Then, create a new one and save the CSV file in an easily accessible place:</li>
</ol>
<div><div><img alt="Figure 10.14 – Access key creation for a user" height="506" src="img/Figure_10.14_B19453.jpg" width="873"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.14 – Access key creation for a user</p>
<ol>
<li value="6">Now, back in Airflow, let’s configure the connection between Airflow and our AWS account.</li>
</ol>
<p>Create a new connection using the Airflow UI, and in the <strong class="bold">Connection Type</strong> field, select <strong class="bold">Amazon S3</strong>. In the <strong class="bold">Extra</strong> field, insert the following line with the credentials retrieved in <em class="italic">step 4</em>:</p>
<pre class="source-code">
{"aws_access_key_id": "your_key", "aws_secret_access_key": "your_secret<strong class="bold">"}</strong></pre>
<p>Your page will look like the following:</p>
<div><div><img alt="Figure 10.15 – The Airflow UI on adding a new AWS S3 connector" height="941" src="img/Figure_10.15_B19453.jpg" width="1282"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.15 – The Airflow UI on adding a new AWS S3 connector</p>
<p>Save it, and open your code editor in your Airflow directory.</p>
<ol>
<li value="7">Now, let’s <a id="_idIndexMarker696"/>add the configurations to our <code>airflow.cfg</code> file. If you are using Docker to host Airflow, add the following lines to your <code>docker-compose.yaml file</code>, under the environment settings:<pre class="source-code">
AIRFLOW__LOGGING__REMOTE_LOGGING: "True"
AIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER: "s3://airflow-cookbook"
AIRFLOW__LOGGING__REMOTE_LOG_CONN_ID: conn_s3
AIRFLOW__LOGGING__ENCRYPT_S3_LOGS: "False"</pre></li>
</ol>
<p>Your <code>docker-compose.yaml</code> file will look similar to this:</p>
<div><div><img alt="Figure 10.16 – Remote logging configuration in docker-compose.yaml" height="589" src="img/Figure_10.16_B19453.jpg" width="940"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.16 – Remote logging configuration in docker-compose.yaml</p>
<p>If you <a id="_idIndexMarker697"/>installed Airflow directly on your local machine, you can instantly change the <code>airflow.cfg</code> file. Change the following lines in <code>airflow.cfg</code> and save it:</p>
<pre class="source-code">
[logging]
# Users must supply a remote location URL (starting with either 's3://...') and an Airflow connection
# id that provides access to the storage location.
remote_logging = True
remote_base_log_folder = s3://airflow-cookbook
remote_log_conn_id = conn_s3
# Use server-side encryption for logs stored in S3
encrypt_s3_logs = False</pre>
<ol>
<li value="8">After the preceding changes, restart your Airflow application.</li>
<li>With your refreshed Airflow, run <code>basic_logging_dag</code> and open your AWS S3. Select the bucket you created in the <em class="italic">Getting ready</em> section, and you should see a new object inside of it, as follows:</li>
</ol>
<div><div><img alt="Figure 10.17 – The AWS S3 airflow-cookbook bucket objects" height="674" src="img/Figure_10.17_B19453.jpg" width="1241"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.17 – The AWS S3 airflow-cookbook bucket objects</p>
<ol>
<li value="10">Then, select <a id="_idIndexMarker698"/>the object created, and you should be able to see more folders related to the tasks executed, as follows:</li>
</ol>
<div><div><img alt="Figure 10.18 – AWS S3 airflow-cookbook showing the remote logs" height="762" src="img/Figure_10.18_B19453.jpg" width="1332"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.18 – AWS S3 airflow-cookbook showing the remote logs</p>
<ol>
<li value="11">Finally, if you select one of the folders, you will see the same file you saw in the <em class="italic">Creating basic logs in Airﬂow</em> recipe. We successfully wrote logs in a remote location!</li>
</ol>
<h2 id="_idParaDest-369"><a id="_idTextAnchor376"/>How it works…</h2>
<p>If you look at this recipe overall, it may seem considerable work. However, remember that we are making a configuration from zero, which generally takes time. Since we are somewhat used to creating an AWS S3 bucket and executing DAGs (see <a href="B19453_02.xhtml#_idTextAnchor064"><em class="italic">Chapter 2</em></a> and <a href="B19453_09.xhtml#_idTextAnchor319"><em class="italic">Chapter 9</em></a>, respectively), let’s focus on setting the remote log configurations.</p>
<p>Our first action <a id="_idIndexMarker699"/>started with creating a connection in Airflow using the access keys generated on AWS. This step is required because, internally, Airflow will use those keys to authenticate in AWS and prove its identity.</p>
<p>Then, we changed the following Airflow configurations as follows:</p>
<pre class="source-code">
AIRFLOW__LOGGING__REMOTE_LOGGING: "True"
AIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER: "s3://airflow-cookbook"
AIRFLOW__LOGGING__REMOTE_LOG_CONN_ID: conn_s3
AIRFLOW__LOGGING__ENCRYPT_S3_LOGS: "False"</pre>
<p>The two first lines are string configurations to set on Airflow whether remote logging is enabled and which bucket path will be used. The last two lines are related to the name of the connection we created on the <code>True</code> if we handle sensitive information.</p>
<p>After restarting Airflow, the configurations will be reflected in our application, and by executing a DAG, we can already see the logs written in the S3 bucket.</p>
<p>As mentioned in the introduction of this recipe, this type of configuration is beneficial not only in big projects but also as a good practice when using Airflow, allowing developers to debug or retrieve information about code output without accessing the cluster or server.</p>
<p>Here, we covered an example using AWS S3, but it is also <a id="_idIndexMarker700"/>possible to use <strong class="bold">Google Cloud Storage</strong> or <strong class="bold">Azure Blog Storage</strong>. You<a id="_idIndexMarker701"/> can read more here: <a href="https://airflow.apache.org/docs/apache-airflow/1.10.13/howto/write-logs.xhtml">https://airflow.apache.org/docs/apache-airflow/1.10.13/howto/write-logs.xhtml</a>.</p>
<p class="callout-heading">Note</p>
<p class="callout">If you don’t want to use remote logging anymore, you can simply remove the environment variables from your <code>docker-compose.yaml</code> or set <code>REMOTE_LOGGING</code> back to <code>False</code>.</p>
<h2 id="_idParaDest-370"><a id="_idTextAnchor377"/>See also</h2>
<p>You can read more about remote logging in S3 on the Apache Airflow official documentation page here: <a href="https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/logging/s3-task-handler.xhtml">https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/logging/s3-task-handler.xhtml</a>.</p>
<h1 id="_idParaDest-371"><a id="_idTextAnchor378"/>Configuring logs in airflow.cfg</h1>
<p>We had <a id="_idIndexMarker702"/>our first contact with the <code>airflow.cfg</code> file in the <em class="italic">Storing log files in a remote location</em> recipe. At a glance, we saw how powerful and handy this configuration file is. There are many ways to customize and improve <a id="_idIndexMarker703"/>Airflow just by editing it.</p>
<p>This exercise will teach how you to enhance your logs by setting applicable configurations in the <code>airflow.cfg</code> file.</p>
<h2 id="_idParaDest-372"><a id="_idTextAnchor379"/>Getting ready</h2>
<p>Refer to the <em class="italic">Technical requirements</em> section for this recipe, since we will handle it with the same technology.</p>
<h3>Airflow DAG code</h3>
<p>To avoid <a id="_idIndexMarker704"/>redundancy and focus on the goal of this recipe, which is to configure remote logging in Airflow, we will use the same DAG as the <em class="italic">Creating basic logs in Airﬂow</em> recipe. However, feel free to create another DAG with a different name but the same code.</p>
<h2 id="_idParaDest-373"><a id="_idTextAnchor380"/>How to do it…</h2>
<p>Since we will use the same DAG code from <em class="italic">Creating basic logs in Airﬂow,</em> let’s jump right to the required configuration to format our logs:</p>
<ol>
<li>Let’s begin by setting the configuration in our <code>docker-compose.yaml</code>. In the environment section, insert the following line and save the file:<pre class="source-code">
AIRFLOW__LOGGING__LOG_FORMAT: "[%(asctime)s] [ %(process)s - %(name)s ] {%(filename)s:%(lineno)d} %(levelname)s - %(message)s"</pre></li>
</ol>
<p>Your <code>docker-compose</code> file<a id="_idIndexMarker705"/> should look like this:</p>
<div><div><img alt="Figure 10.19 – Formatting log configuration in docker-compose.yaml" height="510" src="img/Figure_10.19_B19453.jpg" width="1223"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.19 – Formatting log configuration in docker-compose.yaml</p>
<p>If you<a id="_idIndexMarker706"/> directly edit the <code>airflow.cfg</code> file, search for the <code>log_format</code> variable, and change it to the following line:</p>
<pre class="source-code">
log_format = [%%(asctime)s] [ %%(process)s - %%(name)s ] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s</pre>
<p>Your code will look like this:</p>
<div><div><img alt="Figure 10.20 – log_format inside airflow.cfg" height="94" src="img/Figure_10.20_B19453.jpg" width="1118"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.20 – log_format inside airflow.cfg</p>
<p>Save it, and go to the next step.</p>
<p>We added a few more items in the log line, which we will cover later.</p>
<p class="callout-heading">Note</p>
<p class="callout">Be very attentive here. In the <code>airflow.cfg</code> file, the <code>%</code> character is doubled, unlike in the <code>docker-compose</code> file.</p>
<ol>
<li value="2">Now, let’s <a id="_idIndexMarker707"/>restart Airflow. You can do it by <a id="_idIndexMarker708"/>stopping the Docker container and rerunning it with the following commands:<pre class="source-code">
<strong class="bold">$ docker-compose stop      # Or press Crtl-C</strong>
<strong class="bold">$ docker-compose up</strong></pre></li>
<li>Then, let’s head up to the Airflow UI and run our DAG called <code>basic_logging_dag</code>. On the DAG page, look in the top-right corner and select the play button (depicted by an arrow), followed by <strong class="bold">Trigger DAG</strong>, as follows:</li>
</ol>
<div><div><img alt="Figure 10.21 – basic_logging_dag trigger button on the right side of the page" height="329" src="img/Figure_10.21_B19453.jpg" width="1430"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.21 – basic_logging_dag trigger button on the right side of the page</p>
<p>The DAG will start to run immediately.</p>
<ol>
<li value="4">Now, let’s see the logs generated by one task. I will pick the <code>extract_data</code> task, and the log will look like this:</li>
</ol>
<div><div><img alt="Figure 10.22 – The formatted log output for extract_data task" height="50" src="img/Figure_10.22_B19453.jpg" width="1037"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.22 – The formatted log output for extract_data task</p>
<p>If you look closely, you will see that we now have the process number displayed on the output.</p>
<p class="callout-heading">Note</p>
<p class="callout">If you opt to maintain continuity from the last recipe, <em class="italic">Storing log files in a remote location</em>, remember that your logs are stored in a remote location.</p>
<h2 id="_idParaDest-374"><a id="_idTextAnchor381"/>How it works…</h2>
<p>As we <a id="_idIndexMarker709"/>can see, altering<a id="_idIndexMarker710"/> any logging information is simple, since Airflow uses the Python logging library behind the scenes. Now, let’s take a look at our output:</p>
<div><div><img alt="Figure 10.23 – The formatted log output for the extract_data task" height="50" src="img/Figure_10.23_B19453.jpg" width="1038"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.23 – The formatted log output for the extract_data task</p>
<p>As you can see, before the process name (for example, <code>airflow.task</code>), we also have the number of the running process. It can be helpful information when running multiple processes simultaneously, allowing us to understand which one is taking longer to complete and what is running.</p>
<p>Let’s look at the code we inserted:</p>
<pre class="source-code">
AIRFLOW__LOGGING__LOG_FORMAT: "[%(asctime)s] [ %(process)s - %(name)s ] {%(filename)s:%(lineno)d} %(levelname)s - %(message)s"</pre>
<p>As you can see, variables such as <code>asctime</code>, <code>process</code>, and <code>filename</code> are identical to the ones we saw in <a href="B19453_08.xhtml#_idTextAnchor280"><em class="italic">Chapter 8</em></a>. Also, since a core Python function operates behind the scenes, we can add more information based on the allowed attributes. You can find the list here: <a href="https://docs.python.org/3/library/logging.xhtml#logrecord-attributes">https://docs.python.org/3/library/logging.xhtml#logrecord-attributes</a>.</p>
<h3>Going deeper in airflow.cfg</h3>
<p>Now, let’s go <a id="_idIndexMarker711"/>deeper into Airflow configurations. As you can observe, Airflow resources are orchestrated by the <code>airflow.cfg</code> file. Using a single file, we can determine how to send email notifications (we will cover this in the <em class="italic">Using notiﬁcations operators</em> recipe), when DAGs will reflect a code change, how logs will be displayed, and so on.</p>
<p>It is also<a id="_idIndexMarker712"/> possible to set these configurations by exporting environment variables, and this has priority over the configuration setting on <code>airflow.cfg</code>. This prioritization happens because, internally, Airflow translates the content from <code>airflow.cfg</code> to environment variables, broadly speaking. You can read more here: <a href="https://airflow.apache.org/docs/apache-airflow/stable/cli-and-env-variables-ref.xhtml#environment-variable">https://airflow.apache.org/docs/apache-airflow/stable/cli-and-env-variables-ref.xhtml#environment-variable</a>.</p>
<p>Let’s look at the logging configuration in the Airflow <strong class="bold">REFERENCES</strong> section. We can see many other customization possibilities, such as coloring, a specific format for DAG processors, and extra logs for third-party applications, as shown here:</p>
<div><div><img alt="Figure 10.24 – Airflow documentation for the logging configuration" height="1062" src="img/Figure_10.24_B19453.jpg" width="1268"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.24 – Airflow documentation for the logging configuration</p>
<p>The fantastic part of this documentation is that we have references to configure directly in <code>airflow.cfg</code> or environment variables. You can see the complete reference list here: <a href="https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.xhtml#logging">https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.xhtml#logging</a>.</p>
<p>After we get used to the Airflow dynamics, testing new configurations or formats is straightforward, especially when we have a testing server to do so. However, simultaneously, we<a id="_idIndexMarker713"/> need to be cautious when changing anything internally; otherwise, we can impair our whole application.</p>
<h2 id="_idParaDest-375"><a id="_idTextAnchor382"/>There’s more…</h2>
<p>In <em class="italic">step 1</em>, we mentioned avoiding the use of double <code>%</code> characters when setting the variables in <code>docker-compose</code> – let’s now cover this!</p>
<p>The <code>string</code> variable we pass for <code>docker-compose</code> will be read by an internal Python logging function, which will not recognize the double <code>%</code> pattern. Instead, it will understand the default format for the logs in Airflow needs to be equal to that string variable, and all the DAG logs will look like this:</p>
<div><div><img alt="Figure 10.25 – An error when the environment variable for log_format is not correctly set" height="442" src="img/Figure_10.25_B19453.jpg" width="1229"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.25 – An error when the environment variable for log_format is not correctly set</p>
<p>Now, inside the <code>airflow.cfg</code> file, the double <code>%</code> character is a Bash format pattern that works like a modulo operator.</p>
<h2 id="_idParaDest-376"><a id="_idTextAnchor383"/>See also</h2>
<p>See the whole list of configurations for Airflow here: <a href="https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.xhtml">https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.xhtml</a>.</p>
<h1 id="_idParaDest-377"><a id="_idTextAnchor384"/>Designing advanced monitoring</h1>
<p>After<a id="_idIndexMarker714"/> spending some time learning and practicing logging concepts, we can advance a little more in the subject of monitoring. We can monitor results from all our logging collection work and generate insightful monitoring dashboards and alerts, with the right monitoring message stored.</p>
<p>In this recipe, we will cover the Airflow metrics integrated with StatsD, a platform that collects system statistics, and their purpose to help us achieve a mature pipeline.</p>
<h2 id="_idParaDest-378"><a id="_idTextAnchor385"/>Getting ready</h2>
<p>This exercise will focus on bringing clarity to the Airflow monitoring metrics and how to build a robust architecture to structure it.</p>
<p>As a requirement for this recipe, it is vital to keep in mind the following basic Airflow architecture:</p>
<div><div><img alt="Figure 10.26 – An Airflow high-level architecture diagram" height="574" src="img/Figure_10.26_B19453.jpg" width="713"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.26 – An Airflow high-level architecture diagram</p>
<p>Airflow components, from a<a id="_idIndexMarker715"/> high-level perspective, are composed of the following:</p>
<ul>
<li>A <strong class="bold">web server</strong>, where we can access the Airflow UI.</li>
<li>A relational database to store metadata and other helpful information for use in the DAGs or tasks. To <a id="_idIndexMarker716"/>keep it simple, we will work with just one type of database; however, there can be more than one.</li>
<li>The <strong class="bold">scheduler</strong>, which<a id="_idIndexMarker717"/> will consult the information inside the database to send it to the workers.</li>
<li>A <strong class="bold">Celery</strong> application, responsible<a id="_idIndexMarker718"/> for queueing the requests sent from the scheduler and the workers.</li>
<li>The <strong class="bold">workers</strong>, which<a id="_idIndexMarker719"/> will execute the DAG and tasks.</li>
</ul>
<p>With this in mind, we <a id="_idIndexMarker720"/>can proceed to the next section.</p>
<h2 id="_idParaDest-379"><a id="_idTextAnchor386"/>How to do it…</h2>
<p>Let’s see the main items to design advanced monitoring:</p>
<ul>
<li><strong class="bold">Counters</strong>: As<a id="_idIndexMarker721"/> the name suggests, this metric will provide information about the counts of actions inside Airflow. This metric provides a count of running tasks, failed tasks, and so on. In the following figure, you can see some examples:</li>
</ul>
<div><div><img alt="Figure 10.27 – A list of counter metric examples to monitor Airflow workflows" height="499" src="img/Figure_10.27_B19453.jpg" width="549"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.27 – A list of counter metric examples to monitor Airflow workflows</p>
<ul>
<li><strong class="bold">Timers</strong>: This <a id="_idIndexMarker722"/>metric tells us how long a task or DAG takes to complete or load a file. In the following figure, you can see more:</li>
</ul>
<div><div><img alt="Figure 10.28 – A list of timer examples to monitor Airflow workflows" height="292" src="img/Figure_10.28_B19453.jpg" width="548"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.28 – A list of timer examples to monitor Airflow workflows</p>
<ul>
<li><strong class="bold">Gauges</strong>: Finally, the<a id="_idIndexMarker723"/> last metric type gives us a <a id="_idIndexMarker724"/>more visual overview. Gauges use timers or counters metrics to illustrate whether we are reaching a defined threshold. In the following figure, there are some examples of gauges:</li>
</ul>
<div><div><img alt="Figure 10.29 – A list of gauge examples to be used to monitor Airflow" height="320" src="img/Figure_10.29_B19453.jpg" width="389"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.29 – A list of gauge examples to be used to monitor Airflow</p>
<p>With the metrics defined and on our radar, we can proceed with the architecture design to integrate it.</p>
<ul>
<li><strong class="bold">StatsD</strong>: Now, let’s add <strong class="bold">StatsD</strong> to <a id="_idIndexMarker725"/>the architecture drawing we saw in the <em class="italic">Getting ready</em> section. You will have something like this:</li>
</ul>
<div><div><img alt="Figure 10.30 – StatsD integration and coverage for the Airflow components architecture" height="643" src="img/Figure_10.30_B19453.jpg" width="771"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.30 – StatsD integration and coverage for the Airflow components architecture</p>
<p>StatsD can collect the metrics from all the components inside the dotted rectangle and direct them to a monitoring tool.</p>
<ul>
<li><strong class="bold">Prometheus and Grafana</strong>: Then, we <a id="_idIndexMarker726"/>can plug StatsD into <a id="_idIndexMarker727"/>Prometheus, which serves as one of <a id="_idIndexMarker728"/>Grafana’s data sources. Adding these tools into our architecture will look something like this:</li>
</ul>
<div><div><img alt="Figure 10.31 – A Prometheus and Grafana integration with StatsD and Airflow diagram" height="893" src="img/Figure_10.31_B19453.jpg" width="858"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.31 – A Prometheus and Grafana integration with StatsD and Airflow diagram</p>
<p>Now, let’s understand the components behind this architecture.</p>
<h2 id="_idParaDest-380"><a id="_idTextAnchor387"/>How it works…</h2>
<p>Let’s start <a id="_idIndexMarker729"/>understanding what StatsD is. StatsD is a daemon developed by the Etsy company to aggregate and collect application metrics. Generally, any application can send<a id="_idIndexMarker730"/> metrics using a simple protocol, such as <strong class="bold">User Datagram Protocol</strong> (<strong class="bold">UDP</strong>). With this protocol, the sender doesn’t need to wait for a response from StatsD, making the process simple. After listening and aggregating data for some time, StatsD will send the metrics to output storage, which is Prometheus.</p>
<p>The StatsD integration and installation can be done using the following command:</p>
<pre class="source-code">
pip install 'apache-airflow[statsd]'</pre>
<p>If you want to know more about it, you can refer to the Airflow documentation here: <a href="https://airflow.apache.org/docs/apache-airflow/2.5.1/administration-and-deployment/logging-monitoring/metrics.xhtml#counters">https://airflow.apache.org/docs/apache-airflow/2.5.1/administration-and-deployment/logging-monitoring/metrics.xhtml#counters</a>.</p>
<p>Then, Prometheus and Grafana will gather the metrics and transform them into a more visual resource. You don’t need to worry about this now; we will learn more about it in <a href="B19453_12.xhtml#_idTextAnchor433"><em class="italic">Chapter 12</em></a>.</p>
<p>For <a id="_idIndexMarker731"/>each metric we saw in the three first steps in the <em class="italic">How to do it…</em> section, we can set a threshold to trigger an alert when it has trespassed. All the metrics are presented in the <em class="italic">How to do it…</em> section, and some more can be found here: https://airflow.apache.org/docs/apache-airflow/2.5.1/administration-and-deployment/logging-monitoring/metrics.xhtml#counters.</p>
<h2 id="_idParaDest-381"><a id="_idTextAnchor388"/>There’s more…</h2>
<p>Besides StatsD, there are other tools we can plug into Airflow to track specific metrics or statuses. For example, for a deep error track, we can use <strong class="bold">Sentry</strong>, a<a id="_idIndexMarker732"/> specialized tool used by IT operations teams to provide support and insights. You can learn more about this integration here: <a href="https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/errors.xhtml">https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/errors.xhtml</a>.</p>
<p>On the other hand, if tracking users’ activities is a concern, it is possible to integrate Airflow with Google Analytics. You can learn more here: <a href="https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/tracking-user-activity.xhtml">https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/tracking-user-activity.xhtml</a>.</p>
<h2 id="_idParaDest-382"><a id="_idTextAnchor389"/>See also</h2>
<ul>
<li>Learn more about Airflow architecture here: <a href="https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/logging-architecture.xhtml">https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/logging-architecture.xhtml</a></li>
<li>More information about StatsD is here: <a href="https://www.datadoghq.com/blog/statsd/">https://www.datadoghq.com/blog/statsd/</a></li>
</ul>
<h1 id="_idParaDest-383"><a id="_idTextAnchor390"/>Using notiﬁcation operators</h1>
<p>So far, we <a id="_idIndexMarker733"/>have focused on ensuring that code is well logged and has enough information to provide valid monitoring. Nevertheless, the purpose of having mature and structured pipelines is to avoid the necessity of manual intervention. With busy agendas and other projects, it is hard to constantly look at monitoring dashboards to check whether everything is fine.</p>
<p>Thankfully, Airflow also has native operators to trigger alerts depending on their configured situation. In this recipe, we will configure an email operator to trigger a message every time a pipeline succeeds or fails, allowing us to remediate the problem rapidly.</p>
<h2 id="_idParaDest-384"><a id="_idTextAnchor391"/>Getting ready</h2>
<p>Refer to the <em class="italic">Technical requirements </em>section for this recipe, since we will handle it with the same technology.</p>
<p>In addition to that, you need to create an app password for your Google account. This password will allow our application to authenticate and<a id="_idIndexMarker734"/> use the <strong class="bold">Simple Mail Transfer Protocol</strong> (<strong class="bold">SMTP</strong>) host from Google to trigger emails. You can generate the app password in your Google account at the following link: <a href="https://security.google.com/settings/security/apppasswords">https://security.google.com/settings/security/apppasswords</a>.</p>
<p>Once you access the link, you will be asked to authenticate using your Google credentials, and a new page will appear, similar to the following:</p>
<p class="IMG---Figure"> </p>
<div><div><img alt="Figure 10.32 – The Google app password generation page" height="182" src="img/Figure_10.32_B19453.jpg" width="629"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.32 – The Google app password generation page</p>
<p>In the first box, select <strong class="bold">Mail</strong>, and in the second box, select the device that will use the app password. Since I am using a Macbook, I will select <strong class="bold">Mac</strong>, as shown in the preceding screenshot. Then, click on <strong class="bold">GENERATE</strong>.</p>
<p>A window similar to the following will appear:</p>
<p class="IMG---Figure"> </p>
<div><div><img alt="Figure 10.33 – The Google generated app password pop-up window" height="438" src="img/Figure_10.33_B19453.jpg" width="498"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.33 – The Google generated app password pop-up window</p>
<p>Follow the <a id="_idIndexMarker735"/>steps on the page and save the password in a place you can remember.</p>
<h3>Airflow DAG code</h3>
<p>To avoid redundancy and focus on the goal of this recipe, which is to configure remote logging in<a id="_idIndexMarker736"/> Airflow, we will use the same DAG as the <em class="italic">Creating basic logs in Airﬂow </em>recipe. However, feel free to create another DAG with a different name but the same code.</p>
<p>Nonetheless, you can always find the final code in the GitHub repository here:</p>
<p><a href="https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_10/Using_notifications_operators">https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_10/Using_notifications_operators</a></p>
<h2 id="_idParaDest-385"><a id="_idTextAnchor392"/>How to do it…</h2>
<p>Perform the following steps to try this recipe:</p>
<ol>
<li>Let’s start by configuring the SMTP server in Airflow. Insert the following lines in your <code>docker-compose.yaml</code> file under the environment section:<pre class="source-code">
    # SMTP settings
    AIRFLOW__SMTP__SMTP_HOST: "smtp.gmail.com"
    AIRFLOW__SMTP__SMTP_USER: "your_email_here"
    AIRFLOW__SMTP__SMTP_PASSWORD: "your_app_password_here"
    AIRFLOW__SMTP__SMTP_PORT: 587</pre></li>
</ol>
<p>Your file<a id="_idIndexMarker737"/> should look like this:</p>
<div><div><img alt="Figure 10.34 – docker-compose.yaml with SMTP environment variables" height="597" src="img/Figure_10.34_B19453.jpg" width="969"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.34 – docker-compose.yaml with SMTP environment variables</p>
<p>If you directly <a id="_idIndexMarker738"/>edit the <code>airflow.cfg</code> file, edit the following lines:</p>
<pre class="source-code">
[smtp]
# If you want airflow to send emails on retries, failure, and you want to use
# the airflow.utils.email.send_email_smtp function, you have to configure an
# smtp server here
smtp_host = smtp.gmail.com
smtp_starttls = True
smtp_ssl = False
# Example: smtp_user = airflow
smtp_user = your_email_here
# Example: smtp_password = airflow
smtp_password = your_app_password_here
smtp_port = 587
smtp_mail_from = airflow@example.com
smtp_timeout = 30
smtp_retry_limit = 5</pre>
<p>Don’t forget to restart Airflow after these configurations are saved.</p>
<ol>
<li value="2">Now, let’s<a id="_idIndexMarker739"/> edit our <code>basic_logging_dag</code> DAG to allow it to send emails using <code>EmailOperator</code>. Let’s add to our imports the following line:<pre class="source-code">
from airflow.operators.email import EmailOperator</pre></li>
</ol>
<p>The imports will be organized like this:</p>
<pre class="source-code">
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.operators.email import EmailOperator
from datetime import datetime, timedelta
import logging
# basic_logging_dag DAG code
# ...</pre>
<ol>
<li value="3">In <code>default_args</code>, we will add three new parameters – <code>email</code>, <code>email_on_failure</code>, and <code>email_on_retry</code>. You can see here what it looks like:<pre class="source-code">
# basic_logging_dag DAG imports above this line
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2023, 4, 1),
    'email': ['sample@gmail.com'],
    'email_on_failure': True,
    'email_on_retry': True,
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}
# basic_logging_dag DAG code
# …</pre></li>
</ol>
<p>You don’t <a id="_idIndexMarker740"/>need to worry about these new parameters for now. We will cover them in the <em class="italic">How it </em><em class="italic">works…</em> section.</p>
<ol>
<li value="4">Then, let’s add a new task to our DAG called <code>success_task</code>. If all the other tasks are successful, this one will trigger <code>EmailOperator</code> to alert us. Add the following code to the <code>basic_logging_dag</code> script:<pre class="source-code">
success_task = EmailOperator(
    task_id="success_task",
    to= "g.esppen@gmail.com",
    subject="The pipeline finished successfully!",
    html_content="&lt;h2&gt; Hello World! &lt;/h2&gt;",
    dag=dag
)</pre></li>
<li>Finally, at the end of your script, let’s add the workflow:<pre class="source-code">
extract_task &gt;&gt; transform_task &gt;&gt; load_task &gt;&gt; success_task</pre></li>
</ol>
<p>Don’t forget that you can always check how the final code looks here: <a href="https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_10/Using_noti%EF%AC%81cations_operators">https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_10/Using_noti%EF%AC%81cations_operators</a></p>
<ol>
<li value="6">If you check your <a id="_idIndexMarker741"/>DAG graph, you can see that a new task called <code>success_task</code> appears. It shows our operator is ready to be used. Let’s trigger our DAG by selecting the play button in the top-right corner, as we did in <em class="italic">step 3</em> of the <em class="italic">Configuring logs in </em><em class="italic">airflow.cfg</em> recipe.</li>
</ol>
<p>Your Airflow UI should look like this:</p>
<div><div><img alt="Figure 10.35 – basic_logging_dag showing successful runs for all the tasks" height="444" src="img/Figure_10.35_B19453.jpg" width="1457"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.35 – basic_logging_dag showing successful runs for all the tasks</p>
<ol>
<li value="7">Then, let’s check our email. If everything is well configured, you should see an email similar to the following:</li>
</ol>
<div><div><img alt="Figure 10.36 – An email with a Hello World! Message, indicating that success_task worked" height="376" src="img/Figure_10.36_B19453.jpg" width="991"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.36 – An email with a Hello World! Message, indicating that success_task worked</p>
<p>Our <code>EmailOperator</code> works exactly as expected!</p>
<h2 id="_idParaDest-386"><a id="_idTextAnchor393"/>How it works…</h2>
<p>Let’s start explaining the code by defining what an SMTP server is. An SMTP server is a key component of an email system that enables the transmission of email messages between servers and from clients to servers.</p>
<p>In our case, Google works both as a sender and receiver. We borrow a Gmail host to help send an email from our local machine. However, you don’t need to worry about this when working on a company project; your IT operations team will take care of it.</p>
<p>Now, back to Airflow – once we understand how the SMTP works, its configuration is straightforward. Consulting the reference page for the configurations in Airflow (<a href="https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.xhtml">https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.xhtml</a>), we can see that there is a section dedicated to SMTP, as you can see here:</p>
<div><div><img alt="Figure 10.37 – The Airflow documentation page for the SMTP environment variables" height="493" src="img/Figure_10.37_B19453.jpg" width="884"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.37 – The Airflow documentation page for the SMTP environment variables</p>
<p>Then, all we needed to <a id="_idIndexMarker742"/>do was to set the required parameters to allow the connection between the host (<code>smtp.gmail.com</code>) and Airflow, as you can see here:</p>
<div><div><img alt="Figure 10.38 – A close look at the docker-compose.yaml SMTP settings" height="121" src="img/Figure_10.38_B19453.jpg" width="464"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.38 – A close look at the docker-compose.yaml SMTP settings</p>
<p>Once this step is completed, we will go to our DAG and declare <code>EmailOperator</code>, as shown in the following code:</p>
<pre class="source-code">
success_task = EmailOperator(
    task_id="success_task",
    to="g.esppen@gmail.com",
    subject="The pipeline finished successfully!",
    html_content="&lt;h2&gt; Hello World! &lt;/h2&gt;",
    dag=dag
)</pre>
<p>The parameters of the email are very intuitive and can be set accordingly to whatever is needed. If we delve deeper, we can see that there are plenty of possibilities to make those fields’ values more abstract to adapt to different function results.</p>
<p>It is also <a id="_idIndexMarker743"/>possible to use a formatted email template in <code>html_content</code> and even attach a complete error or log message. You can see more of the allowed parameters here: <a href="https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/email/index.xhtml">https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/email/index.xhtml</a>.</p>
<p>In our case, this operator was triggered when all tasks successfully ran. But what about if there is an error? Let’s go back to <em class="italic">step 3</em> and see <code>default_args</code>:</p>
<pre class="source-code">
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2023, 4, 1),
    'email': ['sample@gmail.com'],
    'email_on_failure': True,
    'email_on_retry': True,
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}</pre>
<p>The two new parameters added (<code>email_on_failure</code> and <code>email_on_retry</code>) address scenarios where the DAG failed or retries a task. The values inside the <code>email</code> parameter list <a id="_idIndexMarker744"/>are the recipients of these emails.</p>
<p>A default email triggered by an error message looks like this:</p>
<div><div><img alt="Figure 10.39 – The Airflow default email for error in a task instance" height="251" src="img/Figure_10.39_B19453.jpg" width="715"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.39 – The Airflow default email for error in a task instance</p>
<h2 id="_idParaDest-387"><a id="_idTextAnchor394"/>There’s more…</h2>
<p>The Airflow notification system is not limited to sending emails and counts, offering useful integrations with Slack, Teams, and Telegram.</p>
<p>TowardsDataScience has a fantastic blog post about how to integrate Airflow with Slack, and you can find it here: <a href="https://towardsdatascience.com/automated-alerts-for-airflow-with-slack-5c6ec766a823">https://towardsdatascience.com/automated-alerts-for-airflow-with-slack-5c6ec766a823</a>.</p>
<p>Not limited to corporate tools, Airflow also has a Discord hook: <a href="https://airflow.apache.org/docs/apache-airflow-providers-discord/stable/_api/airflow/providers/discord/hooks/discord_webhook/index.xhtml">https://airflow.apache.org/docs/apache-airflow-providers-discord/stable/_api/airflow/providers/discord/hooks/discord_webhook/index.xhtml</a>.</p>
<p>The best advice I can give is always to look at Airflow community documentation. As an open source and active platform, there is always a new implementation to help automate and make our daily work easier.</p>
<h1 id="_idParaDest-388"><a id="_idTextAnchor395"/>Using SQL operators for data quality</h1>
<p>Good <strong class="bold">data quality</strong> is <a id="_idIndexMarker745"/>crucial for an organization to <a id="_idIndexMarker746"/>ensure the effectiveness of its data systems. By performing quality checks within the DAG, it is possible to stop pipelines and notify stakeholders before erroneous data is introduced into a production lake or warehouse.</p>
<p>Although plenty of<a id="_idIndexMarker747"/> available tools in the market provide <strong class="bold">data quality checks</strong>, one of the most popular ways to do this is by running SQL queries. As you may have already guessed, Airflow has providers to support those operations.</p>
<p>This recipe will cover the data quality principal topics in the data ingestion process, pointing out the best <code>SQLOperator</code> type to run in those situations.</p>
<h2 id="_idParaDest-389"><a id="_idTextAnchor396"/>Getting ready</h2>
<p>Before starting our exercise, let’s create a<a id="_idIndexMarker748"/> simple <code>customers</code> table. You can see here how it looks:</p>
<div><div><img alt="Figure 10.40 – An example of customers table columns" height="359" src="img/Figure_10.40_B19453.jpg" width="157"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.40 – An example of customers table columns</p>
<p>And the <a id="_idIndexMarker749"/>same table is represented with its schema:</p>
<pre class="source-code">
CREATE TABLE customers (
    customer_id INT PRIMARY KEY,
    first_name VARCHAR(50),
    last_name VARCHAR(50),
    email VARCHAR(100),
    phone_number VARCHAR(20),
    address VARCHAR(200),
    city VARCHAR(50),
    state VARCHAR(50),
    country VARCHAR(50),
    zip_code VARCHAR(20)
);</pre>
<p>You don’t need <a id="_idIndexMarker750"/>to worry about creating this table in a SQL database. This exercise will focus on the data quality factors to be checked, using this table as an example.</p>
<h2 id="_idParaDest-390"><a id="_idTextAnchor397"/>How to do it…</h2>
<p>Here are <a id="_idIndexMarker751"/>the steps to perform this recipe:</p>
<ol>
<li>Let’s start by defining the essential data quality checks that apply as follows:</li>
</ol>
<div><div><img alt="Figure 10.41 – Data quality essential points" height="336" src="img/Figure_10.41_B19453.jpg" width="609"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.41 – Data quality essential points</p>
<ol>
<li value="2">Let’s imagine implementing it using <code>SQLColumnCheckOperator</code>, integrated and installed in our Airflow platform. Let’s now create a simple task to check whether our table has unique IDs and whether all customers have <code>first_name</code>. Our example code looks like this:<pre class="source-code">
id_username_check = SQLColumnCheckOperator(
        task_id="id_username_check",
        conn_id= my_conn,
        table=my_table,
        column_mapping={
            "customer_id": {
                "null_check": {
                    "equal_to": 0,
                    "tolerance": 0,
                },
                "distinct_check": {
                    "equal_to": 1,
                },
            },
            "first_name": {
                "null_check": {"equal_to": 0},
            },
        }
)</pre></li>
<li>Now, let’s<a id="_idIndexMarker752"/> validate whether we ingest the <a id="_idIndexMarker753"/>required count of rows using <code>SQLTableCheckOperator</code>, as follows:<pre class="source-code">
customer_table_rows_count = SQLTableCheckOperator(
    task_id="customer_table_rows_count",
    conn_id= my_conn,
    table=my_table,
    checks={"row_count_check": {
                "check_statement": "COUNT(*) &gt;= 1000"
            }
        }
)</pre></li>
<li>Finally, let’s <a id="_idIndexMarker754"/>ensure the customers in our database<a id="_idIndexMarker755"/> have at least one order. Our example code looks like this:<pre class="source-code">
count_orders_check = SQLColumnCheckOperator(
    task_id="check_columns",
    conn_id=my-conn,
    table=my_table,
    column_mapping={
        "MY_NUM_COL": {
            "min": {"geq_to ": 1}
        }
    }
)</pre></li>
</ol>
<p>The <code>geq_to</code> key stands for <strong class="bold">great or </strong><strong class="bold">equal to</strong>.</p>
<h2 id="_idParaDest-391"><a id="_idTextAnchor398"/>How it works…</h2>
<p>Data quality is a <a id="_idIndexMarker756"/>complex topic encompassing many variables, such as the project or company context, business models, and <strong class="bold">Service Level Agreements</strong> (<strong class="bold">SLAs</strong>) between<a id="_idIndexMarker757"/> teams. Based on this, the goal of this recipe was to offer the core concept of data quality and demonstrate how to first approach using Airflow SQLOperators.</p>
<p>Let’s start with the essential topics in <em class="italic">step 1</em>, as follows:</p>
<div><div><img alt="Figure 10.42 – Data quality essential points" height="336" src="img/Figure_10.42_B19453.jpg" width="609"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.42 – Data quality essential points</p>
<p>In a generic <a id="_idIndexMarker758"/>scenario, those items are the principal topics to be approached and implemented. They will guarantee the minimum data reliability, based on whether the columns are the ones we expected, creating an average value for the row count, ensuring the IDs are unique, and having control of the <code>null</code> and distinct values in specific columns.</p>
<p>Using Airflow, we used the SQL approach to check data. As mentioned at the beginning of this recipe, SQL checks are widespread and popular due to their simplicity and flexibility. Unfortunately, to simulate a scenario like this, we would be required to set up a hard-working local infrastructure, and the best we could come up with is simulating the tasks in Airflow.</p>
<p>Here, we used two <code>SQLOperator</code> subtypes – <code>SQLColumnCheckOperator</code> and <code>SQLTableCheckOperator</code>. As the name suggests, the first operator is more focused on verifying the column’s content by checking whether there are null or distinct values. In the case of <code>customer_id</code>, we verified both scenarios and only null values for <code>first_name</code>, as you can see here:</p>
<pre class="source-code">
column_mapping={
            "customer_id": {
                "null_check": {
                    "equal_to": 0,
                    "tolerance": 0,
                },
                "distinct_check": {
                    "equal_to": 1,
                },
            },
            "first_name": {
                "null_check": {"equal_to": 0},
            },
        }</pre>
<p><code>SQLTableCheckOperator</code> will <a id="_idIndexMarker759"/>perform validations across <a id="_idIndexMarker760"/>the whole table. It allows the insertion of a SQL query to make counts or other operations, as we did to validate the expected number of rows in <em class="italic">step 3</em>, as you can see in the piece of code here:</p>
<pre class="source-code">
<strong class="bold">    checks={"row_count_check": {</strong>
<strong class="bold">                "check_statement": "COUNT(*) &gt;= 1000"</strong>
<strong class="bold">            }</strong>
<strong class="bold">        }</strong></pre>
<p>However, <code>SQLOperator</code> is not limited to these two. In the Airflow documentation, you can see other examples and the complete list of accepted parameters for these functions: <a href="https://airflow.apache.org/docs/apache-airflow/2.1.4/_api/airflow/operators/sql/index.xhtml#module-airflow.operators.sql">https://airflow.apache.org/docs/apache-airflow/2.1.4/_api/airflow/operators/sql/index.xhtml#module-airflow.operators.sql</a>.</p>
<p>A fantastic<a id="_idIndexMarker761"/> operator to check out is <code>SQLIntervalCheckOperator</code>, used to validate historical data and ensure the stored information is concise.</p>
<p>In your data career, you <a id="_idIndexMarker762"/>will see that data quality is a daily topic and concern among teams. The best advice here is to continually search for tools and methods to improve this methodology.</p>
<h2 id="_idParaDest-392"><a id="_idTextAnchor399"/>There’s more…</h2>
<p>We can use additional tools to enhance our data quality checks. One of the recommended tools for this use<a id="_idIndexMarker763"/> is <strong class="bold">GreatExpectations</strong>, an open source platform made in<a id="_idIndexMarker764"/> Python with plenty of integrations, with resources such as<a id="_idIndexMarker765"/> Airflow, <strong class="bold">AWS S3</strong>, and <strong class="bold">Databricks</strong>.</p>
<p>Although it is <a id="_idIndexMarker766"/>a platform you can install in any cluster, <strong class="bold">GreatExpectations</strong> is expanding toward a managed cloud version. You can check more about it on the <a id="_idIndexMarker767"/>official page here: <a href="https://greatexpectations.io/integrations">https://greatexpectations.io/integrations</a>.</p>
<h2 id="_idParaDest-393"><a id="_idTextAnchor400"/>See also</h2>
<ul>
<li><em class="italic">Yu Ishikawa</em> has a nice blog post about other checks you can do using SQL in Airflow: <a href="https://yu-ishikawa.medium.com/apache-airflow-as-a-data-quality-checker-416ca7f5a3ad">https://yu-ishikawa.medium.com/apache-airflow-as-a-data-quality-checker-416ca7f5a3ad</a></li>
<li>More information about data quality in Airflow is available here: <a href="https://docs.astronomer.io/learn/data-quality">https://docs.astronomer.io/learn/data-quality</a></li>
</ul>
<h1 id="_idParaDest-394"><a id="_idTextAnchor401"/>Further reading</h1>
<ul>
<li><a href="https://www.oak-tree.tech/blog/airflow-remote-logging-s3">https://www.oak-tree.tech/blog/airflow-remote-logging-s3</a></li>
<li><a href="https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/connections/aws.xhtml#examples">https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/connections/aws.xhtml#examples</a></li>
<li><a href="https://airflow.apache.org/docs/apache-airflow/stable/howto/email-config.xhtml">https://airflow.apache.org/docs/apache-airflow/stable/howto/email-config.xhtml</a></li>
<li>https://docs.astronomer.io/learn/logging</li>
<li>https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/metrics.xhtml#setup</li>
<li>https://hevodata.com/learn/airflow-monitoring/#aam</li>
<li>https://servian.dev/developing-5-step-data-quality-framework-with-apache-airflow-972488ddb65f</li>
</ul>
</div>
</div></body></html>