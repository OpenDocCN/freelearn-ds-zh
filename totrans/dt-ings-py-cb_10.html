<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer301">
<h1 class="chapter-number" id="_idParaDest-357"><a id="_idTextAnchor364"/>10</h1>
<h1 id="_idParaDest-358"><a id="_idTextAnchor365"/>Logging and Monitoring Your Data Ingest in Airﬂow</h1>
<p>We already know how vital logging and monitoring are to manage applications and systems, and Airflow is no different. In fact, <strong class="bold">Apache Airflow</strong> already has built-in modules to create logs and export them. But what about <span class="No-Break">improving them?</span></p>
<p>In the previous chapter, <em class="italic">Putting Everything Together with Airﬂow</em>, we covered the fundamental aspects of Airflow, how to start our data ingestion, and how to orchestrate a pipeline and use the best data development practices. Now, let’s put into practice the best techniques to enhance logging and monitor <span class="No-Break">Airflow pipelines.</span></p>
<p>In this chapter, you will learn the <span class="No-Break">following recipes:</span></p>
<ul>
<li>Creating basic logs <span class="No-Break">in Airﬂow</span></li>
<li>Storing log files in a <span class="No-Break">remote location</span></li>
<li>Configuring logs <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">airflow.cfg</strong></span></li>
<li>Designing <span class="No-Break">advanced monitoring</span></li>
<li>Using <span class="No-Break">notiﬁcation operators</span></li>
<li>Using SQL operators for <span class="No-Break">data quality</span></li>
</ul>
<h1 id="_idParaDest-359"><a id="_idTextAnchor366"/>Technical requirements</h1>
<p>You can find the code for this chapter in the GitHub repository <span class="No-Break">here: </span><a href="https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook"><span class="No-Break">https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook</span></a><span class="No-Break">.</span></p>
<h2 id="_idParaDest-360"><a id="_idTextAnchor367"/>Installing and running Airflow</h2>
<p>This <a id="_idIndexMarker665"/>chapter requires that Airflow is installed on your local machine. You can install it directly on your <strong class="bold">operating system</strong> (<strong class="bold">OS</strong>) or by using a Docker image. For more information, refer to the <em class="italic">Configuring Docker for Airflow</em> recipe in <a href="B19453_01.xhtml#_idTextAnchor022"><span class="No-Break"><em class="italic">Chapter 1</em></span></a><span class="No-Break">.</span></p>
<p>After following the steps described in <a href="B19453_01.xhtml#_idTextAnchor022"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>, ensure your Airflow runs correctly. You can do that by checking the Airflow UI <span class="No-Break">here: </span><span class="No-Break"><strong class="source-inline">http://localhost:8080</strong></span><span class="No-Break">.</span></p>
<p>If you are using a <a id="_idIndexMarker666"/>Docker container (as I am) to host your Airflow application, you can check its status on the terminal by running the <span class="No-Break">following command:</span></p>
<pre class="source-code">
$ docker ps</pre>
<p>You can see the command <span class="No-Break">running here:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer259">
<img alt="Figure 10.1 – Airflow containers running" height="131" src="image/Figure_9.01_B19453.jpg" width="1407"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.1 – Airflow containers running</p>
<p>For Docker, check the container status<a id="_idIndexMarker667"/> on <strong class="bold">Docker Desktop</strong>, as shown in the <span class="No-Break">following screenshot:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer260">
<img alt="Figure 10.2 – The Docker Desktop version of Airflow containers running" height="455" src="image/Figure_10.02_B19453.jpg" width="1009"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.2 – The Docker Desktop version of Airflow containers running</p>
<h3>Airflow environment variables in docker-compose</h3>
<p>This <a id="_idIndexMarker668"/>section is aimed at users with Airflow <a id="_idIndexMarker669"/>running in a Docker container. If you install it directly on your machine, you can <span class="No-Break">skip it.</span></p>
<p>We need to configure or change Airflow environment variables to complete most of the recipes in this chapter. This kind of configuration is supposed to be done by editing the <strong class="source-inline">airflow.cfg</strong> file. However, this can be tricky if you opt to run your Airflow application <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">docker-compose</strong></span><span class="No-Break">.</span></p>
<p>Ideally, we <a id="_idIndexMarker670"/>should be able to access <a id="_idIndexMarker671"/>the <strong class="source-inline">airflow.cfg</strong> file by mounting a volume in <strong class="source-inline">docker-compose.yaml</strong>, <span class="No-Break">as follows:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer261">
<img alt="Figure 10.3 – docker-compose.yaml volumes" height="139" src="image/Figure_10.03_B19453.jpg" width="470"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.3 – docker-compose.yaml volumes</p>
<p>Nevertheless, instead of reflecting the file in the local machine, it creates a directory named <strong class="source-inline">airflow.cfg</strong>. It is a bug known by the community (see <a href="https://github.com/puckel/docker-airflow/issues/571">https://github.com/puckel/docker-airflow/issues/571</a>) with <span class="No-Break">no resolution.</span></p>
<p>To work around it, we will set all the <strong class="source-inline">airflow.cfg</strong> configurations in <strong class="source-inline">docker-compose.yaml</strong> using the environment variables, as shown in the <span class="No-Break">following example:</span></p>
<pre class="source-code">
# Remote logging configuration
AIRFLOW__LOGGING__REMOTE_LOGGING: "True"</pre>
<p>For users who install and run Airflow directly on their local machine, you can proceed by following the steps that instruct you how to edit the <span class="No-Break"><strong class="source-inline">airflow.cfg</strong></span><span class="No-Break"> file.</span></p>
<h1 id="_idParaDest-361"><a id="_idTextAnchor368"/>Creating basic logs in Airﬂow</h1>
<p>The<a id="_idIndexMarker672"/> internal Airflow logging library is based on the Python built-in logs, which provide flexible and configurable forms to capture and store log messages using different components <a id="_idIndexMarker673"/>of <strong class="bold">directed acyclic graphs</strong> (<strong class="bold">DAGs</strong>). Let’s start this chapter by covering the basic concepts of how Airflow logs work. This knowledge will allow us to apply more advanced concepts and create mature data ingestion pipelines in <span class="No-Break">real-life projects.</span></p>
<p>In this recipe, we will create a simple DAG to generate logs based on the default configurations of Airflow. We will also understand how Airflow internally sets the <span class="No-Break">logging architecture.</span></p>
<h2 id="_idParaDest-362"><a id="_idTextAnchor369"/>Getting ready</h2>
<p>Refer<a id="_idIndexMarker674"/> to the <em class="italic">Technical requirements</em> section for this recipe, since we will <a id="_idIndexMarker675"/>handle it with the <span class="No-Break">same technology.</span></p>
<p>Since we will create a new DAG, let’s create a folder under the <strong class="source-inline">dag/</strong> directory called <strong class="source-inline">basic_logging</strong> and a file inside it called <strong class="source-inline">basic_logging_dag.py</strong> to insert our script. By the end, your folder structure should look like <span class="No-Break">the following:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer262">
<img alt="Figure 10.4 – An Airflow directory with a basic_logging DAG structure" height="273" src="image/Figure_10.04_B19453.jpg" width="305"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.4 – An Airflow directory with a basic_logging DAG structure</p>
<h2 id="_idParaDest-363"><a id="_idTextAnchor370"/>How to do it…</h2>
<p>The goal is to understand how to create logs in Airflow properly so that the DAG script will be <span class="No-Break">pretty straightforward:</span></p>
<ol>
<li>Let’s start by importing the Airflow and <span class="No-Break">Python libraries:</span><pre class="source-code">
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime, timedelta
import logging</pre></li>
<li>Then, let’s get the log configuration we want <span class="No-Break">to use:</span><pre class="source-code">
# Defining the log configuration
logger = logging.getLogger("airflow.task")</pre></li>
<li>Now, let’s <a id="_idIndexMarker676"/>define <strong class="source-inline">default_args</strong> and the DAG object <a id="_idIndexMarker677"/>which Airflow <span class="No-Break">can create:</span><pre class="source-code">
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2023, 4, 1),
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}
dag = DAG(
    'basic_logging_dag',
    default_args=default_args,
    description='A simple ETL job using Python and Airflow',
    schedule_interval=timedelta(days=1),
)</pre></li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">Unlike in <a href="B19453_09.xhtml#_idTextAnchor319"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>, here we will define which tasks belong to this DAG by assigning them to the operator instantiation in <em class="italic">step 5</em> of <span class="No-Break">this recipe.</span></p>
<ol>
<li value="4">Now, let’s create three example functions only to return log messages. The functions will be named after the ETL steps, as you can <span class="No-Break">see here:</span><pre class="source-code">
def extract_data():
    logger.info("Let's extract data")
    pass
def transform_data():
    logger.info("Then transform data")
    pass
def load_data():
    logger.info("Finally load data")
    logger.error("Oh, where is the data?")
    pass</pre></li>
</ol>
<p>Feel free to insert more log levels if you <span class="No-Break">want to.</span></p>
<ol>
<li value="5">For<a id="_idIndexMarker678"/> each function, we will set a task using <strong class="source-inline">PythonOperator</strong> and the <a id="_idIndexMarker679"/><span class="No-Break">execution order:</span><pre class="source-code">
extract_task = PythonOperator(
    task_id='extract_data',
    python_callable=extract_data,
    dag=dag,
)
transform_task = PythonOperator(
    task_id='transform_data',
    python_callable=transform_data,
    dag=dag,
)
load_task = PythonOperator(
    task_id='load_data',
    python_callable=load_data,
    dag=dag,
)
extract_task &gt;&gt; transform_task &gt;&gt; load_task</pre></li>
</ol>
<p>You <a id="_idIndexMarker680"/>can see that we referred the DAG to each task by assigning the <strong class="source-inline">dag</strong> object (defined in <em class="italic">step 4</em>) to a <span class="No-Break"><strong class="source-inline">dag</strong></span><span class="No-Break"> parameter.</span></p>
<p>Save the<a id="_idIndexMarker681"/> file and go to the <span class="No-Break">Airflow UI.</span></p>
<ol>
<li value="6">In the Airflow UI, look for the <strong class="bold">basic_logging_dag</strong> DAG and enable it by clicking the toggle button. The job will start right away, and if you check the <strong class="bold">Graph</strong> vision of the DAG, you should see something similar to the <span class="No-Break">following screenshot:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer263">
<img alt="Figure 10.5 – The DAG Graph view showing the successful state of the tasks" height="566" src="image/Figure_10.05_B19453.jpg" width="1473"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.5 – The DAG Graph view showing the successful state of the tasks</p>
<p>It means the pipeline <span class="No-Break">ran successfully!</span></p>
<ol>
<li value="7">Let’s check the <strong class="source-inline">logs/</strong> directory on our local machine. This directory is at the same level as the <strong class="source-inline">DAGs</strong> folder, where we put <span class="No-Break">our scripts.</span></li>
<li>You can see more folders inside if you open the <strong class="source-inline">logs/</strong> folder. Look for the one beginning with <strong class="source-inline">dag_id= basic_logging</strong> and <span class="No-Break">open it.</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer264">
<img alt="Figure 10.6 – The Airflow logs folder for the basic_logging DAG and its tasks" height="132" src="image/Figure_10.06_B19453.jpg" width="499"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.6 – The Airflow logs folder for the basic_logging DAG and its tasks</p>
<ol>
<li value="9">Now, select <a id="_idIndexMarker682"/>the folder named <strong class="source-inline">task_id=transform_data</strong> and open the log file inside. You should see something like the <a id="_idIndexMarker683"/><span class="No-Break">following screenshot:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer265">
<img alt="Figure 10.7 – Log messages for the transform_data task" height="52" src="image/Figure_10.07_B19453.jpg" width="767"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.7 – Log messages for the transform_data task</p>
<p>As you can see, the logs were printed on the output and even colored accordingly with the log level, where <strong class="bold">INFO</strong> is in green and <strong class="bold">ERROR</strong> is <span class="No-Break">in red.</span></p>
<h2 id="_idParaDest-364"><a id="_idTextAnchor371"/>How it works…</h2>
<p>This exercise was straightforward, but what if I told you that many developers struggle to understand how Airflow creates its logs? It often happens for two reasons – developers are used to inserting <strong class="source-inline">print()</strong> functions instead of logging methods and only check the records in the <span class="No-Break">Airflow UI.</span></p>
<p>Depending on the Airflow configuration, it will not show <strong class="source-inline">print()</strong> messages on the UI, and messages used to debug or find where the code ran can be lost. Also, the Airflow UI has a limit on the number of record lines to show, and Spark error messages can be easily omitted in <span class="No-Break">this case.</span></p>
<p>That’s why it is vital to understand that, by default, Airflow stores all its logs under a <strong class="source-inline">logs/</strong> directory, even organizing it by <strong class="source-inline">dag_id</strong>, <strong class="source-inline">run_id</strong>, and each task separately, as we saw in <em class="italic">step 7</em>. This folder structure can also be changed or improved depending on your needs, and all you need to do is alter the <strong class="source-inline">log_filename_template</strong> variable in <strong class="source-inline">airflow.cfg</strong>. The following is how it is set <span class="No-Break">by default:</span></p>
<pre class="source-code">
# Formatting for how airflow generates file names/paths for each task run.
log_filename_template = dag_id={{ ti.dag_id }}/run_id={{ ti.run_id }}/task_id={{ ti.task_id }}/{%% if ti.map_index &gt;= 0 %%}map_index={{ ti.map_index }}/{%% endif %%}attempt={{ try_number }}.log</pre>
<p>Now, looking<a id="_idIndexMarker684"/> inside the log file, you can see that it is the same as what is on the UI, as shown in the <span class="No-Break">following screenshot:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer266">
<img alt="Figure 10.8 – A complete log message stored in a log file found in the local Airflow log folder" height="611" src="image/Figure_10.08_B19453.jpg" width="1544"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.8 – A complete log message stored in a log file found in the local Airflow log folder</p>
<p>In the first lines, it <a id="_idIndexMarker685"/>is possible to see the internal calls Airflow makes to start a task, and even the specific function names, such as <strong class="source-inline">taskinstance.py</strong> or <strong class="source-inline">standard_task_runner.py</strong>. Those are all internal scripts. Then, we can see our log messages below in <span class="No-Break">the file.</span></p>
<p>If you look closely, you can see that the format for our logs is similar to the Airflow core. It happens for <span class="No-Break">two reasons:</span></p>
<ul>
<li>At the beginning of our code, we used the <strong class="source-inline">getLogger()</strong> method to retrieve the configuration used by the <strong class="source-inline">airflow.task</strong> module, as you can <span class="No-Break">see here:</span><pre class="source-code">
logger = logging.getLogger("airflow.task")</pre></li>
<li><strong class="source-inline">airflow.task</strong> uses the Airflow default configuration to format all logs, which can also be found inside the <strong class="source-inline">airflow.cfg</strong> file. Don’t worry about this now; we will cover it later in the <em class="italic">Configuring logs in </em><span class="No-Break"><em class="italic">airflow.cfg</em></span><span class="No-Break"> recipe.</span></li>
</ul>
<p>After <a id="_idIndexMarker686"/>defining the <strong class="source-inline">logger</strong> variable and setting the logging class configurations, the rest of the script <span class="No-Break">is straightforward.</span></p>
<h2 id="_idParaDest-365"><a id="_idTextAnchor372"/>See also</h2>
<p>You can read more details about Airflow logs on<a id="_idIndexMarker687"/> the Astronomer page <span class="No-Break">here: </span><a href="https://docs.astronomer.io/learn/logging"><span class="No-Break">https://docs.astronomer.io/learn/logging</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-366"><a id="_idTextAnchor373"/>Storing log files in a remote location</h1>
<p>By default, Airflow<a id="_idIndexMarker688"/> stores and organizes its logs in a local folder with easy access for developers, which facilitates the debugging process when something does not go as expected. However, working with larger projects or teams makes giving everyone access to an Airflow instance or server almost impracticable. Besides looking at the DAG console output, there are other ways to allow access to the logging folder without granting access to <span class="No-Break">Airflow’s server.</span></p>
<p>One of the most straightforward solutions is to export logs to external storage, such as S3 or <strong class="bold">Google Cloud Storage</strong>. The<a id="_idIndexMarker689"/> good news is that Airflow already has native support to export records to <span class="No-Break">cloud resources.</span></p>
<p>In this recipe, we will set a configuration in our <strong class="source-inline">airflow.cfg</strong> file that allows the use of the remote logging feature and test it using an <span class="No-Break">example DAG.</span></p>
<h2 id="_idParaDest-367"><a id="_idTextAnchor374"/>Getting ready</h2>
<p>Refer to the <em class="italic">Technical requirements </em>section for <span class="No-Break">this recipe.</span></p>
<h3>AWS S3</h3>
<p>To complete this exercise, it is necessary to <a id="_idIndexMarker690"/>create an <strong class="bold">AWS S3 </strong>bucket. Here are the steps required to <span class="No-Break">accomplish it:</span></p>
<ol>
<li>Create an AWS account by following the steps <span class="No-Break">here: </span><a href="https://docs.aws.amazon.com/accounts/latest/reference/manage-acct-creating.xhtml"><span class="No-Break">https://docs.aws.amazon.com/accounts/latest/reference/manage-acct-creating.xhtml</span></a></li>
<li>Then, proceed to create an S3 bucket, guided by the AWS documentation <span class="No-Break">here: </span><a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/creating-bucket.xhtml"><span class="No-Break">https://docs.aws.amazon.com/AmazonS3/latest/userguide/creating-bucket.xhtml</span></a></li>
</ol>
<p>In my <a id="_idIndexMarker691"/>case, I will create an S3 bucket called <strong class="source-inline">airflow-cookbook</strong> for use in this recipe, as you can see in the <span class="No-Break">following screenshot:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer267">
<img alt="Figure 10.9 – The AWS S3 Create bucket page" height="675" src="image/Figure_10.09_B19453.jpg" width="1027"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.9 – The AWS S3 Create bucket page</p>
<h3>Airflow DAG code</h3>
<p>To avoid<a id="_idIndexMarker692"/> redundancy and focus on the goal of this recipe, which is to configure remote logging in Airflow, we will use the same DAG as the <em class="italic">Creating basic logs in Airﬂow</em> recipe. However, feel free to create another DAG with a different name but the <span class="No-Break">same code.</span></p>
<h2 id="_idParaDest-368"><a id="_idTextAnchor375"/>How to do it…</h2>
<p>Here are the steps to perform <span class="No-Break">this recipe:</span></p>
<ol>
<li>First, let’s create a programmatic user in our AWS account. Airflow will use this user to authenticate<a id="_idIndexMarker693"/> on AWS and will be able to write the logs. On your AWS console, select <strong class="bold">IAM services</strong>, and you will be redirected to a page similar <span class="No-Break">to this:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer268">
<img alt="Figure 10.10 – The AWS IAM main page" height="795" src="image/Figure_10.10_B19453.jpg" width="1295"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.10 – The AWS IAM main page</p>
<ol>
<li value="2">Since this is a test account with a strict purpose, I will ignore the alerts on the <span class="No-Break">IAM dashboard.</span></li>
<li>Then, select <strong class="bold">Users</strong> and <strong class="bold">Add users</strong>, as <span class="No-Break">shown here:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer269">
<img alt="Figure 10.11 – The AWS IAM Users page" height="293" src="image/Figure_10.11_B19453.jpg" width="1221"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.11 – The AWS IAM Users page</p>
<p>On the <strong class="bold">Create user</strong> page, insert a username that is easy to remember, as <span class="No-Break">shown here:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer270">
<img alt="Figure 10.12 – The AWS IAM new user details" height="697" src="image/Figure_10.12_B19453.jpg" width="1268"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.12 – The AWS IAM new user details</p>
<p>Leave the <a id="_idIndexMarker694"/>checkbox unmarked and select <strong class="bold">Next</strong> to add the <span class="No-Break">access policies.</span></p>
<ol>
<li value="4">On the <strong class="bold">Set permissions</strong> page, select <strong class="bold">Attach policies directly</strong> and then look for <strong class="bold">AmazonS3FullAccess</strong> in the <strong class="bold">Permission </strong><span class="No-Break"><strong class="bold">policies</strong></span><span class="No-Break"> checkbox:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer271">
<img alt="Figure 10.13 – AWS IAM set permissions for user creation" height="901" src="image/Figure_10.13_B19453.jpg" width="1196"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.13 – AWS IAM set permissions for user creation</p>
<p>Since this is a testing exercise, we can use full access to the S3 resource. However, remember to attach specific policies to access the resources in a <span class="No-Break">production environment.</span></p>
<p>Select <strong class="bold">Next</strong> and then click on the <strong class="bold">Create </strong><span class="No-Break"><strong class="bold">user</strong></span><span class="No-Break"> button.</span></p>
<ol>
<li value="5">Now, retrieve <a id="_idIndexMarker695"/>the access key by selecting the user you created, go to <strong class="bold">Security credentials</strong>, and scroll down until you see the <strong class="bold">Access keys</strong> box. Then, create a new one and save the CSV file in an easily <span class="No-Break">accessible place:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer272">
<img alt="Figure 10.14 – Access key creation for a user" height="506" src="image/Figure_10.14_B19453.jpg" width="873"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.14 – Access key creation for a user</p>
<ol>
<li value="6">Now, back in Airflow, let’s configure the connection between Airflow and our <span class="No-Break">AWS account.</span></li>
</ol>
<p>Create a new connection using the Airflow UI, and in the <strong class="bold">Connection Type</strong> field, select <strong class="bold">Amazon S3</strong>. In the <strong class="bold">Extra</strong> field, insert the following line with the credentials retrieved in <span class="No-Break"><em class="italic">step 4</em></span><span class="No-Break">:</span></p>
<pre class="source-code">
{"aws_access_key_id": "your_key", "aws_secret_access_key": "your_secret<strong class="bold">"}</strong></pre>
<p>Your page will look like <span class="No-Break">the following:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer273">
<img alt="Figure 10.15 – The Airflow UI on adding a new AWS S3 connector" height="941" src="image/Figure_10.15_B19453.jpg" width="1282"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.15 – The Airflow UI on adding a new AWS S3 connector</p>
<p>Save it, and open your code editor in your <span class="No-Break">Airflow directory.</span></p>
<ol>
<li value="7">Now, let’s <a id="_idIndexMarker696"/>add the configurations to our <strong class="source-inline">airflow.cfg</strong> file. If you are using Docker to host Airflow, add the following lines to your <strong class="source-inline">docker-compose.yaml file</strong>, under the <span class="No-Break">environment settings:</span><pre class="source-code">
AIRFLOW__LOGGING__REMOTE_LOGGING: "True"
AIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER: "s3://airflow-cookbook"
AIRFLOW__LOGGING__REMOTE_LOG_CONN_ID: conn_s3
AIRFLOW__LOGGING__ENCRYPT_S3_LOGS: "False"</pre></li>
</ol>
<p>Your <strong class="source-inline">docker-compose.yaml</strong> file will look similar <span class="No-Break">to this:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer274">
<img alt="Figure 10.16 – Remote logging configuration in docker-compose.yaml" height="589" src="image/Figure_10.16_B19453.jpg" width="940"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.16 – Remote logging configuration in docker-compose.yaml</p>
<p>If you <a id="_idIndexMarker697"/>installed Airflow directly on your local machine, you can instantly change the <strong class="source-inline">airflow.cfg</strong> file. Change the following lines in <strong class="source-inline">airflow.cfg</strong> and <span class="No-Break">save it:</span></p>
<pre class="source-code">
[logging]
# Users must supply a remote location URL (starting with either 's3://...') and an Airflow connection
# id that provides access to the storage location.
remote_logging = True
remote_base_log_folder = s3://airflow-cookbook
remote_log_conn_id = conn_s3
# Use server-side encryption for logs stored in S3
encrypt_s3_logs = False</pre>
<ol>
<li value="8">After the preceding changes, restart your <span class="No-Break">Airflow application.</span></li>
<li>With your refreshed Airflow, run <strong class="source-inline">basic_logging_dag</strong> and open your AWS S3. Select the bucket you created in the <em class="italic">Getting ready</em> section, and you should see a new object inside of it, <span class="No-Break">as follows:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer275">
<img alt="Figure 10.17 – The AWS S3 airflow-cookbook bucket objects" height="674" src="image/Figure_10.17_B19453.jpg" width="1241"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.17 – The AWS S3 airflow-cookbook bucket objects</p>
<ol>
<li value="10">Then, select <a id="_idIndexMarker698"/>the object created, and you should be able to see more folders related to the tasks executed, <span class="No-Break">as follows:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer276">
<img alt="Figure 10.18 – AWS S3 airflow-cookbook showing the remote logs" height="762" src="image/Figure_10.18_B19453.jpg" width="1332"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.18 – AWS S3 airflow-cookbook showing the remote logs</p>
<ol>
<li value="11">Finally, if you select one of the folders, you will see the same file you saw in the <em class="italic">Creating basic logs in Airﬂow</em> recipe. We successfully wrote logs in a <span class="No-Break">remote location!</span></li>
</ol>
<h2 id="_idParaDest-369"><a id="_idTextAnchor376"/>How it works…</h2>
<p>If you look at this recipe overall, it may seem considerable work. However, remember that we are making a configuration from zero, which generally takes time. Since we are somewhat used to creating an AWS S3 bucket and executing DAGs (see <a href="B19453_02.xhtml#_idTextAnchor064"><span class="No-Break"><em class="italic">Chapter 2</em></span></a> and <a href="B19453_09.xhtml#_idTextAnchor319"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>, respectively), let’s focus on setting the remote <span class="No-Break">log configurations.</span></p>
<p>Our first action <a id="_idIndexMarker699"/>started with creating a connection in Airflow using the access keys generated on AWS. This step is required because, internally, Airflow will use those keys to authenticate in AWS and prove <span class="No-Break">its identity.</span></p>
<p>Then, we changed the following Airflow configurations <span class="No-Break">as follows:</span></p>
<pre class="source-code">
AIRFLOW__LOGGING__REMOTE_LOGGING: "True"
AIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER: "s3://airflow-cookbook"
AIRFLOW__LOGGING__REMOTE_LOG_CONN_ID: conn_s3
AIRFLOW__LOGGING__ENCRYPT_S3_LOGS: "False"</pre>
<p>The two first lines are string configurations to set on Airflow whether remote logging is enabled and which bucket path will be used. The last two lines are related to the name of the connection we created on the <strong class="bold">Connection</strong> page in Airflow UI and whether we will encrypt the log messages. This last item must be set as <strong class="source-inline">True</strong> if we handle <span class="No-Break">sensitive information.</span></p>
<p>After restarting Airflow, the configurations will be reflected in our application, and by executing a DAG, we can already see the logs written in the <span class="No-Break">S3 bucket.</span></p>
<p>As mentioned in the introduction of this recipe, this type of configuration is beneficial not only in big projects but also as a good practice when using Airflow, allowing developers to debug or retrieve information about code output without accessing the cluster <span class="No-Break">or server.</span></p>
<p>Here, we covered an example using AWS S3, but it is also <a id="_idIndexMarker700"/>possible to use <strong class="bold">Google Cloud Storage</strong> or <strong class="bold">Azure Blog Storage</strong>. You<a id="_idIndexMarker701"/> can read more <span class="No-Break">here: </span><a href="https://airflow.apache.org/docs/apache-airflow/1.10.13/howto/write-logs.xhtml"><span class="No-Break">https://airflow.apache.org/docs/apache-airflow/1.10.13/howto/write-logs.xhtml</span></a><span class="No-Break">.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">If you don’t want to use remote logging anymore, you can simply remove the environment variables from your <strong class="source-inline">docker-compose.yaml</strong> or set <strong class="source-inline">REMOTE_LOGGING</strong> back <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">False</strong></span><span class="No-Break">.</span></p>
<h2 id="_idParaDest-370"><a id="_idTextAnchor377"/>See also</h2>
<p>You can read more about remote logging in S3 on the Apache Airflow official documentation page <span class="No-Break">here: </span><a href="https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/logging/s3-task-handler.xhtml"><span class="No-Break">https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/logging/s3-task-handler.xhtml</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-371"><a id="_idTextAnchor378"/>Configuring logs in airflow.cfg</h1>
<p>We had <a id="_idIndexMarker702"/>our first contact with the <strong class="source-inline">airflow.cfg</strong> file in the <em class="italic">Storing log files in a remote location</em> recipe. At a glance, we saw how powerful and handy this configuration file is. There are many ways to customize and improve <a id="_idIndexMarker703"/>Airflow just by <span class="No-Break">editing it.</span></p>
<p>This exercise will teach how you to enhance your logs by setting applicable configurations in the <span class="No-Break"><strong class="source-inline">airflow.cfg</strong></span><span class="No-Break"> file.</span></p>
<h2 id="_idParaDest-372"><a id="_idTextAnchor379"/>Getting ready</h2>
<p>Refer to the <em class="italic">Technical requirements</em> section for this recipe, since we will handle it with the <span class="No-Break">same technology.</span></p>
<h3>Airflow DAG code</h3>
<p>To avoid <a id="_idIndexMarker704"/>redundancy and focus on the goal of this recipe, which is to configure remote logging in Airflow, we will use the same DAG as the <em class="italic">Creating basic logs in Airﬂow</em> recipe. However, feel free to create another DAG with a different name but the <span class="No-Break">same code.</span></p>
<h2 id="_idParaDest-373"><a id="_idTextAnchor380"/>How to do it…</h2>
<p>Since we will use the same DAG code from <em class="italic">Creating basic logs in Airﬂow,</em> let’s jump right to the required configuration to format <span class="No-Break">our logs:</span></p>
<ol>
<li>Let’s begin by setting the configuration in our <strong class="source-inline">docker-compose.yaml</strong>. In the environment section, insert the following line and save <span class="No-Break">the file:</span><pre class="source-code">
AIRFLOW__LOGGING__LOG_FORMAT: "[%(asctime)s] [ %(process)s - %(name)s ] {%(filename)s:%(lineno)d} %(levelname)s - %(message)s"</pre></li>
</ol>
<p>Your <strong class="source-inline">docker-compose</strong> file<a id="_idIndexMarker705"/> should look <span class="No-Break">like this:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer277">
<img alt="Figure 10.19 – Formatting log configuration in docker-compose.yaml" height="510" src="image/Figure_10.19_B19453.jpg" width="1223"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.19 – Formatting log configuration in docker-compose.yaml</p>
<p>If you<a id="_idIndexMarker706"/> directly edit the <strong class="source-inline">airflow.cfg</strong> file, search for the <strong class="source-inline">log_format</strong> variable, and change it to the <span class="No-Break">following line:</span></p>
<pre class="source-code">
log_format = [%%(asctime)s] [ %%(process)s - %%(name)s ] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s</pre>
<p>Your code will look <span class="No-Break">like this:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer278">
<img alt="Figure 10.20 – log_format inside airflow.cfg" height="94" src="image/Figure_10.20_B19453.jpg" width="1118"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.20 – log_format inside airflow.cfg</p>
<p>Save it, and go to the <span class="No-Break">next step.</span></p>
<p>We added a few more items in the log line, which we will <span class="No-Break">cover later.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">Be very attentive here. In the <strong class="source-inline">airflow.cfg</strong> file, the <strong class="source-inline">%</strong> character is doubled, unlike in the <span class="No-Break"><strong class="source-inline">docker-compose</strong></span><span class="No-Break"> file.</span></p>
<ol>
<li value="2">Now, let’s <a id="_idIndexMarker707"/>restart Airflow. You can do it by <a id="_idIndexMarker708"/>stopping the Docker container and rerunning it with the <span class="No-Break">following commands:</span><pre class="source-code">
<strong class="bold">$ docker-compose stop      # Or press Crtl-C</strong>
<strong class="bold">$ docker-compose up</strong></pre></li>
<li>Then, let’s head up to the Airflow UI and run our DAG called <strong class="source-inline">basic_logging_dag</strong>. On the DAG page, look in the top-right corner and select the play button (depicted by an arrow), followed by <strong class="bold">Trigger DAG</strong>, <span class="No-Break">as follows:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer279">
<img alt="Figure 10.21 – basic_logging_dag trigger button on the right side of the page" height="329" src="image/Figure_10.21_B19453.jpg" width="1430"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.21 – basic_logging_dag trigger button on the right side of the page</p>
<p>The DAG will start to <span class="No-Break">run immediately.</span></p>
<ol>
<li value="4">Now, let’s see the logs generated by one task. I will pick the <strong class="source-inline">extract_data</strong> task, and the log will look <span class="No-Break">like this:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer280">
<img alt="Figure 10.22 – The formatted log output for extract_data task" height="50" src="image/Figure_10.22_B19453.jpg" width="1037"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.22 – The formatted log output for extract_data task</p>
<p>If you look closely, you will see that we now have the process number displayed on <span class="No-Break">the output.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">If you opt to maintain continuity from the last recipe, <em class="italic">Storing log files in a remote location</em>, remember that your logs are stored in a <span class="No-Break">remote location.</span></p>
<h2 id="_idParaDest-374"><a id="_idTextAnchor381"/>How it works…</h2>
<p>As we <a id="_idIndexMarker709"/>can see, altering<a id="_idIndexMarker710"/> any logging information is simple, since Airflow uses the Python logging library behind the scenes. Now, let’s take a look at <span class="No-Break">our output:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer281">
<img alt="Figure 10.23 – The formatted log output for the extract_data task" height="50" src="image/Figure_10.23_B19453.jpg" width="1038"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.23 – The formatted log output for the extract_data task</p>
<p>As you can see, before the process name (for example, <strong class="source-inline">airflow.task</strong>), we also have the number of the running process. It can be helpful information when running multiple processes simultaneously, allowing us to understand which one is taking longer to complete and what <span class="No-Break">is running.</span></p>
<p>Let’s look at the code <span class="No-Break">we inserted:</span></p>
<pre class="source-code">
AIRFLOW__LOGGING__LOG_FORMAT: "[%(asctime)s] [ %(process)s - %(name)s ] {%(filename)s:%(lineno)d} %(levelname)s - %(message)s"</pre>
<p>As you can see, variables such as <strong class="source-inline">asctime</strong>, <strong class="source-inline">process</strong>, and <strong class="source-inline">filename</strong> are identical to the ones we saw in <a href="B19453_08.xhtml#_idTextAnchor280"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>. Also, since a core Python function operates behind the scenes, we can add more information based on the allowed attributes. You can find the list <span class="No-Break">here: </span><a href="https://docs.python.org/3/library/logging.xhtml#logrecord-attributes"><span class="No-Break">https://docs.python.org/3/library/logging.xhtml#logrecord-attributes</span></a><span class="No-Break">.</span></p>
<h3>Going deeper in airflow.cfg</h3>
<p>Now, let’s go <a id="_idIndexMarker711"/>deeper into Airflow configurations. As you can observe, Airflow resources are orchestrated by the <strong class="source-inline">airflow.cfg</strong> file. Using a single file, we can determine how to send email notifications (we will cover this in the <em class="italic">Using notiﬁcations operators</em> recipe), when DAGs will reflect a code change, how logs will be displayed, and <span class="No-Break">so on.</span></p>
<p>It is also<a id="_idIndexMarker712"/> possible to set these configurations by exporting environment variables, and this has priority over the configuration setting on <strong class="source-inline">airflow.cfg</strong>. This prioritization happens because, internally, Airflow translates the content from <strong class="source-inline">airflow.cfg</strong> to environment variables, broadly speaking. You can read more <span class="No-Break">here: </span><a href="https://airflow.apache.org/docs/apache-airflow/stable/cli-and-env-variables-ref.xhtml#environment-variable"><span class="No-Break">https://airflow.apache.org/docs/apache-airflow/stable/cli-and-env-variables-ref.xhtml#environment-variable</span></a><span class="No-Break">.</span></p>
<p>Let’s look at the logging configuration in the Airflow <strong class="bold">REFERENCES</strong> section. We can see many other customization possibilities, such as coloring, a specific format for DAG processors, and extra logs for third-party applications, as <span class="No-Break">shown here:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer282">
<img alt="Figure 10.24 – Airflow documentation for the logging configuration" height="1062" src="image/Figure_10.24_B19453.jpg" width="1268"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.24 – Airflow documentation for the logging configuration</p>
<p>The fantastic part of this documentation is that we have references to configure directly in <strong class="source-inline">airflow.cfg</strong> or environment variables. You can see the complete reference list <span class="No-Break">here: </span><a href="https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.xhtml#logging"><span class="No-Break">https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.xhtml#logging</span></a><span class="No-Break">.</span></p>
<p>After we get used to the Airflow dynamics, testing new configurations or formats is straightforward, especially when we have a testing server to do so. However, simultaneously, we<a id="_idIndexMarker713"/> need to be cautious when changing anything internally; otherwise, we can impair our <span class="No-Break">whole application.</span></p>
<h2 id="_idParaDest-375"><a id="_idTextAnchor382"/>There’s more…</h2>
<p>In <em class="italic">step 1</em>, we mentioned avoiding the use of double <strong class="source-inline">%</strong> characters when setting the variables in <strong class="source-inline">docker-compose</strong> – let’s now <span class="No-Break">cover this!</span></p>
<p>The <strong class="source-inline">string</strong> variable we pass for <strong class="source-inline">docker-compose</strong> will be read by an internal Python logging function, which will not recognize the double <strong class="source-inline">%</strong> pattern. Instead, it will understand the default format for the logs in Airflow needs to be equal to that string variable, and all the DAG logs will look <span class="No-Break">like this:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer283">
<img alt="Figure 10.25 – An error when the environment variable for log_format is not correctly set" height="442" src="image/Figure_10.25_B19453.jpg" width="1229"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.25 – An error when the environment variable for log_format is not correctly set</p>
<p>Now, inside the <strong class="source-inline">airflow.cfg</strong> file, the double <strong class="source-inline">%</strong> character is a Bash format pattern that works like a <span class="No-Break">modulo operator.</span></p>
<h2 id="_idParaDest-376"><a id="_idTextAnchor383"/>See also</h2>
<p>See the whole list of configurations for Airflow <span class="No-Break">here: </span><a href="https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.xhtml"><span class="No-Break">https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.xhtml</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-377"><a id="_idTextAnchor384"/>Designing advanced monitoring</h1>
<p>After<a id="_idIndexMarker714"/> spending some time learning and practicing logging concepts, we can advance a little more in the subject of monitoring. We can monitor results from all our logging collection work and generate insightful monitoring dashboards and alerts, with the right monitoring <span class="No-Break">message stored.</span></p>
<p>In this recipe, we will cover the Airflow metrics integrated with StatsD, a platform that collects system statistics, and their purpose to help us achieve a <span class="No-Break">mature pipeline.</span></p>
<h2 id="_idParaDest-378"><a id="_idTextAnchor385"/>Getting ready</h2>
<p>This exercise will focus on bringing clarity to the Airflow monitoring metrics and how to build a robust architecture to <span class="No-Break">structure it.</span></p>
<p>As a requirement for this recipe, it is vital to keep in mind the following basic <span class="No-Break">Airflow architecture:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer284">
<img alt="Figure 10.26 – An Airflow high-level architecture diagram" height="574" src="image/Figure_10.26_B19453.jpg" width="713"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.26 – An Airflow high-level architecture diagram</p>
<p>Airflow components, from a<a id="_idIndexMarker715"/> high-level perspective, are composed of <span class="No-Break">the following:</span></p>
<ul>
<li>A <strong class="bold">web server</strong>, where we can access the <span class="No-Break">Airflow UI.</span></li>
<li>A relational database to store metadata and other helpful information for use in the DAGs or tasks. To <a id="_idIndexMarker716"/>keep it simple, we will work with just one type of database; however, there can be more <span class="No-Break">than one.</span></li>
<li>The <strong class="bold">scheduler</strong>, which<a id="_idIndexMarker717"/> will consult the information inside the database to send it to <span class="No-Break">the workers.</span></li>
<li>A <strong class="bold">Celery</strong> application, responsible<a id="_idIndexMarker718"/> for queueing the requests sent from the scheduler and <span class="No-Break">the workers.</span></li>
<li>The <strong class="bold">workers</strong>, which<a id="_idIndexMarker719"/> will execute the DAG <span class="No-Break">and tasks.</span></li>
</ul>
<p>With this in mind, we <a id="_idIndexMarker720"/>can proceed to the <span class="No-Break">next section.</span></p>
<h2 id="_idParaDest-379"><a id="_idTextAnchor386"/>How to do it…</h2>
<p>Let’s see the main items to design <span class="No-Break">advanced monitoring:</span></p>
<ul>
<li><strong class="bold">Counters</strong>: As<a id="_idIndexMarker721"/> the name suggests, this metric will provide information about the counts of actions inside Airflow. This metric provides a count of running tasks, failed tasks, and so on. In the following figure, you can see <span class="No-Break">some examples:</span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer285">
<img alt="Figure 10.27 – A list of counter metric examples to monitor Airflow workflows" height="499" src="image/Figure_10.27_B19453.jpg" width="549"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.27 – A list of counter metric examples to monitor Airflow workflows</p>
<ul>
<li><strong class="bold">Timers</strong>: This <a id="_idIndexMarker722"/>metric tells us how long a task or DAG takes to complete or load a file. In the following figure, you can <span class="No-Break">see more:</span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer286">
<img alt="Figure 10.28 – A list of timer examples to monitor Airflow workflows" height="292" src="image/Figure_10.28_B19453.jpg" width="548"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.28 – A list of timer examples to monitor Airflow workflows</p>
<ul>
<li><strong class="bold">Gauges</strong>: Finally, the<a id="_idIndexMarker723"/> last metric type gives us a <a id="_idIndexMarker724"/>more visual overview. Gauges use timers or counters metrics to illustrate whether we are reaching a defined threshold. In the following figure, there are some examples <span class="No-Break">of gauges:</span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer287">
<img alt="Figure 10.29 – A list of gauge examples to be used to monitor Airflow" height="320" src="image/Figure_10.29_B19453.jpg" width="389"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.29 – A list of gauge examples to be used to monitor Airflow</p>
<p>With the metrics defined and on our radar, we can proceed with the architecture design to <span class="No-Break">integrate it.</span></p>
<ul>
<li><strong class="bold">StatsD</strong>: Now, let’s add <strong class="bold">StatsD</strong> to <a id="_idIndexMarker725"/>the architecture drawing we saw in the <em class="italic">Getting ready</em> section. You will have something <span class="No-Break">like this:</span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer288">
<img alt="Figure 10.30 – StatsD integration and coverage for the Airflow components architecture" height="643" src="image/Figure_10.30_B19453.jpg" width="771"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.30 – StatsD integration and coverage for the Airflow components architecture</p>
<p>StatsD can collect the metrics from all the components inside the dotted rectangle and direct them to a <span class="No-Break">monitoring tool.</span></p>
<ul>
<li><strong class="bold">Prometheus and Grafana</strong>: Then, we <a id="_idIndexMarker726"/>can plug StatsD into <a id="_idIndexMarker727"/>Prometheus, which serves as one of <a id="_idIndexMarker728"/>Grafana’s data sources. Adding these tools into our architecture will look something <span class="No-Break">like this:</span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer289">
<img alt="Figure 10.31 – A Prometheus and Grafana integration with StatsD and Airflow diagram" height="893" src="image/Figure_10.31_B19453.jpg" width="858"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.31 – A Prometheus and Grafana integration with StatsD and Airflow diagram</p>
<p>Now, let’s understand the components behind <span class="No-Break">this architecture.</span></p>
<h2 id="_idParaDest-380"><a id="_idTextAnchor387"/>How it works…</h2>
<p>Let’s start <a id="_idIndexMarker729"/>understanding what StatsD is. StatsD is a daemon developed by the Etsy company to aggregate and collect application metrics. Generally, any application can send<a id="_idIndexMarker730"/> metrics using a simple protocol, such as <strong class="bold">User Datagram Protocol</strong> (<strong class="bold">UDP</strong>). With this protocol, the sender doesn’t need to wait for a response from StatsD, making the process simple. After listening and aggregating data for some time, StatsD will send the metrics to output storage, which <span class="No-Break">is Prometheus.</span></p>
<p>The StatsD integration and installation can be done using the <span class="No-Break">following command:</span></p>
<pre class="source-code">
pip install 'apache-airflow[statsd]'</pre>
<p>If you want to know more about it, you can refer to the Airflow documentation <span class="No-Break">here: </span><a href="https://airflow.apache.org/docs/apache-airflow/2.5.1/administration-and-deployment/logging-monitoring/metrics.xhtml#counters"><span class="No-Break">https://airflow.apache.org/docs/apache-airflow/2.5.1/administration-and-deployment/logging-monitoring/metrics.xhtml#counters</span></a><span class="No-Break">.</span></p>
<p>Then, Prometheus and Grafana will gather the metrics and transform them into a more visual resource. You don’t need to worry about this now; we will learn more about it in <a href="B19453_12.xhtml#_idTextAnchor433"><span class="No-Break"><em class="italic">Chapter 12</em></span></a><span class="No-Break">.</span></p>
<p>For <a id="_idIndexMarker731"/>each metric we saw in the three first steps in the <em class="italic">How to do it…</em> section, we can set a threshold to trigger an alert when it has trespassed. All the metrics are presented in the <em class="italic">How to do it…</em> section, and some more can be found <span class="No-Break">here: </span><span class="No-Break">https://airflow.apache.org/docs/apache-airflow/2.5.1/administration-and-deployment/logging-monitoring/metrics.xhtml#counters</span><span class="No-Break">.</span></p>
<h2 id="_idParaDest-381"><a id="_idTextAnchor388"/>There’s more…</h2>
<p>Besides StatsD, there are other tools we can plug into Airflow to track specific metrics or statuses. For example, for a deep error track, we can use <strong class="bold">Sentry</strong>, a<a id="_idIndexMarker732"/> specialized tool used by IT operations teams to provide support and insights. You can learn more about this integration <span class="No-Break">here: </span><a href="https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/errors.xhtml"><span class="No-Break">https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/errors.xhtml</span></a><span class="No-Break">.</span></p>
<p>On the other hand, if tracking users’ activities is a concern, it is possible to integrate Airflow with Google Analytics. You can learn more <span class="No-Break">here: </span><a href="https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/tracking-user-activity.xhtml"><span class="No-Break">https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/tracking-user-activity.xhtml</span></a><span class="No-Break">.</span></p>
<h2 id="_idParaDest-382"><a id="_idTextAnchor389"/>See also</h2>
<ul>
<li>Learn more about Airflow architecture <span class="No-Break">here: </span><a href="https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/logging-architecture.xhtml"><span class="No-Break">https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/logging-architecture.xhtml</span></a></li>
<li>More information about StatsD is <span class="No-Break">here: </span><a href="https://www.datadoghq.com/blog/statsd/"><span class="No-Break">https://www.datadoghq.com/blog/statsd/</span></a></li>
</ul>
<h1 id="_idParaDest-383"><a id="_idTextAnchor390"/>Using notiﬁcation operators</h1>
<p>So far, we <a id="_idIndexMarker733"/>have focused on ensuring that code is well logged and has enough information to provide valid monitoring. Nevertheless, the purpose of having mature and structured pipelines is to avoid the necessity of manual intervention. With busy agendas and other projects, it is hard to constantly look at monitoring dashboards to check whether everything <span class="No-Break">is fine.</span></p>
<p>Thankfully, Airflow also has native operators to trigger alerts depending on their configured situation. In this recipe, we will configure an email operator to trigger a message every time a pipeline succeeds or fails, allowing us to remediate the <span class="No-Break">problem rapidly.</span></p>
<h2 id="_idParaDest-384"><a id="_idTextAnchor391"/>Getting ready</h2>
<p>Refer to the <em class="italic">Technical requirements </em>section for this recipe, since we will handle it with the <span class="No-Break">same technology.</span></p>
<p>In addition to that, you need to create an app password for your Google account. This password will allow our application to authenticate and<a id="_idIndexMarker734"/> use the <strong class="bold">Simple Mail Transfer Protocol</strong> (<strong class="bold">SMTP</strong>) host from Google to trigger emails. You can generate the app password in your Google account at the following <span class="No-Break">link: </span><a href="https://security.google.com/settings/security/apppasswords"><span class="No-Break">https://security.google.com/settings/security/apppasswords</span></a><span class="No-Break">.</span></p>
<p>Once you access the link, you will be asked to authenticate using your Google credentials, and a new page will appear, similar to <span class="No-Break">the following:</span></p>
<p class="IMG---Figure"> </p>
<div>
<div class="IMG---Figure" id="_idContainer290">
<img alt="Figure 10.32 – The Google app password generation page" height="182" src="image/Figure_10.32_B19453.jpg" width="629"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.32 – The Google app password generation page</p>
<p>In the first box, select <strong class="bold">Mail</strong>, and in the second box, select the device that will use the app password. Since I am using a Macbook, I will select <strong class="bold">Mac</strong>, as shown in the preceding screenshot. Then, click <span class="No-Break">on </span><span class="No-Break"><strong class="bold">GENERATE</strong></span><span class="No-Break">.</span></p>
<p>A window similar to the following <span class="No-Break">will appear:</span></p>
<p class="IMG---Figure"> </p>
<div>
<div class="IMG---Figure" id="_idContainer291">
<img alt="Figure 10.33 – The Google generated app password pop-up window" height="438" src="image/Figure_10.33_B19453.jpg" width="498"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.33 – The Google generated app password pop-up window</p>
<p>Follow the <a id="_idIndexMarker735"/>steps on the page and save the password in a place you <span class="No-Break">can remember.</span></p>
<h3>Airflow DAG code</h3>
<p>To avoid redundancy and focus on the goal of this recipe, which is to configure remote logging in<a id="_idIndexMarker736"/> Airflow, we will use the same DAG as the <em class="italic">Creating basic logs in Airﬂow </em>recipe. However, feel free to create another DAG with a different name but the <span class="No-Break">same code.</span></p>
<p>Nonetheless, you can always find the final code in the GitHub <span class="No-Break">repository here:</span></p>
<p><a href="https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_10/Using_notifications_operators"><span class="No-Break">https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_10/Using_notifications_operators</span></a></p>
<h2 id="_idParaDest-385"><a id="_idTextAnchor392"/>How to do it…</h2>
<p>Perform the following steps to try <span class="No-Break">this recipe:</span></p>
<ol>
<li>Let’s start by configuring the SMTP server in Airflow. Insert the following lines in your <strong class="source-inline">docker-compose.yaml</strong> file under the <span class="No-Break">environment section:</span><pre class="source-code">
    # SMTP settings
    AIRFLOW__SMTP__SMTP_HOST: "smtp.gmail.com"
    AIRFLOW__SMTP__SMTP_USER: "your_email_here"
    AIRFLOW__SMTP__SMTP_PASSWORD: "your_app_password_here"
    AIRFLOW__SMTP__SMTP_PORT: 587</pre></li>
</ol>
<p>Your file<a id="_idIndexMarker737"/> should look <span class="No-Break">like this:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer292">
<img alt="Figure 10.34 – docker-compose.yaml with SMTP environment variables" height="597" src="image/Figure_10.34_B19453.jpg" width="969"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.34 – docker-compose.yaml with SMTP environment variables</p>
<p>If you directly <a id="_idIndexMarker738"/>edit the <strong class="source-inline">airflow.cfg</strong> file, edit the <span class="No-Break">following lines:</span></p>
<pre class="source-code">
[smtp]
# If you want airflow to send emails on retries, failure, and you want to use
# the airflow.utils.email.send_email_smtp function, you have to configure an
# smtp server here
smtp_host = smtp.gmail.com
smtp_starttls = True
smtp_ssl = False
# Example: smtp_user = airflow
smtp_user = your_email_here
# Example: smtp_password = airflow
smtp_password = your_app_password_here
smtp_port = 587
smtp_mail_from = airflow@example.com
smtp_timeout = 30
smtp_retry_limit = 5</pre>
<p>Don’t forget to restart Airflow after these configurations <span class="No-Break">are saved.</span></p>
<ol>
<li value="2">Now, let’s<a id="_idIndexMarker739"/> edit our <strong class="source-inline">basic_logging_dag</strong> DAG to allow it to send emails using <strong class="source-inline">EmailOperator</strong>. Let’s add to our imports the <span class="No-Break">following line:</span><pre class="source-code">
from airflow.operators.email import EmailOperator</pre></li>
</ol>
<p>The imports will be organized <span class="No-Break">like this:</span></p>
<pre class="source-code">
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.operators.email import EmailOperator
from datetime import datetime, timedelta
import logging
# basic_logging_dag DAG code
# ...</pre>
<ol>
<li value="3">In <strong class="source-inline">default_args</strong>, we will add three new parameters – <strong class="source-inline">email</strong>, <strong class="source-inline">email_on_failure</strong>, and <strong class="source-inline">email_on_retry</strong>. You can see here what it <span class="No-Break">looks like:</span><pre class="source-code">
# basic_logging_dag DAG imports above this line
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2023, 4, 1),
    'email': ['sample@gmail.com'],
    'email_on_failure': True,
    'email_on_retry': True,
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}
# basic_logging_dag DAG code
# …</pre></li>
</ol>
<p>You don’t <a id="_idIndexMarker740"/>need to worry about these new parameters for now. We will cover them in the <em class="italic">How it </em><span class="No-Break"><em class="italic">works…</em></span><span class="No-Break"> section.</span></p>
<ol>
<li value="4">Then, let’s add a new task to our DAG called <strong class="source-inline">success_task</strong>. If all the other tasks are successful, this one will trigger <strong class="source-inline">EmailOperator</strong> to alert us. Add the following code to the <span class="No-Break"><strong class="source-inline">basic_logging_dag</strong></span><span class="No-Break"> script:</span><pre class="source-code">
success_task = EmailOperator(
    task_id="success_task",
    to= "g.esppen@gmail.com",
    subject="The pipeline finished successfully!",
    html_content="&lt;h2&gt; Hello World! &lt;/h2&gt;",
    dag=dag
)</pre></li>
<li>Finally, at the end of your script, let’s add <span class="No-Break">the workflow:</span><pre class="source-code">
extract_task &gt;&gt; transform_task &gt;&gt; load_task &gt;&gt; success_task</pre></li>
</ol>
<p>Don’t forget that you can always check how the final code looks <span class="No-Break">here: </span><a href="https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_10/Using_noti%EF%AC%81cations_operators"><span class="No-Break">https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_10/Using_noti%EF%AC%81cations_operators</span></a></p>
<ol>
<li value="6">If you check your <a id="_idIndexMarker741"/>DAG graph, you can see that a new task called <strong class="source-inline">success_task</strong> appears. It shows our operator is ready to be used. Let’s trigger our DAG by selecting the play button in the top-right corner, as we did in <em class="italic">step 3</em> of the <em class="italic">Configuring logs in </em><span class="No-Break"><em class="italic">airflow.cfg</em></span><span class="No-Break"> recipe.</span></li>
</ol>
<p>Your Airflow UI should look <span class="No-Break">like this:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer293">
<img alt="Figure 10.35 – basic_logging_dag showing successful runs for all the tasks" height="444" src="image/Figure_10.35_B19453.jpg" width="1457"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.35 – basic_logging_dag showing successful runs for all the tasks</p>
<ol>
<li value="7">Then, let’s check our email. If everything is well configured, you should see an email similar to <span class="No-Break">the following:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer294">
<img alt="Figure 10.36 – An email with a Hello World! Message, indicating that success_task worked" height="376" src="image/Figure_10.36_B19453.jpg" width="991"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.36 – An email with a Hello World! Message, indicating that success_task worked</p>
<p>Our <strong class="source-inline">EmailOperator</strong> works exactly <span class="No-Break">as expected!</span></p>
<h2 id="_idParaDest-386"><a id="_idTextAnchor393"/>How it works…</h2>
<p>Let’s start explaining the code by defining what an SMTP server is. An SMTP server is a key component of an email system that enables the transmission of email messages between servers and from clients <span class="No-Break">to servers.</span></p>
<p>In our case, Google works both as a sender and receiver. We borrow a Gmail host to help send an email from our local machine. However, you don’t need to worry about this when working on a company project; your IT operations team will take care <span class="No-Break">of it.</span></p>
<p>Now, back to Airflow – once we understand how the SMTP works, its configuration is straightforward. Consulting the reference page for the configurations in Airflow (<a href="https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.xhtml">https://airflow.apache.org/docs/apache-airflow/stable/configurations-ref.xhtml</a>), we can see that there is a section dedicated to SMTP, as you can <span class="No-Break">see here:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer295">
<img alt="Figure 10.37 – The Airflow documentation page for the SMTP environment variables" height="493" src="image/Figure_10.37_B19453.jpg" width="884"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.37 – The Airflow documentation page for the SMTP environment variables</p>
<p>Then, all we needed to <a id="_idIndexMarker742"/>do was to set the required parameters to allow the connection between the host (<strong class="source-inline">smtp.gmail.com</strong>) and Airflow, as you can <span class="No-Break">see here:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer296">
<img alt="Figure 10.38 – A close look at the docker-compose.yaml SMTP settings" height="121" src="image/Figure_10.38_B19453.jpg" width="464"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.38 – A close look at the docker-compose.yaml SMTP settings</p>
<p>Once this step is completed, we will go to our DAG and declare <strong class="source-inline">EmailOperator</strong>, as shown in the <span class="No-Break">following code:</span></p>
<pre class="source-code">
success_task = EmailOperator(
    task_id="success_task",
    to="g.esppen@gmail.com",
    subject="The pipeline finished successfully!",
    html_content="&lt;h2&gt; Hello World! &lt;/h2&gt;",
    dag=dag
)</pre>
<p>The parameters of the email are very intuitive and can be set accordingly to whatever is needed. If we delve deeper, we can see that there are plenty of possibilities to make those fields’ values more abstract to adapt to different <span class="No-Break">function results.</span></p>
<p>It is also <a id="_idIndexMarker743"/>possible to use a formatted email template in <strong class="source-inline">html_content</strong> and even attach a complete error or log message. You can see more of the allowed parameters <span class="No-Break">here: </span><a href="https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/email/index.xhtml"><span class="No-Break">https://airflow.apache.org/docs/apache-airflow/stable/_api/airflow/operators/email/index.xhtml</span></a><span class="No-Break">.</span></p>
<p>In our case, this operator was triggered when all tasks successfully ran. But what about if there is an error? Let’s go back to <em class="italic">step 3</em> and <span class="No-Break">see </span><span class="No-Break"><strong class="source-inline">default_args</strong></span><span class="No-Break">:</span></p>
<pre class="source-code">
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2023, 4, 1),
    'email': ['sample@gmail.com'],
    'email_on_failure': True,
    'email_on_retry': True,
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}</pre>
<p>The two new parameters added (<strong class="source-inline">email_on_failure</strong> and <strong class="source-inline">email_on_retry</strong>) address scenarios where the DAG failed or retries a task. The values inside the <strong class="source-inline">email</strong> parameter list <a id="_idIndexMarker744"/>are the recipients of <span class="No-Break">these emails.</span></p>
<p>A default email triggered by an error message looks <span class="No-Break">like this:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer297">
<img alt="Figure 10.39 – The Airflow default email for error in a task instance" height="251" src="image/Figure_10.39_B19453.jpg" width="715"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.39 – The Airflow default email for error in a task instance</p>
<h2 id="_idParaDest-387"><a id="_idTextAnchor394"/>There’s more…</h2>
<p>The Airflow notification system is not limited to sending emails and counts, offering useful integrations with Slack, Teams, <span class="No-Break">and Telegram.</span></p>
<p>TowardsDataScience has a fantastic blog post about how to integrate Airflow with Slack, and you can find it <span class="No-Break">here: </span><a href="https://towardsdatascience.com/automated-alerts-for-airflow-with-slack-5c6ec766a823"><span class="No-Break">https://towardsdatascience.com/automated-alerts-for-airflow-with-slack-5c6ec766a823</span></a><span class="No-Break">.</span></p>
<p>Not limited to corporate tools, Airflow also has a Discord <span class="No-Break">hook: </span><a href="https://airflow.apache.org/docs/apache-airflow-providers-discord/stable/_api/airflow/providers/discord/hooks/discord_webhook/index.xhtml"><span class="No-Break">https://airflow.apache.org/docs/apache-airflow-providers-discord/stable/_api/airflow/providers/discord/hooks/discord_webhook/index.xhtml</span></a><span class="No-Break">.</span></p>
<p>The best advice I can give is always to look at Airflow community documentation. As an open source and active platform, there is always a new implementation to help automate and make our daily <span class="No-Break">work easier.</span></p>
<h1 id="_idParaDest-388"><a id="_idTextAnchor395"/>Using SQL operators for data quality</h1>
<p>Good <strong class="bold">data quality</strong> is <a id="_idIndexMarker745"/>crucial for an organization to <a id="_idIndexMarker746"/>ensure the effectiveness of its data systems. By performing quality checks within the DAG, it is possible to stop pipelines and notify stakeholders before erroneous data is introduced into a production lake <span class="No-Break">or warehouse.</span></p>
<p>Although plenty of<a id="_idIndexMarker747"/> available tools in the market provide <strong class="bold">data quality checks</strong>, one of the most popular ways to do this is by running SQL queries. As you may have already guessed, Airflow has providers to support <span class="No-Break">those operations.</span></p>
<p>This recipe will cover the data quality principal topics in the data ingestion process, pointing out the best <strong class="source-inline">SQLOperator</strong> type to run in <span class="No-Break">those situations.</span></p>
<h2 id="_idParaDest-389"><a id="_idTextAnchor396"/>Getting ready</h2>
<p>Before starting our exercise, let’s create a<a id="_idIndexMarker748"/> simple <strong class="bold">Entity Relationship Diagram</strong> (<strong class="bold">ERD</strong>) for a <strong class="source-inline">customers</strong> table. You can see here how <span class="No-Break">it looks:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer298">
<img alt="Figure 10.40 – An example of customers table columns" height="359" src="image/Figure_10.40_B19453.jpg" width="157"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.40 – An example of customers table columns</p>
<p>And the <a id="_idIndexMarker749"/>same table is represented with <span class="No-Break">its schema:</span></p>
<pre class="source-code">
CREATE TABLE customers (
    customer_id INT PRIMARY KEY,
    first_name VARCHAR(50),
    last_name VARCHAR(50),
    email VARCHAR(100),
    phone_number VARCHAR(20),
    address VARCHAR(200),
    city VARCHAR(50),
    state VARCHAR(50),
    country VARCHAR(50),
    zip_code VARCHAR(20)
);</pre>
<p>You don’t need <a id="_idIndexMarker750"/>to worry about creating this table in a SQL database. This exercise will focus on the data quality factors to be checked, using this table as <span class="No-Break">an example.</span></p>
<h2 id="_idParaDest-390"><a id="_idTextAnchor397"/>How to do it…</h2>
<p>Here are <a id="_idIndexMarker751"/>the steps to perform <span class="No-Break">this recipe:</span></p>
<ol>
<li>Let’s start by defining the essential data quality checks that apply <span class="No-Break">as follows:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer299">
<img alt="Figure 10.41 – Data quality essential points" height="336" src="image/Figure_10.41_B19453.jpg" width="609"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.41 – Data quality essential points</p>
<ol>
<li value="2">Let’s imagine implementing it using <strong class="source-inline">SQLColumnCheckOperator</strong>, integrated and installed in our Airflow platform. Let’s now create a simple task to check whether our table has unique IDs and whether all customers have <strong class="source-inline">first_name</strong>. Our example code looks <span class="No-Break">like this:</span><pre class="source-code">
id_username_check = SQLColumnCheckOperator(
        task_id="id_username_check",
        conn_id= my_conn,
        table=my_table,
        column_mapping={
            "customer_id": {
                "null_check": {
                    "equal_to": 0,
                    "tolerance": 0,
                },
                "distinct_check": {
                    "equal_to": 1,
                },
            },
            "first_name": {
                "null_check": {"equal_to": 0},
            },
        }
)</pre></li>
<li>Now, let’s<a id="_idIndexMarker752"/> validate whether we ingest the <a id="_idIndexMarker753"/>required count of rows using <strong class="source-inline">SQLTableCheckOperator</strong>, <span class="No-Break">as follows:</span><pre class="source-code">
customer_table_rows_count = SQLTableCheckOperator(
    task_id="customer_table_rows_count",
    conn_id= my_conn,
    table=my_table,
    checks={"row_count_check": {
                "check_statement": "COUNT(*) &gt;= 1000"
            }
        }
)</pre></li>
<li>Finally, let’s <a id="_idIndexMarker754"/>ensure the customers in our database<a id="_idIndexMarker755"/> have at least one order. Our example code looks <span class="No-Break">like this:</span><pre class="source-code">
count_orders_check = SQLColumnCheckOperator(
    task_id="check_columns",
    conn_id=my-conn,
    table=my_table,
    column_mapping={
        "MY_NUM_COL": {
            "min": {"geq_to ": 1}
        }
    }
)</pre></li>
</ol>
<p>The <strong class="source-inline">geq_to</strong> key stands for <strong class="bold">great or </strong><span class="No-Break"><strong class="bold">equal to</strong></span><span class="No-Break">.</span></p>
<h2 id="_idParaDest-391"><a id="_idTextAnchor398"/>How it works…</h2>
<p>Data quality is a <a id="_idIndexMarker756"/>complex topic encompassing many variables, such as the project or company context, business models, and <strong class="bold">Service Level Agreements</strong> (<strong class="bold">SLAs</strong>) between<a id="_idIndexMarker757"/> teams. Based on this, the goal of this recipe was to offer the core concept of data quality and demonstrate how to first approach using <span class="No-Break">Airflow SQLOperators.</span></p>
<p>Let’s start with the essential topics in <em class="italic">step 1</em>, <span class="No-Break">as follows:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer300">
<img alt="Figure 10.42 – Data quality essential points" height="336" src="image/Figure_10.42_B19453.jpg" width="609"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.42 – Data quality essential points</p>
<p>In a generic <a id="_idIndexMarker758"/>scenario, those items are the principal topics to be approached and implemented. They will guarantee the minimum data reliability, based on whether the columns are the ones we expected, creating an average value for the row count, ensuring the IDs are unique, and having control of the <strong class="source-inline">null</strong> and distinct values in <span class="No-Break">specific columns.</span></p>
<p>Using Airflow, we used the SQL approach to check data. As mentioned at the beginning of this recipe, SQL checks are widespread and popular due to their simplicity and flexibility. Unfortunately, to simulate a scenario like this, we would be required to set up a hard-working local infrastructure, and the best we could come up with is simulating the tasks <span class="No-Break">in Airflow.</span></p>
<p>Here, we used two <strong class="source-inline">SQLOperator</strong> subtypes – <strong class="source-inline">SQLColumnCheckOperator</strong> and <strong class="source-inline">SQLTableCheckOperator</strong>. As the name suggests, the first operator is more focused on verifying the column’s content by checking whether there are null or distinct values. In the case of <strong class="source-inline">customer_id</strong>, we verified both scenarios and only null values for <strong class="source-inline">first_name</strong>, as you can <span class="No-Break">see here:</span></p>
<pre class="source-code">
column_mapping={
            "customer_id": {
                "null_check": {
                    "equal_to": 0,
                    "tolerance": 0,
                },
                "distinct_check": {
                    "equal_to": 1,
                },
            },
            "first_name": {
                "null_check": {"equal_to": 0},
            },
        }</pre>
<p><strong class="source-inline">SQLTableCheckOperator</strong> will <a id="_idIndexMarker759"/>perform validations across <a id="_idIndexMarker760"/>the whole table. It allows the insertion of a SQL query to make counts or other operations, as we did to validate the expected number of rows in <em class="italic">step 3</em>, as you can see in the piece of <span class="No-Break">code here:</span></p>
<pre class="source-code">
<strong class="bold">    checks={"row_count_check": {</strong>
<strong class="bold">                "check_statement": "COUNT(*) &gt;= 1000"</strong>
<strong class="bold">            }</strong>
<strong class="bold">        }</strong></pre>
<p>However, <strong class="source-inline">SQLOperator</strong> is not limited to these two. In the Airflow documentation, you can see other examples and the complete list of accepted parameters for these <span class="No-Break">functions: </span><a href="https://airflow.apache.org/docs/apache-airflow/2.1.4/_api/airflow/operators/sql/index.xhtml#module-airflow.operators.sql"><span class="No-Break">https://airflow.apache.org/docs/apache-airflow/2.1.4/_api/airflow/operators/sql/index.xhtml#module-airflow.operators.sql</span></a><span class="No-Break">.</span></p>
<p>A fantastic<a id="_idIndexMarker761"/> operator to check out is <strong class="source-inline">SQLIntervalCheckOperator</strong>, used to validate historical data and ensure the stored information <span class="No-Break">is concise.</span></p>
<p>In your data career, you <a id="_idIndexMarker762"/>will see that data quality is a daily topic and concern among teams. The best advice here is to continually search for tools and methods to improve <span class="No-Break">this methodology.</span></p>
<h2 id="_idParaDest-392"><a id="_idTextAnchor399"/>There’s more…</h2>
<p>We can use additional tools to enhance our data quality checks. One of the recommended tools for this use<a id="_idIndexMarker763"/> is <strong class="bold">GreatExpectations</strong>, an open source platform made in<a id="_idIndexMarker764"/> Python with plenty of integrations, with resources such as<a id="_idIndexMarker765"/> Airflow, <strong class="bold">AWS S3</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="bold">Databricks</strong></span><span class="No-Break">.</span></p>
<p>Although it is <a id="_idIndexMarker766"/>a platform you can install in any cluster, <strong class="bold">GreatExpectations</strong> is expanding toward a managed cloud version. You can check more about it on the <a id="_idIndexMarker767"/>official page <span class="No-Break">here: </span><a href="https://greatexpectations.io/integrations"><span class="No-Break">https://greatexpectations.io/integrations</span></a><span class="No-Break">.</span></p>
<h2 id="_idParaDest-393"><a id="_idTextAnchor400"/>See also</h2>
<ul>
<li><em class="italic">Yu Ishikawa</em> has a nice blog post about other checks you can do using SQL in <span class="No-Break">Airflow: </span><a href="https://yu-ishikawa.medium.com/apache-airflow-as-a-data-quality-checker-416ca7f5a3ad"><span class="No-Break">https://yu-ishikawa.medium.com/apache-airflow-as-a-data-quality-checker-416ca7f5a3ad</span></a></li>
<li>More information about data quality in Airflow is available <span class="No-Break">here: </span><a href="https://docs.astronomer.io/learn/data-quality"><span class="No-Break">https://docs.astronomer.io/learn/data-quality</span></a></li>
</ul>
<h1 id="_idParaDest-394"><a id="_idTextAnchor401"/>Further reading</h1>
<ul>
<li><a href="https://www.oak-tree.tech/blog/airflow-remote-logging-s3"><span class="No-Break">https://www.oak-tree.tech/blog/airflow-remote-logging-s3</span></a></li>
<li><a href="https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/connections/aws.xhtml#examples"><span class="No-Break">https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/connections/aws.xhtml#examples</span></a></li>
<li><a href="https://airflow.apache.org/docs/apache-airflow/stable/howto/email-config.xhtml"><span class="No-Break">https://airflow.apache.org/docs/apache-airflow/stable/howto/email-config.xhtml</span></a></li>
<li><span class="No-Break">https://docs.astronomer.io/learn/logging</span></li>
<li><span class="No-Break">https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/metrics.xhtml#setup</span></li>
<li><span class="No-Break">https://hevodata.com/learn/airflow-monitoring/#aam</span></li>
<li><span class="No-Break">https://servian.dev/developing-5-step-data-quality-framework-with-apache-airflow-972488ddb65f</span></li>
</ul>
</div>
</div></body></html>