<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Large-Scale Data Processing Frameworks</h1>
                </header>
            
            <article>
                
<p>As the volume and complexity of data sources are increasing, deriving value out of data is also becoming increasingly difficult. Ever since Hadoop was made, it has built a massively scalable filesystem, HDFS. It has adopted the MapReduce concepts from functional programming to approach the large-scale data processing challenges. As technology is constantly evolving to overcome the challenges posed by data mining, enterprises are also finding ways to embrace these changes to stay ahead.</p>
<p>In this chapter, we will focus on these data processing solutions:</p>
<ul>
<li style="font-weight: 400">MapReduce</li>
<li style="font-weight: 400">Apache Spark</li>
<li style="font-weight: 400">Spark SQL</li>
<li style="font-weight: 400">Spark Streaming</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">MapReduce</h1>
                </header>
            
            <article>
                
<p>MapReduce is a concept that is borrowed from functional programming. The data processing is broken down into a map phase, where data preparation occurs, and a reduce phase, where the actual results are computed. The reason MapReduce has played an important role is the massive parallelism we can achieve as the data is sharded into multiple distributed servers. Without this advantage, MapReduce cannot really <span>perform </span>well.</p>
<p>Let's take up a simple example to understand how MapReduce works in functional programming:</p>
<ul>
<li style="font-weight: 400">The input data is processed using a mapper function of our choice</li>
<li style="font-weight: 400">The output from the mapper function should be in a state that is consumable by the reduce function</li>
<li style="font-weight: 400">The output from the mapper function is fed to the reduce function to generate the necessary results</li>
</ul>
<p>Let's understand these steps using a simple program. This program uses the following text (randomly created) as input:</p>
<pre class="mce-root">Bangalore,Onion,60<br/>Bangalore,Chilli,10<br/>Bangalore,Pizza,120<br/>Bangalore,Burger,80<br/>NewDelhi,Onion,80<br/>NewDelhi,Chilli,30<br/>NewDelhi,Pizza,150<br/>NewDelhi,Burger,180<br/>Kolkata,Onion,90<br/>Kolkata,Chilli,20<br/>Kolkata,Pizza,120<br/>Kolkata,Burger,160</pre>
<p>The input consists of data with the following fields: <strong>City Name</strong>, <strong>Product Name</strong>, and <strong>Item Price</strong> on that day.</p>
<p>We want to write a program that will show the total cost of all products in a given city. This can be done in many ways. But let's try to approach this using MapReduce and see how it works.</p>
<p>The mapper program is like this:</p>
<pre>#!/usr/bin/env perl -wl<br/><br/>use strict;<br/>use warnings;<br/><br/>while(&lt;STDIN&gt;) {<br/>    chomp;<br/>    my ($city, $product, $cost) = split(',');<br/>    print "$city $cost";<br/>}</pre>
<p>The reduce program is:</p>
<pre>#!/usr/bin/perl<br/><br/>use strict;<br/>use warnings;<br/><br/>my %reduce;<br/><br/>while(&lt;STDIN&gt;) {<br/>    chomp;<br/>    my ($city, $cost) = split(/\s+/);<br/>    $reduce{$city} = 0 if not defined $reduce{$city};<br/>    $reduce{$city} += $cost;<br/>}<br/><br/>print "-" x 24;<br/>printf("%-10s : %s\n", "City", "Total Cost");<br/>print "-" x 24;<br/><br/>foreach my $city (sort keys %reduce) {<br/>    printf("%-10s : %d\n", $city, $reduce{$city});<br/>}</pre>
<p>We create a data pipeline using the UNIX terminal like this:</p>
<pre>[user@node-1 ~]$ cat input.txt | perl map.pl | perl reduce.pl <br/>------------------------<br/>City : Total Cost<br/>------------------------<br/>Bangalore : 270<br/>Kolkata : 390<br/>NewDelhi : 440</pre>
<p>As we can see, the result is as expected. This is a very simple case of MapReduce. Let's try to see what is happening:</p>
<ul>
<li style="font-weight: 400">Each input line is processed by the <kbd>map.pl</kbd> program and prints the city and price</li>
<li style="font-weight: 400">The output from the <kbd>map.pl</kbd> program is fed to <kbd>reduce.pl</kbd>, which performs a <kbd>SUM()</kbd> operation for all records and categorizes them per city</li>
</ul>
<p>Let's shuffle the <kbd>input.txt</kbd> and see if we get the desired results.</p>
<p>Here is the modified <kbd>input.txt</kbd>:</p>
<pre>Bangalore,Onion,60<br/>NewDelhi,Onion,80<br/>Bangalore,Pizza,120<br/>Bangalore,Burger,80<br/>Kolkata,Onion,90<br/>Kolkata,Pizza,120<br/>Kolkata,Chilli,20<br/>NewDelhi,Chilli,30<br/>NewDelhi,Burger,180<br/>Kolkata,Burger,160<br/>NewDelhi,Pizza,150<br/>Bangalore,Chilli,10</pre>
<p><span>And the output from the MapReduce operation is:</span></p>
<pre>[user@node-1 ~]$ cat input-shuffled.txt | perl map.pl | perl reduce.pl <br/>------------------------<br/>City : Total Cost<br/>------------------------<br/>Bangalore : 270<br/>Kolkata : 390<br/>NewDelhi : 440</pre>
<p>There is no difference because both the map and reduce operations are being performed independently in one go. There is no data parallelism here. The entire process can be visualized in this diagram:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/01dc378d-5166-4980-ac5c-8b85fd387c32.png"/></div>
<p>As we can see, there is one copy of the input data after the <strong>Map Phase</strong>, and the final output after <strong>Reduce Phase</strong> is what we are interested in.</p>
<p>Running a single-threaded process is useful and is needed when we don’t have to deal with massive amounts of data. When the input sizes are unbounded and cannot be fit into a single server, we need to start thinking of distributed/parallel algorithms to attack the problem at hand.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hadoop MapReduce</h1>
                </header>
            
            <article>
                
<p>Apache MapReduce is a framework that makes it easier for us to run MapReduce operations on very large, distributed datasets. One of the advantages of Hadoop is a distributed file system that is rack-aware and scalable. The Hadoop job scheduler is intelligent enough to make sure that the computation happens on the nodes where the data is located. This is also a very important aspect as it reduces the amount of network IO.</p>
<p>Let's see how the framework makes it easier to run massively parallel computations with the help of this diagram:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="assets/4b616b8c-1ff0-4e08-b6e7-a197f494f0aa.png"/></div>
<p>This diagram looks a bit more complicated than the previous diagram, but most of the things are done by the Hadoop MapReduce framework itself for us. We still write the code for mapping and reducing our input data.</p>
<p>Let's see in detail what happens when we process our data with the Hadoop MapReduce framework from the preceding diagram:</p>
<ul>
<li style="font-weight: 400">Our input data is broken down into pieces</li>
<li style="font-weight: 400">Each piece of the data is fed to a mapper program</li>
<li style="font-weight: 400">Outputs from all the mapper programs are collected, shuffled, and sorted</li>
<li style="font-weight: 400">Each sorted piece is fed to the reducer program</li>
<li style="font-weight: 400">Outputs from all the reducers are combined to generate the output data</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Streaming MapReduce</h1>
                </header>
            
            <article>
                
<p>Streaming MapReduce is one of the features that is available in the Hadoop MapReduce framework, where we can use any of the external programs to act as Mapper and Reducer. As long as these programs can be executed by the target operating system, they are accepted to run the Map and Reduce tasks.</p>
<p>Here are a few things to keep in mind while writing these programs:</p>
<ul>
<li>These programs should read the input from the <kbd>STDIN</kbd></li>
<li>They should be able to process infinite amount of data (stream) or else they crash</li>
<li>The memory requirements of these programs should be known well ahead of time before they are used in the streaming MapReduce, or else we might see unpredictable behavior</li>
</ul>
<p><span>In the previous section, we have written simple Perl scripts to do mapping and reduction. In the current scenario also, we will use the same programs to understand how they perform our task.</span></p>
<div class="packt_infobox">If you observe carefully, <kbd>map.pl</kbd> can process infinite amounts of data and will not have any memory overhead. But the <kbd>reduce.pl</kbd> program uses the Perl Hash data structure to perform the reduction operation. Here, we might face some memory pressure with real-world data.</div>
<p class="mce-root">In this exercise, we use randomized input data as shown here:</p>
<pre>[user@node-3 ~]$ cat ./input.txt<br/> Bangalore,Onion,60<br/> NewDelhi,Onion,80<br/> Bangalore,Pizza,120<br/> Bangalore,Burger,80<br/> Kolkata,Onion,90<br/> Kolkata,Pizza,120<br/> Kolkata,Chilli,20<br/> NewDelhi,Chilli,30<br/> NewDelhi,Burger,180<br/> Kolkata,Burger,160<br/> NewDelhi,Pizza,150<br/> Bangalore,Chilli,10</pre>
<p>Later, we need to copy the mapper and reducer scripts to all the Hadoop nodes:</p>
<div class="packt_tip packt_infobox">We are using the same Hadoop cluster that's built as part of <a href="220c9e01-7416-4692-8de7-02f6b4373ac5.xhtml" target="_blank">Chapter 10</a>, <em>Production Hadoop Cluster Deployment</em> for this exercise. If you remember, the nodes are master, <kbd>node-1</kbd>, <kbd>node-2</kbd>, and <kbd>node-3</kbd>.</div>
<pre>[user@master ~]$ scp *.pl node-1:~<br/>[user@master ~]$ scp *.pl node-2:~<br/>[user@master ~]$ scp *.pl node-3:~</pre>
<p>In this step, we are copying the input to the <kbd>hadoop /tmp/ directory</kbd>.</p>
<div class="packt_infobox">Please use a sensible directory in your production environments as per your enterprise standards. Here the <kbd>/tmp</kbd> directory is used for illustration purposes only.</div>
<pre>[user@node-3 ~]$ hadoop fs -put ./input.txt /tmp/</pre>
<p>In this step, we are using the Hadoop streaming MapReduce framework to use our scripts for performing the computation:</p>
<div class="packt_tip packt_infobox">The contents of the <kbd>map.pl</kbd> and <kbd>reduce.pl</kbd> are exactly the same as we have used in the previous examples.</div>
<pre>[user@node-3 ~]$ hadoop jar \<br/>    /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar \<br/>    -input hdfs:///tmp/input.txt \<br/>    -output hdfs:///tmp/output-7 \<br/>    -mapper $(pwd)/map.pl \<br/>    -reducer $(pwd)/reduce.pl</pre>
<p>The output is stored in HDFS, which we can view like this:</p>
<pre>[user@node-3 ~]$ hadoop fs -cat /tmp/output-7/part*<br/> NewDelhi, 440<br/> Kolkata, 390<br/> Bangalore, 270<br/>[user@node-3 ~]$</pre>
<p>If we observe carefully, the results match exactly with our traditional program.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Java MapReduce</h1>
                </header>
            
            <article>
                
<p><span>In the previous section, we have seen how to use any arbitrary programming language to run a MapReduce operation on Hadoop. But in most practical scenarios, it's good if we leverage the libraries provided by the Hadoop MapReduce infrastructure as they are powerful and take care of many requirements for us.</span></p>
<p>Let's try to write a simple Java program using the MapReduce libraries and see whether we can generate the same output as in the previous exercises. In this example, we will use the official MapReduce implementation from the official docs.</p>
<p><span>Documents at: </span><a href="https://hadoop.apache.org/docs/r2.8.0/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html">https://hadoop.apache.org/docs/r2.8.0/hadoop-mapreduce-client/hadoop-mapreduce-client-core/MapReduceTutorial.html</a></p>
<p>Since our input is very different from the example, and we <span>also</span><span> </span><span>want to find the Total price of all products in a given city, we have to change the mapper program as per our CSV <kbd>input.txt</kbd> file. The reduce function is the same as the one in the official documents where our mapper function generates a <kbd>&lt;City, Price&gt;</kbd> pair. This can <span>easily </span>be consumed by the existing implementation.</span></p>
<p>We have called our program <kbd>TotalPrice.java</kbd>. <span>Let's see how our source code looks: </span></p>
<pre>[user@node-3 ~]$ cat TotalPrice.java <br/>import java.io.IOException;<br/>import java.util.StringTokenizer;<br/><br/>import org.apache.hadoop.conf.Configuration;<br/>import org.apache.hadoop.fs.Path;<br/>import org.apache.hadoop.io.IntWritable;<br/>import org.apache.hadoop.io.Text;<br/>import org.apache.hadoop.mapreduce.Job;<br/>import org.apache.hadoop.mapreduce.Mapper;<br/>import org.apache.hadoop.mapreduce.Reducer;<br/>import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;<br/>import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;<br/><br/>public class TotalPrice {<br/>  public static class TokenizerMapper extends Mapper&lt;Object, Text, Text, IntWritable&gt;{<br/>    public void map(Object key, Text value, Context context) throws IOException, InterruptedException {<br/>      StringTokenizer itr = new StringTokenizer(value.toString(), ",");<br/>      Text city = new Text(itr.nextToken());<br/>      itr.nextToken();<br/>      IntWritable price = new IntWritable(Integer.parseInt(itr.nextToken()));<br/>      context.write(city, price);<br/>    }<br/>  }<br/><br/>  public static class IntSumReducer extends Reducer&lt;Text,IntWritable,Text,IntWritable&gt; {</pre>
<pre>  private IntWritable result = new IntWritable();<br/><br/>    public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException {<br/>      int sum = 0;<br/>      for (IntWritable val : values) {<br/>        sum += val.get();<br/>      }<br/>      result.set(sum);<br/>      context.write(key, result);<br/>    }<br/>  }<br/><br/>  public static void main(String[] args) throws Exception {<br/>    Configuration conf = new Configuration();<br/>    Job job = Job.getInstance(conf, "TotalPriceCalculator");<br/>    job.setJarByClass(TotalPrice.class);<br/>    job.setMapperClass(TokenizerMapper.class);<br/>    job.setCombinerClass(IntSumReducer.class);<br/>    job.setReducerClass(IntSumReducer.class);<br/>    job.setOutputKeyClass(Text.class);<br/>    job.setOutputValueClass(IntWritable.class);<br/>    FileInputFormat.addInputPath(job, new Path(args[0]));<br/>    FileOutputFormat.setOutputPath(job, new Path(args[1]));<br/>    System.exit(job.waitForCompletion(true) ? 0 : 1);<br/>  }<br/>}</pre>
<p>Once we have the source code, we need to compile it to create a <strong>Java Archive</strong> (<span><strong>JAR</strong>) f</span>ile. It’s done in the following manner:</p>
<pre> [user@node-3 ~]$ javac -cp `hadoop classpath` TotalPrice.java <br/> [user@node-3 ~]$ jar cf tp.jar TotalPrice*.class</pre>
<p>Once we have the JAR file created, we can use the Hadoop command to submit the job to process the <kbd>input.txt</kbd>, and produce the output in the <kbd>/tmp/output-12</kbd> directory:</p>
<div class="packt_tip">As in the case of streaming MapReduce, we need not copy the source to all the Hadoop servers.</div>
<pre> [user@node-3 ~]$ hadoop jar tp.jar TotalPrice /tmp/input.txt /tmp/output-12</pre>
<p>This run should go through fine and will produce the output files in the <kbd>/tmp/output-12</kbd> directory. We can see the contents of the output using this command:</p>
<pre>[user@node-3 ~]$ hadoop fs -cat /tmp/output-12/part*<br/>Bangalore       270<br/>Kolkata 390<br/>NewDelhi        440</pre>
<p>This exactly matches with the previous runs as well.</p>
<p>As we can see, the Hadoop Mapreduce framework has taken all the necessary steps to make sure that the entire pipeline progress is kept within its control, giving us the desired result.</p>
<p>Even though we have used a very simple dataset for our computation, Hadoop Mapreduce makes sure that, regardless of the size of data we are dealing with, the same program we have written before yields the results we are looking for. This makes it a very powerful architecture for batch jobs.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>So far, we have seen that Hadoop Mapreduce is a powerful framework that offers both streaming and batch modes of operation to process vast amounts of data with very simple instructions. Even though Mapreduce was originally the choice of computation framework in Hadoop, it has failed to meet the ever-changing demands of the market, and new architectures were developed to address those concerns. We will learn about one such framework called <strong>Apache Spark</strong> in the next section. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Apache Spark 2</h1>
                </header>
            
            <article>
                
<p>Apache Spark is a general-purpose cluster computing system. It's very well suited for large-scale data processing. It performs 100 times better than Hadoop when run completely in-memory and 10 times better when run entirely from disk. It has a sophisticated directed acyclic graph execution engine that supports an acyclic data flow model.</p>
<p>Apache Spark has first-class support for writing programs in Java, Scala, Python, and R programming languages to cater to a wider audience. It offers more than 80 different operators to build parallel applications without worrying about the underlying infrastructure.</p>
<p>Apache Spark has libraries catering to <strong>Structured Query Language</strong>, known as Spark <strong>SQL</strong>; this supports writing queries in programs using ANSI SQL. It also has support for computing streaming data, which is very much needed in today's real-time data processing requirements such as powering dashboards for interactive user experience systems. Apache Spark also has <strong>machine learning libraries</strong> such as <strong>Mlib</strong>, which caters to running scientific programs. Then it has support for writing programs for data that follows graph data structures, known as <strong>GraphX</strong>. This makes it a really powerful framework that supports most advanced ways of computing.</p>
<p>Apache Spark runs not only on the Hadoop platform but also on a variety of systems, such as Apache Mesos, Kubernetes, Standalone, or the Cloud. This makes it a perfect choice for today's enterprise to chose the way it wants to leverage the power of this system.</p>
<p>In the coming sections, we will learn more about Spark and its ecosystem. We are using Spark 2.2.0 for this exercise.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing Spark using Ambari</h1>
                </header>
            
            <article>
                
<p>From the previous chapter, we have an existing Ambari installation that is running. We will leverage the same installation to add Spark support. Let's see how we can accomplish this.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Service selection in Ambari Admin</h1>
                </header>
            
            <article>
                
<p>Once we log in to the Ambari Admin interface, we see the main cluster that is created. On this page, we click on the <span class="packt_screen">Actions</span> button on the left-hand-side menu. It shows a screen as follows. From this menu, we click on the <span class="packt_screen">Add Service</span> option:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/8100d26e-c7bb-41c3-bdad-fe6a1cf66d35.png" style="width:16.58em;height:23.92em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Add Service Wizard</h1>
                </header>
            
            <article>
                
<p>Once we click on the <span class="packt_screen">Add Service</span> menu item, we are shown a Wizard, where we have to select <span class="packt_screen">Spark 2</span> from the list of all supported services in Ambari. The screen looks like this:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/c122cc80-ffbe-4ae3-9d88-cb240afc89f1.png"/></div>
<p>Click on the <span class="packt_screen">Next</span> button when the service selection is complete.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Server placement</h1>
                </header>
            
            <article>
                
<p>Once the <span class="packt_screen">Spark 2</span> service is selected, other dependent services are also automatically selected for us and we are given a choice to select the placement of the master servers. I have left the default selection as is:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/0ded1e01-9c31-48ea-b1dd-194d6846a4f5.png"/></div>
<p>Click on the <span class="packt_screen">Next</span> button when the changes look good.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Clients and Slaves selection</h1>
                </header>
            
            <article>
                
<p>In this step, we are given a choice to select the list of nodes that act as clients for the masters we have selected in the previous step. We can also select the list of servers on which we can install the client utilities. Make the selection as per your choice:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/681a2f28-675b-4600-b27f-5b4ce2c35997.png"/></div>
<p>Click on the <span class="packt_screen">Next</span> button when the changes are done.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Service customization</h1>
                </header>
            
            <article>
                
<p>Since Hive is also getting installed as part of the <span class="packt_screen">Spark 2</span> selection, we are given a choice to customize the details of the Hive datasource. I have created the database on the master node with the username as <kbd>hive</kbd>, password as <kbd>hive</kbd>, and the database also as <kbd>hive</kbd>. Please choose a strong password while making changes in production.</p>
<p>The customization screen looks like this:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e4fe5fb6-ff5d-4579-b454-50761a0b5c74.png"/></div>
<p>Click on <span class="packt_screen">Next</span> once the changes are done correctly.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Software deployment</h1>
                </header>
            
            <article>
                
<p><span>In this screen, we are shown a summary of the selections we have made so far. Click on <span class="packt_screen">Deploy</span> to start deploying the Spark 2 software on the selected servers. We can always cancel the wizard and start over again in this step if we feel that we have missed any</span> customization<span>:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/631201aa-2cf2-464f-b74c-f862277555ba.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Spark installation progress</h1>
                </header>
            
            <article>
                
<p>In this step, we are shown the progress of Spark software installation and its other dependencies. Once everything is deployed, we are shown a summary of any warnings and errors. As we can see from the following screen, there are some warnings encountered during the installation, which indicates that we need to restart a few services once the wizard is complete. Don't worry its pretty normal to see these errors. We will correct these in the coming steps to have a successfully running Spark system:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e8bcda2c-e9b8-4274-ae3d-20f4788442cd.png"/></div>
<p>Clicking on <span class="packt_screen">Complete</span> finishes the wizard.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Service restarts and cleanup</h1>
                </header>
            
            <article>
                
<p>Since there were warnings during the installation process, we have to restart all the affected components. The restart process is shown in this screen:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/bcbcd61c-c0d3-467d-bc39-02fce56366a6.png"/></div>
<p>Once we give a confirmation, all the associated services will be restarted and we will have a successfully running system.</p>
<p>This finishes the installation of Spark 2 on an existing Hadoop cluster managed by Ambari. We will now learn more about various data structures and libraries in Spark in the coming sections.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Apache Spark data structures</h1>
                </header>
            
            <article>
                
<p>Even though Mapreduce provides a powerful way to process large amounts of data, it is restricted due to several drawbacks:</p>
<ul>
<li>Lack of support for variety of operators</li>
<li>Real-time data processing</li>
<li>Caching the results of data for faster iterations</li>
</ul>
<p>This is to name a few. Since Apache Spark was built from the ground up, it has approached the big data computation problem in a very generic way and has provided the developers with data structures that makes it easier to represent any type of data and use those to compute in a better way.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">RDDs, DataFrames and datasets</h1>
                </header>
            
            <article>
                
<p>At the core of Apache Spark are distributed datasets called <strong>RDD</strong>, also known as <strong>Resilient Distributed Datasets</strong>. These are immutable datasets that are present in the cluster, which are highly available and fault tolerant. The elements in the RDD can be operated in parallel, giving a lot of power to the Spark cluster.</p>
<p>Since the data is already present in storage systems, such as HDFS, RDBMS, S3, and so on, RDDs can easily be created from these external datasources. The API also provides us with the power to create RDDs from existing in-memory data elements.</p>
<p>These RDDs do not have any pre-defined structure. So, they can assume any form and, by leveraging the different operators in the Spark library, we can write powerful programs that give us necessary results without worrying too much about the data complexities.</p>
<p>In order to cater to the RDBMS needs, DataFrames come into play where a DataFrame can be compared with a table in a relational database system. As we know, tables have rows and columns and the structure of the data is known ahead of time. By knowing the structure of the data, several optimizations can be performed during data processing.</p>
<p>Spark datasets are somewhat similar to the DataFrames. But they extend the functionality of the DataFrames by supporting semi-structured data objects with native language objects (Java and Scala). DataFrames are an immutable collection of objects with semantics of a relational schema. Since we are dealing with semi-structured data and native language objects, there is an encoder/decoder system that takes care of automatically converting between the types.</p>
<p>Here is a quick comparison chart:</p>
<table>
<tbody>
<tr>
<td><strong>Feature</strong></td>
<td><strong>RDDs</strong></td>
<td><strong>DataFrame</strong></td>
<td><strong>Dataset</strong></td>
</tr>
<tr>
<td>Data type</td>
<td>Unstructured data</td>
<td>Structured data</td>
<td>Semi-structured data</td>
</tr>
<tr>
<td>Schema requirement</td>
<td>Completely free form</td>
<td>Strict datatypes </td>
<td>Loosely coupled</td>
</tr>
<tr>
<td>Optimization provided by Spark</td>
<td>Not needed as data is unstructured</td>
<td>Leverages optimizations as datatypes are known</td>
<td>Inferred datatypes provide some level of optimization</td>
</tr>
<tr>
<td>High level expressions/filters</td>
<td>Difficult as the data form is complex in nature</td>
<td>Can leverage these as we know the data we are dealing with</td>
<td>Can leverage here too</td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Apache Spark programming</h1>
                </header>
            
            <article>
                
<p>Apache Spark has very good programming language support. It provides first-class support for Java, Scala, Python, and R programming languages. Even though the data structures and operators that are available with the programming languages are similar in nature, we have to use programming-language-specific constructs to achieve the desired logic. Throughout this chapter, we will use Python as the programming language of choice. However, Spark itself is agnostic to these programming languages and produces the same results regardless of the programming language used.</p>
<p>Apache Spark with Python can be used in two different ways. The first way is to launch the <kbd>pyspark</kbd> interactive shell, which helps us run Python instructions. The experience is similar to the Python shell utility. Another way is to write standalone programs that can be invoked using the spark-submit command. In order to use standalone Spark programs, we have to understand the basic structure of a Spark program:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/54fcd556-77c8-4ce0-a7d1-3f698c9163a2.png"/></div>
<p>The typical anatomy of a spark program consists of a main function that executes different operators on the RDDs to generate the desired result. There is support for more than 80 different types of operators in the Spark library. At a high level, we can classify these operators into two types: transformations and actions. Transformation operators convert data from one form to another. Actions generate the result from the data. In order to optimize the resources in the cluster for performance reasons, Apache Spark actually executes the programs in checkpoints. Each checkpoint is arrived at only when there is a action operator. This is one important thing to remember, especially if you are new to programming with Spark. Even the most advanced programmers sometimes get confused about why they don't see the desired result as they did not use any action operator on the data.</p>
<p>Coming back to the <span>preceding </span>diagram, we have a driver program that has main routine which performs several actions/transformations on the data thats stored in a filesystem like HDFS and gives us the desired result. We are aware that RDDs are the basic parallel datastore in the Spark programming language. Spark is intelligent enough to create these RDDs from the seed storage like HDFS and once they are created, it can cache the RDDs in Memory and also make these RDDs highly available by making them fault-tolerant. Even if the copy of the RDD goes offline due to a node crash, future access on the same RDDs will quickly be generated from the computation from which it was originally generated. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sample data for analysis</h1>
                </header>
            
            <article>
                
<p>In order to understand the programming API of spark, we should have a sample dataset on which we can perform some operations to gain confidence. In order to generate this dataset, we will import the sample table from the employees database from the previous chapter.</p>
<p>These are the instructions we follow to generate this dataset:</p>
<p>Log in to the server and switch to Hive user:</p>
<pre>ssh user@node-3<br/>[user@node-3 ~]$ sudo su - hive</pre>
<p>This will put us in a remote shell, where we can dump the table from the MySQL database:</p>
<pre>[hive@node-3 ~]$ mysql -usuperset -A -psuperset -h master employees -e "select * from vw_employee_salaries" &gt; vw_employee_salaries.tsv
[hive@node-3 ~]$ wc -l vw_employee_salaries.tsv 
2844048 vw_employee_salaries.tsv
[hive@node-3 ~]$ </pre>
<p>Next, we should copy the file to Hadoop using the following command:</p>
<pre>[hive@node-3 ~]$ hadoop fs -put ./vw_employee_salaries.tsv /user/hive/employees.csv</pre>
<p>Now, the data preparation is complete as we have successfully copied it to HDFS. We can start using this data with Spark.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Interactive data analysis with pyspark</h1>
                </header>
            
            <article>
                
<p>Apache Spark distribution comes with an interactive shell called <strong>pyspark</strong>. Since we are dealing with interpreted programming languages like Python, we can write interactive programs while learning.</p>
<p>If you remember, we have installed Spark with Apache Ambari. So we have to follow the standard directory locations of Apache Ambari to access the Spark-related binaries:</p>
<pre>[hive@node-3 ~]$ cd /usr/hdp/current/spark2-client/
[hive@node-3 spark2-client]$ ./bin/pyspark 
Python 2.7.5 (default, Aug  4 2017, 00:39:18) 
[GCC 4.8.5 20150623 (Red Hat 4.8.5-16)] on linux2
Type "help", "copyright", "credits" or "license" for more information.
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.2.0.2.6.4.0-91
      /_/

Using Python version 2.7.5 (default, Aug  4 2017 00:39:18)
SparkSession available as 'spark'.
&gt;&gt;&gt; </pre>
<p>The <span>preceding </span>steps launch the interactive Spark shell. </p>
<p>As a first step in understanding Spark's data structures, we will load the <kbd>employees.csv</kbd> file from the HDFS and count the total number of lines in the file using these instructions:</p>
<pre>&gt;&gt;&gt; ds = spark.read.text("employees.csv")
&gt;&gt;&gt; ds.count()
2844048                                                                         
&gt;&gt;&gt; </pre>
<p>As we can see, the count matches with the previous load operation on the Unix shell.</p>
<p>Now, let's try to load the first five records from the file and try to see the schema of the data structure object:</p>
<pre>&gt;&gt;&gt; ds.first()
Row(value=u'emp_no\tbirth_date\tfirst_name\tlast_name\tgender\thire_date\tsalary\tfrom_date\tto_date')
&gt;&gt;&gt; ds.head(5)
[Row(value=u'emp_no\tbirth_date\tfirst_name\tlast_name\tgender\thire_date\tsalary\tfrom_date\tto_date'), Row(value=u'10001\t1953-09-02\tGeorgi\tFacello\tM\t1986-06-26\t60117\t1986-06-26\t1987-06-26'), Row(value=u'10001\t1953-09-02\tGeorgi\tFacello\tM\t1986-06-26\t62102\t1987-06-26\t1988-06-25'), Row(value=u'10001\t1953-09-02\tGeorgi\tFacello\tM\t1986-06-26\t66074\t1988-06-25\t1989-06-25'), Row(value=u'10001\t1953-09-02\tGeorgi\tFacello\tM\t1986-06-26\t66596\t1989-06-25\t1990-06-25')]
&gt;&gt;&gt; ds.printSchema()
root
 |-- value: string (nullable = true)

&gt;&gt;&gt; </pre>
<p>As we can see, even though we have a CSV (tab separated file), Spark has read the file as a normal text file separated by newlines and the schema contains only one value, which is of string datatype.</p>
<p>In this mode of operation, where we treat each record as a line, we can perform only a few types of operations, such as counting all occurrences of a given name:</p>
<pre>&gt;&gt;&gt; ds.filter(ds.value.contains("Georgi")).count()
2323                                                                            
&gt;&gt;&gt; </pre>
<p>This mode of operations is somewhat similar to log processing. But the true power of Spark comes from the power of treating the data as a table with rows and columns, also known as <strong>DataFrames</strong>:</p>
<pre>&gt;&gt;&gt; ds = spark.read.format("csv").option("header", "true").option("delimiter", "\t").load("employees.csv")
&gt;&gt;&gt; ds.count()
2844047   </pre>
<pre>&gt;&gt;&gt; ds.show(5)
+------+----------+----------+---------+------+----------+------+----------+----------+
|emp_no|birth_date|first_name|last_name|gender| hire_date|salary| from_date|   to_date|
+------+----------+----------+---------+------+----------+------+----------+----------+
| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26| 60117|1986-06-26|1987-06-26|
| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26| 62102|1987-06-26|1988-06-25|
| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26| 66074|1988-06-25|1989-06-25|
| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26| 66596|1989-06-25|1990-06-25|
| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26| 66961|1990-06-25|1991-06-25|
+------+----------+----------+---------+------+----------+------+----------+----------+
only showing top 5 rows

&gt;&gt;&gt; </pre>
<pre>&gt;&gt;&gt; ds.printSchema()
root
 |-- emp_no: string (nullable = true)
 |-- birth_date: string (nullable = true)
 |-- first_name: string (nullable = true)
 |-- last_name: string (nullable = true)
 |-- gender: string (nullable = true)
 |-- hire_date: string (nullable = true)
 |-- salary: string (nullable = true)
 |-- from_date: string (nullable = true)
 |-- to_date: string (nullable = true)

&gt;&gt;&gt; </pre>
<p>Now, we can see that Spark has automatically converted the input CSV text into a DataFrame. But all the fields are treated as strings.</p>
<p>Let's try to use the schema inference feature of spark to automatically find the datatype of the fields:</p>
<pre>&gt;&gt;&gt; ds = spark.read.format("csv").option("header", "true").option("delimiter", "\t").option("inferSchema", "true").load("employees.csv")
18/03/25 19:21:15 WARN FileStreamSink: Error while looking for metadata directory.
18/03/25 19:21:15 WARN FileStreamSink: Error while looking for metadata directory.
&gt;&gt;&gt; ds.count()                                                                  
2844047                                                                         
&gt;&gt;&gt; ds.show(2)
+------+-------------------+----------+---------+------+-------------------+------+-------------------+-------------------+
|emp_no|         birth_date|first_name|last_name|gender|          hire_date|salary|          from_date|            to_date|
+------+-------------------+----------+---------+------+-------------------+------+-------------------+-------------------+
| 10001|1953-09-02 00:00:00|    Georgi|  Facello|     M|1986-06-26 00:00:00| 60117|1986-06-26 00:00:00|1987-06-26 00:00:00|
| 10001|1953-09-02 00:00:00|    Georgi|  Facello|     M|1986-06-26 00:00:00| 62102|1987-06-26 00:00:00|1988-06-25 00:00:00|
+------+-------------------+----------+---------+------+-------------------+------+-------------------+-------------------+
only showing top 2 rows

&gt;&gt;&gt; ds.printSchema()
root
 |-- emp_no: integer (nullable = true)
 |-- birth_date: timestamp (nullable = true)
 |-- first_name: string (nullable = true)
 |-- last_name: string (nullable = true)
 |-- gender: string (nullable = true)
 |-- hire_date: timestamp (nullable = true)
 |-- salary: integer (nullable = true)
 |-- from_date: timestamp (nullable = true)
 |-- to_date: timestamp (nullable = true)

&gt;&gt;&gt; </pre>
<p>Now we can see that all the fields have a proper datatype that is closest to the MySQL table definition.</p>
<p>We can apply simple actions on the data to see the results. Let's try to find the total male records:</p>
<pre>&gt;&gt;&gt; ds.filter(ds.gender == "M").count()
1706321 </pre>
<p>Also, try to find the male records that have more than $100K of pay:</p>
<pre>&gt;&gt;&gt; ds.filter(ds.gender == "M").filter(ds.salary &gt; 100000).count()
57317   </pre>
<p>It's so simple, right? There are many more operators that are available for exploration in the official Spark documentation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Standalone application with Spark</h1>
                </header>
            
            <article>
                
<p>In the previous section, we have seen how to use the interactive shell <kbd>pyspark</kbd> to learn the Spark Python API. In this section, we will write a simple Python program that we will run on the Spark cluster. In real-world scenarios, this is how we run our applications on the Spark cluster.</p>
<p>In order to do this, we will write a program called <kbd>MyFirstApp.py</kbd> with the following contents:</p>
<pre>[hive@node-3 ~]$ cat MyFirstApp.py 
from pyspark.sql import SparkSession

# Path to the file in HDFS
csvFile = "employees.csv"

# Create a session for this application
spark = SparkSession.builder.appName("MyFirstApp").getOrCreate()

# Read the CSV File
csvTable = spark.read.format("csv").option("header", "true").option("delimiter", "\t").load(csvFile)

# Print the total number of records in this file
print "Total records in the input : {}".format(csvTable.count())

# Stop the application
spark.stop()
[hive@node-3 ~]$ </pre>
<p>In order to run this program on the Spark cluster, we have to use the spark-submit command, which does the needful in terms of scheduling and coordinating the complete application life cycle:</p>
<pre>[hive@node-3 ~]$ /usr/hdp/current/spark2-client/bin/spark-submit ./MyFirstApp.py 2&gt;&amp;1 | grep -v -e INFO -e WARN
Total records in the input : 2844047</pre>
<p>As expected, those are the total number of records in our input file (excluding the header line).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Spark streaming application</h1>
                </header>
            
            <article>
                
<p>One of the powerful features of spark is building applications that process real-time streaming data and produce real-time results. In order to understand this more, we will write a simple application that tries to find duplicate messages in an input stream and prints all the unique messages.</p>
<p>This kind of application is helpful when we are dealing with an unreliable stream of data and we want to submit only the data that is unique.</p>
<p>The source code for this application is given here:</p>
<pre>[hive@node-3 ~]$ cat StreamingDedup.py 
from pyspark import SparkContext
from pyspark.streaming import StreamingContext

context = SparkContext(appName="StreamingDedup")
stream = StreamingContext(context, 5)

records = stream.socketTextStream("localhost", 5000)
records
    .map(lambda record: (record, 1))
    .reduceByKey(lambda x,y: x + y)
    .pprint()

ssc.start()
ssc.awaitTermination()</pre>
<p>In this application, we connect to a remote service on port <kbd>5000</kbd>, which emits the messages at its own page. The program summarizes the result of operation every 5 seconds as defined in the <kbd>StreamingContext</kbd> parameter.</p>
<p>Now, let's start a simple TCP server using the UNIX netcat command (<kbd>nc</kbd>) and a simple loop:</p>
<pre><span>for i in $(seq 1 10)<br/>do<br/>  for j in $(seq 1 5)<br/>  do<br/>   sleep 1<br/>   tail -n+$(($i * 3)) /usr/share/dict/words | head -3<br/>  done<br/>done | nc -l 5000<br/></span></pre>
<p>After this, submit our program to the spark cluster:</p>
<pre>[hive@node-3 ~]$ /usr/hdp/current/spark2-client/bin/spark-submit ./StreamingDedup.py 2&gt;&amp;1 | grep -v -e INFO -e WARN</pre>
<p>After the program starts, we see the following output:</p>
<pre>-------------------------------------------
Time: 2018-03-26 04:33:45
-------------------------------------------
(u'16-point', 5)
(u'18-point', 5)
(u'1st', 5)

-------------------------------------------
Time: 2018-03-26 04:33:50
-------------------------------------------
(u'2', 5)
(u'20-point', 5)
(u'2,4,5-t', 5)</pre>
<p>We see that every word has exactly 5 as the count, which is expected as we are printing it five times in the Unix command loop.</p>
<p>We can understand this with the help of this diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/39f75dd6-29a5-4c8e-96cb-ad8fe707d20d.png"/></div>
<p><strong>INPUT STREAM</strong> produces a continuous stream of data, which is consumed in real time by the <strong>Spark Program</strong>. After that, the results are printed by eliminating the duplicates</p>
<p>If we see this in chronological order, the data from time zero to time five seconds (<strong>T0</strong> - <strong>T5</strong>) is processed and results are available in <strong>T5</strong> time. Same thing for all other time slots.</p>
<p>In this simple example, we have just learned the basics of how Spark Streaming can be used to build real-time applications.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Spark SQL application</h1>
                </header>
            
            <article>
                
<p>When writing applications using Spark, developers have the option to use SQL on structured data to get the desired results. An example makes this easier for us to understand how to do this:</p>
<pre>[hive@node-3 ~]$ cat SQLApp.py 
from pyspark.sql import SparkSession

# Path to the file in HDFS
csvFile = "employees.csv"

# Create a session for this application
spark = SparkSession.builder.appName("SQLApp").getOrCreate()

# Read the CSV File
csvTable = spark.read.format("csv").option("header", "true").option("delimiter", "\t").load(csvFile)
csvTable.show(3)

# Create a temporary view
csvView = csvTable.createOrReplaceTempView("employees")

# Find the total salary of employees and print the highest salary makers
highPay = spark.sql("SELECT first_name, last_name, emp_no, SUM(salary) AS total FROM employees GROUP BY emp_no, first_name, last_name ORDER BY SUM(salary)")

# Generate list of records
results = highPay.rdd.map(lambda rec: "Total: {}, Emp No: {}, Full Name: {} {}".format(rec.total, rec.emp_no, rec.first_name, rec.last_name)).collect()

# Show the top 5 of them
for r in results[:5]:
    print(r)

# Stop the application
spark.stop()
[hive@node-3 ~]$ </pre>
<p>In this example, we build a DataFrame from <kbd>employees.csv</kbd> and then create a view in memory called <strong>employees</strong>. Later, we can use ANSI SQL to write and execute queries to generate the necessary results.</p>
<p>Since we are interested in finding the top paid employees, the results are shown as expected:</p>
<pre>[hive@node-3 ~]$ /usr/hdp/current/spark2-client/bin/spark-submit ./SQLApp.py 2&gt;&amp;1 | grep -v -e INFO -e WARN
[rdd_10_0]
+------+----------+----------+---------+------+----------+------+----------+----------+
|emp_no|birth_date|first_name|last_name|gender| hire_date|salary| from_date|   to_date|
+------+----------+----------+---------+------+----------+------+----------+----------+
| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26| 60117|1986-06-26|1987-06-26|
| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26| 62102|1987-06-26|1988-06-25|
| 10001|1953-09-02|    Georgi|  Facello|     M|1986-06-26| 66074|1988-06-25|1989-06-25|
+------+----------+----------+---------+------+----------+------+----------+----------+
only showing top 3 rows

Total: 40000.0, Emp No: 15084, Full Name: Aloke Birke
Total: 40000.0, Emp No: 24529, Full Name: Mario Antonakopoulos
Total: 40000.0, Emp No: 30311, Full Name: Tomofumi Coombs
Total: 40000.0, Emp No: 55527, Full Name: Kellyn Ouhyoung
Total: 40000.0, Emp No: 284677, Full Name: Richara Eastman</pre>
<p>As we can see, the simplified API provided by Apache Spark makes it easier to write SQL Queries on top of CSV data (without the need for an RDBMS) to get what we are looking for.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p><span>In this chapter, you looked at the basic concepts of large-scale data processing frameworks and also learned that one of the powerful features of spark is building applications that process real-time streaming data and produce real-time results.</span></p>
<p><span>In the next few chapters, we will discuss how to build real-time data search pipelines with Elasticsearch stack.</span></p>
<p> </p>
<p> </p>


            </article>

            
        </section>
    </body></html>