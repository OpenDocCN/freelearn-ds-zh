["```py\n    $ spark-shell --driver-memory 1G\n\n    ```", "```py\n    scala> import org.apache.spark.SparkConf\n    scala> import org.apache.spark.streaming.{Seconds, StreamingContext}\n    scala> import org.apache.spark.storage.StorageLevel\n    scala> import StorageLevel._\n\n    ```", "```py\n    scala> import org.apache.spark._\n    scala> import org.apache.spark.streaming._\n    scala> import org.apache.spark.streaming.StreamingContext._\n\n    ```", "```py\n    scala> val ssc = new StreamingContext(sc, Seconds(2))\n\n    ```", "```py\n    scala> val lines = ssc.socketTextStream(\"localhost\",8585,MEMORY_ONLY)\n\n    ```", "```py\n    scala> val wordsFlatMap = lines.flatMap(_.split(\" \"))\n\n    ```", "```py\n    scala> val wordsMap = wordsFlatMap.map( w => (w,1))\n\n    ```", "```py\n    scala> val wordCount = wordsMap.reduceByKey( (a,b) => (a+b))\n\n    ```", "```py\n    scala> wordCount.print\n\n    ```", "```py\n    scala> ssc.start\n\n    ```", "```py\n    $ nc -lk 8585\n\n    ```", "```py\n    to be or not to be\n\n    ```", "```py\n    $ wget http://central.maven.org/maven2/org/apache/spark/spark-streaming-twitter_2.10/1.2.0/spark-streaming-twitter_2.10-1.2.0.jar\n    $ wget http://central.maven.org/maven2/org/twitter4j/twitter4j-stream/4.0.2/twitter4j-stream-4.0.2.jar\n    $ wget http://central.maven.org/maven2/org/twitter4j/twitter4j-core/4.0.2/twitter4j-core-4.0.2.jar\n\n    ```", "```py\n    $ spark-shell --jars spark-streaming-twitter_2.10-1.2.0.jar, twitter4j-stream-4.0.2.jar,twitter4j-core-4.0.2.jar\n\n    ```", "```py\n    scala> import org.apache.spark.streaming.twitter._\n    scala> import twitter4j.auth._\n    scala> import twitter4j.conf._\n\n    ```", "```py\n    scala> import org.apache.spark.streaming.{Seconds, StreamingContext}\n\n    ```", "```py\n    scala> import org.apache.spark._\n    scala> import org.apache.spark.streaming._\n    scala> import org.apache.spark.streaming.StreamingContext._\n\n    ```", "```py\n    scala> val ssc = new StreamingContext(sc, Seconds(10))\n\n    ```", "```py\n    scala> val cb = new ConfigurationBuilder\n    scala> cb.setDebugEnabled(true)\n    .setOAuthConsumerKey(\"FKNryYEKeCrKzGV7zuZW4EKeN\")\n    .setOAuthConsumerSecret(\"x6Y0zcTBOwVxpvekSCnGzbi3NYNrM5b8ZMZRIPI1XRC3pDyOs1\")\n     .setOAuthAccessToken(\"31548859-DHbESdk6YoghCLcfhMF88QEFDvEjxbM6Q90eoZTGl\")\n    .setOAuthAccessTokenSecret(\"wjcWPvtejZSbp9cgLejUdd6W1MJqFzm5lByUFZl1NYgrV\")\n    val auth = new OAuthAuthorization(cb.build)\n\n    ```", "```py\n    scala> val tweets = TwitterUtils.createStream(ssc,auth)\n\n    ```", "```py\n    scala> val englishTweets = tweets.filter(_.getLang()==\"en\")\n\n    ```", "```py\n    scala> val status = englishTweets.map(status => status.getText)\n\n    ```", "```py\n    scala> ssc.checkpoint(\"hdfs://localhost:9000/user/hduser/checkpoint\")\n\n    ```", "```py\n    scala> ssc.start\n    scala> ssc.awaitTermination\n\n    ```", "```py\n    scala> :paste\n    import org.apache.spark.streaming.twitter._\n    import twitter4j.auth._\n    import twitter4j.conf._\n    import org.apache.spark.streaming.{Seconds, StreamingContext}\n    import org.apache.spark._\n    import org.apache.spark.streaming._\n    import org.apache.spark.streaming.StreamingContext._\n    val ssc = new StreamingContext(sc, Seconds(10))\n    val cb = new ConfigurationBuilder\n    cb.setDebugEnabled(true).setOAuthConsumerKey(\"FKNryYEKeCrKzGV7zuZW4EKeN\")\n     .setOAuthConsumerSecret(\"x6Y0zcTBOwVxpvekSCnGzbi3NYNrM5b8ZMZRIPI1XRC3pDyOs1\")\n     .setOAuthAccessToken(\"31548859-DHbESdk6YoghCLcfhMF88QEFDvEjxbM6Q90eoZTGl\")\n     .setOAuthAccessTokenSecret(\"wjcWPvtejZSbp9cgLejUdd6W1MJqFzm5lByUFZl1NYgrV\")\n    val auth = new OAuthAuthorization(cb.build)\n    val tweets = TwitterUtils.createStream(ssc,Some(auth))\n    val englishTweets = tweets.filter(_.getLang()==\"en\")\n    val status = englishTweets.map(status => status.getText)\n    status.print\n    ssc.checkpoint(\"hdfs://localhost:9000/checkpoint\")\n    ssc.start\n    ssc.awaitTermination\n\n    ```", "```py\n    $ /opt/infoobjects/kafka/bin/zookeeper-server-start.sh /opt/infoobjects/kafka/config/zookeeper.properties\n\n    ```", "```py\n    $ /opt/infoobjects/kafka/bin/kafka-server-start.sh /opt/infoobjects/kafka/config/server.properties\n\n    ```", "```py\n    $ /opt/infoobjects/kafka/bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test\n\n    ```", "```py\n    $ wget http://central.maven.org/maven2/org/apache/spark/spark-streaming-kafka_2.10/1.2.0/spark-streaming-kafka_2.10-1.2.0.jar\n    $ wget http://central.maven.org/maven2/org/apache/kafka/kafka_2.10/0.8.1/kafka_2.10-0.8.1.jar\n    $ wget http://central.maven.org/maven2/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar\n    $ wget http://central.maven.org/maven2/com/101tec/zkclient/0.4/zkclient-0.4.jar\n\n    ```", "```py\n    $ spark-shell --jars spark-streaming-kafka_2.10-1.2.0.jar, kafka_2.10-0.8.1.jar,metrics-core-2.2.0.jar,zkclient-0.4.jar\n\n    ```", "```py\n    scala> import org.apache.spark.streaming.{Seconds, StreamingContext}\n\n    ```", "```py\n    scala> import org.apache.spark._\n    scala> import org.apache.spark.streaming._\n    scala> import org.apache.spark.streaming.StreamingContext._\n    scala> import org.apache.spark.streaming.kafka.KafkaUtils\n\n    ```", "```py\n    scala> val ssc = new StreamingContext(sc, Seconds(2))\n\n    ```", "```py\n    scala> val zkQuorum = \"localhost:2181\"\n    scala> val group = \"test-group\"\n    scala> val topics = \"test\"\n    scala> val numThreads = 1\n\n    ```", "```py\n    scala> val topicMap = topics.split(\",\").map((_,numThreads.toInt)).toMap\n\n    ```", "```py\n    scala> val lineMap = KafkaUtils.createStream(ssc, zkQuorum, group, topicMap)\n\n    ```", "```py\n    scala> val lines = lineMap.map(_._2)\n\n    ```", "```py\n    scala> val words = lines.flatMap(_.split(\" \"))\n\n    ```", "```py\n    scala> val pair = words.map( x => (x,1))\n\n    ```", "```py\n    scala> val wordCounts = pair.reduceByKeyAndWindow(_ + _, _ - _, Minutes(10), Seconds(2), 2)\n    scala> wordCounts.print\n\n    ```", "```py\n    scala> ssc.checkpoint(\"hdfs://localhost:9000/user/hduser/checkpoint\")\n\n    ```", "```py\n    scala> ssc.start\n    scala> ssc.awaitTermination\n\n    ```", "```py\n    $ /opt/infoobjects/kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test\n\n    ```", "```py\n    scala> val runningCounts = wordCounts.updateStateByKey( (values: Seq[Int], state: Option[Int]) => Some(state.sum + values.sum))\n\n    ```", "```py\n    scala> runningCounts.print\n\n    ```", "```py\n    Scala> :paste\n    import org.apache.spark.streaming.{Seconds, StreamingContext}\n     import org.apache.spark._\n     import org.apache.spark.streaming._\n     import org.apache.spark.streaming.kafka._\n     import org.apache.spark.streaming.StreamingContext._\n     val ssc = new StreamingContext(sc, Seconds(2))\n     val zkQuorum = \"localhost:2181\"\n     val group = \"test-group\"\n     val topics = \"test\"\n     val numThreads = 1\n     val topicMap = topics.split(\",\").map((_,numThreads.toInt)).toMap\n     val lineMap = KafkaUtils.createStream(ssc, zkQuorum, group, topicMap)\n     val lines = lineMap.map(_._2)\n     val words = lines.flatMap(_.split(\" \"))\n     val pairs = words.map(x => (x,1))\n     val runningCounts = pairs.updateStateByKey( (values: Seq[Int], state: Option[Int]) => Some(state.sum + values.sum))\n     runningCounts.print\n    ssc.checkpoint(\"hdfs://localhost:9000/user/hduser/checkpoint\")\n     ssc.start\n     ssc.awaitTermination\n\n    ```"]