<html><head></head><body><div class="chapter" title="Chapter&#xA0;7.&#xA0;Supervised Learning with MLlib &#x2013; Regression"><div class="titlepage"><div><div><h1 class="title"><a id="ch07"/>Chapter 7. Supervised Learning with MLlib – Regression</h1></div></div></div><p>This chapter is divided into the following recipes:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Using linear regression</li><li class="listitem" style="list-style-type: disc">Understanding the cost function</li><li class="listitem" style="list-style-type: disc">Doing linear regression with lasso</li><li class="listitem" style="list-style-type: disc">Doing ridge regression</li></ul></div><div class="section" title="Introduction"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec51"/>Introduction</h1></div></div></div><p>The following is Wikipedia's definition of supervised learning:</p><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>"Supervised learning is the machine learning task of inferring a function from labeled training data."</em></span></p></blockquote></div><p>Supervised learning <a id="id367" class="indexterm"/>has two steps:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Train the algorithm with training dataset; it is like giving questions and their answers first</li><li class="listitem" style="list-style-type: disc">Use test dataset to ask another set of questions to the trained algorithm</li></ul></div><p>There are two types of supervised learning algorithms:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Regression</strong></span>: This<a id="id368" class="indexterm"/> predicts continuous value output, <a id="id369" class="indexterm"/>such as house price.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Classification</strong></span>: This <a id="id370" class="indexterm"/>predicts discreet valued<a id="id371" class="indexterm"/> output (0 or 1) called label, such as whether an e-mail is a spam or not. Classification is not limited to two values; it can have multiple values such as marking an e-mail important, not important, urgent, and so on (0, 1, 2…).</li></ul></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note18"/>Note</h3><p>We are going to cover regression in this chapter and classification in the next.</p></div></div><p>As an example dataset for regression, we will use the recently sold house data of the City of Saratoga, CA, as a training set to train the algorithm. Once the algorithm is trained, we will ask it to predict the <a id="id372" class="indexterm"/>house price by the given size of that house. The following figure illustrates the workflow:</p><div class="mediaobject"><img src="graphics/3056_07_01.jpg" alt="Introduction"/></div><p>Hypothesis, for what it does, may sound like a misnomer here, and you may think that prediction function may be a better name, but the word hypothesis is used for historic reasons.</p><p>If we use only one <a id="id373" class="indexterm"/>feature to predict the outcome, it is called <span class="strong"><strong>bivariate analysis</strong></span>. When <a id="id374" class="indexterm"/>we have multiple features, it is called <span class="strong"><strong>multivariate analysis</strong></span>. In fact, we can have as many features as we like. One such<a id="id375" class="indexterm"/> algorithm, <span class="strong"><strong>support vector machines</strong></span> (<span class="strong"><strong>SVM</strong></span>), which we will cover in the next chapter, in fact, allows you to have an infinite number of features.</p><p>This chapter will cover how we can do supervised learning using MLlib, Spark's machine learning library.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note19"/>Note</h3><p>Mathematical explanations have been provided in as simple a way as possible, but you can feel free to skip math and directly go to <span class="emphasis"><em>How to do it...</em></span> section.</p></div></div></div></div>
<div class="section" title="Using linear regression"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec52"/>Using linear regression</h1></div></div></div><p>Linear regression is the <a id="id376" class="indexterm"/>approach to model the value of a response variable <span class="emphasis"><em>y</em></span>, based on one or more predictor variables or feature <span class="emphasis"><em>x</em></span>.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec76"/>Getting ready</h2></div></div></div><p>Let's use some<a id="id377" class="indexterm"/> housing data to <a id="id378" class="indexterm"/>predict the price of a house based on its size. The following are the sizes and prices of houses in the City of Saratoga, CA, in early 2014:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>House size (sq ft)</p>
</th><th style="text-align: left" valign="bottom">
<p>Price</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>2100</p>
</td><td style="text-align: left" valign="top">
<p>$ 1,620,000</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>2300</p>
</td><td style="text-align: left" valign="top">
<p>$ 1,690,000</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>2046</p>
</td><td style="text-align: left" valign="top">
<p>$ 1,400,000</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>4314</p>
</td><td style="text-align: left" valign="top">
<p>$ 2,000,000</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>1244</p>
</td><td style="text-align: left" valign="top">
<p>$ 1,060,000</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>4608</p>
</td><td style="text-align: left" valign="top">
<p>$ 3,830,000</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>2173</p>
</td><td style="text-align: left" valign="top">
<p>$ 1,230,000</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>2750</p>
</td><td style="text-align: left" valign="top">
<p>$ 2,400,000</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>4010</p>
</td><td style="text-align: left" valign="top">
<p>$ 3,380,000</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>1959</p>
</td><td style="text-align: left" valign="top">
<p>$ 1,480,000</p>
</td></tr></tbody></table></div><p>Here's a <a id="id379" class="indexterm"/>graphical representation of the same:</p><div class="mediaobject"><img src="graphics/3056_07_04.jpg" alt="Getting ready"/></div></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec77"/>How to do it…</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Start the <a id="id380" class="indexterm"/>Spark shell:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell</strong></span>
</pre></div></li><li class="listitem">Import the statistics and related classes:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.linalg.Vectors</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.regression.LabeledPoint</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.regression.LinearRegressionWithSGD</strong></span>
</pre></div></li><li class="listitem">Create the <code class="literal">LabeledPoint</code> array with the house price as the label:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val points = Array(</strong></span>
<span class="strong"><strong>LabeledPoint(1620000,Vectors.dense(2100)),</strong></span>
<span class="strong"><strong>LabeledPoint(1690000,Vectors.dense(2300)),</strong></span>
<span class="strong"><strong>LabeledPoint(1400000,Vectors.dense(2046)),</strong></span>
<span class="strong"><strong>LabeledPoint(2000000,Vectors.dense(4314)),</strong></span>
<span class="strong"><strong>LabeledPoint(1060000,Vectors.dense(1244)),</strong></span>
<span class="strong"><strong>LabeledPoint(3830000,Vectors.dense(4608)),</strong></span>
<span class="strong"><strong>LabeledPoint(1230000,Vectors.dense(2173)),</strong></span>
<span class="strong"><strong>LabeledPoint(2400000,Vectors.dense(2750)),</strong></span>
<span class="strong"><strong>LabeledPoint(3380000,Vectors.dense(4010)),</strong></span>
<span class="strong"><strong>LabeledPoint(1480000,Vectors.dense(1959))</strong></span>
<span class="strong"><strong>)</strong></span>
</pre></div></li><li class="listitem">Create an RDD of the preceding data:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val pricesRDD = sc.parallelize(points)</strong></span>
</pre></div></li><li class="listitem">Train a model using this data using 100 iterations. Here, step size has been kept small to account for very large values of response variables, that is, the house price. The fourth parameter is a fraction of the dataset to use per iteration, and the last parameter is the initial set of weights to be used (weights of different features):<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val model = LinearRegressionWithSGD.train(pricesRDD,100,0.0000006,1.0,Vectors.zeros(1))</strong></span>
</pre></div></li><li class="listitem">Predict the price for a house with 2,500 sq ft:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val prediction = model.predict(Vectors.dense(2500))</strong></span>
</pre></div></li></ol></div><p>House size is just one predictor variable. House price depends upon other variables, such as lot size, age of the house, and so on. The more variables you have, the better your prediction will be.</p></div></div>
<div class="section" title="Understanding cost function"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec53"/>Understanding cost function</h1></div></div></div><p>Cost function or loss function is a very important function in machine learning algorithms. Most<a id="id381" class="indexterm"/> algorithms have some form of cost function and the goal is to minimize that. Parameters, which affect cost function, such as <code class="literal">stepSize</code> in the last recipe, need to be set by hand. Therefore, understanding the whole concept of cost function is very important.</p><p>In this recipe, we <a id="id382" class="indexterm"/>are going to analyze cost function for linear regression. Linear regression is a simple algorithm to understand and it will help readers<a id="id383" class="indexterm"/> understand the role of cost functions for<a id="id384" class="indexterm"/> even complex algorithms.</p><p>Let's go back to linear regression. The goal is to find the best-fitting line so that the mean square of error is minimum. Here, we are referring error as the difference between the value as per the best-fitting line and the actual value of the response variable for the training dataset.</p><p>For a simple case of single predicate variable, the best-fit line can be written as:</p><div class="mediaobject"><img src="graphics/3056_07_02.jpg" alt="Understanding cost function"/></div><p>This function is<a id="id385" class="indexterm"/> also called <span class="strong"><strong>hypothesis function</strong></span>, and can be written as:</p><div class="mediaobject"><img src="graphics/3056_07_03.jpg" alt="Understanding cost function"/></div><p>The goal of the linear regression is to find the best-fit line. On this line, θ<sub>0</sub> represents intercept on the <span class="emphasis"><em>y</em></span> axis and θ<sub>1</sub> represents the slope of the line as obvious from the following equation:</p><div class="mediaobject"><img src="graphics/3056_07_05.jpg" alt="Understanding cost function"/></div><p>We have to choose θ<sub>0</sub> and θ<sub>1</sub> in a way that <span class="emphasis"><em>h(x)</em></span> is closest to <span class="emphasis"><em>y</em></span> for the training dataset. So, for the <span class="emphasis"><em>i</em></span>
<sup>th</sup> data point, the square of distance between the line and data point is:</p><div class="mediaobject"><img src="graphics/3056_07_06.jpg" alt="Understanding cost function"/></div><p>To put it in words, this is the square of the difference between the predicted house price and the<a id="id386" class="indexterm"/> actual price the house got sold for. Now, let's<a id="id387" class="indexterm"/> take average of this value across the training dataset:</p><div class="mediaobject"><img src="graphics/3056_07_07.jpg" alt="Understanding cost function"/></div><p>The preceding equation is called the cost function <span class="emphasis"><em>J</em></span> for linear regression. The goal is to minimize this cost function.</p><div class="mediaobject"><img src="graphics/3056_07_08.jpg" alt="Understanding cost function"/></div><p>This cost function is also called <span class="strong"><strong>squared error function</strong></span>. Both θ<sub>0</sub> and theta θ<sub>1</sub> follow convex curve<a id="id388" class="indexterm"/> independently if they are plotted against <span class="emphasis"><em>J</em></span>.</p><p>Let's take a very simple example of dataset of three values, (1,1), (2,2), and (3,3), to make the calculation easy:</p><div class="mediaobject"><img src="graphics/3056_07_09.jpg" alt="Understanding cost function"/></div><p>Let's assume θ<sub>1</sub> is 0, that is, the best-fit line parallel to the <span class="emphasis"><em>x</em></span> axis. In the first case, assume that the best-fit line is the <span class="emphasis"><em>x</em></span> axis, that is, <span class="emphasis"><em>y=0</em></span>. The following will be the value of the cost function:</p><div class="mediaobject"><img src="graphics/3056_07_10.jpg" alt="Understanding cost function"/></div><p>Now, let's <a id="id389" class="indexterm"/>move this line slightly up to <span class="emphasis"><em>y=1</em></span>. The<a id="id390" class="indexterm"/> following will be the value of the cost function:</p><div class="mediaobject"><img src="graphics/3056_07_11.jpg" alt="Understanding cost function"/></div><p>Now, let's move this line further up to <span class="emphasis"><em>y=2</em></span>. Then, the following will be the value of the cost function:</p><div class="mediaobject"><img src="graphics/3056_07_12.jpg" alt="Understanding cost function"/></div><p>Now, when we move this line further up to <span class="emphasis"><em>y=3</em></span>, the following will be the value of the cost function:</p><div class="mediaobject"><img src="graphics/3056_07_13.jpg" alt="Understanding cost function"/></div><p>Now, let's <a id="id391" class="indexterm"/>move this line further up to <span class="emphasis"><em>y=4</em></span>. The<a id="id392" class="indexterm"/> following will be the value of the cost function:</p><div class="mediaobject"><img src="graphics/3056_07_14.jpg" alt="Understanding cost function"/></div><p>So, you saw that the cost function value first decreased, and then increased again like this:</p><div class="mediaobject"><img src="graphics/3056_07_15.jpg" alt="Understanding cost function"/></div><p>Now, let's<a id="id393" class="indexterm"/> repeat the exercise by making θ<sub>0</sub> 0 and using different values of θ<sub>1</sub>.</p><p>In the<a id="id394" class="indexterm"/> first case, assume the best-fit line is the <span class="emphasis"><em>x</em></span> axis, that is, <span class="emphasis"><em>y=0</em></span>. The following will be the value of the cost function:</p><div class="mediaobject"><img src="graphics/3056_07_16.jpg" alt="Understanding cost function"/></div><p>Now, let's use a slope of 0.5. The following will be the value of the cost function:</p><div class="mediaobject"><img src="graphics/3056_07_17.jpg" alt="Understanding cost function"/></div><p>Now, let's<a id="id395" class="indexterm"/> use a slope of 1. The following<a id="id396" class="indexterm"/> will be the value of the cost function:</p><div class="mediaobject"><img src="graphics/3056_07_18.jpg" alt="Understanding cost function"/></div><p>Now, when we use a slope of 1.5, the following will be the value of the cost function:</p><div class="mediaobject"><img src="graphics/3056_07_19.jpg" alt="Understanding cost function"/></div><p>Now, let's use a slope of 2.0. The following will be the value of the cost function:</p><div class="mediaobject"><img src="graphics/3056_07_20.jpg" alt="Understanding cost function"/></div><p>As you <a id="id397" class="indexterm"/>can see in both the graphs, the minimum <a id="id398" class="indexterm"/>value of <span class="emphasis"><em>J</em></span> is when slope or gradient of curve is 0.</p><div class="mediaobject"><img src="graphics/3056_07_21.jpg" alt="Understanding cost function"/></div><p>When both θ<sub>0</sub> and θ<sub>1</sub> are mapped to a 3D space, it becomes like the shape of a bowl with the minimum value of the cost function being at the bottom of it.</p><p>This approach to arrive<a id="id399" class="indexterm"/> at this minimum is called <span class="strong"><strong>gradient descent</strong></span>. In Spark, the implementation is stochastic gradient descent.</p></div>
<div class="section" title="Doing linear regression with lasso"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec54"/>Doing linear regression with lasso</h1></div></div></div><p>The lasso<a id="id400" class="indexterm"/> is a shrinkage and selection method for linear regression. It minimizes<a id="id401" class="indexterm"/> the usual sum of squared errors, with <a id="id402" class="indexterm"/>a bound on the sum of the absolute values <a id="id403" class="indexterm"/>of the coefficients. It is based on the original lasso paper found at <a class="ulink" href="http://statweb.stanford.edu/~tibs/lasso/lasso.pdf">http://statweb.stanford.edu/~tibs/lasso/lasso.pdf</a>.</p><p>The least square <a id="id404" class="indexterm"/>method we used in the last recipe is also called <span class="strong"><strong>ordinary least squares</strong></span> (<span class="strong"><strong>OLS</strong></span>). OLS has two challenges:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Prediction accuracy</strong></span>: Predictions made using OLS usually have low forecast bias and high <a id="id405" class="indexterm"/>variance. Prediction accuracy can be improved by shrinking some coefficients (or even making them zero). There will be some increase in bias, but overall prediction accuracy will improve.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Interpretation</strong></span>: With a<a id="id406" class="indexterm"/> large number of predictors, it is desirable to find a subset of them that exhibits the strongest effect (correlation).</li></ul></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note20"/>Note</h3><p>Bias versus variance</p><p>There are<a id="id407" class="indexterm"/> two primary reasons behind prediction error: bias and variance. The best way to understand bias and variance is to look at a case <a id="id408" class="indexterm"/>where we are doing predictions on the same dataset multiple times.</p><p>Bias is an<a id="id409" class="indexterm"/> estimate of how far the predicted results are from the actual values, <a id="id410" class="indexterm"/>and variance is an estimate of the difference in predicted values among different predictions.</p><p>Generally, adding more features helps to reduce bias, as can easily be understood. If, in building a prediction model, we have left out some features with significant correlation, it would lead to significant error.</p><p>If your model has high variance, you can remove features to reduce it. A bigger dataset also helps to reduce variance.</p></div></div><p>Here, we<a id="id411" class="indexterm"/> are going to use a simple dataset, which<a id="id412" class="indexterm"/> is ill-posed. An ill-posed dataset is a dataset where the sample data size is smaller than the number of predictors.</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>y</p>
</th><th style="text-align: left" valign="bottom">
<p>x0</p>
</th><th style="text-align: left" valign="bottom">
<p>x1</p>
</th><th style="text-align: left" valign="bottom">
<p>x2</p>
</th><th style="text-align: left" valign="bottom">
<p>x3</p>
</th><th style="text-align: left" valign="bottom">
<p>x4</p>
</th><th style="text-align: left" valign="bottom">
<p>x5</p>
</th><th style="text-align: left" valign="bottom">
<p>x6</p>
</th><th style="text-align: left" valign="bottom">
<p>x7</p>
</th><th style="text-align: left" valign="bottom">
<p>x8</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>5</p>
</td><td style="text-align: left" valign="top">
<p>3</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>2</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>3</p>
</td><td style="text-align: left" valign="top">
<p>2</p>
</td><td style="text-align: left" valign="top">
<p>2</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>2</p>
</td><td style="text-align: left" valign="top">
<p>9</p>
</td><td style="text-align: left" valign="top">
<p>8</p>
</td><td style="text-align: left" valign="top">
<p>8</p>
</td><td style="text-align: left" valign="top">
<p>9</p>
</td><td style="text-align: left" valign="top">
<p>7</p>
</td><td style="text-align: left" valign="top">
<p>9</p>
</td><td style="text-align: left" valign="top">
<p>8</p>
</td><td style="text-align: left" valign="top">
<p>7</p>
</td><td style="text-align: left" valign="top">
<p>9</p>
</td></tr></tbody></table></div><p>You can easily guess that, here, out of nine predictors, only two have a strong correlation with <span class="emphasis"><em>y</em></span>, that is, <span class="emphasis"><em>x0</em></span> and <span class="emphasis"><em>x1</em></span>. We will use this dataset with the lasso algorithm to see its validity.</p><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec78"/>How to do it…</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Start the Spark shell:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell</strong></span>
</pre></div></li><li class="listitem">Import the statistics and related classes:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.linalg.Vectors</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.regression.LabeledPoint</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.regression.LassoWithSGD</strong></span>
</pre></div></li><li class="listitem">Create the <code class="literal">LabeledPoint</code> array with the house price as the label:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val points = Array(</strong></span>
<span class="strong"><strong>LabeledPoint(1,Vectors.dense(5,3,1,2,1,3,2,2,1)),</strong></span>
<span class="strong"><strong>LabeledPoint(2,Vectors.dense(9,8,8,9,7,9,8,7,9))</strong></span>
<span class="strong"><strong>)</strong></span>
</pre></div></li><li class="listitem">Create an RDD of the preceding data:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val rdd = sc.parallelize(points)</strong></span>
</pre></div></li><li class="listitem">Train a model using this data using 100 iterations. Here, the step size and regularization parameter have been set by hand:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val model = LassoWithSGD.train(rdd,100,0.02,2.0)</strong></span>
</pre></div></li><li class="listitem">Check how many predictors have their coefficients set to zero:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; model.weights</strong></span>
<span class="strong"><strong>org.apache.spark.mllib.linalg.Vector = [0.13455106581619633,0.02240732644670294,0.0,0.0,0.0,0.01360995990267153,0.0,0.0,0.0]</strong></span>
</pre></div></li></ol></div><p>As you<a id="id413" class="indexterm"/> can see, six out of nine predictors<a id="id414" class="indexterm"/> have got their coefficients set to zero. This is the primary feature of lasso: any predictor it thinks is not useful, it literally moves them out of equation by setting their coefficients to zero.</p></div></div>
<div class="section" title="Doing ridge regression"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec55"/>Doing ridge regression</h1></div></div></div><p>An alternate <a id="id415" class="indexterm"/>way to lasso to improve prediction quality is ridge regression. While in lasso, a lot of features get their coefficients set to zero and, therefore, eliminated<a id="id416" class="indexterm"/> from an equation, in ridge, predictors or features are penalized, but are never set to zero.</p><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec79"/>How to do it…</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Start the Spark shell:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell</strong></span>
</pre></div></li><li class="listitem">Import the statistics and related classes:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.linalg.Vectors</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.regression.LabeledPoint</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.regression.RidgeRegressionWithSGD</strong></span>
</pre></div></li><li class="listitem">Create the <code class="literal">LabeledPoint</code> array with the house price as the label:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val points = Array(</strong></span>
<span class="strong"><strong>LabeledPoint(1,Vectors.dense(5,3,1,2,1,3,2,2,1)),</strong></span>
<span class="strong"><strong>LabeledPoint(2,Vectors.dense(9,8,8,9,7,9,8,7,9))</strong></span>
<span class="strong"><strong>)</strong></span>
</pre></div></li><li class="listitem">Create an RDD of the preceding data:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val rdd = sc.parallelize(points)</strong></span>
</pre></div></li><li class="listitem">Train a model using this data using 100 iterations. Here, the step size and regularization parameter have been set by hand :<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val model = RidgeRegressionWithSGD.train(rdd,100,0.02,2.0)</strong></span>
</pre></div></li><li class="listitem">Check how many predictors have their coefficients set to zero:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; model.weights</strong></span>
<span class="strong"><strong>org.apache.spark.mllib.linalg.Vector = [0.049805969577244584,0.029883581746346748,0.009961193915448916,0.019922387830897833,0.009961193915448916,0.029883581746346748,0.019922387830897833,0.019922387830897833,0.009961193915448916]</strong></span>
</pre></div></li></ol></div><p>As you can <a id="id417" class="indexterm"/>see, unlike lasso, ridge regression does not assign any predictor coefficients zero, but it does make some very close to zero.</p></div></div></body></html>