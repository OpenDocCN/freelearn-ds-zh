<html><head></head><body><div class="chapter" title="Chapter&#xA0;4.&#xA0;Learning the Na&#xEF;ve Bayes Classification Using Mahout"><div class="titlepage"><div><div><h1 class="title"><a id="ch04"/>Chapter 4. Learning the Naïve Bayes Classification Using Mahout</h1></div></div></div><p>In this chapter, we will use the Naïve Bayes classification algorithm to classify a set of documents. Classifying text documents is a little tricky because of the data preparation steps involved. In this chapter, we will explore the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Conditional probability and the Bayes rule</li><li class="listitem" style="list-style-type: disc">Understanding the Naïve Bayes algorithm</li><li class="listitem" style="list-style-type: disc">Understanding terms used in text classification</li><li class="listitem" style="list-style-type: disc">Using the Naïve Bayes algorithm in Apache Mahout</li></ul></div><div class="section" title="Introducing conditional probability and the Bayes rule"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec19"/>Introducing conditional probability and the Bayes rule</h1></div></div></div><p>Before <a id="id147" class="indexterm"/>learning the Naïve Bayes algorithm, you should have an understanding of conditional probability and the Bayes rule.</p><p>In very simple terms, conditional probability is the probability that something will happen, given that something else has already happened. It is expressed as <span class="emphasis"><em>P(A/B)</em></span>, which can be read as probability of A given B, and it finds the probability of the occurrence of event A once event B has already happened.</p><p>Mathematically, it is defined as follows:</p><p> </p><div class="mediaobject"><img src="graphics/4959OS_04_01.jpg" alt="Introducing conditional probability and the Bayes rule"/></div><p>
</p><p>For example, if you choose a card from a standard card deck and if you were asked about the probability for the card to be a diamond, you would quickly say 13/52 or 0.25, as there are 13 diamond cards in the deck. However, if you then look at the card and declare that it is red, then we will have narrowed the possibilities for the card to 26 possible cards, and the probability that the card is a diamond now is 13/26 = 0.5. So, if we define A as a diamond card and B as a red card, then <span class="emphasis"><em>P(A/B)</em></span> will be the probability of the card being a diamond, given it is red.</p><p>Sometimes, for <a id="id148" class="indexterm"/>a given pair of events, conditional probability is hard to calculate, and Bayes' theorem helps us here by giving the relationship between two conditional probabilities.</p><p>Bayes' <a id="id149" class="indexterm"/>theorem is defined as follows:</p><div class="mediaobject"><img src="graphics/4959OS_04_02.jpg" alt="Introducing conditional probability and the Bayes rule"/></div><p>The terms in the formula are defined as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>P(A)</strong></span>: This is called prior probability or prior</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>P(B/A)</strong></span>: This is called conditional probability or likelihood</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>P(B)</strong></span>: This is called marginal probability</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>P(A/B)</strong></span>: This is called posterior probability or posterior</li></ul></div><p>The following formula is derived only from the conditional probability formula. We can define <span class="emphasis"><em>P(B/A)</em></span> as follows:</p><div class="mediaobject"><img src="graphics/4959OS_04_03.jpg" alt="Introducing conditional probability and the Bayes rule"/></div><p>When rearranged, the formula becomes this:</p><div class="mediaobject"><img src="graphics/4959OS_04_04.jpg" alt="Introducing conditional probability and the Bayes rule"/></div><p>Now, from the preceding conditional probability formula, we get the following:</p><div class="mediaobject"><img src="graphics/4959OS_04_05.jpg" alt="Introducing conditional probability and the Bayes rule"/></div><p>Let's take an example that will help us to understand how Bayes' theorem is applied.</p><p>A cancer test gives a positive result with a probability of 97 percent when the patient is indeed affected by cancer, while it gives a negative result with 99 percent probability when the patient is not affected by cancer. If a patient is drawn at random from a population where 0.2 <a id="id150" class="indexterm"/>percent of the individuals are affected by cancer and he or <a id="id151" class="indexterm"/>she is found to be positive, what is the probability that he or she is indeed affected by cancer? In probabilistic terms, what we know about this problem can be defined as follows:</p><p>
<span class="emphasis"><em>P (positive| cancer) = 0.97</em></span>
</p><p>
<span class="emphasis"><em>P (positive| no cancer) = 1-0.99 = 0.01</em></span>
</p><p>
<span class="emphasis"><em>P (cancer) = 0.002</em></span>
</p><p>
<span class="emphasis"><em>P (no cancer) = 1-0.002= 0.998</em></span>
</p><p>
<span class="emphasis"><em>P (positive) = P (positive| cancer) P (cancer) + P (positive| no cancer) P (no cancer)</em></span>
</p><p>
<span class="emphasis"><em>                   = 0.97*0.002 + 0.01*0.998</em></span>
</p><p>
<span class="emphasis"><em>                    = 0.01192</em></span>
</p><p>Now <span class="emphasis"><em>P (cancer| positive) = (0.97*0.002)/0.01192 = 0.1628</em></span>
</p><p>So even when found positive, the probability of the patient being affected by cancer in this example is around 16 percent.</p></div></div>
<div class="section" title="Understanding the Na&#xEF;ve Bayes algorithm"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec20"/>Understanding the Naïve Bayes algorithm</h1></div></div></div><p>In <a id="id152" class="indexterm"/>Bayes' theorem, we have seen that the outcome is based only on one evidence, but in classification problems, we have multiple evidences and we have to predict the outcome. In Naïve Bayes, we uncouple multiple pieces of evidence and treat each one of them independently. It is defined as follows:</p><p>
<span class="emphasis"><em>P (outcome | multiple Evidence) ) = P (Evidence 1|outcome)* P (Evidence 2|outcome)* P (Evidence 3|outcome) …. /P (Evidence)</em></span>
</p><p>Run this formula for each possible outcome. Since we are trying to classify, each outcome will be called a class. Our task is to look at the evidence (features) to consider how likely it is for it to be of a particular class and then assign it accordingly. The class that has the highest probability gets assigned to that combination of evidences. Let's understand this with an example.</p><p>Let's say that we have data on 1,000 pieces of fruit. They happen to be bananas, apples, or some other fruit. We are aware of three characteristics of each fruit:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Size</strong></span>: They are either long or not long</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Taste</strong></span>: They are either sweet or not sweet</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Color</strong></span>: They are either yellow or not yellow</li></ul></div><p>Assume that we <a id="id153" class="indexterm"/>have a dataset like the following:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Fruit type</p>
</th><th style="text-align: left" valign="bottom">
<p>Taste – sweet</p>
</th><th style="text-align: left" valign="bottom">
<p>Taste – not sweet</p>
</th><th style="text-align: left" valign="bottom">
<p>Color – yellow</p>
</th><th style="text-align: left" valign="bottom">
<p>Color – not yellow</p>
</th><th style="text-align: left" valign="bottom">
<p>Size – long</p>
</th><th style="text-align: left" valign="bottom">
<p>Size – not long</p>
</th><th style="text-align: left" valign="bottom">
<p>Total</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Banana</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>350</p>
</td><td style="text-align: left" valign="top">
<p>150</p>
</td><td style="text-align: left" valign="top">
<p>450</p>
</td><td style="text-align: left" valign="top">
<p>50</p>
</td><td style="text-align: left" valign="top">
<p>400</p>
</td><td style="text-align: left" valign="top">
<p>100</p>
</td><td style="text-align: left" valign="top">
<p>500</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Apple</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>150</p>
</td><td style="text-align: left" valign="top">
<p>150</p>
</td><td style="text-align: left" valign="top">
<p>100</p>
</td><td style="text-align: left" valign="top">
<p>200</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>300</p>
</td><td style="text-align: left" valign="top">
<p>300</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Other</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>150</p>
</td><td style="text-align: left" valign="top">
<p>50</p>
</td><td style="text-align: left" valign="top">
<p>50</p>
</td><td style="text-align: left" valign="top">
<p>150</p>
</td><td style="text-align: left" valign="top">
<p>100</p>
</td><td style="text-align: left" valign="top">
<p>100</p>
</td><td style="text-align: left" valign="top">
<p>200</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>
<span class="strong"><strong>Total</strong></span>
</p>
</td><td style="text-align: left" valign="top">
<p>650</p>
</td><td style="text-align: left" valign="top">
<p>350</p>
</td><td style="text-align: left" valign="top">
<p>600</p>
</td><td style="text-align: left" valign="top">
<p>400</p>
</td><td style="text-align: left" valign="top">
<p>500</p>
</td><td style="text-align: left" valign="top">
<p>500</p>
</td><td style="text-align: left" valign="top">
<p>1000</p>
</td></tr></tbody></table></div><p>Now let's look at the things we have:</p><p>
<span class="emphasis"><em>P (Banana) = 500/1000 = 0.5</em></span>
</p><p>
<span class="emphasis"><em>P (Apple) = 300/1000 = 0.3</em></span>
</p><p>
<span class="emphasis"><em>P (Other) = 200/1000 = 0.2</em></span>
</p><p>Let's look at the probability of the features:</p><p>
<span class="emphasis"><em>P (Sweet) = 650/1000 = 0.65</em></span>
</p><p>
<span class="emphasis"><em>P (Yellow) = 600/1000 = 0.6</em></span>
</p><p>
<span class="emphasis"><em>P (long) = 500/1000 = 0.5</em></span>
</p><p>
<span class="emphasis"><em>P (not Sweet) = 350/1000 = 0.35</em></span>
</p><p>
<span class="emphasis"><em>P (not yellow) = 400/1000= 0.4</em></span>
</p><p>
<span class="emphasis"><em>P (not long) = 500/1000 = 0.5</em></span>
</p><p>Now we want to know what fruit we will have if it is not yellow and not long and sweet. The probability of it being an apple is as follows:</p><p>
<span class="emphasis"><em>P (Apple| sweet, not long, not yellow) = P (sweet | Apple)* P (not long | Apple)* P (not yellow |   Apple)*P (Apple)/P (sweet)* P (not long) *P (not yellow)</em></span>
</p><p>
<span class="emphasis"><em>                                                               = 0.5*1*0.67*0.3/P (Evidence)</em></span>
</p><p>
<span class="emphasis"><em>                                                               = 0.1005/P (Evidence)</em></span>
</p><p>The probability of it being a banana is this:</p><p>
<span class="emphasis"><em>P (banana| sweet, not long, not yellow) = P (sweet | banana)* P (not long | banana)* P (not yellow | banana)*P (banana)/P (sweet)* P (not long) *P (not yellow)</em></span>
</p><p>
<span class="emphasis"><em>                                                                 = 0.7*0.2*0.1*0.5/P (Evidence)</em></span>
</p><p>
<span class="emphasis"><em>                                                                 = 0.007/P (Evidence)</em></span>
</p><p>The probability of it being any other fruit is as follows:</p><p>
<span class="emphasis"><em>P (other fruit| sweet, not long, not yellow) = P (sweet | other fruit)* P (not long | other fruit)* P (not yellow | other fruit)  *P (other fruit)/P (sweet)* P (not long) *P (not yellow)</em></span>
</p><p>
<span class="emphasis"><em>                                                                      = 0.75*0.5*0.75*0.2/P (Evidence)</em></span>
</p><p>
<span class="emphasis"><em>                                                                      = 0.05625/ P (Evidence)</em></span>
</p><p>So from the results, you <a id="id154" class="indexterm"/>can see that if the fruit is sweet, not long, and not yellow, then the highest probability is that it will be an apple. So find out the highest probability and assign the unknown item to that class.</p><p>Naïve Bayes is a very good choice for text classification. Before we move on to text classification using Naïve Bayes in Mahout, let's understand a few terms that are really useful for text classification.</p></div>
<div class="section" title="Understanding the terms used in text classification"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec21"/>Understanding the terms used in text classification</h1></div></div></div><p>To <a id="id155" class="indexterm"/>prepare data so that it can be used by a classifier is a complex process. From raw data, we can collect explanatory and target variables and encode them as <span class="strong"><strong>vectors</strong></span>, which is the input of the classifier.</p><p>Vectors are <a id="id156" class="indexterm"/>ordered lists of values as defined in two-dimensional space. You can take a clue from coordinate geometry as well. A point (3, 4) is a point in the x and y planes. In Mahout, it is different. Here, a vector can have (3, 4) or 10,000 dimensions.</p><p>Mahout provides support for creating vectors. There are two types of vector implementations in Mahout: sparse and dense vectors. There are a few terms that we need to understand for text classification:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Bag of words</strong></span>: This <a id="id157" class="indexterm"/>considers each document as a collection of words. This ignores word order, grammar, and punctuation. So, if every word is a feature, then calculating the feature value of the document word is represented as a token. It is given the value 1 if it is present or 0 if not.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Term frequency</strong></span>: This <a id="id158" class="indexterm"/>considers the word count in the document instead of 0 and 1. So the importance of a word increases with the number of times it appears in the document. Consider the following example sentence:<p>Apple has launched iPhone and it will continue to launch such products. Other competitors are also planning to launch products similar to that of iPhone.</p></li></ul></div><p>The following is the table that represents term frequency:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Term</p>
</th><th style="text-align: left" valign="bottom">
<p>Count</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Apple</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Launch</p>
</td><td style="text-align: left" valign="top">
<p>3</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>iPhone</p>
</td><td style="text-align: left" valign="top">
<p>2</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Product</p>
</td><td style="text-align: left" valign="top">
<p>2</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Plan</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td></tr></tbody></table></div><p>The following techniques are usually applied to come up with this type of table:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Stemming of words</strong></span>: With this, the suffix is removed from the word so "launched", "launches", and "<a id="id159" class="indexterm"/>launch" are all considered as "launch".</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Case normalization</strong></span>: With <a id="id160" class="indexterm"/>this, every term is converted to lowercase.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Stop word removal</strong></span>: There are some words that are almost present in every document. We call these words stop words. During an important feature extraction from a <a id="id161" class="indexterm"/>document, these words come into account and they will not be helpful in the overall calculation. Examples <a id="id162" class="indexterm"/>of these words are "is, are, the, that, and so on." So, while extracting, we will ignore these kind of words.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Inverse document frequency</strong></span>: This is considered as the boost a term gets for being rare. A term should not be too common. If a term occurs in every document, it is <a id="id163" class="indexterm"/>not good for classification. The fewer documents in which a term occurs, the more significant it is likely to be for the documents it does occur in. For a term t, inverse document frequency is calculated as follows:<p>
<span class="emphasis"><em>IDF (t) = 1 + log</em></span> (total number of documents/ number of documents containing t)</p></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Term frequency and inverse term frequency</strong></span>: This is one of the popular representations <a id="id164" class="indexterm"/>of the text. It is the product of term frequency and inverse document frequency, as follows:<p>
<span class="emphasis"><em>TFIDF (t, d) = TF (t, d) * IDF (t)</em></span>
</p></li></ul></div><p>Each document is a feature vector and a collection of documents is a set of these feature vectors and this set works as the input for the classification. Now that we understand the basic concepts behind the vector creation of text documents, let's move on to the next section where we will classify text documents using the Naïve Bayes algorithm.</p></div>
<div class="section" title="Using the Na&#xEF;ve Bayes algorithm in Apache Mahout"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec22"/>Using the Naïve Bayes algorithm in Apache Mahout</h1></div></div></div><p>We will <a id="id165" class="indexterm"/>use a dataset of 20 newsgroups for this exercise. The 20 newsgroups dataset is a standard dataset commonly used for machine learning research. The data is obtained from transcripts of several months of postings <a id="id166" class="indexterm"/>made in 20 Usenet newsgroups from the early 1990s. This dataset consists of messages, one per file. Each file begins with header lines that specify things such as who sent the message, how long it is, what kind of software was used, and the subject. A blank line follows and then the message body follows as unformatted text.</p><p>Download the <code class="literal">20news-bydate.tar.gz</code> dataset from <a class="ulink" href="http://qwone.com/~jason/20Newsgroups/">http://qwone.com/~jason/20Newsgroups/</a>. The following steps are used to build the Naïve Bayes classifier using Mahout:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Create a <code class="literal">20newsdata</code> directory and unzip the data here:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>mkdir /tmp/20newsdata</strong></span>
<span class="strong"><strong>cd /tmp/20newsdata</strong></span>
<span class="strong"><strong>tar –xzvf /tmp/20news-bydate.tar.gz</strong></span>
</pre></div></li><li class="listitem">You will see two folders under <code class="literal">20newsdata: 20news-bydate-test</code> and <code class="literal">20news-bydate-train</code>. Now create another directory called <code class="literal">20newsdataall</code> and merge both the training and test data of the 20 newsgroups.</li><li class="listitem">Come out of the directory and move to the <code class="literal">home</code> directory and execute the following:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>mkdir /tmp/20newsdataall</strong></span>
<span class="strong"><strong>cp –R /20newsdata/*/* /tmp/20newsdataall</strong></span>
</pre></div></li><li class="listitem">Create a directory in Hadoop and save this data in HDFS format:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>hadoop fs –mkdir /user/hue/20newsdata</strong></span>
<span class="strong"><strong>hadoop fs –put /tmp/20newsdataall /user/hue/20newsdata</strong></span>
</pre></div></li><li class="listitem">Convert the raw data into a sequence file. The <code class="literal">seqdirectory</code> command will generate sequence files from a directory. Sequence files are used in Hadoop. A sequence file is a flat file that consists of binary key/value pairs. We are converting the files into sequence files so that it can be processed in Hadoop, which can be done using the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>bin/mahout seqdirectory -i /user/hue/20newsdata/20newsdataall  -o /user/hue/20newsdataseq-out</strong></span>
</pre></div><p>The output of the preceding command can be seen in the following screenshot:</p><div class="mediaobject"><img src="graphics/4959OS_04_06.jpg" alt="Using the Naïve Bayes algorithm in Apache Mahout"/></div></li><li class="listitem">Convert <a id="id167" class="indexterm"/>the sequence file into a <a id="id168" class="indexterm"/>sparse vector using the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>bin/mahout seq2sparse -i /user/hue/20newsdataseq-out/part-m-00000 -o /user/hue/20newsdatavec -lnorm -nv -wt tfidf</strong></span>
</pre></div><p>The terms used in the preceding command are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">lnorm</code>: This is for the output vector to be log normalized</li><li class="listitem" style="list-style-type: disc"><code class="literal">nv</code>: This refers to named vectors</li><li class="listitem" style="list-style-type: disc"><code class="literal">wt</code>: This refers to the kind of weight to use; here, we use <code class="literal">tfidf</code></li></ul></div><p>The output of the preceding command on the console is shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/4959OS_04_07.jpg" alt="Using the Naïve Bayes algorithm in Apache Mahout"/></div></li><li class="listitem">Split the <a id="id169" class="indexterm"/>set of vectors to train and test the model:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>bin/mahout split -i /user/hue/20newsdatavec/tfidf-vectors --trainingOutput /user/hue/20newsdatatrain --testOutput /user/hue/20newsdatatest --randomSelectionPct 40 --overwrite --sequenceFiles -xm sequential</strong></span>
</pre></div><p>The terms used in the preceding command are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">randomSelectionPct</code>: This divides the percentage of data into testing and training datasets. Here, 60 percent is for testing and 40 percent for training.</li><li class="listitem" style="list-style-type: disc"><code class="literal">xm</code>: This refers to the execution method to use: sequential or mapreduce. The default is <code class="literal">mapreduce</code>.</li></ul></div><div class="mediaobject"><img src="graphics/4959OS_04_08.jpg" alt="Using the Naïve Bayes algorithm in Apache Mahout"/></div></li><li class="listitem">Now <a id="id170" class="indexterm"/>train the model:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>bin/mahout trainnb -i /user/hue/20newsdatatrain -el -o /user/hue/model -li /user/hue/labelindex -ow -c</strong></span>
</pre></div></li><li class="listitem">Test the <a id="id171" class="indexterm"/>model using the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>bin/mahout testnb -i /user/hue/20newsdatatest -m /user/hue/model/ -l  /user/hue/labelindex -ow -o /user/hue/results</strong></span>
</pre></div><p>The output of the preceding command on the console is shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/4959OS_04_09.jpg" alt="Using the Naïve Bayes algorithm in Apache Mahout"/></div></li></ol></div><p>We get the <a id="id172" class="indexterm"/>result of our Naïve Bayes classifier for <a id="id173" class="indexterm"/>the 20 newsgroups.</p></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch04lvl1sec23"/>Summary</h1></div></div></div><p>In this chapter, we discussed the Naïve Bayes algorithm. This algorithm is a simplistic yet highly regarded statistical model that is widely used in both industry and academia, and it produces good results on many occasions. We initially discussed conditional probability and the Bayes rule. We then saw an example of the Naïve Bayes algorithm. You learned about the approaches to convert text into a vector format, which is an input for classifiers. Finally, we used the 20 newsgroups dataset to build a classifier using the Naïve Bayes algorithm in Mahout. In the next chapter, we will continue our journey of exploring classification algorithms in Mahout with the Hidden Markov model implementation.</p></div></body></html>