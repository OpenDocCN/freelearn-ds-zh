- en: '*Chapter 7*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Advanced Web Scraping and Data Gathering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Make use of `requests` and `BeautifulSoup` to read various web pages and gather
    data from them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform read operations on XML files and the web using an Application Program
    Interface (API)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make use of regex techniques to scrape useful information from a large and messy
    text corpus
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, you will learn how to gather data from web pages, XML files,
    and APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The previous chapter covered how to create a successful data wrangling pipeline.
    In this chapter, we will build a real-life web scraper using all of the techniques
    that we have learned so far. This chapter builds on the foundation of `BeautifulSoup`
    and introduces various methods for scraping a web page and using an API to gather
    data.
  prefs: []
  type: TYPE_NORMAL
- en: The Basics of Web Scraping and the Beautiful Soup Library
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In today''s connected world, one of the most valued and widely used skill for
    a data wrangling professional is the ability to extract and read data from web
    pages and databases hosted on the web. Most organizations host data on the cloud
    (public or private), and the majority of web microservices these days provide
    some kind of API for the external users to access data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1: Data Wrangling HTTP request and XML/JSON reply](img/C11065_07_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.1: Data wrangling HTTP request and an XML/JSON reply'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It is necessary that, as a data wrangling engineer, you know about the structure
    of web pages and Python libraries so that you are able to extract data from a
    web page. The World Wide Web is an ever-growing, ever-changing universe, in which
    different data exchange protocols and formats are used. A few of these are widely
    used and have become standard.
  prefs: []
  type: TYPE_NORMAL
- en: Libraries in Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Python comes equipped with built-in modules, such as `urllib 3`, which that
    can place HTTP requests over the internet and receive data from the cloud. However,
    these modules operate at a lower level and require deeper knowledge of HTTP protocols,
    encoding, and requests.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will take advantage of two Python libraries in this chapter: `Requests`
    and `BeautifulSoup`. To avoid dealing with HTTP methods on a lower level, we will
    use the `Requests` library. It is an API built on top of pure Python web utility
    libraries, which makes placing HTTP requests easy and intuitive.'
  prefs: []
  type: TYPE_NORMAL
- en: '**BeautifulSoup** is one of the most popular HTML parser packages. It parses
    the HTML content you pass on and builds a detailed tree of all tags and markups
    within the page for easy and intuitive traversal. This tree can be used by a programmer
    to look for certain markup elements (for example, a table, a hyperlink, or a blob
    of text within a particular div ID) to scrape useful data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 81: Using the Requests Library to Get a Response from the Wikipedia
    Home Page'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Wikipedia home page consists of many elements and scripts, all of which
    are a mix of HTML, CSS, and JavaScript code blocks. To read the home page of Wikipedia
    and extract some useful textual information, we need to move step by step, as
    we are not interested in all of the code or markup tags; only some selected portions
    of text.
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, we will peel off the layers of HTML/CSS/JavaScript to pry
    away the information we are interested in.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the `requests` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Assign the home page URL to a variable, `wiki_home`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `get` method from the `requests` library to get a response from this
    page:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To get information about the response object, enter the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: It is a model data structure that's defined in the `requests` library.
  prefs: []
  type: TYPE_NORMAL
- en: The web is an extremely dynamic place. It is possible that the home page of
    Wikipedia will have changed by the time somebody uses your code, or that a particular
    web server will be down and your request will essentially fail. If you proceed
    to write more complex and elaborate code without checking the status of your request,
    then all that subsequent work will be fruitless.
  prefs: []
  type: TYPE_NORMAL
- en: 'A web page request generally comes back with various codes. Here are some of
    the common codes you may encounter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2: Web requests and their description](img/C11065_07_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.2: Web requests and their description'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: So, we write a function to check the code and print out messages as needed.
    These kinds of small helper/utility functions are incredibly useful for complex
    projects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 82: Checking the Status of the Web Request'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next, we will write a small utility function to check the status of the response.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by getting into the a habit of writing small functions to accomplish
    small modular tasks, instead of writing long scripts, which are hard to debug
    and track:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a `status_check` function by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that, along with printing the appropriate message, we are returning either
    1 or -1 from this function. This is important.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Check the response using the `status_check` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.3 : The output of status_check](img/C11065_07_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.3 : The output of status_check'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this chapter, we will not use these returned values, but later, for more
    complex programming activity, you will proceed only if you get one as the return
    value for this function, that is, you will write a conditional statement to check
    the return value and then execute the subsequent code based on that.
  prefs: []
  type: TYPE_NORMAL
- en: Checking the Encoding of the Web Page
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can also write a utility function to check the encoding of the web page.
    Various encodings are possible with any HTML document, although the most popular
    is UTF-8\. Some of the most popular encodings are ASCII, Unicode, and UTF-8\.
    ASCII is the simplest, but it cannot capture the complex symbols used in various
    spoken and written languages all over the world, so UTF-8 has become the almost
    universal standard in web development these days.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we run this function on the Wikipedia home page, we get back the particular
    encoding type that''s used for that page. This function, like the previous one,
    takes the `requests` response object as an argument and returns a value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Check the response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Here, UTF-8 denotes the most popular character encoding scheme that's used in
    the digital medium and on the web today. It employs variable-length encoding with
    1-4 bytes, thereby representing all Unicode characters in various languages around
    the world.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 83: Creating a Function to Decode the Contents of the Response and
    Check its Length'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The final aim of this series of steps is to get a page''s contents as a blob
    of text or as a string object that Python can process afterward. Over the internet,
    data streams move in an encoded format. Therefore, we need to decode the content
    of the response object. For this purpose, we need to perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Write a utility function to decode the contents of the response:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Check the type of the decoded object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We finally got a string object by reading the HTML page!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: Note that the answer in this chapter and in the exercise in Jupyter notebook
    may vary because of updates that have been made to the Wikipedia page.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Check the length of the object and try printing some of it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If you print the first 10,000 characters of this string, it will look some
    similar to this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.4: Output showing a mixed blob of HTML markup tags, text and element
    names, and properties](img/C11065_07_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.4: Output showing a mixed blob of HTML markup tags, text and element
    names, and properties'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Obviously, this is a mixed blob of various HTML markup tags, text, and elements
    names/properties. We cannot hope to extract meaningful information from this without
    using sophisticated functions or methods. Fortunately, the `BeautifulSoup` library
    provides such methods, and we will see how to use them next.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 84: Extracting Human-Readable Text From a BeautifulSoup Object'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It turns out that a `BeautifulSoup` object has a `text` method, which can be
    used just to extract text:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the package and then pass on the whole string (HTML content) to a method
    for parsing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Execute the following code in your notebook:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Find the type of the `txt_dmp:`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Find the length of the `txt_dmp:`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, the length of the text dump is much smaller than the raw HTML's string
    length. This is because `bs4` has parsed through the HTML and extracted only human-readable
    text for further processing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Print the initial portion of this text.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You will see something similar to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.5: Output showing the initial portion of text](img/C11065_07_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.5: Output showing the initial portion of text'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Extracting Text from a Section
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now, let''s move on to a more exciting data wrangling task. If you open the
    Wikipedia home page, you are likely to see a section called **From today''s featured
    article**. This is an excerpt from the day''s prominent article, which is randomly
    selected and promoted on the home page. In fact, this article can also change
    throughout the day:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.6: Sample Wikipedia page highlighting the “From today’s featured
    article” section](img/C11065_07_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.6: Sample Wikipedia page highlighting the "From today''s featured
    article" section'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You need to extract the text from this section. There are number of ways to
    accomplish this task. We will go through a simple and intuitive method for doing
    so here.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we try to identify two indices – the start index and end index of the
    string, which demarcate the start and end of the text we are interested in. In
    the next screenshot, the indices are shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.7: Wikipedia page highlighting the text to be extracted](img/C11065_07_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.7: Wikipedia page highlighting the text to be extracted'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The following code accomplishes the extraction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Note, that we have to add the length of the `From today's featured article`
    string to `idx1` and then pass that as the starting index. This is because idx1
    finds where the *From today's featured article* string starts, not ends.
  prefs: []
  type: TYPE_NORMAL
- en: 'It prints out something like this (this is a sample output):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.8: The extracted text](img/C11065_07_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.8: The extracted text'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Extracting Important Historical Events that Happened on Today's Date
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Next, we will try to extract the text corresponding to the important historical
    events that happened on today''s date. This can generally be found at the bottom-right
    corner as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.9: Wikipedia page highlighting the “On this day” section](img/C11065_07_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.9: Wikipedia page highlighting the "On this day" section'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'So, can we apply the same technique as we did for "**From today''s featured
    article**"? Apparently not, because there is text just below where we want our
    extraction to end, which is not fixed, unlike in the previous case. Note that,
    in the previous exercise, the fixed string "**Recently featured**" occurs at the
    exact place where we want the extraction to stop. So, we could use it in our code.
    However, we cannot do that in this case, and the reason for this is illustrated
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.10: Wikipedia page highlighting the text to be extracted](img/C11065_07_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.10: Wikipedia page highlighting the text to be extracted'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'So, in this section, we just want to find out what the text looks like around
    the main content we are interested in. For that, we must find out the start of
    the string "On this day" and print out the next 1,000 characters, using the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.11: Output of the "On this day" section from Wikipedia](img/C11065_07_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.11: Output of the "On this day" section from Wikipedia'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To address this issue, we need to think differently and use some other methods
    from BeautifulSoup (and write another utility function).
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 85: Using Advanced BS4 Techniques to Extract Relevant Text'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'HTML pages are made of many markup tags, such as <div>, which denotes a division
    of text/images, or <ul>, which denotes lists. We can take advantage of this structure
    and look at the element that contains the text we are interested in. In the Mozilla
    Firefox browser, we can easily do this by right-clicking and selecting the "**Inspect
    Element**" option:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.12: Inspecting elements on Wikipedia](img/C11065_07_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.12: Inspecting elements on Wikipedia'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As you hover over this with the mouse, you will see different portions of the
    page being highlighted. By doing this, it is easy to discover the precise block
    of markup text, that is responsible for the textual information we are interested
    in. Here, we can see that a certain `<ul>` block contains the text:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C11065_07_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.13: Identifying the HTML block that contains text'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now, it is prudent to find the `<div>` tag that contains this `<ul>` block
    within it. By looking around the same screen as before, we find the `<div>` and
    also its ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.14: The tag ul containing the text](img/C11065_07_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.14: The <ul> tag containing the text'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Use the `find_all` method from BeautifulSoup, which scans all the tags of the
    HTML page (and their sub-elements) to find and extract the text associated with
    this particular `<div>` element.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: Note how we are utilizing the 'mp-otd' ID of the <div> to identify it among
    tens of other <div> elements.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The `find_all` method returns a `NavigableString` class, which has a useful
    `text` method associated with it for extraction.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To put these ideas together, we will create an empty list and append the text
    from the `NavigableString` class to this list as we traverse the page:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, if we examine the `text_list` list, we will see that it has three elements.
    If we print the elements, separated by a marker, we will see that the text we
    are interested in appears as the first element!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: In this example, it is the first element of the list that we are interested
    in. However, the exact position will depend on the web page.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.15: The text highlighted](img/C11065_07_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.15: The text highlighted'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Exercise 86: Creating a Compact Function to Extract the "On this Day" Text
    from the Wikipedia Home Page'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we discussed before, it is always good to try to functionalize specific
    tasks, particularly in a web scraping application:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a function, whose only job is to take the URL (as a string) and to return
    the text corresponding to the **On this day** section. The benefit of such a functional
    approach is that you can call this function from any Python script and use it
    anywhere in another program as a standalone module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Extract the text from the "On this day" section on the Wikipedia home page.
    Accept the Wikipedia home page URL as a string. A default URL is provided:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Note how this function utilizes the status check and prints out an error message
    if the request failed. When we test this function with an intentionally incorrect
    URL, it behaves as expected:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Reading Data from XML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: XML, or Extensible Markup Language, is a web markup language that's similar
    to HTML but with significant flexibility (on the part of the user) built in, such
    as the ability to define your own tags. It was one of the most hyped technologies
    in the 1990s and early 2000s. It is a meta-language, that is, a language that
    allows us to define other languages using its mechanics, such as RSS, MathML (a
    mathematical markup language widely used for web publication and the display of
    math-heavy technical information), and so on. XML is also heavily used in regular
    data exchanges over the web, and as a data wrangling professional, you should
    have enough familiarity with its basic features to tap into the data flow pipeline
    whenever you need to extract data for your project.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 87: Creating an XML File and Reading XML Element Objects'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s create some random data to understand the XML data format better. Type
    in the following code snippets:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create an XML file using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This is a triple-quoted string or multiline string. If you print this object,
    you will get the following output. This is an XML-formatted data string in a tree
    structure, as we will see soon, when we parse the structure and tease apart the
    individual parts:![Figure 7.16: The XML file output](img/C11065_07_16.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 7.16: The XML file output'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To process and wrangle with the data, we have to read it as an `Element` object
    using the Python XML parser engine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Exercise 88: Finding Various Elements of Data within a Tree (Element)'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can use the `find` method to search for various pieces of useful data within
    an XML element object and print them (or use them in whatever processing code
    we want) using the `text` method. We can also use the `get` method to extract
    the specific attribute we want:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `find` method to find `Name`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `find` method to find `Surname`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `find` method to find `Phone`. Note the use of the `strip` method to
    strip away any trailing spaces/blanks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `find` method to find `email status` and `actual email`. Note the use
    of the `get` method to extract the status:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Reading from a Local XML File into an ElementTree Object
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can also read from an XML file (saved locally on disk).
  prefs: []
  type: TYPE_NORMAL
- en: This is a fairly common situation where a frontend web scraping module has already
    downloaded a lot of XML files by reading a table of data on the web and now the
    data wrangler needs to parse through this XML file to extract meaningful pieces
    of numerical and textual data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have a file associated with this chapter, called "`xml1.xml`". Please make
    sure you have the file in the same directory that you are running your Jupyter
    Notebook from:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Note how we use the `parse` method to read this XML file. This is slightly different
    than using the `fromstring` method used in the previous exercise, where we were
    directly reading from a string object. This produces an `ElementTree` object instead
    of a simple `Element`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea of building a tree-like object is the same as in the domains of computer
    science and programming:'
  prefs: []
  type: TYPE_NORMAL
- en: There is a root
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are children objects attached to the root
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There could be multiple levels, that is, children of children recursively going
    down
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of the nodes of the tree (root and children alike) have attributes attached
    to them that contain data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tree traversal algorithms can be used to search for a particular attribute
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If provided, special methods can be used to probe a node deeper
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Exercise 89: Traversing the Tree, Finding the Root, and Exploring all Child
    Nodes and their Tags and Attributes'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Every node in the XML tree has tags and attributes. The idea is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C11065_07_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.17: Finding the root and child nodes of an XML tag'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Explore these tags and attributes using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.18: The output showing the extracted XML tags](img/C11065_07_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.18: The output showing the extracted XML tags'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Remember that every XML data file could follow a different naming or structural
    format, but using an element tree approach puts the data into a somewhat structured
    flow that can be explored systematically. Still, it is best to examine the raw
    XML file structure once and understand (even if at a high level) the data format
    before attempting automatic extractions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 90: Using the `text` Method to Extract Meaningful Data'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can almost think of the XML tree as a **list of lists** and index it accordingly:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Access the element `root[0][2]` by using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: So, this points to the '`gdppc`' piece of data. Here, '`gdppc`' is the tag and
    the actual GDP/per capita data is attached to this tag.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use the `text` method to access the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `tag` method to access `gdppc`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Check `root[0]`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Check the tag:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can use the `attrib` method to access it:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: So, `root[0]` is again an element, but it has different a set of tags and attributes
    than `root[0][2]`. This is expected because they are all part of the tree as nodes,
    but each is associated with a different level of data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This last piece of code output is interesting because it returns a dictionary
    object. Therefore, we can just index it by its keys. We will do that in the next
    exercise.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting and Printing the GDP/Per Capita Information Using a Loop
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that we know how to read the GDP/per capita data and how to get a dictionary
    back from the tree, we can easily construct a simple dataset by running a loop
    over the tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: We can put these in a DataFrame or CSV file for saving to a local disk or further
    processing, such as a simple plot!
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 91: Finding All the Neighboring Countries for each Country and Printing
    Them'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we mentioned before, there are efficient search algorithms for tree structures,
    and one such method for XML trees is `findall`. We can use this, for this example,
    to find all the neighbors a country has and print them out.
  prefs: []
  type: TYPE_NORMAL
- en: 'Why do we need to use `findall` over find? Well, because not all the countries
    have an equal number of neighbors and `findall` searches for all the data with
    that tag that is associated with a particular node, and we want to traverse all
    of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The output looks something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C11065_07_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.19: The output that''s generated by using findall'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Exercise 92: A Simple Demo of Using XML Data Obtained by Web Scraping'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the last topic of this chapter, we learned about simple web scraping using
    the `requests` library. So far, we have worked with static XML data, that is,
    data from a local file or a string object we''ve scripted. Now, it is time to
    combine our learning and read XML data directly over the internet (as you are
    expected to do almost all the time):'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will try to read a cooking recipe from a website called [http://www.recipepuppy.com/](http://www.recipepuppy.com/),
    which aggregates links to various other sites with the recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This code will ask the user for input. You have to enter the name of a food
    item. For example, ''chicken tikka'':![Figure 7.20: Demo of scraping from XML
    data](img/C11065_07_20.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 7.20: Demo of scraping from XML data'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We get back data in XML format and read and decode it before creating an XML
    tree out of it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can use another useful method, called `iter`, which basically iterates
    over the nodes under an element. If we traverse the tree and extract the text,
    we get the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.21: The output that’s generated by using iter](img/C11065_07_21.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 7.21: The output that''s generated by using iter'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: We can use the find method to search for the appropriate attribute and extract
    its content. This is the reason it is important to scan through the XML data manually
    and check what attributes are used. Remember, this means scanning the raw string
    data, not the tree structure.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Print the raw string data:![Figure 7.22: The output showing the extracted href
    tags'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C11065_07_22.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.22: The output showing the extracted href tags'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Now we know what tags to search for.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Print all the hyperlinks in the XML data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note the use of `h!=None` and `t!=None`. These are difficult to expect when
    you first run this kind of code. You may get an error because some of the tags
    may return a `None` object, that is, they were empty for some reason in this XML
    data stream. This kind of situation is fairly common and cannot be anticipated
    beforehand. You have to use your Python knowledge and programming intuition to
    get around it if you receive such an error. Here, we are just checking for the
    type of the object and if it is not a `None`, then we need to extract the text
    associated with it.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The final output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.23: The output showing the final output](img/C11065_07_23.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 7.23: The output showing the final output'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Reading Data from an API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fundamentally, an API or Application Programming Interface is some kind of interface
    to a computing resource (for example, an operating system or database table),
    which has a set of exposed methods (function calls) that allow a programmer to
    access particular data or internal features of that resource.
  prefs: []
  type: TYPE_NORMAL
- en: A web API is, as the name suggests, an API over the web. Note that it is not
    a specific technology or programming framework, but an architectural concept.
    Think of an API like a fast food restaurant's customer service center. Internally,
    there are many food items, raw materials, cooking resources, and recipe management
    systems, but all you see are fixed menu items on the board and you can only interact
    through those items. It is like a port that can be accessed using an HTTP protocol
    and is able to deliver data and services if used properly.
  prefs: []
  type: TYPE_NORMAL
- en: Web APIs are extremely popular these days for all kinds of data services. In
    the very first chapter, we talked about how UC San Diego's data science team pulls
    data from Twitter feeds to analyze occurrence of forest fires. For this, they
    do not go to twitter.com and scrape the data by looking at HTML pages and text.
    Instead, they use the Twitter API, which sends this data continuously in a streaming
    format.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, it is very important for a data wrangling professional to understand
    the basics of data extraction from a web API as you are extremely likely to find
    yourself in a situation where large quantities of data must be read through an
    API interface for processing and wrangling. These days, most APIs stream data
    out in JSON format. In this chapter, we will use a free API to read some information
    about various countries around the world in JSON format and process it.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use Python''s built-in `urllib` module for this topic, along with pandas
    to make a DataFrame. So, we can import them now. We will also import Python''s
    `JSON` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Defining the Base URL (or API Endpoint)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, we need to set the base URL. When we are dealing with API microservices,
    this is often called the **API endpoint**. Therefore, look for such a phrase in
    the web service portal you are interested in and use the endpoint URL they give
    you:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: API-based microservices are extremely dynamic in nature in terms of what and
    how they offer their service and data. It can change at any time. At the time
    of this chapter planning, we found this particular API to be a nice choice for
    extracting data easily and without using authorization keys (login or special
    API keys).
  prefs: []
  type: TYPE_NORMAL
- en: For most APIs, however, you need to have your own API key. You get that by registering
    with their service. A basic usage (up to a fixed number of requests or a data
    flow limit) is often free, but after that you will be charged. To register for
    an API key, you often need to enter credit card information.
  prefs: []
  type: TYPE_NORMAL
- en: We wanted to avoid all that hassle to teach you the basics and that's why we
    chose this example, which does not require such authorization. But, depending
    on what kind of data you will encounter in your work, please be prepared to learn
    about using an API key.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 93: Defining and Testing a Function to Pull Country Data from an API'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This particular API serves basic information about countries around the world:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define a function to pull out data when we pass the name of a country as an
    argument. The crux of the operation is contained in the following two lines of
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The first line of code appends the country name as a string to the base URL
    and the second line sends a `get` request to the API endpoint. If all goes well,
    we get back the data, decode it, and read it as a JSON file. This whole exercise
    is coded in the following function, along with some error-handling code wrapped
    around the basic actions we talked about previously:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Test this function by passing some arguments. We pass a correct name and an
    erroneous name. The response is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: This is an example of rudimentary error handling. You have to think about various
    possibilities and put in such code to catch and gracefully respond to user input
    when you are building a real-life web or enterprise application.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.24: Input arguments](img/C11065_07_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.24: Input arguments'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Using the Built-In JSON Library to Read and Examine Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we have already mentioned, JSON looks a lot like a Python dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this exercise, we will use Python''s `json` module to read raw data in that
    format and see what we can process further:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: So, we get a list back when we use the `loads` method from the `json` module.
    It reads a string datatype into a list of dictionaries. In this case, we get only
    one element in the list, so we extract that and check its type to make sure it
    is a dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can quickly check the keys of the dictionary, that is the JSON data (note
    that a full screenshot is not shown here). We can see the relevant country data,
    such as calling codes, population, area, time zones, borders, and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.25: The output of dict_keys](img/C11065_07_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.25: The output of dict_keys'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Printing All the Data Elements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This task is extremely simple given that we have a dictionary at our disposal!
    All we have to do is iterate over the dictionary and print the keys/items pair
    one by one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.26: The output using dict](img/C11065_07_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.26: The output using dict'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note that the items in the dictionary are not of the same type, that is, they
    are not similar objects. Some are floating-point numbers, such as the area, many
    are simple strings, but some are lists or even lists of dictionaries!
  prefs: []
  type: TYPE_NORMAL
- en: This is fairly common with JSON data. The internal data structure of JSON can
    be arbitrarily complex and multilevel, that is, you can have a dictionary of lists
    of dictionaries of dictionaries of lists of lists…. and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It is clear, therefore, that there is no universal method or processing function
    for JSON data format, and you have to write custom loops and functions to extract
    data from such a dictionary object based on your particular needs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will write a small loop to extract the languages spoken in Switzerland.
    First, let''s examine the dictionary closely and see where the language data is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C11065_07_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.27: The tags'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: So, the data is embedded inside a list of dictionaries, which is accessed by
    a particular key of the main dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can write simple two-line code to extract this data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.28: The output showing the languages](img/C11065_07_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.28: The output showing the languages'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Using a Function that Extracts a DataFrame Containing Key Information
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here, we are interested in writing a function that can take a list of countries
    and return a pandas DataFrame with some key information:'
  prefs: []
  type: TYPE_NORMAL
- en: Capital
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Region
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sub-region
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Population
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Latitude/longitude
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Area
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gini index
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time zones
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Currencies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Languages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: This is the kind of wrapper function you are generally expected to write in
    real-life data wrangling tasks, that is, a utility function that can take a user
    argument and output a useful data structure (or a mini database type object) with
    key information extracted over the internet about the item the user is interested
    in.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We will show you the whole function first and then discuss some key points
    about it. It is a slightly complex and long piece of code. However, based on your
    Python- based data wrangling knowledge, you should be able to examine this function
    closely and understand what it is doing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The code has been truncated here. Please find the entire code at the following
    GitHub link and code bundle folder link [https://github.com/TrainingByPackt/Data-Wrangling-with-Python/blob/master/Chapter07/Exercise93-94/Chapter%207%20Topic%203%20Exercises.ipynb](https://github.com/TrainingByPackt/Data-Wrangling-with-Python/blob/master/Lesson07/Exercise93-94/Lesson%207%20Topic%203%20Exercises.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some of the key points about this function:'
  prefs: []
  type: TYPE_NORMAL
- en: It starts by building an empty dictionary of lists. This is the chosen format
    for finally passing to the pandas `DataFrame` method, which can accept such a
    format and returns a nice DataFrame with column names set to the dictionary keys'
    names.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We use the previously defined `get_country_data` function to extract data for
    each country in the user-defined list. For this, we simply iterate over the list
    and call this function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We check the output of the `get_country_data` function. If, for some reason,
    it returns a `None` object, we will know that the API reading was not successful,
    and we will print out a suitable message. Again, this is an example of an error-handling
    mechanism and you must have them in your code. Without such small error checking
    code, your application won't be robust enough for the occasional incorrect input
    or API malfunction!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For many data types, we simply extract the data from the main JSON dictionary
    and append it to the corresponding list in our data dictionary.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, for special data types, such as time zones, currencies, and languages,
    we write a special loop to extract the data without error.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We also take care of the fact that these special data types can have a variable
    length, that is, some countries may have multiple spoken languages, but most will
    have only one entry. So, we check whether the length of the list is greater than
    one and handle the data accordingly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Exercise 94: Testing the Function by Building a Small Database of Countries''
    Information'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Finally, we test this function by passing a list of country names:'
  prefs: []
  type: TYPE_NORMAL
- en: To test its robustness, we pass in an erroneous name – such as 'Turmeric' in
    this case!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'See the output… it detected that it did not get any data back for the incorrect
    entry and printed out a suitable message. The key is that, if you do not have
    the error checking and handling code in your function, then it will stop execution
    on that entry and will not return the expected mini database. To avoid this behavior,
    such error handling code is invaluable:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C11065_07_29.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 7.29: The incorrect entry highlighted'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Finally, the output is a pandas DataFrame, which is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 7.30: The data extracted correctly](img/C11065_07_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.30: The data extracted correctly'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Fundamentals of Regular Expressions (RegEx)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Reg**ular **ex**pressions or **regex** are used to identify whether a pattern
    exists in a given sequence of characters a (string) or not. They help in manipulating
    textual data, which is often a prerequisite for data science projects that involve
    text mining.'
  prefs: []
  type: TYPE_NORMAL
- en: Regex in the Context of Web Scraping
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Web pages are often full of text and while there are some methods in `BeautifulSoup`
    or XML parser to extract raw text, there is no method for the intelligent analysis
    of that text. If, as a data wrangler, you are looking for a particular piece of
    data (for example, email IDs or phone numbers in a special format), you have to
    do a lot of string manipulation on a large corpus to extract email IDs or phone
    numbers. RegEx are very powerful and save data wrangling professional a lot of
    time and effort with string manipulation because they can search for complex textual
    patterns with wildcards of an arbitrary length.
  prefs: []
  type: TYPE_NORMAL
- en: 'RegEx is like a mini-programming language in itself and common ideas are used
    not only Python, but in all widely used web app languages like JavaScript, PHP,
    Perl, and so on. The RegEx module is in-built in Python, and you can import it
    by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Exercise 95: Using the match Method to Check Whether a Pattern matches a String/Sequence'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One of the most common regex methods is `match`. This is used to check for
    an exact or partial match at a beginning of the string (by default):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the RegEx module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a string and a pattern:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Write a conditional expression to check for a match:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding code should give an affirmative answer, that is, "Matches!".
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Test this with a string that only differs in the first letter by making it
    lowercase:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Using the Compile Method to Create a Regex Program
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In a program or module, if we are making heavy use of a particular pattern,
    then it is better to use the `compile` method and create a regex program and then
    call methods on this program.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how you compile a regex program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: This code produced an `SRE.Match` object that has a `span` of (0,6) and the
    matched string of 'Python'. The span here simply denotes the start and end indices
    of the pattern that was matched. These indices may come in handy in a text mining
    program where the subsequent code uses the indices for further search or decision-making
    purposes. We will see some examples of that later.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 96: Compiling Programs to Match Objects'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Compiled objects act like functions in that they return `None` if the pattern
    does not match. Here, we are going to check that by writing a simple conditional.
    This concept will come in handy later when we write a small utility function to
    check for the type of the returned object from regex-compiled programs and act
    accordingly. We cannot be sure whether a pattern will match a given string or
    whether it will appear in a corpus of the text (if we are searching for the pattern
    anywhere within the text). Depending on the situation, we may encounter `Match`
    objects or `None` as the returned value, and we have to handle this gracefully:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the `compile` function in RegEx:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Match it with the first string:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Match it with the second string:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Exercise 97: Using Additional Parameters in Match to Check for Positional Matching'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'By default, `match` looks for pattern matching at the beginning of the given
    string. But sometimes, we need to check matching at a specific location in the
    string:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Match `y` for the second position:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Check for a pattern called `thon` starting from `pos=2`, that is, the third
    character:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Find a match in a different string by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Finding the Number of Words in a List That End with "ing"
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Suppose we want to find out if a given string has the last three letters: ''ing''.
    This kind of query may come up in a text analytics/text mining program where somebody
    is interested in finding instances of present continuous tense words, which are
    highly likely to end with ''ing''. However, other nouns may also end with ''ing''
    (as we will see in this example):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `for` loop to find words ending with ''ing'':'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It looks plain and simple, and you may well wonder what the purpose of using
    a special regex module for this is. A simple string method should have been sufficient.
    Yes, it would have been OK for this particular example, but the whole point of
    using regex is to be able to use very complex string patterns that are not at
    all obvious when it comes to how they are written using simple string methods.
    We will see the real power of regex compared to string methods shortly. But before
    that, let's explore another of the most commonly used methods, called `search`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 98: The search Method in Regex'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`Search` and `match` are related concepts and they both return the same Match
    object. The real difference between them is that **match works for only the first
    match** (either at the beginning of the string or at a specified position, as
    we saw in the previous exercises), whereas **search looks for the pattern anywhere
    in the string** and returns the appropriate position if it finds a match:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `compile` method to find matching strings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Search the string by using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see, the `match` method returns `None` for the input `spring`, and
    we had to write code to print that out explicitly (because in Jupyter notebook,
    nothing will show up for a None object). But `search` returns a `Match` object
    with `span=(3,6)` as it finds the `ing` pattern spanning those positions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Similarly, for the `Ringtone` string, it finds the correct position of the match
    and returns `span=(1,4)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 99: Using the span Method of the `Match` Object to Locate the Position
    of the Matched Pattern'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As you will understand by now, the `span` contained in the `Match` object is
    useful for locating the exact position of the pattern as it appears in the string.
  prefs: []
  type: TYPE_NORMAL
- en: Intitialize `prog` with pattern ing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Create a function to return a tuple of start and end positions of match.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Print the words ending with ing in the start or end position.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'Exercise 100: Examples of Single Character Pattern Matching with search'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now, we will start getting into the real usage of regex with examples of various
    useful pattern matching. First, we will explore single-character matching. We
    will also use the `group` method, which essentially returns the matched pattern
    in a string format so that we can print and process it easily:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Dot (.) matches any single character except a newline character:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`\w` (lowercase w) matches any single letter, digit, or underscore:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`\W` (uppercase W) matches anything not covered with `\w:`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`\s` (lowercase s) matches a single whitespace character, such as a space,
    newline, tab, or return:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`\d` matches numerical digits 0 – 9:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Exercise 101: Examples of Pattern Matching at the Start or End of a String'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will match patterns with strings. The focus is to find
    out whether the pattern is present at the start or the end of the string:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Write a function to handle cases where match is not found, that is, to handle
    `None` objects as returns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use `^` (Caret) to match a pattern at the start of the string:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use `$` (dollar sign) to match a pattern at the end of the string:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Exercise 102: Examples of Pattern Matching with Multiple Characters'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now, we will turn to more exciting and useful pattern matching with examples
    of multiple characters matching. You should start seeing and appreciating the
    real power of regex by now.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For these examples and exercises, also try to think how you would implement
    them without regex, that is, by using simple string methods and any other logic
    that you can think of. Then, compare that solution to the ones implemented with
    regex for brevity and efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use `*` to match 0 or more repetitions of the preceding `RE`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Using `+` causes the resulting RE to match 1 or more repetitions of the preceding
    RE:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`?` causes the resulting RE to match precisely 0 or 1 repetitions of the preceding
    RE:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Exercise 103: Greedy versus Non-Greedy Matching'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The standard (default) mode of pattern matching in regex is greedy, that is,
    the program tries to match as much as it can. Sometimes, this behavior is natural,
    but, in some cases, you may want to match minimally:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The greedy way of matching a string is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'So, the preceding regex found both tags with the <> pattern, but what if we
    wanted to match the first tag only and stop there. We can use `?` by inserting
    it after any regex expression to make it non-greedy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Exercise 104: Controlling Repetitions to Match'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In many situations, we want to have precise control over how many repetitions
    of the pattern we want to match in a text. This can be done in a few ways, which
    we will show examples of here:'
  prefs: []
  type: TYPE_NORMAL
- en: '`{m}` specifies exactly `m` copies of RE to match. Fewer matches cause a non-match
    and returns `None:`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`{m,n}` specifies exactly `m` to `n` copies of `RE` to match:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Omitting `m` specifies a lower bound of zero:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE126]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE127]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Omitting `n` specifies an infinite upper bound:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE128]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE129]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`{m,n}?` specifies `m` to `n` copies of RE to match in a non-greedy fashion:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE130]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE131]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Exercise 105: Sets of Matching Characters'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To match an arbitrarily complex pattern, we need to be able to include a logical
    combination of characters together as a bunch. Regex gives us that kind of capability:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following examples demonstrate such uses of regex. `[x,y,z]` matches x,
    y, or z:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE132]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE133]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: A range of characters can be matched inside the set using -. This is one of
    the most widely used regex techniques!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Suppose we want to pick out an email address from a text. Email addresses are
    generally of the form `<some name>@<some domain name>.<some domain identifier>`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE134]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE135]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Look at the regex pattern inside the [ … ]. It is ''`a-zA-Z''`. This covers
    all alphabets, including lowercase and uppercase! With this one simple regex,
    you are able to match any (pure) alphabetical string for that part of the email.
    Now, the next pattern is ''`@`'', which is added to the previous regex by a ''`+`''
    character. This is the way to build up a complex regex: by adding/stacking up
    individual regex patterns. We also use the same `[a-zA-Z]` for the email domain
    name and add a ''`.com`'' at the end to complete the pattern as a valid email
    address. Why \.? Because, by itself, DOT (.) is used as a special modifier in
    regex, but here we want to use DOT (.) just as DOT (.), not as a modifier. So,
    we need to precede it by a ''\''.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: So, with this regex, we could extract the first email address perfectly but
    got '`No match`' with the second one.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What happened with the second email ID?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The regex could not capture it because it had the number '12' in the name! That
    pattern is not captured by the expression [a-zA-Z].
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s change that and add the digits as well:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE136]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE137]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, we catch the first email ID perfectly. But what's going on with the second
    one? Again, we got a mismatch. The reason is that we changed the .com to .org
    in that email, and in our regex expression, that portion was hardcoded as `.com`,
    so it did not find a match.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s try to address this in the following regex:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE138]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE139]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this regex, we used the fact that most domain identifiers have 2 or 3 characters,
    so we used `[a-zA-Z]{2,3}` to capture that.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What happened with the second email ID? This is an example of the small tweaks
    that you can make to stay ahead of telemarketers who want to scrape online forums
    or any other corpus of text and extract your email ID. If you do not want your
    email to be found, you can change `@` to `[AT]` and . to `[DOT]` ,and hopefully
    that can beat some regex techniques (but not all)!
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 106: The use of OR in Regex using the OR Operator'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Because regex patterns are like complex and compact logical constructors themselves,
    it makes perfect sense that we want to combine them to construct even more complex
    programs when needed. We can do that by using the `|` operator:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example demonstrates the use of the OR operator:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE140]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE141]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: So, here, we are trying to extract patterns of 10-digit numbers that could be
    phone numbers. Note the use of `{10}` to denote exactly 10-digit numbers in the
    pattern. But the second number could not be matched for obvious reasons – it had
    '-' symbols inserted in between groups of numbers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use multiple smaller regexes and logically combine them by using the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE142]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE143]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Phone numbers are written in a myriad of ways and if you search on the web,
    you will see examples of very complex regexes (written not only in Python but
    other widely used languages, for web apps such as JavaScript, C++, PHP, Perl,
    and so on) for capturing phone numbers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create four strings and execute `print_match` on them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE144]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE145]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`The findall` Method'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The last regex method that we will learn in this chapter is `findall`. Essentially,
    it is a **search-and-aggregate** method, that is, it puts all the instances that
    match with the regex pattern in a given text and returns them in a list. This
    is extremely useful, as we can just count the length of the returned list to count
    the number of occurrences or pick and use the returned pattern-matched words one
    by one as we see fit.
  prefs: []
  type: TYPE_NORMAL
- en: Note that, although we are giving short examples of single sentences in this
    chapter, you will often deal with a large corpus of text when using a RegEx.
  prefs: []
  type: TYPE_NORMAL
- en: 'In those cases you are likely to get many matches from a single regex pattern
    search. For all of those cases, the `findall` method is going to be the most useful:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE146]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE147]'
  prefs: []
  type: TYPE_PRE
- en: 'Activity 9: Extracting the Top 100 eBooks from Gutenberg'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Project Gutenberg encourages the creation and distribution of eBooks by encouraging
    volunteer efforts to digitize and archive cultural works. This activity aims to
    scrape the URL of Project Gutenberg's Top 100 eBooks to identify the eBooks' links.
    It uses BeautifulSoup4 to parse the HTML and regular expression code to identify
    the Top 100 eBook file numbers.
  prefs: []
  type: TYPE_NORMAL
- en: You can use those book ID numbers to download the book into your local drive
    if you want.
  prefs: []
  type: TYPE_NORMAL
- en: Head over to the supplied Jupyter notebook (in the GitHub repository) to work
    on this activity.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the steps that will help you solve this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Import the necessary libraries, including `regex` and `beautifulsoup`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check the SSL certificate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Read the HTML from the URL.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a small function to check the status of the web request.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Decode the response and pass this on to BeautifulSoup for HTML parsing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find all the `href` tags and store them in the list of links. Check what the
    list looks like – print the first 30 elements.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use a regular expression to find the numeric digits in these links. These are
    the file numbers for the top 100 eBooks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the empty list to hold the file numbers over an appropriate range
    and use `regex` to find the numeric digits in the link `href` string. Use the
    `findall` method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What does the `soup` object's text look like? Use the .`text` method and print
    only the first 2,000 characters (do not print the whole thing, as it is too long).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Search in the extracted text (using a regular expression) from the soup object
    to find the names of the top 100 eBooks (yesterday's ranking).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a starting index. It should point at the text *Top 100 Ebooks yesterday*.
    Use the `splitlines` method of soup.text. It splits the lines of text of the soup
    object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Loop 1-100 to add the strings of the next 100 lines to this temporary list.
    Hint: use the `splitlines` method.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use a regular expression to extract only text from the name strings and append
    it to an empty list. Use `match` and `span` to find the indices and use them.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 315.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Activity 10: Building Your Own Movie Database by Reading an API'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this activity, you will build a complete movie database by communicating
    and interfacing with a free API. You will learn about obtaining a unique user
    key that must be used when your program tries to access the API. This activity
    will teach you general chapters about working with an API, which are fairly common
    for other highly popular API services such as Google or Twitter. Therefore, after
    doing this exercise, you will be confident about writing more complex programs
    to scrape data from such services.
  prefs: []
  type: TYPE_NORMAL
- en: 'The aims of this activity are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: To retrieve and print basic data about a movie (the title is entered by the
    user) from the web (OMDb database)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a poster of the movie can be found, it downloads the file and saves it at
    a user-specified location
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These are the steps that will help you solve this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Import `urllib.request`, `urllib.parse`, `urllib.error`, and `json`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the secret API key (you have to get one from the OMDb website and use that;
    it has a daily limit of 1,000) from a JSON file stored in the same folder in a
    variable, by using `json.loads.`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Obtain a key and store it in JSON as `APIkeys.json`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Open the `APIkeys.json` file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assign the OMDb portal ([http://www.omdbapi.com/?](http://www.omdbapi.com/?))
    as a string to a variable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a variable called `apikey` with the last portion of the URL (`&apikey=secretapikey`),
    where `secretapikey` is your own API key.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a utility function called `print_json` to print the movie data from a
    JSON file (which we will get from the portal).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a utility function to download a poster of the movie based on the information
    from the JSON dataset and save it in your local folder. Use the `os` module. The
    poster data is stored in the JSON key `Poster`. Use the Python command to open
    a file and write the poster data. Close the file after you're done. This function
    will save the poster data as an image file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a utility function called `search_movie` to search for a movie by its
    name, print the downloaded `JSON` data, and save the movie poster in the local
    folder. Use a `try-except` loop for this. Use the previously created `serviceurl`
    and `apikey` variables. You have to pass on a dictionary with a key, `t`, and
    the movie name as the corresponding value to the `urllib.parse.urlencode()` function
    and then add the `serviceurl` and `apikey` to the output of the function to construct
    the full URL. This URL will be used to access the data. The `JSON` data has a
    key called `Response`. If it is `True`, that means the read was successful. Check
    this before processing the data. If it's not successful, then print the `JSON`
    key `Error`, which will contain the appropriate error message returned by the
    movie database.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Test the `search_movie` function by entering `Titanic`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Test the `search_movie` function by entering `"Random_error"` (obviously, this
    will not be found, and you should be able to check whether your error catching
    code is working properly).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Note:'
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 320.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we went through several important concepts and learning modules
    related to advanced data gathering and web scraping. We started by reading data
    from web pages using two of the most popular Python libraries – `requests` and
    `BeautifulSoup`. In this task, we utilized the previous chapter's knowledge about
    the general structure of HTML pages and their interaction with Python code. We
    extracted meaningful data from the Wikipedia home page during this process.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we learned how to read data from XML and JSON files, two of the most widely
    used data streaming/exchange formats on the web. For the XML part, we showed you
    how to traverse the tree-structure data string efficiently to extract key information.
    For the JSON part, we mixed it with reading data from the web using an API (Application
    Program Interface). The API we consumed was RESTful, which is one of the major
    standards in Web API.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of this chapter, we went through a detailed exercise of using regex
    techniques in tricky string-matching problems to scrape useful information from
    a large and messy text corpus, parsed from HTML. This chapter should come in extremely
    handy for string and text processing tasks in your data wrangling career.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about databases with Python.
  prefs: []
  type: TYPE_NORMAL
