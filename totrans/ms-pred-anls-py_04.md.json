["```py\n>>> news = pd.read_csv('OnlineNewsPopularity.csv',sep=',')\n>>> news.columns\n\n```", "```py\nIndex(['url', ' timedelta', ' n_tokens_title', ' n_tokens_content',        ' n_unique_tokens', ' n_non_stop_words', ' n_non_stop_unique_tokens',        ' num_hrefs', ' num_self_hrefs', ' num_imgs', ' num_videos',        ' average_token_length', ' num_keywords', ' data_channel_is_lifestyle',        ' data_channel_is_entertainment', ' data_channel_is_bus',        ' data_channel_is_socmed', ' data_channel_is_tech',        ' data_channel_is_world', ' kw_min_min', ' kw_max_min', ' kw_avg_min',        ' kw_min_max', ' kw_max_max', ' kw_avg_max', ' kw_min_avg',        ' kw_max_avg', ' kw_avg_avg', ' self_reference_min_shares',        ' self_reference_max_shares', ' self_reference_avg_sharess',        ' weekday_is_monday', ' weekday_is_tuesday', ' weekday_is_wednesday',        ' weekday_is_thursday', ' weekday_is_friday', ' weekday_is_saturday',        ' weekday_is_sunday', ' is_weekend', ' LDA_00', ' LDA_01', ' LDA_02',        ' LDA_03', ' LDA_04', ' global_subjectivity',        ' global_sentiment_polarity', ' global_rate_positive_words',        ' global_rate_negative_words', ' rate_positive_words',        ' rate_negative_words', ' avg_positive_polarity',        ' min_positive_polarity', ' max_positive_polarity',        ' avg_negative_polarity', ' min_negative_polarity',        ' max_negative_polarity', ' title_subjectivity',        ' title_sentiment_polarity', ' abs_title_subjectivity',        ' abs_title_sentiment_polarity', ' shares'],       dtype='object')\n\n```", "```py\n>>> news.columns = [ x.strip() for x in news.columns]\n\n```", "```py\n>>> news['shares'].plot(kind='hist',bins=100)\n\n```", "```py\n>>> news['shares'].map( lambda x: np.log10(x) ).plot(kind='hist',bins=100)\n\n```", "```py\n>>> news_trimmed_features = news.ix[:,'timedelta':'shares']\n\n```", "```py\n>>> log_values = list(news_trimmed_features.columns[news_trimmed_features.describe().reset_index().loc[7][1:]>1])\n>>> for l in log_values:\n…    news_trimmed_features[l] = np.log10(news_trimmed_features[l]+1)\n\n```", "```py\n>>> news_trimmed_features = news_trimmed_features.replace([np.inf, -np.inf], np.nan)\n\n```", "```py\n>>> news_trimmed_features = news_trimmed_features.fillna(method='pad')\n\n```", "```py\n>>> news_response = news_trimmed_features['shares']\n>>> news_trimmed_features = news_trimmed_features.ix[:,'timedelta':'abs_title_sentiment_polarity']\n\n```", "```py\n>>> news_trimmed_features = news_trimmed_features.drop('weekday_is_sunday',1)\n>>> news_trimmed_features = news_trimmed_features.drop('LDA_00',1)\n\n```", "```py\n>>> from sklearn import cross_validation\n>>> news_features_train, news_features_test, news_shares_train, news_shares_test = \\\n>>> cross_validation.train_test_split(news_trimmed_features, news_response, test_size=0.4, random_state=0)\n\n```", "```py\n>>> from sklearn import linear_model\n>>> lmodel = linear_model.LinearRegression().fit(news_features_train, news_shares_train)\n>>> plt.scatter(news_shares_train,lmodel.predict(news_features_train),color='black')\n>>> plt.xlabel('Observed')\n>>> plt.ylabel('Predicted')\n\n```", "```py\n>>> plt.scatter(news_shares_test,lmodel.predict(news_features_test),color='red')\n>>> plt.xlabel('Observed')\n>>> plt.ylabel('Predicted')\n\n```", "```py\n>>> lmodel.score(news_features_train, news_shares_train)\n>>> lmodel.score(news_features_test, news_shares_test)\n\n```", "```py\n>>> ix = np.argsort(abs(lmodel.coef_))[::-1][:]\n>>> news_trimmed_features.columns[ix]\n\n```", "```py\nIndex([u'n_unique_tokens', u'n_non_stop_unique_tokens', u'n_non_stop_words',        u'kw_avg_avg', u'global_rate_positive_words',        u'self_reference_avg_sharess', u'global_subjectivity', u'LDA_02',        u'num_keywords', u'self_reference_max_shares', u'n_tokens_content',        u'LDA_03', u'LDA_01', u'data_channel_is_entertainment', u'num_hrefs',        u'num_self_hrefs', u'global_sentiment_polarity', u'kw_max_max',        u'is_weekend', u'rate_positive_words', u'LDA_04',        u'average_token_length', u'min_positive_polarity',        u'data_channel_is_bus', u'data_channel_is_world', u'num_videos',        u'global_rate_negative_words', u'data_channel_is_lifestyle',        u'num_imgs', u'avg_positive_polarity', u'abs_title_subjectivity',        u'data_channel_is_socmed', u'n_tokens_title', u'kw_max_avg',        u'self_reference_min_shares', u'rate_negative_words',        u'title_sentiment_polarity', u'weekday_is_tuesday',        u'min_negative_polarity', u'weekday_is_wednesday',        u'max_positive_polarity', u'title_subjectivity', u'weekday_is_thursday',        u'data_channel_is_tech', u'kw_min_avg', u'kw_min_max', u'kw_avg_max',        u'timedelta', u'kw_avg_min', u'kw_max_min', u'max_negative_polarity',        u'kw_min_min', u'avg_negative_polarity', u'weekday_is_saturday',        u'weekday_is_friday', u'weekday_is_monday',        u'abs_title_sentiment_polarity'],       dtype='object')\n\n```", "```py\n>>> import statsmodels\n>>> import statsmodels.api as sm\n>>> import statsmodels.formula.api as smf\n\n```", "```py\n>>> results = sm.OLS(news_response, news_trimmed_features).fit()\n>>> results.summary()\n\n```", "```py\n>>> model_formula = news_response.name+\" ~ \"+\" + \".join(news_trimmed_features.columns)\n\n```", "```py\nshares ~ timedelta + n_tokens_title + n_tokens_content + n_unique_tokens + n_non_stop_words + n_non_stop_unique_tokens + num_hrefs + num_self_hrefs + num_imgs + num_videos + average_token_length + num_keywords + data_channel_is_lifestyle + data_channel_is_entertainment + data_channel_is_bus + data_channel_is_socmed + data_channel_is_tech + data_channel_is_world + kw_min_min + kw_max_min + kw_avg_min + kw_min_max + kw_max_max + kw_avg_max + kw_min_avg + kw_max_avg + kw_avg_avg + self_reference_min_shares + self_reference_max_shares + self_reference_avg_sharess + weekday_is_monday + weekday_is_tuesday + weekday_is_wednesday + weekday_is_thursday + weekday_is_friday + weekday_is_saturday + is_weekend + LDA_01 + LDA_02 + LDA_03 + LDA_04 + global_subjectivity + global_sentiment_polarity + global_rate_positive_words + global_rate_negative_words + rate_positive_words + rate_negative_words + avg_positive_polarity + min_positive_polarity + max_positive_polarity + avg_negative_polarity + min_negative_polarity + max_negative_polarity + title_subjectivity + title_sentiment_polarity + abs_title_subjectivity + abs_title_sentiment_polarity\n\n```", "```py\n>>> news_all_data = pd.concat([news_trimmed_features,news_response],axis=1)\n>>> results = smf.ols(formula=model_formula,data=news_all_data).fit()\n\n```", "```py\n>>> students = pd.read_csv('student-mat.csv',sep=';')\n>>> students.boxplot(by='school',column=['G1','G2','G3'])\n\n```", "```py\n>>> model_formula = \"G3 ~ \"+\" + \".join(students.columns[1:len(students.columns)-3])\n\n```", "```py\n>>> results = smf.gee(model_formula,\"school\",data=students).fit()\n>>> results.summary()\n\n```", "```py\n>>> results = smf.mixedlm(model_formula,groups=\"school\",data=students).fit()\n>>> results.summary()\n\n```", "```py\n>>> statsmodels.tsa.arima_model.ARMA()\n\n```", "```py\n>>> lmodel_ridge = linear_model.RidgeCV().fit(news_features_train, news_shares_train)\n>>> lmodel_ridge.alpha_\n\n```", "```py\n>>> lmodel_ridge.score(news_features_test, news_shares_test)\n\n```", "```py\n>>> lmodel_lasso = linear_model.LassoCV(max_iter=10000).fit(news_features_train, news_shares_train)\n>>> lmodel_lasso.alpha_\n\n```", "```py\n>>> from sklearn.linear_model import ElasticNetCV\n>>> lmodel_enet = ElasticNetCV().fit(news_features_train, news_shares_train)\n>>> lmodel_enet.score(news_features_test, news_shares_test)\n\n```", "```py\n>>> probs = np.arange(0.01,1,0.01)\n>>> entropy = [ -1*np.log2(p)*p for p in probs]\n>>> plt.plot(probs,entropy)\n>>> plt.xlabel('y')\n>>>plt.ylabel('Entropy')\n\n```", "```py\n>>> from sklearn.tree import DecisionTreeRegressor\n>>> max_depths = [2,4,6,8,32,64,128,256]\n>>> dtrees = []\n>>> for m in max_depths:\n…    dtrees.append(DecisionTreeRegressor(min_samples_leaf=20,max_depth=m).\\\n…    fit(news_features_train, news_shares_train))\n\n```", "```py\n>>> r2_values = []\n>>> for d in dtrees:\n…    r2_values.append(d.score(news_features_test, news_shares_test))\n>>> plt.plot(max_depths,r2_values,color='red')\n>>> plt.xlabel('maximum depth')\n>>> plt.ylabel('r-squared')\n\n```", "```py\n>>> from sklearn import ensemble\n>>> rforests = []\n>>> num_trees = [2,4,6,8,32,64,128,256]\n>>> for n in num_trees:\n …   rforests.\\\n …   append(ensemble.RandomForestRegressor(n_estimators=n,min_samples_leaf=20).\\\n …   fit(news_features_train, news_shares_train))\n\n```", "```py\n>>> r2_values_rforest = []\n>>> for f in rforests:\n …   r2_values_rforest.append(f.score(news_features_test, news_shares_test))\n>>> plt.plot(num_trees,r2_values_rforest,color='red')\n>>> plot.xlabel('Number of Trees')\n>>> plot.ylabel('r-squared')\n\n```", "```py\n>>> ix = np.argsort(abs(f[5].feature_importances_))[::-1]\n>>> news_trimmed_features.columns[ix]\n\n```", "```py\n Index(['kw_avg_avg', 'self_reference_avg_sharess', 'timedelta', 'LDA_01',        'kw_max_avg', 'n_unique_tokens', 'data_channel_is_tech', 'LDA_02',        'self_reference_min_shares', 'n_tokens_content', 'LDA_03', 'kw_avg_max',        'global_rate_negative_words', 'avg_negative_polarity',        'global_rate_positive_words', 'average_token_length', 'num_hrefs',        'is_weekend', 'global_subjectivity', 'kw_avg_min',        'n_non_stop_unique_tokens', 'kw_min_max', 'global_sentiment_polarity',        'kw_max_min', 'LDA_04', 'kw_min_avg', 'min_positive_polarity',        'num_self_hrefs', 'avg_positive_polarity', 'self_reference_max_shares',        'title_sentiment_polarity', 'max_positive_polarity', 'n_tokens_title',        'abs_title_sentiment_polarity', 'abs_title_subjectivity',        'title_subjectivity', 'min_negative_polarity', 'num_imgs',        'data_channel_is_socmed', 'rate_negative_words', 'num_videos',        'max_negative_polarity', 'rate_positive_words', 'kw_min_min',        'num_keywords', 'data_channel_is_entertainment', 'weekday_is_wednesday',        'data_channel_is_lifestyle', 'weekday_is_friday', 'weekday_is_monday',        'kw_max_max', 'data_channel_is_bus', 'data_channel_is_world',        'n_non_stop_words', 'weekday_is_saturday', 'weekday_is_tuesday',        'weekday_is_thursday'],       dtype='object')\n\n```", "```py\n>>> def parse_line(l):\n…      try:\n…            return l.split(\",\")\n…    except:\n…         print(\"error in processing {0}\".format(l))\n\n```", "```py\n>>> songs = sc.textFile('/Users/jbabcock/Downloads/YearPredictionMSD.txt').\\\nmap(lambda x : parse_line(x)).\\\ntoDF()\n\n```", "```py\n>>> from pyspark.mllib.regression import LabeledPoint\n>>> songs_labeled = songs.map( lambda x: LabeledPoint(x[0],x[1:]) )\n\n```", "```py\n>>> songs_train = songs_labeled.zipWithIndex().\\\nfilter( lambda x: x[1] < 463715).\\\nmap( lambda x: x[0] )\n>>> songs_test = songs_labeled.zipWithIndex().\\\nfilter( lambda x: x[1] >= 463715).\\\nmap( lambda x: x[0] )\n\n```", "```py\n>>> from pyspark.mllib.tree import RandomForest\n>>> rf = RandomForest.trainRegressor(songs_train,{},50,\"auto\",\"variance\",10,32)\n>>> prediction = rf.predict(songs_test.map(lambda x: x.features))\n>>> predictedObserved = songs_test.map(lambda lp: lp.label).zip(prediction)\n\n```", "```py\n>>> from pyspark.mllib.evaluation import RegressionMetrics\n>>> RegressionMetrics(predictedObserved).r2\n\n```"]