<html><head></head><body><div><div><h1 id="_idParaDest-38"><a id="_idTextAnchor037"/>Chapter 2: Core Operations with spaCy</h1>
			<p>In this chapter, you will learn the core operations with spaCy, such as creating a language pipeline, tokenizing the text, and breaking the text into its sentences. </p>
			<p>First, you'll learn what a language processing pipeline is and the pipeline components. We'll continue with general spaCy conventions – important classes and class organization – to help you to better understand spaCy library organization and develop a solid understanding of the library itself. </p>
			<p>You will then learn about the first pipeline component – <strong class="bold">Tokenizer</strong>. You'll also learn about an important linguistic concept – <strong class="bold">lemmatization</strong> – along with its applications in <strong class="bold">natural language understanding</strong> (<strong class="bold">NLU</strong>). Following that, we will cover <strong class="bold">container classes</strong> and <strong class="bold">spaCy data structures</strong> in detail. We will finish the chapter with useful spaCy features that you'll use in everyday NLP development. </p>
			<p>We're going to cover the following main topics in this chapter:</p>
			<ul>
				<li>Overview of spaCy conventions</li>
				<li>Introducing tokenization</li>
				<li>Understanding lemmatization</li>
				<li>spaCy container objects</li>
				<li>More spaCy features</li>
			</ul>
			<h1 id="_idParaDest-39"><a id="_idTextAnchor038"/>Technical requirements </h1>
			<p>The chapter code can be found at the book's GitHub repository: <a href="https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter02">https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter02</a></p>
			<h1 id="_idParaDest-40"><a id="_idTextAnchor039"/>Overview of spaCy conventions</h1>
			<p>Every NLP application consists of <a id="_idIndexMarker059"/>several steps of processing the text. As you can see in the first chapter, we have always created instances called <code>nlp</code> and <code>doc</code>. But what did we do exactly?</p>
			<p>When we call <code>nlp</code> on our text, spaCy applies some processing steps. The first step is tokenization to produce a <code>Doc</code> object. The <code>Doc</code> object is then processed further with a <code>Doc</code> and then passes it to the <a id="_idIndexMarker063"/>next component:</p>
			<div><div><img src="img/B16570_02_01.jpg" alt="Figure 2.1 – A high-level view of the processing pipeline&#13;&#10;" width="1650" height="238"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.1 – A high-level view of the processing pipeline</p>
			<p>A spaCy pipeline object is created <a id="_idIndexMarker064"/>when we load a language model. We load an English model and initialize a pipeline in the following code segment:</p>
			<pre> import spacy
 nlp = spacy.load("en_core_web_md")
 doc = nlp("I went there")</pre>
			<p>What happened exactly in the preceding code is as follows: </p>
			<ol>
				<li>We started by importing <code>spaCy</code>. </li>
				<li>In the second line, <code>spacy.load()</code> returned a <code>Language</code> class instance, <code>nlp</code>. The <code>Language</code> class is <em class="italic">the text processing pipeline</em>. </li>
				<li>After that, we applied <code>nlp</code> on the sample sentence <code>I went there</code> and got a <code>Doc</code> class instance, <code>doc</code>.</li>
			</ol>
			<p>The <code>Language</code> class applies all of the preceding pipeline steps to your input sentence behind the scenes. After applying <code>nlp</code> to the sentence, the <code>Doc</code> object contains tokens that are tagged, lemmatized, and marked as entities if the token is an entity (we will go into detail about what are those and how it's done later). Each pipeline component has a well-defined task:</p>
			<div><div><img src="img/B16570_02_02.jpg" alt="Figure 2.2 – Pipeline components and tasks&#13;&#10;" width="1608" height="580"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.2 – Pipeline components and tasks</p>
			<p>The spaCy language <a id="_idIndexMarker065"/>processing pipeline always <em class="italic">depends on the statistical model</em> and its capabilities. This is why we always load a language model with <code>spacy.load()</code> as the first step in our code. </p>
			<p>Each component corresponds to a <code>spaCy</code> class. <code>spaCy</code> classes have self-explanatory names such as <code>Language</code> and <code>Doc</code> classes – let's see all of the processing pipeline classes and their duties:</p>
			<div><div><img src="img/B16570_02_03.jpg" alt="Figure 2.3 – spaCy processing pipeline classes&#13;&#10;" width="1648" height="1483"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.3 – spaCy processing pipeline classes</p>
			<p>Don't be intimated by the <a id="_idIndexMarker066"/>number of classes; each class has unique features to help you process your text better. </p>
			<p>There are more data structures to represent text data and language data. Container classes such as Doc hold information about sentences, words, and the text. There are also container classes other than Doc:</p>
			<div><div><img src="img/B16570_02_04.jpg" alt="Figure 2.4 – spaCy container classes&#13;&#10;" width="1541" height="633"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.4 – spaCy container classes</p>
			<p>Finally, spaCy provides <a id="_idIndexMarker067"/>helper classes for vectors, language vocabulary, and annotations. We'll see the <code>Vocab</code> class often in this book. <code>Vocab</code> represents a language's vocabulary. Vocab contains all the words of the language model we loaded:</p>
			<div><div><img src="img/B16570_02_05.jpg" alt="Figure 2.5 – spaCy helper classes&#13;&#10;" width="1393" height="808"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.5 – spaCy helper classes</p>
			<p>The spaCy library's backbone data structures are <code>Doc</code> and <code>Vocab</code>. The <code>Doc</code> object abstracts the text by <a id="_idIndexMarker068"/>owning the sequence of tokens and all their properties. The <code>Vocab</code> object provides a centralized set of strings and lexical attributes to all the other classes. This way spaCy avoids storing multiple copies of linguistic data:</p>
			<div><div><img src="img/B16570_02_06.jpg" alt="Figure 2.6 – spaCy architecture&#13;&#10;" width="1631" height="1252"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.6 – spaCy architecture</p>
			<p>You can divide the objects composing the preceding spaCy architecture into two: <strong class="bold">containers</strong> and <strong class="bold">processing pipeline components</strong>. In this chapter, we'll first learn about two basic components, <strong class="bold">Tokenizer</strong> and <strong class="bold">Lemmatizer</strong>, then we'll explore <strong class="bold">Container</strong> objects further.</p>
			<p>spaCy does all these <a id="_idIndexMarker069"/>operations for us behind the scenes, allowing us to concentrate on our own application's development. With this level of abstraction, using spaCy for NLP application development is no coincidence. Let's start with the <code>Tokenizer</code> class and see what it offers for us; then we will explore all the container classes one by one throughout the chapter. </p>
			<h1 id="_idParaDest-41"><a id="_idTextAnchor040"/>Introducing tokenization</h1>
			<p>We saw in <em class="italic">Figure 2.1</em> that the first step in a <a id="_idIndexMarker070"/>text processing pipeline is tokenization. Tokenization is always the first operation because all the other operations require the tokens.</p>
			<p>Tokenization simply means splitting the sentence into its tokens. A <strong class="bold">token</strong> is a unit of semantics. You can <a id="_idIndexMarker071"/>think of a token as the smallest meaningful part of a piece of text. Tokens can be words, numbers, punctuation, currency symbols, and any other meaningful symbols that are the building blocks of a sentence. The following are examples of tokens:</p>
			<ul>
				<li><code>USA</code></li>
				<li><code>N.Y.</code></li>
				<li><code>city</code></li>
				<li><code>33</code></li>
				<li><code>3rd</code></li>
				<li><code>! </code></li>
				<li><code>…</code></li>
				<li><code>?</code></li>
				<li><code>'s</code></li>
			</ul>
			<p>Input to the spaCy tokenizer is a Unicode text and the result is a <code>Doc</code> object. The following code shows the tokenization process:</p>
			<pre> import spacy
 nlp = spacy.load("en_core_web_md")
 doc = nlp("I own a ginger cat.")
 print ([token.text for token in doc])
 ['I', 'own', 'a', 'ginger', 'cat', '.']</pre>
			<p>The following is what we just did: </p>
			<ol>
				<li value="1">We start by importing <code>spaCy</code>. </li>
				<li>Then we loaded the English language model via the <code>en</code> shortcut to create an instance of the <code>nlp</code> <code>Language</code> class. </li>
				<li>Next, we apply the <code>nlp</code> object to the input sentence to create a <code>Doc</code> object, <code>doc</code>. A <code>Doc</code> object is a container for a sequence of <code>Token</code> objects. spaCy generates the <code>Token</code> objects implicitly when we created the <code>Doc</code> object. </li>
				<li>Finally, we print a list of the preceding sentence's tokens. </li>
			</ol>
			<p>That's it, we made the <a id="_idIndexMarker072"/>tokenization with just three lines of code. You can visualize the tokenization with indexing as follows:</p>
			<div><div><img src="img/B16570_02_07.jpg" alt="Figure 2.7 – Tokenization of “I own a ginger cat.”&#13;&#10;" width="732" height="203"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.7 – Tokenization of "I own a ginger cat."</p>
			<p>As the examples suggest, tokenization can indeed be tricky. There are many aspects we should pay attention to: punctuation, whitespaces, numbers, and so on. Splitting from the whitespaces with <code>text.split(" ")</code> might be tempting and looks like it is working for the example sentence <em class="italic">I own a ginger cat</em>. </p>
			<p>How about the sentence <code>"It's been a crazy week!!!"</code>? If we make a <code>split(" ")</code> the resulting tokens would be <code>It's</code>, <code>been</code>, <code>a</code>, <code>crazy</code>, <code>week!!!</code>, which is not what you want. First of all, <code>It's</code> is not one token, it's two tokens: <code>it</code> and <code>'s</code>. <code>week!!!</code> is not a valid token as the punctuation is not split correctly. Moreover, <code>!!!</code> should be tokenized <a id="_idIndexMarker073"/>per symbol and should generate three <em class="italic">!</em>'s. (This may not look like an important detail, but trust me, it is important for<em class="italic"> sentiment analysis</em>. We'll cover sentiment analysis in <a href="B16570_08_Final_JM_ePub.xhtml#_idTextAnchor137"><em class="italic">Chapter 8</em></a><em class="italic">, Text Classification with spaCy</em>.) Let's see what spaCy tokenizer has generated:</p>
			<pre> import spacy
 nlp = spacy.load("en_core_web_md")
 doc = nlp("It's been a crazy week!!!")
 print ([token.text for token in doc])
['It', "'s", 'been', 'a', 'crazy', 'week', '!', '!', '!']</pre>
			<p>This time the sentence is split as follows:</p>
			<div><div><img src="img/B16570_02_08.jpg" alt="Figure 2.8 – Tokenization of apostrophe and punctuations marks&#13;&#10;" width="1066" height="221"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.8 – Tokenization of apostrophe and punctuations marks</p>
			<p>How does spaCy know where to split the sentence? Unlike other parts of the pipeline, the tokenizer doesn't need a statistical model. Tokenization is based on language-specific rules. You can see examples the language specified data here: <a href="https://github.com/explosion/spaCy/tree/master/spacy/lang">https://github.com/explosion/spaCy/tree/master/spacy/lang</a>. </p>
			<p>Tokenizer exceptions define rules for exceptions, such as <code>it's</code> , <code>don't</code> , <code>won't</code>, abbreviations, and so on. If you look at the rules for English: <a href="https://github.com/explosion/spaCy/blob/master/spacy/lang/en/tokenizer_exceptions.py">https://github.com/explosion/spaCy/blob/master/spacy/lang/en/tokenizer_exceptions.py</a>, you will see that rules look like <code>{ORTH: "n't", LEMMA: "not"}</code>, which describes the splitting rule for <code>n't</code> to the tokenizer. </p>
			<p>The prefixes, suffixes, and infixes mostly describe how to deal with punctuation – for example, we split at a <a id="_idIndexMarker074"/>period if it is at the end of the sentence, otherwise, most probably it's part of an abbreviation such as N.Y. and we shouldn't touch it. Here, <code>ORTH</code> means the text and <code>LEMMA</code> means the base word form without any inflections. The following example shows you the execution of the spaCy tokenization algorithm:</p>
			<div><div><img src="img/B16570_02_09.jpg" alt="Figure 2.9 – spaCy performing tokenization with exception rules (image taken from spaCy tokenization guidelines (https://spacy.io/usage/linguistic-features#tokenization))" width="1273" height="911"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.9 – spaCy performing tokenization with exception rules (image taken from spaCy tokenization guidelines (https://spacy.io/usage/linguistic-features#tokenization))</p>
			<p>Tokenization rules depend on the grammatical rules of the individual language. Punctuation rules such as splitting periods, commas, or exclamation marks are more or less similar for many languages; however, some rules are specific to the individual language, such as <a id="_idIndexMarker075"/>abbreviation words and apostrophe usage. spaCy supports each language having its own specific rules by allowing hand-coded data and rules, as each language has its own subclass.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">spaCy provides non-destructive tokenization, which means that we <a id="_idIndexMarker076"/>always will be able to recover the original text from the tokens. Whitespace and punctuation information is preserved during tokenization, so the input text is preserved as it is.</p>
			<p>Every <code>Language</code> object contains a <code>Tokenizer</code> object. The <code>Tokenizer</code> class is the class that performs the tokenization. You don't often call this class directly when you create a <code>Doc</code> class instance, while Tokenizer class acts behind the scenes. When we want to customize the tokenization, we need to interact with this class. Let's see how it is done.</p>
			<h2 id="_idParaDest-42"><a id="_idTextAnchor041"/>Customizing the tokenizer</h2>
			<p>When we work with a <a id="_idIndexMarker077"/>specific domain such as medicine, insurance, or finance, we often come across words, abbreviations, and entities that needs special attention. Most domains that you'll process have characteristic words and phrases that need custom tokenization rules. Here's how to add a special case rule to an existing <code>Tokenizer</code> class instance:</p>
			<pre> import spacy
 from spacy.symbols import ORTH
 nlp = spacy.load("en_core_web_md")
 doc = nlp("lemme that")
 print([w.text for w in doc])
['lemme', 'that']
 special_case = [{ORTH: "lem"}, {ORTH: "me"}]
 nlp.tokenizer.add_special_case("lemme", special_case)
 print([w.text for w in nlp("lemme that")])
['lem', 'me', 'that']</pre>
			<p>Here is what we did: </p>
			<ol>
				<li value="1">We again started by importing <code>spacy</code>. </li>
				<li>Then, we imported the <code>ORTH</code> symbol, which means orthography; that is, text. </li>
				<li>We continued with creating a <code>Language</code> class object, <code>nlp</code>, and created a Doc object, <code>doc</code>. </li>
				<li>We defined a special case, where the word <code>lemme</code> should tokenize as two tokens, <code>lem</code> and <code>me</code>. </li>
				<li>We added the rule to the <code>nlp</code> object's tokenizer. </li>
				<li>The last line exhibits how the fresh rule works.</li>
			</ol>
			<p>When we define custom rules, punctuation <a id="_idIndexMarker078"/>splitting rules will still apply. Our special case will be recognized as a result, even if it's surrounded by punctuation. The tokenizer will divide punctuation step by step, and apply the same process to the remaining substring:</p>
			<pre> print([w.text for w in nlp("lemme!")])
['lem', 'me', '!']</pre>
			<p>If you define a special case rule with punctuation, the special case rule will take precedence over the punctuation splitting:</p>
			<pre> nlp.tokenizer.add_special_case("...lemme...?", [{"ORTH": "...lemme...?"}])
 print([w.text for w in nlp("...lemme...?")])
'...lemme...?'</pre>
			<p class="callout-heading">Pro tip</p>
			<p class="callout">Modify the tokenizer by adding new rules only if you really need to. Trust me, you can get quite unexpected results with custom rules. One of the cases where you really need it is when working with Twitter text, which is usually full of hashtags and special symbols. If you have social media text, first feed some sentences into the spaCy NLP pipeline and see how the tokenization works out.</p>
			<h2 id="_idParaDest-43"><a id="_idTextAnchor042"/>Debugging the tokenizer</h2>
			<p>The spaCy library has a <a id="_idIndexMarker079"/>tool for debugging: <code>nlp.tokenizer.explain(sentence</code>). It returns (<code>tokenizer rule/pattern, token</code>) <strong class="bold">tuples</strong> to help us <a id="_idIndexMarker080"/>understand what happened exactly during the tokenization. Let's see an example:</p>
			<pre> import spacy
 nlp = spacy.load("en_core_web_md")
 text = "Let's go!"
 doc = nlp(text)
 tok_exp = nlp.tokenizer.explain(text)
 for t in tok_exp:
     print(t[1], "\t", t[0])
Let    SPECIAL-1
's     SPECIAL-2
go     TOKEN
!      SUFFIX</pre>
			<p>In the preceding code, we imported <code>spacy</code> and created a <code>Language</code> class instance, <code>nlp</code>, as usual. Then we created a <code>Doc</code> class instance with the sentence <code>Let's go!</code>. After that, we asked the <code>Tokenizer</code> class instance, <code>tokenizer</code>, of <code>nlp</code> for an explanation of the tokenization of this sentence. <code>nlp.tokenizer.explain()</code> explained the rules that the tokenizer used one by one.</p>
			<p>After splitting a sentence into its tokens, it's time to split a text into its sentences. </p>
			<h2 id="_idParaDest-44"><a id="_idTextAnchor043"/>Sentence segmentation</h2>
			<p>We saw that breaking a <a id="_idIndexMarker081"/>sentence into its tokens is not a straightforward task at all. How about breaking a text into sentences? It's indeed a bit more <a id="_idIndexMarker082"/>complicated to mark where a sentence starts and ends due to the same reasons of punctuation, abbreviations, and so on.</p>
			<p>A Doc object's sentences are available via the <code>doc.sents</code> property:</p>
			<pre> import spacy
 nlp = spacy.load("en_core_web_md")
 text = "I flied to N.Y yesterday. It was around 5 pm."
 doc = nlp(text)
 for sent in doc.sents:
     print(sent.text)
I flied to N.Y yesterday.
It was around 5 pm.</pre>
			<p>Determining sentence boundaries is a more complicated task than tokenization. As a result, spaCy uses the dependency parser to perform sentence segmentation. This is a unique feature of spaCy – no other library puts such a sophisticated idea into practice. The results are very accurate in general, unless you process text of a very specific genre, such as from the conversation domain, or social media text. </p>
			<p>Now we know how to segment a text into sentences and tokenize the sentences. We're ready to process the tokens one by one. Let's start with lemmatization, a commonly used operation in semantics including sentiment analysis. </p>
			<h1 id="_idParaDest-45"><a id="_idTextAnchor044"/>Understanding lemmatization</h1>
			<p>A <strong class="bold">lemma</strong> is the <a id="_idIndexMarker083"/>base form of a token. You can <a id="_idIndexMarker084"/>think of a lemma as the form in which the token appears in a dictionary. For instance, the lemma of <em class="italic">eating</em> is <em class="italic">eat</em>; the lemma of <em class="italic">eats</em> is <em class="italic">eat</em>; <em class="italic">ate</em> similarly maps to <em class="italic">eat</em>. Lemmatization is the process of reducing the word forms to their lemmas. The following code is a quick example of how to do lemmatization with spaCy:</p>
			<pre> import spacy
 nlp = spacy.load("en_core_web_md")
 doc = nlp("I went there for working and worked for 3 years.")
 for token in doc:
     print(token.text, token.lemma_)
I <code>-PRON-</code>
went go
there
for for
working work
and and
worked work
for for
3 3
years year
. .</pre>
			<p>By now, you should be <a id="_idIndexMarker085"/>familiar with what the first three lines of the code do. Recall that we import the <code>spacy</code> library, load an English model using <code>spacy.load</code>, create a pipeline, and apply the pipeline to the preceding sentence to get a Doc object. Here we iterated over tokens to get their text and lemmas. </p>
			<p>In the first line you see <code>–PRON-</code>, which doesn't look like a real token. This is a <strong class="bold">pronoun lemma</strong>, a special <a id="_idIndexMarker086"/>token for lemmas of personal pronouns. This is an exception for semantic purposes: the personal pronouns <em class="italic">you</em>, <em class="italic">I</em>, <em class="italic">me</em>, <em class="italic">him</em>, <em class="italic">his</em>, and so on look different, but in terms of meaning, they're in the same group. spaCy offers this trick for the pronoun lemmas. </p>
			<p>No worries if all of this sounds too abstract – let's see lemmatization in action with a real-world example.</p>
			<h2 id="_idParaDest-46"><a id="_idTextAnchor045"/>Lemmatization in NLU</h2>
			<p>Lemmatization is an important step in NLU. We'll go <a id="_idIndexMarker087"/>over an example in this subsection. Suppose that you design an NLP pipeline for a ticket booking system. Your application processes a customer's sentence, extracts <a id="_idIndexMarker088"/>necessary information from it, and then passes it to the booking API.</p>
			<p>The NLP pipeline wants to extract the form of the travel (a flight, bus, or train), the destination city, and the date. The first thing the application needs to verify is the means of travel:</p>
			<pre>fly – flight – airway – airplane - plane
bus 
railway – train </pre>
			<p>We have this list of keywords and we want to recognize the means of travel by searching the tokens in the keywords list. The most compact way of doing this search is by looking up the token's lemma. Consider the following customer sentences:</p>
			<pre>List me all flights to Atlanta.
I need a flight to NY.
I flew to Atlanta yesterday evening and forgot my baggage.</pre>
			<p>Here, we don't need to include all word forms of the verb <em class="italic">fly</em> (<em class="italic">fly</em>, <em class="italic">flying</em>, <em class="italic">flies</em>, <em class="italic">flew</em>, and <em class="italic">flown</em>) in the keywords list and similar for the word <code>flight</code>; we reduced all possible variants to the base forms – <em class="italic">fly</em> and <em class="italic">flight</em>. Don't think of English only; languages such as Spanish, German, and Finnish have many word forms from a single lemma as well.</p>
			<p>Lemmatization also comes in handy when we want to recognize the destination city. There are many nicknames available for global cities and the booking API can process only the official names. The default tokenizer and lemmatizer won't know the difference between the official name and the nickname. In this case, you can add special rules, as we saw in the <em class="italic">Introducing tokenization</em> section. The following code plays a small trick:</p>
			<pre> import spacy
 from spacy.symbols import ORTH, LEMMA
 nlp = spacy.load('en')
 special_case = [{ORTH: 'Angeltown', LEMMA: 'Los Angeles'}]
 nlp.tokenizer.add_special_case(u'Angeltown', special_case)
 doc = nlp(u'I am flying to Angeltown')
 for token in doc:
     print(token.text, token.lemma_)
I -PRON-
am be
flying fly
to to
Angeltown Los Angeles</pre>
			<p>We defined a <a id="_idIndexMarker089"/>special case for the word <code>Angeltown</code> by replacing its lemma with the official name <code>Los Angeles</code>. Then we added this <a id="_idIndexMarker090"/>special case to the <code>Tokenizer</code> instance. When we print the token lemmas, we see that <code>Angeltown</code> maps to <code>Los Angeles</code> as we wished.</p>
			<h2 id="_idParaDest-47"><a id="_idTextAnchor046"/>Understanding the difference between lemmatization and stemming</h2>
			<p>A lemma is the base form of a <a id="_idIndexMarker091"/>word and is always a <a id="_idIndexMarker092"/>member of the language's vocabulary. The stem does not have to be a valid word at all. For instance, the lemma of <em class="italic">improvement</em> is <em class="italic">improvement</em>, but the stem is <em class="italic">improv</em>. You can think of the stem as the smallest part of the word that carries the meaning. Compare the following examples:</p>
			<pre>Word                Lemma
university        university
universe          universe
universal         universal
universities     university
universes        universe
improvement  improvement
improvements improvements
improves         improve</pre>
			<p>The preceding word-lemma examples <a id="_idIndexMarker093"/>show how lemma is calculated by following the grammatical rules of the language. Here, the lemma of a plural form is the <a id="_idIndexMarker094"/>singular form, and the lemma of a third-person verb is the base form of the verb. Let's compare them to the following examples of word-stem pairs:</p>
			<pre>Word                Stem
university        univers
universe          univer
universal         univers
universities     universi
universes        univers
improvement  improv
improvements improv
improves         improv</pre>
			<p>The first and the most important point to notice in the preceding examples is that the lemma does not have to be a valid word in the language. The second point is that many words can map to the same stem. Also, words from different grammatical categories can map to the same stem; here for instance, the noun <em class="italic">improvement</em> and the verb <em class="italic">improves</em> both map to <em class="italic">improv</em>. </p>
			<p>Though stems are not valid words, they still carry meaning. That's why stemming is commonly used in NLU applications. </p>
			<p>Stemming algorithms don't know anything about the grammar of the language. This class of algorithms works rather by trimming some common suffixes and prefixes from the beginning or end of the word. </p>
			<p>Stemming algorithms are rough, they cut the word from head and tail. There are several stemming algorithms available for English, including Porter and Lancaster. You can play with different stemming algorithms on NLTK's demo page at <a href="https://text-processing.com/demo/stem/">https://text-processing.com/demo/stem/</a>.</p>
			<p>Lemmatization, on the <a id="_idIndexMarker095"/>other hand, takes the morphological analysis of the words into consideration. To do so, it is important to obtain the dictionaries for the <a id="_idIndexMarker096"/>algorithm to look through in order to link the form back to its lemma.</p>
			<p>spaCy provides lemmatization via dictionary lookup and each language has its own dictionary.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Both stemming and lemmatization have their own advantages. Stemming gives very good results if you apply only statistical algorithms to the text, without further semantic processing such as pattern lookup, entity extraction, coreference resolution, and so on. Also stemming can trim a big corpus to a more moderate size and give you a compact representation. If you also use linguistic features in your pipeline or make a keyword search, include lemmatization. Lemmatization algorithms are accurate but come with a cost in terms of computation.</p>
			<h1 id="_idParaDest-48"><a id="_idTextAnchor047"/>spaCy container objects</h1>
			<p>At the beginning of this chapter, we saw a list of container objects including <strong class="bold">Doc</strong>, <strong class="bold">Token</strong>, <strong class="bold">Span</strong>, and <strong class="bold">Lexeme</strong>. We <a id="_idIndexMarker097"/>already used Token and Doc in our code. In this subsection, we'll see the properties of the container objects in detail. </p>
			<p>Using container objects, we can access the linguistic properties that spaCy assigns to the text. A container object is a logical representation of the text units such as a document, a token, or a slice of the document. </p>
			<p>Container objects in spaCy follow the natural structure of the text: a document is composed of sentences and sentences are composed of tokens. </p>
			<p>We most widely use Doc, Token, and Span objects in development, which represent a document, a single token, and a phrase, respectively. A container can contain other containers, for instance a <a id="_idIndexMarker098"/>document contains tokens and spans.</p>
			<p>Let's explore each class and its useful properties one by one.</p>
			<h2 id="_idParaDest-49"><a id="_idTextAnchor048"/>Doc</h2>
			<p>We created <a id="_idIndexMarker099"/>Doc objects in <a id="_idIndexMarker100"/>our code to represent the text, so you might have already figured out that Doc represents a text. </p>
			<p>We already know how to create a Doc object:</p>
			<pre> doc = nlp("I like cats.") </pre>
			<p><code>doc.text</code> returns a Unicode representation of the document text:</p>
			<pre> doc.text
I like cats.</pre>
			<p>The building block of a Doc object is Token, hence when you iterate a Doc you get Token objects as items:</p>
			<pre> for token in doc:
     print(token.text)
I
like
cats
.</pre>
			<p>The same logic applies to indexing:</p>
			<pre> doc[1]
like</pre>
			<p>The length of a Doc is the number of tokens it includes:</p>
			<pre> len(doc)
4</pre>
			<p>We already saw how to get the text's sentences. <code>doc.sents</code> returns an iterator to the list of sentences. Each sentence is a Span object:</p>
			<pre> doc = nlp("This is a sentence. This is the second sentence")
 doc.sents
&lt;generator object at 0x7f21dc565948&gt;
 sentences = list(doc.sents)
 sentences
["This is a sentence.", "This is the second sentence."]</pre>
			<p><code>doc.ents</code> gives named <a id="_idIndexMarker101"/>entities of the text. The result is a list of Span objects. We'll see <a id="_idIndexMarker102"/>named entities in detail later – for now, think of them as proper nouns:</p>
			<pre> doc = nlp("I flied to New York with Ashley.")
 doc.ents
(New York, Ashley)</pre>
			<p>Another syntactic property is <code>doc.noun_chunks</code>. It yields the noun phrases found in the text:</p>
			<pre> doc = nlp("Sweet brown fox jumped over the fence.")
 list(doc.noun_chunks)
[Sweet brown fox, the fence]</pre>
			<p><code>doc.lang_</code> returns the language that <code>doc</code> created:</p>
			<pre> doc.lang_
'en'</pre>
			<p>A useful method for serialization is <code>doc.to_json</code>. This is how to convert a Doc object to JSON:</p>
			<pre> doc = nlp("Hi")
 json_doc = doc.to_json()
{
  "text": "Hi",
  "ents": [],
  "sents": [{"start": 0, "end": 3}],
  "tokens": [{"id": 0, "start": 0, "end": 3, "pos": "INTJ", "tag": "UH", "dep": "ROOT", "head": 0}]
}</pre>
			<p class="callout-heading">Pro tip</p>
			<p class="callout">You might have noticed that we call <code>doc.lang_</code>, not <code>doc.lang</code>. <code>doc.lang</code> returns the language ID, whereas <code>doc.lang_</code> returns the Unicode string of the language, that is, the name of the language. You can see the same convention with Token features in the following, for instance, <code>token.lemma_</code>,<code> token.tag_</code>, and <code>token.pos_</code>.</p>
			<p>The Doc object has very useful <a id="_idIndexMarker103"/>properties with which you can <a id="_idIndexMarker104"/>understand a sentence's syntactic properties and use them in your own applications. Let's move on to the Token object and see what it offers.</p>
			<h2 id="_idParaDest-50"><a id="_idTextAnchor049"/>Token</h2>
			<p>A Token object <a id="_idIndexMarker105"/>represents a word. Token objects <a id="_idIndexMarker106"/>are the building blocks of Doc and Span objects. In this section, we will cover the following properties of the <code>Token</code> class:</p>
			<ul>
				<li><code>token.text</code></li>
				<li><code>token.text_with_ws </code></li>
				<li><code>token.i</code></li>
				<li><code>token.idx</code></li>
				<li><code>token.doc</code></li>
				<li><code>token.sent</code></li>
				<li><code>token.is_sent_start</code></li>
				<li><code>token.ent_type</code></li>
			</ul>
			<p>We usually don't <a id="_idIndexMarker107"/>construct a Token object directly, rather we construct a <a id="_idIndexMarker108"/>Doc object then access its tokens:</p>
			<pre> doc = nlp("Hello Madam!")
 doc[0]
Hello</pre>
			<p><code>token.text</code> is similar to <code>doc.text</code> and provides the underlying Unicode string:</p>
			<pre> doc[0].text
Hello</pre>
			<p><code>token.text_with_ws</code> is a similar property. It provides the text with a trailing whitespace if present in the <code>doc</code>:</p>
			<pre> doc[0].text_with_ws
'Hello '
 doc[2].text_with_ws
'!"</pre>
			<p>Finding the length of a token is similar to finding the length of a Python string:</p>
			<pre> len(doc[0])
5</pre>
			<p><code>token.i</code> gives the index of the token in <code>doc</code>:</p>
			<pre> token = doc[2]
 token.i
2</pre>
			<p><code>token.idx</code> provides the token's character offset (the character position) in <code>doc</code>:</p>
			<pre> doc[0].idx
0
 doc[1].idx
6</pre>
			<p>We can also access the <code>doc</code> that created the <code>token</code> as follows:</p>
			<pre> token = doc[0]
 token.doc
Hello Madam!</pre>
			<p>Getting the sentence that the <code>token</code> belongs to is done in a similar way to accessing the <code>doc</code> that created the <code>token</code>:</p>
			<pre> token = doc[1]
 token.sent
Hello Madam!</pre>
			<p><code>token.is_sent_start</code> is another <a id="_idIndexMarker109"/>useful property; it returns a <a id="_idIndexMarker110"/>Boolean indicating whether the token starts a sentence:</p>
			<pre> doc = nlp("He entered the room. Then he nodded.")
 doc[0].is_sent_start
True
 doc[5].is_sent_start
True
 doc[6].is_sent_start
False</pre>
			<p>These are the basic properties of the Token object that you'll use every day. There is another set of properties that are more related to syntax and semantics. We already saw how to calculate the token lemma in the previous section:</p>
			<pre> doc = nlp("I went there.")
 doc[1].lemma_
'go'</pre>
			<p>You already learned that <code>doc.ents</code> gives the named entities of the document. If you want to learn what sort of entity the token is, use <code>token.ent_type_</code>:</p>
			<pre> doc = nlp("President Trump visited Mexico City.")
 doc.ents
(Trump, Mexico City)
 doc[1].ent_type_
'PERSON'
 doc[3].ent_type_
'GPE'  # country, city, state
 doc[4].ent_type_
'GPE'  # country, city, state
 doc[0].ent_type_
''  # not an entity</pre>
			<p>Two syntactic features <a id="_idIndexMarker111"/>related to POS tagging are <code>token.pos_</code> and <code>token.tag</code>. We'll learn <a id="_idIndexMarker112"/>what they are and how to use them in the next chapter.</p>
			<p>Another set of syntactic features comes from the dependency parser. These features are <code>dep_</code>, <code>head_</code>, <code>conj_</code>, <code>lefts_</code>, <code>rights_</code>, <code>left_edge_</code>, and <code>right_edge_</code>. We'll cover them in the next chapter as well.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">It is totally normal if you don't remember all the features afterward. If you don't remember the name of a feature, you can always do <code>dir(token)</code> or <code>dir(doc)</code>. Calling <code>dir()</code> will print all the features and methods available on the object. </p>
			<p>The Token object has a rich set of features, enabling us to process the text from head to toe. Let's move on to the Span object and see what it offers for us.</p>
			<h2 id="_idParaDest-51"><a id="_idTextAnchor050"/>Span</h2>
			<p>Span objects represent <a id="_idIndexMarker113"/>phrases or segments of the text. Technically, a <a id="_idIndexMarker114"/>Span has to be a contiguous sequence of tokens. We usually don't initialize Span objects, rather we slice a Doc object:</p>
			<pre> doc = nlp("I know that you have been to USA.")
 doc[2:4]
"that you"</pre>
			<p>Trying to slice an invalid index will raise an <code>IndexError</code>. Most indexing and slicing rules of Python strings are applicable to Doc slicing as well:</p>
			<pre> doc = nlp("President Trump visited Mexico City.")
 doc[4:]  # end index empty means rest of the string
City.
 doc[3:-1]  # minus indexes are supported
 doc[6:]
Traceback (most recent call last):
  File "&lt;stdin&gt;", line 1, in &lt;module&gt;
  File "span.pyx", line 166, in spacy.tokens.span.Span.__repr__
  File "span.pyx", line 503, in spacy.tokens.span.Span.text.__get__
  File "span.pyx", line 190, in spacy.tokens.span.Span.__getitem__
IndexError: [E201] Span index out of range.
 doc[1:1]  # empty spans are not allowed
Traceback (most recent call last):
  File "&lt;stdin&gt;", line 1, in &lt;module&gt;
  File "span.pyx", line 166, in spacy.tokens.span.Span.__repr__
  File "span.pyx", line 503, in spacy.tokens.span.Span.text.__get__
  File "span.pyx", line 190, in spacy.tokens.span.Span.__getitem__
IndexError: [E201] Span index out of range.</pre>
			<p>There is one more way to <a id="_idIndexMarker115"/>create a Span – we can make a character-level slice of a Doc object with <code>char_span</code> :</p>
			<pre> doc = nlp("You love Atlanta since you're 20.")
 doc.char_span(4, 16)
love Atlanta</pre>
			<p>The building blocks of a <a id="_idIndexMarker116"/>Span object are Token objects. If you iterate over a Span object you get Token objects:</p>
			<pre> doc = nlp("You went there after you saw me")
 span = doc[2:4]
 for token in span:
     print(token)
there
after</pre>
			<p>You can think of the Span object as a <em class="italic">junior</em> Doc object, indeed it's a view of the Doc object it's created from. Hence most of the features of Doc are applicable to Span as well. For instance, <code>len</code> is identical:</p>
			<pre> doc = nlp("Hello Madam!")
 span = doc[1:2]
 len(span)
1</pre>
			<p>Span object also supports indexing. The result of slicing a Span object is another Span object:</p>
			<pre> doc = nlp("You went there after you saw me")
 span = doc[2:6]
 span 
there after you saw
 subspan = span[1:3]
after you</pre>
			<p><code>char_spans</code> also works on <a id="_idIndexMarker117"/>Span objects. Remember the <a id="_idIndexMarker118"/>Span class is a junior Doc class, so we can create character-indexed spans on Span objects as well:</p>
			<pre> doc = nlp("You went there after you saw me")
 span = doc[2:6]
 span.char_span(15,24)
after you</pre>
			<p>Just like a Token knows the Doc object it's created from; Span also knows the Doc object it's created from:</p>
			<pre> doc = nlp("You went there after you saw me")
 span = doc[2:6]
 span.doc
You went there after you saw me
 span.sent
You went there after you saw me</pre>
			<p>We can also locate the <code>Span</code> in the original <code>Doc</code>:</p>
			<pre> doc = nlp("You went there after you saw me")
 span = doc[2:6]
 span.start
2
 span.end
6
 span.start_char
9
 span.end_char
28</pre>
			<p><code>span.start</code> is the index of the <a id="_idIndexMarker119"/>first token of the Span and <code>span.start_char</code> is the start offset of the Span at character level. </p>
			<p>If you <a id="_idIndexMarker120"/>want a brand-new Doc object, you can call <code>span.as_doc()</code>. It copies the data into a new Doc object:</p>
			<pre> doc = nlp("You went there after you saw me") 
 span = doc[2:6]
 type(span)
&lt;class 'spacy.tokens.span.Span'&gt;
 small_doc = span.as_doc()
 type(small_doc)
&lt;class 'spacy.tokens.doc.Doc'&gt;</pre>
			<p><code>span.ents</code>, <code>span.sent</code>, <code>span.text</code>, and <code>span.text_wth_ws</code> are similar to their corresponding Doc and Token methods.</p>
			<p>Dear readers, we have reached the end of an exhaustive section. We'll now go through a few more features and methods for more detailed text analysis in the next section.</p>
			<h1 id="_idParaDest-52"><a id="_idTextAnchor051"/>More spaCy features</h1>
			<p>Most of the <a id="_idIndexMarker121"/>NLP development is token and span oriented; that is, it processes tags, dependency relations, tokens themselves, and phrases. Most of the time we eliminate small words and words without much meaning; we process URLs differently, and so on. What we do <a id="_idIndexMarker122"/>sometimes depends on the <strong class="bold">token shape</strong> (token is a short word or token looks like an URL string) or more semantical features (such as the token is an article, or the token is a conjunction). In this section, we will see these features of tokens with examples. We'll start with features related to the token shape:</p>
			<pre> doc = nlp("Hello, hi!")
 doc[0].lower_
'hello'</pre>
			<p><code>token.lower_</code> returns <a id="_idIndexMarker123"/>the token in lowercase. The return value is a Unicode string and this feature is equivalent to <code>token.text.lower()</code>.</p>
			<p><code>is_lower</code> and <code>is_upper</code> are similar to their Python string method counterparts, <code>islower()</code> and <code>isupper()</code>. <code>is_lower</code> returns <code>True</code> if all the characters are lowercase, while <code>is_upper</code> does the same with uppercase:</p>
			<pre> doc = nlp("HELLO, Hello, hello, hEllO")
 doc[0].is_upper
True
 doc[0].is_lower
False
 doc[1].is_upper
False
 doc[1].is_lower
False</pre>
			<p><code>is_alpha</code> returns <code>True</code> if all the characters of the token are alphabetic letters. Examples of nonalphabetic characters are numbers, punctuation, and whitespace: </p>
			<pre>doc = nlp("Cat and Cat123")
 doc[0].is_alpha
True
 doc[2].is_alpha
False</pre>
			<p><code>is_ascii</code> returns <code>True</code> if all the characters of token are ASCII characters.</p>
			<pre> doc = nlp("Hamburg and Göttingen")
 doc[0].is_ascii
True
 doc[2].is_ascii
False</pre>
			<p><code>is_digit</code> returns <code>True</code> if all the <a id="_idIndexMarker124"/>characters of the token are numbers:</p>
			<pre> doc = nlp("Cat Cat123 123")
 doc[0].is_digit
False
 doc[1].is_digit
False
 doc[2].is_digit
True</pre>
			<p><code>is_punct</code> returns <code>True</code> if the token is a punctuation mark:</p>
			<pre> doc = nlp("You, him and Sally")
 doc[1]
,
 doc[1].is_punct
True</pre>
			<p><code>is_left_punct</code> and <code>is_right_punct</code> return <code>True</code> if the token is a left punctuation mark or right punctuation mark, respectively. A right punctuation mark can be any mark that closes a left punctuation mark, such as right brackets, &gt; or ». Left punctuation marks are similar, with the left brackets &lt; and « as some examples:</p>
			<pre>doc = nlp("( [ He said yes. ] )")
doc[0]
(
doc[0].is_left_punct
True
doc[1]
[
doc[1].is_left_punct
True
doc[-1]
)
doc[-1].is_right_punct
True
doc[-2]
]
doc[-2].is_right_punct
True</pre>
			<p><code>is_space</code> returns <code>True</code> if the <a id="_idIndexMarker125"/>token is only whitespace characters:</p>
			<pre> doc = nlp(" ")
 doc[0]
 len(doc[0])
1
 doc[0].is_space
True
 doc = nlp("  ")
 doc[0]
 len(doc[0])
2
 doc[0].is_space
True</pre>
			<p><code>is_bracket</code> returns <code>True</code> for bracket characters:</p>
			<pre> doc = nlp("( You said [1] and {2} is not applicable.)")
 doc[0].is_bracket, doc[-1].is_bracket
(True, True)
 doc[3].is_bracket, doc[5].is_bracket
(True, True)
 doc[7].is_bracket, doc[9].is_bracket
(True, True)</pre>
			<p><code>is_quote</code> returns <code>True</code> for <a id="_idIndexMarker126"/>quotation marks:</p>
			<pre> doc = nlp("( You said '1\" is not applicable.)")
 doc[3]
'
 doc[3].is_quote
True
 doc[5]
"
 doc[5].is_quote
True</pre>
			<p><code>is_currency</code> returns <code>True</code> for currency symbols such as <code>$</code> and <code>€</code> (this method was implemented by myself):</p>
			<pre> doc = nlp("I paid 12$ for the tshirt.")
 doc[3]
$
 doc[3].is_currency
True</pre>
			<p><code>like_url</code>, <code>like_num</code>, and <code>like_email</code> are methods about the token shape and return <code>True</code> if the token <a id="_idIndexMarker127"/>looks like a URL, a number, or an email, respectively. These methods are very handy when we want to process social media text and scraped web pages:</p>
			<pre> doc = nlp("I emailed you at least 100 times")
 doc[-2]
100
 doc[-2].like_num
True
 doc = nlp("I emailed you at least hundred times")
 doc[-2]
hundred
 doc[-2].like_num
True doc = nlp("My email is duygu@packt.com and you can visit me under https://duygua.github.io any time you want.")
 doc[3]
duygu@packt.com
 doc[3].like_email
True
 doc[10]
https://duygua.github.io/
 doc[10].like_url
True</pre>
			<p><code>token.shape_</code> is an unusual feature – there is nothing similar in other NLP libraries. It returns a string that shows a token's orthographic features. Numbers are replaced with <code>d</code>, uppercase letters are <a id="_idIndexMarker128"/>replaced with <code>X</code>, and lowercase letters are replaced with <code>x</code>. You can use the result string as a feature in your machine learning algorithms, and token shapes can be correlated to text sentiment:</p>
			<pre> doc = nlp("Girl called Kathy has a nickname Cat123.")
 for token in doc:
     print(token.text, token.shape_)
Girl Xxxx
called xxxx
Kathy Xxxxx
has xxx
a x
nickname xxxx
Cat123 Xxxddd
. .</pre>
			<p><code>is_oov</code> and <code>is_stop</code> are semantic features, as opposed to the preceding shape features. <code>is_oov</code> returns <code>True</code> if the <a id="_idIndexMarker129"/>token is <strong class="bold">Out Of Vocabulary</strong> (<strong class="bold">OOV</strong>), that is, not in the Doc object's vocabulary. OOV words are unknown words to the language model, and thus also to the processing pipeline components:</p>
			<pre> doc = nlp("I visited Jenny at Mynks Resort")
 for token in doc:
     print(token, token.is_oov)
I False
visited False
Jenny False
at False
Mynks True
Resort False</pre>
			<p><code>is_stop</code> is a feature that is <a id="_idIndexMarker130"/>frequently used by machine learning algorithms. Often, we filter words that do not carry much meaning, such as <em class="italic">the</em>, <em class="italic">a</em>, <em class="italic">an</em>, <em class="italic">and</em>, <em class="italic">just</em>, <em class="italic">with</em>, and so on. Such words are called stop words. Each language has their own stop word list, and you can access English stop words here <a href="https://github.com/explosion/spaCy/blob/master/spacy/lang/en/stop_words.py">https://github.com/explosion/spaCy/blob/master/spacy/lang/en/stop_words.py</a>:</p>
			<pre> doc = nlpI just want to inform you that I was with the principle.")
 for token in doc:
     print(token, token.is_stop)
I True
just True
want False
to True
inform False
you True
that True
I True
was True
with True
the True
principle False
. False</pre>
			<p>We have exhausted the <a id="_idIndexMarker131"/>list of spaCy's syntactic, semantic, and orthographic features. Unsurprisingly, many methods focused on the Token object as a token is the syntactic unit of a text.</p>
			<h1 id="_idParaDest-53"><a id="_idTextAnchor052"/>Summary</h1>
			<p>We have now reached the end of an exhaustive chapter of spaCy core operations and the basic features of spaCy. This chapter gave you a comprehensive picture of spaCy library classes and methods. We made a deep dive into language processing pipelining and learned about pipeline components. We also covered a basic yet important syntactic task: tokenization. We continued with the linguistic concept of lemmatization and you learned a real-world application of a spaCy feature. We explored spaCy container classes in detail and finalized the chapter with precise and useful spaCy features. At this point, you have a good grasp of spaCy language pipelining and you are confident about accomplishing bigger tasks.</p>
			<p>In the next chapter, we will dive into spaCy's full linguistic power. You'll discover linguistic features including spaCy's most used features: the POS tagger, dependency parser, named entities, and entity linking.</p>
		</div>
	</div></body></html>