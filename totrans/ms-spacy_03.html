<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer036">
			<h1 id="_idParaDest-38"><a id="_idTextAnchor037"/>Chapter 2: Core Operations with spaCy</h1>
			<p>In this chapter, you will learn the core operations with spaCy, such as creating a language pipeline, tokenizing the text, and breaking the text into its sentences. </p>
			<p>First, you'll learn what a language processing pipeline is and the pipeline components. We'll continue with general spaCy conventions – important classes and class organization – to help you to better understand spaCy library organization and develop a solid understanding of the library itself. </p>
			<p>You will then learn about the first pipeline component – <strong class="bold">Tokenizer</strong>. You'll also learn about an important linguistic concept – <strong class="bold">lemmatization</strong> – along with its applications in <strong class="bold">natural language understanding</strong> (<strong class="bold">NLU</strong>). Following that, we will cover <strong class="bold">container classes</strong> and <strong class="bold">spaCy data structures</strong> in detail. We will finish the chapter with useful spaCy features that you'll use in everyday NLP development. </p>
			<p>We're going to cover the following main topics in this chapter:</p>
			<ul>
				<li>Overview of spaCy conventions</li>
				<li>Introducing tokenization</li>
				<li>Understanding lemmatization</li>
				<li>spaCy container objects</li>
				<li>More spaCy features</li>
			</ul>
			<h1 id="_idParaDest-39"><a id="_idTextAnchor038"/>Technical requirements </h1>
			<p>The chapter code can be found at the book's GitHub repository: <a href="https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter02">https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter02</a></p>
			<h1 id="_idParaDest-40"><a id="_idTextAnchor039"/>Overview of spaCy conventions</h1>
			<p>Every NLP application consists of <a id="_idIndexMarker059"/>several steps of processing the text. As you can see in the first chapter, we have always created instances called <strong class="source-inline">nlp</strong> and <strong class="source-inline">doc</strong>. But what did we do exactly?</p>
			<p>When we call <strong class="source-inline">nlp</strong> on our text, spaCy applies some processing steps. The first step is tokenization to produce a <strong class="source-inline">Doc</strong> object. The <strong class="source-inline">Doc</strong> object is then processed further with a <strong class="bold">tagger</strong>, a <strong class="bold">parser</strong>, and an <strong class="bold">entity recognizer</strong>. This way of <a id="_idIndexMarker060"/>processing <a id="_idIndexMarker061"/>the text is called a <strong class="bold">language processing pipeline</strong>. Each pipeline component <a id="_idIndexMarker062"/>returns the processed <strong class="source-inline">Doc</strong> and then passes it to the <a id="_idIndexMarker063"/>next component:</p>
			<div>
				<div id="_idContainer027" class="IMG---Figure">
					<img src="Images/B16570_02_01.jpg" alt="Figure 2.1 – A high-level view of the processing pipeline&#13;&#10;" width="1650" height="238"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.1 – A high-level view of the processing pipeline</p>
			<p>A spaCy pipeline object is created <a id="_idIndexMarker064"/>when we load a language model. We load an English model and initialize a pipeline in the following code segment:</p>
			<p class="source-code"> import spacy</p>
			<p class="source-code"> nlp = spacy.load("en_core_web_md")</p>
			<p class="source-code"> doc = nlp("I went there")</p>
			<p>What happened exactly in the preceding code is as follows: </p>
			<ol>
				<li>We started by importing <strong class="source-inline">spaCy</strong>. </li>
				<li>In the second line, <strong class="source-inline">spacy.load()</strong> returned a <strong class="source-inline">Language</strong> class instance, <strong class="source-inline">nlp</strong>. The <strong class="source-inline">Language</strong> class is <em class="italic">the text processing pipeline</em>. </li>
				<li>After that, we applied <strong class="source-inline">nlp</strong> on the sample sentence <strong class="source-inline">I went there</strong> and got a <strong class="source-inline">Doc</strong> class instance, <strong class="source-inline">doc</strong>.</li>
			</ol>
			<p>The <strong class="source-inline">Language</strong> class applies all of the preceding pipeline steps to your input sentence behind the scenes. After applying <strong class="source-inline">nlp</strong> to the sentence, the <strong class="source-inline">Doc</strong> object contains tokens that are tagged, lemmatized, and marked as entities if the token is an entity (we will go into detail about what are those and how it's done later). Each pipeline component has a well-defined task:</p>
			<div>
				<div id="_idContainer028" class="IMG---Figure">
					<img src="Images/B16570_02_02.jpg" alt="Figure 2.2 – Pipeline components and tasks&#13;&#10;" width="1608" height="580"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.2 – Pipeline components and tasks</p>
			<p>The spaCy language <a id="_idIndexMarker065"/>processing pipeline always <em class="italic">depends on the statistical model</em> and its capabilities. This is why we always load a language model with <strong class="source-inline">spacy.load()</strong> as the first step in our code. </p>
			<p>Each component corresponds to a <strong class="source-inline">spaCy</strong> class. <strong class="source-inline">spaCy</strong> classes have self-explanatory names such as <strong class="bold">Language</strong>, <strong class="bold">Doc</strong>, and <strong class="bold">Vocab</strong>. We already used <strong class="source-inline">Language</strong> and <strong class="source-inline">Doc</strong> classes – let's see all of the processing pipeline classes and their duties:</p>
			<div>
				<div id="_idContainer029" class="IMG---Figure">
					<img src="Images/B16570_02_03.jpg" alt="Figure 2.3 – spaCy processing pipeline classes&#13;&#10;" width="1648" height="1483"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.3 – spaCy processing pipeline classes</p>
			<p>Don't be intimated by the <a id="_idIndexMarker066"/>number of classes; each class has unique features to help you process your text better. </p>
			<p>There are more data structures to represent text data and language data. Container classes such as Doc hold information about sentences, words, and the text. There are also container classes other than Doc:</p>
			<div>
				<div id="_idContainer030" class="IMG---Figure">
					<img src="Images/B16570_02_04.jpg" alt="Figure 2.4 – spaCy container classes&#13;&#10;" width="1541" height="633"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.4 – spaCy container classes</p>
			<p>Finally, spaCy provides <a id="_idIndexMarker067"/>helper classes for vectors, language vocabulary, and annotations. We'll see the <strong class="source-inline">Vocab</strong> class often in this book. <strong class="source-inline">Vocab</strong> represents a language's vocabulary. Vocab contains all the words of the language model we loaded:</p>
			<div>
				<div id="_idContainer031" class="IMG---Figure">
					<img src="Images/B16570_02_05.jpg" alt="Figure 2.5 – spaCy helper classes&#13;&#10;" width="1393" height="808"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.5 – spaCy helper classes</p>
			<p>The spaCy library's backbone data structures are <strong class="source-inline">Doc</strong> and <strong class="source-inline">Vocab</strong>. The <strong class="source-inline">Doc</strong> object abstracts the text by <a id="_idIndexMarker068"/>owning the sequence of tokens and all their properties. The <strong class="source-inline">Vocab</strong> object provides a centralized set of strings and lexical attributes to all the other classes. This way spaCy avoids storing multiple copies of linguistic data:</p>
			<div>
				<div id="_idContainer032" class="IMG---Figure">
					<img src="Images/B16570_02_06.jpg" alt="Figure 2.6 – spaCy architecture&#13;&#10;" width="1631" height="1252"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.6 – spaCy architecture</p>
			<p>You can divide the objects composing the preceding spaCy architecture into two: <strong class="bold">containers</strong> and <strong class="bold">processing pipeline components</strong>. In this chapter, we'll first learn about two basic components, <strong class="bold">Tokenizer</strong> and <strong class="bold">Lemmatizer</strong>, then we'll explore <strong class="bold">Container</strong> objects further.</p>
			<p>spaCy does all these <a id="_idIndexMarker069"/>operations for us behind the scenes, allowing us to concentrate on our own application's development. With this level of abstraction, using spaCy for NLP application development is no coincidence. Let's start with the <strong class="source-inline">Tokenizer</strong> class and see what it offers for us; then we will explore all the container classes one by one throughout the chapter. </p>
			<h1 id="_idParaDest-41"><a id="_idTextAnchor040"/>Introducing tokenization</h1>
			<p>We saw in <em class="italic">Figure 2.1</em> that the first step in a <a id="_idIndexMarker070"/>text processing pipeline is tokenization. Tokenization is always the first operation because all the other operations require the tokens.</p>
			<p>Tokenization simply means splitting the sentence into its tokens. A <strong class="bold">token</strong> is a unit of semantics. You can <a id="_idIndexMarker071"/>think of a token as the smallest meaningful part of a piece of text. Tokens can be words, numbers, punctuation, currency symbols, and any other meaningful symbols that are the building blocks of a sentence. The following are examples of tokens:</p>
			<ul>
				<li><strong class="source-inline">USA</strong></li>
				<li><strong class="source-inline">N.Y.</strong></li>
				<li><strong class="source-inline">city</strong></li>
				<li><strong class="source-inline">33</strong></li>
				<li><strong class="source-inline">3rd</strong></li>
				<li><strong class="source-inline">! </strong></li>
				<li><strong class="source-inline">…</strong></li>
				<li><strong class="source-inline">?</strong></li>
				<li><strong class="source-inline">'s</strong></li>
			</ul>
			<p>Input to the spaCy tokenizer is a Unicode text and the result is a <strong class="source-inline">Doc</strong> object. The following code shows the tokenization process:</p>
			<p class="source-code"> import spacy</p>
			<p class="source-code"> nlp = spacy.load("en_core_web_md")</p>
			<p class="source-code"> doc = nlp("I own a ginger cat.")</p>
			<p class="source-code"> print ([token.text for token in doc])</p>
			<p class="source-code"> ['I', 'own', 'a', 'ginger', 'cat', '.']</p>
			<p>The following is what we just did: </p>
			<ol>
				<li value="1">We start by importing <strong class="source-inline">spaCy</strong>. </li>
				<li>Then we loaded the English language model via the <strong class="source-inline">en</strong> shortcut to create an instance of the <strong class="source-inline">nlp</strong> <strong class="source-inline">Language</strong> class. </li>
				<li>Next, we apply the <strong class="source-inline">nlp</strong> object to the input sentence to create a <strong class="source-inline">Doc</strong> object, <strong class="source-inline">doc</strong>. A <strong class="source-inline">Doc</strong> object is a container for a sequence of <strong class="source-inline">Token</strong> objects. spaCy generates the <strong class="source-inline">Token</strong> objects implicitly when we created the <strong class="source-inline">Doc</strong> object. </li>
				<li>Finally, we print a list of the preceding sentence's tokens. </li>
			</ol>
			<p>That's it, we made the <a id="_idIndexMarker072"/>tokenization with just three lines of code. You can visualize the tokenization with indexing as follows:</p>
			<div>
				<div id="_idContainer033" class="IMG---Figure">
					<img src="Images/B16570_02_07.jpg" alt="Figure 2.7 – Tokenization of “I own a ginger cat.”&#13;&#10;" width="732" height="203"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.7 – Tokenization of "I own a ginger cat."</p>
			<p>As the examples suggest, tokenization can indeed be tricky. There are many aspects we should pay attention to: punctuation, whitespaces, numbers, and so on. Splitting from the whitespaces with <strong class="source-inline">text.split(" ")</strong> might be tempting and looks like it is working for the example sentence <em class="italic">I own a ginger cat</em>. </p>
			<p>How about the sentence <strong class="source-inline">"It's been a crazy week!!!"</strong>? If we make a <strong class="source-inline">split(" ")</strong> the resulting tokens would be <strong class="source-inline">It's</strong>, <strong class="source-inline">been</strong>, <strong class="source-inline">a</strong>, <strong class="source-inline">crazy</strong>, <strong class="source-inline">week!!!</strong>, which is not what you want. First of all, <strong class="source-inline">It's</strong> is not one token, it's two tokens: <strong class="source-inline">it</strong> and <strong class="source-inline">'s</strong>. <strong class="source-inline">week!!!</strong> is not a valid token as the punctuation is not split correctly. Moreover, <strong class="source-inline">!!!</strong> should be tokenized <a id="_idIndexMarker073"/>per symbol and should generate three <em class="italic">!</em>'s. (This may not look like an important detail, but trust me, it is important for<em class="italic"> sentiment analysis</em>. We'll cover sentiment analysis in <a href="B16570_08_Final_JM_ePub.xhtml#_idTextAnchor137"><em class="italic">Chapter 8</em></a><em class="italic">, Text Classification with spaCy</em>.) Let's see what spaCy tokenizer has generated:</p>
			<p class="source-code"> import spacy</p>
			<p class="source-code"> nlp = spacy.load("en_core_web_md")</p>
			<p class="source-code"> doc = nlp("It's been a crazy week!!!")</p>
			<p class="source-code"> print ([token.text for token in doc])</p>
			<p class="source-code">['It', "'s", 'been', 'a', 'crazy', 'week', '!', '!', '!']</p>
			<p>This time the sentence is split as follows:</p>
			<div>
				<div id="_idContainer034" class="IMG---Figure">
					<img src="Images/B16570_02_08.jpg" alt="Figure 2.8 – Tokenization of apostrophe and punctuations marks&#13;&#10;" width="1066" height="221"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.8 – Tokenization of apostrophe and punctuations marks</p>
			<p>How does spaCy know where to split the sentence? Unlike other parts of the pipeline, the tokenizer doesn't need a statistical model. Tokenization is based on language-specific rules. You can see examples the language specified data here: <a href="https://github.com/explosion/spaCy/tree/master/spacy/lang">https://github.com/explosion/spaCy/tree/master/spacy/lang</a>. </p>
			<p>Tokenizer exceptions define rules for exceptions, such as <strong class="source-inline">it's</strong> , <strong class="source-inline">don't</strong> , <strong class="source-inline">won't</strong>, abbreviations, and so on. If you look at the rules for English: <a href="https://github.com/explosion/spaCy/blob/master/spacy/lang/en/tokenizer_exceptions.py">https://github.com/explosion/spaCy/blob/master/spacy/lang/en/tokenizer_exceptions.py</a>, you will see that rules look like <strong class="source-inline">{ORTH: "n't", LEMMA: "not"}</strong>, which describes the splitting rule for <strong class="source-inline">n't</strong> to the tokenizer. </p>
			<p>The prefixes, suffixes, and infixes mostly describe how to deal with punctuation – for example, we split at a <a id="_idIndexMarker074"/>period if it is at the end of the sentence, otherwise, most probably it's part of an abbreviation such as N.Y. and we shouldn't touch it. Here, <strong class="source-inline">ORTH</strong> means the text and <strong class="source-inline">LEMMA</strong> means the base word form without any inflections. The following example shows you the execution of the spaCy tokenization algorithm:</p>
			<div>
				<div id="_idContainer035" class="IMG---Figure">
					<img src="Images/B16570_02_09.jpg" alt="Figure 2.9 – spaCy performing tokenization with exception rules (image taken from spaCy tokenization guidelines (https://spacy.io/usage/linguistic-features#tokenization))" width="1273" height="911"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.9 – spaCy performing tokenization with exception rules (image taken from spaCy tokenization guidelines (https://spacy.io/usage/linguistic-features#tokenization))</p>
			<p>Tokenization rules depend on the grammatical rules of the individual language. Punctuation rules such as splitting periods, commas, or exclamation marks are more or less similar for many languages; however, some rules are specific to the individual language, such as <a id="_idIndexMarker075"/>abbreviation words and apostrophe usage. spaCy supports each language having its own specific rules by allowing hand-coded data and rules, as each language has its own subclass.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">spaCy provides non-destructive tokenization, which means that we <a id="_idIndexMarker076"/>always will be able to recover the original text from the tokens. Whitespace and punctuation information is preserved during tokenization, so the input text is preserved as it is.</p>
			<p>Every <strong class="source-inline">Language</strong> object contains a <strong class="source-inline">Tokenizer</strong> object. The <strong class="source-inline">Tokenizer</strong> class is the class that performs the tokenization. You don't often call this class directly when you create a <strong class="source-inline">Doc</strong> class instance, while Tokenizer class acts behind the scenes. When we want to customize the tokenization, we need to interact with this class. Let's see how it is done.</p>
			<h2 id="_idParaDest-42"><a id="_idTextAnchor041"/>Customizing the tokenizer</h2>
			<p>When we work with a <a id="_idIndexMarker077"/>specific domain such as medicine, insurance, or finance, we often come across words, abbreviations, and entities that needs special attention. Most domains that you'll process have characteristic words and phrases that need custom tokenization rules. Here's how to add a special case rule to an existing <strong class="source-inline">Tokenizer</strong> class instance:</p>
			<p class="source-code"> import spacy</p>
			<p class="source-code"> from spacy.symbols import ORTH</p>
			<p class="source-code"> nlp = spacy.load("en_core_web_md")</p>
			<p class="source-code"> doc = nlp("lemme that")</p>
			<p class="source-code"> print([w.text for w in doc])</p>
			<p class="source-code">['lemme', 'that']</p>
			<p class="source-code"> special_case = [{ORTH: "lem"}, {ORTH: "me"}]</p>
			<p class="source-code"> nlp.tokenizer.add_special_case("lemme", special_case)</p>
			<p class="source-code"> print([w.text for w in nlp("lemme that")])</p>
			<p class="source-code">['lem', 'me', 'that']</p>
			<p>Here is what we did: </p>
			<ol>
				<li value="1">We again started by importing <strong class="source-inline">spacy</strong>. </li>
				<li>Then, we imported the <strong class="source-inline">ORTH</strong> symbol, which means orthography; that is, text. </li>
				<li>We continued with creating a <strong class="source-inline">Language</strong> class object, <strong class="source-inline">nlp</strong>, and created a Doc object, <strong class="source-inline">doc</strong>. </li>
				<li>We defined a special case, where the word <strong class="source-inline">lemme</strong> should tokenize as two tokens, <strong class="source-inline">lem</strong> and <strong class="source-inline">me</strong>. </li>
				<li>We added the rule to the <strong class="source-inline">nlp</strong> object's tokenizer. </li>
				<li>The last line exhibits how the fresh rule works.</li>
			</ol>
			<p>When we define custom rules, punctuation <a id="_idIndexMarker078"/>splitting rules will still apply. Our special case will be recognized as a result, even if it's surrounded by punctuation. The tokenizer will divide punctuation step by step, and apply the same process to the remaining substring:</p>
			<p class="source-code"> print([w.text for w in nlp("lemme!")])</p>
			<p class="source-code">['lem', 'me', '!']</p>
			<p>If you define a special case rule with punctuation, the special case rule will take precedence over the punctuation splitting:</p>
			<p class="source-code"> nlp.tokenizer.add_special_case("...lemme...?", [{"ORTH": "...lemme...?"}])</p>
			<p class="source-code"> print([w.text for w in nlp("...lemme...?")])</p>
			<p class="source-code">'...lemme...?'</p>
			<p class="callout-heading">Pro tip</p>
			<p class="callout">Modify the tokenizer by adding new rules only if you really need to. Trust me, you can get quite unexpected results with custom rules. One of the cases where you really need it is when working with Twitter text, which is usually full of hashtags and special symbols. If you have social media text, first feed some sentences into the spaCy NLP pipeline and see how the tokenization works out.</p>
			<h2 id="_idParaDest-43"><a id="_idTextAnchor042"/>Debugging the tokenizer</h2>
			<p>The spaCy library has a <a id="_idIndexMarker079"/>tool for debugging: <strong class="source-inline">nlp.tokenizer.explain(sentence</strong>). It returns (<strong class="source-inline">tokenizer rule/pattern, token</strong>) <strong class="bold">tuples</strong> to help us <a id="_idIndexMarker080"/>understand what happened exactly during the tokenization. Let's see an example:</p>
			<p class="source-code"> import spacy</p>
			<p class="source-code"> nlp = spacy.load("en_core_web_md")</p>
			<p class="source-code"> text = "Let's go!"</p>
			<p class="source-code"> doc = nlp(text)</p>
			<p class="source-code"> tok_exp = nlp.tokenizer.explain(text)</p>
			<p class="source-code"> for t in tok_exp:</p>
			<p class="source-code">     print(t[1], "\t", t[0])</p>
			<p class="source-code">Let    SPECIAL-1</p>
			<p class="source-code">'s     SPECIAL-2</p>
			<p class="source-code">go     TOKEN</p>
			<p class="source-code">!      SUFFIX</p>
			<p>In the preceding code, we imported <strong class="source-inline">spacy</strong> and created a <strong class="source-inline">Language</strong> class instance, <strong class="source-inline">nlp</strong>, as usual. Then we created a <strong class="source-inline">Doc</strong> class instance with the sentence <strong class="source-inline">Let's go!</strong>. After that, we asked the <strong class="source-inline">Tokenizer</strong> class instance, <strong class="source-inline">tokenizer</strong>, of <strong class="source-inline">nlp</strong> for an explanation of the tokenization of this sentence. <strong class="source-inline">nlp.tokenizer.explain()</strong> explained the rules that the tokenizer used one by one.</p>
			<p>After splitting a sentence into its tokens, it's time to split a text into its sentences. </p>
			<h2 id="_idParaDest-44"><a id="_idTextAnchor043"/>Sentence segmentation</h2>
			<p>We saw that breaking a <a id="_idIndexMarker081"/>sentence into its tokens is not a straightforward task at all. How about breaking a text into sentences? It's indeed a bit more <a id="_idIndexMarker082"/>complicated to mark where a sentence starts and ends due to the same reasons of punctuation, abbreviations, and so on.</p>
			<p>A Doc object's sentences are available via the <strong class="source-inline">doc.sents</strong> property:</p>
			<p class="source-code"> import spacy</p>
			<p class="source-code"> nlp = spacy.load("en_core_web_md")</p>
			<p class="source-code"> text = "I flied to N.Y yesterday. It was around 5 pm."</p>
			<p class="source-code"> doc = nlp(text)</p>
			<p class="source-code"> for sent in doc.sents:</p>
			<p class="source-code">     print(sent.text)</p>
			<p class="source-code">I flied to N.Y yesterday.</p>
			<p class="source-code">It was around 5 pm.</p>
			<p>Determining sentence boundaries is a more complicated task than tokenization. As a result, spaCy uses the dependency parser to perform sentence segmentation. This is a unique feature of spaCy – no other library puts such a sophisticated idea into practice. The results are very accurate in general, unless you process text of a very specific genre, such as from the conversation domain, or social media text. </p>
			<p>Now we know how to segment a text into sentences and tokenize the sentences. We're ready to process the tokens one by one. Let's start with lemmatization, a commonly used operation in semantics including sentiment analysis. </p>
			<h1 id="_idParaDest-45"><a id="_idTextAnchor044"/>Understanding lemmatization</h1>
			<p>A <strong class="bold">lemma</strong> is the <a id="_idIndexMarker083"/>base form of a token. You can <a id="_idIndexMarker084"/>think of a lemma as the form in which the token appears in a dictionary. For instance, the lemma of <em class="italic">eating</em> is <em class="italic">eat</em>; the lemma of <em class="italic">eats</em> is <em class="italic">eat</em>; <em class="italic">ate</em> similarly maps to <em class="italic">eat</em>. Lemmatization is the process of reducing the word forms to their lemmas. The following code is a quick example of how to do lemmatization with spaCy:</p>
			<p class="source-code"> import spacy</p>
			<p class="source-code"> nlp = spacy.load("en_core_web_md")</p>
			<p class="source-code"> doc = nlp("I went there for working and worked for 3 years.")</p>
			<p class="source-code"> for token in doc:</p>
			<p class="source-code">     print(token.text, token.lemma_)</p>
			<p class="source-code">I <strong class="source-inline">-PRON-</strong></p>
			<p class="source-code">went go</p>
			<p class="source-code">there</p>
			<p class="source-code">for for</p>
			<p class="source-code">working work</p>
			<p class="source-code">and and</p>
			<p class="source-code">worked work</p>
			<p class="source-code">for for</p>
			<p class="source-code">3 3</p>
			<p class="source-code">years year</p>
			<p class="source-code">. .</p>
			<p>By now, you should be <a id="_idIndexMarker085"/>familiar with what the first three lines of the code do. Recall that we import the <strong class="source-inline">spacy</strong> library, load an English model using <strong class="source-inline">spacy.load</strong>, create a pipeline, and apply the pipeline to the preceding sentence to get a Doc object. Here we iterated over tokens to get their text and lemmas. </p>
			<p>In the first line you see <strong class="source-inline">–PRON-</strong>, which doesn't look like a real token. This is a <strong class="bold">pronoun lemma</strong>, a special <a id="_idIndexMarker086"/>token for lemmas of personal pronouns. This is an exception for semantic purposes: the personal pronouns <em class="italic">you</em>, <em class="italic">I</em>, <em class="italic">me</em>, <em class="italic">him</em>, <em class="italic">his</em>, and so on look different, but in terms of meaning, they're in the same group. spaCy offers this trick for the pronoun lemmas. </p>
			<p>No worries if all of this sounds too abstract – let's see lemmatization in action with a real-world example.</p>
			<h2 id="_idParaDest-46"><a id="_idTextAnchor045"/>Lemmatization in NLU</h2>
			<p>Lemmatization is an important step in NLU. We'll go <a id="_idIndexMarker087"/>over an example in this subsection. Suppose that you design an NLP pipeline for a ticket booking system. Your application processes a customer's sentence, extracts <a id="_idIndexMarker088"/>necessary information from it, and then passes it to the booking API.</p>
			<p>The NLP pipeline wants to extract the form of the travel (a flight, bus, or train), the destination city, and the date. The first thing the application needs to verify is the means of travel:</p>
			<p class="source-code">fly – flight – airway – airplane - plane</p>
			<p class="source-code">bus </p>
			<p class="source-code">railway – train </p>
			<p>We have this list of keywords and we want to recognize the means of travel by searching the tokens in the keywords list. The most compact way of doing this search is by looking up the token's lemma. Consider the following customer sentences:</p>
			<p class="source-code">List me all flights to Atlanta.</p>
			<p class="source-code">I need a flight to NY.</p>
			<p class="source-code">I flew to Atlanta yesterday evening and forgot my baggage.</p>
			<p>Here, we don't need to include all word forms of the verb <em class="italic">fly</em> (<em class="italic">fly</em>, <em class="italic">flying</em>, <em class="italic">flies</em>, <em class="italic">flew</em>, and <em class="italic">flown</em>) in the keywords list and similar for the word <strong class="source-inline">flight</strong>; we reduced all possible variants to the base forms – <em class="italic">fly</em> and <em class="italic">flight</em>. Don't think of English only; languages such as Spanish, German, and Finnish have many word forms from a single lemma as well.</p>
			<p>Lemmatization also comes in handy when we want to recognize the destination city. There are many nicknames available for global cities and the booking API can process only the official names. The default tokenizer and lemmatizer won't know the difference between the official name and the nickname. In this case, you can add special rules, as we saw in the <em class="italic">Introducing tokenization</em> section. The following code plays a small trick:</p>
			<p class="source-code"> import spacy</p>
			<p class="source-code"> from spacy.symbols import ORTH, LEMMA</p>
			<p class="source-code"> nlp = spacy.load('en')</p>
			<p class="source-code"> special_case = [{ORTH: 'Angeltown', LEMMA: 'Los Angeles'}]</p>
			<p class="source-code"> nlp.tokenizer.add_special_case(u'Angeltown', special_case)</p>
			<p class="source-code"> doc = nlp(u'I am flying to Angeltown')</p>
			<p class="source-code"> for token in doc:</p>
			<p class="source-code">     print(token.text, token.lemma_)</p>
			<p class="source-code">I -PRON-</p>
			<p class="source-code">am be</p>
			<p class="source-code">flying fly</p>
			<p class="source-code">to to</p>
			<p class="source-code">Angeltown Los Angeles</p>
			<p>We defined a <a id="_idIndexMarker089"/>special case for the word <strong class="source-inline">Angeltown</strong> by replacing its lemma with the official name <strong class="source-inline">Los Angeles</strong>. Then we added this <a id="_idIndexMarker090"/>special case to the <strong class="source-inline">Tokenizer</strong> instance. When we print the token lemmas, we see that <strong class="source-inline">Angeltown</strong> maps to <strong class="source-inline">Los Angeles</strong> as we wished.</p>
			<h2 id="_idParaDest-47"><a id="_idTextAnchor046"/>Understanding the difference between lemmatization and stemming</h2>
			<p>A lemma is the base form of a <a id="_idIndexMarker091"/>word and is always a <a id="_idIndexMarker092"/>member of the language's vocabulary. The stem does not have to be a valid word at all. For instance, the lemma of <em class="italic">improvement</em> is <em class="italic">improvement</em>, but the stem is <em class="italic">improv</em>. You can think of the stem as the smallest part of the word that carries the meaning. Compare the following examples:</p>
			<p class="source-code">Word                Lemma</p>
			<p class="source-code">university        university</p>
			<p class="source-code">universe          universe</p>
			<p class="source-code">universal         universal</p>
			<p class="source-code">universities     university</p>
			<p class="source-code">universes        universe</p>
			<p class="source-code">improvement  improvement</p>
			<p class="source-code">improvements improvements</p>
			<p class="source-code">improves         improve</p>
			<p>The preceding word-lemma examples <a id="_idIndexMarker093"/>show how lemma is calculated by following the grammatical rules of the language. Here, the lemma of a plural form is the <a id="_idIndexMarker094"/>singular form, and the lemma of a third-person verb is the base form of the verb. Let's compare them to the following examples of word-stem pairs:</p>
			<p class="source-code">Word                Stem</p>
			<p class="source-code">university        univers</p>
			<p class="source-code">universe          univer</p>
			<p class="source-code">universal         univers</p>
			<p class="source-code">universities     universi</p>
			<p class="source-code">universes        univers</p>
			<p class="source-code">improvement  improv</p>
			<p class="source-code">improvements improv</p>
			<p class="source-code">improves         improv</p>
			<p>The first and the most important point to notice in the preceding examples is that the lemma does not have to be a valid word in the language. The second point is that many words can map to the same stem. Also, words from different grammatical categories can map to the same stem; here for instance, the noun <em class="italic">improvement</em> and the verb <em class="italic">improves</em> both map to <em class="italic">improv</em>. </p>
			<p>Though stems are not valid words, they still carry meaning. That's why stemming is commonly used in NLU applications. </p>
			<p>Stemming algorithms don't know anything about the grammar of the language. This class of algorithms works rather by trimming some common suffixes and prefixes from the beginning or end of the word. </p>
			<p>Stemming algorithms are rough, they cut the word from head and tail. There are several stemming algorithms available for English, including Porter and Lancaster. You can play with different stemming algorithms on NLTK's demo page at <a href="https://text-processing.com/demo/stem/">https://text-processing.com/demo/stem/</a>.</p>
			<p>Lemmatization, on the <a id="_idIndexMarker095"/>other hand, takes the morphological analysis of the words into consideration. To do so, it is important to obtain the dictionaries for the <a id="_idIndexMarker096"/>algorithm to look through in order to link the form back to its lemma.</p>
			<p>spaCy provides lemmatization via dictionary lookup and each language has its own dictionary.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Both stemming and lemmatization have their own advantages. Stemming gives very good results if you apply only statistical algorithms to the text, without further semantic processing such as pattern lookup, entity extraction, coreference resolution, and so on. Also stemming can trim a big corpus to a more moderate size and give you a compact representation. If you also use linguistic features in your pipeline or make a keyword search, include lemmatization. Lemmatization algorithms are accurate but come with a cost in terms of computation.</p>
			<h1 id="_idParaDest-48"><a id="_idTextAnchor047"/>spaCy container objects</h1>
			<p>At the beginning of this chapter, we saw a list of container objects including <strong class="bold">Doc</strong>, <strong class="bold">Token</strong>, <strong class="bold">Span</strong>, and <strong class="bold">Lexeme</strong>. We <a id="_idIndexMarker097"/>already used Token and Doc in our code. In this subsection, we'll see the properties of the container objects in detail. </p>
			<p>Using container objects, we can access the linguistic properties that spaCy assigns to the text. A container object is a logical representation of the text units such as a document, a token, or a slice of the document. </p>
			<p>Container objects in spaCy follow the natural structure of the text: a document is composed of sentences and sentences are composed of tokens. </p>
			<p>We most widely use Doc, Token, and Span objects in development, which represent a document, a single token, and a phrase, respectively. A container can contain other containers, for instance a <a id="_idIndexMarker098"/>document contains tokens and spans.</p>
			<p>Let's explore each class and its useful properties one by one.</p>
			<h2 id="_idParaDest-49"><a id="_idTextAnchor048"/>Doc</h2>
			<p>We created <a id="_idIndexMarker099"/>Doc objects in <a id="_idIndexMarker100"/>our code to represent the text, so you might have already figured out that Doc represents a text. </p>
			<p>We already know how to create a Doc object:</p>
			<p class="source-code"> doc = nlp("I like cats.") </p>
			<p><strong class="source-inline">doc.text</strong> returns a Unicode representation of the document text:</p>
			<p class="source-code"> doc.text</p>
			<p class="source-code">I like cats.</p>
			<p>The building block of a Doc object is Token, hence when you iterate a Doc you get Token objects as items:</p>
			<p class="source-code"> for token in doc:</p>
			<p class="source-code">     print(token.text)</p>
			<p class="source-code">I</p>
			<p class="source-code">like</p>
			<p class="source-code">cats</p>
			<p class="source-code">.</p>
			<p>The same logic applies to indexing:</p>
			<p class="source-code"> doc[1]</p>
			<p class="source-code">like</p>
			<p>The length of a Doc is the number of tokens it includes:</p>
			<p class="source-code"> len(doc)</p>
			<p class="source-code">4</p>
			<p>We already saw how to get the text's sentences. <strong class="source-inline">doc.sents</strong> returns an iterator to the list of sentences. Each sentence is a Span object:</p>
			<p class="source-code"> doc = nlp("This is a sentence. This is the second sentence")</p>
			<p class="source-code"> doc.sents</p>
			<p class="source-code">&lt;generator object at 0x7f21dc565948&gt;</p>
			<p class="source-code"> sentences = list(doc.sents)</p>
			<p class="source-code"> sentences</p>
			<p class="source-code">["This is a sentence.", "This is the second sentence."]</p>
			<p><strong class="source-inline">doc.ents</strong> gives named <a id="_idIndexMarker101"/>entities of the text. The result is a list of Span objects. We'll see <a id="_idIndexMarker102"/>named entities in detail later – for now, think of them as proper nouns:</p>
			<p class="source-code"> doc = nlp("I flied to New York with Ashley.")</p>
			<p class="source-code"> doc.ents</p>
			<p class="source-code">(New York, Ashley)</p>
			<p>Another syntactic property is <strong class="source-inline">doc.noun_chunks</strong>. It yields the noun phrases found in the text:</p>
			<p class="source-code"> doc = nlp("Sweet brown fox jumped over the fence.")</p>
			<p class="source-code"> list(doc.noun_chunks)</p>
			<p class="source-code">[Sweet brown fox, the fence]</p>
			<p><strong class="source-inline">doc.lang_</strong> returns the language that <strong class="source-inline">doc</strong> created:</p>
			<p class="source-code"> doc.lang_</p>
			<p class="source-code">'en'</p>
			<p>A useful method for serialization is <strong class="source-inline">doc.to_json</strong>. This is how to convert a Doc object to JSON:</p>
			<p class="source-code"> doc = nlp("Hi")</p>
			<p class="source-code"> json_doc = doc.to_json()</p>
			<p class="source-code">{</p>
			<p class="source-code">  "text": "Hi",</p>
			<p class="source-code">  "ents": [],</p>
			<p class="source-code">  "sents": [{"start": 0, "end": 3}],</p>
			<p class="source-code">  "tokens": [{"id": 0, "start": 0, "end": 3, "pos": "INTJ", "tag": "UH", "dep": "ROOT", "head": 0}]</p>
			<p class="source-code">}</p>
			<p class="callout-heading">Pro tip</p>
			<p class="callout">You might have noticed that we call <strong class="source-inline">doc.lang_</strong>, not <strong class="source-inline">doc.lang</strong>. <strong class="source-inline">doc.lang</strong> returns the language ID, whereas <strong class="source-inline">doc.lang_</strong> returns the Unicode string of the language, that is, the name of the language. You can see the same convention with Token features in the following, for instance, <strong class="source-inline">token.lemma_</strong>,<strong class="source-inline"> token.tag_</strong>, and <strong class="source-inline">token.pos_</strong>.</p>
			<p>The Doc object has very useful <a id="_idIndexMarker103"/>properties with which you can <a id="_idIndexMarker104"/>understand a sentence's syntactic properties and use them in your own applications. Let's move on to the Token object and see what it offers.</p>
			<h2 id="_idParaDest-50"><a id="_idTextAnchor049"/>Token</h2>
			<p>A Token object <a id="_idIndexMarker105"/>represents a word. Token objects <a id="_idIndexMarker106"/>are the building blocks of Doc and Span objects. In this section, we will cover the following properties of the <strong class="source-inline">Token</strong> class:</p>
			<ul>
				<li><strong class="source-inline">token.text</strong></li>
				<li><strong class="source-inline">token.text_with_ws </strong></li>
				<li><strong class="source-inline">token.i</strong></li>
				<li><strong class="source-inline">token.idx</strong></li>
				<li><strong class="source-inline">token.doc</strong></li>
				<li><strong class="source-inline">token.sent</strong></li>
				<li><strong class="source-inline">token.is_sent_start</strong></li>
				<li><strong class="source-inline">token.ent_type</strong></li>
			</ul>
			<p>We usually don't <a id="_idIndexMarker107"/>construct a Token object directly, rather we construct a <a id="_idIndexMarker108"/>Doc object then access its tokens:</p>
			<p class="source-code"> doc = nlp("Hello Madam!")</p>
			<p class="source-code"> doc[0]</p>
			<p class="source-code">Hello</p>
			<p><strong class="source-inline">token.text</strong> is similar to <strong class="source-inline">doc.text</strong> and provides the underlying Unicode string:</p>
			<p class="source-code"> doc[0].text</p>
			<p class="source-code">Hello</p>
			<p><strong class="source-inline">token.text_with_ws</strong> is a similar property. It provides the text with a trailing whitespace if present in the <strong class="source-inline">doc</strong>:</p>
			<p class="source-code"> doc[0].text_with_ws</p>
			<p class="source-code">'Hello '</p>
			<p class="source-code"> doc[2].text_with_ws</p>
			<p class="source-code">'!"</p>
			<p>Finding the length of a token is similar to finding the length of a Python string:</p>
			<p class="source-code"> len(doc[0])</p>
			<p class="source-code">5</p>
			<p><strong class="source-inline">token.i</strong> gives the index of the token in <strong class="source-inline">doc</strong>:</p>
			<p class="source-code"> token = doc[2]</p>
			<p class="source-code"> token.i</p>
			<p class="source-code">2</p>
			<p><strong class="source-inline">token.idx</strong> provides the token's character offset (the character position) in <strong class="source-inline">doc</strong>:</p>
			<p class="source-code"> doc[0].idx</p>
			<p class="source-code">0</p>
			<p class="source-code"> doc[1].idx</p>
			<p class="source-code">6</p>
			<p>We can also access the <strong class="source-inline">doc</strong> that created the <strong class="source-inline">token</strong> as follows:</p>
			<p class="source-code"> token = doc[0]</p>
			<p class="source-code"> token.doc</p>
			<p class="source-code">Hello Madam!</p>
			<p>Getting the sentence that the <strong class="source-inline">token</strong> belongs to is done in a similar way to accessing the <strong class="source-inline">doc</strong> that created the <strong class="source-inline">token</strong>:</p>
			<p class="source-code"> token = doc[1]</p>
			<p class="source-code"> token.sent</p>
			<p class="source-code">Hello Madam!</p>
			<p><strong class="source-inline">token.is_sent_start</strong> is another <a id="_idIndexMarker109"/>useful property; it returns a <a id="_idIndexMarker110"/>Boolean indicating whether the token starts a sentence:</p>
			<p class="source-code"> doc = nlp("He entered the room. Then he nodded.")</p>
			<p class="source-code"> doc[0].is_sent_start</p>
			<p class="source-code">True</p>
			<p class="source-code"> doc[5].is_sent_start</p>
			<p class="source-code">True</p>
			<p class="source-code"> doc[6].is_sent_start</p>
			<p class="source-code">False</p>
			<p>These are the basic properties of the Token object that you'll use every day. There is another set of properties that are more related to syntax and semantics. We already saw how to calculate the token lemma in the previous section:</p>
			<p class="source-code"> doc = nlp("I went there.")</p>
			<p class="source-code"> doc[1].lemma_</p>
			<p class="source-code">'go'</p>
			<p>You already learned that <strong class="source-inline">doc.ents</strong> gives the named entities of the document. If you want to learn what sort of entity the token is, use <strong class="source-inline">token.ent_type_</strong>:</p>
			<p class="source-code"> doc = nlp("President Trump visited Mexico City.")</p>
			<p class="source-code"> doc.ents</p>
			<p class="source-code">(Trump, Mexico City)</p>
			<p class="source-code"> doc[1].ent_type_</p>
			<p class="source-code">'PERSON'</p>
			<p class="source-code"> doc[3].ent_type_</p>
			<p class="source-code">'GPE'  # country, city, state</p>
			<p class="source-code"> doc[4].ent_type_</p>
			<p class="source-code">'GPE'  # country, city, state</p>
			<p class="source-code"> doc[0].ent_type_</p>
			<p class="source-code">''  # not an entity</p>
			<p>Two syntactic features <a id="_idIndexMarker111"/>related to POS tagging are <strong class="source-inline">token.pos_</strong> and <strong class="source-inline">token.tag</strong>. We'll learn <a id="_idIndexMarker112"/>what they are and how to use them in the next chapter.</p>
			<p>Another set of syntactic features comes from the dependency parser. These features are <strong class="source-inline">dep_</strong>, <strong class="source-inline">head_</strong>, <strong class="source-inline">conj_</strong>, <strong class="source-inline">lefts_</strong>, <strong class="source-inline">rights_</strong>, <strong class="source-inline">left_edge_</strong>, and <strong class="source-inline">right_edge_</strong>. We'll cover them in the next chapter as well.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">It is totally normal if you don't remember all the features afterward. If you don't remember the name of a feature, you can always do <strong class="source-inline">dir(token)</strong> or <strong class="source-inline">dir(doc)</strong>. Calling <strong class="source-inline">dir()</strong> will print all the features and methods available on the object. </p>
			<p>The Token object has a rich set of features, enabling us to process the text from head to toe. Let's move on to the Span object and see what it offers for us.</p>
			<h2 id="_idParaDest-51"><a id="_idTextAnchor050"/>Span</h2>
			<p>Span objects represent <a id="_idIndexMarker113"/>phrases or segments of the text. Technically, a <a id="_idIndexMarker114"/>Span has to be a contiguous sequence of tokens. We usually don't initialize Span objects, rather we slice a Doc object:</p>
			<p class="source-code"> doc = nlp("I know that you have been to USA.")</p>
			<p class="source-code"> doc[2:4]</p>
			<p class="source-code">"that you"</p>
			<p>Trying to slice an invalid index will raise an <strong class="source-inline">IndexError</strong>. Most indexing and slicing rules of Python strings are applicable to Doc slicing as well:</p>
			<p class="source-code"> doc = nlp("President Trump visited Mexico City.")</p>
			<p class="source-code"> doc[4:]  # end index empty means rest of the string</p>
			<p class="source-code">City.</p>
			<p class="source-code"> doc[3:-1]  # minus indexes are supported</p>
			<p class="source-code"> doc[6:]</p>
			<p class="source-code">Traceback (most recent call last):</p>
			<p class="source-code">  File "&lt;stdin&gt;", line 1, in &lt;module&gt;</p>
			<p class="source-code">  File "span.pyx", line 166, in spacy.tokens.span.Span.__repr__</p>
			<p class="source-code">  File "span.pyx", line 503, in spacy.tokens.span.Span.text.__get__</p>
			<p class="source-code">  File "span.pyx", line 190, in spacy.tokens.span.Span.__getitem__</p>
			<p class="source-code">IndexError: [E201] Span index out of range.</p>
			<p class="source-code"> doc[1:1]  # empty spans are not allowed</p>
			<p class="source-code">Traceback (most recent call last):</p>
			<p class="source-code">  File "&lt;stdin&gt;", line 1, in &lt;module&gt;</p>
			<p class="source-code">  File "span.pyx", line 166, in spacy.tokens.span.Span.__repr__</p>
			<p class="source-code">  File "span.pyx", line 503, in spacy.tokens.span.Span.text.__get__</p>
			<p class="source-code">  File "span.pyx", line 190, in spacy.tokens.span.Span.__getitem__</p>
			<p class="source-code">IndexError: [E201] Span index out of range.</p>
			<p>There is one more way to <a id="_idIndexMarker115"/>create a Span – we can make a character-level slice of a Doc object with <strong class="source-inline">char_span</strong> :</p>
			<p class="source-code"> doc = nlp("You love Atlanta since you're 20.")</p>
			<p class="source-code"> doc.char_span(4, 16)</p>
			<p class="source-code">love Atlanta</p>
			<p>The building blocks of a <a id="_idIndexMarker116"/>Span object are Token objects. If you iterate over a Span object you get Token objects:</p>
			<p class="source-code"> doc = nlp("You went there after you saw me")</p>
			<p class="source-code"> span = doc[2:4]</p>
			<p class="source-code"> for token in span:</p>
			<p class="source-code">     print(token)</p>
			<p class="source-code">there</p>
			<p class="source-code">after</p>
			<p>You can think of the Span object as a <em class="italic">junior</em> Doc object, indeed it's a view of the Doc object it's created from. Hence most of the features of Doc are applicable to Span as well. For instance, <strong class="source-inline">len</strong> is identical:</p>
			<p class="source-code"> doc = nlp("Hello Madam!")</p>
			<p class="source-code"> span = doc[1:2]</p>
			<p class="source-code"> len(span)</p>
			<p class="source-code">1</p>
			<p>Span object also supports indexing. The result of slicing a Span object is another Span object:</p>
			<p class="source-code"> doc = nlp("You went there after you saw me")</p>
			<p class="source-code"> span = doc[2:6]</p>
			<p class="source-code"> span </p>
			<p class="source-code">there after you saw</p>
			<p class="source-code"> subspan = span[1:3]</p>
			<p class="source-code">after you</p>
			<p><strong class="source-inline">char_spans</strong> also works on <a id="_idIndexMarker117"/>Span objects. Remember the <a id="_idIndexMarker118"/>Span class is a junior Doc class, so we can create character-indexed spans on Span objects as well:</p>
			<p class="source-code"> doc = nlp("You went there after you saw me")</p>
			<p class="source-code"> span = doc[2:6]</p>
			<p class="source-code"> span.char_span(15,24)</p>
			<p class="source-code">after you</p>
			<p>Just like a Token knows the Doc object it's created from; Span also knows the Doc object it's created from:</p>
			<p class="source-code"> doc = nlp("You went there after you saw me")</p>
			<p class="source-code"> span = doc[2:6]</p>
			<p class="source-code"> span.doc</p>
			<p class="source-code">You went there after you saw me</p>
			<p class="source-code"> span.sent</p>
			<p class="source-code">You went there after you saw me</p>
			<p>We can also locate the <strong class="source-inline">Span</strong> in the original <strong class="source-inline">Doc</strong>:</p>
			<p class="source-code"> doc = nlp("You went there after you saw me")</p>
			<p class="source-code"> span = doc[2:6]</p>
			<p class="source-code"> span.start</p>
			<p class="source-code">2</p>
			<p class="source-code"> span.end</p>
			<p class="source-code">6</p>
			<p class="source-code"> span.start_char</p>
			<p class="source-code">9</p>
			<p class="source-code"> span.end_char</p>
			<p class="source-code">28</p>
			<p><strong class="source-inline">span.start</strong> is the index of the <a id="_idIndexMarker119"/>first token of the Span and <strong class="source-inline">span.start_char</strong> is the start offset of the Span at character level. </p>
			<p>If you <a id="_idIndexMarker120"/>want a brand-new Doc object, you can call <strong class="source-inline">span.as_doc()</strong>. It copies the data into a new Doc object:</p>
			<p class="source-code"> doc = nlp("You went there after you saw me") </p>
			<p class="source-code"> span = doc[2:6]</p>
			<p class="source-code"> type(span)</p>
			<p class="source-code">&lt;class 'spacy.tokens.span.Span'&gt;</p>
			<p class="source-code"> small_doc = span.as_doc()</p>
			<p class="source-code"> type(small_doc)</p>
			<p class="source-code">&lt;class 'spacy.tokens.doc.Doc'&gt;</p>
			<p><strong class="source-inline">span.ents</strong>, <strong class="source-inline">span.sent</strong>, <strong class="source-inline">span.text</strong>, and <strong class="source-inline">span.text_wth_ws</strong> are similar to their corresponding Doc and Token methods.</p>
			<p>Dear readers, we have reached the end of an exhaustive section. We'll now go through a few more features and methods for more detailed text analysis in the next section.</p>
			<h1 id="_idParaDest-52"><a id="_idTextAnchor051"/>More spaCy features</h1>
			<p>Most of the <a id="_idIndexMarker121"/>NLP development is token and span oriented; that is, it processes tags, dependency relations, tokens themselves, and phrases. Most of the time we eliminate small words and words without much meaning; we process URLs differently, and so on. What we do <a id="_idIndexMarker122"/>sometimes depends on the <strong class="bold">token shape</strong> (token is a short word or token looks like an URL string) or more semantical features (such as the token is an article, or the token is a conjunction). In this section, we will see these features of tokens with examples. We'll start with features related to the token shape:</p>
			<p class="source-code"> doc = nlp("Hello, hi!")</p>
			<p class="source-code"> doc[0].lower_</p>
			<p class="source-code">'hello'</p>
			<p><strong class="source-inline">token.lower_</strong> returns <a id="_idIndexMarker123"/>the token in lowercase. The return value is a Unicode string and this feature is equivalent to <strong class="source-inline">token.text.lower()</strong>.</p>
			<p><strong class="source-inline">is_lower</strong> and <strong class="source-inline">is_upper</strong> are similar to their Python string method counterparts, <strong class="source-inline">islower()</strong> and <strong class="source-inline">isupper()</strong>. <strong class="source-inline">is_lower</strong> returns <strong class="source-inline">True</strong> if all the characters are lowercase, while <strong class="source-inline">is_upper</strong> does the same with uppercase:</p>
			<p class="source-code"> doc = nlp("HELLO, Hello, hello, hEllO")</p>
			<p class="source-code"> doc[0].is_upper</p>
			<p class="source-code">True</p>
			<p class="source-code"> doc[0].is_lower</p>
			<p class="source-code">False</p>
			<p class="source-code"> doc[1].is_upper</p>
			<p class="source-code">False</p>
			<p class="source-code"> doc[1].is_lower</p>
			<p class="source-code">False</p>
			<p><strong class="source-inline">is_alpha</strong> returns <strong class="source-inline">True</strong> if all the characters of the token are alphabetic letters. Examples of nonalphabetic characters are numbers, punctuation, and whitespace: </p>
			<p class="source-code">doc = nlp("Cat and Cat123")</p>
			<p class="source-code"> doc[0].is_alpha</p>
			<p class="source-code">True</p>
			<p class="source-code"> doc[2].is_alpha</p>
			<p class="source-code">False</p>
			<p><strong class="source-inline">is_ascii</strong> returns <strong class="source-inline">True</strong> if all the characters of token are ASCII characters.</p>
			<p class="source-code"> doc = nlp("Hamburg and Göttingen")</p>
			<p class="source-code"> doc[0].is_ascii</p>
			<p class="source-code">True</p>
			<p class="source-code"> doc[2].is_ascii</p>
			<p class="source-code">False</p>
			<p><strong class="source-inline">is_digit</strong> returns <strong class="source-inline">True</strong> if all the <a id="_idIndexMarker124"/>characters of the token are numbers:</p>
			<p class="source-code"> doc = nlp("Cat Cat123 123")</p>
			<p class="source-code"> doc[0].is_digit</p>
			<p class="source-code">False</p>
			<p class="source-code"> doc[1].is_digit</p>
			<p class="source-code">False</p>
			<p class="source-code"> doc[2].is_digit</p>
			<p class="source-code">True</p>
			<p><strong class="source-inline">is_punct</strong> returns <strong class="source-inline">True</strong> if the token is a punctuation mark:</p>
			<p class="source-code"> doc = nlp("You, him and Sally")</p>
			<p class="source-code"> doc[1]</p>
			<p class="source-code">,</p>
			<p class="source-code"> doc[1].is_punct</p>
			<p class="source-code">True</p>
			<p><strong class="source-inline">is_left_punct</strong> and <strong class="source-inline">is_right_punct</strong> return <strong class="source-inline">True</strong> if the token is a left punctuation mark or right punctuation mark, respectively. A right punctuation mark can be any mark that closes a left punctuation mark, such as right brackets, &gt; or ». Left punctuation marks are similar, with the left brackets &lt; and « as some examples:</p>
			<p class="source-code">doc = nlp("( [ He said yes. ] )")</p>
			<p class="source-code">doc[0]</p>
			<p class="source-code">(</p>
			<p class="source-code">doc[0].is_left_punct</p>
			<p class="source-code">True</p>
			<p class="source-code">doc[1]</p>
			<p class="source-code">[</p>
			<p class="source-code">doc[1].is_left_punct</p>
			<p class="source-code">True</p>
			<p class="source-code">doc[-1]</p>
			<p class="source-code">)</p>
			<p class="source-code">doc[-1].is_right_punct</p>
			<p class="source-code">True</p>
			<p class="source-code">doc[-2]</p>
			<p class="source-code">]</p>
			<p class="source-code">doc[-2].is_right_punct</p>
			<p class="source-code">True</p>
			<p><strong class="source-inline">is_space</strong> returns <strong class="source-inline">True</strong> if the <a id="_idIndexMarker125"/>token is only whitespace characters:</p>
			<p class="source-code"> doc = nlp(" ")</p>
			<p class="source-code"> doc[0]</p>
			<p class="source-code"> len(doc[0])</p>
			<p class="source-code">1</p>
			<p class="source-code"> doc[0].is_space</p>
			<p class="source-code">True</p>
			<p class="source-code"> doc = nlp("  ")</p>
			<p class="source-code"> doc[0]</p>
			<p class="source-code"> len(doc[0])</p>
			<p class="source-code">2</p>
			<p class="source-code"> doc[0].is_space</p>
			<p class="source-code">True</p>
			<p><strong class="source-inline">is_bracket</strong> returns <strong class="source-inline">True</strong> for bracket characters:</p>
			<p class="source-code"> doc = nlp("( You said [1] and {2} is not applicable.)")</p>
			<p class="source-code"> doc[0].is_bracket, doc[-1].is_bracket</p>
			<p class="source-code">(True, True)</p>
			<p class="source-code"> doc[3].is_bracket, doc[5].is_bracket</p>
			<p class="source-code">(True, True)</p>
			<p class="source-code"> doc[7].is_bracket, doc[9].is_bracket</p>
			<p class="source-code">(True, True)</p>
			<p><strong class="source-inline">is_quote</strong> returns <strong class="source-inline">True</strong> for <a id="_idIndexMarker126"/>quotation marks:</p>
			<p class="source-code"> doc = nlp("( You said '1\" is not applicable.)")</p>
			<p class="source-code"> doc[3]</p>
			<p class="source-code">'</p>
			<p class="source-code"> doc[3].is_quote</p>
			<p class="source-code">True</p>
			<p class="source-code"> doc[5]</p>
			<p class="source-code">"</p>
			<p class="source-code"> doc[5].is_quote</p>
			<p class="source-code">True</p>
			<p><strong class="source-inline">is_currency</strong> returns <strong class="source-inline">True</strong> for currency symbols such as <strong class="source-inline">$</strong> and <strong class="source-inline">€</strong> (this method was implemented by myself):</p>
			<p class="source-code"> doc = nlp("I paid 12$ for the tshirt.")</p>
			<p class="source-code"> doc[3]</p>
			<p class="source-code">$</p>
			<p class="source-code"> doc[3].is_currency</p>
			<p class="source-code">True</p>
			<p><strong class="source-inline">like_url</strong>, <strong class="source-inline">like_num</strong>, and <strong class="source-inline">like_email</strong> are methods about the token shape and return <strong class="source-inline">True</strong> if the token <a id="_idIndexMarker127"/>looks like a URL, a number, or an email, respectively. These methods are very handy when we want to process social media text and scraped web pages:</p>
			<p class="source-code"> doc = nlp("I emailed you at least 100 times")</p>
			<p class="source-code"> doc[-2]</p>
			<p class="source-code">100</p>
			<p class="source-code"> doc[-2].like_num</p>
			<p class="source-code">True</p>
			<p class="source-code"> doc = nlp("I emailed you at least hundred times")</p>
			<p class="source-code"> doc[-2]</p>
			<p class="source-code">hundred</p>
			<p class="source-code"> doc[-2].like_num</p>
			<p class="source-code">True doc = nlp("My email is duygu@packt.com and you can visit me under https://duygua.github.io any time you want.")</p>
			<p class="source-code"> doc[3]</p>
			<p class="source-code">duygu@packt.com</p>
			<p class="source-code"> doc[3].like_email</p>
			<p class="source-code">True</p>
			<p class="source-code"> doc[10]</p>
			<p class="source-code">https://duygua.github.io/</p>
			<p class="source-code"> doc[10].like_url</p>
			<p class="source-code">True</p>
			<p><strong class="source-inline">token.shape_</strong> is an unusual feature – there is nothing similar in other NLP libraries. It returns a string that shows a token's orthographic features. Numbers are replaced with <strong class="source-inline">d</strong>, uppercase letters are <a id="_idIndexMarker128"/>replaced with <strong class="source-inline">X</strong>, and lowercase letters are replaced with <strong class="source-inline">x</strong>. You can use the result string as a feature in your machine learning algorithms, and token shapes can be correlated to text sentiment:</p>
			<p class="source-code"> doc = nlp("Girl called Kathy has a nickname Cat123.")</p>
			<p class="source-code"> for token in doc:</p>
			<p class="source-code">     print(token.text, token.shape_)</p>
			<p class="source-code">Girl Xxxx</p>
			<p class="source-code">called xxxx</p>
			<p class="source-code">Kathy Xxxxx</p>
			<p class="source-code">has xxx</p>
			<p class="source-code">a x</p>
			<p class="source-code">nickname xxxx</p>
			<p class="source-code">Cat123 Xxxddd</p>
			<p class="source-code">. .</p>
			<p><strong class="source-inline">is_oov</strong> and <strong class="source-inline">is_stop</strong> are semantic features, as opposed to the preceding shape features. <strong class="source-inline">is_oov</strong> returns <strong class="source-inline">True</strong> if the <a id="_idIndexMarker129"/>token is <strong class="bold">Out Of Vocabulary</strong> (<strong class="bold">OOV</strong>), that is, not in the Doc object's vocabulary. OOV words are unknown words to the language model, and thus also to the processing pipeline components:</p>
			<p class="source-code"> doc = nlp("I visited Jenny at Mynks Resort")</p>
			<p class="source-code"> for token in doc:</p>
			<p class="source-code">     print(token, token.is_oov)</p>
			<p class="source-code">I False</p>
			<p class="source-code">visited False</p>
			<p class="source-code">Jenny False</p>
			<p class="source-code">at False</p>
			<p class="source-code">Mynks True</p>
			<p class="source-code">Resort False</p>
			<p><strong class="source-inline">is_stop</strong> is a feature that is <a id="_idIndexMarker130"/>frequently used by machine learning algorithms. Often, we filter words that do not carry much meaning, such as <em class="italic">the</em>, <em class="italic">a</em>, <em class="italic">an</em>, <em class="italic">and</em>, <em class="italic">just</em>, <em class="italic">with</em>, and so on. Such words are called stop words. Each language has their own stop word list, and you can access English stop words here <a href="https://github.com/explosion/spaCy/blob/master/spacy/lang/en/stop_words.py">https://github.com/explosion/spaCy/blob/master/spacy/lang/en/stop_words.py</a>:</p>
			<p class="source-code"> doc = nlpI just want to inform you that I was with the principle.")</p>
			<p class="source-code"> for token in doc:</p>
			<p class="source-code">     print(token, token.is_stop)</p>
			<p class="source-code">I True</p>
			<p class="source-code">just True</p>
			<p class="source-code">want False</p>
			<p class="source-code">to True</p>
			<p class="source-code">inform False</p>
			<p class="source-code">you True</p>
			<p class="source-code">that True</p>
			<p class="source-code">I True</p>
			<p class="source-code">was True</p>
			<p class="source-code">with True</p>
			<p class="source-code">the True</p>
			<p class="source-code">principle False</p>
			<p class="source-code">. False</p>
			<p>We have exhausted the <a id="_idIndexMarker131"/>list of spaCy's syntactic, semantic, and orthographic features. Unsurprisingly, many methods focused on the Token object as a token is the syntactic unit of a text.</p>
			<h1 id="_idParaDest-53"><a id="_idTextAnchor052"/>Summary</h1>
			<p>We have now reached the end of an exhaustive chapter of spaCy core operations and the basic features of spaCy. This chapter gave you a comprehensive picture of spaCy library classes and methods. We made a deep dive into language processing pipelining and learned about pipeline components. We also covered a basic yet important syntactic task: tokenization. We continued with the linguistic concept of lemmatization and you learned a real-world application of a spaCy feature. We explored spaCy container classes in detail and finalized the chapter with precise and useful spaCy features. At this point, you have a good grasp of spaCy language pipelining and you are confident about accomplishing bigger tasks.</p>
			<p>In the next chapter, we will dive into spaCy's full linguistic power. You'll discover linguistic features including spaCy's most used features: the POS tagger, dependency parser, named entities, and entity linking.</p>
		</div>
	</div></body></html>