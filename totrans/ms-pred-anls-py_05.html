<html><head></head><body>
  <div><div><div><div><div><h1 class="title"><a id="ch05"/>Chapter 5. Putting Data in its Place – Classification Methods and Analysis</h1></div></div></div><p>In the previous chapter, we explored methods for analyzing data whose outcome is a continuous variable, such as the purchase volume for a customer account or the expected number of days until cancellation of a subscription service. However, many of the outcomes for data in business analyses are discrete—they may only take a limited number of values. For example, a movie review can be 1–5 stars (but only integers), a customer can cancel or renew a subscription, or an online advertisement can be clicked or ignored.</p><p>The methods used to model and predict outcomes for such data are similar to the regression models we covered in the previous chapter. Moreover, sometimes we might want to convert a regression problem into a classification problem: for instance, rather than predicting customer spending patterns in a month, we might be more interested in whether it is above a certain threshold that is meaningful from a business perspective, and assign values in our training data as 0 (below the threshold) and 1 (above) depending upon this cutoff. In some scenarios, this might increase the noise in our classification: imagine if many customers' personal expenditures were right near the threshold we set for this model, making it very hard to learn an accurate model. In other cases, making the outcome discrete will help us hone in on the question we are interested in answering. Imagine the customer expenditure data is well separated above and below our threshold, but that there is wide variation in values above the cutoff. In this scenario, a regression model would try to minimize overall error in the model by fitting the trends in larger data points that disproportionately influence the total value of the error, rather than achieving our actual goal of identifying high- and low- spending customers.</p><p>In addition to these considerations, some data is inherently not modeled effectively by regression analyses. For instance, consider the scenario in which we are trying to predict which ad out of a set of five a customer is most likely to click. We could encode these ads with the numerical values ranging from 1 to 5, but they do not have a natural ordering that would make sense in a regression problem—2 is not greater than 1, it is simply a label denoting which of the five categories an ad belongs to. In this scenario, it will make more sense to encode the labels of the dataset as a vector of length <code class="literal">5</code> and place a <code class="literal">1</code> in the column corresponding to the ad, which will make all labels equivalent from the perspective of the algorithm.</p><p>With these points in mind, in the following exercises, we will cover:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Encoding data responses as categorical outcomes</li><li class="listitem" style="list-style-type: disc">Building classification models with both balanced and skewed data</li><li class="listitem" style="list-style-type: disc">Evaluating the accuracy of classification models</li><li class="listitem" style="list-style-type: disc">Assessing the benefits and shortcomings of different classification methods</li></ul></div><div><div><div><div><h1 class="title"><a id="ch05lvl1sec27"/>Logistic regression</h1></div></div></div><p>We will start our exploration of classifier algorithms with one of the most commonly used classification <a id="id307" class="indexterm"/>models: logistic regression. Logistic regression is similar to the linear regression method discussed in <a class="link" href="ch04.html" title="Chapter 4. Connecting the Dots with Models – Regression Methods">Chapter 4</a>, <em>Connecting the Dots with Models – Regression Methods</em>, with the major difference being that instead of directly computing a linear combination of the inputs, it compresses the output of a linear model through a function that constrains outputs to be in the range <code class="literal">[0,1]</code>. As we will see, this is in fact a kind of "generalized linear model that we discussed in the last <a class="link" href="ch04.html" title="Chapter 4. Connecting the Dots with Models – Regression Methods">Chapter 4</a>, <em>Connecting the Dots with Models – Regression Methods</em>, recall that in linear regression, the predicted output is given by:</p><div><img src="img/B04881_05_01.jpg" alt="Logistic regression"/></div><p>where <code class="literal">Y</code> is the response variable for all <code class="literal">n</code> members of a dataset, <code class="literal">X</code> is an <code class="literal">n</code> by <code class="literal">m</code> matrix of <code class="literal">m</code> features for each of the n rows of data, and <code class="literal">βT</code> is a column vector of <code class="literal">m</code> coefficients (Recall that the <code class="literal">T</code> operator represents the transpose of a vector or matrix. Here we transpose the coefficients so they are of dimension <code class="literal">mx1</code>, so that we can form a product with the matrix <code class="literal">X</code> with is <code class="literal">nxm</code>), which gives the change in the response expected for a 1-unit change in a particular feature. Thus, taking the dot product of <code class="literal">X</code> and <code class="literal">β</code> (multiplying each coefficient by its corresponding feature and summing over the features) gives the predicted response. In logistic regression, we begin instead with the formula:</p><div><img src="img/B04881_05_02.jpg" alt="Logistic regression"/></div><p>where the <code class="literal">logistic</code> function is:</p><div><img src="img/B04881_05_03.jpg" alt="Logistic regression"/></div><p>You can see the<a id="id308" class="indexterm"/> behavior of the logistic function by plotting using the following code in a notebook session:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; %matplotlib inline</strong>
<strong>… import pandas as pd</strong>
<strong>… import matplotlib.pyplot as plt</strong>
<strong>… import numpy as np</strong>
<strong>… plt.style.use('ggplot')</strong>
<strong>&gt;&gt;&gt; input = np.arange(-10,10,1)</strong>
<strong>&gt;&gt;&gt; output = 1/(1+np.exp(-input))</strong>
<strong>&gt;&gt;&gt; pd.DataFrame({"input":input,"output":output}).plot(x='input',y='output')</strong>
</pre></div><div><img src="img/B04881_05_04.jpg" alt="Logistic regression"/><div><p>Figure 1: The Output of the Logistic Function for a Continuous Input</p></div></div><p>As you can see in Figure 1, the logistic function takes the output of the linear regression and transforms it using a sigmoid (an S-shaped function): as the linear regression value becomes larger, the exponential term tends toward 0, making the output <code class="literal">1</code>. Conversely, as the linear regression value becomes negative, the exponential term becomes very large, and the output becomes <code class="literal">0</code>.</p><p>How can we interpret <a id="id309" class="indexterm"/>the coefficients in this model, given that it is no longer modeling a simple linear trend? Because of the logistic transform, the coefficients no longer represent an expected increase in response per 1-unit increase in the predictor. To develop a similar interpretation, we start with the observation that the logistic regression equation represents the probability of a given observation, <em>x</em>, being a member of class <code class="literal">1</code> (assuming the response variable for the data falls into two classes—see the following for a discussion of cases where the number of classes is <em>&gt; 2</em>). We could also write a similar equation to represent the probability of a given observation being class <code class="literal">0</code>, which is given by:</p><div><img src="img/B04881_05_05.jpg" alt="Logistic regression"/></div><p>Now, we can take the natural logarithm of these two probabilities get finally:</p><div><img src="img/B04881_05_06.jpg" alt="Logistic regression"/></div><p>In other words, the outcome of the linear response now represents the natural logarithm of the ratio between the probability of class <em>1</em> and class <em>0</em>. This quantity is also referred to as the log-odds or the logit function, and is equivalent to the inverse of the logistic function. In this formula, a 1-unit change in the coefficient <em>β</em> will lead to a 1-unit increase in the log-odds, allowing us a way to interpret the coefficients in this model.</p><p>You may recall from <a class="link" href="ch04.html" title="Chapter 4. Connecting the Dots with Models – Regression Methods">Chapter 4</a>, <em>Connecting the Dots with Models – Regression Methods</em>, that in <a id="id310" class="indexterm"/>
<strong>Generalized Linear Models</strong> (<strong>GLMs</strong>), link function transforms the linear response to a nonlinear range. In logistic regression, the logit function is the link function. While a full discussion of the various types of GLMs is outside the scope of this book, we refer the interested reader to more comprehensive treatments of this topic (Madsen, Henrik, and Poul Thyregod. <em>Introduction to general and generalized linear models</em>. CRC Press, 2010; Madsen, Henrik, and Poul Thyregod. Introduction to general and generalized linear models. CRC Press, 2010: Hardin, James William, Joseph M. Hilbe, and Joseph Hilbe. Generalized linear models and extensions. Stata press, 2007.).</p><p>If you have been <a id="id311" class="indexterm"/>reading carefully, you may realize that we have contradicted ourselves in the discussion above. On the one hand, we want to fit data in which the only allowable outcome is either a <code class="literal">0</code> or a <code class="literal">1</code>. One the other hand, our logistic function (and the log-odds) can take a value between <code class="literal">0</code> and <code class="literal">1</code>, continuously. Thus, to correctly apply this model, we will need to choose a threshold between <code class="literal">0</code> and <code class="literal">1</code> to classify the outputs of the regression: if a value is above this threshold, we consider the observation as class <code class="literal">1</code>, otherwise <code class="literal">0</code>. The simplest threshold to choose would be half, and indeed for balanced dataset with an equal number of positive and negative examples, this is a reasonable choice. However, in many cases that we encounter in the real world (such as ad clicks or subscriptions), the number of positive outcomes is much fewer than the negatives. If we optimize a logistic regression model using such an imbalanced dataset, the optimal parameters will identify few observations as positive. Thus, using half as a cutoff will inaccurately classify many negatives as class 1 (positive) and result in a high false positive rate.</p><p>We have a few options to address this problem of imbalanced classes in our data. The first is to simply tune the threshold for the logistic function to consider the outcome as 1, which <a id="id312" class="indexterm"/>we can do visually using the <strong>receiver operator characteristic</strong> (<strong>ROC</strong>) curve, described in more detail in the following exercises. We could also rebalance our training data such that half represents a reasonable value, by selecting an equal number of positive and negative examples. In case we were worried about making a biased choice among the many negative examples, we could repeat this process many times and average the results—this process is known as Bagging and was described in more detail in <a class="link" href="ch04.html" title="Chapter 4. Connecting the Dots with Models – Regression Methods">Chapter 4</a>, <em>Connecting the Dots with Models – Regression Methods</em>, in the context of Random Forest regression models. Finally, we could simply penalize errors on the few positive examples by assigning a weight to them in the error function that is greater than the more numerous negative examples. More details on reweighting appear as follows.</p><div><div><div><div><h2 class="title"><a id="ch05lvl2sec49"/>Multiclass logistic classifiers: multinomial regression</h2></div></div></div><p>While we have dealt thus far with the simple of example of a two-class problem , we could imagine <a id="id313" class="indexterm"/>scenarios <a id="id314" class="indexterm"/>in which there are multiple classes: for example, predicting which of a set of items a customer will select in an online store. For these sorts of problems, we can<a id="id315" class="indexterm"/> imagine extending the logistic regression to <code class="literal">K</code> classes, where <em>K &gt; 2</em>. Recall that taking e to the power of the logit function gives:</p><div><img src="img/B04881_05_07.jpg" alt="Multiclass logistic classifiers: multinomial regression"/></div><p>In a two-class problem, this value compares the ratio of the probability that <em>Y=1</em> to all other values, with the only other value being <code class="literal">0</code>. We could imagine running instead a series of logistic regression models for <em>K</em> classes, where <code class="literal">e(Logit(x))</code> gives the ratio of the probability of <em>Y = class k</em> to any of other class. We would then up with a series of <em>K</em> expressions for <code class="literal">e(Xβ)</code> with different regression coefficients. Because we want to constrain the outcome to be in the range <code class="literal">0</code> to <code class="literal">–1</code>, we can divide the output of any of the <em>K</em> models by the sum of all <em>K</em> models using the formula:</p><div><img src="img/B04881_05_08.jpg" alt="Multiclass logistic classifiers: multinomial regression"/></div><p>This equation also known as the <code class="literal">softmax</code> function. It is used extensively in neural network models (which we will cover in <a class="link" href="ch07.html" title="Chapter 7. Learning from the Bottom Up – Deep Networks and Unsupervised Features">Chapter 7</a>, <em>Learning from the Bottom Up – Deep Networks and Unsupervised Features</em>). It has the nice property that even for extreme values of <code class="literal">e(xβ)</code> for a given class k, the overall value of the function cannot go beyond 1. Thus, we can keep outliers in the dataset while limiting their influence on the overall accuracy of the model (since otherwise they would tend to dominate the overall value of an error function, such as the squared error we used in linear regression in <a class="link" href="ch04.html" title="Chapter 4. Connecting the Dots with Models – Regression Methods">Chapter 4</a>, <em>Connecting the Dots with Models – Regression Methods</em>).</p><p>To keep the current presentation less complex, we will examine only a 2-class problem in the following exercises. However, keep in mind that as with logistic regression, the other methods discussed in the following can be extended to work with multiple classes as well. Additionally, we will demonstrate a full multiclass problem in <a class="link" href="ch07.html" title="Chapter 7. Learning from the Bottom Up – Deep Networks and Unsupervised Features">Chapter 7</a>, <em>Learning from the Bottom Up – Deep Networks and Unsupervised Features</em> using neural networks.</p><p>Now that we have <a id="id316" class="indexterm"/>covered <a id="id317" class="indexterm"/>what logistic <a id="id318" class="indexterm"/>regression is and the problem it is designed to solve, let us prepare a dataset for use with this and other classification methods. In addition to working through a practical example of fitting and interpreting a logistic regression model, we will use this a starting point to examine other classification algorithms.</p></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec50"/>Formatting a dataset for classification problems</h2></div></div></div><p>In this example, we <a id="id319" class="indexterm"/>will use a census dataset with rows representing the characteristic of an adult US citizen (Kohavi, Ron. <em>Scaling Up the Accuracy of Naive-Bayes Classifiers: A Decision-Tree Hybrid</em>. KDD. Vol. 96. 1996). The objective is to predict whether an individual's income is above or below the average income of $55,000 a year. Let us start by loading the dataset into a pandas data frame and examining the first few rows using the following commands:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; census = pd.read_csv('census.data',header=None)</strong>
<strong>&gt;&gt;&gt; census.head()</strong>
</pre></div><div><img src="img/B04881_05_09.jpg" alt="Formatting a dataset for classification problems"/></div><p>Why did we use the argument (<code class="literal">header = None</code>) to load the data? Unlike some of the other datasets we have examined in previous chapters, the column names for the census data are contained in a separate file. These feature names will be helpful in interpreting the<a id="id320" class="indexterm"/> results, so let us parse them from the dataset description file:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; headers_file = open('census.headers')</strong>
<strong>&gt;&gt;&gt; headers = []</strong>
<strong>&gt;&gt;&gt; for line in headers_file:</strong>
<strong>&gt;&gt;&gt;    if len(line.split(':'))&gt;1: # colon indicates line is a column description</strong>
<strong>&gt;&gt;&gt;        headers.append(line.split(':')[0]) # the column name precedes the colon</strong>
<strong>&gt;&gt;&gt; headers = headers[15:] # the filter in the if (…) statement above is not 100 percent accurate, need to remove first 15 elements</strong>
<strong>&gt;&gt;&gt; headers.append('income') # add label for the response variable in the last column</strong>
<strong>&gt;&gt;&gt; census.columns = headers # set the column names in the dataframe to be extracted names</strong>
</pre></div><p>Now that we have the column names appended to the dataset, we can see that the response variable, income, needs to be re-encoded. In the input data, it is coded as a string, but since scikit-learn is unable to take a string as an input, we need to convert it into a <code class="literal">0</code> or <code class="literal">1</code> label using the following code:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; census.income = census.income.map( lambda x: 0 if x==' &lt;=50K' else 1)</strong>
</pre></div><p>Here, we have used a lambda expression to apply an anonymous function (a function without a name defined in the rest of the program) to the data. The conditional expression within the map (…) call takes <code class="literal">x</code> as an input and returns either <code class="literal">0</code> or <code class="literal">1</code>. We could just as easily have formally defined such as function, but especially for expressions we do not intend to reuse, lambda expressions provide an easy way to specify such transformations without crowding our code with a lot of functions that do not have general utility.</p><p>Let us take a moment and look at the distribution of the different income classes by plotting a histogram with income as the value on the vertical axis:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; census.plot(kind='hist', y='income')</strong>
</pre></div><div><img src="img/B04881_05_10.jpg" alt="Formatting a dataset for classification problems"/></div><p>Notice that observations <a id="id321" class="indexterm"/>with the label <code class="literal">1</code> are about 50 percent less prevalent than those with a label of <code class="literal">0</code>. As we discussed previously, this is a situation in which a simple threshold of half in evaluating the class probabilities will lead to an inaccurate model, and we should keep this data skew in mind as we evaluate the performance later.</p><p>In addition to our outcome variable, many of the features in this dataset are also categorical: we will need to re-encode them as well before fitting our model. We can do so in two steps: first, let us find the number of unique elements in each of these columns and map them to integer values using a dictionary. To begin, we check whether each column in the data frame is categorical (<code class="literal">dtype</code> equal to <strong>object</strong>), and, if so, we add its index into the list of columns we want to convert:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; categorical_features = [e for e,t in enumerate(census.dtypes) if t=='object' ]</strong>
</pre></div><p>Now that we have the column numbers we want to convert, we need to make a mapping of each column from a string to a label from <em>1</em> to <em>k</em>, where <em>k</em> is the number of categories:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; categorical_dicts = []</strong>
<strong>&gt;&gt;&gt; for c in categorical_features:</strong>
<strong>&gt;&gt;&gt;    categorical_dicts.append(dict( (i,e) for (e,i) in enumerate(census[headers[c]].unique()) ))</strong>
</pre></div><p>Notice that we first extract the unique elements of each column and then use the enumerate function on this list of unique elements to generate the labels we need. By converting this indexed list into a dictionary where the keys are the unique elements of a column and the values are the labels, we have exactly the mapping we need to re-encode the categorical string variables in this data as integers.</p><p>Now we can create a second copy of the data using the mapping dictionary we generated above:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; census_categorical = census</strong>
<strong>&gt;&gt;&gt; for e,c in enumerate(categorical_features):</strong>
<strong>&gt;&gt;&gt;   census_categorical[headers[c]] = \ census_categorical[headers[c]].\</strong>
<strong>map(categorical_dicts[e].get)</strong>
</pre></div><p>Now, we can use <a id="id322" class="indexterm"/>scikit-learn's one-hot encoder to transform these integer values into a series of columns, of which only one is set to <code class="literal">1</code>, representing which of the k classes this row belongs to. To use the one-hot encoder, we also need to know how many categories each of the columns has, which we can do with the following command by storing the size of each mapping dictionary:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; n_values = [len(d) for d in categorical_dicts] </strong>
</pre></div><p>We then apply the one-hot encoder:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; from sklearn.preprocessing import OneHotEncoder</strong>
<strong>&gt;&gt;&gt; census_categorical_one_hot = OneHotEncoder(categorical_features=categorical_features, n_values=n_values).fit_transform(census_categorical[headers[:-1]])</strong>
</pre></div><p>From here we have the data in the right format to fit our logistic regression. As with our examples in <a class="link" href="ch04.html" title="Chapter 4. Connecting the Dots with Models – Regression Methods">Chapter 4</a>, <em>Connecting the Dots with Models – Regression Methods</em>, we need to split our data into training and test sets, specifying the fraction of data in the test set (<em>0.4</em>). We also set the random number generator seed to 0 so that we can replicate the analysis later by generating the same random set of numbers:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; from scipy import sparse</strong>
<strong>&gt;&gt;&gt; from sklearn import cross_validation</strong>
<strong>&gt;&gt;&gt; census_features_train, census_features_test, census_income_train, census_income_test = \</strong>
<strong>&gt;&gt;&gt; cross_validation.train_test_split(census_categorical_one_hot, \</strong>
<strong>&gt;&gt;&gt; census_categorical['income'], test_size=0.4, random_state=0)</strong>
</pre></div><p>Now that we have prepared training and test data, we can fit a logistic regression model to the dataset. How can we find the optimal parameters (coefficients) of this model? We will examine two options.</p><p>The first approach, known as <a id="id323" class="indexterm"/>
<strong>stochastic gradient descent</strong> (<strong>SGD</strong>), calculates the change in the error function at a given data point and adjusts the parameters to account for this error. For an individual data point, this will result in a poor fit, but if we repeat this process over the whole training set several times, the coefficients will converge to the desired values. The term <strong>stochastic</strong> in this method's name refers to the fact that this <a id="id324" class="indexterm"/>optimization is achieved by following the gradient (first derivative) of the loss function with respect to a given data point in a random order over the dataset. Stochastic methods such as this one often scale well to large datasets because they allow us to only examine the data individually or in small batches rather than utilizing the whole dataset at once, allowing us to parallelize the learning procedure or at least not use all the memory on our machine to process large volumes of data.</p><p>In contrast, the optimization methods implemented by default for the logistic regression function in scikit-learn are known as <a id="id325" class="indexterm"/>
<strong>second-order methods</strong>. SGD, because it adjusts the model parameter values using the first derivative of the error function, is known as a first-order method. Second-order methods can be beneficial in cases where the first derivative is changing very slowly, as we will see in the following, and to find the optimal value in cases where the error function follows complex patterns.</p><p>Let us look at each of these methods in more detail.</p></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec51"/>Learning pointwise updates with stochastic gradient descent</h2></div></div></div><p>How do we find the<a id="id326" class="indexterm"/> optimal parameters for our logistic regression model using stochastic updates? Recall that we are trying to optimize the probabilities:</p><div><img src="img/B04881_05_11.jpg" alt="Learning pointwise updates with stochastic gradient descent"/></div><p>f we want to optimize the probability of each individual point in our dataset, we want to maximize the value of the equation, known as the Likelihood as it scores the probability of given point being class <code class="literal">1</code> (or <code class="literal">0</code>) based on the model;</p><div><img src="img/B04881_05_12.jpg" alt="Learning pointwise updates with stochastic gradient descent"/></div><p>You can see that if the real label <code class="literal">yi</code> is <code class="literal">1</code>, and the model gives high probability of <code class="literal">1</code>, then we maximize the value of <code class="literal">F(zi)</code> (since the exponent of the second term is <code class="literal">0</code>, making it <code class="literal">1</code>, while the first term in the product is simply the value of <code class="literal">F(zi))</code>. Conversely, if the real label of <code class="literal">yi</code> is <code class="literal">0</code>, then we want the model to maximize the value of <code class="literal">(1-F(zi))</code>, which is the probability of class <code class="literal">0</code> under the model.</p><p>Thus, each point will<a id="id327" class="indexterm"/> contribute to the likelihood by the probability of its real class. It is usually easier to work <a id="id328" class="indexterm"/>with sums than products, so we can take the logarithm of the likelihood equation and sum over all elements in the data set using:</p><div><img src="img/B04881_05_13.jpg" alt="Learning pointwise updates with stochastic gradient descent"/></div><p>To find the optimal value of the parameters, we just take the first partial derivative with respect to the parameters of this equation (the regression coefficients) and solve for the value of <em>β</em> that maximizes the likelihood equation by setting the derivative equal to <code class="literal">0</code> and finding the value of <em>β</em> as illustrated below:</p><div><img src="img/B04881_05_14.jpg" alt="Learning pointwise updates with stochastic gradient descent"/></div><p>This is the direction we want to update the coefficients β in order to move it closer to the optimum. Thus, for each data point, we can make an update of the following form:</p><div><img src="img/B04881_05_15.jpg" alt="Learning pointwise updates with stochastic gradient descent"/></div><p>where <code class="literal">α</code> is the learning<a id="id329" class="indexterm"/> rate (which we use to control the magnitude by which the coefficients can change in<a id="id330" class="indexterm"/> each step – usually a smaller learning rate will prevent large changes in value and converge to a better model, but will take longer), t is the current optimization step, and <em>t-1</em> is the previous step. Recall that in <a class="link" href="ch04.html" title="Chapter 4. Connecting the Dots with Models – Regression Methods">Chapter 4</a>, <em>Connecting the Dots with Models – Regression Methods</em> we discussed the concept of regularization, in which we can use a penalty term <em>λ</em> to control the magnitude of our coefficients. We can do the same here: if the regularization term in our likelihood is given by (to penalize the squared sum of the coefficients, which is the <em>L2</em> norm from Ridge Regression in <a class="link" href="ch04.html" title="Chapter 4. Connecting the Dots with Models – Regression Methods">Chapter 4</a>, <em>Connecting the Dots with Models – Regression Methods</em>):</p><div><img src="img/B04881_05_16.jpg" alt="Learning pointwise updates with stochastic gradient descent"/></div><p>Then once we take the first derivate, we have:</p><div><img src="img/B04881_05_17.jpg" alt="Learning pointwise updates with stochastic gradient descent"/></div><p>And the final update equation becomes:</p><div><img src="img/B04881_05_18.jpg" alt="Learning pointwise updates with stochastic gradient descent"/></div><p>We can see, this regularization penalty has the effect of shrinking the amount by which we modify the coefficients β at any given step.</p><p>As we mentioned previously, stochastic updates are especially efficient for large datasets as we only have to examine each data point one at a time. One downside of this approach is that we need to run the optimization long enough to make sure the parameters converge. For example, we could monitor the change in the coefficient values as we take the derivative with respect to each data point, and stop when the values cease changing. Depending upon the dataset, this could occur quickly or take a long time. A second downside is that following the gradient of the error function along the first derivative will not always lead to the fastest solution. Second-order methods allow us to overcome some of these deficits.</p></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec52"/>Jointly optimizing all parameters with second-order methods</h2></div></div></div><p>In the case <a id="id331" class="indexterm"/>of logistic regression, our objective function is convex (see aside), meaning that whichever <a id="id332" class="indexterm"/>optimization method we choose should be able to converge to the global optimum. However, we could imagine other scenarios: for example, the surface of the likelihood equation plotted as a function of the inputs could vary slowly in a long ravine toward its global optimum. In such a case, we would like to find the direction to move the coefficients that is the optimal tradeoff between the rate of change and the <strong>rate of the rate of change</strong>, represented by the second derivative of the likelihood.  Finding this tradeoff allows the optimization routine to traverse slowly varying<a id="id333" class="indexterm"/> regions quickly. This kind of strategy is represented by the class of so-called <strong>Newton methods</strong>, which minimizes equations of the following form:</p><div><img src="img/B04881_05_19.jpg" alt="Jointly optimizing all parameters with second-order methods"/></div><p>Where <code class="literal">f(x*)</code> is the objective function we are trying to minimize, such as the logistic regression error, and x are the values (such as the model coefficients) that do minimize regression likelihood, <code class="literal">x*</code> are the inputs which optimize the value of the function (such as the optimal coefficients β), and <code class="literal">xt</code> are the value of these parameters at the current step of the optimization (there is admittedly some abuse of notation here: in the rest of the chapter, <em>x</em> are the input rows, where here we use <em>x</em> to represent a parameter value in a model). The name <strong>Newton method</strong> is due to the father of physics, Isaac Newton, who described an early version of this procedure (Ypma, Tjalling J. <em>Historical development of the Newton-Raphson method</em>. SIAM review 37.4 (1995): 531-551).  The minimization involves finding the direction we should move the parameters at the current stage, <code class="literal">xt</code>, in order to find the minimal value of <code class="literal">f</code>. We can use a Taylor Expansion from Calculus to approximate the value of the preceding function:</p><div><img src="img/B04881_05_20.jpg" alt="Jointly optimizing all parameters with second-order methods"/></div><p>We want to find the value of <code class="literal">Δx</code> which maximizes the function, since this is the direction we wish to move the parameters, just as in gradient descent. We can obtain this optimal direction by solving for the point where the gradient of the function becomes <code class="literal">0</code> with respect to <code class="literal">Δx</code>, to give:</p><div><img src="img/B04881_05_21.jpg" alt="Jointly optimizing all parameters with second-order methods"/></div><p>Thus, when <code class="literal">f′(x)</code> is changing slowly (small <code class="literal">f″(x)</code>), we take larger steps to change the value of the <a id="id334" class="indexterm"/>parameters, and vice versa.</p><p>One of the most commonly<a id="id335" class="indexterm"/> used second order methods for logistic regression is <strong>iteratively reweighted least squares</strong> (<strong>IRLS</strong>). To show how it works let us translate the equations <a id="id336" class="indexterm"/>above to our logistic regression model. We already known <code class="literal">f'(x)</code>, since this is just our formula from above used for stochastic gradient descent:</p><div><img src="img/B04881_05_22.jpg" alt="Jointly optimizing all parameters with second-order methods"/></div><p>What about the second derivative of the likelihood? We can solve for it as well</p><div><img src="img/B04881_05_23.jpg" alt="Jointly optimizing all parameters with second-order methods"/></div><p>Here we are still writing this equation as a solution for a single data point. In second order methods we are not usually going to use stochastic updates, so we need to apply the formula to all data points. For the gradient (first derivative), this gives the sum:</p><div><img src="img/B04881_05_24.jpg" alt="Jointly optimizing all parameters with second-order methods"/></div><p>For second <a id="id337" class="indexterm"/>derivative, we<a id="id338" class="indexterm"/> can express the result as a matrix. The matrix of pairwise second derivatives is known also known as the Hessian matrix.</p><div><img src="img/B04881_05_25.jpg" alt="Jointly optimizing all parameters with second-order methods"/></div><p>Where I is the identity matrix (a matrix with 1 on the diagonal and 0 elsewhere), and A contains the second derivative evaluated for each pair of points <em>i </em>and <em>j</em>. Thus, if we use these expressions to make a Newton update, we have:</p><div><img src="img/B04881_05_26.jpg" alt="Jointly optimizing all parameters with second-order methods"/></div><p>Because the second derivative appears in the denominator, we use a matrix inverse (given by the <em>-1</em> exponent) to perform this operation. If you look closely at the denominator, we have the product <em>XT X</em> weighted by the elements of <em>A</em>, and in the numerator we have <em>X(Y-F(X))</em>. This resembles the equation for ordinary linear regression that we saw in <a class="link" href="ch04.html" title="Chapter 4. Connecting the Dots with Models – Regression Methods">Chapter 4</a>, <em>Connecting the Dots with Models – Regression Methods</em>! In essence, this update is performing a stepwise linear regression weighted at each pass by <em>A</em> (whose values change as we update the coefficients), thus giving the method its name. One of the shortcomings of IRLS is that we need to repeatedly invert a Hessian matrix that will become quite large as the number of parameters and data points grows. Thus, we might try to find ways to approximate this matrix instead of explicitly calculating it. One method commonly used for this purpose is the Limited Memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) algorithm (Liu, Dong C., and Jorge Nocedal. <em>On the limited memory BFGS method for large scale optimization</em>. Mathematical programming 45.1-3 (1989): 503-528), which uses the last k updates of the algorithm to calculate an approximation of the Hessian matrix instead of explicitly solving for it in each stage.</p><p>In both SGD and <a id="id339" class="indexterm"/>Newton <a id="id340" class="indexterm"/>methods, we have theoretical confidence that both methods will eventually converge to the correct (globally optimal) parameter values due to a property of the likelihood function known as <a id="id341" class="indexterm"/>convexity. Mathematically, a convex function F fulfills the condition:</p><div><img src="img/B04881_05_27.jpg" alt="Jointly optimizing all parameters with second-order methods"/></div><p>Conceptually, this means that for two points <code class="literal">x1</code> and <code class="literal">x2</code>, the value of F for points between them (the left-hand side of the equation) is less than or equal to the straight line between the points (the right-hand side of the equation, which gives a linear combination of the function value at the two points). Thus, a convex function will have a global minimum between the points <code class="literal">x1</code> or <code class="literal">x2</code>. Graphically, you can see this by plotting the following in a python notebook</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; input = np.arange(10)-5</strong>
<strong>&gt;&gt;&gt; parabola = [a*a for a in input]</strong>
<strong>&gt;&gt;&gt; line = [-1*a for a in input-10]</strong>
<strong>&gt;&gt;&gt; plt.plot(input,parabola)</strong>
<strong>&gt;&gt;&gt; plt.plot(input,line,color='blue')</strong>
</pre></div><div><img src="img/B04881_05_28.jpg" alt="Jointly optimizing all parameters with second-order methods"/></div><p>The parabola is a convex function because the values between <code class="literal">x1</code> and <code class="literal">x2</code> (the two points where the blue line intersects with the parabola) are always below the blue line representing <code class="literal">α(F(x1))+(1-α) (F(x2))</code>. As you can see, the parabola also has a global minimum between these two points.</p><p>When we are dealing with matrices such as the Hessian referenced previously, this condition is fulfilled by each element of the matrix being <em>≥ 0</em>, a property known as positive semidefinite, meaning any vector multiplied by this matrix on either side (xTHx) yields a value <code class="literal">≥ 0</code>. This means the function has a global minimum, and if our solution converges to a set of <a id="id342" class="indexterm"/>coefficients, we can be guaranteed that they represent the best parameters for<a id="id343" class="indexterm"/> the model, not a local minimum.</p><p>We noted previously that we could potentially offset imbalanced distribution of classes in our data by reweighting individual points during training. In the formulas for either SGD or IRLS, we could apply a weight wi for each data point, increasing or decreasing its relative contribution to the value of the likelihood and the updates made during each iteration of the optimization algorithm.</p><p>Now that we have described how to obtain the optimal parameters of the logistic regression model, let us return to our example and apply these methods to our data.</p></div></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch05lvl1sec28"/>Fitting the model</h1></div></div></div><p>We can use either the SGD or second-order methods to fit the logistic regression model to our data. Let <a id="id344" class="indexterm"/>us compare the results using SGD; we fit the model using the following command:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; log_model_sgd = linear_model.SGDClassifier(alpha=10,loss='log',penalty='l2',n_iter=1000, fit_intercept=False).fit(census_features_train,census_income_train)</strong>
</pre></div><p>Where the parameter <code class="literal">log</code> for loss specifies that this is a logistic regression that we are training, and <code class="literal">n_iter</code> specifies the number of times we iterate over the training data to perform SGD, alpha represents the weight on the regularization term, and we specify that we do not want to fit the intercept to make comparison to other methods more straightforward (since the method of fitting the intercept could differ between optimizers). The penalty argument specifies the regularization penalty, which we saw in <a class="link" href="ch04.html" title="Chapter 4. Connecting the Dots with Models – Regression Methods">Chapter 4</a>, <em>Connecting the Dots with Models – Regression Methods</em>, already for ridge regression. As <code class="literal">l2</code> is the only penalty we can use with second-order methods, we choose <code class="literal">l2</code> here as well to allow comparison between the methods. We can examine the resulting model coefficients by referencing the coeff_ property of the model object:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; log_model_sgd.coef_</strong>
</pre></div><p>Compare these coefficients to the second-order fit we obtain using the following command:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; log_model_newton = linear_model.LogisticRegression(penalty='l2',solver='lbfgs', fit_intercept=False).fit(census_features_train,census_income_train</strong>
</pre></div><p>Like the SGD model, we<a id="id345" class="indexterm"/> remove the intercept fit to allow the most direct comparison of the coefficients produced by the two methods., We find that the coefficients are not identical, with the output of the SGD model containing several larger coefficients. Thus, we see in practice that even with similar models and a convex objective function, different optimization methods can give different parameter results. However, we can see that the results are highly correlated based on a pairwise scatterplot of the coefficients:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; plt.scatter(log_model_newton.coef_,log_model_sgd.coef_)</strong>
<strong>&gt;&gt;&gt; plt.xlim(-0.08,0.08)</strong>
<strong>&gt;&gt;&gt; plt.ylim(-0.08,0.08)</strong>
<strong>&gt;&gt;&gt; plt.xlabel('Newton Coefficent')</strong>
<strong>&gt;&gt;&gt; plt.ylabel('SGD Coefficient')</strong>
</pre></div><div><img src="img/B04881_05_29.jpg" alt="Fitting the model"/></div><p>The fact that the SGD model has larger coefficients gives us a hint as to what might be causing the difference: perhaps SGD is more sensitive to differences in scale between features? Let <a id="id346" class="indexterm"/>us evaluate this hypothesis by using the <strong>StandardScaler</strong> introduced in <a class="link" href="ch03.html" title="Chapter 3. Finding Patterns in the Noise – Clustering and Unsupervised Learning">Chapter 3</a>, <em>Finding Patterns in the Noise – Clustering and Unsupervised Learning</em> in the context of K-means clustering to normalize the features before running the SGD model using the following commands:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; from sklearn.preprocessing import StandardScaler</strong>
<strong>&gt;&gt;&gt; census_features_train_sc= StandardScaler().fit_transform(X=census_features_train.todense())</strong>
</pre></div><p>Recall that we need to turn the features matrix to a dense format since StandardScaler does not accept a sparse matrix as input. Now, if we retrain the SGD using the same arguments and plot the result versus the Newton method, we find the coefficients are much closer:</p><div><img src="img/B04881_05_30.jpg" alt="Fitting the model"/></div><p>This example should underscore the fact that the optimizer is sometimes as important as the actual algorithm, and may determine what steps we should take in data normalization.</p></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch05lvl1sec29"/>Evaluating classification models</h1></div></div></div><p>Now that we have fit <a id="id347" class="indexterm"/>a classification model, we can examine the accuracy on the test set. One common tool for performing this kind of analysis is the <strong>Receiver Operator Characteristic</strong> (<strong>ROC</strong>) curve. To draw an ROC curve, we select a particular cutoff for the classifier (here, a value between <code class="literal">0</code> and <code class="literal">1</code> above which we consider a data point to be classified as <a id="id348" class="indexterm"/>a positive, or 1) and ask what fraction of 1s are correctly classified by this cutoff (true positive rate) and, concurrently, what fraction of negatives are incorrectly predicted to be positive (false positive rate) based on this threshold. Mathematically, this is represented by choosing a threshold and computing four values:</p><div><pre class="programlisting">
<strong>TP = true positives = # of class 1 points above the threshold</strong>
<strong>FP = false positives = # of class 0 points above the threshold</strong>
<strong>TN = true negatives = # of class 0 points below the threshold</strong>
<strong>FN = false negatives = # of class 1 points below the threshold</strong>
</pre></div><p>The <strong>true positive rate</strong> (<strong>TPR</strong>) plotted <a id="id349" class="indexterm"/>by the ROC is then <em>TP/(TP+FN)</em>, while the <strong>false positive rate</strong> (<strong>FPR</strong>) is <em>FP/(FP+TN)</em>.</p><p>If both rates <a id="id350" class="indexterm"/>are  equal, then this is no better than random.  In other words, at whatever cutoff we choose, a prediction of class 1 by the model is equally likely regardless if the point is actually positive or negative. Thus, a diagonal line from the lower left to the upper right hand represent the performance of a classifier made through randomly choosing labels for data points, since the true positive and false positive rates are always equal. Conversely, if the classifier exhibits better than random performance, the true positive rate rises much faster as correctly classified points are enriched above the threshold. Integrating the <strong>area under the curve</strong> (<strong>AUC</strong>) of the <a id="id351" class="indexterm"/>ROC curve, which has a maximum of 1, is a common way to report the accuracy of classifier methods. To find the best threshold to use for classification, we find the point on this curve where the ratio between true positive and false positive rates is maximal.</p><p>In our example, this is important because <code class="literal">1</code> is less frequent than <code class="literal">0</code>. As we mentioned in the beginning of this chapter when we were examining the data set, this can cause problems in training a classification model. While the naïve choice would be to consider events with predicted probability above 0.5 as 1, in practice we find that due to this dataset imbalance, a lower threshold is optimal as the solution is biased toward the zeros. This effect can become even more extreme in highly skewed data: consider an example where only 1 in 1,000 points have label 1. We could have an excellent classifier that predicts that every data point is 0: it is 99.9% percent accurate! However, it would not be very useful in identifying rare events. There are a few ways we could counteract this bias besides adjusting the threshold in the AUC.</p><p>One way would be to rebalance the model by constructing a training set that is 50 percent 1s and 50 percent 0s. We can then evaluate the performance on the unbalanced test dataset. If the imbalance is very large, our rebalanced training set might contain only a small number of the possible variation in the 0s: thus, to generate a model representative of the entire dataset, we may want to construct many such datasets and average the<a id="id352" class="indexterm"/> results of the models generated from them. This approach is not dissimilar to the Bagging method used in constructing Random Forest models, as we saw in <a class="link" href="ch04.html" title="Chapter 4. Connecting the Dots with Models – Regression Methods">Chapter 4</a>, <em>Connecting the Dots with Models – Regression Methods</em>.</p><p>Secondly, we can use our knowledge of the imbalance to change the contribution of each data point as we optimize the parameters. For example, in the SGD equations, we can penalize errors on 1s 1,000 times as much as errors on 0s. This weight will then correct the bias in the model.</p><p>Our interpretation of the AUC is also changed in very imbalanced datasets. While an overall AUC of 0.9 might be considered good, if the ratio between the TPR and FPR at a false positive rate of 0.001 (the fraction of data containing the rare class) is not &gt; 1, it indicates we may have to search through a large amount of the head of the ranking to enrich the rare events. Thus, while the overall accuracy appears good, the accuracy in the range of data we most These scenarios are not uncommon in practice. For example, ad clicks are usually much less frequent than non-clicks, as are responses to sales inquiries. Visually, a classifier that is not well-fit to imbalanced data would be indicated by an ROC curve where the difference between TPR and FPR is greatest near the middle of the curve (<em>~0.5</em>). Conversely, in an ROC curve of a classifier that is appropriately adapted to rare events, most of the area is contained to the left hand side (rising sharply from a cutoff of 0 and leveling off to the right), representing enrichment of positives at high thresholds.</p><p>Note that false positive rate and false negative rate are just two examples of accuracy metrics we could compute. We may also be interested in knowing, above a given cutoff for model score, 1) how many of our positive examples are classified (recall) and what percentage of points above this threshold are actually positive 2) precision. These are calculated as:</p><p><em>Precision = TP/(TP+FP)</em></p><p><em>Recall = TP/(TP+FN)</em></p><p>In fact, recall is identical to the true positive rate. While the ROC curve allows us to evaluate whether the model generates true positive predictions at a greater rate than false positives, comparing precision versus recall gives a sense of how reliable and complete the predictions above a given score threshold are. We could have very high precision, but only be able to detect a minority of the overall positive examples. Conversely, we could have high recall at the cost of low precision as we incur false positives by lowering the score threshold to call positives in our model. The tradeoff between these can be<a id="id353" class="indexterm"/> application specific. For example, if the model is largely exploratory, such as a classifier used to generate potential sales leads for marketing, then we accept a fairly low precision since the value of each positive is quite high even if the true predictions are interspersed with noise. On the other hand, in a model for spam identification, we may want to err on the side of high precision, since the cost of incorrectly moving a valid business email to the user's trash folder may be higher than the occasional piece of unwanted mail that gets through the filter. Finally, we could also consider performance metrics that are appropriate even for imbalanced data, because they represent a tradeoff between the precision and recall for majority and minority classes. These include the F-measure:</p><div><img src="img/B04881_05_31.jpg" alt="Evaluating classification models"/></div><p>And Matthew's correlation coefficient (Matthews, Brian W. <em>Comparison of the predicted and observed secondary structure of T4 phage lysozyme</em>. Biochimica et Biophysica Acta (BBA)-Protein Structure 405.2 (1975): 442-451.):</p><div><img src="img/B04881_05_32.jpg" alt="Evaluating classification models"/></div><p>Returning to our example, we have two choices in how we could compute the predictions from our model: either a class label (<code class="literal">0</code> or <code class="literal">1</code>) or a probability of a particular individual being class <code class="literal">1</code>. For computing the ROC curve, we want the second choice, since this will allow us to evaluate the accuracy of the classifier over a range of probabilities used as a threshold for classification:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; train_prediction = log_model_newton.predict_proba(census_features_train)</strong>
<strong>&gt;&gt;&gt; test_prediction = log_model_newton.predict_proba(census_features_test)</strong>
</pre></div><p>With this probability, we can we see visually that our model gives a subpar accuracy using the following code<a id="id354" class="indexterm"/> to plot the ROC curve for the training and test sets:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt;  from sklearn import metrics</strong>
<strong>&gt;&gt;&gt; fpr_train, tpr_train, thresholds_train = metrics.roc_curve(np.array(census_income_train),\</strong>
<strong>   np.array(train_prediction[:,1]), pos_label=1)</strong>
<strong>&gt;&gt;&gt; fpr_test, tpr_test, thresholds_test = metrics.roc_curve(np.array(census_income_test),\</strong>
<strong>   np.array(test_prediction[:,1]), pos_label=1)</strong>
<strong>&gt;&gt;&gt; plt.plot(fpr_train, tpr_train)</strong>
<strong>&gt;&gt;&gt; plt.plot(fpr_test, tpr_test)</strong>
<strong>&gt;&gt;&gt; plt.xlabel('False Positive Rate')</strong>
<strong>&gt;&gt;&gt; plt.ylabel('True Positive Rate')</strong>
</pre></div><div><img src="img/B04881_05_33.jpg" alt="Evaluating classification models"/></div><p>Numerically, we find that the AUC of the test and training set is little better than random (<code class="literal">0.5</code>), as both the commands:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; metrics.auc(fpr_train,tpr_train)</strong>
</pre></div><p>and</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; metrics.auc(fpr_test,tpr_test)</strong>
</pre></div><p>give results of ~ 0.6.</p><p>If possible, we would like to improve the performance of our classified—how can we diagnose the problems with <a id="id355" class="indexterm"/>our existing logistic regression model and work toward a better prediction?</p><div><div><div><div><h2 class="title"><a id="ch05lvl2sec53"/>Strategies for improving classification models</h2></div></div></div><p>Confronted with this less<a id="id356" class="indexterm"/> than desirable performance, we typically have a few options:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Train with more data</li><li class="listitem" style="list-style-type: disc">Regularize the model to reduce over-fitting</li><li class="listitem" style="list-style-type: disc">Choose another algorithm</li></ul></div><p>In our example with an under-performing logistic regression model, which option makes most sense?</p><p>Let us consider take the first option, that we simply need more data to improve performance. In some cases, we may not have enough data in our training set to represent the patterns we observe in the test set. If this were the case, we would expect to see our performance on the test set improve as we increase the size of the training set used to build the model. However, we do not always have the convenience of getting more data. In this example, we don't actually have more data to train with; even if is possible to collect more data in theory, in practice it may be too expensive to justify the cost, or we may need to make a decision before more data will be available.</p><p>What about over-fitting? In other words, perhaps our model is precisely tuned to the patterns in the training set, but does not generalize to the test set. Like the first option, we will observe better performance on the training set than the test set. However, the solution is not necessarily to add more data, but rather to prune features to make the model more general. In the preceding scenario, we see that the performance on both training and test is similar, so this does not seem like the most likely explanation.</p><p>Finally, we might try another algorithm. To do so, let us consider what the limitations of our current model are. For one, the logistic regression only incorporates single features: it has no way to represent interactions between them. For instance, it can only model the effect of marital status, not marital status conditional on education and age. It is perhaps not surprising that these factors probably in combination predict income, but not necessarily individually. It may help to look at the <a id="id357" class="indexterm"/>values of the coefficients, and to do so, we will need to map the original column headers to column names in our one-hot encoding, where each of the categorical features is now represented by several columns. In this format, the numerical columns are appended to the end of the data frame, so we need to add them last to the list of columns. The following code remaps the column headers using the mapping of category to one-hot position we calculated earlier:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; expanded_headers = []</strong>
<strong>&gt;&gt;&gt; non_categorical_headers = []</strong>
<strong>&gt;&gt;&gt; categorical_index = 0</strong>
<strong>&gt;&gt;&gt; for e,h in enumerate(np.array(census.columns[:-1])):</strong>
<strong> …   if e in set(categorical_features):</strong>
<strong> …       unsorted_category = np.array([h+key for key in categorical_dicts[categorical_index].keys()]) # appends the category label h to each feature 'key' </strong>
<strong> …       category_indices = np.array(list(categorical_dicts[categorical_index].values())) # gets the mapping from category label h to the position in the one-hot array    </strong>
<strong> …       expanded_headers+=list(unsorted_category[np.argsort(category_indices)]) # resort the category values in the same order as they appear in the one-hot encoding</strong>
<strong>…        categorical_index+=1 # increment to the next categorical feature</strong>
<strong>…    else:</strong>
<strong>…        non_categorical_headers+=[h]</strong>
<strong>… expanded_headers+=non_categorical_headers</strong>
</pre></div><p>We can check that the individual coefficient make sense: keep in mind that the sort function arranges items in ascending order, so to find the largest coefficients we sort by the negative value:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; expanded_headers[np.argsort(-1*log_model.coef_[0])]</strong>
<strong>array(['capital-gain', 'capital-loss', 'hours-per-week', 'age',        'education-num', 'marital-status Married-civ-spouse',        'relationship Husband', 'sex Male', 'occupation Exec-managerial',        'education Bachelors', 'occupation Prof-specialty',        'education Masters', 'relationship Wife', 'education Prof-school',        'workclass Self-emp-inc', 'education Doctorate',        'workclass Local-gov', 'workclass Federal-gov',        'workclass Self-emp-not-inc', 'race White',        'occupation Tech-support', 'occupation Protective-serv',        'workclass State-gov', 'occupation Sales', … </strong>
</pre></div><p>Logically, the order <a id="id358" class="indexterm"/>appears to make sense, since we would expect age and education to be important predictors of income. However, we see that only <em>~1/3rd</em> of the features have a large influence on the model through the following plot:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; plt.bar(np.arange(108),np.sort(log_model_newton.coef_[0]))</strong>
</pre></div><div><img src="img/B04881_05_34.jpg" alt="Strategies for improving classification models"/></div><p>Thus, it looks like the model is only able to learn information from a subet of features. We could potentially try to generate interaction features by making combinatorial labels (for example, a binary flag representing married and maximum education level as Master's Degree) by taking the product of all features with each other. Generating potential nonlinear features in this way is known as polynomial expansion, since we are taking single coefficient terms and converting them into products that have squared, cubic, or higher power relationships. However, for the purposes of this example, will try some alternative algorithms.</p></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch05lvl1sec30"/>Separating Nonlinear boundaries with Support vector machines</h1></div></div></div><p>In our previous example of logistic regression, we assumed implicitly that every point in the training set might be useful in defining the boundary between the two classes we are trying to separate. In practice we may only need a small number of data points to define this boundary, with additional information simply adding noise to the classification. This concept, that classification might be improved by using only a small number of <a id="id359" class="indexterm"/>critical data points, is the key features of the <strong>support vector machine</strong> (<strong>SVM</strong>) model.</p><p>In its basic form, the SVM is similar to the linear models we have seen before, using the following equation:</p><div><img src="img/B04881_05_35.jpg" alt="Separating Nonlinear boundaries with Support vector machines"/></div><p>where <code class="literal">b</code> is an intercept, and β is the vector of coefficients such as we have seen in regression models. We can see a simple rule that a point <code class="literal">X</code> is classified as class <code class="literal">1</code> if <code class="literal">F(x) ≥ 1</code>, and class <code class="literal">-1</code> if <code class="literal">F(x) ≤ –1</code>. Geometrically, we can understand this as the distance from the plane to the point <code class="literal">x</code>, where β is a vector sitting orthogonal (at a right angle) to the plane. If the two classes are ideally separated, then the width between the two classes represented by 1/<img src="img/B04881_05_36.jpg" alt="Separating Nonlinear boundaries with Support vector machines"/> is as large as possible; thus, in finding the optimal value of β, we would to minimize the norm <img src="img/B04881_05_36.jpg" alt="Separating Nonlinear boundaries with Support vector machines"/>. At the same time, we want to minimize the error in assigning labels to the data. Thus, we can have a loss function that minimizes the tradeoff between these two objectives:</p><div><img src="img/B04881_05_37.jpg" alt="Separating Nonlinear boundaries with Support vector machines"/></div><p>where <code class="literal">y</code> is the correct label of <code class="literal">x</code>. When <code class="literal">x</code> is correctly classified, <code class="literal">y(xβ+b) ≥ 1</code>, and we overall subtract from the values of <code class="literal">L</code>. Conversely, when we incorrectly predict <code class="literal">x</code>.</p><p>Note that the || here represent the Euclidean norm, or:</p><div><img src="img/B04881_05_38.jpg" alt="Separating Nonlinear boundaries with Support vector machines"/></div><p><code class="literal">y(xβ+b) &lt; 1</code>, and we thus add to the value of <code class="literal">L</code>. If we want to minimize the value of <code class="literal">L</code>, we could find the optimal value of <code class="literal">β</code> and <em>b</em> by taking the derivative of this function and setting it to 0. Starting with <code class="literal">β</code>:</p><div><img src="img/B04881_05_39.jpg" alt="Separating Nonlinear boundaries with Support vector machines"/></div><p>Similarly, for <code class="literal">b</code>:</p><div><img src="img/B04881_05_40.jpg" alt="Separating Nonlinear boundaries with Support vector machines"/></div><p>Plugging these back into the loss function equation we get:</p><div><img src="img/B04881_05_41.jpg" alt="Separating Nonlinear boundaries with Support vector machines"/></div><p>Two things are important <a id="id360" class="indexterm"/>here. First, only some of the <code class="literal">α</code> need to be nonzero. The rest can be set to <code class="literal">0</code>, meaning only a small number of points influence the choice of optimal model parameters. These points are the support vectors that give the algorithm its name, which lie along the boundary between the two classes. Note that in practice we would not use the above version of the error function, but rather a <strong>soft-margin</strong> formulation<a id="id361" class="indexterm"/> in which we use the <a id="id362" class="indexterm"/>
<strong>hinge loss</strong>:</p><div><img src="img/B04881_05_42.jpg" alt="Separating Nonlinear boundaries with Support vector machines"/></div><p>This means that we only penalize points if they are on the wrong side of the separating hyperplane, and then by the magnitude of their misclassification error. This allows the SVM to be applied even in cases where the data is not linearly separable by allowing the algorithm to make mistakes according to the hinge loss penalty. For full <a id="id363" class="indexterm"/>details please consult references (Cortes, Corinna, and Vladimir Vapnik. <strong>Support-vector networks</strong>. Machine learning 20.3 (1995): 273-297; Burges, Christopher JC. <em>A tutorial on support vector machines for pattern recognition</em>. Data mining and knowledge discovery 2.2 (1998): 121-167.).</p><p>Second, we now see that the solution depends on the inputs <em>x</em> only through the product of individual points. In fact, we could replace this product with any function <code class="literal">K(xi,xj)</code>, where <code class="literal">K</code> is a so-called <a id="id364" class="indexterm"/>
<strong>kernel function</strong> representing the similarity between <code class="literal">xi</code> and <code class="literal">xj</code>. This can be particularly useful when trying to capture nonlinear relationships between data points. For example, consider data points along a parabola in two-dimensional space, where <code class="literal">x2</code> (the vertical axis) is the square of <code class="literal">x1</code> (the horizontal). Normally, we could not draw a straight line to separate points above and below the parabola. However, if we first mapped the points using the function <code class="literal">x1</code>, <code class="literal">sqrt(x2)</code>, we can now linearly separate them. We saw the effectiveness of this nonlinear mapping in <a class="link" href="ch03.html" title="Chapter 3. Finding Patterns in the Noise – Clustering and Unsupervised Learning">Chapter 3</a>, <em>Finding Patterns in the Noise – Clustering and Unsupervised Learning</em>, when we use the Gaussian Kernel to separate the nonlinear boundary between concentric circles using Spectral K-Means clustering.</p><p>Besides creating a linear <a id="id365" class="indexterm"/>decision boundary between data points that are not linearly separable in the input space, Kernel functions also allow us to calculate similarities between objects that have no vector representation, such as graphs (nodes and edges) or collections of words. The objects do not need to be the same length, either, as long as we can compute a similarity. These facts are due to a result known as Mercer's Theorem, which guarantees that Kernel functions which are <em>&gt;=0</em> for all pairs of inputs represent a valid inner product <img src="img/B04881_05_43.jpg" alt="Separating Nonlinear boundaries with Support vector machines"/> between inputs x mapped into a linearly separable space using the mapping represented by φ (Hofmann, Thomas, Bernhard Schölkopf, and Alexander J. Smola. <em>Kernel methods in machine learning</em>. The annals of statistics (2008): 1171-1220). This mapping could be explicit, such as the square root function applied to the parabolic input in the example above. However, we do not actually need the mapping at all, since the kernel is guaranteed to represent the similarity between the mapped inputs. Indeed, the mapping could even be performed in an infinite-dimensional space that we can not explicitly represent, as is the case with the Gaussian kernel we will describe as follows.</p><p>Now that we have covered some of the intuition behind SVMs, let us see if it can improve the performance of our classification model by fitting an SVM to the data.</p><div><div><div><div><h2 class="title"><a id="ch05lvl2sec54"/>Fitting and SVM to the census data</h2></div></div></div><p>For this example, we <a id="id366" class="indexterm"/>will try the default kernel function for the SVM model in scikit-learn, which is a <a id="id367" class="indexterm"/>Gaussian kernel, which you may recognize as the same equation used in a normal distribution function. We previously used the Gaussian Kernel in the context of Spectral Clustering in <a class="link" href="ch03.html" title="Chapter 3. Finding Patterns in the Noise – Clustering and Unsupervised Learning">Chapter 3</a>, <em>Finding Patterns in the Noise – Clustering and Unsupervised Learning</em>, as a reminder, the formula is:</p><div><img src="img/B04881_05_44.jpg" alt="Fitting and SVM to the census data"/></div><p>In essence, this function translates the difference between two data points into the range <code class="literal">1</code> (when they are equal and the exponent becomes 0) and <code class="literal">0</code> (when the difference is very large and the exponent tends toward a very large negative number). The parameter <em>γ</em> represents the standard deviation, or bandwith, which controls how quickly the value of the function tends towards zero as the difference between the points increases. Small values of the bandwith will make the numerator a larger negative number, thus shrinking the kernel value to <em>0</em>.</p><p>As we mentioned <a id="id368" class="indexterm"/>preceding, the Gaussian kernel represented mapping the inputs <em>x</em> into an infinite dimensional space. We can see this if we expand the value of the kernel function using an infinite series:</p><div><img src="img/B04881_05_45.jpg" alt="Fitting and SVM to the census data"/></div><p>Thus, the Gaussian kernel captures a similarity in an infinite dimensional features space.</p><p>We fit the SVM model to the training data using the following commands:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; from sklearn import svm</strong>
<strong>&gt;&gt;&gt; svm_model = svm.SVC(probability=True,kernel='rbf').fit(census_features_train.toarray(),census_income_train)</strong>
<strong>&gt;&gt;&gt; train_prediction = svm_model.predict_proba(census_features_train.toarray())</strong>
<strong>&gt;&gt;&gt; test_prediction = svm_model.predict_proba(census_features_test.toarray())</strong>
<strong>&gt;&gt;&gt; fpr_train, tpr_train, thresholds_train = metrics.roc_curve(np.array(census_income_train),\</strong>
<strong>   np.array(train_prediction[:,1]), pos_label=1)</strong>
<strong>&gt;&gt;&gt; fpr_test, tpr_test, thresholds_test = metrics.roc_curve(np.array(census_income_test),\</strong>
<strong>   np.array(test_prediction[:,1]), pos_label=1)</strong>
<strong>&gt;&gt;&gt; plt.plot(fpr_train, tpr_train)</strong>
<strong>&gt;&gt;&gt; plt.plot(fpr_test, tpr_test)</strong>
<strong>&gt;&gt;&gt; plt.xlabel('False Positive Rate')</strong>
<strong>&gt;&gt;&gt; plt.ylabel('True Positive Rate')</strong>
</pre></div><p>However, upon plotting the ROC curve for the results, we find that we have not improved very much over the logistic regression:</p><div><img src="img/B04881_05_46.jpg" alt="Fitting and SVM to the census data"/></div><p>It may be difficult to see, but the red line in the upper left-hand corner of the image is the performance on the training set, while the blue line is the performance on the test set. Thus, we are <a id="id369" class="indexterm"/>in a situation such as we described previously, where the model almost perfectly predicts the training data but poorly generalizes to the test set.</p><p>In some sense, we were able to make progress because we used a nonlinear function to represent similarity within our data. However, the model now fits our data too well. If we wanted to experiment more with SVM models, we could tune a number of parameters: we could change kernel function, adjust the bandwidth of the Gaussian kernel (or the particular hyper parameters of whichever kernel function we chose), or tune the amount by which we penalize errors in classification. However, for our next step of algorithm optimization, we will instead switch gears  and try to incorporate nonlinearity with many weak models instead of one overfit model, a concept known as <a id="id370" class="indexterm"/>boosting.</p></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec55"/>Boosting – combining small models to improve accuracy</h2></div></div></div><p>In the previous<a id="id371" class="indexterm"/> examples, we have implicitly assumed that there is a single model that can describe all the <a id="id372" class="indexterm"/>patterns present in our data set. What if, instead, a different model were best suited for a pattern represented by a subset of data, and only by combining models representing many of these smaller patterns can we can get an accurate picture? This is the intuition behind boosting—we start with a weak individual model, determine which points it correctly classifies, and fit additional models to compensate for points missed by this first model. While each additional model is also relatively poor on its own, by successively adding these weak models that capture a certain subset of the data, we gradually arrive at an accurate prediction overall. Furthermore, because each of the models in this group is fit to only a subset of the data, we have to worry less about over-fitting. While the general idea of boosting can be applied to many models, let us look at an example using the decision trees we covered in <a class="link" href="ch04.html" title="Chapter 4. Connecting the Dots with Models – Regression Methods">Chapter 4</a>, <em>Connecting the Dots with Models – Regression Methods</em>.</p></div><div><div><div><div><h2 class="title"><a id="ch05lvl2sec56"/>Gradient boosted decision trees</h2></div></div></div><p>Recall that in <a class="link" href="ch04.html" title="Chapter 4. Connecting the Dots with Models – Regression Methods">Chapter 4</a>, <em>Connecting the Dots with Models – Regression Methods</em>, we achieved greater <a id="id373" class="indexterm"/>predictive power in our regression task by averaging over a set of trees with random<a id="id374" class="indexterm"/> features. Gradient boosted decision trees (Breiman, Leo. Arcing the edge. Technical Report 486, Statistics Department, University of California at Berkeley, 1997; Friedman, Jerome H. <em>Greedy function approximation: a gradient boosting machine</em>. Annals of statistics (2001): 1189-1232; Friedman, Jerome H. <em>Stochastic gradient boosting</em>. Computational Statistics and Data Analysis 38.4 (2002): 367-378.) follow a similar strategy, but instead of choosing random features with each step, we greedily optimize at each point. The general algorithm is:</p><div><ol class="orderedlist arabic"><li class="listitem">Start with a constant value, such as the average response across the input data. This is the baseline model, <em>F0</em>.</li><li class="listitem">Fit a decision tree <em>h</em> to the training data, usually limiting it to have very shallow depth, with the target as the <a id="id375" class="indexterm"/><strong>pseudo-residuals</strong> for each point <code class="literal">i</code> given by:<div><img src="img/B04881_05_50.jpg" alt="Gradient boosted decision trees"/></div></li><li class="listitem">Conceptually, the pseudo-residual for a given loss function L (such as the squared error that we studied in <a class="link" href="ch04.html" title="Chapter 4. Connecting the Dots with Models – Regression Methods">Chapter 4</a>, <em>Connecting the Dots with Models – Regression Methods</em> or the hinge loss for the SVM described above) is the derivative of the loss function with respect to the value of the current model <em>F</em> at a point <code class="literal">yi</code>. While a standard residual would just give the difference between the predicted and observed value, the pseudo-residual represents how rapidly the loss is changing at a given point, and thus in what direction we need to move the model parameters to better classify this point.</li><li class="listitem">To step 1, add the value of the tree in step 2 multiplied by an optimal step <code class="literal">γ</code> and a learning rate <code class="literal">α</code>:<div><img src="img/B04881_05_47.jpg" alt="Gradient boosted decision trees"/></div></li><li class="listitem">We could either choose a <code class="literal">γ</code> that is optimal for the whole tree, or for each individual leaf node, and we can determine the optimal value using a method such as the Newton optimization we discussed above.</li><li class="listitem">Repeat steps 1–3 until convergence.</li></ol></div><p>The goal is that by<a id="id376" class="indexterm"/> fitting several <a id="id377" class="indexterm"/>weaker trees, in aggregate they make better predictions as they sequentially are fit to compensate for the remaining residuals in the model at each step. In practice, we also choose only a subset of the training data to fit the trees at each stage, which should further reduce the possibility of over-fitting. Let us examine this theory on our dataset by fitting a model with 200 trees with a maximum depth of 5:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; from sklearn.ensemble import GradientBoostingClassifier</strong>
<strong>&gt;&gt;&gt; gbm = GradientBoostingClassifier(n_estimators=200, learning_rate=1.0,\</strong>
<strong>… max_depth=5, random_state=0).fit(census_features_train.toarray(),census_income_train) </strong>
<strong>&gt;&gt;&gt; train_prediction = gbm.predict_proba(census_features_train.toarray())  </strong>
<strong>&gt;&gt;&gt; test_prediction = gbm.predict_proba(census_features_test.toarray())  </strong>
<strong>&gt;&gt;&gt; fpr_train, tpr_train, thresholds_train = metrics.roc_curve(np.array(census_income_train),\</strong>
<strong>…   np.array(train_prediction[:,1]), pos_label=1)</strong>
<strong>&gt;&gt;&gt;fpr_test, tpr_test, thresholds_test = metrics.roc_curve(np.array(census_income_test),\</strong>
<strong>…   np.array(test_prediction[:,1]), pos_label=1) </strong>
</pre></div><p>Now, when we plot the results, we see a remarkable increase in accuracy on the test set:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; plt.plot(fpr_train, tpr_train)</strong>
<strong>&gt;&gt;&gt; plt.plot(fpr_test, tpr_test)</strong>
<strong>&gt;&gt;&gt; plt.xlabel('False Positive Rate')</strong>
<strong>&gt;&gt;&gt; plt.ylabel('True Positive Rate')</strong>
</pre></div><p> </p><div><img src="img/B04881_05_48.jpg" alt="Gradient boosted decision trees"/></div><p>Similar to the <a id="id378" class="indexterm"/>random <a id="id379" class="indexterm"/>forest model, we can examine the importance of features as determined by the loss in accuracy upon shuffling their values among data points:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; np.array(expanded_headers)[np.argsort(gbm.feature_importances_)]</strong>
<strong>array(['native-country Outlying-US(Guam-USVI-etc)',        'native-country Holand-Netherlands', 'native-country Laos',        'native-country Hungary', 'native-country Honduras',        'workclass Never-worked', 'native-country Nicaragua',        'education Preschool', 'marital-status Married-AF-spouse',        'native-country Portugal', 'occupation Armed-Forces',        'native-country Trinadad&amp;Tobago', 'occupation Priv-house-serv',        'native-country Dominican-Republic', 'native-country Hong',        'native-country Greece', 'native-country El-Salvador',        'workclass Without-pay', 'native-country Columbia',        'native-country Yugoslavia', 'native-country Thailand',        'native-country Scotland', 'native-country Puerto-Rico',        'education 1st-4th', 'education 5th-6th'</strong>
</pre></div><p>Note that this is not directly comparable to the same evaluation we performed for the logistic regression model as the importance here is not determined by whether the feature predicts positively or negatively, which is implicit in the sign of the coefficients in the logistic regression.</p><p>Also note that there is <a id="id380" class="indexterm"/>a subtler <a id="id381" class="indexterm"/>problem here with interpreting the output coefficients: many of our features are actually individual categories of a common feature, such as country of origin or education level. What we are really interested in is the importance of the overall feature, not the individual levels. Thus, to quantify feature importance more accurately, we could average the importance over all columns containing categories belonging to a common feature.</p><p>If we wanted to further tune the performance of the gbm model, we could perform a search of different values for the number of trees, the depth of those trees, the learning rate (<code class="literal">α</code> in the formulas above), and <code class="literal">min_samples_leaf</code> (which determines the minimum number of data points that need to be present to split the data form a bottom-most split, or leaf, in the tree), among others. As a rule of thumb, making deeper trees will increase the risk of over-fitting, but shallower trees will requires a larger number of models to achieve good accuracy. Similarly, a lower learning rate will also control over-fitting by reducing the contribution of any single tree to the model score, but again may require a tradeoff in more models to achieve the desired level of predictive accuracy. The balance between these parameters may be guided both by the application (how accurate the model should be to contribute meaningfully to a business problem) as well as performance considerations (if the model needs to run online in a website, for example, a smaller number of trees that occupy less memory may be beneficial and worth a somewhat reduced accuracy).</p></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch05lvl1sec31"/>Comparing classification methods</h1></div></div></div><p>In this chapter we <a id="id382" class="indexterm"/>have examined classification using logistic regression, support vector machines, and <a id="id383" class="indexterm"/>gradient <a id="id384" class="indexterm"/>boosted decision trees. In what scenarios should we prefer one algorithm over another?</p><p>For logistic regression, the data ideally will be linearly separable (the exponent in the formula for the logistic regression, after all, is essentially the same as the SVM equation for a separating hyperplane). If our goal is inference (producing a unit increase in response per 1-unit increase of input measurement, as we described in <a class="link" href="ch01.html" title="Chapter 1. From Data to Decisions – Getting Started with Analytic Applications">Chapter 1</a>, <em>From Data to Decisions – Getting Started with Analytic Applications</em>) then the coefficients and log-odds values will be helpful. The stochastic gradient method can also be helpful in cases where we are unable to process all the data concurrently, while the second order methods we discussed may be easier to employ on un-normalized data. Finally, in the context of serializing model parameters and using these results to score new data, the logistic regression is attractive in that it is represented by a vector of numbers and is thus easily stored.</p><p>Support vector machines, as we discussed, can accommodate complex nonlinear boundaries between inputs. They can also be used on data without a vector representation, or data of different lengths, making them quite flexible. However, they require more computational resources for fitting as well as scoring.</p><p>Gradient boosted <a id="id385" class="indexterm"/>decision<a id="id386" class="indexterm"/> trees<a id="id387" class="indexterm"/> can fit nonlinear boundaries between inputs, but only certain kinds. Consider that a decision tree splits a dataset into two groups at each decision node. Thus, the resulting boundaries represent a series of hyperplanes in the m-dimensional space of the dataset, but only split along a particular dimension at each pass and only in a straight line. Thus, these planes will not necessarily capture the nonlinearity possible with the SVM, but if the data can be separated in this piecewise fashion a GBM may perform well.</p><p>The flowchart below gives a general overview from choosing among the classification methods we have discussed. Also, keep in mind that the Random Forest algorithm we discussed in <a class="link" href="ch04.html" title="Chapter 4. Connecting the Dots with Models – Regression Methods">Chapter 4</a>, <em>Connecting the Dots with Models – Regression Methods</em> may also be applied for classification, while the SVM and GBM models describe in this chapter have forms that may be applied for regression.</p><p> </p><div><img src="img/B04881_05_49.jpg" alt="Comparing classification methods"/></div></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch05lvl1sec32"/>Case study: fitting classifier models in pyspark</h1></div></div></div><p>Now that we have <a id="id388" class="indexterm"/>examined several algorithms for fitting classifier models in the scikit-learn library, let us look at how we might implement a similar model in PySpark. We can use the same census dataset from earlier in this chapter, and start by loading the data using a textRdd after starting the spark context:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; censusRdd = sc.textFile('census.data')</strong>
</pre></div><p>Next we need to split the data into individual fields, and strip whitespace</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; censusRddSplit = censusRdd.map(lambda x: [e.strip() for e in x.split(',')])</strong>
</pre></div><p>Now, as before, we need to determine which of our features are categorical and need to be re-encoded using one-hot encoding. We do this by taking a single row and asking whether the string in each position represent a digit (is not a categorical variable):</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; categoricalFeatures = [e for e,i in enumerate(censusRddSplit.take(1)[0]) if i.isdigit()==False]</strong>
<strong>&gt;&gt;&gt; allFeatures = [e for e,i in enumerate(censusRddSplit.take(1)[0])]</strong>
</pre></div><p>Now, as before, we need to collect a dictionary representing the string-to-position mapping of each categorical label to a place in the one-hot-encoding vector:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; categoricalMaps = []</strong>
<strong>&gt;&gt;&gt; for c in categoricalFeatures:</strong>
<strong>…    catDict = censusRddSplit.map(lambda x: x[c] if len(x) &gt; c else None).\</strong>
<strong>…    filter(lambda x: x is not None).\</strong>
<strong>…    distinct().\</strong>
<strong>…    zipWithIndex().\</strong>
<strong>…    collectAsMap()</strong>
<strong>…    censusRddSplit.map(lambda x: x[c]).take(1)</strong>
<strong>…    categoricalMaps.append(catDict)</strong>
</pre></div><p>Next, we calculate what the total length of the one-hot encoding vector should be to represent all the features. We subtract two from this value because the last categorical features is income, which has two values and which we use as the label for the data:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; expandedFeatures = 0</strong>
<strong>&gt;&gt;&gt; for c in categoricalMaps:</strong>
<strong>…    expandedFeatures += len(c)</strong>
<strong>expandedFeatures += len(allFeatures)-len(categoricalFeatures)-2</strong>
</pre></div><p>Now, we use a map <a id="id389" class="indexterm"/>function to turn all of our data into labeled point objects for use in logistic regression. To do so, we extract the label for each row from the last element in the vector, then instantiate an empty vector using the length of the one-hot-encoded feature set we calculated preceding. We use two indices: one for which categorical variable we are accessing (to index the right dictionary to perform our mapping), and a second to record where in the feature vector we are (since for categorical variables we will skip over k spaces for a given variable, where k is the number of categories in that variable).</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; def formatPoint(p):</strong>
<strong>…      if p[-1] == '&lt;=50K':</strong>
<strong>…          label = 0</strong>
<strong>…      else:</strong>
<strong> …         label = 1</strong>
<strong>…      vector = [0.0]*expandedFeatures</strong>
<strong>…      categoricalIndex = 0</strong>
<strong>…      categoricalVariable = 0</strong>
<strong>…      for e,c in enumerate(p[:-1]):</strong>
<strong>…          if e in categoricalFeatures:</strong>
<strong> …             vector[categoricalIndex + categoricalMaps[categoricalVariable][c]]=1</strong>
<strong>…              categoricalIndex += len(categoricalMaps[categoricalVariable])</strong>
<strong>…              categoricalVariable +=1</strong>
<strong> …         else:</strong>
<strong> …             vector[e] = c</strong>
<strong>…              categoricalIndex += 1</strong>
<strong>…      return LabeledPoint(label,vector)</strong>
</pre></div><p>We apply this function to all data points</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; censusRddLabeled = censusRddSplit.map(lambda x: formatPoint(x))</strong>
</pre></div><p>Now that our data is in the right format, we can run logistic regression:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; from pyspark.mllib.classification import LogisticRegressionWithLBFGS</strong>
<strong>&gt;&gt;&gt; censusLogistic = LogisticRegressionWithLBFGS.train(censusRddLabeled )</strong>
</pre></div><p>To access the weights<a id="id390" class="indexterm"/> from the resulting model, we can inspect the weights parameter:</p><div><pre class="programlisting">
<strong>&gt;&gt;&gt; censusLogistic.weights</strong>
</pre></div><p>If we wanted to apply the generated model to a new dataset, we can use the <code class="literal">predict()</code> method of <code class="literal">censusLogistic</code> on a new feature vector. The steps described above are similar to the data processing we used for the scikit-learn example, but can ultimately scale to larger datasets.</p></div></div>


  <div><div><div><div><div><h1 class="title"><a id="ch05lvl1sec33"/>Summary</h1></div></div></div><p>In this chapter, you've examined how to use classification models and some of the strategies for improving model performance. In addition to transforming categorical features, you've looked at the interpretation of logistic regression accuracy using the ROC curve. In attempting to improve model performance, we demonstrated the use of SVMs and were able to increase performance on the training set the cost of overfitting. Finally, we were able to achieve good performance on the test set through gradient boosted decision trees. Taken together with the material in <a class="link" href="ch04.html" title="Chapter 4. Connecting the Dots with Models – Regression Methods">Chapter 4</a>, <em>Connecting the Dots with Models – Regression Methods</em>, you should now have a full toolkit of methods for continuous and categorical outcomes, which you can apply to problems in main domains.</p></div></div>
</body></html>