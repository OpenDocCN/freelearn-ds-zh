<html><head></head><body><div class="chapter" title="Chapter&#xA0;8.&#xA0;Multiplying Performance with Parallel Computing"><div class="titlepage"><div><div><h1 class="title"><a id="ch08"/>Chapter 8. Multiplying Performance with Parallel Computing</h1></div></div></div><p>In this chapter, we will learn how to write and execute a parallel R code, where different parts of the code run simultaneously. So far, we have learned various ways to optimize the performance of R programs running serially, that is in a single process. This does not take full advantage of the computing power of modern CPUs with multiple cores. Parallel computing allows us to tap into all the computational <a id="id263" class="indexterm"/>resources available and to speed up the execution of R programs by many times. We will examine the different types of parallelism and how to implement them in R, and we will take a closer look at a few performance considerations when designing the parallel architecture of R programs.</p><p>This chapter covers the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Data parallelism versus task parallelism</li><li class="listitem" style="list-style-type: disc">Implementing data parallel algorithms</li><li class="listitem" style="list-style-type: disc">Implementing task parallel algorithms</li><li class="listitem" style="list-style-type: disc">Executing tasks in parallel on a cluster of computers</li><li class="listitem" style="list-style-type: disc">Shared memory versus distributed memory parallelism</li><li class="listitem" style="list-style-type: disc">Optimizing parallel performance</li></ul></div><div class="section" title="Data parallelism versus task parallelism"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec42"/>Data parallelism versus task parallelism</h1></div></div></div><p>Many modern software applications are designed to run computations in parallel in order to take advantage of the multiple CPU cores available on almost any computer today. Many R <a id="id264" class="indexterm"/>programs <a id="id265" class="indexterm"/>can similarly be written in order to run in parallel. However, the extent of possible parallelism depends on the computing task involved. On one side of the scale are <span class="strong"><strong>embarrassingly parallel</strong></span> tasks, where there are no dependencies between the parallel subtasks; such tasks can be made to run in parallel very easily. An example of this is, building an ensemble of decision trees in a random forest algorithm—randomized decision trees can be built independently from one another and in parallel across tens or hundreds of CPUs, and can be combined to form the random forest. On the other end of the scale are tasks that cannot be parallelized, as each step of the task depends on the results of the previous step. One such example is a depth-first search of a tree, where the subtree to search at each step depends on the path taken in previous steps. Most algorithms fall somewhere in between with some steps that must run serially and some that can run in parallel. With this in mind, careful thought must be given when designing a parallel code that works correctly and efficiently.</p><p>Often an R program has some parts that have to be run serially and other parts that can run in parallel. Before making the effort to parallelize any of the R code, it is useful to have <a id="id266" class="indexterm"/>an estimate of the potential performance gains that can be achieved. <span class="strong"><strong>Amdahl's law</strong></span> provides a way to estimate the best attainable performance gain when you convert a code from serial to parallel execution. It divides a computing task into its serial and potentially-parallel parts and states that the time needed to execute the task in parallel will be no less than this formula:</p><p><span class="emphasis"><em>T(n) = T(1)(P + (1-P)/n)</em></span>, where:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>T(n)</em></span> is the time taken to execute the task using <span class="emphasis"><em>n</em></span> parallel processes</li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>P</em></span> is the proportion of the whole task that is strictly serial</li></ul></div><p>The theoretical best possible speed up of the parallel algorithm is thus:</p><p><span class="emphasis"><em>S(n) = T(1) / T(n) = 1 / (P + (1-P)/n)</em></span></p><p>For example, given a task that takes 10 seconds to execute on one processor, where half of the task can be run in parallel, then the best possible time to run it on four processors is <span class="emphasis"><em>T(4) = 10(0.5 + (1-0.5)/4) = 6.25 </em></span>seconds.</p><p>The theoretical best possible speed up of the parallel algorithm with four processors is <span class="emphasis"><em>1 / (0.5 + (1-0.5)/4) = 1.6x</em></span>.</p><p>The following figure shows you how the theoretical best possible execution time decreases as more CPU cores are added. Notice that the execution time reaches a limit that is just above five seconds. This corresponds to the half of the task that must be run serially, where parallelism does not help.</p><div class="mediaobject"><img src="graphics/9263OS_08_01.jpg" alt="Data parallelism versus task parallelism"/><div class="caption"><p>Best possible execution time versus number of CPU cores</p></div></div><p>In general, Amdahl's law means that the fastest execution time for any parallelized algorithm is limited by the time needed for the serial portions of the algorithm. Bear in mind that <a id="id267" class="indexterm"/>Amdahl's law <a id="id268" class="indexterm"/>provides only a theoretical estimate. It does not account for the overheads of parallel computing (such as starting and coordinating tasks) and assumes that the parallel portions of the algorithm are infinitely scalable. In practice, these factors might significantly limit the performance gains of parallelism, so use Amdahl's law only to get a rough estimate of the maximum speedup possible.</p><p>There are two main classes of parallelism: data parallelism and task parallelism. Understanding these concepts helps to determine what types of tasks can be modified to run in parallel.</p><p>In <span class="strong"><strong>data parallelism</strong></span>, a dataset <a id="id269" class="indexterm"/>is divided into multiple partitions. Different partitions are distributed to multiple processors, and the same task is executed on each partition of data. Take for example, the task of finding the maximum value in a vector dataset, say one that has one billion numeric data points. A serial algorithm to do this would look like the following code, which iterates over every element of the data in sequence to search for the largest value. (This code is intentionally verbose to illustrate how the algorithm works; in practice, the <code class="literal">max()</code> function in R, though also serial in nature, is much faster.)</p><div class="informalexample"><pre class="programlisting">serialmax &lt;- function(data) {
    max = -Inf
    for (i in data) {
        if (i &gt; max)
            max = i
    }
    return max
}</pre></div><p>One way to <a id="id270" class="indexterm"/>parallelize this <a id="id271" class="indexterm"/>algorithm is to split the data into partitions. If we have a computer with eight CPU cores, we can split the data into eight partitions of 125 million numbers each. Here is the pseudocode for how to perform the same task in parallel:</p><div class="informalexample"><pre class="programlisting"># Run this in parallel across 8 CPU cores
part.results &lt;- run.in.parallel(serialmax(data.part))
# Compute global max
global.max &lt;- serialmax(part.results)</pre></div><p>This pseudocode runs eight instances of <code class="literal">serialmax()</code>in parallel—one for each data partition—to find the local maximum value in each partition. Once all the partitions have been processed, the algorithm finds the global maximum value by finding the largest value among the local maxima. This parallel algorithm works because the global maximum of a dataset must be the largest of the local maxima from all the partitions.</p><p>The following figure depicts data parallelism pictorially. The key behind data parallel algorithms is that each partition of data can be processed independently of the other partitions, and the results from all the partitions can be combined to compute the final results. This is similar to the mechanism of the MapReduce framework from Hadoop. Data parallelism allows algorithms to scale up easily as data volume increases—as more data is added to the dataset, more computing nodes can be added to a cluster to process new partitions of data.</p><div class="mediaobject"><img src="graphics/9263OS_08_02.jpg" alt="Data parallelism versus task parallelism"/><div class="caption"><p>Data parallelism</p></div></div><p>Other examples of<a id="id272" class="indexterm"/> computations and algorithms that can be run in a data parallel way include:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Element-wise matrix operations such as addition and subtraction</strong></span>: The matrices can be partitioned and the operations are applied to each pair of partitions.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Means</strong></span>: The sums and number of elements in each partition can be added to find the global sum and number of elements from which the mean can be computed.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>K-means clustering</strong></span>: After data partitioning, the K centroids are distributed to all the partitions. Finding the closest centroid is performed in parallel and independently across the partitions. The centroids are updated by first, calculating the sums and the counts of their respective members in parallel, and then consolidating them in a single process to get the global means.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Frequent itemset mining using the Partition algorithm</strong></span>: In the first pass, the frequent itemsets are mined from each partition of data to generate a global set of candidate itemsets; in the second pass, the supports of the candidate itemsets are summed from each partition to filter out the globally infrequent ones.</li></ul></div><p>The other <a id="id273" class="indexterm"/>main class <a id="id274" class="indexterm"/>of parallelism<a id="id275" class="indexterm"/> is <span class="strong"><strong>task parallelism</strong></span>, where tasks are distributed to and executed on different processors in parallel. The tasks on each processor might be the same or different, and the data that they act on might also be the same or different. The key difference between task parallelism and data parallelism is that the data is not divided into partitions. An example of a task parallel algorithm performing the same task on the same data is the training of a random forest model. A random forest is a collection of decision trees built independently on the same data. During the training process for a particular tree, a random subset of the data is chosen as the training set, and the variables to consider at each branch of the tree are also selected randomly. Hence, even though the same data is used, the trees are different from one another. In order to train a random forest of say 100 decision trees, the workload could be distributed to a computing cluster with 100 processors, with each processor building one tree. All the processors perform the same task on the same data (or exact copies of the data), but the data is not partitioned.</p><p>The parallel tasks can also be different. For example, computing a set of summary statistics on the same set of data can be done in a task parallel way. Each process can be assigned to compute a different statistic—the mean, standard deviation, percentiles, and so on.</p><p>Pseudocode <a id="id276" class="indexterm"/>of a task <a id="id277" class="indexterm"/>parallel algorithm might look like this:</p><div class="informalexample"><pre class="programlisting"># Run 4 tasks in parallel across 4 cores
for (task in tasks)
    run.in.parallel(task)
# Collect the results of the 4 tasks
results &lt;- collect.parallel.output()
# Continue processing after all 4 tasks are complete</pre></div></div></div>
<div class="section" title="Implementing data parallel algorithms"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec43"/>Implementing data parallel algorithms</h1></div></div></div><p>Several R packages allow code to be executed in parallel. The <code class="literal">parallel</code> package that comes with R provides the foundation for most parallel computing capabilities in other packages. Let's see how it works with an example.</p><p>This example<a id="id278" class="indexterm"/> involves finding documents that match a regular expression. Regular expression matching is a fairly computational expensive task, depending on the complexity of the regular expression. The corpus, or set of documents, for this example is a sample of the Reuters-21578 dataset for the topic corporate acquisitions (<code class="literal">acq</code>) from the <code class="literal">tm</code> package. Because this dataset contains only 50 documents, they are replicated 100,000 times to form a corpus of 5 million documents so that parallelizing the code will lead to meaningful savings in execution times.</p><div class="informalexample"><pre class="programlisting">library(tm)
data("acq")
textdata &lt;- rep(sapply(content(acq), content), 1e5)</pre></div><p>The task is to find documents that match the regular expression <code class="literal">\d+(,\d+)? mln dlrs</code>, which represents monetary amounts in millions of dollars. In this regular expression, <code class="literal">\d+</code> matches a string of one or more digits, and <code class="literal">(,\d+)?</code> optionally matches a comma followed by one more digits. For example, the strings <code class="literal">12 mln dlrs</code>, <code class="literal">1,234 mln dlrs</code> and <code class="literal">123,456,789 mln dlrs</code> will match the regular expression. First, we will measure the execution time to find these documents serially with <code class="literal">grepl()</code>:</p><div class="informalexample"><pre class="programlisting">pattern &lt;- "\\d+(,\\d+)? mln dlrs"
system.time(res1 &lt;- <span class="strong"><strong>grepl(pattern, textdata)</strong></span>)
##   user  system elapsed 
## 65.601   0.114  65.721</pre></div><p>Next, we will modify the code to run in parallel and measure the execution time on a computer with four CPU cores:</p><div class="informalexample"><pre class="programlisting">library(parallel)
detectCores()
## [1] 4
cl &lt;- makeCluster(detectCores())
part &lt;- clusterSplit(cl, seq_along(textdata))
text.partitioned &lt;- lapply(part, function(p) textdata[p])
system.time(res2 &lt;- unlist(
    <span class="strong"><strong>parSapply(cl, text.partitioned, grepl, pattern = pattern)</strong></span>
)) 
##  user  system elapsed 
## 3.708   8.007  50.806 
stopCluster(cl)</pre></div><p>In this code, the <code class="literal">detectCores()</code> function reveals how many CPU cores are available on the machine, where this code is executed. Before running any parallel code, <code class="literal">makeCluster()</code> is called to create a local cluster of processing nodes with all four CPU cores. The <a id="id279" class="indexterm"/>corpus is then split into four partitions using the <code class="literal">clusterSplit()</code> function to determine the ideal split of the corpus such that each partition has roughly the same number of documents.</p><p>The actual parallel execution of <code class="literal">grepl()</code> on each partition of the corpus is carried out by the <code class="literal">parSapply()</code> function. Each processing node in the cluster is given a copy of the partition of data that it is supposed to process along with the code to be executed and other variables that are needed to run the code (in this case, the <code class="literal">pattern</code> argument). When all four processing nodes have completed their tasks, the results are combined in a similar fashion to <code class="literal">sapply()</code>.</p><p>Finally, the cluster is destroyed by calling <code class="literal">stopCluster()</code>.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip07"/>Tip</h3><p>It is good practice to ensure that <code class="literal">stopCluster()</code> is always called in production code, even if an error occurs during execution. This can be done as follows:</p><div class="informalexample"><pre class="programlisting">doSomethingInParallel &lt;- function(...) {
    cl &lt;- makeCluster(...)
    on.exit(stopCluster(cl))
    # do something
}</pre></div></div></div><p>In this example, running the task in parallel on four processors resulted in a 23 percent reduction in the execution time. This is not in proportion to the amount of compute resources used to <a id="id280" class="indexterm"/>perform the task; with four times as many CPU cores working on it, a perfectly parallelizable task might experience as much as a 75 percent runtime reduction. However, remember Amdahl's law—the speed of parallel code is limited by the serial parts, which includes the overheads of parallelization. In this case, calling <code class="literal">makeCluster()</code> with the default arguments <a id="id281" class="indexterm"/>creates a <span class="strong"><strong>socket-based cluster</strong></span>. When such a cluster is created, additional copies of R are run as workers. The workers communicate with the master R process using network sockets, hence the name. The worker R processes are initialized with the relevant packages loaded, and data partitions are serialized and sent to each worker process. These overheads can be significant, especially in data parallel algorithms where large volumes of data needs to be transferred to the worker processes.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note12"/>Note</h3><p>Besides <code class="literal">parSapply()</code>, <code class="literal">parallel</code> also provides the <code class="literal">parApply()</code> and <code class="literal">parLapply()</code> functions; these functions are analogous to the standard <code class="literal">sapply()</code>, <code class="literal">apply()</code>, and <code class="literal">lapply()</code> functions, respectively. In addition, the <code class="literal">parLapplyLB()</code> and <code class="literal">parSapplyLB()</code> functions provide load balancing, which is useful when the execution of each parallel task takes variable amounts of time. Finally, <code class="literal">parRapply()</code> and <code class="literal">parCapply()</code> are parallel row and column <code class="literal">apply()</code> functions for matrices.</p></div></div><p>On non-Windows systems, <code class="literal">parallel</code> supports another type of cluster that often incurs less <a id="id282" class="indexterm"/>overheads—<span class="strong"><strong>forked clusters</strong></span>. In these clusters, new worker processes are forked from the parent R process with a copy of the data. However, the data is not actually copied in the memory unless it is modified by a child process. This means that, compared to socket-based clusters, initializing child processes is quicker and the memory usage is often lower.</p><p>Another advantage of using forked clusters is that <code class="literal">parallel</code> provides a convenient and concise way to run tasks on them via the <code class="literal">mclapply()</code>, <code class="literal">mcmapply()</code>, and <code class="literal">mcMap()</code> functions. (These functions start with <code class="literal">mc</code> because they were originally a part of the <code class="literal">multicore</code> package) There is no need to explicitly create and destroy the cluster, as these functions do this automatically. We can simply call <code class="literal">mclapply()</code> and state the number of worker processes to fork via the <code class="literal">mc.cores</code> argument:</p><div class="informalexample"><pre class="programlisting">system.time(res3 &lt;- unlist(
    mclapply(text.partitioned, grepl, pattern = pattern,
             mc.cores = detectCores())
))
##    user  system elapsed 
## 127.012   0.350  33.264</pre></div><p>This shows a 49 percent reduction in execution time compared to the serial version, and 35 percent <a id="id283" class="indexterm"/>reduction compared to parallelizing using a socket-based cluster. For this example, forked clusters provide the best performance.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note13"/>Note</h3><p>Due to differences in system configuration, you might see very different results when you try the examples in this chapter in your own environment. When you develop parallel code, it is important to test the code in an environment that is similar to the one that it will eventually run in.</p></div></div></div>
<div class="section" title="Implementing task parallel algorithms"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec44"/>Implementing task parallel algorithms</h1></div></div></div><p>Let's now see <a id="id284" class="indexterm"/>how to implement a task parallel algorithm using both socket-based and forked clusters. We will look at how to run the same task and different tasks on workers in a cluster.</p><div class="section" title="Running the same task on workers in a cluster"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec25"/>Running the same task on workers in a cluster</h2></div></div></div><p>To demonstrate <a id="id285" class="indexterm"/>how to run the same task on a cluster, the task for this example is to generate 500 million Poisson random numbers. We will do this by using L'Ecuyer's combined multiple-recursive generator, which is the only random number generator in base R that supports multiple streams to generate random numbers in parallel. The random number generator is selected by calling the <code class="literal">RNGkind()</code> function.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note14"/>Note</h3><p>We cannot just use any random number generator in parallel because the randomness of the data depends on the algorithm used to generate random data and the seed value given to each parallel task. Most other algorithms were not designed to produce random numbers in multiple parallel streams, and might produce multiple highly correlated streams of numbers, or worse, multiple identical streams!</p></div></div><p>First, we will measure the execution time of the serial algorithm:</p><div class="informalexample"><pre class="programlisting">RNGkind("L'Ecuyer-CMRG")
nsamples &lt;- 5e8
lambda &lt;- 10
system.time(random1 &lt;- <span class="strong"><strong>rpois(nsamples, lambda)</strong></span>)
##   user  system elapsed
## 51.905   0.636  52.544</pre></div><p>To generate the random numbers on a cluster, we will first distribute the task evenly among the workers. In the following code, the integer vector <code class="literal">samples.per.process</code> contains the number of random numbers that each worker needs to generate on a four-core CPU. The <code class="literal">seq()</code> function produces <code class="literal">ncores+1</code> numbers evenly distributed between <code class="literal">0</code> and <code class="literal">nsamples</code>, with the first number being <code class="literal">0</code> and the next <code class="literal">ncores</code> numbers indicating <a id="id286" class="indexterm"/>the approximate cumulative number of samples across the worker processes. The <code class="literal">round()</code> function rounds off these numbers into integers and <code class="literal">diff()</code> computes the difference between them to give the number of random numbers that each worker process should generate.</p><div class="informalexample"><pre class="programlisting">cores &lt;- detectCores()
cl &lt;- makeCluster(ncores)
samples.per.process &lt;-
    diff(round(seq(0, nsamples, length.out = ncores+1)))</pre></div><p>Before we can generate the random numbers on a cluster, each worker needs a different seed from which it can generate a stream of random numbers. The seeds need to be set on all the workers before running the task, to ensure that all the workers generate different random numbers.</p><p>For a socket-based cluster, we can call <code class="literal">clusterSetRNGStream()</code> to set the seeds for the workers, then run the random number generation task on the cluster. When the task is completed, we call <code class="literal">stopCluster()</code> to shut down the cluster:</p><div class="informalexample"><pre class="programlisting">clusterSetRNGStream(cl)
system.time(random2 &lt;- unlist(
    <span class="strong"><strong>parLapply(cl, samples.per.process, rpois,</strong></span>
               <span class="strong"><strong>lambda = lambda)</strong></span>
))
##  user  system elapsed 
## 5.006   3.000  27.436
stopCluster(cl)</pre></div><p>Using four parallel processes in a socket-based cluster reduces the execution time by 48 percent. The performance of this type of cluster for this example is better than that of the data parallel example because there is less data to copy to the worker processes—only an integer that indicates how many random numbers to generate.</p><p>Next, we run the same task on a forked cluster (again, this is not supported on Windows). The <code class="literal">mclapply()</code> function can set the random number seeds for each worker for us, when the <code class="literal">mc.set.seed</code> argument is set to <code class="literal">TRUE</code>; we do not need to call <code class="literal">clusterSetRNGStream()</code>. Otherwise, the code is similar to that of the socket-based cluster:</p><div class="informalexample"><pre class="programlisting">system.time(random3 &lt;- unlist(
    <span class="strong"><strong>mclapply(samples.per.process, rpois,</strong></span>
             <span class="strong"><strong>lambda = lambda,</strong></span>
             <span class="strong"><strong>mc.set.seed = TRUE, mc.cores = ncores)</strong></span>
)) 
##   user  system elapsed 
## 76.283   7.272  25.052</pre></div><p>On our <a id="id287" class="indexterm"/>test machine, the execution time of the forked cluster is slightly faster, but close to that of the socket-based cluster, indicating that the overheads for this task are similar for both types of clusters.</p></div><div class="section" title="Running different tasks on workers in a cluster"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec26"/>Running different tasks on workers in a cluster</h2></div></div></div><p>So far, we have executed the same tasks on each parallel process. The <span class="emphasis"><em>parallel</em></span> package also allows <a id="id288" class="indexterm"/>different tasks to be executed on different workers. For this example, the task is to generate not only Poisson random numbers, but also uniform, normal, and exponential random numbers. As before, we start by measuring the time to perform this task serially:</p><div class="informalexample"><pre class="programlisting">RNGkind("L'Ecuyer-CMRG")
nsamples &lt;- 5e7
pois.lambda &lt;- 10
system.time(random1 &lt;- <span class="strong"><strong>list(pois = rpois(nsamples,</strong></span>
                                          <span class="strong"><strong>pois.lambda),</strong></span>
                            <span class="strong"><strong>unif = runif(nsamples),</strong></span>
                            <span class="strong"><strong>norm = rnorm(nsamples),</strong></span>
                            <span class="strong"><strong>exp = rexp(nsamples))</strong></span>)
##   user  system elapsed 
## 14.180   0.384  14.570</pre></div><p>In order to run different tasks on different workers on socket-based clusters, a list of function calls and their associated arguments must be passed to <code class="literal">parLapply()</code>. This is a bit cumbersome, but <code class="literal">parallel</code> unfortunately does not provide an easier interface to run different tasks on a socket-based cluster. In the following code, the function calls are represented as a list of lists, where the first element of each sublist is the name of the function that runs on a worker, and the second element contains the function arguments. The function <code class="literal">do.call()</code> is used to call the given function with the given arguments.</p><div class="informalexample"><pre class="programlisting">cores &lt;- detectCores()
cl &lt;- makeCluster(cores)
calls &lt;- list(pois = list("rpois", list(n = nsamples,
                                        lambda = pois.lambda)),
              unif = list("runif", list(n = nsamples)),
              norm = list("rnorm", list(n = nsamples)),
              exp = list("rexp", list(n = nsamples)))
clusterSetRNGStream(cl)
system.time(
    random2 &lt;- <span class="strong"><strong>parLapply(cl, calls,</strong></span>
                         <span class="strong"><strong>function(call) {</strong></span>
                             <span class="strong"><strong>do.call(call[[1]], call[[2]])</strong></span>
                         <span class="strong"><strong>})</strong></span>
)
##  user  system elapsed 
## 2.185   1.629  10.403
stopCluster(cl)</pre></div><p>On forked <a id="id289" class="indexterm"/>clusters on non-Windows machines, the <code class="literal">mcparallel()</code> and <code class="literal">mccollect()</code> functions offer a more intuitive way to run different tasks on different workers. For each task, <code class="literal">mcparallel()</code> sends the given task to an available worker. Once all the workers have been assigned their tasks, <code class="literal">mccollect()</code> waits for the workers to complete their tasks and collects the results from all the workers.</p><div class="informalexample"><pre class="programlisting">mc.reset.stream()
system.time({
    jobs &lt;- list()
    jobs[[1]] &lt;- <span class="strong"><strong>mcparallel(rpois(nsamples, pois.lambda),</strong></span>
                            <span class="strong"><strong>"pois", mc.set.seed = TRUE)</strong></span>
    jobs[[2]] &lt;- <span class="strong"><strong>mcparallel(runif(nsamples),</strong></span>
                            <span class="strong"><strong>"unif", mc.set.seed = TRUE)</strong></span>
    jobs[[3]] &lt;- <span class="strong"><strong>mcparallel(rnorm(nsamples),</strong></span>
                            <span class="strong"><strong>"norm", mc.set.seed = TRUE)</strong></span>
    jobs[[4]] &lt;- <span class="strong"><strong>mcparallel(rexp(nsamples),</strong></span>
                            <span class="strong"><strong>"exp", mc.set.seed = TRUE)</strong></span>
    random3 &lt;- <span class="strong"><strong>mccollect(jobs)</strong></span>
})
##   user  system elapsed 
## 14.535   3.569   7.972</pre></div><p>Notice that we also had to call <code class="literal">mc.reset.stream()</code> to set the seeds for random number generation in each worker. This was not necessary when we used <code class="literal">mclapply()</code>, which calls <code class="literal">mc.reset.stream()</code> for us. However, <code class="literal">mcparallel()</code> does not, so we need to call it ourselves.</p></div></div>
<div class="section" title="Executing tasks in parallel on a cluster of computers"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec45"/>Executing tasks in parallel on a cluster of computers</h1></div></div></div><p>By using the <code class="literal">parallel</code> package, we are not limited to running parallel code on a single<a id="id290" class="indexterm"/> computer; we can also do it on a cluster of computers. This allows much larger computational tasks to be performed, irrespective of whether we use data parallelism or task parallelism. Only socket-based clusters can be used for this purpose, as processes cannot be forked onto a different computer.</p><p>There are many ways to set up a cluster of computers to work with R. To keep things simple, all computers in the cluster should have the same configuration for R—the same version of R, installed in the same directories, installed with the same versions of any packages required, and running on the same operating system. The examples in this section have been tested on a cluster of three computers running Ubuntu 14.04—one master node and two worker nodes.</p><p>The master and worker nodes should be on the same network and able to communicate with each other via SSH (port 22) and one other port for exchanging data and code. This communications port can be set with the <code class="literal">R_PARALLEL_PORT</code> environment variable. If it is not set, R will randomly choose a port in the range 11000 to 11999.</p><p>By default, SSH is used to launch R on the workers. First, ensure that the SSH server is set up and running on all the worker nodes.</p><p>For Windows worker nodes, download and install Cygwin from <a class="ulink" href="http://www.cygwin.com">http://www.cygwin.com</a>. When prompted to install additional packages, install the <code class="literal">openssh</code> package. Then, right-click on the <span class="strong"><strong>Cygwin</strong></span> icon and select <span class="strong"><strong>Run as Administrator</strong></span>. In the terminal window that opens, run the following code to set up the SSH server. The <code class="literal">ssh-host-config</code> command configures the SSH server with the default settings. The <code class="literal">chmod 400</code> command sets the permissions on the generated security keys so that only the user who owns the keys can read them, and <code class="literal">cygrunsrv -S sshd</code> starts the SSH server.</p><div class="informalexample"><pre class="programlisting">$ ssh-host-config -y -c "tty ntsec"
$ chmod 400 /etc/ssh_*_key
$ cygrunsrv -S sshd</pre></div><p>Other operating systems like Linux normally come with an installed SSH server. Consult the documentation of your operating system for how to set it up.</p><p>The master node should be able to connect to the worker nodes using key-based authentication, as using password-based authentication to run a cluster might not always work. Use the following commands to set up key-based authentication. Windows <a id="id291" class="indexterm"/>users should run this from within a Cygwin terminal.</p><div class="informalexample"><pre class="programlisting"># Run all these commands on the master node
# Generate an RSA key pair without password
$ ssh-keygen -t rsa
$ chmod 400 .ssh/id_rsa
# Copy public key to worker node (run for every worker)
$ ssh-copy-id -i .ssh/id_rsa.pub worker_username@worker_address
# Test connection (run for every worker)
$ ssh worker_username@worker_address
# You should be able to log in without entering a password</pre></div><p>Once all the computers are set up, we can run parallel tasks on the cluster just as before. The only change needed is in the call to <code class="literal">makeCluster()</code>, where the IP addresses or domain names of the worker nodes must be provided instead of the number of workers to create local workers. In the following example, replace the IP addresses with the IP addresses of your master and worker nodes.</p><div class="informalexample"><pre class="programlisting">workers &lt;- c("192.168.213.225", "192.168.213.226")
nworkers &lt;- length(workers)
cl &lt;- makeCluster(workers, master = "192.168.213.138")</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note15"/>Note</h3><p>If you are having trouble to start the cluster by automatically using <code class="literal">makeCluster()</code>, add the <code class="literal">manual=TRUE</code> argument to the call to <code class="literal">makeCluster()</code>, then follow the instructions given, to start the worker processes on each of the worker nodes.</p></div></div><p>The code that sends the tasks to the workers for execution is then the same as before:</p><div class="informalexample"><pre class="programlisting">clusterSetRNGStream(cl)
samples.per.process &lt;- c(2.5e8, 2.5e8)
lambda &lt;- 10
random &lt;- unlist(
    parLapply(cl, samples.per.process,
              function(n, lambda) rpois(n, lambda),
              lambda)
)
stopCluster(cl)</pre></div><p>Because a <a id="id292" class="indexterm"/>cluster of computers has to communicate over a network, the bandwidth and latency of the network connections play a critical role in the performance of the whole cluster. It is best that the nodes are in the same location, connected by a high-speed network where data and code can be exchanged between the master and worker nodes speedily.</p></div>
<div class="section" title="Shared memory versus distributed memory parallelism"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec46"/>Shared memory versus distributed memory parallelism</h1></div></div></div><p>In the examples that we have seen so far, data is copied from the master process or node to each worker. This is <a id="id293" class="indexterm"/>called <span class="strong"><strong>distributed memory</strong></span> parallelism, where each process has its <a id="id294" class="indexterm"/>own memory space. In other words, each process needs to have its own copy of the data that it needs to work on, even if multiple processes are working on the same<a id="id295" class="indexterm"/> data. This is the typical way to distribute data in a cluster of computers because the workers in the cluster cannot access each other's RAM, so they need their own copy of the data.</p><p>However, this can result in huge redundancies when you run a parallel code on multiple processes on a single computer. If a dataset takes up 5 GB of memory, then running four parallel processes could result in five copies of the data in memory—one for the master and four for the workers—occupying a total of 25 GB. Earlier, we saw that forked clusters might not suffer from this problem, as most operating systems do not make copies of the data in the memory unless it is modified by one of the workers. However, this is not guaranteed. On socket-based clusters, because new instances of R are created, new copies of the data are made for each worker.</p><p>Contrast this <a id="id296" class="indexterm"/>with <span class="strong"><strong>shared memory</strong></span> parallelism, where all the workers share a single copy of the data. This not only saves the memory, but also reduces the time needed to initialize and shut down the cluster, as the data does not need to be copied.</p><p>Although the <code class="literal">parallel</code> package does not provide support for shared memory parallelism by default, we can achieve it by using the right data structures. One example for this is <code class="literal">big.matrix</code> from the <code class="literal">bigmemory</code> package that we learned about in the previous chapter (not available for Windows at the time of writing). In <a class="link" href="ch07.html" title="Chapter 7. Processing Large Datasets with Limited RAM">Chapter 7</a>, <span class="emphasis"><em>Processing Large Datasets with Limited RAM</em></span>, we used <code class="literal">big.matrix</code> for its memory-mapped file <a id="id297" class="indexterm"/>capabilities; in this chapter, we will take advantage of it as a shared memory object for <a id="id298" class="indexterm"/>parallel workers. Besides taking the form of memory-mapped files on disk, <code class="literal">big.matrix</code> objects can also be fully in-memory objects that behave just like standard R matrices. The key difference is that <code class="literal">big.matrix</code> objects are not copied according to the usual R rules for copying objects that we examined in <a class="link" href="ch06.html" title="Chapter 6. Simple Tweaks to Use Less RAM">Chapter 6</a>, <span class="emphasis"><em>Simple Tweaks to Use Less RAM</em></span>. Instead, they are only copied when a call to <code class="literal">deepcopy()</code> is made. Let's see what this looks like in practice. First, we will create a <code class="literal">big.matrix</code> <code class="literal">a</code>, then a new variable <code class="literal">b</code> that points to <code class="literal">a</code>.</p><div class="informalexample"><pre class="programlisting">library(bigmemory)
a &lt;- big.matrix(3, 3)
a[, ]
##      [,1] [,2] [,3]
## [1,]   NA   NA   NA
## [2,]   NA   NA   NA
## [3,]   NA   NA   NA
b &lt;- a
b[, ]
##      [,1] [,2] [,3]
## [1,]   NA   NA   NA
## [2,]   NA   NA   NA
## [3,]   NA   NA   NA</pre></div><p>Next, we will modify the contents of <code class="literal">b</code>. Under R's normal data copying rules, the data should be copied in the memory so that the contents of <code class="literal">a</code> are not modified. However, that is not the case:</p><div class="informalexample"><pre class="programlisting">b[, ] &lt;- diag(3)
b[, ]
##      [,1] [,2] [,3]
## [1,]    1    0    0
## [2,]    0    1    0
## [3,]    0    0    1
a[, ]
##      [,1] [,2] [,3]
## [1,]    1    0    0
## [2,]    0    1    0
## [3,]    0    0    1</pre></div><p>Clearly, <code class="literal">a</code> and <code class="literal">b</code> are the same object. A peek under their hoods at their pointers to the data confirms this:</p><div class="informalexample"><pre class="programlisting">a
## An object of class "big.matrix"
## Slot "address":
## &lt;pointer: 0x7fab5e2b8750&gt;
b
## An object of class "big.matrix"
## Slot "address":
## &lt;pointer: 0x7fab5e2b8750&gt;</pre></div><p>Now, let's see <a id="id299" class="indexterm"/>how this impacts the performance of parallel code. This example uses a matrix with two <a id="id300" class="indexterm"/>variables and 50 million observations and the equivalent <code class="literal">big.matrix</code>:</p><div class="informalexample"><pre class="programlisting">r &lt;- 5e7
m &lt;- matrix(rnorm(r * 2), r, 2)
bm &lt;- as.big.matrix(m)</pre></div><p>The task here is to compute the absolute difference for each pair of numbers. First, we will measure the execution time using a socket-based cluster on the matrix:</p><div class="informalexample"><pre class="programlisting">cl &lt;- makeCluster(detectCores())
part &lt;- clusterSplit(cl, seq_len(r))
system.time(res &lt;- unlist(
    parLapply(cl, part,
              function(part, data) {
                  abs(data[part, 1] - data[part, 2])
              },
              m)
))
##  user  system elapsed 
## 5.199   1.856  <span class="strong"><strong>10.590</strong></span> 
stopCluster(cl)</pre></div><p>It took 10.6 seconds on four CPU cores, and each thread consumed 1.01 GB of RAM, as shown in the following screenshot taken from Mac OS X's Activity Monitor:</p><div class="mediaobject"><img src="graphics/9263OS_08_03.jpg" alt="Shared memory versus distributed memory parallelism"/><div class="caption"><p>Memory consumption of socket-based cluster using matrix data</p></div></div><p>Now, let's use <code class="literal">big.matrix</code> to see if there is a difference in speed and memory efficiency. In order to pass <code class="literal">big.matrix</code> to each worker process, we need to use <code class="literal">describe()</code> to pass the metadata of <code class="literal">big.matrix</code> to each process. Within each process, <code class="literal">attach.big.matrix()</code> must be called to access <code class="literal">big.matrix</code>. Also notice that <code class="literal">library(bigmemory)</code> is called within the function. This is required because each worker is a <a id="id301" class="indexterm"/>new R process <a id="id302" class="indexterm"/>so any packages required to run the task must be loaded on the workers as well.</p><div class="informalexample"><pre class="programlisting">cl &lt;- makeCluster(detectCores())
system.time(res2 &lt;- unlist(
    parLapply(cl, part,
              function(part, data.desc) {
                  library(bigmemory)
                  data &lt;- attach.big.matrix(data.desc)
                  abs(data[part, 1] - data[part, 2])
              },
              describe(bm))
))
##  user  system elapsed 
## 1.278   0.692   2.956 
stopCluster(cl)</pre></div><p>This version ran must faster with a 72 percent saving in the execution time from not making copies of the matrix! Furthermore, each R process took up only about 373 MB of memory, as shown in the following figure in the <code class="literal">Private Mem</code> column. 774 MB of the memory was shared from the parent process, most of which was the <code class="literal">big.matrix</code> object.</p><div class="mediaobject"><img src="graphics/9263OS_08_04.jpg" alt="Shared memory versus distributed memory parallelism"/><div class="caption"><p>Memory consumption of forked cluster using big.matrix data.</p></div></div><p>Shared memory parallelism worked in this case because the worker processed only read from the <a id="id303" class="indexterm"/>data but did not write to it. Designing parallel algorithms that write to shared memory is <a id="id304" class="indexterm"/>much trickier and outside the scope of this book. Much care must be take to avoid <span class="strong"><strong>race conditions</strong></span>, which are conflicts and programming errors that arise when worker processes that read from and write to the same memory locations are not properly coordinated. This can lead to the data being corrupted.</p></div>
<div class="section" title="Optimizing parallel performance"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec47"/>Optimizing parallel performance</h1></div></div></div><p>Throughout the examples in this chapter, we saw various factors that affect the performance of parallel code.</p><p>One overhead in running a parallel R code is in setting up the cluster. By default, <code class="literal">makeCluster()</code> instructs the worker processes to load the <code class="literal">methods</code> package when they start. This <a id="id305" class="indexterm"/>can take a good amount of time, so if the task to be run does not require <span class="emphasis"><em>methods</em></span>, this behavior can be disabled by passing <code class="literal">methods=FALSE</code> to <code class="literal">makeCluster()</code>.</p><p>One of the biggest obstacles to parallel performance is the copying and transmission of data between the master process and the worker process. This obstacle can be large when you run parallel tasks on a cluster of computers, as many factors such as limited network bandwidth, and data encryption slow down the transmission of data even before any computations can be done. Even on a single computer, unnecessary copying of data in memory takes up precious seconds that can multiply as the data grows. This can also happen the other way around, for example in the random number generation examples, where the input data is small but the output is large.</p><p>One way to minimize these data communication overheads is to use shared memory objects, as we saw in the preceding section. Data compression can also help in some circumstances, provided the computational time to compress and decompress the data is relatively short. Another option is to store the data, including the results of any intermediate computations, at each worker node, and reserve internode data communication to only what is required to coordinate the tasks. An example of this is MapReduce from Hadoop, which we will explore in <a class="link" href="ch10.html" title="Chapter 10. R and Big Data">Chapter 10</a>, <span class="emphasis"><em>R and Big Data</em></span>.</p><p>Although there are ways to minimize the costs of data communication, sometimes these overheads far exceed the gains from parallelization, and we are better off running the code in series. It can be difficult to calculate the trade-offs between the performance gains and increased overheads of parallelizing code. When in doubt, conduct small experiments like we have done in this chapter.</p></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec48"/>Summary</h1></div></div></div><p>In this chapter, we learned about two classes of parallelism: data parallelism and task parallelism. Data parallelism is good for tasks that can be performed in parallel on partitions of a dataset. The dataset to be processed is split into partitions and each partition is processed on a different worker processes. Task parallelism, on the other hand, divides a set of similar or different tasks to amongst the worker processes. In either case, Amdahl's law states that the maximum improvement in speed that can be achieved by parallelizing code is limited by the proportion of that code that can be parallelized.</p><p>R supports both types of parallelism using the <span class="emphasis"><em>parallel</em></span> package. We learned how to implement both data parallel and task parallel algorithms using socket-based clusters and forked clusters. We also learned how to run tasks in parallel on a cluster of computers using socket-based clusters.</p><p>The examples in this chapter demonstrated that the improvement in performance by parallelizing code depends on a great variety of factors—the type of cluster, whether the task is run on a single computer or on a cluster, the volume of data exchanged between nodes, the complexity of the individual subtasks, and so on. While techniques such as shared-memory parallelism can mitigate some of the bottlenecks, parallel computing is a complex discipline that takes much experience and skill to get well executed. Used correctly, the payoffs in speed and efficiency can be significant.</p><p>For a deeper look at parallel computing in R, see <span class="emphasis"><em>Parallel R</em></span> by Q. Ethan McCallum and Stephen Weston.</p><p>In the next chapter, we will look beyond the boundaries of R to tap on the processing power of specialized data processing platforms like analytical databases.</p></div></body></html>