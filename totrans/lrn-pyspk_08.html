<html><head></head><body>
  <div id="sbo-rt-content"><div class="chapter" title="Chapter 8. TensorFrames"><div class="titlepage"><div><div><h1 class="title"><a id="ch08"/>Chapter 8. TensorFrames</h1></div></div></div><p>This chapter will provide a high-level primer on the burgeoning field of Deep Learning and the reasons why it is important. It will provide the fundamentals surrounding feature learning and neural networks required for deep learning. As well, this chapter will provide a quick start for TensorFrames for Apache Spark.</p><p>In this chapter, you will learn about:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">What is Deep Learning?</li><li class="listitem" style="list-style-type: disc">A primer on feature learning</li><li class="listitem" style="list-style-type: disc">What is feature engineering?</li><li class="listitem" style="list-style-type: disc">What is TensorFlow?</li><li class="listitem" style="list-style-type: disc">Introducing TensorFrames</li><li class="listitem" style="list-style-type: disc">TensorFrames – quick start</li></ul></div><p>As you can see in the preceding breakdown, we will be initially discussing deep learning – more specifically we will start with neural networks.</p><div class="section" title="What is Deep Learning?"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec56"/>What is Deep Learning?</h1></div></div></div><p>Deep Learning is <a id="id493" class="indexterm"/>part of a family of machine learning techniques based on <span class="emphasis"><em>learning</em></span> representations of data. Deep Learning is loosely based on our brain's own neural networks, the purpose of this structure is to provide a large number of highly interconnected elements (in biological systems, this would be the neurons in our brains); there are approximately 100 billion neurons in our brain, each connected to approximately 10,000 other neurons, resulting in a mind-boggling 1015 synaptic connections. These elements work together to solve problems through learning processes – examples include pattern recognition and data classification.</p><p>Learning within this architecture involves modifications of the connections between the interconnected elements similar to how our own brains make adjustments to the synaptic connections between neurons:</p><div class="mediaobject"><img src="images/B05793_08_01.jpg" alt="What is Deep Learning?"/></div><p>Source: <span class="emphasis"><em>Wikimedia Commons: File: Réseau de neurones.jpg</em></span>; <a class="ulink" href="https://commons.wikimedia.org/wiki/File:R%C3%A9seau_de_neurones.jpg">https://commons.wikimedia.org/wiki/File:Réseau_de_neurones.jpg</a>.</p><p>The traditional <a id="id494" class="indexterm"/>algorithmic approach involves programming known steps or quantities, that is, you already know the steps to solve a specific problem, now repeat the solution and make it run faster. Neural networks are an interesting paradigm because neural networks learn by example and are not actually programmed to perform a specific task per se. This makes the training process in neural networks (and Deep Learning) very important in that you must provide good examples for the neural network to learn from otherwise it will "learn" the wrong thing (that is, provide unpredictable results).</p><p>The most <a id="id495" class="indexterm"/>common approach to building an artificial neural network involves the creation of three layers: input, hidden, and outer; as noted in the following diagram:</p><div class="mediaobject"><img src="images/B05793_08_02.jpg" alt="What is Deep Learning?"/></div><p>Each layer is comprised of one or more nodes with connections (that is, flow of data) between each of these nodes, as noted in the preceding diagram. Input nodes are passive in that they receive data, but do not modify the information. The nodes in the hidden and output layers will actively modify the data. For example, the connections from the three nodes in the <a id="id496" class="indexterm"/>input layer to one of the nodes in the first hidden layer is noted in the following diagram:</p><div class="mediaobject"><img src="images/B05793_08_03.jpg" alt="What is Deep Learning?"/></div><p>Referring to a signal processing neural network example, each input (denoted as <span class="inlinemediaobject"><img src="images/B05793_08_22.jpg" alt="What is Deep Learning?"/></span>) has a weight applied to it (<span class="inlinemediaobject"><img src="images/B05793_08_23.jpg" alt="What is Deep Learning?"/></span>), which produces a new value. In this case, one of the hidden nodes (<span class="inlinemediaobject"><img src="images/B05793_08_24.jpg" alt="What is Deep Learning?"/></span>) is the result of three modified input nodes:</p><div class="mediaobject"><img src="images/B05793_08_04.jpg" alt="What is Deep Learning?"/></div><p>There is also a <a id="id497" class="indexterm"/>bias applied to the sum in a form of a constant that also gets adjusted during the training process. The sum (the <span class="emphasis"><em>h</em></span>
<span class="emphasis"><em>1</em></span> in our example) passes through so-called activation function that determines the output of the neuron. Some examples of such activation functions are presented in the following image:</p><div class="mediaobject"><img src="images/B05793_08_20.jpg" alt="What is Deep Learning?"/></div><p>This process is repeated for each node in the hidden layers as well as the output layer. The output node is the accumulation of all the weights applied to the input values for every active layer node. The learning process is the result of many iterations running in parallel, applying and reapplying these weights (in this scenario).</p><p>Neural networks <a id="id498" class="indexterm"/>appear in all the different sizes and shapes. The most popular are single- and multi-layer feedforward networks that resemble the one presented earlier; such structures (even with two layers and one neuron!) neuron in the output layer are capable of solving simple regression problems (such as linear and logistic) to highly complex regression and classification tasks (with many hidden layers and a number of neurons). Another type commonly used are self-organizing maps, sometimes referred to as Kohonen networks, due to Teuvo Kohonen, a Finnish researcher who first proposed such structures. The structures are trained <span class="emphasis"><em>without-a-teacher</em></span>, that is, they do not require a target (an unsupervised learning paradigm). Such structures are used most commonly to solve clustering problems where the aim is to find an underlying pattern in the data.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note54"/>Note</h3><p>For more <a id="id499" class="indexterm"/>information about neural network types, we suggest checking this document: <a class="ulink" href="http://www.ieee.cz/knihovna/Zhang/Zhang100-ch03.pdf">http://www.ieee.cz/knihovna/Zhang/Zhang100-ch03.pdf</a>
</p></div></div><p>Note that there are many other interesting deep learning libraries in addition to TensorFlow; including, but not limited, to Theano, Torch, Caffe, Microsoft Cognitive Toolkit (CNTK), mxnet, and DL4J.</p><div class="section" title="The need for neural networks and Deep Learning"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec84"/>The need for neural networks and Deep Learning</h2></div></div></div><p>There are many <a id="id500" class="indexterm"/>potential applications with neural networks (and Deep Learning). Some of the more popular ones include facial recognition, handwritten digit<a id="id501" class="indexterm"/> identification, game playing, speech recognition, language translation, and object classification. The key aspect here is that it involves learning and pattern recognition.</p><p>While neural networks have been around for a long time (at least within the context of the history of computer science), they have become more popular now because of the overarching themes: advances and availability of distributed computing and advances in research:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Advances and availability of distributed computing and hardware</strong></span>: Distributed <a id="id502" class="indexterm"/>computing frameworks such as<a id="id503" class="indexterm"/> Apache Spark allows you to complete more training iterations faster by being able to run more models in parallel to determine the optimal parameters for your machine learning models. With the prevalence of GPUs – graphic processing units that were originally designed for displaying graphics – these processors are adept at performing the resource intensive mathematical computations required for machine learning. Together with cloud computing, it becomes easier to harness the<a id="id504" class="indexterm"/> power of distributed computing <a id="id505" class="indexterm"/>and GPUs due to the lower up-front costs, minimal time to deployment, and easier to deploy elastic scale.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Advances in deep learning research</strong></span>: These hardware advances have helped return neural networks to the forefront of data sciences with projects such as TensorFlow as well as other popular ones such as Theano, Caffe, Torch, Microsoft Cognitive Toolkit (CNTK), mxnet, and DL4J.</li></ul></div><p>To dive<a id="id506" class="indexterm"/> deeper<a id="id507" class="indexterm"/> into these topics, two good references include:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>Lessons Learned from Deploying Deep Learning at Scale</em></span> (<a class="ulink" href="http://blog.algorithmia.com/deploying-deep-learning-cloud-services/">http://blog.algorithmia.com/deploying-deep-learning-cloud-services/</a>): This blog post by the folks at Algorithmia discuss their learnings on deploying deep learning solutions at scale.</li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>Neural Networks by Christos Stergio and Dimitrios Siganos</em></span> (<a class="ulink" href="http://bit.ly/2hNSWar">http://bit.ly/2hNSWar</a>): A great primer on neural networks.</li></ul></div><p>As noted previously, Deep Learning is part of a family of machine learning methods based on learning representations of data. In the case of learning representations, this can also be defined<a id="id508" class="indexterm"/> as <span class="emphasis"><em>feature learning</em></span>. What makes Deep Learning so exciting is that it has the potential to replace or minimize the need for <span class="emphasis"><em>manual</em></span> feature engineering. Deep Learning will allow the machine to not just learn a specific task, but also learn the <span class="emphasis"><em>features</em></span> needed for that task. More succinctly, automating feature engineering or teaching machines <span class="emphasis"><em>to learn how to learn</em></span> (a great reference on feature learning is Stanford's <a id="id509" class="indexterm"/>Unsupervised Feature Learning and Deep Learning tutorial: <a class="ulink" href="http://deeplearning.stanford.edu/tutorial/">http://deeplearning.stanford.edu/tutorial/</a>).</p><p>Breaking these concepts down to the<a id="id510" class="indexterm"/> fundamentals, let's start with a <span class="emphasis"><em>feature</em></span>. As observed in Christopher Bishop's <span class="emphasis"><em>Pattern Recognition and machine learning</em></span> (Berlin: Springer. ISBN 0-387-31073-8. 2006) and as noted in the previous chapters on MLlib and ML, a feature is a measurable property of the phenomenon being observed.</p><p>If you are more<a id="id511" class="indexterm"/> familiar in the domain of statistics, a <span class="emphasis"><em>feature</em></span> would be in reference to the independent variables (<span class="emphasis"><em>x<sub>1</sub>, x<sub>2</sub>, …, x<sub>n</sub></em></span>) within a stochastic linear regression model:</p><div class="mediaobject"><img src="images/B05793_08_05.jpg" alt="The need for neural networks and Deep Learning"/></div><p>In this specific example, <span class="emphasis"><em>y</em></span> is the dependent variable and <span class="emphasis"><em>x</em></span>
<span class="emphasis"><em>i</em></span> are the independent variables.</p><p>Within the<a id="id512" class="indexterm"/> context of machine learning scenarios, examples of <a id="id513" class="indexterm"/>features include:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Restaurant recommendations</strong></span>: Features include the reviews, ratings, and other content <a id="id514" class="indexterm"/>and user profile attributes related to the <a id="id515" class="indexterm"/>restaurant. A good example of this model is the <span class="emphasis"><em>Yelp Food Recommendation System</em></span>: <a class="ulink" href="http://cs229.stanford.edu/proj2013/SawantPai-YelpFoodRecommendationSystem.pdf">http://cs229.stanford.edu/proj2013/SawantPai-YelpFoodRecommendationSystem.pdf</a>).</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Handwritten Digit recognition</strong></span>: Features<a id="id516" class="indexterm"/> include block wise histograms (count of pixels along 2D directions), holes, stroke detection, and so on. Examples include:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>Handwritten Digit Classification</em></span>: <a class="ulink" href="http://ttic.uchicago.edu/~smaji/projects/digits/">http://ttic.uchicago.edu/~smaji/projects/digits/</a></li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>Recognizing Handwritten Digits and Characters</em></span>: <a class="ulink" href="http://cs231n.stanford.edu/reports/vishnu_final.pdf">http://cs231n.stanford.edu/reports/vishnu_final.pdf</a></li></ul></div></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Image Processing</strong></span>: Features <a id="id517" class="indexterm"/>include the points, edges, and objects within the image; some good examples include:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Seminar: Feature extraction by André Aichert, <a class="ulink" href="http://home.in.tum.de/~aichert/featurepres.pdf">http://home.in.tum.de/~aichert/featurepres.pdf</a></li><li class="listitem" style="list-style-type: disc">University of Washington Computer Science &amp; Engineering CSE455: Computer Vision Lecture 6, <a class="ulink" href="https://courses.cs.washington.edu/courses/cse455/09wi/Lects/lect6.pdf">https://courses.cs.washington.edu/courses/cse455/09wi/Lects/lect6.pdf</a></li></ul></div></li></ul></div><p>Feature engineering<a id="id518" class="indexterm"/> is about determining which of these features (for example, in statistics, the independent variables) are important in defining the model that you are creating. Typically, it involves the process of using domain knowledge to create the features to allow the ML models to work.</p><div class="blockquote"><blockquote class="blockquote"><p>Coming up with features is difficult, time-consuming, requires expert knowledge.</p><p>"Applied machine learning" is basically feature engineering.</p><p>—Andrew Ng, Machine Learning and AI via Brain simulations (<a class="ulink" href="http://helper.ipam.ucla.edu/publications/gss2012/gss2012_10595.pdf">http://helper.ipam.ucla.edu/publications/gss2012/gss2012_10595.pdf</a>)</p></blockquote></div></div><div class="section" title="What is feature engineering?"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec85"/>What is feature engineering?</h2></div></div></div><p>Typically, performing<a id="id519" class="indexterm"/> feature engineering involves concepts such as feature selection (selecting a subset of the original feature set) or feature extraction (building a new set of features from the original feature set):</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">In <span class="emphasis"><em>feature selection</em></span>, based <a id="id520" class="indexterm"/>on domain knowledge, you can filter the variables that you think define the model (for example, predicting football scores based on number of turnovers). Often data analysis techniques such as regression and <a id="id521" class="indexterm"/>classification can also be used to help you determine this.</li><li class="listitem" style="list-style-type: disc">In <span class="emphasis"><em>feature extraction</em></span>, the idea is to transform the data from a high dimensional space (that is, many different independent variables) to a smaller space of fewer dimensions. Continuing the football analogy, this would be the quarterback rating, which <a id="id522" class="indexterm"/>is based on several selected features (e.g. completions, touchdowns, interceptions, average gain<a id="id523" class="indexterm"/> per pass attempt, etc.). A common approach for feature extraction within the linear data transformation space is <span class="strong"><strong>principal component analysis</strong></span> (<span class="strong"><strong>PCA</strong></span>): <a class="ulink" href="http://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html#principal-component-analysis-pca">http://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html#principal-component-analysis-pca</a>. Other common mechanisms<a id="id524" class="indexterm"/> include:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>Nonlinear dimensionality reduction</em></span>: <a class="ulink" href="https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction">https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction</a></li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>Multilinear subspace learning</em></span>: <a class="ulink" href="https://en.wikipedia.org/wiki/Multilinear_subspace_learning">https://en.wikipedia.org/wiki/Multilinear_subspace_learning</a><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip34"/>Tip</h3><p>A good<a id="id525" class="indexterm"/> reference on the topic of feature selection versus feature extraction is <span class="emphasis"><em>What is dimensionality reduction?</em></span> <span class="emphasis"><em>What is the difference between feature selection and extraction?</em></span> <a class="ulink" href="http://datascience.stackexchange.com/questions/130/what-is-dimensionality-reduction-what-is-the-difference-between-feature-selecti/132#132">http://datascience.stackexchange.com/questions/130/what-is-dimensionality-reduction-what-is-the-difference-between-feature-selecti/132#132</a>
</p></div></div></li></ul></div></li></ul></div></div><div class="section" title="Bridging the data and algorithm"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec86"/>Bridging the data and algorithm</h2></div></div></div><p>Let's bridge <a id="id526" class="indexterm"/>the feature and feature engineering definitions within the context of feature selection using the example of restaurant recommendations:</p><div class="mediaobject"><img src="images/B05793_08_06.jpg" alt="Bridging the data and algorithm"/></div><p>While this is <a id="id527" class="indexterm"/>a simplified model, the analogy describes the basic premise of applied machine learning. It would be up to a data scientist to analyze the data to determine the key features of this restaurant recommendation model.</p><p>In our restaurant recommendation case, while it may be easy to presume that geolocation and cuisine type are major factors, it will require some digging into the data to understand how the user (that is, restaurant-goer) has chosen their preference for a restaurant. Different restaurants often have different characteristics or weights for the mode.</p><p>For example, the key features for high-end restaurant catering businesses are often related to location (that is, proximity to their customer's location), the ability to make reservations for large parties, and the diversity of the wine list:</p><div class="mediaobject"><img src="images/B05793_08_07.jpg" alt="Bridging the data and algorithm"/></div><p>Meanwhile, for <a id="id528" class="indexterm"/>specialty restaurants, often few of those previous factors are involved; instead, the focus is on the reviews, ratings, social media buzz, and, possibly whether the restaurant is good for kids:</p><div class="mediaobject"><img src="images/B05793_08_08.jpg" alt="Bridging the data and algorithm"/></div><p>The ability to segment these different restaurants (and their target audience) is a key facet of applied machine learning. It can be an arduous process where you try different models and algorithms with different variables and weights and then retry after iteratively training and<a id="id529" class="indexterm"/> testing many different combinations. But note how this time consuming iterative approach itself is its own process that can potentially be automated? This is the key context of building algorithms of helping machines <span class="emphasis"><em>learn to learn</em></span>: Deep Learning has the potential to automating the learning process when building our models.</p></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="What is TensorFlow?"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec57"/>What is TensorFlow?</h1></div></div></div><p>TensorFlow<a id="id530" class="indexterm"/> is a Google open source software library for numerical computation using data flow graphs; that is, an open source machine learning library focusing on Deep Learning. Based loosely on neural networks, TensorFlow is the culmination of the work of Google's Brain Team researchers and engineers to apply Deep Learning to Google products and build production models for various Google teams including (but not limited to) search, photos, and speech.</p><p>Built on C++ with a Python interface, it has quickly become one of the most popular Deep Learning projects in a short amount of time. The following screenshot denotes a Google Trends comparison between four popular deep learning libraries; note the spike around November 8th - 14th, 2015 (when TensorFlow was announced) and the rapid rise over the last year (this snapshot was taken late December 2016):</p><div class="mediaobject"><img src="images/B05793_08_09.jpg" alt="What is TensorFlow?"/></div><p>Another way to <a id="id531" class="indexterm"/>measure the popularity of TensorFlow is to note that TensorFlow is the most popular machine learning framework on GitHub per <a class="ulink" href="http://www.theverge.com/2016/4/13/11420144/google-machine-learning-tensorflow-upgrade">http://www.theverge.com/2016/4/13/11420144/google-machine-learning-tensorflow-upgrade</a>. Note that TensorFlow was only released in November 2015 and in two months it had already become the most popular forked ML GitHub repository. In the following diagram, you can review the GitHub Repositories Created in 2015 (Interactive Visualization) per <a class="ulink" href="http://donnemartin.com/viz/pages/2015">http://donnemartin.com/viz/pages/2015</a>:</p><div class="mediaobject"><img src="images/B05793_08_10.jpg" alt="What is TensorFlow?"/></div><p>As noted previously, TensorFlow performs numerical computation using data flow graphs. When <a id="id532" class="indexterm"/>thinking about graph (as per the previous chapter on GraphFrames), the node (or vertices) of this graph represent mathematical operations while the graph edges represent the multidimensional arrays (that is, tensors) that communicate between the different nodes (that is, mathematical operations). </p><p>Referring to the following diagram, <code class="literal">t</code>
<code class="literal">1</code> is a <span class="strong"><strong>2x3</strong></span> matrix while <code class="literal">t</code>
<code class="literal">2</code> is a <span class="strong"><strong>3x2</strong></span> matrix; these are the tensors (or edges of the tensor graph). The node is the mathematical operations represented as <code class="literal">op</code>
<code class="literal">1</code>:</p><div class="mediaobject"><img src="images/B05793_08_11.jpg" alt="What is TensorFlow?"/></div><p>In this example, <code class="literal">op</code>
<code class="literal">1</code> is a matrix multiplication operation represented by the following diagram, though<a id="id533" class="indexterm"/> this could be any of the many mathematics operations available in TensorFlow:</p><div class="mediaobject"><img src="images/B05793_08_12.jpg" alt="What is TensorFlow?"/></div><p>Together, to perform your numerical computations within the graph, there is a flow of multidimensional arrays (that is, tensors) between the mathematical operations (nodes) - that is, the flow of tensors, or <span class="emphasis"><em>TensorFlow</em></span>.</p><p>To better understand how TensorFlow works, let's start by installing TensorFlow within your Python <a id="id534" class="indexterm"/>environment (initially sans Spark). For the full instructions, please refer to TensorFlow | Download and Setup: <a class="ulink" href="https://www.tensorflow.org/versions/r0.12/get_started/os_setup.html">https://www.tensorflow.org/versions/r0.12/get_started/os_setup.html</a>.</p><p>For this<a id="id535" class="indexterm"/> chapter, let's focus on the Python <code class="literal">pip</code> package management system installation on Linux or Mac OS.</p><div class="section" title="Installing Pip"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec87"/>Installing Pip</h2></div></div></div><p>Ensure <a id="id536" class="indexterm"/>that you have installed <code class="literal">pip</code>; if have not, please use the following <a id="id537" class="indexterm"/>commands to install the Python package installation manager for Ubuntu/Linux:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># Ubuntu/Linux 64-bit </strong></span>
<span class="strong"><strong>$ sudo apt-get install python-pip python-dev</strong></span>
</pre></div><p>For Mac OS, you would use the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># macOS </strong></span>
<span class="strong"><strong>$ sudo easy_install pip </strong></span>
<span class="strong"><strong>$ sudo easy_install --upgrade six</strong></span>
</pre></div><p>Note, for Ubuntu/Linux, you may also want to upgrade <code class="literal">pip</code> as the pip within the Ubuntu repository is old and may not be compatible with newer packages. To do this, you can run the command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># Ubuntu/Linux pip upgrade</strong></span>
<span class="strong"><strong>$ pip install --upgrade pip </strong></span>
</pre></div></div><div class="section" title="Installing TensorFlow"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec88"/>Installing TensorFlow</h2></div></div></div><p>To install<a id="id538" class="indexterm"/> TensorFlow (with <code class="literal">pip</code> already installed), you only need to execute the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ pip install tensorflow</strong></span>
</pre></div><p>If you have a computer that has GPU support, you can <span class="emphasis"><em>instead</em></span> use the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ pip install tensorflow-gpu</strong></span>
</pre></div><p>Note that if the preceding command does not work, there are specific instructions to install TensorFlow with GPU support based on your Python version (that is, 2.7, 3.4, or 3.5) and GPU support.</p><p>For example, if I wanted to install TensorFlow on Python 2.7 with GPU enabled on Mac OS, execute the following commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># macOS, GPU enabled, Python 2.7: </strong></span>
<span class="strong"><strong>$ export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/mac/gpu/tensorflow_gpu-0.12.0rc1-py2-none-any.whl</strong></span>
<span class="strong"><strong># Python 2 </strong></span>
<span class="strong"><strong>$ sudo pip install --upgrade $TF_BINARY_URL</strong></span>
</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note55"/>Note</h3><p>Please<a id="id539" class="indexterm"/> refer to <a class="ulink" href="https://www.tensorflow.org/versions/r0.12/get_started/os_setup.html">https://www.tensorflow.org/versions/r0.12/get_started/os_setup.html</a> for<a id="id540" class="indexterm"/> the latest installation instructions.</p></div></div></div><div class="section" title="Matrix multiplication using constants"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec89"/>Matrix multiplication using constants</h2></div></div></div><p>To better <a id="id541" class="indexterm"/>describe tensors and how TensorFlow<a id="id542" class="indexterm"/> works, let's start with a matrix<a id="id543" class="indexterm"/> multiplication calculation involving two constants. As noted in the following diagram, we have <code class="literal">c</code>
<code class="literal">1</code> (<span class="strong"><strong>3x1</strong></span> matrix) and <code class="literal">c</code>
<code class="literal">2</code> (<span class="strong"><strong>1x3</strong></span> matrix), where the operation (<code class="literal">op</code>
<code class="literal">1</code>) is a matrix multiplication:</p><div class="mediaobject"><img src="images/B05793_08_13.jpg" alt="Matrix multiplication using constants"/></div><p>We <a id="id544" class="indexterm"/>will <a id="id545" class="indexterm"/>now<a id="id546" class="indexterm"/> define <code class="literal">c</code>
<code class="literal">1</code> (<span class="strong"><strong>1x3</strong></span> matrix) and <code class="literal">c</code>
<code class="literal">2</code> (<span class="strong"><strong>3x1</strong></span> matrix) using the following code:</p><div class="informalexample"><pre class="programlisting"># Import TensorFlow
import tensorflow as tf
# Setup the matrix
#   c1: 1x3 matrix
#   c2: 3x1 matrix
c1 = tf.constant([[3., 2., 1.]])
c2 = tf.constant([[-1.], [2.], [1.]])</pre></div><p>Now that<a id="id547" class="indexterm"/> we have our constants, let's<a id="id548" class="indexterm"/> run our matrix multiplication using the <a id="id549" class="indexterm"/>following code. Within the context of a TensorFlow graph, recall that the nodes in the graph are called operations (or <code class="literal">ops</code>). The following matrix multiplication is the <code class="literal">ops</code>, while the two matrices (<code class="literal">c</code>
<code class="literal">1</code>, <code class="literal">c</code>
<code class="literal">2</code>) are the tensors (typed multi-dimensional array). An <code class="literal">op</code> takes zero or more tensors as its input, performs the operation such as a mathematical calculation, with the output being zero or more tensors in the form of <code class="literal">numpy ndarray</code> objects (<a class="ulink" href="http://www.numpy.org/">http://www.numpy.org/</a>) or <code class="literal">tensorflow::Tensor interfaces</code> in C, C++:</p><div class="informalexample"><pre class="programlisting"># m3: matrix multiplication (m1 x m3)
mp = tf.matmul(c1, c2)</pre></div><p>Now that this TensorFlow graph has been established, execution of this operation (for example, in this case, the matrix multiplication) is done within the context of a <code class="literal">session</code>; the <code class="literal">session</code> places the graph <code class="literal">ops</code> into the CPU or GPU (that is, devices) to be executed:</p><div class="informalexample"><pre class="programlisting"># Launch the default graph
s = tf.Session()

# run: Execute the ops in graph
r = s.run(mp)
print(r)</pre></div><p>With the output being:</p><div class="informalexample"><pre class="programlisting"># [[ 2.]]</pre></div><p>Once you have completed your operations, you can close the session:</p><div class="informalexample"><pre class="programlisting"># Close the Session when completed
s.close()</pre></div></div><div class="section" title="Matrix multiplication using placeholders"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec90"/>Matrix multiplication using placeholders</h2></div></div></div><p>Now we <a id="id550" class="indexterm"/>will perform the same task<a id="id551" class="indexterm"/> as before, except this time, we will use <a id="id552" class="indexterm"/>tensors instead of constants. As noted in the following diagram, we will start off with two matrices (<code class="literal">m1: 3x1, m2: 1x3</code>) using the same values as in the previous section:</p><div class="mediaobject"><img src="images/B05793_08_14.jpg" alt="Matrix multiplication using placeholders"/></div><p>Within <a id="id553" class="indexterm"/>TensorFlow, we will<a id="id554" class="indexterm"/> use <code class="literal">placeholder</code> to define our two tensors as<a id="id555" class="indexterm"/> per the following code snippet:</p><div class="informalexample"><pre class="programlisting"># Setup placeholder for your model
#   t1: placeholder tensor
#   t2: placeholder tensor
t1 = tf.placeholder(tf.float32)
t2 = tf.placeholder(tf.float32)

# t3: matrix multiplication (m1 x m3)
tp = tf.matmul(t1, t2)</pre></div><p>The <a id="id556" class="indexterm"/>advantage of this approach is <a id="id557" class="indexterm"/>that, with placeholders you can use the<a id="id558" class="indexterm"/> same operations (that is, in this case, the matrix multiplication) with tensors of different sizes and shape (provided they meet the criteria of the operation). Like the operations in the previous section, let's define two matrices and execute the graph (with a simplified session execution).</p><div class="section" title="Running the model"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec09"/>Running the model</h3></div></div></div><p>The following code snippet is similar to the code snippet in the previous section, except that it now uses placeholders instead of constants:</p><div class="informalexample"><pre class="programlisting"># Define input matrices
m1 = [[3., 2., 1.]]
m2 = [[-1.], [2.], [1.]]

# Execute the graph within a session
with tf.Session() as s:
     print(s.run([tp], feed_dict={t1:m1, t2:m2}))</pre></div><p>With the output being both the value, as well as the data type:</p><div class="informalexample"><pre class="programlisting">[array([[ 2.]], dtype=float32)]</pre></div></div><div class="section" title="Running another model"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec10"/>Running another model</h3></div></div></div><p>Now that we have a graph (albeit a simple one) using <code class="literal">placeholders</code>, we can use different tensors to perform the same operation using different input matrices. As noted in the following figure, we have <code class="literal">m1</code> (4x1) and <code class="literal">m2</code> (1x4):</p><div class="mediaobject"><img src="images/B05793_08_15.jpg" alt="Running another model"/></div><p>Because <a id="id559" class="indexterm"/>we're using <code class="literal">placeholders</code>, we <a id="id560" class="indexterm"/>can easily reuse the same graph<a id="id561" class="indexterm"/> within a new session using new input:</p><div class="informalexample"><pre class="programlisting"># setup input matrices
m1 = [[3., 2., 1., 0.]]
m2 = [[-5.], [-4.], [-3.], [-2.]]
# Execute the graph within a session
with tf.Session() as s:
     print(s.run([tp], feed_dict={t1:m1, t2:m2}))</pre></div><p>With the output being:</p><div class="informalexample"><pre class="programlisting">[array([[-26.]], dtype=float32)]</pre></div></div></div><div class="section" title="Discussion"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec91"/>Discussion</h2></div></div></div><p>As noted previously, TensorFlow provides users with the ability to perform deep learning using Python libraries by representing computations as graphs where the tensors represent the data (edges of the graph) and operations represent what is to be executed (for example, mathematical computations) (vertices of the graph).</p><p>For more<a id="id562" class="indexterm"/> information, please refer to:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">TensorFlow | Get Started | Basic Usage <a class="ulink" href="https://www.tensorflow.org/get_started/get_started#basic_usage">https://www.tensorflow.org/get_started/get_started#basic_usage</a></li><li class="listitem" style="list-style-type: disc">Shannon McCormick's Neural Network and Google TensorFlow <a class="ulink" href="http://www.slideshare.net/ShannonMcCormick4/neural-networks-and-google-tensor-flow">http://www.slideshare.net/ShannonMcCormick4/neural-networks-and-google-tensor-flow</a></li></ul></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Introducing TensorFrames"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec58"/>Introducing TensorFrames</h1></div></div></div><p>At the time <a id="id563" class="indexterm"/>of writing, TensorFrames is an experimental binding for Apache Spark; it was introduced in early 2016, shortly after the release of TensorFlow. With TensorFrames, one can manipulate Spark DataFrames with TensorFlow programs. Referring to the tensor diagrams in the previous section, we have updated the figure to show how Spark DataFrames work with TensorFlow, as shown in the following diagram:</p><div class="mediaobject"><img src="images/B05793_08_16.jpg" alt="Introducing TensorFrames"/></div><p>As noted in the<a id="id564" class="indexterm"/> preceding diagram, TensorFrames provides a bridge between Spark DataFrames and TensorFlow. This allows you to take your DataFrames and apply them as input into your TensorFlow computation graph. TensorFrames also allows you to take the TensorFlow computation graph output and push it back into DataFrames so you can continue your downstream Spark processing.</p><p>In terms of common usage scenarios for TensorFrames, these typically include the following:</p><p>
<span class="strong"><strong>Utilize TensorFlow with your data</strong></span>
</p><p>The integration of TensorFlow and Apache Spark with TensorFrames allows data scientists to <a id="id565" class="indexterm"/>expand their analytics, streaming, graph, and machine learning capabilities to include Deep Learning via TensorFlow. This allows you to both train and deploy models at scale.</p><p>
<span class="strong"><strong>Parallel training to determine optimal hyperparameters</strong></span>
</p><p>When <a id="id566" class="indexterm"/>building deep learning models, there are several configuration parameters (that is, hyperparameters) that impact on how the model is trained. Common in Deep Learning/artificial neural networks are hyperparameters that define the learning rate (if the rate is high it will learn quickly, but it may not take into account highly variable input - that is, it will not learn well if the rate and variability in the data is too high) and the number of neurons in each layer of your neural network (too many neurons results in noisy estimates, while too few neurons will result in the network not learning well).</p><p>As observed in <span class="emphasis"><em>Deep Learning with Apache Spark and TensorFlow</em></span> (<a class="ulink" href="https://databricks.com/blog/2016/01/25/deep-learning-with-apache-spark-and-tensorflow.html">https://databricks.com/blog/2016/01/25/deep-learning-with-apache-spark-and-tensorflow.html</a>), using Spark with TensorFlow to help find the best set of hyperparameters for neural network training resulted in an order of magnitude reduction in training time and a 34% lower error rate for the handwritten digit recognition dataset.</p><p>For more information<a id="id567" class="indexterm"/> on Deep Learning and hyperparameters, please <a id="id568" class="indexterm"/>refer to:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>Optimizing Deep Learning Hyper-Parameters Through an Evolutionary Algorithm</em></span> <a class="ulink" href="http://ornlcda.github.io/MLHPC2015/presentations/4-Steven.pdf">http://ornlcda.github.io/MLHPC2015/presentations/4-Steven.pdf</a></li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>CS231n Convolutional Network Networks for Visual Recognition</em></span> <a class="ulink" href="http://cs231n.github.io/">http://cs231n.github.io/</a></li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>Deep Learning with Apache Spark and TensorFlow</em></span> <a class="ulink" href="https://databricks.com/blog/2016/01/25/deep-learning-with-apache-spark-and-tensorflow.html">https://databricks.com/blog/2016/01/25/deep-learning-with-apache-spark-and-tensorflow.html</a></li></ul></div><p>At the time of writing, TensorFrames is officially supported as of Apache Spark 1.6 (Scala 2.10), though most contributions are currently focused on Spark 2.0 (Scala 2.11). The easiest way to <a id="id569" class="indexterm"/>use TensorFrames is to access it via Spark Packages (<a class="ulink" href="https://spark-packages.org">https://spark-packages.org</a>).</p></div></div>


  <div id="sbo-rt-content"><div class="section" title="TensorFrames – quick start"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec59"/>TensorFrames – quick start</h1></div></div></div><p>After all this<a id="id570" class="indexterm"/> preamble, let's jump start our use of TensorFrames with this quick start tutorial. You can download and use the full notebook within <a id="id571" class="indexterm"/>Databricks Community Edition at <a class="ulink" href="http://bit.ly/2hwGyuC">http://bit.ly/2hwGyuC</a>.</p><p>You can also run this from the PySpark shell (or other Spark environments), like any other Spark package:</p><div class="informalexample"><pre class="programlisting"># The version we're using in this notebook
$SPARK_HOME/bin/pyspark --packages tjhunter:tensorframes:0.2.2-s_2.10  

# Or use the latest version 
$SPARK_HOME/bin/pyspark --packages databricks:tensorframes:0.2.3-s_2.10</pre></div><p>Note, you will only use one of the above commands (that is, not both). For more information, please <a id="id572" class="indexterm"/>refer to the <code class="literal">databricks/tensorframes</code> GitHub repository (<a class="ulink" href="https://github.com/databricks/tensorframes">https://github.com/databricks/tensorframes</a>).</p><div class="section" title="Configuration and setup"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec92"/>Configuration and setup</h2></div></div></div><p>Please follow<a id="id573" class="indexterm"/> the configuration and setup steps in the <a id="id574" class="indexterm"/>following order:</p><div class="section" title="Launching a Spark cluster"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec11"/>Launching a Spark cluster</h3></div></div></div><p>Launch a <a id="id575" class="indexterm"/>Spark cluster using Spark 1.6 (Hadoop 1) and Scala 2.10. This has been tested with Spark 1.6, Spark 1.6.2, and Spark 1.6.3 (Hadoop 1) on Databricks Community Edition (<a class="ulink" href="http://databricks.com/try-databricks">http://databricks.com/try-databricks</a>).</p></div><div class="section" title="Creating a TensorFrames library"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec12"/>Creating a TensorFrames library</h3></div></div></div><p>Create a<a id="id576" class="indexterm"/> library to attach TensorFrames 0.2.2 to your cluster: <code class="literal">tensorframes-0.2.2-s_2.10</code>. Refer to <a class="link" href="ch07.html" title="Chapter 7. GraphFrames">Chapter 7</a>, <span class="emphasis"><em>GraphFrames</em></span> to recall how to create a library.</p></div><div class="section" title="Installing TensorFlow on your cluster"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec13"/>Installing TensorFlow on your cluster</h3></div></div></div><p>In a <a id="id577" class="indexterm"/>notebook, run one of the following commands to install TensorFlow. This has been tested with TensorFlow 0.9 CPU edition:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">TensorFlow 0.9, Ubuntu/Linux 64-bit, CPU only, Python 2.7:<div class="informalexample"><pre class="programlisting">/databricks/python/bin/pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.9.0rc0-cp27-none-linux_x86_64.whl</pre></div></li><li class="listitem" style="list-style-type: disc">TensorFlow 0.9, Ubuntu/Linux 64-bit, GPU enabled, Python 2.7:<div class="informalexample"><pre class="programlisting">/databricks/python/bin/pip install https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.9.0rc0-cp27-none-linux_x86_64.whl</pre></div></li></ul></div><p>The following is the <code class="literal">pip</code> install command that will install TensorFlow on to the Apache Spark driver:</p><div class="informalexample"><pre class="programlisting">%sh
/databricks/python/bin/pip install https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.9.0rc0-cp27-none-linux_x86_64.whl</pre></div><p>A successful installation should have something similar to the following output:</p><div class="informalexample"><pre class="programlisting">Collecting tensorflow==0.9.0rc0 from https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.9.0rc0-cp27-none-linux_x86_64.whl Downloading https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-0.9.0rc0-cp27-none-linux_x86_64.whl (27.6MB) Requirement already satisfied (use --upgrade to upgrade): numpy&gt;=1.8.2 in /databricks/python/lib/python2.7/site-packages (from tensorflow==0.9.0rc0) Requirement already satisfied (use --upgrade to upgrade): six&gt;=1.10.0 in /usr/lib/python2.7/dist-packages (from tensorflow==0.9.0rc0) Collecting protobuf==3.0.0b2 (from tensorflow==0.9.0rc0) Downloading protobuf-3.0.0b2-py2.py3-none-any.whl (326kB) Requirement already satisfied (use --upgrade to upgrade): wheel in /databricks/python/lib/python2.7/site-packages (from tensorflow==0.9.0rc0) Requirement already satisfied (use --upgrade to upgrade): setuptools in /databricks/python/lib/python2.7/site-packages (from protobuf==3.0.0b2-&gt;tensorflow==0.9.0rc0) Installing collected packages: protobuf, tensorflow Successfully installed protobuf-3.0.0b2 tensorflow-0.9.0rc0</pre></div><p>Upon successful installation of TensorFlow, detach and reattach the notebook where you just ran this command. Your cluster is now configured; you can run pure TensorFlow programs on the driver, or TensorFrames examples on the whole cluster.</p></div></div><div class="section" title="Using TensorFlow to add a constant to an existing column"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec93"/>Using TensorFlow to add a constant to an existing column</h2></div></div></div><p>This is a <a id="id578" class="indexterm"/>simple TensorFrames program where <a id="id579" class="indexterm"/>the <code class="literal">op</code> is to perform a simple addition. Note that the original source code can be found in the <code class="literal">databricks/tensorframes</code> GitHub repository. This is in reference to the TensorFrames <code class="literal">Readme.md</code> | <span class="emphasis"><em>How to Run in Python</em></span> section (<a class="ulink" href="https://github.com/databricks/tensorframes#how-to-run-in-python">https://github.com/databricks/tensorframes#how-to-run-in-python</a>).</p><p>The first thing <a id="id580" class="indexterm"/>we will do is import <code class="literal">TensorFlow</code>, <code class="literal">TensorFrames</code>, and <code class="literal">pyspark.sql.row</code> to create a DataFrame based on an RDD of floats:</p><div class="informalexample"><pre class="programlisting"># Import TensorFlow, TensorFrames, and Row
import tensorflow as tf
import tensorframes as tfs
from pyspark.sql import Row

# Create RDD of floats and convert into DataFrame `df`
rdd = [Row(x=float(x)) for x in range(10)]
df = sqlContext.createDataFrame(rdd)</pre></div><p>To view the <code class="literal">df</code> DataFrame generated by the RDD of floats, we can use the <code class="literal">show</code> command:</p><div class="informalexample"><pre class="programlisting">df.show()</pre></div><p>This produces the following result:</p><div class="mediaobject"><img src="images/B05793_08_17.jpg" alt="Using TensorFlow to add a constant to an existing column"/></div><div class="section" title="Executing the Tensor graph"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec14"/>Executing the Tensor graph</h3></div></div></div><p>As noted <a id="id581" class="indexterm"/>previously, this tensor graph consists of adding 3 to the tensor created by the <code class="literal">df</code> DataFrame generated by the RDD of floats. We will now execute the following code snippet: </p><div class="informalexample"><pre class="programlisting"># Run TensorFlow program executes:
#   The 'op' performs the addition (i.e. 'x' + '3')
#   Place the data back into a DataFrame
with tf.Graph().as_default() as g:

#   The placeholder that corresponds to column 'x'.
#   The shape of the placeholder is automatically
#   inferred from the DataFrame.
    x = tfs.block(df, "x")
    
    # The output that adds 3 to x
    z = tf.add(x, 3, name='z')
    
    # The resulting `df2` DataFrame
    df2 = tfs.map_blocks(z, df)

# Note that 'z' is the tensor output from the
# 'tf.add' operation
print z

## Output
Tensor("z:0", shape=(?,), dtype=float64)</pre></div><p>Here are<a id="id582" class="indexterm"/> some specific call outs for the preceding code snippet:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">x</code> utilizes <code class="literal">tfs.block</code>, where the <code class="literal">block</code> builds a block placeholder based on the content of a column in a DataFrame</li><li class="listitem" style="list-style-type: disc"><code class="literal">z</code> is the output tensor from the TensorFlow <code class="literal">add</code> method (<code class="literal">tf.add</code>)</li><li class="listitem" style="list-style-type: disc"><code class="literal">df2</code> is the new DataFrame, which adds an extra column to the <code class="literal">df</code> DataFrame with the <code class="literal">z</code> tensor block by block</li></ul></div><p>While <code class="literal">z</code> is the tensor (as noted in the preceding output), for us to work with the results of the TensorFlow program, we will utilize the <code class="literal">df2</code> dataframe. The output from <code class="literal">df2.show()</code> is as follows:</p><div class="mediaobject"><img src="images/B05793_08_18.jpg" alt="Executing the Tensor graph"/></div></div></div><div class="section" title="Blockwise reducing operations example"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec94"/>Blockwise reducing operations example</h2></div></div></div><p>In this next<a id="id583" class="indexterm"/> section, we will show how to work <a id="id584" class="indexterm"/>with blockwise reducing operations. Specifically, we will compute the sum and min of field vectors, working with blocks of rows for more efficient processing.</p><div class="section" title="Building a DataFrame of vectors"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec15"/>Building a DataFrame of vectors</h3></div></div></div><p>First, we will <a id="id585" class="indexterm"/>create a one-column DataFrame of vectors:</p><div class="informalexample"><pre class="programlisting"># Build a DataFrame of vectors
data = [Row(y=[float(y), float(-y)]) for y in range(10)]
df = sqlContext.createDataFrame(data)
df.show()</pre></div><p>The output is as follows:</p><div class="mediaobject"><img src="images/B05793_08_19.jpg" alt="Building a DataFrame of vectors"/></div></div><div class="section" title="Analysing the DataFrame"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec16"/>Analysing the DataFrame</h3></div></div></div><p>We need to <a id="id586" class="indexterm"/>analyze the DataFrame to determine what its shape is (that is, dimensions of the vectors). For example, in the following snippet, we use the <code class="literal">tfs.print_schema</code> command for the <code class="literal">df</code> DataFrame:</p><div class="informalexample"><pre class="programlisting"># Print the information gathered by TensorFlow to check the content of the DataFrame
tfs.print_schema(df)

## Output
root 
|-- y: array (nullable = true) double[?,?]</pre></div><p>Notice the <code class="literal">double[?,?]</code>, meaning that TensorFlow does not know the dimensions of the vectors:</p><div class="informalexample"><pre class="programlisting"># Because the dataframe contains vectors, we need to analyze it
# first to find the dimensions of the vectors.
df2 = tfs.analyze(df)

# The information gathered by TF can be printed 
# to check the content:
tfs.print_schema(df2)

## Output
root 
|-- y: array (nullable = true) double[?,2] </pre></div><p>Upon analysis of the <code class="literal">df2 </code>DataFrame, TensorFlow has inferred that <code class="literal">y</code> contains vectors of size 2. For small tensors (scalars and vectors), TensorFrames usually infers the shapes of the tensors without requiring a preliminary analysis. If it cannot do so, an error message will indicate that you need to run the DataFrame through <code class="literal">tfs.analyze()</code> first.</p></div><div class="section" title="Computing elementwise sum and min of all vectors"><div class="titlepage"><div><div><h3 class="title"><a id="ch08lvl3sec17"/>Computing elementwise sum and min of all vectors</h3></div></div></div><p>Now, let's<a id="id587" class="indexterm"/> analyze<a id="id588" class="indexterm"/> the <code class="literal">df</code> DataFrame to compute the <code class="literal">sum</code> and the elementwise <code class="literal">min</code> of all the vectors using <code class="literal">tf.reduce_sum</code> and <code class="literal">tf.reduce_min</code>:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">tf.reduce_sum</code>: Computes<a id="id589" class="indexterm"/> the sum of elements across dimensions of a tensor, for example, if <code class="literal">x = [[3, 2, 1], [-1, 2, 1]]</code> then <code class="literal">tf.reduce_sum(x) ==&gt; 8</code>. More information can be found<a id="id590" class="indexterm"/> at: <code class="literal">https://www.tensorflow.org/api_docs/python/tf/reduce_sum</code>.</li><li class="listitem" style="list-style-type: disc"><code class="literal">tf.reduce_min</code>: Computes<a id="id591" class="indexterm"/> the minimum of elements across dimensions of a tensor, for example, if <code class="literal">x = [[3, 2, 1], [-1, 2, 1]]</code> then <code class="literal">tf.reduce_min(x) ==&gt; -1</code>. More <a id="id592" class="indexterm"/>information can be found at: <code class="literal">https://www.tensorflow.org/api_docs/python/tf/reduce_min</code>.</li></ul></div><p>The following <a id="id593" class="indexterm"/>code<a id="id594" class="indexterm"/> snippet allows us to perform efficient elementwise reductions using TensorFlow, where the source data is within a DataFrame:</p><div class="informalexample"><pre class="programlisting"># Note: First, let's make a copy of the 'y' column. 
# This is an inexpensive operation in Spark 2.0+
df3 = df2.select(df2.y, df2.y.alias("z"))

# Execute the Tensor Graph
with tf.Graph().as_default() as g:

  # The placeholders. 
  # Note the special name that end with '_input':
    y_input = tfs.block(df3, 'y', tf_name="y_input")
    z_input = tfs.block(df3, 'z', tf_name="z_input")
    
    # Perform elementwise sum and minimum 
    y = tf.reduce_sum(y_input, [0], name='y')
    z = tf.reduce_min(z_input, [0], name='z')
    
# The resulting dataframe
(data_sum, data_min) = tfs.reduce_blocks([y, z], df3)

# The finalresults are numpy arrays:
print "Elementwise sum: %s and minimum: %s " % (data_sum, data_min)

## Output
Elementwise sum: [ 45. -45.] and minimum: [ 0. -9.] </pre></div><p>With a few lines of TensorFlow code with TensorFrames, we can take the data stored within the <code class="literal">df</code> DataFrame and execute a Tensor Graph to perform element wise sum and min, merge<a id="id595" class="indexterm"/> the <a id="id596" class="indexterm"/>data back into a DataFrame, and (in our case) print out the final values.</p></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec60"/>Summary</h1></div></div></div><p>In this chapter, we have reviewed the fundamentals of neural networks and Deep Learning, including the components of feature engineering. With all this new excitement in Deep Learning, we introduced TensorFlow and how it can work closely together with Apache Spark through TensorFrames.</p><p>TensorFrames is a powerful deep learning tool that allows data scientists and engineers to work with TensorFlow with data stored in Spark DataFrames. This allows you to expand the capabilities of Apache Spark to a powerful deep learning toolset that is based on the learning process of neural networks. To help continue your Deep Learning journey, the following are some great TensorFlow and TensorFrames resources:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">TensorFlow: <a class="ulink" href="https://www.tensorflow.org/">https://www.tensorflow.org/</a></li><li class="listitem" style="list-style-type: disc">TensorFlow | Get Started: <a class="ulink" href="https://www.tensorflow.org/get_started/get_started">https://www.tensorflow.org/get_started/get_started</a></li><li class="listitem" style="list-style-type: disc">TensorFlow | Guides: <a class="ulink" href="https://www.tensorflow.org/tutorials/">https://www.tensorflow.org/tutorials/</a></li><li class="listitem" style="list-style-type: disc">Deep Learning on Databricks: <a class="ulink" href="https://databricks.com/blog/2016/12/21/deep-learning-on-databricks.html">https://databricks.com/blog/2016/12/21/deep-learning-on-databricks.html</a></li><li class="listitem" style="list-style-type: disc">TensorFrames (GitHub): <a class="ulink" href="https://github.com/databricks/tensorframes">https://github.com/databricks/tensorframes</a></li><li class="listitem" style="list-style-type: disc">TensorFrames User Guide: <a class="ulink" href="https://github.com/databricks/tensorframes/wiki/TensorFrames-user-guide">https://github.com/databricks/tensorframes/wiki/TensorFrames-user-guide</a></li></ul></div></div></div>
</body></html>