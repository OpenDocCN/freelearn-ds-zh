["```py\nval sc: SparkContext\nval sqlContext = new org.apache.spark.sql.SQLContext(sc)\n```", "```py\n    $ spark-shell --driver-memory 1G\n\n    ```", "```py\n    scala> val hc = new org.apache.spark.sql.hive.HiveContext(sc)\n\n    ```", "```py\n    scala>  hc.sql(\"create table if not exists person(first_name string, last_name string, age int) row format delimited fields terminated by ','\")\n\n    ```", "```py\n    $ mkdir person\n    $ echo \"Barack,Obama,53\" >> person/person.txt\n    $ echo \"George,Bush,68\" >> person/person.txt\n    $ echo \"Bill,Clinton,68\" >> person/person.txt\n\n    ```", "```py\n    scala> hc.sql(\"load data local inpath \\\"/home/hduser/person\\\" into table person\")\n\n    ```", "```py\n    scala> hc.sql(\"load data inpath \\\"/user/hduser/person\\\" into table person\")\n\n    ```", "```py\n    scala> val persons = hc.sql(\"from person select first_name,last_name,age\")\n    scala> persons.collect.foreach(println)\n\n    ```", "```py\n    scala> hc.sql(\"create table person2 as select first_name, last_name from person;\")\n\n    ```", "```py\n    scala> hc.sql(\"create table person2 like person location '/user/hive/warehouse/person'\")\n\n    ```", "```py\n    scala> hc.sql(\"create table people_by_last_name(last_name string,count int)\")\n    scala> hc.sql(\"create table people_by_age(age int,count int)\")\n\n    ```", "```py\n    scala> hc.sql(\"\"\"from person\n     insert overwrite table people_by_last_name\n     select last_name, count(distinct first_name)\n     group by last_name\n    insert overwrite table people_by_age\n     select age, count(distinct first_name)\n     group by age; \"\"\")\n\n    ```", "```py\n    $ spark-shell --driver-memory 1G\n\n    ```", "```py\n    scala> import sqlContext.implicits._\n\n    ```", "```py\n    scala> case class Person(first_name:String,last_name:String,age:Int)\n\n    ```", "```py\n    $ mkdir person\n    $ echo \"Barack,Obama,53\" >> person/person.txt\n    $ echo \"George,Bush,68\" >> person/person.txt\n    $ echo \"Bill,Clinton,68\" >> person/person.txt\n    $ hdfs dfs -put person person\n\n    ```", "```py\n    scala> val p = sc.textFile(\"hdfs://localhost:9000/user/hduser/person\")\n\n    ```", "```py\n    val pmap = p.map( line => line.split(\",\"))\n\n    ```", "```py\n    scala> val personRDD = pmap.map( p => Person(p(0),p(1),p(2).toInt))\n\n    ```", "```py\n    scala> val personDF = personRDD.toDF\n\n    ```", "```py\n    scala> personDF.registerTempTable(\"person\")\n\n    ```", "```py\n    scala> val people = sql(\"select * from person\")\n\n    ```", "```py\n    scala> people.collect.foreach(println)\n\n    ```", "```py\n    $ spark-shell --driver-memory 1G\n\n    ```", "```py\n    scala> import sqlContext.implicit._\n\n    ```", "```py\n    scala> import org.apache.spark.sql._\n    scala> import org.apache.spark.sql.types._\n\n    ```", "```py\n    $ mkdir person\n    $ echo \"Barack,Obama,53\" >> person/person.txt\n    $ echo \"George,Bush,68\" >> person/person.txt\n    $ echo \"Bill,Clinton,68\" >> person/person.txt\n    $ hdfs dfs -put person person\n\n    ```", "```py\n    scala> val p = sc.textFile(\"hdfs://localhost:9000/user/hduser/person\")\n\n    ```", "```py\n    scala> val pmap = p.map( line => line.split(\",\"))\n\n    ```", "```py\n    scala> val personData = pmap.map( p => Row(p(0),p(1),p(2).toInt))\n\n    ```", "```py\n    scala> val schema = StructType(\n     Array(StructField(\"first_name\",StringType,true),\n    StructField(\"last_name\",StringType,true),\n    StructField(\"age\",IntegerType,true)\n    ))\n\n    ```", "```py\n    scala> val personDF = sqlContext.createDataFrame(personData,schema)\n\n    ```", "```py\n    scala> personDF.registerTempTable(\"person\")\n\n    ```", "```py\n    scala> val persons = sql(\"select * from person\")\n\n    ```", "```py\n    scala> persons.collect.foreach(println)\n\n    ```", "```py\nStructType(fields: Array[StructField])\n```", "```py\nStructField(name: String, dataType: DataType, nullable: Boolean = true, metadata: Metadata = Metadata.empty)\n```", "```py\n    $ mkdir person\n    $ echo \"Barack,Obama,53\" >> person/person.txt\n    $ echo \"George,Bush,68\" >> person/person.txt\n    $ echo \"Bill,Clinton,68\" >> person/person.txt\n\n    ```", "```py\n    $ hdfs dfs -put person /user/hduser/person\n\n    ```", "```py\n    $ spark-shell --driver-memory 1G\n\n    ```", "```py\n    scala> import sqlContext.implicits._\n\n    ```", "```py\n    scala> case class Person(firstName: String, lastName: String, age:Int)\n\n    ```", "```py\n    scala> val personRDD = sc.textFile(\"hdfs://localhost:9000/user/hduser/person\").map(_.split(\"\\t\")).map(p => Person(p(0),p(1),p(2).toInt))\n\n    ```", "```py\n    scala> val person = personRDD.toDF\n\n    ```", "```py\n    scala> person.registerTempTable(\"person\")\n\n    ```", "```py\n    scala> val sixtyPlus = sql(\"select * from person where age > 60\")\n\n    ```", "```py\n    scala> sixtyPlus.collect.foreach(println)\n\n    ```", "```py\n    scala> sixtyPlus.saveAsParquetFile(\"hdfs://localhost:9000/user/hduser/sp.parquet\")\n\n    ```", "```py\n    $ hdfs dfs -ls sp.parquet\n\n    ```", "```py\n    scala> val parquetDF = sqlContext.load(\"hdfs://localhost:9000/user/hduser/sp.parquet\")\n\n    ```", "```py\n    scala> \n    parquetDF\n    .registerTempTable(\"sixty_plus\")\n\n    ```", "```py\n    scala> sql(\"select * from sixty_plus\")\n\n    ```", "```py\n    hive> create table person_parquet like person stored as parquet\n\n    ```", "```py\n    hive> insert overwrite table person_parquet select * from person;\n\n    ```", "```py\nscala> sqlContext.setConf(\"spark.sql.parquet.binaryAsString\",\"true\")\n\n```", "```py\nscala>sixtyPlus.write.parquet(\"hdfs://localhost:9000/user/hduser/sp.parquet\")\n\n```", "```py\nscala>val parquetDF = sqlContext.read.parquet(\"hdfs://localhost:9000/user/hduser/sp.parquet\")\n\n```", "```py\n    \"firstName\" : \"Bill\"\n    ```", "```py\n    { \"firstName\" : \"Bill\", \"lastName\": \"Clinton\", \"age\": 68 }\n    ```", "```py\n    [{ \"firstName\" : \"Bill\", \"lastName\": \"Clinton\", \"age\": 68 },{\"firstName\": \"Barack\",\"lastName\": \"Obama\", \"age\": 43}]\n    ```", "```py\n    $ mkdir jsondata\n    $ vi jsondata/person.json\n    {\"first_name\" : \"Barack\", \"last_name\" : \"Obama\", \"age\" : 53}\n    {\"first_name\" : \"George\", \"last_name\" : \"Bush\", \"age\" : 68 }\n    {\"first_name\" : \"Bill\", \"last_name\" : \"Clinton\", \"age\" : 68 }\n\n    ```", "```py\n    $ hdfs dfs -put jsondata /user/hduser/jsondata\n\n    ```", "```py\n    $ spark-shell --driver-memory 1G\n\n    ```", "```py\n    scala> val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n\n    ```", "```py\n    scala> import sqlContext.implicits._\n\n    ```", "```py\n    scala> val person = sqlContext.jsonFile(\"hdfs://localhost:9000/user/hduser/jsondata\")\n\n    ```", "```py\n    scala> person.registerTempTable(\"person\")\n\n    ```", "```py\n    scala> val sixtyPlus = sql(\"select * from person where age > 60\")\n\n    ```", "```py\n    scala> sixtyPlus.collect.foreach(println)\n    ```", "```py\n    scala> sixtyPlus.toJSON.saveAsTextFile(\"hdfs://localhost:9000/user/hduser/sp\")\n\n    ```", "```py\n    $ hdfs dfs -ls sp\n\n    ```", "```py\n[{\"firstName\":\"Barack\", \"lastName\":\"Obama\"},{\"firstName\":\"Bill\", \"lastName\":\"Clinton\"}]\n```", "```py\nscala> val person = sqlContext.read.json (\"hdfs://localhost:9000/user/hduser/jsondata\")\n\n```", "```py\n    CREATE TABLE 'person' (\n      'person_id' int(11) NOT NULL AUTO_INCREMENT,\n      'first_name' varchar(30) DEFAULT NULL,\n      'last_name' varchar(30) DEFAULT NULL,\n      'gender' char(1) DEFAULT NULL,\n      'age' tinyint(4) DEFAULT NULL,\n      PRIMARY KEY ('person_id')\n    )\n    ```", "```py\n    Insert into person values('Barack','Obama','M',53);\n    Insert into person values('Bill','Clinton','M',71);\n    Insert into person values('Hillary','Clinton','F',68);\n    Insert into person values('Bill','Gates','M',69);\n    Insert into person values('Michelle','Obama','F',51);\n    ```", "```py\n    $ spark-shell --driver-class-path/path-to-mysql-jar/mysql-connector-java-5.1.34-bin.jar\n\n    ```", "```py\n    scala> val url=\"jdbc:mysql://localhost:3306/hadoopdb\"\n\n    ```", "```py\n    scala> val prop = new java.util.Properties\n    scala> prop.setProperty(\"user\",\"hduser\")\n    scala> prop.setProperty(\"password\",\"********\")\n\n    ```", "```py\n     scala> val people = sqlContext.read.jdbc(url,\"person\",prop)\n\n    ```", "```py\n    scala> people.show\n\n    ```", "```py\n    scala> val males = sqlContext.read.jdbc(url,\"person\",Array(\"gender='M'\"),prop)\n    scala> males.show\n\n    ```", "```py\n    scala> val first_names = people.select(\"first_name\")\n    scala> first_names.show\n\n    ```", "```py\n    scala> val below60 = people.filter(people(\"age\") < 60)\n    scala> below60.show\n\n    ```", "```py\n    scala> val grouped = people.groupBy(\"gender\")\n\n    ```", "```py\n    scala> val gender_count = grouped.count\n    scala> gender_count.show\n\n    ```", "```py\n    scala> val avg_age = grouped.avg(\"age\")\n    scala> avg_age.show\n\n    ```", "```py\n    scala> gender_count.write.jdbc(url,\"gender_count\",prop)\n\n    ```", "```py\n    scala> people.write.parquet(\"people.parquet\")\n\n    ```", "```py\n    scala> people.write.json(\"people.json\")\n\n    ```", "```py\n    $ spark-shell --driver-memory 1G\n\n    ```", "```py\n    scala> val people = sqlContext.read.load(\"hdfs://localhost:9000/user/hduser/people.parquet\") \n\n    ```", "```py\n    scala> val people = sqlContext.read.format(\"org.apache.spark.sql.parquet\").load(\"hdfs://localhost:9000/user/hduser/people.parquet\") \n\n    ```", "```py\n    scala> val people = sqlContext.read.format(\"parquet\").load(\"hdfs://localhost:9000/user/hduser/people.parquet\") \n\n    ```", "```py\n    scala> val people = people.write.format(\"json\").mode(\"append\").save (\"hdfs://localhost:9000/user/hduser/people.json\") \n\n    ```"]