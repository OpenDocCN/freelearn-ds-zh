- en: Chapter 5. Putting Data in its Place – Classification Methods and Analysis
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第五章：将数据放在合适的位置——分类方法和分析
- en: In the previous chapter, we explored methods for analyzing data whose outcome
    is a continuous variable, such as the purchase volume for a customer account or
    the expected number of days until cancellation of a subscription service. However,
    many of the outcomes for data in business analyses are discrete—they may only
    take a limited number of values. For example, a movie review can be 1–5 stars
    (but only integers), a customer can cancel or renew a subscription, or an online
    advertisement can be clicked or ignored.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们探讨了分析结果为连续变量的数据的方法，例如客户账户的购买量或订阅服务取消前预期的天数。然而，商业分析中的许多结果都是离散的——它们可能只取有限数量的值。例如，电影评论可以是1到5星（但只有整数），客户可以取消或续订订阅，或者在线广告可以被点击或忽略。
- en: 'The methods used to model and predict outcomes for such data are similar to
    the regression models we covered in the previous chapter. Moreover, sometimes
    we might want to convert a regression problem into a classification problem: for
    instance, rather than predicting customer spending patterns in a month, we might
    be more interested in whether it is above a certain threshold that is meaningful
    from a business perspective, and assign values in our training data as 0 (below
    the threshold) and 1 (above) depending upon this cutoff. In some scenarios, this
    might increase the noise in our classification: imagine if many customers'' personal
    expenditures were right near the threshold we set for this model, making it very
    hard to learn an accurate model. In other cases, making the outcome discrete will
    help us hone in on the question we are interested in answering. Imagine the customer
    expenditure data is well separated above and below our threshold, but that there
    is wide variation in values above the cutoff. In this scenario, a regression model
    would try to minimize overall error in the model by fitting the trends in larger
    data points that disproportionately influence the total value of the error, rather
    than achieving our actual goal of identifying high- and low- spending customers.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 用于建模和预测此类数据的方法与我们上一章中讨论的回归模型类似。此外，有时我们可能希望将回归问题转换为分类问题：例如，我们可能更感兴趣的是预测客户在一个月内的消费模式是否超过了一个从商业角度来看有意义的阈值，并在我们的训练数据中将值分配为0（低于阈值）和1（高于），根据这个截止点。在某些情况下，这可能会增加我们分类中的噪声：想象一下，如果许多客户的个人支出接近我们为该模型设置的阈值，这将使得学习一个准确模型变得非常困难。在其他情况下，使结果离散将帮助我们聚焦于我们感兴趣回答的问题。想象一下，如果客户支出数据在阈值上下都很好地分离，但在截止点以上存在广泛的数值变化。在这种情况下，回归模型会尝试通过拟合较大数据点的趋势来最小化模型的整体误差，这些数据点不成比例地影响总误差值，而不是实现我们实际的目标，即识别高消费和低消费客户。
- en: In addition to these considerations, some data is inherently not modeled effectively
    by regression analyses. For instance, consider the scenario in which we are trying
    to predict which ad out of a set of five a customer is most likely to click. We
    could encode these ads with the numerical values ranging from 1 to 5, but they
    do not have a natural ordering that would make sense in a regression problem—2
    is not greater than 1, it is simply a label denoting which of the five categories
    an ad belongs to. In this scenario, it will make more sense to encode the labels
    of the dataset as a vector of length `5` and place a `1` in the column corresponding
    to the ad, which will make all labels equivalent from the perspective of the algorithm.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些考虑因素之外，一些数据本身就不适合通过回归分析进行有效建模。例如，考虑这样一个场景：我们试图预测在五则广告中，客户最有可能点击哪一则。我们可以用1到5的数值来编码这些广告，但它们没有自然的顺序，这在回归问题中是没有意义的——2并不比1大，它仅仅是一个标签，表示广告属于五个类别中的哪一个。在这种情况下，将数据集的标签编码为一个长度为`5`的向量，并在对应广告的列中放置一个`1`，从算法的角度来看，这将使所有标签都等价。
- en: 'With these points in mind, in the following exercises, we will cover:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些点，在接下来的练习中，我们将涵盖以下内容：
- en: Encoding data responses as categorical outcomes
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据响应编码为分类结果
- en: Building classification models with both balanced and skewed data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用平衡和倾斜数据构建分类模型
- en: Evaluating the accuracy of classification models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估分类模型的准确性
- en: Assessing the benefits and shortcomings of different classification methods
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估不同分类方法的优缺点
- en: Logistic regression
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: 'We will start our exploration of classifier algorithms with one of the most
    commonly used classification models: logistic regression. Logistic regression
    is similar to the linear regression method discussed in [Chapter 4](ch04.html
    "Chapter 4. Connecting the Dots with Models – Regression Methods"), *Connecting
    the Dots with Models – Regression Methods*, with the major difference being that
    instead of directly computing a linear combination of the inputs, it compresses
    the output of a linear model through a function that constrains outputs to be
    in the range `[0,1]`. As we will see, this is in fact a kind of "generalized linear
    model that we discussed in the last [Chapter 4](ch04.html "Chapter 4. Connecting
    the Dots with Models – Regression Methods"), *Connecting the Dots with Models
    – Regression Methods*, recall that in linear regression, the predicted output
    is given by:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开始探索分类算法，从最常用的分类模型之一：逻辑回归。逻辑回归类似于第 4 章[连接点与模型 - 回归方法](ch04.html "第 4 章. 连接点与模型
    – 回归方法")中讨论的线性回归方法，主要区别在于它不是直接计算输入的线性组合，而是通过一个函数压缩线性模型的输出，使得输出被限制在 `[0,1]` 范围内。我们将看到，这实际上是一种“我们上一次在第
    4 章[连接点与模型 – 回归方法](ch04.html "第 4 章. 连接点与模型 – 回归方法")中讨论的广义线性模型”，回想一下，在线性回归中，预测输出由以下公式给出：
- en: '![Logistic regression](img/B04881_05_01.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归](img/B04881_05_01.jpg)'
- en: 'where `Y` is the response variable for all `n` members of a dataset, `X` is
    an `n` by `m` matrix of `m` features for each of the n rows of data, and `βT`
    is a column vector of `m` coefficients (Recall that the `T` operator represents
    the transpose of a vector or matrix. Here we transpose the coefficients so they
    are of dimension `mx1`, so that we can form a product with the matrix `X` with
    is `nxm`), which gives the change in the response expected for a 1-unit change
    in a particular feature. Thus, taking the dot product of `X` and `β` (multiplying
    each coefficient by its corresponding feature and summing over the features) gives
    the predicted response. In logistic regression, we begin instead with the formula:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 `Y` 是数据集所有 `n` 个成员的响应变量，`X` 是一个 `n` 行 `m` 列的矩阵，表示每行数据的 `m` 个特征，`βT` 是一个 `m`
    个系数的列向量（回想一下，`T` 运算符代表向量或矩阵的转置。在这里，我们转置系数，使其维度为 `mx1`，这样我们就可以与矩阵 `X`（其维度为 `nxm`）形成乘积），它给出了对于特定特征的
    1 单位变化预期的响应变化。因此，通过计算 `X` 和 `β` 的点积（将每个系数与其对应特征相乘并按特征求和）可以得到预测的响应。在逻辑回归中，我们开始时使用的是以下公式：
- en: '![Logistic regression](img/B04881_05_02.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归](img/B04881_05_02.jpg)'
- en: 'where the `logistic` function is:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 其中逻辑函数为：
- en: '![Logistic regression](img/B04881_05_03.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归](img/B04881_05_03.jpg)'
- en: 'You can see the behavior of the logistic function by plotting using the following
    code in a notebook session:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过在笔记本会话中使用以下代码来绘制逻辑函数的行为：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![Logistic regression](img/B04881_05_04.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归](img/B04881_05_04.jpg)'
- en: 'Figure 1: The Output of the Logistic Function for a Continuous Input'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：连续输入的逻辑函数输出
- en: 'As you can see in Figure 1, the logistic function takes the output of the linear
    regression and transforms it using a sigmoid (an S-shaped function): as the linear
    regression value becomes larger, the exponential term tends toward 0, making the
    output `1`. Conversely, as the linear regression value becomes negative, the exponential
    term becomes very large, and the output becomes `0`.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 1 所示，逻辑函数通过使用 S 形函数（sigmoid）将线性回归的输出进行转换：随着线性回归值变大，指数项趋向于 0，使得输出为 `1`。相反，当线性回归值变负时，指数项变得非常大，输出变为
    `0`。
- en: 'How can we interpret the coefficients in this model, given that it is no longer
    modeling a simple linear trend? Because of the logistic transform, the coefficients
    no longer represent an expected increase in response per 1-unit increase in the
    predictor. To develop a similar interpretation, we start with the observation
    that the logistic regression equation represents the probability of a given observation,
    *x*, being a member of class `1` (assuming the response variable for the data
    falls into two classes—see the following for a discussion of cases where the number
    of classes is *> 2*). We could also write a similar equation to represent the
    probability of a given observation being class `0`, which is given by:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型不再模拟简单的线性趋势的情况下，我们如何解释这个模型中的系数？由于逻辑变换，系数不再代表预测变量每增加1单位时响应的预期增加。为了发展类似的解释，我们从观察开始，即逻辑回归方程表示给定观察值*x*属于类别`1`的概率（假设数据中的响应变量分为两个类别——见以下关于类别数大于*2*的情况的讨论）。我们也可以写出类似的方程来表示给定观察值属于类别`0`的概率，该概率如下：
- en: '![Logistic regression](img/B04881_05_05.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归](img/B04881_05_05.jpg)'
- en: 'Now, we can take the natural logarithm of these two probabilities get finally:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以取这两个概率的自然对数，最终得到：
- en: '![Logistic regression](img/B04881_05_06.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归](img/B04881_05_06.jpg)'
- en: In other words, the outcome of the linear response now represents the natural
    logarithm of the ratio between the probability of class *1* and class *0*. This
    quantity is also referred to as the log-odds or the logit function, and is equivalent
    to the inverse of the logistic function. In this formula, a 1-unit change in the
    coefficient *β* will lead to a 1-unit increase in the log-odds, allowing us a
    way to interpret the coefficients in this model.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，线性响应的结果现在表示的是类别*1*和类别*0*之间概率比的自然对数。这个量也被称为对数几率或logit函数，并且等同于逻辑函数的逆。在这个公式中，系数*β*的1单位变化将导致对数几率的1单位增加，这为我们提供了解释这个模型中系数的方法。
- en: 'You may recall from [Chapter 4](ch04.html "Chapter 4. Connecting the Dots with
    Models – Regression Methods"), *Connecting the Dots with Models – Regression Methods*,
    that in **Generalized Linear Models** (**GLMs**), link function transforms the
    linear response to a nonlinear range. In logistic regression, the logit function
    is the link function. While a full discussion of the various types of GLMs is
    outside the scope of this book, we refer the interested reader to more comprehensive
    treatments of this topic (Madsen, Henrik, and Poul Thyregod. *Introduction to
    general and generalized linear models*. CRC Press, 2010; Madsen, Henrik, and Poul
    Thyregod. Introduction to general and generalized linear models. CRC Press, 2010:
    Hardin, James William, Joseph M. Hilbe, and Joseph Hilbe. Generalized linear models
    and extensions. Stata press, 2007.).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '您可能还记得来自[第4章](ch04.html "第4章. 通过模型连接点 – 回归方法")*通过模型连接点 – 回归方法*的内容，在**广义线性模型**（**GLMs**）中，连接函数将线性响应转换为非线性范围。在逻辑回归中，logit函数是连接函数。虽然对各种类型GLM的全面讨论超出了本书的范围，但我们建议感兴趣的读者参考更全面的主题处理（Madsen,
    Henrik, 和 Poul Thyregod. *广义和广义线性模型导论*. CRC Press, 2010; Madsen, Henrik, 和 Poul
    Thyregod. 广义和广义线性模型导论. CRC Press, 2010: Hardin, James William, Joseph M. Hilbe,
    和 Joseph Hilbe. 广义线性模型及其扩展. Stata press, 2007.）。'
- en: 'If you have been reading carefully, you may realize that we have contradicted
    ourselves in the discussion above. On the one hand, we want to fit data in which
    the only allowable outcome is either a `0` or a `1`. One the other hand, our logistic
    function (and the log-odds) can take a value between `0` and `1`, continuously.
    Thus, to correctly apply this model, we will need to choose a threshold between
    `0` and `1` to classify the outputs of the regression: if a value is above this
    threshold, we consider the observation as class `1`, otherwise `0`. The simplest
    threshold to choose would be half, and indeed for balanced dataset with an equal
    number of positive and negative examples, this is a reasonable choice. However,
    in many cases that we encounter in the real world (such as ad clicks or subscriptions),
    the number of positive outcomes is much fewer than the negatives. If we optimize
    a logistic regression model using such an imbalanced dataset, the optimal parameters
    will identify few observations as positive. Thus, using half as a cutoff will
    inaccurately classify many negatives as class 1 (positive) and result in a high
    false positive rate.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细阅读，你可能会意识到我们在上面的讨论中自相矛盾。一方面，我们希望拟合只有允许的结果是`0`或`1`的数据。另一方面，我们的逻辑函数（和对数几率）可以取`0`到`1`之间的值，连续变化。因此，为了正确应用此模型，我们需要在`0`和`1`之间选择一个阈值来分类回归的输出：如果值高于此阈值，我们将其视为类别`1`，否则为`0`。最简单的阈值选择是半数，实际上对于正例和反例数量相等的平衡数据集，这是一个合理的选择。然而，在现实世界中我们遇到的许多情况下（例如广告点击或订阅），正例的数量远少于反例。如果我们使用这样的不平衡数据集优化逻辑回归模型，最佳参数将识别出很少的观察结果为正例。因此，使用半数作为截止值将不准确地分类许多反例为类别1（正例），并导致高误报率。
- en: We have a few options to address this problem of imbalanced classes in our data.
    The first is to simply tune the threshold for the logistic function to consider
    the outcome as 1, which we can do visually using the **receiver operator characteristic**
    (**ROC**) curve, described in more detail in the following exercises. We could
    also rebalance our training data such that half represents a reasonable value,
    by selecting an equal number of positive and negative examples. In case we were
    worried about making a biased choice among the many negative examples, we could
    repeat this process many times and average the results—this process is known as
    Bagging and was described in more detail in [Chapter 4](ch04.html "Chapter 4. Connecting
    the Dots with Models – Regression Methods"), *Connecting the Dots with Models
    – Regression Methods*, in the context of Random Forest regression models. Finally,
    we could simply penalize errors on the few positive examples by assigning a weight
    to them in the error function that is greater than the more numerous negative
    examples. More details on reweighting appear as follows.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有几个选项来解决数据中类别不平衡的问题。第一个选项是简单地调整逻辑函数的阈值，将其结果视为1，这可以通过**接收器操作特征**（**ROC**）曲线进行视觉调整，以下练习中将有更详细的描述。我们也可以重新平衡我们的训练数据，使得一半代表一个合理的值，通过选择相同数量的正例和反例。如果我们担心在许多反例中做出有偏的选择，我们可以多次重复这个过程并平均结果——这个过程被称为Bagging，在[第4章](ch04.html
    "第4章。通过模型连接点——回归方法")“通过模型连接点——回归方法”中描述得更为详细，这是在随机森林回归模型的背景下。最后，我们可以通过在误差函数中为它们分配一个大于更多反例的权重来简单地惩罚少数正例的误差。关于重新加权的更多细节将在以下内容中呈现。
- en: 'Multiclass logistic classifiers: multinomial regression'
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多类逻辑分类器：多项式回归
- en: 'While we have dealt thus far with the simple of example of a two-class problem
    , we could imagine scenarios in which there are multiple classes: for example,
    predicting which of a set of items a customer will select in an online store.
    For these sorts of problems, we can imagine extending the logistic regression
    to `K` classes, where *K > 2*. Recall that taking e to the power of the logit
    function gives:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然到目前为止我们只处理了简单的双类问题示例，但我们可以想象存在多个类别的场景：例如，预测客户在在线商店中选择的一组商品中的哪一个。对于这类问题，我们可以想象将逻辑回归扩展到`K`个类别，其中`K
    > 2`。回想一下，将e的对数函数的幂取值给出：
- en: '![Multiclass logistic classifiers: multinomial regression](img/B04881_05_07.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![多类逻辑分类器：多项式回归](img/B04881_05_07.jpg)'
- en: 'In a two-class problem, this value compares the ratio of the probability that
    *Y=1* to all other values, with the only other value being `0`. We could imagine
    running instead a series of logistic regression models for *K* classes, where
    `e(Logit(x))` gives the ratio of the probability of *Y = class k* to any of other
    class. We would then up with a series of *K* expressions for `e(Xβ)` with different
    regression coefficients. Because we want to constrain the outcome to be in the
    range `0` to `–1`, we can divide the output of any of the *K* models by the sum
    of all *K* models using the formula:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在两分类问题中，这个值比较的是 *Y=1* 的概率与其他所有值的比率，唯一的其他值是 `0`。我们可以想象运行一系列针对 *K* 个类别的逻辑回归模型，其中
    `e(Logit(x))` 给出 *Y = class k* 的概率与其他任何类别的比率。然后我们将得到一系列关于 `e(Xβ)` 的 *K* 个表达式，具有不同的回归系数。因为我们希望将结果约束在
    `0` 到 `–1` 的范围内，我们可以使用以下公式将任何 *K* 个模型的输出除以所有 *K* 个模型的总和：
- en: '![Multiclass logistic classifiers: multinomial regression](img/B04881_05_08.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![多元逻辑分类器：多项式回归](img/B04881_05_08.jpg)'
- en: This equation also known as the `softmax` function. It is used extensively in
    neural network models (which we will cover in [Chapter 7](ch07.html "Chapter 7. Learning
    from the Bottom Up – Deep Networks and Unsupervised Features"), *Learning from
    the Bottom Up – Deep Networks and Unsupervised Features*). It has the nice property
    that even for extreme values of `e(xβ)` for a given class k, the overall value
    of the function cannot go beyond 1\. Thus, we can keep outliers in the dataset
    while limiting their influence on the overall accuracy of the model (since otherwise
    they would tend to dominate the overall value of an error function, such as the
    squared error we used in linear regression in [Chapter 4](ch04.html "Chapter 4. Connecting
    the Dots with Models – Regression Methods"), *Connecting the Dots with Models
    – Regression Methods*).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程也称为 `softmax` 函数。它在神经网络模型（我们将在[第7章](ch07.html "第7章. 从底部学习 – 深度网络和无监督特征")
    *从底部学习 – 深度网络和无监督特征* 中介绍）中得到广泛的应用。它有一个很好的特性，即即使对于给定类别 k 的 `e(xβ)` 的极端值，函数的整体值也不会超过
    1。因此，我们可以在数据集中保留异常值，同时限制它们对模型整体准确性的影响（因为否则它们会倾向于主导错误函数的整体值，例如我们在[第4章](ch04.html
    "第4章. 使用模型连接点 – 回归方法") *使用模型连接点 – 回归方法* 中使用的平方误差）。
- en: To keep the current presentation less complex, we will examine only a 2-class
    problem in the following exercises. However, keep in mind that as with logistic
    regression, the other methods discussed in the following can be extended to work
    with multiple classes as well. Additionally, we will demonstrate a full multiclass
    problem in [Chapter 7](ch07.html "Chapter 7. Learning from the Bottom Up – Deep
    Networks and Unsupervised Features"), *Learning from the Bottom Up – Deep Networks
    and Unsupervised Features* using neural networks.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使当前演示更加简单，我们将在以下练习中仅考察一个两分类问题。然而，请记住，就像逻辑回归一样，以下讨论的其他方法也可以扩展到处理多个类别。此外，我们将在[第7章](ch07.html
    "第7章. 从底部学习 – 深度网络和无监督特征") *从底部学习 – 深度网络和无监督特征* 中演示一个完整的多元分类问题，使用神经网络。
- en: Now that we have covered what logistic regression is and the problem it is designed
    to solve, let us prepare a dataset for use with this and other classification
    methods. In addition to working through a practical example of fitting and interpreting
    a logistic regression model, we will use this a starting point to examine other
    classification algorithms.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了逻辑回归是什么以及它旨在解决的问题，让我们准备一个数据集以供此以及其他分类方法使用。除了处理拟合和解释逻辑回归模型的实际例子外，我们还将以此作为起点来检查其他分类算法。
- en: Formatting a dataset for classification problems
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为分类问题格式化数据集
- en: 'In this example, we will use a census dataset with rows representing the characteristic
    of an adult US citizen (Kohavi, Ron. *Scaling Up the Accuracy of Naive-Bayes Classifiers:
    A Decision-Tree Hybrid*. KDD. Vol. 96\. 1996). The objective is to predict whether
    an individual''s income is above or below the average income of $55,000 a year.
    Let us start by loading the dataset into a pandas data frame and examining the
    first few rows using the following commands:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用一个人口普查数据集，其中行代表美国成年公民的特征（Kohavi, Ron. *提高朴素贝叶斯分类器的准确性：决策树混合*. KDD.
    第96卷. 1996年）。目标是预测个人的年收入是否高于或低于平均年收入55,000美元。让我们首先使用以下命令将数据集加载到pandas数据框中，并检查前几行：
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![Formatting a dataset for classification problems](img/B04881_05_09.jpg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![数据集格式化以用于分类问题](img/B04881_05_09.jpg)'
- en: 'Why did we use the argument (`header = None`) to load the data? Unlike some
    of the other datasets we have examined in previous chapters, the column names
    for the census data are contained in a separate file. These feature names will
    be helpful in interpreting the results, so let us parse them from the dataset
    description file:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们使用参数（`header = None`）来加载数据？与我们在前几章中检查的一些其他数据集不同，人口普查数据的列名包含在一个单独的文件中。这些特征名称将有助于解释结果，因此让我们从数据集描述文件中解析它们：
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now that we have the column names appended to the dataset, we can see that
    the response variable, income, needs to be re-encoded. In the input data, it is
    coded as a string, but since scikit-learn is unable to take a string as an input,
    we need to convert it into a `0` or `1` label using the following code:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将列名附加到数据集中，我们可以看到响应变量，收入，需要重新编码。在输入数据中，它被编码为字符串，但由于scikit-learn无法接受字符串作为输入，我们需要使用以下代码将其转换为`0`或`1`标签：
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Here, we have used a lambda expression to apply an anonymous function (a function
    without a name defined in the rest of the program) to the data. The conditional
    expression within the map (…) call takes `x` as an input and returns either `0`
    or `1`. We could just as easily have formally defined such as function, but especially
    for expressions we do not intend to reuse, lambda expressions provide an easy
    way to specify such transformations without crowding our code with a lot of functions
    that do not have general utility.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用lambda表达式将匿名函数（在程序的其他部分没有定义名称的函数）应用于数据。map（…）调用中的条件表达式将`x`作为输入并返回`0`或`1`。我们也可以正式定义这样的函数，但对于我们不想重用的表达式，lambda表达式提供了一种简单的方式来指定这种转换，而不会使我们的代码充斥着许多没有通用用途的函数。
- en: 'Let us take a moment and look at the distribution of the different income classes
    by plotting a histogram with income as the value on the vertical axis:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们花点时间看看不同收入等级的分布，通过绘制一个以收入为垂直轴值的直方图来观察：
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![Formatting a dataset for classification problems](img/B04881_05_10.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![数据集格式化以用于分类问题](img/B04881_05_10.jpg)'
- en: Notice that observations with the label `1` are about 50 percent less prevalent
    than those with a label of `0`. As we discussed previously, this is a situation
    in which a simple threshold of half in evaluating the class probabilities will
    lead to an inaccurate model, and we should keep this data skew in mind as we evaluate
    the performance later.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，标签为`1`的观测值大约比标签为`0`的观测值少50%。正如我们之前讨论的，这是一个简单地将一半作为评估类概率的阈值会导致模型不准确的情况，我们在评估性能时应该记住这种数据偏差。
- en: 'In addition to our outcome variable, many of the features in this dataset are
    also categorical: we will need to re-encode them as well before fitting our model.
    We can do so in two steps: first, let us find the number of unique elements in
    each of these columns and map them to integer values using a dictionary. To begin,
    we check whether each column in the data frame is categorical (`dtype` equal to
    **object**), and, if so, we add its index into the list of columns we want to
    convert:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们的结果变量外，这个数据集中许多特征也是分类的：在拟合模型之前，我们还需要将它们重新编码。我们可以分两步来做：首先，让我们找出这些列中每列的唯一元素数量，并使用字典将它们映射到整数值。为此，我们检查数据框中的每一列是否为分类类型（`dtype`等于**object**），如果是，我们将它的索引添加到我们想要转换的列的列表中：
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now that we have the column numbers we want to convert, we need to make a mapping
    of each column from a string to a label from *1* to *k*, where *k* is the number
    of categories:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经得到了我们想要转换的列号，我们需要将每个列从字符串映射到从*1*到*k*的标签，其中*k*是类别的数量：
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Notice that we first extract the unique elements of each column and then use
    the enumerate function on this list of unique elements to generate the labels
    we need. By converting this indexed list into a dictionary where the keys are
    the unique elements of a column and the values are the labels, we have exactly
    the mapping we need to re-encode the categorical string variables in this data
    as integers.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们首先提取每列的唯一元素，然后使用`enumerate`函数对这个唯一元素列表进行操作以生成我们需要的标签。通过将这个索引列表转换为字典，其中键是列的唯一元素，值是标签，我们就得到了重新编码这个数据集中分类字符串变量为整数的精确映射。
- en: 'Now we can create a second copy of the data using the mapping dictionary we
    generated above:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用上面生成的映射字典创建数据的第二个副本：
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, we can use scikit-learn''s one-hot encoder to transform these integer
    values into a series of columns, of which only one is set to `1`, representing
    which of the k classes this row belongs to. To use the one-hot encoder, we also
    need to know how many categories each of the columns has, which we can do with
    the following command by storing the size of each mapping dictionary:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用scikit-learn的一热编码器将这些整数值转换成一系列列，其中只有一列被设置为`1`，表示这一行属于k个类别中的哪一个。为了使用一热编码器，我们还需要知道每一列有多少个类别，我们可以通过以下命令通过存储每个映射字典的大小来实现：
- en: '[PRE8]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We then apply the one-hot encoder:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们应用一热编码器：
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'From here we have the data in the right format to fit our logistic regression.
    As with our examples in [Chapter 4](ch04.html "Chapter 4. Connecting the Dots
    with Models – Regression Methods"), *Connecting the Dots with Models – Regression
    Methods*, we need to split our data into training and test sets, specifying the
    fraction of data in the test set (*0.4*). We also set the random number generator
    seed to 0 so that we can replicate the analysis later by generating the same random
    set of numbers:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里我们有了适合我们的逻辑回归的正确格式的数据。正如我们在[第4章](ch04.html "第4章。通过模型连接点——回归方法")中的例子一样，*通过模型连接点——回归方法*，我们需要将我们的数据分成训练集和测试集，指定测试集中数据的比例（*0.4*）。我们还设置了随机数生成器的种子为0，这样我们可以在以后通过生成相同的随机数集来重复分析：
- en: '[PRE10]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now that we have prepared training and test data, we can fit a logistic regression
    model to the dataset. How can we find the optimal parameters (coefficients) of
    this model? We will examine two options.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了训练数据和测试数据，我们可以将逻辑回归模型拟合到数据集上。我们如何找到这个模型的最优参数（系数）？我们将检查两个选项。
- en: The first approach, known as **stochastic gradient descent** (**SGD**), calculates
    the change in the error function at a given data point and adjusts the parameters
    to account for this error. For an individual data point, this will result in a
    poor fit, but if we repeat this process over the whole training set several times,
    the coefficients will converge to the desired values. The term **stochastic**
    in this method's name refers to the fact that this optimization is achieved by
    following the gradient (first derivative) of the loss function with respect to
    a given data point in a random order over the dataset. Stochastic methods such
    as this one often scale well to large datasets because they allow us to only examine
    the data individually or in small batches rather than utilizing the whole dataset
    at once, allowing us to parallelize the learning procedure or at least not use
    all the memory on our machine to process large volumes of data.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种方法，被称为**随机梯度下降**（**SGD**），计算在给定数据点上的误差函数的变化，并调整参数以考虑这种误差。对于单个数据点，这会导致拟合不良，但如果我们多次在整个训练集上重复这个过程，系数将收敛到所需的值。这个方法名称中的**随机**一词指的是这种优化是通过在数据集上以随机顺序跟随损失函数相对于给定数据点的梯度（一阶导数）来实现的。像这样随机的方法通常可以很好地扩展到大型数据集，因为它们允许我们只单独或以小批量检查数据，而不是一次利用整个数据集，这使得我们可以并行化学习过程或至少在处理大量数据时不会使用我们机器上的所有内存。
- en: In contrast, the optimization methods implemented by default for the logistic
    regression function in scikit-learn are known as **second-order methods**. SGD,
    because it adjusts the model parameter values using the first derivative of the
    error function, is known as a first-order method. Second-order methods can be
    beneficial in cases where the first derivative is changing very slowly, as we
    will see in the following, and to find the optimal value in cases where the error
    function follows complex patterns.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 与之相反，scikit-learn中默认实现的逻辑回归函数的优化方法被称为**二阶方法**。SGD（随机梯度下降），因为它使用误差函数的一阶导数来调整模型参数值，所以被称为一阶方法。在第一导数变化非常缓慢的情况下，二阶方法可能是有益的，正如我们将在下面看到的那样，以及在误差函数遵循复杂模式的情况下寻找最优值。
- en: Let us look at each of these methods in more detail.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看看这些方法中的每一个。
- en: Learning pointwise updates with stochastic gradient descent
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用随机梯度下降进行点更新学习
- en: 'How do we find the optimal parameters for our logistic regression model using
    stochastic updates? Recall that we are trying to optimize the probabilities:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何使用随机更新找到我们的逻辑回归模型的最优参数？回想一下，我们正在尝试优化概率：
- en: '![Learning pointwise updates with stochastic gradient descent](img/B04881_05_11.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![使用随机梯度下降进行点更新学习](img/B04881_05_11.jpg)'
- en: f we want to optimize the probability of each individual point in our dataset,
    we want to maximize the value of the equation, known as the Likelihood as it scores
    the probability of given point being class `1` (or `0`) based on the model;
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要优化数据集中每个单独点的概率，我们希望最大化方程的值，这个方程被称为似然，因为它根据模型评估给定点属于类别`1`（或`0`）的概率；
- en: '![Learning pointwise updates with stochastic gradient descent](img/B04881_05_12.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![使用随机梯度下降学习点更新](img/B04881_05_12.jpg)'
- en: You can see that if the real label `yi` is `1`, and the model gives high probability
    of `1`, then we maximize the value of `F(zi)` (since the exponent of the second
    term is `0`, making it `1`, while the first term in the product is simply the
    value of `F(zi))`. Conversely, if the real label of `yi` is `0`, then we want
    the model to maximize the value of `(1-F(zi))`, which is the probability of class
    `0` under the model.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到，如果真实标签`yi`是`1`，并且模型给出高概率`1`，那么我们就会最大化`F(zi)`的值（因为第二项的指数是`0`，使其为`1`，而乘积中的第一项仅仅是`F(zi)`的值）。相反，如果`yi`的真实标签是`0`，那么我们希望模型最大化`(1-F(zi))`的值，这是模型下类别`0`的概率。
- en: 'Thus, each point will contribute to the likelihood by the probability of its
    real class. It is usually easier to work with sums than products, so we can take
    the logarithm of the likelihood equation and sum over all elements in the data
    set using:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，每个点将通过其实际类别的概率对似然做出贡献。通常，处理和比处理乘积更容易，因此我们可以对似然方程取对数，并使用以下方式对数据集中的所有元素进行求和：
- en: '![Learning pointwise updates with stochastic gradient descent](img/B04881_05_13.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![使用随机梯度下降学习点更新](img/B04881_05_13.jpg)'
- en: 'To find the optimal value of the parameters, we just take the first partial
    derivative with respect to the parameters of this equation (the regression coefficients)
    and solve for the value of *β* that maximizes the likelihood equation by setting
    the derivative equal to `0` and finding the value of *β* as illustrated below:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到参数的最优值，我们只需对这一方程（回归系数）的参数取一阶偏导数，并求解最大化似然方程的*β*值，通过将导数设置为`0`并找到*β*的值，如下所示：
- en: '![Learning pointwise updates with stochastic gradient descent](img/B04881_05_14.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![使用随机梯度下降学习点更新](img/B04881_05_14.jpg)'
- en: 'This is the direction we want to update the coefficients β in order to move
    it closer to the optimum. Thus, for each data point, we can make an update of
    the following form:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们想要更新系数β的方向，以便将其移动到最优点附近。因此，对于每个数据点，我们可以进行以下形式的更新：
- en: '![Learning pointwise updates with stochastic gradient descent](img/B04881_05_15.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![使用随机梯度下降学习点更新](img/B04881_05_15.jpg)'
- en: 'where `α` is the learning rate (which we use to control the magnitude by which
    the coefficients can change in each step – usually a smaller learning rate will
    prevent large changes in value and converge to a better model, but will take longer),
    t is the current optimization step, and *t-1* is the previous step. Recall that
    in [Chapter 4](ch04.html "Chapter 4. Connecting the Dots with Models – Regression
    Methods"), *Connecting the Dots with Models – Regression Methods* we discussed
    the concept of regularization, in which we can use a penalty term *λ* to control
    the magnitude of our coefficients. We can do the same here: if the regularization
    term in our likelihood is given by (to penalize the squared sum of the coefficients,
    which is the *L2* norm from Ridge Regression in [Chapter 4](ch04.html "Chapter 4. Connecting
    the Dots with Models – Regression Methods"), *Connecting the Dots with Models
    – Regression Methods*):'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 其中`α`是学习率（我们用它来控制每次步骤中系数可以改变的大小——通常较小的学习率可以防止值的大幅变化并收敛到更好的模型，但会花费更长的时间），`t`是当前的优化步骤，而`t-1`是前一步。回想一下，在[第4章](ch04.html
    "第4章. 使用模型连接点——回归方法")中，我们讨论了正则化的概念，其中我们可以使用惩罚项`λ`来控制系数的大小。我们在这里也可以这样做：如果我们的似然中的正则化项由以下给出（惩罚系数的平方和，这是[第4章](ch04.html
    "第4章. 使用模型连接点——回归方法")中岭回归的*L2*范数）：
- en: '![Learning pointwise updates with stochastic gradient descent](img/B04881_05_16.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![使用随机梯度下降学习点更新](img/B04881_05_16.jpg)'
- en: 'Then once we take the first derivate, we have:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，一旦我们取一阶导数，我们就有：
- en: '![Learning pointwise updates with stochastic gradient descent](img/B04881_05_17.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![使用随机梯度下降学习点更新](img/B04881_05_17.jpg)'
- en: 'And the final update equation becomes:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的更新方程变为：
- en: '![Learning pointwise updates with stochastic gradient descent](img/B04881_05_18.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![使用随机梯度下降学习点更新](img/B04881_05_18.jpg)'
- en: We can see, this regularization penalty has the effect of shrinking the amount
    by which we modify the coefficients β at any given step.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，这种正则化惩罚的效果是减少我们在任何给定步骤中修改系数 β 的幅度。
- en: As we mentioned previously, stochastic updates are especially efficient for
    large datasets as we only have to examine each data point one at a time. One downside
    of this approach is that we need to run the optimization long enough to make sure
    the parameters converge. For example, we could monitor the change in the coefficient
    values as we take the derivative with respect to each data point, and stop when
    the values cease changing. Depending upon the dataset, this could occur quickly
    or take a long time. A second downside is that following the gradient of the error
    function along the first derivative will not always lead to the fastest solution.
    Second-order methods allow us to overcome some of these deficits.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前提到的，随机更新对于大数据集特别有效，因为我们只需要逐个检查每个数据点。这种方法的一个缺点是我们需要运行足够长时间的优化以确保参数收敛。例如，我们可以监控随着我们对每个数据点求导而系数值的变化，并在值停止变化时停止。根据数据集的不同，这可能很快发生，也可能需要很长时间。第二个缺点是沿着误差函数的第一导数跟踪梯度并不总是导致最快的解决方案。二阶方法使我们能够克服一些这些缺点。
- en: Jointly optimizing all parameters with second-order methods
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用二阶方法联合优化所有参数
- en: 'In the case of logistic regression, our objective function is convex (see aside),
    meaning that whichever optimization method we choose should be able to converge
    to the global optimum. However, we could imagine other scenarios: for example,
    the surface of the likelihood equation plotted as a function of the inputs could
    vary slowly in a long ravine toward its global optimum. In such a case, we would
    like to find the direction to move the coefficients that is the optimal tradeoff
    between the rate of change and the **rate of the rate of change**, represented
    by the second derivative of the likelihood. Finding this tradeoff allows the optimization
    routine to traverse slowly varying regions quickly. This kind of strategy is represented
    by the class of so-called **Newton methods**, which minimizes equations of the
    following form:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在逻辑回归的情况下，我们的目标函数是凸函数（见附图），这意味着我们选择的任何优化方法都应该能够收敛到全局最优解。然而，我们可以想象其他场景：例如，似然方程的表面作为输入函数的函数可能在通向其全局最优解的长峡谷中缓慢变化。在这种情况下，我们希望找到移动系数的方向，这是变化率和**变化率的变化率**之间的最佳权衡，后者由似然函数的二阶导数表示。找到这种权衡使得优化过程能够快速穿越缓慢变化的区域。这种策略由所谓的**牛顿方法**类表示，它最小化以下形式的方程：
- en: '![Jointly optimizing all parameters with second-order methods](img/B04881_05_19.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![使用二阶方法联合优化所有参数](img/B04881_05_19.jpg)'
- en: 'Where `f(x*)` is the objective function we are trying to minimize, such as
    the logistic regression error, and x are the values (such as the model coefficients)
    that do minimize regression likelihood, `x*` are the inputs which optimize the
    value of the function (such as the optimal coefficients β), and `xt` are the value
    of these parameters at the current step of the optimization (there is admittedly
    some abuse of notation here: in the rest of the chapter, *x* are the input rows,
    where here we use *x* to represent a parameter value in a model). The name **Newton
    method** is due to the father of physics, Isaac Newton, who described an early
    version of this procedure (Ypma, Tjalling J. *Historical development of the Newton-Raphson
    method*. SIAM review 37.4 (1995): 531-551). The minimization involves finding
    the direction we should move the parameters at the current stage, `xt`, in order
    to find the minimal value of `f`. We can use a Taylor Expansion from Calculus
    to approximate the value of the preceding function:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 `f(x*)` 是我们试图最小化的目标函数，例如逻辑回归误差，x 是那些最小化回归似然值的值（例如模型系数），`x*` 是优化函数值（例如最优系数
    β）的输入，`xt` 是优化当前步骤中这些参数的值（这里确实有一些符号的滥用：在本书的其余部分，*x* 是输入行，而在这里我们用 *x* 来表示模型中的参数值）。**牛顿方法**的名字来源于物理学的奠基人艾萨克·牛顿，他描述了这种过程的早期版本（Ypma,
    Tjalling J. *牛顿-拉夫森方法的史前发展*. SIAM评论37.4 (1995): 531-551）。最小化涉及找到在当前阶段 `xt` 应该移动参数的方向，以找到
    `f` 的最小值。我们可以使用微积分中的泰勒展开来近似前一个函数的值：'
- en: '![Jointly optimizing all parameters with second-order methods](img/B04881_05_20.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![使用二阶方法联合优化所有参数](img/B04881_05_20.jpg)'
- en: 'We want to find the value of `Δx` which maximizes the function, since this
    is the direction we wish to move the parameters, just as in gradient descent.
    We can obtain this optimal direction by solving for the point where the gradient
    of the function becomes `0` with respect to `Δx`, to give:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要找到 `Δx` 的值，以使函数最大化，因为这是我们希望移动参数的方向，就像在梯度下降中一样。我们可以通过求解函数梯度相对于 `Δx` 变为 `0`
    的点来获得这个最优方向，从而得到：
- en: '![Jointly optimizing all parameters with second-order methods](img/B04881_05_21.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![使用二阶方法联合优化所有参数](img/B04881_05_21.jpg)'
- en: Thus, when `f′(x)` is changing slowly (small `f″(x)`), we take larger steps
    to change the value of the parameters, and vice versa.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当 `f′(x)` 变化缓慢（`f″(x)` 较小）时，我们采取更大的步长来改变参数的值，反之亦然。
- en: 'One of the most commonly used second order methods for logistic regression
    is **iteratively reweighted least squares** (**IRLS**). To show how it works let
    us translate the equations above to our logistic regression model. We already
    known `f''(x)`, since this is just our formula from above used for stochastic
    gradient descent:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 对于逻辑回归来说，最常用的二阶方法之一是**迭代加权最小二乘法**（**IRLS**）。为了展示它是如何工作的，让我们将上面的方程转换为我们的逻辑回归模型。我们已经知道
    `f'(x)`，因为这只是我们上面用于随机梯度下降的公式：
- en: '![Jointly optimizing all parameters with second-order methods](img/B04881_05_22.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![使用二阶方法联合优化所有参数](img/B04881_05_22.jpg)'
- en: What about the second derivative of the likelihood? We can solve for it as well
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，关于似然函数的二阶导数呢？我们也可以求解它
- en: '![Jointly optimizing all parameters with second-order methods](img/B04881_05_23.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![使用二阶方法联合优化所有参数](img/B04881_05_23.jpg)'
- en: 'Here we are still writing this equation as a solution for a single data point.
    In second order methods we are not usually going to use stochastic updates, so
    we need to apply the formula to all data points. For the gradient (first derivative),
    this gives the sum:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们仍然将这个方程写作单个数据点的解。在二阶方法中，我们通常不会使用随机更新，因此我们需要将公式应用于所有数据点。对于梯度（一阶导数），这给出总和：
- en: '![Jointly optimizing all parameters with second-order methods](img/B04881_05_24.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![使用二阶方法联合优化所有参数](img/B04881_05_24.jpg)'
- en: For second derivative, we can express the result as a matrix. The matrix of
    pairwise second derivatives is known also known as the Hessian matrix.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 对于二阶导数，我们可以将其表示为一个矩阵。成对二阶导数的矩阵也被称为Hessian矩阵。
- en: '![Jointly optimizing all parameters with second-order methods](img/B04881_05_25.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![使用二阶方法联合优化所有参数](img/B04881_05_25.jpg)'
- en: 'Where I is the identity matrix (a matrix with 1 on the diagonal and 0 elsewhere),
    and A contains the second derivative evaluated for each pair of points *i* and
    *j*. Thus, if we use these expressions to make a Newton update, we have:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 I 是单位矩阵（对角线上的元素为 1，其余位置为 0），而 A 包含了对每对点 *i* 和 *j* 评估的二阶导数。因此，如果我们使用这些表达式进行牛顿更新，我们就有：
- en: '![Jointly optimizing all parameters with second-order methods](img/B04881_05_26.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![使用二阶方法联合优化所有参数](img/B04881_05_26.jpg)'
- en: 'Because the second derivative appears in the denominator, we use a matrix inverse
    (given by the *-1* exponent) to perform this operation. If you look closely at
    the denominator, we have the product *XT X* weighted by the elements of *A*, and
    in the numerator we have *X(Y-F(X))*. This resembles the equation for ordinary
    linear regression that we saw in [Chapter 4](ch04.html "Chapter 4. Connecting
    the Dots with Models – Regression Methods"), *Connecting the Dots with Models
    – Regression Methods*! In essence, this update is performing a stepwise linear
    regression weighted at each pass by *A* (whose values change as we update the
    coefficients), thus giving the method its name. One of the shortcomings of IRLS
    is that we need to repeatedly invert a Hessian matrix that will become quite large
    as the number of parameters and data points grows. Thus, we might try to find
    ways to approximate this matrix instead of explicitly calculating it. One method
    commonly used for this purpose is the Limited Memory Broyden–Fletcher–Goldfarb–Shanno
    (L-BFGS) algorithm (Liu, Dong C., and Jorge Nocedal. *On the limited memory BFGS
    method for large scale optimization*. Mathematical programming 45.1-3 (1989):
    503-528), which uses the last k updates of the algorithm to calculate an approximation
    of the Hessian matrix instead of explicitly solving for it in each stage.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '由于二阶导数出现在分母中，我们使用矩阵逆（由 *-1* 指数给出）来执行此操作。如果你仔细观察分母，我们有一个由 *A* 的元素加权的 *XT X*
    的乘积，而在分子中我们有 *X(Y-F(X))*。这与我们在[第 4 章](ch04.html "第 4 章. 使用模型连接点 – 回归方法")中看到的普通线性回归方程相似，*使用模型连接点
    – 回归方法*！本质上，这个更新是在每次迭代时通过 *A*（其值随着我们更新系数而变化）进行加权逐步线性回归，从而赋予该方法其名称。IRLS 的一个缺点是我们需要反复求逆一个将随着参数和数据点数量的增加而变得相当大的
    Hessian 矩阵。因此，我们可能会尝试找到方法来近似这个矩阵而不是显式地计算它。用于此目的的一种常用方法是有限记忆 Broyden–Fletcher–Goldfarb–Shanno
    (L-BFGS) 算法（Liu, Dong C. 和 Jorge Nocedal. *关于大规模优化的有限记忆 BFGS 方法*. 数学规划 45.1-3
    (1989): 503-528），它使用算法的最后 k 次更新来计算 Hessian 矩阵的近似值，而不是在每个阶段显式求解它。'
- en: 'In both SGD and Newton methods, we have theoretical confidence that both methods
    will eventually converge to the correct (globally optimal) parameter values due
    to a property of the likelihood function known as convexity. Mathematically, a
    convex function F fulfills the condition:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在 SGD 和牛顿法中，我们由于似然函数的一个称为凸性的性质，对这两种方法最终收敛到正确的（全局最优）参数值有理论上的信心。从数学上讲，一个凸函数 F
    满足以下条件：
- en: '![Jointly optimizing all parameters with second-order methods](img/B04881_05_27.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![使用二阶方法联合优化所有参数](img/B04881_05_27.jpg)'
- en: Conceptually, this means that for two points `x1` and `x2`, the value of F for
    points between them (the left-hand side of the equation) is less than or equal
    to the straight line between the points (the right-hand side of the equation,
    which gives a linear combination of the function value at the two points). Thus,
    a convex function will have a global minimum between the points `x1` or `x2`.
    Graphically, you can see this by plotting the following in a python notebook
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲，这意味着对于两个点 `x1` 和 `x2`，它们之间（等式左侧）的 F 值小于或等于两点之间的直线（等式右侧，给出两点函数值的线性组合）。因此，凸函数将在
    `x1` 或 `x2` 之间有一个全局最小值。在 Python 笔记本中绘制以下内容可以图形化地看到这一点
- en: '[PRE11]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![Jointly optimizing all parameters with second-order methods](img/B04881_05_28.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![使用二阶方法联合优化所有参数](img/B04881_05_28.jpg)'
- en: The parabola is a convex function because the values between `x1` and `x2` (the
    two points where the blue line intersects with the parabola) are always below
    the blue line representing `α(F(x1))+(1-α) (F(x2))`. As you can see, the parabola
    also has a global minimum between these two points.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 抛物线是一个凸函数，因为位于 `x1` 和 `x2`（蓝色线与抛物线相交的两个点）之间的值始终低于代表 `α(F(x1))+(1-α) (F(x2))`
    的蓝色线。正如你所见，抛物线在这两点之间也有一个全局最小值。
- en: When we are dealing with matrices such as the Hessian referenced previously,
    this condition is fulfilled by each element of the matrix being *≥ 0*, a property
    known as positive semidefinite, meaning any vector multiplied by this matrix on
    either side (xTHx) yields a value `≥ 0`. This means the function has a global
    minimum, and if our solution converges to a set of coefficients, we can be guaranteed
    that they represent the best parameters for the model, not a local minimum.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们处理如之前提到的Hessian矩阵这样的矩阵时，这个条件通过矩阵的每个元素都满足 *≥ 0* 来满足，这是一个称为正半定性的属性，意味着任何向量乘以这个矩阵的任一边（xTHx）都会得到一个值
    `≥ 0`。这意味着函数有一个全局最小值，并且如果我们的解收敛到一组系数，我们可以保证它们代表模型的最佳参数，而不是局部最小值。
- en: We noted previously that we could potentially offset imbalanced distribution
    of classes in our data by reweighting individual points during training. In the
    formulas for either SGD or IRLS, we could apply a weight wi for each data point,
    increasing or decreasing its relative contribution to the value of the likelihood
    and the updates made during each iteration of the optimization algorithm.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到，我们可以在训练过程中通过重新加权单个点来潜在地补偿数据集中类的不平衡分布。在SGD或IRLS的公式中，我们可以为每个数据点应用一个权重wi，增加或减少其在似然值和优化算法每次迭代中更新的相对贡献。
- en: Now that we have described how to obtain the optimal parameters of the logistic
    regression model, let us return to our example and apply these methods to our
    data.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经描述了如何获得逻辑回归模型的最佳参数，让我们回到我们的例子，并将这些方法应用于我们的数据。
- en: Fitting the model
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型拟合
- en: 'We can use either the SGD or second-order methods to fit the logistic regression
    model to our data. Let us compare the results using SGD; we fit the model using
    the following command:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用SGD或二次方法将逻辑回归模型拟合到我们的数据。让我们使用SGD比较结果；我们使用以下命令拟合模型：
- en: '[PRE12]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Where the parameter `log` for loss specifies that this is a logistic regression
    that we are training, and `n_iter` specifies the number of times we iterate over
    the training data to perform SGD, alpha represents the weight on the regularization
    term, and we specify that we do not want to fit the intercept to make comparison
    to other methods more straightforward (since the method of fitting the intercept
    could differ between optimizers). The penalty argument specifies the regularization
    penalty, which we saw in [Chapter 4](ch04.html "Chapter 4. Connecting the Dots
    with Models – Regression Methods"), *Connecting the Dots with Models – Regression
    Methods*, already for ridge regression. As `l2` is the only penalty we can use
    with second-order methods, we choose `l2` here as well to allow comparison between
    the methods. We can examine the resulting model coefficients by referencing the
    coeff_ property of the model object:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 其中损失参数 `log` 指定这是一个我们正在训练的逻辑回归，`n_iter` 指定我们迭代训练数据以执行SGD的次数，alpha代表正则化项的权重，我们指定我们不想拟合截距以使与其他方法的比较更简单（因为不同优化器的拟合截距方法可能不同）。惩罚参数指定正则化惩罚，我们在[第4章](ch04.html
    "第4章. 使用模型连接点 – 回归方法")中已经看到了，*使用模型连接点 – 回归方法*，对于岭回归。由于`l2`是我们可以在二次方法中使用的唯一惩罚，我们在这里也选择`l2`，以便比较方法。我们可以通过引用模型对象的coeff_属性来检查结果模型系数：
- en: '[PRE13]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Compare these coefficients to the second-order fit we obtain using the following
    command:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些系数与我们使用以下命令获得的二次拟合进行比较：
- en: '[PRE14]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Like the SGD model, we remove the intercept fit to allow the most direct comparison
    of the coefficients produced by the two methods., We find that the coefficients
    are not identical, with the output of the SGD model containing several larger
    coefficients. Thus, we see in practice that even with similar models and a convex
    objective function, different optimization methods can give different parameter
    results. However, we can see that the results are highly correlated based on a
    pairwise scatterplot of the coefficients:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 与SGD模型一样，我们移除了截距拟合，以便最直接地比较两种方法产生的系数。我们发现系数并不相同，SGD模型的输出包含几个较大的系数。因此，在实践中，我们看到即使具有类似模型和凸目标函数，不同的优化方法也可以给出不同的参数结果。然而，我们可以通过系数的双变量散点图看到，结果高度相关：
- en: '[PRE15]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![Fitting the model](img/B04881_05_29.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![模型拟合](img/B04881_05_29.jpg)'
- en: 'The fact that the SGD model has larger coefficients gives us a hint as to what
    might be causing the difference: perhaps SGD is more sensitive to differences
    in scale between features? Let us evaluate this hypothesis by using the **StandardScaler**
    introduced in [Chapter 3](ch03.html "Chapter 3. Finding Patterns in the Noise
    – Clustering and Unsupervised Learning"), *Finding Patterns in the Noise – Clustering
    and Unsupervised Learning* in the context of K-means clustering to normalize the
    features before running the SGD model using the following commands:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: SGD 模型具有更大的系数这一事实给我们一个提示，关于可能造成差异的原因：也许 SGD 对特征之间的尺度差异更敏感？让我们通过使用 [第 3 章](ch03.html
    "第 3 章。在噪声中寻找模式 – 聚类和无监督学习") 中介绍的 **StandardScaler** 来评估这个假设，在 K-means 聚类的情况下，在运行
    SGD 模型之前对特征进行归一化，使用以下命令：
- en: '[PRE16]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Recall that we need to turn the features matrix to a dense format since StandardScaler
    does not accept a sparse matrix as input. Now, if we retrain the SGD using the
    same arguments and plot the result versus the Newton method, we find the coefficients
    are much closer:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们需要将特征矩阵转换为密集格式，因为 StandardScaler 不接受稀疏矩阵作为输入。现在，如果我们使用相同的参数重新训练 SGD 并将结果与牛顿法进行比较，我们会发现系数要接近得多：
- en: '![Fitting the model](img/B04881_05_30.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![拟合模型](img/B04881_05_30.jpg)'
- en: This example should underscore the fact that the optimizer is sometimes as important
    as the actual algorithm, and may determine what steps we should take in data normalization.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子应该强调优化器有时与实际算法一样重要，并可能决定我们在数据归一化中应采取的步骤。
- en: Evaluating classification models
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估分类模型
- en: 'Now that we have fit a classification model, we can examine the accuracy on
    the test set. One common tool for performing this kind of analysis is the **Receiver
    Operator Characteristic** (**ROC**) curve. To draw an ROC curve, we select a particular
    cutoff for the classifier (here, a value between `0` and `1` above which we consider
    a data point to be classified as a positive, or 1) and ask what fraction of 1s
    are correctly classified by this cutoff (true positive rate) and, concurrently,
    what fraction of negatives are incorrectly predicted to be positive (false positive
    rate) based on this threshold. Mathematically, this is represented by choosing
    a threshold and computing four values:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经拟合了一个分类模型，我们可以检查测试集上的准确性。进行这种分析的一个常用工具是 **接收者操作特征**（**ROC**）曲线。为了绘制 ROC
    曲线，我们选择分类器的特定截止值（在这里，一个介于 `0` 和 `1` 之间的值，高于此值我们认为数据点被分类为正，或 1），并询问这个截止值正确分类的 1
    的比例（真正率），同时，基于这个阈值，有多少负数被错误地预测为正（假正率）。从数学上讲，这表示选择一个阈值并计算四个值：
- en: '[PRE17]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The **true positive rate** (**TPR**) plotted by the ROC is then *TP/(TP+FN)*,
    while the **false positive rate** (**FPR**) is *FP/(FP+TN)*.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ROC 曲线绘制的 **真正率**（**TPR**）是 *TP/(TP+FN)*，而 **假正率**（**FPR**）是 *FP/(FP+TN)*。
- en: If both rates are equal, then this is no better than random. In other words,
    at whatever cutoff we choose, a prediction of class 1 by the model is equally
    likely regardless if the point is actually positive or negative. Thus, a diagonal
    line from the lower left to the upper right hand represent the performance of
    a classifier made through randomly choosing labels for data points, since the
    true positive and false positive rates are always equal. Conversely, if the classifier
    exhibits better than random performance, the true positive rate rises much faster
    as correctly classified points are enriched above the threshold. Integrating the
    **area under the curve** (**AUC**) of the ROC curve, which has a maximum of 1,
    is a common way to report the accuracy of classifier methods. To find the best
    threshold to use for classification, we find the point on this curve where the
    ratio between true positive and false positive rates is maximal.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这两个比率相等，那么这并不比随机选择更好。换句话说，无论我们选择什么截止值，模型对类别 1 的预测都是等可能的，无论该点实际上是正还是负。因此，从左下角到右上角的斜线代表通过随机选择数据点的标签创建的分类器的性能，因为真正率和假正率总是相等的。相反，如果分类器表现出优于随机性能，随着正确分类的点在阈值之上增加，真正率会更快地上升。将
    ROC 曲线下方的面积（**AUC**）积分，其最大值为 1，是报告分类方法准确性的常见方式。为了找到用于分类的最佳阈值，我们找到曲线上真正率和假正率比率最大的点。
- en: 'In our example, this is important because `1` is less frequent than `0`. As
    we mentioned in the beginning of this chapter when we were examining the data
    set, this can cause problems in training a classification model. While the naïve
    choice would be to consider events with predicted probability above 0.5 as 1,
    in practice we find that due to this dataset imbalance, a lower threshold is optimal
    as the solution is biased toward the zeros. This effect can become even more extreme
    in highly skewed data: consider an example where only 1 in 1,000 points have label
    1\. We could have an excellent classifier that predicts that every data point
    is 0: it is 99.9% percent accurate! However, it would not be very useful in identifying
    rare events. There are a few ways we could counteract this bias besides adjusting
    the threshold in the AUC.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，这一点很重要，因为 `1` 的出现频率低于 `0`。正如我们在本章开头检查数据集时提到的，这可能导致在训练分类模型时出现问题。虽然直观的选择是将预测概率高于
    0.5 的事件视为 1，但在实践中我们发现，由于这个数据集的不平衡，一个较低的阈值是最佳选择，因为解决方案偏向于零。在高度倾斜的数据中，这种影响可能会更加明显：考虑一个只有
    1,000 个点中有 1 个标签为 1 的例子。我们可能有一个非常出色的分类器，它预测每个数据点都是 0：它有 99.9% 的准确率！然而，它对识别稀有事件并不太有用。除了调整
    AUC 的阈值之外，我们还有几种方法可以抵消这种偏差。
- en: 'One way would be to rebalance the model by constructing a training set that
    is 50 percent 1s and 50 percent 0s. We can then evaluate the performance on the
    unbalanced test dataset. If the imbalance is very large, our rebalanced training
    set might contain only a small number of the possible variation in the 0s: thus,
    to generate a model representative of the entire dataset, we may want to construct
    many such datasets and average the results of the models generated from them.
    This approach is not dissimilar to the Bagging method used in constructing Random
    Forest models, as we saw in [Chapter 4](ch04.html "Chapter 4. Connecting the Dots
    with Models – Regression Methods"), *Connecting the Dots with Models – Regression
    Methods*.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 一种方法是通过构建一个 50% 为 1s 和 50% 为 0s 的训练集来重新平衡模型。然后我们可以评估在未平衡测试数据集上的性能。如果不平衡性非常大，我们的重新平衡训练集可能只包含
    0s 的可能变化的一小部分：因此，为了生成代表整个数据集的模型，我们可能需要构建许多这样的数据集，并平均从它们生成的模型的成果。这种方法与我们在第 4 章中看到的用于构建随机森林模型的
    Bagging 方法并不相似，*连接点与模型 – 回归方法*。
- en: Secondly, we can use our knowledge of the imbalance to change the contribution
    of each data point as we optimize the parameters. For example, in the SGD equations,
    we can penalize errors on 1s 1,000 times as much as errors on 0s. This weight
    will then correct the bias in the model.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们可以在优化参数的过程中，利用我们对不平衡数据的了解来改变每个数据点的贡献。例如，在 SGD 方程中，我们可以将 1s 上的错误惩罚比 0s 上的错误惩罚高
    1,000 倍。这个权重将随后纠正模型中的偏差。
- en: Our interpretation of the AUC is also changed in very imbalanced datasets. While
    an overall AUC of 0.9 might be considered good, if the ratio between the TPR and
    FPR at a false positive rate of 0.001 (the fraction of data containing the rare
    class) is not > 1, it indicates we may have to search through a large amount of
    the head of the ranking to enrich the rare events. Thus, while the overall accuracy
    appears good, the accuracy in the range of data we most These scenarios are not
    uncommon in practice. For example, ad clicks are usually much less frequent than
    non-clicks, as are responses to sales inquiries. Visually, a classifier that is
    not well-fit to imbalanced data would be indicated by an ROC curve where the difference
    between TPR and FPR is greatest near the middle of the curve (*~0.5*). Conversely,
    in an ROC curve of a classifier that is appropriately adapted to rare events,
    most of the area is contained to the left hand side (rising sharply from a cutoff
    of 0 and leveling off to the right), representing enrichment of positives at high
    thresholds.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在非常不平衡的数据集中，我们对 AUC 的解释也发生了变化。虽然整体 AUC 为 0.9 可能被认为是好的，但如果在假阳性率为 0.001（包含稀有类别的数据比例）时，TPR
    和 FPR 之间的比率不大于 1，这表明我们可能需要搜索排名前列的大量数据来丰富稀有事件。因此，尽管整体准确率看起来很好，但我们最关心的数据范围内的准确率可能并不高。这些场景在实践中并不少见。例如，广告点击通常比非点击少得多，销售咨询的回复也是如此。从视觉上看，一个不适合不平衡数据的分类器会在
    ROC 曲线上显示出 TPR 和 FPR 差距在曲线中间最大（*~0.5*）。相反，对于一个适当调整以适应稀有事件的分类器的 ROC 曲线，大部分区域都包含在曲线的左侧（从
    0 的截止点急剧上升，然后向右平缓），这代表着在高阈值下正例的丰富。
- en: 'Note that false positive rate and false negative rate are just two examples
    of accuracy metrics we could compute. We may also be interested in knowing, above
    a given cutoff for model score, 1) how many of our positive examples are classified
    (recall) and what percentage of points above this threshold are actually positive
    2) precision. These are calculated as:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，假正例率和假负例率只是我们可能计算的准确性指标的两个例子。我们可能还感兴趣知道，在模型分数的给定截止值以上，1）我们有多少个正例被分类（召回率）以及超过此阈值的点的实际正例百分比
    2）精确度。这些计算如下：
- en: '*Precision = TP/(TP+FP)*'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '*精确度 = TP/(TP+FP)*'
- en: '*Recall = TP/(TP+FN)*'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '*召回率 = TP/(TP+FN)*'
- en: 'In fact, recall is identical to the true positive rate. While the ROC curve
    allows us to evaluate whether the model generates true positive predictions at
    a greater rate than false positives, comparing precision versus recall gives a
    sense of how reliable and complete the predictions above a given score threshold
    are. We could have very high precision, but only be able to detect a minority
    of the overall positive examples. Conversely, we could have high recall at the
    cost of low precision as we incur false positives by lowering the score threshold
    to call positives in our model. The tradeoff between these can be application
    specific. For example, if the model is largely exploratory, such as a classifier
    used to generate potential sales leads for marketing, then we accept a fairly
    low precision since the value of each positive is quite high even if the true
    predictions are interspersed with noise. On the other hand, in a model for spam
    identification, we may want to err on the side of high precision, since the cost
    of incorrectly moving a valid business email to the user''s trash folder may be
    higher than the occasional piece of unwanted mail that gets through the filter.
    Finally, we could also consider performance metrics that are appropriate even
    for imbalanced data, because they represent a tradeoff between the precision and
    recall for majority and minority classes. These include the F-measure:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，召回率与真正例率相同。虽然 ROC 曲线允许我们评估模型是否以比假正例更高的比率生成真正例预测，但比较精确度与召回率可以让我们了解给定分数阈值以上的预测的可靠性和完整性。我们可能有非常高的精确度，但只能检测到整体正例中的少数。相反，我们可能以牺牲低精确度为代价获得高召回率，因为我们通过降低分数阈值来调用模型中的正例，从而产生假正例。这些之间的权衡可能因应用而异。例如，如果模型主要是探索性的，例如用于为营销生成潜在销售线索的分类器，那么我们可以接受相当低的精确度，因为即使真正的预测中夹杂着噪声，每个正例的价值也相当高。另一方面，在用于垃圾邮件识别的模型中，我们可能希望偏向于高精确度，因为将有效的商业电子邮件错误地移动到用户的垃圾文件夹中的成本可能高于偶尔通过过滤器的垃圾邮件。最后，我们还可以考虑适用于不平衡数据的性能指标，因为它们代表了多数类和少数类之间的精确度与召回率的权衡。这些包括
    F 度量：
- en: '![Evaluating classification models](img/B04881_05_31.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![评估分类模型](img/B04881_05_31.jpg)'
- en: 'And Matthew''s correlation coefficient (Matthews, Brian W. *Comparison of the
    predicted and observed secondary structure of T4 phage lysozyme*. Biochimica et
    Biophysica Acta (BBA)-Protein Structure 405.2 (1975): 442-451.):'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 以及马修斯相关系数（马修斯，布莱恩·W. *比较预测和观察到的 T4 噬菌体溶菌酶的二级结构*. 生物化学与生物物理学报（BBA）-蛋白质结构 405.2（1975）：442-451.）：
- en: '![Evaluating classification models](img/B04881_05_32.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![评估分类模型](img/B04881_05_32.jpg)'
- en: 'Returning to our example, we have two choices in how we could compute the predictions
    from our model: either a class label (`0` or `1`) or a probability of a particular
    individual being class `1`. For computing the ROC curve, we want the second choice,
    since this will allow us to evaluate the accuracy of the classifier over a range
    of probabilities used as a threshold for classification:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 返回到我们的例子，我们在如何从模型中计算预测结果上有两种选择：要么是一个类别标签（`0` 或 `1`），要么是一个特定个体被分类为 `1` 的概率。对于计算
    ROC 曲线，我们希望选择第二种，因为这将允许我们在一系列用作分类阈值的概率范围内评估分类器的准确性：
- en: '[PRE18]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'With this probability, we can we see visually that our model gives a subpar
    accuracy using the following code to plot the ROC curve for the training and test
    sets:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码绘制训练集和测试集的 ROC 曲线，我们可以直观地看到我们的模型在以下方面给出了低于平均的准确性：
- en: '[PRE19]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![Evaluating classification models](img/B04881_05_33.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![评估分类模型](img/B04881_05_33.jpg)'
- en: 'Numerically, we find that the AUC of the test and training set is little better
    than random (`0.5`), as both the commands:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 从数值上讲，我们发现测试集和训练集的 AUC 略好于随机（`0.5`），因为以下两个命令：
- en: '[PRE20]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: and
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: and
- en: '[PRE21]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: give results of ~ 0.6.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: give results of ~ 0.6.
- en: If possible, we would like to improve the performance of our classified—how
    can we diagnose the problems with our existing logistic regression model and work
    toward a better prediction?
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如果可能的话，我们希望提高我们分类的性能——我们如何诊断现有逻辑回归模型的问题并朝着更好的预测努力？
- en: Strategies for improving classification models
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提高分类模型策略
- en: 'Confronted with this less than desirable performance, we typically have a few
    options:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 面对这种不尽如人意的表现，我们通常有几个选择：
- en: Train with more data
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用更多数据进行训练
- en: Regularize the model to reduce over-fitting
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对模型进行正则化以减少过拟合
- en: Choose another algorithm
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择另一个算法
- en: In our example with an under-performing logistic regression model, which option
    makes most sense?
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们关于表现不佳的逻辑回归模型的例子中，哪个选项最有意义？
- en: Let us consider take the first option, that we simply need more data to improve
    performance. In some cases, we may not have enough data in our training set to
    represent the patterns we observe in the test set. If this were the case, we would
    expect to see our performance on the test set improve as we increase the size
    of the training set used to build the model. However, we do not always have the
    convenience of getting more data. In this example, we don't actually have more
    data to train with; even if is possible to collect more data in theory, in practice
    it may be too expensive to justify the cost, or we may need to make a decision
    before more data will be available.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑第一个选项，即我们仅仅需要更多的数据来提高性能。在某些情况下，我们可能没有足够的数据在我们的训练集中来代表我们在测试集中观察到的模式。如果情况是这样，我们预计随着我们增加用于构建模型的训练集的大小，我们的测试集性能将得到改善。然而，我们并不总是有获取更多数据的便利。在这个例子中，我们实际上没有更多的数据来训练；即使理论上可以收集更多的数据，在实践中可能成本太高，无法证明其合理性，或者我们可能需要在更多数据可用之前做出决定。
- en: What about over-fitting? In other words, perhaps our model is precisely tuned
    to the patterns in the training set, but does not generalize to the test set.
    Like the first option, we will observe better performance on the training set
    than the test set. However, the solution is not necessarily to add more data,
    but rather to prune features to make the model more general. In the preceding
    scenario, we see that the performance on both training and test is similar, so
    this does not seem like the most likely explanation.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，过拟合怎么办？换句话说，也许我们的模型精确地调整到了训练集中的模式，但不能推广到测试集。像第一个选项一样，我们将观察到训练集上的性能优于测试集。然而，解决方案并不一定是添加更多数据，而是修剪特征以使模型更具普遍性。在前面的场景中，我们看到训练集和测试集的性能相似，所以这并不像是最可能的解释。
- en: 'Finally, we might try another algorithm. To do so, let us consider what the
    limitations of our current model are. For one, the logistic regression only incorporates
    single features: it has no way to represent interactions between them. For instance,
    it can only model the effect of marital status, not marital status conditional
    on education and age. It is perhaps not surprising that these factors probably
    in combination predict income, but not necessarily individually. It may help to
    look at the values of the coefficients, and to do so, we will need to map the
    original column headers to column names in our one-hot encoding, where each of
    the categorical features is now represented by several columns. In this format,
    the numerical columns are appended to the end of the data frame, so we need to
    add them last to the list of columns. The following code remaps the column headers
    using the mapping of category to one-hot position we calculated earlier:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可能尝试另一种算法。为了这样做，让我们考虑我们当前模型的局限性。一方面，逻辑回归仅包含单个特征：它没有表示它们之间相互作用的方法。例如，它只能模拟婚姻状态的影响，但不能模拟基于教育和年龄的婚姻状态。这些因素可能组合起来预测收入，但并不一定单独预测。查看系数的值可能会有所帮助，为此，我们需要将原始列标题映射到我们的独热编码中的列名，其中每个分类特征现在由几个列表示。在这个格式中，数值列附加到数据框的末尾，因此我们需要将它们添加到最后到列列表中。以下代码使用我们之前计算的类别到独热位置的映射重新映射列标题：
- en: '[PRE22]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We can check that the individual coefficient make sense: keep in mind that
    the sort function arranges items in ascending order, so to find the largest coefficients
    we sort by the negative value:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以检查单个系数是否合理：记住，排序函数按升序排列项目，因此要找到最大的系数，我们需要按负值排序：
- en: '[PRE23]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Logically, the order appears to make sense, since we would expect age and education
    to be important predictors of income. However, we see that only *~1/3rd* of the
    features have a large influence on the model through the following plot:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 从逻辑上讲，顺序似乎是有意义的，因为我们预计年龄和教育是收入的重要预测因素。然而，我们发现只有 *~1/3rd* 的特征通过以下图示对模型有重大影响：
- en: '[PRE24]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![Strategies for improving classification models](img/B04881_05_34.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![提高分类模型策略](img/B04881_05_34.jpg)'
- en: Thus, it looks like the model is only able to learn information from a subet
    of features. We could potentially try to generate interaction features by making
    combinatorial labels (for example, a binary flag representing married and maximum
    education level as Master's Degree) by taking the product of all features with
    each other. Generating potential nonlinear features in this way is known as polynomial
    expansion, since we are taking single coefficient terms and converting them into
    products that have squared, cubic, or higher power relationships. However, for
    the purposes of this example, will try some alternative algorithms.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，看起来这个模型只能从特征的一个子集中学习信息。我们可能尝试通过组合标签生成交互特征（例如，一个表示已婚和最高教育水平为硕士学位的二进制标志）通过所有特征相互之间的乘积。以这种方式生成潜在的非线性特征被称为多项式展开，因为我们正在将单个系数项转换为具有平方、立方或更高幂关系的乘积。然而，为了本例的目的，我们将尝试一些替代算法。
- en: Separating Nonlinear boundaries with Support vector machines
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用支持向量机分离非线性边界
- en: In our previous example of logistic regression, we assumed implicitly that every
    point in the training set might be useful in defining the boundary between the
    two classes we are trying to separate. In practice we may only need a small number
    of data points to define this boundary, with additional information simply adding
    noise to the classification. This concept, that classification might be improved
    by using only a small number of critical data points, is the key features of the
    **support vector machine** (**SVM**) model.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前的逻辑回归示例中，我们隐含地假设训练集中的每个点都可能有助于定义我们试图分离的两个类之间的边界。在实践中，我们可能只需要少量数据点来定义这个边界，额外的信息只是给分类添加噪声。这个概念，即通过仅使用少量关键数据点来提高分类，是支持向量机（SVM）模型的关键特征。
- en: 'In its basic form, the SVM is similar to the linear models we have seen before,
    using the following equation:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在其基本形式中，支持向量机（SVM）与之前我们所见的线性模型相似，使用以下方程：
- en: '![Separating Nonlinear boundaries with Support vector machines](img/B04881_05_35.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![使用支持向量机分离非线性边界](img/B04881_05_35.jpg)'
- en: 'where `b` is an intercept, and β is the vector of coefficients such as we have
    seen in regression models. We can see a simple rule that a point `X` is classified
    as class `1` if `F(x) ≥ 1`, and class `-1` if `F(x) ≤ –1`. Geometrically, we can
    understand this as the distance from the plane to the point `x`, where β is a
    vector sitting orthogonal (at a right angle) to the plane. If the two classes
    are ideally separated, then the width between the two classes represented by 1/![Separating
    Nonlinear boundaries with Support vector machines](img/B04881_05_36.jpg) is as
    large as possible; thus, in finding the optimal value of β, we would to minimize
    the norm ![Separating Nonlinear boundaries with Support vector machines](img/B04881_05_36.jpg).
    At the same time, we want to minimize the error in assigning labels to the data.
    Thus, we can have a loss function that minimizes the tradeoff between these two
    objectives:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 `b` 是截距，β 是系数向量，正如我们在回归模型中所见。我们可以看到一条简单的规则，即点 `X` 被分类为类别 `1` 如果 `F(x) ≥ 1`，如果
    `F(x) ≤ –1` 则被分类为类别 `-1`。从几何上讲，我们可以理解为这是平面到点 `x` 的距离，其中 β 是一个垂直（成直角）于平面的向量。如果两个类别理想地分离，那么由
    1/![使用支持向量机分离非线性边界](img/B04881_05_36.jpg) 表示的两个类别之间的宽度尽可能大；因此，在寻找 β 的最优值时，我们希望最小化
    ![使用支持向量机分离非线性边界](img/B04881_05_36.jpg) 的范数。同时，我们希望最小化分配标签到数据中的错误。因此，我们可以有一个损失函数，它最小化这两个目标之间的权衡：
- en: '![Separating Nonlinear boundaries with Support vector machines](img/B04881_05_37.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![使用支持向量机分离非线性边界](img/B04881_05_37.jpg)'
- en: where `y` is the correct label of `x`. When `x` is correctly classified, `y(xβ+b)
    ≥ 1`, and we overall subtract from the values of `L`. Conversely, when we incorrectly
    predict `x`.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 `y` 是 `x` 的正确标签。当 `x` 被正确分类时，`y(xβ+b) ≥ 1`，并且我们从 `L` 的值中整体减去。相反，当我们错误地预测
    `x` 时。
- en: 'Note that the || here represent the Euclidean norm, or:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这里的 || 表示欧几里得范数，或：
- en: '![Separating Nonlinear boundaries with Support vector machines](img/B04881_05_38.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![使用支持向量机分离非线性边界](img/B04881_05_38.jpg)'
- en: '`y(xβ+b) < 1`, and we thus add to the value of `L`. If we want to minimize
    the value of `L`, we could find the optimal value of `β` and *b* by taking the
    derivative of this function and setting it to 0\. Starting with `β`:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '`y(xβ+b) < 1`，因此我们向`L`的值中添加。如果我们想最小化`L`的值，我们可以通过求这个函数的导数并将其设置为0来找到`β`和`b`的最优值。从`β`开始：'
- en: '![Separating Nonlinear boundaries with Support vector machines](img/B04881_05_39.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![使用支持向量机分离非线性边界](img/B04881_05_39.jpg)'
- en: 'Similarly, for `b`:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，对于`b`：
- en: '![Separating Nonlinear boundaries with Support vector machines](img/B04881_05_40.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![使用支持向量机分离非线性边界](img/B04881_05_40.jpg)'
- en: 'Plugging these back into the loss function equation we get:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些值代入损失函数方程，我们得到：
- en: '![Separating Nonlinear boundaries with Support vector machines](img/B04881_05_41.jpg)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![使用支持向量机分离非线性边界](img/B04881_05_41.jpg)'
- en: 'Two things are important here. First, only some of the `α` need to be nonzero.
    The rest can be set to `0`, meaning only a small number of points influence the
    choice of optimal model parameters. These points are the support vectors that
    give the algorithm its name, which lie along the boundary between the two classes.
    Note that in practice we would not use the above version of the error function,
    but rather a **soft-margin** formulation in which we use the **hinge loss**:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有两个重要的事情。首先，只有一些`α`需要非零。其余的可以设置为`0`，这意味着只有少数几个点会影响最优模型参数的选择。这些点是支持向量，它们位于两个类别的边界上。请注意，在实践中，我们不会使用上述错误函数版本，而是使用**软边界**公式，其中我们使用**铰链损失**：
- en: '![Separating Nonlinear boundaries with Support vector machines](img/B04881_05_42.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![使用支持向量机分离非线性边界](img/B04881_05_42.jpg)'
- en: 'This means that we only penalize points if they are on the wrong side of the
    separating hyperplane, and then by the magnitude of their misclassification error.
    This allows the SVM to be applied even in cases where the data is not linearly
    separable by allowing the algorithm to make mistakes according to the hinge loss
    penalty. For full details please consult references (Cortes, Corinna, and Vladimir
    Vapnik. **Support-vector networks**. Machine learning 20.3 (1995): 273-297; Burges,
    Christopher JC. *A tutorial on support vector machines for pattern recognition*.
    Data mining and knowledge discovery 2.2 (1998): 121-167.).'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '这意味着我们只有在点位于分离超平面的错误一侧时才会对其进行惩罚，并且根据它们的误分类误差的大小进行惩罚。这允许SVM在数据无法线性分离的情况下应用，允许算法根据铰链损失惩罚进行错误。有关详细信息，请参阅参考文献（Cortes,
    Corinna, and Vladimir Vapnik. **支持向量机**. Machine learning 20.3 (1995): 273-297;
    Burges, Christopher JC. *支持向量机在模式识别中的应用教程*. Data mining and knowledge discovery
    2.2 (1998): 121-167.）。'
- en: Second, we now see that the solution depends on the inputs *x* only through
    the product of individual points. In fact, we could replace this product with
    any function `K(xi,xj)`, where `K` is a so-called **kernel function** representing
    the similarity between `xi` and `xj`. This can be particularly useful when trying
    to capture nonlinear relationships between data points. For example, consider
    data points along a parabola in two-dimensional space, where `x2` (the vertical
    axis) is the square of `x1` (the horizontal). Normally, we could not draw a straight
    line to separate points above and below the parabola. However, if we first mapped
    the points using the function `x1`, `sqrt(x2)`, we can now linearly separate them.
    We saw the effectiveness of this nonlinear mapping in [Chapter 3](ch03.html "Chapter 3. Finding
    Patterns in the Noise – Clustering and Unsupervised Learning"), *Finding Patterns
    in the Noise – Clustering and Unsupervised Learning*, when we use the Gaussian
    Kernel to separate the nonlinear boundary between concentric circles using Spectral
    K-Means clustering.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们现在看到解只通过单个点的乘积依赖于输入*x*。实际上，我们可以用任何函数`K(xi,xj)`替换这个乘积，其中`K`是一个所谓的**核函数**，表示`xi`和`xj`之间的相似性。这在尝试捕捉数据点之间的非线性关系时特别有用。例如，考虑二维空间中抛物线上的数据点，其中`x2`（垂直轴）是`x1`（水平轴）的平方。通常，我们无法画一条直线来分离抛物线上下方的点。然而，如果我们首先使用函数`x1`，`sqrt(x2)`映射这些点，我们现在可以线性地分离它们。我们在[第3章](ch03.html
    "第3章. 在噪声中寻找模式 – 聚类和无监督学习")中看到了这种非线性映射的有效性，*在噪声中寻找模式 – 聚类和无监督学习*，当我们使用高斯核通过谱K-Means聚类分离同心圆之间的非线性边界时。
- en: 'Besides creating a linear decision boundary between data points that are not
    linearly separable in the input space, Kernel functions also allow us to calculate
    similarities between objects that have no vector representation, such as graphs
    (nodes and edges) or collections of words. The objects do not need to be the same
    length, either, as long as we can compute a similarity. These facts are due to
    a result known as Mercer''s Theorem, which guarantees that Kernel functions which
    are *>=0* for all pairs of inputs represent a valid inner product ![Separating
    Nonlinear boundaries with Support vector machines](img/B04881_05_43.jpg) between
    inputs x mapped into a linearly separable space using the mapping represented
    by φ (Hofmann, Thomas, Bernhard Schölkopf, and Alexander J. Smola. *Kernel methods
    in machine learning*. The annals of statistics (2008): 1171-1220). This mapping
    could be explicit, such as the square root function applied to the parabolic input
    in the example above. However, we do not actually need the mapping at all, since
    the kernel is guaranteed to represent the similarity between the mapped inputs.
    Indeed, the mapping could even be performed in an infinite-dimensional space that
    we can not explicitly represent, as is the case with the Gaussian kernel we will
    describe as follows.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '除了在输入空间中线性不可分的数据点之间创建线性决策边界之外，核函数还允许我们计算没有向量表示的对象之间的相似性，例如图（节点和边）或单词集合。这些对象也不需要具有相同的长度，只要我们能计算出一个相似性即可。这些事实归功于一个称为
    Mercer 定理的结果，该定理保证对于所有输入对，*>=0* 的核函数代表了一个有效的内积 ![使用支持向量机分离非线性边界](img/B04881_05_43.jpg)，将输入
    x 映射到由 φ 表示的线性可分空间中，φ 是一个映射（Hofmann, Thomas, Bernhard Schölkopf, 和 Alexander J.
    Smola. *机器学习中的核方法*. 统计学年刊 (2008): 1171-1220）。这种映射可以是显式的，例如在上面的例子中应用于抛物线输入的平方根函数。然而，我们实际上并不需要这种映射，因为核函数保证了能够表示映射输入之间的相似性。实际上，映射甚至可以在一个我们无法显式表示的无限维空间中执行，正如我们接下来要描述的高斯核函数那样。'
- en: Now that we have covered some of the intuition behind SVMs, let us see if it
    can improve the performance of our classification model by fitting an SVM to the
    data.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经了解了 SVM 的一些基本直觉，让我们看看通过将 SVM 应用于数据，它是否可以提高我们分类模型的表现。
- en: Fitting and SVM to the census data
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将 SVM 应用于人口普查数据
- en: 'For this example, we will try the default kernel function for the SVM model
    in scikit-learn, which is a Gaussian kernel, which you may recognize as the same
    equation used in a normal distribution function. We previously used the Gaussian
    Kernel in the context of Spectral Clustering in [Chapter 3](ch03.html "Chapter 3. Finding
    Patterns in the Noise – Clustering and Unsupervised Learning"), *Finding Patterns
    in the Noise – Clustering and Unsupervised Learning*, as a reminder, the formula
    is:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将尝试 scikit-learn 中 SVM 模型的默认核函数，这是一个高斯核函数，你可能认识它是正态分布函数中使用的相同方程。我们之前在
    [第 3 章](ch03.html "第 3 章. 在噪声中寻找模式 – 聚类和无监督学习") 的谱聚类上下文中使用了高斯核函数，*在噪声中寻找模式 – 聚类和无监督学习*，作为提醒，公式如下：
- en: '![Fitting and SVM to the census data](img/B04881_05_44.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![将 SVM 应用于人口普查数据](img/B04881_05_44.jpg)'
- en: In essence, this function translates the difference between two data points
    into the range `1` (when they are equal and the exponent becomes 0) and `0` (when
    the difference is very large and the exponent tends toward a very large negative
    number). The parameter *γ* represents the standard deviation, or bandwith, which
    controls how quickly the value of the function tends towards zero as the difference
    between the points increases. Small values of the bandwith will make the numerator
    a larger negative number, thus shrinking the kernel value to *0*.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，这个函数将两个数据点之间的差异转换到 `1`（当它们相等且指数变为 0 时）和 `0`（当差异非常大且指数趋向于一个非常大的负数时）。参数
    *γ* 代表标准差，或带宽，它控制函数值随着点之间差异的增加而趋向于零的速度。带宽较小的值将使分子成为一个更大的负数，从而将核值缩小到 *0*。
- en: 'As we mentioned preceding, the Gaussian kernel represented mapping the inputs
    *x* into an infinite dimensional space. We can see this if we expand the value
    of the kernel function using an infinite series:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，高斯核函数表示将输入 *x* 映射到一个无限维空间。如果我们通过一个无限级数展开核函数的值，我们可以看到这一点：
- en: '![Fitting and SVM to the census data](img/B04881_05_45.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![将 SVM 应用于人口普查数据](img/B04881_05_45.jpg)'
- en: Thus, the Gaussian kernel captures a similarity in an infinite dimensional features
    space.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，高斯核函数捕捉了无限维特征空间中的相似性。
- en: 'We fit the SVM model to the training data using the following commands:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下命令将 SVM 模型拟合到训练数据：
- en: '[PRE25]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'However, upon plotting the ROC curve for the results, we find that we have
    not improved very much over the logistic regression:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在绘制结果 ROC 曲线时，我们发现我们在逻辑回归上并没有取得很大的进步：
- en: '![Fitting and SVM to the census data](img/B04881_05_46.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![将 SVM 拟合到人口普查数据](img/B04881_05_46.jpg)'
- en: It may be difficult to see, but the red line in the upper left-hand corner of
    the image is the performance on the training set, while the blue line is the performance
    on the test set. Thus, we are in a situation such as we described previously,
    where the model almost perfectly predicts the training data but poorly generalizes
    to the test set.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 可能难以看清，但图像左上角的红色线条是训练集上的性能，而蓝色线条是测试集上的性能。因此，我们处于之前描述的那种情况，即模型几乎完美地预测了训练数据，但泛化到测试集上却表现不佳。
- en: 'In some sense, we were able to make progress because we used a nonlinear function
    to represent similarity within our data. However, the model now fits our data
    too well. If we wanted to experiment more with SVM models, we could tune a number
    of parameters: we could change kernel function, adjust the bandwidth of the Gaussian
    kernel (or the particular hyper parameters of whichever kernel function we chose),
    or tune the amount by which we penalize errors in classification. However, for
    our next step of algorithm optimization, we will instead switch gears and try
    to incorporate nonlinearity with many weak models instead of one overfit model,
    a concept known as boosting.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在某种意义上，我们能够取得进展是因为我们使用非线性函数来表示数据中的相似性。然而，现在的模型拟合我们的数据太好了。如果我们想更多地实验 SVM 模型，我们可以调整许多参数：我们可以改变核函数，调整高斯核的带宽（或我们选择的核函数的特定超参数），或者调整我们惩罚分类错误的程度。然而，对于我们的下一步算法优化，我们将改变方向，尝试用许多弱模型而不是一个过拟合的模型来引入非线性，这个概念被称为提升法。
- en: Boosting – combining small models to improve accuracy
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提升法 – 通过组合小型模型来提高准确性
- en: In the previous examples, we have implicitly assumed that there is a single
    model that can describe all the patterns present in our data set. What if, instead,
    a different model were best suited for a pattern represented by a subset of data,
    and only by combining models representing many of these smaller patterns can we
    can get an accurate picture? This is the intuition behind boosting—we start with
    a weak individual model, determine which points it correctly classifies, and fit
    additional models to compensate for points missed by this first model. While each
    additional model is also relatively poor on its own, by successively adding these
    weak models that capture a certain subset of the data, we gradually arrive at
    an accurate prediction overall. Furthermore, because each of the models in this
    group is fit to only a subset of the data, we have to worry less about over-fitting.
    While the general idea of boosting can be applied to many models, let us look
    at an example using the decision trees we covered in [Chapter 4](ch04.html "Chapter 4. Connecting
    the Dots with Models – Regression Methods"), *Connecting the Dots with Models
    – Regression Methods*.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的例子中，我们隐含地假设有一个单一的模型可以描述我们数据集中存在的所有模式。如果，相反，一个不同的模型最适合于数据子集表示的模式，并且只有通过组合代表许多这些较小模式的模型，我们才能得到一个准确的图像呢？这就是提升法的直觉——我们从一个弱个体模型开始，确定它正确分类的点，并为这个模型遗漏的点拟合额外的模型。虽然每个额外的模型本身也相对较差，但通过逐步添加这些捕捉数据某个子集的弱模型，我们逐渐达到整体准确的预测。此外，因为组中的每个模型都只拟合数据的一个子集，所以我们不必过于担心过拟合。虽然提升法的基本思想可以应用于许多模型，但让我们看看一个例子，使用我们在[第
    4 章](ch04.html "第 4 章。用模型连接点 – 回归方法")中介绍的决策树，*用模型连接点 – 回归方法*。
- en: Gradient boosted decision trees
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度提升决策树
- en: 'Recall that in [Chapter 4](ch04.html "Chapter 4. Connecting the Dots with Models
    – Regression Methods"), *Connecting the Dots with Models – Regression Methods*,
    we achieved greater predictive power in our regression task by averaging over
    a set of trees with random features. Gradient boosted decision trees (Breiman,
    Leo. Arcing the edge. Technical Report 486, Statistics Department, University
    of California at Berkeley, 1997; Friedman, Jerome H. *Greedy function approximation:
    a gradient boosting machine*. Annals of statistics (2001): 1189-1232; Friedman,
    Jerome H. *Stochastic gradient boosting*. Computational Statistics and Data Analysis
    38.4 (2002): 367-378.) follow a similar strategy, but instead of choosing random
    features with each step, we greedily optimize at each point. The general algorithm
    is:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '回想一下，在[第4章](ch04.html "第4章. 通过模型连接点 – 回归方法")中，*通过模型连接点 – 回归方法*，我们通过在具有随机特征的树集合上平均来提高了回归任务的预测能力。梯度提升决策树（Breiman,
    Leo. Arcing the edge. 技术报告486，加州大学伯克利分校统计学系，1997；Friedman, Jerome H. *贪婪函数逼近：梯度提升机*.
    统计学年鉴（2001）：1189-1232；Friedman, Jerome H. *随机梯度提升*. 计算统计学与数据分析 38.4 (2002): 367-378。）遵循类似的策略，但不是在每一步选择随机特征，而是在每个点上贪婪地优化。通用算法如下：'
- en: Start with a constant value, such as the average response across the input data.
    This is the baseline model, *F0*.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从一个常数值开始，例如输入数据中的平均响应值。这是基线模型，*F0*。
- en: Fit a decision tree *h* to the training data, usually limiting it to have very
    shallow depth, with the target as the **pseudo-residuals** for each point `i`
    given by:![Gradient boosted decision trees](img/B04881_05_50.jpg)
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将决策树*h*拟合到训练数据上，通常限制其深度非常浅，目标是将每个点`i`的**伪残差**作为目标，由以下公式给出：![梯度提升决策树](img/B04881_05_50.jpg)
- en: Conceptually, the pseudo-residual for a given loss function L (such as the squared
    error that we studied in [Chapter 4](ch04.html "Chapter 4. Connecting the Dots
    with Models – Regression Methods"), *Connecting the Dots with Models – Regression
    Methods* or the hinge loss for the SVM described above) is the derivative of the
    loss function with respect to the value of the current model *F* at a point `yi`.
    While a standard residual would just give the difference between the predicted
    and observed value, the pseudo-residual represents how rapidly the loss is changing
    at a given point, and thus in what direction we need to move the model parameters
    to better classify this point.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从概念上讲，对于给定的损失函数L（例如我们在[第4章](ch04.html "第4章. 通过模型连接点 – 回归方法")中研究的平方误差，或上述SVM的hinge损失）的伪残差是损失函数相对于当前模型*F*在点`yi`的值的导数。虽然标准残差只是预测值和观察值之间的差异，但伪残差表示损失在给定点的变化速度，以及我们需要将模型参数移动的方向以更好地分类这个点。
- en: To step 1, add the value of the tree in step 2 multiplied by an optimal step
    `γ` and a learning rate `α`:![Gradient boosted decision trees](img/B04881_05_47.jpg)
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 步骤1：将步骤2中树的值乘以一个最优步长`γ`和学习率`α`：![梯度提升决策树](img/B04881_05_47.jpg)
- en: We could either choose a `γ` that is optimal for the whole tree, or for each
    individual leaf node, and we can determine the optimal value using a method such
    as the Newton optimization we discussed above.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以选择一个对整个树最优的`γ`值，或者对每个单独的叶节点，我们可以使用如上所述的牛顿优化方法来确定最优值。
- en: Repeat steps 1–3 until convergence.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤1-3，直到收敛。
- en: 'The goal is that by fitting several weaker trees, in aggregate they make better
    predictions as they sequentially are fit to compensate for the remaining residuals
    in the model at each step. In practice, we also choose only a subset of the training
    data to fit the trees at each stage, which should further reduce the possibility
    of over-fitting. Let us examine this theory on our dataset by fitting a model
    with 200 trees with a maximum depth of 5:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是通过拟合多个较弱的树，它们在逐步拟合时总体上能做出更好的预测，以补偿模型在每个步骤中剩余的残差。在实践中，我们也在每个阶段只选择训练数据的一个子集来拟合树，这应该进一步减少过拟合的可能性。让我们通过拟合一个具有200棵树和最大深度为5的模型来检验这个理论：
- en: '[PRE26]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now, when we plot the results, we see a remarkable increase in accuracy on
    the test set:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当我们绘制结果时，我们看到测试集上的准确率有显著提高：
- en: '[PRE27]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![Gradient boosted decision trees](img/B04881_05_48.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![梯度提升决策树](img/B04881_05_48.jpg)'
- en: 'Similar to the random forest model, we can examine the importance of features
    as determined by the loss in accuracy upon shuffling their values among data points:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 与随机森林模型类似，我们可以通过在数据点之间随机打乱它们的值来检查特征的重要性，这会影响准确率的损失：
- en: '[PRE28]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Note that this is not directly comparable to the same evaluation we performed
    for the logistic regression model as the importance here is not determined by
    whether the feature predicts positively or negatively, which is implicit in the
    sign of the coefficients in the logistic regression.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这与我们对逻辑回归模型进行的相同评估并不直接可比，因为这里的重要性不是由特征是否预测正面或负面决定的，这在逻辑回归系数的符号中是隐含的。
- en: 'Also note that there is a subtler problem here with interpreting the output
    coefficients: many of our features are actually individual categories of a common
    feature, such as country of origin or education level. What we are really interested
    in is the importance of the overall feature, not the individual levels. Thus,
    to quantify feature importance more accurately, we could average the importance
    over all columns containing categories belonging to a common feature.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，在解释输出系数时存在一个更微妙的问题：我们中的许多特征实际上是共同特征的个别类别，例如原产国或教育水平。我们真正感兴趣的是整体特征的重要性，而不是个别级别。因此，为了更准确地量化特征重要性，我们可以对包含属于共同特征的类别列的平均重要性进行平均。
- en: If we wanted to further tune the performance of the gbm model, we could perform
    a search of different values for the number of trees, the depth of those trees,
    the learning rate (`α` in the formulas above), and `min_samples_leaf` (which determines
    the minimum number of data points that need to be present to split the data form
    a bottom-most split, or leaf, in the tree), among others. As a rule of thumb,
    making deeper trees will increase the risk of over-fitting, but shallower trees
    will requires a larger number of models to achieve good accuracy. Similarly, a
    lower learning rate will also control over-fitting by reducing the contribution
    of any single tree to the model score, but again may require a tradeoff in more
    models to achieve the desired level of predictive accuracy. The balance between
    these parameters may be guided both by the application (how accurate the model
    should be to contribute meaningfully to a business problem) as well as performance
    considerations (if the model needs to run online in a website, for example, a
    smaller number of trees that occupy less memory may be beneficial and worth a
    somewhat reduced accuracy).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要进一步调整gbm模型的性能，我们可以搜索不同数量的树、树的深度、学习率（公式上方的`α`）和`min_samples_leaf`（它决定了需要存在于数据中以便从树的底部分裂（或叶子）的最小数据点数），以及其他一些参数。作为一个经验法则，使树更深会增加过拟合的风险，但较浅的树需要更多的模型来实现良好的准确度。同样，较低的学习率也会通过减少单个树对模型得分的贡献来控制过拟合，但可能需要更多的模型来达到所需的预测准确度。这些参数之间的平衡可能既受应用（模型应该有多准确才能对业务问题产生有意义的贡献）的影响，也受性能考虑（例如，如果模型需要在网站上在线运行，那么占用较少内存的较少树可能是有益的，并且值得略微降低准确度）。
- en: Comparing classification methods
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较分类方法
- en: In this chapter we have examined classification using logistic regression, support
    vector machines, and gradient boosted decision trees. In what scenarios should
    we prefer one algorithm over another?
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了使用逻辑回归、支持向量机和梯度提升决策树进行分类。在什么情况下我们应该优先选择一种算法而不是另一种？
- en: For logistic regression, the data ideally will be linearly separable (the exponent
    in the formula for the logistic regression, after all, is essentially the same
    as the SVM equation for a separating hyperplane). If our goal is inference (producing
    a unit increase in response per 1-unit increase of input measurement, as we described
    in [Chapter 1](ch01.html "Chapter 1. From Data to Decisions – Getting Started
    with Analytic Applications"), *From Data to Decisions – Getting Started with Analytic
    Applications*) then the coefficients and log-odds values will be helpful. The
    stochastic gradient method can also be helpful in cases where we are unable to
    process all the data concurrently, while the second order methods we discussed
    may be easier to employ on un-normalized data. Finally, in the context of serializing
    model parameters and using these results to score new data, the logistic regression
    is attractive in that it is represented by a vector of numbers and is thus easily
    stored.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 对于逻辑回归，数据理想情况下将是线性可分的（毕竟，逻辑回归公式中的指数本质上与支持向量机（SVM）的分离超平面方程相同）。如果我们的目标是推理（在输入测量每增加1个单位时产生响应单位增加，正如我们在[第1章](ch01.html
    "第1章. 从数据到决策 – 分析应用入门")中描述的，*从数据到决策 – 分析应用入门*），那么系数和对数几率值将是有帮助的。在无法同时处理所有数据的情况下，随机梯度下降法也可能是有帮助的，而我们在讨论的第二阶方法可能更容易应用于未归一化的数据。最后，在序列化模型参数和使用这些结果对新数据进行评分的背景下，逻辑回归因其表示为一个数字向量并且易于存储而具有吸引力。
- en: Support vector machines, as we discussed, can accommodate complex nonlinear
    boundaries between inputs. They can also be used on data without a vector representation,
    or data of different lengths, making them quite flexible. However, they require
    more computational resources for fitting as well as scoring.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们讨论的，支持向量机（SVM）可以适应输入之间的复杂非线性边界。它们也可以用于没有向量表示的数据，或者长度不同的数据，这使得它们相当灵活。然而，它们在拟合以及评分时需要更多的计算资源。
- en: Gradient boosted decision trees can fit nonlinear boundaries between inputs,
    but only certain kinds. Consider that a decision tree splits a dataset into two
    groups at each decision node. Thus, the resulting boundaries represent a series
    of hyperplanes in the m-dimensional space of the dataset, but only split along
    a particular dimension at each pass and only in a straight line. Thus, these planes
    will not necessarily capture the nonlinearity possible with the SVM, but if the
    data can be separated in this piecewise fashion a GBM may perform well.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升决策树可以在输入之间拟合非线性边界，但仅限于某些类型。考虑到决策树在每个决策节点将数据集分为两组。因此，产生的边界代表数据集 m 维空间中的一系列超平面，但每次只沿特定维度分割，并且只沿直线分割。因此，这些平面不一定能够捕捉到支持向量机（SVM）可能具有的非线性，但如果数据可以以这种方式分割，GBM
    可能会表现良好。
- en: The flowchart below gives a general overview from choosing among the classification
    methods we have discussed. Also, keep in mind that the Random Forest algorithm
    we discussed in [Chapter 4](ch04.html "Chapter 4. Connecting the Dots with Models
    – Regression Methods"), *Connecting the Dots with Models – Regression Methods*
    may also be applied for classification, while the SVM and GBM models describe
    in this chapter have forms that may be applied for regression.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的流程图概述了从我们讨论的分类方法中选择的一般情况。同时，请记住，我们在[第4章](ch04.html "第4章. 通过模型连接点 – 回归方法")中讨论的随机森林算法，*通过模型连接点
    – 回归方法*也可以用于分类，而本章中描述的支持向量机（SVM）和梯度提升树（GBM）模型具有可能用于回归的形式。
- en: '![Comparing classification methods](img/B04881_05_49.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![比较分类方法](img/B04881_05_49.jpg)'
- en: 'Case study: fitting classifier models in pyspark'
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究：在 PySpark 中拟合分类器模型
- en: 'Now that we have examined several algorithms for fitting classifier models
    in the scikit-learn library, let us look at how we might implement a similar model
    in PySpark. We can use the same census dataset from earlier in this chapter, and
    start by loading the data using a textRdd after starting the spark context:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经检查了 scikit-learn 库中用于拟合分类器模型的几个算法，让我们看看我们如何在 PySpark 中实现一个类似模型。我们可以使用本章前面提到的相同的普查数据集，并从启动
    spark 上下文后使用 textRdd 加载数据开始：
- en: '[PRE29]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Next we need to split the data into individual fields, and strip whitespace
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要将数据分割成单个字段，并去除空白字符
- en: '[PRE30]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Now, as before, we need to determine which of our features are categorical
    and need to be re-encoded using one-hot encoding. We do this by taking a single
    row and asking whether the string in each position represent a digit (is not a
    categorical variable):'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，就像之前一样，我们需要确定我们的哪些特征是分类的，需要使用独热编码重新编码。我们通过取单一行并询问每个位置的字符串是否代表一个数字（不是一个分类变量）来完成此操作：
- en: '[PRE31]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now, as before, we need to collect a dictionary representing the string-to-position
    mapping of each categorical label to a place in the one-hot-encoding vector:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，就像之前一样，我们需要收集一个字典，表示每个分类标签到独热编码向量位置的字符串到位置的映射：
- en: '[PRE32]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Next, we calculate what the total length of the one-hot encoding vector should
    be to represent all the features. We subtract two from this value because the
    last categorical features is income, which has two values and which we use as
    the label for the data:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们计算表示所有特征的独热编码向量的总长度。我们从该值中减去两个，因为最后一个分类特征是收入，它有两个值，我们将其用作数据的标签：
- en: '[PRE33]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now, we use a map function to turn all of our data into labeled point objects
    for use in logistic regression. To do so, we extract the label for each row from
    the last element in the vector, then instantiate an empty vector using the length
    of the one-hot-encoded feature set we calculated preceding. We use two indices:
    one for which categorical variable we are accessing (to index the right dictionary
    to perform our mapping), and a second to record where in the feature vector we
    are (since for categorical variables we will skip over k spaces for a given variable,
    where k is the number of categories in that variable).'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们使用映射函数将所有数据转换为用于逻辑回归的标记点对象。为此，我们从向量的最后一个元素提取每行的标签，然后使用我们之前计算的独热编码特征集的长度实例化一个空向量。我们使用两个索引：一个用于访问哪个分类变量（以索引正确的字典执行映射），另一个用于记录在特征向量中的位置（因为对于分类变量，我们将跳过给定变量的k个空间，其中k是该变量的类别数）。
- en: '[PRE34]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: We apply this function to all data points
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将此函数应用于所有数据点
- en: '[PRE35]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Now that our data is in the right format, we can run logistic regression:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们数据格式正确，我们可以运行逻辑回归：
- en: '[PRE36]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'To access the weights from the resulting model, we can inspect the weights
    parameter:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问结果模型的权重，我们可以检查权重参数：
- en: '[PRE37]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: If we wanted to apply the generated model to a new dataset, we can use the `predict()`
    method of `censusLogistic` on a new feature vector. The steps described above
    are similar to the data processing we used for the scikit-learn example, but can
    ultimately scale to larger datasets.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想将生成的模型应用于新的数据集，我们可以在新的特征向量上使用`censusLogistic`的`predict()`方法。上述步骤与我们用于scikit-learn示例的数据处理步骤类似，但最终可以扩展到更大的数据集。
- en: Summary
  id: totrans-254
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you've examined how to use classification models and some of
    the strategies for improving model performance. In addition to transforming categorical
    features, you've looked at the interpretation of logistic regression accuracy
    using the ROC curve. In attempting to improve model performance, we demonstrated
    the use of SVMs and were able to increase performance on the training set the
    cost of overfitting. Finally, we were able to achieve good performance on the
    test set through gradient boosted decision trees. Taken together with the material
    in [Chapter 4](ch04.html "Chapter 4. Connecting the Dots with Models – Regression
    Methods"), *Connecting the Dots with Models – Regression Methods*, you should
    now have a full toolkit of methods for continuous and categorical outcomes, which
    you can apply to problems in main domains.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了如何使用分类模型以及一些提高模型性能的策略。除了转换分类特征外，你还研究了使用ROC曲线解释逻辑回归准确性的方法。在尝试提高模型性能时，我们展示了SVMs的使用，并能够在训练集上提高性能，尽管代价是过拟合。最后，我们通过梯度提升决策树在测试集上实现了良好的性能。结合[第4章](ch04.html
    "第4章。通过模型连接点 – 回归方法")中的材料，“通过模型连接点 – 回归方法”，你现在应该拥有一套完整的方法，可以应用于连续和分类结果的问题，这些问题可以在主要领域中使用。
