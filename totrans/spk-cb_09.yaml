- en: Chapter 9. Unsupervised Learning with MLlib
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will cover how we can do unsupervised learning using MLlib, Spark's
    machine learning library.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter is divided into the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Clustering using k-means
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dimensionality reduction with principal component analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dimensionality reduction with singular value decomposition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following is Wikipedia''s definition of unsupervised learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '*"In machine learning, the problem of unsupervised learning is that of trying
    to find hidden structure in unlabeled data."*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'In contrast to supervised learning where we have labeled data to train an algorithm,
    in unsupervised learning we ask the algorithm to find a structure on its own.
    Let''s take a look at the following sample dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Introduction](img/3056_09_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As you can see from the preceding graph, the data points are forming two clusters
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Introduction](img/3056_09_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In fact, clustering is the most common type of unsupervised learning algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering using k-means
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cluster analysis or clustering is the process of grouping data into multiple
    groups so that the data in one group is similar to the data in other groups.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are a few examples where clustering is used:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Market segmentation**: Dividing the target market into multiple segments
    so that the needs of each segment can be served better'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Social network analysis**: Finding a coherent group of people in the social
    network for ad targeting through a social networking site such as Facebook'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data center computing clusters**: Putting a set of computers together to
    improve performance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Astronomical data analysis**: Understanding astronomical data and events
    such as galaxy formations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Real estate**: Identifying neighborhoods based on similar features'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text analysis**: Dividing text documents, such as novels or essays, into
    genres'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The k-means algorithm is best illustrated using imagery, so let''s look at
    our sample figure again:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Clustering using k-means](img/3056_09_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The first step in k-means is to randomly select two points called **cluster
    centroids**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Clustering using k-means](img/3056_09_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The k-means algorithm is an iterative algorithm and works in two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cluster assignment step**: This algorithm will go through each data point
    and, depending upon which centroid it is nearer to, it will be assigned that centroid
    and, in turn, the cluster it represents'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Move centroid step**: This algorithm will take each centroid and move it
    to the mean of the data points in the cluster'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s see how our data looks after the cluster assignment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Clustering using k-means](img/3056_09_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now let''s move the cluster centroids to the mean value of the data points
    in a cluster, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Clustering using k-means](img/3056_09_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In this case, one iteration is enough and further iterations will not move the
    cluster centroids. For most real data, multiple iterations are required to move
    the centroid to the final position.
  prefs: []
  type: TYPE_NORMAL
- en: The k-means algorithm takes a number of clusters as input.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s use some different housing data from the City of Saratoga, CA. This
    time, we are going to take lot size and house price:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Lot size | House price (in $1,000) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 12839 | 2405 |'
  prefs: []
  type: TYPE_TB
- en: '| 10000 | 2200 |'
  prefs: []
  type: TYPE_TB
- en: '| 8040 | 1400 |'
  prefs: []
  type: TYPE_TB
- en: '| 13104 | 1800 |'
  prefs: []
  type: TYPE_TB
- en: '| 10000 | 2351 |'
  prefs: []
  type: TYPE_TB
- en: '| 3049 | 795 |'
  prefs: []
  type: TYPE_TB
- en: '| 38768 | 2725 |'
  prefs: []
  type: TYPE_TB
- en: '| 16250 | 2150 |'
  prefs: []
  type: TYPE_TB
- en: '| 43026 | 2724 |'
  prefs: []
  type: TYPE_TB
- en: '| 44431 | 2675 |'
  prefs: []
  type: TYPE_TB
- en: '| 40000 | 2930 |'
  prefs: []
  type: TYPE_TB
- en: '| 1260 | 870 |'
  prefs: []
  type: TYPE_TB
- en: '| 15000 | 2210 |'
  prefs: []
  type: TYPE_TB
- en: '| 10032 | 1145 |'
  prefs: []
  type: TYPE_TB
- en: '| 12420 | 2419 |'
  prefs: []
  type: TYPE_TB
- en: '| 69696 | 2750 |'
  prefs: []
  type: TYPE_TB
- en: '| 12600 | 2035 |'
  prefs: []
  type: TYPE_TB
- en: '| 10240 | 1150 |'
  prefs: []
  type: TYPE_TB
- en: '| 876 | 665 |'
  prefs: []
  type: TYPE_TB
- en: '| 8125 | 1430 |'
  prefs: []
  type: TYPE_TB
- en: '| 11792 | 1920 |'
  prefs: []
  type: TYPE_TB
- en: '| 1512 | 1230 |'
  prefs: []
  type: TYPE_TB
- en: '| 1276 | 975 |'
  prefs: []
  type: TYPE_TB
- en: '| 67518 | 2400 |'
  prefs: []
  type: TYPE_TB
- en: '| 9810 | 1725 |'
  prefs: []
  type: TYPE_TB
- en: '| 6324 | 2300 |'
  prefs: []
  type: TYPE_TB
- en: '| 12510 | 1700 |'
  prefs: []
  type: TYPE_TB
- en: '| 15616 | 1915 |'
  prefs: []
  type: TYPE_TB
- en: '| 15476 | 2278 |'
  prefs: []
  type: TYPE_TB
- en: '| 13390 | 2497.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 1158 | 725 |'
  prefs: []
  type: TYPE_TB
- en: '| 2000 | 870 |'
  prefs: []
  type: TYPE_TB
- en: '| 2614 | 730 |'
  prefs: []
  type: TYPE_TB
- en: '| 13433 | 2050 |'
  prefs: []
  type: TYPE_TB
- en: '| 12500 | 3330 |'
  prefs: []
  type: TYPE_TB
- en: '| 15750 | 1120 |'
  prefs: []
  type: TYPE_TB
- en: '| 13996 | 4100 |'
  prefs: []
  type: TYPE_TB
- en: '| 10450 | 1655 |'
  prefs: []
  type: TYPE_TB
- en: '| 7500 | 1550 |'
  prefs: []
  type: TYPE_TB
- en: '| 12125 | 2100 |'
  prefs: []
  type: TYPE_TB
- en: '| 14500 | 2100 |'
  prefs: []
  type: TYPE_TB
- en: '| 10000 | 1175 |'
  prefs: []
  type: TYPE_TB
- en: '| 10019 | 2047.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 48787 | 3998 |'
  prefs: []
  type: TYPE_TB
- en: '| 53579 | 2688 |'
  prefs: []
  type: TYPE_TB
- en: '| 10788 | 2251 |'
  prefs: []
  type: TYPE_TB
- en: '| 11865 | 1906 |'
  prefs: []
  type: TYPE_TB
- en: 'Let''s convert this data into a **comma-separated value** (**CSV**) file called
    `saratoga.c` `sv` and draw it as a scatter plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](img/3056_09_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Finding a number of clusters is a tricky task. Here, we have the advantage
    of visual inspection, which is not available for data on hyperplanes (more than
    three dimensions). Let''s roughly divide the data into four clusters as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](img/3056_09_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We will run the k-means algorithm to do the same and see how close our results
    come.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Load `sarataga.csv` to HDFS:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Start the Spark shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import statistics and related classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load `saratoga.csv` as an RDD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Transform the data into an RDD of dense vectors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the model for four clusters and five iterations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Collect `parsedData` as a local scala collection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Predict the cluster for the 0th element:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now let''s compare the cluster assignments by k-means versus the ones we have
    done individually. The k-means algorithm gives the cluster IDs starting from 0\.
    Once you inspect the data, you find out the following mapping between the A to
    D cluster IDs we gave versus k-means: A=>3, B=>1, C=>0, D=>2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, let's pick some of the data from different parts of the chart and predict
    which cluster it belongs to.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s look at the house (18) data, which has a lot size of 876 sq ft and is
    priced at $665K:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, look at the data for house (35) with a lot size of 15,750 sq ft and a
    price of $1.12 million:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now look at the house (6) data, which has a lot size of 38,768 sq ft and is
    priced at $2.725 million:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now look at the house (15) data, which has a lot size of 69,696 sq ft and is
    priced at $2.75 million:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You can test the prediction capability with more data. Let's do some neighborhood
    analysis to see what meaning these clusters carry. Most of the houses in cluster
    3 are near downtown. The cluster 2 houses are on hilly terrain.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we dealt with a very small set of features; common sense and
    visual inspection would also lead us to the same conclusions. The beauty of the
    k-means algorithm is that it does the clustering on the data with an unlimited
    number of features. It is a great tool to use when you have a raw data and would
    like to know the patterns in that data.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction with principal component analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dimensionality reduction is the process of reducing the number of dimensions
    or features. A lot of real data contains a very high number of features. It is
    not uncommon to have thousands of features. Now, we need to drill down to features
    that matter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Dimensionality reduction serves several purposes such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Data compression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the number of dimensions is reduced, it reduces the disk footprint and
    memory footprint. Last but not least; it helps algorithms to run much faster.
    It also helps reduce highly correlated dimensions to one.
  prefs: []
  type: TYPE_NORMAL
- en: Humans can only visualize three dimensions, but data can have a much higher
    number of dimensions. Visualization can help find hidden patterns in the data.
    Dimensionality reduction helps visualization by compacting multiple features into
    one.
  prefs: []
  type: TYPE_NORMAL
- en: The most popular algorithm for dimensionality reduction is **principal component
    analysis** (**PCA**).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the following dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dimensionality reduction with principal component analysis](img/3056_09_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s say the goal is to divide this two-dimensional data into one dimension.
    The way to do that would be to find a line on which we can project this data.
    Let''s find a line that is good for projecting this data on:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dimensionality reduction with principal component analysis](img/3056_09_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This is the line that has the shortest projected distance from the data points.
    Let''s explain it further by dropping the shortest lines from each data point
    to this projected line:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dimensionality reduction with principal component analysis](img/3056_09_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Another way to look at it is that we have to find a line to project the data
    on so that the sum of the square distances of the data points from this line is
    minimized. These gray line segments are also called **projection errors**.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's look at the three features of the housing data of the City of Saratoga,
    CA—that is, house size, lot size, and price. Using PCA, we will merge the house
    size and lot size features into one feature— *z*. Let's call this feature **z
    density of a house**.
  prefs: []
  type: TYPE_NORMAL
- en: It is worth noting that it is not always possible to give meaning to the new
    feature created. In this case, it is easy as we have only two features to combine
    and we can use our common sense to combine the effect of the two. In a more practical
    case, you may have 1,000 features that you are trying to project to 100 features.
    It may not be possible to give real-life meaning to each of those 100 features.
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, we will derive the housing density using PCA and then we will
    do linear regression to see how this density affects the house price.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a preprocessing stage before we delve into PCA: **feature scaling**.
    Feature scaling comes into the picture when two features have ranges that are
    at very different scales. Here, house size varies in the range of 800 sq ft to
    7,000 sq ft, while the lot size varies between 800 sq ft to a few acres.'
  prefs: []
  type: TYPE_NORMAL
- en: Why did we not have to do feature scaling before? The answer is that we really
    did not have to put features on a level playing field. Gradient descent is another
    area where feature scaling is very useful.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are different ways of doing feature scaling:'
  prefs: []
  type: TYPE_NORMAL
- en: Dividing a feature value with a maximum value that will put every feature in
    the ![Getting ready](img/3056_09_22.jpg) range
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dividing a feature value with the range, that is, maximum value - minimum value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Subtracting a feature value by its mean and then dividing by the range
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Subtracting a feature value by its mean and then dividing by the standard deviation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We are going to use the fourth choice to scale in the best way possible. The
    following is the data we are going to use for this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '| House size | Lot size | Scaled house size | Scaled lot size | House price
    (in $1,000) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 2524 | 12839 | -0.025 | -0.231 | 2405 |'
  prefs: []
  type: TYPE_TB
- en: '| 2937 | 10000 | 0.323 | -0.4 | 2200 |'
  prefs: []
  type: TYPE_TB
- en: '| 1778 | 8040 | -0.654 | -0.517 | 1400 |'
  prefs: []
  type: TYPE_TB
- en: '| 1242 | 13104 | -1.105 | -0.215 | 1800 |'
  prefs: []
  type: TYPE_TB
- en: '| 2900 | 10000 | 0.291 | -0.4 | 2351 |'
  prefs: []
  type: TYPE_TB
- en: '| 1218 | 3049 | -1.126 | -0.814 | 795 |'
  prefs: []
  type: TYPE_TB
- en: '| 2722 | 38768 | 0.142 | 1.312 | 2725 |'
  prefs: []
  type: TYPE_TB
- en: '| 2553 | 16250 | -0.001 | -0.028 | 2150 |'
  prefs: []
  type: TYPE_TB
- en: '| 3681 | 43026 | 0.949 | 1.566 | 2724 |'
  prefs: []
  type: TYPE_TB
- en: '| 3032 | 44431 | 0.403 | 1.649 | 2675 |'
  prefs: []
  type: TYPE_TB
- en: '| 3437 | 40000 | 0.744 | 1.385 | 2930 |'
  prefs: []
  type: TYPE_TB
- en: '| 1680 | 1260 | -0.736 | -0.92 | 870 |'
  prefs: []
  type: TYPE_TB
- en: '| 2260 | 15000 | -0.248 | -0.103 | 2210 |'
  prefs: []
  type: TYPE_TB
- en: '| 1660 | 10032 | -0.753 | -0.398 | 1145 |'
  prefs: []
  type: TYPE_TB
- en: '| 3251 | 12420 | 0.587 | -0.256 | 2419 |'
  prefs: []
  type: TYPE_TB
- en: '| 3039 | 69696 | 0.409 | 3.153 | 2750 |'
  prefs: []
  type: TYPE_TB
- en: '| 3401 | 12600 | 0.714 | -0.245 | 2035 |'
  prefs: []
  type: TYPE_TB
- en: '| 1620 | 10240 | -0.787 | -0.386 | 1150 |'
  prefs: []
  type: TYPE_TB
- en: '| 876 | 876 | -1.414 | -0.943 | 665 |'
  prefs: []
  type: TYPE_TB
- en: '| 1889 | 8125 | -0.56 | -0.512 | 1430 |'
  prefs: []
  type: TYPE_TB
- en: '| 4406 | 11792 | 1.56 | -0.294 | 1920 |'
  prefs: []
  type: TYPE_TB
- en: '| 1885 | 1512 | -0.564 | -0.905 | 1230 |'
  prefs: []
  type: TYPE_TB
- en: '| 1276 | 1276 | -1.077 | -0.92 | 975 |'
  prefs: []
  type: TYPE_TB
- en: '| 3053 | 67518 | 0.42 | 3.023 | 2400 |'
  prefs: []
  type: TYPE_TB
- en: '| 2323 | 9810 | -0.195 | -0.412 | 1725 |'
  prefs: []
  type: TYPE_TB
- en: '| 3139 | 6324 | 0.493 | -0.619 | 2300 |'
  prefs: []
  type: TYPE_TB
- en: '| 2293 | 12510 | -0.22 | -0.251 | 1700 |'
  prefs: []
  type: TYPE_TB
- en: '| 2635 | 15616 | 0.068 | -0.066 | 1915 |'
  prefs: []
  type: TYPE_TB
- en: '| 2298 | 15476 | -0.216 | -0.074 | 2278 |'
  prefs: []
  type: TYPE_TB
- en: '| 2656 | 13390 | 0.086 | -0.198 | 2497.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 1158 | 1158 | -1.176 | -0.927 | 725 |'
  prefs: []
  type: TYPE_TB
- en: '| 1511 | 2000 | -0.879 | -0.876 | 870 |'
  prefs: []
  type: TYPE_TB
- en: '| 1252 | 2614 | -1.097 | -0.84 | 730 |'
  prefs: []
  type: TYPE_TB
- en: '| 2141 | 13433 | -0.348 | -0.196 | 2050 |'
  prefs: []
  type: TYPE_TB
- en: '| 3565 | 12500 | 0.852 | -0.251 | 3330 |'
  prefs: []
  type: TYPE_TB
- en: '| 1368 | 15750 | -0.999 | -0.058 | 1120 |'
  prefs: []
  type: TYPE_TB
- en: '| 5726 | 13996 | 2.672 | -0.162 | 4100 |'
  prefs: []
  type: TYPE_TB
- en: '| 2563 | 10450 | 0.008 | -0.373 | 1655 |'
  prefs: []
  type: TYPE_TB
- en: '| 1551 | 7500 | -0.845 | -0.549 | 1550 |'
  prefs: []
  type: TYPE_TB
- en: '| 1993 | 12125 | -0.473 | -0.274 | 2100 |'
  prefs: []
  type: TYPE_TB
- en: '| 2555 | 14500 | 0.001 | -0.132 | 2100 |'
  prefs: []
  type: TYPE_TB
- en: '| 1572 | 10000 | -0.827 | -0.4 | 1175 |'
  prefs: []
  type: TYPE_TB
- en: '| 2764 | 10019 | 0.177 | -0.399 | 2047.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 7168 | 48787 | 3.887 | 1.909 | 3998 |'
  prefs: []
  type: TYPE_TB
- en: '| 4392 | 53579 | 1.548 | 2.194 | 2688 |'
  prefs: []
  type: TYPE_TB
- en: '| 3096 | 10788 | 0.457 | -0.353 | 2251 |'
  prefs: []
  type: TYPE_TB
- en: '| 2003 | 11865 | -0.464 | -0.289 | 1906 |'
  prefs: []
  type: TYPE_TB
- en: Let's take the scaled house size and scaled house price data and save it as
    `scaledhousedata.csv`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Load `scaledhousedata.csv` to HDFS:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Start the Spark shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import statistics and related classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load `saratoga.csv` as an RDD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Transform the data into an RDD of dense vectors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a `RowMatrix` from `parsedData`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compute one principal component:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Project the rows to the linear space spanned by the principal component:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert the projected `RowMatrix` back to the RDD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Save `projectedRDD` back to HDFS:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we will use this projected feature, which we decided to call housing density,
    plot it against the house price, and see whether any new pattern emerges:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the HDFS directory `phdata` to the local directory `phdata`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Trim start and end brackets in the data and load the data into MS Excel, next
    to the house price.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following is the plot of the house price versus the housing density:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](img/3056_09_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s draw some patterns in this data as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How to do it…](img/3056_09_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: What patterns do we see here? For moving from a very high-density to low-density
    housing, people are ready to pay a heavy premium. As the housing density reduces,
    this premium flattens out. For example, people will pay a heavy premium to move
    from condominiums and town-homes to a single-family home, but the premium on a
    single- family home with a 3-acre lot size is not going to be much different from
    a single-family house with a 2-acre lot size in a comparable built-up area.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction with singular value decomposition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Often, the original dimensions do not represent data in the best way possible.
    As we saw in PCA, you can, sometimes, project the data to fewer dimensions and
    still retain most of the useful information.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, the best approach is to align dimensions along the features that
    exhibit most of the variations. This approach helps to eliminate dimensions that
    are not representative of the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the following figure again, which shows the best-fit line on
    two dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dimensionality reduction with singular value decomposition](img/3056_09_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The projection line shows the best approximation of the original data with
    one dimension. If we take the points where the gray line is intersecting with
    the black line and isolates the black line, we will have a reduced representation
    of the original data with as much variation retained as possible, as shown in
    the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dimensionality reduction with singular value decomposition](img/3056_09_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s draw a line perpendicular to the first projection line, as shown in
    the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dimensionality reduction with singular value decomposition](img/3056_09_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This line captures as much variation as possible along the second dimension
    of the original dataset. It does a bad job at approximating the original data
    as this dimension exhibits less variation to start with. It is possible to use
    these projection lines to generate a set of uncorrelated data points that will
    show subgroupings in the original data, not visible at first glance.
  prefs: []
  type: TYPE_NORMAL
- en: This is the basic idea behind SVD. Take a high dimension, a highly variable
    set of data points, and reduce it to a lower dimensional space that exposes the
    structure of the original data more clearly and orders it from the most variation
    to the least. What makes SVD very useful, especially for NLP application, is that
    you can simply ignore variation below a certain threshold to massively reduce
    the original data, making sure that the original relationship interests are retained.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s get slightly into the theory now. SVD is based on a theorem from linear
    algebra that a rectangular matrix A can be broken down into a product of three
    matrices—an orthogonal matrix U, a diagonal matrix S, and the transpose of an
    orthogonal matrix V. We can show it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dimensionality reduction with singular value decomposition](img/3056_09_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*U* and *V* are orthogonal matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Dimensionality reduction with singular value decomposition](img/3056_09_16.jpg)![Dimensionality
    reduction with singular value decomposition](img/3056_09_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The columns of *U* are orthonormal eigenvectors of ![Dimensionality reduction
    with singular value decomposition](img/3056_09_18.jpg) and the columns of *V*
    are orthonormal eigenvectors of ![Dimensionality reduction with singular value
    decomposition](img/3056_09_19.jpg). *S* is a diagonal matrix containing the square
    roots of eigenvalues from *U* or *V* in descending order.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s look at an example of a term-document matrix. We are going to look at
    two new items about the US presidential elections. The following are the links
    to the two documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fox**: [http://www.foxnews.com/politics/2015/03/08/top-2016-gop-presidential-hopefuls-return-to-iowa-to-hone-message-including/](http://www.foxnews.com/politics/2015/03/08/top-2016-gop-presidential-hopefuls-return-to-iowa-to-hone-message-including/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Npr**: [http://www.npr.org/blogs/itsallpolitics/2015/03/09/391704815/in-iowa-2016-has-begun-at-least-for-the-republican-party](http://www.npr.org/blogs/itsallpolitics/2015/03/09/391704815/in-iowa-2016-has-begun-at-least-for-the-republican-party)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s build the presidential candidate matrix out of these two news items:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting ready](img/3056_09_20.jpg)![Getting ready](img/3056_09_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Let's put this matrix in a CSV file and then put it in HDFS. We will apply SVD
    to this matrix and analyze the results.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Load `scaledhousedata.csv` to HDFS:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Start the Spark shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import statistics and related classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load `pres.csv` as an RDD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Transform data into an RDD of dense vectors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a `RowMatrix` from `parsedData`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compute `svd`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the `U` factor (eigenvector):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the matrix of singular values (eigenvalues):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the `V` factor (eigenvector):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If you look at `s`, you will realize that it gave a much higher score to the
    Npr article than to the Fox article.
  prefs: []
  type: TYPE_NORMAL
