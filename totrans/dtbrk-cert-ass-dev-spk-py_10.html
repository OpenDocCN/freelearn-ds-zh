<html><head></head><body>
		<div><h1 id="_idParaDest-245" class="chapter-number"><a id="_idTextAnchor246"/>10</h1>
			<h1 id="_idParaDest-246"><a id="_idTextAnchor247"/>Mock Test 2</h1>
			<h1 id="_idParaDest-247"><a id="_idTextAnchor248"/>Questions</h1>
			<p>Try your hand at these practice questions to test your knowledge of Apache Spark:</p>
			<p><strong class="bold">Question 1</strong>:</p>
			<p>What is a task in Spark?</p>
			<ol class="margin-left">
				<li class="Alphabets">The unit of work performed for each data partition within a task is the slots</li>
				<li class="Alphabets">A task is the second-smallest entity that can be executed within Spark</li>
				<li class="Alphabets">Tasks featuring wide dependencies can be combined into a single task</li>
				<li class="Alphabets">A task is the smallest component that can be executed within Spark</li>
			</ol>
			<p><strong class="bold">Question 2</strong>:</p>
			<p>What is the role of an executor in Spark?</p>
			<ol class="margin-left">
				<li class="Alphabets">The executor’s role is to request the transformation of operations into a directed acyclic graph (DAG)</li>
				<li class="Alphabets">There can only be one executor within a Spark environment</li>
				<li class="Alphabets">Executors are tasked with executing the assignments provided to them by the driver</li>
				<li class="Alphabets">The executor schedules queries for execution</li>
			</ol>
			<p><strong class="bold">Question 3</strong>:</p>
			<p>Which of the following is one of the tasks of Adaptive Query Execution in Spark?</p>
			<ol class="margin-left">
				<li class="Alphabets">Adaptive Query Execution collects runtime statistics during query execution to optimize query plans</li>
				<li class="Alphabets">Adaptive Query Execution is responsible for distributing tasks to executors</li>
				<li class="Alphabets">Adaptive Query Execution is responsible for wide operations in Spark</li>
				<li class="Alphabets">Adaptive Query Execution is responsible for fault tolerance in Spark</li>
			</ol>
			<p><strong class="bold">Question 4</strong>:</p>
			<p>Which is the lowest level in Spark’s execution hierarchy?</p>
			<ol class="margin-left">
				<li class="Alphabets">Task</li>
				<li class="Alphabets">Slot</li>
				<li class="Alphabets">Job</li>
				<li class="Alphabets">Stage</li>
			</ol>
			<p><strong class="bold">Question 5</strong>:</p>
			<p>Which one of these operations is an action?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>DataFrame.count()</code></li>
				<li class="Alphabets"><code>DataFrame.filter()</code></li>
				<li class="Alphabets"><code>DataFrame.select()</code></li>
				<li class="Alphabets"><code>DataFrame.groupBy()</code></li>
			</ol>
			<p><strong class="bold">Question 6</strong>:</p>
			<p>Which of the following describes the characteristics of the DataFrame API?</p>
			<ol class="margin-left">
				<li class="Alphabets">The DataFrame API is based on resilient distributed dataset (RDD) at the backend</li>
				<li class="Alphabets">The DataFrame API is available in Scala, but it is not available in Python</li>
				<li class="Alphabets">The DataFrame API does not have data manipulation functions</li>
				<li class="Alphabets">The DataFrame API is used for distributing tasks in executors</li>
			</ol>
			<p><strong class="bold">Question 7</strong>:</p>
			<p>Which of the following statements is accurate about executors?</p>
			<ol class="margin-left">
				<li class="Alphabets">Slots are not a part of an executor</li>
				<li class="Alphabets">Executors are able to run tasks in parallel via slots</li>
				<li class="Alphabets">Executors are always equal to tasks</li>
				<li class="Alphabets">An executor is responsible for distributing tasks for a job</li>
			</ol>
			<p><strong class="bold">Question 8</strong>:</p>
			<p>Which of the following statements is accurate about the Spark driver?</p>
			<ol class="margin-left">
				<li class="Alphabets">There are multiple drivers in a Spark application</li>
				<li class="Alphabets">Slots are a part of a driver</li>
				<li class="Alphabets">Drivers execute tasks in parallel</li>
				<li class="Alphabets">It is the responsibility of the Spark driver to transform operations into DAG computations</li>
			</ol>
			<p><strong class="bold">Question 9</strong>:</p>
			<p>Which one of these operations is a wide transformation?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>DataFrame.show()</code></li>
				<li class="Alphabets"><code>DataFrame.groupBy()</code></li>
				<li class="Alphabets"><code>DataFrame.repartition()</code></li>
				<li class="Alphabets"><code>DataFrame.select()</code></li>
				<li class="Alphabets"><code>DataFrame.filter()</code></li>
			</ol>
			<p><strong class="bold">Question 10</strong>:</p>
			<p>Which of the following statements is correct about lazy evaluation?</p>
			<ol class="margin-left">
				<li class="Alphabets">Execution is triggered by transformations</li>
				<li class="Alphabets">Execution is triggered by actions</li>
				<li class="Alphabets">Statements are executed as they appear in the code</li>
				<li class="Alphabets">Spark distributes tasks to different executors</li>
			</ol>
			<p><strong class="bold">Question 11</strong>:</p>
			<p>Which of the following is true about DAGs in Spark?</p>
			<ol class="margin-left">
				<li class="Alphabets">DAGs are lazily evaluated</li>
				<li class="Alphabets">DAGs can be scaled horizontally in Spark</li>
				<li class="Alphabets">DAGs are responsible for processing partitions in an optimized and distributed fashion</li>
				<li class="Alphabets">DAG is comprised of tasks that can run in parallel</li>
			</ol>
			<p><strong class="bold">Question 12</strong>:</p>
			<p>Which of the following statements is true about Spark’s fault tolerance mechanism?</p>
			<ol class="margin-left">
				<li class="Alphabets">Spark achieves fault tolerance via DAGs</li>
				<li class="Alphabets">It is the responsibility of the executor to enable fault tolerance in Spark</li>
				<li class="Alphabets">Because of fault tolerance, Spark can recompute any failed RDD</li>
				<li class="Alphabets">Spark builds a fault-tolerant layer on top of the legacy RDD data system, which by itself is not fault tolerant</li>
			</ol>
			<p><strong class="bold">Question 13</strong>:</p>
			<p>What is the core of Spark’s fault-tolerant mechanism?</p>
			<ol class="margin-left">
				<li class="Alphabets">RDD is at the core of Spark, which is fault tolerant by design</li>
				<li class="Alphabets">Data partitions, since data can be recomputed</li>
				<li class="Alphabets">DataFrame is at the core of Spark since it is immutable</li>
				<li class="Alphabets">Executors ensure that Spark remains fault tolerant</li>
			</ol>
			<p><strong class="bold">Question 14</strong>:</p>
			<p>What is accurate about jobs in Spark?</p>
			<ol class="margin-left">
				<li class="Alphabets">Different stages in a job may be executed in parallel</li>
				<li class="Alphabets">Different stages in a job cannot be executed in parallel</li>
				<li class="Alphabets">A task consists of many jobs</li>
				<li class="Alphabets">A stage consists of many jobs</li>
			</ol>
			<p><strong class="bold">Question 15</strong>:</p>
			<p>What is accurate about a shuffle in Spark?</p>
			<ol class="margin-left">
				<li class="Alphabets">In a shuffle, data is sent to multiple partitions to be processed</li>
				<li class="Alphabets">In a shuffle, data is sent to a single partition to be processed</li>
				<li class="Alphabets">A shuffle is an action that triggers evaluation in Spark</li>
				<li class="Alphabets">In a shuffle, all data remains in memory to be processed</li>
			</ol>
			<p><strong class="bold">Question 16</strong>:</p>
			<p>What is accurate about the cluster manager in Spark?</p>
			<ol class="margin-left">
				<li class="Alphabets">The cluster manager is responsible for managing resources for Spark</li>
				<li class="Alphabets">The cluster manager is responsible for working with executors directly</li>
				<li class="Alphabets">The cluster manager is responsible for creating query plans</li>
				<li class="Alphabets">The cluster manager is responsible for optimizing DAGs</li>
			</ol>
			<p><strong class="bold">Question 17</strong>:</p>
			<p>The following code block needs to take the sum and average of the <code>salary</code> column for each department in the <code>df</code> DataFrame. Then, it should calculate the sum and maximum value for the <code>bonus</code> column:</p>
			<pre class="source-code">
df.___1___ ("department").___2___ (sum("salary").alias("sum_salary"), ___3___ ("salary").alias("avg_salary"), sum("bonus").alias("sum_bonus"), ___4___("bonus").alias("max_bonus") )</pre>			<p>Choose the answer that correctly fills the blanks in the code block to accomplish this:</p>
			<ol class="margin-left">
				<li class="Alphabets"><ol><li class="lower-roman"><code>groupBy</code></li><li class="lower-roman"><code>agg</code></li><li class="lower-roman"><code>avg</code></li><li class="lower-roman"><code>max</code></li></ol></li>
				<li class="Alphabets"><ol><li class="lower-roman"><code>filter</code></li><li class="lower-roman"><code>agg</code></li><li class="lower-roman"><code>avg</code></li><li class="lower-roman"><code>max</code></li></ol></li>
				<li class="Alphabets"><ol><li class="lower-roman"><code>groupBy</code></li><li class="lower-roman"><code>avg</code></li><li class="lower-roman"><code>agg</code></li><li class="lower-roman"><code>max</code></li></ol></li>
				<li class="Alphabets"><ol><li class="lower-roman"><code>groupBy</code></li><li class="lower-roman"><code>agg</code></li><li class="lower-roman"><code>avg</code></li><li class="lower-roman"><code>avg</code></li></ol></li>
			</ol>
			<p><strong class="bold">Question 18</strong>:</p>
			<p>The following code block contains an error. The code block needs to join the <code>salaryDf</code> DataFrame with the bigger <code>employeeDf</code> DataFrame on the <code>employeeID</code> column:</p>
			<pre class="source-code">
salaryDf.join(employeeDf, "employeeID", how="broadcast")</pre>			<p>Identify the error:</p>
			<ol class="margin-left">
				<li class="Alphabets">Instead of <code>join</code>, the code should use <code>innerJoin</code></li>
				<li class="Alphabets"><code>broadcast</code> is not a <code>join</code> type in Spark for joining two DataFrames</li>
				<li class="Alphabets"><code>salaryDf</code> and <code>employeeDf</code> should be swapped</li>
				<li class="Alphabets">In the <code>how</code> parameter, <code>crossJoin</code> should be used instead of <code>broadcast</code></li>
			</ol>
			<p><strong class="bold">Question 19</strong>:</p>
			<p>Which of the following code blocks shuffles the <code>df</code> DataFrame to have 20 partitions instead of 5 partitions?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>df.repartition(5)</code></li>
				<li class="Alphabets"><code>df.repartition(20)</code></li>
				<li class="Alphabets"><code>df.coalesce(20)</code></li>
				<li class="Alphabets"><code>df.coalesce(5)</code></li>
			</ol>
			<p><strong class="bold">Question 20</strong>:</p>
			<p>Which of the following operations will trigger evaluation?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>df.filter()</code></li>
				<li class="Alphabets"><code>df.distinct()</code></li>
				<li class="Alphabets"><code>df.intersect()</code></li>
				<li class="Alphabets"><code>df.join()</code></li>
				<li class="Alphabets"><code>df.count()</code></li>
			</ol>
			<p><strong class="bold">Question 21</strong>:</p>
			<p>Which of the following code blocks returns unique values for the <code>age</code> and <code>name</code> columns in the <code>df</code> DataFrame in its respective columns where all values are unique in these columns?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>df.select('age').join(df.select('name'), </code><code>col(state)==col('name'), 'inner').show()</code></li>
				<li class="Alphabets"><code>df.select(col('age'), </code><code>col('name')).agg({'*': 'count'}).show()</code></li>
				<li class="Alphabets"><code>df.select('age', 'name').distinct().show()</code></li>
				<li class="Alphabets"><code>df.select('age').unionAll(df.select('name')).distinct().show()</code></li>
			</ol>
			<p><strong class="bold">Question 22</strong>:</p>
			<p>Which of the following code blocks returns the count of the total number of rows in the <code>df</code> DataFrame?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>df.count()</code></li>
				<li class="Alphabets"><code>df.select(col('state'), </code><code>col('department')).agg({'*': 'count'}).show()</code></li>
				<li class="Alphabets"><code>df.select('state', 'department').distinct().show()</code></li>
				<li class="Alphabets"><code>df.select('state').union(df.select('department')).distinct().show()</code></li>
			</ol>
			<p><strong class="bold">Question 23</strong>:</p>
			<p>The following code block contains an error. The code block should save the <code>df</code> DataFrame at the <code>filePath</code> path as a new parquet file:</p>
			<pre class="source-code">
df.write.mode("append").parquet(filePath)</pre>			<p>Identify the error:</p>
			<ol class="margin-left">
				<li class="Alphabets">The code block should have <code>overwrite</code> instead of <code>append</code> as an option</li>
				<li class="Alphabets">The code should be <code>write.parquet</code> instead of <code>write.mode</code></li>
				<li class="Alphabets">The <code>df.write</code> operation cannot be called directly from the DataFrame</li>
				<li class="Alphabets">The first part of the code should be <code>df.write.mode(append)</code></li>
			</ol>
			<p><strong class="bold">Question 24</strong>:</p>
			<p>Which of the following code blocks adds a <code>salary_squared</code> column to the <code>df</code> DataFrame that is the square of the <code>salary</code> column?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>df.withColumnRenamed("salary_squared", </code><code>pow(col("salary"), 2))</code></li>
				<li class="Alphabets"><code>df.withColumn("salary_squared", col("salary"*2))</code></li>
				<li class="Alphabets"><code>df.withColumn("salary_squared", </code><code>pow(col("salary"), 2))</code></li>
				<li class="Alphabets"><code>df.withColumn("salary_squared", square(col("salary")))</code></li>
			</ol>
			<p><strong class="bold">Question 25</strong>:</p>
			<p>Which of the following code blocks performs a join in which the small <code>salaryDf</code> DataFrame is sent to all executors so that it can be joined with the <code>employeeDf</code> DataFrame on the <code>employeeSalaryID</code> and <code>EmployeeID</code> columns, respectively?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>employeeDf.join(salaryDf, "employeeDf.employeeID == </code><code>salaryDf.employeeSalaryID", "inner")</code></li>
				<li class="Alphabets"><code>employeeDf.join(salaryDf, "employeeDf.employeeID == </code><code>salaryDf.employeeSalaryID", "broadcast")</code></li>
				<li class="Alphabets"><code>employeeDf.join(broadcast(salaryDf), employeeDf.employeeID == </code><code>salaryDf.employeeSalaryID)</code></li>
				<li class="Alphabets"><code>salaryDf.join(broadcast(employeeDf), employeeDf.employeeID == </code><code>salaryDf.employeeSalaryID)</code></li>
			</ol>
			<p><strong class="bold">Question 26</strong>:</p>
			<p>Which of the following code blocks performs an outer join between the <code>salarydf</code> DataFrame and the <code>employeedf</code> DataFrame, using the <code>employeeID</code> and <code>salaryEmployeeID</code> columns as join keys respectively?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>Salarydf.join(employeedf, "outer", salarydf.employeedf == </code><code>employeeID.salaryEmployeeID)</code></li>
				<li class="Alphabets"><code>salarydf.join(employeedf, employeeID == </code><code>salaryEmployeeID)</code></li>
				<li class="Alphabets"><code>salarydf.join(employeedf, salarydf.salaryEmployeeID == </code><code>employeedf.employeeID, "outer")</code></li>
				<li class="Alphabets"><code>salarydf.join(employeedf, salarydf.employeeID == </code><code>employeedf.salaryEmployeeID, "outer")</code></li>
			</ol>
			<p><strong class="bold">Question 27</strong>:</p>
			<p>Which of the following pieces of code would print the schema of the <code>df</code> DataFrame?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>df.rdd.printSchema</code></li>
				<li class="Alphabets"><code>df.rdd.printSchema()</code></li>
				<li class="Alphabets"><code>df.printSchema</code></li>
				<li class="Alphabets"><code>df.printSchema()</code></li>
			</ol>
			<p><strong class="bold">Question 28</strong>:</p>
			<p>Which of the following code blocks performs a left join between the <code>salarydf</code> DataFrame and the <code>employeedf</code> DataFrame, using the <code>employeeID</code> column?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>salaryDf.join(employeeDf, salaryDf["employeeID"] == </code><code>employeeDf["employeeID"], "outer")</code></li>
				<li class="Alphabets"><code>salaryDf.join(employeeDf, salaryDf["employeeID"] == </code><code>employeeDf["employeeID"], "left")</code></li>
				<li class="Alphabets"><code>salaryDf.join(employeeDf, salaryDf["employeeID"] == </code><code>employeeDf["employeeID"], "inner")</code></li>
				<li class="Alphabets"><code>salaryDf.join(employeeDf, salaryDf["employeeID"] == </code><code>employeeDf["employeeID"], "right")</code></li>
			</ol>
			<p><strong class="bold">Question 29</strong>:</p>
			<p>Which of the following code blocks aggregates the <code>bonus</code> column of the <code>df</code> DataFrame in ascending order with <code>nulls</code> being last?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>df.agg(asc_nulls_last("bonus").alias("bonus_agg"))</code></li>
				<li class="Alphabets"><code>df.agg(asc_nulls_first("bonus").alias("bonus_agg"))</code></li>
				<li class="Alphabets"><code>df.agg(asc_nulls_last("bonus", asc).alias("bonus_agg"))</code></li>
				<li class="Alphabets"><code>df.agg(asc_nulls_first("bonus", asc).alias("bonus_agg"))</code></li>
			</ol>
			<p><strong class="bold">Question 30</strong>:</p>
			<p>The following code block contains an error. The code block should return a DataFrame by joining the <code>employeeDf</code> and <code>salaryDf</code> DataFrames on the <code>employeeID</code> and <code>employeeSalaryID</code> columns, respectively, excluding the <code>bonus</code> and <code>department</code> columns from the <code>employeeDf</code> DataFrame and the <code>salary</code> column from the <code>salaryDf</code> DataFrame in the final DataFrame.</p>
			<pre class="source-code">
employeeDf.groupBy(salaryDf, employeeDf.employeeID == salaryDf.employeeSalaryID, "inner").delete("bonus", "department", "salary")</pre>			<p>Identify the error:</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>groupBy</code> should be replaced with the <code>innerJoin</code> operator</li>
				<li class="Alphabets"><code>groupBy</code> should be replaced with a <code>join</code> operator and <code>delete</code> should be replaced with <code>drop</code></li>
				<li class="Alphabets"><code>groupBy</code> should be replaced with the <code>crossJoin</code> operator and <code>delete</code> should be replaced with <code>withColumn</code></li>
				<li class="Alphabets"><code>groupBy</code> should be replaced with a <code>join</code> operator and <code>delete</code> should be replaced with <code>withColumnRenamed</code></li>
			</ol>
			<p><strong class="bold">Question 31</strong>:</p>
			<p>Which of the following code blocks reads a <code>/loc/example.csv</code> CSV file as a <code>df</code> DataFrame?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>df = </code><code>spark.read.csv("/loc/example.csv")</code></li>
				<li class="Alphabets"><code>df = </code><code>spark.mode("csv").read("/loc/example.csv")</code></li>
				<li class="Alphabets"><code>df = </code><code>spark.read.path("/loc/example.csv")</code></li>
				<li class="Alphabets"><code>df = </code><code>spark.read().csv("/loc/example.csv")</code></li>
			</ol>
			<p><strong class="bold">Question 32</strong>:</p>
			<p>Which of the following code blocks reads a parquet file at the <code>my_path</code> location using a schema file named <code>my_schema</code>?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>spark.read.schema(my_schema).format("parquet").load(my_path)</code></li>
				<li class="Alphabets"><code>spark.read.schema("my_schema").format("parquet").load(my_path)</code></li>
				<li class="Alphabets"><code>spark.read.schema(my_schema).parquet(my_path)</code></li>
				<li class="Alphabets"><code>spark.read.parquet(my_path).schema(my_schema)</code></li>
			</ol>
			<p><strong class="bold">Question 33</strong>:</p>
			<p>We want to find the number of records in the resulting DataFrame when we join the <code>employeedf</code> and <code>salarydf</code> DataFrames on the <code>employeeID</code> and <code>employeeSalaryID</code> columns respectively. Which code blocks should be executed to achieve this?</p>
			<ol>
				<li><code>.</code><code>filter(~isnull(col(department)))</code></li>
				<li><code>.</code><code>count()</code></li>
				<li><code>employeedf.join(salarydf, col("employeedf.employeeID")==col("salarydf.employeeSalaryID"))</code></li>
				<li><code>employeedf.join(salarydf, employeedf. employeeID ==salarydf. </code><code>employeeSalaryID, how='inner')</code></li>
				<li><code>.</code><code>filter(col(department).isnotnull())</code></li>
				<li><code>.</code><code>sum(col(department))</code><ol><li class="Alphabets">3, 1, 6</li><li class="Alphabets">3, 1, 2</li><li class="Alphabets">4, 2</li><li class="Alphabets">3, 5, 2</li></ol></li>
			</ol>
			<p><strong class="bold">Question 34</strong>:</p>
			<p>Which of the following code blocks returns a copy of the <code>df</code> DataFrame where the name of the <code>state</code> column has been changed to <code>stateID</code>?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>df.withColumnRenamed("state", "stateID")</code></li>
				<li class="Alphabets"><code>df.withColumnRenamed("stateID", "state")</code></li>
				<li class="Alphabets"><code>df.withColumn("state", "stateID")</code></li>
				<li class="Alphabets"><code>df.withColumn("stateID", "state")</code></li>
			</ol>
			<p><strong class="bold">Question 35</strong>:</p>
			<p>Which of the following code blocks returns a copy of the <code>df</code> DataFrame where the <code>salary</code> column has been converted to <code>integer</code>?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>df.col("salary").cast("integer"))</code></li>
				<li class="Alphabets"><code>df.withColumn("salary", col("salary").castType("integer"))</code></li>
				<li class="Alphabets"><code>df.withColumn("salary", col("salary").convert("integerType()"))</code></li>
				<li class="Alphabets"><code>df.withColumn("salary", col("salary").cast("integer"))</code></li>
			</ol>
			<p><strong class="bold">Question 36</strong>:</p>
			<p>Which of the following code blocks splits a <code>df</code> DataFrame in half with the exact same values even when the code is run multiple times?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>df.randomSplit([0.5, </code><code>0.5], seed=123)</code></li>
				<li class="Alphabets"><code>df.split([0.5, </code><code>0.5], seed=123)</code></li>
				<li class="Alphabets"><code>df.split([0.5, 0.5])</code></li>
				<li class="Alphabets"><code>df.randomSplit([0.5, 0.5])</code></li>
			</ol>
			<p><strong class="bold">Question 37</strong>:</p>
			<p>Which of the following code blocks sorts the <code>df</code> DataFrame by two columns, <code>salary</code> and <code>department</code>, where <code>salary</code> is in ascending order and <code>department</code> is in descending order?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>df.sort("salary", asc("department"))</code></li>
				<li class="Alphabets"><code>df.sort("salary", desc(department))</code></li>
				<li class="Alphabets"><code>df.sort(col(salary)).desc(col(department))</code></li>
				<li class="Alphabets"><code>df.sort("salary", desc("department"))</code></li>
			</ol>
			<p><strong class="bold">Question 38</strong>:</p>
			<p>Which of the following code blocks calculates the average of the <code>bonus</code> column from the <code>salaryDf</code> DataFrame and adds that in a new column called <code>average_bonus</code>?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>salaryDf.avg("bonus").alias("average_bonus"))</code></li>
				<li class="Alphabets"><code>salaryDf.agg(avg("bonus").alias("average_bonus"))</code></li>
				<li class="Alphabets"><code>salaryDf.agg(sum("bonus").alias("average_bonus"))</code></li>
				<li class="Alphabets"><code>salaryDf.agg(average("bonus").alias("average_bonus"))</code></li>
			</ol>
			<p><strong class="bold">Question 39</strong>:</p>
			<p>Which of the following code blocks saves the <code>df</code> DataFrame in the <code>/FileStore/file.csv</code> location as a CSV file and throws an error if a file already exists in the location?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>df.write.mode("error").csv("/FileStore/file.csv")</code></li>
				<li class="Alphabets"><code>df.write.mode.error.csv("/FileStore/file.csv")</code></li>
				<li class="Alphabets"><code>df.write.mode("exception").csv("/FileStore/file.csv")</code></li>
				<li class="Alphabets"><code>df.write.mode("exists").csv("/FileStore/file.csv")</code></li>
			</ol>
			<p><strong class="bold">Question 40</strong>:</p>
			<p>Which of the following code blocks reads the <code>my_csv.csv</code> CSV file located at <code>/my_path/</code> into a DataFrame?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>spark.read().mode("csv").path("/my_path/my_csv.csv")</code></li>
				<li class="Alphabets"><code>spark.read.format("csv").path("/my_path/my_csv.csv")</code></li>
				<li class="Alphabets"><code>spark.read("csv", "/my_path/my_csv.csv")</code></li>
				<li class="Alphabets"><code>spark.read.csv("/my_path/my_csv.csv")</code></li>
			</ol>
			<p><strong class="bold">Question 41</strong>:</p>
			<p>Which of the following code blocks displays the top 100 rows of the <code>df</code> DataFrame, where the <code>salary</code> column is present, in descending order?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>df.sort(asc(value)).show(100)</code></li>
				<li class="Alphabets"><code>df.sort(col("value")).show(100)</code></li>
				<li class="Alphabets"><code>df.sort(col("value").desc()).show(100)</code></li>
				<li class="Alphabets"><code>df.sort(col("value").asc()).print(100)</code></li>
			</ol>
			<p><strong class="bold">Question 42</strong>:</p>
			<p>Which of the following code blocks creates a DataFrame that shows the mean of the <code>salary</code> column of the <code>salaryDf</code> DataFrame based on the <code>department</code> and <code>state</code> columns, where <code>age</code> is greater than <code>35</code> and the returned DataFrame should be sorted in ascending order by the <code>employeeID</code> column such that there are no nulls in that column?</p>
			<ol>
				<li><code>salaryDf.filter(col("age") &gt; </code><code>35)</code></li>
				<li><code>.</code><code>filter(col("employeeID")</code></li>
				<li><code>.</code><code>filter(col("employeeID").isNotNull())</code></li>
				<li><code>.</code><code>groupBy("department")</code></li>
				<li><code>.</code><code>groupBy("department", "state")</code></li>
				<li><code>.</code><code>agg(avg("salary").alias("mean_salary"))</code></li>
				<li><code>.</code><code>agg(average("salary").alias("mean_salary"))</code></li>
				<li><code>.</code><code>orderBy("employeeID")</code><ol><li class="Alphabets">1, 2, 5, 6, 8</li><li class="Alphabets">1, 3, 5, 6, 8</li><li class="Alphabets">1, 3, 6, 7, 8</li><li class="Alphabets">1, 2, 4, 6, 8</li></ol></li>
			</ol>
			<p><strong class="bold">Question 43</strong>:</p>
			<p>The following code block contains an error. The code block should return a new DataFrame without the <code>employee</code> and <code>salary</code> columns and with an additional <code>fixed_value</code> column, which has a value of <code>100</code>.</p>
			<pre class="source-code">
df.withColumnRenamed(fixed_value).drop('employee', 'salary')</pre>			<p>Identify the error:</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>withcolumnRenamed</code> should be replaced with <code>withcolumn</code> and the <code>lit()</code> function should be used to fill the <code>100</code> value</li>
				<li class="Alphabets"><code>withcolumnRenamed</code> should be replaced with <code>withcolumn</code></li>
				<li class="Alphabets"><code>employee</code> and <code>salary</code> should be swapped in a <code>drop</code> function</li>
				<li class="Alphabets">The <code>lit()</code> function call is missing</li>
			</ol>
			<p><strong class="bold">Question 44</strong>:</p>
			<p>Which of the following code blocks returns the basic statistics for numeric and string columns of the <code>df</code> DataFrame?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>df.describe()</code></li>
				<li class="Alphabets"><code>df.detail()</code></li>
				<li class="Alphabets"><code>df.head()</code></li>
				<li class="Alphabets"><code>df.explain()</code></li>
			</ol>
			<p><strong class="bold">Question 45</strong>:</p>
			<p>Which of the following code blocks returns the top 5 rows of the <code>df</code> DataFrame?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>df.select(5)</code></li>
				<li class="Alphabets"><code>df.head(5)</code></li>
				<li class="Alphabets"><code>df.top(5)</code></li>
				<li class="Alphabets"><code>df.show()</code></li>
			</ol>
			<p><strong class="bold">Question 46</strong>:</p>
			<p>Which of the following code blocks creates a new DataFrame with the <code>department</code>, <code>age</code>, and <code>salary</code> columns from the <code>df</code> DataFrame?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>df.select("department", "</code><code>age", "salary")</code></li>
				<li class="Alphabets"><code>df.drop("department", "</code><code>age", "salary")</code></li>
				<li class="Alphabets"><code>df.filter("department", "</code><code>age", "salary")</code></li>
				<li class="Alphabets"><code>df.where("department", "</code><code>age", "salary")</code></li>
			</ol>
			<p><strong class="bold">Question 47</strong>:</p>
			<p>Which of the following code blocks creates a new DataFrame with three columns, <code>department</code>, <code>age</code>, and <code>max_salary</code>, which has the maximum salary for each employee from each department and each age group from the <code>df</code> DataFrame?</p>
			<pre class="source-code">
df.___1___ (["department", "age"]).___2___ (___3___ ("salary").alias("max_salary"))</pre>			<p>Identify the correct answer:</p>
			<ol class="margin-left">
				<li class="Alphabets"><ol><li class="lower-roman"><code>filter</code></li><li class="lower-roman"><code>agg</code></li><li class="lower-roman"><code>max</code></li></ol></li>
				<li class="Alphabets"><ol><li class="lower-roman">groupBy</li><li class="lower-roman">agg</li><li class="lower-roman">max</li></ol></li>
				<li class="Alphabets"><ol><li class="lower-roman">filter</li><li class="lower-roman">agg</li><li class="lower-roman">sum</li></ol></li>
				<li class="Alphabets"><ol><li class="lower-roman">groupBy</li><li class="lower-roman">agg</li><li class="lower-roman">sum</li></ol></li>
			</ol>
			<p><strong class="bold">Question 48</strong>:</p>
			<p>The following code block contains an error. The code block should return a new DataFrame, filtered by the rows, where the <code>salary</code> column is greater than or equal to <code>1000</code> in the <code>df</code> DataFrame.</p>
			<pre class="source-code">
df.filter(F(salary) &gt;= 1000)</pre>			<p>Identify the error:</p>
			<ol class="margin-left">
				<li class="Alphabets">Instead of <code>filter()</code>, <code>where()</code> should be used</li>
				<li class="Alphabets">The <code>F(salary)</code> operation should be replaced with <code>F.col("salary")</code></li>
				<li class="Alphabets">Instead of <code>&gt;=</code>, the <code>&gt;</code> operator should be used</li>
				<li class="Alphabets">The argument to the <code>where</code> method should be <code>"salary &gt; </code><code>1000"</code></li>
			</ol>
			<p><strong class="bold">Question 49</strong>:</p>
			<p>Which of the following code blocks returns a copy of the <code>df</code> DataFrame where the <code>department</code> column has been renamed <code>business_unit</code>?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>df.withColumn(["department", "business_unit"])</code></li>
				<li class="Alphabets"><code>itemsDf.withColumn("department").alias("business_unit")</code></li>
				<li class="Alphabets"><code>itemsDf.withColumnRenamed("department", "business_unit")</code></li>
				<li class="Alphabets"><code>itemsDf.withColumnRenamed("business_unit", "department")</code></li>
			</ol>
			<p><strong class="bold">Question 50</strong>:</p>
			<p>Which of the following code blocks returns a DataFrame with the total count of employees in each department from the <code>df</code> DataFrame?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>df.groupBy("department").agg(count("*").alias("total_employees"))</code></li>
				<li class="Alphabets"><code>df.filter("department").agg(count("*").alias("total_employees"))</code></li>
				<li class="Alphabets"><code>df.groupBy("department").agg(sum("*").alias("total_employees"))</code></li>
				<li class="Alphabets"><code>df.filter("department").agg(sum("*").alias("total_employees"))</code></li>
			</ol>
			<p><strong class="bold">Question 51</strong>:</p>
			<p>Which of the following code blocks returns a DataFrame with the <code>employee</code> column from the <code>df</code> DataFrame case to the <code>string</code> type?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>df.withColumn("employee", col("employee").cast_type("string"))</code></li>
				<li class="Alphabets"><code>df.withColumn("employee", col("employee").cast("string"))</code></li>
				<li class="Alphabets"><code>df.withColumn("employee", col("employee").cast_type("stringType()"))</code></li>
				<li class="Alphabets"><code>df.withColumnRenamed("employee", col("employee").cast("string"))</code></li>
			</ol>
			<p><strong class="bold">Question 52</strong>:</p>
			<p>Which of the following code blocks returns a DataFrame with a new <code>fixed_value</code> column, which has <code>Z</code> in all rows in the df DataFrame?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>df.withColumn("fixed_value", F.lit("Z"))</code></li>
				<li class="Alphabets"><code>df.withColumn("fixed_value", F("Z"))</code></li>
				<li class="Alphabets"><code>df.withColumnRenamed("fixed_value", F.lit("Z"))</code></li>
				<li class="Alphabets"><code>df.withColumnRenamed("fixed_value", lit("Z"))</code></li>
			</ol>
			<p><strong class="bold">Question 53</strong>:</p>
			<p>Which of the following code blocks returns a new DataFrame with a new <code>upper_string</code> column, which is the capitalized version of the <code>employeeName</code> column in the <code>df</code> DataFrame?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>df.withColumnRenamed('employeeName', upper(df.upper_string))</code></li>
				<li class="Alphabets"><code>df.withColumnRenamed('upper_string', upper(df.employeeName))</code></li>
				<li class="Alphabets"><code>df.withColumn('upper_string', upper(df.employeeName))</code></li>
				<li class="Alphabets"><code>df.withColumn(' </code><code>employeeName', upper(df.upper_string))</code></li>
			</ol>
			<p><strong class="bold">Question 54</strong>:</p>
			<p>The following code block contains an error. The code block is supposed to capitalize the employee names using a udf:</p>
			<pre class="source-code">
capitalize_udf = udf(lambda x: x.upper(), StringType())
df_with_capitalized_names = df.withColumn("capitalized_name", capitalize("employee"))</pre>			<p>Identify the error:</p>
			<ol class="margin-left">
				<li class="Alphabets">The <code>capitalize_udf</code> function should be called instead of <code>capitalize</code></li>
				<li class="Alphabets">The <code>udf</code> function, <code>capitalize_udf</code>, is not capitalizing correctly</li>
				<li class="Alphabets">Instead of <code>StringType()</code>, <code>IntegerType()</code> should be used</li>
				<li class="Alphabets">Instead of <code>df.withColumn("capitalized_name", capitalize("employee"))</code>, it should use <code>df.withColumn("employee", capitalize("capitalized_name"))</code></li>
			</ol>
			<p><strong class="bold">Question 55</strong>:</p>
			<p>The following code block contains an error. The code block is supposed to sort the <code>df</code> DataFrame by salary in ascending order. Then, it should sort based on the <code>bonus</code> column, putting <code>nulls</code> last.</p>
			<pre class="source-code">
df.orderBy ('salary', asc_nulls_first(col('bonus')))</pre>			<p>Identify the error:</p>
			<ol class="margin-left">
				<li class="Alphabets">The <code>salary</code> column should be sorted in descending order and <code>desc_nulls_last</code> should be used instead of <code>asc_nulls_first</code>. Moreover, it should be wrapped in a <code>col()</code> operator.</li>
				<li class="Alphabets">The <code>salary</code> column should be wrapped by the <code>col()</code> operator.</li>
				<li class="Alphabets">The <code>bonus</code> column should be sorted in a descending way, putting nulls last.</li>
				<li class="Alphabets">The <code>bonus</code> column should be sorted by <code>desc_nulls_first()</code> instead.</li>
			</ol>
			<p><strong class="bold">Question 56</strong>:</p>
			<p>The following code block contains an error. The code block needs to group the <code>df</code> DataFrame based on the <code>department</code> column and calculate the total salary and average salary for each department.</p>
			<pre class="source-code">
df.filter("department").agg(sum("salary").alias("sum_salary"), avg("salary").alias("avg_salary"))</pre>			<p>Identify the error:</p>
			<ol class="margin-left">
				<li class="Alphabets">The <code>avg</code> method should also be called through the <code>agg</code> function</li>
				<li class="Alphabets">Instead of <code>filter</code>, <code>groupBy</code> should be used</li>
				<li class="Alphabets">The <code>agg</code> method syntax is incorrect</li>
				<li class="Alphabets">Instead of filtering on <code>department</code>, the code should filter on <code>salary</code></li>
			</ol>
			<p><strong class="bold">Question 57</strong>:</p>
			<p>Which code block will write the <code>df</code> DataFrame as a parquet file on the <code>filePath</code> path partitioning it on the <code>department</code> column?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>df.write.partitionBy("department").parquet(filePath)</code></li>
				<li class="Alphabets"><code>df.write.partition("department").parquet(filePath)</code></li>
				<li class="Alphabets"><code>df.write.parquet("department").partition(filePath)</code></li>
				<li class="Alphabets"><code>df.write.coalesce("department").parquet(filePath)</code></li>
			</ol>
			<p><strong class="bold">Question 58</strong>:</p>
			<p>The <code>df</code> DataFrame contains columns <code>[employeeID, salary, department]</code>. Which of the following pieces of code would return the <code>df</code> DataFrame with only columns <code>[</code><code>employeeID, salary]</code>?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>df.drop("department")</code></li>
				<li class="Alphabets"><code>df.select(col(employeeID))</code></li>
				<li class="Alphabets"><code>df.drop("department", "salary")</code></li>
				<li class="Alphabets"><code>df.select("employeeID", "department")</code></li>
			</ol>
			<p><strong class="bold">Question 59</strong>:</p>
			<p>Which of the following code blocks returns a new DataFrame with the same columns as the <code>df</code> DataFrame, except for the <code>salary</code> column?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>df.drop(col("salary"))</code></li>
				<li class="Alphabets"><code>df.delete(salary)</code></li>
				<li class="Alphabets"><code>df.drop(salary)</code></li>
				<li class="Alphabets"><code>df.delete("salary")</code></li>
			</ol>
			<p><strong class="bold">Question 60</strong>:</p>
			<p>The following code block contains an error. The code block should return the <code>df</code> DataFrame with <code>employeeID</code> renamed as <code>employeeIdColumn</code>.</p>
			<pre class="source-code">
df.withColumnRenamed("employeeIdColumn", "employeeID")</pre>			<p>Identify the error:</p>
			<ol class="margin-left">
				<li class="Alphabets">Instead of <code>withColumnRenamed</code>, the <code>withColumn</code> method should be used</li>
				<li class="Alphabets">Instead of <code>withColumnRenamed</code>, the <code>withColumn</code> method should be used and the <code>"employeeIdColumn"</code> argument should be swapped with the <code>"</code><code>employeeID"</code> argument</li>
				<li class="Alphabets">The <code>"employeeIdColumn"</code> and <code>"employeeID"</code> arguments should be swapped</li>
				<li class="Alphabets"><code>withColumnRenamed</code> is not a method for DataFrames</li>
			</ol>
			<h2 id="_idParaDest-248"><a id="_idTextAnchor249"/>Answers</h2>
			<ol>
				<li>D</li>
				<li>C</li>
				<li>A</li>
				<li>A</li>
				<li>A</li>
				<li>A</li>
				<li>B</li>
				<li>D</li>
				<li>C</li>
				<li>B</li>
				<li>C</li>
				<li>C</li>
				<li>A</li>
				<li>B</li>
				<li>A</li>
				<li>A</li>
				<li>A</li>
				<li>B</li>
				<li>B</li>
				<li>E</li>
				<li>C</li>
				<li>A</li>
				<li>A</li>
				<li>C</li>
				<li>C</li>
				<li>D</li>
				<li>D</li>
				<li>B</li>
				<li>A</li>
				<li>B</li>
				<li>A</li>
				<li>A</li>
				<li>C</li>
				<li>A</li>
				<li>D</li>
				<li>A</li>
				<li>D</li>
				<li>B</li>
				<li>A</li>
				<li>D</li>
				<li>C</li>
				<li>B</li>
				<li>A</li>
				<li>A</li>
				<li>B</li>
				<li>A</li>
				<li>B</li>
				<li>B</li>
				<li>C</li>
				<li>A</li>
				<li>B</li>
				<li>A</li>
				<li>C</li>
				<li>A</li>
				<li>A</li>
				<li>B</li>
				<li>A</li>
				<li>A</li>
				<li>A</li>
				<li>C</li>
			</ol>
		</div>
	</body></html>