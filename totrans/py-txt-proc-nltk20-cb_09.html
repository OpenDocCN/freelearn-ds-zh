<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch09"/>Chapter 9. Parsing Specific Data</h1></div></div></div><p>In this chapter, we will cover:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Parsing dates and times with Dateutil</li><li class="listitem" style="list-style-type: disc">Time zone lookup and conversion</li><li class="listitem" style="list-style-type: disc">Tagging temporal expressions with Timex</li><li class="listitem" style="list-style-type: disc">Extracting URLs from HTML with lxml</li><li class="listitem" style="list-style-type: disc">Cleaning and stripping HTML</li><li class="listitem" style="list-style-type: disc">Converting HTML entities with BeautifulSoup</li><li class="listitem" style="list-style-type: disc">Detecting and converting character encodings</li></ul></div><div><div><div><div><h1 class="title"><a id="ch09lvl1sec89"/>Introduction</h1></div></div></div><p>This chapter covers parsing specific kinds of data, focusing primarily on dates, times, and HTML. Luckily, there are a number of useful libraries for accomplishing this, so we don't have to delve into tricky and overly complicated regular expressions. These libraries can be great complements to the NLTK:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">dateutil</code>: <a id="id717" class="indexterm"/>Provides date/time parsing and time zone conversion</li><li class="listitem" style="list-style-type: disc"><code class="literal">timex</code>: <a id="id718" class="indexterm"/>Can identify time words in text</li><li class="listitem" style="list-style-type: disc"><a id="id719" class="indexterm"/><code class="literal">lxml</code> and <code class="literal">BeautifulSoup</code>: <a id="id720" class="indexterm"/>Can parse, clean, and convert HTML</li><li class="listitem" style="list-style-type: disc"><code class="literal">chardet</code>: <a id="id721" class="indexterm"/>Detects the character encoding of text</li></ul></div><p>The libraries can be useful for pre-processing text before passing it to an NLTK object, or post-processing text that has been processed and extracted using NLTK. Here's an example that ties many of these tools together.</p><p>Let's say you need to parse a blog article about a restaurant. You can use <code class="literal">lxml</code> or <code class="literal">BeautifulSoup</code> to extract the article text, outbound links, and the date and time when the article was written. The date and time can then be parsed to a Python <code class="literal">datetime</code> object with <code class="literal">dateutil</code>. Once you have the article text, you can use <code class="literal">chardet</code> to ensure it's UTF-8 before cleaning out the HTML and running it through NLTK-based part-of-speech tagging, chunk extraction, and/or text classification, to create additional metadata about the article. If there's an event happening at the restaurant, you may be able to discover that by looking at the time words identified by <code class="literal">timex</code>. The point of this example is that real-world text processing often requires more than just NLTK-based natural language processing, and the functionality covered in this chapter can help with those additional requirements.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch09lvl1sec90"/>Parsing dates and times with Dateutil</h1></div></div></div><a id="id722" class="indexterm"/><a id="id723" class="indexterm"/><a id="id724" class="indexterm"/><a id="id725" class="indexterm"/><p>If you need to parse dates and times in Python, there is no better library than <code class="literal">dateutil</code>. The <code class="literal">parser</code> module can parse <code class="literal">datetime</code> strings in many more formats than can be shown here, while the <code class="literal">tz</code> module provides everything you need for looking up time zones. Combined, these modules make it quite easy to parse strings into time zone aware <code class="literal">datetime</code> objects.</p><div><div><div><div><h2 class="title"><a id="ch09lvl2sec327"/>Getting ready</h2></div></div></div><p>You can install <code class="literal">dateutil</code> using <code class="literal">pip</code> or <code class="literal">easy_install</code>, that is <code class="literal">sudo pip install dateutil</code> or <code class="literal">sudo easy_install dateutil</code>. Complete documentation can be found at <a class="ulink" href="http://labix.org/python-dateutil">http://labix.org/python-dateutil</a>.</p></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec328"/>How to do it...</h2></div></div></div><p>Let's dive into a few parsing examples:</p><div><pre class="programlisting">&gt;&gt;&gt; from dateutil import parser
&gt;&gt;&gt; parser.parse('Thu Sep 25 10:36:28 2010')
datetime.datetime(2010, 9, 25, 10, 36, 28)
&gt;&gt;&gt; parser.parse('Thursday, 25. September 2010 10:36AM')
datetime.datetime(2010, 9, 25, 10, 36)
&gt;&gt;&gt; parser.parse('9/25/2010 10:36:28')
datetime.datetime(2010, 9, 25, 10, 36, 28)
&gt;&gt;&gt; parser.parse('9/25/2010')
datetime.datetime(2010, 9, 25, 0, 0)
&gt;&gt;&gt; parser.parse('2010-09-25T10:36:28Z')
datetime.datetime(2010, 9, 25, 10, 36, 28, tzinfo=tzutc())</pre></div><p>As you can see, all it takes is importing the <code class="literal">parser</code> module and calling the <code class="literal">parse()</code> function with a <code class="literal">datetime</code> string. The parser will do its best to return a sensible <code class="literal">datetime</code> object, but if it cannot parse the string, it will raise a <code class="literal">ValueError</code>.</p></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec329"/>How it works...</h2></div></div></div><a id="id726" class="indexterm"/><a id="id727" class="indexterm"/><a id="id728" class="indexterm"/><a id="id729" class="indexterm"/><p>The parser does not use regular expressions. Instead, it looks for recognizable tokens and does its best to guess what those tokens refer to. The order of these tokens matters, for example, some cultures use a date format that looks like <em>Month/Day/Year</em> (the default order) while others use a <em>Day/Month/Year</em> format. To deal with this, the <code class="literal">parse()</code> function takes an optional keyword argument <code class="literal">dayfirst</code>, which defaults to <code class="literal">False</code>. If you set it to <code class="literal">True</code>, it can correctly parse dates in the latter format.</p><div><pre class="programlisting">&gt;&gt;&gt; parser.parse('25/9/2010', dayfirst=True)
datetime.datetime(2010, 9, 25, 0, 0)</pre></div><p>Another ordering issue can occur with two-digit years. For example, <code class="literal">'10-9-25'</code> is ambiguous. Since <code class="literal">dateutil</code> defaults to the <em>Month-Day-Year</em> format, <code class="literal">'10-9-25'</code> is parsed to the year 2025. But if you pass <code class="literal">yearfirst=True</code> into <code class="literal">parse()</code>, it will be parsed to the year 2010.</p><div><pre class="programlisting">&gt;&gt;&gt; parser.parse('10-9-25')
datetime.datetime(2025, 10, 9, 0, 0)
&gt;&gt;&gt; parser.parse('10-9-25', yearfirst=True)
datetime.datetime(2010, 9, 25, 0, 0)</pre></div></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec330"/>There's more...</h2></div></div></div><p>The <code class="literal">dateutil</code> parser can also do <a id="id730" class="indexterm"/>
<strong>fuzzy parsing</strong>, which allows it to ignore extraneous characters in a <code class="literal">datetime</code> string. With the default value of <code class="literal">False</code>, <code class="literal">parse()</code> will raise a <code class="literal">ValueError</code> when it encounters unknown tokens. But if <code class="literal">fuzzy=True</code>, then a <code class="literal">datetime</code> object can usually be returned.</p><div><pre class="programlisting">&gt;&gt;&gt; try:
...    parser.parse('9/25/2010 at about 10:36AM')
... except ValueError:
...    'cannot parse'
'cannot parse'
&gt;&gt;&gt; parser.parse('9/25/2010 at about 10:36AM', fuzzy=True)
datetime.datetime(2010, 9, 25, 10, 36)</pre></div></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec331"/>See also</h2></div></div></div><p>In the next recipe, we'll use the <code class="literal">tz</code> module from <code class="literal">dateutil</code> to do time zone lookup and conversion.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch09lvl1sec91"/>Time zone lookup and conversion</h1></div></div></div><p>Most <code class="literal">datetime</code> objects returned from the <code class="literal">dateutil</code> parser are <em>naive</em>, meaning they don't have an explicit <code class="literal">tzinfo</code>, which specifies the time zone and UTC offset. In the previous recipe, only one of the examples had a <code class="literal">tzinfo</code>, and that's because it's in the standard ISO format for UTC date and time strings. <strong>UTC</strong>
<a id="id731" class="indexterm"/> is the coordinated universal time, and is the same as GMT. <strong>ISO</strong> is the <a id="id732" class="indexterm"/>
<strong>International Standards Organization</strong>, which among other things, specifies standard date and time formatting.</p><p>Python <code class="literal">datetime</code> objects can either be <em>naive</em> or <em>aware</em>. If a <code class="literal">datetime</code> object has a <code class="literal">tzinfo</code>, then it is aware. Otherwise the <code class="literal">datetime</code> is naive. To make a naive <code class="literal">datetime</code> object time zone aware, you must give it an explicit <code class="literal">tzinfo</code>. However, the Python <code class="literal">datetime</code> library only defines an abstract base class for <code class="literal">tzinfo</code>, and leaves it up to the others to actually implement <code class="literal">tzinfo</code> creation. This is where the <code class="literal">tz</code> module of <code class="literal">dateutil</code> comes in—it provides everything you need to lookup time zones from your OS time zone data.</p><div><div><div><div><h2 class="title"><a id="ch09lvl2sec332"/>Getting ready</h2></div></div></div><a id="id733" class="indexterm"/><p>
<code class="literal">dateutil</code> should be installed using <code class="literal">pip</code> or <code class="literal">easy_install</code>. You should also make sure your operating system has time zone data. On Linux, this is usually found in <code class="literal">/usr/share/zoneinfo</code>, and the Ubuntu package is called <code class="literal">tzdata</code>. If you have a number of files and directories in <code class="literal">/usr/share/zoneinfo</code>, such as <code class="literal">America/</code>, <code class="literal">Europe/</code>, and so on, then you should be ready to proceed. The following examples show directory paths for Ubuntu Linux.</p></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec333"/>How to do it...</h2></div></div></div><p>Let's start by getting a UTC <code class="literal">tzinfo</code> object. This can be done by calling <code class="literal">tz.tzutc()</code>, and you can check that the offset is <strong>0</strong> by calling the <code class="literal">utcoffset()</code> method with a UTC <code class="literal">datetime</code> object.</p><div><pre class="programlisting">&gt;&gt;&gt; from dateutil import tz
&gt;&gt;&gt; tz.tzutc()
tzutc()
&gt;&gt;&gt; import datetime
&gt;&gt;&gt; tz.tzutc().utcoffset(datetime.datetime.utcnow())
datetime.timedelta(0)</pre></div><a id="id734" class="indexterm"/><p>To get <code class="literal">tzinfo</code> objects for other time zones, you can pass in a time zone file path to the <code class="literal">gettz()</code> function.</p><div><pre class="programlisting">&gt;&gt;&gt; tz.gettz('US/Pacific')
tzfile('/usr/share/zoneinfo/US/Pacific')
&gt;&gt;&gt; tz.gettz('US/Pacific').utcoffset(datetime.datetime.utcnow())
datetime.timedelta(-1, 61200)
&gt;&gt;&gt; tz.gettz('Europe/Paris')
tzfile('/usr/share/zoneinfo/Europe/Paris')
&gt;&gt;&gt; tz.gettz('Europe/Paris').utcoffset(datetime.datetime.utcnow())
datetime.timedelta(0, 7200)</pre></div><p>You can see the UTC offsets are <code class="literal">timedelta</code> objects, where the first number is <em>days</em>, and the second number is <em>seconds</em>.</p><div><div><h3 class="title"><a id="tip08"/>Tip</h3><p>If you're storing <code class="literal">datetimes</code> in a database, it's a good idea to store them all in UTC to eliminate any time zone ambiguity. Even if the database can recognize time zones, it's still a good practice.</p></div></div><p>To convert a non-UTC <code class="literal">datetime</code> object<a id="id735" class="indexterm"/> to UTC, it must be made time zone aware. If you try to convert a naive <code class="literal">datetime</code> to UTC, you'll get a <code class="literal">ValueError</code> exception. To make a naive <code class="literal">datetime</code> time zone aware, you simply call the <code class="literal">replace()</code> method with the correct <code class="literal">tzinfo</code>. Once a <code class="literal">datetime</code> object has a <code class="literal">tzinfo</code>, then UTC conversion can be performed by calling the <code class="literal">astimezone()</code> method with <code class="literal">tz.tzutc()</code>.</p><div><pre class="programlisting">&gt;&gt;&gt; pst = tz.gettz('US/Pacific')
&gt;&gt;&gt; dt = datetime.datetime(2010, 9, 25, 10, 36)
&gt;&gt;&gt; dt.tzinfo
&gt;&gt;&gt; dt.astimezone(tz.tzutc())
Traceback (most recent call last):
  File "/usr/lib/python2.6/doctest.py", line 1248, in __run
  compileflags, 1) in test.globs
  File "&lt;doctest __main__[22]&gt;", line 1, in &lt;module&gt;
  dt.astimezone(tz.tzutc())
ValueError: astimezone() cannot be applied to a naive datetime
&gt;&gt;&gt; dt.replace(tzinfo=pst)
datetime.datetime(2010, 9, 25, 10, 36, tzinfo=tzfile('/usr/share/zoneinfo/US/Pacific'))
&gt;&gt;&gt; dt.replace(tzinfo=pst).astimezone(tz.tzutc())
datetime.datetime(2010, 9, 25, 17, 36, tzinfo=tzutc())</pre></div></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec334"/>How it works...</h2></div></div></div><p>The <code class="literal">tzutc</code> and <code class="literal">tzfile</code> objects are both subclasses of <code class="literal">tzinfo</code>. As such, they know the correct UTC offset for time zone conversion (which is 0 for <code class="literal">tzutc</code>). A <a id="id736" class="indexterm"/>
<code class="literal">tzfile</code> object knows how to read your operating system's <code class="literal">zoneinfo</code> files to get the necessary offset data. The <code class="literal">replace()</code> method of a <code class="literal">datetime</code> object does what its name implies—it replaces attributes. Once a <code class="literal">datetime</code> has a <code class="literal">tzinfo</code>, the <code class="literal">astimezone()</code> method will be able to convert the time using the UTC offsets, and then replace the current <code class="literal">tzinfo</code> with the new <code class="literal">tzinfo</code>.</p><div><div><h3 class="title"><a id="note35"/>Note</h3><p>Note that both <code class="literal">replace()</code> and <code class="literal">astimezone()</code> return <strong>new</strong> <code class="literal">datetime</code> objects. They do not modify the current object.</p></div></div></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec335"/>There's more...</h2></div></div></div><p>You can pass a <code class="literal">tzinfos</code> keyword argument into the <code class="literal">dateutil</code> parser to detect otherwise unrecognized time zones.</p><div><pre class="programlisting">&gt;&gt;&gt; parser.parse('Wednesday, Aug 4, 2010 at 6:30 p.m. (CDT)', fuzzy=True)
datetime.datetime(2010, 8, 4, 18, 30)
&gt;&gt;&gt; tzinfos = {'CDT': tz.gettz('US/Central')}
&gt;&gt;&gt; parser.parse('Wednesday, Aug 4, 2010 at 6:30 p.m. (CDT)', fuzzy=True, tzinfos=tzinfos)
datetime.datetime(2010, 8, 4, 18, 30, tzinfo=tzfile('/usr/share/zoneinfo/US/Central'))</pre></div><p>In the first instance, we get a naive <code class="literal">datetime</code> since the time zone is not recognized. However, when we pass in the <code class="literal">tzinfos</code> mapping, we get a time zone aware <code class="literal">datetime</code>.</p><div><div><div><div><h3 class="title"><a id="ch09lvl3sec83"/>Local time zone</h3></div></div></div><a id="id737" class="indexterm"/><p>If you want to lookup your local time zone, you can call <code class="literal">tz.tzlocal()</code>, which will use whatever your operating system thinks is the local time zone. In Ubuntu Linux, this is usually specified in the <code class="literal">/etc/timezone</code> file.</p></div><div><div><div><div><h3 class="title"><a id="ch09lvl3sec84"/>Custom offsets</h3></div></div></div><p>You can create your own <code class="literal">tzinfo</code> object with a custom UTC offset using the <code class="literal">tzoffset</code> object. A custom offset of one hour can be created as follows:</p><div><pre class="programlisting">&gt;&gt;&gt; tz.tzoffset('custom', 3600)
tzoffset('custom', 3600)</pre></div><p>You must provide a name as the first argument, and the offset time in seconds as the second argument.</p></div></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec336"/>See also</h2></div></div></div><p>The previous recipe covers parsing <code class="literal">datetime</code> strings with <code class="literal">dateutil.parser</code>.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch09lvl1sec92"/>Tagging temporal expressions with Timex</h1></div></div></div><a id="id738" class="indexterm"/><a id="id739" class="indexterm"/><p>The NLTK project has a little known <code class="literal">contrib</code> repository that contains, among other things, a module called <code class="literal">timex.py</code> that can tag temporal expressions. A <a id="id740" class="indexterm"/>
<strong>temporal expression</strong> is just one or more time words, such as "this week", or "next month". These are ambiguous expressions that are relative to some other point in time, like when the text was written. The <code class="literal">timex</code> module provides a way to annotate text so these expressions can be extracted for further analysis. More on TIMEX can be found at <a class="ulink" href="http://timex2.mitre.org/">http://timex2.mitre.org/</a>.</p><div><div><div><div><h2 class="title"><a id="ch09lvl2sec337"/>Getting ready</h2></div></div></div><p>The <code class="literal">timex.py</code> module is part of the <code class="literal">nltk_contrib</code> package, which is separate from the current version of NLTK. This means you need to install it yourself, or use the <code class="literal">timex.py</code> module that is included with the book's code download. You can also download <code class="literal">timex.py</code> directly from <a class="ulink" href="http://code.google.com/p/nltk/source/browse/trunk/nltk_contrib/nltk_contrib/timex.py">http://code.google.com/p/nltk/source/browse/trunk/nltk_contrib/nltk_contrib/timex.py</a>.</p><p>If you want to install the entire <code class="literal">nltk_contrib</code> package, you can check out the source at <a class="ulink" href="http://nltk.googlecode.com/svn/trunk/">http://nltk.googlecode.com/svn/trunk/</a> and do <code class="literal">sudo python setup.py install</code> from within the <code class="literal">nltk_contrib</code> folder. If you do this, you'll need to do <code class="literal">from nltk_contrib import timex</code> instead of just <code class="literal">import timex</code> as done in the following <em>How to do it...</em> section.</p><p>For this recipe, you have to download the <code class="literal">timex.py</code> module into the same folder as the rest of the code, so that <code class="literal">import timex</code> does not cause an <code class="literal">ImportError</code>.</p><p>You'll also need to get the <code class="literal">egenix-mx-base</code> package installed. This is a C extension library for Python, so if you have all the correct Python development headers installed, you should be able to do <code class="literal">sudo pip install egenix-mx-base</code> or <code class="literal">sudo easy_install egenix-mx-base</code>. If you're running Ubuntu Linux, you can instead do <code class="literal">sudo apt-get install python-egenix-mxdatetime</code>. If none of those work, you can go to <a class="ulink" href="http://www.egenix.com/products/python/mxBase/">http://www.egenix.com/products/python/mxBase/</a> to download the package and find installation instructions.</p></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec338"/>How to do it...</h2></div></div></div><a id="id741" class="indexterm"/><a id="id742" class="indexterm"/><a id="id743" class="indexterm"/><p>Using <code class="literal">timex</code> is very simple: pass a string into the <code class="literal">timex.tag()</code> function and get back an annotated string. The annotations will be XML <code class="literal">TIMEX</code> tags surrounding each temporal expression.</p><div><pre class="programlisting">&gt;&gt;&gt; import timex
&gt;&gt;&gt; timex.tag("Let's go sometime this week")
"Let's go sometime &lt;TIMEX2&gt;this week&lt;/TIMEX2&gt;"
&gt;&gt;&gt; timex.tag("Tomorrow I'm going to the park.")
"&lt;TIMEX2&gt;Tomorrow&lt;/TIMEX2&gt; I'm going to the park."</pre></div></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec339"/>How it works...</h2></div></div></div><p>The implementation of <code class="literal">timex.py</code> is essentially over 300 lines of conditional regular expression matches. When one of the known expressions match, it creates a <code class="literal">RelativeDateTime</code> object (from the <code class="literal">mx.DateTime</code> module). This <code class="literal">RelativeDateTime</code> is then converted back to a string with surrounding <code class="literal">TIMEX</code> tags and replaces the original matched string in the text.</p></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec340"/>There's more...</h2></div></div></div><p>
<code class="literal">timex</code> is smart enough not to tag expressions that have already been tagged, so it's ok to pass <code class="literal">TIMEX</code> tagged text into the <code class="literal">tag()</code> function.</p><div><pre class="programlisting">&gt;&gt;&gt; timex.tag("Let's go sometime &lt;TIMEX2&gt;this week&lt;/TIMEX2&gt;")
"Let's go sometime &lt;TIMEX2&gt;this week&lt;/TIMEX2&gt;"</pre></div></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec341"/>See also</h2></div></div></div><p>In the next recipe, we'll be extracting URLs from HTML, but the same modules and techniques can be used to extract the <code class="literal">TIMEX</code> tagged expressions for further processing.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch09lvl1sec93"/>Extracting URLs from HTML with lxml</h1></div></div></div><a id="id744" class="indexterm"/><p>A common task when parsing HTML is extracting links. This is one of the core functions of every general web crawler. There are a number of Python libraries for parsing HTML, and <code class="literal">lxml</code> is one of the best. As you'll see, it comes with some great helper functions geared specifically towards link extraction.</p><div><div><div><div><h2 class="title"><a id="ch09lvl2sec342"/>Getting ready</h2></div></div></div><a id="id745" class="indexterm"/><p>
<code class="literal">lxml</code> is a Python binding for the C libraries <code class="literal">libxml2</code> and <code class="literal">libxslt</code>. This makes it a very fast XML and HTML parsing library, while still being <em>pythonic</em>. However, that also means you need to install the C libraries for it to work. Installation instructions are at <a class="ulink" href="http://codespe">http://codespe</a>
<a class="ulink" href="http://ak.net/lxml/installation.html">ak.net/lxml/installation.html</a>. However, if you're running Ubuntu Linux, installation is as easy as <code class="literal">sudo apt-get install python-lxml</code>.</p></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec343"/>How to do it...</h2></div></div></div><p>
<code class="literal">lxml</code> comes with an <code class="literal">html</code> module designed specifically for parsing HTML. Using the <code class="literal">fromstring()</code> function, we can parse an HTML string, then get a list of all the links. The <a id="id746" class="indexterm"/>
<code class="literal">iterlinks()</code> method generates four-tuples of the form <code class="literal">(element, attr, link, pos)</code>:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">element</code>: This is the parsed node of the anchor tag from which the <code class="literal">link</code> is extracted. If you're just interested in the <code class="literal">link</code>, you can ignore this.</li><li class="listitem" style="list-style-type: disc"><code class="literal">attr</code>: This is the attribute the <code class="literal">link</code> came from, which is usually <code class="literal">href</code>.</li><li class="listitem" style="list-style-type: disc"><code class="literal">link</code>: This is the actual URL extracted from the anchor tag.</li><li class="listitem" style="list-style-type: disc"><code class="literal">pos</code>: This is the numeric index of the anchor tag in the document. The first tag has a <code class="literal">pos</code> of <code class="literal">0</code>, the second has a <code class="literal">pos</code> of <code class="literal">1</code>, and so on.</li></ul></div><p>Following is some code to demonstrate:</p><div><pre class="programlisting">&gt;&gt;&gt; from lxml import html
&gt;&gt;&gt; doc = html.fromstring('Hello &lt;a href="/world"&gt;world&lt;/a&gt;')
&gt;&gt;&gt; links = list(doc.iterlinks())
&gt;&gt;&gt; len(links)
1
&gt;&gt;&gt; (el, attr, link, pos) = links[0]
&gt;&gt;&gt; attr
'href'
&gt;&gt;&gt; link
'/world'
&gt;&gt;&gt; pos
0</pre></div></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec344"/>How it works...</h2></div></div></div><a id="id747" class="indexterm"/><p>
<code class="literal">lxml</code> parses the HTML into an <code class="literal">ElementTree</code>. This is a tree structure of parent nodes and child nodes, where each node represents an HTML tag, and contains all the corresponding attributes of that tag. Once the tree is created, it can be iterated on to find elements, such as the <code class="literal">a</code> or <a id="id748" class="indexterm"/>
<strong>anchor tag</strong>. The core tree handling code is in the <code class="literal">lxml.etree</code> module, while the <code class="literal">lxml.html</code> module contains only HTML-specific functions for creating and iterating a tree. For complete documentation, see the lxml tutorial: <a class="ulink" href="http://codespeak.net/lxml/tutorial.html">http://codespeak.net/lxml/tutorial.html</a>.</p></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec345"/>There's more...</h2></div></div></div><p>You'll notice in the previous code that the link is <strong>relative</strong>, meaning it's not an absolute URL. We can make it <strong>absolute</strong> by calling the <code class="literal">make_links_absolute()</code> method with a base URL before extracting the links.</p><div><pre class="programlisting">&gt;&gt;&gt; doc.make_links_absolute('http://hello')
&gt;&gt;&gt; abslinks = list(doc.iterlinks())
&gt;&gt;&gt; (el, attr, link, pos) = abslinks[0]
&gt;&gt;&gt; link
'http://hello/world'</pre></div><div><div><div><div><h3 class="title"><a id="ch09lvl3sec85"/>Extracting links directly</h3></div></div></div><p>If you don't want to do anything other than extract links, you can call the <a id="id749" class="indexterm"/>
<code class="literal">iterlinks()</code> function with an HTML string.</p><div><pre class="programlisting">&gt;&gt;&gt; links = list(html.iterlinks('Hello &lt;a href="/world"&gt;world&lt;/a&gt;'))
&gt;&gt;&gt; links[0][2]
'/world'</pre></div></div><div><div><div><div><h3 class="title"><a id="ch09lvl3sec86"/>Parsing HTML from URLs or files</h3></div></div></div><p>Instead of parsing an HTML string using the <code class="literal">fromstring()</code> function, you can call the <code class="literal">parse()</code> function with a URL or file name. For example, <code class="literal">html.parse("http://my/url")</code> or <code class="literal">html.parse("/path/to/file")</code>. The result will be the same as if you loaded the URL or file into a string yourself, then called <code class="literal">fromstring()</code>.</p></div><div><div><div><div><h3 class="title"><a id="ch09lvl3sec87"/>Extracting links with XPaths</h3></div></div></div><a id="id750" class="indexterm"/><a id="id751" class="indexterm"/><p>Instead of using the <code class="literal">iterlinks()</code> method, you can also get links using the <code class="literal">xpath()</code> method, which is a general way to extract whatever you want from HTML or XML parse trees.</p><div><pre class="programlisting">&gt;&gt;&gt; doc.xpath('//a/@href')[0]
'http://hello/world'</pre></div><p>For more on XPath syntax, see <a class="ulink" href="http://www.w3schools.com/XPath/xpath_syntax.asp">http://www.w3schools.com/XPath/xpath_syntax.asp</a>.</p></div></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec346"/>See also</h2></div></div></div><p>In the next recipe, we'll cover cleaning and stripping HTML.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch09lvl1sec94"/>Cleaning and stripping HTML</h1></div></div></div><p>Cleaning up text is one of the unfortunate but entirely necessary aspects of text processing. When it comes to parsing HTML, you probably don't want to deal with any embedded JavaScript or CSS, and are only interested in the tags and text. Or you may want to remove the HTML entirely, and process only the text. This recipe covers how to do both of these pre-processing actions.</p><div><div><div><div><h2 class="title"><a id="ch09lvl2sec347"/>Getting ready</h2></div></div></div><p>You'll need to install <code class="literal">lxml</code>. See the previous recipe or <a class="ulink" href="http://codespeak.net/lxml/installation.html">http://codespeak.net/lxml/installation.html</a> for installation instructions. You'll also need NLTK installed for stripping HTML.</p></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec348"/>How to do it...</h2></div></div></div><a id="id752" class="indexterm"/><p>We can use the <code class="literal">clean_html()</code> function in the <code class="literal">lxml.html.clean</code> module to remove unnecessary HTML tags and embedded JavaScript from an HTML string.</p><div><pre class="programlisting">&gt;&gt;&gt; import lxml.html.clean
&gt;&gt;&gt; lxml.html.clean.clean_html('&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body onload=loadfunc()&gt;my text&lt;/body&gt;&lt;/html&gt;')
'&lt;div&gt;&lt;body&gt;my text&lt;/body&gt;&lt;/div&gt;'</pre></div><p>The result is much cleaner and easier to deal with. The full module path to the <code class="literal">clean_html()</code> function is used because there's also has a <code class="literal">clean_html()</code> function in the <code class="literal">nltk.util</code> module, but its purpose is different. The <code class="literal">nltk.util.clean_html()</code> function removes all HTML tags when you just want the text.</p><div><pre class="programlisting">&gt;&gt;&gt; import nltk.util
&gt;&gt;&gt; nltk.util.clean_html('&lt;div&gt;&lt;body&gt;my text&lt;/body&gt;&lt;/div&gt;')
'my text'</pre></div></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec349"/>How it works...</h2></div></div></div><a id="id753" class="indexterm"/><p>The <code class="literal">lxml.html.clean_html()</code> function parses the HTML string into a tree, then iterates over and removes all nodes that should be removed. It also cleans nodes of unnecessary attributes (such as embedded JavaScript) using regular expression matching and substitution.</p><a id="id754" class="indexterm"/><p>The <code class="literal">nltk.util.clean_html()</code> function performs a bunch of regular expression substitutions to remove HTML tags. To be safe, it's best to strip the HTML after cleaning it to ensure the regular expressions will match.</p></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec350"/>There's more...</h2></div></div></div><p>The <code class="literal">lxml.html.clean</code> module defines a default <code class="literal">Cleaner</code> class that's used when you call <code class="literal">clean_html()</code>. You can customize the behavior of this class by creating your own instance and calling its <code class="literal">clean_html()</code> method. For more details on this class, see  <a class="ulink" href="http://codespeak.net/lxml/lxmlhtml.html">http://codespeak.net/lxml/lxmlhtml.html</a>.</p></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec351"/>See also</h2></div></div></div><p>The <code class="literal">lxml.html</code> module was introduced in the previous recipe for parsing HTML and extracting links. In the next recipe, we'll cover un-escaping HTML entities.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch09lvl1sec95"/>Converting HTML entities with BeautifulSoup</h1></div></div></div><a id="id755" class="indexterm"/><a id="id756" class="indexterm"/><a id="id757" class="indexterm"/><p>HTML entities are strings such as <code class="literal">&amp;amp;</code> or <code class="literal">&amp;lt;</code>. These are encodings of normal ASCII characters that have special uses in HTML. For example, <code class="literal">&amp;lt;</code> is the entity for <code class="literal">&lt;</code>. You can't just have <code class="literal">&lt;</code> within HTML tags because it is the beginning character for an HTML tag, hence the need to escape it and define the <code class="literal">&amp;lt;</code> entity. The entity code for &amp; is <code class="literal">&amp;amp</code>; which, as we've just seen, is the beginning character for an entity code. If you need to process the text within an HTML document, then you'll want to convert these entities back to their normal characters so you can recognize them and handle them appropriately.</p><div><div><div><div><h2 class="title"><a id="ch09lvl2sec352"/>Getting ready</h2></div></div></div><p>You'll need to install <code class="literal">BeautifulSoup</code>, which you should be able to do with <code class="literal">sudo pip install BeautifulSoup</code> or <code class="literal">sudo easy_install BeautifulSoup</code>. You can read more about <code class="literal">BeautifulSoup</code> at <a class="ulink" href="http://www.crummy.com/software/BeautifulSoup/">http://www.crummy.com/software/BeautifulSoup/</a>.</p></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec353"/>How to do it...</h2></div></div></div><a id="id758" class="indexterm"/><a id="id759" class="indexterm"/><a id="id760" class="indexterm"/><p>
<code class="literal">BeautifulSoup</code> is an HTML parser library that also contains an XML parser called <code class="literal">BeautifulStoneSoup</code>. This is what we can use for entity conversion. It's quite simple: create an instance of <code class="literal">BeautifulStoneSoup</code> given a string containing HTML entities and specify the keyword argument <code class="literal">convertEntities='html'</code>. Convert this instance to a string, and you'll get the ASCII representation of the HTML entities.</p><div><pre class="programlisting">&gt;&gt;&gt; from BeautifulSoup import BeautifulStoneSoup
&gt;&gt;&gt; unicode(BeautifulStoneSoup('&amp;lt;', convertEntities='html'))
u'&lt;'
&gt;&gt;&gt; unicode(BeautifulStoneSoup('&amp;amp;', convertEntities='html'))
u'&amp;'</pre></div><p>It's ok to run the string through multiple times, as long as the ASCII characters are not by themselves. If your string is just a single ASCII character for an HTML entity, that character will be lost.</p><div><pre class="programlisting">&gt;&gt;&gt; unicode(BeautifulStoneSoup('&lt;', convertEntities='html'))
u''
&gt;&gt;&gt; unicode(BeautifulStoneSoup('&lt; ', convertEntities='html'))
u'&lt; '</pre></div><p>To make sure the character isn't lost, all that's required is to have another character in the string that is not part of an entity code.</p></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec354"/>How it works...</h2></div></div></div><p>To convert the HTML entities, <code class="literal">BeautifulStoneSoup</code> looks for tokens that look like an entity and replaces them with their corresponding value in the <code class="literal">htmlentitydefs.name2codepoint</code> dictionary from the Python standard library. It can do this if the entity token is within an HTML tag, or when it's in a normal string.</p></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec355"/>There's more...</h2></div></div></div><a id="id761" class="indexterm"/><p>
<code class="literal">BeautifulSoup</code> is an excellent HTML and XML parser in its own right, and can be a great alternative to <code class="literal">lxml</code>. It's particularly good at handling malformed HTML. You can read more about how to use it at <a class="ulink" href="http://www.crummy.com/software/BeautifulSoup/documentation.html">http://www.crummy.com/software/BeautifulSoup/documentation.html</a>.</p><div><div><div><div><h3 class="title"><a id="ch09lvl3sec88"/>Extracting URLs with BeautifulSoup</h3></div></div></div><a id="id762" class="indexterm"/><p>Here's an example of using <code class="literal">BeautifulSoup</code> to extract URLs, like we did in the <em>Extracting URLs from HTML with lxml</em> recipe. You first create the <code class="literal">soup</code> with an HTML string, call the <code class="literal">findAll()</code> method with <code class="literal">'a'</code> to get all anchor tags, and pull out the <code class="literal">'href'</code> attribute to get the URLs.</p><div><pre class="programlisting">&gt;&gt;&gt; from BeautifulSoup import BeautifulSoup
&gt;&gt;&gt; soup = BeautifulSoup('Hello &lt;a href="/world"&gt;world&lt;/a&gt;')
&gt;&gt;&gt; [a['href'] for a in soup.findAll('a')]
[u'/world']</pre></div></div></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec356"/>See also</h2></div></div></div><p>In the <em>Extracting URLs from HTML with lxml</em> recipe, we covered how to use <code class="literal">lxml</code> to extract URLs from an HTML string, and we covered <em>Cleaning and stripping HTML</em> after that recipe.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch09lvl1sec96"/>Detecting and converting character encodings</h1></div></div></div><a id="id763" class="indexterm"/><a id="id764" class="indexterm"/><p>A common occurrence with text processing is finding text that has a non-standard character encoding. Ideally, all text would be ASCII or UTF-8, but that's just not the reality. In cases when you have non-ASCII or non-UTF-8 text and you don't know what the character encoding is, you'll need to detect it and convert the text to a standard encoding before further processing it.</p><div><div><div><div><h2 class="title"><a id="ch09lvl2sec357"/>Getting ready</h2></div></div></div><p>You'll need to install the <code class="literal">chardet</code> module, using <code class="literal">sudo pip install chardet</code> or <code class="literal">sudo easy_install chardet</code>. You can learn more about <code class="literal">chardet</code> at <a class="ulink" href="http://chardet.feedparser.org/">http://chardet.feedparser.org/</a>.</p></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec358"/>How to do it...</h2></div></div></div><p>Encoding detection and conversion functions are provided in <code class="literal">encoding.py</code>. These are simple wrapper functions around the <code class="literal">chardet</code> module. To detect the encoding of a string, call <code class="literal">encoding.detect()</code>. You'll get back a <code class="literal">dict</code> containing two attributes: <code class="literal">confidence</code> and <code class="literal">encoding</code>. <code class="literal">confidence</code> is a probability of how confident <code class="literal">chardet</code> is that the value for <code class="literal">encoding</code> is correct.</p><div><pre class="programlisting"># -*- coding: utf-8 -*-
import chardet

def detect(s):
  try:
    return chardet.detect(s)
  except UnicodeDecodeError:
    return chardet.detect(s.encode('utf-8'))

  def convert(s):
    encoding = detect(s)['encoding']
    
    if encoding == 'utf-8':
      return unicode(s)
    else:
      return unicode(s, encoding)</pre></div><p>Here's some example code using <code class="literal">detect()</code> to determine character encoding:</p><div><pre class="programlisting">&gt;&gt;&gt; import encoding
&gt;&gt;&gt; encoding.detect('ascii')
{'confidence': 1.0, 'encoding': 'ascii'}
&gt;&gt;&gt; encoding.detect(u'abcdé')
{'confidence': 0.75249999999999995, 'encoding': 'utf-8'}
&gt;&gt;&gt; encoding.detect('\222\222\223\225')
{'confidence': 0.5, 'encoding': 'windows-1252'}</pre></div><a id="id765" class="indexterm"/><a id="id766" class="indexterm"/><p>To convert a string to a standard <code class="literal">unicode</code> encoding, call <code class="literal">encoding.convert()</code>. This will decode the string from its original encoding, then re-encode it as UTF-8.</p><div><pre class="programlisting">&gt;&gt;&gt; encoding.convert('ascii')
u'ascii'	
&gt;&gt;&gt; encoding.convert(u'abcdé')
u'abcd\\xc3\\xa9'
&gt;&gt;&gt; encoding.convert('\222\222\223\225')
u'\u2019\u2019\u201c\u2022'</pre></div></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec359"/>How it works...</h2></div></div></div><a id="id767" class="indexterm"/><p>The <code class="literal">detect()</code> function is a wrapper around <a id="id768" class="indexterm"/>
<code class="literal">chardet.detect()</code> which can handle <code class="literal">UnicodeDecodeError</code> exceptions. In these cases, the string is encoded in UTF-8 before trying to detect the encoding.</p><p>The <code class="literal">convert()</code> function<a id="id769" class="indexterm"/> first calls <code class="literal">detect()</code> to get the <code class="literal">encoding</code>, then returns a <code class="literal">unicode</code> string with the <code class="literal">encoding</code> as the second argument. By passing the <code class="literal">encoding</code> into <code class="literal">unicode()</code>, the string is decoded from the original encoding, allowing it to be re-encoded into a standard encoding.</p></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec360"/>There's more...</h2></div></div></div><p>The comment at the top of the module, <code class="literal"># -*- coding: utf-8 -*-</code>, is a hint to the Python interpreter, telling it which encoding to use for the strings in the code. This is helpful for when you have non-ASCII strings in your source code, and is documented in detail at <a class="ulink" href="http://www.python.org/dev/peps/pep-0263/">http://www.python.org/dev/peps/pep-0263/</a>.</p><div><div><div><div><h3 class="title"><a id="ch09lvl3sec89"/>Converting to ASCII</h3></div></div></div><p>If you want pure ASCII text, with non-ASCII characters converted to ASCII equivalents, or dropped if there is no equivalent character, then you can use the <a id="id770" class="indexterm"/>
<code class="literal">unicodedata.normalize()</code> function.</p><div><pre class="programlisting">&gt;&gt;&gt; import unicodedata
&gt;&gt;&gt; unicodedata.normalize('NFKD', u'abcd\xe9').encode('ascii', 'ignore')
'abcde'</pre></div><p>Specifying <code class="literal">'NFKD'</code> as the first argument ensures the non-ASCII characters are replaced with their equivalent ASCII versions, and the final call to <code class="literal">encode()</code> with <code class="literal">'ignore'</code> as the second argument will remove any extraneous unicode characters.</p></div></div><div><div><div><div><h2 class="title"><a id="ch09lvl2sec361"/>See also</h2></div></div></div><p>Encoding detection and conversion is a recommended first step before doing HTML processing with <code class="literal">lxml</code> or <code class="literal">BeautifulSoup</code>, covered in the <em>Extracting URLs from HTML with lxml</em> and <em>Converting HTML entities with BeautifulSoup</em> recipes.</p></div></div></body></html>