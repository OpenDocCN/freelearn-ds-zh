- en: Chapter 6. Words and Pixels – Working with Unstructured Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most of the data we have looked at thus far is composed of rows and columns
    with numerical or categorical values. This sort of information fits in both traditional
    spreadsheet software and the interactive Python notebooks used in the previous
    exercises. However, data is increasingly available in both this form, usually
    called structured data, and more complex formats such as images and free text.
    These other data types, also known as unstructured data, are more challenging
    than tabular information to parse and transform into features that can be used
    in machine learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: What makes unstructured data challenging to use? It is challenging largely because
    images and text are extremely high dimensional, consisting of a much larger number
    of columns or features than we have seen previously. For example, this means that
    a document may have thousands of words, or an image thousands of individual pixels.
    Each of these components may individually or in complex combinations comprise
    a feature for our algorithms. However, to use these data types in prediction,
    we need to somehow distill this extremely complex data into common features or
    trends that might be used effectively in a model. This often involves both removing
    noise from these data types and finding simpler representations. At the same time,
    the greater inherent complexity of these data types potentially captures more
    information than available in tabular datasets, or may reveal information that
    is not available in any other source.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will explore unstructured data by:'
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning raw text through stemming, stop word removal, and other normalizations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using tokenization and n-grams to find common patterns in textual data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalizing image data and removing noise
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decomposing images into lower dimensional features through several common matrix
    factorization algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with textual data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following example, we will consider the problem of separating text messages
    sent between cell phone users. Some of these messages are spam advertisements,
    and the objective is to separate these from normal communications (Almeida, Tiago
    A., José María G. Hidalgo, and Akebo Yamakami. *Contributions to the study of
    SMS spam filtering: new collection and results.* Proceedings of the 11th ACM symposium
    on Document engineering. ACM, 2011). By looking for patterns of words that are
    typically found in spam advertisements, we could potentially derive a smart filter
    that would automatically remove these messages from a user''s inbox. However,
    while in previous chapters we were concerned with fitting a predictive model for
    this kind of problem, here we will be shifting focus to cleaning up the data,
    removing noise, and extracting features. Once these tasks are done, either simple
    or lower-dimensional features can be input into many of the algorithms we have
    already studied.'
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning textual data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let us start by loading and inspecting the data using the following commands.
    Note that we need to supply column names for this data ourselves:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cleaning textual data](img/B04881_06_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The dataset consists of two columns: the first contains the label (`spam` or
    `ham`) indicating whether the message is an advertisement or a normal message,
    respectively. The second column contains the text of the message. Right at the
    start, we can see a number of problems with using this raw text as input to an
    algorithm to predict the spam/nonspam label:'
  prefs: []
  type: TYPE_NORMAL
- en: The text of each message contains a mixture of upper and lower case letters,
    but this capitalization does not affect the meaning of a word.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many words (*to*, *he*, *the*, and so on) are common, but tell us relatively
    little about the message.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Other issues are subtler:'
  prefs: []
  type: TYPE_NORMAL
- en: When we compare words such as *larger* and *largest*, the most information about
    the meaning of the words is carried by the root, *large*—differentiating between
    the two forms may actually prevent us from capturing common information about
    the presence of the word *large* in a text, since the count of this stem in the
    message will be divided between the variants. Looking only at individual words
    does not tell us about the context in which they are used. Indeed, it may be more
    informative to consider sets of words.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even for words that do not fall into the common category, such as *and*, *the*,
    and *to*, it is sometimes unclear whether a word is present in a document because
    it is common across all documents or whether it contains special information about
    a particular document. For example, in a set of online movie reviews, words such
    as *character* and *film* will appear frequently, but do not help to distinguish
    one review from another since they are common across all reviews. Because the
    English language has a large vocabulary, the size of the resulting feature set
    could be enormous.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let us start by cleaning up the text before delving into the other feature
    issues. We can base by lowercasing each word in the text using the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We then apply this function to each message using the map function we have
    seen in previous examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Inspecting the resulting we can verify that all the letters are now indeed
    lowercase:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cleaning textual data](img/B04881_06_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we want to remove common words and trim the remaining vocabulary to just
    the stem portion of the word that is most useful for predictive modeling. We do
    this using the **natural language toolkit** (**NLTK**) library (Bird, Steven.
    *NLTK: the natural language toolkit*. Proceedings of the COLING/ACL on Interactive
    presentation sessions. Association for Computational Linguistics, 2006.). The
    list of stop words is part of the dataset associated for download with this library;
    if this is your first time opening NLTK, you can use the `nltk.download()` command
    to open a **graphical user interface** (**GUI**) where you can select the content
    you wish to copy to your local machine using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We then define a function to perform stemming:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we again use a lambda function to perform this operation on each message,
    and visually inspect the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![Cleaning textual data](img/B04881_06_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: For example, you can see the stem *joke* has been extracted from *joking*, and
    *avail* from *available*.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have performed lower casing and stemming, the messages are in relatively
    cleaned up form, and we can proceed to generate features for predictive modeling
    from this data.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting features from textual data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In perhaps the simplest possible feature for text data, we use a binary vector
    of *0s* and *1s* to simply record the presence or absence of each word in our
    vocabulary in each message. To do this we can utilize the `CountVectorizer` function
    in the `scikit-learn` library, using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'By default, the result is stored as a *sparse vector*, which means that only
    the non-zero elements are held in memory. To calculate the total size of this
    vector we need to transform it back into a *dense* vector (where all elements,
    even 0, are stored in memory):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: By checking the length of the feature vector created for the first message,
    we can see that it creates a vector of length 7,468 for each message with 1 and
    0 indicating the presence or absence, respectively, of a particular word out of
    all words in this document list.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can check that this length is in fact the same as the vocabulary (union
    of all unique words in the messages) using the following command to extract the
    `vocabulary_ element` of the vectorizer, which also gives a value of 7,468:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that this increases the size of the resulting feature by about 10-fold
    by again inspecting the length of the first row using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'However, even after calculating n-grams, we still have not accounted for the
    fact that some words or n-grams might be common across all messages and thus provide
    little information in distinguishing spam from nonspam. To account for this, instead
    of simply recording the presence or absence of a word (or n-gram), we might compare
    the frequency of words within a document to the frequency across all documents.
    This ratio, the **term-frequency-inverse document frequency** (**tf-idf**) is
    calculated in the simplest form as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Extracting features from textual data](img/B04881_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Where *ti* is a particular term (word or n-gram), *dj* is a particular document,
    *D* is the number of documents, *Vj* is the set of words in document *j*, and
    *vk* is a particular word in document *j*. The subscripted 1 in this formula is
    known as an **Indicator Function**, which returns `1` if the subscripted condition
    is `true`, and `0` otherwise. In essence, this formula compares the frequency
    (count) of a word within a document to the number of documents that contain this
    word. As the number of documents containing the word decreases, the denominator
    decreases, and thus the overall formula becomes larger from dividing by a value
    much less than `1`. This is balanced by the frequency of the word within a document
    in the numerator. Thus, the `tf-idf` score will more heavily weight words that
    are present at greater frequency within a document compared to those common among
    all documents and thus might be indicative of special features of a particular
    message.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the formula above represents only the simplest version of this expression.
    There are also variants in which we might logarithmically transform the counts
    (to offset the bias from large documents), or scale the numerator by the maximum
    frequency found for any term within a document (again, to offset bias that longer
    documents could have higher term frequencies than shorter documents by virtue
    of simply having more words) (Manning, Christopher D., Prabhakar Raghavan, and
    Hinrich Schütze. *Scoring, term weighting and the vector space model.* Introduction
    to Information Retrieval 100 (2008): 2-4.). We can apply `tf-idf` to the spam
    data using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see the effect of this transformation by taking the maximum value across
    rows using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Where the '1' argument to max indicates that the function is applied along rows
    (instead of columns, which would be specified with '0' ). When our features consisted
    only of binary values, the maximum across each rows would be 1, but we can see
    that it is now a float value.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final text feature we will discuss is concerned with condensing our feature
    set. Simply put, as we consider larger and larger vocabularies, we will encounter
    many words that are so infrequent as to almost never appear. However, from a computational
    standpoint, even a single instance of a word in one document is enough to expand
    the number of columns in our text features for all documents. Given this, instead
    of directly recording whether a word is present, we might think of compressing
    this space requirement so that we use fewer columns to represent the same dataset.
    While in some cases, two words might map to the same column, in practice this
    happens infrequently enough due to the long-tailed distribution of word frequencies
    that it can serve as a handy way to reduce the dimensionality of our text data.
    To perform this mapping, we make use of a hash function that takes as input a
    word and outputs a random number (column location) that is keyed to the value
    of that string. The number of columns we ultimately map to in our transformed
    dataset is controlled by the `n_features` argument to the `HashingVectorizer`,
    which we can apply to our dataset using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Using dimensionality reduction to simplify datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Even though using the `HashingVectorizer` allows us to reduce the data to a
    set of 1,024 columns from a feature set that was much larger, we are still left
    with many variables in our dataset. Intuition tells us that some of these features,
    either before or after the application of the `HashingVectorizer`, are probably
    correlated. For example, a set of words may co-occur in a document that is spam.
    If we use n-grams and the words are adjacent to one another, we could pick up
    on this feature, but not if the words are simply present in the message but separated
    by other text. The latter might occur, for example, if some common terms are in
    the first sentence of the message, while others are near the end.More broadly,
    given a large set of variables such as we have already seen for textual data,
    we might ask whether we could represent these data using a more compact set of
    features. In other words, is there an underlying pattern to the variation in thousands
    of variables that may be extracted by calculating a much smaller number of features
    representing patterns of correlation between individual variables? In a sense,
    we already saw several examples of this idea in [Chapter 3](ch03.html "Chapter 3. Finding
    Patterns in the Noise – Clustering and Unsupervised Learning"), *Finding Patterns
    in the Noise – Clustering and Unsupervised Learning*, in which we reduced the
    complexity of a dataset by aggregating individual datapoints into clusters. In
    the following examples, we have a similar goal, but rather than aggregating individual
    datapoints, we want to capture groups of correlated variables.
  prefs: []
  type: TYPE_NORMAL
- en: While we might achieve this goal in part through the variable selection techniques
    such as regularization, which we discussed in the [Chapter 4](ch04.html "Chapter 4. Connecting
    the Dots with Models – Regression Methods"), *Connecting the Dots with Models
    – Regression Methods*, we do not necessarily want to remove variables, but rather
    capture their common patterns of variation.
  prefs: []
  type: TYPE_NORMAL
- en: Let us examine some of the common methods of dimensionality reduction and how
    we might choose between them for a given problem.
  prefs: []
  type: TYPE_NORMAL
- en: Principal component analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the most commonly used methods of dimensionality reduction is **Principal
    Component Analysis** (**PCA**). Conceptually, PCA computes the axes along which
    the variation in the data is greatest. You may recall that in [Chapter 3](ch03.html
    "Chapter 3. Finding Patterns in the Noise – Clustering and Unsupervised Learning"),
    *Finding Patterns in the Noise – Clustering and Unsupervised Learning*, we calculated
    the eigenvalues of the adjacency matrix of a dataset to perform spectral clustering.
    In PCA, we also want to find the eigenvalue of the dataset, but here, instead
    of any adjacency matrix, we will use the covariance matrix of the data, which
    is the relative variation within and between columns. The covariance for columns
    `xi` and `xj` in the data matrix `X` is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Principal component analysis](img/B04881_06_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This is the average product of the offsets from the mean column values. We
    saw this value before when we computed the correlation coefficient in [Chapter
    3](ch03.html "Chapter 3. Finding Patterns in the Noise – Clustering and Unsupervised
    Learning"), *Finding Patterns in the Noise – Clustering and Unsupervised Learning*,
    as it is the denominator of the Pearson coefficient. Let us use a simple example
    to illustrate how PCA works. We will make a dataset in which the six columns are
    derived from the same underlying normal distribution, one of which is given reversed
    in sign, using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that each of our columns has mean `0` and standard deviation `1`. If this
    were not the case, we could use the scikit-learn utility StandardScaler as we
    discussed in [Chapter 3](ch03.html "Chapter 3. Finding Patterns in the Noise –
    Clustering and Unsupervised Learning"), *Finding Patterns in the Noise – Clustering
    and Unsupervised Learning*, when we normalized data for use in k means clustering.
    We might simply center the variables at `0` and use the resulting covariance matrix
    if we believe that the differences in scale of the variables are important to
    our problem. Otherwise, differences in scale will tend to be reflected by the
    differing variance values within the columns of the data, so our resulting PCA
    will reflect not only correlations within variables but also their differences
    in magnitude. If we do not want to emphasize these differences and are only interested
    in the relative correlation among variables, we can also divide each column of
    the data by its standard deviation to give each column a variance of 1\. We could
    also potentially run PCA not on the covariance matrix, but the Pearson correlation
    matrix between variables, which is already naturally scaled to 0 and a constant
    range of a values (from -1 to 1) (Kromrey, Jeffrey D., and Lynn Foster-Johnson.
    *Mean centering in moderated multiple regression: Much ado about nothing.* Educational
    and Psychological Measurement 58.1 (1998): 42-67.). For now, we can compute the
    covariance matrix of our data with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Recalling our discussion of spectral clustering in [Chapter 3](ch03.html "Chapter 3. Finding
    Patterns in the Noise – Clustering and Unsupervised Learning"), *Finding Patterns
    in the Noise – Clustering and Unsupervised Learning*, if we consider the covariance
    matrix as a stretching operation on a vector, then, if we find the vectors that
    lie along these directions of distortion, we have in a sense found the axes that
    define the variation in the data. If we then compare the eigenvalues of these
    vectors, we could determine if one or more of these directions reflect a greater
    proportion of the overall variation of the data. Let us compute the eigenvalues
    and vectors of the covariance matrix using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives the following eigenvalue variable as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see that most of the eigenvalues are effectively zero, except the second.
    This reflects the fact that the data we constructed, despite having six columns,
    is effectively derived from only one dataset (a normal distribution). Another
    important property of these eigenvectors is that they are orthogonal, which means
    that they are at right angles to each other in n-dimensional space: if we were
    to take a dot product between them, it would be 0, and they thus represent independent
    vectors that, when linearly combined, can be used to represent the dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we were to multiply the data by the eigenvector corresponding to this second
    eigenvalue, we would project the data from a six-dimensional to a one-dimensional
    space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we needed to transpose the data to have the 100 rows and 6 columns,
    as we initially constructed it as a list of 6 columns, which NumPy interprets
    as instead having 6 rows and 100 columns. The resulting histogram is as shown
    in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Principal component analysis](img/B04881_06_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In other words, by projecting the data onto the axis of greatest variance,
    we have recovered that fact that this six-column data was actually generated from
    a single distribution. Now if we instead use the PCA command, we get a similar
    result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'When we extract the `explained_variance_ratio_`, the algorithm has effectively
    taken the preceding eigenvalues, ordered them by magnitude, and divided by the
    largest one, giving:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'If we were to plot these as a barplot, a visualization known as a `scree plot`
    could help us determine how many underlying components are represented in our
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This generates the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Principal component analysis](img/B04881_06_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Evidently, only the first component carries any variance, represented by the
    height of the bar, with all other components being near 0 and so not appearing
    in the plot. This sort of visual analysis is comparable to how we looked for an
    elbow in the inertia function for k-means in [Chapter 3](ch03.html "Chapter 3. Finding
    Patterns in the Noise – Clustering and Unsupervised Learning"), *Finding Patterns
    in the Noise – Clustering and Unsupervised Learning*, as a function of k to determine
    how many clusters were present in the data.We can also extract the data projected
    onto the first principal components and see a similar plot as shown previously
    when we projected the data onto an eigenvector of the covariance matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![Principal component analysis](img/B04881_06_38.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Why are they not exactly identical? While conceptually PCA computes the eigenvalues
    of the covariance matrix, in practice most packages do not actually implement
    the calculation we illustrated previously for purposes of numerical efficiency.
    Instead, they employ a matrix operation known as the **Singular Value Decomposition**
    (**SVD**), which seeks to represent a covariance matrix of *X* as a set of lower
    dimensional row and column matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Principal component analysis](img/B04881_06_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Where if *X* is an *n* by *m*, W may be *n* by *k*, where *k << m*. Here, σ
    represents a matrix with 0 everywhere but the diagonal, which contains non-zero
    entries. Thus, the covariance matrix is represented as the product of two smaller
    matrices and a scaling factor given by the diagonal elements in σ. Instead of
    calculating all eigenvectors of the covariance matrix, as we did previously, we
    can ask only for the k columns or WT we think are likely to be significant judged
    by the sort of scree plot analysis we demonstrated above. However, when we project
    the data onto the principal components we obtain through this method, the calculation
    of the SVD can potentially give different signs to the projection of the data
    on the principal components, even if the relative magnitude and signs of these
    components remains the same. Thus, when we look at the scores assigned to a given
    row of data after projecting it onto the first k principal components, we should
    analyze them relative to other values in the dataset, just as when we examined
    the coordinates produced by Multidimensional Scaling in [Chapter 3](ch03.html
    "Chapter 3. Finding Patterns in the Noise – Clustering and Unsupervised Learning"),
    *Finding Patterns in the Noise – Clustering and Unsupervised Learning*. Details
    of the SVD calculation used by the default scikit-learn implementation of PCA
    are given in (Tipping, Michael E., and Christopher M. Bishop. *Probabilistic principal
    component analysis*. Journal of the Royal Statistical Society: Series B (Statistical
    Methodology) 61.3 (1999): 611-622.).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have examined conceptually what PCA calculates, let us see if it
    can help us reduce the dimensionality of our text dataset. Let us run PCA on the
    n-gram feature set from above, asking for 100 components. Note that because the
    original dataset is a sparse matrix and PCA requires a dense matrix as an input,
    we need to convert it using `toarray()`. Also, to retain the right dimensionality
    for use with the PCA fit function, we need to transpose the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'If we make a scree plot of total variance explained by the first `10` principal
    components of this dataset, we see that we will probably require a relatively
    large number of variables to capture the variation in our data since the upward
    trend in variance explained is relatively smooth:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '![Principal component analysis](img/B04881_06_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We could also visualize this by looking at the cumulative variance explained
    using *k* components using the following curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '![Principal component analysis](img/B04881_06_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'A word on normalization: in practice, for document data, we might not want
    to scale the data by subtracting the mean and dividing by the variance as the
    data is mostly binary. Instead, we would just apply the SVD to a binary matrix
    or perhaps the tF-idf scores we computed previously, an approach also known as
    **Latent Semantic Indexing** (**LSI**) (Berry, Michael W., Susan T. Dumais, and
    Gavin W. O''Brien. *Using linear algebra for intelligent information retrieval*.
    SIAM review 37.4 (1995): 573-595; Laham, T. K. L. D., and Peter Foltz. *Learning
    human-like knowledge by singular value decomposition: A progress report*. Advances
    in Neural Information Processing Systems 10: Proceedings of the 1997 Conference.
    Vol. 10\. MIT Press, 1998.). CUR decomposition and nonnegative matrix factorization'
  prefs: []
  type: TYPE_NORMAL
- en: 'What drawbacks might there be to using PCA to reduce the dimensionality of
    a dataset? For one, the components (covariance matrix eigenvectors) generated
    by PCA are still essentially mathematical entities: the patterns in variables
    represented by these axes might not actually correspond to any element of the
    data, but rather a linear combination of them. This representation is not always
    easily interpretable, and can particularly difficult when trying to convey the
    results of such analyses to domain experts to generate subject-matter specific
    insights. Second, the fact that PCA produces negative values in its eigenvectors,
    even for positive-only data such as text (where a term cannot be negatively present
    in a document, just 0, 1, a count, or a frequency), is due to the fact that the
    data is linearly combined using these factors. In other words, positive and negative
    values may be summed together when we project the data onto its components through
    matrix multiplication, yielding an overall positive value for the projection.
    Again, it may be preferable to have factors that give some insight into the structure
    of the data itself, for example, by giving a factor that consists of binary indicators
    for a group of words that tend to co-occur in a particular group of documents.
    These goals are addressed by two other matrix factorization techniques: CUR Decomposition
    and Non-negative Matrix Factorization.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Like the SVD used in PCA, CUR attempts to represent a matrix of data X as a
    product of lower dimensional matrices. Here, instead of eigenvectors, the CUR
    decomposition attempts to find the set of columns and rows of the matrix that
    best represent the dataset as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Principal component analysis](img/B04881_06_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Where *C* is a matrix of *c* columns of the original dataset, *R* is a set
    of *r* rows from the original dataset, and *U* is a matrix of scaling factors.
    The *c* columns and *r* rows used in this reconstruction are sampled from the
    columns and rows of the original matrix, with probability proportional to the
    `leverage score`, given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Principal component analysis](img/B04881_06_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Where *lvj* is the statistical leverage for column (row) *j*, *k* is the number
    of components in the SVD of *X*, and *vj* are the *jth* elements of these *k*
    component vectors. Thus, columns (rows) are sampled with high probability if they
    contribute significantly to the overall norm of the matrix''s singular values,
    meaning they are also have a major influence on the reconstruction error from
    the SVD (for example, how well the SVD approximates the original matrix) (Chatterjee,
    Samprit, and Ali S. Hadi. Sensitivity analysis in linear regression. Vol. 327\.
    John Wiley & Sons, 2009; Bodor, András, et al. *rCUR: an R package for CUR matrix
    decomposition*. BMC bioinformatics 13.1 (2012): 1).'
  prefs: []
  type: TYPE_NORMAL
- en: While this decomposition is not expected to approximate the original dataset
    with the same accuracy as the SVD approach used in PCA, the resulting factors
    may be easier to interpret since they are actual elements of the original dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Please note that while we use SVD to determine sampling probabilities for the
    columns and rows, the final factorization of CUR does not.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many algorithms for generating a CUR decomposition (Mahoney, Michael
    W., and Petros Drineas. *CUR matrix decompositions for improved data analysis.
    Proceedings of the National Academy of Sciences 106.3 (2009): 697-702\. Boutsidis,
    Christos, and David P. Woodruff. Optimal cur matrix decompositions*. Proceedings
    of the 46th Annual ACM Symposium on Theory of Computing. ACM, 2014). CUR decomposition
    is implemented in the `pymf` library, and we can call it using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The `crank` and `rrank` parameters indicate how many rows and columns, respectively,
    should be chosen from the original matrix in the process of performing the decomposition.
    We can then examine which columns (words from the vocabulary) were chosen in this
    reconstruction using the following commands to print these significant words whose
    indices are contained in the cur object''s .`_cid` (column index) element. First
    we need to collect a list of all words in the vocabulary of our spam dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the `vocabulary_` variable returned by the `CountVectorizer` is a dictionary
    giving the positions of terms in the array to which they are mapped, we need to
    construct our array by placing the word at the position given by this dictionary.
    Now we can print the corresponding words using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Like CUR, nonnegative matrix factorization attempts to find a set of positive
    components that represents the structure of a dataset (Lee, Daniel D., and H.
    Sebastian Seung. *Learning the parts of objects by non-negative matrix factorization.*
    Nature 401.6755 (1999): 788-791; Lee, Daniel D., and H. Sebastian Seung. *Algorithms
    for non-negative matrix factorization.* Advances in neural information processing
    systems. 2001.; P. Paatero, U. Tapper (1994). Paatero, Pentti, and Unto Tapper.
    *Positive matrix factorization: A non-negative factor model with optimal utilization
    of error estimates of data values*. Environmetrics 5.2 (1994): 111-126\. Anttila,
    Pia, et al. *Source identification of bulk wet deposition in Finland by positive
    matrix factorization*. Atmospheric Environment 29.14 (1995): 1705-1718.). Similarly,
    it tries to reconstruct the data using:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Principal component analysis](img/B04881_06_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Where *W* and *H* are lower dimensional matrices that when multiplied, reconstruct
    *X*; all three of *W*, *H*, and *X* are constrained to have no negative values.
    Thus, the columns of *X* are linear combinations of *W*, using *H* as the coefficients.
    For example, if the rows of *X* are words and the columns are documents, then
    each document in *X* is represented as a linear combination of underlying document
    types in *W* with weighted given by *H*. Like the elements returned by CUR decomposition,
    the components *W* from nonnegative matrix factorization are potentially more
    interpretable than the eigenvectors we get from PCA.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several algorithms to compute *W* and *H*, with one of the simplest
    being through multiplicative updates (Lee, Daniel D., and H. Sebastian Seung.
    *Algorithms for non-negative matrix factorization*. Advances in neural information
    processing systems. 2001). For example, if we want to minimize the Euclidean distance
    between *X* and *WH*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Principal component analysis](img/B04881_06_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We can calculate the derivative of this value with respective to *W*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Principal component analysis](img/B04881_06_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Then to update *W* we multiply at each step by this gradient:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Principal component analysis](img/B04881_06_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'And the same for *H*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Principal component analysis](img/B04881_06_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'These steps are repeated until the values of *W* and *H* converge. Let us examine
    what components we retrieve from our text data when we use NMF to extract components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: We can then look at the words represented by the components in NMF, where the
    words have a large value in the components matrix resulting from the decomposition.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that they appear to capture distinct groups of words, but are any
    correlated with distinguishing spam versus nonspam? We can transform our original
    data using the NMF decomposition, which will give the weights for linearly combining
    these features (for example, the weights to linearly combine the 10 basis documents
    we get from the decomposition to reconstruct the message) using the command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let us plot the average weight assigned to each of these `nmf` factors
    for the normal and spam messages. We can do this by plotting a bar chart where
    the *x* axis are the 10 `nmf` factors, and the *y* axis are the average weight
    assigned to this factor for a subset of documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '![Principal component analysis](img/B04881_06_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '![Principal component analysis](img/B04881_06_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Promisingly, the factors 8 and 9 seem to have very different average weights
    between these two classes of messages. In fact, we may need fewer than 10 factors
    to represent the data, since these two classes may well correspond to the underlying
    spam versus nonspam messages.
  prefs: []
  type: TYPE_NORMAL
- en: Latent Dirichlet Allocation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A related method of decomposing data into an interpretable set of features
    is **Latent Dirichlet Allocation** (**LDA**), a method initially developed for
    textual and genetics data that has since been extended to other areas (Blei, David
    M., Andrew Y. Ng, and Michael I. Jordan. *Latent dirichlet allocation*. the Journal
    of machine Learning research 3 (2003): 993-1022\. Pritchard, Jonathan K., Matthew
    Stephens, and Peter Donnelly. *Inference of population structure using multilocus
    genotype data*. Genetics 155.2 (2000): 945-959.). Unlike the methods we looked
    at previously, where the data is represented as a set of lower dimensional matrices
    that, when multiplied, approximate the original data, LDA uses a probability model.
    This model is often explained using a plate diagram that illustrates the dependencies
    among the variables, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Latent Dirichlet Allocation](img/B04881_06_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'What exactly does this diagram describe? It is what is known as a generative
    model: a set of instructions by which to generate a probability distribution over
    documents. The idea is comparable to a distribution such as the Gaussian ''bell-curve''
    you are probably familiar with, except here instead of drawing real numbers from
    the distribution we sample documents. Generative models may be contrasted with
    the predictive methods which we have seen in previous chapters that attempts to
    fit the data to a response (as in the regression or classification models we have
    studied in [Chapters 4](ch04.html "Chapter 4. Connecting the Dots with Models
    – Regression Methods"), *Connecting the Dots with Models – Regression Methods*,
    and [Chapter 5](ch05.html "Chapter 5. Putting Data in its Place – Classification
    Methods and Analysis"), *Putting Data in its Place – Classification Methods and
    Analysis*), instead of simply generate samples of the data according to a distribution.
    The plate diagram represents the components of this generative model, and we can
    think of this model as the following series of steps to generate a document:Initialize
    a Dirichlet distribution to choose from a set of topics. These topics are analogous
    to the components we found in NMF, and can be thought of as *basis documents*
    representing groups of commonly co-occurring words. The Dirichlet distribution
    is given by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Latent Dirichlet Allocation](img/B04881_06_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The preceding formula gives the probability of observing a given distribution
    of items (here topics) among `K` classes and can be used to sample a vector of
    `K` class memberships (for example, sample a random vector giving what fraction
    of documents in the collection belong to a given topic). The alpha parameter in
    the Dirichlet distribution is used as an exponent of the K category probabilities
    and increases the significance ascribed to a particular component (for example,
    a more frequent topic). The term `B` is the beta function, which is simply a normalization
    term. We use the Dirichlet distribution in step 1 to generate a per-topic probability
    distribution for a document *i*. This distribution would be, for example, a series
    of weights that sum to 1 giving the relative probability that a document belongs
    to a given topic. This is the parameter θ in the plate diagram. M represents the
    number of documents in our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: For each of the *N* word positions in the document, choose a topic Z from the
    distribution θ. Each of the M topics has a Dirichlet distribution with parameter
    β instead of giving per word probabilities, given by ϕ. Use this distribution
    to choose word in each N position in a document.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 2–4 for each word position for each document in a dataset to generate
    a group of documents.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the previous diagram, the numbers (**M**, **N**, **K**) inside the rectangles
    indicate the number of time that the variables represented by circles are generated
    in the generative model. Thus, the words w, being innermost, are generated *N
    × M* times. You can also notice that the rectangles enclose variables that are
    generated the same number of times, while arrows indicate dependence among variables
    during this data generation process. You can also now appreciate where the name
    of this model comes from, as a document is latently allocated among many topics,
    just as we used the factors in NMF to find linear combinations of 'basis documents'
    that could reconstruct our observed data.
  prefs: []
  type: TYPE_NORMAL
- en: 'This recipe can also be used to find a set of topics (for example word probability
    distributions) that fit a dataset, assuming the model described previously was
    used to generate the documents. Without going into the full details of the derivation,
    we randomly initialize a fixed number of *K* topics and run the model, as described
    previously, by always sampling a document''s topic, given all other documents,
    and a word, given the probability of all other words in the document. We then
    update the parameters of the model based on the observed data and use the updated
    probabilities to generate the data again. Over many iterations, this process,
    known as Gibbs sampling, will converge from randomly initialized values to a set
    of model parameters that best fit the observed document data. Let us now fit an
    LDA model to the spam dataset using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'As with NMF, we can examine the highest probability words for each topic using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Likewise, we can see if these topics represent a meaningful separation between
    the spam and nonspam messages. First we find the topic distribution among the
    10 latent topics for each document using the following `transform` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'This is analogous to the weights we calculated in NMF. We can now plot the
    average topic weight for each message class as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '![Latent Dirichlet Allocation](img/B04881_06_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '![Latent Dirichlet Allocation](img/B04881_06_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Again, promisingly, we find a different average weight for topic 5 for spam
    than nonspam, indicating that the LDA model has successfully separated out the
    axis of variation we are interested in for classification purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Using dimensionality reduction in predictive modeling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The analysis we have outlined previously has been largely devoted to trying
    to extract a lower-dimensional representation of a text collection by finding
    a smaller set of components that capture the variation among individual documents.
    In some cases, this sort of analysis can be useful as an exploratory data analysis
    tool, which, like the clustering techniques we described in [Chapter 3](ch03.html
    "Chapter 3. Finding Patterns in the Noise – Clustering and Unsupervised Learning"),
    *Finding Patterns in the Noise – Clustering and Unsupervised Learning*, allows
    us to understand the structure in a dataset. We might even combine clustering
    and dimensionality reduction, which is in essence the idea of spectral clustering
    as we examined in [Chapter 3](ch03.html "Chapter 3. Finding Patterns in the Noise
    – Clustering and Unsupervised Learning"), *Finding Patterns in the Noise – Clustering
    and Unsupervised Learning* using SVD to reduce the adjacency matrix to a more
    compact representation and then clustering this reduced space to yield a cleaner
    separation between datapoints.
  prefs: []
  type: TYPE_NORMAL
- en: Like the groups assigned through clustering, we can also potentially use the
    components derived from these dimensionality reduction methods as features in
    a predictive model. For example, the NMF components we extracted previously could
    be used as inputs to a classification model to separate spam from nonspam messages.
    We have even seen this use earlier, as the online news popularity dataset we used
    in [Chapter 4](ch04.html "Chapter 4. Connecting the Dots with Models – Regression
    Methods"), *Connecting the Dots with Models – Regression Methods*, had columns
    derived from LDA topics. Like the regularization methods we saw in [Chapter 4](ch04.html
    "Chapter 4. Connecting the Dots with Models – Regression Methods"), *Connecting
    the Dots with Models – Regression Methods*, dimensionality reduction can help
    reduce overfitting by extracting the underlying correlations among variables since
    these lower-dimensional variables are often less noisy than using the whole feature
    space. Now that we have seen how dimensionality reduction could help us find structure
    in textual data, let us examine another class of potentially high-dimensional
    data found in images.
  prefs: []
  type: TYPE_NORMAL
- en: Images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like textual data, images are potentially noisy and complex. Furthermore, unlike
    language, which has a structure of words, paragraphs, and sentences, images have
    no predefined rules that we might use to simplify raw data. Thus, much of image
    analysis will involve extracting patterns from the input's features, which are
    ideally interpretable to a human analyst based only on the input pixels.
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning image data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the common operations we will perform on images is to enhance contrast
    or change their color scale. For example, let us start with an example image of
    a coffee cup from the `skimage` package, which you can import and visualize using
    the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cleaning image data](img/B04881_06_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In Python, this image is represented as a three-dimensional matrix with the
    dimensions corresponding to height, width, and color channels. In many applications,
    the color is not of interest, and instead we are trying to determine common shapes
    or features in a set of images that may be differentiated based on grey scale
    alone. We can easily convert this image into a grey scale version using the commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '![Cleaning image data](img/B04881_06_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'A frequent task in image analysis is to identify different regions or objects
    within an image. This can be made more difficult if the pixels are clumped into
    one region (for example, if there is very strong shadow or a strong light in the
    image), rather than evenly distributed along the intensity spectrum. To identify
    different objects, it is often desirable to have these intensities evenly distributed,
    which we can do by performing histogram equalization using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '![Cleaning image data](img/B04881_06_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'To see the effect of this normalization, we can plot the histogram of pixels
    by intensity before and after the transformation with the command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives the following pixel distribution for the uncorrected image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Cleaning image data](img/B04881_06_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The `ravel()` command used here is used to flatten the 2-d array we started
    with into a single vector that may be input to the histogram function. Similarly,
    we can plot the distribution of pixel intensities following normalization using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '![Cleaning image data](img/B04881_06_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Thresholding images to highlight objects
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another common task for image analysis is to identify individual objects within
    a single image. To do so, we need to choose a threshold to binarize an image into
    white and black regions and separate overlapping objects. For the former, we can
    use thresholding algorithms such as Otsu thresholding (Otsu, Nobuyuki. *A threshold
    selection method from gray-level histograms.* Automatica 11.285-296 (1975): 23-27),
    which uses a *structuring element* (such as disk with n pixels) and attempts to
    find a pixel intensity, which will best separate pixels inside that structuring
    element into two classes (for example, black and white). We can imagine rolling
    a disk over an entire image and doing this calculation, resulting in either a
    local value within the disk or a global value that separates the image into foreground
    and background. We can then turn the image into a binary mask by thresholding
    pixels above or below this value.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate, let us consider a picture of coins, where we want to separate
    the coins from their background. We can visualize the histogram-equalized coin
    image using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '![Thresholding images to highlight objects](img/B04881_06_32.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'One problem we can see is that the background has a gradient of illumination
    increasing toward the upper left corner of the image. This difference doesn''t
    change the distinction between background and objects (coins), but because part
    of the background is in the same intensity range as the coins, it will make it
    difficult to separate out the coins themselves. To subtract the background, we
    can use the closing function, which sequentially erodes (removes white regions
    with size less than the structuring element) and then dilates (if there is a white
    pixel within the structuring element, all elements within the structuring element
    are flipped to white). In practice, this means we remove small white specks and
    enhance regions of remaining light color. If we then subtract this from the image,
    we subtract the background, as illustrated here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '![Thresholding images to highlight objects](img/B04881_06_33.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that we have removes the background, we can apply the Otsu thresholding
    algorithm mentioned previously to find the ideal pixel to separate the image into
    background and object using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '![Thresholding images to highlight objects](img/B04881_06_34.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The image has now been segmented into coins and non-coin regions. We could use
    this segmented image to count the number coins, to highlight the coins in the
    original image using the regions obtained above as a *mask*, for example if we
    want to record pixel data only from the coin regions as part of a predictive modeling
    feature using image data.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction for image analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once we have our images appropriately cleaned, how can we turn them into more
    general features for modeling? One approach is to try to capture common patterns
    of variation between a group of images using the same dimensionality reduction
    techniques as we used previously for document data. Instead of words in documents,
    we have patterns of pixels within an image, but otherwise the same algorithms
    and analysis largely apply. As an example, let us consider a set of images of
    faces ([http://www.geocities.ws/senthilirtt/Senthil%20Face%20Database%20Version1](http://www.geocities.ws/senthilirtt/Senthil%20Face%20Database%20Version1))
    which we can load and examine using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '![Dimensionality reduction for image analysis](img/B04881_06_35.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'For each of these two-dimenional images, we want to convert it into a vector
    just as we did when we plotted the pixel frequency histograms during our discussion
    of normalization. We will also construct a set where the average pixel intensity
    across faces has been subtracted from each pixel, yielding each face as an offset
    from the *average face* in the data through the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'We consider two possible ways to factor faces into a more general features.
    The first is to use PCA to extract the major vectors of variation in this data—these
    vectors happen to also look like faces. Since they are formed from the eigenvalues
    of the covariance matrix, these sorts of features are sometimes known as eigenfaces.
    The following commands illustrate the result of performing PCA on the face dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '![Dimensionality reduction for image analysis](img/B04881_06_36.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'How much variation in the face data is captured by the principal components?
    In contrast to the document data, we can see that using PCA even with only three
    components allows to explain around two-thirds of the variation in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '![Dimensionality reduction for image analysis](img/B04881_06_37.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We could also apply NMF, as we described previously, to find a set of basis
    faces. You can notice from the preceding heatmap that the eigenfaces we extracted
    can have negative values, which highlights one of the interpretational difficulties
    we mentioned previously: we cannot really have negative pixels (since , so a latent
    feature with negative elements is hard to interpret. In contrast, the components
    we extract using NMF will look much more like elements of the original dataset,
    as shown below using the commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '![Dimensionality reduction for image analysis](img/B04881_06_39.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Unlike the eigenfaces, which resemble averaged versions of many images, the
    NMF components extracted from this data look like individual faces. While we will
    not go through the exercise here, we could even apply LDA to image data to find
    topics represented by distributions of pixels and indeed it has been used for
    this purpose (Yu, Hua, and Jie Yang. *A direct LDA algorithm for high-dimensional
    data—with application to face recognition*. Pattern recognition 34.10 (2001):
    2067-2070; Thomaz, Carlos E., et al. *Using a maximum uncertainty LDA-based approach
    to classify and analyse MR brain images*. Medical Image Computing and Computer-Assisted
    Intervention–MICCAI 2004\. Springer Berlin Heidelberg, 2004\. 291-300.).'
  prefs: []
  type: TYPE_NORMAL
- en: While the dimensionality reduction techniques we have discussed previously are
    useful in the context of understanding datasets, clustering, or modeling, they
    are also potentially useful in storing compressed versions of data. Particularly
    in model services such as the one we will develop in [Chapter 8](ch08.html "Chapter 8. Sharing
    Models with Prediction Services"), *Sharing Models with Prediction Services*,
    being able to store a smaller version of the data can reduce system load and provide
    an easier way to process incoming data into a form that can be understood by a
    predictive model. We can quickly extract the few components we need, for example,
    from a new piece of text data, without having to persist the entire record.
  prefs: []
  type: TYPE_NORMAL
- en: 'Case Study: Training a Recommender System in PySpark'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To close this chapter, let us look at an example of how we might generate a
    large-scale recommendation system using dimensionality reduction. The dataset
    we will work with comes from a set of user transactions from an online store (Chen,
    Daqing, Sai Laing Sain, and Kun Guo. *Data mining for the online retail industry:
    A case study of RFM model-based customer segmentation using data mining*. Journal
    of Database Marketing & Customer Strategy Management 19.3 (2012): 197-208). In
    this model, we will input a matrix in which the rows are users and the columns
    represent items in the catalog of an e-commerce site. Items purchased by a user
    are indicated by a 1\. Our goal is to factorize this matrix into 1 x k *user factors*
    (row components) and k x 1 *item factors* (column components) using k components.
    Then, presented with a new user and their purchase history, we can predict what
    items they are like to buy in the future, and thus what we might recommend to
    them on a homepage. The steps to do so are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a user''s prior purchase history as *a* vector *p*. We imagine this
    vector is the product of an unknown *user factor* component *u* with the item
    factors *i* we obtained through matrix factorization: each element of the vector
    *p* is then the dot product of this unknown user factor with the item factor for
    a given item. Solve for the unknown user factor *u* in the equation:![Case Study:
    Training a Recommender System in PySpark](img/B04881_06_12.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Given the item factors *i* and the purchase history *p*, using matrix.Use the
    resulting user factor *u*, take the dot product with each item factor to obtain
    and sort by the result to determine a list of the top ranked items.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that we have described what is happening *under the hood* in this example,
    we can begin to parse this data using the following commands. First, we create
    a parsing function to read the 2nd and 7th columns of the data containing the
    item ID and user ID, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we read in the file and convert the user and item IDs, which are both
    string, into a numerical index by incrementing a counter as we add unique items
    to a dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Next, we convert the resulting array of purchase into an `rdd` and convert the
    resulting entries into Rating objects -- a (user, item, rating) tuple. Here, we
    will just indicate that the purchase occurred by giving a rating of 1.0 to all
    observed purchases, but we could just as well have a system where the ratings
    indicate user preference (such as movie ratings) and follow a numerical scale.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can fit the matrix factorization model using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: The algorithm for matrix factorization used in PySpark is **Alternating Least
    Squares** (**ALS**), which has parameters for the number of row (column) components
    chosen (`k`) and a regularization parameter λ which we did not specify here, but
    functions similarly to its role in the regression algorithms we studied in [Chapter
    4](ch04.html "Chapter 4. Connecting the Dots with Models – Regression Methods"),
    *Connecting the Dots with Models – Regression Methods*, by constraining the values
    in the row (column) vectors from becoming too large and potentially causing overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: We could try several values of k and λ, and measure the mean squared error between
    the observed and predicted matrix (from multiplying the row factors by the column
    factors) to determine the optimal values.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have obtained a good fit, we can use the `predict` and `predictAll`
    methods of the model object to obtain predictions for new users, and the persist
    it on disk using the `save` method.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have examined complex, unstructured data. We cleaned and
    tokenized text and examined several ways of extracting features from documents
    in a way that could be incorporated into predictive models such as n-grams and
    tf-idf scores. We also examined dimensionality reduction techniques, such as the
    HashingVectorizer, matrix decompositions, such as PCA, CUR, NMF, and probabilistic
    models, such as LDA. We also examined image data, including normalization and
    thresholding operations, and how we can use dimensionality reduction techniques
    to find common patterns among images. Finally, we used a matrix factorization
    algorithm to prototype a recommender system in PySpark.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next section, you will also look at image data, but in a different context:
    trying to capture complex features from these data using sophisticated deep learning
    models.'
  prefs: []
  type: TYPE_NORMAL
