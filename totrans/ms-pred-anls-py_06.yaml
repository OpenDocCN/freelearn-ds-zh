- en: Chapter 6. Words and Pixels – Working with Unstructured Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most of the data we have looked at thus far is composed of rows and columns
    with numerical or categorical values. This sort of information fits in both traditional
    spreadsheet software and the interactive Python notebooks used in the previous
    exercises. However, data is increasingly available in both this form, usually
    called structured data, and more complex formats such as images and free text.
    These other data types, also known as unstructured data, are more challenging
    than tabular information to parse and transform into features that can be used
    in machine learning algorithms.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: What makes unstructured data challenging to use? It is challenging largely because
    images and text are extremely high dimensional, consisting of a much larger number
    of columns or features than we have seen previously. For example, this means that
    a document may have thousands of words, or an image thousands of individual pixels.
    Each of these components may individually or in complex combinations comprise
    a feature for our algorithms. However, to use these data types in prediction,
    we need to somehow distill this extremely complex data into common features or
    trends that might be used effectively in a model. This often involves both removing
    noise from these data types and finding simpler representations. At the same time,
    the greater inherent complexity of these data types potentially captures more
    information than available in tabular datasets, or may reveal information that
    is not available in any other source.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will explore unstructured data by:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning raw text through stemming, stop word removal, and other normalizations
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using tokenization and n-grams to find common patterns in textual data
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalizing image data and removing noise
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decomposing images into lower dimensional features through several common matrix
    factorization algorithms
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with textual data
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following example, we will consider the problem of separating text messages
    sent between cell phone users. Some of these messages are spam advertisements,
    and the objective is to separate these from normal communications (Almeida, Tiago
    A., José María G. Hidalgo, and Akebo Yamakami. *Contributions to the study of
    SMS spam filtering: new collection and results.* Proceedings of the 11th ACM symposium
    on Document engineering. ACM, 2011). By looking for patterns of words that are
    typically found in spam advertisements, we could potentially derive a smart filter
    that would automatically remove these messages from a user''s inbox. However,
    while in previous chapters we were concerned with fitting a predictive model for
    this kind of problem, here we will be shifting focus to cleaning up the data,
    removing noise, and extracting features. Once these tasks are done, either simple
    or lower-dimensional features can be input into many of the algorithms we have
    already studied.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning textual data
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let us start by loading and inspecting the data using the following commands.
    Note that we need to supply column names for this data ourselves:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先使用以下命令加载数据并检查它。注意，我们需要自己提供此数据的列名：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This gives the following output:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了以下输出：
- en: '![Cleaning textual data](img/B04881_06_13.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![清洗文本数据](img/B04881_06_13.jpg)'
- en: 'The dataset consists of two columns: the first contains the label (`spam` or
    `ham`) indicating whether the message is an advertisement or a normal message,
    respectively. The second column contains the text of the message. Right at the
    start, we can see a number of problems with using this raw text as input to an
    algorithm to predict the spam/nonspam label:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含两列：第一列包含标签（`spam` 或 `ham`），分别表示消息是否为广告或普通消息。第二列包含消息的文本。一开始，我们可以看到使用这种原始文本作为算法预测垃圾邮件/非垃圾邮件标签输入时存在的一些问题：
- en: The text of each message contains a mixture of upper and lower case letters,
    but this capitalization does not affect the meaning of a word.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每条消息的文本包含大小写字母的混合，但这种大写形式并不影响单词的意义。
- en: Many words (*to*, *he*, *the*, and so on) are common, but tell us relatively
    little about the message.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多单词（如 *to*、*he*、*the* 等）很常见，但关于消息的信息相对较少。
- en: 'Other issues are subtler:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 其他问题更为微妙：
- en: When we compare words such as *larger* and *largest*, the most information about
    the meaning of the words is carried by the root, *large*—differentiating between
    the two forms may actually prevent us from capturing common information about
    the presence of the word *large* in a text, since the count of this stem in the
    message will be divided between the variants. Looking only at individual words
    does not tell us about the context in which they are used. Indeed, it may be more
    informative to consider sets of words.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们比较诸如 *larger* 和 *largest* 这样的单词时，关于单词意义的最多信息是由词根 *large* 承载的——区分这两种形式实际上可能阻止我们捕捉到关于单词
    *large* 在文本中出现的共同信息，因为消息中这个词根的计数将分布在各种变体之间。仅仅查看单个单词并不能告诉我们它们使用的上下文。实际上，考虑单词集合可能更有信息量。
- en: Even for words that do not fall into the common category, such as *and*, *the*,
    and *to*, it is sometimes unclear whether a word is present in a document because
    it is common across all documents or whether it contains special information about
    a particular document. For example, in a set of online movie reviews, words such
    as *character* and *film* will appear frequently, but do not help to distinguish
    one review from another since they are common across all reviews. Because the
    English language has a large vocabulary, the size of the resulting feature set
    could be enormous.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使对于不属于常见类别的单词，如 *and*、*the* 和 *to*，有时也不清楚一个单词是否出现在文档中，因为它在所有文档中都常见，或者它是否包含关于特定文档的特殊信息。例如，在一组在线电影评论中，像
    *character* 和 *film* 这样的单词会频繁出现，但它们并不能帮助区分不同的评论，因为它们在所有评论中都常见。由于英语词汇量很大，结果特征集的大小可能非常大。
- en: 'Let us start by cleaning up the text before delving into the other feature
    issues. We can base by lowercasing each word in the text using the following function:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先从清理文本开始，然后再深入研究其他特征问题。我们可以通过以下函数将文本中的每个单词转换为小写：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We then apply this function to each message using the map function we have
    seen in previous examples:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们然后将此函数应用于每条消息，使用我们在之前的示例中看到的映射函数：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Inspecting the resulting we can verify that all the letters are now indeed
    lowercase:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 检查结果可以验证所有字母现在确实都是小写的：
- en: '![Cleaning textual data](img/B04881_06_14.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![清洗文本数据](img/B04881_06_14.jpg)'
- en: 'Next, we want to remove common words and trim the remaining vocabulary to just
    the stem portion of the word that is most useful for predictive modeling. We do
    this using the **natural language toolkit** (**NLTK**) library (Bird, Steven.
    *NLTK: the natural language toolkit*. Proceedings of the COLING/ACL on Interactive
    presentation sessions. Association for Computational Linguistics, 2006.). The
    list of stop words is part of the dataset associated for download with this library;
    if this is your first time opening NLTK, you can use the `nltk.download()` command
    to open a **graphical user interface** (**GUI**) where you can select the content
    you wish to copy to your local machine using the following commands:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We then define a function to perform stemming:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Finally, we again use a lambda function to perform this operation on each message,
    and visually inspect the results:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![Cleaning textual data](img/B04881_06_15.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
- en: For example, you can see the stem *joke* has been extracted from *joking*, and
    *avail* from *available*.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have performed lower casing and stemming, the messages are in relatively
    cleaned up form, and we can proceed to generate features for predictive modeling
    from this data.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Extracting features from textual data
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In perhaps the simplest possible feature for text data, we use a binary vector
    of *0s* and *1s* to simply record the presence or absence of each word in our
    vocabulary in each message. To do this we can utilize the `CountVectorizer` function
    in the `scikit-learn` library, using the following commands:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'By default, the result is stored as a *sparse vector*, which means that only
    the non-zero elements are held in memory. To calculate the total size of this
    vector we need to transform it back into a *dense* vector (where all elements,
    even 0, are stored in memory):'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: By checking the length of the feature vector created for the first message,
    we can see that it creates a vector of length 7,468 for each message with 1 and
    0 indicating the presence or absence, respectively, of a particular word out of
    all words in this document list.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: 'We can check that this length is in fact the same as the vocabulary (union
    of all unique words in the messages) using the following command to extract the
    `vocabulary_ element` of the vectorizer, which also gives a value of 7,468:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We can see that this increases the size of the resulting feature by about 10-fold
    by again inspecting the length of the first row using:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'However, even after calculating n-grams, we still have not accounted for the
    fact that some words or n-grams might be common across all messages and thus provide
    little information in distinguishing spam from nonspam. To account for this, instead
    of simply recording the presence or absence of a word (or n-gram), we might compare
    the frequency of words within a document to the frequency across all documents.
    This ratio, the **term-frequency-inverse document frequency** (**tf-idf**) is
    calculated in the simplest form as:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '![Extracting features from textual data](img/B04881_06_01.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![从文本数据中提取特征](img/B04881_06_01.jpg)'
- en: Where *ti* is a particular term (word or n-gram), *dj* is a particular document,
    *D* is the number of documents, *Vj* is the set of words in document *j*, and
    *vk* is a particular word in document *j*. The subscripted 1 in this formula is
    known as an **Indicator Function**, which returns `1` if the subscripted condition
    is `true`, and `0` otherwise. In essence, this formula compares the frequency
    (count) of a word within a document to the number of documents that contain this
    word. As the number of documents containing the word decreases, the denominator
    decreases, and thus the overall formula becomes larger from dividing by a value
    much less than `1`. This is balanced by the frequency of the word within a document
    in the numerator. Thus, the `tf-idf` score will more heavily weight words that
    are present at greater frequency within a document compared to those common among
    all documents and thus might be indicative of special features of a particular
    message.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*t*i是一个特定的术语（单词或n-gram），*d*j是一个特定的文档，*D*是文档的数量，*V*j是文档*j*中的单词集合，*v*k是文档*j*中的一个特定单词。这个公式中的下标`1`被称为**指示函数**，如果下标条件为`true`则返回`1`，否则返回`0`。本质上，这个公式比较了一个单词在文档中的频率（计数）与包含这个单词的文档数量。随着包含该单词的文档数量的减少，分母减少，因此整体公式在除以一个远小于`1`的值时变得更大。这通过分子中单词在文档中的频率来平衡。因此，`tf-idf`分数将更重视在文档中频率更高的单词，相对于在所有文档中都常见的单词，这些单词可能表明特定消息的特殊特征。
- en: 'Note that the formula above represents only the simplest version of this expression.
    There are also variants in which we might logarithmically transform the counts
    (to offset the bias from large documents), or scale the numerator by the maximum
    frequency found for any term within a document (again, to offset bias that longer
    documents could have higher term frequencies than shorter documents by virtue
    of simply having more words) (Manning, Christopher D., Prabhakar Raghavan, and
    Hinrich Schütze. *Scoring, term weighting and the vector space model.* Introduction
    to Information Retrieval 100 (2008): 2-4.). We can apply `tf-idf` to the spam
    data using the following commands:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '注意，上面的公式仅代表这个表达式的最简单版本。还有一些变体，我们可能会对计数进行对数变换（以抵消来自大型文档的偏差），或者通过文档中任何术语的最大频率来缩放分子（再次，为了抵消较长的文档可能由于拥有更多单词而具有比短文档更高的术语频率的偏差）（Manning,
    Christopher D., Prabhakar Raghavan, and Hinrich Schütze. *评分、术语加权和向量空间模型.* 信息检索导论
    100 (2008): 2-4）。我们可以使用以下命令对垃圾邮件数据应用`tf-idf`：'
- en: '[PRE10]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We can see the effect of this transformation by taking the maximum value across
    rows using:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用以下方法来观察这种变换的影响：
- en: '[PRE11]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Where the '1' argument to max indicates that the function is applied along rows
    (instead of columns, which would be specified with '0' ). When our features consisted
    only of binary values, the maximum across each rows would be 1, but we can see
    that it is now a float value.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 当`max`函数的`1`参数指示函数是沿着行（而不是列，列可以通过`0`指定）应用时。当我们的特征仅由二进制值组成时，每行的最大值会是`1`，但我们现在可以看到它是一个浮点值。
- en: 'The final text feature we will discuss is concerned with condensing our feature
    set. Simply put, as we consider larger and larger vocabularies, we will encounter
    many words that are so infrequent as to almost never appear. However, from a computational
    standpoint, even a single instance of a word in one document is enough to expand
    the number of columns in our text features for all documents. Given this, instead
    of directly recording whether a word is present, we might think of compressing
    this space requirement so that we use fewer columns to represent the same dataset.
    While in some cases, two words might map to the same column, in practice this
    happens infrequently enough due to the long-tailed distribution of word frequencies
    that it can serve as a handy way to reduce the dimensionality of our text data.
    To perform this mapping, we make use of a hash function that takes as input a
    word and outputs a random number (column location) that is keyed to the value
    of that string. The number of columns we ultimately map to in our transformed
    dataset is controlled by the `n_features` argument to the `HashingVectorizer`,
    which we can apply to our dataset using the following commands:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要讨论的最后一个文本特征是关于压缩我们的特征集。简单来说，当我们考虑越来越大的词汇表时，我们会遇到许多非常罕见的单词，以至于几乎从未出现过。然而，从计算的角度来看，即使一个文档中的一个单词的实例也足以增加我们所有文档文本特征中的列数。鉴于这一点，我们可能认为可以通过压缩空间需求来减少列数，从而用更少的列来表示相同的数据集。虽然在某些情况下，两个单词可能映射到同一列，但由于单词频率的长尾分布，这种情况在实践中很少发生，这可以作为一种方便的方法来降低我们文本数据的维度。为了执行这种映射，我们使用一个哈希函数，该函数将单词作为输入并输出一个随机数（列位置），该随机数与该字符串的值相关联。我们最终映射到转换后的数据集中的列数由`HashingVectorizer`的`n_features`参数控制，我们可以使用以下命令将其应用于我们的数据集：
- en: '[PRE12]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Using dimensionality reduction to simplify datasets
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用降维简化数据集
- en: Even though using the `HashingVectorizer` allows us to reduce the data to a
    set of 1,024 columns from a feature set that was much larger, we are still left
    with many variables in our dataset. Intuition tells us that some of these features,
    either before or after the application of the `HashingVectorizer`, are probably
    correlated. For example, a set of words may co-occur in a document that is spam.
    If we use n-grams and the words are adjacent to one another, we could pick up
    on this feature, but not if the words are simply present in the message but separated
    by other text. The latter might occur, for example, if some common terms are in
    the first sentence of the message, while others are near the end.More broadly,
    given a large set of variables such as we have already seen for textual data,
    we might ask whether we could represent these data using a more compact set of
    features. In other words, is there an underlying pattern to the variation in thousands
    of variables that may be extracted by calculating a much smaller number of features
    representing patterns of correlation between individual variables? In a sense,
    we already saw several examples of this idea in [Chapter 3](ch03.html "Chapter 3. Finding
    Patterns in the Noise – Clustering and Unsupervised Learning"), *Finding Patterns
    in the Noise – Clustering and Unsupervised Learning*, in which we reduced the
    complexity of a dataset by aggregating individual datapoints into clusters. In
    the following examples, we have a similar goal, but rather than aggregating individual
    datapoints, we want to capture groups of correlated variables.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管使用`HashingVectorizer`可以将数据减少到从更大的特征集到1,024列，但我们数据集中仍然存在许多变量。直觉告诉我们，这些特征中的一些，无论是在应用`HashingVectorizer`之前还是之后，可能存在相关性。例如，一组单词可能在一个垃圾邮件文档中同时出现。如果我们使用n-gram并且单词相互相邻，我们可能会注意到这个特征，但如果单词只是出现在消息中但被其他文本分隔，则不会。例如，如果某些常见术语出现在消息的第一句话中，而其他术语则接近结尾。更广泛地说，考虑到我们已看到的大量变量，例如文本数据，我们可能会问是否可以用更紧凑的特征集来表示这些数据。换句话说，是否有潜在的规律可以描述成千上万的变量变化，这些变量可以通过计算代表个体变量之间相关性的更少数量的特征来提取？在某种程度上，我们在[第3章](ch03.html
    "第3章。在噪声中寻找模式 – 聚类和无监督学习")中已经看到了几个这个想法的例子，即通过将单个数据点聚合到聚类中来降低数据集的复杂性。在以下例子中，我们有一个类似的目标，但不是聚合单个数据点，而是想要捕捉相关变量的组。
- en: While we might achieve this goal in part through the variable selection techniques
    such as regularization, which we discussed in the [Chapter 4](ch04.html "Chapter 4. Connecting
    the Dots with Models – Regression Methods"), *Connecting the Dots with Models
    – Regression Methods*, we do not necessarily want to remove variables, but rather
    capture their common patterns of variation.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们可能通过变量选择技术如正则化（我们在[第4章](ch04.html "第4章. 通过模型连接点 – 回归方法")中讨论过）部分实现这一目标，即“通过模型连接点
    – 回归方法”，但我们并不一定想要删除变量，而是要捕捉它们变化的共同模式。
- en: Let us examine some of the common methods of dimensionality reduction and how
    we might choose between them for a given problem.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考察一些常见的降维方法，以及对于给定问题我们如何在这之间进行选择。
- en: Principal component analysis
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主成分分析
- en: 'One of the most commonly used methods of dimensionality reduction is **Principal
    Component Analysis** (**PCA**). Conceptually, PCA computes the axes along which
    the variation in the data is greatest. You may recall that in [Chapter 3](ch03.html
    "Chapter 3. Finding Patterns in the Noise – Clustering and Unsupervised Learning"),
    *Finding Patterns in the Noise – Clustering and Unsupervised Learning*, we calculated
    the eigenvalues of the adjacency matrix of a dataset to perform spectral clustering.
    In PCA, we also want to find the eigenvalue of the dataset, but here, instead
    of any adjacency matrix, we will use the covariance matrix of the data, which
    is the relative variation within and between columns. The covariance for columns
    `xi` and `xj` in the data matrix `X` is given by:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 最常用的降维方法之一是**主成分分析**（**PCA**）。从概念上讲，PCA计算数据变化最大的轴。你可能还记得，在[第3章](ch03.html "第3章.
    在噪声中寻找模式 – 聚类和无监督学习")中，“在噪声中寻找模式 – 聚类和无监督学习”，我们计算了数据集的邻接矩阵的特征值以执行谱聚类。在PCA中，我们同样想要找到数据集的特征值，但在这里，我们不会使用任何邻接矩阵，而是会使用数据的协方差矩阵，它是列之间的相对变化。数据矩阵`X`中列`xi`和`xj`的协方差由以下公式给出：
- en: '![Principal component analysis](img/B04881_06_02.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![主成分分析](img/B04881_06_02.jpg)'
- en: 'This is the average product of the offsets from the mean column values. We
    saw this value before when we computed the correlation coefficient in [Chapter
    3](ch03.html "Chapter 3. Finding Patterns in the Noise – Clustering and Unsupervised
    Learning"), *Finding Patterns in the Noise – Clustering and Unsupervised Learning*,
    as it is the denominator of the Pearson coefficient. Let us use a simple example
    to illustrate how PCA works. We will make a dataset in which the six columns are
    derived from the same underlying normal distribution, one of which is given reversed
    in sign, using the following commands:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这是均值列值偏移的平均乘积。我们在[第3章](ch03.html "第3章. 在噪声中寻找模式 – 聚类和无监督学习")中计算相关系数时见过这个值，作为皮尔逊系数的分母。让我们用一个简单的例子来说明PCA是如何工作的。我们将创建一个数据集，其中六个列是从同一个基本正态分布中得出的，其中一个列的符号被反转，使用以下命令：
- en: '[PRE13]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Note that each of our columns has mean `0` and standard deviation `1`. If this
    were not the case, we could use the scikit-learn utility StandardScaler as we
    discussed in [Chapter 3](ch03.html "Chapter 3. Finding Patterns in the Noise –
    Clustering and Unsupervised Learning"), *Finding Patterns in the Noise – Clustering
    and Unsupervised Learning*, when we normalized data for use in k means clustering.
    We might simply center the variables at `0` and use the resulting covariance matrix
    if we believe that the differences in scale of the variables are important to
    our problem. Otherwise, differences in scale will tend to be reflected by the
    differing variance values within the columns of the data, so our resulting PCA
    will reflect not only correlations within variables but also their differences
    in magnitude. If we do not want to emphasize these differences and are only interested
    in the relative correlation among variables, we can also divide each column of
    the data by its standard deviation to give each column a variance of 1\. We could
    also potentially run PCA not on the covariance matrix, but the Pearson correlation
    matrix between variables, which is already naturally scaled to 0 and a constant
    range of a values (from -1 to 1) (Kromrey, Jeffrey D., and Lynn Foster-Johnson.
    *Mean centering in moderated multiple regression: Much ado about nothing.* Educational
    and Psychological Measurement 58.1 (1998): 42-67.). For now, we can compute the
    covariance matrix of our data with the following command:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '注意，我们每一列的均值都是 `0`，标准差是 `1`。如果不是这样，我们可以使用我们在[第 3 章](ch03.html "第 3 章。在噪声中寻找模式
    – 聚类和无监督学习")中讨论的 scikit-learn 工具 StandardScaler，即 *在噪声中寻找模式 – 聚类和无监督学习*，当我们对数据进行归一化以用于
    k 均值聚类时。如果我们认为变量的尺度差异对我们问题很重要，我们可以简单地将变量中心化到 `0`，并使用得到的协方差矩阵。否则，尺度差异将倾向于通过数据列中的不同方差值来反映，因此我们的结果
    PCA 将不仅反映变量之间的相关性，还会反映它们的大小差异。如果我们不想强调这些差异，并且只对变量之间的相对相关性感兴趣，我们还可以将数据中的每一列除以其标准差，使每一列的方差为
    1。我们还可以潜在地运行 PCA，不是在协方差矩阵上，而是在变量之间的皮尔逊相关矩阵上，该矩阵已经自然地缩放到 0 和一个常数范围（从 -1 到 1）的值（Kromrey,
    Jeffrey D. 和 Lynn Foster-Johnson. *调节多重回归中的均值中心化：无事生非.* 教育与心理测量 58.1 (1998): 42-67）。现在，我们可以使用以下命令计算我们数据的协方差矩阵：'
- en: '[PRE14]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Recalling our discussion of spectral clustering in [Chapter 3](ch03.html "Chapter 3. Finding
    Patterns in the Noise – Clustering and Unsupervised Learning"), *Finding Patterns
    in the Noise – Clustering and Unsupervised Learning*, if we consider the covariance
    matrix as a stretching operation on a vector, then, if we find the vectors that
    lie along these directions of distortion, we have in a sense found the axes that
    define the variation in the data. If we then compare the eigenvalues of these
    vectors, we could determine if one or more of these directions reflect a greater
    proportion of the overall variation of the data. Let us compute the eigenvalues
    and vectors of the covariance matrix using:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 回想我们在[第 3 章](ch03.html "第 3 章。在噪声中寻找模式 – 聚类和无监督学习")中关于谱聚类的讨论，即 *在噪声中寻找模式 – 聚类和无监督学习*，如果我们把协方差矩阵看作是对向量的拉伸操作，那么，如果我们找到沿着这些扭曲方向的向量，我们就在某种程度上找到了定义数据变化的轴。如果我们然后比较这些向量的特征值，我们可以确定这些方向中是否有一个或多个反映了数据整体变化的一个更大比例。让我们使用以下命令计算协方差矩阵的特征值和向量：
- en: '[PRE15]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This gives the following eigenvalue variable as:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了以下特征值变量：
- en: '[PRE16]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'You can see that most of the eigenvalues are effectively zero, except the second.
    This reflects the fact that the data we constructed, despite having six columns,
    is effectively derived from only one dataset (a normal distribution). Another
    important property of these eigenvectors is that they are orthogonal, which means
    that they are at right angles to each other in n-dimensional space: if we were
    to take a dot product between them, it would be 0, and they thus represent independent
    vectors that, when linearly combined, can be used to represent the dataset.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，除了第二个之外，大多数特征值实际上都是零。这反映了我们构建的数据，尽管有六个列，实际上只来源于一个数据集（一个正态分布）。这些特征向量的另一个重要特性是它们是正交的，这意味着在
    n 维空间中它们彼此垂直：如果我们把它们之间的点积取出来，它将是 0，因此它们代表独立的向量，当它们线性组合时，可以用来表示数据集。
- en: 'If we were to multiply the data by the eigenvector corresponding to this second
    eigenvalue, we would project the data from a six-dimensional to a one-dimensional
    space:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将数据乘以与第二个特征值对应的特征向量，我们将把数据从六维空间投影到一维空间：
- en: '[PRE17]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Note that we needed to transpose the data to have the 100 rows and 6 columns,
    as we initially constructed it as a list of 6 columns, which NumPy interprets
    as instead having 6 rows and 100 columns. The resulting histogram is as shown
    in the following:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们需要转置数据以获得100行和6列，因为我们最初将其构建为一个包含6列的列表，而NumPy将其解释为有6行和100列。结果直方图如下所示：
- en: '![Principal component analysis](img/B04881_06_17.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![主成分分析](img/B04881_06_17.jpg)'
- en: 'In other words, by projecting the data onto the axis of greatest variance,
    we have recovered that fact that this six-column data was actually generated from
    a single distribution. Now if we instead use the PCA command, we get a similar
    result:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，通过将数据投影到最大方差轴上，我们恢复了这样一个事实，即这六个列数据实际上是从一个单一分布生成的。现在，如果我们使用PCA命令，我们会得到类似的结果：
- en: '[PRE18]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'When we extract the `explained_variance_ratio_`, the algorithm has effectively
    taken the preceding eigenvalues, ordered them by magnitude, and divided by the
    largest one, giving:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们提取`explained_variance_ratio_`时，算法实际上已经取了前面的特征值，按大小排序，然后除以最大的一个，得到：
- en: '[PRE19]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'If we were to plot these as a barplot, a visualization known as a `scree plot`
    could help us determine how many underlying components are represented in our
    data:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将这些数据绘制成条形图，一种称为“特征值分布图”的可视化可以帮助我们确定数据中代表了多少个潜在成分：
- en: '[PRE20]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This generates the following plot:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这生成了以下图表：
- en: '![Principal component analysis](img/B04881_06_18.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![主成分分析](img/B04881_06_18.jpg)'
- en: 'Evidently, only the first component carries any variance, represented by the
    height of the bar, with all other components being near 0 and so not appearing
    in the plot. This sort of visual analysis is comparable to how we looked for an
    elbow in the inertia function for k-means in [Chapter 3](ch03.html "Chapter 3. Finding
    Patterns in the Noise – Clustering and Unsupervised Learning"), *Finding Patterns
    in the Noise – Clustering and Unsupervised Learning*, as a function of k to determine
    how many clusters were present in the data.We can also extract the data projected
    onto the first principal components and see a similar plot as shown previously
    when we projected the data onto an eigenvector of the covariance matrix:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，只有第一个成分携带任何方差，由条形的高度表示，其他所有成分都接近0，因此在图表中不显示。这种视觉分析方法类似于我们在[第3章](ch03.html
    "第3章. 在噪声中寻找模式 – 聚类和无监督学习")中寻找k-means的惯性函数中的肘部，*在噪声中寻找模式 – 聚类和无监督学习*，作为k的函数来确定数据中存在多少个簇。我们还可以提取投影到第一个主成分上的数据，并看到与之前将数据投影到协方差矩阵的特征向量上时相似的图表：
- en: '[PRE21]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![Principal component analysis](img/B04881_06_38.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![主成分分析](img/B04881_06_38.jpg)'
- en: 'Why are they not exactly identical? While conceptually PCA computes the eigenvalues
    of the covariance matrix, in practice most packages do not actually implement
    the calculation we illustrated previously for purposes of numerical efficiency.
    Instead, they employ a matrix operation known as the **Singular Value Decomposition**
    (**SVD**), which seeks to represent a covariance matrix of *X* as a set of lower
    dimensional row and column matrices:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么它们不完全相同？虽然从概念上讲，PCA计算协方差矩阵的特征值，但在实践中，大多数软件包为了数值效率并没有实际实现我们之前展示的计算。相反，它们采用一种称为**奇异值分解**（**SVD**）的矩阵运算，试图将*X*的协方差矩阵表示为一系列低维行和列矩阵：
- en: '![Principal component analysis](img/B04881_06_03.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![主成分分析](img/B04881_06_03.jpg)'
- en: 'Where if *X* is an *n* by *m*, W may be *n* by *k*, where *k << m*. Here, σ
    represents a matrix with 0 everywhere but the diagonal, which contains non-zero
    entries. Thus, the covariance matrix is represented as the product of two smaller
    matrices and a scaling factor given by the diagonal elements in σ. Instead of
    calculating all eigenvectors of the covariance matrix, as we did previously, we
    can ask only for the k columns or WT we think are likely to be significant judged
    by the sort of scree plot analysis we demonstrated above. However, when we project
    the data onto the principal components we obtain through this method, the calculation
    of the SVD can potentially give different signs to the projection of the data
    on the principal components, even if the relative magnitude and signs of these
    components remains the same. Thus, when we look at the scores assigned to a given
    row of data after projecting it onto the first k principal components, we should
    analyze them relative to other values in the dataset, just as when we examined
    the coordinates produced by Multidimensional Scaling in [Chapter 3](ch03.html
    "Chapter 3. Finding Patterns in the Noise – Clustering and Unsupervised Learning"),
    *Finding Patterns in the Noise – Clustering and Unsupervised Learning*. Details
    of the SVD calculation used by the default scikit-learn implementation of PCA
    are given in (Tipping, Michael E., and Christopher M. Bishop. *Probabilistic principal
    component analysis*. Journal of the Royal Statistical Society: Series B (Statistical
    Methodology) 61.3 (1999): 611-622.).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have examined conceptually what PCA calculates, let us see if it
    can help us reduce the dimensionality of our text dataset. Let us run PCA on the
    n-gram feature set from above, asking for 100 components. Note that because the
    original dataset is a sparse matrix and PCA requires a dense matrix as an input,
    we need to convert it using `toarray()`. Also, to retain the right dimensionality
    for use with the PCA fit function, we need to transpose the result:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'If we make a scree plot of total variance explained by the first `10` principal
    components of this dataset, we see that we will probably require a relatively
    large number of variables to capture the variation in our data since the upward
    trend in variance explained is relatively smooth:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![Principal component analysis](img/B04881_06_19.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
- en: 'We could also visualize this by looking at the cumulative variance explained
    using *k* components using the following curve:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![Principal component analysis](img/B04881_06_21.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
- en: 'A word on normalization: in practice, for document data, we might not want
    to scale the data by subtracting the mean and dividing by the variance as the
    data is mostly binary. Instead, we would just apply the SVD to a binary matrix
    or perhaps the tF-idf scores we computed previously, an approach also known as
    **Latent Semantic Indexing** (**LSI**) (Berry, Michael W., Susan T. Dumais, and
    Gavin W. O''Brien. *Using linear algebra for intelligent information retrieval*.
    SIAM review 37.4 (1995): 573-595; Laham, T. K. L. D., and Peter Foltz. *Learning
    human-like knowledge by singular value decomposition: A progress report*. Advances
    in Neural Information Processing Systems 10: Proceedings of the 1997 Conference.
    Vol. 10\. MIT Press, 1998.). CUR decomposition and nonnegative matrix factorization'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: 'What drawbacks might there be to using PCA to reduce the dimensionality of
    a dataset? For one, the components (covariance matrix eigenvectors) generated
    by PCA are still essentially mathematical entities: the patterns in variables
    represented by these axes might not actually correspond to any element of the
    data, but rather a linear combination of them. This representation is not always
    easily interpretable, and can particularly difficult when trying to convey the
    results of such analyses to domain experts to generate subject-matter specific
    insights. Second, the fact that PCA produces negative values in its eigenvectors,
    even for positive-only data such as text (where a term cannot be negatively present
    in a document, just 0, 1, a count, or a frequency), is due to the fact that the
    data is linearly combined using these factors. In other words, positive and negative
    values may be summed together when we project the data onto its components through
    matrix multiplication, yielding an overall positive value for the projection.
    Again, it may be preferable to have factors that give some insight into the structure
    of the data itself, for example, by giving a factor that consists of binary indicators
    for a group of words that tend to co-occur in a particular group of documents.
    These goals are addressed by two other matrix factorization techniques: CUR Decomposition
    and Non-negative Matrix Factorization.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: 'Like the SVD used in PCA, CUR attempts to represent a matrix of data X as a
    product of lower dimensional matrices. Here, instead of eigenvectors, the CUR
    decomposition attempts to find the set of columns and rows of the matrix that
    best represent the dataset as:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '![Principal component analysis](img/B04881_06_04.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
- en: 'Where *C* is a matrix of *c* columns of the original dataset, *R* is a set
    of *r* rows from the original dataset, and *U* is a matrix of scaling factors.
    The *c* columns and *r* rows used in this reconstruction are sampled from the
    columns and rows of the original matrix, with probability proportional to the
    `leverage score`, given by:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '![Principal component analysis](img/B04881_06_05.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
- en: 'Where *lvj* is the statistical leverage for column (row) *j*, *k* is the number
    of components in the SVD of *X*, and *vj* are the *jth* elements of these *k*
    component vectors. Thus, columns (rows) are sampled with high probability if they
    contribute significantly to the overall norm of the matrix''s singular values,
    meaning they are also have a major influence on the reconstruction error from
    the SVD (for example, how well the SVD approximates the original matrix) (Chatterjee,
    Samprit, and Ali S. Hadi. Sensitivity analysis in linear regression. Vol. 327\.
    John Wiley & Sons, 2009; Bodor, András, et al. *rCUR: an R package for CUR matrix
    decomposition*. BMC bioinformatics 13.1 (2012): 1).'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: While this decomposition is not expected to approximate the original dataset
    with the same accuracy as the SVD approach used in PCA, the resulting factors
    may be easier to interpret since they are actual elements of the original dataset.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Please note that while we use SVD to determine sampling probabilities for the
    columns and rows, the final factorization of CUR does not.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many algorithms for generating a CUR decomposition (Mahoney, Michael
    W., and Petros Drineas. *CUR matrix decompositions for improved data analysis.
    Proceedings of the National Academy of Sciences 106.3 (2009): 697-702\. Boutsidis,
    Christos, and David P. Woodruff. Optimal cur matrix decompositions*. Proceedings
    of the 46th Annual ACM Symposium on Theory of Computing. ACM, 2014). CUR decomposition
    is implemented in the `pymf` library, and we can call it using the following commands:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The `crank` and `rrank` parameters indicate how many rows and columns, respectively,
    should be chosen from the original matrix in the process of performing the decomposition.
    We can then examine which columns (words from the vocabulary) were chosen in this
    reconstruction using the following commands to print these significant words whose
    indices are contained in the cur object''s .`_cid` (column index) element. First
    we need to collect a list of all words in the vocabulary of our spam dataset:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Since the `vocabulary_` variable returned by the `CountVectorizer` is a dictionary
    giving the positions of terms in the array to which they are mapped, we need to
    construct our array by placing the word at the position given by this dictionary.
    Now we can print the corresponding words using:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Like CUR, nonnegative matrix factorization attempts to find a set of positive
    components that represents the structure of a dataset (Lee, Daniel D., and H.
    Sebastian Seung. *Learning the parts of objects by non-negative matrix factorization.*
    Nature 401.6755 (1999): 788-791; Lee, Daniel D., and H. Sebastian Seung. *Algorithms
    for non-negative matrix factorization.* Advances in neural information processing
    systems. 2001.; P. Paatero, U. Tapper (1994). Paatero, Pentti, and Unto Tapper.
    *Positive matrix factorization: A non-negative factor model with optimal utilization
    of error estimates of data values*. Environmetrics 5.2 (1994): 111-126\. Anttila,
    Pia, et al. *Source identification of bulk wet deposition in Finland by positive
    matrix factorization*. Atmospheric Environment 29.14 (1995): 1705-1718.). Similarly,
    it tries to reconstruct the data using:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '![Principal component analysis](img/B04881_06_06.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
- en: Where *W* and *H* are lower dimensional matrices that when multiplied, reconstruct
    *X*; all three of *W*, *H*, and *X* are constrained to have no negative values.
    Thus, the columns of *X* are linear combinations of *W*, using *H* as the coefficients.
    For example, if the rows of *X* are words and the columns are documents, then
    each document in *X* is represented as a linear combination of underlying document
    types in *W* with weighted given by *H*. Like the elements returned by CUR decomposition,
    the components *W* from nonnegative matrix factorization are potentially more
    interpretable than the eigenvectors we get from PCA.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several algorithms to compute *W* and *H*, with one of the simplest
    being through multiplicative updates (Lee, Daniel D., and H. Sebastian Seung.
    *Algorithms for non-negative matrix factorization*. Advances in neural information
    processing systems. 2001). For example, if we want to minimize the Euclidean distance
    between *X* and *WH*:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '![Principal component analysis](img/B04881_06_07.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
- en: 'We can calculate the derivative of this value with respective to *W*:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '![Principal component analysis](img/B04881_06_08.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
- en: 'Then to update *W* we multiply at each step by this gradient:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '![Principal component analysis](img/B04881_06_09.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
- en: 'And the same for *H*:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '![Principal component analysis](img/B04881_06_10.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
- en: 'These steps are repeated until the values of *W* and *H* converge. Let us examine
    what components we retrieve from our text data when we use NMF to extract components:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We can then look at the words represented by the components in NMF, where the
    words have a large value in the components matrix resulting from the decomposition.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that they appear to capture distinct groups of words, but are any
    correlated with distinguishing spam versus nonspam? We can transform our original
    data using the NMF decomposition, which will give the weights for linearly combining
    these features (for example, the weights to linearly combine the 10 basis documents
    we get from the decomposition to reconstruct the message) using the command:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now let us plot the average weight assigned to each of these `nmf` factors
    for the normal and spam messages. We can do this by plotting a bar chart where
    the *x* axis are the 10 `nmf` factors, and the *y* axis are the average weight
    assigned to this factor for a subset of documents:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们绘制每个这些`nmf`因素分配给正常和垃圾邮件的平均权重。我们可以通过绘制一个条形图来实现，其中*x*轴是10个`nmf`因素，而*y*轴是分配给这个因素的平均权重，对于文档的一个子集：
- en: '[PRE30]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '![Principal component analysis](img/B04881_06_22.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![主成分分析](img/B04881_06_22.jpg)'
- en: '[PRE31]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '![Principal component analysis](img/B04881_06_23.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![主成分分析](img/B04881_06_23.jpg)'
- en: Promisingly, the factors 8 and 9 seem to have very different average weights
    between these two classes of messages. In fact, we may need fewer than 10 factors
    to represent the data, since these two classes may well correspond to the underlying
    spam versus nonspam messages.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 令人鼓舞的是，因素8和9在这两类消息之间似乎具有非常不同的平均权重。实际上，我们可能需要少于10个因素来表示数据，因为这两类可能很好地对应于潜在的垃圾邮件与非垃圾邮件消息。
- en: Latent Dirichlet Allocation
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 潜在狄利克雷分配
- en: 'A related method of decomposing data into an interpretable set of features
    is **Latent Dirichlet Allocation** (**LDA**), a method initially developed for
    textual and genetics data that has since been extended to other areas (Blei, David
    M., Andrew Y. Ng, and Michael I. Jordan. *Latent dirichlet allocation*. the Journal
    of machine Learning research 3 (2003): 993-1022\. Pritchard, Jonathan K., Matthew
    Stephens, and Peter Donnelly. *Inference of population structure using multilocus
    genotype data*. Genetics 155.2 (2000): 945-959.). Unlike the methods we looked
    at previously, where the data is represented as a set of lower dimensional matrices
    that, when multiplied, approximate the original data, LDA uses a probability model.
    This model is often explained using a plate diagram that illustrates the dependencies
    among the variables, as shown in the following diagram:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '一种将数据分解为可解释特征集的相关方法是**潜在狄利克雷分配**（**LDA**），这是一种最初为文本和基因数据开发的方法，后来已扩展到其他领域（Blei,
    David M., Andrew Y. Ng, 和 Michael I. Jordan. *潜在狄利克雷分配*. 机器学习研究杂志 3 (2003): 993-1022。Pritchard,
    Jonathan K., Matthew Stephens, 和 Peter Donnelly. *使用多座位点基因型数据推断种群结构*. 遗传学 155.2
    (2000): 945-959）。与之前我们探讨的方法不同，那些方法将数据表示为一系列低维矩阵的集合，这些矩阵相乘可以近似原始数据，LDA使用一个概率模型。这个模型通常使用一个板图来解释，该图说明了变量之间的依赖关系，如下面的图所示：'
- en: '![Latent Dirichlet Allocation](img/B04881_06_24.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![潜在狄利克雷分配](img/B04881_06_24.jpg)'
- en: 'What exactly does this diagram describe? It is what is known as a generative
    model: a set of instructions by which to generate a probability distribution over
    documents. The idea is comparable to a distribution such as the Gaussian ''bell-curve''
    you are probably familiar with, except here instead of drawing real numbers from
    the distribution we sample documents. Generative models may be contrasted with
    the predictive methods which we have seen in previous chapters that attempts to
    fit the data to a response (as in the regression or classification models we have
    studied in [Chapters 4](ch04.html "Chapter 4. Connecting the Dots with Models
    – Regression Methods"), *Connecting the Dots with Models – Regression Methods*,
    and [Chapter 5](ch05.html "Chapter 5. Putting Data in its Place – Classification
    Methods and Analysis"), *Putting Data in its Place – Classification Methods and
    Analysis*), instead of simply generate samples of the data according to a distribution.
    The plate diagram represents the components of this generative model, and we can
    think of this model as the following series of steps to generate a document:Initialize
    a Dirichlet distribution to choose from a set of topics. These topics are analogous
    to the components we found in NMF, and can be thought of as *basis documents*
    representing groups of commonly co-occurring words. The Dirichlet distribution
    is given by the following formula:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '![Latent Dirichlet Allocation](img/B04881_06_11.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
- en: The preceding formula gives the probability of observing a given distribution
    of items (here topics) among `K` classes and can be used to sample a vector of
    `K` class memberships (for example, sample a random vector giving what fraction
    of documents in the collection belong to a given topic). The alpha parameter in
    the Dirichlet distribution is used as an exponent of the K category probabilities
    and increases the significance ascribed to a particular component (for example,
    a more frequent topic). The term `B` is the beta function, which is simply a normalization
    term. We use the Dirichlet distribution in step 1 to generate a per-topic probability
    distribution for a document *i*. This distribution would be, for example, a series
    of weights that sum to 1 giving the relative probability that a document belongs
    to a given topic. This is the parameter θ in the plate diagram. M represents the
    number of documents in our dataset.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: For each of the *N* word positions in the document, choose a topic Z from the
    distribution θ. Each of the M topics has a Dirichlet distribution with parameter
    β instead of giving per word probabilities, given by ϕ. Use this distribution
    to choose word in each N position in a document.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 2–4 for each word position for each document in a dataset to generate
    a group of documents.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the previous diagram, the numbers (**M**, **N**, **K**) inside the rectangles
    indicate the number of time that the variables represented by circles are generated
    in the generative model. Thus, the words w, being innermost, are generated *N
    × M* times. You can also notice that the rectangles enclose variables that are
    generated the same number of times, while arrows indicate dependence among variables
    during this data generation process. You can also now appreciate where the name
    of this model comes from, as a document is latently allocated among many topics,
    just as we used the factors in NMF to find linear combinations of 'basis documents'
    that could reconstruct our observed data.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的图中，矩形内的数字（**M**、**N**、**K**）表示圆圈所代表的变量在生成模型中生成的次数。因此，作为最内层的单词w被生成*N × M*次。你还可以注意到，矩形包围了生成相同次数的变量，而箭头则表示在此数据生成过程中变量之间的依赖关系。你现在也可以理解这个模型名称的由来，因为文档在许多主题中潜在分配，正如我们在NMF中使用因子来找到可以重建我们观察到的数据的'基础文档'的线性组合。
- en: 'This recipe can also be used to find a set of topics (for example word probability
    distributions) that fit a dataset, assuming the model described previously was
    used to generate the documents. Without going into the full details of the derivation,
    we randomly initialize a fixed number of *K* topics and run the model, as described
    previously, by always sampling a document''s topic, given all other documents,
    and a word, given the probability of all other words in the document. We then
    update the parameters of the model based on the observed data and use the updated
    probabilities to generate the data again. Over many iterations, this process,
    known as Gibbs sampling, will converge from randomly initialized values to a set
    of model parameters that best fit the observed document data. Let us now fit an
    LDA model to the spam dataset using the following commands:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 此配方也可以用来找到一组主题（例如单词概率分布），这些主题适合数据集，假设之前描述的模型用于生成文档。不深入推导的细节，我们随机初始化一个固定的*K*个主题数量，并按照之前描述的方式运行模型，即始终采样一个文档的主题，给定所有其他文档，以及一个单词，给定文档中所有其他单词的概率。然后我们根据观察到的数据更新模型的参数，并使用更新的概率再次生成数据。经过多次迭代，这个被称为Gibbs抽样的过程将从随机初始化的值收敛到一组最佳拟合观察到的文档数据的模型参数。现在让我们使用以下命令将LDA模型拟合到垃圾邮件数据集：
- en: '[PRE32]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'As with NMF, we can examine the highest probability words for each topic using:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 与NMF一样，我们可以使用以下方法检查每个主题的最高概率单词：
- en: '[PRE33]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Likewise, we can see if these topics represent a meaningful separation between
    the spam and nonspam messages. First we find the topic distribution among the
    10 latent topics for each document using the following `transform` command:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以看到这些主题是否代表了垃圾邮件和非垃圾邮件之间的有意义分离。首先，我们使用以下`transform`命令找到每个文档在10个潜在主题中的主题分布：
- en: '[PRE34]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'This is analogous to the weights we calculated in NMF. We can now plot the
    average topic weight for each message class as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们在NMF中计算的权重类似。现在我们可以按如下方式绘制每个消息类的平均主题权重：
- en: '[PRE35]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '![Latent Dirichlet Allocation](img/B04881_06_25.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![潜在狄利克雷分配](img/B04881_06_25.jpg)'
- en: '[PRE36]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '![Latent Dirichlet Allocation](img/B04881_06_26.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![潜在狄利克雷分配](img/B04881_06_26.jpg)'
- en: Again, promisingly, we find a different average weight for topic 5 for spam
    than nonspam, indicating that the LDA model has successfully separated out the
    axis of variation we are interested in for classification purposes.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，令人鼓舞的是，我们发现对于垃圾邮件和非垃圾邮件，主题5的平均权重不同，这表明LDA模型已成功分离出我们用于分类目的感兴趣的变化轴。
- en: Using dimensionality reduction in predictive modeling
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在预测建模中使用降维
- en: The analysis we have outlined previously has been largely devoted to trying
    to extract a lower-dimensional representation of a text collection by finding
    a smaller set of components that capture the variation among individual documents.
    In some cases, this sort of analysis can be useful as an exploratory data analysis
    tool, which, like the clustering techniques we described in [Chapter 3](ch03.html
    "Chapter 3. Finding Patterns in the Noise – Clustering and Unsupervised Learning"),
    *Finding Patterns in the Noise – Clustering and Unsupervised Learning*, allows
    us to understand the structure in a dataset. We might even combine clustering
    and dimensionality reduction, which is in essence the idea of spectral clustering
    as we examined in [Chapter 3](ch03.html "Chapter 3. Finding Patterns in the Noise
    – Clustering and Unsupervised Learning"), *Finding Patterns in the Noise – Clustering
    and Unsupervised Learning* using SVD to reduce the adjacency matrix to a more
    compact representation and then clustering this reduced space to yield a cleaner
    separation between datapoints.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Like the groups assigned through clustering, we can also potentially use the
    components derived from these dimensionality reduction methods as features in
    a predictive model. For example, the NMF components we extracted previously could
    be used as inputs to a classification model to separate spam from nonspam messages.
    We have even seen this use earlier, as the online news popularity dataset we used
    in [Chapter 4](ch04.html "Chapter 4. Connecting the Dots with Models – Regression
    Methods"), *Connecting the Dots with Models – Regression Methods*, had columns
    derived from LDA topics. Like the regularization methods we saw in [Chapter 4](ch04.html
    "Chapter 4. Connecting the Dots with Models – Regression Methods"), *Connecting
    the Dots with Models – Regression Methods*, dimensionality reduction can help
    reduce overfitting by extracting the underlying correlations among variables since
    these lower-dimensional variables are often less noisy than using the whole feature
    space. Now that we have seen how dimensionality reduction could help us find structure
    in textual data, let us examine another class of potentially high-dimensional
    data found in images.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Images
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like textual data, images are potentially noisy and complex. Furthermore, unlike
    language, which has a structure of words, paragraphs, and sentences, images have
    no predefined rules that we might use to simplify raw data. Thus, much of image
    analysis will involve extracting patterns from the input's features, which are
    ideally interpretable to a human analyst based only on the input pixels.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning image data
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the common operations we will perform on images is to enhance contrast
    or change their color scale. For example, let us start with an example image of
    a coffee cup from the `skimage` package, which you can import and visualize using
    the following commands:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'This produces the following image:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '![Cleaning image data](img/B04881_06_27.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
- en: 'In Python, this image is represented as a three-dimensional matrix with the
    dimensions corresponding to height, width, and color channels. In many applications,
    the color is not of interest, and instead we are trying to determine common shapes
    or features in a set of images that may be differentiated based on grey scale
    alone. We can easily convert this image into a grey scale version using the commands:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '![Cleaning image data](img/B04881_06_28.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
- en: 'A frequent task in image analysis is to identify different regions or objects
    within an image. This can be made more difficult if the pixels are clumped into
    one region (for example, if there is very strong shadow or a strong light in the
    image), rather than evenly distributed along the intensity spectrum. To identify
    different objects, it is often desirable to have these intensities evenly distributed,
    which we can do by performing histogram equalization using the following commands:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '![Cleaning image data](img/B04881_06_29.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
- en: 'To see the effect of this normalization, we can plot the histogram of pixels
    by intensity before and after the transformation with the command:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'This gives the following pixel distribution for the uncorrected image:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '![Cleaning image data](img/B04881_06_30.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
- en: 'The `ravel()` command used here is used to flatten the 2-d array we started
    with into a single vector that may be input to the histogram function. Similarly,
    we can plot the distribution of pixel intensities following normalization using:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '![Cleaning image data](img/B04881_06_31.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
- en: Thresholding images to highlight objects
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another common task for image analysis is to identify individual objects within
    a single image. To do so, we need to choose a threshold to binarize an image into
    white and black regions and separate overlapping objects. For the former, we can
    use thresholding algorithms such as Otsu thresholding (Otsu, Nobuyuki. *A threshold
    selection method from gray-level histograms.* Automatica 11.285-296 (1975): 23-27),
    which uses a *structuring element* (such as disk with n pixels) and attempts to
    find a pixel intensity, which will best separate pixels inside that structuring
    element into two classes (for example, black and white). We can imagine rolling
    a disk over an entire image and doing this calculation, resulting in either a
    local value within the disk or a global value that separates the image into foreground
    and background. We can then turn the image into a binary mask by thresholding
    pixels above or below this value.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate, let us consider a picture of coins, where we want to separate
    the coins from their background. We can visualize the histogram-equalized coin
    image using the following commands:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '![Thresholding images to highlight objects](img/B04881_06_32.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
- en: 'One problem we can see is that the background has a gradient of illumination
    increasing toward the upper left corner of the image. This difference doesn''t
    change the distinction between background and objects (coins), but because part
    of the background is in the same intensity range as the coins, it will make it
    difficult to separate out the coins themselves. To subtract the background, we
    can use the closing function, which sequentially erodes (removes white regions
    with size less than the structuring element) and then dilates (if there is a white
    pixel within the structuring element, all elements within the structuring element
    are flipped to white). In practice, this means we remove small white specks and
    enhance regions of remaining light color. If we then subtract this from the image,
    we subtract the background, as illustrated here:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '![Thresholding images to highlight objects](img/B04881_06_33.jpg)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
- en: 'Now that we have removes the background, we can apply the Otsu thresholding
    algorithm mentioned previously to find the ideal pixel to separate the image into
    background and object using the following commands:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '![Thresholding images to highlight objects](img/B04881_06_34.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
- en: The image has now been segmented into coins and non-coin regions. We could use
    this segmented image to count the number coins, to highlight the coins in the
    original image using the regions obtained above as a *mask*, for example if we
    want to record pixel data only from the coin regions as part of a predictive modeling
    feature using image data.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction for image analysis
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once we have our images appropriately cleaned, how can we turn them into more
    general features for modeling? One approach is to try to capture common patterns
    of variation between a group of images using the same dimensionality reduction
    techniques as we used previously for document data. Instead of words in documents,
    we have patterns of pixels within an image, but otherwise the same algorithms
    and analysis largely apply. As an example, let us consider a set of images of
    faces ([http://www.geocities.ws/senthilirtt/Senthil%20Face%20Database%20Version1](http://www.geocities.ws/senthilirtt/Senthil%20Face%20Database%20Version1))
    which we can load and examine using the following commands:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '![Dimensionality reduction for image analysis](img/B04881_06_35.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
- en: 'For each of these two-dimenional images, we want to convert it into a vector
    just as we did when we plotted the pixel frequency histograms during our discussion
    of normalization. We will also construct a set where the average pixel intensity
    across faces has been subtracted from each pixel, yielding each face as an offset
    from the *average face* in the data through the following commands:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We consider two possible ways to factor faces into a more general features.
    The first is to use PCA to extract the major vectors of variation in this data—these
    vectors happen to also look like faces. Since they are formed from the eigenvalues
    of the covariance matrix, these sorts of features are sometimes known as eigenfaces.
    The following commands illustrate the result of performing PCA on the face dataset:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '![Dimensionality reduction for image analysis](img/B04881_06_36.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
- en: 'How much variation in the face data is captured by the principal components?
    In contrast to the document data, we can see that using PCA even with only three
    components allows to explain around two-thirds of the variation in the dataset:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '![Dimensionality reduction for image analysis](img/B04881_06_37.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
- en: 'We could also apply NMF, as we described previously, to find a set of basis
    faces. You can notice from the preceding heatmap that the eigenfaces we extracted
    can have negative values, which highlights one of the interpretational difficulties
    we mentioned previously: we cannot really have negative pixels (since , so a latent
    feature with negative elements is hard to interpret. In contrast, the components
    we extract using NMF will look much more like elements of the original dataset,
    as shown below using the commands:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '![Dimensionality reduction for image analysis](img/B04881_06_39.jpg)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
- en: 'Unlike the eigenfaces, which resemble averaged versions of many images, the
    NMF components extracted from this data look like individual faces. While we will
    not go through the exercise here, we could even apply LDA to image data to find
    topics represented by distributions of pixels and indeed it has been used for
    this purpose (Yu, Hua, and Jie Yang. *A direct LDA algorithm for high-dimensional
    data—with application to face recognition*. Pattern recognition 34.10 (2001):
    2067-2070; Thomaz, Carlos E., et al. *Using a maximum uncertainty LDA-based approach
    to classify and analyse MR brain images*. Medical Image Computing and Computer-Assisted
    Intervention–MICCAI 2004\. Springer Berlin Heidelberg, 2004\. 291-300.).'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: While the dimensionality reduction techniques we have discussed previously are
    useful in the context of understanding datasets, clustering, or modeling, they
    are also potentially useful in storing compressed versions of data. Particularly
    in model services such as the one we will develop in [Chapter 8](ch08.html "Chapter 8. Sharing
    Models with Prediction Services"), *Sharing Models with Prediction Services*,
    being able to store a smaller version of the data can reduce system load and provide
    an easier way to process incoming data into a form that can be understood by a
    predictive model. We can quickly extract the few components we need, for example,
    from a new piece of text data, without having to persist the entire record.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: 'Case Study: Training a Recommender System in PySpark'
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To close this chapter, let us look at an example of how we might generate a
    large-scale recommendation system using dimensionality reduction. The dataset
    we will work with comes from a set of user transactions from an online store (Chen,
    Daqing, Sai Laing Sain, and Kun Guo. *Data mining for the online retail industry:
    A case study of RFM model-based customer segmentation using data mining*. Journal
    of Database Marketing & Customer Strategy Management 19.3 (2012): 197-208). In
    this model, we will input a matrix in which the rows are users and the columns
    represent items in the catalog of an e-commerce site. Items purchased by a user
    are indicated by a 1\. Our goal is to factorize this matrix into 1 x k *user factors*
    (row components) and k x 1 *item factors* (column components) using k components.
    Then, presented with a new user and their purchase history, we can predict what
    items they are like to buy in the future, and thus what we might recommend to
    them on a homepage. The steps to do so are as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a user''s prior purchase history as *a* vector *p*. We imagine this
    vector is the product of an unknown *user factor* component *u* with the item
    factors *i* we obtained through matrix factorization: each element of the vector
    *p* is then the dot product of this unknown user factor with the item factor for
    a given item. Solve for the unknown user factor *u* in the equation:![Case Study:
    Training a Recommender System in PySpark](img/B04881_06_12.jpg)'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Given the item factors *i* and the purchase history *p*, using matrix.Use the
    resulting user factor *u*, take the dot product with each item factor to obtain
    and sort by the result to determine a list of the top ranked items.
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that we have described what is happening *under the hood* in this example,
    we can begin to parse this data using the following commands. First, we create
    a parsing function to read the 2nd and 7th columns of the data containing the
    item ID and user ID, respectively:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Next, we read in the file and convert the user and item IDs, which are both
    string, into a numerical index by incrementing a counter as we add unique items
    to a dictionary:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Next, we convert the resulting array of purchase into an `rdd` and convert the
    resulting entries into Rating objects -- a (user, item, rating) tuple. Here, we
    will just indicate that the purchase occurred by giving a rating of 1.0 to all
    observed purchases, but we could just as well have a system where the ratings
    indicate user preference (such as movie ratings) and follow a numerical scale.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Now we can fit the matrix factorization model using the following commands:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: The algorithm for matrix factorization used in PySpark is **Alternating Least
    Squares** (**ALS**), which has parameters for the number of row (column) components
    chosen (`k`) and a regularization parameter λ which we did not specify here, but
    functions similarly to its role in the regression algorithms we studied in [Chapter
    4](ch04.html "Chapter 4. Connecting the Dots with Models – Regression Methods"),
    *Connecting the Dots with Models – Regression Methods*, by constraining the values
    in the row (column) vectors from becoming too large and potentially causing overfitting.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark 中使用的矩阵分解算法是**交替最小二乘法**（**ALS**），它具有选择行（列）组件数量（`k`）的参数和一个正则化参数 λ，我们在这里没有指定，但它与我们在第
    4 章[“通过模型连接点 – 回归方法”](ch04.html "Chapter 4. Connecting the Dots with Models –
    Regression Methods")中研究的回归算法中的角色类似，*通过模型连接点 – 回归方法*，通过限制行（列）向量中的值不会变得过大，从而可能引起过拟合。
- en: We could try several values of k and λ, and measure the mean squared error between
    the observed and predicted matrix (from multiplying the row factors by the column
    factors) to determine the optimal values.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以尝试几个 k 和 λ 的值，并测量观测值和预测矩阵（通过将行因子乘以列因子）之间的均方误差，以确定最佳值。
- en: Once we have obtained a good fit, we can use the `predict` and `predictAll`
    methods of the model object to obtain predictions for new users, and the persist
    it on disk using the `save` method.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们获得了一个良好的拟合，我们就可以使用模型对象的 `predict` 和 `predictAll` 方法来获取对新用户的预测，并使用 `save`
    方法将其持久化到磁盘上。
- en: Summary
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have examined complex, unstructured data. We cleaned and
    tokenized text and examined several ways of extracting features from documents
    in a way that could be incorporated into predictive models such as n-grams and
    tf-idf scores. We also examined dimensionality reduction techniques, such as the
    HashingVectorizer, matrix decompositions, such as PCA, CUR, NMF, and probabilistic
    models, such as LDA. We also examined image data, including normalization and
    thresholding operations, and how we can use dimensionality reduction techniques
    to find common patterns among images. Finally, we used a matrix factorization
    algorithm to prototype a recommender system in PySpark.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们检查了复杂、非结构化的数据。我们清理和标记了文本，并检查了将文档特征提取到可以纳入预测模型（如 n-gram 和 tf-idf 分数）的几种方法。我们还检查了降维技术，如
    HashingVectorizer，矩阵分解，如 PCA、CUR、NMF，以及概率模型，如 LDA。我们还检查了图像数据，包括归一化和阈值操作，以及我们如何使用降维技术来找到图像之间的共同模式。最后，我们使用矩阵分解算法在
    PySpark 中原型化了一个推荐系统。
- en: 'In the next section, you will also look at image data, but in a different context:
    trying to capture complex features from these data using sophisticated deep learning
    models.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，你还将查看图像数据，但处于不同的背景：尝试使用复杂的深度学习模型从这些数据中捕获复杂特征。
