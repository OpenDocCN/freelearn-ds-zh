- en: Chapter 6. Words and Pixels – Working with Unstructured Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章：文字与像素 - 处理非结构化数据
- en: Most of the data we have looked at thus far is composed of rows and columns
    with numerical or categorical values. This sort of information fits in both traditional
    spreadsheet software and the interactive Python notebooks used in the previous
    exercises. However, data is increasingly available in both this form, usually
    called structured data, and more complex formats such as images and free text.
    These other data types, also known as unstructured data, are more challenging
    than tabular information to parse and transform into features that can be used
    in machine learning algorithms.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们查看的大部分数据都是由行和列组成的，包含数值或分类值。这类信息既适合传统的电子表格软件，也适合之前练习中使用的交互式Python笔记本。然而，数据越来越多地以这种形式（通常称为结构化数据）和更复杂的格式（如图像和自由文本）提供。这些其他数据类型，也称为非结构化数据，比表格信息更难以解析和转换成机器学习算法中可用的特征。
- en: What makes unstructured data challenging to use? It is challenging largely because
    images and text are extremely high dimensional, consisting of a much larger number
    of columns or features than we have seen previously. For example, this means that
    a document may have thousands of words, or an image thousands of individual pixels.
    Each of these components may individually or in complex combinations comprise
    a feature for our algorithms. However, to use these data types in prediction,
    we need to somehow distill this extremely complex data into common features or
    trends that might be used effectively in a model. This often involves both removing
    noise from these data types and finding simpler representations. At the same time,
    the greater inherent complexity of these data types potentially captures more
    information than available in tabular datasets, or may reveal information that
    is not available in any other source.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 什么使得非结构化数据难以使用？这主要是因为图像和文本具有极高的维度，包含的列或特征数量比我们之前看到的要多得多。例如，这意味着一个文档可能有数千个单词，或一个图像有数千个单独的像素。这些组件可能单独或以复杂组合的形式构成我们算法的特征。然而，为了在预测中使用这些数据类型，我们需要以某种方式将这些极其复杂的数据提炼成通用的特征或趋势，这些特征或趋势可能在模型中有效使用。这通常涉及从这些数据类型中去除噪声并找到更简单的表示。同时，这些数据类型的更大内在复杂性可能比表格数据集包含的信息更多，或者可能揭示在其他任何来源中不可获得的信息。
- en: 'In this chapter, we will explore unstructured data by:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将通过以下方式探索非结构化数据：
- en: Cleaning raw text through stemming, stop word removal, and other normalizations
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过词干提取、停用词去除和其他规范化方法清理原始文本
- en: Using tokenization and n-grams to find common patterns in textual data
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用标记化和n-gram在文本数据中寻找共同模式
- en: Normalizing image data and removing noise
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正规化图像数据并去除噪声
- en: Decomposing images into lower dimensional features through several common matrix
    factorization algorithms
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过几种常见的矩阵分解算法将图像分解为低维特征
- en: Working with textual data
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理文本数据
- en: 'In the following example, we will consider the problem of separating text messages
    sent between cell phone users. Some of these messages are spam advertisements,
    and the objective is to separate these from normal communications (Almeida, Tiago
    A., José María G. Hidalgo, and Akebo Yamakami. *Contributions to the study of
    SMS spam filtering: new collection and results.* Proceedings of the 11th ACM symposium
    on Document engineering. ACM, 2011). By looking for patterns of words that are
    typically found in spam advertisements, we could potentially derive a smart filter
    that would automatically remove these messages from a user''s inbox. However,
    while in previous chapters we were concerned with fitting a predictive model for
    this kind of problem, here we will be shifting focus to cleaning up the data,
    removing noise, and extracting features. Once these tasks are done, either simple
    or lower-dimensional features can be input into many of the algorithms we have
    already studied.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们将考虑分离手机用户之间发送的短信的问题。其中一些信息是垃圾广告，目标是将这些信息与正常通信（Almeida, Tiago A., José
    María G. Hidalgo, 和 Akebo Yamakami. *对短信垃圾邮件过滤研究的新贡献：新的收集和结果.* 第11届ACM文档工程研讨会论文集。ACM，2011）区分开来。通过寻找在垃圾广告中通常发现的单词模式，我们可能能够开发出一个智能过滤器，自动从用户的收件箱中移除这些信息。然而，在之前的章节中，我们关注的是为这类问题拟合预测模型，而在这里，我们将重点转向清理数据、去除噪声和提取特征。一旦完成这些任务，简单或低维特征就可以输入到我们已研究的许多算法中。
- en: Cleaning textual data
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 清理文本数据
- en: 'Let us start by loading and inspecting the data using the following commands.
    Note that we need to supply column names for this data ourselves:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先使用以下命令加载数据并检查它。注意，我们需要自己提供此数据的列名：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This gives the following output:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了以下输出：
- en: '![Cleaning textual data](img/B04881_06_13.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![清洗文本数据](img/B04881_06_13.jpg)'
- en: 'The dataset consists of two columns: the first contains the label (`spam` or
    `ham`) indicating whether the message is an advertisement or a normal message,
    respectively. The second column contains the text of the message. Right at the
    start, we can see a number of problems with using this raw text as input to an
    algorithm to predict the spam/nonspam label:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含两列：第一列包含标签（`spam` 或 `ham`），分别表示消息是否为广告或普通消息。第二列包含消息的文本。一开始，我们可以看到使用这种原始文本作为算法预测垃圾邮件/非垃圾邮件标签输入时存在的一些问题：
- en: The text of each message contains a mixture of upper and lower case letters,
    but this capitalization does not affect the meaning of a word.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每条消息的文本包含大小写字母的混合，但这种大写形式并不影响单词的意义。
- en: Many words (*to*, *he*, *the*, and so on) are common, but tell us relatively
    little about the message.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多单词（如 *to*、*he*、*the* 等）很常见，但关于消息的信息相对较少。
- en: 'Other issues are subtler:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 其他问题更为微妙：
- en: When we compare words such as *larger* and *largest*, the most information about
    the meaning of the words is carried by the root, *large*—differentiating between
    the two forms may actually prevent us from capturing common information about
    the presence of the word *large* in a text, since the count of this stem in the
    message will be divided between the variants. Looking only at individual words
    does not tell us about the context in which they are used. Indeed, it may be more
    informative to consider sets of words.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们比较诸如 *larger* 和 *largest* 这样的单词时，关于单词意义的最多信息是由词根 *large* 承载的——区分这两种形式实际上可能阻止我们捕捉到关于单词
    *large* 在文本中出现的共同信息，因为消息中这个词根的计数将分布在各种变体之间。仅仅查看单个单词并不能告诉我们它们使用的上下文。实际上，考虑单词集合可能更有信息量。
- en: Even for words that do not fall into the common category, such as *and*, *the*,
    and *to*, it is sometimes unclear whether a word is present in a document because
    it is common across all documents or whether it contains special information about
    a particular document. For example, in a set of online movie reviews, words such
    as *character* and *film* will appear frequently, but do not help to distinguish
    one review from another since they are common across all reviews. Because the
    English language has a large vocabulary, the size of the resulting feature set
    could be enormous.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使对于不属于常见类别的单词，如 *and*、*the* 和 *to*，有时也不清楚一个单词是否出现在文档中，因为它在所有文档中都常见，或者它是否包含关于特定文档的特殊信息。例如，在一组在线电影评论中，像
    *character* 和 *film* 这样的单词会频繁出现，但它们并不能帮助区分不同的评论，因为它们在所有评论中都常见。由于英语词汇量很大，结果特征集的大小可能非常大。
- en: 'Let us start by cleaning up the text before delving into the other feature
    issues. We can base by lowercasing each word in the text using the following function:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先从清理文本开始，然后再深入研究其他特征问题。我们可以通过以下函数将文本中的每个单词转换为小写：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We then apply this function to each message using the map function we have
    seen in previous examples:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们然后将此函数应用于每条消息，使用我们在之前的示例中看到的映射函数：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Inspecting the resulting we can verify that all the letters are now indeed
    lowercase:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 检查结果可以验证所有字母现在确实都是小写的：
- en: '![Cleaning textual data](img/B04881_06_14.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![清洗文本数据](img/B04881_06_14.jpg)'
- en: 'Next, we want to remove common words and trim the remaining vocabulary to just
    the stem portion of the word that is most useful for predictive modeling. We do
    this using the **natural language toolkit** (**NLTK**) library (Bird, Steven.
    *NLTK: the natural language toolkit*. Proceedings of the COLING/ACL on Interactive
    presentation sessions. Association for Computational Linguistics, 2006.). The
    list of stop words is part of the dataset associated for download with this library;
    if this is your first time opening NLTK, you can use the `nltk.download()` command
    to open a **graphical user interface** (**GUI**) where you can select the content
    you wish to copy to your local machine using the following commands:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '接下来，我们希望删除常见单词并修剪剩余词汇表，仅保留对预测建模最有用的单词的词干部分。我们使用 **自然语言工具包**（**NLTK**）库（Bird,
    Steven. *NLTK: the natural language toolkit*. Proceedings of the COLING/ACL on
    Interactive presentation sessions. Association for Computational Linguistics,
    2006）来完成这项操作。停用词表是该库关联下载的数据集的一部分；如果您是第一次打开 NLTK，可以使用 `nltk.download()` 命令打开一个
    **图形用户界面**（**GUI**），在那里您可以使用以下命令选择要复制到本地计算机的内容：'
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We then define a function to perform stemming:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们定义一个函数来执行词干提取：
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Finally, we again use a lambda function to perform this operation on each message,
    and visually inspect the results:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们再次使用 lambda 函数对每个消息执行此操作，并直观地检查结果：
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![Cleaning textual data](img/B04881_06_15.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![清洗文本数据](img/B04881_06_15.jpg)'
- en: For example, you can see the stem *joke* has been extracted from *joking*, and
    *avail* from *available*.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，您可以看到从 *joking* 中提取了词干 *joke*，以及从 *available* 中提取了 *avail*。
- en: Now that we have performed lower casing and stemming, the messages are in relatively
    cleaned up form, and we can proceed to generate features for predictive modeling
    from this data.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了小写化和词干提取，消息处于相对清洁的状态，我们可以从这个数据中生成用于预测建模的特征。
- en: Extracting features from textual data
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从文本数据中提取特征
- en: 'In perhaps the simplest possible feature for text data, we use a binary vector
    of *0s* and *1s* to simply record the presence or absence of each word in our
    vocabulary in each message. To do this we can utilize the `CountVectorizer` function
    in the `scikit-learn` library, using the following commands:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在可能的最简单文本数据特征中，我们使用由 *0s* 和 *1s* 组成的二进制向量来简单地记录词汇表中每个单词在每个消息中的存在或不存在。为此，我们可以利用
    `scikit-learn` 库中的 `CountVectorizer` 函数，使用以下命令：
- en: '[PRE6]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'By default, the result is stored as a *sparse vector*, which means that only
    the non-zero elements are held in memory. To calculate the total size of this
    vector we need to transform it back into a *dense* vector (where all elements,
    even 0, are stored in memory):'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，结果存储为 *稀疏向量*，这意味着只有非零元素被保留在内存中。为了计算这个向量的总大小，我们需要将其转换回 *密集向量*（其中所有元素，包括0，都存储在内存中）：
- en: '[PRE7]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: By checking the length of the feature vector created for the first message,
    we can see that it creates a vector of length 7,468 for each message with 1 and
    0 indicating the presence or absence, respectively, of a particular word out of
    all words in this document list.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 通过检查为第一条消息创建的特征向量长度，我们可以看到它为每个消息创建了一个长度为7,468的向量，其中1和0分别表示特定单词在文档列表中的存在或不存在。
- en: 'We can check that this length is in fact the same as the vocabulary (union
    of all unique words in the messages) using the following command to extract the
    `vocabulary_ element` of the vectorizer, which also gives a value of 7,468:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下命令检查这个长度实际上与词汇表（消息中所有唯一单词的并集）相同，该命令提取向量化器的 `vocabulary_` 元素，它也给出了7,468的值：
- en: '[PRE8]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We can see that this increases the size of the resulting feature by about 10-fold
    by again inspecting the length of the first row using:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，通过再次检查第一行的长度，这个操作将结果特征的大小增加了大约10倍：
- en: '[PRE9]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'However, even after calculating n-grams, we still have not accounted for the
    fact that some words or n-grams might be common across all messages and thus provide
    little information in distinguishing spam from nonspam. To account for this, instead
    of simply recording the presence or absence of a word (or n-gram), we might compare
    the frequency of words within a document to the frequency across all documents.
    This ratio, the **term-frequency-inverse document frequency** (**tf-idf**) is
    calculated in the simplest form as:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，即使在计算了 n-gram 之后，我们仍未考虑到某些单词或 n-gram 可能跨越所有消息，因此提供的信息很少，难以区分垃圾邮件和非垃圾邮件。为了解决这个问题，我们可能不会简单地记录单词（或
    n-gram）的存在或不存在，而是比较文档中单词的频率与所有文档的频率。这个比率，即 **词频-逆文档频率**（**tf-idf**），以最简单形式计算如下：
- en: '![Extracting features from textual data](img/B04881_06_01.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![从文本数据中提取特征](img/B04881_06_01.jpg)'
- en: Where *ti* is a particular term (word or n-gram), *dj* is a particular document,
    *D* is the number of documents, *Vj* is the set of words in document *j*, and
    *vk* is a particular word in document *j*. The subscripted 1 in this formula is
    known as an **Indicator Function**, which returns `1` if the subscripted condition
    is `true`, and `0` otherwise. In essence, this formula compares the frequency
    (count) of a word within a document to the number of documents that contain this
    word. As the number of documents containing the word decreases, the denominator
    decreases, and thus the overall formula becomes larger from dividing by a value
    much less than `1`. This is balanced by the frequency of the word within a document
    in the numerator. Thus, the `tf-idf` score will more heavily weight words that
    are present at greater frequency within a document compared to those common among
    all documents and thus might be indicative of special features of a particular
    message.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*t*i是一个特定的术语（单词或n-gram），*d*j是一个特定的文档，*D*是文档的数量，*V*j是文档*j*中的单词集合，*v*k是文档*j*中的一个特定单词。这个公式中的下标`1`被称为**指示函数**，如果下标条件为`true`则返回`1`，否则返回`0`。本质上，这个公式比较了一个单词在文档中的频率（计数）与包含这个单词的文档数量。随着包含该单词的文档数量的减少，分母减少，因此整体公式在除以一个远小于`1`的值时变得更大。这通过分子中单词在文档中的频率来平衡。因此，`tf-idf`分数将更重视在文档中频率更高的单词，相对于在所有文档中都常见的单词，这些单词可能表明特定消息的特殊特征。
- en: 'Note that the formula above represents only the simplest version of this expression.
    There are also variants in which we might logarithmically transform the counts
    (to offset the bias from large documents), or scale the numerator by the maximum
    frequency found for any term within a document (again, to offset bias that longer
    documents could have higher term frequencies than shorter documents by virtue
    of simply having more words) (Manning, Christopher D., Prabhakar Raghavan, and
    Hinrich Schütze. *Scoring, term weighting and the vector space model.* Introduction
    to Information Retrieval 100 (2008): 2-4.). We can apply `tf-idf` to the spam
    data using the following commands:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '注意，上面的公式仅代表这个表达式的最简单版本。还有一些变体，我们可能会对计数进行对数变换（以抵消来自大型文档的偏差），或者通过文档中任何术语的最大频率来缩放分子（再次，为了抵消较长的文档可能由于拥有更多单词而具有比短文档更高的术语频率的偏差）（Manning,
    Christopher D., Prabhakar Raghavan, and Hinrich Schütze. *评分、术语加权和向量空间模型.* 信息检索导论
    100 (2008): 2-4）。我们可以使用以下命令对垃圾邮件数据应用`tf-idf`：'
- en: '[PRE10]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We can see the effect of this transformation by taking the maximum value across
    rows using:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用以下方法来观察这种变换的影响：
- en: '[PRE11]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Where the '1' argument to max indicates that the function is applied along rows
    (instead of columns, which would be specified with '0' ). When our features consisted
    only of binary values, the maximum across each rows would be 1, but we can see
    that it is now a float value.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 当`max`函数的`1`参数指示函数是沿着行（而不是列，列可以通过`0`指定）应用时。当我们的特征仅由二进制值组成时，每行的最大值会是`1`，但我们现在可以看到它是一个浮点值。
- en: 'The final text feature we will discuss is concerned with condensing our feature
    set. Simply put, as we consider larger and larger vocabularies, we will encounter
    many words that are so infrequent as to almost never appear. However, from a computational
    standpoint, even a single instance of a word in one document is enough to expand
    the number of columns in our text features for all documents. Given this, instead
    of directly recording whether a word is present, we might think of compressing
    this space requirement so that we use fewer columns to represent the same dataset.
    While in some cases, two words might map to the same column, in practice this
    happens infrequently enough due to the long-tailed distribution of word frequencies
    that it can serve as a handy way to reduce the dimensionality of our text data.
    To perform this mapping, we make use of a hash function that takes as input a
    word and outputs a random number (column location) that is keyed to the value
    of that string. The number of columns we ultimately map to in our transformed
    dataset is controlled by the `n_features` argument to the `HashingVectorizer`,
    which we can apply to our dataset using the following commands:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要讨论的最后一个文本特征是关于压缩我们的特征集。简单来说，当我们考虑越来越大的词汇表时，我们会遇到许多非常罕见的单词，以至于几乎从未出现过。然而，从计算的角度来看，即使一个文档中的一个单词的实例也足以增加我们所有文档文本特征中的列数。鉴于这一点，我们可能认为可以通过压缩空间需求来减少列数，从而用更少的列来表示相同的数据集。虽然在某些情况下，两个单词可能映射到同一列，但由于单词频率的长尾分布，这种情况在实践中很少发生，这可以作为一种方便的方法来降低我们文本数据的维度。为了执行这种映射，我们使用一个哈希函数，该函数将单词作为输入并输出一个随机数（列位置），该随机数与该字符串的值相关联。我们最终映射到转换后的数据集中的列数由`HashingVectorizer`的`n_features`参数控制，我们可以使用以下命令将其应用于我们的数据集：
- en: '[PRE12]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Using dimensionality reduction to simplify datasets
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用降维简化数据集
- en: Even though using the `HashingVectorizer` allows us to reduce the data to a
    set of 1,024 columns from a feature set that was much larger, we are still left
    with many variables in our dataset. Intuition tells us that some of these features,
    either before or after the application of the `HashingVectorizer`, are probably
    correlated. For example, a set of words may co-occur in a document that is spam.
    If we use n-grams and the words are adjacent to one another, we could pick up
    on this feature, but not if the words are simply present in the message but separated
    by other text. The latter might occur, for example, if some common terms are in
    the first sentence of the message, while others are near the end.More broadly,
    given a large set of variables such as we have already seen for textual data,
    we might ask whether we could represent these data using a more compact set of
    features. In other words, is there an underlying pattern to the variation in thousands
    of variables that may be extracted by calculating a much smaller number of features
    representing patterns of correlation between individual variables? In a sense,
    we already saw several examples of this idea in [Chapter 3](ch03.html "Chapter 3. Finding
    Patterns in the Noise – Clustering and Unsupervised Learning"), *Finding Patterns
    in the Noise – Clustering and Unsupervised Learning*, in which we reduced the
    complexity of a dataset by aggregating individual datapoints into clusters. In
    the following examples, we have a similar goal, but rather than aggregating individual
    datapoints, we want to capture groups of correlated variables.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管使用`HashingVectorizer`可以将数据减少到从更大的特征集到1,024列，但我们数据集中仍然存在许多变量。直觉告诉我们，这些特征中的一些，无论是在应用`HashingVectorizer`之前还是之后，可能存在相关性。例如，一组单词可能在一个垃圾邮件文档中同时出现。如果我们使用n-gram并且单词相互相邻，我们可能会注意到这个特征，但如果单词只是出现在消息中但被其他文本分隔，则不会。例如，如果某些常见术语出现在消息的第一句话中，而其他术语则接近结尾。更广泛地说，考虑到我们已看到的大量变量，例如文本数据，我们可能会问是否可以用更紧凑的特征集来表示这些数据。换句话说，是否有潜在的规律可以描述成千上万的变量变化，这些变量可以通过计算代表个体变量之间相关性的更少数量的特征来提取？在某种程度上，我们在[第3章](ch03.html
    "第3章。在噪声中寻找模式 – 聚类和无监督学习")中已经看到了几个这个想法的例子，即通过将单个数据点聚合到聚类中来降低数据集的复杂性。在以下例子中，我们有一个类似的目标，但不是聚合单个数据点，而是想要捕捉相关变量的组。
- en: While we might achieve this goal in part through the variable selection techniques
    such as regularization, which we discussed in the [Chapter 4](ch04.html "Chapter 4. Connecting
    the Dots with Models – Regression Methods"), *Connecting the Dots with Models
    – Regression Methods*, we do not necessarily want to remove variables, but rather
    capture their common patterns of variation.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们可能通过变量选择技术如正则化（我们在[第4章](ch04.html "第4章. 通过模型连接点 – 回归方法")中讨论过）部分实现这一目标，即“通过模型连接点
    – 回归方法”，但我们并不一定想要删除变量，而是要捕捉它们变化的共同模式。
- en: Let us examine some of the common methods of dimensionality reduction and how
    we might choose between them for a given problem.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考察一些常见的降维方法，以及对于给定问题我们如何在这之间进行选择。
- en: Principal component analysis
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主成分分析
- en: 'One of the most commonly used methods of dimensionality reduction is **Principal
    Component Analysis** (**PCA**). Conceptually, PCA computes the axes along which
    the variation in the data is greatest. You may recall that in [Chapter 3](ch03.html
    "Chapter 3. Finding Patterns in the Noise – Clustering and Unsupervised Learning"),
    *Finding Patterns in the Noise – Clustering and Unsupervised Learning*, we calculated
    the eigenvalues of the adjacency matrix of a dataset to perform spectral clustering.
    In PCA, we also want to find the eigenvalue of the dataset, but here, instead
    of any adjacency matrix, we will use the covariance matrix of the data, which
    is the relative variation within and between columns. The covariance for columns
    `xi` and `xj` in the data matrix `X` is given by:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 最常用的降维方法之一是**主成分分析**（**PCA**）。从概念上讲，PCA计算数据变化最大的轴。你可能还记得，在[第3章](ch03.html "第3章.
    在噪声中寻找模式 – 聚类和无监督学习")中，“在噪声中寻找模式 – 聚类和无监督学习”，我们计算了数据集的邻接矩阵的特征值以执行谱聚类。在PCA中，我们同样想要找到数据集的特征值，但在这里，我们不会使用任何邻接矩阵，而是会使用数据的协方差矩阵，它是列之间的相对变化。数据矩阵`X`中列`xi`和`xj`的协方差由以下公式给出：
- en: '![Principal component analysis](img/B04881_06_02.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![主成分分析](img/B04881_06_02.jpg)'
- en: 'This is the average product of the offsets from the mean column values. We
    saw this value before when we computed the correlation coefficient in [Chapter
    3](ch03.html "Chapter 3. Finding Patterns in the Noise – Clustering and Unsupervised
    Learning"), *Finding Patterns in the Noise – Clustering and Unsupervised Learning*,
    as it is the denominator of the Pearson coefficient. Let us use a simple example
    to illustrate how PCA works. We will make a dataset in which the six columns are
    derived from the same underlying normal distribution, one of which is given reversed
    in sign, using the following commands:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这是均值列值偏移的平均乘积。我们在[第3章](ch03.html "第3章. 在噪声中寻找模式 – 聚类和无监督学习")中计算相关系数时见过这个值，作为皮尔逊系数的分母。让我们用一个简单的例子来说明PCA是如何工作的。我们将创建一个数据集，其中六个列是从同一个基本正态分布中得出的，其中一个列的符号被反转，使用以下命令：
- en: '[PRE13]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Note that each of our columns has mean `0` and standard deviation `1`. If this
    were not the case, we could use the scikit-learn utility StandardScaler as we
    discussed in [Chapter 3](ch03.html "Chapter 3. Finding Patterns in the Noise –
    Clustering and Unsupervised Learning"), *Finding Patterns in the Noise – Clustering
    and Unsupervised Learning*, when we normalized data for use in k means clustering.
    We might simply center the variables at `0` and use the resulting covariance matrix
    if we believe that the differences in scale of the variables are important to
    our problem. Otherwise, differences in scale will tend to be reflected by the
    differing variance values within the columns of the data, so our resulting PCA
    will reflect not only correlations within variables but also their differences
    in magnitude. If we do not want to emphasize these differences and are only interested
    in the relative correlation among variables, we can also divide each column of
    the data by its standard deviation to give each column a variance of 1\. We could
    also potentially run PCA not on the covariance matrix, but the Pearson correlation
    matrix between variables, which is already naturally scaled to 0 and a constant
    range of a values (from -1 to 1) (Kromrey, Jeffrey D., and Lynn Foster-Johnson.
    *Mean centering in moderated multiple regression: Much ado about nothing.* Educational
    and Psychological Measurement 58.1 (1998): 42-67.). For now, we can compute the
    covariance matrix of our data with the following command:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '注意，我们每一列的均值都是 `0`，标准差是 `1`。如果不是这样，我们可以使用我们在[第 3 章](ch03.html "第 3 章。在噪声中寻找模式
    – 聚类和无监督学习")中讨论的 scikit-learn 工具 StandardScaler，即 *在噪声中寻找模式 – 聚类和无监督学习*，当我们对数据进行归一化以用于
    k 均值聚类时。如果我们认为变量的尺度差异对我们问题很重要，我们可以简单地将变量中心化到 `0`，并使用得到的协方差矩阵。否则，尺度差异将倾向于通过数据列中的不同方差值来反映，因此我们的结果
    PCA 将不仅反映变量之间的相关性，还会反映它们的大小差异。如果我们不想强调这些差异，并且只对变量之间的相对相关性感兴趣，我们还可以将数据中的每一列除以其标准差，使每一列的方差为
    1。我们还可以潜在地运行 PCA，不是在协方差矩阵上，而是在变量之间的皮尔逊相关矩阵上，该矩阵已经自然地缩放到 0 和一个常数范围（从 -1 到 1）的值（Kromrey,
    Jeffrey D. 和 Lynn Foster-Johnson. *调节多重回归中的均值中心化：无事生非.* 教育与心理测量 58.1 (1998): 42-67）。现在，我们可以使用以下命令计算我们数据的协方差矩阵：'
- en: '[PRE14]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Recalling our discussion of spectral clustering in [Chapter 3](ch03.html "Chapter 3. Finding
    Patterns in the Noise – Clustering and Unsupervised Learning"), *Finding Patterns
    in the Noise – Clustering and Unsupervised Learning*, if we consider the covariance
    matrix as a stretching operation on a vector, then, if we find the vectors that
    lie along these directions of distortion, we have in a sense found the axes that
    define the variation in the data. If we then compare the eigenvalues of these
    vectors, we could determine if one or more of these directions reflect a greater
    proportion of the overall variation of the data. Let us compute the eigenvalues
    and vectors of the covariance matrix using:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 回想我们在[第 3 章](ch03.html "第 3 章。在噪声中寻找模式 – 聚类和无监督学习")中关于谱聚类的讨论，即 *在噪声中寻找模式 – 聚类和无监督学习*，如果我们把协方差矩阵看作是对向量的拉伸操作，那么，如果我们找到沿着这些扭曲方向的向量，我们就在某种程度上找到了定义数据变化的轴。如果我们然后比较这些向量的特征值，我们可以确定这些方向中是否有一个或多个反映了数据整体变化的一个更大比例。让我们使用以下命令计算协方差矩阵的特征值和向量：
- en: '[PRE15]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This gives the following eigenvalue variable as:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了以下特征值变量：
- en: '[PRE16]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'You can see that most of the eigenvalues are effectively zero, except the second.
    This reflects the fact that the data we constructed, despite having six columns,
    is effectively derived from only one dataset (a normal distribution). Another
    important property of these eigenvectors is that they are orthogonal, which means
    that they are at right angles to each other in n-dimensional space: if we were
    to take a dot product between them, it would be 0, and they thus represent independent
    vectors that, when linearly combined, can be used to represent the dataset.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，除了第二个之外，大多数特征值实际上都是零。这反映了我们构建的数据，尽管有六个列，实际上只来源于一个数据集（一个正态分布）。这些特征向量的另一个重要特性是它们是正交的，这意味着在
    n 维空间中它们彼此垂直：如果我们把它们之间的点积取出来，它将是 0，因此它们代表独立的向量，当它们线性组合时，可以用来表示数据集。
- en: 'If we were to multiply the data by the eigenvector corresponding to this second
    eigenvalue, we would project the data from a six-dimensional to a one-dimensional
    space:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将数据乘以与第二个特征值对应的特征向量，我们将把数据从六维空间投影到一维空间：
- en: '[PRE17]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Note that we needed to transpose the data to have the 100 rows and 6 columns,
    as we initially constructed it as a list of 6 columns, which NumPy interprets
    as instead having 6 rows and 100 columns. The resulting histogram is as shown
    in the following:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们需要转置数据以获得100行和6列，因为我们最初将其构建为一个包含6列的列表，而NumPy将其解释为有6行和100列。结果直方图如下所示：
- en: '![Principal component analysis](img/B04881_06_17.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![主成分分析](img/B04881_06_17.jpg)'
- en: 'In other words, by projecting the data onto the axis of greatest variance,
    we have recovered that fact that this six-column data was actually generated from
    a single distribution. Now if we instead use the PCA command, we get a similar
    result:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，通过将数据投影到最大方差轴上，我们恢复了这样一个事实，即这六个列数据实际上是从一个单一分布生成的。现在，如果我们使用PCA命令，我们会得到类似的结果：
- en: '[PRE18]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'When we extract the `explained_variance_ratio_`, the algorithm has effectively
    taken the preceding eigenvalues, ordered them by magnitude, and divided by the
    largest one, giving:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们提取`explained_variance_ratio_`时，算法实际上已经取了前面的特征值，按大小排序，然后除以最大的一个，得到：
- en: '[PRE19]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'If we were to plot these as a barplot, a visualization known as a `scree plot`
    could help us determine how many underlying components are represented in our
    data:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将这些数据绘制成条形图，一种称为“特征值分布图”的可视化可以帮助我们确定数据中代表了多少个潜在成分：
- en: '[PRE20]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This generates the following plot:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这生成了以下图表：
- en: '![Principal component analysis](img/B04881_06_18.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![主成分分析](img/B04881_06_18.jpg)'
- en: 'Evidently, only the first component carries any variance, represented by the
    height of the bar, with all other components being near 0 and so not appearing
    in the plot. This sort of visual analysis is comparable to how we looked for an
    elbow in the inertia function for k-means in [Chapter 3](ch03.html "Chapter 3. Finding
    Patterns in the Noise – Clustering and Unsupervised Learning"), *Finding Patterns
    in the Noise – Clustering and Unsupervised Learning*, as a function of k to determine
    how many clusters were present in the data.We can also extract the data projected
    onto the first principal components and see a similar plot as shown previously
    when we projected the data onto an eigenvector of the covariance matrix:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，只有第一个成分携带任何方差，由条形的高度表示，其他所有成分都接近0，因此在图表中不显示。这种视觉分析方法类似于我们在[第3章](ch03.html
    "第3章. 在噪声中寻找模式 – 聚类和无监督学习")中寻找k-means的惯性函数中的肘部，*在噪声中寻找模式 – 聚类和无监督学习*，作为k的函数来确定数据中存在多少个簇。我们还可以提取投影到第一个主成分上的数据，并看到与之前将数据投影到协方差矩阵的特征向量上时相似的图表：
- en: '[PRE21]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![Principal component analysis](img/B04881_06_38.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![主成分分析](img/B04881_06_38.jpg)'
- en: 'Why are they not exactly identical? While conceptually PCA computes the eigenvalues
    of the covariance matrix, in practice most packages do not actually implement
    the calculation we illustrated previously for purposes of numerical efficiency.
    Instead, they employ a matrix operation known as the **Singular Value Decomposition**
    (**SVD**), which seeks to represent a covariance matrix of *X* as a set of lower
    dimensional row and column matrices:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么它们不完全相同？虽然从概念上讲，PCA计算协方差矩阵的特征值，但在实践中，大多数软件包为了数值效率并没有实际实现我们之前展示的计算。相反，它们采用一种称为**奇异值分解**（**SVD**）的矩阵运算，试图将*X*的协方差矩阵表示为一系列低维行和列矩阵：
- en: '![Principal component analysis](img/B04881_06_03.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![主成分分析](img/B04881_06_03.jpg)'
- en: 'Where if *X* is an *n* by *m*, W may be *n* by *k*, where *k << m*. Here, σ
    represents a matrix with 0 everywhere but the diagonal, which contains non-zero
    entries. Thus, the covariance matrix is represented as the product of two smaller
    matrices and a scaling factor given by the diagonal elements in σ. Instead of
    calculating all eigenvectors of the covariance matrix, as we did previously, we
    can ask only for the k columns or WT we think are likely to be significant judged
    by the sort of scree plot analysis we demonstrated above. However, when we project
    the data onto the principal components we obtain through this method, the calculation
    of the SVD can potentially give different signs to the projection of the data
    on the principal components, even if the relative magnitude and signs of these
    components remains the same. Thus, when we look at the scores assigned to a given
    row of data after projecting it onto the first k principal components, we should
    analyze them relative to other values in the dataset, just as when we examined
    the coordinates produced by Multidimensional Scaling in [Chapter 3](ch03.html
    "Chapter 3. Finding Patterns in the Noise – Clustering and Unsupervised Learning"),
    *Finding Patterns in the Noise – Clustering and Unsupervised Learning*. Details
    of the SVD calculation used by the default scikit-learn implementation of PCA
    are given in (Tipping, Michael E., and Christopher M. Bishop. *Probabilistic principal
    component analysis*. Journal of the Royal Statistical Society: Series B (Statistical
    Methodology) 61.3 (1999): 611-622.).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '其中，如果*X*是一个*n*行*m*列的矩阵，W可能是一个*n*行*k*列的矩阵，其中*k*远小于*m*。在这里，σ代表一个对角线以外的所有元素都是0的矩阵，对角线上的元素是非零的。因此，协方差矩阵表示为两个较小的矩阵的乘积和一个由σ的对角线元素给出的缩放因子。与之前我们计算协方差矩阵的所有特征向量不同，我们可以只要求k列或WT，我们认为它们可能是有意义的，根据我们上面展示的散点图分析。然而，当我们通过这种方法将数据投影到主成分上时，奇异值分解的计算可能会给数据在主成分上的投影带来不同的符号，即使这些成分的相对大小和符号保持不变。因此，当我们查看将数据投影到前k个主成分后分配给特定行的分数时，我们应该将它们与其他数据集中的值进行比较，就像我们在[第3章](ch03.html
    "第3章. 在噪声中寻找模式 – 聚类和无监督学习")中检查多维尺度产生的坐标时一样，*在噪声中寻找模式 – 聚类和无监督学习*。默认scikit-learn
    PCA实现中使用的SVD计算的详细信息见(Tipping, Michael E.，and Christopher M. Bishop. *概率主成分分析*.
    英国皇家统计学会会刊：系列B（统计方法）61.3 (1999): 611-622.).'
- en: 'Now that we have examined conceptually what PCA calculates, let us see if it
    can help us reduce the dimensionality of our text dataset. Let us run PCA on the
    n-gram feature set from above, asking for 100 components. Note that because the
    original dataset is a sparse matrix and PCA requires a dense matrix as an input,
    we need to convert it using `toarray()`. Also, to retain the right dimensionality
    for use with the PCA fit function, we need to transpose the result:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经从概念上了解了PCA计算的内容，让我们看看它是否可以帮助我们减少文本数据集的维度。让我们对上面的n-gram特征集运行PCA，请求100个成分。请注意，因为原始数据集是一个稀疏矩阵，而PCA需要一个密集矩阵作为输入，我们需要使用`toarray()`将其转换。此外，为了保留与PCA拟合函数使用的正确维度，我们需要转置结果：
- en: '[PRE22]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'If we make a scree plot of total variance explained by the first `10` principal
    components of this dataset, we see that we will probably require a relatively
    large number of variables to capture the variation in our data since the upward
    trend in variance explained is relatively smooth:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将这个数据集的前10个主成分解释的总方差绘制成散点图，我们会看到我们可能需要相对较多的变量来捕捉数据的变异，因为解释的方差上升趋势相对平滑：
- en: '[PRE23]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![Principal component analysis](img/B04881_06_19.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![主成分分析](img/B04881_06_19.jpg)'
- en: 'We could also visualize this by looking at the cumulative variance explained
    using *k* components using the following curve:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以通过查看使用*k*个成分解释的累积方差来可视化这一点，如下面的曲线所示：
- en: '[PRE24]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![Principal component analysis](img/B04881_06_21.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![主成分分析](img/B04881_06_21.jpg)'
- en: 'A word on normalization: in practice, for document data, we might not want
    to scale the data by subtracting the mean and dividing by the variance as the
    data is mostly binary. Instead, we would just apply the SVD to a binary matrix
    or perhaps the tF-idf scores we computed previously, an approach also known as
    **Latent Semantic Indexing** (**LSI**) (Berry, Michael W., Susan T. Dumais, and
    Gavin W. O''Brien. *Using linear algebra for intelligent information retrieval*.
    SIAM review 37.4 (1995): 573-595; Laham, T. K. L. D., and Peter Foltz. *Learning
    human-like knowledge by singular value decomposition: A progress report*. Advances
    in Neural Information Processing Systems 10: Proceedings of the 1997 Conference.
    Vol. 10\. MIT Press, 1998.). CUR decomposition and nonnegative matrix factorization'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '关于归一化的一点说明：在实践中，对于文档数据，我们可能不希望通过减去均值并除以方差来缩放数据，因为数据大多是二进制的。相反，我们只需对二进制矩阵或我们之前计算过的tF-idf分数应用SVD，这种方法也称为**潜在语义索引**（**LSI**）(**Latent
    Semantic Indexing**)（Berry, Michael W., Susan T. Dumais, and Gavin W. O''Brien.
    *Using linear algebra for intelligent information retrieval*. SIAM review 37.4
    (1995): 573-595; Laham, T. K. L. D., and Peter Foltz. *Learning human-like knowledge
    by singular value decomposition: A progress report*. Advances in Neural Information
    Processing Systems 10: Proceedings of the 1997 Conference. Vol. 10\. MIT Press,
    1998.）。'
- en: 'What drawbacks might there be to using PCA to reduce the dimensionality of
    a dataset? For one, the components (covariance matrix eigenvectors) generated
    by PCA are still essentially mathematical entities: the patterns in variables
    represented by these axes might not actually correspond to any element of the
    data, but rather a linear combination of them. This representation is not always
    easily interpretable, and can particularly difficult when trying to convey the
    results of such analyses to domain experts to generate subject-matter specific
    insights. Second, the fact that PCA produces negative values in its eigenvectors,
    even for positive-only data such as text (where a term cannot be negatively present
    in a document, just 0, 1, a count, or a frequency), is due to the fact that the
    data is linearly combined using these factors. In other words, positive and negative
    values may be summed together when we project the data onto its components through
    matrix multiplication, yielding an overall positive value for the projection.
    Again, it may be preferable to have factors that give some insight into the structure
    of the data itself, for example, by giving a factor that consists of binary indicators
    for a group of words that tend to co-occur in a particular group of documents.
    These goals are addressed by two other matrix factorization techniques: CUR Decomposition
    and Non-negative Matrix Factorization.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PCA降低数据集维度的潜在缺点可能有哪些？首先，PCA生成的成分（协方差矩阵的特征向量）本质上仍然是数学实体：这些轴所表示的变量模式可能实际上并不对应于数据的任何元素，而是它们的线性组合。这种表示并不总是容易理解，尤其是在尝试将此类分析的结果传达给领域专家以生成特定主题见解时尤其困难。其次，PCA在其特征向量中产生负值，即使对于只有正值的文本数据（在一个文档中，一个术语不能以负值出现，只能是0、1、计数或频率），这也是由于数据是使用这些因素进行线性组合的。换句话说，当我们通过矩阵乘法将数据投影到其成分上时，正负值可能会相加，从而产生投影的整体正值。再次，可能更希望有能够提供对数据本身结构的洞察力的因素，例如，通过提供一个由一组在特定文档组中倾向于共同出现的单词的二进制指示符组成的因素。这些目标通过两种其他矩阵分解技术得到解决：CUR分解和非负矩阵分解。
- en: 'Like the SVD used in PCA, CUR attempts to represent a matrix of data X as a
    product of lower dimensional matrices. Here, instead of eigenvectors, the CUR
    decomposition attempts to find the set of columns and rows of the matrix that
    best represent the dataset as:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 与PCA中使用的SVD类似，CUR试图将数据矩阵X表示为低维矩阵的乘积。在这里，CUR分解试图找到矩阵的列和行集合，以最佳方式表示数据集，如下所示：
- en: '![Principal component analysis](img/B04881_06_04.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![主成分分析](img/B04881_06_04.jpg)'
- en: 'Where *C* is a matrix of *c* columns of the original dataset, *R* is a set
    of *r* rows from the original dataset, and *U* is a matrix of scaling factors.
    The *c* columns and *r* rows used in this reconstruction are sampled from the
    columns and rows of the original matrix, with probability proportional to the
    `leverage score`, given by:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *C* 是原始数据集的 *c* 列矩阵，*R* 是原始数据集的 *r* 行集合，*U* 是缩放因子矩阵。在此重建中使用的 *c* 列和 *r* 行是从原始矩阵的列和行中采样的，其概率与`杠杆分数`成正比，该分数由以下给出：
- en: '![Principal component analysis](img/B04881_06_05.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![主成分分析](img/B04881_06_05.jpg)'
- en: 'Where *lvj* is the statistical leverage for column (row) *j*, *k* is the number
    of components in the SVD of *X*, and *vj* are the *jth* elements of these *k*
    component vectors. Thus, columns (rows) are sampled with high probability if they
    contribute significantly to the overall norm of the matrix''s singular values,
    meaning they are also have a major influence on the reconstruction error from
    the SVD (for example, how well the SVD approximates the original matrix) (Chatterjee,
    Samprit, and Ali S. Hadi. Sensitivity analysis in linear regression. Vol. 327\.
    John Wiley & Sons, 2009; Bodor, András, et al. *rCUR: an R package for CUR matrix
    decomposition*. BMC bioinformatics 13.1 (2012): 1).'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 *lvj* 是列（行）*j* 的统计杠杆，*k* 是 *X* 的奇异值分解中的分量数量，而 *vj* 是这些 *k* 个分量向量的第 *j* 个元素。因此，如果列（行）对矩阵奇异值总体范数的贡献显著，它们将以高概率被采样，这意味着它们也对从奇异值分解（例如，奇异值分解如何逼近原始矩阵）的重建误差有重大影响（Chatterjee,
    Samprit, 和 Ali S. Hadi. 线性回归中的敏感性分析. 第327卷. 约翰·威利与 Sons, 2009; Bodor, András,
    等人. *rCUR: 用于 CUR 矩阵分解的 R 包*. 生物信息学杂志 13.1 (2012): 1）。'
- en: While this decomposition is not expected to approximate the original dataset
    with the same accuracy as the SVD approach used in PCA, the resulting factors
    may be easier to interpret since they are actual elements of the original dataset.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这种分解可能不会像 PCA 中使用的奇异值分解方法那样精确地逼近原始数据集，但结果因子可能更容易解释，因为它们是原始数据集的实际元素。
- en: Note
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Please note that while we use SVD to determine sampling probabilities for the
    columns and rows, the final factorization of CUR does not.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，虽然我们使用奇异值分解来确定列和行的采样概率，但 CUR 的最终分解并不这样做。
- en: 'There are many algorithms for generating a CUR decomposition (Mahoney, Michael
    W., and Petros Drineas. *CUR matrix decompositions for improved data analysis.
    Proceedings of the National Academy of Sciences 106.3 (2009): 697-702\. Boutsidis,
    Christos, and David P. Woodruff. Optimal cur matrix decompositions*. Proceedings
    of the 46th Annual ACM Symposium on Theory of Computing. ACM, 2014). CUR decomposition
    is implemented in the `pymf` library, and we can call it using the following commands:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '存在许多生成 CUR 分解的算法（Mahoney, Michael W. 和 Petros Drineas. *CUR 矩阵分解以提高数据分析*. 美国国家科学院院刊
    106.3 (2009): 697-702. Boutsidis, Christos 和 David P. Woodruff. 最优 CUR 矩阵分解*.
    第 46 届年度 ACM 理论计算研讨会. ACM, 2014）。CUR 分解在 `pymf` 库中实现，我们可以使用以下命令调用它：'
- en: '[PRE25]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The `crank` and `rrank` parameters indicate how many rows and columns, respectively,
    should be chosen from the original matrix in the process of performing the decomposition.
    We can then examine which columns (words from the vocabulary) were chosen in this
    reconstruction using the following commands to print these significant words whose
    indices are contained in the cur object''s .`_cid` (column index) element. First
    we need to collect a list of all words in the vocabulary of our spam dataset:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '`crank` 和 `rrank` 参数指示在执行分解过程中应从原始矩阵中选择多少行和列。然后我们可以使用以下命令来打印出在这次重建中选择的列（词汇表中的单词）的索引包含在
    cur 对象的 .`_cid`（列索引）元素中，以检查这些显著单词。首先我们需要收集我们垃圾邮件数据集词汇表中的所有单词列表：'
- en: '[PRE26]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Since the `vocabulary_` variable returned by the `CountVectorizer` is a dictionary
    giving the positions of terms in the array to which they are mapped, we need to
    construct our array by placing the word at the position given by this dictionary.
    Now we can print the corresponding words using:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 `CountVectorizer` 返回的 `vocabulary_` 变量是一个字典，它给出了术语在映射到的数组中的位置，我们需要通过放置由这个字典给出的位置上的单词来构建我们的数组。现在我们可以使用以下命令打印出相应的单词：
- en: '[PRE27]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Like CUR, nonnegative matrix factorization attempts to find a set of positive
    components that represents the structure of a dataset (Lee, Daniel D., and H.
    Sebastian Seung. *Learning the parts of objects by non-negative matrix factorization.*
    Nature 401.6755 (1999): 788-791; Lee, Daniel D., and H. Sebastian Seung. *Algorithms
    for non-negative matrix factorization.* Advances in neural information processing
    systems. 2001.; P. Paatero, U. Tapper (1994). Paatero, Pentti, and Unto Tapper.
    *Positive matrix factorization: A non-negative factor model with optimal utilization
    of error estimates of data values*. Environmetrics 5.2 (1994): 111-126\. Anttila,
    Pia, et al. *Source identification of bulk wet deposition in Finland by positive
    matrix factorization*. Atmospheric Environment 29.14 (1995): 1705-1718.). Similarly,
    it tries to reconstruct the data using:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '与 CUR 类似，非负矩阵分解试图找到一组正成分来表示数据集的结构（Lee, Daniel D., 和 H. Sebastian Seung. *通过非负矩阵分解学习物体的部分*.
    自然 401.6755 (1999): 788-791; Lee, Daniel D., 和 H. Sebastian Seung. *非负矩阵分解的算法*.
    神经信息处理系统进展. 2001.; P. Paatero, U. Tapper (1994). Paatero, Pentti, 和 Unto Tapper.
    *正矩阵分解：一种非负因子模型，具有对数据值误差估计的最优利用*. 环境计量学 5.2 (1994): 111-126\. Anttila, Pia, 等人.
    *通过正矩阵分解识别芬兰的大量湿沉降源*. 大气环境 29.14 (1995): 1705-1718.). 同样，它试图使用以下方法重建数据：'
- en: '![Principal component analysis](img/B04881_06_06.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![主成分分析](img/B04881_06_06.jpg)'
- en: Where *W* and *H* are lower dimensional matrices that when multiplied, reconstruct
    *X*; all three of *W*, *H*, and *X* are constrained to have no negative values.
    Thus, the columns of *X* are linear combinations of *W*, using *H* as the coefficients.
    For example, if the rows of *X* are words and the columns are documents, then
    each document in *X* is represented as a linear combination of underlying document
    types in *W* with weighted given by *H*. Like the elements returned by CUR decomposition,
    the components *W* from nonnegative matrix factorization are potentially more
    interpretable than the eigenvectors we get from PCA.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *W* 和 *H* 是低维矩阵，当相乘时可以重建 *X*；*W*、*H* 和 *X* 的所有三个矩阵都被限制为没有负值。因此，*X* 的列是 *W*
    的线性组合，使用 *H* 作为系数。例如，如果 *X* 的行是单词，列是文档，那么 *X* 中的每个文档都表示为 *W* 中基本文档类型的线性组合，其权重由
    *H* 给定。与 CUR 分解返回的元素一样，非负矩阵分解的 *W* 成分可能比我们从 PCA 得到的特征向量更容易解释。
- en: 'There are several algorithms to compute *W* and *H*, with one of the simplest
    being through multiplicative updates (Lee, Daniel D., and H. Sebastian Seung.
    *Algorithms for non-negative matrix factorization*. Advances in neural information
    processing systems. 2001). For example, if we want to minimize the Euclidean distance
    between *X* and *WH*:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 计算矩阵 *W* 和 *H* 的算法有好几种，其中最简单的一种是通过乘性更新（Lee, Daniel D., 和 H. Sebastian Seung.
    *非负矩阵分解的算法*. 神经信息处理系统进展. 2001）。例如，如果我们想最小化 *X* 和 *WH* 之间的欧几里得距离：
- en: '![Principal component analysis](img/B04881_06_07.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![主成分分析](img/B04881_06_07.jpg)'
- en: 'We can calculate the derivative of this value with respective to *W*:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以计算这个值相对于 *W* 的导数：
- en: '![Principal component analysis](img/B04881_06_08.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![主成分分析](img/B04881_06_08.jpg)'
- en: 'Then to update *W* we multiply at each step by this gradient:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 然后为了更新 *W*，我们在每一步乘以这个梯度：
- en: '![Principal component analysis](img/B04881_06_09.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![主成分分析](img/B04881_06_09.jpg)'
- en: 'And the same for *H*:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 *H* 也是如此：
- en: '![Principal component analysis](img/B04881_06_10.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![主成分分析](img/B04881_06_10.jpg)'
- en: 'These steps are repeated until the values of *W* and *H* converge. Let us examine
    what components we retrieve from our text data when we use NMF to extract components:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤会重复进行，直到 *W* 和 *H* 的值收敛。让我们看看当我们使用 NMF 提取成分时，从我们的文本数据中检索到的成分：
- en: '[PRE28]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We can then look at the words represented by the components in NMF, where the
    words have a large value in the components matrix resulting from the decomposition.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以查看 NMF 成分所表示的单词，其中单词在分解后的成分矩阵中有较大的值。
- en: 'We can see that they appear to capture distinct groups of words, but are any
    correlated with distinguishing spam versus nonspam? We can transform our original
    data using the NMF decomposition, which will give the weights for linearly combining
    these features (for example, the weights to linearly combine the 10 basis documents
    we get from the decomposition to reconstruct the message) using the command:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到它们似乎捕捉到不同的单词组，但它们是否与区分垃圾邮件和非垃圾邮件相关？我们可以使用 NMF 分解来转换我们的原始数据，这将给出线性组合这些特征的权重（例如，将分解得到的
    10 个基文档线性组合以重建消息的权重）使用以下命令：
- en: '[PRE29]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now let us plot the average weight assigned to each of these `nmf` factors
    for the normal and spam messages. We can do this by plotting a bar chart where
    the *x* axis are the 10 `nmf` factors, and the *y* axis are the average weight
    assigned to this factor for a subset of documents:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们绘制每个这些`nmf`因素分配给正常和垃圾邮件的平均权重。我们可以通过绘制一个条形图来实现，其中*x*轴是10个`nmf`因素，而*y*轴是分配给这个因素的平均权重，对于文档的一个子集：
- en: '[PRE30]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '![Principal component analysis](img/B04881_06_22.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![主成分分析](img/B04881_06_22.jpg)'
- en: '[PRE31]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '![Principal component analysis](img/B04881_06_23.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![主成分分析](img/B04881_06_23.jpg)'
- en: Promisingly, the factors 8 and 9 seem to have very different average weights
    between these two classes of messages. In fact, we may need fewer than 10 factors
    to represent the data, since these two classes may well correspond to the underlying
    spam versus nonspam messages.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 令人鼓舞的是，因素8和9在这两类消息之间似乎具有非常不同的平均权重。实际上，我们可能需要少于10个因素来表示数据，因为这两类可能很好地对应于潜在的垃圾邮件与非垃圾邮件消息。
- en: Latent Dirichlet Allocation
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 潜在狄利克雷分配
- en: 'A related method of decomposing data into an interpretable set of features
    is **Latent Dirichlet Allocation** (**LDA**), a method initially developed for
    textual and genetics data that has since been extended to other areas (Blei, David
    M., Andrew Y. Ng, and Michael I. Jordan. *Latent dirichlet allocation*. the Journal
    of machine Learning research 3 (2003): 993-1022\. Pritchard, Jonathan K., Matthew
    Stephens, and Peter Donnelly. *Inference of population structure using multilocus
    genotype data*. Genetics 155.2 (2000): 945-959.). Unlike the methods we looked
    at previously, where the data is represented as a set of lower dimensional matrices
    that, when multiplied, approximate the original data, LDA uses a probability model.
    This model is often explained using a plate diagram that illustrates the dependencies
    among the variables, as shown in the following diagram:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '一种将数据分解为可解释特征集的相关方法是**潜在狄利克雷分配**（**LDA**），这是一种最初为文本和基因数据开发的方法，后来已扩展到其他领域（Blei,
    David M., Andrew Y. Ng, 和 Michael I. Jordan. *潜在狄利克雷分配*. 机器学习研究杂志 3 (2003): 993-1022。Pritchard,
    Jonathan K., Matthew Stephens, 和 Peter Donnelly. *使用多座位点基因型数据推断种群结构*. 遗传学 155.2
    (2000): 945-959）。与之前我们探讨的方法不同，那些方法将数据表示为一系列低维矩阵的集合，这些矩阵相乘可以近似原始数据，LDA使用一个概率模型。这个模型通常使用一个板图来解释，该图说明了变量之间的依赖关系，如下面的图所示：'
- en: '![Latent Dirichlet Allocation](img/B04881_06_24.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![潜在狄利克雷分配](img/B04881_06_24.jpg)'
- en: 'What exactly does this diagram describe? It is what is known as a generative
    model: a set of instructions by which to generate a probability distribution over
    documents. The idea is comparable to a distribution such as the Gaussian ''bell-curve''
    you are probably familiar with, except here instead of drawing real numbers from
    the distribution we sample documents. Generative models may be contrasted with
    the predictive methods which we have seen in previous chapters that attempts to
    fit the data to a response (as in the regression or classification models we have
    studied in [Chapters 4](ch04.html "Chapter 4. Connecting the Dots with Models
    – Regression Methods"), *Connecting the Dots with Models – Regression Methods*,
    and [Chapter 5](ch05.html "Chapter 5. Putting Data in its Place – Classification
    Methods and Analysis"), *Putting Data in its Place – Classification Methods and
    Analysis*), instead of simply generate samples of the data according to a distribution.
    The plate diagram represents the components of this generative model, and we can
    think of this model as the following series of steps to generate a document:Initialize
    a Dirichlet distribution to choose from a set of topics. These topics are analogous
    to the components we found in NMF, and can be thought of as *basis documents*
    representing groups of commonly co-occurring words. The Dirichlet distribution
    is given by the following formula:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图究竟描述了什么？它被称为生成模型：一组生成文档概率分布的指令。这个想法与你可能熟悉的正态分布“钟形曲线”类似，但在这里，我们不是从分布中抽取实数，而是抽样文档。生成模型可以与我们在前几章中看到的预测方法进行对比，这些方法试图将数据拟合到响应（例如，我们在第
    4 章[Chapters 4](ch04.html "Chapter 4. Connecting the Dots with Models – Regression
    Methods")、*Connecting the Dots with Models – Regression Methods*和第 5 章[Chapter
    5](ch05.html "Chapter 5. Putting Data in its Place – Classification Methods and
    Analysis")、*Putting Data in its Place – Classification Methods and Analysis*中研究的回归或分类模型）中看到的，而不是简单地根据分布生成数据的样本。平板图表示这个生成模型的组件，我们可以将这个模型视为以下一系列步骤来生成一个文档：初始化一个狄利克雷分布来从一组主题中进行选择。这些主题类似于我们在
    NMF 中找到的组件，可以被视为代表一组共同出现的单词的*基础文档*。狄利克雷分布由以下公式给出：
- en: '![Latent Dirichlet Allocation](img/B04881_06_11.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![潜在狄利克雷分配](img/B04881_06_11.jpg)'
- en: The preceding formula gives the probability of observing a given distribution
    of items (here topics) among `K` classes and can be used to sample a vector of
    `K` class memberships (for example, sample a random vector giving what fraction
    of documents in the collection belong to a given topic). The alpha parameter in
    the Dirichlet distribution is used as an exponent of the K category probabilities
    and increases the significance ascribed to a particular component (for example,
    a more frequent topic). The term `B` is the beta function, which is simply a normalization
    term. We use the Dirichlet distribution in step 1 to generate a per-topic probability
    distribution for a document *i*. This distribution would be, for example, a series
    of weights that sum to 1 giving the relative probability that a document belongs
    to a given topic. This is the parameter θ in the plate diagram. M represents the
    number of documents in our dataset.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的公式给出了观察给定分布（此处为主题）在 `K` 个类别中的概率，并且可以用来抽样一个 `K` 个类别成员的向量（例如，抽样一个随机向量，给出集合中属于特定主题的文档比例）。狄利克雷分布中的
    alpha 参数用作 K 类别概率的指数，并增加分配给特定成分（例如，更频繁的主题）的重要性。术语 `B` 是贝塔函数，它只是一个归一化项。我们在步骤 1
    中使用狄利克雷分布来生成文档 *i* 的每个主题的概率分布。这个分布将是，例如，一系列权重，它们的和为 1，给出文档属于特定主题的相对概率。这是平板图中的参数
    θ。M 代表我们数据集中的文档数量。
- en: For each of the *N* word positions in the document, choose a topic Z from the
    distribution θ. Each of the M topics has a Dirichlet distribution with parameter
    β instead of giving per word probabilities, given by ϕ. Use this distribution
    to choose word in each N position in a document.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于文档中的每个 *N* 个单词位置，从分布 θ 中选择一个主题 Z。每个 M 个主题都有一个参数 β 的狄利克雷分布，而不是给出每个单词的概率，给出
    ϕ。使用这个分布来选择文档中每个 N 位置的单词。
- en: Repeat steps 2–4 for each word position for each document in a dataset to generate
    a group of documents.
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对数据集中的每个文档的每个单词位置重复步骤 2–4，以生成一组文档。
- en: In the previous diagram, the numbers (**M**, **N**, **K**) inside the rectangles
    indicate the number of time that the variables represented by circles are generated
    in the generative model. Thus, the words w, being innermost, are generated *N
    × M* times. You can also notice that the rectangles enclose variables that are
    generated the same number of times, while arrows indicate dependence among variables
    during this data generation process. You can also now appreciate where the name
    of this model comes from, as a document is latently allocated among many topics,
    just as we used the factors in NMF to find linear combinations of 'basis documents'
    that could reconstruct our observed data.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的图中，矩形内的数字（**M**、**N**、**K**）表示圆圈所代表的变量在生成模型中生成的次数。因此，作为最内层的单词w被生成*N × M*次。你还可以注意到，矩形包围了生成相同次数的变量，而箭头则表示在此数据生成过程中变量之间的依赖关系。你现在也可以理解这个模型名称的由来，因为文档在许多主题中潜在分配，正如我们在NMF中使用因子来找到可以重建我们观察到的数据的'基础文档'的线性组合。
- en: 'This recipe can also be used to find a set of topics (for example word probability
    distributions) that fit a dataset, assuming the model described previously was
    used to generate the documents. Without going into the full details of the derivation,
    we randomly initialize a fixed number of *K* topics and run the model, as described
    previously, by always sampling a document''s topic, given all other documents,
    and a word, given the probability of all other words in the document. We then
    update the parameters of the model based on the observed data and use the updated
    probabilities to generate the data again. Over many iterations, this process,
    known as Gibbs sampling, will converge from randomly initialized values to a set
    of model parameters that best fit the observed document data. Let us now fit an
    LDA model to the spam dataset using the following commands:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 此配方也可以用来找到一组主题（例如单词概率分布），这些主题适合数据集，假设之前描述的模型用于生成文档。不深入推导的细节，我们随机初始化一个固定的*K*个主题数量，并按照之前描述的方式运行模型，即始终采样一个文档的主题，给定所有其他文档，以及一个单词，给定文档中所有其他单词的概率。然后我们根据观察到的数据更新模型的参数，并使用更新的概率再次生成数据。经过多次迭代，这个被称为Gibbs抽样的过程将从随机初始化的值收敛到一组最佳拟合观察到的文档数据的模型参数。现在让我们使用以下命令将LDA模型拟合到垃圾邮件数据集：
- en: '[PRE32]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'As with NMF, we can examine the highest probability words for each topic using:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 与NMF一样，我们可以使用以下方法检查每个主题的最高概率单词：
- en: '[PRE33]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Likewise, we can see if these topics represent a meaningful separation between
    the spam and nonspam messages. First we find the topic distribution among the
    10 latent topics for each document using the following `transform` command:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以看到这些主题是否代表了垃圾邮件和非垃圾邮件之间的有意义分离。首先，我们使用以下`transform`命令找到每个文档在10个潜在主题中的主题分布：
- en: '[PRE34]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'This is analogous to the weights we calculated in NMF. We can now plot the
    average topic weight for each message class as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们在NMF中计算的权重类似。现在我们可以按如下方式绘制每个消息类的平均主题权重：
- en: '[PRE35]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '![Latent Dirichlet Allocation](img/B04881_06_25.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![潜在狄利克雷分配](img/B04881_06_25.jpg)'
- en: '[PRE36]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '![Latent Dirichlet Allocation](img/B04881_06_26.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![潜在狄利克雷分配](img/B04881_06_26.jpg)'
- en: Again, promisingly, we find a different average weight for topic 5 for spam
    than nonspam, indicating that the LDA model has successfully separated out the
    axis of variation we are interested in for classification purposes.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，令人鼓舞的是，我们发现对于垃圾邮件和非垃圾邮件，主题5的平均权重不同，这表明LDA模型已成功分离出我们用于分类目的感兴趣的变化轴。
- en: Using dimensionality reduction in predictive modeling
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在预测建模中使用降维
- en: The analysis we have outlined previously has been largely devoted to trying
    to extract a lower-dimensional representation of a text collection by finding
    a smaller set of components that capture the variation among individual documents.
    In some cases, this sort of analysis can be useful as an exploratory data analysis
    tool, which, like the clustering techniques we described in [Chapter 3](ch03.html
    "Chapter 3. Finding Patterns in the Noise – Clustering and Unsupervised Learning"),
    *Finding Patterns in the Noise – Clustering and Unsupervised Learning*, allows
    us to understand the structure in a dataset. We might even combine clustering
    and dimensionality reduction, which is in essence the idea of spectral clustering
    as we examined in [Chapter 3](ch03.html "Chapter 3. Finding Patterns in the Noise
    – Clustering and Unsupervised Learning"), *Finding Patterns in the Noise – Clustering
    and Unsupervised Learning* using SVD to reduce the adjacency matrix to a more
    compact representation and then clustering this reduced space to yield a cleaner
    separation between datapoints.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前概述的分析主要致力于通过找到一组较小的成分来提取文本集合的较低维表示，这些成分可以捕捉单个文档之间的变化。在某些情况下，这种分析可以作为探索性数据分析工具是有用的，就像我们在[第3章](ch03.html
    "第3章. 在噪声中寻找模式 – 聚类和无监督学习")中描述的聚类技术一样，*在噪声中寻找模式 – 聚类和无监督学习*，它使我们能够理解数据集的结构。我们甚至可以将聚类和降维结合起来，这本质上是我们考察的[第3章](ch03.html
    "第3章. 在噪声中寻找模式 – 聚类和无监督学习")中谱聚类的想法，*在噪声中寻找模式 – 聚类和无监督学习*，使用SVD将邻接矩阵减少到更紧凑的表示，然后对这一减少的空间进行聚类，以产生数据点之间更清晰的分离。
- en: Like the groups assigned through clustering, we can also potentially use the
    components derived from these dimensionality reduction methods as features in
    a predictive model. For example, the NMF components we extracted previously could
    be used as inputs to a classification model to separate spam from nonspam messages.
    We have even seen this use earlier, as the online news popularity dataset we used
    in [Chapter 4](ch04.html "Chapter 4. Connecting the Dots with Models – Regression
    Methods"), *Connecting the Dots with Models – Regression Methods*, had columns
    derived from LDA topics. Like the regularization methods we saw in [Chapter 4](ch04.html
    "Chapter 4. Connecting the Dots with Models – Regression Methods"), *Connecting
    the Dots with Models – Regression Methods*, dimensionality reduction can help
    reduce overfitting by extracting the underlying correlations among variables since
    these lower-dimensional variables are often less noisy than using the whole feature
    space. Now that we have seen how dimensionality reduction could help us find structure
    in textual data, let us examine another class of potentially high-dimensional
    data found in images.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 与通过聚类分配的组一样，我们也可以潜在地使用这些降维方法得到的成分作为预测模型中的特征。例如，我们之前提取的NMF成分可以用作分类模型的输入，以区分垃圾邮件和非垃圾邮件。我们甚至在此之前已经看到了这种用法，因为我们使用的在线新闻流行度数据集，在[第4章](ch04.html
    "第4章. 使用模型连接点 – 回归方法")中，*使用模型连接点 – 回归方法*，其列是从LDA主题中派生出来的。与我们在[第4章](ch04.html "第4章.
    使用模型连接点 – 回归方法")中看到的正则化方法一样，降维可以通过提取变量之间的潜在相关性来帮助减少过拟合，因为这些低维变量通常比使用整个特征空间更少噪声。现在我们已经看到降维如何帮助我们找到文本数据中的结构，让我们考察图像中发现的另一类可能的高维数据。
- en: Images
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像
- en: Like textual data, images are potentially noisy and complex. Furthermore, unlike
    language, which has a structure of words, paragraphs, and sentences, images have
    no predefined rules that we might use to simplify raw data. Thus, much of image
    analysis will involve extracting patterns from the input's features, which are
    ideally interpretable to a human analyst based only on the input pixels.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 与文本数据一样，图像可能存在噪声和复杂性。此外，与具有单词、段落和句子结构的语言不同，图像没有我们可以用来简化原始数据的预定义规则。因此，图像分析的大部分工作将涉及从输入特征中提取模式，这些模式理想情况下仅基于输入像素对人类分析师来说是可解释的。
- en: Cleaning image data
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 清洗图像数据
- en: 'One of the common operations we will perform on images is to enhance contrast
    or change their color scale. For example, let us start with an example image of
    a coffee cup from the `skimage` package, which you can import and visualize using
    the following commands:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在图像上执行的一种常见操作是增强对比度或改变它们的颜色范围。例如，让我们从一个来自`skimage`包的咖啡杯示例图像开始，您可以使用以下命令导入和可视化它：
- en: '[PRE37]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'This produces the following image:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下图像：
- en: '![Cleaning image data](img/B04881_06_27.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![清洗图像数据](img/B04881_06_27.jpg)'
- en: 'In Python, this image is represented as a three-dimensional matrix with the
    dimensions corresponding to height, width, and color channels. In many applications,
    the color is not of interest, and instead we are trying to determine common shapes
    or features in a set of images that may be differentiated based on grey scale
    alone. We can easily convert this image into a grey scale version using the commands:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，这个图像被表示为一个三维矩阵，其维度对应于高度、宽度和颜色通道。在许多应用中，颜色并不重要，我们试图确定一组图像中常见的形状或特征，这些图像可以根据灰度级别区分。我们可以轻松地使用以下命令将此图像转换为灰度版本：
- en: '[PRE38]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '![Cleaning image data](img/B04881_06_28.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![清洗图像数据](img/B04881_06_28.jpg)'
- en: 'A frequent task in image analysis is to identify different regions or objects
    within an image. This can be made more difficult if the pixels are clumped into
    one region (for example, if there is very strong shadow or a strong light in the
    image), rather than evenly distributed along the intensity spectrum. To identify
    different objects, it is often desirable to have these intensities evenly distributed,
    which we can do by performing histogram equalization using the following commands:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像分析中，一个常见的任务是识别图像中的不同区域或对象。如果像素聚集在一个区域（例如，如果图像中有非常强烈的阴影或强烈的光线），而不是沿着强度谱均匀分布，这可能会使任务变得更加困难。为了识别不同的对象，通常希望这些强度均匀分布，我们可以通过以下命令执行直方图均衡化来实现这一点：
- en: '[PRE39]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '![Cleaning image data](img/B04881_06_29.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![清洗图像数据](img/B04881_06_29.jpg)'
- en: 'To see the effect of this normalization, we can plot the histogram of pixels
    by intensity before and after the transformation with the command:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看这种归一化的效果，我们可以使用以下命令绘制变换前后像素强度的直方图：
- en: '[PRE40]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'This gives the following pixel distribution for the uncorrected image:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了未校正图像的以下像素分布：
- en: '![Cleaning image data](img/B04881_06_30.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![清洗图像数据](img/B04881_06_30.jpg)'
- en: 'The `ravel()` command used here is used to flatten the 2-d array we started
    with into a single vector that may be input to the histogram function. Similarly,
    we can plot the distribution of pixel intensities following normalization using:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这里使用的`ravel()`命令用于将我们开始的二维数组展平成一个可能输入到直方图函数的单个向量。同样，我们可以使用以下命令绘制归一化后像素强度的分布：
- en: '[PRE41]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '![Cleaning image data](img/B04881_06_31.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![清洗图像数据](img/B04881_06_31.jpg)'
- en: Thresholding images to highlight objects
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过阈值化图像突出显示对象
- en: 'Another common task for image analysis is to identify individual objects within
    a single image. To do so, we need to choose a threshold to binarize an image into
    white and black regions and separate overlapping objects. For the former, we can
    use thresholding algorithms such as Otsu thresholding (Otsu, Nobuyuki. *A threshold
    selection method from gray-level histograms.* Automatica 11.285-296 (1975): 23-27),
    which uses a *structuring element* (such as disk with n pixels) and attempts to
    find a pixel intensity, which will best separate pixels inside that structuring
    element into two classes (for example, black and white). We can imagine rolling
    a disk over an entire image and doing this calculation, resulting in either a
    local value within the disk or a global value that separates the image into foreground
    and background. We can then turn the image into a binary mask by thresholding
    pixels above or below this value.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '对于图像分析来说，另一个常见的任务是识别单张图像中的单个对象。为此，我们需要选择一个阈值将图像二值化为白色和黑色区域，并分离重叠的对象。对于前者，我们可以使用阈值化算法，如Otsu阈值化（Otsu,
    Nobuyuki. *从灰度直方图中选择阈值的方法.* 自动化 11.285-296 (1975): 23-27），它使用一个*结构元素*（例如n像素的圆盘）并试图找到一个像素强度，这个强度将最好地将该结构元素内的像素分为两类（例如，黑色和白色）。我们可以想象将一个圆盘在整个图像上滚动并进行这种计算，结果要么是在圆盘内的局部值，要么是一个全局值，它将图像分为前景和背景。然后我们可以通过阈值化高于或低于此值的像素将图像转换为二值掩码。'
- en: 'To illustrate, let us consider a picture of coins, where we want to separate
    the coins from their background. We can visualize the histogram-equalized coin
    image using the following commands:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明，让我们考虑一张硬币的图片，我们想要将硬币从背景中分离出来。我们可以使用以下命令可视化直方图均衡化的硬币图像：
- en: '[PRE42]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '![Thresholding images to highlight objects](img/B04881_06_32.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![通过阈值化图像突出显示对象](img/B04881_06_32.jpg)'
- en: 'One problem we can see is that the background has a gradient of illumination
    increasing toward the upper left corner of the image. This difference doesn''t
    change the distinction between background and objects (coins), but because part
    of the background is in the same intensity range as the coins, it will make it
    difficult to separate out the coins themselves. To subtract the background, we
    can use the closing function, which sequentially erodes (removes white regions
    with size less than the structuring element) and then dilates (if there is a white
    pixel within the structuring element, all elements within the structuring element
    are flipped to white). In practice, this means we remove small white specks and
    enhance regions of remaining light color. If we then subtract this from the image,
    we subtract the background, as illustrated here:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '我们可以看到的一个问题是背景有一个光照梯度，向图像的左上角增加。这种差异不会改变背景和对象（硬币）之间的区别，但由于背景的一部分与硬币具有相同的强度范围，这将使得分离硬币本身变得困难。为了减去背景，我们可以使用闭合函数，它依次侵蚀（移除小于结构元素的白色区域）然后膨胀（如果结构元素内有白色像素，结构元素内的所有元素都翻转成白色）。在实践中，这意味着我们移除小的白色斑点并增强剩余的浅色区域。如果我们然后从图像中减去这部分，就像这里所示，我们就可以减去背景：  '
- en: '[PRE43]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '![Thresholding images to highlight objects](img/B04881_06_33.jpg)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![突出显示对象的图像阈值化](img/B04881_06_33.jpg)'
- en: 'Now that we have removes the background, we can apply the Otsu thresholding
    algorithm mentioned previously to find the ideal pixel to separate the image into
    background and object using the following commands:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经移除了背景，我们可以应用之前提到的Otsu阈值化算法，使用以下命令找到理想的像素来将图像分割成背景和对象：
- en: '[PRE44]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '![Thresholding images to highlight objects](img/B04881_06_34.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![突出显示对象的图像阈值化](img/B04881_06_34.jpg)'
- en: The image has now been segmented into coins and non-coin regions. We could use
    this segmented image to count the number coins, to highlight the coins in the
    original image using the regions obtained above as a *mask*, for example if we
    want to record pixel data only from the coin regions as part of a predictive modeling
    feature using image data.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现在图像已经被分割成硬币和非硬币区域。我们可以使用这个分割后的图像来计数硬币的数量，例如，如果我们只想从硬币区域记录像素数据作为使用图像数据进行预测建模特征的一部分，我们可以使用上面获得区域作为*掩码*来突出显示原始图像中的硬币。
- en: Dimensionality reduction for image analysis
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图像分析的降维
- en: 'Once we have our images appropriately cleaned, how can we turn them into more
    general features for modeling? One approach is to try to capture common patterns
    of variation between a group of images using the same dimensionality reduction
    techniques as we used previously for document data. Instead of words in documents,
    we have patterns of pixels within an image, but otherwise the same algorithms
    and analysis largely apply. As an example, let us consider a set of images of
    faces ([http://www.geocities.ws/senthilirtt/Senthil%20Face%20Database%20Version1](http://www.geocities.ws/senthilirtt/Senthil%20Face%20Database%20Version1))
    which we can load and examine using the following commands:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们适当地清理了我们的图像，我们如何将它们转换成更通用的建模特征呢？一种方法是通过使用我们之前用于文档数据的相同降维技术来尝试捕捉一组图像之间的共同变化模式。在文档中，我们用单词表示，而在图像中，我们有像素的模式，但除此之外，相同的算法和分析在很大程度上适用。作为一个例子，让我们考虑一组人脸图像([http://www.geocities.ws/senthilirtt/Senthil%20Face%20Database%20Version1](http://www.geocities.ws/senthilirtt/Senthil%20Face%20Database%20Version1))，我们可以使用以下命令加载和检查：
- en: '[PRE45]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '![Dimensionality reduction for image analysis](img/B04881_06_35.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![图像分析的降维](img/B04881_06_35.jpg)'
- en: 'For each of these two-dimenional images, we want to convert it into a vector
    just as we did when we plotted the pixel frequency histograms during our discussion
    of normalization. We will also construct a set where the average pixel intensity
    across faces has been subtracted from each pixel, yielding each face as an offset
    from the *average face* in the data through the following commands:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这两维图像中的每一个，我们希望将其转换成一个向量，就像我们在讨论归一化时绘制像素频率直方图时做的那样。我们还将构建一个集合，其中从每个像素中减去人脸的平均像素强度，从而得到每个脸相对于数据中的*平均脸*的偏移，以下命令：
- en: '[PRE46]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We consider two possible ways to factor faces into a more general features.
    The first is to use PCA to extract the major vectors of variation in this data—these
    vectors happen to also look like faces. Since they are formed from the eigenvalues
    of the covariance matrix, these sorts of features are sometimes known as eigenfaces.
    The following commands illustrate the result of performing PCA on the face dataset:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑了两种将面孔分解为更一般特征的可能方法。第一种是使用PCA来提取这些数据中的主要变化向量——这些向量碰巧看起来也像面孔。由于它们是由协方差矩阵的特征值形成的，这类特征有时被称为eigenfaces。以下命令说明了在面部数据集上执行PCA的结果：
- en: '[PRE47]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '![Dimensionality reduction for image analysis](img/B04881_06_36.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![图像分析的降维](img/B04881_06_36.jpg)'
- en: 'How much variation in the face data is captured by the principal components?
    In contrast to the document data, we can see that using PCA even with only three
    components allows to explain around two-thirds of the variation in the dataset:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 面部数据中的多少变化可以通过主成分来捕捉？与文档数据相比，我们可以看到，即使只使用三个成分，PCA也能解释数据集中大约三分之二的变化：
- en: '[PRE48]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '![Dimensionality reduction for image analysis](img/B04881_06_37.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![图像分析的降维](img/B04881_06_37.jpg)'
- en: 'We could also apply NMF, as we described previously, to find a set of basis
    faces. You can notice from the preceding heatmap that the eigenfaces we extracted
    can have negative values, which highlights one of the interpretational difficulties
    we mentioned previously: we cannot really have negative pixels (since , so a latent
    feature with negative elements is hard to interpret. In contrast, the components
    we extract using NMF will look much more like elements of the original dataset,
    as shown below using the commands:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以像之前描述的那样应用NMF，以找到一组基面孔。你可以从前面的热图中注意到，我们提取的eigenfaces可以具有负值，这突出了我们之前提到的一个解释困难：我们实际上不可能有负像素（因为
    ，所以具有负元素的潜在特征很难解释。相比之下，我们使用NMF提取的成分将看起来更像是原始数据集的元素，如下所示，使用以下命令：
- en: '[PRE49]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '![Dimensionality reduction for image analysis](img/B04881_06_39.jpg)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![图像分析的降维](img/B04881_06_39.jpg)'
- en: 'Unlike the eigenfaces, which resemble averaged versions of many images, the
    NMF components extracted from this data look like individual faces. While we will
    not go through the exercise here, we could even apply LDA to image data to find
    topics represented by distributions of pixels and indeed it has been used for
    this purpose (Yu, Hua, and Jie Yang. *A direct LDA algorithm for high-dimensional
    data—with application to face recognition*. Pattern recognition 34.10 (2001):
    2067-2070; Thomaz, Carlos E., et al. *Using a maximum uncertainty LDA-based approach
    to classify and analyse MR brain images*. Medical Image Computing and Computer-Assisted
    Intervention–MICCAI 2004\. Springer Berlin Heidelberg, 2004\. 291-300.).'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '与类似于许多图像平均版本的eigenfaces不同，从这些数据中提取的NMF成分看起来像单个面孔。虽然我们在这里不会进行练习，但我们甚至可以将LDA应用于图像数据以找到由像素分布表示的主题，实际上它已经被用于这个目的（Yu,
    Hua, 和 Jie Yang. 《高维数据直接LDA算法——应用于人脸识别》. 模式识别 34.10 (2001): 2067-2070; Thomaz,
    Carlos E. 等. 《基于最大不确定性的LDA方法用于分类和分析MR脑图像》. 医学图像计算和计算机辅助干预——MICCAI 2004. Springer
    Berlin Heidelberg, 2004. 291-300.）。'
- en: While the dimensionality reduction techniques we have discussed previously are
    useful in the context of understanding datasets, clustering, or modeling, they
    are also potentially useful in storing compressed versions of data. Particularly
    in model services such as the one we will develop in [Chapter 8](ch08.html "Chapter 8. Sharing
    Models with Prediction Services"), *Sharing Models with Prediction Services*,
    being able to store a smaller version of the data can reduce system load and provide
    an easier way to process incoming data into a form that can be understood by a
    predictive model. We can quickly extract the few components we need, for example,
    from a new piece of text data, without having to persist the entire record.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们之前讨论的降维技术在理解数据集、聚类或建模的上下文中很有用，但它们在存储数据的压缩版本方面也可能很有用。特别是在我们将在第8章[Chapter
    8](ch08.html "Chapter 8. Sharing Models with Prediction Services")中开发的模型服务中，能够存储数据的小版本可以减少系统负载，并提供一种更容易将传入数据处理成预测模型可以理解的形式的方法。例如，我们可以快速提取所需的少量成分，例如，从新的文本数据中，而不必持久化整个记录。
- en: 'Case Study: Training a Recommender System in PySpark'
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究：在PySpark中训练推荐系统
- en: 'To close this chapter, let us look at an example of how we might generate a
    large-scale recommendation system using dimensionality reduction. The dataset
    we will work with comes from a set of user transactions from an online store (Chen,
    Daqing, Sai Laing Sain, and Kun Guo. *Data mining for the online retail industry:
    A case study of RFM model-based customer segmentation using data mining*. Journal
    of Database Marketing & Customer Strategy Management 19.3 (2012): 197-208). In
    this model, we will input a matrix in which the rows are users and the columns
    represent items in the catalog of an e-commerce site. Items purchased by a user
    are indicated by a 1\. Our goal is to factorize this matrix into 1 x k *user factors*
    (row components) and k x 1 *item factors* (column components) using k components.
    Then, presented with a new user and their purchase history, we can predict what
    items they are like to buy in the future, and thus what we might recommend to
    them on a homepage. The steps to do so are as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '为了结束这一章，让我们看看如何使用降维技术生成一个大规模推荐系统的示例。我们将使用的数据集来自一家在线商店的用户交易记录（Chen, Daqing,
    Sai Laing Sain, 和 Kun Guo. *数据挖掘在在线零售行业中的应用：基于数据挖掘的 RFM 模型客户细分案例研究*. 数据库营销与客户战略管理杂志
    19.3 (2012): 197-208）。在这个模型中，我们将输入一个矩阵，其中行表示用户，列代表电子商务网站目录中的项目。用户购买的项目用 1 表示。我们的目标是使用
    k 个组件将这个矩阵分解为 1 x k 的 *用户因素*（行成分）和 k x 1 的 *项目因素*（列成分）。然后，面对一个新用户及其购买历史，我们可以预测他们未来可能购买的项目，从而在主页上向他们推荐可能的产品。这样做的方法如下：'
- en: 'Consider a user''s prior purchase history as *a* vector *p*. We imagine this
    vector is the product of an unknown *user factor* component *u* with the item
    factors *i* we obtained through matrix factorization: each element of the vector
    *p* is then the dot product of this unknown user factor with the item factor for
    a given item. Solve for the unknown user factor *u* in the equation:![Case Study:
    Training a Recommender System in PySpark](img/B04881_06_12.jpg)'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将用户的先前购买历史视为 *一个* 向量 *p*。我们想象这个向量是未知的 *用户因素* 成分 *u* 与我们通过矩阵分解获得的项目因素 *i* 的乘积：向量
    *p* 的每个元素然后是未知用户因素与给定项目因素的点积。在方程中求解未知用户因素 *u*：![案例研究：在 PySpark 中训练推荐系统](img/B04881_06_12.jpg)
- en: Given the item factors *i* and the purchase history *p*, using matrix.Use the
    resulting user factor *u*, take the dot product with each item factor to obtain
    and sort by the result to determine a list of the top ranked items.
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 考虑到项目因素 *i* 和购买历史 *p*，使用矩阵。使用得到的用户因素 *u*，与每个项目因素进行点积运算以获得排序结果，从而确定排名靠前的项目列表。
- en: 'Now that we have described what is happening *under the hood* in this example,
    we can begin to parse this data using the following commands. First, we create
    a parsing function to read the 2nd and 7th columns of the data containing the
    item ID and user ID, respectively:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经描述了在这个例子中“幕后”发生的事情，我们可以开始使用以下命令解析这些数据。首先，我们创建一个解析函数来读取包含项目 ID 和用户 ID 的第
    2 列和第 7 列的数据：
- en: '[PRE50]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Next, we read in the file and convert the user and item IDs, which are both
    string, into a numerical index by incrementing a counter as we add unique items
    to a dictionary:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们读取文件并将用户和项目 ID（都是字符串）转换为数值索引，通过在向字典添加唯一项目时递增计数器来完成：
- en: '[PRE51]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Next, we convert the resulting array of purchase into an `rdd` and convert the
    resulting entries into Rating objects -- a (user, item, rating) tuple. Here, we
    will just indicate that the purchase occurred by giving a rating of 1.0 to all
    observed purchases, but we could just as well have a system where the ratings
    indicate user preference (such as movie ratings) and follow a numerical scale.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将得到的购买数组转换为 `rdd`，并将结果条目转换为 Rating 对象——一个 (用户, 项目, 评分) 元组。在这里，我们将仅通过给所有观察到的购买项目分配
    1.0 的评分来表示购买的发生，但也可以有一个系统，其中评分表示用户偏好（如电影评分）并遵循数值尺度。
- en: '[PRE52]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Now we can fit the matrix factorization model using the following commands:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用以下命令拟合矩阵分解模型：
- en: '[PRE53]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: The algorithm for matrix factorization used in PySpark is **Alternating Least
    Squares** (**ALS**), which has parameters for the number of row (column) components
    chosen (`k`) and a regularization parameter λ which we did not specify here, but
    functions similarly to its role in the regression algorithms we studied in [Chapter
    4](ch04.html "Chapter 4. Connecting the Dots with Models – Regression Methods"),
    *Connecting the Dots with Models – Regression Methods*, by constraining the values
    in the row (column) vectors from becoming too large and potentially causing overfitting.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark 中使用的矩阵分解算法是**交替最小二乘法**（**ALS**），它具有选择行（列）组件数量（`k`）的参数和一个正则化参数 λ，我们在这里没有指定，但它与我们在第
    4 章[“通过模型连接点 – 回归方法”](ch04.html "Chapter 4. Connecting the Dots with Models –
    Regression Methods")中研究的回归算法中的角色类似，*通过模型连接点 – 回归方法*，通过限制行（列）向量中的值不会变得过大，从而可能引起过拟合。
- en: We could try several values of k and λ, and measure the mean squared error between
    the observed and predicted matrix (from multiplying the row factors by the column
    factors) to determine the optimal values.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以尝试几个 k 和 λ 的值，并测量观测值和预测矩阵（通过将行因子乘以列因子）之间的均方误差，以确定最佳值。
- en: Once we have obtained a good fit, we can use the `predict` and `predictAll`
    methods of the model object to obtain predictions for new users, and the persist
    it on disk using the `save` method.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们获得了一个良好的拟合，我们就可以使用模型对象的 `predict` 和 `predictAll` 方法来获取对新用户的预测，并使用 `save`
    方法将其持久化到磁盘上。
- en: Summary
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have examined complex, unstructured data. We cleaned and
    tokenized text and examined several ways of extracting features from documents
    in a way that could be incorporated into predictive models such as n-grams and
    tf-idf scores. We also examined dimensionality reduction techniques, such as the
    HashingVectorizer, matrix decompositions, such as PCA, CUR, NMF, and probabilistic
    models, such as LDA. We also examined image data, including normalization and
    thresholding operations, and how we can use dimensionality reduction techniques
    to find common patterns among images. Finally, we used a matrix factorization
    algorithm to prototype a recommender system in PySpark.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们检查了复杂、非结构化的数据。我们清理和标记了文本，并检查了将文档特征提取到可以纳入预测模型（如 n-gram 和 tf-idf 分数）的几种方法。我们还检查了降维技术，如
    HashingVectorizer，矩阵分解，如 PCA、CUR、NMF，以及概率模型，如 LDA。我们还检查了图像数据，包括归一化和阈值操作，以及我们如何使用降维技术来找到图像之间的共同模式。最后，我们使用矩阵分解算法在
    PySpark 中原型化了一个推荐系统。
- en: 'In the next section, you will also look at image data, but in a different context:
    trying to capture complex features from these data using sophisticated deep learning
    models.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，你还将查看图像数据，但处于不同的背景：尝试使用复杂的深度学习模型从这些数据中捕获复杂特征。
