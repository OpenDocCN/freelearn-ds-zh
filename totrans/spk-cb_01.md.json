["```py\n    $ wget http://d3kbcqa49mib13.cloudfront.net/spark-1.4.0-bin-hadoop2.4.tgz\n\n    ```", "```py\n    $ tar -zxf spark-1.4.0-bin-hadoop2.4.tgz\n\n    ```", "```py\n    $ sudo mv spark-1.4.0-bin-hadoop2.4 spark\n\n    ```", "```py\n    $ sudo mv spark/conf/* /etc/spark\n\n    ```", "```py\n    $ sudo mkdir -p /opt/infoobjects\n\n    ```", "```py\n    $ sudo mv spark /opt/infoobjects/\n\n    ```", "```py\n    $ sudo chown -R root:root /opt/infoobjects/spark\n\n    ```", "```py\n    $ sudo chmod -R 755 /opt/infoobjects/spark\n\n    ```", "```py\n    $ cd /opt/infoobjects/spark\n\n    ```", "```py\n    $ sudo ln -s /etc/spark conf\n\n    ```", "```py\n    $ echo \"export PATH=$PATH:/opt/infoobjects/spark/bin\" >> /home/hduser/.bashrc\n\n    ```", "```py\n    $ sudo mkdir -p /var/log/spark\n\n    ```", "```py\n    $ sudo chown -R hduser:hduser /var/log/spark\n\n    ```", "```py\n    $ mkdir /tmp/spark\n\n    ```", "```py\n    $ cd /etc/spark\n    $ echo \"export HADOOP_CONF_DIR=/opt/infoobjects/hadoop/etc/hadoop\" >> spark-env.sh\n    $ echo \"export YARN_CONF_DIR=/opt/infoobjects/hadoop/etc/Hadoop\" >> spark-env.sh\n    $ echo \"export SPARK_LOG_DIR=/var/log/spark\" >> spark-env.sh\n    $ echo \"export SPARK_WORKER_DIR=/tmp/spark\" >> spark-env.sh\n\n    ```", "```py\n    $ echo \"export _JAVA_OPTIONS=\\\"-XX:MaxPermSize=1G\\\"\"  >> /home/hduser/.bashrc\n\n    ```", "```py\n    $ wget https://github.com/apache/spark/archive/branch-1.4.zip\n\n    ```", "```py\n    $ gunzip branch-1.4.zip\n\n    ```", "```py\n    $ cd spark\n\n    ```", "```py\n    $ mvn -Pyarn -Phadoop-2.4 -Dhadoop.version=2.4.0 -Phive -DskipTests clean package\n\n    ```", "```py\n    $ sudo mv spark/conf /etc/\n\n    ```", "```py\n    $ sudo mv spark /opt/infoobjects/spark\n\n    ```", "```py\n    $ sudo chown -R root:root /opt/infoobjects/spark\n\n    ```", "```py\n    $ sudo chmod -R 755 /opt/infoobjects/spark\n\n    ```", "```py\n    $ cd /opt/infoobjects/spark\n\n    ```", "```py\n    $ sudo ln -s /etc/spark conf\n\n    ```", "```py\n    $ echo \"export PATH=$PATH:/opt/infoobjects/spark/bin\" >> /home/hduser/.bashrc\n\n    ```", "```py\n    $ sudo mkdir -p /var/log/spark\n\n    ```", "```py\n    $ sudo chown -R hduser:hduser /var/log/spark\n\n    ```", "```py\n    $ mkdir /tmp/spark\n\n    ```", "```py\n    $ cd /etc/spark\n    $ echo \"export HADOOP_CONF_DIR=/opt/infoobjects/hadoop/etc/hadoop\" >> spark-env.sh\n    $ echo \"export YARN_CONF_DIR=/opt/infoobjects/hadoop/etc/Hadoop\" >> spark-env.sh\n    $ echo \"export SPARK_LOG_DIR=/var/log/spark\" >> spark-env.sh\n    $ echo \"export SPARK_WORKER_DIR=/tmp/spark\" >> spark-env.sh\n\n    ```", "```py\n    $ echo \"export AWS_ACCESS_KEY_ID=\\\"AKIAOD7M2LOWATFXFKQ\\\"\" >> /home/hduser/.bashrc\n    $ echo \"export AWS_SECRET_ACCESS_KEY=\\\"+Xr4UroVYJxiLiY8DLT4DLT4D4sxc3ijZGMx1D3pfZ2q\\\"\" >> /home/hduser/.bashrc\n    $ echo \"export PATH=$PATH:/opt/infoobjects/spark/ec2\" >> /home/hduser/.bashrc\n\n    ```", "```py\n    $ cd /home/hduser\n    $ spark-ec2 -k <key-pair> -i <key-file> -s <num-slaves> launch <cluster-name>\n\n    ```", "```py\n    $ spark-ec2 -k kp-spark -i /home/hduser/keypairs/kp-spark.pem --hadoop-major-version 2  -s 3 launch spark-cluster\n\n    ```", "```py\n    $ spark-ec2 -k kp-spark -i /home/hduser/keypairs/kp-spark.pem -z us-east-1b --hadoop-major-version 2  -s 3 launch spark-cluster\n\n    ```", "```py\n    $ spark-ec2 -k kp-spark -i /home/hduser/keypairs/kp-spark.pem --hadoop-major-version 2 -ebs-vol-size 10 -s 3 launch spark-cluster\n\n    ```", "```py\n    $ spark-ec2 -k kp-spark -i /home/hduser/keypairs/kp-spark.pem -spot-price=0.15 --hadoop-major-version 2  -s 3 launch spark-cluster\n\n    ```", "```py\n    $ spark-ec2 -k kp-spark -i /home/hduser/kp/kp-spark.pem  login spark-cluster\n\n    ```", "```py\n    $ ephemeral-hdfs/bin/hadoop version\n    Hadoop 2.0.0-chd4.2.0\n\n    ```", "```py\n    $ persistent-hdfs/bin/hadoop version\n    Hadoop 2.0.0-chd4.2.0\n\n    ```", "```py\n    $ cd spark/conf\n\n    ```", "```py\n        $ mv log4j.properties.template log4j.properties\n\n        ```", "```py\n        $ vi log4j.properties\n\n        ```", "```py\n    $ spark-ec2/copydir spark/conf\n\n    ```", "```py\n    $ spark-ec2 destroy spark-cluster\n\n    ```", "```py\nMaster\nm1.zettabytes.com\nSlaves\ns1.zettabytes.com\ns2.zettabytes.com\ns3.zettabytes.com\ns4.zettabytes.com\ns5.zettabytes.com\n\n```", "```py\n    $ echo \"export PATH=$PATH:/opt/infoobjects/spark/sbin\" >> /home/hduser/.bashrc\n\n    ```", "```py\n    hduser@m1.zettabytes.com~] start-master.sh\n\n    ```", "```py\n    hduser@s1.zettabytes.com~] spark-class org.apache.spark.deploy.worker.Worker spark://m1.zettabytes.com:7077\n\n    ```", "```py\n    hduser@m1.zettabytes.com~] echo \"s1.zettabytes.com\" >> conf/slaves\n    hduser@m1.zettabytes.com~] echo \"s2.zettabytes.com\" >> conf/slaves\n    hduser@m1.zettabytes.com~] echo \"s3.zettabytes.com\" >> conf/slaves\n    hduser@m1.zettabytes.com~] echo \"s4.zettabytes.com\" >> conf/slaves\n    hduser@m1.zettabytes.com~] echo \"s5.zettabytes.com\" >> conf/slaves\n\n    ```", "```py\n    val sparkContext = new SparkContext(new SparkConf().setMaster(\"spark://m1.zettabytes.com:7077\")\n\n    ```", "```py\n    $ spark-shell --master spark://master:7077\n\n    ```", "```py\n$ echo \"export SPARK_DAEMON_MEMORY=1g\" >> /opt/infoobjects/spark/conf/spark-env.sh\n\n```", "```py\n$ echo \"export SPARK_WORKER_INSTANCES=2\" >> /opt/infoobjects/spark/conf/spark-env.sh\n\n```", "```py\n$ echo \"export SPARK_WORKER_CORES=12\" >> /opt/infoobjects/spark/conf/spark-env.sh\n\n```", "```py\n$ echo \"export SPARK_WORKER_MEMORY=24g\" >> /opt/infoobjects/spark/conf/spark-env.sh\n\n```", "```py\n    $ spark-submit --conf spark.cores.max=12\n\n    ```", "```py\n    $ spark-submit --conf spark.executor.memory=8g\n\n    ```", "```py\n    $ sudo apt-key adv --keyserver keyserver.ubuntu.com --recv E56151BF DISTRO=$(lsb_release -is | tr '[:upper:]' '[:lower:]') CODENAME=$(lsb_release -cs)\n    $ sudo vi /etc/apt/sources.list.d/mesosphere.list\n\n    deb http://repos.mesosphere.io/Ubuntu trusty main\n\n    ```", "```py\n    $ sudo apt-get -y update\n\n    ```", "```py\n    $ sudo apt-get -y install mesos\n\n    ```", "```py\n    $ \n    hdfs dfs\n     -put spark-1.4.0-bin-hadoop2.4.tgz spark-1.4.0-bin-hadoop2.4.tgz\n\n    ```", "```py\n    $ sudo vi spark-env.sh\n    export MESOS_NATIVE_LIBRARY=/usr/local/lib/libmesos.so\n    export SPARK_EXECUTOR_URI= hdfs://localhost:9000/user/hduser/spark-1.4.0-bin-hadoop2.4.tgz\n\n    ```", "```py\n    val conf = new SparkConf().setMaster(\"mesos://host:5050\")\n    val sparkContext = new SparkContext(conf)\n\n    ```", "```py\n    $ spark-shell --master mesos://host:5050\n\n    ```", "```py\n    conf.set(\"spark.mesos.coarse\",\"true\")\n\n    ```", "```py\n    HADOOP_CONF_DIR: to write to HDFS\n    YARN_CONF_DIR: to connect to YARN ResourceManager\n    $ cd /opt/infoobjects/spark/conf (or /etc/spark)\n    $ sudo vi spark-env.sh\n    export HADOOP_CONF_DIR=/opt/infoobjects/hadoop/etc/Hadoop\n    export YARN_CONF_DIR=/opt/infoobjects/hadoop/etc/hadoop\n\n    ```", "```py\n    $ spark-submit --class path.to.your.Class --master yarn-client [options] <app jar> [app options]\n\n    ```", "```py\n    $ spark-submit --class com.infoobjects.TwitterFireHose --master yarn-client --num-executors 3 --driver-memory 4g --executor-memory 2g --executor-cores 1 target/sparkio.jar 10\n\n    ```", "```py\n    $ spark-shell --master yarn-client\n\n    ```", "```py\n    $ spark-submit --class path.to.your.Class --master yarn-cluster [options] <app jar> [app options]\n\n    ```", "```py\n    $ spark-submit --class com.infoobjects.TwitterFireHose --master yarn-cluster --num-executors 3 --driver-memory 4g --executor-memory 2g --executor-cores 1 targe\n    t/sparkio.jar 10\n\n    ```", "```py\n    $ wget https://github.com/amplab/tachyon/archive/v<version>.zip\n\n    ```", "```py\n    $ unzip  v-<version>.zip\n\n    ```", "```py\n    $ mv tachyon-<version> tachyon\n\n    ```", "```py\n    $ cd tachyon\n    $ mvn -Dhadoop.version=2.4.0 clean package -DskipTests=true\n    $ cd conf\n    $ sudo mkdir -p /var/tachyon/journal\n    $ sudo chown -R hduser:hduser /var/tachyon/journal\n    $ sudo mkdir -p /var/tachyon/ramdisk\n    $ sudo chown -R hduser:hduser /var/tachyon/ramdisk\n\n    $ mv tachyon-env.sh.template tachyon-env.sh\n    $ vi tachyon-env.sh\n\n    ```", "```py\n    export TACHYON_UNDERFS_ADDRESS=$TACHYON_HOME/underfs\n\n    ```", "```py\n    export TACHYON_UNDERFS_ADDRESS=hdfs://localhost:9000\n\n    ```", "```py\n    -Dtachyon.master.journal.folder=/var/tachyon/journal/\n\n    export TACHYON_RAM_FOLDER=/var/tachyon/ramdisk\n\n    $ sudo mkdir -p /var/log/tachyon\n    $ sudo chown -R hduser:hduser /var/log/tachyon\n    $ vi log4j.properties\n\n    ```", "```py\n    $ sudo vi core-site.xml\n    <configuration>\n    <property>\n     <name>fs.tachyon.impl</name>\n     <value>tachyon.hadoop.TFS</value>\n     </property>\n    </configuration>\n    $ cd ~\n    $ sudo mv tachyon /opt/infoobjects/\n    $ sudo chown -R root:root /opt/infoobjects/tachyon\n    $ sudo chmod -R 755 /opt/infoobjects/tachyon\n\n    ```", "```py\n    $ echo \"export PATH=$PATH:/opt/infoobjects/tachyon/bin\" >> /home/hduser/.bashrc\n\n    ```", "```py\n    $ tachyon format\n    $ tachyon-start.sh local //you need to enter root password as RamFS needs to be formatted\n\n    ```", "```py\n    $ tachyon runTest Basic CACHE_THROUGH\n\n    ```", "```py\n    $ tachyon-stop.sh\n\n    ```", "```py\n    $ spark-shell\n    scala> val words = sc.textFile(\"tachyon://localhost:19998/words\")\n    scala> words.count\n    scala> words.saveAsTextFile(\"tachyon://localhost:19998/w2\")\n    scala> val person = sc.textFile(\"hdfs://localhost:9000/user/hduser/person\")\n    scala> import org.apache.spark.api.java._\n    scala> person.persist(StorageLevels.OFF_HEAP)\n\n    ```"]