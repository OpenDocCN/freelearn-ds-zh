<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Structures That Work</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will cover:</p>
<ul>
<li>Using geospatial views</li>
<li>Using triggers to populate the geometry column</li>
<li>Structuring spatial data with table inheritance</li>
<li>Extending inheritance â€“ table partitioning</li>
<li>Normalizing imports</li>
<li>Normalizing internal overlays</li>
<li>Using polygon overlays for proportional census estimates</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction</h1>
                </header>
            
            <article>
                
<p>This chapter focuses on ways to structure data using the functionality provided by the combination of PostgreSQL and PostGIS. These will be useful approaches for structuring and cleaning up imported data, converting tabular data into spatial data <em>on the fly</em> when it is entered, and maintaining relationships between tables and datasets using functionality endemic to the powerful combination of PostgreSQL and PostGIS. There are three categories of techniques with which we will leverage these functionalities: automatic population and modification of data using views and triggers, object orientation using PostgreSQL table inheritance, and using PostGIS functions (stored procedures) to reconstruct and normalize problematic data.</p>
<p>Automatic population of data is where the chapter begins. By leveraging PostgreSQL views and triggers, we can create ad hoc and flexible solutions to create connections between and within the tables. By extension, and for more formal or structured cases, PostgreSQL provides table inheritance and table partitioning, which allow for explicit hierarchical relationships between tables. This can be useful in cases where an object inheritance model enforces data relationships that either represent the data better, thereby resulting in greater efficiencies, or reduce the administrative overhead of maintaining and accessing the datasets over time. With PostGIS extending that functionality, the inheritance can apply not just to the commonly used table attributes, but to leveraging spatial relationships between tables, resulting in greater query efficiency with very large datasets. Finally, we will explore PostGIS SQL patterns that provide table normalization of data inputs, so datasets that come from flat filesystems or are not normalized can be converted to a form we would expect in a database.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using geospatial views</h1>
                </header>
            
            <article>
                
<p>Views in PostgreSQL allow the ad hoc representation of data and data relationships in alternate forms. In this recipe, we'll be using views to allow for the automatic creation of point data based on tabular inputs. We can imagine a case where the input stream of data is non-spatial, but includes longitude and latitude or some other coordinates. We would like to automatically show this data as points in space.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>We can create a view as a representation of spatial data pretty easily. The syntax for creating a view is similar to creating a table, for example:</p>
<pre><strong>CREATE VIEW viewname AS 
  SELECT...</strong> </pre>
<p>In the preceding command line, our <kbd>SELECT</kbd> query manipulates the data for us. Let's start with a small dataset. In this case, we will start with some random points, which could be real data.</p>
<p>First, we create the table from which the view will be constructed, as follows:</p>
<pre><strong>-- Drop the table in case it exists 
DROP TABLE IF EXISTS chp02.xwhyzed CASCADE;  
CREATE TABLE chp02.xwhyzed 
-- This table will contain numeric x, y, and z values 
( 
  x numeric, 
  y numeric, 
  z numeric 
) 
WITH (OIDS=FALSE); 
ALTER TABLE chp02.xwhyzed OWNER TO me; 
-- We will be disciplined and ensure we have a primary key 
ALTER TABLE chp02.xwhyzed ADD COLUMN gid serial; 
ALTER TABLE chp02.xwhyzed ADD PRIMARY KEY (gid);</strong> </pre>
<p>Now, let's populate this with the data for testing using the following query:</p>
<pre><strong>INSERT INTO chp02.xwhyzed (x, y, z) 
  VALUES (random()*5, random()*7, random()*106); 
INSERT INTO chp02.xwhyzed (x, y, z) 
  VALUES (random()*5, random()*7, random()*106); 
INSERT INTO chp02.xwhyzed (x, y, z) 
  VALUES (random()*5, random()*7, random()*106); 
INSERT INTO chp02.xwhyzed (x, y, z) 
  VALUES (random()*5, random()*7, random()*106);</strong> </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Now, to create the view, we will use the following query:</p>
<pre><strong>-- Ensure we don't try to duplicate the view</strong>
<strong>DROP VIEW IF EXISTS chp02.xbecausezed;</strong>
<strong>-- Retain original attributes, but also create a point   attribute from x and y</strong>
<strong>CREATE VIEW chp02.xbecausezed AS</strong>
<strong>SELECT x, y, z, ST_MakePoint(x,y)</strong>
<strong>FROM chp02.xwhyzed;</strong>
  </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Our view is really a simple transformation of the existing data using PostGIS's <kbd>ST_MakePoint</kbd> function. The <kbd>ST_MakePoint</kbd> function takes the input of two numbers to create a PostGIS point, and in this case our view simply uses our <em>x</em> and <em>y</em> values to populate the data. Any time there is an update to the table to add a new record with <em>x</em> and <em>y</em> values, the view will populate a point, which is really useful for data that is constantly being updated.</p>
<p>There are two disadvantages to this approach. The first is that we have not declared our spatial reference system in the view, so any software consuming these points will not know the coordinate system we are using, that is, whether it is a geographic (latitude/longitude) or a planar coordinate system. We will address this problem shortly. The second problem is that many software systems accessing these points may not automatically detect and use the spatial information from the table. This problem is addressed in the <em>Using triggers to populate the geometry column</em> recipe.</p>
<div class="packt_infobox">The <strong>spatial reference system identifier</strong> (<strong>SRID</strong>) allows us to specify the coordinate system for a given dataset. The numbering system is a simple integer value to specify a given coordinate system. SRIDs are derived originally from the <strong>European Petroleum Survey Group</strong> (<strong>EPSG</strong>) and are now maintained by the Surveying and Positioning Committee of the International Association of <strong>Oil and Gas Producers</strong> (<strong>OGP</strong>). Useful tools for SRIDs are spatial reference (<a href="http://spatialreference.org"><span class="URLPACKT">http://spatialreference.org</span></a>) and Prj2EPSG (<a href="http://prj2epsg.org/search"><span class="URLPACKT">http://prj2epsg.org/search</span></a>).</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>To address the first problem mentioned in the <em>How it works...</em> section, we can simply wrap our existing <kbd>ST_MakePoint</kbd> function in another function specifying the SRID as <kbd>ST_SetSRID</kbd>, as shown in the following query:</p>
<pre><strong>-- Ensure we don't try to duplicate the view 
DROP VIEW IF EXISTS chp02.xbecausezed; 
-- Retain original attributes, but also create a point   attribute from x and y 
CREATE VIEW chp02.xbecausezed AS 
  SELECT x, y, z, ST_SetSRID(ST_MakePoint(x,y), 3734) -- Add ST_SetSRID 
  FROM chp02.xwhyzed;</strong> </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>The <em>Using triggers to populate the geometry column</em> recipe</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using triggers to populate the geometry column</h1>
                </header>
            
            <article>
                
<p>In this recipe, we imagine that we have ever increasing data in our database, which needs spatial representation; however, in this case we want a hardcoded geometry column to be updated each time an insertion happens on the database, converting our <em>x</em> and <em>y</em> values to geometry as and when they are inserted into the database.</p>
<p>The advantage of this approach is that the geometry is then registered in the <kbd>geometry_columns</kbd> view, and therefore this approach works reliably with more PostGIS client types than creating a new geospatial view. This also provides the advantage of allowing for a spatial index that can significantly speed up a variety of queries.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>We will start by creating another table of random points with <kbd>x</kbd>, <kbd>y</kbd>, and <kbd>z</kbd> values, as shown in the following query:</p>
<pre><strong>DROP TABLE IF EXISTS chp02.xwhyzed1 CASCADE; 
CREATE TABLE chp02.xwhyzed1 
( 
  x numeric, 
  y numeric, 
  z numeric 
) 
WITH (OIDS=FALSE); 
ALTER TABLE chp02.xwhyzed1 OWNER TO me; 
ALTER TABLE chp02.xwhyzed1 ADD COLUMN gid serial; 
ALTER TABLE chp02.xwhyzed1 ADD PRIMARY KEY (gid); 
 
INSERT INTO chp02.xwhyzed1 (x, y, z) 
  VALUES (random()*5, random()*7, random()*106); 
INSERT INTO chp02.xwhyzed1 (x, y, z) 
  VALUES (random()*5, random()*7, random()*106); 
INSERT INTO chp02.xwhyzed1 (x, y, z) 
  VALUES (random()*5, random()*7, random()*106); 
INSERT INTO chp02.xwhyzed1 (x, y, z) 
  VALUES (random()*5, random()*7, random()*106); </strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Now we need a geometry column to populate. By default, the geometry column will be populated with null values. We populate a geometry column using the following query:</p>
<pre><strong>SELECT AddGeometryColumn ('chp02','xwhyzed1','geom',3734,'POINT',2);</strong> </pre>
<p>We now have a column called <kbd>geom</kbd> with an SRID of <kbd>3734</kbd>; that is, a point geometry type in two dimensions. Since we have <kbd>x</kbd>, <kbd>y</kbd>, and <kbd>z</kbd> data, we could, in principle, populate a 3D point table using a similar approach.</p>
<p>Since all the geometry values are currently null, we will populate them using an <kbd>UPDATE</kbd> statement as follows:</p>
<pre><strong>UPDATE chp02.xwhyzed1 
  SET the_geom = ST_SetSRID(ST_MakePoint(x,y), 3734);</strong> </pre>
<p>The query here is simple when broken down. We update the <kbd>xwhyzed1</kbd>Â table and set the <kbd>the_geom</kbd> column using <kbd>ST_MakePoint</kbd>, construct our point using the <kbd>x</kbd> and <kbd>y</kbd> columns, and wrap it in an <kbd>ST_SetSRID</kbd> function in order to apply the appropriate spatial reference information. So far, we have just set the table up. Now, we need to create a trigger in order to continue to populate this information once the table is in use. The first part of the trigger is a new populated geometry function using the following query:</p>
<pre><strong>CREATE OR REPLACE FUNCTION chp02.before_insertXYZ() 
  RETURNS trigger AS 
$$ 
BEGIN 
 
if NEW.geom is null then 
   NEW.geom = ST_SetSRID(ST_MakePoint(NEW.x,NEW.y), 3734); 
end if; 
RETURN NEW; 
END; 
 
$$ 
LANGUAGE 'plpgsql';</strong> </pre>
<p>In essence, we have created a function that does exactly what we did manually: update the table's geometry column with the combination of <kbd>ST_SetSRID</kbd> and <kbd>ST_MakePoint</kbd>, but only to the new registers being inserted, and not to all the table.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>While we have a function created, we have not yet applied it as a trigger to the table. Let us do that here as follows:</p>
<pre><strong>CREATE TRIGGER popgeom_insert</strong><br/><strong>  BEFORE INSERT ON chp02.xwhyzed1</strong><br/><strong>  FOR EACH ROW EXECUTE PROCEDURE chp02.before_insertXYZ();</strong></pre>
<p>Let's assume that the general geometry column update has not taken place yet, then the original five registers still have their geometry column in <kbd>null</kbd>. Now, once the trigger has been activated, any inserts into our table should be populated with new geometry records. Let us do a test insert using the following query:</p>
<pre><strong>INSERT INTO chp02.xwhyzed1 (x, y, z)</strong><br/><strong>  VALUES (random()*5, random()*7, 106),</strong><br/><strong>  (random()*5, random()*7, 107),</strong><br/><strong>  (random()*5, random()*7, 108),</strong><br/><strong>  (random()*5, random()*7, 109),</strong><br/><strong>  (random()*5, random()*7, 110);</strong></pre>
<p>Check the rows to verify that the <kbd>geom</kbd> columns are updated with the command:</p>
<pre><strong>SELECT * FROM chp02.xwhyzed1;</strong></pre>
<p>Or use <kbd>pgAdmin</kbd>:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/228a3c05-cbc1-4e11-b229-dcf0370c8e88.png"/></div>
<p class="packt_figure">After applying the general update, then all the registers will have a value on their <kbd>geom</kbd> column:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/36e423c6-31fa-4611-9106-9608a6658827.png"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Extending further...</h1>
                </header>
            
            <article>
                
<p>So far, we've implemented an <kbd>insert</kbd> trigger. What if the value changes for a particular row? In that case, we will require a separate update trigger. We'll change our original function to test the <kbd>UPDATE</kbd> case, and we'll use <kbd>WHEN</kbd> in our trigger to constrain updates to the column being changed.</p>
<p>Also, note that the following function is written with the assumption that the user wants to always update the changing geometries based on the changing values:</p>
<pre><strong>CREATE OR REPLACE FUNCTION chp02.before_insertXYZ() 
  RETURNS trigger AS 
$$ 
BEGIN 
if (TG_OP='INSERT') then 
  if (NEW.geom is null) then 
    NEW.geom = ST_SetSRID(ST_MakePoint(NEW.x,NEW.y), 3734); 
  end if; 
ELSEIF (TG_OP='UPDATE') then 
  NEW.geom = ST_SetSRID(ST_MakePoint(NEW.x,NEW.y), 3734);<br/>end if;</strong></pre>
<pre><strong>RETURN NEW; 
END; 
 
$$ 
LANGUAGE 'plpgsql'; 
 
CREATE TRIGGER popgeom_insert 
  BEFORE INSERT ON chp02.xwhyzed1 
  FOR EACH ROW EXECUTE PROCEDURE chp02.before_insertXYZ(); 
 
CREATE trigger popgeom_update 
  BEFORE UPDATE ON chp02.xwhyzed1 
  FOR EACH ROW  
  WHEN (OLD.X IS DISTINCT FROM NEW.X OR OLD.Y IS DISTINCT FROM  
    NEW.Y) 
  EXECUTE PROCEDURE chp02.before_insertXYZ();</strong> </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li>The <em>Using geospatial views</em> recipe</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Structuring spatial data with table inheritance</h1>
                </header>
            
            <article>
                
<p>An unusual and useful property of the PostgreSQL database is that it allows for object inheritance models as they apply to tables. This means that we can have parent/child relationships between tables and leverage that to structure the data in meaningful ways. In our example, we will apply this to hydrology data. This data can be points, lines, polygons, or more complex structures, but they have one commonality: they are explicitly linked in a physical sense and inherently related; they are all about water. Water/hydrology is an excellent natural system to model this way, as our ways of modeling it spatially can be quite mixed depending on scales, details, the data collection process, and a host of other factors.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>The data we will be using is hydrology data that has been modified from engineering <em>blue lines</em> (see the following screenshot), that is, hydrologic data that is very detailed and is meant to be used at scales approaching 1:600. The data in its original application aided, as breaklines, in detailed digital terrain modeling.</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/dee607cd-b195-44a2-b68f-d00ab45c69bd.png"/></div>
<p>While useful in itself, the data was further manipulated, separating the linear features from area features, with additional polygonization of the area features, as shown in the following screenshot:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/cbba571c-8f5e-451b-93e2-382ea0d45cd8.png"/></div>
<p>Finally, the data was classified into basic waterway categories, as follows:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/98c7b472-407a-4618-bde6-668416f084c0.png"/></div>
<p>In addition, a process was undertaken to generate centerlines for polygon features such as streams, which are effectively linear features, as follows:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/b7a60213-43b3-4386-970e-da5f209e1949.png"/></div>
<p>Hence, we have three separate but related datasets:</p>
<ul>
<li><kbd>cuyahoga_hydro_polygon</kbd></li>
<li><kbd>cuyahoga_hydro_polyline</kbd></li>
<li><kbd>cuyahoga_river_centerlines</kbd></li>
</ul>
<p>Now, let us look at the structure of the tabular data. Unzip the hydrology file from the book repository and go to that directory. The <kbd>ogrinfo</kbd> utility can help us with this, as shown in the following command:</p>
<pre><strong>&gt; ogrinfo cuyahoga_hydro_polygon.shp -al -so</strong>  </pre>
<p>The output is as follows:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/e90c2a9b-ba18-42dd-9d26-677f877d92ed.png" style="width:39.92em;height:29.75em;"/></div>
<p>Executing this query on each of the shapefiles, we see the following fields that are common to all the shapefiles:</p>
<ul>
<li><kbd>name</kbd></li>
<li><kbd>hyd_type</kbd></li>
<li><kbd>geom_type</kbd></li>
</ul>
<p>It is by understanding our common fields that we can apply inheritance to completely structure our data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Now that we know our common fields, creating an inheritance model is easy. First, we will create a parent table with the fields common to all the tables, using the following query:</p>
<pre><strong>CREATE TABLE chp02.hydrology ( 
  gid SERIAL PRIMARY KEY, 
  "name"      text, 
  hyd_type    text, 
  geom_type   text, 
  the_geom    geometry 
);</strong> </pre>
<p>If you are paying attention, you will note that we also added a <kbd>geometry</kbd> field as all of our shapefiles implicitly have this commonality. With inheritance, every record inserted in any of the child tables will also be saved in our parent table, only these records will be stored without the extra fields specified for the child tables.</p>
<p>To establish inheritance for a given table, we need to declare only the additional fields that the child table contains using the following query:</p>
<pre><strong>CREATE TABLE chp02.hydrology_centerlines ( 
  "length"    numeric 
) INHERITS (chp02.hydrology); 
 
CREATE TABLE chp02.hydrology_polygon ( 
  area    numeric, 
  perimeter    numeric 
) INHERITS (chp02.hydrology); 
 
CREATE TABLE chp02.hydrology_linestring ( 
  sinuosity    numeric 
) INHERITS (chp02.hydrology_centerlines);</strong> </pre>
<p>Now, we are ready to load our data using the following commands:</p>
<ul>
<li><kbd>shp2pgsql -s 3734 -a -i -I -W LATIN1 -g the_geom cuyahoga_hydro_polygon chp02.hydrology_polygon | psql -U me -d postgis_cookbook</kbd></li>
<li><kbd>shp2pgsql -s 3734 -a -i -I -W LATIN1 -g the_geom cuyahoga_hydro_polyline chp02.hydrology_linestring | psql -U me -d postgis_cookbook</kbd></li>
<li><kbd>shp2pgsql -s 3734 -a -i -I -W LATIN1 -g the_geom cuyahoga_river_centerlines chp02.hydrology_centerlines | psql -U me -d postgis_cookbook</kbd></li>
</ul>
<p>If we view our parent table, we will see all the records in all the child tables. The following is a screenshot of fields in <kbd>hydrology</kbd>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/ec505dd1-c9c9-430a-8fa1-0b2fa16302f9.png" style="width:35.33em;height:40.50em;"/></div>
<p>Compare that to the fields available in <kbd>hydrology_linestring</kbd> that will reveal specific fields of interest:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6ac3acbf-428c-406c-a158-e1ab36fb935c.png" style="width:39.08em;height:36.58em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>PostgreSQL table inheritance allows us to enforce essentially hierarchical relationships between tables. In this case, we leverage inheritance to allow for commonality between related datasets. Now, if we want to query data from these tables, we can query directly from the parent table as follows, depending on whether we want a mix of geometries or just a targeted dataset:</p>
<pre><strong>SELECT * FROM chp02.hydrology</strong> </pre>
<p>From any of the child tables, we could use the following query:</p>
<pre><strong>SELECT * FROM chp02.hydrology_polygon</strong> </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>It is possible to extend this concept in order to leverage and optimize storage and querying by using the <kbd>CHECK</kbd> constrains in conjunction with inheritance. For more info, see the <em>Extending inheritance â€“ table partitioning</em> recipe.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Extending inheritance â€“ table partitioning</h1>
                </header>
            
            <article>
                
<p>Table partitioning is an approach specific to PostgreSQL that extends inheritance to model tables that typically do not vary from each other in the available fields, but where the child tables represent logical partitioning of the data based on a variety of factors, be it time, value ranges, classifications, or in our case, spatial relationships. The advantages of partitioning include improved query performance due to smaller indexes and targeted scans of data, bulk loads, and deletes that bypass the costs of vacuuming. It can thus be used to put commonly used data on faster and more expensive storage, and the remaining data on slower and cheaper storage. In combination with PostGIS, we get the novel power of spatial partitioning, which is a really powerful feature for large datasets.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>We could use many examples of large datasets that could benefit from partitioning. In our case, we will use a contour dataset. Contours are useful ways to represent terrain data, as they are well established and thus commonly interpreted. Contours can also be used to compress terrain data into linear representations, thus allowing it to be shown in conjunction with other data easily.</p>
<p>The problem is, the storage of contour data can be quite expensive. Two-foot contours for a single US county can take 20 to 40 GB, and storing such data for a larger area such as a region or nation can become quite prohibitive from the standpoint of accessing the appropriate portion of the dataset in a performant way.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>The first step in this case may be to prepare the data. If we had a monolithic contour table called <kbd>cuy_contours_2</kbd>, we could choose to clip the data to a series of rectangles that will serve as our table partitions; in this case, <kbd>chp02.contour_clip</kbd>, using the following query:</p>
<pre><strong>CREATE TABLE chp02.contour_2_cm_only AS 
  SELECT contour.elevation, contour.gid, contour.div_10, contour.div_20, contour.div_50, 
  contour.div_100, cc.id, ST_Intersection(contour.the_geom, cc.the_geom) AS the_geom FROM 
    chp02.cuy_contours_2 AS contour, chp02.contour_clip as cc 
    WHERE ST_Within(contour.the_geom,cc.the_geom 
      OR 
      ST_Crosses(contour.the_geom,cc.the_geom);</strong> </pre>
<p>We are performing two tests here in our query. We are using <kbd>ST_Within</kbd>, which tests whether a given contour is entirely within our area of interest. If so, we perform an intersection; the resultant geometry should just be the geometry of the contour.</p>
<p>The <kbd>ST_Crosses</kbd> function checks whether the contour crosses the boundary of the geometry we are testing. This should capture all the geometries lying partially inside and partially outside our areas. These are the ones that we will truly intersect to get the resultant shape.</p>
<p>In our case, it is easier and we don't require this step. Our contour shapes are already individual shapefiles clipped to rectangular boundaries, as shown in the following screenshot:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/fe831299-188c-4bc0-96e4-eb3047a474a2.png" style="width:28.58em;height:28.33em;"/></div>
<p>Since the data is already clipped into the chunks needed for our partitions, we can just continue to create the appropriate partitions.</p>
<p>Much like with inheritance, we start by creating our parent table using the following query:</p>
<pre><strong>CREATE TABLE chp02.contours 
( 
  gid serial NOT NULL, 
  elevation integer, 
  __gid double precision, 
  the_geom geometry(MultiLineStringZM,3734), 
  CONSTRAINT contours_pkey PRIMARY KEY (gid) 
) 
WITH ( 
  OIDS=FALSE 
);</strong> </pre>
<p>Here again, we maintain our constraints, such as <kbd>PRIMARY KEY,</kbd> and specify the geometry type (<kbd>MultiLineStringZM</kbd>), not because these will propagate to the child tables, but for any client software accessing the parent table to anticipate such constraints.</p>
<p>Now we may begin to create tables that inherit from our parent table. In the process, we will create a <kbd>CHECK</kbd> constraint specifying the limits of our associated geometry using the following query:</p>
<pre><strong>CREATE TABLE chp02.contour_N2260630 
  (CHECK<br/>    (ST_CoveredBy(the_geom,ST_GeomFromText<br/>      ('POLYGON((2260000, 630000, 2260000 635000, 2265000 635000,<br/>                 2265000 630000, 2260000 630000))',3734)<br/>    )<br/>  )) INHERITS (chp02.contours);</strong> </pre>
<p>We can complete the table structure for partitioning the contours with similar <kbd>CREATE TABLE</kbd> queries for our remaining tables, as follows:</p>
<pre><strong>CREATE TABLE chp02.contour_N2260635 
  (CHECK <br/>    (ST_CoveredBy(the_geom,ST_GeomFromText<br/>      ('POLYGON((2260000 635000, 2260000 640000,<br/>                 2265000 640000, 2265000 635000, 2260000 635000))', 3734) 
    )<br/>  )) INHERITS (chp02.contours); 
CREATE TABLE chp02.contour_N2260640 
  (CHECK<br/>    (ST_CoveredBy(the_geom,ST_GeomFromText<br/>      ('POLYGON((2260000 640000, 2260000 645000, 2265000 645000,<br/>                 2265000 640000, 2260000 640000))', 3734) 
    )<br/>  )) INHERITS (chp02.contours); 
CREATE TABLE chp02.contour_N2265630 
  (CHECK<br/>    (ST_CoveredBy(the_geom,ST_GeomFromText <br/>      ('POLYGON((2265000 630000, 2265000 635000, 2270000 635000,<br/>                 2270000 630000, 2265000 630000))', 3734)<br/>    )<br/>  )) INHERITS (chp02.contours); 
CREATE TABLE chp02.contour_N2265635 
  (CHECK <br/>    (ST_CoveredBy(the_geom,ST_GeomFromText<br/>      ('POLYGON((2265000 635000, 2265000 640000, 2270000 640000,<br/>                 2270000 635000, 2265000 635000))', 3734) 
    )<br/>  )) INHERITS (chp02.contours); 
CREATE TABLE chp02.contour_N2265640 
  (CHECK <br/>    (ST_CoveredBy(the_geom,ST_GeomFromText<br/>      ('POLYGON((2265000 640000, 2265000 645000, 2270000 645000,<br/>                 2270000 640000, 2265000 640000))', 3734) 
    )<br/>  )) INHERITS (chp02.contours); 
CREATE TABLE chp02.contour_N2270630 
  (CHECK <br/>    (ST_CoveredBy(the_geom,ST_GeomFromText<br/>      ('POLYGON((2270000 630000, 2270000 635000, 2275000 635000, <br/>                 2275000 630000, 2270000 630000))', 3734) 
    )<br/>  )) INHERITS (chp02.contours); 
CREATE TABLE chp02.contour_N2270635 
  (CHECK <br/>    (ST_CoveredBy(the_geom,ST_GeomFromText<br/>      ('POLYGON((2270000 635000, 2270000 640000, 2275000 640000, <br/>                 2275000 635000, 2270000 635000))', 3734) 
    )<br/>  )) INHERITS (chp02.contours); 
CREATE TABLE chp02.contour_N2270640 
  (CHECK<br/>    (ST_CoveredBy(the_geom,ST_GeomFromText     <br/>      ('POLYGON((2270000 640000, 2270000 645000, 2275000 645000, <br/>                 2275000 640000, 2270000 640000))', 3734) 
    )<br/>  )) INHERITS (chp02.contours);</strong> </pre>
<p class="mce-root">And now we can load our contours shapefiles found in the <kbd>contours1</kbd> ZIP file into each of our child tables, using the following command, by replacing the filename. If we wanted to, we could even implement a trigger on the parent table, which would place each insert into its correct child table, though this might incur performance costs:</p>
<pre><strong>shp2pgsql -s 3734 -a -i -I -W LATIN1 -g the_geom N2265630 chp02.contour_N2265630 | psql -U me -d postgis_cookbook</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>The <kbd>CHECK</kbd> constraint in combination with inheritance is all it takes to build a table partitioning. In this case, we're using a bounding box as our <kbd>CHECK</kbd> constraint and simply inheriting the columns from the parent table. Now that we have this in place, queries against the parent table will check our <kbd>CHECK</kbd> constraints first before employing a query.</p>
<p>This also allows us to place any of our lesser-used contour tables on cheaper and slower storage, thus allowing for cost-effective optimizations of large datasets. This structure is also beneficial for rapidly changing data, as updates can be applied to an entire area; the entire table for that area can be efficiently dropped and repopulated without traversing across the dataset.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>For more on table inheritance in general, particularly the flexibility associated with the usage of alternate columns in the child table, see the previous recipe, <em>Structuring spatial data with table inheritance</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Normalizing imports</h1>
                </header>
            
            <article>
                
<p>Often, data used in a spatial database is imported from other sources. As such, it may not be in a form that is useful for our current application. In such a case, it may be useful to write functions that will aid in transforming the data into a form that is more useful for our application. This is particularly the case when going from flat file formats, such as shapefiles, to relational databases such as PostgreSQL.</p>
<div class="packt_infobox">A shapefile is a de facto as well as a format specification for the storage of spatial data, and is probably the most common delivery format for spatial data. A shapefile, in spite of its name, is never just one file, but a collection of files. It consists of at least <kbd>*.shp</kbd> (which contains geometry), <kbd>*.shx</kbd> (an index file), and <kbd>*.dbf</kbd> (which contains the tabular information for the shapefile). It is a powerful and useful format, but as a flat file, it is inherently non-relational. Each geometry is associated in a one-to-one relationship with each row in a table.</div>
<p>There are many structures that might serve as a proxy for relational stores in a shapefile. We will explore one here: a single field with delimited text for multiple relations. This is a not-too-uncommon hack to encode multiple relationships into a flat file. The other common approach is to create multiple fields to store what in a relational arrangement would be a single field.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>The dataset we will be working with is a trails dataset that has linear extents for a set of trails in a park system. The data is the typical data that comes from the GIS world; as a flat shapefile, there are no explicit relational constructs in the data.</p>
<p>First, unzip the <kbd>trails.zip</kbd> file and use the command line to go into it, then load the data using the following command:</p>
<pre><strong>shp2pgsql -s 3734 -d -i -I -W LATIN1 -g the_geom trails chp02.trails | psql -U me -d postgis_cookbook</strong></pre>
<p>Looking at the linear data, we have some categories for the use type:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/e73e89c3-d887-45c7-85dd-14d2b2d8092a.png" style="width:45.17em;height:33.83em;"/></div>
<p>We want to retain this information as well as the name. Unfortunately, the <kbd>label_name</kbd> field is a messy field with a variety of related names concatenated with an ampersand (<kbd>&amp;</kbd>), as shown in the following query:</p>
<pre><strong>SELECT DISTINCT label_name FROM chp02.trails 
  WHERE label_name LIKE '%&amp;%' LIMIT 10; </strong> </pre>
<p>It will return the following output:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/740e10f2-cda8-4acc-8e49-6b3b54802548.png" style="width:39.00em;height:25.00em;"/></div>
<p>This is where the normalization of our table will begin.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>The first thing we need to do is find all the fields that don't have ampersands and use those as our unique list of available trails. In our case, we can do this, as every trail has at least one segment that is uniquely named and not associated with another trail name. This approach will not work with all datasets, so be careful in understanding your data before applying this approach to that data.</p>
<p>To select the fields ordered without ampersands, we use the following query:</p>
<pre><strong>SELECT DISTINCT label_name, res 
  FROM chp02.trails 
  WHERE label_name NOT LIKE '%&amp;%' 
  ORDER BY label_name, res;</strong> </pre>
<p>It will return the following output:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/45c659e3-b010-4b13-9124-daaec6ca0ca6.png" style="width:43.50em;height:34.00em;"/></div>
<p>Next, we want to search for all the records that match any of these unique trail names. This will give us the list of records that will serve as relations. The first step in doing this search is to append the percent (<kbd>%</kbd>) signs to our unique list in order to build a string on which we can search using a <kbd>LIKE</kbd> query:</p>
<pre><strong>SELECT '%' || label_name || '%' AS label_name, label_name as   label, res FROM 
  (SELECT DISTINCT label_name, res 
    FROM chp02.trails 
    WHERE label_name NOT LIKE '%&amp;%' 
    ORDER BY label_name, res 
  ) AS label;</strong> </pre>
<p>Finally, we'll use this in the context of a <kbd>WITH</kbd> block to do the normalization itself. This will provide us with a table of unique IDs for each segment in our first column, along with the associated <kbd>label</kbd> column. For good measure, we will do this as a <kbd>CREATE TABLE</kbd> procedure, as shown in the following query:</p>
<pre><strong>CREATE TABLE chp02.trails_names AS WITH labellike AS 
( 
SELECT '%' || label_name || '%' AS label_name, label_name as   label, res FROM 
  (SELECT DISTINCT label_name, res 
    FROM chp02.trails 
    WHERE label_name NOT LIKE '%&amp;%' 
    ORDER BY label_name, res 
  ) AS label 
) 
SELECT t.gid, ll.label, ll.res 
  FROM chp02.trails AS t, labellike AS ll 
  WHERE t.label_name LIKE ll.label_name 
  AND 
  t.res = ll.res 
  ORDER BY gid;</strong></pre>
<p class="mce-root">If we view the first rows of the table created,Â <kbd>trails_names</kbd>, we have the following output with <kbd>pgAdmin</kbd>:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/61a39308-ad2c-4f0a-8fd1-887f98d6b16e.png" style="width:21.33em;height:25.92em;"/></div>
<p>Now that we have a table of the relations, we need a table of the geometries associated with <kbd>gid</kbd>. This, in comparison, is quite easy, as shown in the following query:</p>
<pre><strong>CREATE TABLE chp02.trails_geom AS 
  SELECT gid, the_geom 
  FROM chp02.trails;</strong> </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>In this example, we have generated a unique list of possible records in conjunction with a search for the associated records, in order to build table relationships. In one table, we have the geometry and a unique ID of each spatial record; in another table, we have the names associated with each of those unique IDs. Now we can explicitly leverage those relationships.</p>
<p>First, we need to establish our unique IDs as primary keys, as follows:</p>
<pre><strong>ALTER TABLE chp02.trails_geom ADD PRIMARY KEY (gid);</strong> </pre>
<p>Now we can use that <kbd>PRIMARY KEY</kbd> as a <kbd>FOREIGN KEY</kbd> in our <kbd>trails_names</kbd> table using the following query:</p>
<pre><strong>ALTER TABLE chp02.trails_names ADD FOREIGN KEY (gid) REFERENCES chp02.trails_geom(gid);</strong> </pre>
<p>This step isn't strictly necessary, but does enforce referential integrity for queries such as the following:</p>
<pre><strong>SELECT geo.gid, geo.the_geom, names.label FROM 
  chp02.trails_geom AS geo, chp02.trails_names AS names 
  WHERE geo.gid = names.gid;</strong></pre>
<p class="mce-root">The output is as follows:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/1196c509-656a-4907-a288-ca9b161ca303.png" style="width:18.33em;height:18.17em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>If we had multiple fields we wanted to normalize, we could write <kbd>CREATE TABLE</kbd> queries for each of them.</p>
<p>It is interesting to note that the approach framed in this recipe is not limited to cases where we have a delimited field. This approach can provide a relatively generic solution to the problem of normalizing flat files. For example, if we have a case where we have multiple fields to represent relational info, such as <kbd>label1</kbd>, <kbd>label2</kbd>, <kbd>label3</kbd>, or similar multiple attribute names for a single record, we can write a simple query to concatenate them together before feeding that info into our query.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Normalizing internal overlays</h1>
                </header>
            
            <article>
                
<p>Data from an external source can have issues in the table structure as well as in the topology, endemic to the geospatial data itself. Take, for example, the problem of data with overlapping polygons. If our dataset has polygons that overlap with internal overlays, then queries for area, perimeter, and other metrics may not produce predictable or consistent results.</p>
<p>There are a few approaches that can solve the problem of polygon datasets with internal overlays. The general approach presented here was originally proposed by <em>Kevin Neufeld</em> of <em>Refractions Research</em>.</p>
<p>Over the course of writing our query, we will also produce a solution for converting polygons to linestrings.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>First, unzip the <kbd>use_area.zip</kbd> file and go into it using the command line; then, load the dataset using the following command:</p>
<pre><strong>shp2pgsql -s 3734 -d -i -I -W LATIN1 -g the_geom cm_usearea_polygon chp02.use_area | psql -U me -d postgis_cookbook</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Now that the data is loaded into a table in the database, we can leverage PostGIS to flatten and get the union of the polygons, so that we have a normalized dataset. The first step in doing so using this approach will be to convert the <em>polygons</em> to <em>linestrings</em>. We can then link those <em>linestrings</em> and convert them back to <em>polygons</em>, representing the union of all the <em>polygon</em> inputs. We will perform the following tasks:</p>
<ol>
<li>Convert polygons to linestrings</li>
<li>Convert linestrings back to polygons</li>
<li>Find the center points of the resultant polygons</li>
<li>Use the resultant points to query tabular relationships</li>
</ol>
<p>To convert polygons to linestrings, we'll need to extract just the portions of the polygons we want using <kbd>ST_ExteriorRing</kbd>, convert those parts to points using <kbd>ST_DumpPoints</kbd>, and then connect those points back into lines like a connect-the-dots coloring book using <kbd>ST_MakeLine</kbd>.</p>
<p>Breaking it down further, <kbd>ST_ExteriorRing (the_geom)</kbd> will grab just the outer boundary of our polygons. But <kbd>ST_ExteriorRing</kbd> returns polygons, so we need to take that output and create a line from it. The easiest way to do this is to convert it to points using <kbd>ST_DumpPoints</kbd> and then connect those points. By default, the <kbd>Dump</kbd> function returns an object called a <kbd>geometry_dump</kbd>, which is not just simple geometry, but the geometry in combination with an array of integers. The easiest way to return the geometry alone is the leverage object notation to extract just the geometry portion of <kbd>geometry_dump,</kbd> as follows:</p>
<pre><strong>(ST_DumpPoints(geom)).geom</strong> </pre>
<p>Piecing the geometry back together with <kbd>ST_ExteriorRing</kbd> is done using the following query:</p>
<pre><strong>SELECT (ST_DumpPoints(ST_ExteriorRing(geom))).geom</strong> </pre>
<p>This should give us a listing of points in order from the exterior rings of all the points from which we want to construct our lines using <kbd>ST_MakeLine</kbd>, as shown in the following query:</p>
<pre><strong>SELECT ST_MakeLine(geom) FROM ( 
  SELECT (ST_DumpPoints(ST_ExteriorRing(geom))).geom) AS linpoints</strong> </pre>
<p>Since the preceding approach is a process we may want to use in many other places, it might be prudent to create a function from this using the following query:</p>
<pre><strong>CREATE OR REPLACE FUNCTION chp02.polygon_to_line(geometry) 
  RETURNS geometry AS 
$BODY$ 
 
  SELECT ST_MakeLine(geom) FROM ( 
    SELECT (ST_DumpPoints(ST_ExteriorRing((ST_Dump($1)).geom))).geom 
 
  ) AS linpoints 
$BODY$ 
  LANGUAGE sql VOLATILE; 
ALTER FUNCTION chp02.polygon_to_line(geometry) 
  OWNER TO me;</strong> </pre>
<p>Now that we have the <kbd>polygon_to_line</kbd> function, we still need to force the linking of overlapping lines in our particular case. The <kbd>ST_Union</kbd> function will aid in this, as shown in the following query:</p>
<pre><strong>SELECT ST_Union(the_geom) AS geom FROM ( 
  SELECT chp02.polygon_to_line(geom) AS geom FROM 
    chp02.use_area 
  ) AS unioned;</strong> </pre>
<p>Now let's convert linestrings back to polygons, and for this we can polygonize the result using <kbd>ST_Polygonize</kbd>, as shown in the following query:</p>
<pre><strong>SELECT ST_Polygonize(geom) AS geom FROM ( 
  SELECT ST_Union(the_geom) AS geom FROM ( 
    SELECT chp02.polygon_to_line(geom) AS geom FROM 
    chp02.use_area 
  ) AS unioned 
) as polygonized;</strong> </pre>
<p>The <kbd>ST_Polygonize</kbd> function will create a single multi polygon, so we need to explode this into multiple single polygon geometries if we are to do anything useful with it. While we are at it, we might as well do the following within a <kbd>CREATE TABLE</kbd> statement:</p>
<pre><strong>CREATE TABLE chp02.use_area_alt AS ( 
  SELECT (ST_Dump(the_geom)).geom AS the_geom FROM ( 
    SELECT ST_Polygonize(the_geom) AS the_geom FROM ( 
      SELECT ST_Union(the_geom) AS the_geom FROM ( 
        SELECT chp02.polygon_to_line(the_geom) AS the_geom<br/>        FROM chp02.use_area 
      ) AS unioned 
    ) as polygonized 
  ) AS exploded 
);</strong> </pre>
<p class="mce-root">We will be performing spatial queries against this geometry, so we should create an index in order to ensure our query performs well, as shown in the following query:</p>
<pre><strong>CREATE INDEX chp02_use_area_alt_the_geom_gist 
  ON chp02.use_area_alt 
  USING gist(the_geom);</strong> </pre>
<p>In order to find the appropriate table information from the original geometry and apply that back to our resultant geometries, we will perform a point-in-polygon query. For that, we first need to calculate centroids on the resultant geometry:</p>
<pre><strong>CREATE TABLE chp02.use_area_alt_p AS 
  SELECT ST_SetSRID(ST_PointOnSurface(the_geom), 3734) AS  
    the_geom FROM 
    chp02.use_area_alt; 
ALTER TABLE chp02.use_area_alt_p ADD COLUMN gid serial; 
ALTER TABLE chp02.use_area_alt_p ADD PRIMARY KEY (gid);</strong> </pre>
<p>And as always, create a spatial index using the following query:</p>
<pre><strong>CREATE INDEX chp02_use_area_alt_p_the_geom_gist 
  ON chp02.use_area_alt_p 
  USING gist(the_geom);</strong> </pre>
<p>The centroids then structure our point-in-polygon (<kbd>ST_Intersects</kbd>) relationship between the original tabular information and resultant polygons, using the following query:</p>
<pre><strong>CREATE TABLE chp02.use_area_alt_relation AS 
SELECT points.gid, cu.location FROM 
  chp02.use_area_alt_p AS points, 
  chp02.use_area AS cu 
    WHERE ST_Intersects(points.the_geom, cu.the_geom);</strong></pre>
<p>If we view the first rows of the table, we can see it links the identifier of points to their respective locations:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/f23cff08-dac5-435e-a138-95f9e215cc74.png" style="width:17.00em;height:24.58em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Our essential approach here is to look at the underlying topology of the geometry and reconstruct a topology that is non-overlapping, and then use the centroids of that new geometry to construct a query that establishes the relationship to the original data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>At this stage, we can optionally establish a framework for referential integrity using a foreign key, as follows:</p>
<pre><strong>ALTER TABLE chp02.use_area_alt_relation ADD FOREIGN KEY (gid) REFERENCES chp02.use_area_alt_p (gid);</strong> </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using polygon overlays for proportional census estimates</h1>
                </header>
            
            <article>
                
<p>PostgreSQL functions abound for the aggregation of tabular data, including <kbd>sum</kbd>, <kbd>count</kbd>, <kbd>min</kbd>, <kbd>max</kbd>, and so on. PostGIS as a framework does not explicitly have spatial equivalents of these, but this does not prevent us from building functions using the aggregate functions from PostgreSQL in concert with PostGIS's spatial functionality.</p>
<p>In this recipe, we will explore spatial summarization with the United States census data. The US census data, by nature, is aggregated data. This is done intentionally to protect the privacy of citizens. But when it comes to doing analyses with this data, the aggregate nature of the data can become problematic. There are some tricks to disaggregate data. Amongst the simplest of these is the use of a proportional sum, which we will do in this exercise.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>The problem at hand is that a proposed trail has been drawn in order to provide services for the public. This example could apply to road construction or even finding sites for commercial properties for the purpose of provisioning services.</p>
<p>First, unzip the <kbd>trail_census.zip</kbd> file, then perform a quick data load using the following commands from the unzipped folder:</p>
<pre><strong>shp2pgsql -s 3734 -d -i -I -W LATIN1 -g the_geom census chp02.trail_census | psql -U me -d postgis_cookbook</strong>
<strong>shp2pgsql -s 3734 -d -i -I -W LATIN1 -g the_geom trail_alignment_proposed_buffer chp02.trail_buffer | psql -U me -d postgis_cookbook</strong>
<strong>shp2pgsql -s 3734 -d -i -I -W LATIN1 -g the_geom trail_alignment_proposed chp02.trail_alignment_prop | psql -U me -d postgis_cookbook</strong></pre>
<p>The preceding commands will produce the following outputs:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/2e1a9e32-06fd-496d-9623-aa47bb59958a.png" style="width:43.67em;height:34.17em;"/></div>
<p class="packt_figure">If we view the proposed trail in our favorite desktop GIS, we have the following:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/02ce64b3-b125-4afe-b247-6d99e788dabd.png" style="width:21.25em;height:32.75em;"/></div>
<p>In our case, we want to know the population within 1 mile of the trail, assuming that persons living within 1 mile of the trail are the ones most likely to use it, and thus most likely to be served by it.</p>
<p>To find out the population near this proposed trail, we overlay census block group population density information. Illustrated in the next screenshot is a 1-mile buffer around the proposed trail:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/0554efef-2029-4dc8-a325-a1a3f4a1b898.png" style="width:44.67em;height:33.50em;"/></div>
<p>One of the things we might note about this census data is the wide range of census densities and census block group sizes. An approach to calculating the population would be to simply select all census blocks that intersect our area, as shown in the following screenshot:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="assets/d3452784-b8b6-4916-a4e4-996f34b1f12f.png" style="width:38.33em;height:28.83em;"/></div>
<p>This is a simple procedure that gives us an estimate of 130 to 288 people living within 1 mile of the trail, but looking at the shape of the selection, we can see that we are overestimating the population by taking the complete blocks in our estimate.</p>
<p>Similarly, if we just used the block groups whose centroids lay within 1 mile of our proposed trail alignment, we would underestimate the population.</p>
<p>Instead, we will make some useful assumptions. Block groups are designed to be moderately homogeneous within the block group population distribution. Assuming that this holds true for our data, we can assume that for a given block group, if 50% of the block group is within our target area, we can attribute half of the population of that block group to our estimate. Apply this to all our block groups, sum them, and we have a refined estimate that is likely to be better than pure intersects or centroid queries. Thus, we employ a proportional sum.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>As the problem of a proportional sum is a generic problem, it could apply to many problems. We will write the underlying proportioning as a function. A function takes inputs and returns a value. In our case, we want our proportioning function to take two geometries, that is, the geometry of our buffered trail and block groups as well as the value we want proportioned, and we want it to return the proportioned value:</p>
<pre><strong>CREATE OR REPLACE FUNCTION chp02.proportional_sum(geometry,   geometry, numeric) 
  RETURNS numeric AS 
$BODY$ 
-- SQL here 
$BODY$ 
  LANGUAGE sql VOLATILE;</strong> </pre>
<p>Now, for the purpose of our calculation, for any given intersection of buffered area and block group, we want to find the proportion that the intersection is over the overall block group. Then this value should be multiplied by the value we want to scale.</p>
<p>In SQL, the function looks like the following query:</p>
<pre><strong>SELECT $3 * areacalc FROM 
  (SELECT (ST_Area(ST_Intersection($1, $2)) / ST_Area($2)):: numeric AS areacalc 
  ) AS areac;</strong> </pre>
<p>The preceding query in its full form looks as follows:</p>
<pre><strong>CREATE OR REPLACE FUNCTION chp02.proportional_sum(geometry,   geometry, numeric) 
  RETURNS numeric AS 
$BODY$ 
    SELECT $3 * areacalc FROM 
      (SELECT (ST_Area(ST_Intersection($1,           $2))/ST_Area($2))::numeric AS areacalc 
      ) AS areac 
; 
$BODY$ 
  LANGUAGE sql VOLATILE;</strong> </pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Since we have written the query as a function, the query uses the <kbd>SELECT</kbd> statement to loop through all available records and give us a proportioned population. Astute readers will note that we have not yet done any work on summarization; we have only worked on the proportionality portion of the problem. We can do the summarization upon calling the function using PostgreSQL's built-in aggregate functions. What is neat about this approach is that we need not just apply a sum, but we could also calculate other aggregates such as min or max. In the following example, we will just apply a sum:</p>
<pre><strong>SELECT ROUND(SUM(chp02.proportional_sum(a.the_geom, b.the_geom, b.pop))) FROM 
  chp02.trail_buffer AS a, chp02.trail_census as b 
  WHERE ST_Intersects(a.the_geom, b.the_geom) 
  GROUP BY a.gid;</strong> </pre>
<p>The value returned is quite different (a population of 96,081), which is more likely to be accurate.</p>


            </article>

            
        </section>
    </body></html>