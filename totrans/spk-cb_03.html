<html><head></head><body><div class="chapter" title="Chapter&#xA0;3.&#xA0;External Data Sources"><div class="titlepage"><div><div><h1 class="title"><a id="ch03"/>Chapter 3. External Data Sources</h1></div></div></div><p>One of the strengths of Spark is that it provides a single runtime that can connect with various underlying data sources.</p><p>In this chapter, we will connect to different data sources. This chapter is divided into the following recipes:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Loading data from the local filesystem</li><li class="listitem" style="list-style-type: disc">Loading data from HDFS</li><li class="listitem" style="list-style-type: disc">Loading data from HDFS using a custom InputFormat</li><li class="listitem" style="list-style-type: disc">Loading data from Amazon S3</li><li class="listitem" style="list-style-type: disc">Loading data from Apache Cassandra</li><li class="listitem" style="list-style-type: disc">Loading data from relational databases</li></ul></div><div class="section" title="Introduction"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec23"/>Introduction</h1></div></div></div><p>Spark provides a unified runtime for big data. HDFS, which is Hadoop's filesystem, is the most used storage <a id="id140" class="indexterm"/>platform for Spark as it provides cost-effective storage for unstructured and semi-structured data on commodity hardware. Spark is not limited to HDFS and can work with any Hadoop-supported storage.</p><p>Hadoop supported storage means a storage format that can work with Hadoop's <code class="literal">InputFormat</code> and <code class="literal">OutputFormat</code> interfaces. <code class="literal">InputFormat</code> is responsible for creating <code class="literal">InputSplits</code> from <a id="id141" class="indexterm"/>input data and dividing it further into <a id="id142" class="indexterm"/>records. <code class="literal">OutputFormat</code> is responsible for writing to storage.</p><p>We will start with writing to the local filesystem and then move over to loading data from HDFS. In the <span class="emphasis"><em>Loading data from HDFS</em></span> recipe, we will cover the most common file format: regular text files. In the next recipe, we will cover how to use any <code class="literal">InputFormat</code> interface to load data in Spark. We will also explore loading data stored in Amazon S3, a leading cloud storage platform.</p><p>We will explore loading data from Apache Cassandra, which is a NoSQL database. Finally, we will explore loading data from a relational database.</p></div></div>
<div class="section" title="Loading data from the local filesystem"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec24"/>Loading data from the local filesystem</h1></div></div></div><p>Though the<a id="id143" class="indexterm"/> local filesystem is not a good fit to store big <a id="id144" class="indexterm"/>data due to disk size limitations and lack of distributed nature, technically you can load data in distributed systems using the local filesystem. But then the file/directory you are accessing has to be available on each node.</p><p>Please note that if you are planning to use this feature to load side data, it is not a good idea. To load side data, Spark has a broadcast variable feature, which will be discussed in upcoming chapters.</p><p>In this recipe, we will look at how to load data in Spark from the local filesystem.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec35"/>How to do it...</h2></div></div></div><p>Let's start with the example of Shakespeare's "to be or not to be":</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Create the <code class="literal">words</code> directory by using the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ mkdir words</strong></span>
</pre></div></li><li class="listitem">Get into the <code class="literal">words</code> directory:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cd words</strong></span>
</pre></div></li><li class="listitem">Create the <code class="literal">sh.txt</code> text file and enter <code class="literal">"to be or not to be"</code> in it:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ echo "to be or not to be" &gt; sh.txt</strong></span>
</pre></div></li><li class="listitem">Start the Spark shell:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell</strong></span>
</pre></div></li><li class="listitem">Load the <code class="literal">words</code> directory as RDD:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val words = sc.textFile("file:///home/hduser/words")</strong></span>
</pre></div></li><li class="listitem">Count the number of lines:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; words.count</strong></span>
</pre></div></li><li class="listitem">Divide the line (or lines) into multiple words:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val wordsFlatMap = words.flatMap(_.split("\\W+"))</strong></span>
</pre></div></li><li class="listitem">Convert <code class="literal">word</code> to (word,1)—that is, output <code class="literal">1</code> as the value for each occurrence of <code class="literal">word</code> as a key:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val wordsMap = wordsFlatMap.map( w =&gt; (w,1))</strong></span>
</pre></div></li><li class="listitem">Use the <code class="literal">reduceByKey</code> method to add the number of occurrences for each word as a key (this function works on two consecutive values at a time, represented by <code class="literal">a</code> and <code class="literal">b</code>):<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val wordCount = wordsMap.reduceByKey( (a,b) =&gt; (a+b))</strong></span>
</pre></div></li><li class="listitem">Print the RDD:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; wordCount.collect.foreach(println)</strong></span>
</pre></div></li><li class="listitem">Doing <a id="id145" class="indexterm"/>all of the preceding operations<a id="id146" class="indexterm"/> in one step is as follows:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; sc.textFile("file:///home/hduser/ words"). flatMap(_.split("\\W+")).map( w =&gt; (w,1)). reduceByKey( (a,b) =&gt; (a+b)).foreach(println)</strong></span>
</pre></div></li></ol></div><p>This gives the following output:</p><div class="mediaobject"><img src="graphics/B03056_03_01.jpg" alt="How to do it..."/></div></div></div>
<div class="section" title="Loading data from HDFS"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec25"/>Loading data from HDFS</h1></div></div></div><p>HDFS is the most widely used big data storage system. One of the reasons for the wide adoption of <a id="id147" class="indexterm"/>HDFS is schema-on-read. What this means is that HDFS does <a id="id148" class="indexterm"/>not put any restriction on data when data is being written. Any and all kinds of data are welcome and can be stored in a raw format. This feature makes it ideal storage for raw unstructured data and semi-structured data.</p><p>When it comes to reading data, even unstructured data needs to be given some structure to make sense. Hadoop uses <code class="literal">InputFormat</code> to determine how to read the data. Spark provides complete support for Hadoop's <code class="literal">InputFormat</code> so anything that can be read by Hadoop can be read by Spark as well.</p><p>The default <code class="literal">InputFormat</code> is <code class="literal">TextInputFormat</code>. <code class="literal">TextInputFormat</code> takes the byte offset of a line as a key and the content of a line as a value. Spark uses the <code class="literal">sc.textFile</code> method to read using <code class="literal">TextInputFormat</code>. It ignores the byte offset and creates an RDD of strings.</p><p>Sometimes the filename itself contains useful information, for example, time-series data. In that case, you may want to read each file separately. The <code class="literal">sc.wholeTextFiles</code> method allows you to do that. It creates an RDD with the filename and path (for example, <code class="literal">hdfs://localhost:9000/user/hduser/words</code>) as a key and the content of the whole file as the value.</p><p>Spark also supports reading various serialization and compression-friendly formats such as Avro, Parquet, and JSON using DataFrames. These formats will be covered in coming chapters.</p><p>In this recipe, we will look at how to load data in the Spark shell from HDFS.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec36"/>How to do it...</h2></div></div></div><p>Let's do the <a id="id149" class="indexterm"/>word count, which counts the number of occurrences of<a id="id150" class="indexterm"/> each word. In this recipe, we will load data from HDFS:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Create the <code class="literal">words</code> directory by using the following command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ mkdir words</strong></span>
</pre></div></li><li class="listitem">Change the directory to <code class="literal">words</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cd words</strong></span>
</pre></div></li><li class="listitem">Create the <code class="literal">sh.txt text</code> file and enter <code class="literal">"to be or not to be"</code> in it:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ echo "to be or not to be" &gt; sh.txt</strong></span>
</pre></div></li><li class="listitem">Start the Spark shell:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell</strong></span>
</pre></div></li><li class="listitem">Load the <code class="literal">words</code> directory as the RDD:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val words = sc.textFile("hdfs://localhost:9000/user/hduser/words")</strong></span>
</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note05"/>Note</h3><p>The <code class="literal">sc.textFile</code> method also supports passing an additional argument for the number of partitions. By default, Spark creates one partition for each <code class="literal">InputSplit</code> class, which roughly corresponds to one block.</p><p>You can ask for a higher number of partitions. It works really well for compute-intensive jobs such as in machine learning. As one partition cannot contain more than one block, having fewer partitions than blocks is not allowed.</p></div></div></li><li class="listitem">Count the number of lines (the result will be <code class="literal">1</code>):<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; words.count</strong></span>
</pre></div></li><li class="listitem">Divide the line (or lines) into multiple words:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val wordsFlatMap = words.flatMap(_.split("\\W+"))</strong></span>
</pre></div></li><li class="listitem">Convert word to (word,1)—that is, output <code class="literal">1</code> as a value for each occurrence of <code class="literal">word</code> as a key:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val wordsMap = wordsFlatMap.map( w =&gt; (w,1))</strong></span>
</pre></div></li><li class="listitem">Use the <code class="literal">reduceByKey</code> method to add the number of occurrences of each word as a key (this function works on two consecutive values at a time, represented by <code class="literal">a</code> and <code class="literal">b</code>):<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val wordCount = wordsMap.reduceByKey( (a,b) =&gt; (a+b))</strong></span>
</pre></div></li><li class="listitem">Print the RDD:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; wordCount.collect.foreach(println)</strong></span>
</pre></div></li><li class="listitem">Doing all of the preceding operations in one step is as follows:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; sc.textFile("hdfs://localhost:9000/user/hduser/words"). flatMap(_.split("\\W+")).map( w =&gt; (w,1)). reduceByKey( (a,b) =&gt; (a+b)).foreach(println)</strong></span>
</pre></div></li></ol></div><p>This<a id="id151" class="indexterm"/> gives<a id="id152" class="indexterm"/> the following output:</p><div class="mediaobject"><img src="graphics/B03056_03_01.jpg" alt="How to do it..."/></div></div><div class="section" title="There's more…"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec37"/>There's more…</h2></div></div></div><p>Sometimes we need to access the whole file at once. Sometimes the filename contains useful data like in the case of time-series. Sometimes you need to process more than one line as a record. <code class="literal">sparkContext.wholeTextFiles</code> comes to the rescue here. We will look at weather dataset from <a class="ulink" href="ftp://ftp.ncdc.noaa.gov/pub/data/noaa/">ftp://ftp.ncdc.noaa.gov/pub/data/noaa/</a>.</p><p>Here's what a top-level directory looks like:</p><div class="mediaobject"><img src="graphics/B03056_03_02.jpg" alt="There's more…"/></div><p>Looking into a particular year directory—for example, 1901 resembles the following screenshot:</p><div class="mediaobject"><img src="graphics/B03056_03_03.jpg" alt="There's more…"/></div><p>Data here is <a id="id153" class="indexterm"/>divided in such a way that each filename contains useful <a id="id154" class="indexterm"/>information, that is, USAF-WBAN-year, where USAF is the US air force station number and WBAN is the weather bureau army navy location number.</p><p>You will also notice that all files are compressed as gzip with a <code class="literal">.gz</code> extension. Compression is handled automatically so all you need to do is to upload data in HDFS. We will come back to this dataset in the coming chapters.</p><p>Since the whole dataset is not large, it can be uploaded in HDFS in the pseudo-distributed mode also:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Download data:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ wget -r ftp://ftp.ncdc.noaa.gov/pub/data/noaa/</strong></span>
</pre></div></li><li class="listitem">Load the weather data in HDFS:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hdfs dfs -put ftp.ncdc.noaa.gov/pub/data/noaa weather/</strong></span>
</pre></div></li><li class="listitem">Start the Spark shell:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell</strong></span>
</pre></div></li><li class="listitem">Load weather data for 1901 in the RDD:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val weatherFileRDD = sc.wholeTextFiles("hdfs://localhost:9000/user/hduser/weather/1901")</strong></span>
</pre></div></li><li class="listitem">Cache weather in the RDD so that it is not recomputed every time it's accessed:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val weatherRDD = weatherFileRDD.cache</strong></span>
</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note06"/>Note</h3><p>In Spark, there are various StorageLevels at which the RDD can be persisted. <code class="literal">rdd.cache</code> is a shorthand for the <code class="literal">rdd.persist(MEMORY_ONLY)</code> StorageLevel.</p></div></div></li><li class="listitem">Count the<a id="id155" class="indexterm"/> number of elements:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; weatherRDD.count</strong></span>
</pre></div></li><li class="listitem">Since the <a id="id156" class="indexterm"/>whole contents of a file are loaded as an element, we need to manually interpret the data, so let's load the first element:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val firstElement = weatherRDD.first</strong></span>
</pre></div></li><li class="listitem">Read the value of the first RDD:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val firstValue = firstElement._2</strong></span>
</pre></div><p>The <code class="literal">firstElement</code> contains tuples in the form (string, string). Tuples can be accessed in two ways:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Using a positional function starting with <code class="literal">_1</code>.</li><li class="listitem" style="list-style-type: disc">Using the <code class="literal">productElement</code> method, for example, <code class="literal">tuple.productElement(0)</code>. Indexes here start with <code class="literal">0</code> like most other methods.</li></ul></div></li><li class="listitem">Split <code class="literal">firstValue</code> by lines:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val firstVals = firstValue.split("\\n")</strong></span>
</pre></div></li><li class="listitem">Count the number of elements in <code class="literal">firstVals</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; firstVals.size</strong></span>
</pre></div></li><li class="listitem">The schema of weather data is very rich with the position of the text working as a delimiter. You can get more information about schemas at the national weather service website. Let's get wind speed, which is from section 66-69 (in meter/sec):<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val windSpeed = firstVals.map(line =&gt; line.substring(65,69)</strong></span>
</pre></div></li></ol></div></div></div>
<div class="section" title="Loading data from HDFS using a custom InputFormat"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec26"/>Loading data from HDFS using a custom InputFormat</h1></div></div></div><p>Sometimes<a id="id157" class="indexterm"/> you need to load data in a <a id="id158" class="indexterm"/>specific format and <code class="literal">TextInputFormat</code> is <a id="id159" class="indexterm"/>not a good fit for that. Spark provides two methods for this purpose:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">sparkContext.hadoopFile</code>: This supports the old MapReduce API</li><li class="listitem" style="list-style-type: disc"><code class="literal">sparkContext.newAPIHadoopFile</code>: This supports the new MapReduce API</li></ul></div><p>These two<a id="id160" class="indexterm"/> methods provide support for <a id="id161" class="indexterm"/>all of Hadoop's built-in InputFormats <a id="id162" class="indexterm"/>interfaces as well as any custom <code class="literal">InputFormat</code>.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec38"/>How to do it...</h2></div></div></div><p>We are going to load text data in key-value format and load it in Spark using <code class="literal">KeyValueTextInputFormat</code>:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Create the <code class="literal">currency</code> directory by using the following command:<div class="informalexample"><pre class="programlisting">$ mkdir currency</pre></div></li><li class="listitem">Change the current directory to <code class="literal">currency</code>:<div class="informalexample"><pre class="programlisting">$ cd currency</pre></div></li><li class="listitem">Create the <code class="literal">na.txt</code> text file and enter currency values in key-value format delimited by tab (key: country, value: currency):<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ vi na.txt</strong></span>
<span class="strong"><strong>United States of America        US Dollar</strong></span>
<span class="strong"><strong>Canada  Canadian Dollar</strong></span>
<span class="strong"><strong>Mexico  Peso</strong></span>
</pre></div><p>You can create more files for each continent.</p></li><li class="listitem">Upload the <code class="literal">currency</code> folder to HDFS:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hdfs dfs -put currency /user/hduser/currency</strong></span>
</pre></div></li><li class="listitem">Start the Spark shell:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell</strong></span>
</pre></div></li><li class="listitem">Import statements:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.hadoop.io.Text</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat</strong></span>
</pre></div></li><li class="listitem">Load the <code class="literal">currency</code> directory as the RDD:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>val currencyFile = sc.newAPIHadoopFile("hdfs://localhost:9000/user/hduser/currency",classOf[KeyValueTextInputFormat],classOf[Text],classOf[Text])</strong></span>
</pre></div></li><li class="listitem">Convert it from tuple of (Text,Text) to tuple of (String,String):<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>val currencyRDD = currencyFile.map( t =&gt; (t._1.toString,t._2.toString))</strong></span>
</pre></div></li><li class="listitem">Count the number of elements in the RDD:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; currencyRDD.count</strong></span>
</pre></div></li><li class="listitem">Print the values:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; currencyRDD.collect.foreach(println)</strong></span>
</pre></div><div class="mediaobject"><img src="graphics/B03056_03_04.jpg" alt="How to do it..."/></div></li></ol></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note07"/>Note</h3><p>You<a id="id163" class="indexterm"/> can use this approach <a id="id164" class="indexterm"/>to load data<a id="id165" class="indexterm"/> in any Hadoop-supported <code class="literal">InputFormat</code> interface.</p></div></div></div></div>
<div class="section" title="Loading data from Amazon S3"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec27"/>Loading data from Amazon S3</h1></div></div></div><p>Amazon <span class="strong"><strong>Simple Storage Service</strong></span> (<span class="strong"><strong>S3</strong></span>) provides developers and IT teams with a secure, durable, and <a id="id166" class="indexterm"/>scalable storage platform. The biggest advantage<a id="id167" class="indexterm"/> of Amazon S3 is that there is no up-front<a id="id168" class="indexterm"/> IT investment and companies can build capacity (just by clicking a button a button) as they need.</p><p>Though Amazon S3 can be used with any compute platform, it integrates really well with Amazon's cloud services such <a id="id169" class="indexterm"/>as Amazon <span class="strong"><strong>Elastic Compute Cloud</strong></span> (<span class="strong"><strong>EC2</strong></span>) and Amazon <span class="strong"><strong>Elastic Block Storage</strong></span> (<span class="strong"><strong>EBS</strong></span>). For this <a id="id170" class="indexterm"/>reason, companies who use <span class="strong"><strong>Amazon Web Services</strong></span> (<span class="strong"><strong>AWS</strong></span>) are likely to have significant data is already stored on <a id="id171" class="indexterm"/>Amazon S3.</p><p>This makes a good case for loading data in Spark from Amazon S3 and that is exactly what this recipe is about.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec39"/>How to do it...</h2></div></div></div><p>Let's start with the <a id="id172" class="indexterm"/>AWS portal:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Go to <a class="ulink" href="http://aws.amazon.com">http://aws.amazon.com</a> and log in with your username and password.</li><li class="listitem">Once logged in, navigate to <span class="strong"><strong>Storage &amp; Content Delivery</strong></span> | <span class="strong"><strong>S3</strong></span> | <span class="strong"><strong>Create Bucket</strong></span>:<div class="mediaobject"><img src="graphics/B03056_03_05.jpg" alt="How to do it..."/></div></li><li class="listitem">Enter<a id="id173" class="indexterm"/> the bucket name—for example, <code class="literal">com.infoobjects.wordcount</code>. Please make sure you enter a unique bucket name (no two S3 buckets can have the same name globally).</li><li class="listitem">Select <span class="strong"><strong>Region</strong></span>, click on <span class="strong"><strong>Create</strong></span>, and then on the bucket name you created and you <a id="id174" class="indexterm"/>will see the following screen:<div class="mediaobject"><img src="graphics/B03056_03_06.jpg" alt="How to do it..."/></div></li><li class="listitem">Click on <span class="strong"><strong>Create Folder</strong></span> and enter <code class="literal">words</code> as the folder name.</li><li class="listitem">Create the <code class="literal">sh.txt</code> text file on the local filesystem:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ echo "to be or not to be" &gt; sh.txt</strong></span>
</pre></div></li><li class="listitem">Navigate to <span class="strong"><strong>Words</strong></span> | <span class="strong"><strong>Upload</strong></span> | <span class="strong"><strong>Add Files</strong></span> and choose <code class="literal">sh.txt</code> from the dialog box, as shown in the following screenshot:<div class="mediaobject"><img src="graphics/B03056_03_07.jpg" alt="How to do it..."/></div></li><li class="listitem">Click <a id="id175" class="indexterm"/>on <span class="strong"><strong>Start Upload</strong></span>.</li><li class="listitem">Select <span class="strong"><strong>sh.txt</strong></span> and<a id="id176" class="indexterm"/> click on <span class="strong"><strong>Properties</strong></span> and it will show you details of the file:<div class="mediaobject"><img src="graphics/B03056_03_08.jpg" alt="How to do it..."/></div></li><li class="listitem">Set <code class="literal">AWS_ACCESS_KEY</code> and <code class="literal">AWS_SECRET_ACCESS_KEY</code> as environment variables.</li><li class="listitem">Open the Spark shell and load the <code class="literal">words</code> directory from <code class="literal">s3</code> in the <code class="literal">words</code> RDD:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt;  val words = sc.textFile("s3n://com.infoobjects.wordcount/words")</strong></span>
</pre></div></li></ol></div><p>Now the RDD is<a id="id177" class="indexterm"/> loaded and you can continue doing regular<a id="id178" class="indexterm"/> transformations and actions on the RDD.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note08"/>Note</h3><p>Sometimes there is <a id="id179" class="indexterm"/>confusion between <code class="literal">s3://</code> and <code class="literal">s3n://</code>. <code class="literal">s3n://</code> means a<a id="id180" class="indexterm"/> regular file sitting in the S3 bucket but readable and writable by the outside world. This filesystem puts a 5 GB limit on the file size.</p><p><code class="literal">s3://</code> means an HDFS file sitting in the S3 bucket. It is a block-based filesystem. The filesystem requires you to dedicate a bucket for this filesystem. There is no limit on file size in this system.</p></div></div></div></div>
<div class="section" title="Loading data from Apache Cassandra"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec28"/>Loading data from Apache Cassandra</h1></div></div></div><p>Apache Cassandra <a id="id181" class="indexterm"/>is a NoSQL database with a masterless<a id="id182" class="indexterm"/> ring cluster structure. While HDFS is a good fit for <a id="id183" class="indexterm"/>streaming data access, it does not work well with random access. For example, HDFS will work well when your average file size is 100 MB and you want to read the whole file. If you frequently access the <span class="emphasis"><em>n</em></span>th line in a file or some other part as a record, HDFS would be too slow.</p><p>Relational databases have traditionally provided a solution to that, providing low latency, random access, but they do not work well with big data. NoSQL databases such as Cassandra fill the gap by providing relational database type access but in a distributed architecture on commodity servers.</p><p>In this recipe, we will load data from Cassandra as a Spark RDD. To make that happen Datastax, the company behind Cassandra, has contributed <code class="literal">spark-cassandra-connector</code>. This connector lets you load Cassandra tables as Spark RDDs, write Spark RDDs back to Cassandra, and execute CQL queries.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec40"/>How to do it...</h2></div></div></div><p>Perform the following steps to load data from Cassandra:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Create a keyspace named <code class="literal">people</code> in Cassandra using the CQL shell:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>cqlsh&gt; CREATE KEYSPACE people WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1 };</strong></span>
</pre></div></li><li class="listitem">Create a column family (from CQL 3.0 onwards, it can also be called a <span class="strong"><strong>table</strong></span>) <code class="literal">person</code> in newer versions of Cassandra:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>cqlsh&gt; create columnfamily person(id int primary key,first_name varchar,last_name varchar);</strong></span>
</pre></div></li><li class="listitem">Insert a few records in the column family:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>cqlsh&gt; insert into person(id,first_name,last_name) values(1,'Barack','Obama');</strong></span>
<span class="strong"><strong>cqlsh&gt; insert into person(id,first_name,last_name) values(2,'Joe','Smith');</strong></span>
</pre></div></li><li class="listitem">Add<a id="id184" class="indexterm"/> Cassandra connector dependency to SBT:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>"com.datastax.spark" %% "spark-cassandra-connector" % 1.2.0</strong></span>
</pre></div></li><li class="listitem">You can<a id="id185" class="indexterm"/> also add the Cassandra dependency to Maven:<div class="informalexample"><pre class="programlisting">&lt;dependency&gt;
  &lt;groupId&gt;com.datastax.spark&lt;/groupId&gt;
  &lt;artifactId&gt;spark-cassandra-connector_2.10&lt;/artifactId&gt;
  &lt;version&gt;1.2.0&lt;/version&gt;
&lt;/dependency&gt;</pre></div><p>Alternatively, you can also download the <code class="literal">spark-cassandra-connector</code> JAR to use directly with the Spark shell:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ wget http://central.maven.org/maven2/com/datastax/spark/spark-cassandra-connector_2.10/1.1.0/spark-cassandra-connector_2.10-1.2.0.jar</strong></span>
</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note09"/>Note</h3><p>If you would like to build the <code class="literal">uber</code> JAR with all dependencies, refer to the <span class="emphasis"><em>There's more…</em></span> section.</p></div></div></li><li class="listitem">Now start the Spark shell.</li><li class="listitem">Set the <code class="literal">spark.cassandra.connection.host</code> property in the Spark shell:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; sc.getConf.set("spark.cassandra.connection.host", "localhost")</strong></span>
</pre></div></li><li class="listitem">Import Cassandra-specific libraries:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import com.datastax.spark.connector._</strong></span>
</pre></div></li><li class="listitem">Load the <code class="literal">person</code> column family as an RDD:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val personRDD = sc.cassandraTable("people","person")</strong></span>
</pre></div></li><li class="listitem">Count the number of records in the RDD:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; personRDD.count</strong></span>
</pre></div></li><li class="listitem">Print data in the RDD:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; personRDD.collect.foreach(println)</strong></span>
</pre></div></li><li class="listitem">Retrieve the first row:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val firstRow = personRDD.first</strong></span>
</pre></div></li><li class="listitem">Get the column names:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; firstRow.columnNames</strong></span>
</pre></div></li><li class="listitem">Cassandra <a id="id186" class="indexterm"/>can also be accessed through <a id="id187" class="indexterm"/>Spark SQL. It has a wrapper around <code class="literal">SQLContext</code> called <code class="literal">CassandraSQLContext</code>; let's load it:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val cc = new org.apache.spark.sql.cassandra.CassandraSQLContext(sc)</strong></span>
</pre></div></li><li class="listitem">Load the <code class="literal">person</code> data as <code class="literal">SchemaRDD</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val p = cc.sql("select * from people.person")</strong></span>
</pre></div></li><li class="listitem">Retrieve the <code class="literal">person</code> data:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; p.collect.foreach(println)</strong></span>
</pre></div></li></ol></div></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec41"/>There's more...</h2></div></div></div><p>Spark Cassandra's connector library has a lot of dependencies. The connector itself and several of its<a id="id188" class="indexterm"/> dependencies are third-party to Spark and are not available as part of the Spark installation.</p><p>These dependencies need to be made available to the driver as well as executors at runtime. One way to do this is to bundle all transitive dependencies, but that is a laborious and error-prone process. The recommended approach is to bundle all the dependencies along with the connector library. This will result in a fat JAR, popularly known as the <code class="literal">uber</code> JAR.</p><p>SBT provides the <code class="literal">sbt-assembly</code> plugin, which makes creating <code class="literal">uber</code> JARs very easy. The following are the steps to create an <code class="literal">uber</code> JAR for <code class="literal">spark-cassandra-connector</code>. These steps are general enough so that you can use them to create any <code class="literal">uber</code> JAR:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Create a folder named <code class="literal">uber</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ mkdir uber</strong></span>
</pre></div></li><li class="listitem">Change the directory to <code class="literal">uber</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cd uber</strong></span>
</pre></div></li><li class="listitem">Open the SBT prompt:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sbt</strong></span>
</pre></div></li><li class="listitem">Give this project a name <code class="literal">sc-uber</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; set name := "sc-uber"</strong></span>
</pre></div></li><li class="listitem">Save the session:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; session save</strong></span>
</pre></div></li><li class="listitem">Exit the session:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; exit</strong></span>
</pre></div><p>This will create <code class="literal">build.sbt</code>, <code class="literal">project</code>, and <code class="literal">target</code> folders in the <code class="literal">uber</code> folder as shown in the following screenshot:</p><div class="mediaobject"><img src="graphics/B03056_03_09.jpg" alt="There's more..."/></div></li><li class="listitem">Add the <code class="literal">spark-cassandra-driver</code> dependency to <code class="literal">build.sbt</code> at the end after<a id="id189" class="indexterm"/> leaving a blank line as shown in the <a id="id190" class="indexterm"/>following screenshot:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ vi buid.sbt</strong></span>
</pre></div><div class="mediaobject"><img src="graphics/B03056_03_10.jpg" alt="There's more..."/></div></li><li class="listitem">We will use <code class="literal">MergeStrategy.first</code> as the default. Besides that, there are some files, such as <code class="literal">manifest.mf</code>, that every JAR bundles for metadata, and we can simply discard them. We are going to use <code class="literal">MergeStrategy.discard</code> for that. The following is the screenshot of <code class="literal">build.sbt</code> with <code class="literal">assemblyMergeStrategy</code> added:<div class="mediaobject"><img src="graphics/B03056_03_11.jpg" alt="There's more..."/></div></li><li class="listitem">Now create <code class="literal">plugins.sbt</code> in the <code class="literal">project</code> folder and type the following for the <code class="literal">sbt-assembly</code> plugin:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.12.0")</strong></span>
</pre></div></li><li class="listitem">We are ready to build (<code class="literal">assembly</code>) a JAR now:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ sbt assembly</strong></span>
</pre></div><p>The <code class="literal">uber</code> JAR is now created in <code class="literal">target/scala-2.10/sc-uber-assembly-0.1-SNAPSHOT.jar</code>.</p></li><li class="listitem">Copy it <a id="id191" class="indexterm"/>to a suitable location where you keep<a id="id192" class="indexterm"/> all third-party JARs—for example, <code class="literal">/home/hduser/thirdparty</code>—and rename it to an easier name (unless you like longer names):<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ mv thirdparty/sc-uber-assembly-0.1-SNAPSHOT.jar  thirdparty/sc-uber.jar</strong></span>
</pre></div></li><li class="listitem">Load the Spark shell with the <code class="literal">uber</code> JAR using <code class="literal">--jars</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell --jars thirdparty/sc-uber.jar</strong></span>
</pre></div></li><li class="listitem">To submit the Scala code to a cluster, you can call <code class="literal">spark-submit</code> with the same JARS option:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-submit --jars thirdparty/sc-uber.jar</strong></span>
</pre></div></li></ol></div><div class="section" title="Merge strategies in sbt-assembly"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec01"/>Merge strategies in sbt-assembly</h3></div></div></div><p>If multiple JARs have files with the same name and the same relative path, the default merge strategy for the <code class="literal">sbt-assembly</code> plugin is to verify that content is same for all the files and error out otherwise. This strategy is called <code class="literal">MergeStrategy.deduplicate</code>.</p><p>The following <a id="id193" class="indexterm"/>are the available merge strategies in the <code class="literal">sbt-assembly plugin</code>:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Strategy name</p>
</th><th style="text-align: left" valign="bottom">
<p>Description</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p><code class="literal">MergeStrategy.deduplicate</code></p>
</td><td style="text-align: left" valign="top">
<p>The default strategy</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">MergeStrategy.first</code></p>
</td><td style="text-align: left" valign="top">
<p>Picks first file according to classpath</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">MergeStrategy.last</code></p>
</td><td style="text-align: left" valign="top">
<p>Picks last file according to classpath</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">MergeStrategy.singleOrError</code></p>
</td><td style="text-align: left" valign="top">
<p>Errors out (merge conflict not expected)</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">MergeStrategy.concat</code></p>
</td><td style="text-align: left" valign="top">
<p>Concatenates all matching files together</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">MergeStrategy.filterDistinctLines</code></p>
</td><td style="text-align: left" valign="top">
<p>Concatenates leaving out duplicates</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">MergeStrategy.rename</code></p>
</td><td style="text-align: left" valign="top">
<p>Renames files</p>
</td></tr></tbody></table></div></div></div></div>
<div class="section" title="Loading data from relational databases"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec29"/>Loading data from relational databases</h1></div></div></div><p>A lot <a id="id194" class="indexterm"/>of important data lies in relational databases<a id="id195" class="indexterm"/> that Spark needs to query. JdbcRDD is a Spark feature that allows relational tables to be loaded as RDDs. This recipe will explain how to<a id="id196" class="indexterm"/> use JdbcRDD.</p><p>Spark SQL to be introduced in the next chapter includes a data source for JDBC. This should be preferred over the current recipe as results are returned as DataFrames (to be introduced in the next chapter), which can be easily processed by Spark SQL and also joined with other data sources.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec42"/>Getting ready</h2></div></div></div><p>Please make sure that the JDBC driver JAR is visible on the client node and all slaves nodes on which executor will run.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec43"/>How to do it...</h2></div></div></div><p>Perform the<a id="id197" class="indexterm"/> following steps to load data from relational<a id="id198" class="indexterm"/> databases:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Create a table named <code class="literal">person</code> in MySQL using the following DDL:<div class="informalexample"><pre class="programlisting">CREATE TABLE 'person' (
  'person_id' int(11) NOT NULL AUTO_INCREMENT,
  'first_name' varchar(30) DEFAULT NULL,
  'last_name' varchar(30) DEFAULT NULL,
  'gender' char(1) DEFAULT NULL,
  PRIMARY KEY ('person_id');
)</pre></div></li><li class="listitem">Insert some data:<div class="informalexample"><pre class="programlisting">Insert into person values('Barack','Obama','M');
Insert into person values('Bill','Clinton','M');
Insert into person values('Hillary','Clinton','F');</pre></div></li><li class="listitem">Download <code class="literal">mysql-connector-java-x.x.xx-bin.jar</code> from <a class="ulink" href="http://dev.mysql.com/downloads/connector/j/">http://dev.mysql.com/downloads/connector/j/</a>.</li><li class="listitem">Make the MySQL driver available to the Spark shell and launch it:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell --jars /path-to-mysql-jar/mysql-connector-java-5.1.29-bin.jar</strong></span>
</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note10"/>Note</h3><p>Please note that <code class="literal">path-to-mysql-jar</code> is not the actual path name. You should use the actual path name.</p></div></div></li><li class="listitem">Create variables for the username, password, and JDBC URL:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val url="jdbc:mysql://localhost:3306/hadoopdb"</strong></span>
<span class="strong"><strong>scala&gt; val username = "hduser"</strong></span>
<span class="strong"><strong>scala&gt; val password = "******"</strong></span>
</pre></div></li><li class="listitem">Import JdbcRDD:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.rdd.JdbcRDD</strong></span>
</pre></div></li><li class="listitem">Import JDBC-related classes:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import java.sql.{Connection, DriverManager, ResultSet}</strong></span>
</pre></div></li><li class="listitem">Create an instance of the JDBC driver:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; Class.forName("com.mysql.jdbc.Driver").newInstance</strong></span>
</pre></div></li><li class="listitem">Load JdbcRDD:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val myRDD = new JdbcRDD( sc, () =&gt;</strong></span>
<span class="strong"><strong>DriverManager.getConnection(url,username,password) ,</strong></span>
<span class="strong"><strong>"select first_name,last_name,gender from person limit ?, ?",</strong></span>
<span class="strong"><strong>1, 5, 2, r =&gt; r.getString("last_name") + ", " + r.getString("first_name"))</strong></span>
</pre></div></li><li class="listitem">Now query the results:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; myRDD.count</strong></span>
<span class="strong"><strong>scala&gt; myRDD.foreach(println)</strong></span>
</pre></div></li><li class="listitem">Save the RDD to HDFS:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; myRDD.saveAsTextFile("hdfs://localhost:9000/user/hduser/person")</strong></span>
</pre></div></li></ol></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec44"/>How it works…</h2></div></div></div><p>JdbcRDD is <a id="id199" class="indexterm"/>an RDD that executes a SQL query on a <a id="id200" class="indexterm"/>JDBC connection and retrieves the results. The <a id="id201" class="indexterm"/>following is a JdbcRDD constructor:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>JdbcRDD( SparkContext, getConnection: () =&gt; Connection,</strong></span>
<span class="strong"><strong>sql: String, lowerBound: Long, upperBound: Long,</strong></span>
<span class="strong"><strong>numPartitions: Int,  mapRow: (ResultSet) =&gt; T =</strong></span>
<span class="strong"><strong> JdbcRDD.resultSetToObjectArray)</strong></span>
</pre></div><p>The two ?'s are bind variables for a prepared statement inside JdbcRDD. The first ? is for the offset (lower bound), that is, which row should we start computing with, the second ? is for the limit (upper bound), that is, how many rows should we read.</p><p>JdbcRDD is a great way to load data in Spark directly from relational databases on an ad-hoc basis. If you would like to load data in bulk from RDBMS, there are other approaches that would work better, for example, Apache Sqoop is a powerful tool that imports and exports data from relational databases to HDFS.</p></div></div></body></html>