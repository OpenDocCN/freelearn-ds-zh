["```py\n$ docker ps\n```", "```py\n# Remote logging configuration\nAIRFLOW__LOGGING__REMOTE_LOGGING: \"True\"\n```", "```py\n    from airflow import DAG\n    from airflow.operators.python_operator import PythonOperator\n    from datetime import datetime, timedelta\n    import logging\n    ```", "```py\n    # Defining the log configuration\n    logger = logging.getLogger(\"airflow.task\")\n    ```", "```py\n    default_args = {\n        'owner': 'airflow',\n        'depends_on_past': False,\n        'start_date': datetime(2023, 4, 1),\n        'retries': 1,\n        'retry_delay': timedelta(minutes=5)\n    }\n    dag = DAG(\n        'basic_logging_dag',\n        default_args=default_args,\n        description='A simple ETL job using Python and Airflow',\n        schedule_interval=timedelta(days=1),\n    )\n    ```", "```py\n    def extract_data():\n        logger.info(\"Let's extract data\")\n        pass\n    def transform_data():\n        logger.info(\"Then transform data\")\n        pass\n    def load_data():\n        logger.info(\"Finally load data\")\n        logger.error(\"Oh, where is the data?\")\n        pass\n    ```", "```py\n    extract_task = PythonOperator(\n        task_id='extract_data',\n        python_callable=extract_data,\n        dag=dag,\n    )\n    transform_task = PythonOperator(\n        task_id='transform_data',\n        python_callable=transform_data,\n        dag=dag,\n    )\n    load_task = PythonOperator(\n        task_id='load_data',\n        python_callable=load_data,\n        dag=dag,\n    )\n    extract_task >> transform_task >> load_task\n    ```", "```py\n# Formatting for how airflow generates file names/paths for each task run.\nlog_filename_template = dag_id={{ ti.dag_id }}/run_id={{ ti.run_id }}/task_id={{ ti.task_id }}/{%% if ti.map_index >= 0 %%}map_index={{ ti.map_index }}/{%% endif %%}attempt={{ try_number }}.log\n```", "```py\n    logger = logging.getLogger(\"airflow.task\")\n    ```", "```py\n{\"aws_access_key_id\": \"your_key\", \"aws_secret_access_key\": \"your_secret\"}\n```", "```py\n    AIRFLOW__LOGGING__REMOTE_LOGGING: \"True\"\n    AIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER: \"s3://airflow-cookbook\"\n    AIRFLOW__LOGGING__REMOTE_LOG_CONN_ID: conn_s3\n    AIRFLOW__LOGGING__ENCRYPT_S3_LOGS: \"False\"\n    ```", "```py\n[logging]\n# Users must supply a remote location URL (starting with either 's3://...') and an Airflow connection\n# id that provides access to the storage location.\nremote_logging = True\nremote_base_log_folder = s3://airflow-cookbook\nremote_log_conn_id = conn_s3\n# Use server-side encryption for logs stored in S3\nencrypt_s3_logs = False\n```", "```py\nAIRFLOW__LOGGING__REMOTE_LOGGING: \"True\"\nAIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER: \"s3://airflow-cookbook\"\nAIRFLOW__LOGGING__REMOTE_LOG_CONN_ID: conn_s3\nAIRFLOW__LOGGING__ENCRYPT_S3_LOGS: \"False\"\n```", "```py\n    AIRFLOW__LOGGING__LOG_FORMAT: \"[%(asctime)s] [ %(process)s - %(name)s ] {%(filename)s:%(lineno)d} %(levelname)s - %(message)s\"\n    ```", "```py\nlog_format = [%%(asctime)s] [ %%(process)s - %%(name)s ] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s\n```", "```py\n    $ docker-compose stop      # Or press Crtl-C\n    $ docker-compose up\n    ```", "```py\nAIRFLOW__LOGGING__LOG_FORMAT: \"[%(asctime)s] [ %(process)s - %(name)s ] {%(filename)s:%(lineno)d} %(levelname)s - %(message)s\"\n```", "```py\npip install 'apache-airflow[statsd]'\n```", "```py\n        # SMTP settings\n        AIRFLOW__SMTP__SMTP_HOST: \"smtp.gmail.com\"\n        AIRFLOW__SMTP__SMTP_USER: \"your_email_here\"\n        AIRFLOW__SMTP__SMTP_PASSWORD: \"your_app_password_here\"\n        AIRFLOW__SMTP__SMTP_PORT: 587\n    ```", "```py\n[smtp]\n# If you want airflow to send emails on retries, failure, and you want to use\n# the airflow.utils.email.send_email_smtp function, you have to configure an\n# smtp server here\nsmtp_host = smtp.gmail.com\nsmtp_starttls = True\nsmtp_ssl = False\n# Example: smtp_user = airflow\nsmtp_user = your_email_here\n# Example: smtp_password = airflow\nsmtp_password = your_app_password_here\nsmtp_port = 587\nsmtp_mail_from = airflow@example.com\nsmtp_timeout = 30\nsmtp_retry_limit = 5\n```", "```py\n    from airflow.operators.email import EmailOperator\n    ```", "```py\nfrom airflow import DAG\nfrom airflow.operators.python_operator import PythonOperator\nfrom airflow.operators.email import EmailOperator\nfrom datetime import datetime, timedelta\nimport logging\n# basic_logging_dag DAG code\n# ...\n```", "```py\n    # basic_logging_dag DAG imports above this line\n    default_args = {\n        'owner': 'airflow',\n        'depends_on_past': False,\n        'start_date': datetime(2023, 4, 1),\n        'email': ['sample@gmail.com'],\n        'email_on_failure': True,\n        'email_on_retry': True,\n        'retries': 1,\n        'retry_delay': timedelta(minutes=5)\n    }\n    # basic_logging_dag DAG code\n    # …\n    ```", "```py\n    success_task = EmailOperator(\n        task_id=\"success_task\",\n        to= \"g.esppen@gmail.com\",\n        subject=\"The pipeline finished successfully!\",\n        html_content=\"<h2> Hello World! </h2>\",\n        dag=dag\n    )\n    ```", "```py\n    extract_task >> transform_task >> load_task >> success_task\n    ```", "```py\nsuccess_task = EmailOperator(\n    task_id=\"success_task\",\n    to=\"g.esppen@gmail.com\",\n    subject=\"The pipeline finished successfully!\",\n    html_content=\"<h2> Hello World! </h2>\",\n    dag=dag\n)\n```", "```py\ndefault_args = {\n    'owner': 'airflow',\n    'depends_on_past': False,\n    'start_date': datetime(2023, 4, 1),\n    'email': ['sample@gmail.com'],\n    'email_on_failure': True,\n    'email_on_retry': True,\n    'retries': 1,\n    'retry_delay': timedelta(minutes=5)\n}\n```", "```py\nCREATE TABLE customers (\n    customer_id INT PRIMARY KEY,\n    first_name VARCHAR(50),\n    last_name VARCHAR(50),\n    email VARCHAR(100),\n    phone_number VARCHAR(20),\n    address VARCHAR(200),\n    city VARCHAR(50),\n    state VARCHAR(50),\n    country VARCHAR(50),\n    zip_code VARCHAR(20)\n);\n```", "```py\n    id_username_check = SQLColumnCheckOperator(\n            task_id=\"id_username_check\",\n            conn_id= my_conn,\n            table=my_table,\n            column_mapping={\n                \"customer_id\": {\n                    \"null_check\": {\n                        \"equal_to\": 0,\n                        \"tolerance\": 0,\n                    },\n                    \"distinct_check\": {\n                        \"equal_to\": 1,\n                    },\n                },\n                \"first_name\": {\n                    \"null_check\": {\"equal_to\": 0},\n                },\n            }\n    )\n    ```", "```py\n    customer_table_rows_count = SQLTableCheckOperator(\n        task_id=\"customer_table_rows_count\",\n        conn_id= my_conn,\n        table=my_table,\n        checks={\"row_count_check\": {\n                    \"check_statement\": \"COUNT(*) >= 1000\"\n                }\n            }\n    )\n    ```", "```py\n    count_orders_check = SQLColumnCheckOperator(\n        task_id=\"check_columns\",\n        conn_id=my-conn,\n        table=my_table,\n        column_mapping={\n            \"MY_NUM_COL\": {\n                \"min\": {\"geq_to \": 1}\n            }\n        }\n    )\n    ```", "```py\ncolumn_mapping={\n            \"customer_id\": {\n                \"null_check\": {\n                    \"equal_to\": 0,\n                    \"tolerance\": 0,\n                },\n                \"distinct_check\": {\n                    \"equal_to\": 1,\n                },\n            },\n            \"first_name\": {\n                \"null_check\": {\"equal_to\": 0},\n            },\n        }\n```", "```py\n    checks={\"row_count_check\": {\n                \"check_statement\": \"COUNT(*) >= 1000\"\n            }\n        }\n```"]