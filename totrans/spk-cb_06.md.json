["```py\n    $ spark-shell\n\n    ```", "```py\n    Scala> import org.apache.spark.mllib.linalg.{Vectors,Vector}\n\n    ```", "```py\n    scala> val dvPerson = Vectors.dense(160.0,69.0,24.0)\n\n    ```", "```py\n    scala> val svPerson = Vectors.sparse(3,Array(0,1,2),Array(160.0,69.0,24.0))\n\n    ```", "```py\ndef dense(values: Array[Double]): Vector\n```", "```py\ndef sparse(size: Int, indices: Array[Int], values: Array[Double]): Vector\n```", "```py\n    $spark-shell\n\n    ```", "```py\n    scala> import org.apache.spark.mllib.linalg.{Vectors,Vector}\n\n    ```", "```py\n    scala> import org.apache.spark.mllib.regression.LabeledPoint\n\n    ```", "```py\n    scala> val willBuySUV = LabeledPoint(1.0,Vectors.dense(300.0,80,40))\n\n    ```", "```py\n    scala> val willNotBuySUV = LabeledPoint(0.0,Vectors.dense(150.0,60,25))\n\n    ```", "```py\n    scala> val willBuySUV = LabeledPoint(1.0,Vectors.sparse(3,Array(0,1,2),Array(300.0,80,40)))\n\n    ```", "```py\n    scala> val willNotBuySUV = LabeledPoint(0.0,Vectors.sparse(3,Array(0,1,2),Array(150.0,60,25)))\n\n    ```", "```py\n    $vi person_libsvm.txt (libsvm indices start with 1)\n    0  1:150 2:60 3:25\n    1  1:300 2:80 3:40\n\n    ```", "```py\n    $ hdfs dfs -put person_libsvm.txt person_libsvm.txt\n\n    ```", "```py\n    scala> import org.apache.spark.mllib.util.MLUtils\n    scala> import org.apache.spark.rdd.RDD\n\n    ```", "```py\n    scala> val persons = MLUtils.loadLibSVMFile(sc,\"person_libsvm.txt\")\n\n    ```", "```py\n    $spark-shell\n\n    ```", "```py\n    scala> import org.apache.spark.mllib.linalg.{Vectors,Matrix, Matrices}\n\n    ```", "```py\n    scala> val people = Matrices.dense(3,2,Array(150d,60d,25d, 300d,80d,40d))\n\n    ```", "```py\n    scala> val personRDD = sc.parallelize(List(Vectors.dense(150,60,25), Vectors.dense(300,80,40)))\n\n    ```", "```py\n    scala> import org.apache.spark.mllib.linalg.distributed.{IndexedRow, IndexedRowMatrix,RowMatrix, CoordinateMatrix, MatrixEntry}\n\n    ```", "```py\n    scala> val personMat = new RowMatrix(personRDD)\n\n    ```", "```py\n    scala> print(personMat.numRows)\n\n    ```", "```py\n    scala> print(personMat.numCols)\n\n    ```", "```py\n    scala> val personRDD = sc.parallelize(List(IndexedRow(0L, Vectors.dense(150,60,25)), IndexedRow(1L, Vectors.dense(300,80,40))))\n\n    ```", "```py\n    scala> val pirmat = new IndexedRowMatrix(personRDD)\n\n    ```", "```py\n    scala> print(pirmat.numRows)\n\n    ```", "```py\n    scala> print(pirmat.numCols)\n\n    ```", "```py\n    scala> val personMat = pirmat.toRowMatrix\n\n    ```", "```py\n    scala> val meRDD = sc.parallelize(List(\n     MatrixEntry(0,0,150),\n     MatrixEntry(1,0,60),\n    MatrixEntry(2,0,25),\n    MatrixEntry(0,1,300),\n    MatrixEntry(1,1,80),\n    MatrixEntry(2,1,40)\n    ))\n\n    ```", "```py\n    scala> val pcmat = new CoordinateMatrix(meRDD)\n\n    ```", "```py\n    scala> print(pcmat.numRows)\n\n    ```", "```py\n    scala> print(pcmat.numCols)\n\n    ```", "```py\n    $ spark-shell\n\n    ```", "```py\n    scala> import org.apache.spark.mllib.linalg.{Vectors,Vector}\n    scala> import org.apache.spark.mllib.stat.Statistics\n\n    ```", "```py\n    scala> val personRDD = sc.parallelize(List(Vectors.dense(150,60,25), Vectors.dense(300,80,40)))\n\n    ```", "```py\n    scala> val summary = Statistics.colStats(personRDD)\n\n    ```", "```py\n    scala> print(summary.mean)\n\n    ```", "```py\n    scala> print(summary.variance)\n\n    ```", "```py\n    scala> print(summary.numNonzeros)\n\n    ```", "```py\n    scala> print(summary.count)\n\n    ```", "```py\n    scala> print(summary.max)\n\n    ```", "```py\n    $ spark-shell\n\n    ```", "```py\n    scala> import org.apache.spark.mllib.linalg._\n    scala> import org.apache.spark.mllib.stat.Statistics\n\n    ```", "```py\n    scala> val sizes = sc.parallelize(List(2100, 2300, 2046, 4314, 1244, 4608, 2173, 2750, 4010, 1959.0))\n\n    ```", "```py\n    scala> val prices = sc.parallelize(List(1620000 , 1690000, 1400000, 2000000, 1060000, 3830000, 1230000, 2400000, 3380000, 1480000.00))\n\n    ```", "```py\n    scala> val correlation = Statistics.corr(sizes,prices)\n    correlation: Double = 0.8577177736252577 \n\n    ```", "```py\n    scala> val correlation = Statistics.corr(sizes,prices)\n\n    ```", "```py\n    scala> val correlation = Statistics.corr(sizes,prices,\"spearman\")\n\n    ```", "```py\n    $ spark-shell\n\n    ```", "```py\n    scala> import org.apache.spark.mllib.stat.Statistics\n    scala> import org.apache.spark.mllib.linalg.{Vector,Vectors}\n    scala> import org.apache.spark.mllib.linalg.{Matrix, Matrices}\n\n    ```", "```py\n    scala> val dems = Vectors.dense(32.0,41.0)\n\n    ```", "```py\n    scala> val reps= Vectors.dense(28.0,25.0)\n\n    ```", "```py\n    scala> val indies = Vectors.dense(34.0,26.0)\n\n    ```", "```py\n    scala> val dfit = Statistics.chiSqTest(dems)\n    scala> val rfit = Statistics.chiSqTest(reps)\n    scala> val ifit = Statistics.chiSqTest(indies)\n\n    ```", "```py\n    scala> print(dfit)\n    scala> print(rfit)\n    scala> print(ifit)\n\n    ```", "```py\n    scala> val mat = Matrices.dense(2,3,Array(32.0,41.0, 28.0,25.0, 34.0,26.0))\n\n    ```", "```py\n    scala> val in = Statistics.chiSqTest(mat)\n\n    ```", "```py\n    scala> print(in)\n\n    ```", "```py\n    $ spark-shell\n\n    ```", "```py\n    scala> import org.apache.spark.mllib.linalg.{Vector,Vectors}\n    scala> import org.apache.spark.mllib.regression.LabeledPoint\n    scala> import org.apache.spark.ml.classification.LogisticRegression\n\n    ```", "```py\n    scala> val lebron = LabeledPoint(1.0,Vectors.dense(80.0,250.0))\n\n    ```", "```py\n    scala> val tim = LabeledPoint(0.0,Vectors.dense(70.0,150.0))\n\n    ```", "```py\n    scala> val brittany = LabeledPoint(1.0,Vectors.dense(80.0,207.0))\n\n    ```", "```py\n    scala> val stacey = LabeledPoint(0.0,Vectors.dense(65.0,120.0))\n\n    ```", "```py\n    scala> val trainingRDD = sc.parallelize(List(lebron,tim,brittany,stacey))\n\n    ```", "```py\n    scala> val trainingDF = trainingRDD.toDF\n\n    ```", "```py\n    scala> val estimator = new LogisticRegression\n\n    ```", "```py\n    scala> val transformer = estimator.fit(trainingDF)\n\n    ```", "```py\n    scala> val john = Vectors.dense(90.0,270.0)\n\n    ```", "```py\n    scala> val tom = Vectors.dense(62.0,120.0)\n\n    ```", "```py\n    scala> val testRDD = sc.parallelize(List(john,tom))\n\n    ```", "```py\n    scala> case class Feature(v:Vector)\n\n    ```", "```py\n    scala> val featuresRDD = testRDD.map( v => Feature(v))\n\n    ```", "```py\n    scala> val featuresDF = featuresRDD.toDF(\"features\")\n\n    ```", "```py\n    scala> val predictionsDF = transformer.transform(featuresDF)\n\n    ```", "```py\n    scala> predictionsDF.foreach(println)\n\n    ```", "```py\n    scala> val shorterPredictionsDF = predictionsDF.select(\"features\",\"prediction\")\n\n    ```", "```py\n    scala> val playerDF = shorterPredictionsDF.toDF(\"features\",\"isBasketBallPlayer\")\n\n    ```", "```py\n    scala> playerDF.printSchema\n\n    ```"]