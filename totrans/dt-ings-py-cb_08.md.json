["```py\n$ python3 –-version\nPython 3.8.10\n```", "```py\n    import csv\n    import logging\n    logging.basicConfig(filename='our_application.log', level=logging.INFO)\n    ```", "```py\n    def get_csv_first_line (csv_file):\n        logging.info(f\"Starting function to read first line\")\n        try:\n            with open(csv_file, 'r') as file:\n                logging.info(f\" Opening and reading the CSV file\")\n                reader = csv.reader(file)\n                first_row = next(reader)\n            return first_row\n        except Exception as e:\n            logging.error(f\"Error when reading the CSV file: {e}\")\n            raise\n    ```", "```py\n    get_csv_first_line(\"listings.csv\")\n    ```", "```py\nimport logging\nlogging.basicConfig(filename='our_application.log', level=logging.INFO)\n```", "```py\nlogging.info(f\"Starting function to read first line\")\nlogging.info(f\"Opening and reading the CSV file\")\n```", "```py\nlogging.error(f\"Error when reading the CSV file: {e}\")\n```", "```py\nINFO:root:Starting function to read first line\nINFO:root:Reading file\n```", "```py\n$ python3 –-version\nPython 3.8.10\n```", "```py\n    import csv\n    import logging\n    logging.basicConfig(filename='our_application.log', level=logging.DEBUG)\n    ```", "```py\n    logging.debug(f\"Start testing function\")\n    ```", "```py\n    def gets_csv_first_line (csv_file):\n        logging.info(f\"Starting function to read first line\")\n        try:\n            with open(csv_file, 'r') as file:\n                logging.info(f\"Reading file\")\n                reader = csv.reader(file)\n                first_row = next(reader)\n                logging.debug(f\"Finished without problems\")\n            return first_row\n        except Exception as e:\n            logging.debug(f\"Entered into a exception\")\n            logging.error(f\"Error when reading the CSV file: {e}\")\n            logging.critical(f\"This is a critical error, and the application needs to stop!\")\n            raise\n    ```", "```py\n    logging.warning(f\"Starting the function to get the first line of a CSV\")\n    gets_csv_first_line(\"listings.csv\")\n    ```", "```py\nlogging.basicConfig(filename='our_application.log', level=logging.DEBUG)\n```", "```py\nDef gets_csv_first_line(csv_file):\n    logger.debug(f\"Start testing function\")\n    logger.info(f\"Starting function to read first line\")\n    try:\n        with open(csv_file, 'r') as file:\n            logger.info(f\"Reading file\")\n            reader = csv.reader(file)\n            first_row = next(reader)\n            logger.debug(f\"Finished without problems\")\n        return first_row\n    except Exception as e:\n        logger.debug(f\"Entered into a exception\")\n        logger.error(f\"Error when reading the CSV file: {e}\")\n        logger.critical(f\"This is a critical error, and the application needs to stop!\")\n        raise\n```", "```py\n    [loggers]\n    keys=root,data_ingest\n    [handlers]\n    keys=fileHandler, consoleHandler\n    [formatters]\n    keys=logFormatter\n    [logger_root]\n    level=DEBUG\n    handlers=fileHandler\n    [logger_data_ingest]\n    level=DEBUG\n    handlers=fileHandler, consoleHandler\n    qualname=data_ingest\n    propagate=0\n    [handler_consoleHandler]\n    class=StreamHandler\n    level=DEBUG\n    formatter=logFormatter\n    args=(sys.stdout,)\n    [handler_fileHandler]\n    class=FileHandler\n    level=DEBUG\n    formatter=logFormatter\n    args=('data_ingest.log', 'a')\n    [formatter_logFormatter]\n    format=%(asctime)s - %(name)s - %(levelname)s - %(message)s\n    ```", "```py\n    import csv\n    import logging\n    from logging import config\n    # Loading configuration file\n    config.fileConfig(\"logging.conf\")\n    # Creates a log configuration\n    logger = logging.getLogger(\"data_ingest\")\n    def gets_csv_first_line(csv_file):\n        …\n    ```", "```py\n    gets_csv_first_line(\"listings.\"sv\")\n    ```", "```py\n[loggers]\n[handlers]\n[formatters]\n```", "```py\n[logger_root]\n[logger_data_ingest]\n[handler_consoleHandler]\n[handler_fileHandler]\n```", "```py\n(...)\nconfig.fileConfig(\"logging.conf\")\nlogger = logging.getLogger(\"data_ingest\")\n(...)\n```", "```py\n$ python3 –-version\nPython 3.8.10\n```", "```py\n    import os\n    ```", "```py\n    def get_file_size(file_name, s_megabytes=False):\n    ```", "```py\n        file_stats = os.stat(file_name)\n    ```", "```py\n        if s_megabytes:\n            return f\"The file size in megabytes is: {file_stats.st_size / (1024 * 1024)}\"\n        return f\"The file size in bytes is: {file_stats.st_size}\"\n    ```", "```py\n    file_name = \"listings.csv\"\n    get_file_size(file_name)\n    ```", "```py\nfile_stats = os.stat(file_name)\n```", "```py\nfile_stats.st_size\n```", "```py\nfile_stats.st_size / (1024 * 1024)\n```", "```py\nfrom pyspark.serializers import PickleSerializer, AutoBatchedSerializer\ndef _to_java_object_rdd(rdd):\n    \"\"\" Return a JavaRDD of Object by unpickling\n    It will convert each Python object into Java object by Pyrolite, whenever the\n    RDD is serialized in batch or not.\n    \"\"\"\n    rdd = rdd._reserialize(AutoBatchedSerializer(PickleSerializer()))\n    return rdd.ctx._jvm.org.apache.spark.mllib.api.python.SerDe.pythonToJava(rdd._jrdd, True)\ndef estimate_df_size(df):\n    JavaObj = _to_java_object_rdd(df.rdd)\n    nbytes = spark._jvm.org.apache.spark.util.SizeEstimator.estimate(JavaObj)\nreturn nbytes\n```", "```py\nestimate_df_size(df)\n```", "```py\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder \\\n      .master(\"local[1]\") \\\n      .appName(\"chapter8_monitoring\") \\\n      .config(\"spark.executor.memory\", '3g') \\\n      .config(\"spark.executor.cores\", '3') \\\n      .config(\"spark.cores.max\", '3') \\\n      .getOrCreate()\n```", "```py\ndf_json = spark.read.option(\"multiline\",\"true\") \\\n                    .json('github_events.json')\ndf_json.show()\n```", "```py\n    spark\n    ```"]