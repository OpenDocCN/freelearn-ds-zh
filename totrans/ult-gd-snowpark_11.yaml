- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction to Snowpark Container Services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Containers represent a contemporary method for packaging code in diverse languages,
    ensuring seamless portability and consistency across various environments. This
    is particularly true for advanced AI/ML models and comprehensive data-centric
    applications. These modern data products often handle vast volumes of proprietary
    data, presenting challenges in efficiently creating, developing, and scaling workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Developers and data scientists often spend more time managing computing resources
    and clusters than addressing core business challenges. With its unique features,
    Snowpark Container Services offers a seamless solution to this problem. It allows
    applications and **large language models** (**LLMs**) to be executed on containers
    directly within the Snowflake Data Cloud, reducing the time and effort spent on
    resource management. This chapter will help you learn about deploying apps and
    LLMs on containers within Snowpark.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Snowpark Container Services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up Snowpark Container Services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up a Snowpark Container Service job
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying LLMs with Snowpark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To set up the environment, please refer to the technical requirements in the
    previous chapter. Docker Client and Desktop are also required; you can install
    Docker from [https://docs.docker.com/get-docker/](https://huggingface.co/docs/hub/en/security-tokens).
  prefs: []
  type: TYPE_NORMAL
- en: We’ll also be using the Hugging Face API. To obtain the Hugging Face API token,
    sign up at [https://huggingface.co/](https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark).
  prefs: []
  type: TYPE_NORMAL
- en: The supporting materials are available at [https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark](https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark).
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Snowpark Container Services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Snowpark Container Services represents a comprehensive managed container solution
    tailored to facilitate the deployment, management, and scaling of containerized
    applications within the Snowflake environment. Users can experience the convenience
    of executing containerized workloads directly within Snowflake, eliminating the
    need to transfer data outside the Snowflake ecosystem for processing. Snowpark
    Container Services introduces an **Open Container Initiative** (**OCI**) runtime
    execution environment meticulously optimized for Snowflake, which empowers users
    to flawlessly execute OCI images while leveraging the robust capabilities of Snowflake’s
    data platform.
  prefs: []
  type: TYPE_NORMAL
- en: Snowpark Container Services extends Snowpark’s capability, empowering developers
    with a trusted and familiar environment to process non-SQL code seamlessly within
    Snowflake’s governed data domain. This enables applications to effortlessly perform
    tasks such as connecting to Snowflake, executing SQL queries within a Snowflake
    virtual warehouse, accessing data files in a Snowflake stage, and processing data
    with Snowpark models. This streamlined integration fosters an environment conducive
    to efficient collaboration and focused development efforts within teams.
  prefs: []
  type: TYPE_NORMAL
- en: Developers can create containers tailored to their needs that offer configurable
    hardware options, including GPU support, enabling a wide range of AI/ML and application
    workloads within Snowflake through Snowpark. For instance, data science teams
    can expedite ML tasks by leveraging Python libraries for training and inference
    while executing resource-intensive generative AI models such as LLMs. App developers
    can craft and deploy user interfaces using popular frameworks, and data engineers
    can execute optimized logic within the same processing engine handling SQL or
    Python DataFrame operations.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will understand how data security works in Snowpark
    Container Services.
  prefs: []
  type: TYPE_NORMAL
- en: Note on Snowpark Container Services
  prefs: []
  type: TYPE_NORMAL
- en: 'At the time of writing this chapter, Snowpark Container Services are currently
    in a private preview phase. Please note that once they become available to all
    users, there may be slight variations in the API methods compared to what is described
    in this book. We encourage you to monitor the book''s GitHub repository for any
    new changes and updates to the code contents: [https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark](https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark)'
  prefs: []
  type: TYPE_NORMAL
- en: Data security in Snowpark Container Services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Snowpark Container Services facilitates the secure deployment of full-stack
    applications, LLMs, and other advanced data products directly within the data
    environment. This new runtime option under Snowpark streamlines the deployment,
    management, and scaling of containerized workloads, including jobs, services,
    and service functions, leveraging Snowflake-managed infrastructure with customizable
    hardware configurations, such as GPUs. By adopting this innovative runtime, users
    can bypass the complexities of managing compute resources and container clusters,
    allowing seamless integration of sophisticated AI/ML models and applications without
    compromising data security. With containers operating within the Snowflake environment,
    there’s no need to transfer governed data outside of Snowflake, minimizing exposure
    to potential security risks. This ensures a secure and robust ecosystem for leveraging
    internally developed solutions or third-party offerings, such as Snowflake Native
    Apps, accessible through the Snowflake Marketplace.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will look at the components of Snowpark Containers.
  prefs: []
  type: TYPE_NORMAL
- en: Components of Snowpark Containers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Snowpark Container Services offers a streamlined and fully managed approach
    to the life cycle management of containerized applications and AI/ML models. Unlike
    other solutions, it provides a cohesive solution that necessitates piecing together
    disparate components such as container registries, management services, and computing
    platforms. Consolidating these elements eliminates the burden of managing computing
    resources and clusters, thereby accelerating the development and deployment of
    data applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, Snowpark Container Services simplifies container hosting and deployment
    by offering a combination of simplicity and scalability. Developers only need
    to provide their containers, and Snowflake handles the hosting and scaling without
    requiring extensive knowledge of Kubernetes. Developers can interact with the
    service using SQL, CLI, or Python interfaces, catering to diverse preferences
    and workloads. Snowpark Containers has two distinct execution options to accommodate
    various application requirements: services jobs through using service function,
    and compute pools. The following diagram shows the different components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Snowpark Container components](img/B19923_08_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – Snowpark Container components
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at each of the options:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Services**: A service in Snowflake operates continuously, much like a web
    service, until explicitly terminated. These services are hosted on secure ingress
    endpoints and typically host application frontends or APIs. They remain continuously
    available to handle on-demand requests.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Jobs**: These are processes with specific time limits, often initiated manually
    or scheduled regularly. They encompass various tasks, such as launching container
    images for machine learning training on GPUs or executing steps within a data
    pipeline using diverse languages, frameworks, or libraries encapsulated in containers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Service functions**: Functions are time-limited processes designed to receive
    input, execute specific actions, and be triggered repeatedly by events, leveraging
    your containerized environments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Compute pools**: A compute pool comprising one or more **virtual machine**
    (**VM**) nodes serves as the infrastructure upon which Snowflake executes your
    jobs and services.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Snowpark Container Services also enables developers to deploy applications directly
    within their end customers’ Snowflake accounts using the aforementioned components.
    This allows them to securely install and operate state-of-the-art offerings, such
    as hosted notebooks and LLMs, within their Snowflake environment, safeguarding
    the provider’s intellectual property.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will cover how to set up Snowpark Container Services.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up Snowpark Container Services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we’ll lay down the groundwork necessary for exploring Snowpark
    Container Services. We will use Docker to create an OCI-compliant image to deploy
    to Snowpark. We’ll start by creating Snowflake objects.
  prefs: []
  type: TYPE_NORMAL
- en: Creating Snowflake objects
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To create Snowflake objects, follow these steps in Snowsight with the **ACCOUNTADMIN**
    role:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a role named **test_role** using the following command. This role will
    be used for our Snowpark application:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will print the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.2 – A Snowflake role](img/B19923_08_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 – A Snowflake role
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a database and grant access to the database role by running the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will display the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.3 – Granting access](img/B19923_08_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 – Granting access
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be granting access to a warehouse for this role by executing the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will create a security integration for Snowflake services to access
    the resources securely by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.4 – Security integration](img/B19923_08_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 – Security integration
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will bind the service endpoint on the account to this role by running
    the following command. This allows access to the service endpoint from the public
    ingress:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will display the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.5 – Binding the service endpoint](img/B19923_08_5.0.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 – Binding the service endpoint
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we will create a compute pool and assign it to the role by running
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will display the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.6 – A compute pool](img/B19923_08_6.0.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.6 – A compute pool
  prefs: []
  type: TYPE_NORMAL
- en: 'We have now created a role, **test_role**, and the necessary Snowflake objects
    we will use for the container services. Now, grant the role to the user you are
    logged into by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that we have the role configured and ready to use, let’s create the necessary
    database-scoped objects:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Select the database by running the following command:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a schema named **MY_SCHEMA** by running the following code:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Create an image repository that stores the container image by running the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once the image is created, you’ll see the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.7 – An image repository](img/B19923_08_7.0.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.7 – An image repository
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, create a stage that is used to upload the files by running the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You’ll see the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.8 – Stage creation](img/B19923_08_8.0.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.8 – Stage creation
  prefs: []
  type: TYPE_NORMAL
- en: We will be using the HuffPost dataset, which is available on Kaggle. The dataset
    is provided in our code repository. The dataset delineates approximately 200,000
    headlines from 2012 through May 2018, with an additional 10,000 from May 2018
    to 2022, reflecting adjustments in the website’s operational dynamics.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will set up the services.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Flask** is a lightweight web framework that allows developers to easily create
    web applications in Python. It is designed to be flexible, modular, and easy to
    use, making it a popular choice for building web applications of all sizes. Flask
    is particularly well-suited for building small to medium-sized web applications,
    as it provides just enough functionality to get the job done without adding unnecessary
    complexity.'
  prefs: []
  type: TYPE_NORMAL
- en: Flask is used for a wide range of applications, including building web APIs,
    developing microservices, and creating simple web applications. Its flexibility
    and simplicity make it a popular choice for developers who want to quickly prototype
    and deploy web applications. Additionally, Flask can be easily extended with a
    variety of third-party libraries and tools, making it a powerful and versatile
    tool for building web applications in Python.
  prefs: []
  type: TYPE_NORMAL
- en: We will be utilizing Flask to write our service code that runs a persisting
    service to take HTTPS calls.
  prefs: []
  type: TYPE_NORMAL
- en: Note on filter service
  prefs: []
  type: TYPE_NORMAL
- en: Filter service, which we are discussing in the next section, is just one simple
    example since our focus is more on explaining how to set up Snowpark Container
    Services rather than building a complex application. By following similar steps,
    any other use case can be developed.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the filter service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will set up a service called `filter_service`, which filters
    the table based on a unique ID. We will perform the following steps to set up
    the service.
  prefs: []
  type: TYPE_NORMAL
- en: Service code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You’ll find a Python application encompassing the code for crafting the filter
    service on the code repository. To initiate, download the provided zip file to
    a designated directory. Upon download completion, proceed to extract its contents.
    You’ll encounter a `service` directory containing the service code within the
    extracted files. The directory consists of the Docker file, `filter_service.py`,
    and the templates for the UI.
  prefs: []
  type: TYPE_NORMAL
- en: Filter Service in Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following Python script includes the core logic of our service, encapsulating
    a minimalistic HTTP server based on Flask, and designed to filter the table based
    on input. It serves a dual purpose: handling filter requests from Snowflake service
    functions and furnishing a web UI for submitting filter requests.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The filter function facilitates communication between a Snowflake service function
    and the service. This function is adorned with the `@app.post()` decoration, signifying
    its capability to handle HTTP `POST` requests directed to the `/filter` path.
    Upon receiving such requests, the function processes and sends back the filter
    results encapsulated within the request body:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The UI function segment orchestrates a web form presentation and manages filter
    requests submitted via the web form. Decorated with the `@app.route()` decorator,
    this function is designated to handle requests targeting the `/ui` path. Upon
    receiving an HTTP `GET` request for this path, the server delivers a simple HTML
    form prompting the user to input a string. Subsequently, upon form submission,
    an HTTP `POST` request is dispatched, and the server processes it, returning the
    original string encapsulated within an HTTP response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The `readiness_probe` function, adorned with the `@app.get()` decorator, is
    primed to handle requests directed to `/healthcheck`. This function is pivotal
    for Snowflake to verify the service’s readiness. When Snowflake initiates a container,
    it dispatches an HTTP `GET` request to this path as a health probe, ensuring that
    only healthy containers handle incoming traffic. The function’s implementation
    is flexible, accommodating various actions to ascertain the service’s readiness.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will look at the Dockerfile in the directory.
  prefs: []
  type: TYPE_NORMAL
- en: The Dockerfile
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Dockerfile serves as a blueprint for constructing an image using Docker.
    It includes directives on installing the Flask library within the Docker container.
    The Dockerfile consists of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The code within `filter_service.py` relies on Flask to efficiently handle HTTP
    requests.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will examine the UI templates.
  prefs: []
  type: TYPE_NORMAL
- en: UI templates
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The UI template files are located at `/template/basic_ui.html`. They render
    a web form for the filter service’s publicly exposed endpoint. This form is displayed
    when the public endpoint URL is loaded in a web browser with `/ui` appended. Users
    can input a string via this form, and upon submission, the service filters the
    table with submitted row given as string within an HTTP response.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will cover the service function.
  prefs: []
  type: TYPE_NORMAL
- en: The service function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A service function serves as a conduit for communicating with your service.
    A `CREATE FUNCTION` command with specified parameters, such as the `filter_doc_udf`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This function, for instance, accepts a string as input and returns a string,
    with the `SERVICE` property designating the service (`filter_service`) and the
    `ENDPOINT` property specifying the user-friendly endpoint name (`filterendpoint`).
    The `AS '/filter'` designation denotes the path for the service, tying it to the
    corresponding function within `filter_service.py`. Thus, invoking this function
    triggers Snowflake to dispatch a request to the designated path within the service
    container.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will build the Docker image.
  prefs: []
  type: TYPE_NORMAL
- en: Building the Docker image
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will construct the image using the Linux/AMD64 base, which
    is compatible with Snowpark, and dispatch it to your account’s image repository.
    To build the Docker image, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Obtain the repository URL by executing the following SQL command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will display all the image repositories:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.9 – The image repositories](img/B19923_08_9.0.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.9 – The image repositories
  prefs: []
  type: TYPE_NORMAL
- en: The **repository_url** column in the output furnishes the essential URL, and
    the hostname delineated in the repository URL denotes the registry hostname.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following commands require Docker Desktop to be installed in the system.
    You can install it from [https://www.docker.com/products/docker-desktop/](https://www.docker.com/products/docker-desktop/)
    before proceeding with the commands. Next, in the local terminal window, switch
    to the **service** directory containing the unzipped files and execute the subsequent
    **docker build** command using the Docker CLI:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 8.10 – The Docker build command](img/B19923_08_10.0.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.10 – The Docker build command
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll authenticate Docker with Snowflake. To authenticate Docker with
    the Snowflake registry, execute the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Specify your Snowflake username for the username parameter. Docker will prompt
    you for your password. Use the Snowflake password to authenticate:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.11 – Repository login](img/B19923_08_11.0.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 8.11 – Repository login
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, upload the image to the Docker registry by executing the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.12 – Repository push](img/B19923_08_12.0.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.12 – Repository push
  prefs: []
  type: TYPE_NORMAL
- en: The image is now available in the registry for deployment into Container Services.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will examine how to deploy the service, but it is always
    best practice to test your build locally before pushing it to the Snowflake repository.
    This part is not explained in this section as it is beyond the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we’ll guide you through deploying the service and establishing
    a service function to facilitate communication with it. We will start by deploying
    the service, which requires the existing compute pool. Let’s start by checking
    the compute pool by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 8.13 – The compute pool](img/B19923_08_13.0.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.13 – The compute pool
  prefs: []
  type: TYPE_NORMAL
- en: If it’s in the **STARTING** state, you’ll need to wait until it transitions
    to **ACTIVE** or **IDLE**.
  prefs: []
  type: TYPE_NORMAL
- en: Now that the pool is active, we can create the service in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the service
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can create the service by running it using `test_role`. To do that, run
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: We are using the image that we have built to deploy the service. The service
    should be created within Snowflake.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the service is created, you can execute the following SQL command to check
    its status:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should show that the service is running. The information about the
    service can be obtained by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This will display the details, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.14 – Service information](img/B19923_08_14.0.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.14 – Service information
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will create the service function.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a service function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The service function performs the filter function and associates it with an
    endpoint. To create a service function, execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Here, the `SERVICE` property links the UDF with the `filter_service` service,
    while the `ENDPOINT` property associates it with the `filterendpoint` endpoint
    within the service. The `AS '/filter'` specification denotes the HTTP path leading
    to the filter server, which can be located within the service code.
  prefs: []
  type: TYPE_NORMAL
- en: Once the previous SQL statement is executed correctly, you can see the service
    function you created in Snowsight under **Functions**.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.15 – The service function](img/B19923_08_15.0.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.15 – The service function
  prefs: []
  type: TYPE_NORMAL
- en: Now the function is ready to be executed.
  prefs: []
  type: TYPE_NORMAL
- en: Executing the function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will switch to the context we created earlier in the chapter by running
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'You should get the following confirmation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.16 – Function execution](img/B19923_08_16.0.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.16 – Function execution
  prefs: []
  type: TYPE_NORMAL
- en: 'With the context set up, you can initiate communication with the filter service
    by invoking the service function within a query. To call the `filter_doc_udf`
    service function, execute the following `SELECT` statement, providing a sample
    input string (`''122880''`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Upon executing this query, Snowflake dispatches a `POST` request to the service
    endpoint (`filterendpoint`). Upon receiving the request, the service utilizes
    the input string to filter the table for `UNIQUE_ID` and sends back the appropriate
    row in the response, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.17 – The filter function](img/B19923_08_17.0.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.17 – The filter function
  prefs: []
  type: TYPE_NORMAL
- en: 'The service exposes its endpoint publicly but still securely behind the Snowflake
    authentication mechanism, as specified in the inline specification provided within
    the `CREATE SERVICE` command. Consequently, you can access a web UI that the service
    exposes to the internet and send requests to the service from a web browser. To
    find the URL of the public endpoint the service exposes, execute the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'To access the web UI, append `/ui` to the endpoint URL and paste it into the
    web browser. This action triggers the execution of the `ui()` function specified
    in the `filter_service.py` script:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.18 – The service UI](img/B19923_08_18.0.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.18 – The service UI
  prefs: []
  type: TYPE_NORMAL
- en: Please note that the first time you access the endpoint URL, you’ll be prompted
    to log in to Snowflake. Ensure you log in as the same user who created the service
    to guarantee you possess the necessary privileges.
  prefs: []
  type: TYPE_NORMAL
- en: We have successfully deployed the service and the components. In the next section,
    we will look at the service job.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a Snowpark Container Service job
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will create a simple job to connect to a Snowflake table
    and conduct some feature engineering tasks by generating new columns. Subsequently,
    we’ll save the resultant data to the same table within the Snowflake environment.
    Unlike services, jobs are short-lived, providing a one-time execution of tasks.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will set up the container job.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the job
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the job, instead of the Flask server implementation for services, we’ll
    utilize a straightforward `main.py` file to execute the job action. We will perform
    the following steps to set up the job.
  prefs: []
  type: TYPE_NORMAL
- en: Job code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The code for this section is in our GitHub repository under the `chapter_8`
    folder. The folder contains the following files, which are required for the job.
  prefs: []
  type: TYPE_NORMAL
- en: The main.py file
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The `main.py` file is the core Python script for orchestrating the job’s execution.
    At its heart lies the following `run_job()` function, invoked when the script
    is executed. This function plays a pivotal role in reading environment variables
    and utilizing them to set default values for various parameters that are essential
    for connecting to Snowflake.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: While Snowflake automatically populates some parameters when the image runs
    within its environment, explicit provision is required when testing the image
    locally. The `run_job()` function gets a table name and column to perform feature
    engineering from the spec.
  prefs: []
  type: TYPE_NORMAL
- en: The Dockerfile
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Dockerfile encapsulates all the necessary commands required to build an
    image using Docker. This file resembles what we’ve previously implemented in our
    service section, ensuring consistency and coherence across different Snowpark
    Container Services environment components.
  prefs: []
  type: TYPE_NORMAL
- en: The job specification file
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The following job specification file provides Snowflake with essential container
    configuration information. Snowflake leverages the information provided in the
    `my_job_spec.yaml` specification file to configure and execute your job seamlessly.
    In addition to mandatory fields such as `container.name` and `container.image`,
    this specification file includes optional fields such as `container.args`, which
    list the arguments required for job execution.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Notably, the `--query` argument specifies the query to be executed when the
    job runs, while the `--result_table` argument identifies the table where the query
    results will be stored.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will deploy the job.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the job
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To upload your job specification file (`my_job_spec.yaml`) into the Snowflake
    environment, you have a couple of options for uploading it to the designated stage:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Snowsight web interface**: Utilizing the Snowsight web interface offers a
    user-friendly approach to uploading your job specification file. Following the
    instructions we have covered in previous chapters, you can effortlessly navigate
    the process and ensure successful integration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SnowSQL command-line interface (CLI)**: Alternatively, you can use the SnowSQL
    CLI to execute the file upload process by executing the following **PUT** command
    syntax:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 8.19 – Job upload](img/B19923_08_19.0.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.19 – Job upload
  prefs: []
  type: TYPE_NORMAL
- en: Now that the job file has been uploaded, we will execute the job in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Executing the job
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To kick off the execution of a job, you’ll utilize the `EXECUTE SERVICE` command,
    which acts as the catalyst for launching the specified task. Run the following
    command to trigger the job (this command may change since we are in private preview
    at the time of writing):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, you can use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The specified compute pool, `snowpark_cs_compute_pool`, determines the allocation
    of computational resources necessary for the job’s successful execution. The `@snowpark_cs_stage`
    notation denotes the designated stage within Snowflake where the job specification
    file is stored, facilitating seamless access to the required configuration details.
    The `my_job_spec.yaml` file refers to the specific configuration file containing
    the instructions and parameters for executing the job seamlessly. Successful execution
    of the command should display the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.20 – Job execution](img/B19923_08_20.0.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.20 – Job execution
  prefs: []
  type: TYPE_NORMAL
- en: Upon execution, the job performs the specified SQL statement and saves the resultant
    data to a designated table, as outlined within the job specification file (`my_job_spec.yaml`).
    It’s crucial to note that the execution of the SQL statement does not occur within
    the Docker container itself. Instead, the container connects with Snowflake, leveraging
    a Snowflake warehouse to execute the SQL statement efficiently. The `EXECUTE SERVICE`
    command returns the output containing vital information, including the Snowflake-assigned
    **UUID** (short for **Universally Unique Identifier**) of the job. This UUID serves
    as a unique identifier for the executed job, aiding in tracking and monitoring
    its progress and status.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will deploy an LLM for Snowpark Container Services.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying LLMs with Snowpark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Modern enterprises increasingly demand that LLMs be harnessed with proprietary
    data. Open source and proprietary models play pivotal roles in enabling this transition.
    However, the main challenge is finding a robust platform capable of effectively
    leveraging LLMs’ power. Snowflake empowers organizations to apply near-magical
    generative AI transformations to their data. By leveraging advanced LLM models
    within Snowflake, organizations can efficiently operate with large volumes of
    data, enabling generative AI use cases. In this section, we will discuss deploying
    LLM models within Snowpark Container Services.
  prefs: []
  type: TYPE_NORMAL
- en: In this walk-through, we’ll explore how to harness publicly accessible data
    to demonstrate the transformative capabilities of Snowflake’s ecosystem by deploying
    the Llama 2 LLM from the Hugging Face repository.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Llama 2 by Meta, housed within Hugging Face’s library, epitomizes advanced **natural
    language processing** (**NLP**) technology. As stipulated by Meta’s specific terms
    of service, you’ll need a Hugging Face token to access Llama 2 with Hugging Face.
    Please visit [https://huggingface.co/docs/hub/en/security-tokens](https://huggingface.co/docs/hub/en/security-tokens)
    to learn more.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the LLM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will start by preparing the LLM by utilizing our convenient wrapper around
    the Hugging Face Transformers API, and harness the capabilities of Llama 2 7B
    from Hugging Face. To achieve this, run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Make sure to replace `HF_AUTH_TOKEN` with your token from Hugging Face. The
    code creates the model registry and assigns the model from the Hugging Face registry.
    The model is obtained from the Hugging Face registry and directly imported into
    Snowpark.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will register the model within Snowpark ML.
  prefs: []
  type: TYPE_NORMAL
- en: Registering the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Next, we’ll utilize the model registry’s `log_model` API within Snowpark ML
    to register the model. This involves specifying a model name and a version string
    and providing the model obtained in the previous step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see an output similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.21 – Model registration](img/B19923_08_21.0.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.21 – Model registration
  prefs: []
  type: TYPE_NORMAL
- en: The model is now registered in the registry. Now that the model is ready, we
    will deploy it to Container Services.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the model to Snowpark Container Services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, let us deploy the model to our designated compute pool. Once the deployment
    process is initiated, the model will become accessible as a Snowpark Container
    Services endpoint. Run the following code to deploy the model to Container Services.
    To run this step, you may need to alter your compute pool to include a GPU instance,
    or you can create a new compute pool with a GPU instance.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: This streamlined deployment process highlights how Snowpark ML simplifies the
    deployment of LLMs, handling the creation of the corresponding Snowpark Container
    Services `SERVICE` definition, packaging the model within a Docker image along
    with its runtime dependencies, and launching the service within the specified
    compute pool.
  prefs: []
  type: TYPE_NORMAL
- en: 'After executing the code, you should see a similar output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.22 – Deploying the LLM model to Snowpark Container Services](img/B19923_08_22.0.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.22 – Deploying the LLM model to Snowpark Container Services
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will execute this model in the container.
  prefs: []
  type: TYPE_NORMAL
- en: Note on model deployment
  prefs: []
  type: TYPE_NORMAL
- en: Only the snippets required for explanation are shown in this section. The complete
    code is available in the **chapter_8.ipynb** notebook in GitHub. You should be
    mindful of the model deployment step as it takes considerable time and resources.
  prefs: []
  type: TYPE_NORMAL
- en: Running the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Invoke the model by supplying the subset of the `NEWS_CATEGORY` table with
    the `inputs` column containing the prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'This yields a Snowpark DataFrame with an output column containing the model’s
    response for each row. The raw response intersperses text with the expected JSON
    output, exemplified as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Deploying and executing an LLM model is very easy with Snowpark Container Services.
  prefs: []
  type: TYPE_NORMAL
- en: We will conclude the chapter with a summary.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored Snowpark Container Services, a powerful solution
    designed to simplify the deployment and management of containerized applications
    within the Snowflake ecosystem. We discussed the distinction between jobs and
    services within Snowpark Container Services, highlighting their respective functionalities
    and use cases. We demonstrated how to effectively configure, deploy, and manage
    jobs and services through practical implementation examples.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we delved into containerization through Snowpark ML, showcasing
    how Snowflake users can seamlessly leverage advanced ML models within their environment.
    By integrating a language model from Hugging Face, we illustrated how Snowpark
    ML facilitates the integration of containerized models, enabling sophisticated
    NLP tasks directly within Snowflake. Overall, this chapter equips you with the
    knowledge and tools to harness the transformative potential of SCS and Snowpark
    ML in your data-driven initiatives.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, Snowpark Container Services offers a compelling value proposition
    for businesses seeking efficient and scalable data processing solutions. By enabling
    secure execution of containerized workloads directly within Snowflake, Snowpark
    eliminates the need for data movement, ensuring data integrity and reducing latency.
    Additionally, Snowpark simplifies the development and deployment of data applications,
    allowing teams to focus on innovation rather than infrastructure management. Automated
    container management further streamlines operational tasks, enhancing overall
    productivity and agility.
  prefs: []
  type: TYPE_NORMAL
- en: With this, we conclude the book. Thank you for reading.
  prefs: []
  type: TYPE_NORMAL
