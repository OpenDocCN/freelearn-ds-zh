<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xml:lang="en-US">
<head>
<meta charset="utf-8"/>
<meta name="generator" content="packt"/>
<title>11 Adding Statistics Insights: Associations</title>
<link rel="stylesheet" type="text/css" href="../styles/stylesheet1.css"/>
<link rel="stylesheet" type="text/css" href="../styles/stylesheet2.css"/>

</head>
<body>

<section id="adding-statistics-insights-associations" class="level1 pkt" data-number="12">
<h1 data-number="12">11 Adding Statistics Insights: Associations</h1>
<p>As you saw in the previous chapter, enriching your data can be done through the application of complex algorithms. In addition to distances and linear programming, statistics can often be the ultimate weapon for data analysis. We will explain the basic concepts of some statistical procedures that aim to extract relevant insights about the associations between variables from your data.</p>
<p>In this chapter, you will learn about the following topics:</p>
<ul>
<li>Exploring associations between variables</li>
<li>Correlation between numeric variables</li>
<li>Correlation between categorical and numeric variables</li>
</ul>
<section id="technical-requirements-10" class="level2" data-number="12.1">
<h2 data-number="12.1">Technical requirements</h2>
<p>This chapter requires you to have a working internet connection and <strong>Power BI Desktop</strong> already installed on your machine. You must have properly configured the R and Python engines and IDEs as outlined in <em>Chapter 2</em>, <em>Configuring R with Power BI</em>, and <em>Chapter 3</em>, <em>Configuring Python with Power BI</em>.</p>
</section>
<section id="exploring-associations-between-variables" class="level2" data-number="12.2">
<h2 data-number="12.2">Exploring associations between variables</h2>
<p>At first glance, you might wonder what the point of finding relationships between variables is. Being able to understand the behavior of a pair of variables and identify a pattern in their behavior helps business owners identify key factors to divert certain indicators of company health to their benefit. Knowing the pattern that binds the trend of two variables gives you the power to predict with some certainty one of them by knowing the other. So, knowing the tools to uncover these patterns gives you a kind of analytical superpower that is always appealing to business owners.</p>
<p>In general, two variables are <strong>associated</strong> with each other when the values of one of them are in some way related to the values of the other. When you can somehow measure the extent of the association of two variables, it is called <strong>correlation</strong>. The concept of correlation is immediately applicable in a case where the two variables are numerical. Let's see how.</p>
</section>
<section id="correlation-between-numeric-variables" class="level2" data-number="12.3">
<h2 data-number="12.3">Correlation between numeric variables</h2>
<p>The first thing we generally do to understand whether there is an association between two numeric variables is to represent them on the two Cartesian axes in order to obtain a <strong>scatterplot</strong>:</p>
<figure>
<img src="../media/file266.png" alt="Figure 11.1 – A simple scatterplot" /><figcaption aria-hidden="true">Figure 11.1 – A simple scatterplot</figcaption>
</figure>
<p>Using a scatterplot, it is possible to identify three important characteristics of a possible association:</p>
<ul>
<li><p><strong>Direction</strong>: It can be <em>positive</em> (increasing), <em>negative</em> (decreasing), or <em>not defined</em> (no association found or increasing and decreasing). If the increment of a variable corresponds to the increment of the other, the direction is positive; otherwise, it is negative:</p>
<figure>
<img src="../media/file267.png" alt="Figure 11.2 – Direction types of the association" /><figcaption aria-hidden="true">Figure 11.2 – Direction types of the association</figcaption>
</figure></li>
<li><p><strong>Form</strong>: It describes the general form that association takes in its simplest sense. Obviously, there are many possible forms, but there are some that are more common, such as <em>linear</em> and <em>curvilinear</em> (nonlinear) forms:</p>
<figure>
<img src="../media/file268.png" alt="Figure 11.3 – Shapes of the association" /><figcaption aria-hidden="true">Figure 11.3 – Shapes of the association</figcaption>
</figure></li>
<li><strong>Strength</strong>: The strength of an association is determined by how close the points in the scatterplot follow the line that draws the generic shape of the association.</li>
</ul>
<figure>
<img src="../media/file269.png" alt="Figure 11.4 – Strength of the association" /><figcaption aria-hidden="true">Figure 11.4 – Strength of the association</figcaption>
</figure>
<p>As these visual patterns become measurable through the application of mathematical concepts, we can define different types of correlations between numeric variables.</p>
<section id="karl-pearsons-correlation-coefficient" class="level3" data-number="12.3.1">
<h3 data-number="12.3.1">Karl Pearson’s correlation coefficient</h3>
<p><strong>Pearson’s correlation coefficient</strong> (<strong>r</strong>) measures the degree of linearity of the association between the variables under analysis. This means that if <em>r = 1</em> or <em>r = -1</em>, then the two variables are in a <em>perfect linear relationship</em>. The sign of the coefficient determines the direction of the association. If instead <em>r is close to 0</em> indifferently on the negative or positive side, it means that the association between the variables is <em>very weak</em>. The coefficient cannot take values outside the range [-1, 1].</p>
<p>The square of Pearson's coefficient (<em>R<sup>2</sup></em>) is called the <strong>coefficient of determination</strong> and measures the percentage of <strong>variance</strong> (which measures the dispersion of observations from their mean value) in one variable due to the variance of the other variable, assuming the association is linear. For example, if the correlation coefficient <em>r</em> between a person's weight and height variables measures 0.45, it can be said that about 20% (0.45 x 0.45) of the change (variance) in a person's weight is due to the change in their height.</p>
<p>Calculating the correlation coefficient r is rarely done by hand, as any data management platform provides a function that easily calculates it. If you are curious, given a dataset of <em>n</em> entities, identify the variables <em>x</em> and <em>y</em> for which you want to calculate the correlation coefficient – this is the formula that allows you to calculate it:</p>
<figure>
<img src="../media/file270.png" alt="Figure 11.5 – Formula for the Pearson correlation coefficient" /><figcaption aria-hidden="true">Figure 11.5 – Formula for the Pearson correlation coefficient</figcaption>
</figure>
<p>The <em>x̄</em> and <em>ȳ</em> values correspond to the mean values of the homonymous variables in the dataset. Keep in mind that Pearson’s correlation function is <strong>symmetric</strong>, meaning that the order of the columns for which it is calculated does not matter.</p>
<p>Examples of the correlation coefficient <em>r</em> calculated for some specific associations (<strong>Boigelot distributions</strong>) are shown in <em>Figure 11.6</em>:</p>
<figure>
<img src="../media/file271.png" alt="Figure 11.6 – Pearson’s correlation calculated over Boigelot distributions" /><figcaption aria-hidden="true">Figure 11.6 – Pearson’s correlation calculated over Boigelot distributions</figcaption>
</figure>
<p>The first row of scatterplots in <em>Figure 11.6</em> gives an idea of how the magnitude of the correlation measures the strength of the association tending toward a linear relationship.</p>
<p>The second row shows that, regardless of the angle of the linear relationship, the correlation coefficient <em>r</em> is always equal to 1 in absolute value, that is, it always correctly identifies the linear relationship and its direction. The only exception is the case of the horizontal linear relationship at the center, for which the coefficient <em>r</em> is undefined, since for all values of <em>x</em> there is a single value of <em>y</em>.</p>
<p>The third row shows one of the most important limitations of the correlation coefficient:</p>
<blockquote>
<p><strong>Important Note</strong></p>
<p>Pearson's correlation coefficient is not capable of detecting a pattern in an association between two variables as much as the latter are nonlinear.</p>
</blockquote>
<p>These are the other limitations:</p>
<ul>
<li>With a very small dataset size (say, 3-6 observations), it might appear that an association is present even though it does not exist.</li>
<li><p>The correlation coefficient is very sensitive to outliers (observations located far from most others). It can also happen that outliers give a false idea of the existence of association:</p>
<figure>
<img src="../media/file272.png" alt="Figure 11.7 – Wrong correlation values due to outliers" /><figcaption aria-hidden="true">Figure 11.7 – Wrong correlation values due to outliers</figcaption>
</figure>
<p>We will cover outliers in more detail in <em>Chapter 12</em>, <em>Adding Statistics Insights, Outliers, and Missing Values</em>.</p></li>
<li><p>If the observations in the dataset are divided into different clusters, this may induce a false sense of association:</p>
<figure>
<img src="../media/file273.png" alt="Figure 11.8 – Wrong correlation value due to clusters" /><figcaption aria-hidden="true">Figure 11.8 – Wrong correlation value due to clusters</figcaption>
</figure></li>
<li>The variables used in the calculation of the correlation coefficient must be defined on a continuous scale. For variables based on a discrete scale, such as the 1-10 rating of a service, you must use Spearman's rank correlation, which we will look at in the next section.</li>
<li><p>If in an association there is a variable that has unequal variability with a range of values of a second variable, we are faced with a case of <strong>heteroscedasticity</strong>. The scatterplot assumes the typical shape of a cone, like the one in <em>Figure 11.1</em>. In this case, the correlation coefficient could identify an incorrect linear relationship (more than one linear relationship would satisfy the conic form of the scatterplot):</p>
<figure>
<img src="../media/file274.png" alt="Figure 11.9 – Wrong correlation value due to heteroscedasticity" /><figcaption aria-hidden="true">Figure 11.9 – Wrong correlation value due to heteroscedasticity</figcaption>
</figure></li>
<li>The variables involved in calculating the Pearson correlation coefficient should not be highly <strong>skewed</strong>, that is, the distribution of the variables must not be distorted or asymmetrical with respect to a symmetrical bell curve (normal distribution, where the mean, median, and mode are equal), otherwise a reduction in the true size of the correlation magnitude could occur:</li>
</ul>
<figure>
<img src="../media/file275.png" alt="Figure 11.10 – Distribution skewness types" /><figcaption aria-hidden="true">Figure 11.10 – Distribution skewness types</figcaption>
</figure>
<p>Having clarified these limitations, the first question that comes up is: how can I calculate the correlation between two numeric variables when the association is nonlinear, when the variables are based on an ordinal scale, or when they have a skewed distribution? One of the possible solutions to this problem was provided by Charles Spearman.</p>
</section>
<section id="charles-spearmans-correlation-coefficient" class="level3" data-number="12.3.2">
<h3 data-number="12.3.2">Charles Spearman’s correlation coefficient</h3>
<p>Charles Spearman introduced a nonparametric alternative to Pearson's correlation. A <strong>nonparametric statistical method</strong> refers to the fact that it does not assume that the data on which it is calculated follows specific models described by a handful of parameters. This method is summarized in a new correlation coefficient called <strong>Spearman’s rank-order correlation coefficient</strong> (denoted by <strong>ρ</strong>, <em>rho</em>). Spearman's coefficient measures the strength and direction of the association between two variables once their observations have been ranked according to their value. The formula that calculates it is as follows:</p>
<figure>
<img src="../media/file276.png" alt="Figure 11.11 – Spearman’s rank-order correlation coefficient formula" /><figcaption aria-hidden="true">Figure 11.11 – Spearman’s rank-order correlation coefficient formula</figcaption>
</figure>
<p>The value <em>D</em> is the difference between the two ranks of each observation. To learn more details about the calculation, take a look at the references. Keep in mind that Spearman’s correlation function is also symmetric.</p>
<p>Spearman's correlation coefficient ranges from -1 to +1. The sign of the coefficient indicates whether it is a positive or negative monotone association.</p>
<blockquote>
<p><strong>Important Features of Spearman's Correlation</strong></p>
<p>Because Spearman's correlation applies to ranks, it provides a measure of a <strong>monotonic association</strong> between two continuous random variables and is the best-fitting correlation coefficient for ordinal variables. Because of the way it is calculated, as opposed to Pearson's, Spearman's correlation is <em>robust to outliers</em>.</p>
</blockquote>
<p>Remember that an association is said to be monotonic if it is increasing over its entire domain or decreasing over its entire domain (not a combination of the two):</p>
<figure>
<img src="../media/file277.png" alt="Figure 11.12 – Monotonic and non-monotonic associations" /><figcaption aria-hidden="true">Figure 11.12 – Monotonic and non-monotonic associations</figcaption>
</figure>
<blockquote>
<p><strong>Important Note</strong></p>
<p>It’s important to check the monotonicity of the association, since in general the</p>
<p>correlation coefficients are not able to accurately describe non-monotonic relationships.</p>
</blockquote>
<p>Calculating the Spearman correlation coefficient for the Boigelot distributions, we get this:</p>
<figure>
<img src="../media/file278.png" alt="Figure 11.13 – Spearman’s correlation calculated over Boigelot distributions" /><figcaption aria-hidden="true">Figure 11.13 – Spearman’s correlation calculated over Boigelot distributions</figcaption>
</figure>
<p>Even Spearman's correlation fails to capture the nonlinear patterns you observe in the third row of distributions in <em>Figure 11.13</em>. And this is due not to the nonlinearity of the above associations, but to their non-monotonicity. In contrast, for monotonic nonlinear associations, Spearman's correlation is better suited than Pearson's:</p>
<figure>
<img src="../media/file279.png" alt="Figure 11.14 – Pearson’s and Spearman’s correlation over a nonlinear monotonic association" /><figcaption aria-hidden="true">Figure 11.14 – Pearson’s and Spearman’s correlation over a nonlinear monotonic association</figcaption>
</figure>
<p>Spearman's correlation is not the only one that uses data ranking in its calculation. Kendall's also uses this strategy. Let's take a look at its features.</p>
</section>
<section id="maurice-kendalls-correlation-coefficient" class="level3" data-number="12.3.3">
<h3 data-number="12.3.3">Maurice Kendall’s correlation coefficient</h3>
<p><strong>Kendall’s rank correlation coefficient</strong> (<strong>τ</strong>, tau) is also a nonparametric method of detecting associations between two variables. Its calculation is based on the concept of <strong>concordant pairs</strong> and <strong>discordant pairs</strong> (check the <em>References</em> section for more details). The formula useful in calculating Kendall's correlation coefficient is as follows:</p>
<figure>
<img src="../media/file280.png" alt="Figure 11.15 – Kendall’s correlation coefficient formula" /><figcaption aria-hidden="true">Figure 11.15 – Kendall’s correlation coefficient formula</figcaption>
</figure>
<p>The <em>n<sub>c</sub></em> value represents the number of concordant pairs, and the <em>n<sub>d</sub></em> value represents the number of discordant pairs. Like the other correlation functions, Kendall’s is symmetric too.</p>
<p>All of the assumptions made for Spearman's correlation also apply to Kendall's. Here is a comparison between the two correlations:</p>
<ul>
<li>Both correlations handle ordinal data and nonlinear continuous monotonic data very well.</li>
<li>Both correlations are robust to outliers.</li>
<li>Kendall correlation is preferred to Spearman correlation when the sample size is small and when it has many tied ranks.</li>
<li>Kendall’s coefficient is usually smaller than Spearman’s.</li>
<li>Kendall's correlation has more computational complexity than Spearman's.</li>
</ul>
<p>That said, let's use all three correlation coefficients in a real case.</p>
</section>
<section id="description-of-a-real-case" class="level3" data-number="12.3.4">
<h3 data-number="12.3.4">Description of a real case</h3>
<p>Your boss has asked you to carry out a world population analysis requested by a client. Specifically, you need to understand whether there is a relationship and of what magnitude between life expectancy and per capita gross domestic product (GDP) over the years.</p>
<p>Looking around the web for useful data for the purpose, you realize that there is a portal that can help your case named <strong>Gapminder</strong> (<a href="https://www.gapminder.org/">https://www.gapminder.org/</a>). It fights devastating misconceptions and promotes a fact-based worldview everyone can understand combining data from multiple sources into unique coherent time series that can’t be found elsewhere. The portal precisely exposes data on life expectancy (<a href="http://bit.ly/life-expectancy-data">http://bit.ly/life-expectancy-data</a>) and data on GDP per capita (<a href="http://bit.ly/gdp-per-capita-data">http://bit.ly/gdp-per-capita-data</a>). In addition, there is someone who has transformed the data to collect it in a more suitable form for our purposes and has shared his work with the community in a CSV file (<a href="http://bit.ly/gdp-life-expect-data">http://bit.ly/gdp-life-expect-data</a>).</p>
<p>Awesome! You have everything you need to start your analysis in Python.</p>
</section>
<section id="implementing-correlation-coefficients-in-python" class="level3" data-number="12.3.5">
<h3 data-number="12.3.5">Implementing correlation coefficients in Python</h3>
<p>To run the code in this section, you need to install the Seaborn module in your <code>pbi_powerquery_env</code> environment. As you've probably learned by now, proceed as follows:</p>
<ol>
<li>Open the Anaconda prompt.</li>
<li>Enter the command <code>conda activate pbi_powerquery_env</code>.</li>
<li>Enter the command <code>pip install seaborn</code>.</li>
</ol>
<p>The code you'll find in this section is available in the file <code>01-gdp-life-expectancy-analysis-in-python.py</code> in the <code>Chapter11\Python</code> folder.</p>
<p>At this point, let's take a quick look at the data in the above CSV file, while also importing the forms needed for the rest of the operations:</p>
<pre><code>import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sb
dataset_url = &#39;http://bit.ly/gdp-life-expect-data&#39;
df = pd.read_csv(dataset_url)
df.head()
# If you’re not using VS Code run this instead
# print(df.head())</code></pre>
<p>You'll see something like this:</p>
<figure>
<img src="../media/file281.png" alt="Figure 11.16 – A sample of the dataset about GDP and life expectancy" /><figcaption aria-hidden="true">Figure 11.16 – A sample of the dataset about GDP and life expectancy</figcaption>
</figure>
<p>The variables we are interested in are <code>lifeExp</code> and <code>gdpPercap</code>. Before drawing a scatterplot of the two, let's also take a look at the distribution of each of them. So let's define the functions we will use for the plots and draw the distributions:</p>
<pre><code>def distPlot(data, var, title, xlab, ylab, bins=100):
    hplot = sb.distplot(data[var], kde=False, bins=bins)
    plt.title(title, fontsize=18)
    plt.xlabel(xlab, fontsize=16)
    plt.ylabel(ylab, fontsize=16)
    
    return hplot
def scatterPlot(data, varx, vary, title, xlab, ylab):
    hplot = sb.scatterplot(varx, vary, data=data)
    plt.title(title, fontsize=18)
    plt.xlabel(xlab, fontsize=16)
    plt.ylabel(ylab, fontsize=16)
    
    return hplot
distPlot(data=df, var=&#39;lifeExp&#39;, title=&#39;Life Expectancy&#39;, xlab=&#39;Life Expectancy years&#39;, ylab=&#39;Frequency&#39;)
# In case you&#39;re not using a Jupyter notebook run also the following:
# plt.show()
distPlot(data=df, var=&#39;gdpPercap&#39;, title=&#39;GDP / capita&#39;, xlab=&#39;GDP / capita ($)&#39;, ylab=&#39;Frequency&#39;)
# In case you&#39;re not using a Jupyter notebook run also the following:
# plt.show()</code></pre>
<p>The plots created are as follows:</p>
<figure>
<img src="../media/file282.png" alt="Figure 11.17 – Distributions of life expectancy and GDP variables" /><figcaption aria-hidden="true">Figure 11.17 – Distributions of life expectancy and GDP variables</figcaption>
</figure>
<p>As you can see, while the life expectancy distribution more or less approximates the normal distribution, the GDP distribution is totally positive-skewed. Should there be a significant association between the two variables, you should then expect a likely non-linear scatter plot. Let's check it out:</p>
<pre><code>scatterPlot(data=df, varx=&#39;lifeExp&#39;, vary=&#39;gdpPercap&#39;, title=&#39;Life Expectancy vs GDP/Capita&#39;, xlab=&#39;lifeExp&#39;, ylab=&#39;gdpPercap&#39;)
# In case you&#39;re not using a Jupyter notebook run also the following:
# plt.show()</code></pre>
<p>You get this:</p>
<figure>
<img src="../media/file283.png" alt="Figure 11.18 – Scatterplot between life expectancy and GDP per capita" /><figcaption aria-hidden="true">Figure 11.18 – Scatterplot between life expectancy and GDP per capita</figcaption>
</figure>
<p>From <em>Figure 11.18</em>, you can see that the association between the two variables is obvious and, as expected, is nonlinear (background arrow). What is more, outliers are present that are highlighted at the top. You are therefore faced with two assumptions that invalidate Pearson's correlation. Fortunately, the association is monotonically increasing, so the Spearman and Kendall correlations should detect the pattern more accurately.</p>
<p>The Python implementation of the three correlation coefficients is already made available by pandas. Therefore, correlation analysis is straightforward:</p>
<pre><code>df[[&#39;lifeExp&#39;,&#39;gdpPercap&#39;]].corr(method=&#39;pearson&#39;)
df[[&#39;lifeExp&#39;,&#39;gdpPercap&#39;]].corr(method=&#39;spearman&#39;)
df[[&#39;lifeExp&#39;,&#39;gdpPercap&#39;]].corr(method=&#39;kendall&#39;)</code></pre>
<p>The <code>corr()</code> function of a pandas dataframe returns the correlation calculation for each pair of its numeric columns. Since the correlation functions are symmetric (that is, the order of the columns for which it is computed does not matter), the three calls return three dataframes each having all the numeric features as rows and columns, and the correlations between the features as values:</p>
<figure>
<img src="../media/file284.png" alt="Figure 11.19 – Correlations between life expectancy and GDP per capita" /><figcaption aria-hidden="true">Figure 11.19 – Correlations between life expectancy and GDP per capita</figcaption>
</figure>
<p>As you can see, all three correlations identify a positive association between the two variables. As expected, the strength detected by Pearson's correlation is the weakest (<em>r = 0.58</em>). In contrast, the Spearman and Kendall correlations have a higher magnitude, <em>ρ = 0.83</em> and <em>τ = 0.64</em>, respectively. Especially from Spearman's correlation, it is evident that the two variables are strongly correlated.</p>
<p>It often happens that in a data science project, you have to select the most predictive variables towards a target variable to predict when you have a large number of columns. The technique of correlation can certainly help us in this: assuming that the target variable is GDP per capita, we could decide to keep all those variables as predictors that have a correlation with the target variable greater than 0.7. You understand that in this case, it is important to consider the right correlation method, otherwise, you would risk rejecting a variable that is strongly correlated to the target one.</p>
<p>Great! Let's see how to replicate this analysis using R as well.</p>
</section>
<section id="implementing-correlation-coefficients-in-r" class="level3" data-number="12.3.6">
<h3 data-number="12.3.6">Implementing correlation coefficients in R</h3>
<p>One of the packages we recommend using in R for calculating correlation coefficients is <strong>corrr</strong> (<a href="https://github.com/tidymodels/corrr">https://github.com/tidymodels/corrr</a>). It allows you to explore and rearrange the tibbles returned by the <code>correlate()</code> function very easily according to the practices suggested by Tidyverse. Therefore, you need to install the <code>corrr</code> package in the most recent version of CRAN R you have:</p>
<ol>
<li>Open RStudio and make sure it is referencing your latest CRAN R (version 4.0.2 in our case).</li>
<li>Click on the <strong>Console</strong> window and enter this command: <code>install.packages('corrr')</code>. Then press <em>Enter</em>.</li>
</ol>
<p>The code you'll find in this section is available in the file <code>01-gdp-life-expectancy-analysis-in-r.R</code> in the <code>Chapter11\R</code> folder.</p>
<p>So let's proceed to import the necessary libraries, load the data from the CSV file on the web into a tibble, and display the first few lines:</p>
<pre><code>library(readr)
library(dplyr)
library(corrr)
library(ggplot2)
dataset_url &lt;- &#39;http://bit.ly/gdp-life-expect-data&#39;
tbl &lt;- read_csv(dataset_url)
tbl</code></pre>
<p>You will see this in the console:</p>
<figure>
<img src="../media/file285.png" alt="Figure 11.20 – First rows of the population tibble" /><figcaption aria-hidden="true">Figure 11.20 – First rows of the population tibble</figcaption>
</figure>
<p>Also, in this case, we define the functions necessary to draw a distribution plot and a scatterplot and use them to generate the plots of the distributions of the variables <code>lifeExp</code> and <code>gdpPercap</code>:</p>
<pre><code>distPlot &lt;- function(data, var, title, xlab, ylab, bins=100) {
    p &lt;- ggplot( data=data, aes_string(x=var) ) + 
        geom_histogram( bins=bins, fill=&quot;royalblue3&quot;, color=&quot;steelblue1&quot;, alpha=0.9) +
        ggtitle(title) + xlab(xlab) + ylab(ylab) +
        theme( plot.title = element_text(size=20), axis.title = element_text(size=16) )
    return(p)
}
scatterPlot &lt;- function(data, varx, vary, title, xlab, ylab) {
    p &lt;- ggplot( data=data, aes_string(x=varx, y=vary)) + 
        geom_point(
            color=&#39;steelblue1&#39;, fill=&#39;royalblue3&#39;,
            shape=21, alpha=0.8, size=3
        ) +
        ggtitle(title) + xlab(xlab) + ylab(ylab) +
        theme( plot.title = element_text(size=20), axis.title = element_text(size=16) )
    return(p)
}
distPlot(data = tbl, var = &#39;lifeExp&#39;, title = &#39;Life Expectancy&#39;,          xlab = &#39;Life Expectancy (years)&#39;, ylab = &#39;Frequency&#39;)
distPlot(data = tbl, var = &#39;gdpPercap&#39;, title = &#39;GDP / capita&#39;, xlab = &#39;GDP / capita ($)&#39;, ylab = &#39;Frequency&#39;)</code></pre>
<p>These are the plots you get:</p>
<figure>
<img src="../media/file286.png" alt="Figure 11.21 – Distribution plots of Life Expectancy and GDP per capita" /><figcaption aria-hidden="true">Figure 11.21 – Distribution plots of Life Expectancy and GDP per capita</figcaption>
</figure>
<p>You can get the scatterplot between the two variables as follows:</p>
<pre><code>scatterPlot(data = tbl, varx = &#39;lifeExp&#39;, vary = &#39;gdpPercap&#39;, title = &#39;Life Expectancy vs GDP/Capita&#39;, xlab = &#39;lifeExp&#39;, ylab = &#39;gdpPercap&#39;)</code></pre>
<p>This is the graph you get:</p>
<figure>
<img src="../media/file287.png" alt="Figure 11.22 – Scatterplot between Life Expectancy and GDP per capita" /><figcaption aria-hidden="true">Figure 11.22 – Scatterplot between Life Expectancy and GDP per capita</figcaption>
</figure>
<p>Correlation matrices (persisted in tibbles) are obtained straightforwardly via the <code>corrr</code> package’s <code>correlate()</code> function as follows:</p>
<pre><code>tbl %&gt;% select( lifeExp, gdpPercap ) %&gt;% correlate( method = &#39;pearson&#39; )
tbl %&gt;% select( lifeExp, gdpPercap ) %&gt;% correlate( method = &#39;spearman&#39; )
tbl %&gt;% select( lifeExp, gdpPercap ) %&gt;% correlate( method = &#39;kendall&#39; )</code></pre>
<p>These are the console results:</p>
<figure>
<img src="../media/file288.png" alt="Figure 11.23 – Correlation coefficients in tibbles" /><figcaption aria-hidden="true">Figure 11.23 – Correlation coefficients in tibbles</figcaption>
</figure>
<p>Pretty simple, right!? Now that you know how to get correlation coefficients in both Python and R, let's implement what you learned in Power BI.</p>
</section>
<section id="implementing-correlation-coefficients-in-power-bi-with-python-and-r" class="level3" data-number="12.3.7">
<h3 data-number="12.3.7">Implementing correlation coefficients in Power BI with Python and R</h3>
<p>Power BI has the ability to introduce a minimum of statistical analysis for the data that has been loaded into the data model thanks to DAX. You can find a list of statistical functions you can use at this link: <code>http://bit.ly/dax-stats-func</code>. As for the simple <em>Pearson correlation</em>, you can use it for columns in an already loaded table thanks to the predefined quick measures, which behind the scenes add some sometimes non-trivial DAX code for you. For more details, you can click on this link: <a href="http://bit.ly/power-bi-corr-coef">http://bit.ly/power-bi-corr-coef</a>. However, there is no easy way to implement the Spearman and Kendall correlation coefficients.</p>
<p>Also, you have probably heard of the <strong>correlation plot</strong> available in Microsoft's AppSource (<a href="https://bit.ly/power-bi-corr-plot">https://bit.ly/power-bi-corr-plot</a>). You might think it would be possible to use that to get the correlation coefficients. First of all, the plot only displays Pearson correlation coefficients and not Spearman and Kendall ones. Also, you cannot extract coefficients from a visual or custom visual to persist them in a table in order to use them for later calculations (for example, feature selection). So the correlation plot idea is totally off the mark in this case.</p>
<p>The only way to proceed is to use Python or R. Let's see how to do that.</p>
<p>In this case, due to the simplicity of the code, we will implement the correlation coefficients in both Python and R in one project.</p>
<p>First, make sure that Power BI Desktop references the correct versions of Python and R in <strong>Options</strong>. Then follow these steps:</p>
<ol>
<li><p>Click on <strong>Get Data</strong>, select <strong>Web</strong>, and click on <strong>Connect</strong>:</p>
<figure>
<img src="../media/file289.png" alt="Figure 11.24 – Get data from the web" /><figcaption aria-hidden="true">Figure 11.24 – Get data from the web</figcaption>
</figure></li>
<li>Enter the <code>http://bit.ly/gdp-life-expect-data</code> string into the <strong>URL</strong> textbox and click <strong>OK</strong>.</li>
<li>You’ll see a preview of the data. Then click <strong>Transform Data</strong>.</li>
<li>Click <strong>Transform</strong> on the ribbon and then <strong>Run Python script</strong>.</li>
<li><p>Enter the following Python script and then click <strong>OK</strong>:</p>
<pre><code>import pandas as pd
corr_df = dataset.corr(method=&#39;pearson&#39;)
# You need to convert row names into a column
# in order to make it visible in Power BI
corr_df.index.name = &#39;rowname&#39;
corr_df.reset_index(inplace=True)</code></pre>
<p>You can find this code in the file <code>02-gdp-life-expectancy-analysis-in-power-bi-with-python.py</code> in the <code>Chapter11\Python</code> folder.</p></li>
<li>We are only interested in the data in <code>corr_df</code>. So, click on its <strong>Table</strong> value.</li>
<li>You’ll see the preview of the Pearson correlation coefficients between all the numeric columns of the dataset.</li>
<li>Click <strong>Home</strong> on the ribbon and then click <strong>Close &amp; Apply</strong>.</li>
<li>Repeat steps 1 to 3.</li>
<li>Click <strong>Transform</strong> on the ribbon and then <strong>Run R script</strong>.</li>
<li><p>Enter the following R script and then click <strong>OK</strong>:</p>
<pre><code>library(dplyr)
library(corrr)
# You need to select only numeric columns
# in order to make correlate() work
corr_tbl &lt;- tbl %&gt;% 
    select( where(is.numeric) ) %&gt;% 
    correlate( method = &#39;spearman&#39; )</code></pre>
<p>You can find this code in the file <code>02-gdp-life-expectancy-analysis-in-power-bi-with-r.R</code> in the <code>Chapter11\R</code> folder.</p></li>
<li>We are only interested in the data in <code>corr_tbl</code>. So, click its <strong>Table</strong> value.</li>
<li>You’ll see the preview of the Spearman correlation coefficients between all the numeric columns of the dataset.</li>
<li>Click <strong>Home</strong> on the ribbon and then click <strong>Close &amp; Apply</strong>.</li>
</ol>
<p>Awesome! You have just calculated the correlation coefficients according to Pearson and Spearman for the numeric columns of a source dataset in Power BI with Python and R. Simple, isn't it?</p>
<p>You're probably wondering, <em>Okay, all clear on the numeric variables. What if I had categorical (non-numeric) variables? How can I calculate the correlation between them? And what about the correlation between a numeric variable and a categorical one?</em> Let's take a look at how to approach this type of analysis.</p>
</section>
</section>
<section id="correlation-between-categorical-and-numeric-variables" class="level2" data-number="12.4">
<h2 data-number="12.4">Correlation between categorical and numeric variables</h2>
<p>We have shown that, in the case of two numeric variables, you can get a sense of the association between them by taking a look at their scatterplot. Clearly, this strategy cannot be used when one or both variables are categorical. Note that a variable is <strong>categorical</strong> (or qualitative, or nominal) when it takes on values that are names or labels. For example, smartphone operating systems (iOS, Android, Linux, and so on).</p>
<p>Let's see how to analyze the case of two categorical variables.</p>
<section id="considering-both-variables-categorical" class="level3" data-number="12.4.1">
<h3 data-number="12.4.1">Considering both variables categorical</h3>
<p>So, is there a graphical representation that helps us understand whether there is a significant association between two categorical variables? The answer is yes and its name is a <strong>mosaic plot</strong>. In this section, we will take the Titanic disaster dataset as a reference dataset. In order to have an idea of what a mosaic plot looks like, let's take into consideration the variables <code>Survived</code> (which takes values <code>1</code> and <code>0</code>) and <code>Pclass</code> (<em>passenger class</em>, which takes values <code>1</code>, <code>2</code>, and <code>3</code>). Since we want to study the association between these two variables, we consider the following mosaic plot generated by them:</p>
<figure>
<img src="../media/file290.png" alt="Figure 11.25 – Mosaic plot of the variables Survived and Pclass" /><figcaption aria-hidden="true">Figure 11.25 – Mosaic plot of the variables Survived and Pclass</figcaption>
</figure>
<p>In short, the objective of the mosaic plot is to provide, at a glance, the strength of the association between the individual elements of each variable through the color of the tiles that represent the pairs of elements in question.</p>
<blockquote>
<p><strong>Important Note</strong></p>
<p>Basically, the more the color of the tile tends toward <em>dark blue</em>, the more there are observations represented by that tile compared to the expected quantity in the case of variables independent of each other.</p>
<p>On the other hand, the more the color of the tile tends toward <em>dark red</em>, the less there are observations represented by that tile compared to the expected quantity in the case of variables independent of each other.</p>
</blockquote>
<p>Looking at the top row of tiles in <em>Figure 11.25</em> associated with <em>Survived = 0</em>, we can say that, among the non-survivors, more than 50% of the people belong to the third class and the number is larger than expected if there was no association between survivors and classes. So, in general, we can say that there is a <em>positive association</em> of medium strength between the non-survivors and the third-class people. In other words, it is quite likely to find third-class people among non-survivors. In contrast, the number of first-class people among non-survivors is far lower than would be expected if there were no association between survivors and classes. So there is a strong negative association between the non-survivors and the first-class people. Thus, it is very likely that first-class people are not present among non-survivors.</p>
<p>Focusing instead on the bottom row of tiles in <em>Figure 11.25</em>, we have confirmation of what we have already said: it is very likely to find first-class people among the survivors and very unlikely to find third-class people.</p>
<p>The numbers behind the plot in <em>Figure 11.25</em> are based on conditional probabilities and can be found by calculating the <strong>contingency table</strong> (also called <strong>crosstab</strong>) generated by the various elements of each of the categorical variables under analysis:</p>
<figure>
<img src="../media/file291.png" alt="Figure 11.26 – Mosaic plot of the variables Survived and Pclass" /><figcaption aria-hidden="true">Figure 11.26 – Mosaic plot of the variables Survived and Pclass</figcaption>
</figure>
<p>See the <em>Reference</em> section for more details about contingency tables and mosaic plots.</p>
<p>But in a nutshell, how do we numerically determine the global strength of the association between two categorical variables? The answer lies in the coefficient highlighted in <em>Figure 11.26</em>, namely <em>Cramér's V</em>. Let's see what this is all about.</p>
<section id="harald-cramérs-correlation-coefficient" class="level4" data-number="12.4.1.1">
<h4 data-number="12.4.1.1">Harald Cramér’s correlation coefficient</h4>
<p><strong>Cramér’s correlation coefficient</strong> (<strong>V</strong>) measures the strength of the association between two categorical variables and it ranges from 0 to +1 (it doesn’t admit negative values, contrary to the coefficients seen until now). This coefficient is based on <strong>Pearson's chi-square (χ2) statistic</strong>, which is used to test whether two categorical variables are independent. Cramér's V formula is as follows:</p>
<figure>
<img src="../media/file292.png" alt="Figure 11.27 – Cramér’s V formula" /><figcaption aria-hidden="true">Figure 11.27 – Cramér’s V formula</figcaption>
</figure>
<p>In the formula of <em>Figure 11.27</em>, the value <em>N</em> is the sample size and <em>k</em> is the smallest number of distinct elements among those of each categorical variable in the association.</p>
<p>Cramér’s V coefficient is a symmetric function and the guidelines of <em>Figure 11.28</em> can be used to determine the magnitude of its effect size:</p>
<figure>
<img src="../media/file293.png" alt="Figure 11.28 – Cramér’s V effect size ranges" /><figcaption aria-hidden="true">Figure 11.28 – Cramér’s V effect size ranges</figcaption>
</figure>
<p>The fact that <em>V</em> is a symmetric function leads to an important loss of information. Let's see why.</p>
</section>
<section id="henri-theils-uncertainty-coefficient" class="level4" data-number="12.4.1.2">
<h4 data-number="12.4.1.2">Henri Theil’s uncertainty coefficient</h4>
<p>Suppose you have the two categorical variables <code>IsFraudster</code> and <code>Hobby</code> in the following dataset:</p>
<figure>
<img src="../media/file294.png" alt="Figure 11.29 – Sample dataset of categorical variables" /><figcaption aria-hidden="true">Figure 11.29 – Sample dataset of categorical variables</figcaption>
</figure>
<p>Each value of the <code>Hobby</code> variable can be associated with a unique value of the <code>IsFraudster</code> variable (for example, <em>Chess → Fraudster</em>). However, the reverse is not true (for example, <em>Fraudster → [Chess, Body building]</em>). Therefore, the strength of the <em>Hobby → Fraudster</em> association (I know <code>Hobby</code> and need to determine <code>IsFraudster</code>) is of higher magnitude than the <em>Fraudster → Hobby</em> association (I know <code>IsFraudster</code> and need to determine <code>Hobby</code>). Unfortunately, the use of Cramér's coefficient <em>V</em> causes this distinction to be lost, since it is a symmetric function. To maintain the asymmetric nature of the relation, we must introduce Theil's uncertainty coefficient.</p>
<p><strong>Theil's uncertainty coefficient</strong>, also called the <strong>entropy coefficient</strong>, between two variables, <em>X</em> and <em>Y</em>, has a range of [0, 1] and it is defined as follows:</p>
<figure>
<img src="../media/file295.png" alt="Figure 11.30 – Theil&#39;s uncertainty coefficient formula" /><figcaption aria-hidden="true">Figure 11.30 – Theil's uncertainty coefficient formula</figcaption>
</figure>
<p>It is based on the concept of <strong>entropy</strong>, which provides information about the variation or diversity (and therefore uncertainty) of the information contained in one variable, given by <em>H(X)</em>. Then it’s also based on the <strong>conditional entropy</strong> (or <strong>joint entropy</strong>) <em>H(X|Y)</em>, which measures data diversity associated with the two variables <em>X</em> and <em>Y</em>.</p>
<p>For example, always considering the Titanic disaster dataset, the coefficient <em>U(Survived|Pcalss)</em> is 0.06; conversely, <em>U(Pcalss|Survived)</em> is 0.09.</p>
<p>Let's see which coefficient to consider when dealing with associations in which one variable is numeric and the other is categorical.</p>
</section>
</section>
<section id="considering-a-numeric-variable-and-a-categorical-one" class="level3" data-number="12.4.2">
<h3 data-number="12.4.2">Considering a numeric variable and a categorical one</h3>
<p>If you want to graphically represent an association between a numeric variable and a categorical variable, the boxplot or the violin plot are the types of graphic representation for you. If you have already come across the problem of having to represent the distribution of a variable by highlighting key statistics, then you should be familiar with a <strong>boxplot</strong>:</p>
<figure>
<img src="../media/file296.png" alt="Figure 11.31 – Graphical explanation of a boxplot" /><figcaption aria-hidden="true">Figure 11.31 – Graphical explanation of a boxplot</figcaption>
</figure>
<p>A <strong>violin plot</strong> is nothing but a combination of a histogram/distribution plot and a boxplot for the same variable:</p>
<figure>
<img src="../media/file297.png" alt="Figure 11.32 – Graphical explanation of a violin plot" /><figcaption aria-hidden="true">Figure 11.32 – Graphical explanation of a violin plot</figcaption>
</figure>
<p>You will find more details about boxplots and violin plots in the references.</p>
<p>When you need to relate a numeric variable to a categorical one, you can build a violin plot for each element of the categorical variable. To go back to the Titanic disaster dataset example, taking the <code>Pclass</code> (categorical) and <code>Age</code> (numeric) variables into account, you get this multiple violin plot:</p>
<figure>
<img src="../media/file298.png" alt="Figure 11.33 – Graphical explanation of a violin plot" /><figcaption aria-hidden="true">Figure 11.33 – Graphical explanation of a violin plot</figcaption>
</figure>
<p>If you look carefully inside each violin plot, you will see a white dot for each thin black boxplot plotted within it. Those dots represent the mean of each distribution of the <code>Age</code> variable for each element of the <code>Pclass</code> variable. Since they are arranged at fairly distinct heights from each other, it is possible that the <code>Pclass</code> variable is a good predictor for the <code>Age</code> variable. But how do we measure the strength of the association between numeric and categorical variables? The answer is given to us by the <em>correlation ratio</em>.</p>
<section id="karl-pearsons-correlation-ratio" class="level4" data-number="12.4.2.1">
<h4 data-number="12.4.2.1">Karl Pearson’s correlation ratio</h4>
<p>It’s again thanks to Karl Pearson that we have a tool to calculate the degree of nonlinear association between a categorical and a numerical variable. This is the <strong>correlation ratio</strong> (<strong>η</strong>, eta), which was introduced during the study of analysis of variance (<strong>ANOVA</strong>), and it ranges from 0 to +1.</p>
<p>Just as <em>r<sup>2</sup></em> can be interpreted as the percentage of variance in one variable explained linearly by the other, <em>η<sup>2</sup></em> (also called the <strong>intraclass correlation coefficient</strong>) represents the percentage of variance in the dependent (target) variable explained <em>linearly or nonlinearly</em> by the independent (predictor) variable. For this interpretation to be valid, it requires that the dependent variable be numeric and the independent variable be categorical.</p>
<p>The correlation ratio is an asymmetric function only if the categorical variable is an ordinal one (that is, days of week can be transformed into integers), otherwise, it only makes sense in one way. The interclass correlation coefficient formula (from which we immediately derive the correlation ratio by applying the square root) is as follows:</p>
<figure>
<img src="../media/file299.png" alt="Figure 11.34 – Correlation ratio’s formula" /><figcaption aria-hidden="true">Figure 11.34 – Correlation ratio’s formula</figcaption>
</figure>
<p>The value is the mean of <em>y</em> broken down by category <em>x</em> and is the mean of the whole <em>y</em> (cross-category). Since <em>σ</em> represents the <em>variance</em> of a variable, we can read <em>η<sup>2</sup></em> as the ratio of the dispersion of the variable <em>y</em> weighted for each category of <em>x</em> to the full dispersion of <em>y</em>.</p>
<blockquote>
<p><strong>Important Note</strong></p>
<p>The more <em>η<sup>2</sup></em> tends to 1, the less the observations disperse around their mean value for each individual category. Therefore, the total dispersion of the numerical variable is all due to the breakdown in categories, and not to the individual dispersions for each category. That is why in this case we can say that there is a strong association between the numerical variable and the categorical variable.</p>
</blockquote>
<p>To better understand the concept laid out above, consider analyzing the grades taken in three subjects. If the grades are dispersed for each subject, you have a certain eta. If, on the other hand, the grades coincide for each subject, then eta equals 1. This statement is represented in <em>Figure 11.35</em>, where each observation is plotted as a point in the violin plots:</p>
<figure>
<img src="../media/file300.png" alt="Figure 11.35 – Differences in η after changing grade distributions per topic" /><figcaption aria-hidden="true">Figure 11.35 – Differences in η after changing grade distributions per topic</figcaption>
</figure>
<p>So, let's now implement the correlation coefficients described in this section in Python.</p>
</section>
</section>
<section id="implementing-correlation-coefficients-in-python-1" class="level3" data-number="12.4.3">
<h3 data-number="12.4.3">Implementing correlation coefficients in Python</h3>
<p>The Python community is very lucky because Shaked Zychlinski developed a library with many data analysis tools, including a function that takes into account the data type of the columns of a pandas dataframe and generates a dataframe with the appropriate correlations. This library is <strong>Dython</strong> (<a href="http://shakedzy.xyz/dython/">http://shakedzy.xyz/dython/</a>) and can be installed via <code>pip</code>. Moreover, if you want to create mosaic plots, you must also install <strong>statsmodels</strong> (<a href="https://www.statsmodels.org/">https://www.statsmodels.org/</a>) in your <code>pbi_powerquery_env</code> environment. As you've probably learned by now, proceed as follows:</p>
<ol>
<li>Open the Anaconda prompt.</li>
<li>Enter the command <code>conda activate pbi_powerquery_env</code>.</li>
<li>Enter the command <code>pip install dython</code>.</li>
<li>Enter the command <code>pip install statsmodels</code>.</li>
</ol>
<p>The code you'll find in this section is available in the file <code>03-titanic-disaster-analysis-in-python.py</code> in the <code>Chapter11\Python</code> folder.</p>
<p>Of all the utilities in Dython, we need the ones in the <code>nominal</code> module. After loading the main libraries, we also create a helper function to draw a violin plot.</p>
<p>At this point, you can load the Titanic disaster data from a CSV file exposed on the web and transform the numerical columns <code>Survived</code> and <code>Pclass</code> (passenger class) into strings, since they are categorical variables:</p>
<pre><code>dataset_url = &#39;http://bit.ly/titanic-data-csv&#39;
df = pd.read_csv(dataset_url)
categ_cols = [&#39;Survived&#39;, &#39;Pclass&#39;]
df[categ_cols] = df[categ_cols].astype(str)</code></pre>
<p>It is then possible to calculate <em>Cramér's V</em> coefficient between the above two categorical variables:</p>
<pre><code>cramers_v(df[&#39;Survived&#39;], df[&#39;Pclass&#39;])</code></pre>
<p>It returns a value of <code>0.34</code>, indicating a medium association strength.</p>
<p>You can also calculate <em>Theil's U</em> uncertainty coefficient of the <code>Survived</code> variable given the <code>Pclass</code> variable:</p>
<pre><code>theils_u(df[&#39;Survived&#39;], df[&#39;Pclass&#39;])</code></pre>
<p>The returned value is <code>0.087</code>. Conversely, you can calculate the same coefficient for the <code>Pclass</code> variable given the <code>Survived</code> one, in order to show the asymmetry of the function:</p>
<pre><code>theils_u(df[&#39;Pclass&#39;], df[&#39;Survived&#39;])</code></pre>
<p>This one returns <code>0.058</code>. So it’s clear that the association <em>Pclass → Survived</em> is stronger than the opposite one.</p>
<p>How about calculating the <em>correlation ratio η</em> between the variables <code>Age</code> (passenger age) and <code>Pclass</code>? Let's do that:</p>
<pre><code>correlation_ratio(categories=df[&#39;Pclass&#39;], measurements=df[&#39;Age&#39;])</code></pre>
<p>The result is <code>0.366</code>. Now, what if you want to get a correlation value for each couple of columns regardless of their data type? In this case, the <code>associations()</code> function is our friend. You just have to specify whether you want to use Theil’s U coefficient (<code>nom_nom_assoc = 'theil'</code>) or Cramér’s V one (<code>nom_nom_assoc = 'cramer'</code>) for categorical variables and that’s it:</p>
<pre><code>ass = associations(df, nom_nom_assoc = &#39;theil&#39;,
                   num_num_assoc = &#39;pearson&#39;,
                   figsize=(10,10), clustering=True)</code></pre>
<p>As a result, you'll get a beautiful heatmap that helps you understand at a glance which columns have the strongest correlation:</p>
<figure>
<img src="../media/file301.png" alt="Figure 11.36 – Correlation heatmap" /><figcaption aria-hidden="true">Figure 11.36 – Correlation heatmap</figcaption>
</figure>
<p>As you may have noticed, you can also select the type of correlation to be used between numeric variables (Pearson, Spearman, Kendall) via the <code>num_num_assoc</code> parameter. Moreover, you can access the coefficient dataframe using the code <code>ass['corr']</code>.</p>
<p>Let's now see how to implement the same things in R.</p>
</section>
<section id="implementing-correlation-coefficients-in-r-1" class="level3" data-number="12.4.4">
<h3 data-number="12.4.4">Implementing correlation coefficients in R</h3>
<p>There is no R package on CRAN similar to Dython that allows you to calculate correlations between columns in a tibble regardless of their data type. Nor is there a package in which the correlation ratio as previously defined is defined. Therefore, based on the source code of the Dython package, we created them from scratch.</p>
<p>Instead, we used some CRAN packages that expose some of the correlation functions introduced in the previous sections. In particular, these packages are as follows:</p>
<ul>
<li><strong>rstatix</strong> (<a href="https://github.com/kassambara/rstatix">https://github.com/kassambara/rstatix</a>), an intuitive pipe-friendly framework for basic statistical tests. We used it for the <code>cramer_v()</code> function.</li>
<li><strong>DescTools</strong> (<a href="https://andrisignorell.github.io/DescTools/">https://andrisignorell.github.io/DescTools/</a>), a collection of miscellaneous basic statistics functions. We used it for the <code>UncertCoef()</code> function.</li>
<li><strong>vcd</strong> (<a href="https://cran.r-project.org/web/packages/vcd/index.html">https://cran.r-project.org/web/packages/vcd/index.html</a>), a collection of visualization techniques and tools for categorical data.</li>
<li><strong>sjPlot</strong> (<a href="https://strengejacke.github.io/sjPlot/">https://strengejacke.github.io/sjPlot/</a>), a collection of plotting and table output functions for data visualization.</li>
</ul>
<p>So, you need to install these packages in the most recent version of CRAN R you have:</p>
<ol>
<li>Open RStudio and make sure it is referencing your latest CRAN R (version 4.0.2 in our case).</li>
<li>Click on the <strong>Console</strong> window and enter this command: <code>install.packages('rstatix')</code>. Then press <em>Enter</em>.</li>
<li>Click on the <strong>Console</strong> window and enter this command: <code>install.packages('DescTools')</code>. Then press <em>Enter</em>.</li>
<li>Click on the <strong>Console</strong> window and enter this command: <code>install.packages('vcd')</code>. Then press <em>Enter</em>.</li>
<li>Click on the <strong>Console</strong> window and enter this command: <code>install.packages('sjPlot')</code>. Then press <em>Enter</em>.</li>
</ol>
<p>The code you'll find in this section is available in the file <code>03-titanic-survive-class-analysis-in-r.R</code> in the <code>Chapter11\R</code> folder.</p>
<p>After loading the most important libraries, again we created a helper function to graph a violin plot. After that, we defined the functions <code>correlation_ratio()</code> to calculate eta and <code>calc_corr()</code> to calculate the correlation of a tibble regardless of the data type of the given columns.</p>
<p>At this point, you can load the Titanic disaster data from a CSV file exposed on the web and transform the numerical columns <code>Survived</code> and <code>Pclass</code> (passenger class) into factors, since they are categorical variables:</p>
<pre><code>dataset_url &lt;- &#39;http://bit.ly/titanic-data-csv&#39;
tbl &lt;- read_csv(dataset_url)
tbl &lt;- tbl %&gt;% 
    mutate( across(c(&#39;Survived&#39;, &#39;Pclass&#39;), as.factor) )</code></pre>
<p>It is then possible to calculate <em>Cramér's V</em> coefficient between the above two categorical variables:</p>
<pre><code>rstatix::cramer_v(x=tbl$Survived, y=tbl$Pclass)</code></pre>
<p>It returns a value of <code>0.34</code>, indicating a medium association strength.</p>
<p>You can also calculate <em>Theil's U</em> uncertainty coefficient of the <code>Survived</code> variable given the <code>Pclass</code> variable:</p>
<pre><code>DescTools::UncertCoef(tbl$Survived, tbl$Pclass, direction = &#39;row&#39;)</code></pre>
<p>The returned value is <code>0.087</code>. Also, in this case, we can calculate the <em>correlation ratio η</em> between the numeric variable <code>Age</code> and the categorical <code>Pclass</code> one:</p>
<pre><code>correlation_ratio( categories = tbl$Pclass, measurements = tbl$Age, numeric_replace_value = 0)</code></pre>
<p>The result is <code>0.366</code>. Now you can get the correlation value for each couple of columns regardless of their data type. We implemented the <code>calc_corr()</code> function for this purpose. You just have to specify whether you want to use Theil’s U coefficient (<code>theil_uncert=True</code>) or Cramér’s V one (<code>theil_uncert=False</code>) for categorical variables and that’s it:</p>
<pre><code># Create two data frames having the only column containing the tibble column names as values
row &lt;- data.frame(row=names(tbl))
col &lt;- data.frame(col=names(tbl))
# Create the cross join data frame from the previous two ones
ass &lt;- tidyr::crossing(row, col)
# Add the corr column containing correlation values
corr_tbl &lt;- ass %&gt;% 
    mutate( corr = map2_dbl(row, col, ~ calc_corr(data = tbl, row_name = .x, col_name = .y, theil_uncert = T)) )
corr_tbl</code></pre>
<p>You'll see something like this as a result:</p>
<figure>
<img src="../media/file302.png" alt="Figure 11.37 – Correlation tibble you get from the Titanic one" /><figcaption aria-hidden="true">Figure 11.37 – Correlation tibble you get from the Titanic one</figcaption>
</figure>
<p>You can also plot a heatmap of the correlation tibble just created using the following code:</p>
<pre><code>corr_tbl %&gt;% 
    ggplot( aes(x=row, y=col, fill=corr) ) +
    geom_tile() +
    geom_text(aes(row, col, label = round(corr, 2)), color = &quot;white&quot;, size = 4)</code></pre>
<p>Here is the result:</p>
<figure>
<img src="../media/file303.png" alt="Figure 11.38 – Heatmap of the correlation tibble" /><figcaption aria-hidden="true">Figure 11.38 – Heatmap of the correlation tibble</figcaption>
</figure>
<p>Awesome! You have just implemented all of the correlation coefficients studied in this chapter in R as well. Let's now see how to implement them in Power BI, both with Python and with R.</p>
</section>
<section id="implementing-correlation-coefficients-in-power-bi-with-python-and-r-1" class="level3" data-number="12.4.5">
<h3 data-number="12.4.5">Implementing correlation coefficients in Power BI with Python and R</h3>
<p>Obviously, Power BI was not born as an advanced statistical analysis tool, so you will not find, for example, all you need to implement all the correlation coefficients seen in the previous sections. In this case, the support given by Python and R is of fundamental importance for this purpose.</p>
<p>Also, in this case, due to the simplicity of the code, we will implement the correlation coefficients in both Python and R in one project.</p>
<p>First, make sure that Power BI Desktop references the correct versions of Python and R in <strong>Options</strong>. Then follow these steps:</p>
<ol>
<li>Click on <strong>Get Data</strong>, select <strong>Web</strong>, and click on <strong>Connect</strong>.</li>
<li>Enter the <code>http://bit.ly/titanic-dataset-csv</code> URL into the <strong>URL</strong> textbox and click <strong>OK</strong>.</li>
<li>You’ll see a preview of the data. Then click <strong>Transform Data</strong>.</li>
<li>Click <strong>Transform</strong> on the ribbon and then <strong>Run Python script</strong>.</li>
<li>Enter the script you can find in the file <code>04-correlation-analysis-in-power-bi-with-python.py</code> into the <code>Chapter11\Python</code> folder.</li>
<li>We are only interested in the data in <code>result_df</code>. So, click its <strong>Table</strong> value.</li>
<li>You’ll see the preview of all correlation coefficients between all the columns of the dataset.</li>
<li>Click <strong>Home</strong> on the ribbon and then click <strong>Close &amp; Apply</strong>.</li>
<li>Repeat steps 1 to 3.</li>
<li>Click <strong>Transform</strong> on the ribbon and then <strong>Run R script</strong>.</li>
<li>Enter the script you can find in the file <code>04-correlation-analysis-in-power-bi-with-r.R</code> in the <code>Chapter11\R</code> folder.</li>
<li>We are only interested in the data in <code>corr_tbl</code>. So, click its <strong>Table</strong> value.</li>
<li>You’ll see the preview of all correlation coefficients between all the columns of the dataset.</li>
<li>Click <strong>Home</strong> on the ribbon and then click <strong>Close &amp; Apply</strong>.</li>
</ol>
<p>Amazing! You have just calculated the correlation coefficients for all the numeric columns of a source dataset in Power BI with both Python and R!</p>
</section>
</section>
<section id="summary-10" class="level2" data-number="12.5">
<h2 data-number="12.5">Summary</h2>
<p>In this chapter, you learned how to calculate the correlation coefficient for two numerical variables according to Pearson, Spearman, and Kendall. Then you also learned how to calculate it for two categorical variables thanks to Cramér's V and Theil's uncertainty coefficient. Finally, you also learned how to calculate it for one numeric and one categorical variable thanks to the correlation ratio.</p>
<p>In the next chapter, you will see how statistics are really important for determining outliers and imputing missing values in your dataset.</p>
</section>
<section id="references-7" class="level2" data-number="12.6">
<h2 data-number="12.6">References</h2>
<p>For additional reading, check out the following books and articles:</p>
<ol>
<li><em>Spearman's Rank-Order Correlation</em> (<a href="https://statistics.laerd.com/statistical-guides/spearmans-rank-order-correlation-statistical-guide.php">https://statistics.laerd.com/statistical-guides/spearmans-rank-order-correlation-statistical-guide.php</a>)</li>
<li><em>Concordant Pairs and Discordant Pairs</em> (<a href="https://www.statisticshowto.com/concordant-pairs-discordant-pairs/">https://www.statisticshowto.com/concordant-pairs-discordant-pairs/</a>)</li>
<li><em>Mosaic Plot and Chi-Square Test</em> (<a href="https://towardsdatascience.com/mosaic-plot-and-chi-square-test-c41b1a527ce4">https://towardsdatascience.com/mosaic-plot-and-chi-square-test-c41b1a527ce4</a>)</li>
<li><em>Understanding Boxplots</em> (<a href="https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51">https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51</a>)</li>
<li><em>Violin plots explained</em> (<a href="https://towardsdatascience.com/violin-plots-explained-fb1d115e023d">https://towardsdatascience.com/violin-plots-explained-fb1d115e023d</a>)</li>
<li><em>The Search for Categorical Correlation</em> (<a href="https://towardsdatascience.com/the-search-for-categorical-correlation-a1cf7f1888c9">https://towardsdatascience.com/the-search-for-categorical-correlation-a1cf7f1888c9</a>)</li>
</ol>
</section>
</section>
</body>
</html>
