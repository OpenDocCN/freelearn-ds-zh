- en: Python Geoprocessing with Hadoop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most of the examples in this book worked with relatively small datasets using
    a single computer. But as data gets larger, the datasets and even individual files
    may be spread out over a cluster of machines. Working with big data requires different
    tools. In this chapter, you will learn how to use Apache Hadoop to work with big
    data, and the Esri GIS tools for Hadoop to work with the big data spatially.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will teach you how to:'
  prefs: []
  type: TYPE_NORMAL
- en: Install Linux
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Install and run Docker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Install and configure a Hadoop environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Work with files in HDFS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basic queries using Hive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Install the Esri GIS tools for Hadoop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform spatial queries in Hive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is Hadoop?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Hadoop is an open-source framework for working with large quantities of data
    spread across a single computer to thousands of computers. Hadoop is composed
    of four modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hadoop Core**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hadoop Distributed File System** (**HDFS**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Yet Another Resource Negotiator** (**YARN**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MapReduce**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Hadoop Core makes up the components needed to run the other three modules.
    HDFS is a Java-based file system that has been designed to be distributed and
    is capable of storing large files across many machines. By large files, we are
    talking terabytes. YARN manages the resources and scheduling in your Hadoop framework.
    The MapReduce engine allows you to process data in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: There are several other projects that can be installed to work with the Hadoop
    framework. In this chapter, you will use Hive and Ambari. Hive allows you to read
    and write data using SQL. You will use Hive to run the spatial queries on your
    data at the end of the chapter. Ambari provides a web user interface to Hadoop
    and Hive. In this chapter, you will use it to upload files and to enter your queries.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have an overview of Hadoop, the next section will show you how
    to set up your environment.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the Hadoop framework
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you will not configure each of the Hadoop framework components
    yourself. You will run a Docker image, which requires you to install Docker. Currently,
    Docker runs on Windows 10 Pro or Enterprise, but it runs much better on Linux
    or Mac. Hadoop also runs on Windows but requires you to build it from source,
    and so it will be much easier to run it on Linux. Also, the Docker image you will
    use is running Linux, so getting familiar with Linux may be beneficial. In this
    section, you will learn how to install Linux.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Linux
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first step to set up the Hadoop framework is to install Linux. You will
    need to get a copy of a Linux operating system. There are many flavors of Linux.
    You can choose whichever version you like, however, this chapter was written using
    CentOS 7 because most of the tools you will be installing have also been tested
    on CentOS. CentOS is a Red Hat-based version of Linux. You can download an ISO
    at: [https://www.centos.org/](https://www.centos.org/). Select Get CentOS Now.
    Then, select DVD image. Choose a mirror to download the ISO.
  prefs: []
  type: TYPE_NORMAL
- en: After downloading the image, you can burn it to a disk using Windows. Once you
    have burned the disk, place it in the machine that will run Linux and start it.
    The installation will prompt you along the way. Two steps to pay attention to
    are the software selection step and the partitioning. For software selection,
    choose GNOME Desktop. This will provide a sufficient base system with a popular
    GUI. If you have another file system on the computer, you can overwrite it or
    select the free space on the partitioning screen.
  prefs: []
  type: TYPE_NORMAL
- en: For a more detailed explanation of how to install Linux, Google is your friend.
    There are many excellent walkthroughs and YouTube videos that will walk you through
    it. Unfortunately, it looks as though the CentOS website does not have an installation
    manual for CentOS 7.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Docker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Docker provides the software so that you can run containers. A **container**
    is an executable that contains everything you need to run the software it contains.
    For example, if I have a Linux system configured to run Hadoop, Hive, and Ambari,
    and I create a container from it, I can give you the container, and when you run
    it, it will contain everything you need for that system to work, no matter the
    configuration or software installed on your computer. The same applies if I give
    that container image to any other person. It will always run the same. A container
    is not a virtual machine. A virtual machine is an abstraction at the hardware
    level and a container is an abstraction at the application layer. A container
    has everything you need to run a piece of software. For this chapter, that is
    all you need to know.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you have Linux installed and have an understanding of what Docker
    is, you can install a copy of Docker. Using your terminal, enter the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command uses the `curl` application to download and install the
    latest version of Docker. The parameters tell `curl` to, in order, fail silently
    on server errors, do not show progress, report any errors, and redirect if the
    server says the location has changed. The output of the `curl` command is piped
    - `|` - to the `sh` (Bash shell) to execute.
  prefs: []
  type: TYPE_NORMAL
- en: 'When Docker has installed, you can run it by executing the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous command uses `sudo` to run the command as an administrator (`root`).
    Think of this as right-clicking in Windows and selecting the Run as administrator
    option. The next command is `systemctl`. This is how you start services in Linux.
    Lastly, `start docker` does exactly that, it starts `docker`. If you receive an
    error when executing the earlier mentioned command that mentions sudoers, your
    user may not have permission to run applications as the `root`. You will need
    to log in as the `root` (or use the `su` command) and edit the text file at `/etc/sudoers`.
    Add the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous line will give you permission to use `sudo`. Your `/etc/sudoers`
    file should look like the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/737f6f46-0fa2-4f2d-94ce-b99d42273c68.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that you have `docker` running, you can download the image we will load
    which contains the Hadoop framework.
  prefs: []
  type: TYPE_NORMAL
- en: Install Hortonworks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Instead of installing Hadoop and all the other components, you will use a preconfigured
    Docker image. Hortonworks has a Data Platform Sandbox that already has a container
    which you can load in Docker. To download it, go to [https://hortonworks.com/downloads/#sandbox](https://hortonworks.com/downloads/#sandbox) and
    select DOWNLOAD FOR DOCKER.
  prefs: []
  type: TYPE_NORMAL
- en: You will also need to install the `start_sandox_hdp_version.sh` script. This
    will simplify the launching of the container in Docker. You can download the script
    from GitHub at: [https://gist.github.com/orendain/8d05c5ac0eecf226a6fed24a79e5d71a](https://gist.github.com/orendain/8d05c5ac0eecf226a6fed24a79e5d71a)[.](https://raw.githubusercontent.com/hortonworks/data-tutorials/master/tutorials/hdp/sandbox-deployment-and-install-guide/assets/start_sandbox-hdp.sh.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you will need to load the image in Docker. The following command will show
    you how:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous command loads the image into Docker. The image name will be similar
    to `HDP_2.6.3_docker_10_11_2017.tar`, but it will change depending on your version.
    To see that the sandbox has been loaded, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The output, if you have no other containers, should look as it does in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/56dc5a44-976b-4eb5-9a74-9b5f075fc8f2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In order to use the web-based GUI Ambari, you will want to have a domain name
    established for the sandbox. To do that, you will need the IP address of the container.
    You can get it by running two commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The first command will have the `container ID`, and the second command will
    take the `container ID` and return a lot of information, with the IP address being
    towards the end. Or, you can take advantage of the Linux command line and just
    get the IP address by using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The previous command wraps the previously mentioned commands into a single command.
    The `docker inspect` command takes the output of `docker ps` as the `container
    ID`. It does so by wrapping it in `$()`, but it also passes a filter so that only
    the `ID` is returned. Then, the `inspect` command also includes a filter to only
    return the IP address. The text between the `{{}}` is a Go template. The output
    of this command should be an IP address, for example, 172.17.0.2.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you have the IP address of the image, you should update your host''s
    file using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The previous command redirects the output of the `echo`—which is the text you
    want in your `/etc/hosts` file and sends it to the `sudo tee -a /etc/hosts` command.
    This second command uses `sudo` to run as `root`. The `tee` command sends the
    output to a file and to the terminal (`STDOUT`). The `-a` tells `tee` to append
    to the file, and `/etc/hosts` is the file you want to append. Now, in your browser,
    you will be able to use names instead of the IP address.
  prefs: []
  type: TYPE_NORMAL
- en: Now you are ready to launch the image and browse to your Hadoop framework.
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, you will launch your Hadoop image and learn how to connect
    using `ssh` and Ambari. You will also move files and perform a basic Hive query.
    Once you understand how to interact with the framework, the next section will
    show you how to use a spatial query.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, from the terminal, launch the Hortonworks Sandbox using the provided
    Bash script. The following command will show you how:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous command executes the script you downloaded with the sandbox. Again,
    it used `sudo` to run as `root`. Depending on your machine, it may take some time
    to completely load and start all the services. When it is done, your terminal
    should look like it does in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/da573ba6-8cc1-4b01-b171-4d716cd3555b.png)'
  prefs: []
  type: TYPE_IMG
- en: Connecting via Secure Shell
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that the sandbox is running, you can connect using Secure Shell (SSH).
    The secure shell allows you to log in remotely to another machine. Open a new
    terminal and enter the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous command uses `ssh` to connect as user `raj_ops` to the `localhost`
    (`127.0.0.1`) on port `2222`. You will get a warning that the authenticity of
    the host cannot be established. We did not create any keys for `ssh`. Just type
    `yes` and you will be prompted for the password. The user `raj_ops` has the password
    `raj_ops`. Your terminal prompt should now look like the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: If your terminal is like it is in the previous code, you are now logged into
    the container.
  prefs: []
  type: TYPE_NORMAL
- en: For more information on users, their permissions, and configuring the sandbox,
    go to the following page: [https://hortonworks.com/tutorial/learning-the-ropes-of-the-hortonworks-sandbox/](https://hortonworks.com/tutorial/learning-the-ropes-of-the-hortonworks-sandbox/)
  prefs: []
  type: TYPE_NORMAL
- en: You can now use most Linux commands to navigate the container. You can now download
    and move files around, run Hive, and all your other tools from the command line.
    This section has already been heavy enough on Linux, so you will not use the command
    line exclusively in this chapter. Instead, the next section will show you how
    to execute these tasks in Ambari, a web-based GUI for executing tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Ambari
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Ambari is a UI for making the management of Hadoop easier. In the previous
    section, you learned how to implement `ssh` into the container. From there you
    could manage Hadoop, run Hive queries, download data, and add it to the HDFS file
    system. Ambari makes all of this much simpler, especially if you are not familiar
    with the command line. To open Ambari, browse to the URL as: [http://sandbox.hortonworks.com:8080/](http://sandbox.hortonworks.com:8080/).'
  prefs: []
  type: TYPE_NORMAL
- en: The Ambari URL depends on your installation. If you have followed the instructions
    in this chapter, then this will be your URL. You must also have started the server
    from the Docker image.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will be directed to the Ambari login page. Enter the user/password combination
    of `raj_ops`/`raj_ops`, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/65c4ceb9-cceb-4d6a-b092-ff998fcf28ee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After logging in, you will see the Ambari Dashboard. It will look like it does
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3eae97d4-ff3a-4642-86be-336f5f71d124.png)'
  prefs: []
  type: TYPE_IMG
- en: On the left, you have a list of services. The main portion of the window contains
    the metrics, and the top menu bar has tabs for different functions. In this chapter,
    you will use the square comprised of nine smaller squares. Hover over the square
    icon and you will see a drop-down for the files view.
  prefs: []
  type: TYPE_NORMAL
- en: This is the `root` directory of the HDFS file system.
  prefs: []
  type: TYPE_NORMAL
- en: When connected to the container via `ssh`, run the `hdfs dfs -ls /` command
    and you will see the same directory structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'From here, you can upload files. To try it out, open a text editor and create
    a simple CSV. This example will use the following data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Save the CSV file and then click the Upload button in Ambari. You will be able
    to drag and drop the CSV to the browser. Ambari added the file to the HDFS file
    system on the container:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bdd196ed-7da5-4458-afca-231aea3689d5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now that you have data loaded in the container, you can query it in Hive using
    SQL. Using the square icon again, select the drop-down for Hive View 2.0\. You
    should see a workspace as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/384475d8-8bd7-4fc7-93e9-3d5d53389ba0.png)'
  prefs: []
  type: TYPE_IMG
- en: In Hive, you have worksheets. On the worksheet, you have the database you are
    connected to, which in this case is the default. Underneath that, you have the
    main query window. To the right, you have a list of existing tables. Lastly, scrolling
    down, you will see the Execute button, and under that is where the results will
    be loaded.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the query pane, enter the SQL query as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous query is a basic select all in SQL. The results will be shown
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5d00e72c-b222-454c-93bc-ca3253e11597.png)'
  prefs: []
  type: TYPE_IMG
- en: Esri GIS tools for Hadoop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With your environment set up and some basic knowledge of Ambari, HDFS, and Hive,
    you will now learn how to add a spatial component to your queries. To do so, we
    will use the Esri GIS tools for Hadoop.
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to download the files located at the GitHub repository, which
    is located at: [https://github.com/Esri/gis-tools-for-hadoop](https://github.com/Esri/gis-tools-for-hadoop).
    You will be using Ambari to move the files to HDFS not the container, so download
    these files to your local machine.
  prefs: []
  type: TYPE_NORMAL
- en: Esri has a tutorial for downloading the files by using `ssh` to connect to the
    container and then using `git` to clone the repository. You can follow these instructions
    here: [https://github.com/Esri/gis-tools-for-hadoop/wiki/GIS-Tools-for-Hadoop-for-Beginners](https://github.com/Esri/gis-tools-for-hadoop/wiki/GIS-Tools-for-Hadoop-for-Beginners).
  prefs: []
  type: TYPE_NORMAL
- en: 'You can download the files by using the GitHub Clone or download button on
    the right-hand side of the repository. To unzip the archive, use one of the following
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The first command will unzip the file in the current directory, which is most
    likely the `Downloads` folder of your home directory. The second command will
    unzip the file, but by passing `-d` and a path, it will unzip to that location.
    In this case, this is the `root` of my home directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you have the files unzipped, you can open the Files View in Ambari
    by selecting it from the box icon drop-down menu. Select Upload and a modal will
    open, allowing you to drop a file. On your local machine, browse to the location
    of the Esri **Java ARchive** (JAR) files. If you moved the zip to your home directory,
    the path will be similar to `/home/pcrickard/gis-tools-for-hadoop-master/samples/lib`.
    You will have three JAR files:'
  prefs: []
  type: TYPE_NORMAL
- en: '`esri-geometry-api-2.0.0.jar`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spatial-sdk-hive-2.0.0.jar`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spatial-sdk-json-2.0.0.jar`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Move each of these three files to the `root` folder in Ambari. This is the `/`
    directory, which is the default location that opens when you launch Files View.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you would normally move the data to HDFS as well, however, you did that
    in the previous example. In this example, you will leave the data files on your
    local machine and you will learn how you can load them into a Hive table without
    being on HDFS.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you are ready to execute the spatial query in Hive. From the box icon drop-down,
    select Hive View 2.0\. In the query pane, enter the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the preceding code will take some time depending on your machine. The
    end result will look like it does in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d09ff04-db16-420f-8c00-85d0ebc55456.png)'
  prefs: []
  type: TYPE_IMG
- en: The previous code and results were presented without explanation so that you
    could get the example to work and see the output. Following that, the code will
    be explained block by block.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first block of code is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This block adds the JAR files from the HDFS location. In this case, it is the
    `/` folder. Once the code loads the JAR files, it can then create the functions
    `ST_Point` and `ST_Contains` by calling the classes in the JAR files. A JAR file
    may contain many Java files (classes). The order of the `add jar` statements matter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following block drops two tables—`earthquakes` and `counties`. If you had
    never run the example, you could skip these lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, the code creates the tables for `earthquakes` and `counties`. The `earthquakes`
    table is created and each field and type are passed to `CREATE`. The row format
    is specified as CSV—the `'',''`. Lastly, it is in a text file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The `counties` table is created in a similar fashion by passing the field names
    and types to `CREATE`, but the data is in JSON format and will use the `com.esri.hadoop.hive.serde.EsriJSonSerDe`
    class in the JAR `spatial-sdk-json-2.0.0` that you imported. `STORED AS INPUTFORMAT`
    and `OUTPUTFORMAT` are required for Hive to know how to parse and work with the
    JSON data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The next two blocks load the data into the created tables. The data exists
    on your local machine and not on HDFS. To use the local data without first loading
    it in HDFS, you can use the `LOCAL` command with `LOAD DATA INPATH` and specify
    the local path of the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'With the JAR files loaded and the tables created and populated with data, you
    can now run a spatial query using the two defined functions—`ST_Point` and `ST_Contains`.
    These are used as in the examples from [Chapter 3](42c1ea5a-7372-4688-bb7f-fc3822248562.xhtml), *Introduction
    to Geodatabases*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous query selects the `name` of the county and the `count` of earthquakes
    by passing the county geometry and the location of each earthquake as a point
    to `ST_Contains`. The results are shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d09ff04-db16-420f-8c00-85d0ebc55456.png)'
  prefs: []
  type: TYPE_IMG
- en: HDFS and Hive in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This book is about Python for geospatial development, so in this section, you
    will learn how to use Python for HDFS operations and Hive queries. There are several
    database wrapper libraries with Python and Hadoop, but it does not seem like a
    single library has become a standout go-to library, and others, like Snakebite,
    don't appear ready to run on Python 3\. In this section, you will learn how to
    use two libraries—PyHive and PyWebHDFS. You will also learn how you can use the
    Python subprocess module to execute HDFS and Hive commands.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get PyHive, you can use `conda` and the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'You may also need to install the `sasl` library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous libraries will give you the ability to run Hive queries from Python.
    You will also want to be able to move files to HDFS. To do so, you can install
    `pywebhdfs`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command will install the library, and as always, you can also
    use `pip` install or use any other method.
  prefs: []
  type: TYPE_NORMAL
- en: With the libraries installed, let's first look at `pywebhdfs`.
  prefs: []
  type: TYPE_NORMAL
- en: The documentation for `pywebhdfs` is located at: [http://pythonhosted.org/pywebhdfs/](http://pythonhosted.org/pywebhdfs/)
  prefs: []
  type: TYPE_NORMAL
- en: 'To make a connection in Python, you need to know the location of your Hive
    server. If you have followed this chapter, particularly the configuration changes
    in `/etc/hosts`—you can do so using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The previous code imports the `PyWebHdfsClient` as `h`. It then creates the
    connection to the HDFS file system running in the container. The container is
    mapped to `sandbox. hortonworks.com`, and HDFS is on port `50070`. Since the examples
    have been using the `raj_ops` user, the code did so as well.
  prefs: []
  type: TYPE_NORMAL
- en: The functions now available to the `hdfs` variable are similar to your standard
    terminal commands, but with a different name—`mkdir` is now `make_dir` and `ls`
    is `list_dir`. To delete a file or directory, you will use `delete_file_dir`.
    The `make` and `delete` commands will return `True` if they are successful.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the `root` directory of our HDFS file system using Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The previous code issued the `list_dir` command (`ls` equivalent) and assigned
    it to `ls`. The result is a dictionary with all the files and folders in the directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see a single record, you can use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The previous code gets to the individual records by using the dictionary keys
    `FileStatuses` and `FileStatus`.
  prefs: []
  type: TYPE_NORMAL
- en: To get the keys in a dictionary, you can use `.keys(). ls.keys()` which returns
    `[FileStatuses]`, and `ls['FileStatuses'].keys()` which returns `['FileStatus']`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the previous code is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Each file or directory contains several pieces of data, but most importantly
    the type, owner, and permissions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step to running a Hive query example is to move our data files from
    the local machine to HDFS. Using Python, you can accomplish this using the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The previous code creates a directory called `samples` with the permissions
    `755`. In Linux, permissions are based on read (`4`), write (`2`), and execute
    (`1`) for three types of users—owner, and group, other. So, permissions of `755`
    mean that the owner has read, write, and execute permissions (4+2+1 =7), and that
    the group and others have read and execute (4+1=5).
  prefs: []
  type: TYPE_NORMAL
- en: Next, the code opens and reads the CSV file we want to transfer to HDFS and
    assigns it to the variable `d`. The code then creates the `sample.csv` file in
    the `samples` directory, passing the contents of `d`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To verify that the file was created, you can read the contents of the file
    using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The output of the previous code will be a string of the CSV file. It was created
    successfully.
  prefs: []
  type: TYPE_NORMAL
- en: 'Or, you can use the following code to get the `status` and details of the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous code will return the details as follows, but only if the file
    or directory exists. If it does not, the preceding code will raise a `FileNotFound`
    error. You can wrap the preceding code in a `try`...`except` block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: With the data file transferred to HDFS, you can move on to querying the data
    with Hive.
  prefs: []
  type: TYPE_NORMAL
- en: The documentation for PyHive is located at: [https://github.com/dropbox/PyHive](https://github.com/dropbox/PyHive)
  prefs: []
  type: TYPE_NORMAL
- en: 'Using `pyhive`, the following code will create a table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous code imports `pyhive` as `hive`. It creates a connection and gets
    the `cursor`. Lastly, it executes a Hive statement. Once you have the connection
    and the `cursor`, you can make your SQL queries by wrapping them in the `.execute()`
    method. To load the data from the CSV in HDFS into the table and then to select
    all, you would use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code uses the `execute()` method two more times to load the data
    and then executes select all. Using `fetchall()`, the results are passed to the
    `result` variable and will look like they do in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Working with `pyhive` is just like working with `psycopg2` — the Python library
    for connecting to PostgreSQL. Most database wrapper libraries are very similar
    in that you make a connection, get a `cursor`, and then execute statements. Results
    can be retrieved by grabbing all, one, or next (iterable).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to set up a Hadoop environment. This required
    you to install Linux and Docker to download an image from Hortonworks, and to
    learn the ropes of that environment. Much of this chapter was spent on the environment
    and how to perform a spatial query using the GUI tools provided. This is because
    the Hadoop environment is complex and without a proper understanding, it would
    be hard to fully understand how to use it with Python. Lastly, you learned how
    to use HDFS and Hive in Python. The Python libraries for working with Hadoop,
    Hive, and HDFS are still developing. This chapter provided you with a foundation
    so that when these libraries improve, you will have enough knowledge of Hadoop
    and the accompanying technologies to implement these new Python libraries.
  prefs: []
  type: TYPE_NORMAL
