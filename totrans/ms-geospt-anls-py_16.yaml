- en: Python Geoprocessing with Hadoop
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Hadoop 进行 Python 地理处理
- en: Most of the examples in this book worked with relatively small datasets using
    a single computer. But as data gets larger, the datasets and even individual files
    may be spread out over a cluster of machines. Working with big data requires different
    tools. In this chapter, you will learn how to use Apache Hadoop to work with big
    data, and the Esri GIS tools for Hadoop to work with the big data spatially.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的大多数示例都使用了相对较小的数据集，并且在一个计算机上工作。但随着数据量的增加，数据集甚至单个文件可能会分布在机器集群中。处理大数据需要不同的工具。在本章中，你将学习如何使用
    Apache Hadoop 处理大数据，以及 Esri GIS 工具用于 Hadoop 以空间方式处理大数据。
- en: 'This chapter will teach you how to:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将教你如何：
- en: Install Linux
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装 Linux
- en: Install and run Docker
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装和运行 Docker
- en: Install and configure a Hadoop environment
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装和配置 Hadoop 环境
- en: Work with files in HDFS
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 HDFS 中处理文件
- en: Basic queries using Hive
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Hive 进行基本查询
- en: Install the Esri GIS tools for Hadoop
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装 Esri GIS 工具用于 Hadoop
- en: Perform spatial queries in Hive
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Hive 中执行空间查询
- en: What is Hadoop?
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是 Hadoop？
- en: 'Hadoop is an open-source framework for working with large quantities of data
    spread across a single computer to thousands of computers. Hadoop is composed
    of four modules:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 是一个开源框架，用于处理分布在单台计算机到数千台计算机上的大量数据。Hadoop 由四个模块组成：
- en: '**Hadoop Core**'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hadoop 核心**'
- en: '**Hadoop Distributed File System** (**HDFS**)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hadoop 分布式文件系统**（**HDFS**）'
- en: '**Yet Another Resource Negotiator** (**YARN**)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**另一个资源协调器**（**YARN**）'
- en: '**MapReduce**'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MapReduce**'
- en: The Hadoop Core makes up the components needed to run the other three modules.
    HDFS is a Java-based file system that has been designed to be distributed and
    is capable of storing large files across many machines. By large files, we are
    talking terabytes. YARN manages the resources and scheduling in your Hadoop framework.
    The MapReduce engine allows you to process data in parallel.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 核心构成了运行其他三个模块所需的所有组件。HDFS 是一个基于 Java 的文件系统，它被设计成分布式，并且能够在多台机器上存储大量文件。当我们说大量文件时，我们指的是千兆字节级别的文件。YARN
    管理你的 Hadoop 框架中的资源和调度。MapReduce 引擎允许你并行处理数据。
- en: There are several other projects that can be installed to work with the Hadoop
    framework. In this chapter, you will use Hive and Ambari. Hive allows you to read
    and write data using SQL. You will use Hive to run the spatial queries on your
    data at the end of the chapter. Ambari provides a web user interface to Hadoop
    and Hive. In this chapter, you will use it to upload files and to enter your queries.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个其他项目可以安装以与 Hadoop 框架一起使用。在本章中，你将使用 Hive 和 Ambari。Hive 允许你使用 SQL 读取和写入数据。你将在本章末使用
    Hive 运行数据的空间查询。Ambari 为 Hadoop 和 Hive 提供了一个网络用户界面。在本章中，你将使用它上传文件并输入你的查询。
- en: Now that you have an overview of Hadoop, the next section will show you how
    to set up your environment.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经对 Hadoop 有了一个概述，下一节将展示如何设置你的环境。
- en: Installing the Hadoop framework
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装 Hadoop 框架
- en: In this chapter, you will not configure each of the Hadoop framework components
    yourself. You will run a Docker image, which requires you to install Docker. Currently,
    Docker runs on Windows 10 Pro or Enterprise, but it runs much better on Linux
    or Mac. Hadoop also runs on Windows but requires you to build it from source,
    and so it will be much easier to run it on Linux. Also, the Docker image you will
    use is running Linux, so getting familiar with Linux may be beneficial. In this
    section, you will learn how to install Linux.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将不会自己配置 Hadoop 框架的每个组件。你将运行一个 Docker 镜像，这需要你安装 Docker。目前，Docker 在 Windows
    10 专业版或企业版上运行，但在 Linux 或 Mac 上运行得更好。Hadoop 也可以在 Windows 上运行，但需要你从源代码构建，因此它将在 Linux
    上运行得更容易。此外，你将使用的 Docker 镜像正在运行 Linux，因此熟悉 Linux 可能会有所帮助。在本节中，你将学习如何安装 Linux。
- en: Installing Linux
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装 Linux
- en: The first step to set up the Hadoop framework is to install Linux. You will
    need to get a copy of a Linux operating system. There are many flavors of Linux.
    You can choose whichever version you like, however, this chapter was written using
    CentOS 7 because most of the tools you will be installing have also been tested
    on CentOS. CentOS is a Red Hat-based version of Linux. You can download an ISO
    at: [https://www.centos.org/](https://www.centos.org/). Select Get CentOS Now.
    Then, select DVD image. Choose a mirror to download the ISO.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 设置 Hadoop 框架的第一步是安装 Linux。你需要获取一个 Linux 操作系统的副本。Linux 有很多版本。你可以选择你喜欢的任何版本，然而，这一章是使用
    CentOS 7 编写的，因为大多数你将要安装的工具也已经在 CentOS 上进行了测试。CentOS 是基于 Red Hat 的 Linux 版本。你可以在以下网址下载
    ISO：[https://www.centos.org/](https://www.centos.org/)。选择“立即获取 CentOS”。然后，选择 DVD
    图像。选择一个镜像来下载 ISO。
- en: After downloading the image, you can burn it to a disk using Windows. Once you
    have burned the disk, place it in the machine that will run Linux and start it.
    The installation will prompt you along the way. Two steps to pay attention to
    are the software selection step and the partitioning. For software selection,
    choose GNOME Desktop. This will provide a sufficient base system with a popular
    GUI. If you have another file system on the computer, you can overwrite it or
    select the free space on the partitioning screen.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 下载镜像后，你可以使用Windows将其烧录到磁盘上。一旦你烧录了磁盘，将其放入将要运行Linux的机器中并启动它。安装过程中会有提示。需要特别注意的两个步骤是软件选择步骤和分区。在软件选择步骤中，选择GNOME桌面。这将提供一个带有流行GUI的足够的基础系统。如果你在计算机上还有其他文件系统，你可以覆盖它或在分区屏幕上选择分区上的空闲空间。
- en: For a more detailed explanation of how to install Linux, Google is your friend.
    There are many excellent walkthroughs and YouTube videos that will walk you through
    it. Unfortunately, it looks as though the CentOS website does not have an installation
    manual for CentOS 7.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对于如何安装Linux的更详细说明，谷歌是你的朋友。有许多优秀的教程和YouTube视频会带你完成这个过程。不幸的是，看起来CentOS网站没有CentOS
    7的安装手册。
- en: Installing Docker
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装Docker
- en: Docker provides the software so that you can run containers. A **container**
    is an executable that contains everything you need to run the software it contains.
    For example, if I have a Linux system configured to run Hadoop, Hive, and Ambari,
    and I create a container from it, I can give you the container, and when you run
    it, it will contain everything you need for that system to work, no matter the
    configuration or software installed on your computer. The same applies if I give
    that container image to any other person. It will always run the same. A container
    is not a virtual machine. A virtual machine is an abstraction at the hardware
    level and a container is an abstraction at the application layer. A container
    has everything you need to run a piece of software. For this chapter, that is
    all you need to know.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Docker提供了软件，以便您能够运行容器。**容器**是一个可执行的程序，它包含了运行其中软件所需的所有内容。例如，如果我有一个配置了运行Hadoop、Hive和Ambari的Linux系统，并且从它创建了一个容器，我可以把容器给你，当你运行它时，它将包含运行该系统所需的所有内容，无论你的计算机配置或安装了什么软件。如果我把这个容器镜像给任何其他人，它也会始终以相同的方式运行。容器不是虚拟机。虚拟机是在硬件层面的抽象，而容器是在应用层面的抽象。容器包含了运行软件所需的一切。对于本章，这就是你需要知道的所有内容。
- en: 'Now that you have Linux installed and have an understanding of what Docker
    is, you can install a copy of Docker. Using your terminal, enter the following
    command:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经安装了Linux并且了解了Docker是什么，你可以安装一个Docker副本。使用你的终端，输入以下命令：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The preceding command uses the `curl` application to download and install the
    latest version of Docker. The parameters tell `curl` to, in order, fail silently
    on server errors, do not show progress, report any errors, and redirect if the
    server says the location has changed. The output of the `curl` command is piped
    - `|` - to the `sh` (Bash shell) to execute.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的命令使用`curl`应用程序下载并安装Docker的最新版本。参数告诉`curl`在服务器错误发生时静默失败，不显示进度，报告任何错误，并在服务器表示位置已更改时进行重定向。`curl`命令的输出被管道`-
    | -`传递到`sh`（Bash shell）以执行。
- en: 'When Docker has installed, you can run it by executing the following command:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 当Docker安装完成后，你可以通过执行以下命令来运行它：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The previous command uses `sudo` to run the command as an administrator (`root`).
    Think of this as right-clicking in Windows and selecting the Run as administrator
    option. The next command is `systemctl`. This is how you start services in Linux.
    Lastly, `start docker` does exactly that, it starts `docker`. If you receive an
    error when executing the earlier mentioned command that mentions sudoers, your
    user may not have permission to run applications as the `root`. You will need
    to log in as the `root` (or use the `su` command) and edit the text file at `/etc/sudoers`.
    Add the following line:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 上一条命令使用`sudo`以管理员（`root`）的身份运行命令。想象一下在Windows上右键点击并选择以管理员身份运行选项。下一条命令是`systemctl`。这是在Linux中启动服务的方式。最后，`start
    docker`正是这样做的，它启动了`docker`。如果你在执行前面提到的命令时收到提及sudoers的错误，那么你的用户可能没有权限以`root`身份运行应用程序。你需要以`root`身份登录（或使用`su`命令）并编辑`/etc/sudoers`中的文本文件。添加以下行：
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The previous line will give you permission to use `sudo`. Your `/etc/sudoers`
    file should look like the following screenshot:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 上一行将赋予你使用`sudo`的权限。你的`/etc/sudoers`文件应该看起来像以下截图：
- en: '![](img/737f6f46-0fa2-4f2d-94ce-b99d42273c68.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/737f6f46-0fa2-4f2d-94ce-b99d42273c68.png)'
- en: Now that you have `docker` running, you can download the image we will load
    which contains the Hadoop framework.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经运行了 `docker`，你可以下载我们将要加载的包含 Hadoop 框架的镜像。
- en: Install Hortonworks
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装 Hortonworks
- en: Instead of installing Hadoop and all the other components, you will use a preconfigured
    Docker image. Hortonworks has a Data Platform Sandbox that already has a container
    which you can load in Docker. To download it, go to [https://hortonworks.com/downloads/#sandbox](https://hortonworks.com/downloads/#sandbox) and
    select DOWNLOAD FOR DOCKER.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 与安装 Hadoop 和所有其他组件相比，你将使用预配置的 Docker 镜像。Hortonworks 有一个数据平台沙盒，它已经有一个可以在 Docker
    中加载的容器。要下载它，请访问 [https://hortonworks.com/downloads/#sandbox](https://hortonworks.com/downloads/#sandbox)
    并选择“为 Docker 下载”。
- en: You will also need to install the `start_sandox_hdp_version.sh` script. This
    will simplify the launching of the container in Docker. You can download the script
    from GitHub at: [https://gist.github.com/orendain/8d05c5ac0eecf226a6fed24a79e5d71a](https://gist.github.com/orendain/8d05c5ac0eecf226a6fed24a79e5d71a)[.](https://raw.githubusercontent.com/hortonworks/data-tutorials/master/tutorials/hdp/sandbox-deployment-and-install-guide/assets/start_sandbox-hdp.sh.)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要安装 `start_sandox_hdp_version.sh` 脚本。这将简化在 Docker 中启动容器。你可以从 GitHub 下载脚本：[https://gist.github.com/orendain/8d05c5ac0eecf226a6fed24a79e5d71a](https://gist.github.com/orendain/8d05c5ac0eecf226a6fed24a79e5d71a)[.](https://raw.githubusercontent.com/hortonworks/data-tutorials/master/tutorials/hdp/sandbox-deployment-and-install-guide/assets/start_sandbox-hdp.sh.)
- en: 'Now you will need to load the image in Docker. The following command will show
    you how:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你需要在 Docker 中加载镜像。以下命令将向你展示如何操作：
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The previous command loads the image into Docker. The image name will be similar
    to `HDP_2.6.3_docker_10_11_2017.tar`, but it will change depending on your version.
    To see that the sandbox has been loaded, run the following command:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的命令将镜像加载到 Docker 中。镜像名称将类似于 `HDP_2.6.3_docker_10_11_2017.tar`，但它将根据你的版本而变化。要查看沙盒是否已加载，请运行以下命令：
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The output, if you have no other containers, should look as it does in the
    following screenshot:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有其他容器，输出应该看起来如下截图所示：
- en: '![](img/56dc5a44-976b-4eb5-9a74-9b5f075fc8f2.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/56dc5a44-976b-4eb5-9a74-9b5f075fc8f2.png)'
- en: 'In order to use the web-based GUI Ambari, you will want to have a domain name
    established for the sandbox. To do that, you will need the IP address of the container.
    You can get it by running two commands:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用基于 Web 的 GUI Ambari，你将需要为沙盒设置一个域名。为此，你需要容器的 IP 地址。你可以通过运行两个命令来获取它：
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The first command will have the `container ID`, and the second command will
    take the `container ID` and return a lot of information, with the IP address being
    towards the end. Or, you can take advantage of the Linux command line and just
    get the IP address by using the following command:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个命令将包含 `container ID`，第二个命令将获取 `container ID` 并返回大量信息，其中 IP 地址位于末尾。或者，你可以利用
    Linux 命令行，只需使用以下命令即可获取 IP 地址：
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The previous command wraps the previously mentioned commands into a single command.
    The `docker inspect` command takes the output of `docker ps` as the `container
    ID`. It does so by wrapping it in `$()`, but it also passes a filter so that only
    the `ID` is returned. Then, the `inspect` command also includes a filter to only
    return the IP address. The text between the `{{}}` is a Go template. The output
    of this command should be an IP address, for example, 172.17.0.2.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的命令将之前提到的命令封装成一个单独的命令。`docker inspect` 命令将 `docker ps` 的输出作为 `container ID`。它是通过将输出封装在
    `$()` 中来实现的，但它也传递了一个过滤器，以便只返回 `ID`。然后，`inspect` 命令还包括一个过滤器，只返回 IP 地址。`{{}}` 之间的文本是一个
    Go 模板。此命令的输出应该是一个 IP 地址，例如，172.17.0.2。
- en: 'Now that you have the IP address of the image, you should update your host''s
    file using the following command:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经有了镜像的 IP 地址，你应该使用以下命令更新你的主机文件：
- en: '[PRE7]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The previous command redirects the output of the `echo`—which is the text you
    want in your `/etc/hosts` file and sends it to the `sudo tee -a /etc/hosts` command.
    This second command uses `sudo` to run as `root`. The `tee` command sends the
    output to a file and to the terminal (`STDOUT`). The `-a` tells `tee` to append
    to the file, and `/etc/hosts` is the file you want to append. Now, in your browser,
    you will be able to use names instead of the IP address.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的命令将 `echo` 的输出重定向——这是你想要放在 `/etc/hosts` 文件中的文本——并发送到 `sudo tee -a /etc/hosts`
    命令。第二个命令使用 `sudo` 以 `root` 身份运行。`tee` 命令将输出发送到文件和终端（`STDOUT`）。`-a` 告诉 `tee` 向文件追加，而
    `/etc/hosts` 是你想要追加的文件。现在，在你的浏览器中，你将能够使用名称而不是 IP 地址。
- en: Now you are ready to launch the image and browse to your Hadoop framework.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop basics
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, you will launch your Hadoop image and learn how to connect
    using `ssh` and Ambari. You will also move files and perform a basic Hive query.
    Once you understand how to interact with the framework, the next section will
    show you how to use a spatial query.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: 'First, from the terminal, launch the Hortonworks Sandbox using the provided
    Bash script. The following command will show you how:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The previous command executes the script you downloaded with the sandbox. Again,
    it used `sudo` to run as `root`. Depending on your machine, it may take some time
    to completely load and start all the services. When it is done, your terminal
    should look like it does in the following screenshot:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/da573ba6-8cc1-4b01-b171-4d716cd3555b.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
- en: Connecting via Secure Shell
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that the sandbox is running, you can connect using Secure Shell (SSH).
    The secure shell allows you to log in remotely to another machine. Open a new
    terminal and enter the following command:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The previous command uses `ssh` to connect as user `raj_ops` to the `localhost`
    (`127.0.0.1`) on port `2222`. You will get a warning that the authenticity of
    the host cannot be established. We did not create any keys for `ssh`. Just type
    `yes` and you will be prompted for the password. The user `raj_ops` has the password
    `raj_ops`. Your terminal prompt should now look like the following line:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: If your terminal is like it is in the previous code, you are now logged into
    the container.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: For more information on users, their permissions, and configuring the sandbox,
    go to the following page: [https://hortonworks.com/tutorial/learning-the-ropes-of-the-hortonworks-sandbox/](https://hortonworks.com/tutorial/learning-the-ropes-of-the-hortonworks-sandbox/)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: You can now use most Linux commands to navigate the container. You can now download
    and move files around, run Hive, and all your other tools from the command line.
    This section has already been heavy enough on Linux, so you will not use the command
    line exclusively in this chapter. Instead, the next section will show you how
    to execute these tasks in Ambari, a web-based GUI for executing tasks.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Ambari
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Ambari is a UI for making the management of Hadoop easier. In the previous
    section, you learned how to implement `ssh` into the container. From there you
    could manage Hadoop, run Hive queries, download data, and add it to the HDFS file
    system. Ambari makes all of this much simpler, especially if you are not familiar
    with the command line. To open Ambari, browse to the URL as: [http://sandbox.hortonworks.com:8080/](http://sandbox.hortonworks.com:8080/).'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: The Ambari URL depends on your installation. If you have followed the instructions
    in this chapter, then this will be your URL. You must also have started the server
    from the Docker image.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: 'You will be directed to the Ambari login page. Enter the user/password combination
    of `raj_ops`/`raj_ops`, as shown in the following screenshot:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 您将被引导到Ambari登录页面。输入用户/密码组合`raj_ops`/`raj_ops`，如下截图所示：
- en: '![](img/65c4ceb9-cceb-4d6a-b092-ff998fcf28ee.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/65c4ceb9-cceb-4d6a-b092-ff998fcf28ee.png)'
- en: 'After logging in, you will see the Ambari Dashboard. It will look like it does
    in the following screenshot:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 登录后，您将看到Ambari仪表板。它看起来如下截图所示：
- en: '![](img/3eae97d4-ff3a-4642-86be-336f5f71d124.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3eae97d4-ff3a-4642-86be-336f5f71d124.png)'
- en: On the left, you have a list of services. The main portion of the window contains
    the metrics, and the top menu bar has tabs for different functions. In this chapter,
    you will use the square comprised of nine smaller squares. Hover over the square
    icon and you will see a drop-down for the files view.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在左侧，您有一个服务列表。窗口的主要部分包含指标，顶部菜单栏有不同功能的标签页。在本章中，您将使用由九个小方块组成的方形。将鼠标悬停在方形图标上，您将看到一个文件视图的下拉菜单。
- en: This is the `root` directory of the HDFS file system.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这是HDFS文件系统的`root`目录。
- en: When connected to the container via `ssh`, run the `hdfs dfs -ls /` command
    and you will see the same directory structure.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 当通过`ssh`连接到容器时，运行`hdfs dfs -ls /`命令，您将看到相同的目录结构。
- en: 'From here, you can upload files. To try it out, open a text editor and create
    a simple CSV. This example will use the following data:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，您可以上传文件。为了尝试一下，打开一个文本编辑器并创建一个简单的CSV文件。此示例将使用以下数据：
- en: '[PRE11]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Save the CSV file and then click the Upload button in Ambari. You will be able
    to drag and drop the CSV to the browser. Ambari added the file to the HDFS file
    system on the container:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 保存CSV文件，然后在Ambari中点击上传按钮。您可以将CSV拖放到浏览器中。Ambari将文件添加到容器上的HDFS文件系统：
- en: '![](img/bdd196ed-7da5-4458-afca-231aea3689d5.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/bdd196ed-7da5-4458-afca-231aea3689d5.png)'
- en: 'Now that you have data loaded in the container, you can query it in Hive using
    SQL. Using the square icon again, select the drop-down for Hive View 2.0\. You
    should see a workspace as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经将数据加载到容器中，您可以使用SQL在Hive中查询它。再次使用方形图标，选择Hive View 2.0的下拉菜单。您应该看到一个如下所示的工作区：
- en: '![](img/384475d8-8bd7-4fc7-93e9-3d5d53389ba0.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/384475d8-8bd7-4fc7-93e9-3d5d53389ba0.png)'
- en: In Hive, you have worksheets. On the worksheet, you have the database you are
    connected to, which in this case is the default. Underneath that, you have the
    main query window. To the right, you have a list of existing tables. Lastly, scrolling
    down, you will see the Execute button, and under that is where the results will
    be loaded.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在Hive中，您有工作表。在工作表中，您连接到的数据库是您正在使用的数据库，在这种情况下是默认数据库。在其下方，您有主查询窗口。在右侧，您有一个现有表的列表。最后，向下滚动，您将看到执行按钮，结果将加载在该按钮下方。
- en: 'In the query pane, enter the SQL query as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在查询面板中，输入以下SQL查询：
- en: '[PRE12]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The previous query is a basic select all in SQL. The results will be shown
    as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的查询是一个基本的SQL全选查询。结果将如下所示：
- en: '![](img/5d00e72c-b222-454c-93bc-ca3253e11597.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5d00e72c-b222-454c-93bc-ca3253e11597.png)'
- en: Esri GIS tools for Hadoop
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Esri GIS工具用于Hadoop
- en: With your environment set up and some basic knowledge of Ambari, HDFS, and Hive,
    you will now learn how to add a spatial component to your queries. To do so, we
    will use the Esri GIS tools for Hadoop.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置好环境并具备一些关于Ambari、HDFS和Hive的基本知识后，您现在将学习如何将空间组件添加到您的查询中。为此，我们将使用Esri的Hadoop
    GIS工具。
- en: The first step is to download the files located at the GitHub repository, which
    is located at: [https://github.com/Esri/gis-tools-for-hadoop](https://github.com/Esri/gis-tools-for-hadoop).
    You will be using Ambari to move the files to HDFS not the container, so download
    these files to your local machine.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是下载位于GitHub仓库中的文件，该仓库位于：[https://github.com/Esri/gis-tools-for-hadoop](https://github.com/Esri/gis-tools-for-hadoop)。您将使用Ambari将文件移动到HDFS而不是容器中，因此请将这些文件下载到您的本地机器上。
- en: Esri has a tutorial for downloading the files by using `ssh` to connect to the
    container and then using `git` to clone the repository. You can follow these instructions
    here: [https://github.com/Esri/gis-tools-for-hadoop/wiki/GIS-Tools-for-Hadoop-for-Beginners](https://github.com/Esri/gis-tools-for-hadoop/wiki/GIS-Tools-for-Hadoop-for-Beginners).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Esri有一个教程，说明如何使用`ssh`连接到容器，然后使用`git`克隆仓库来下载文件。您可以在此处遵循这些说明：[https://github.com/Esri/gis-tools-for-hadoop/wiki/GIS-Tools-for-Hadoop-for-Beginners](https://github.com/Esri/gis-tools-for-hadoop/wiki/GIS-Tools-for-Hadoop-for-Beginners)。
- en: 'You can download the files by using the GitHub Clone or download button on
    the right-hand side of the repository. To unzip the archive, use one of the following
    commands:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过使用存储库右侧的 GitHub Clone 或下载按钮来下载文件。要解压存档，请使用以下命令之一：
- en: '[PRE13]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The first command will unzip the file in the current directory, which is most
    likely the `Downloads` folder of your home directory. The second command will
    unzip the file, but by passing `-d` and a path, it will unzip to that location.
    In this case, this is the `root` of my home directory.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 第一条命令将在当前目录中解压文件，这很可能是您家目录中的 `Downloads` 文件夹。第二条命令将解压文件，但通过传递 `-d` 和一个路径，它将解压到该位置。在这种情况下，这是我的家目录的
    `root`。
- en: 'Now that you have the files unzipped, you can open the Files View in Ambari
    by selecting it from the box icon drop-down menu. Select Upload and a modal will
    open, allowing you to drop a file. On your local machine, browse to the location
    of the Esri **Java ARchive** (JAR) files. If you moved the zip to your home directory,
    the path will be similar to `/home/pcrickard/gis-tools-for-hadoop-master/samples/lib`.
    You will have three JAR files:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经解压了文件，可以通过从图标下拉菜单中选择它来在 Ambari 中打开文件视图。选择上传，将打开一个模态窗口，允许您拖放文件。在您的本地机器上，浏览到
    Esri **Java ARchive** (JAR) 文件的位置。如果您将 zip 文件移动到您的家目录，路径将类似于 `/home/pcrickard/gis-tools-for-hadoop-master/samples/lib`。您将有三个
    JAR 文件：
- en: '`esri-geometry-api-2.0.0.jar`'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`esri-geometry-api-2.0.0.jar`'
- en: '`spatial-sdk-hive-2.0.0.jar`'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spatial-sdk-hive-2.0.0.jar`'
- en: '`spatial-sdk-json-2.0.0.jar`'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spatial-sdk-json-2.0.0.jar`'
- en: Move each of these three files to the `root` folder in Ambari. This is the `/`
    directory, which is the default location that opens when you launch Files View.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 将这三个文件中的每一个移动到 Ambari 的 `root` 文件夹。这是 `/` 目录，这是您启动文件视图时默认打开的位置。
- en: Next, you would normally move the data to HDFS as well, however, you did that
    in the previous example. In this example, you will leave the data files on your
    local machine and you will learn how you can load them into a Hive table without
    being on HDFS.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您通常会也将数据移动到 HDFS，然而，您在之前的示例中已经那样做了。在这个示例中，您将保留本地机器上的数据文件，您将学习如何在不位于 HDFS
    的情况下将它们加载到 Hive 表中。
- en: 'Now you are ready to execute the spatial query in Hive. From the box icon drop-down,
    select Hive View 2.0\. In the query pane, enter the following code:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经准备好在 Hive 中执行空间查询。从图标下拉菜单中选择 Hive View 2.0。在查询面板中，输入以下代码：
- en: '[PRE14]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Running the preceding code will take some time depending on your machine. The
    end result will look like it does in the following image:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面的代码将花费一些时间，具体取决于您的机器。最终结果将类似于以下图像所示：
- en: '![](img/6d09ff04-db16-420f-8c00-85d0ebc55456.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6d09ff04-db16-420f-8c00-85d0ebc55456.png)'
- en: The previous code and results were presented without explanation so that you
    could get the example to work and see the output. Following that, the code will
    be explained block by block.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码和结果没有解释，这样您可以运行示例并查看输出。之后，代码将逐块进行解释。
- en: 'The first block of code is shown as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 第一段代码如下所示：
- en: '[PRE15]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This block adds the JAR files from the HDFS location. In this case, it is the
    `/` folder. Once the code loads the JAR files, it can then create the functions
    `ST_Point` and `ST_Contains` by calling the classes in the JAR files. A JAR file
    may contain many Java files (classes). The order of the `add jar` statements matter.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码块将 HDFS 位置的 JAR 文件添加到其中。在这种情况下，它是 `/` 文件夹。一旦代码加载了 JAR 文件，它就可以通过调用 JAR 文件中的类来创建函数
    `ST_Point` 和 `ST_Contains`。一个 JAR 文件可能包含许多 Java 文件（类）。`add jar` 语句的顺序很重要。
- en: 'The following block drops two tables—`earthquakes` and `counties`. If you had
    never run the example, you could skip these lines:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码块将删除两个表——`earthquakes` 和 `counties`。如果您从未运行过示例，可以跳过这些行：
- en: '[PRE16]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next, the code creates the tables for `earthquakes` and `counties`. The `earthquakes`
    table is created and each field and type are passed to `CREATE`. The row format
    is specified as CSV—the `'',''`. Lastly, it is in a text file:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，代码将创建 `earthquakes` 和 `counties` 表。创建 `earthquakes` 表，并将每个字段和类型传递给 `CREATE`。行格式指定为
    CSV——`','`。最后，它是一个文本文件：
- en: '[PRE17]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The `counties` table is created in a similar fashion by passing the field names
    and types to `CREATE`, but the data is in JSON format and will use the `com.esri.hadoop.hive.serde.EsriJSonSerDe`
    class in the JAR `spatial-sdk-json-2.0.0` that you imported. `STORED AS INPUTFORMAT`
    and `OUTPUTFORMAT` are required for Hive to know how to parse and work with the
    JSON data:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The next two blocks load the data into the created tables. The data exists
    on your local machine and not on HDFS. To use the local data without first loading
    it in HDFS, you can use the `LOCAL` command with `LOAD DATA INPATH` and specify
    the local path of the data:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'With the JAR files loaded and the tables created and populated with data, you
    can now run a spatial query using the two defined functions—`ST_Point` and `ST_Contains`.
    These are used as in the examples from [Chapter 3](42c1ea5a-7372-4688-bb7f-fc3822248562.xhtml), *Introduction
    to Geodatabases*:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The previous query selects the `name` of the county and the `count` of earthquakes
    by passing the county geometry and the location of each earthquake as a point
    to `ST_Contains`. The results are shown as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d09ff04-db16-420f-8c00-85d0ebc55456.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
- en: HDFS and Hive in Python
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This book is about Python for geospatial development, so in this section, you
    will learn how to use Python for HDFS operations and Hive queries. There are several
    database wrapper libraries with Python and Hadoop, but it does not seem like a
    single library has become a standout go-to library, and others, like Snakebite,
    don't appear ready to run on Python 3\. In this section, you will learn how to
    use two libraries—PyHive and PyWebHDFS. You will also learn how you can use the
    Python subprocess module to execute HDFS and Hive commands.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 'To get PyHive, you can use `conda` and the following command:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'You may also need to install the `sasl` library:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The previous libraries will give you the ability to run Hive queries from Python.
    You will also want to be able to move files to HDFS. To do so, you can install
    `pywebhdfs`:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The preceding command will install the library, and as always, you can also
    use `pip` install or use any other method.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: With the libraries installed, let's first look at `pywebhdfs`.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: The documentation for `pywebhdfs` is located at: [http://pythonhosted.org/pywebhdfs/](http://pythonhosted.org/pywebhdfs/)
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: 'To make a connection in Python, you need to know the location of your Hive
    server. If you have followed this chapter, particularly the configuration changes
    in `/etc/hosts`—you can do so using the following code:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The previous code imports the `PyWebHdfsClient` as `h`. It then creates the
    connection to the HDFS file system running in the container. The container is
    mapped to `sandbox. hortonworks.com`, and HDFS is on port `50070`. Since the examples
    have been using the `raj_ops` user, the code did so as well.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: The functions now available to the `hdfs` variable are similar to your standard
    terminal commands, but with a different name—`mkdir` is now `make_dir` and `ls`
    is `list_dir`. To delete a file or directory, you will use `delete_file_dir`.
    The `make` and `delete` commands will return `True` if they are successful.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the `root` directory of our HDFS file system using Python:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The previous code issued the `list_dir` command (`ls` equivalent) and assigned
    it to `ls`. The result is a dictionary with all the files and folders in the directory.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'To see a single record, you can use the following code:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The previous code gets to the individual records by using the dictionary keys
    `FileStatuses` and `FileStatus`.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: To get the keys in a dictionary, you can use `.keys(). ls.keys()` which returns
    `[FileStatuses]`, and `ls['FileStatuses'].keys()` which returns `['FileStatus']`.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the previous code is shown as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Each file or directory contains several pieces of data, but most importantly
    the type, owner, and permissions.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step to running a Hive query example is to move our data files from
    the local machine to HDFS. Using Python, you can accomplish this using the following
    code:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The previous code creates a directory called `samples` with the permissions
    `755`. In Linux, permissions are based on read (`4`), write (`2`), and execute
    (`1`) for three types of users—owner, and group, other. So, permissions of `755`
    mean that the owner has read, write, and execute permissions (4+2+1 =7), and that
    the group and others have read and execute (4+1=5).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Next, the code opens and reads the CSV file we want to transfer to HDFS and
    assigns it to the variable `d`. The code then creates the `sample.csv` file in
    the `samples` directory, passing the contents of `d`.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 'To verify that the file was created, you can read the contents of the file
    using the following code:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The output of the previous code will be a string of the CSV file. It was created
    successfully.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 'Or, you can use the following code to get the `status` and details of the file:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The previous code will return the details as follows, but only if the file
    or directory exists. If it does not, the preceding code will raise a `FileNotFound`
    error. You can wrap the preceding code in a `try`...`except` block:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: With the data file transferred to HDFS, you can move on to querying the data
    with Hive.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: The documentation for PyHive is located at: [https://github.com/dropbox/PyHive](https://github.com/dropbox/PyHive)
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 'Using `pyhive`, the following code will create a table:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The previous code imports `pyhive` as `hive`. It creates a connection and gets
    the `cursor`. Lastly, it executes a Hive statement. Once you have the connection
    and the `cursor`, you can make your SQL queries by wrapping them in the `.execute()`
    method. To load the data from the CSV in HDFS into the table and then to select
    all, you would use the following code:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The preceding code uses the `execute()` method two more times to load the data
    and then executes select all. Using `fetchall()`, the results are passed to the
    `result` variable and will look like they do in the following output:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码使用了两次 `execute()` 方法来加载数据，然后执行了全选操作。使用 `fetchall()`，结果被传递到 `result` 变量中，其输出将如下所示：
- en: '[PRE34]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Working with `pyhive` is just like working with `psycopg2` — the Python library
    for connecting to PostgreSQL. Most database wrapper libraries are very similar
    in that you make a connection, get a `cursor`, and then execute statements. Results
    can be retrieved by grabbing all, one, or next (iterable).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `pyhive` 与使用 `psycopg2` —— 连接到 PostgreSQL 的 Python 库——的感觉相似。大多数数据库包装库都非常相似，你只需建立连接，获取一个
    `cursor`，然后执行语句。结果可以通过获取所有、一个或下一个（可迭代）来检索。
- en: Summary
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned how to set up a Hadoop environment. This required
    you to install Linux and Docker to download an image from Hortonworks, and to
    learn the ropes of that environment. Much of this chapter was spent on the environment
    and how to perform a spatial query using the GUI tools provided. This is because
    the Hadoop environment is complex and without a proper understanding, it would
    be hard to fully understand how to use it with Python. Lastly, you learned how
    to use HDFS and Hive in Python. The Python libraries for working with Hadoop,
    Hive, and HDFS are still developing. This chapter provided you with a foundation
    so that when these libraries improve, you will have enough knowledge of Hadoop
    and the accompanying technologies to implement these new Python libraries.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了如何设置 Hadoop 环境。这需要你安装 Linux 和 Docker，从 Hortonworks 下载镜像，并学习该环境的使用方法。本章的大部分内容都花在了环境和如何使用提供的
    GUI 工具执行空间查询上。这是因为 Hadoop 环境很复杂，如果没有正确的理解，就很难完全理解如何使用 Python 来操作它。最后，你学习了如何在 Python
    中使用 HDFS 和 Hive。用于与 Hadoop、Hive 和 HDFS 一起工作的 Python 库仍在开发中。本章为你提供了一个基础，这样当这些库得到改进时，你将拥有足够的关于
    Hadoop 和相关技术的知识，以实现这些新的 Python 库。
