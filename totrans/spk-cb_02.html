<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch02"/>Chapter 2. Developing Applications with Spark</h1></div></div></div><p>In this chapter, we will cover:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Exploring the Spark shell</li><li class="listitem" style="list-style-type: disc">Developing a Spark application in Eclipse with Maven</li><li class="listitem" style="list-style-type: disc">Developing Spark applications in Eclipse with SBT</li><li class="listitem" style="list-style-type: disc">Developing a Spark application in Intellij IDEA with Maven</li><li class="listitem" style="list-style-type: disc">Developing a Spark application in Intellij IDEA with SBT</li></ul></div><div><div><div><div><h1 class="title"><a id="ch02lvl1sec17"/>Introduction</h1></div></div></div><p>To create production quality Spark jobs/application, it is useful to use various <strong>integrated development environments</strong> (<strong>IDEs</strong>) and build tools. This chapter will cover various IDEs and build tools.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch02lvl1sec18"/>Exploring the Spark shell</h1></div></div></div><p>Spark comes bundled with a REPL shell, which is a wrapper around the Scala shell. Though the Spark<a id="id93" class="indexterm"/> shell looks like a command line for simple things, in reality a lot of complex queries can also be executed using it. This chapter explores different development environments in which Spark applications can be developed.</p><div><div><div><div><h2 class="title"><a id="ch02lvl2sec29"/>How to do it...</h2></div></div></div><p>Hadoop MapReduce's word count becomes very simple with the Spark shell. In this recipe, we <a id="id94" class="indexterm"/>are going to create a simple 1-line text file, upload it to the <strong>Hadoop distributed file system</strong> (<strong>HDFS</strong>), and use Spark to count occurrences of words. Let's see how:</p><div><ol class="orderedlist arabic"><li class="listitem">Create the <code class="literal">words</code> directory by using the following command:<div><pre class="programlisting">
<strong>$ mkdir words</strong>
</pre></div></li><li class="listitem">Get into the <code class="literal">words</code> directory:<div><pre class="programlisting">
<strong>$ cd words</strong>
</pre></div></li><li class="listitem">Create a <code class="literal">sh.txt</code> text file and enter <code class="literal">"to be or not to be"</code> in it:<div><pre class="programlisting">
<strong>$ echo "to be or not to be" &gt; sh.txt</strong>
</pre></div></li><li class="listitem">Start the Spark shell:<div><pre class="programlisting">
<strong>$ spark-shell</strong>
</pre></div></li><li class="listitem">Load the <code class="literal">words</code> directory as RDD:<div><pre class="programlisting">
<strong>Scala&gt; val words = sc.textFile("hdfs://localhost:9000/user/hduser/words")</strong>
</pre></div></li><li class="listitem">Count the number of lines ( result: 1):<div><pre class="programlisting">
<strong>Scala&gt; words.count</strong>
</pre></div></li><li class="listitem">Divide the line (or lines) into multiple words:<div><pre class="programlisting">
<strong>Scala&gt; val wordsFlatMap = words.flatMap(_.split("\\W+"))</strong>
</pre></div></li><li class="listitem">Convert <code class="literal">word</code> to (word,1)—that is, output <code class="literal">1</code> as the value for each occurrence of <code class="literal">word</code> as <a id="id95" class="indexterm"/>a key:<div><pre class="programlisting">
<strong>Scala&gt; val wordsMap = wordsFlatMap.map( w =&gt; (w,1))</strong>
</pre></div></li><li class="listitem">Use the <code class="literal">reduceByKey</code> method to add the number of occurrences for each word as a key (the function works on two consecutive values at a time represented by <code class="literal">a</code> and <code class="literal">b</code>):<div><pre class="programlisting">
<strong>Scala&gt; val wordCount = wordsMap.reduceByKey( (a,b) =&gt; (a+b))</strong>
</pre></div></li><li class="listitem">Sort the results:<div><pre class="programlisting">
<strong>Scala&gt; val wordCountSorted = wordCount.sortByKey(true)</strong>
</pre></div></li><li class="listitem">Print the RDD:<div><pre class="programlisting">
<strong>Scala&gt; wordCountSorted.collect.foreach(println)</strong>
</pre></div></li><li class="listitem">Doing all of the preceding operations in one step is as follows:<div><pre class="programlisting">
<strong>Scala&gt; sc.textFile("hdfs://localhost:9000/user/hduser/words"). flatMap(_.split("\\W+")).map( w =&gt; (w,1)). reduceByKey( (a,b) =&gt; (a+b)).sortByKey(true).collect.foreach(println)</strong>
</pre></div></li></ol></div><p>This gives us the following output:</p><div><pre class="programlisting">
<strong>(or,1)</strong>
<strong>(to,2)</strong>
<strong>(not,1)</strong>
<strong>(be,2)</strong>
</pre></div><p>Now you understand the basics, load HDFS with a large amount of text—for example, stories—and see the magic.</p><p>If you have the files in a compressed format, you can load them as is in HDFS. Both Hadoop and Spark have codecs for unzipping, which they use based on file extensions.</p><p>When <code class="literal">wordsFlatMap</code> was converted to <code class="literal">wordsMap</code> RDD, there was an implicit conversion. This converts RDD into <code class="literal">PairRDD</code>. This is an implicit conversion, which does not require<a id="id96" class="indexterm"/> anything to be done. If you are doing it in Scala code, please add the following <code class="literal">import</code> statement:</p><div><pre class="programlisting">import org.apache.spark.SparkContext._</pre></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch02lvl1sec19"/>Developing Spark applications in Eclipse with Maven</h1></div></div></div><p>Maven as <a id="id97" class="indexterm"/>a build tool has become<a id="id98" class="indexterm"/> the de-facto standard over the years. It's not surprising if we look little deeper into the promise Maven brings. Maven <a id="id99" class="indexterm"/>has two primary features and they<a id="id100" class="indexterm"/> are:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Convention over configuration</strong>: Build tools prior to Maven gave developers freedom about where<a id="id101" class="indexterm"/> to put source files, where to put test files, where to put compiled files, and so on. Maven takes away that freedom. With this freedom, all the confusion about locations also goes. In Maven, there is a specific directory structure for everything. The following table shows a few of the most common locations:<div><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><tbody><tr><td style="text-align: left" valign="top">
<p><code class="literal">/src/main/scala</code></p>
</td><td style="text-align: left" valign="top">
<p>Source code in Scala</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">/src/main/java</code></p>
</td><td style="text-align: left" valign="top">
<p>Source code in Java</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">/src/main/resources</code></p>
</td><td style="text-align: left" valign="top">
<p>Resources to be used by source code such as configuration files</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">/src/test/scala</code></p>
</td><td style="text-align: left" valign="top">
<p>Test code in Scala</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">/src/test/java</code></p>
</td><td style="text-align: left" valign="top">
<p>Test code in Java</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">/src/test/resources</code></p>
</td><td style="text-align: left" valign="top">
<p>Resources to be used by test code such as configuration files</p>
</td></tr></tbody></table></div></li><li class="listitem" style="list-style-type: disc"><strong>Declarative dependency management</strong>: In Maven, every library is defined by following three coordinates:<div><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><tbody><tr><td style="text-align: left" valign="top">
<p><code class="literal">groupId</code></p>
</td><td style="text-align: left" valign="top">
<p>A logical way of grouping libraries similar to a package in Java/Scala, which has to be at least the domain name you own—for example, <code class="literal">org.apache.spark</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">artifactId</code></p>
</td><td style="text-align: left" valign="top">
<p>The name of the project and JAR</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">version</code></p>
</td><td style="text-align: left" valign="top">
<p>Standard version numbers</p>
</td></tr></tbody></table></div></li></ul></div><p>In <code class="literal">pom.xml</code> (the configuration file that tells Maven all the information about a project), dependencies are declared in the form of these three coordinates. There is no need to search over the Internet and download, unpack, and copy libraries. All you need to do is to provide three coordinates of the dependency JAR you need and Maven will do the rest for you. The following is an example of using a JUnit dependency:</p><div><pre class="programlisting">&lt;dependency&gt;
  &lt;groupId&gt;junit&lt;/groupId&gt;
  &lt;artifactId&gt;junit&lt;/artifactId&gt;
  &lt;version&gt;4.12&lt;/version&gt;
&lt;/dependency&gt;</pre></div><p>This<a id="id102" class="indexterm"/> makes dependency management<a id="id103" class="indexterm"/> including transitive<a id="id104" class="indexterm"/> dependencies very easy. Build tools that came after Maven such as SBT and Gradle also follow these two rules as-is and provide enhancements in other aspects.</p><div><div><div><div><h2 class="title"><a id="ch02lvl2sec30"/>Getting ready</h2></div></div></div><p>From this recipe<a id="id105" class="indexterm"/> onwards, this chapter assumes you have installed Eclipse. Please visit <a class="ulink" href="http://www.eclipse.org">http://www.eclipse.org</a> for details.</p></div><div><div><div><div><h2 class="title"><a id="ch02lvl2sec31"/>How to do it...</h2></div></div></div><p>Let's see how to install the Maven plugin for Eclipse:</p><div><ol class="orderedlist arabic"><li class="listitem">Open Eclipse and navigate to <strong>Help</strong> | <strong>Install New Software</strong>.</li><li class="listitem">Click on the <strong>Work with</strong> drop-down menu.</li><li class="listitem">Select the &lt;eclipse version&gt; update site.</li><li class="listitem">Click on <strong>Collaboration tools</strong>.</li><li class="listitem">Check Maven's integration with Eclipse, as in the following screenshot:<div><img src="img/B03056_02_01.jpg" alt="How to do it..."/></div></li><li class="listitem">Click<a id="id106" class="indexterm"/> on <strong>Next</strong> and then <a id="id107" class="indexterm"/>click on <strong>Finish</strong>.<p>There will be a prompt to restart Eclipse and Maven will be installed after the restart.</p></li></ol></div><p>Now let's <a id="id108" class="indexterm"/>see how we can install the Scala plugin for Eclipse:</p><div><ol class="orderedlist arabic"><li class="listitem">Open Eclipse and navigate to <strong>Help</strong> | <strong>Install New Software</strong>.</li><li class="listitem">Click on the <strong>Work with</strong> drop-down menu.</li><li class="listitem">Select the &lt;eclipse version&gt; update site.</li><li class="listitem">Type <code class="literal">http://download.scala-ide.org/sdk/helium/e38/scala210/stable/site</code>.</li><li class="listitem">Press<a id="id109" class="indexterm"/> <em>Enter</em>.</li><li class="listitem">Select <a id="id110" class="indexterm"/><strong>Scala IDE for Eclipse</strong>:<div><img src="img/B03056_02_02.jpg" alt="How to do it..."/></div></li><li class="listitem">Click<a id="id111" class="indexterm"/> on <strong>Next</strong> and then click on <strong>Finish</strong>. You will be prompted to restart Eclipse and Scala will be installed after the restart.</li><li class="listitem">Navigate<a id="id112" class="indexterm"/> to <strong>Window</strong> | <strong>Open Perspective</strong> | <strong>Scala</strong>.</li></ol></div><p>Eclipse<a id="id113" class="indexterm"/> is <a id="id114" class="indexterm"/>now ready for Scala development!</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch02lvl1sec20"/>Developing Spark applications in Eclipse with SBT</h1></div></div></div><p><strong>Simple Build Tool</strong> (<strong>SBT</strong>) is <a id="id115" class="indexterm"/>a build tool made especially for Scala-based development. SBT follows Maven-based naming <a id="id116" class="indexterm"/>conventions and declarative <a id="id117" class="indexterm"/>dependency management.</p><p>SBT <a id="id118" class="indexterm"/>provides the following enhancements over Maven:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Dependencies are in the form of key-value pairs in the <code class="literal">build.sbt</code> file as opposed to <code class="literal">pom.xml</code> in Maven</li><li class="listitem" style="list-style-type: disc">It provides a shell that makes it very handy to perform build operations</li><li class="listitem" style="list-style-type: disc">For simple projects without dependencies, you do not even need the <code class="literal">build.sbt</code> file</li></ul></div><p>In <code class="literal">build.sbt</code>, the first line is the project definition:</p><div><pre class="programlisting">lazy val root = (project in file("."))</pre></div><p>Each project has an immutable map of key-value pairs. This map is changed by settings in SBT like so:</p><div><pre class="programlisting">lazy val root = (project in file("."))
  settings(
    name := "wordcount"
  )</pre></div><p>Every change in the settings leads to a new map, as it's an immutable map.</p><div><div><div><div><h2 class="title"><a id="ch02lvl2sec32"/>How to do it...</h2></div></div></div><p>Here's how we go about adding the <code class="literal">sbteclipse</code> plugin:</p><div><ol class="orderedlist arabic"><li class="listitem">Add this to the global plugin file:<div><pre class="programlisting">
<strong>$ mkdir /home/hduser/.sbt/0.13/plugins</strong>
<strong>$ echo addSbtPlugin("com.typesafe.sbteclipse" % "sbteclipse-plugin" % "2.5.0" )  &gt; /home/hduser/.sbt/0.12/plugins/plugin.sbt</strong>
</pre></div><p>Alternatively, you can add the following to your project:</p><div><pre class="programlisting">
<strong>$ cd &lt;project-home&gt;</strong>
<strong>$ echo addSbtPlugin("com.typesafe.sbteclipse" % "sbteclipse-plugin" % "2.5.0" )  &gt; plugin.sbt</strong>
</pre></div></li><li class="listitem">Start <a id="id119" class="indexterm"/>the <code class="literal">sbt</code> shell <a id="id120" class="indexterm"/>without any arguments:<div><pre class="programlisting">
<strong>$sbt</strong>
</pre></div></li><li class="listitem">Type <code class="literal">eclipse</code> and <a id="id121" class="indexterm"/>it will make an Eclipse-ready project:<div><pre class="programlisting">
<strong>$ eclipse</strong>
</pre></div></li><li class="listitem">Now you can navigate to <strong>File</strong> | <strong>Import</strong> | <strong>Import existing project into workspace</strong> to load the project into Eclipse.</li></ol></div><p>Now you can develop the Spark application in Scala using Eclipse and SBT.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch02lvl1sec21"/>Developing a Spark application in IntelliJ IDEA with Maven</h1></div></div></div><p>IntelliJ IDEA <a id="id122" class="indexterm"/>comes bundled<a id="id123" class="indexterm"/> with support for Maven. We <a id="id124" class="indexterm"/>will see how to create a new Maven project in this recipe.</p><div><div><div><div><h2 class="title"><a id="ch02lvl2sec33"/>How to do it...</h2></div></div></div><p>Perform the following steps to develop a Spark application on IntelliJ IDEA with Maven:</p><div><ol class="orderedlist arabic"><li class="listitem">Select <strong>Maven</strong> in new project window and click on <strong>Next</strong>:<div><img src="img/B03056_02_03.jpg" alt="How to do it..."/></div></li><li class="listitem">Enter <a id="id125" class="indexterm"/>three dimensions<a id="id126" class="indexterm"/> of<a id="id127" class="indexterm"/> the project:<div><img src="img/B03056_02_04.jpg" alt="How to do it..."/></div></li><li class="listitem">Enter<a id="id128" class="indexterm"/> the project's <a id="id129" class="indexterm"/>name <a id="id130" class="indexterm"/>and location:<div><img src="img/B03056_02_05.jpg" alt="How to do it..."/></div></li><li class="listitem">Click<a id="id131" class="indexterm"/> on <strong>Finish</strong> and<a id="id132" class="indexterm"/> the Maven <a id="id133" class="indexterm"/>project is ready.</li></ol></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch02lvl1sec22"/>Developing a Spark application in IntelliJ IDEA with SBT</h1></div></div></div><p>Before Eclipse<a id="id134" class="indexterm"/> became famous, IntelliJ IDEA <a id="id135" class="indexterm"/>was considered<a id="id136" class="indexterm"/> best of the breed in IDEs. IDEA has not shed its former glory yet and a lot of developers love IDEA. IDEA also has a community edition, which is free. IDEA provides native support for SBT, which makes it ideal for SBT and Scala development.</p><div><div><div><div><h2 class="title"><a id="ch02lvl2sec34"/>How to do it...</h2></div></div></div><p>Perform<a id="id137" class="indexterm"/> the following steps to develop a <a id="id138" class="indexterm"/>Spark application on IntelliJ IDEA <a id="id139" class="indexterm"/>with SBT:</p><div><ol class="orderedlist arabic"><li class="listitem">Add the <code class="literal">sbt-idea</code> plugin.</li><li class="listitem">Add to the global plugin file:<div><pre class="programlisting">
<strong>$mkdir /home/hduser/.sbt/0.13/plugins</strong>
<strong>$echo addSbtPlugin("com.github.mpeltone" % "sbt-idea" % "1.6.0" )  &gt; /home/hduser/.sbt/0.12/plugins/plugin.sbt</strong>
</pre></div><p>Alternatively, you can add to your project as well:</p><div><pre class="programlisting">
<strong>$cd &lt;project-home&gt;</strong>
<strong>$ echo addSbtPlugin("com.github.mpeltone" % "sbt-idea" % "1.6.0" ) &gt; plugin.sbt</strong>
</pre></div></li></ol></div><p>IDEA is ready to use with SBT.</p><p>Now you can develop Spark code using Scala and build using SBT.</p></div></div></body></html>