- en: Chapter 5. Spark Data Analysis with Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ultimate goal of processing data is to use the results for answering business
    questions. It is very important to understand the data that is being used to answer
    the business questions. To understand the data better, various tabulation methods,
    charting, and plotting techniques are used. Visual representation of the data
    reinforces the understanding of the underlying data. Because of this, data visualization
    is used extensively in data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: There are different terms that are used in various publications to mean the
    analysis of data for answering business questions. Data analysis, data analytics,
    and business intelligence, are some of the ubiquitous terms floating around. This
    chapter is not going to delve into the discussion on the meaning, similarities,
    or differences of these terms. On the other hand, the focus is going to be on
    how to bridge the gap between two major activities typically done by data scientists
    or data analysts. The first one being data processing. The second one is the use
    of the processed data to do analysis with the help of charting and plotting. Data
    analysis is the forte of data analysts and data scientists. This chapter is going
    to focus on the usage of Spark and Python to process the data, and produce charts
    and plots.
  prefs: []
  type: TYPE_NORMAL
- en: In many data analysis use cases, a super-set of data is processed and the reduced
    resultant dataset is used for the data analysis. This is specifically valid in
    the case of big data analysis where a small set of processed data is used for
    analysis. Depending on the use case, for various data analysis needs, appropriate
    data processing is done as a prerequisite. Most of the use cases that are going
    to be covered in this chapter fall into this model, where the first step deals
    with the necessary data processing, and the second step deals with the charting
    and plotting required for the data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: In typical data analysis use cases, the chain of activities involves an extensive
    and multi-staged **Extract**, **Transform**, and **Load** (**ETL**) pipeline ending
    with a data analysis platform or application. The end result of this chain of
    activities includes, but is not limited to, tables of summary data and various
    visual representations of the data in the form of charts and plots. Since Spark
    can process data from heterogeneous distributed data sources very effectively,
    the huge ETL pipeline that existed in legacy data analysis applications can be
    consolidated into self-contained applications that do the data processing and
    data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Charting and plotting libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up a dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Capturing the high-level details of the data analysis use cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Various charts and plots
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Charting and plotting libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Python is a programming language heavily used by data analysts and data scientists
    these days. There are numerous scientific and statistical data processing libraries
    available, as well as charting and plotting libraries, that can be used in Python
    programs. Python is also widely used as a programming language to develop data
    processing applications in Spark. This brings in a great flexibility to have a
    unified data processing and data analysis framework with Spark, Python and Python
    libraries, enabling us to do scientific and statistical processing, and charting
    and plotting. There are numerous such libraries that work with Python. Out of
    all those, the**NumPy** and **SciPy **libraries are being used here to do numerical,
    statistical, and scientific data processing. The **matplotlib **library is being
    used here to do the charting and plotting that produces 2D images.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is very important to make sure that the  **NumPy**, **SciPy** and **matplotlib**
    Python libraries are working fine with the Python installation before attempting
    the code samples given in this chapter. This has to be tested and verified in
    isolation before using it in Spark applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'The block diagram shown in *Figure 1* gives the overall structure of the application
    stack:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Charting and plotting libraries](img/image_05_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many public Datasets available for the consumption of the general
    public that can be used for education, research, and development purposes. The
    MovieLens website lets users rate and personalize movie recommendations. GroupLens
    Research published the rating Datasets from MovieLens. These datasets are available
    for download from their website, [http://grouplens.org/datasets/movielens/](http://grouplens.org/datasets/movielens/).
    In this chapter, the MovieLens 100K Dataset is being used to demonstrate the usage
    of distributed data processing with Spark in conjunction with Python, NumPy, SciPy,
    and matplotlib.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: On the GroupLens Research website for the dataset download, apart from the preceding
    dataset, there are more voluminous datasets such as MovieLens 1M dataset, MovieLens
    10M dataset, MovieLens 20M dataset, and MovieLens latest datasets available for
    download. Once the reader is quite familiar with the programs and has achieved
    a sufficient level of comfort playing around with data, these additional datasets
    can be used by the reader to do their own analysis work to strengthen the knowledge
    acquired from this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The MovieLens 100K dataset has data in multiple files. The following are the
    ones that are going to be used in the data analysis use cases of this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '`u.user`: The demographic information about the users who have rated movies.
    The structure of the dataset is given as follows, reproduced as it is from the
    README file accompanying the dataset:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: User ID
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Age
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Gender
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Occupation
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Zip code
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`u.item`: The information about the movies that are rated by the users. The
    structure of the dataset is given as follows, reproduced as it is from the README
    file accompanying the dataset:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Movie ID
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Movie title
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Release date
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Video release date
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: IMDb URL
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Unknown
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Action
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Adventure
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Animation
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Children's
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Comedy
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Crime
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentary
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Drama
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Fantasy
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Film-Noir
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Horror
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Musical
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Mystery
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Romance
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Sci-Fi
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Thriller
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: War
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Western
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Data analysis use cases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following list captures the high-level details of the data analysis use
    cases. Most of the use cases are revolving around the creation of various charts
    and plots:'
  prefs: []
  type: TYPE_NORMAL
- en: Plot the age distribution of the users who have rated the movies using a histogram.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Plot the age probability density chart of the users using the same data used
    to plot the histogram.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Plot the summary of the age distribution data to find the minimum, 25^(th) percentile,
    median, 75^(th) percentile, and maximum ages of the users.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Plot multiple charts or plots on the same figure to have a side-by-side comparison
    of the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a bar chart capturing the top 10 occupations in terms of the number of
    users who have rated the movies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a stacked bar chart capturing the number of male and female users by
    their occupation who have rated the movies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a pie chart capturing the bottom 10 occupations in terms of the number
    of who have rated the movies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a donut chart capturing the top 10 zip codes in terms of the number of
    who have rated the movies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using three occupation categories, create box plots capturing the summary statistics
    of the users who have rated the movies. All three box plots have to be drawn on
    a single figure to enable comparison.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a bar chart capturing the number of movies by their genre.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a scatter plot capturing the top 10 years in terms of the number of movies
    released in each year.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a scatter plot capturing the top 10 years in terms of the number of movies
    released in each year. In this plot, instead of points in the plot, create circles
    with the area proportional to the number of movies released in that year.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a line graph with two datasets with one dataset being the number of action
    movies released over the last 10 years and the other dataset being the number
    of drama movies released over the last 10 years to facilitate a comparison.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In all the preceding use cases, when it comes to implementation, Spark is used
    to process the data and prepare the required dataset. Once the required processed
    data is available in Spark DataFrame, it is collected into the driver program.
    In other words, the data is transferred from the distributed collection of Spark
    into a local collection, as tuples in the Python program, for charting and plotting.
    For charting and plotting, Python needs the data locally. It cannot use Spark
    DataFrames directly to do charting and plotting.
  prefs: []
  type: TYPE_NORMAL
- en: Charts and plots
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section is going to focus on creating various charts and plots to visually
    represent various aspects of the MovieLens 100K Dataset that are related to the
    use cases described in the preceding section. The charts and plots drawing process
    described throughout this chapter follows a pattern. Here are the important steps
    in that pattern of activities:'
  prefs: []
  type: TYPE_NORMAL
- en: Read data from the data file using Spark.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make the data available in a Spark DataFrame.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply the necessary data processing using DataFrame API.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The processing is mainly to make available only the minimal and required data
    for charting and plotting purposes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transfer the processed data from Spark DataFrame to the local Python collection
    object in the Spark Driver program.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the charting and plotting libraries to generate the figures using the data
    available in the Python collection objects.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Histogram
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A histogram is generally used to show how a given numerical dataset is distributed
    over consecutive non-overlapping intervals of equal size. The interval or bin
    size is chosen based on the dataset. The bin or interval represents the ranges
    of data. In this use case, the dataset consists of the ages of the users. In this
    case it does not make sense to have a bin size of 100 as there will be only one
    bin and the entire dataset will fall into it. The height of the bars representing
    the bins indicates the frequency of data items in that bin or interval.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following set of commands are used to bring up the Python REPL of Spark,
    followed by the programs to do the data processing, charting, and plotting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding section, the user dataset was read line by line to form the
    RDD. From the RDD, a Spark DataFrame was created. Using Spark SQL, another Spark
    DataFrame was created containing only the age column. The summary of that Spark
    DataFrame was displayed to show the summary statistics of the contents; the contents
    were collected into a local Python collection object. Using the collected data,
    a histogram of the age column was plotted, as given in *Figure 2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Histogram](img/image_05_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2
  prefs: []
  type: TYPE_NORMAL
- en: Density plot
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is another plot that is very close to a histogram. It is the density plot.
    Whenever there is a finite data sample with a need to estimate the probability
    density function of a random variable, density plots are used heavily. A histogram
    doesn't show when data smooths out or when there is continuity in the data points.
    For that purpose, density plots are used.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since a histogram and density plot are used for similar purposes, but show different
    behavior for the same data, generally, a histogram and density plot are used side
    by side in many applications.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 3* is a density plot drawn for the same dataset that is used to plot
    the histogram.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As a continuation of the same Python REPL of Spark, run the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![Density plot](img/image_05_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding section, the same Spark DataFrame that was created containing
    only the age column was used and the contents were collected into a local Python
    collection object. Using the collected data, a density plot of the age column
    was plotted as given in *Figure 3*, with the line space from 0 to 100 representing
    the age.
  prefs: []
  type: TYPE_NORMAL
- en: If multiple charts or plots are to be looked at side by side, the **matplotlib**
    library provides ways to do that. Figure 4 shows a histogram and a box plot side
    by side.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a continuation of the same Python REPL of Spark, run the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![Density plot](img/image_05_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding section, the same Spark DataFrame that was created containing
    only the age column was used, and the contents were collected into a local Python
    collection object. Using the collected data, a histogram of the age column was
    plotted along with a box plot containing indicators for the minimum, 25^(th) percentile,
    median, 75^(th) percentile, and maximum values, as given in *Figure 4*. When drawing
    multiple charts or plots in one figure, for a way to control the layout, look
    at the method call `plt.subplot(121)`. This is talking about the selection of
    the plot laid out in one row and two columns, and selects the first one. In the
    same way, `plt.subplot(122)` talks about the selection of the plot laid out in
    one row and two columns, and selects the second one.
  prefs: []
  type: TYPE_NORMAL
- en: Bar chart
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bar charts can be drawn in different ways. The most common one is where the
    bars are standing vertically on the *X* axis. Another variation is where the bars
    are drawn on the *Y* axis and have the bars laid out horizontally. *Figure 5*
    shows a horizontal bar chart.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is very common to get confused between a histogram and bar chart. The important
    difference is that a histogram is used to plot continuous but finite numerical
    values, but a bar chart is used to represent categorical data.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a continuation of the same Python REPL of Spark, run the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![Bar chart](img/image_05_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding section, a Spark DataFrame was created containing the top 10
    occupations of the users in terms of the number of users who have rated movies.
    The data was collected into a Python collection object to plot the bar chart.
  prefs: []
  type: TYPE_NORMAL
- en: Stacked bar chart
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The bar chart that was drawn in the preceding section gives the top 10 user
    occupations in terms of the number of users. But that does not give details about
    how that number is made up in terms of the gender of the users. In this kind of
    situation, it is good to use a stacked bar chart with each bar showing the counts
    by gender. *Figure 6* shows a stacked bar chart.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a continuation of the same Python REPL of Spark, run the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![Stacked bar chart](img/image_05_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding section, a Spark DataFrame was created containing only the
    occupation and gender columns. A cross-tab operation was done on that to produce
    another Spark DataFrame, which produced columns for occupation, male user count,
    and female user count. In the first Spark DataFrame, containing the occupation
    and gender columns, both are non-numerical columns and, because of that, it doesn't
    make sense to draw as chart or plot based on that data. But if a cross-tab operation
    is done on these two column values, for every distinct occupation field, then
    the counts of values of the gender column will be available. In this way, the
    occupation field becomes a categorical variable and it makes sense to draw a bar
    chart with the data. Since there are only two gender values in this data, it makes
    sense to have a stacked bar chart to see the total as well as the proportions
    of male and female user counts in each occupation category.
  prefs: []
  type: TYPE_NORMAL
- en: There are many statistical and mathematical functions available within DataFrames
    in Spark. The cross-tab operation on a Spark DataFrame comes in very handy in
    this kind of situation. With huge datasets, the cross-tab operation can become
    very processor-intensive and time consuming, but the distributed processing capabilities
    of Spark are a great help in this kind of situation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark SQL comes with lots of mathematical and statistical data processing capabilities.
    The preceding sections used the `describe().show()` method on the `SparkDataFrame`
    objects. In those Spark DataFrames, the preceding method acted on the available
    numeric columns. There will be situations where there are multiple numeric columns
    and, in those situations, the preceding method has the ability to pick and choose
    the desired columns for getting the summary statistics. Similarly, there are methods
    to find covariance, correlation, and so on, on the data from the Spark DataFrame.
    The following code snippet demonstrates these methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Pie chart
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If there is a need to visually represent a dataset to explain the whole-part
    relationship, a pie chart is very commonly used. *Figure 7* shows a pie chart.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a continuation of the same Python REPL of Spark, run the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![Pie chart](img/image_05_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding section, a Spark DataFrame was created containing the bottom
    10 occupations of the users in terms of the number of users who have rated movies.
    The data was collected into a Python collection object to plot the pie chart.
  prefs: []
  type: TYPE_NORMAL
- en: Donut chart
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Pie charts can be drawn in different forms. One such form, the donut chart,
    is often used these days. Figure 8 shows this donut chart variation of the pie
    chart.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a continuation of the same Python REPL of Spark, run the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![Donut chart](img/image_05_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding section, a Spark DataFrame was created containing the top 10
    zip codes of the users in terms of the number of users who live in that area and
    who have rated movies. The data was collected into a Python collection object
    to plot the donut chart.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Compared to the other figures in this book, the title of *Figure 8* is given
    in the middle. It is done using the `text()` method rather than using the `title()`
    method. This method can be used to print watermark text on the charts and plots.
  prefs: []
  type: TYPE_NORMAL
- en: Box plot
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Frequently, it is a common requirement to compare the summary statistics of
    different datasets in one figure. The box plot is a very common plot used to capture
    the summary statistics of a dataset in an intuitive way. The following section
    does exactly the same, and to do this, *Figure 9* shows multiple box plots on
    a single figure.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a continuation of the same Python REPL of Spark, run the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![Box plot](img/image_05_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding section, a Spark DataFrame was created with occupation and
    age columns for each of three occupations: administrator, engineer, and programmer.
    Box plots were created for each of these datasets on one figure, which contains
    indicators for the minimum, 25^(th) percentile, median, 75^(th) percentile, maximum,
    and outlier values for each of the datasets to facilitate comparison. The box
    plot for the programmer occupation shows two value points represented by the `+`
    symbol. They are outlier values.'
  prefs: []
  type: TYPE_NORMAL
- en: Vertical bar chart
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the preceding sections, the main dataset used for eliciting various charting
    and plotting use cases was the user data. The dataset that is taken up next is
    the movie dataset. In many datasets, to produce various charts and plots it will
    be a requirement to make the data suitable for the appropriate figure. Spark is
    rich with features to do the data processing.
  prefs: []
  type: TYPE_NORMAL
- en: The following use case demonstrates the preparation of data by applying some
    aggregation and using Spark SQL; the desired dataset is prepared for a classic
    bar chart containing the counts of movies by genre. *Figure 10* shows the bar
    chart after applying the aggregation operation on the movie data.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a continuation of the same Python REPL of Spark, run the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![Vertical bar chart](img/image_05_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding section, a `SparkDataFrame` was created with the movie dataset.
    The genre of the movie was captured in separate columns. Across the dataset, an
    aggregation was done using Spark SQL, a new `SparkDataFrame` summary was created,
    and the data values were collected into a Python collection object. Since there
    are too many columns in the dataset, a Python function was used to convert that
    kind of data structure into a dictionary object containing the column name as
    the key and the selected single row value is the value of the key. From that dictionary,
    two datasets were created and a bar chart is drawn.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When working with Spark, Python is used to develop data analysis applications,
    and it is almost certain that there are going to be many charts and plots. Instead
    of trying out all the code samples given in this chapter on the Python REPL for
    Spark, it is better to use IPython notebook as the IDE so that the code and the
    results can be seen together. The download section of this book contains the IPython
    notebook that contains all this code and the results. Readers can directly start
    using this.
  prefs: []
  type: TYPE_NORMAL
- en: Scatter plot
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Scatter plots are very commonly used for plotting values that have two variables,
    such as a point in Cartesian space having an `X` value and `Y` value. In this
    movie dataset, the number of movies released in a given year shows this kind of
    behavior. In the scatter plots, typically, the values represented at the intersection
    points of the `X` coordinate and `Y` coordinate are points. Because of recent
    technology developments and the availability of sophisticated graphics packages,
    many use different shapes and colors to represent the points. In the following
    scatter plot, shown in *Figure 11*, small circles having a uniform area with random
    colors have been used to represent the values. When employing such intuitive and
    clever techniques to represent the points in scatter plots, care must be taken
    to make sure that it does not defeat the purpose and loses the simplicity that
    scatter plots offer to convey the behavior of the data. Simple and elegant shapes
    that do not clutter the Cartesian space are ideal for such non-point representations
    of the values.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a continuation of the same Python REPL of Spark, run the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![Scatter plot](img/image_05_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding section, a `SparkDataFrame` was used to collect the top 10
    years in terms of the number of movies released in that year and the values were
    collected into a Python collection object and a scatter plot was drawn.
  prefs: []
  type: TYPE_NORMAL
- en: Enhanced scatter plot
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Figure 11* is a very simple and elegant scatter plot, but it does not really
    convey the comparative behavior of a given plotted value as compared to the other
    values in the same space. For that, instead of representing the points as fixed-radius
    circles, if the points are drawn as circles with the area proportional to the
    value, that will give a different perspective. Figure 12 is going to show the
    scatter plot with the same data, but with circles that have a proportional area
    to represent the points.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As a continuation in the same Python REPL of Spark, run the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![Enhanced scatter plot](img/image_05_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding section, the same dataset was used for *Figure 11* to draw
    the same scatter plot. Instead of plotting the points with uniform area circles,
    the points were drawn with proportionate area circles.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In all these code samples, the charts and plots are displayed using the show
    method. There are methods in matplotlib to save the generated charts and plots
    to disk, and they can be used for e-mailing, publishing to dashboards, and more.
  prefs: []
  type: TYPE_NORMAL
- en: Line graph
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are similarities between a scatter plot and a line graph. A scatter plot
    is ideal for representing individual data points, but taking all the points together
    gives a trend. A line graph also represents individual data points, but the points
    are connected. This is ideal for seeing the transition from one point to another
    point. In one figure, multiple line graphs can be drawn, enabling the comparison
    of two datasets. The preceding use case used a scatter plot to represent the number
    of movies released over a few years. Those numbers are just discrete data points
    plotted on one figure. If the need is to see a trend of how movie releases are
    changing over the years, a line graph is ideal. Similarly, if there is a need
    to compare movie releases over the years for two different genres, then one line
    can be used for each genre and both can be plotted on a single line graph. *Figure
    13* is a line graph with multiple datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a continuation of the same Python REPL of Spark, run the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![Line graph](img/image_05_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding section, Spark DataFrames were created to get the datasets
    for the number of action movies and drama movies released over the period of the
    last 10 years. The data was collected into Python collection objects and line
    graphs were drawn in the same figure.
  prefs: []
  type: TYPE_NORMAL
- en: Python, in conjunction with the matplotlib library, is very rich in terms of
    methods to produce publication-quality charts and plots. Spark can be used as
    the workhorse for processing the data coming from heterogeneous sources of data,
    and the results can also be saved to a wide variety of data formats.
  prefs: []
  type: TYPE_NORMAL
- en: Those who are exposed to the Python data analysis library **pandas** will find
    it easy to understand the material covered in this chapter because Spark DataFrames
    designed from the ground up by taking inspiration from the R DataFrame as well
    as **pandas**.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter has covered only a few sample charts and plots that can be created
    using the **matplotlib** library. The main idea of this chapter was to help the
    reader understand the capability of using this library in conjunction with Spark,
    where Spark is doing the data processing, and **matplotlib** is doing the charting
    and plotting.
  prefs: []
  type: TYPE_NORMAL
- en: The data file used in this chapter is read from a local filesystem. Instead
    of this, it can be read from HDFS or any other Spark-supported data source.
  prefs: []
  type: TYPE_NORMAL
- en: When using Spark as the primary framework for data processing, the most important
    point to keep in mind is that any possible data processing is to be done by Spark,
    mainly because Spark can do data processing in the best way. Only the processed
    data is to be returned to the Spark driver program for doing the charting and
    plotting.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information please refer to following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.numpy.org/](http://www.numpy.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://www.scipy.org/](http://www.scipy.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://matplotlib.org/](http://matplotlib.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://movielens.org/](https://movielens.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://grouplens.org/datasets/movielens/](http://grouplens.org/datasets/movielens/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://pandas.pydata.org/](http://pandas.pydata.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Processed data is used for data analysis. Data analysis requires a deep understanding
    of the processed data. Charts and plots enhance the understanding of the characteristics
    of the underlying data. In essence, for a data analysis application, data processing,
    charting, and plotting are essential. This chapter has covered the usage of Spark
    with Python, in conjunction with Python charting and plotting libraries, for developing
    data analysis applications.
  prefs: []
  type: TYPE_NORMAL
- en: In most organizations, business requirements are driving the need to build data
    processing applications involving the real-time ingestion of data, in various
    shapes and forms, with tremendous velocity. This mandates the need to process
    a stream of incoming data to the organizational data sink. The next chapter is
    going to discuss Spark Streaming, which is a library that works on top of Spark
    and enables the processing of various types of data streams.
  prefs: []
  type: TYPE_NORMAL
