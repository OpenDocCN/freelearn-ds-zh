["```py\n__global__ void dense_eval(int num_outputs, int num_inputs, int relu, int sigmoid, float * w, float * b, float * x, float *y, int batch_size, int w_t, int b_t, float delta)\n```", "```py\n{\n int i = blockDim.x*blockIdx.x + threadIdx.x;\n\n if (i < num_outputs)\n {\n```", "```py\nfor(int k=0; k < batch_size; k++)\n { \n```", "```py\ndouble temp = 0.0f;\n for (int j = 0; j < num_inputs; j++)\n {\n   temp += ((double) w[(num_inputs)*i + j ] ) * ( (double) x[k*num_inputs + j]);\n }\n temp += (double) b[i];\n y[k * num_outputs + i] = (float) temp;  \n}\n```", "```py\nif( w_t >= 0 && i == (w_t / num_inputs))\n {\n int j = w_t % num_inputs;\n for(int k=0; k < batch_size; k++)\n  y[k*num_outputs + i] += delta*x[k*num_inputs+j];\n}\nif( b_t >= 0 && i == b_t )\n {\n  for(int k=0; k < batch_size; k++)\n  y[k*num_outputs + i] += delta;\n }\n```", "```py\nDenseEvalCode = '''\n#define _RELU(x) ( ((x) > 0.0f) ? (x) : 0.0f )\n#define _SIGMOID(x) ( 1.0f / (1.0f + expf(-(x)) ))\n```", "```py\nif(relu > 0 || sigmoid > 0)\nfor(int k=0; k < batch_size; k++)\n { \n   float temp = y[k * num_outputs + i];\n   if (relu > 0)\n    temp = _RELU(temp);\n   if (sigmoid > 0)\n    temp = _SIGMOID(temp);\n   y[k * num_outputs + i] = temp; \n  }\n }\n return;\n}\n'''\neval_mod = SourceModule(DenseEvalCode)\neval_ker = eval_mod.get_function('dense_eval')\n```", "```py\nfrom __future__ import division\nimport pycuda.autoinit\nimport pycuda.driver as drv\nfrom pycuda import gpuarray\nfrom pycuda.compiler import SourceModule\nfrom pycuda.elementwise import ElementwiseKernel\nimport numpy as np\nfrom Queue import Queue\nimport csv\nimport time\n```", "```py\nclass DenseLayer:\n    def __init__(self, num_inputs=None, num_outputs=None, weights=None, b=None, stream=None, relu=False, sigmoid=False, delta=None):\n        self.stream = stream\n\n        if delta is None:\n            self.delta = np.float32(0.001)\n        else:\n            self.delta = np.float32(delta)\n\n        if weights is None:\n            weights = np.random.rand(num_outputs, num_inputs) - .5\n            self.num_inputs = np.int32(num_inputs)\n        self.num_outputs = np.int32(num_outputs) \n\n        if type(weights) != pycuda.gpuarray.GPUArray:\n            self.weights = gpuarray.to_gpu_async(np.array(weights, \n            dtype=np.float32) , stream = self.stream)\n        else:\n            self.weights = weights\n\n        if num_inputs is None or num_outputs is None:\n            self.num_inputs = np.int32(self.weights.shape[1])\n            self.num_outputs = np.int32(self.weights.shape[0])\n\n        else:\n            self.num_inputs = np.int32(num_inputs)\n            self.num_outputs = np.int32(num_outputs)\n\n        if b is None:\n            b = gpuarray.zeros((self.num_outputs,),dtype=np.float32)\n\n        if type(b) != pycuda.gpuarray.GPUArray:\n            self.b = gpuarray.to_gpu_async(np.array(b, \n            dtype=np.float32) , stream = self.stream)\n        else:\n            self.b = b \n\n        self.relu = np.int32(relu)\n        self.sigmoid = np.int32(sigmoid)\n\n        self.block = (32,1,1)\n        self.grid = (int(np.ceil(self.num_outputs / 32)), 1,1)\n```", "```py\ndef eval_(self, x, y=None, batch_size=None, stream=None, delta=None, w_t = None, b_t = None):\n\nif stream is None:\n    stream = self.stream\n\nif type(x) != pycuda.gpuarray.GPUArray:\n    x = gpuarray.to_gpu_async(np.array(x,dtype=np.float32), stream=self.stream)\n\nif batch_size is None:\n    if len(x.shape) == 2:\n        batch_size = np.int32(x.shape[0])\n    else:\n        batch_size = np.int32(1)\n\nif delta is None:\n    delta = self.delta\n\ndelta = np.float32(delta)\n\nif w_t is None:\n    w_t = np.int32(-1)\n\nif b_t is None:\n    b_t = np.int32(-1)\n\nif y is None:\n    if batch_size == 1:\n        y = gpuarray.empty((self.num_outputs,), dtype=np.float32)\n    else:\n        y = gpuarray.empty((batch_size, self.num_outputs), dtype=np.float32)\n\n    eval_ker(self.num_outputs, self.num_inputs, self.relu, self.sigmoid, self.weights, self.b, x, y, np.int32(batch_size), w_t, b_t, delta , block=self.block, grid=self.grid , stream=stream)\n\n return y\n```", "```py\nSoftmaxExpCode='''\n__global__ void softmax_exp( int num, float *x, float *y, int batch_size)\n{\n int i = blockIdx.x * blockDim.x + threadIdx.x;\n\n if (i < num)\n {\n  for (int k=0; k < batch_size; k++)\n  {\n   y[num*k + i] = expf(x[num*k+i]);\n  }\n }\n}\n'''\nexp_mod = SourceModule(SoftmaxExpCode)\nexp_ker = exp_mod.get_function('softmax_exp')\n\nSoftmaxMeanCode='''\n__global__ void softmax_mean( int num, float *x, float *y, int batch_size)\n{\n int i = blockDim.x*blockIdx.x + threadIdx.x;\n\n if (i < batch_size)\n {\n  float temp = 0.0f;\n\n  for(int k=0; k < num; k++)\n   temp += x[i*num + k];\n\n  for(int k=0; k < num; k++)\n   y[i*num+k] = x[i*num+k] / temp;\n }\n\n return;\n}'''\n\nmean_mod = SourceModule(SoftmaxMeanCode)\nmean_ker = mean_mod.get_function('softmax_mean')\n```", "```py\nclass SoftmaxLayer:\n    def __init__(self, num=None, stream=None):\n     self.num = np.int32(num)\n     self.stream = stream\n```", "```py\ndef eval_(self, x, y=None, batch_size=None, stream=None):\n if stream is None:\n stream = self.stream\n\n if type(x) != pycuda.gpuarray.GPUArray:\n  temp = np.array(x,dtype=np.float32)\n  x = gpuarray.to_gpu_async( temp , stream=stream)\n\n if batch_size==None:\n  if len(x.shape) == 2:\n   batch_size = np.int32(x.shape[0])\n  else:\n   batch_size = np.int32(1)\n else:\n  batch_size = np.int32(batch_size)\n\n if y is None:\n  if batch_size == 1:\n   y = gpuarray.empty((self.num,), dtype=np.float32)\n else:\n  y = gpuarray.empty((batch_size, self.num), dtype=np.float32)\n\n exp_ker(self.num, x, y, batch_size, block=(32,1,1), grid=(int( np.ceil( self.num / 32) ), 1, 1), stream=stream)\n\n mean_ker(self.num, y, y, batch_size, block=(32,1,1), grid=(int( np.ceil( batch_size / 32)), 1,1), stream=stream)\n\n return y\n\n```", "```py\nMAX_ENTROPY = 1\n\ndef cross_entropy(predictions=None, ground_truth=None):\n\n if predictions is None or ground_truth is None:\n  raise Exception(\"Error! Both predictions and ground truth must be float32 arrays\")\n\n p = np.array(predictions).copy()\n y = np.array(ground_truth).copy()\n\n if p.shape != y.shape:\n  raise Exception(\"Error! Both predictions and ground_truth must have same shape.\")\n\n if len(p.shape) != 2:\n  raise Exception(\"Error! Both predictions and ground_truth must be 2D arrays.\")\n\n total_entropy = 0\n\n for i in range(p.shape[0]):\n  for j in range(p.shape[1]):\n   if y[i,j] == 1: \n    total_entropy += min( np.abs( np.nan_to_num( np.log( p[i,j] ) ) ) , MAX_ENTROPY) \n   else: \n    total_entropy += min( np.abs( np.nan_to_num( np.log( 1 - p[i,j] ) ) ), MAX_ENTROPY)\n\n return total_entropy / p.size\n```", "```py\nclass SequentialNetwork:\n def __init__(self, layers=None, delta=None, stream = None, max_batch_size=32, max_streams=10, epochs = 10):\n\n self.network = []\n self.network_summary = []\n self.network_mem = []\n\n if stream is not None:\n  self.stream = stream\n else:\n  self.stream = drv.Stream()\n\n if delta is None:\n  delta = 0.0001\n\n self.delta = delta\n self.max_batch_size=max_batch_size\n self.max_streams = max_streams\n self.epochs = epochs\n\n if layers is not None:\n  for layer in layers:\n   add_layer(self, layer)\n```", "```py\ndef add_layer(self, layer):\n if layer['type'] == 'dense':\n  if len(self.network) == 0:\n   num_inputs = layer['num_inputs']\n  else:\n   num_inputs = self.network_summary[-1][2]\n\n  num_outputs = layer['num_outputs']\n  sigmoid = layer['sigmoid']\n  relu = layer['relu']\n  weights = layer['weights']\n  b = layer['bias']\n\n  self.network.append(DenseLayer(num_inputs=num_inputs, num_outputs=num_outputs, sigmoid=sigmoid, relu=relu, weights=weights, b=b))\n  self.network_summary.append( ('dense', num_inputs, num_outputs))\n\n  if self.max_batch_size > 1:\n   if len(self.network_mem) == 0:\nself.network_mem.append(gpuarray.empty((self.max_batch_size, self.network_summary[-1][1]), dtype=np.float32))\n self.network_mem.append(gpuarray.empty((self.max_batch_size, self.network_summary[-1][2] ), dtype=np.float32 ) ) \n else:\n if len(self.network_mem) == 0:\n self.network_mem.append( gpuarray.empty( (self.network_summary[-1][1], ), dtype=np.float32 ) )\n self.network_mem.append( gpuarray.empty((self.network_summary[-1][2], ), dtype=np.float32 ) ) \n\n elif layer['type'] == 'softmax':\n\n  if len(self.network) == 0:\n   raise Exception(\"Error! Softmax layer can't be first!\")\n\n  if self.network_summary[-1][0] != 'dense':\n   raise Exception(\"Error! Need a dense layer before a softmax layer!\")\n\n  num = self.network_summary[-1][2]\n  self.network.append(SoftmaxLayer(num=num))\n  self.network_summary.append(('softmax', num, num))\n\n  if self.max_batch_size > 1:\n   self.network_mem.append(gpuarray.empty((self.max_batch_size, self.network_summary[-1][2] ), dtype=np.float32)) \n  else:\n   self.network_mem.append( gpuarray.empty((self.network_summary[-1][2], ), dtype=np.float32))\n```", "```py\ndef predict(self, x, stream=None):\n\n if stream is None:\n  stream = self.stream\n\n if type(x) != np.ndarray:\n  temp = np.array(x, dtype = np.float32)\n  x = temp\n\n if(x.size == self.network_mem[0].size):\n  self.network_mem[0].set_async(x, stream=stream)\n else:\n\n  if x.size > self.network_mem[0].size:\n   raise Exception(\"Error: batch size too large for input.\")\n\n  x0 = np.zeros((self.network_mem[0].size,), dtype=np.float32)\n  x0[0:x.size] = x.ravel()\n  self.network_mem[0].set_async(x0.reshape( self.network_mem[0].shape), stream=stream)\n\n if(len(x.shape) == 2):\n  batch_size = x.shape[0]\n else:\n  batch_size = 1\n```", "```py\nfor i in xrange(len(self.network)):\n self.network[i].eval_(x=self.network_mem[i], y= self.network_mem[i+1], batch_size=batch_size, stream=stream)\n```", "```py\ny = self.network_mem[-1].get_async(stream=stream)\n\nif len(y.shape) == 2:\n y = y[0:batch_size, :]\n\nreturn y\n```", "```py\ndef partial_predict(self, layer_index=None, w_t=None, b_t=None, partial_mem=None, stream=None, batch_size=None, delta=None):\n\n self.network[layer_index].eval_(x=self.network_mem[layer_index], y = partial_mem[layer_index+1], batch_size=batch_size, stream = stream, w_t=w_t, b_t=b_t, delta=delta)\n\n for i in xrange(layer_index+1, len(self.network)):\n  self.network[i].eval_(x=partial_mem[i], y =partial_mem[i+1], batch_size=batch_size, stream = stream)\n```", "```py\ndef bsgd(self, training=None, labels=None, delta=None, max_streams = None, batch_size = None, epochs = 1, training_rate=0.01):\n\n training_rate = np.float32(training_rate)\n\n training = np.float32(training)\n labels = np.float32(labels)\n\n if( training.shape[0] != labels.shape[0] ):\n  raise Exception(\"Number of training data points should be same as labels!\")\n\n if max_streams is None:\n  max_streams = self.max_streams\n\n if epochs is None:\n epochs = self.epochs\n\n if delta is None:\n delta = self.delta\n\n streams = []\n bgd_mem = []\n\n # create the streams needed for training\n for _ in xrange(max_streams):\n  streams.append(drv.Stream())\n  bgd_mem.append([])\n\n # allocate memory for each stream\n for i in xrange(len(bgd_mem)):\n  for mem_bank in self.network_mem:\n   bgd_mem[i].append( gpuarray.empty_like(mem_bank) )\n```", "```py\nnum_points = training.shape[0]\n\nif batch_size is None:\n batch_size = self.max_batch_size\n\nindex = range(training.shape[0])\n\nfor k in xrange(epochs): \n\n print '-----------------------------------------------------------'\n print 'Starting training epoch: %s' % k\n print 'Batch size: %s , Total number of training samples: %s' % (batch_size, num_points)\n print '-----------------------------------------------------------'\n\n all_grad = []\n\n np.random.shuffle(index)\n```", "```py\nfor r in xrange(int(np.floor(training.shape[0]/batch_size))):\n\n batch_index = index[r*batch_size:(r+1)*batch_size] \n\n batch_training = training[batch_index, :]\n batch_labels = labels[batch_index, :]\n\n batch_predictions = self.predict(batch_training)\n\n cur_entropy = cross_entropy(predictions=batch_predictions, ground_truth=batch_labels)\n\n print 'entropy: %s' % cur_entropy\n```", "```py\nfor i in xrange(len(self.network)):\n\n if self.network_summary[i][0] != 'dense':\n  continue\n\n all_weights = Queue()\n\n grad_w = np.zeros((self.network[i].weights.size,), dtype=np.float32)\n grad_b = np.zeros((self.network[i].b.size,), dtype=np.float32)\n\n for w in xrange( self.network[i].weights.size ):\n  all_weights.put( ('w', np.int32(w) ) )\n\n for b in xrange( self.network[i].b.size ):\n  all_weights.put(('b', np.int32(b) ) )\n```", "```py\nwhile not all_weights.empty():\n\n stream_weights = Queue()\n\n for j in xrange(max_streams):\n\n  if all_weights.empty():\n    break\n\n  wb = all_weights.get()\n\n  if wb[0] == 'w':\n   w_t = wb[1]\n   b_t = None\n  elif wb[0] == 'b':\n   b_t = wb[1]\n   w_t = None\n\n  stream_weights.put( wb )\n\n  self.partial_predict(layer_index=i, w_t=w_t, b_t=b_t, partial_mem=bgd_mem[j], stream=streams[j], batch_size=batch_size, delta=delta)\n```", "```py\nfor j in xrange(max_streams):\n\n if stream_weights.empty():\n  break\n\n wb = stream_weights.get()\n\n w_predictions = bgd_mem[j][-1].get_async(stream=streams[j])\n\n w_entropy = cross_entropy(predictions=w_predictions[ :batch_size,:], ground_truth=batch_labels)\n\n if wb[0] == 'w':\n  w_t = wb[1]\n  grad_w[w_t] = -(w_entropy - cur_entropy) / delta\n\n elif wb[0] == 'b':\n  b_t = wb[1]\n  grad_b[b_t] = -(w_entropy - cur_entropy) / delta\n```", "```py\nall_grad.append([np.reshape(grad_w,self.network[i].weights.shape) , grad_b])\n```", "```py\nfor i in xrange(len(self.network)):\n if self.network_summary[i][0] == 'dense':\n  new_weights = self.network[i].weights.get()\n  new_weights += training_rate*all_grad[i][0]\n  new_bias = self.network[i].b.get()\n  new_bias += training_rate*all_grad[i][1]\n  self.network[i].weights.set(new_weights)\n  self.network[i].b.set(new_bias)\n```", "```py\ndef condition_data(data, means=None, stds=None):\n\n if means is None:\n  means = np.mean(data, axis=0)\n\n if stds is None:\n  stds = np.std(data, axis = 0)\n\n conditioned_data = data.copy()\n conditioned_data -= means\n conditioned_data /= stds\n\n return (conditioned_data, means, stds)\n```", "```py\nif __name__ == '__main__':\n to_class = { 'Iris-setosa' : [1,0,0] , 'Iris-versicolor' : [0,1,0], 'Iris-virginica' : [0,0,1]}\n\n iris_data = []\n iris_labels = []\n```", "```py\nwith open('C:/Users/btuom/examples/9/iris.data', 'rb') as csvfile:\n csvreader = csv.reader(csvfile, delimiter=',')\n for row in csvreader:\n  newrow = []\n  if len(row) != 5:\n   break\n  for i in range(4):\n   newrow.append(row[i])\n  iris_data.append(newrow)\n  iris_labels.append(to_class[row[4]])\n```", "```py\niris_len = len(iris_data)\nshuffled_index = list(range(iris_len))\nnp.random.shuffle(shuffled_index)\n\niris_data = np.float32(iris_data)\niris_labels = np.float32(iris_labels)\niris_data = iris_data[shuffled_index, :]\niris_labels = iris_labels[shuffled_index,:]\n\nt_len = (2*iris_len) // 3\n\niris_train = iris_data[:t_len, :]\nlabel_train = iris_labels[:t_len, :]\n\niris_test = iris_data[t_len:,:]\nlabel_test = iris_labels[t_len:, :]\n```", "```py\nsn = SequentialNetwork( max_batch_size=32 )\n```", "```py\n\nsn.add_layer({'type' : 'dense', 'num_inputs' : 4, 'num_outputs' : 10, 'relu': True, 'sigmoid': False, 'weights' : None, 'bias' : None} ) \nsn.add_layer({'type' : 'dense', 'num_inputs' : 10, 'num_outputs' : 15, 'relu': True, 'sigmoid': False, 'weights': None, 'bias' : None} ) \nsn.add_layer({'type' : 'dense', 'num_inputs' : 15, 'num_outputs' : 20, 'relu': True, 'sigmoid': False, 'weights': None, 'bias' : None} ) \nsn.add_layer({'type' : 'dense', 'num_inputs' : 20, 'num_outputs' : 3, 'relu': True, 'sigmoid': False, 'weights': None , 'bias': None } ) \nsn.add_layer({'type' : 'softmax'})\n```", "```py\nctrain, means, stds = condition_data(iris_train)\n\nt1 = time()\nsn.bsgd(training=ctrain, labels=label_train, batch_size=16, max_streams=20, epochs=100 , delta=0.0001, training_rate=1)\ntraining_time = time() - t1\n```", "```py\nhits = 0\nctest, _, _ = condition_data(iris_test, means=means, stds=stds)\nfor i in range(ctest.shape[0]):\n if np.argmax(sn.predict(ctest[i,:])) == np.argmax( label_test[i,:]):\n  hits += 1\n```", "```py\nprint 'Percentage Correct Classifications: %s' % (float(hits ) / ctest.shape[0])\nprint 'Total Training Time: %s' % training_time\n```"]