- en: Chapter 8. Polishing Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When working with data, you will usually find that it may not always be perfect
    or clean in the means of missing values, outliers and similar anomalies. Handling
    and cleaning imperfect or so-called dirty data is part of every data scientist's
    daily life, and even more, it can take up to 80 percent of the time we actually
    deal with the data!
  prefs: []
  type: TYPE_NORMAL
- en: 'Dataset errors are often due to the inadequate data acquisition methods, but
    instead of repeating and tweaking the data collection process, it is usually better
    (in the means of saving money, time and other resources) or unavoidable to polish
    the data by a few simple functions and algorithms. In this chapter, we will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Different use cases of the `na.rm` argument of various functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `na.action` and related functions to get rid of missing data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Several packages that offer a user-friendly way of data imputation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `outliers` package with several statistical tests for extreme values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to implement Lund's outlier test on our own as a brain teaser
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Referring to some robust methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The types and origins of missing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we have to take a quick look at the possible different sources of missing
    data to identify why and how we usually get missing values. There are quite a
    few different reasons for data loss, which can be categorized into 3 different
    types.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the main cause of missing data might be a malfunctioning device
    or the human factor of incorrectly entering data. **Missing Completely at Random**
    (**MCAR**) means that every value in the dataset has the same probability of being
    missed, so no systematic error or distortion is to be expected due to missing
    data, and nor can we explain the pattern of missing values. This is the best situation
    if we have `NA` (meaning: no answer, not applicable or not available) values in
    our data set.'
  prefs: []
  type: TYPE_NORMAL
- en: But a much more frequent and unfortunate type of missing data is **Missing at
    Random** (**MAR**) compared to MCAR. In the case of MAR, the pattern of missing
    values is known or at least can be identified, although it has nothing to do with
    the actual missing values. For example, one might think of a population where
    males are more loners or lazier compared to females, thus they prefer not to answer
    all the questions in a survey – regardless of the actual question. So it's not
    that the males are not giving away their salary due to the fact that they make
    more or less compared to females, but they tend to skip a few questions in the
    questionnaire at random.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This classification and typology of missing data was first proposed by Donald
    B. Rubin in 1976 in his *Inference and Missing Data*, published in *Biometrika
    63(3): 581—592*, later reviewed and extended in a book jointly written by *Roderick
    J. A. Little* (2002): *Statistical Analysis with Missing Data*, *Wiley* – which
    is well worth of reading for further details.'
  prefs: []
  type: TYPE_NORMAL
- en: And the worst scenario would be **Missing Not at Random** (**MNAR**), where
    data is missing for a specific reason that is highly related to the actual question,
    which classifies missing values as nonignorable non-response.
  prefs: []
  type: TYPE_NORMAL
- en: This happens pretty often in surveys with sensitive questions or due to design
    flaws in the research preparation. In such cases, data is missing due to some
    latent process going on in the background, which is often the thing we wanted
    to come to know better with the help of the research – which can turn out to be
    a rather cumbersome situation.
  prefs: []
  type: TYPE_NORMAL
- en: 'So how can we resolve these problems? Sometimes it''s relatively easy. For
    example, if we have lot of observations, MCAR is not a real problem at all due
    to the law of large numbers, as the probability of having missing value(s) is
    the same for each observation. We basically have two options to deal with unknown
    or missing data:'
  prefs: []
  type: TYPE_NORMAL
- en: Removing missing values and/or observations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replacing missing values with some estimates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying missing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The easiest way of dealing with missing values, especially with MCAR data, is
    simply removing all the observations with any missing values. If we want to exclude
    every row of a `matrix` or `data.frame` object which has at least one missing
    value, we can use the `complete.cases` function from the `stats` package to identify
    those.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a quick start, let''s see how many rows have at least one missing value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This is around 1.5 percent of the quarter million rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see what the distribution of `NA` looks like within different columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: By-passing missing values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So it seems that missing data relatively frequently occurs with the time-related
    variables, but we have no missing values among the flight identifiers and dates.
    On the other hand, if one value is missing for a flight, the chances are rather
    high that some other variables are missing as well – out of the overall number
    of 3,622 cases with at least one missing value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Okay, let's see what we have done here! First, we have called the `apply` function
    to transform the values of `data.frame` to `0` or `1`, where `0` stands for an
    observed, while `1` means a missing value. Then we computed the correlation coefficients
    of this newly created matrix, which of course returned a lot of missing values
    due to fact that some columns had only one unique value without any variability,
    as shown in the warning message. For this, we had to specify the `na.rm` parameter
    to be `TRUE`, so that the `mean` function would return a real value instead of
    an `NA`, by removing the missing values among the correlation coefficients returned
    by the `cor` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'So one option is the heavy use of the `na.rm` argument, which is supported
    by most functions that are sensitive to missing data—to name a few from the `base`
    and `stats` packages: `mean`, `median`, `sum`, `max` and `min`.'
  prefs: []
  type: TYPE_NORMAL
- en: To compile the complete list of functions that have the `na.rm` argument in
    the base package, we can follow the steps described in a very interesting SO answer
    located at [http://stackoverflow.com/a/17423072/564164](http://stackoverflow.com/a/17423072/564164).
    I found this answer motivating because I truly believe in the power of analyzing
    the tools we use for analysis, or in other words, spending some time on understanding
    how R works in the background.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s make a list of all the functions found in `baseenv` (the environment
    of the `base` package) along with the complete function arguments and body:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we can `Filter` all those functions from the returned list, which have
    `na.rm` among the formal arguments via the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This can be easily applied to any R package by changing the environment variable
    to for example `''package:stats''` in the case of the `stats` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: So these are the functions that have the `na.rm` argument in the `base` and
    the `stats` packages, where we have seen that the fastest and easiest way of ignoring
    missing values in single function calls (without actually removing the `NA` values
    from the dataset) is setting `na.rm` to `TRUE`. But why doesn't `na.rm` default
    to `TRUE`?
  prefs: []
  type: TYPE_NORMAL
- en: Overriding the default arguments of a function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you are annoyed by the fact that most functions return `NA` if your R object
    includes missing values, then you can override those by using some custom wrapper
    functions, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Another option might be to write a custom package which would override the
    factory defaults of the `base` and `stats` function, like in the `rapportools`
    package, which includes miscellaneous helper functions with sane defaults for
    reporting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The problem with this approach is that you''ve just permanently overridden
    those functions listed, so you''ll need to restart your R session or detach the
    `rapportools` package to reset to the standard arguments, like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'A more general solution to override the default arguments of a function is
    to rely on some nifty features of the `Defaults` package, which is although not
    under active maintenance, but it does the job:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Please note that here we had to update the default argument value of `mean.default`
    instead of simply trying to tweak `mean`, as that latter would result in an error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This is due to the fact that `mean` is an `S3` method without any formal arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Either methods you prefer, you can automatically call those functions when R
    starts by adding a few lines of code in your `Rprofile` file.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can customize the R environment via a global or user-specific `Rprofile`
    file. This is a normal R script which is usually placed in the user's home directory
    with a leading dot in the file name, which is run every time a new R session is
    started. There you can call any R functions wrapped in the `.First` or `.Last`
    functions to be run at the start or at the end of the R session. Such useful additions
    might be loading some R packages, printing custom greetings or KPI metrics from
    a database, or for example installing the most recent versions of all R packages.
  prefs: []
  type: TYPE_NORMAL
- en: But it's probably better not to tweak your R environment in such a non-standard
    way, as you might soon experience some esoteric and unexpected errors or silent
    malfunctions in your analysis.
  prefs: []
  type: TYPE_NORMAL
- en: For example, I've got used to working in a temporary directory at all times
    by specifying `setwd('/tmp')` in my `Rprofile`, which is very useful if you start
    R sessions frequently for some quick jobs. On the other hand, it's really frustrating
    to spend 15 minutes of your life debugging why some random R function does not
    seem to do its job, and why it's returning some file not found error messages
    instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'So please be warned: if you update the factory default arguments of R functions,
    do not ever think of ranting about some new bugs you have found in some major
    functions of base R on the R mailing lists, before trying to reproduce those errors
    in a vanilla R session with starting R with the --`vanilla` command line option.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting rid of missing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An alternative way of using the `na.rm` argument in R functions is removing
    `NA` from the dataset before passing that to the analysis functions. This means
    that we are removing the missing values from the dataset permanently, so that
    they won''t cause any problems at later stages in the analysis. For this, we could
    use either the `na.omit` or the `na.exclude` functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The only difference between these two functions is the class of the `na.action`
    attribute of the returned R object, which are `omit` and `exclude` respectively.
    This minor difference is only important when modelling. The `na.exclude` function
    returns `NA` for residuals and predictions, while `na.omit` suppresses those elements
    of the vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Important thing to note in case of tabular data, like a `matrix` or `data.frame`,
    these functions remove the whole row if it contains at least one missing value.
    For a quick demo, let''s create a matrix with 3 columns and 3 rows with values
    incrementing from 1 to 9, but replacing all values divisible by 4 with `NA`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: As seen here, we can find the row numbers of the removed cases in the `na.action`
    attribute.
  prefs: []
  type: TYPE_NORMAL
- en: Filtering missing data before or during the actual analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s suppose we want to calculate the `mean` of the actual length of flights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is `NA` of course, because as identified previously, this variable
    contains missing values, and almost every R operation with `NA` results in `NA`.
    So let''s overcome this issue as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Any performance issues there? Or other means of deciding which method to use?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The first glance at the performance of these options computed with the help
    of the `microbenchmark` package (please see the *Loading text files of reasonable
    size* section in the [Chapter 1](ch01.html "Chapter 1. Hello, Data!"), *Hello
    Data* for more details) suggests that using `na.rm` is the better solution in
    case of a single function call.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, if we want to reuse the data at some later phase in the analysis,
    it is more viable and effective to omit the missing values and observations only
    once from the dataset, instead of always specifying `na.rm` to be `TRUE`.
  prefs: []
  type: TYPE_NORMAL
- en: Data imputation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'And sometimes omitting missing values is not reasonable or possible at all,
    for example due to the low number of observations or if it seems that missing
    data is not random. Data imputation is a real alternative in such situations,
    and this method can replace `NA` with some real values based on various algorithms,
    such as filling empty cells with:'
  prefs: []
  type: TYPE_NORMAL
- en: A known scalar
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The previous value appearing in the column (hot-deck)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A random element from the same column
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The most frequent value in the column
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different values from the same column with given probability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicted values based on regression or machine learning models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The hot-deck method is often used while joining multiple datasets together.
    In such a situation, the `roll` argument of `data.table` can be very useful and
    efficient, otherwise be sure to check out the `hotdeck` function in the `VIM`
    package, which offers some really useful ways of visualizing missing data. But
    when dealing with an already given column of a dataset, we have some other simple
    options as well.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, imputing a known scalar is a pretty simple situation, where we
    know that all missing values are for example due to some research design patterns.
    Let's think of a database that stores the time you arrived to and left the office
    every weekday, and by computing the difference between those two, we can analyze
    the number of work hours spent in the office from day to day. If this variable
    returns `NA` for a time period, actually it means that we were outside of the
    office all day, so thus the computed value should be zero instead of `NA`.
  prefs: []
  type: TYPE_NORMAL
- en: 'And not just in theory, but this is pretty easy to implement in R as well (example
    is continued from the previous demo code where we defined `m` with two missing
    values):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, replacing missing values with a random number, a `sample` of other
    values or with the `mean` of a variable can be done relatively easily:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Which can be even easier with the `impute` function from the `Hmisc` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'It seems that we have preserved the value of the arithmetic mean of course,
    but you should be aware of some very serious side-effects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: When replacing missing values with the mean, the variance of the transformed
    variable will be naturally lower compared to the original distribution. This can
    be extremely problematic in some situations, where some more sophisticated methods
    are needed.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling missing values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Besides the previous mentioned univariate methods, you may also fit models on
    the complete cases in the dataset, rather than fitting those models on the remaining
    rows to estimate the missing values. Or in a nutshell, we are replacing the missing
    values with multivariate predictions.
  prefs: []
  type: TYPE_NORMAL
- en: There are a plethora of related functions and packages, for example you might
    be interested in checking the `transcan` function in the `Hmisc` package, or the
    `imputeR` package, which includes a wide variety of models for imputing categorical
    and continuous variables as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Most of the imputation methods and models are for one type of variable: either
    continuous or categorical. In case of mixed-type dataset, we typically use different
    algorithms to handle the different types of missing data. The problem with this
    approach is that some of the possible relations between different types of data
    might be ignored, resulting in some partial models.'
  prefs: []
  type: TYPE_NORMAL
- en: To overcome this issue, and to save a few pages in the book on the description
    of the traditional regression and other related methods for data imputation (although
    you can find some related methods in the [Chapter 5](ch05.html "Chapter 5. Building
    Models (authored by Renata Nemeth and Gergely Toth)"), *Buildings Models (authored
    by Renata Nemeth and Gergely Toth)* and the [Chapter 6](ch06.html "Chapter 6. Beyond
    the Linear Trend Line (authored by Renata Nemeth and Gergely Toth)"), *Beyond
    the Linear Trend Line (authored by Renata Nemeth and Gergely Toth)*), we will
    concentrate on a non-parametric method that can handle categorical and continuous
    variables at the same time via a very user-friendly interface in the `missForest`
    package.
  prefs: []
  type: TYPE_NORMAL
- en: This iterative procedure fits a random forest model on the available data in
    order to predict the missing values. As our `hflights` data is relatively large
    for such a process and running the sample code would takes ages, we will rather
    use the standard `iris` dataset in the next examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'First let''s see the original structure of the dataset, which does not include
    any missing values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s load the package and add some missing values (completely at random)
    to the dataset in the means of producing a reproducible minimal example for the
    forthcoming models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: So now we have around 20 percent of missing values in each column, which is
    also stated in the bottom row of the preceding summary. The number of completely
    random missing values is between 28 and 33 cases per variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step should be building the random forest models to replace the missing
    values with real numbers and factor levels. As we also have the original dataset,
    we can use that complete matrix to test the performance of the method via the
    `xtrue` argument, which computes and returns the error rate when we call the function
    with `verbose`. This is useful in such didactical examples to show how the model
    and predictions improves from iteration to iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The algorithm ran for 5 iterations before stopping, when it seemed that the
    error rate was not improving any further. The returned `missForest` object includes
    a few other values besides the imputed dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The Out of Box error is an estimate on how good our model was based on the **normalized
    root mean squared error computed** (**NRMSE**) for numeric values and the **proportion
    of falsely classified** (**PFC**) entries for factors. And as we also provided
    the complete dataset for the previously run model, we also get the true imputation
    error ratio – which is pretty close to the above estimates.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Please find more details on random forests and related machine learning topics
    in the [Chapter 10](ch10.html "Chapter 10. Classification and Clustering"), *Classification
    and Clustering*.
  prefs: []
  type: TYPE_NORMAL
- en: But how does this approach compare to a much simpler imputation method, like
    replacing missing values with the mean?
  prefs: []
  type: TYPE_NORMAL
- en: Comparing different imputation methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the comparison, only the first four columns of the `iris` dataset will be
    used, thus it is not dealing with the factor variable at the moment. Let''s prepare
    this demo dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'In `iris_mean`, we replace all the missing values to the mean of the actual
    columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'And in `iris_forest`, we predict the missing values by fitting random forest
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s simply check the accuracy of the two models by comparing the correlations
    of `iris_mean` and `iris_forest` with the complete `iris` dataset. For `iris_forest`,
    we will extract the actual imputed dataset from the `ximp` attribute, and we will
    silently ignore the factor variable of the original `iris` table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: These results suggest that the nonparametric random forest model did a lot better
    job compared to the simple univariate solution of replacing missing values with
    the mean.
  prefs: []
  type: TYPE_NORMAL
- en: Not imputing missing values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Please note that these methods have their drawbacks likewise. Replacing the
    missing values with a predicted one often lacks any error term and residual variance
    with most models.
  prefs: []
  type: TYPE_NORMAL
- en: This also means that we are lowering the variability, and overestimating some
    association in the dataset at the same time, which can seriously affect the results
    of our data analysis. For this, some simulation techniques were introduced in
    the past to overcome the problem of distorting the dataset and our hypothesis
    tests with some arbitrary models.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple imputation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The basic idea behind multiple imputation is to fit models several times in
    a row on the missing values. This Monte Carlo method usually creates some (like
    3 to 10) parallel versions of the simulated complete dataset, each of these is
    analyzed separately, and then we combine the results to produce the actual estimates
    and confidence intervals. See for example the `aregImpute` function from the `Hmisc`
    package for more details.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, do we really have to remove or impute missing values in all
    cases? For more details on this question, please see the last section of this
    chapter. But before that, let's get to know some other requirements for polishing
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Extreme values and outliers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An outlier or extreme value is defined as a data point that deviates so far
    from the other observations, that it becomes suspicious to be generated by a totally
    different mechanism or simply by error. Identifying outliers is important because
    those extreme values can:'
  prefs: []
  type: TYPE_NORMAL
- en: Increase error variance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Influence estimates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decrease normality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Or in other words, let's say your raw dataset is a piece of rounded stone to
    be used as a perfect ball in some game, which has to be cleaned and polished before
    actually using it. The stone has some small holes on its surface, like missing
    values in the data, which should be filled – with data imputation.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the stone does not only has holes on its surface, but some
    mud also covers some parts of the item, which is to be removed. But how can we
    distinguish mud from the real stone? In this section, we will focus on what the
    `outliers` package and some related methods have to offer for identifying extreme
    values.
  prefs: []
  type: TYPE_NORMAL
- en: 'As this package has some conflicting function names with the `randomForest`
    package (automatically loaded by the `missForest` package), it''s wise to detach
    the latter before heading to the following examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The `outlier` function returns the value with the largest difference from the
    mean, which, contrary to its name, not necessarily have to be an outlier. Instead,
    the function can be used to give the analyst an idea about which values can be
    outliers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'So there was a flight with more than 16 hours of delay before actually taking
    off! This is impressive, isn''t it? Let''s see if it''s normal to be so late:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Well, `mean` is around 10 minutes, but as it''s even larger than the third
    quarter and the `median` is zero, it''s not that hard to guess that the relatively
    large mean is due to some extreme values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '![Extreme values and outliers](img/2028OS_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding boxplot clearly shows that most flights were delayed by only
    a few minutes, and the interquartile range is around 10 minutes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: All the blue circles in the preceding image are the whiskers are possible extreme
    values, as being higher than the 1.5 IQR of the upper quartile. But how can we
    (statistically) test a value?
  prefs: []
  type: TYPE_NORMAL
- en: Testing extreme values
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `outliers` package comes with several bundled extreme value detection algorithms,
    like:'
  prefs: []
  type: TYPE_NORMAL
- en: Dixon's Q test (`dixon.test`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grubb's test (`grubbs.test`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Outlying and inlying variance (`cochran.test`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chi-squared test (`chisq.out.test`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These functions are extremely easy to use. Just pass a vector to the statistical
    tests and the returning p-value of the significance test will clearly indicate
    if the data has any outliers. For example, let''s test 10 random numbers between
    0 and 1 against a relatively large number to verify it''s an extreme value in
    this small sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'But unfortunately, we cannot use these convenient functions in our live dataset,
    as the methods assume normal distribution, which is definitely not true in our
    cases as we all know from experience: flights tend to be late more often than
    arriving a lot sooner to their destinations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For this, we should use some more robust methods, such as the `mvoutlier` package,
    or some very simple approaches like Lund suggested around 40 years ago. This test
    basically computes the distance of each value from the mean with the help of a
    very simple linear regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Just to verify we are now indeed measuring the distance from the mean:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s compute the critical value based on the F distribution and two helper
    variables (where `a` stands for the alpha value and `n` represents the number
    of cases):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Which can be passed to Lund''s formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s see how many values have a higher standardized residual than this
    computed critical value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: But do we really have to remove these outliers from our data? Aren't extreme
    values normal? Sometimes these artificial edits in the raw data, like imputing
    missing values or removing outliers, makes more trouble than it's worth.
  prefs: []
  type: TYPE_NORMAL
- en: Using robust methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fortunately, there are some robust methods for analyzing datasets, which are
    generally less sensitive to extreme values. These robust statistical methods have
    been developed since 1960, but there are some well-known related methods from
    even earlier, like using the median instead of the mean as a central tendency.
    Robust methods are often used when the underlying distribution of our data is
    not considered to follow the Gaussian curve, so most good old regression models
    do not work (see more details in the [Chapter 5](ch05.html "Chapter 5. Building
    Models (authored by Renata Nemeth and Gergely Toth)"), *Buildings Models (authored
    by Renata Nemeth and Gergely Toth)* and the [Chapter 6](ch06.html "Chapter 6. Beyond
    the Linear Trend Line (authored by Renata Nemeth and Gergely Toth)"), *Beyond
    the Linear Trend Line (authored by Renata Nemeth and Gergely Toth)*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take the traditional linear regression example of predicting the sepal
    length of iris flowers based on the petal length with some missing data. For this,
    we will use the previously defined `miris` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'So it seems that our estimate for the sepal and petal length ratio is around
    `0.42`, which is not too far from the real value by the way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The difference between the estimated and real coefficients is due to the artificially
    introduced missing values in a previous section. Can we produce even better estimates?
    We might impute the missing data with any of the previously mentioned methods,
    or instead we should rather fit a robust linear regression from the `MASS` package
    predicting `Sepal.Length` with the `Petal.Length` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s compare the coefficients of the models run against the original
    (full) and the simulated data (with missing values):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'To be honest, there''s not much difference between the standard linear regression
    and the robust version. Surprised? Well, the dataset included missing values completely
    at random, but what happens if the dataset includes other types of missing values
    or an outlier? Let''s verify this by simulating some dirtier data issues (with
    updating the sepal length of the first observation from `1.4` to `14` – let''s
    say due to a data input error) and rebuilding the models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: It seems that the `lm` model's performance decreased a lot, while the coefficients
    of the robust model are almost identical to the original model regardless of the
    outlier in the data. We can conclude that robust methods are pretty impressive
    and powerful tools when it comes to extreme values! For more information on the
    related methods already implemented in R, visit the related CRAN Task View at
    [http://cran.r-project.org/web/views/Robust.html](http://cran.r-project.org/web/views/Robust.html).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter focused on some of the hardest challenges in data analysis in the
    means of cleansing data, and we covered the most important topics on missing and
    extreme values. Depending on your field of interest or industry you are working
    for, dirty data can be a rare or major issue (for example I've seen some projects
    in the past when regular expressions were applied to a `JSON` file to make that
    valid), but I am sure you will find the next chapter interesting and useful despite
    your background – where we will learn about multivariate statistical techniques.
  prefs: []
  type: TYPE_NORMAL
