- en: Chapter 9.  Designing Spark Applications
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章 设计 Spark 应用程序
- en: Think functionally. Think application functionality designed like a pipeline
    with each piece plumbed together doing some part of the whole job in hand. It
    is all about processing data, and that is what Spark does in a highly versatile
    manner. Data processing starts with the seed data that gets into the processing
    pipeline. The seed data can be a new piece of data that is ingested into the system,
    or it can be some kind of master dataset that lives in the enterprise data store
    and needs to be sliced and diced to produce different views to serve various purposes
    and business needs. It is this slicing and dicing that is going to be the norm
    when designing and developing data processing applications.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 以函数式思维。以像管道一样设计的应用程序功能，每个部分都连接在一起，完成整个工作的某个部分。这全部关于数据处理，这正是 Spark 以高度灵活的方式所做的事情。数据处理从进入处理管道的种子数据开始。种子数据可以是系统摄取的新数据，也可以是某种存储在企业数据存储中的主数据集，需要被切割和重组以产生不同的视图，服务于各种目的和业务需求。在设计和发展数据处理应用程序时，这种切割和重组将成为常态。
- en: Any application development exercise starts with a study of the domain, the
    business requirement `s, and the technical tool selection. It is not going to
    be different here. Even though this chapter is going to see the design and development
    of a Spark application, the initial focus will be on the overall architecture
    of data processing applications, use cases, the data, and the applications that
    transform the data from one state to another. Spark is just a driver that assembles
    data processing logic and data together, using its highly powerful infrastructure
    to produce the desired results.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 任何应用程序开发练习都是从研究领域、业务需求`s`和技术工具选择开始的。这里也不例外。尽管本章将探讨 Spark 应用的设计和开发，但最初的焦点将放在数据处理应用程序的整体架构、用例、数据以及将数据从一种状态转换到另一种状态的应用程序上。Spark
    只是一个将数据处理逻辑和数据组合在一起的驱动程序，利用其高度强大的基础设施产生所需的结果。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下主题：
- en: Lambda Architecture
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lambda 架构
- en: Microblogging with Spark
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Spark 进行微博
- en: Data dictionaries
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据字典
- en: Coding style
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码风格
- en: Data ingestion
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据摄取
- en: Lambda Architecture
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Lambda 架构
- en: Application architecture is very important for any kind of software development.
    It is the blueprint that decides how the software has to be built with a good
    amount of generality and the capability to customize when needed. For common application
    needs, some popular architectures are available, and there is no need for any
    ground-up architecture effort in order to use them. These public architecture
    frameworks are designed by some of the best minds for the benefit of the general
    public. These popular architectures are very useful because there is no barrier
    to entry, and they are used by so many people. There are popular architectures
    available for web application development, data processing, and so on.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序架构对于任何类型的软件开发都非常重要。它是决定软件如何构建的蓝图，具有相当程度的通用性和在需要时定制的功能。对于常见的应用程序需求，有一些流行的架构可供选择，使用它们不需要任何从头开始的架构努力。这些公共架构框架是由一些最优秀的大脑为公众的利益而设计的。这些流行的架构非常有用，因为它们没有进入障碍，并且被许多人使用。有流行的架构可供网络应用程序开发、数据处理等使用。
- en: 'Lambda Architecture is a recent and popular architecture that''s ideal for
    developing data processing applications. There are many tools and technologies
    available in the market to develop data processing applications. But independent
    of the technology, how the data processing application components are layered
    and composed together is driven by the architectural framework. That is why Lambda
    Architecture is a technology-agnostic architecture framework and, depending on
    the need, the appropriate technology choice can be made to develop the individual
    components. *Figure 1* captures the essence of Lambda Architecture:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Lambda 架构是一种近期流行且理想的架构，适用于开发数据处理应用程序。市场上有很多工具和技术可用于开发数据处理应用程序。但独立于技术，数据处理应用程序组件的分层和组合是由架构框架驱动的。这就是为什么
    Lambda 架构是一个技术无关的架构框架，根据需要，可以选择适当的技术来开发各个组件。*图1* 捕捉了 Lambda 架构的精髓：
- en: '![Lambda Architecture](img/image_09_001.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![Lambda 架构](img/image_09_001.jpg)'
- en: Figure 1
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图1
- en: 'Lambda Architecture consists of three layers:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Lambda 架构由三层组成：
- en: The batch layer is the main data store. Any kind of processing happens on this
    dataset. This is the golden dataset.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批处理层是主要的数据存储。任何类型的处理都发生在这个数据集上。这是金数据集。
- en: The serving layer processes the master dataset and prepares views for specific
    purposes, and they are termed purposed views here. This intermediate step of processing
    is required to serve the queries, or for generating outputs for specific needs.
    The queries and the specific dataset preparation don't directly access the master
    dataset.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务层处理主数据集并为特定目的准备视图，在这里它们被称为预定视图。这个处理的中介步骤是必要的，用于服务查询或为特定需求生成输出。查询和特定数据集准备不直接访问主数据集。
- en: The speed layer is all about data stream processing. The stream of data is processed
    in a real-time fashion and volatile real-time views are prepared if that is a
    business need. The queries or specific processes generating outputs may consume
    data from both the purposed data views and real-time views.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 速度层完全是关于数据流处理。数据流以实时方式处理，如果这是业务需求，则会准备易变的实时视图。查询或特定生成输出的过程可能从预定的数据视图和实时视图中消耗数据。
- en: Using the principles of Lambda Architecture to architect a big data processing
    system, Spark is going to be used here as a data processing tool. Spark fits nicely
    into all the data processing requirements in all three distinct layers.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Lambda架构的原则来构建大数据处理系统，这里将使用Spark作为数据处理工具。Spark非常适合所有三个不同层次的数据处理需求。
- en: This chapter is going to discuss some selected data processing use cases of
    a microblogging application. The application functionality, its deployment infrastructure,
    and the scalability factors are beyond the scope of this work. In a typical batch
    layer, the master dataset can be plain splittable serialization formats or NoSQL
    data stores, depending on the data access methods. If the application use cases
    are all batch operations, then standard serialization formats will be sufficient.
    But if the use cases mandate random access, NoSQL data stores will be ideal. Here,
    for the sake of simplicity, all the data files are stored in plain text files
    locally.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将讨论一些选定的微博应用数据处理用例。应用程序功能、其部署基础设施和可扩展性因素超出了本工作的范围。在典型的批处理层中，主数据集可以是普通的可分割序列化格式或NoSQL数据存储，具体取决于数据访问方法。如果应用程序用例都是批量操作，则标准序列化格式就足够了。但如果用例需要随机访问，NoSQL数据存储将是理想的。在这里，为了简化，所有数据文件都存储在本地纯文本文件中。
- en: Typical application development culminates in a completely functional application.
    But here, the use cases are realized in Spark data processing applications. Data
    processing always works as a part of the functionality of the main application
    and it is scheduled to run in batch mode or run as a listener waiting for data
    and processing it. So, corresponding to each of the use cases, individual Spark
    applications are developed, and they can be scheduled or made to run in listener
    mode as the case may be.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的应用程序开发最终会形成一个完全功能的应用程序。但在这里，用例是在Spark数据处理应用程序中实现的。数据处理始终作为主应用程序功能的一部分，并计划以批量模式运行或作为监听器等待数据并处理它。因此，针对每个用例，都会开发单独的Spark应用程序，并且可以根据情况安排或使其以监听器模式运行。
- en: Microblogging with Lambda Architecture
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Lambda架构进行微博
- en: Blogging has been around for couple of decades in various forms. In the initial
    days of blogging as a medium of publication, only professional or aspiring writers
    published articles through the medium of blogs. It spread the false notion that
    only serious content is published through blogs. In recent years, the concept
    of microblogging included the general public in the culture of blogging. Microblogs
    are sudden outbursts of the thought processes of people in the form of a few sentences,
    photos, videos, or links. Sites such as Twitter and Tumblr popularized this culture
    at the biggest scale possible with hundreds of millions of active users using
    the site.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 博客作为一种发布媒介已经存在了几十年，形式各异。在博客初期作为发布媒介的日子里，只有专业或渴望成为作家的作者通过博客发布文章。这导致了一个错误的观念，即只有严肃的内容才会通过博客发布。近年来，微博的概念让公众也加入了博客文化。微博是以几句话、照片、视频或链接的形式突然爆发出的个人思维过程。像Twitter和Tumblr这样的网站通过数亿活跃用户使用网站的方式，在最大规模上普及了这种文化。
- en: An overview of SfbMicroBlog
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SfbMicroBlog概述
- en: '**SfbMicroBlog**is a microblogging application with millions of users posting
    short messages. A new user who is going to use this application needs to sign
    up with a username and password. To post messages, users have to sign in first.
    The only thing users can do without signing in is read public messages posted
    by users. Users can follow other users. The act of following is a one-way relationship.
    If user A follows user B, user A can see all the messages posted by user B; at
    the same time, user B cannot see the messages posted by user A, because user B
    is not following user A. By default, all the messages posted by all the users
    are public messages and can be seen by everybody. But users have settings to make
    messages visible only to users who are following the message owner. After becoming
    a follower, unfollowing is also allowed.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**SfbMicroBlog**是一个拥有数百万用户发布短消息的微博应用。即将使用此应用的新用户需要使用用户名和密码进行注册。要发布消息，用户必须先登录。用户在不登录的情况下唯一能做的就是阅读用户发布的公开消息。用户可以关注其他用户。关注是一种单向关系。如果用户A关注用户B，用户A可以看到用户B发布的所有消息；同时，用户B不能看到用户A发布的消息，因为用户B没有关注用户A。默认情况下，所有用户发布的消息都是公开消息，可以被所有人看到。但用户有设置，可以使消息只对关注消息所有者的用户可见。成为关注者后，取消关注也是允许的。'
- en: Usernames have to be unique across all users. A username and password are required
    to sign in. Every user must have a primary e-mail address, and without that the
    signup process will not be complete. For extra security and password recovery,
    an alternate e-mail address or mobile phone number can be saved in the profile.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 用户名必须在所有用户中是唯一的。登录需要用户名和密码。每个用户都必须有一个主要电子邮件地址，没有这个地址，注册过程将不会完成。为了额外的安全性和密码恢复，可以在个人资料中保存备用电子邮件地址或手机号码。
- en: 'Messages cannot exceed the a of 140 characters. Messages can contain words
    prefixed with the # symbol to group them under various topics. Messages can contain
    usernames prefixed with the @ symbol to directly address users through messages
    that are posted. In other words, users can address any other user in their messages
    without being a follower.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 消息长度不得超过140个字符。消息可以包含以#符号开头的前缀词，以将它们分组到不同的主题下。消息可以包含以@符号开头的前缀用户名，以通过发布的消息直接向用户发送。换句话说，用户可以在他们的消息中提及任何其他用户，而无需成为其关注者。
- en: Once posted, the messages cannot be changed. Once posted, the messages cannot
    be deleted.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦发布，消息就不能更改。一旦发布，消息就不能删除。
- en: Getting familiar with data
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 熟悉数据
- en: 'All pieces of data that come to the master dataset come through a stream. The
    data stream is processed, an appropriate header for each message is inspected,
    and the right action to store it in the data store is done. The following list
    contains the important data items that come into the store through the same stream:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 所有进入主数据集的数据都通过一个流进入。数据流被处理，检查每条消息的适当标题，并执行将其存储在数据存储中的正确操作。以下列表包含通过同一流进入存储的重要数据项：
- en: '**User**: This dataset contains the user details when a user signs in or when
    a user''s data gets changed'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户**: 此数据集包含用户在登录或用户数据变更时的详细信息'
- en: '**Follower**: This dataset contains the relationship data that gets captured
    when a user opts to follow another user'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关注者**: 此数据集包含当用户选择关注另一个用户时捕获的关系数据'
- en: '**Message**: This dataset contains the messages posted by registered users'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**消息**: 此数据集包含注册用户发布的消息'
- en: 'This list of datasets forms the golden dataset. Based on this master dataset,
    various views are created that cater to the needs of the vital business functions
    in the application. The following list contains the important views of the master
    dataset:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据集列表构成了黄金数据集。基于此主数据集，创建了各种视图，以满足应用中关键业务功能的需求。以下列表包含主数据集的重要视图：
- en: '**Messages by users**: This view contains messages posted by each user in the
    system. When a given user wants to see the messages posted by him/her, the data
    generated by this view is used. This is also used by the given user''s followers.
    This is a situation where the main dataset is used for a specific purpose. The
    message dataset gives all the required data for this view.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户消息**: 此视图包含系统中每个用户发布的消息。当特定用户想要查看他们自己发布的消息时，使用此视图生成的数据。这也被给定用户的关注者使用。这是一个主要数据集用于特定目的的情况。消息数据集为该视图提供了所有所需的数据。'
- en: '**Messages to users**: In the messages, specific users can be addressed by
    prefixing the @ symbol followed by the addressee''s username. This data view contains
    the users addressed with the @ symbol and the corresponding messages. There is
    a limitation enforced in the implementation: you can only have one addressee in
    one message.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用户消息**：在消息中，可以通过在收件人用户名前加上@符号来指定特定用户。此数据视图包含使用@符号指定的用户和相应的消息。在实现中有一个限制：一条消息中只能有一个收件人。'
- en: '**Tagged messages**: In the messages, words prefixed with the # symbol becomes
    searchable messages. For example, the word #spark in a message signifies that
    the message is searchable by the word #spark. For a given hashtag, users can see
    all the public messages and the messages of users whom he/she is following in
    one list. This view contains pairs of the hashtag and the corresponding messages.
    There is a limitation enforced in the implementation: you can only have one tag
    in one message.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标记消息**：在消息中，以#符号开头的前缀单词成为可搜索的消息。例如，消息中的#spark单词表示该消息可以通过#spark进行搜索。对于给定的标签，用户可以看到所有公开消息以及他/她关注的用户的消息，都在一个列表中。此视图包含标签及其对应消息的配对。在实现中有一个限制：一条消息中只能有一个标签。'
- en: '**Follower users**: This view contains the list of users who are following
    a given user. In *Figure 2*, users **U1** and **U3** are in the list of users
    following **U4**.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关注用户**：此视图包含关注给定用户的用户列表。在*图2*中，用户**U1**和**U3**在关注**U4**的用户列表中。'
- en: '**Followed users**: This view contains the list of users who are followed by
    a given user. In *Figure 2*, users **U2** and **U4** are in the list of users
    who are followed by user **U1**:'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关注用户**：此视图包含由给定用户关注的用户列表。在*图2*中，用户**U2**和**U4**在用户**U1**关注的用户列表中：'
- en: '![Getting familiar with data](img/image_09_002.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![熟悉数据](img/image_09_002.jpg)'
- en: Figure 2
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图2
- en: 'In a nutshell, *Figure 3* gives the Lambda Architecture view of the solution
    and gives details of the datasets and the corresponding views:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，*图3*给出了Lambda架构视图的解决方案，并提供了数据集和相应视图的详细信息：
- en: '![Getting familiar with data](img/image_09_003-1.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![熟悉数据](img/image_09_003-1.jpg)'
- en: Figure 3
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图3
- en: Setting the data dictionary
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置数据字典
- en: The data dictionary describes the data, its meaning, and its relationship with
    other data items. For the SfbMicroBlog application, the data dictionary is going
    to be a very minimalistic one to implement the selected use cases. Using this
    as a base, readers can expand and implement their own data items and include data
    processing use cases. The data dictionary is given for all the master datasets,
    as well as the data views.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 数据字典描述了数据、其含义以及与其他数据项的关系。对于SfbMicroBlog应用，数据字典将是一个非常简约的，以实现选定的用例。以此为基础，读者可以扩展并实现他们自己的数据项，并包括数据处理用例。数据字典提供了所有主数据集以及数据视图。
- en: 'The following table shows the data items of the user dataset:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格显示了用户数据集的数据项：
- en: '| **User data** | **Type** | **Purpose** |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| **用户数据** | **类型** | **用途** |'
- en: '| Id | Long | Used to uniquely identify a user, as well as being the vertex
    identifier in the user relationship graph |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| Id | 长整型 | 用于唯一标识用户，同时也是用户关系图中的顶点标识符 |'
- en: '| Username | String | Used to uniquely identify users of the system |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| Username | 字符串 | 用于唯一标识系统用户 |'
- en: '| First name | String | Used to capture the first name of the user |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 名字 | 字符串 | 用于捕获用户的第一个名字 |'
- en: '| Last name | String | Used to capture the last name of the user |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 姓氏 | 字符串 | 用于捕获用户的姓氏 |'
- en: '| E-mail | String | Used to communicate with users |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 电子邮件 | 字符串 | 用于与用户通信 |'
- en: '| Alternate e-mail | String | Used for password recovery |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 备用电子邮件 | 字符串 | 用于密码恢复 |'
- en: '| Primary phone | String | Used for password recovery |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 主要电话 | 字符串 | 用于密码恢复 |'
- en: 'The following table captures the data items of the follower dataset:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格捕获了关注者数据集的数据项：
- en: '| **Follower data** | **Type** | **Purpose** |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| **关注者数据** | **类型** | **用途** |'
- en: '| Follower username | String | Used to identify who the follower is |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 关注者用户名 | 字符串 | 用于识别关注者 |'
- en: '| Followed username | String | Used to identify who is being followed |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 被关注用户名 | 字符串 | 用于识别被关注的人 |'
- en: 'The following table captures the data items of the message dataset:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格捕获了消息数据集的数据项：
- en: '| **Message data** | **Type** | **Purpose** |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| **消息数据** | **类型** | **用途** |'
- en: '| Username | String | Used to capture the user who posted the message |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| Username | 字符串 | 用于捕获发布消息的用户 |'
- en: '| Message Id | Long | Used to uniquely identify a message |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 消息ID | 长整型 | 用于唯一标识一条消息 |'
- en: '| Message | String | Used to capture the message that is being posted |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 消息 | 字符串 | 用于捕获正在发布的消息 |'
- en: '| Timestamp | Long | Used to capture the time at which the message is posted
    |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 时间戳 | 长整型 | 用于捕获消息发布的时间 |'
- en: 'The following table captures the data items of the Message to users view:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格捕获了消息到用户视图的数据项：
- en: '| **Message to users data** | **Type** | **Purpose** |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| **消息到用户数据** | **类型** | **用途** |'
- en: '| From username | String | Used to capture the user who posted the message
    |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 发送者用户名 | 字符串 | 用于捕获发布消息的用户 |'
- en: '| To username | String | Used to capture the user to whom the message is addressed;
    it is the username that is prefixed with the @ symbol |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 目标用户名 | 字符串 | 用于捕获消息的目标用户；它是前缀为@符号的用户名 |'
- en: '| Message Id | Long | Used to uniquely identify a message |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 消息ID | 长整型 | 用于唯一标识一条消息 |'
- en: '| Message | String | Used to capture the message that is being posted |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 消息 | 字符串 | 用于捕获正在发布的消息 |'
- en: '| Timestamp | Long | Used to capture the time at which the message is posted
    |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 时间戳 | 长整型 | 用于捕获消息发布的时间 |'
- en: 'The following table captures the data items of the Tagged messages view:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格捕获了标记消息视图的数据项：
- en: '| **Tagged messages data** | **Type** | **Purpose** |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| **标记消息数据** | **类型** | **用途** |'
- en: '| Hashtag | String | The word that is prefixed with the # symbol |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 标签 | 字符串 | 前缀为#符号的单词 |'
- en: '| Username | String | Used to capture the user who posted the message |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 用户名 | 字符串 | 用于捕获发布消息的用户 |'
- en: '| Message Id | Long | Used to uniquely identify a message |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 消息ID | 长整型 | 用于唯一标识一条消息 |'
- en: '| Message | String | Used to capture the message that is being posted |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 消息 | 字符串 | 用于捕获正在发布的消息 |'
- en: '| Timestamp | Long | Used to capture the time at which the message is posted
    |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 时间戳 | 长整型 | 用于捕获消息发布的时间 |'
- en: The follower relationship of the users is pretty straightforward and consists
    of the pairs of user identification numbers persisted in a data store.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 用户的关注关系非常直接，由存储在数据存储中的用户标识号对组成。
- en: Implementing Lambda Architecture
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现Lambda架构
- en: The concept of Lambda Architecture was introduced in the beginning of this chapter.
    Since it is a technology-agnostic architecture framework, when designing applications
    with it, it is imperative to capture the technology choices used in specific implementations.
    The following sections do exactly that.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Lambda架构的概念在本章的开头被引入。由于它是一个与技术无关的架构框架，当使用它设计应用程序时，捕捉特定实现中使用的具体技术选择是至关重要的。以下各节正是这样做的。
- en: Batch layer
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批处理层
- en: The core of the batch layer is a data store. For big data applications, there
    are plenty of choices for data stores. Typically, **Hadoop Distributed File System**
    (**HDFS**) in conjunction with Hadoop YARN is the current and accepted platform
    in which data is stored, mainly because of the ability to partition and distribute
    data across the Hadoop cluster.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 批处理层的核心是一个数据存储。对于大数据应用，数据存储的选择有很多。通常，**Hadoop分布式文件系统**（**HDFS**）与Hadoop YARN结合是当前和被接受的平台，数据存储在其中，主要是因为它能够在Hadoop集群中分区和分布数据。
- en: 'There are two types of data access any persistence store supports:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 任何持久存储支持两种类型的数据访问：
- en: Batch write/read
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批处理读写
- en: Random write/read
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机读写
- en: Both of these need separate data storage solutions. For batch data operations,
    typically splittable serialization formats such as Avro and Parquet are used.
    For random data operations, typically NoSQL data stores are used. Some of these
    NoSQL solutions sit on top of HDFS and some don't. It doesn't matter whether they
    are on top of HDFS or not, they provide partitioning and distribution of data.
    So depending on the use case and the distributed platform that is in use, appropriate
    solutions can be used.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这两者都需要单独的数据存储解决方案。对于批数据处理操作，通常使用可分割的序列化格式，如Avro和Parquet。对于随机数据操作，通常使用NoSQL数据存储。其中一些NoSQL解决方案建立在HDFS之上，而另一些则不是。它们是否建立在HDFS之上并不重要，它们都提供了数据的分区和分布。因此，根据用例和正在使用的分布式平台，可以使用适当的解决方案。
- en: When it comes to the storage of the data in HDFS, commonly used formats such
    as XML and JSON fail because HDFS partitions and distributes the files. When that
    happens, these formats have opening tags and ending tags, and splits at random
    locations in the file make the data dirty. Because of that, splittable file formats
    such as Avro or Parquet are efficient for storing in HDFS.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 当谈到在HDFS中存储数据时，常用的格式如XML和JSON失败，因为HDFS会分区和分发文件。当这种情况发生时，这些格式有开标签和结束标签，并且文件中的随机位置分割会使数据变得混乱。因此，可分割的文件格式如Avro或Parquet在HDFS中存储时效率更高。
- en: When it comes to the NoSQL data store solutions, there are many choices in the
    market, especially from the open source world. Some of these NoSQL data stores,
    such as Hbase, sit on top of HDFS. Some of the NoSQL data stores, such as Cassandra
    and Riak, don't need HDFS, can be deployed on regular operating systems, and can
    be deployed in master-less fashion so that there is no single point of failure
    in a cluster. The choice of the NoSQL store is again dependent on the usage of
    a particular technology within the organization, the production support contracts
    in place, and many other parameters.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 当谈到NoSQL数据存储解决方案时，市场上有很多选择，尤其是来自开源世界的。其中一些NoSQL数据存储，如Hbase，位于HDFS之上。还有一些NoSQL数据存储，如Cassandra和Riak，不需要HDFS，可以在常规操作系统上部署，并且可以以无主节点的方式部署，这样在集群中就没有单点故障。NoSQL存储的选择再次取决于组织内部特定技术的使用、现有的生产支持合同以及许多其他参数。
- en: Tip
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: This book doesn't recommend a given set of data store technologies for usage
    in conjunction with Spark because Spark drivers are available in abundance for
    most popular serialization formats and NoSQL data stores. In other words, most
    of the data store vendors have started supporting Spark in big way. Another interesting
    trend these days is that many of the prominent ETL tools have started supporting
    Spark, and because of that, those who are using such ETL tools may use Spark applications
    within their ETL processing pipelines.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 本书不推荐使用一组特定的数据存储技术与Spark一起使用，因为Spark驱动程序对于大多数流行的序列化格式和NoSQL数据存储都是丰富的。换句话说，大多数数据存储供应商已经开始大规模支持Spark。另一个有趣的趋势是，许多突出的ETL工具开始支持Spark，因此使用此类ETL工具的人可以在他们的ETL处理管道中使用Spark应用程序。
- en: In this application, neither an HDFS-based nor any NoSQL-based data store is
    being used in order to maintain simplicity and to avoid the complex infrastructure
    setup required to run the application for the readers. Throughout, the data is
    stored on the local system in text file formats. Readers who are interested in
    trying out the examples on HDFS or other NoSQL data stores may go ahead and try
    them, with some changes to the data write/read part of the application.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个应用中，既没有使用基于HDFS的数据存储，也没有使用任何基于NoSQL的数据存储，目的是为了保持简单，并避免为读者运行应用所需的复杂基础设施设置。在整个过程中，数据都存储在本地系统上的文本文件格式中。对尝试在HDFS或其他NoSQL数据存储上运行示例感兴趣的读者可以尝试，只需对应用的数据读写部分进行一些修改。
- en: Serving layer
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 服务层
- en: The serving layer can be implemented in Spark using various methods. If the
    data is not structured and is purely object-based, then the low-level RDD-based
    method is suitable. If the data is structured, a DataFrame is ideal. The use case
    that is being discussed here is dealing with structured data and hence wherever
    possible the Spark SQL library is going to be used. From the data stores, data
    is read and RDDs are created. The RDDs are converted to DataFrames and all the
    serving needs are accomplished using Spark SQL. In this way, the code is going
    to be succinct and easy to understand.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 服务层可以使用Spark通过各种方法实现。如果数据是非结构化的且完全是基于对象的，那么基于低级RDD的方法是合适的。如果数据是结构化的，DataFrame是理想的。这里讨论的使用案例是处理结构化数据，因此尽可能使用Spark
    SQL库。从数据存储中读取数据并创建RDD。将RDD转换为DataFrame，并使用Spark SQL完成所有服务需求。这样，代码将简洁易懂。
- en: Speed layer
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 速度层
- en: The speed layer is going to be implemented as a Spark Streaming application
    using Kafka as the broker with its own producers producing the messages. The Spark
    Streaming application will act as the consumer to the Kafka topics and receive
    the data that is getting produced. As discussed in the chapter covering Spark
    Streaming, the producers can be the Kafka console producer or any other producer
    supported by Kafka. But the Spark Streaming application working as the consumer
    here is not going to implement the logic of persisting the processed messages
    to the text file as they are not generally used in real-world use cases. Using
    this application as a base, readers can implement their own persistence mechanism.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 速度层将作为一个使用Kafka作为代理的Spark Streaming应用程序来实现，该代理有自己的生产者来产生消息。Spark Streaming应用程序将作为Kafka主题的消费者，接收正在产生的数据。如Spark
    Streaming章节所述，生产者可以是Kafka控制台生产者或Kafka支持的任何其他生产者。但在此作为消费者的Spark Streaming应用程序不会实现将处理后的消息持久化到文本文件的逻辑，因为这些在现实世界的用例中通常不使用。以这个应用程序为基础，读者可以实施他们自己的持久化机制。
- en: Queries
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 查询
- en: 'The queries are all generated from the speed layer and serving layer. Since
    the data is available in the form of DataFrames, as mentioned before, all the
    queries for the use case are implemented using Spark SQL. The obvious reason is
    that Spark SQL works as a consolidation technology that unifies the data sources
    and destinations. When readers are using the samples from this book and when they
    are ready to implement it in their real-world use cases, the overall methodology
    can remain the same, but the data sources and destinations may differ. The following
    are some of the queries that can be generated from the serving layer. It is up
    to the imagination of the readers to make the required changes to the data dictionary
    and be able to write these views or queries:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 所有查询都来自速度层和服务层。由于数据以DataFrame的形式提供，如前所述，所有针对用例的查询都是使用Spark SQL实现的。显而易见的原因是Spark
    SQL作为一种统一数据源和目的地的技术。当读者使用本书中的示例，并且准备在现实世界的用例中实施时，整体方法可以保持不变，但数据源和目的地可能不同。以下是一些可以从服务层生成的查询。读者可以根据自己的想象对数据字典进行必要的更改，以便能够编写这些视图或查询：
- en: Find the messages that are grouped by a given hashtag
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到按给定标签分组的消息
- en: Find the messages that are addressed to a given user
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到发送给指定用户的消息
- en: Find the followers of a given user
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到指定用户的粉丝
- en: Find the followed users of a given user
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到指定用户的关注者
- en: Working with Spark applications
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与Spark应用程序一起工作
- en: 'The workhorse of this application is the data processing engine consisting
    of many Spark applications. In general, they can be classified into the following
    types:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 此应用程序的工作核心是数据处理引擎，由许多Spark应用程序组成。通常，它们可以分为以下类型：
- en: 'A Spark Streaming application to ingest data: This is the main listener application
    that receives the data coming as a stream and stores it in the appropriate master
    dataset.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用于摄取数据的Spark Streaming应用程序：这是主要监听应用程序，它接收作为流传入的数据并将其存储在适当的总数据集中。
- en: 'A Spark application to create purposed views and queries: This is the application
    that is used to create various purposed views from the master datasets. Apart
    from that, the queries are also included in this application.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用于创建目的视图和查询的Spark应用程序：这是用于从主数据集中创建各种目的视图的应用程序。除此之外，查询也包括在这个应用程序中。
- en: 'A Spark GraphX application to do custom data processing: This is the application
    that is used to process the user-follower relationship.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用于执行自定义数据处理的Spark GraphX应用程序：这是用于处理用户关注关系的应用程序。
- en: All these applications are developed independently and they are submitted independently,
    but the stream processing application will be always running as a listener application
    to process the incoming messages. Apart from the main data streaming application,
    all the other applications are scheduled like regular jobs, such as cron jobs
    in a UNIX system. In this application, all these applications are producing various
    purposed views. The scheduling depends on the kind of application and how much
    delay is affordable between the main dataset and the views. It completely depends
    on the business functions. So this chapter is going to focus on Spark application
    development rather than scheduling, to keep the focus on the lessons learned in
    the earlier chapters.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些应用都是独立开发的，并且独立提交，但流处理应用将始终作为一个监听应用运行，以处理传入的消息。除了主要的数据流应用外，所有其他应用都像常规作业一样进行调度，例如UNIX系统中的cron作业。在这个应用中，所有这些应用都在生成各种目的的视图。调度取决于应用类型以及主数据集和视图之间可以接受的延迟量。这完全取决于业务功能。因此，本章将专注于Spark应用开发，而不是调度，以保持对早期章节中学到的教训的关注。
- en: Tip
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: It is not ideal to persist the data from the speed layer into text files when
    implementing real-world use cases. For simplicity, all the data is stored in text
    files to empower all levels of reader with the simplest setup. The speed layer
    implementation using Spark Streaming is a skeleton implementation without the
    persistence logic. Readers can enhance this to introduce persistence to their
    desired data stores.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现实际应用场景时，将速度层的数据持续存储到文本文件中并不是最佳选择。为了简化，所有数据都存储在文本文件中，以便让所有层次的读者都能以最简单的设置使用。使用Spark
    Streaming实现的速度层是一个没有持久化逻辑的骨架实现。读者可以增强这一点，将持久化引入他们希望的数据存储中。
- en: Coding style
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码风格
- en: Coding style has been discussed and lots of Spark application programming has
    been  done in the earlier chapters. By now, it has been proven in this book that
    Spark application development can be done in Scala, Python, and R. In most of
    the earlier chapters, the languages of choice were Scala and Python. In this chapter,
    the same trend will continue. Only for the Spark GraphX application, since there
    is no Python support, will the application be developed in Scala alone.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 代码风格已在早期章节中讨论，并且已经进行了大量的Spark应用程序编程。到目前为止，本书已经证明Spark应用开发可以使用Scala、Python和R语言进行。在大多数早期章节中，选择的语言是Scala和Python。在本章中，这一趋势将继续。只有对于Spark
    GraphX应用，由于没有Python支持，应用将仅使用Scala开发。
- en: The style of coding is going to be simple and to the point. The error handling
    and other best practices of application development are avoided deliberately to
    focus on the Spark features. In this chapter, wherever possible, the code is run
    from the appropriate language's Spark REPL. Since the anatomy of the complete
    application and the scripts to compile, build, and run them as applications have
    already been covered in the chapter that discussed Spark Streaming, the source
    code download will have it available as complete ready-to-run applications. Moreover,
    the chapter covering Spark Streaming discussed the anatomy of a complete Spark
    application, including the scripts to build and run Spark applications. The same
    methodology will be used in the applications that are going to be developed in
    this chapter too. When running such standalone Spark applications, as discussed
    in the initial chapters of this book, readers can enable Spark monitoring and
    see how the application is behaving. For the sake of brevity, these discussions
    will not be taken up again here.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 代码风格将简单直接。为了专注于Spark功能，故意避免了错误处理和其他最佳实践的应用开发。在本章中，尽可能从适当语言的Spark REPL运行代码。由于完整的应用程序结构和构建、编译和作为应用程序运行它们的脚本已在讨论Spark
    Streaming的章节中介绍，源代码下载将提供完整、可运行的应用程序。此外，讨论Spark Streaming的章节还介绍了完整Spark应用程序的结构，包括构建和运行Spark应用程序的脚本。同样的方法也将用于本章将要开发的应用程序。当运行此类独立Spark应用程序时，如本书最初章节所述，读者可以启用Spark监控并查看应用程序的行为。为了简洁起见，这些讨论将不再在此处进行。
- en: Setting up the source code
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置源代码
- en: '*Figure 4* shows the structure of the source code and the data directories
    that are being used in this chapter. A description of each of them is not provided
    here as the reader should be familiar with them, and they have been covered in
    [Chapter 6](ch06.html "Chapter 6.  Spark Stream Processing"), *Spark Stream Processing*.
    There are external library file dependency requirements for running the programs
    using Kafka. For that, the instructions to download the JAR file are in the `TODO.txt`
    file in the `lib`folders. The `submitPy.sh` and `submit.sh` files use some of
    the `Kafka` libraries in the Kafta installation as well. All these external JAR
    file dependencies have already been covered in [Chapter 6](ch06.html "Chapter 6. 
    Spark Stream Processing"), *Spark Stream Processing*.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4*展示了本章中使用的源代码结构和数据目录的结构。这里没有提供每个部分的描述，因为读者应该熟悉它们，并且它们已在[第6章](ch06.html "第6章。Spark流处理")中介绍，*Spark流处理*。运行使用
    Kafka 的程序需要外部库文件依赖项。为此，在 `lib` 文件夹中的 `TODO.txt` 文件中有下载 JAR 文件的说明。`submitPy.sh`
    和 `submit.sh` 文件还使用了 Kafka 安装中的某些 Kafka 库。所有这些外部 JAR 文件依赖项已在[第6章](ch06.html "第6章。Spark流处理")中介绍，*Spark流处理*。'
- en: '![Setting up the source code](img/image_09_004.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![设置源代码](img/image_09_004.jpg)'
- en: Figure 4
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图4
- en: Understanding data ingestion
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解数据摄取
- en: The Spark Streaming application works as the listener application that receives
    the data from its producers. Since Kafka is going to be used as the message broker,
    the Spark Streaming application will be its consumer application, listening to
    the topics for the messages sent by its producers. Since the master dataset in
    the batch layer has the following datasets, it is ideal to have individual Kafka
    topics for each of the topics, along with the datasets.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Streaming 应用程序作为监听应用程序，接收来自其生产者的数据。由于 Kafka 将用作消息代理，Spark Streaming 应用程序将成为其消费者应用程序，监听由其生产者发送的主题。由于批处理层中的主数据集包含以下数据集，因此为每个主题以及数据集创建单独的
    Kafka 主题是理想的。
- en: User dataset:  User
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户数据集：User
- en: 'Follower dataset: Follower'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关注者数据集：Follower
- en: 'Message dataset: Message'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消息数据集：Message
- en: '*Figure 5* provides an overall picture of the Kafka-based Spark Streaming application
    structure:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '*图5*展示了基于 Kafka 的 Spark Streaming 应用程序结构的整体视图：'
- en: '![Understanding data ingestion](img/image_09_005.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![理解数据摄取](img/image_09_005.jpg)'
- en: Figure 5
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图5
- en: Since the Kafka setup has already been covered in [Chapter 6](ch06.html "Chapter 6. 
    Spark Stream Processing"), *Spark Stream Processing*, only the application code
    is covered here.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Kafka 设置已在[第6章](ch06.html "第6章。Spark流处理")中介绍，*Spark流处理*，因此这里只介绍应用程序代码。
- en: The following scripts are run from a terminal window. Make sure that the `$KAFKA_HOME`
    environment variable is pointing to the directory where Kafka is installed. Also,
    it is very important to start Zookeeper, the Kafka server, the Kafka producer,
    and the Spark Streaming log event data processing application in separate terminal
    windows. Once the necessary Kafka topics are created as shown in the scripts,
    the appropriate producers have to start producing messages. Refer to the Kafka
    setup details that have already been covered in [Chapter 6](ch06.html "Chapter 6. 
    Spark Stream Processing"), *Spark Stream Processing*, before proceeding further.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 以下脚本需要在终端窗口中运行。请确保环境变量 `$KAFKA_HOME` 指向 Kafka 安装的目录。此外，非常重要的一点是，需要在单独的终端窗口中启动
    Zookeeper、Kafka 服务器、Kafka 生产者和 Spark Streaming 日志事件数据处理应用程序。一旦脚本中显示的必要 Kafka 主题创建完成，相应的生产者必须开始产生消息。在继续之前，请参考已在[第6章](ch06.html
    "第6章。Spark流处理")中介绍的 Kafka 设置细节，*Spark流处理*。
- en: 'Try the following commands in the terminal window prompt:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在终端窗口提示符中尝试以下命令：
- en: '[PRE0]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This section provides the details of the Scala code for the Kafka topic consumer
    application that processes the messages produced by the Kafka producer. The assumption
    before running the following code snippet is that Kafka is up and running, the
    required producers are producing messages, and then, if the application is run,
    it will start consuming the messages. The Scala program for the data ingestion
    is run by submitting it to the Spark cluster. Starting from the Scala directory,
    as shown in *Figure 4*, first compile the program and then run it. The `README.txt`
    file is to be consulted for additional instructions. The two following commands
    are to be executed to compile and run the program:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了处理 Kafka 生成者产生的消息的 Kafka 主题消费者应用程序的 Scala 代码的详细信息。在运行以下代码片段之前，假设 Kafka
    正在运行，所需的生成者正在产生消息，然后，如果运行应用程序，它将开始消费消息。运行数据摄取的 Scala 程序是通过将其提交到 Spark 集群来完成的。从
    *图 4* 所示的 Scala 目录开始，首先编译程序然后运行它。需要查阅 `README.txt` 文件以获取额外说明。需要执行以下两个命令来编译和运行程序：
- en: '[PRE1]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The following code is the program listing that is to be compiled and run using
    the preceding commands:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码是要使用前面命令编译和运行的程序列表：
- en: '[PRE2]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The Python program for the data ingestion is run by submitting it to the Spark
    cluster. Starting from the Python directory, as shown in *Figure 4*, run the program.
    The `README.txt`file is to be consulted for additional instructions. All the Kafka
    installation requirements are valid, even when running this Python program. The
    following command is to be followed for running the program. Since Python is an
    interpreted language, there is no compilation required here:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 运行数据摄取的 Python 程序是通过将其提交到 Spark 集群来完成的。从 *图 4* 所示的 Python 目录开始，运行程序。需要查阅 `README.txt`
    文件以获取额外说明。即使运行此 Python 程序，所有 Kafka 安装要求仍然有效。运行程序的命令如下。由于 Python 是一种解释型语言，这里不需要编译：
- en: '[PRE3]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The following code snippet is the Python implementation of the same application:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码片段是相同应用的 Python 实现：
- en: '[PRE4]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Generating purposed views and queries
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成目的视图和查询
- en: 'The following implementations in Scala and Python are the application that
    creates the purposed views and queries discussed in the earlier sections of this
    chapter. At the Scala REPL prompt, try the following statements:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的 Scala 和 Python 实现是创建本章前面部分讨论的目的视图和查询的应用程序。在 Scala REPL 提示符下，尝试以下语句：
- en: '[PRE5]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: These steps complete the process of loading all the required data from persistent
    stores into DataFrames. Here, the data comes from text files. In real-world use
    cases, it may come from popular NoSQL data stores, traditional RDBMS tables, or
    from Avro or Parquet serialized data stores loaded from HDFS.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤完成了将所有必需数据从持久存储加载到 DataFrame 的过程。在这里，数据来自文本文件。在实际应用场景中，数据可能来自流行的 NoSQL 数据存储、传统的
    RDBMS 表，或从 HDFS 加载的 Avro 或 Parquet 序列化数据存储。
- en: 'The following section uses these DataFrames and creates various purposed views
    and queries:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 下文使用这些 DataFrame 创建了各种目的的视图和查询：
- en: '[PRE6]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In the preceding Scala code snippet, the dataset and DataFrame-based programming
    model is being used because the programming language of choice was Scala. Now,
    since Python is not a strongly typed language, the Dataset API is not supported
    in Python, hence the following Python code uses the traditional RDD-based programming
    model of Spark in conjunction with the DataFrame-based programming model. At the
    Python REPL prompt, try the following statements:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的 Scala 代码片段中，由于选择的语言是 Scala，因此使用了数据集和 DataFrame 基于的编程模型。现在，由于 Python 不是一个强类型语言，Python
    中不支持 Dataset API，因此下面的 Python 代码使用了 Spark 的传统 RDD 基于的编程模型与 DataFrame 基于的编程模型结合。在
    Python REPL 提示符下，尝试以下语句：
- en: '[PRE7]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'These steps complete the process of loading all the required data from persistent
    stores into DataFrames. Here, the data comes from text files. In real-world use
    cases, it may come from popular NoSQL data stores, traditional RDBMS tables, or
    from Avro or Parquet serialized data stores loaded from HDFS. The following section
    uses these DataFrames and creates various purposed views and queries:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤完成了将所有必需数据从持久存储加载到 DataFrame 的过程。在这里，数据来自文本文件。在实际应用场景中，数据可能来自流行的 NoSQL 数据存储、传统的
    RDBMS 表，或从 HDFS 加载的 Avro 或 Parquet 序列化数据存储。下文使用这些 DataFrame 创建了各种目的的视图和查询：
- en: '[PRE8]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The purposed views and queries required to implement the use cases are developed
    as a single application. But in reality, it is not a good design practice to have
    all the views and queries in one application. It is good to separate them by persisting
    the views and refreshing them at regular intervals. If using only one application,
    caching and the use of custom-made context objects that are broadcasted to the
    Spark cluster could be employed to access the views.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 实现用例所需的目的视图和查询被开发为一个单一的应用程序。但在现实中，将所有视图和查询放在一个应用程序中并不是一个好的设计实践。通过持久化视图并在定期间隔刷新它们来分离它们是更好的做法。如果只使用一个应用程序，可以采用缓存和使用广播到Spark集群的自定义上下文对象来访问视图。
- en: Understanding custom data processes
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解自定义数据处理
- en: The views created here were created to serve various queries and to produce
    desired outputs. There are some other classes of data processing applications
    that are often developed to implement real-world use cases. From the Lambda Architecture
    perspective, this also falls into the serving layer. The reason why these custom
    data processes fall into the serving layer is mainly because most of these use
    or process data from the master dataset and create views or outputs. It is also
    very possible for the custom processed data to remain as a view, and the following
    use case is one of such cases.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这里创建的视图是为了服务于各种查询并生成所需的输出。还有一些其他类别的数据处理应用程序通常被开发来实施现实世界的用例。从Lambda架构的角度来看，这也属于服务层。这些自定义数据处理之所以归入服务层，主要是因为它们大多数都使用或处理来自主数据集的数据，并创建视图或输出。自定义处理的数据保持为视图的可能性也非常大，以下用例就是其中之一。
- en: 'In the SfbMicroBlog microblogging application, it is a very common requirement
    to see whether a given user A is in some way connected to user B in a direct follower
    relationship or in a transitive way. This use case can be implemented using a
    graph data structure to see whether the two users in question are in the same
    connected component, whether they are connected in a transitive way, or whether
    they are not connected at all. For this, a graph is constructed with all the users
    as the vertices and the follow relationship as edges using a Spark GraphX library-based
    Spark application. At the Scala REPL prompt, try the following statements:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在SfbMicroBlog微博应用程序中，查看给定用户A是否以某种方式直接或间接地与用户B相关联是一个非常常见的需求。这个用例可以通过使用图数据结构来实现，以查看两个相关用户是否属于同一个连通分量，是否以间接方式连接，或者根本不连接。为此，使用基于Spark
    GraphX库的Spark应用程序构建了一个包含所有用户作为顶点和关注关系作为边的图。在Scala REPL提示符下，尝试以下语句：
- en: '[PRE9]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The user graph with the users in the vertices and the connection relationship
    forming the edges is done. On this graph data structure, run the graph processing
    algorithm, the connected component algorithm. The following code snippet does
    this:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 完成了包含用户作为顶点和连接关系形成边的用户图的构建。在这个图数据结构上运行图处理算法，即连通分量算法。以下代码片段实现了这一点：
- en: '[PRE10]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The connected component graph, `cc`, and its triplets, `ccTriplets`, are created,
    and this can now be used to run various queries. Since the graph is an RDD-based
    data structure, if it is necessary to do queries, converting the graph RDD to
    DataFrames is a common practice. The following code demonstrates this:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 创建了连通分量图`cc`及其三元组`ccTriplets`，现在可以使用它来运行各种查询。由于图是一个基于RDD的数据结构，如果需要进行查询，将图RDD转换为DataFrames是一种常见的做法。以下代码演示了这一点：
- en: '[PRE11]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Using the preceding implementation of a purposed view to get a list of users
    and their connected component identification numbers, if there is a need to find
    out whether two users are connected, just read the records of those two users
    and see whether they have the same connected component identification number.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前面实现的目的视图来获取用户列表及其连通分量识别号，如果需要找出两个用户是否连接，只需读取这两个用户的记录并查看它们是否具有相同的连通分量识别号。
- en: References
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'For more information, visit the following links:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 更多信息，请访问以下链接：
- en: '[http://lambda-architecture.net/](http://lambda-architecture.net/)'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://lambda-architecture.net/](http://lambda-architecture.net/)'
- en: '[https://www.dre.vanderbilt.edu/~schmidt/PDF/Context-Object-Pattern.pdf](https://www.dre.vanderbilt.edu/~schmidt/PDF/Context-Object-Pattern.pdf)'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.dre.vanderbilt.edu/~schmidt/PDF/Context-Object-Pattern.pdf](https://www.dre.vanderbilt.edu/~schmidt/PDF/Context-Object-Pattern.pdf)'
- en: Summary
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter concludes the book with one single application's use cases, implemented
    using the Spark concepts learned in the earlier chapters of the book. From a data
    processing application architecture perspective, this chapter covered the Lambda
    Architecture as a technology-agnostic architectural framework for data processing
    applications, which has huge applicability in the big data application development
    space.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 本章以一个单一应用的使用案例结束本书，该案例使用了本书前几章学到的Spark概念来实现。从数据处理应用架构的角度来看，本章介绍了Lambda架构作为数据处理应用的技术无关性架构框架，在大数据应用开发领域具有巨大的适用性。
- en: From a data processing application development perspective, RDD-based Spark
    programming, Dataset-based Spark programming, Spark SQL-based DataFrames to process
    structured data, the Spark Streaming-based listener program that constantly listens
    to the incoming messages and processes them, and the Spark GraphX-based application
    to process follower relationships have been covered. The use cases covered so
    far have immense scope for readers to add their own functionalities and enhance
    the application use cases discussed in this chapter.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据处理应用开发的角度来看，已经涵盖了基于RDD的Spark编程、基于Dataset的Spark编程、基于Spark SQL的DataFrames来处理结构化数据、基于Spark
    Streaming的监听程序，该程序持续监听传入的消息并处理它们，以及基于Spark GraphX的应用来处理关注者关系。到目前为止所涵盖的使用案例为读者提供了巨大的空间来添加他们自己的功能并增强本章讨论的应用用例。
