- en: Chapter 9.  Designing Spark Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Think functionally. Think application functionality designed like a pipeline
    with each piece plumbed together doing some part of the whole job in hand. It
    is all about processing data, and that is what Spark does in a highly versatile
    manner. Data processing starts with the seed data that gets into the processing
    pipeline. The seed data can be a new piece of data that is ingested into the system,
    or it can be some kind of master dataset that lives in the enterprise data store
    and needs to be sliced and diced to produce different views to serve various purposes
    and business needs. It is this slicing and dicing that is going to be the norm
    when designing and developing data processing applications.
  prefs: []
  type: TYPE_NORMAL
- en: Any application development exercise starts with a study of the domain, the
    business requirement `s, and the technical tool selection. It is not going to
    be different here. Even though this chapter is going to see the design and development
    of a Spark application, the initial focus will be on the overall architecture
    of data processing applications, use cases, the data, and the applications that
    transform the data from one state to another. Spark is just a driver that assembles
    data processing logic and data together, using its highly powerful infrastructure
    to produce the desired results.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Lambda Architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Microblogging with Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data dictionaries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coding style
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data ingestion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lambda Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Application architecture is very important for any kind of software development.
    It is the blueprint that decides how the software has to be built with a good
    amount of generality and the capability to customize when needed. For common application
    needs, some popular architectures are available, and there is no need for any
    ground-up architecture effort in order to use them. These public architecture
    frameworks are designed by some of the best minds for the benefit of the general
    public. These popular architectures are very useful because there is no barrier
    to entry, and they are used by so many people. There are popular architectures
    available for web application development, data processing, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lambda Architecture is a recent and popular architecture that''s ideal for
    developing data processing applications. There are many tools and technologies
    available in the market to develop data processing applications. But independent
    of the technology, how the data processing application components are layered
    and composed together is driven by the architectural framework. That is why Lambda
    Architecture is a technology-agnostic architecture framework and, depending on
    the need, the appropriate technology choice can be made to develop the individual
    components. *Figure 1* captures the essence of Lambda Architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Lambda Architecture](img/image_09_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1
  prefs: []
  type: TYPE_NORMAL
- en: 'Lambda Architecture consists of three layers:'
  prefs: []
  type: TYPE_NORMAL
- en: The batch layer is the main data store. Any kind of processing happens on this
    dataset. This is the golden dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The serving layer processes the master dataset and prepares views for specific
    purposes, and they are termed purposed views here. This intermediate step of processing
    is required to serve the queries, or for generating outputs for specific needs.
    The queries and the specific dataset preparation don't directly access the master
    dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The speed layer is all about data stream processing. The stream of data is processed
    in a real-time fashion and volatile real-time views are prepared if that is a
    business need. The queries or specific processes generating outputs may consume
    data from both the purposed data views and real-time views.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the principles of Lambda Architecture to architect a big data processing
    system, Spark is going to be used here as a data processing tool. Spark fits nicely
    into all the data processing requirements in all three distinct layers.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter is going to discuss some selected data processing use cases of
    a microblogging application. The application functionality, its deployment infrastructure,
    and the scalability factors are beyond the scope of this work. In a typical batch
    layer, the master dataset can be plain splittable serialization formats or NoSQL
    data stores, depending on the data access methods. If the application use cases
    are all batch operations, then standard serialization formats will be sufficient.
    But if the use cases mandate random access, NoSQL data stores will be ideal. Here,
    for the sake of simplicity, all the data files are stored in plain text files
    locally.
  prefs: []
  type: TYPE_NORMAL
- en: Typical application development culminates in a completely functional application.
    But here, the use cases are realized in Spark data processing applications. Data
    processing always works as a part of the functionality of the main application
    and it is scheduled to run in batch mode or run as a listener waiting for data
    and processing it. So, corresponding to each of the use cases, individual Spark
    applications are developed, and they can be scheduled or made to run in listener
    mode as the case may be.
  prefs: []
  type: TYPE_NORMAL
- en: Microblogging with Lambda Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Blogging has been around for couple of decades in various forms. In the initial
    days of blogging as a medium of publication, only professional or aspiring writers
    published articles through the medium of blogs. It spread the false notion that
    only serious content is published through blogs. In recent years, the concept
    of microblogging included the general public in the culture of blogging. Microblogs
    are sudden outbursts of the thought processes of people in the form of a few sentences,
    photos, videos, or links. Sites such as Twitter and Tumblr popularized this culture
    at the biggest scale possible with hundreds of millions of active users using
    the site.
  prefs: []
  type: TYPE_NORMAL
- en: An overview of SfbMicroBlog
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**SfbMicroBlog**is a microblogging application with millions of users posting
    short messages. A new user who is going to use this application needs to sign
    up with a username and password. To post messages, users have to sign in first.
    The only thing users can do without signing in is read public messages posted
    by users. Users can follow other users. The act of following is a one-way relationship.
    If user A follows user B, user A can see all the messages posted by user B; at
    the same time, user B cannot see the messages posted by user A, because user B
    is not following user A. By default, all the messages posted by all the users
    are public messages and can be seen by everybody. But users have settings to make
    messages visible only to users who are following the message owner. After becoming
    a follower, unfollowing is also allowed.'
  prefs: []
  type: TYPE_NORMAL
- en: Usernames have to be unique across all users. A username and password are required
    to sign in. Every user must have a primary e-mail address, and without that the
    signup process will not be complete. For extra security and password recovery,
    an alternate e-mail address or mobile phone number can be saved in the profile.
  prefs: []
  type: TYPE_NORMAL
- en: 'Messages cannot exceed the a of 140 characters. Messages can contain words
    prefixed with the # symbol to group them under various topics. Messages can contain
    usernames prefixed with the @ symbol to directly address users through messages
    that are posted. In other words, users can address any other user in their messages
    without being a follower.'
  prefs: []
  type: TYPE_NORMAL
- en: Once posted, the messages cannot be changed. Once posted, the messages cannot
    be deleted.
  prefs: []
  type: TYPE_NORMAL
- en: Getting familiar with data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'All pieces of data that come to the master dataset come through a stream. The
    data stream is processed, an appropriate header for each message is inspected,
    and the right action to store it in the data store is done. The following list
    contains the important data items that come into the store through the same stream:'
  prefs: []
  type: TYPE_NORMAL
- en: '**User**: This dataset contains the user details when a user signs in or when
    a user''s data gets changed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Follower**: This dataset contains the relationship data that gets captured
    when a user opts to follow another user'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Message**: This dataset contains the messages posted by registered users'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This list of datasets forms the golden dataset. Based on this master dataset,
    various views are created that cater to the needs of the vital business functions
    in the application. The following list contains the important views of the master
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Messages by users**: This view contains messages posted by each user in the
    system. When a given user wants to see the messages posted by him/her, the data
    generated by this view is used. This is also used by the given user''s followers.
    This is a situation where the main dataset is used for a specific purpose. The
    message dataset gives all the required data for this view.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Messages to users**: In the messages, specific users can be addressed by
    prefixing the @ symbol followed by the addressee''s username. This data view contains
    the users addressed with the @ symbol and the corresponding messages. There is
    a limitation enforced in the implementation: you can only have one addressee in
    one message.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tagged messages**: In the messages, words prefixed with the # symbol becomes
    searchable messages. For example, the word #spark in a message signifies that
    the message is searchable by the word #spark. For a given hashtag, users can see
    all the public messages and the messages of users whom he/she is following in
    one list. This view contains pairs of the hashtag and the corresponding messages.
    There is a limitation enforced in the implementation: you can only have one tag
    in one message.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Follower users**: This view contains the list of users who are following
    a given user. In *Figure 2*, users **U1** and **U3** are in the list of users
    following **U4**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Followed users**: This view contains the list of users who are followed by
    a given user. In *Figure 2*, users **U2** and **U4** are in the list of users
    who are followed by user **U1**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Getting familiar with data](img/image_09_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2
  prefs: []
  type: TYPE_NORMAL
- en: 'In a nutshell, *Figure 3* gives the Lambda Architecture view of the solution
    and gives details of the datasets and the corresponding views:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting familiar with data](img/image_09_003-1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3
  prefs: []
  type: TYPE_NORMAL
- en: Setting the data dictionary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The data dictionary describes the data, its meaning, and its relationship with
    other data items. For the SfbMicroBlog application, the data dictionary is going
    to be a very minimalistic one to implement the selected use cases. Using this
    as a base, readers can expand and implement their own data items and include data
    processing use cases. The data dictionary is given for all the master datasets,
    as well as the data views.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table shows the data items of the user dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **User data** | **Type** | **Purpose** |'
  prefs: []
  type: TYPE_TB
- en: '| Id | Long | Used to uniquely identify a user, as well as being the vertex
    identifier in the user relationship graph |'
  prefs: []
  type: TYPE_TB
- en: '| Username | String | Used to uniquely identify users of the system |'
  prefs: []
  type: TYPE_TB
- en: '| First name | String | Used to capture the first name of the user |'
  prefs: []
  type: TYPE_TB
- en: '| Last name | String | Used to capture the last name of the user |'
  prefs: []
  type: TYPE_TB
- en: '| E-mail | String | Used to communicate with users |'
  prefs: []
  type: TYPE_TB
- en: '| Alternate e-mail | String | Used for password recovery |'
  prefs: []
  type: TYPE_TB
- en: '| Primary phone | String | Used for password recovery |'
  prefs: []
  type: TYPE_TB
- en: 'The following table captures the data items of the follower dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Follower data** | **Type** | **Purpose** |'
  prefs: []
  type: TYPE_TB
- en: '| Follower username | String | Used to identify who the follower is |'
  prefs: []
  type: TYPE_TB
- en: '| Followed username | String | Used to identify who is being followed |'
  prefs: []
  type: TYPE_TB
- en: 'The following table captures the data items of the message dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Message data** | **Type** | **Purpose** |'
  prefs: []
  type: TYPE_TB
- en: '| Username | String | Used to capture the user who posted the message |'
  prefs: []
  type: TYPE_TB
- en: '| Message Id | Long | Used to uniquely identify a message |'
  prefs: []
  type: TYPE_TB
- en: '| Message | String | Used to capture the message that is being posted |'
  prefs: []
  type: TYPE_TB
- en: '| Timestamp | Long | Used to capture the time at which the message is posted
    |'
  prefs: []
  type: TYPE_TB
- en: 'The following table captures the data items of the Message to users view:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Message to users data** | **Type** | **Purpose** |'
  prefs: []
  type: TYPE_TB
- en: '| From username | String | Used to capture the user who posted the message
    |'
  prefs: []
  type: TYPE_TB
- en: '| To username | String | Used to capture the user to whom the message is addressed;
    it is the username that is prefixed with the @ symbol |'
  prefs: []
  type: TYPE_TB
- en: '| Message Id | Long | Used to uniquely identify a message |'
  prefs: []
  type: TYPE_TB
- en: '| Message | String | Used to capture the message that is being posted |'
  prefs: []
  type: TYPE_TB
- en: '| Timestamp | Long | Used to capture the time at which the message is posted
    |'
  prefs: []
  type: TYPE_TB
- en: 'The following table captures the data items of the Tagged messages view:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Tagged messages data** | **Type** | **Purpose** |'
  prefs: []
  type: TYPE_TB
- en: '| Hashtag | String | The word that is prefixed with the # symbol |'
  prefs: []
  type: TYPE_TB
- en: '| Username | String | Used to capture the user who posted the message |'
  prefs: []
  type: TYPE_TB
- en: '| Message Id | Long | Used to uniquely identify a message |'
  prefs: []
  type: TYPE_TB
- en: '| Message | String | Used to capture the message that is being posted |'
  prefs: []
  type: TYPE_TB
- en: '| Timestamp | Long | Used to capture the time at which the message is posted
    |'
  prefs: []
  type: TYPE_TB
- en: The follower relationship of the users is pretty straightforward and consists
    of the pairs of user identification numbers persisted in a data store.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Lambda Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The concept of Lambda Architecture was introduced in the beginning of this chapter.
    Since it is a technology-agnostic architecture framework, when designing applications
    with it, it is imperative to capture the technology choices used in specific implementations.
    The following sections do exactly that.
  prefs: []
  type: TYPE_NORMAL
- en: Batch layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The core of the batch layer is a data store. For big data applications, there
    are plenty of choices for data stores. Typically, **Hadoop Distributed File System**
    (**HDFS**) in conjunction with Hadoop YARN is the current and accepted platform
    in which data is stored, mainly because of the ability to partition and distribute
    data across the Hadoop cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two types of data access any persistence store supports:'
  prefs: []
  type: TYPE_NORMAL
- en: Batch write/read
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random write/read
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both of these need separate data storage solutions. For batch data operations,
    typically splittable serialization formats such as Avro and Parquet are used.
    For random data operations, typically NoSQL data stores are used. Some of these
    NoSQL solutions sit on top of HDFS and some don't. It doesn't matter whether they
    are on top of HDFS or not, they provide partitioning and distribution of data.
    So depending on the use case and the distributed platform that is in use, appropriate
    solutions can be used.
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to the storage of the data in HDFS, commonly used formats such
    as XML and JSON fail because HDFS partitions and distributes the files. When that
    happens, these formats have opening tags and ending tags, and splits at random
    locations in the file make the data dirty. Because of that, splittable file formats
    such as Avro or Parquet are efficient for storing in HDFS.
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to the NoSQL data store solutions, there are many choices in the
    market, especially from the open source world. Some of these NoSQL data stores,
    such as Hbase, sit on top of HDFS. Some of the NoSQL data stores, such as Cassandra
    and Riak, don't need HDFS, can be deployed on regular operating systems, and can
    be deployed in master-less fashion so that there is no single point of failure
    in a cluster. The choice of the NoSQL store is again dependent on the usage of
    a particular technology within the organization, the production support contracts
    in place, and many other parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This book doesn't recommend a given set of data store technologies for usage
    in conjunction with Spark because Spark drivers are available in abundance for
    most popular serialization formats and NoSQL data stores. In other words, most
    of the data store vendors have started supporting Spark in big way. Another interesting
    trend these days is that many of the prominent ETL tools have started supporting
    Spark, and because of that, those who are using such ETL tools may use Spark applications
    within their ETL processing pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: In this application, neither an HDFS-based nor any NoSQL-based data store is
    being used in order to maintain simplicity and to avoid the complex infrastructure
    setup required to run the application for the readers. Throughout, the data is
    stored on the local system in text file formats. Readers who are interested in
    trying out the examples on HDFS or other NoSQL data stores may go ahead and try
    them, with some changes to the data write/read part of the application.
  prefs: []
  type: TYPE_NORMAL
- en: Serving layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The serving layer can be implemented in Spark using various methods. If the
    data is not structured and is purely object-based, then the low-level RDD-based
    method is suitable. If the data is structured, a DataFrame is ideal. The use case
    that is being discussed here is dealing with structured data and hence wherever
    possible the Spark SQL library is going to be used. From the data stores, data
    is read and RDDs are created. The RDDs are converted to DataFrames and all the
    serving needs are accomplished using Spark SQL. In this way, the code is going
    to be succinct and easy to understand.
  prefs: []
  type: TYPE_NORMAL
- en: Speed layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The speed layer is going to be implemented as a Spark Streaming application
    using Kafka as the broker with its own producers producing the messages. The Spark
    Streaming application will act as the consumer to the Kafka topics and receive
    the data that is getting produced. As discussed in the chapter covering Spark
    Streaming, the producers can be the Kafka console producer or any other producer
    supported by Kafka. But the Spark Streaming application working as the consumer
    here is not going to implement the logic of persisting the processed messages
    to the text file as they are not generally used in real-world use cases. Using
    this application as a base, readers can implement their own persistence mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: Queries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The queries are all generated from the speed layer and serving layer. Since
    the data is available in the form of DataFrames, as mentioned before, all the
    queries for the use case are implemented using Spark SQL. The obvious reason is
    that Spark SQL works as a consolidation technology that unifies the data sources
    and destinations. When readers are using the samples from this book and when they
    are ready to implement it in their real-world use cases, the overall methodology
    can remain the same, but the data sources and destinations may differ. The following
    are some of the queries that can be generated from the serving layer. It is up
    to the imagination of the readers to make the required changes to the data dictionary
    and be able to write these views or queries:'
  prefs: []
  type: TYPE_NORMAL
- en: Find the messages that are grouped by a given hashtag
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find the messages that are addressed to a given user
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find the followers of a given user
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find the followed users of a given user
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with Spark applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The workhorse of this application is the data processing engine consisting
    of many Spark applications. In general, they can be classified into the following
    types:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A Spark Streaming application to ingest data: This is the main listener application
    that receives the data coming as a stream and stores it in the appropriate master
    dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A Spark application to create purposed views and queries: This is the application
    that is used to create various purposed views from the master datasets. Apart
    from that, the queries are also included in this application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A Spark GraphX application to do custom data processing: This is the application
    that is used to process the user-follower relationship.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All these applications are developed independently and they are submitted independently,
    but the stream processing application will be always running as a listener application
    to process the incoming messages. Apart from the main data streaming application,
    all the other applications are scheduled like regular jobs, such as cron jobs
    in a UNIX system. In this application, all these applications are producing various
    purposed views. The scheduling depends on the kind of application and how much
    delay is affordable between the main dataset and the views. It completely depends
    on the business functions. So this chapter is going to focus on Spark application
    development rather than scheduling, to keep the focus on the lessons learned in
    the earlier chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is not ideal to persist the data from the speed layer into text files when
    implementing real-world use cases. For simplicity, all the data is stored in text
    files to empower all levels of reader with the simplest setup. The speed layer
    implementation using Spark Streaming is a skeleton implementation without the
    persistence logic. Readers can enhance this to introduce persistence to their
    desired data stores.
  prefs: []
  type: TYPE_NORMAL
- en: Coding style
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Coding style has been discussed and lots of Spark application programming has
    been  done in the earlier chapters. By now, it has been proven in this book that
    Spark application development can be done in Scala, Python, and R. In most of
    the earlier chapters, the languages of choice were Scala and Python. In this chapter,
    the same trend will continue. Only for the Spark GraphX application, since there
    is no Python support, will the application be developed in Scala alone.
  prefs: []
  type: TYPE_NORMAL
- en: The style of coding is going to be simple and to the point. The error handling
    and other best practices of application development are avoided deliberately to
    focus on the Spark features. In this chapter, wherever possible, the code is run
    from the appropriate language's Spark REPL. Since the anatomy of the complete
    application and the scripts to compile, build, and run them as applications have
    already been covered in the chapter that discussed Spark Streaming, the source
    code download will have it available as complete ready-to-run applications. Moreover,
    the chapter covering Spark Streaming discussed the anatomy of a complete Spark
    application, including the scripts to build and run Spark applications. The same
    methodology will be used in the applications that are going to be developed in
    this chapter too. When running such standalone Spark applications, as discussed
    in the initial chapters of this book, readers can enable Spark monitoring and
    see how the application is behaving. For the sake of brevity, these discussions
    will not be taken up again here.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the source code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Figure 4* shows the structure of the source code and the data directories
    that are being used in this chapter. A description of each of them is not provided
    here as the reader should be familiar with them, and they have been covered in
    [Chapter 6](ch06.html "Chapter 6.  Spark Stream Processing"), *Spark Stream Processing*.
    There are external library file dependency requirements for running the programs
    using Kafka. For that, the instructions to download the JAR file are in the `TODO.txt`
    file in the `lib`folders. The `submitPy.sh` and `submit.sh` files use some of
    the `Kafka` libraries in the Kafta installation as well. All these external JAR
    file dependencies have already been covered in [Chapter 6](ch06.html "Chapter 6. 
    Spark Stream Processing"), *Spark Stream Processing*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Setting up the source code](img/image_09_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4
  prefs: []
  type: TYPE_NORMAL
- en: Understanding data ingestion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Spark Streaming application works as the listener application that receives
    the data from its producers. Since Kafka is going to be used as the message broker,
    the Spark Streaming application will be its consumer application, listening to
    the topics for the messages sent by its producers. Since the master dataset in
    the batch layer has the following datasets, it is ideal to have individual Kafka
    topics for each of the topics, along with the datasets.
  prefs: []
  type: TYPE_NORMAL
- en: User dataset:  User
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Follower dataset: Follower'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Message dataset: Message'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 5* provides an overall picture of the Kafka-based Spark Streaming application
    structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding data ingestion](img/image_09_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5
  prefs: []
  type: TYPE_NORMAL
- en: Since the Kafka setup has already been covered in [Chapter 6](ch06.html "Chapter 6. 
    Spark Stream Processing"), *Spark Stream Processing*, only the application code
    is covered here.
  prefs: []
  type: TYPE_NORMAL
- en: The following scripts are run from a terminal window. Make sure that the `$KAFKA_HOME`
    environment variable is pointing to the directory where Kafka is installed. Also,
    it is very important to start Zookeeper, the Kafka server, the Kafka producer,
    and the Spark Streaming log event data processing application in separate terminal
    windows. Once the necessary Kafka topics are created as shown in the scripts,
    the appropriate producers have to start producing messages. Refer to the Kafka
    setup details that have already been covered in [Chapter 6](ch06.html "Chapter 6. 
    Spark Stream Processing"), *Spark Stream Processing*, before proceeding further.
  prefs: []
  type: TYPE_NORMAL
- en: 'Try the following commands in the terminal window prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This section provides the details of the Scala code for the Kafka topic consumer
    application that processes the messages produced by the Kafka producer. The assumption
    before running the following code snippet is that Kafka is up and running, the
    required producers are producing messages, and then, if the application is run,
    it will start consuming the messages. The Scala program for the data ingestion
    is run by submitting it to the Spark cluster. Starting from the Scala directory,
    as shown in *Figure 4*, first compile the program and then run it. The `README.txt`
    file is to be consulted for additional instructions. The two following commands
    are to be executed to compile and run the program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code is the program listing that is to be compiled and run using
    the preceding commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The Python program for the data ingestion is run by submitting it to the Spark
    cluster. Starting from the Python directory, as shown in *Figure 4*, run the program.
    The `README.txt`file is to be consulted for additional instructions. All the Kafka
    installation requirements are valid, even when running this Python program. The
    following command is to be followed for running the program. Since Python is an
    interpreted language, there is no compilation required here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code snippet is the Python implementation of the same application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Generating purposed views and queries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following implementations in Scala and Python are the application that
    creates the purposed views and queries discussed in the earlier sections of this
    chapter. At the Scala REPL prompt, try the following statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: These steps complete the process of loading all the required data from persistent
    stores into DataFrames. Here, the data comes from text files. In real-world use
    cases, it may come from popular NoSQL data stores, traditional RDBMS tables, or
    from Avro or Parquet serialized data stores loaded from HDFS.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following section uses these DataFrames and creates various purposed views
    and queries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding Scala code snippet, the dataset and DataFrame-based programming
    model is being used because the programming language of choice was Scala. Now,
    since Python is not a strongly typed language, the Dataset API is not supported
    in Python, hence the following Python code uses the traditional RDD-based programming
    model of Spark in conjunction with the DataFrame-based programming model. At the
    Python REPL prompt, try the following statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'These steps complete the process of loading all the required data from persistent
    stores into DataFrames. Here, the data comes from text files. In real-world use
    cases, it may come from popular NoSQL data stores, traditional RDBMS tables, or
    from Avro or Parquet serialized data stores loaded from HDFS. The following section
    uses these DataFrames and creates various purposed views and queries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The purposed views and queries required to implement the use cases are developed
    as a single application. But in reality, it is not a good design practice to have
    all the views and queries in one application. It is good to separate them by persisting
    the views and refreshing them at regular intervals. If using only one application,
    caching and the use of custom-made context objects that are broadcasted to the
    Spark cluster could be employed to access the views.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding custom data processes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The views created here were created to serve various queries and to produce
    desired outputs. There are some other classes of data processing applications
    that are often developed to implement real-world use cases. From the Lambda Architecture
    perspective, this also falls into the serving layer. The reason why these custom
    data processes fall into the serving layer is mainly because most of these use
    or process data from the master dataset and create views or outputs. It is also
    very possible for the custom processed data to remain as a view, and the following
    use case is one of such cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the SfbMicroBlog microblogging application, it is a very common requirement
    to see whether a given user A is in some way connected to user B in a direct follower
    relationship or in a transitive way. This use case can be implemented using a
    graph data structure to see whether the two users in question are in the same
    connected component, whether they are connected in a transitive way, or whether
    they are not connected at all. For this, a graph is constructed with all the users
    as the vertices and the follow relationship as edges using a Spark GraphX library-based
    Spark application. At the Scala REPL prompt, try the following statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The user graph with the users in the vertices and the connection relationship
    forming the edges is done. On this graph data structure, run the graph processing
    algorithm, the connected component algorithm. The following code snippet does
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The connected component graph, `cc`, and its triplets, `ccTriplets`, are created,
    and this can now be used to run various queries. Since the graph is an RDD-based
    data structure, if it is necessary to do queries, converting the graph RDD to
    DataFrames is a common practice. The following code demonstrates this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Using the preceding implementation of a purposed view to get a list of users
    and their connected component identification numbers, if there is a need to find
    out whether two users are connected, just read the records of those two users
    and see whether they have the same connected component identification number.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information, visit the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://lambda-architecture.net/](http://lambda-architecture.net/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.dre.vanderbilt.edu/~schmidt/PDF/Context-Object-Pattern.pdf](https://www.dre.vanderbilt.edu/~schmidt/PDF/Context-Object-Pattern.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter concludes the book with one single application's use cases, implemented
    using the Spark concepts learned in the earlier chapters of the book. From a data
    processing application architecture perspective, this chapter covered the Lambda
    Architecture as a technology-agnostic architectural framework for data processing
    applications, which has huge applicability in the big data application development
    space.
  prefs: []
  type: TYPE_NORMAL
- en: From a data processing application development perspective, RDD-based Spark
    programming, Dataset-based Spark programming, Spark SQL-based DataFrames to process
    structured data, the Spark Streaming-based listener program that constantly listens
    to the incoming messages and processes them, and the Spark GraphX-based application
    to process follower relationships have been covered. The use cases covered so
    far have immense scope for readers to add their own functionalities and enhance
    the application use cases discussed in this chapter.
  prefs: []
  type: TYPE_NORMAL
