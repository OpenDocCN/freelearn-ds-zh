- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deploying and Managing ML Models with Snowpark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The seamless deployment and effective management of models have become pivotal
    components of developing data science with Snowpark. The previous chapter covered
    how to prepare the data and train the model. This chapter delves into the intricacies
    of leveraging Snowpark to deploy and manage **machine learning** (**ML**) models
    efficiently, from deployment to integration with feature stores and model registries,
    exploring the essentials of streamlining ML models in Snowpark.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Deploying ML models in Snowpark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing Snowpark model data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Please refer to the *Technical requirements* section in the previous chapter
    for environment setup.
  prefs: []
  type: TYPE_NORMAL
- en: Supporting materials are available at [https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark](https://github.com/PacktPublishing/The-Ultimate-Guide-To-Snowpark).
  prefs: []
  type: TYPE_NORMAL
- en: Deploying ML models in Snowpark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the preceding chapter, we learned about how to develop ML models. Now that
    the models are ready, we must deploy them into Snowpark. To make it easier for
    developers to deploy the models, the Snowpark ML library consists of functions
    that encompass the introduction of a new development interface and additional
    functionalities aimed at securely facilitating the deployment of both features
    and models. Snowpark MLOps seamlessly complements the Snowpark ML Development
    API by offering advanced model management capabilities and integrated deployment
    functionalities within the Snowflake ecosystem. In the following subsections,
    we will explore the model registry and deploy the model for inference to obtain
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Snowpark ML model registry
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A **model registry** is a centralized repository that enables model developers
    to organize, share, and publish ML models efficiently. It streamlines collaboration
    among teams and stakeholders, facilitating the collaborative management of the
    lifecycle of all models within an organization. Organizing models is crucial for
    tracking various versions, quickly identifying the latest, and gaining insights
    into each model’s hyperparameters. A well-structured model registry enhances reproducibility
    and compelling comparison of results. It also allows tracking and analyzing model
    accuracy metrics, empowering informed decisions and continuous improvement. The
    following diagram shows the deployment of a model into a model registry:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – Deploying a model into a model registry](img/B19923_06_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – Deploying a model into a model registry
  prefs: []
  type: TYPE_NORMAL
- en: The model registry is a Python API that manages models within the Snowflake
    environment, offering scalable, secure deployment and management capabilities
    for models within Snowflake. The Snowpark model registry is built upon a native
    Snowflake model entity, incorporating built-in versioning support for more streamlined
    management of models.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To illustrate the model registration process, we’ll efficiently craft a streamlined
    XGBoost model using minimal parameters, leveraging grid search on the *Bike Sharing*
    dataset. The `BSD_TRAINING` table prepared in the previous chapter is the foundational
    dataset for constructing our XGBoost model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we are making a feature list and finding label and output columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This will print out the following DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Model DataFrame](img/B19923_06_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – Model DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: 'For the sake of simplicity, we will focus on optimizing two parameters within
    XGBoost:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This code employs Snowflake’s ML module to perform a grid search for hyperparameter
    tuning on a gradient boosting regressor. It explores combinations of `max_depth`
    and `min_child_weight` within specified ranges, aiming to optimize the model based
    on the input and label columns provided.
  prefs: []
  type: TYPE_NORMAL
- en: 'The subsequent logical progression involves partitioning the dataset into training
    and testing sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This division is essential to facilitate model fitting, allowing us to train
    the model on the designated training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting the optimum parameter
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Having successfully trained our dataset using the XGBoost model, the next imperative
    is identifying optimal parameter values defined through grid search. Remarkably
    similar to the process in the `scikit-learn` package, Snowpark ML offers a comparable
    methodology. The ensuing code mirrors the steps in extracting these optimal parameters
    and subsequently visualizing them, demystifying the process for seamless comprehension:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code uses `pandas`, `seaborn`, and `matplotlib` to analyze and
    visualize grid search results from a Snowpark ML model. It extracts the parameters,
    such as `max_depth` and `min_child_weight`, along with the corresponding **mean
    absolute percentage error** (**MAPE**) values for evaluation. The following code
    showcases the values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code creates a `pandas` DataFrame named `gs_results_df` from
    listed `max_depth`, `min_child_weight`, and `mape` values. It then utilizes `seaborn`
    to generate a line plot, visualizing the relationship between learning rates,
    MAPE scores, and different numbers of estimators. Finally, the `matplotlib` `plt.show()`
    command displays the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – Data plot](img/B19923_06_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – Data plot
  prefs: []
  type: TYPE_NORMAL
- en: 'Upon careful observation of the previous plot, it becomes evident that a `max_depth`
    value of `8` paired with a `min_child_weight` learning rate of `2` yields the
    optimal results. It’s noteworthy that, akin to `scikit-learn`, Snowpark ML offers
    streamlined methods for extracting these optimal parameters, simplifying the process
    for enhanced user convenience:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The code transforms the Snowpark ML grid search results into a format compatible
    with `scikit-learn` and then retrieves the best estimator, representing the model
    with optimal hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – Snowpark ML grid search result](img/B19923_06_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – Snowpark ML grid search result
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will use the Snowpark model registry to log the model.
  prefs: []
  type: TYPE_NORMAL
- en: Logging the optimal model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With our optimal model in hand, the pivotal phase of the model registry unfolds.
    Much akin to the previously created model, we can extend this process to encompass
    multiple models, registering each through the model registry. In this case, we
    will be registering only our optimal model. We’ll delve into a step-by-step exploration
    of how models can be registered and seamlessly deployed.
  prefs: []
  type: TYPE_NORMAL
- en: Note on the Model Registry and Feature Store
  prefs: []
  type: TYPE_NORMAL
- en: While we write this chapter, both the Model Registry and Feature Store are in
    private preview. Once they are open to all, the API methods might be slightly
    different from what we see in this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to create a registry to log our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code prepares the essential details to log a model in the model
    registry:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: It extracts the optimal model, as determined by the grid search, and retrieves
    specific hyperparameters such as `max_depth`, `min_child_weight`, and optimal
    parameter values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Having completed all necessary steps for model registration, the preceding
    code seamlessly integrates the gathered information to register our optimal XGBoost
    model officially in the model registry:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The code assigns a name (`bike_model_xg_boost`) and version (`1`) to the model
    and logs it into the registry with associated details, including the sample input
    data and specific options. Additionally, it sets a custom metric, MAPE (`mean_abs_pct_err`),
    for the registered model with its corresponding value (`optimal_mape`). To verify
    successful registration, execute the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This will confirm whether our XGBoost model and the gradient boost model (only
    the XGBoost model are steps shown here to avoid unnecessary repetition) are appropriately
    listed in the model registry:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – Model registered in the model registry](img/B19923_06_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 – Model registered in the model registry
  prefs: []
  type: TYPE_NORMAL
- en: In the iterative journey of experimentation with diverse models and varied parameter
    configurations, we diligently register each model within the model registry through
    a structured methodology that ensures that each model, fine-tuned and optimized,
    is stored efficiently for future use. In the next section, we will deploy the
    model from the registry using Snowpark MLOps and predict its results.
  prefs: []
  type: TYPE_NORMAL
- en: Model deployment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the preceding sections, we navigated the intricate landscape of deploying
    models through complex **user-defined functions** (**UDFs**) or stored procedures.
    However, the new Snowpark model registry simplifies the cumbersome process. It
    enhances the maintainability of models by providing a streamlined and standardized
    framework for handling predictive models in a production setting. This shift in
    methodology optimizes operational efficiency and aligns seamlessly with contemporary
    practices in the dynamic field of data science. A standard model deployment would
    follow this naming convention:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This functionality provides a comprehensive view of models transitioning from
    registration to deployment within the system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – Bike model deployment](img/B19923_06_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 – Bike model deployment
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s leverage our deployed model to infer predictions for the test data
    and assess the accuracy of our predictions against actual outcomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The code initiates a `ModelReference` object, linking to a specific model within
    the registry by referencing its name and version. Subsequently, it leverages this
    reference to predict the provided test data using the specified deployment, resulting
    in a Snowpark DataFrame (`result_sdf`). Finally, it displays the expected results
    through the `show()` method, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7 – Model result DataFrame](img/B19923_06_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 – Model result DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: Having observed a comprehensive cycle encompassing model development, registration,
    and deployment, it’s noteworthy that this process is replicable for any model-building
    endeavor through the model registry. In the subsequent section, we will elucidate
    several beneficial methods inherent in the model registry, elevating its usability
    and augmenting the overall modeling experience. Now that we have deployed the
    model, we will look at other model registry methods.
  prefs: []
  type: TYPE_NORMAL
- en: Model registry methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Beyond the functionality outlined for model deployment, the model registry extends
    its utility with several beneficial methods designed for effective model maintenance
    and housekeeping activities. In this section, we will explore a selection of these
    methods to enhance our understanding of their practical applications. We will
    start with model metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Model metrics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Linking metrics to your model version is a pivotal feature within the model
    registry. This functionality serves as a fundamental aspect, providing a systematic
    means to gauge the performance of each model version distinctly. By associating
    metrics, users gain valuable insights into the efficacy of different iterations,
    facilitating informed decision-making based on the quantitative evaluation of
    model performance across various versions. It also helps in automating the pipeline,
    thereby retraining if model metrics drop below the threshold value. This deliberate
    metrics integration enriches the comprehensive model management capabilities and
    establishes a structured framework for ongoing model evaluation and refinement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding line sets a custom metric, `mean_abs_pct_err`, for a specific
    model version in the model registry, assigning the calculated MAPE value to quantify
    the model’s performance. It enhances the model registry’s ability to track and
    evaluate the effectiveness of different model versions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This will print the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8 – MAPE value of mean_abs_pct_err](img/B19923_06_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 – MAPE value of mean_abs_pct_err
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to setting, we can retrieve the value of a specific custom metric,
    `mean_abs_pct_err`, associated with a particular model version from the model
    registry. It allows users to access and analyze quantitative performance metrics
    for practical model evaluation and comparison across different versions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Much like retrieving a specific metric for a deployed model, an analogous approach
    allows us to access a comprehensive list of all associated metrics for a given
    deployed model. This facilitates a holistic understanding of the model’s performance,
    providing a detailed overview of various metrics related to its evaluation and
    contributing to a thorough analysis of its effectiveness:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.9 – Metrics of mean_abs_pct_err](img/B19923_06_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.9 – Metrics of mean_abs_pct_err
  prefs: []
  type: TYPE_NORMAL
- en: We can find the value of metrics from the model in the registry. In the next
    section, we will cover model tags and descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: Model tags and descriptions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Setting a tag name and description for a deployed model is crucial for effective
    experiment tracking and documentation. Tags and descriptions provide context and
    insights into the model’s purpose, configuration, and notable characteristics.
    This aids in maintaining a structured and informative record, enhancing reproducibility,
    and facilitating a more comprehensive analysis of experiment outcomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The provided code first sets a tag named `stage` with the `experiment_1` value
    for a specific model version in the model registry. This tagging is a contextual
    marker for the model’s purpose or usage. The subsequent line retrieves and displays,
    in a tabular format, the names of all models along with their associated tags,
    showcasing the tagged information for each model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.10 – Model tags](img/B19923_06_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.10 – Model tags
  prefs: []
  type: TYPE_NORMAL
- en: 'Another noteworthy aspect is the flexibility to modify and remove tags as necessary,
    allowing for a dynamic adjustment of our experiment design. This capability empowers
    users to iteratively refine contextual information associated with a model, providing
    meaningful and evolving tags. The ability to alter and remove tags enhances experiment
    design flexibility. It ensures that the documentation and context surrounding
    models can adapt to changing insights and requirements throughout the experimentation
    lifecycle:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The provided code initiates the removal of a specific tag, named `usage`, from
    a particular model version within the model registry. Following this operation,
    the subsequent line retrieves and displays, in a tabular format, the names of
    all models along with their associated tags. This showcases the updated information
    after removing the specified tag, providing a comprehensive view of models and
    their altered tag configurations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.11 – Model tags removed](img/B19923_06_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.11 – Model tags removed
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also provide descriptive information for a deployed model, offering
    valuable context and aiding future references. The ability to furnish a meaningful
    description enhances the comprehensibility of the model’s purpose, configuration,
    or other pertinent details. The ensuing code block, which is self-explanatory
    and mirrors the process of setting tags, enables the assignment of a descriptive
    narrative to a deployed model, ensuring that vital information is encapsulated
    for reference in subsequent analyses or experiments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The model description is set and can be retrieved to display on the screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.12 – Model description](img/B19923_06_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.12 – Model description
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the model tags and description set, we will examine how to
    access the registry history.
  prefs: []
  type: TYPE_NORMAL
- en: Registry history
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Accessing the registry history is an invaluable capability, offering a chronological
    account of model versions, associated metrics, tags, and descriptions. This historical
    perspective enhances transparency in model development and empowers data scientists
    to make informed decisions, track performance trends, and iterate on model improvements
    with precision. The ML registry, coupled with its history-tracking feature, thus
    emerges as a pivotal asset in the data science arsenal, fostering a structured
    and efficient approach to model development, deployment, and ongoing refinement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The code retrieves and converts the entire history of the model registry into
    a `pandas` DataFrame, presenting a comprehensive tabular view of all recorded
    events and changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.13 – Registry history](img/B19923_06_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.13 – Registry history
  prefs: []
  type: TYPE_NORMAL
- en: 'Narrowing down the search in the registry history is a common practice, and
    it can be achieved by specifying a model name and version. This targeted filtering
    allows for more focused exploration, aligning with typical preferences when navigating
    the model registry history:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This code fetches and converts the specific history of a particular model version,
    identified by its name and version, into a `pandas` DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.14 – Registry history filter](img/B19923_06_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.14 – Registry history filter
  prefs: []
  type: TYPE_NORMAL
- en: The resulting DataFrame offers a detailed chronological record of all events
    and changes associated with that specific model version within the registry. In
    the next section, we will learn about operations on the model registry.
  prefs: []
  type: TYPE_NORMAL
- en: Model registry operations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the contemporary landscape of ML, the lifecycle of models is continually
    contracting, leading to shorter durations for deployed models. Concurrently, experiments
    with varying parameters generate many models, and their subsequent deployments
    are registered. This proliferation necessitates a thoughtful approach to model
    management, including periodic cleanup processes to maintain a streamlined and
    efficient model registry:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code deletes a specific deployment instance identified by the
    model’s name, version, and deployment name from the model registry, ensuring efficient
    cleanup and management of deployed models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.15 – Deleting a specific deployment](img/B19923_06_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.15 – Deleting a specific deployment
  prefs: []
  type: TYPE_NORMAL
- en: 'It serves as a method to remove obsolete or undesired deployments. We can also
    delete a whole model from the registry by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Similar to deleting a deployment, this code will delete a model from the model
    registry:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.16 – Deleting a model](img/B19923_06_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.16 – Deleting a model
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the entire model has been deleted from the registry. In the
    next section, we will look at the benefits of a model registry.
  prefs: []
  type: TYPE_NORMAL
- en: Benefits of the model registry in the model lifecycle
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Snowpark model registry streamlines the management of ML models throughout
    their lifecycle. Let’s delve into how the model registry in Snowpark can assist
    in various stages of the ML model lifecycle:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model development**: During the development phase, data scientists can use
    Snowpark to build, train, and validate ML models directly within Snowflake. The
    model registry provides a centralized location to store and version control these
    models, making it easier to track changes, compare performance, and collaborate
    with team members.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model deployment**: Once a model is trained and validated, it needs to be
    deployed into production environments for inference. The model registry facilitates
    seamless deployment by providing a standardized interface to deploy models across
    different environments. This ensures consistency and reliability in model deployment
    processes.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model monitoring**: Monitoring the performance of deployed models is crucial
    for detecting drift and ensuring continued accuracy over time. The model registry
    can integrate with monitoring tools to track model performance metrics, such as
    accuracy, precision, recall, and F1-score, enabling proactive maintenance and
    optimization.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model governance**: Ensuring compliance with regulatory requirements and
    organizational policies is essential for responsible AI deployment. The model
    registry supports governance by providing capabilities for access control, audit
    logging, and versioning. This helps organizations maintain visibility and control
    over the entire model lifecycle.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model retraining and updating**: ML models need to be periodically retrained
    and updated to adapt to changing data distributions and business requirements.
    The model registry simplifies this process by enabling data scientists to seamlessly
    retrain models using updated data and algorithms while preserving the lineage
    and history of model versions.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model retirement**: As models become obsolete or are replaced by newer versions,
    they need to be retired gracefully. The model registry facilitates the retirement
    process by archiving outdated models, documenting reasons for retirement, and
    ensuring that relevant stakeholders are notified of changes.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The model registry offers an organized framework for model management and provides
    functionalities for efficient housekeeping, including setting and tracking metrics,
    tags, and descriptions. The registry’s history-tracking capabilities have emerged
    as a valuable feature, allowing users to gain insights into the evolution of models
    over time. Tags and descriptions offer context and facilitate experiment tracking
    for accessing and filtering the registry history, enabling a comprehensive view
    of model-related activities. Overall, the model registry emerges as a powerful
    addition to Snowpark ML, centralizing model management, facilitating experimentation,
    and ensuring a streamlined and organized approach to model development and deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the model registry in Snowpark plays a pivotal role in streamlining
    the ML model lifecycle, from development and deployment to monitoring, governance,
    retraining, and retirement. By providing a centralized platform for managing models,
    it helps organizations maximize the value of their ML investments while minimizing
    operational overhead and risks.
  prefs: []
  type: TYPE_NORMAL
- en: Managing Snowpark model data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we covered the deployment of ML models using the model
    registry. This section will look at managing Snowpark Models using feature stores.
    Snowpark ML Feature Store simplifies the feature engineering process and is integral
    to ML, significantly influencing model performance based on the quality of features
    employed. This chapter will help us learn about using feature stores and managing
    Snowpark models.
  prefs: []
  type: TYPE_NORMAL
- en: Snowpark Feature Store
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Snowpark Feature Store is an integrated solution for data scientists and
    ML engineers. It facilitates the creation, storage, management, and serving of
    ML features for model training and inference and is accessible through the Snowpark
    ML library. The feature store defines, manages, and retrieves features, supported
    by a managed infrastructure for feature metadata management and continuous feature
    processing. Its primary function is to make these features readily available for
    reuse in the ongoing development of future ML models. Feature stores play a pivotal
    role in operationalizing data input, tracking, and governance within the realm
    of feature engineering for ML:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.17 – Feature Store](img/B19923_06_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.17 – Feature Store
  prefs: []
  type: TYPE_NORMAL
- en: By leveraging the Snowpark Feature Store, which is designed to simplify and
    enhance this process by offering increased efficiency for data scientists and
    ML practitioners. ML teams can uphold a singular and updated source of truth for
    model training, versioning, and inference features. We will use the *Bike Sharing*
    dataset and the ML model developed in the previous section to showcase how the
    Feature Store enhances the model development and deployment cycle.
  prefs: []
  type: TYPE_NORMAL
- en: Benefits of Feature Store
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Utilizing feature stores provides several benefits for ML initiatives. Firstly,
    they enable feature reuse by saving developed features, allowing them to be quickly
    accessed and repurposed for new ML models, thereby saving time and effort. Secondly,
    they ensure feature consistency by providing a centralized registry for all ML
    features, maintaining consistent definitions and documentation across teams. Thirdly,
    feature stores help maintain peak model performance by centralizing feature pipelines,
    ensuring consistency between training and inference, and continuously monitoring
    data pipelines for any discrepancies.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, feature stores enhance security and data governance by providing
    detailed information about each ML model’s training data and deployment data,
    facilitating iteration and debugging. Integrating feature stores with cloud data
    warehouses enhances data security, ensuring the protection of both models and
    training data. Lastly, feature stores foster collaboration between teams by offering
    a centralized platform for the development, storage, modification, and sharing
    of ML features, promoting cross-team collaboration and idea-sharing for multiple
    business applications.
  prefs: []
  type: TYPE_NORMAL
- en: Feature stores versus data warehouses
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Delving into the distinction between feature stores and data warehouses sheds
    light on their collaborative role in enhancing value within ML projects.
  prefs: []
  type: TYPE_NORMAL
- en: Similarities – shared traits and functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Both feature stores and data warehouses exhibit parallels in their operational
    methodologies. They rely on **Extract, Transform, Load** (**ETL**) pipelines to
    facilitate data management and accessibility. Additionally, they serve as repositories
    endowed with metadata, fostering seamless data sharing and utilization across
    organizational teams.
  prefs: []
  type: TYPE_NORMAL
- en: End users – tailored utility
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A notable deviation lies in their primary user base. Data warehouses traditionally
    cater to analysts entrenched in the generation of comprehensive business reports,
    delving into historical data for strategic insights. Conversely, feature stores
    cater specifically to data scientists immersed in the development of predictive
    ML models. While the latter may draw from data warehouses for supplementary insights,
    their core function revolves around leveraging feature stores for streamlined
    model development and inference.
  prefs: []
  type: TYPE_NORMAL
- en: Data types – structural variances
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Structurally, data warehouses house domain-specific data within relational databases
    characterized by well-defined schemas. This structured format facilitates streamlined
    querying and retrieval of pertinent information, ideal for analytical endeavors.
    Conversely, feature stores house a distinct array of feature values crucial for
    ML model training. These values encompass both quantitative and categorical variables,
    enriching the model development process with granular insights.
  prefs: []
  type: TYPE_NORMAL
- en: ETL pipelines – divergent trajectories
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The operational dynamics of ETL pipelines further accentuate the disparity between
    feature stores and data warehouses. ETL processes within data warehouses predominantly
    focus on data cleansing and transformation, ensuring data accuracy and coherence
    within the defined schema. In contrast, feature store pipelines embark on a more
    intricate journey, encompassing data extraction, transformation, and feature engineering.
    The transformations within feature stores often entail sophisticated computations
    and aggregations to distill intricate insights vital for model training and inference,
    underscoring their pivotal role in the ML lifecycle.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve grasped the essence of feature stores, comprehending their significance
    and differentiation from data warehouses, let’s delve deeper into the various
    components comprising a feature store.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the subsequent section, we’ll embark on the creation of a rudimentary feature
    store tailored to the *Bike Sharing* dataset, focusing solely on weather-related
    features. The process entails the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Feature store creation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feature entity creation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Selecting and transforming weather features
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Creating a feature view
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generating datasets enriched with the feature view
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Constructing an ML model empowered by the enriched dataset
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Facilitating predictions based on the trained model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s discuss each of them in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a feature store
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Initiating work with the Snowflake Feature Store involves establishing a new
    feature store or connecting to an existing one. This is accomplished by furnishing
    specific details to the `FeatureStore` constructor, including a Snowpark session,
    database name, feature store name, and default warehouse name. The `creation_mode`
    parameter is crucial in determining whether a new feature store should be created
    if it does not exist. To implement this functionality, we’ll use the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: This will open a session to the feature store and allow it to be accessed in
    the Snowpark session. The next step will be to set up a feature entity on this
    feature store.
  prefs: []
  type: TYPE_NORMAL
- en: Creating feature entities
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Entities are fundamental elements linked with features and feature views, providing
    the cornerstone for feature lookups by defining join keys. Users can generate
    novel entities and formally register them within the feature store, thereby fostering
    connections and relationships between various features. This code creates an entity
    named `WEATHER` with an `ID` join key, registers it in the feature store (`fs`),
    and then displays a list of entities in the feature store:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This generates the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.18 – Feature entity](img/B19923_06_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.18 – Feature entity
  prefs: []
  type: TYPE_NORMAL
- en: The `ENTITY_WEATHER` entity has been created with the ID as the join key. The
    next step is to set up feature views.
  prefs: []
  type: TYPE_NORMAL
- en: Creating feature views
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Within a feature store, feature views act as comprehensive pipelines, systematically
    transforming raw data into interconnected features at regular intervals. These
    feature views are materialized from designated source tables, ensuring incremental
    and efficient updates as fresh data is introduced. In our previous chapter, we
    explored a dataset that comprised various weather-related features. To preprocess
    this data effectively, we employed a Snowpark pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'Through this pipeline, we performed transformations on the `SEASON` and `WEATHER`
    columns using one-hot encoding techniques. Additionally, we normalized the `TEMP`
    column to ensure consistency and facilitate model training. Given that we thoroughly
    discussed each step of this pipeline in our previous chapter, we’ll be revisiting
    it briefly, focusing more on a high-level overview rather than delving into detailed
    explanations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'This code block utilizes Snowflake’s ML capabilities for data preprocessing.
    It imports necessary modules such as preprocessing functions and the `Pipeline`
    class. The code creates a new `ID` column with unique identifiers for each row
    and drops unnecessary columns. It defines lists of categorical columns and their
    transformed versions after one-hot encoding, along with columns to be normalized.
    Additionally, it specifies categories for each categorical column, likely for
    encoding purposes, facilitating effective ML model processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: In the first step, an ordinal encoder (`OE`) is applied to transform categorical
    columns specified in the `CATEGORICAL_COLUMNS` list into their one-hot encoded
    versions, as defined by the `CATEGORICAL_COLUMNS_OHE` list. The `categories` parameter
    specifies the categories for each categorical column, likely used for encoding
    purposes.
  prefs: []
  type: TYPE_NORMAL
- en: In the second step, a min-max scaler (`MMS`) is used to normalize columns specified
    in the `MIN_MAX_COLUMNS` list. This scaler ensures that values in these columns
    are scaled to a specific range, typically between `0` and `1`, while preserving
    their relative proportions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The preprocessing pipeline is then applied to the `df` DataFrame using the
    fit-transform paradigm, where the pipeline is first fit to the data to learn parameters
    (for example, category mappings for ordinal encoding), and then applied to transform
    the DataFrame. The transformed DataFrame is then displayed using the `show()`
    method. Overall, this code prepares the DataFrame for further analysis or model
    training by preprocessing its columns using the specified pipeline. The resultant
    DataFrame is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.19 – Transformed DataFrame](img/B19923_06_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.19 – Transformed DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: 'Throughout the model-building process, various models are constructed using
    subsets of features, such as weather features and time-related features. Additionally,
    models are developed using combined data to ascertain superior performance. To
    expedite the model-building process and reduce data engineering overheads, we’ll
    organize weather-related features into a dedicated feature view. Subsequently,
    we’ll leverage this feature view to generate datasets and construct an XGBoost
    model in the ensuing section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The code selects specific columns from the DataFrame to create a feature DataFrame
    (`feature_df`). Then, it constructs a feature view named `WEATHER_FEATURES` associated
    with the previously defined entity and registers it in the feature store (`fs`)
    with version `V1`. The resulting DataFrame is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.20 – Feature DataFrame](img/B19923_06_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.20 – Feature DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: Once developed, features can be systematically stored in the feature store,
    fostering their availability for reuse or seamless sharing among various ML models
    and teams. This functionality significantly accelerates the creation of new ML
    models, eliminating the redundancy of building each feature from scratch. In the
    same way, we can create another feature view as rental features by combining similar
    features.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the dataset
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Once our feature pipelines are meticulously configured and ready, we can initiate
    their deployment to generate training data. Subsequently, these feature pipelines
    become instrumental in facilitating model prediction, marking the seamless transition
    from feature engineering to the practical application of ML models:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Creating training data becomes straightforward as materialized feature views
    inherently encompass crucial metadata such as join keys and timestamps for **point-in-time**
    (**PIT**) lookup:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.21 – Training data](img/B19923_06_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.21 – Training data
  prefs: []
  type: TYPE_NORMAL
- en: The process primarily involves supplying spine data—termed so because it serves
    as the foundational structure enriched by feature joins. In our case, spine data
    encompasses the feature to be predicted—`COUNT`—along with the join key column
    ID. Moreover, the flexibility to generate datasets with subsets of features within
    the feature view is available through slicing. Now that we have the training data
    ready, we will use it to train the model and predict the data output using the
    feature store.
  prefs: []
  type: TYPE_NORMAL
- en: The preparation of all data—both for training and operational use—requires meticulous
    handling through feature pipelines. These pipelines, resembling traditional data
    pipelines, aggregate, validate, and transform data output in a format suitable
    for input into the ML model. Properly orchestrated feature pipelines ensure that
    data is refined before being fed into the model, maintaining the integrity and
    relevance of features derived from the training process.
  prefs: []
  type: TYPE_NORMAL
- en: Model training
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We covered the model-building process extensively in the previous chapter,
    so in this section, we will focus on building it using the training dataset generated
    from feature views from the feature store. We are using a similar method outlined
    in the previous chapter in training a gradient boost model but just using feature
    views:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The elegance of Snowpark surfaces in this simplicity, as no significant modifications
    are needed to train a model using feature views seamlessly. We will create testing
    data to test the model for accuracy using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This creates a test DataFrame (`test_df`) by selecting the `ID` column from
    the first three rows of `spine_df`. Then, it retrieves and displays feature values
    for the test data frame using the feature store and training data generated from
    feature views:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.22 – Testing data](img/B19923_06_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.22 – Testing data
  prefs: []
  type: TYPE_NORMAL
- en: Now that the testing data is ready, we can predict the model using this data
    to get the results.
  prefs: []
  type: TYPE_NORMAL
- en: Model prediction
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this section, we will use the testing data generated from the feature store
    to make a prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The prediction displays the results with the predicted count value, showing
    the number of customers using shared bikes at the given hour:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.23 – Model prediction](img/B19923_06_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.23 – Model prediction
  prefs: []
  type: TYPE_NORMAL
- en: This shows how easy and improved it is to use a feature store to build a Snowpark
    ML model. In the next section, we will highlight some benefits of using feature
    stores.
  prefs: []
  type: TYPE_NORMAL
- en: When to utilize versus when to avoid feature stores
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Feature stores are particularly advantageous in ML processes when there’s a
    need for efficient feature management and reuse across multiple models or teams.
    They shine in the following scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature reuse**: Features need to be reused or shared between different ML
    models or teams, reducing redundant efforts in feature engineering'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Consistency and governance**: Ensuring consistent definitions, documentation,
    and governance of features across diverse ML projects or teams is critical'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model performance**: Maintaining peak model performance by ensuring consistency
    between feature definitions in training and inference pipelines, thus avoiding
    performance degradation due to discrepancies'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data collaboration**: Fostering collaboration between different teams or
    stakeholders involved in ML projects by offering a centralized platform for feature
    development, storage, modification, and sharing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: Handling large volumes of features and data efficiently, especially
    in environments where data is continuously evolving or being updated'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'However, feature stores may not be necessary in the following scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Simple models**: For simple models with few features and minimal complexity,
    the overhead of setting up and maintaining a feature store may outweigh its benefits'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Static data**: In cases where the data is relatively static and doesn’t require
    frequent updates or feature engineering, the need for a feature store may be limited'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Limited collaboration**: When ML projects involve a small, tightly-knit team
    working on isolated tasks without the need for extensive collaboration or feature
    sharing, the use of a feature store may be unnecessary'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resource constraints**: Organizations with limited resources or infrastructure
    may find it challenging to implement and maintain a feature store effectively'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In summary, while feature stores offer numerous benefits for efficient feature
    management in ML projects, their adoption should be carefully considered based
    on the specific needs and constraints of each project or organization.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter discussed the model registry and the importance of meaningful tags
    and descriptions, offering context and facilitating experiment tracking. We also
    highlighted different methods of operating with the model registry. We navigated
    through the capabilities of the Snowflake Feature Store within the Snowpark ML
    ecosystem and how to utilize it for managing Snowflake models.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about developing native applications using
    the Snowpark framework.
  prefs: []
  type: TYPE_NORMAL
