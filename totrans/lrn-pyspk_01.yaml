- en: Chapter 1. Understanding Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Spark is a powerful open source processing engine originally developed
    by Matei Zaharia as a part of his PhD thesis while at UC Berkeley. The first version
    of Spark was released in 2012\. Since then, in 2013, Zaharia co-founded and has
    become the CTO at Databricks; he also holds a professor position at Stanford,
    coming from MIT. At the same time, the Spark codebase was donated to the Apache
    Software Foundation and has become its flagship project.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark is fast, easy to use framework, that allows you to solve a wide
    variety of complex data problems whether semi-structured, structured, streaming,
    and/or machine learning / data sciences. It also has become one of the largest
    open source communities in big data with more than 1,000 contributors from 250+
    organizations and with 300,000+ Spark Meetup community members in more than 570+
    locations worldwide.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will provide a primer to understanding Apache Spark. We
    will explain the concepts behind Spark Jobs and APIs, introduce the Spark 2.0
    architecture, and explore the features of Spark 2.0.
  prefs: []
  type: TYPE_NORMAL
- en: 'The topics covered are:'
  prefs: []
  type: TYPE_NORMAL
- en: What is Apache Spark?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark Jobs and APIs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Review of Resilient Distributed Datasets (RDDs), DataFrames, and Datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Review of Catalyst Optimizer and Project Tungsten
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Review of the Spark 2.0 architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is Apache Spark?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Apache Spark is an open-source powerful distributed querying and processing
    engine. It provides flexibility and extensibility of MapReduce but at significantly
    higher speeds: Up to 100 times faster than Apache Hadoop when data is stored in
    memory and up to 10 times when accessing disk.'
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark allows the user to read, transform, and aggregate data, as well
    as train and deploy sophisticated statistical models with ease. The Spark APIs
    are accessible in Java, Scala, Python, R and SQL. Apache Spark can be used to
    build applications or package them up as libraries to be deployed on a cluster
    or perform *quick* analytics interactively through notebooks (like, for instance,
    Jupyter, Spark-Notebook, Databricks notebooks, and Apache Zeppelin).
  prefs: []
  type: TYPE_NORMAL
- en: 'Apache Spark exposes a host of libraries familiar to data analysts, data scientists
    or researchers who have worked with Python''s `pandas` or R''s `data.frames` or
    `data.tables`. It is important to note that while Spark DataFrames will be *familiar*
    to `pandas` or `data.frames` / `data.tables` users, there are some differences
    so please temper your expectations. Users with more of a SQL background can use
    the language to shape their data as well. Also, delivered with Apache Spark are
    several already implemented and tuned algorithms, statistical models, and frameworks:
    MLlib and ML for machine learning, GraphX and GraphFrames for graph processing,
    and Spark Streaming (DStreams and Structured). Spark allows the user to combine
    these libraries seamlessly in the same application.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Apache Spark can easily run locally on a laptop, yet can also easily be deployed
    in standalone mode, over YARN, or Apache Mesos - either on your local cluster
    or in the cloud. It can read and write from a diverse data sources including (but
    not limited to) HDFS, Apache Cassandra, Apache HBase, and S3:'
  prefs: []
  type: TYPE_NORMAL
- en: '![What is Apache Spark?](img/B05793_01_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: Apache Spark is the smartphone of Big Data [http://bit.ly/1QsgaNj](http://bit.ly/1QsgaNj)'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For more information, please refer to: Apache Spark is the Smartphone of Big
    Data at [http://bit.ly/1QsgaNj](http://bit.ly/1QsgaNj)'
  prefs: []
  type: TYPE_NORMAL
- en: Spark Jobs and APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will provide a short overview of the Apache Spark Jobs and
    APIs. This provides the necessary foundation for the subsequent section on Spark
    2.0 architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Execution process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Any Spark application spins off a single driver process (that can contain multiple
    jobs) on the *master* node that then directs executor processes (that contain
    multiple tasks) distributed to a number of *worker* nodes as noted in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Execution process](img/B05793_01_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The driver process determines the number and the composition of the task processes
    directed to the executor nodes based on the graph generated for the given job.
    Note, that any worker node can execute tasks from a number of different jobs.
  prefs: []
  type: TYPE_NORMAL
- en: 'A Spark job is associated with a chain of object dependencies organized in
    a direct acyclic graph (DAG) such as the following example generated from the
    Spark UI. Given this, Spark can optimize the scheduling (for example, determine
    the number of tasks and workers required) and execution of these tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Execution process](img/B05793_01_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For more information on the DAG scheduler, please refer to [http://bit.ly/29WTiK8](http://bit.ly/29WTiK8).
  prefs: []
  type: TYPE_NORMAL
- en: Resilient Distributed Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Apache Spark is built around a distributed collection of immutable Java Virtual
    Machine (JVM) objects called Resilient Distributed Datasets (RDDs for short).
    As we are working with Python, it is important to note that the Python data is
    stored within these JVM objects. More of this will be discussed in the subsequent
    chapters on RDDs and DataFrames. These objects allow any job to perform calculations
    very quickly. RDDs are calculated against, cached, and stored in-memory: a scheme
    that results in orders of magnitude faster computations compared to other traditional
    distributed frameworks like Apache Hadoop.'
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, RDDs expose some coarse-grained transformations (such as `map(...)`,
    `reduce(...)`, and `filter(...)` which we will cover in greater detail in Chapter
    2, *Resilient Distributed Datasets*), keeping the flexibility and extensibility
    of the Hadoop platform to perform a wide variety of calculations. RDDs apply and
    log transformations to the data in parallel, resulting in both increased speed
    and fault-tolerance. By registering the transformations, RDDs provide data lineage
    - a form of an ancestry tree for each intermediate step in the form of a graph.
    This, in effect, guards the RDDs against data loss - if a partition of an RDD
    is lost it still has enough information to recreate that partition instead of
    simply depending on replication.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you want to learn more about data lineage check this link [http://ibm.co/2ao9B1t](http://ibm.co/2ao9B1t)
    .
  prefs: []
  type: TYPE_NORMAL
- en: 'RDDs have two sets of parallel operations: *transformations* (which return
    pointers to new RDDs) and *actions* (which return values to the driver after running
    a computation); we will cover these in greater detail in later chapters.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For the latest list of transformations and actions, please refer to the Spark
    Programming Guide at [http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations](http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations).
  prefs: []
  type: TYPE_NORMAL
- en: 'RDD transformation operations are *lazy* in a sense that they do not compute
    their results immediately. The transformations are only computed when an action
    is executed and the results need to be returned to the driver. This delayed execution
    results in more fine-tuned queries: Queries that are optimized for performance.
    This optimization starts with Apache Spark''s DAGScheduler – the stage oriented
    scheduler that transforms using *stages* as seen in the preceding screenshot.
    By having separate RDD *transformations* and *actions*, the DAGScheduler can perform
    optimizations in the query including being able to avoid *shuffling*, the data
    (the most resource intensive task).'
  prefs: []
  type: TYPE_NORMAL
- en: For more information on the DAGScheduler and optimizations (specifically around
    narrow or wide dependencies), a great reference is the *Narrow vs. Wide Transformations*
    section in *High Performance Spark* in *[Chapter 5](ch05.html "Chapter 5. Introducing
    MLlib"), Effective Transformations* ([https://smile.amazon.com/High-Performance-Spark-Practices-Optimizing/dp/1491943203](https://smile.amazon.com/High-Performance-Spark-Practices-Optimizing/dp/1491943203)).
  prefs: []
  type: TYPE_NORMAL
- en: DataFrames
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DataFrames, like RDDs, are immutable collections of data distributed among the
    nodes in a cluster. However, unlike RDDs, in DataFrames data is organized into
    named columns.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you are familiar with Python's `pandas` or R `data.frames`, this is a similar
    concept.
  prefs: []
  type: TYPE_NORMAL
- en: DataFrames were designed to make large data sets processing even easier. They
    allow developers to formalize the structure of the data, allowing higher-level
    abstraction; in that sense DataFrames resemble tables from the relational database
    world. DataFrames provide a domain specific language API to manipulate the distributed
    data and make Spark accessible to a wider audience, beyond specialized data engineers.
  prefs: []
  type: TYPE_NORMAL
- en: One of the major benefits of DataFrames is that the Spark engine initially builds
    a logical execution plan and executes generated code based on a physical plan
    determined by a cost optimizer. Unlike RDDs that can be significantly slower on
    Python compared with Java or Scala, the introduction of DataFrames has brought
    performance parity across all the languages.
  prefs: []
  type: TYPE_NORMAL
- en: Datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Introduced in Spark 1.6, the goal of Spark Datasets is to provide an API that
    allows users to easily express transformations on domain objects, while also providing
    the performance and benefits of the robust Spark SQL execution engine. Unfortunately,
    at the time of writing this book Datasets are only available in Scala or Java.
    When they are available in PySpark we will cover them in future editions.
  prefs: []
  type: TYPE_NORMAL
- en: Catalyst Optimizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Spark SQL is one of the most technically involved components of Apache Spark
    as it powers both SQL queries and the DataFrame API. At the core of Spark SQL
    is the Catalyst Optimizer. The optimizer is based on functional programming constructs
    and was designed with two purposes in mind: To ease the addition of new optimization
    techniques and features to Spark SQL and to allow external developers to extend
    the optimizer (for example, adding data source specific rules, support for new
    data types, and so on):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Catalyst Optimizer](img/B05793_01_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For more information, check out *Deep Dive into Spark SQL''s Catalyst Optimizer*
    ([http://bit.ly/271I7Dk](http://bit.ly/271I7Dk)) and *Apache Spark DataFrames:
    Simple and Fast Analysis of Structured Data* ([http://bit.ly/29QbcOV](http://bit.ly/29QbcOV))'
  prefs: []
  type: TYPE_NORMAL
- en: Project Tungsten
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Tungsten is the codename for an umbrella project of Apache Spark's execution
    engine. The project focuses on improving the Spark algorithms so they use memory
    and CPU more efficiently, pushing the performance of modern hardware closer to
    its limits.
  prefs: []
  type: TYPE_NORMAL
- en: 'The efforts of this project focus, among others, on:'
  prefs: []
  type: TYPE_NORMAL
- en: Managing memory explicitly so the overhead of JVM's object model and garbage
    collection are eliminated
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing algorithms and data structures that exploit the memory hierarchy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating code in runtime so the applications can exploit modern compliers
    and optimize for CPUs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eliminating virtual function dispatches so that multiple CPU calls are reduced
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilizing low-level programming (for example, loading immediate data to CPU
    registers) speed up the memory access and optimizing Spark's engine to efficiently
    compile and execute simple loops
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For more information, please refer to
  prefs: []
  type: TYPE_NORMAL
- en: '*Project Tungsten: Bringing Apache Spark Closer to Bare Metal* ([https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html](https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html))'
  prefs: []
  type: TYPE_NORMAL
- en: '*Deep Dive into Project Tungsten: Bringing Spark Closer to Bare Metal* [SSE
    2015 Video and Slides] ([https://spark-summit.org/2015/events/deep-dive-into-project-tungsten-bringing-spark-closer-to-bare-metal/](https://spark-summit.org/2015/events/deep-dive-into-project-tungsten-bringing-spark-closer-to-bare-metal/))
    and'
  prefs: []
  type: TYPE_NORMAL
- en: '*Apache Spark as a Compiler: Joining a Billion Rows per Second on a Laptop*
    ([https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html](https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html))'
  prefs: []
  type: TYPE_NORMAL
- en: Spark 2.0 architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The introduction of Apache Spark 2.0 is the recent major release of the Apache
    Spark project based on the key learnings from the last two years of development
    of the platform:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Spark 2.0 architecture](img/B05793_01_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: Apache Spark 2.0: Faster, Easier, and Smarter [http://bit.ly/2ap7qd5](http://bit.ly/2ap7qd5)'
  prefs: []
  type: TYPE_NORMAL
- en: The three overriding themes of the Apache Spark 2.0 release surround performance
    enhancements (via Tungsten Phase 2), the introduction of structured streaming,
    and unifying Datasets and DataFrames. We will describe the Datasets as they are
    part of Spark 2.0 even though they are currently only available in Scala and Java.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Refer to the following presentations by key Spark committers for more information
    about Apache Spark 2.0:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Reynold Xin''s Apache Spark 2.0: Faster, Easier, and Smarter* webinar [http://bit.ly/2ap7qd5](http://bit.ly/2ap7qd5)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Michael Armbrust''s Structuring Spark: DataFrames, Datasets, and Streaming*
    [http://bit.ly/2ap7qd5](http://bit.ly/2ap7qd5)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Tathagata Das'' A Deep Dive into Spark Streaming* [http://bit.ly/2aHt1w0](http://bit.ly/2aHt1w0)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Joseph Bradley''s Apache Spark MLlib 2.0 Preview: Data Science and Production*
    [http://bit.ly/2aHrOVN](http://bit.ly/2aHrOVN)'
  prefs: []
  type: TYPE_NORMAL
- en: Unifying Datasets and DataFrames
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we stated out that Datasets (at the time of writing
    this book) are only available in Scala or Java. However, we are providing the
    following context to better understand the direction of Spark 2.0.
  prefs: []
  type: TYPE_NORMAL
- en: Datasets were introduced in 2015 as part of the Apache Spark 1.6 release. The
    goal for datasets was to provide a type-safe, programming interface. This allowed
    developers to work with semi-structured data (like JSON or key-value pairs) with
    compile time type safety (that is, production applications can be checked for
    errors before they run). Part of the reason why Python does not implement a Dataset
    API is because Python is not a type-safe language.
  prefs: []
  type: TYPE_NORMAL
- en: Just as important, the Datasets API contain high-level domain specific language
    operations such as `sum()`, `avg()`, `join()`, and `group()`. This latter trait
    means that you have the flexibility of traditional Spark RDDs but the code is
    also easier to express, read, and write. Similar to DataFrames, Datasets can take
    advantage of Spark's catalyst optimizer by exposing expressions and data fields
    to a query planner and making use of Tungsten's fast in-memory encoding.
  prefs: []
  type: TYPE_NORMAL
- en: 'The history of the Spark APIs is denoted in the following diagram noting the
    progression from RDD to DataFrame to Dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Unifying Datasets and DataFrames](img/B05793_01_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: From Webinar Apache Spark 1.5: What is the difference between a DataFrame
    and a RDD? [http://bit.ly/29JPJSA](http://bit.ly/29JPJSA)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The unification of the DataFrame and Dataset APIs has the potential of creating
    breaking changes to backwards compatibility. This was one of the main reasons
    Apache Spark 2.0 was a major release (as opposed to a 1.x minor release which
    would have minimized any breaking changes). As you can see from the following
    diagram, DataFrame and Dataset both belong to the new Dataset API introduced as
    part of Apache Spark 2.0:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Unifying Datasets and DataFrames](img/B05793_01_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: A Tale of Three Apache Spark APIs: RDDs, DataFrames, and Datasets [http://bit.ly/2accSNA](http://bit.ly/2accSNA)'
  prefs: []
  type: TYPE_NORMAL
- en: 'As noted previously, the Dataset API provides a type-safe, object-oriented
    programming interface. Datasets can take advantage of the Catalyst optimizer by
    exposing expressions and data fields to the query planner and Project Tungsten''s
    Fast In-memory encoding. But with DataFrame and Dataset now unified as part of
    Apache Spark 2.0, DataFrame is now an alias for the Dataset Untyped API. More
    specifically:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Introducing SparkSession
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the past, you would potentially work with `SparkConf`, `SparkContext`, `SQLContext`,
    and `HiveContext` to execute your various Spark queries for configuration, Spark
    context, SQL context, and Hive context respectively. The `SparkSession` is essentially
    the combination of these contexts including `StreamingContext`.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, instead of writing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'now you can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'or:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The `SparkSession` is now the entry point for reading data, working with metadata,
    configuring the session, and managing the cluster resources.
  prefs: []
  type: TYPE_NORMAL
- en: Tungsten phase 2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The fundamental observation of the computer hardware landscape when the project
    started was that, while there were improvements in *price per performance* in
    RAM memory, disk, and (to an extent) network interfaces, the *price per performance*
    advancements for CPUs were not the same. Though hardware manufacturers could put
    more cores in each socket (i.e. improve performance through parallelization),
    there were no significant improvements in the actual core speed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Project Tungsten was introduced in 2015 to make significant changes to the
    Spark engine with the focus on improving performance. The first phase of these
    improvements focused on the following facets:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Memory Management and Binary Processing**: Leveraging application semantics
    to manage memory explicitly and eliminate the overhead of the JVM object model
    and garbage collection'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cache-aware computation**: Algorithms and data structures to exploit memory
    hierarchy'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Code generation**: Using code generation to exploit modern compilers and
    CPUs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram is the updated Catalyst engine to denote the inclusion
    of Datasets. As you see at the right of the diagram (right of the Cost Model),
    **Code Generation** is used against the selected physical plans to generate the
    underlying RDDs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Tungsten phase 2](img/B05793_01_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: Structuring Spark: DataFrames, Datasets, and Streaming [http://bit.ly/2cJ508x](http://bit.ly/2cJ508x)'
  prefs: []
  type: TYPE_NORMAL
- en: 'As part of Tungsten Phase 2, there is the push into *whole-stage* code generation.
    That is, the Spark engine will now generate the byte code at compile time for
    the entire Spark stage instead of just for specific jobs or tasks. The primary
    facets surrounding these improvements include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**No virtual function dispatches**: This reduces multiple CPU calls that can
    have a profound impact on performance when dispatching billions of times'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Intermediate data in memory vs CPU registers**: Tungsten Phase 2 places intermediate
    data into CPU registers. This is an order of magnitude reduction in the number
    of cycles to obtain data from the CPU registers instead of from memory'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Loop unrolling and SIMD**: Optimize Apache Spark''s execution engine to take
    advantage of modern compilers and CPUs'' ability to efficiently compile and execute
    simple `for` loops (as opposed to complex function call graphs)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For a more in-depth review of Project Tungsten, please refer to:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Apache Spark Key Terms, Explained* [https://databricks.com/blog/2016/06/22/apache-spark-key-terms-explained.html](https://databricks.com/blog/2016/06/22/apache-spark-key-terms-explained.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Apache Spark as a Compiler: Joining a Billion Rows per Second on a Laptop*
    [https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html](https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Project Tungsten: Bringing Apache Spark Closer to Bare Metal* [https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html](https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Structured Streaming
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As quoted by Reynold Xin during Spark Summit East 2016:'
  prefs: []
  type: TYPE_NORMAL
- en: '"The simplest way to perform streaming analytics is not having to *reason*
    about streaming."'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This is the underlying foundation for building Structured Streaming. While streaming
    is powerful, one of the key issues is that streaming can be difficult to build
    and maintain. While companies such as Uber, Netflix, and Pinterest have Spark
    Streaming applications running in production, they also have dedicated teams to
    ensure the systems are highly available.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For a high-level overview of Spark Streaming, please review Spark Streaming:
    What Is It and Who''s Using It? [http://bit.ly/1Qb10f6](http://bit.ly/1Qb10f6)'
  prefs: []
  type: TYPE_NORMAL
- en: 'As implied previously, there are many things that can go wrong when operating
    Spark Streaming (and any streaming system for that matter) including (but not
    limited to) late events, partial outputs to the final data source, state recovery
    on failure, and/or distributed reads/writes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Structured Streaming](img/B05793_01_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: A Deep Dive into Structured Streaming [http://bit.ly/2aHt1w0](http://bit.ly/2aHt1w0)'
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, to simplify Spark Streaming, there is now a single API that addresses
    both batch and streaming within the Apache Spark 2.0 release. More succinctly,
    the high-level streaming API is now built on top of the Apache Spark SQL Engine.
    It runs the same queries as you would with Datasets/DataFrames providing you with
    all the performance and optimization benefits as well as benefits such as event
    time, windowing, sessions, sources, and sinks.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Altogether, Apache Spark 2.0 not only unified DataFrames and Datasets but also
    unified streaming, interactive, and batch queries. This opens a whole new set
    of use cases including the ability to aggregate data into a stream and then serving
    it using traditional JDBC/ODBC, to change queries at run time, and/or to build
    and apply ML models in for many scenario in a variety of latency use cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Continuous applications](img/B05793_01_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: Apache Spark Key Terms, Explained [https://databricks.com/blog/2016/06/22/apache-spark-key-terms-explained.html](https://databricks.com/blog/2016/06/22/apache-spark-key-terms-explained.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Together, you can now build end-to-end **continuous applications**, in which
    you can issue the same queries to batch processing as to real-time data, perform
    ETL, generate reports, update or track specific data in the stream.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For more information on continuous applications, please refer to Matei Zaharia''s
    blog post *Continuous Applications: Evolving Streaming in Apache Spark 2.0 - A
    foundation for end-to-end real-time applications* [http://bit.ly/2aJaSOr](http://bit.ly/2aJaSOr).'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we reviewed what is Apache Spark and provided a primer on Spark
    Jobs and APIs. We also provided a primer on Resilient Distributed Datasets (RDDs),
    DataFrames, and Datasets; we will dive further into RDDs and DataFrames in subsequent
    chapters. We also discussed how DataFrames can provide faster query performance
    in Apache Spark due to the Spark SQL Engine's Catalyst Optimizer and Project Tungsten.
    Finally, we also provided a high-level overview of the Spark 2.0 architecture
    including the Tungsten Phase 2, Structured Streaming, and Unifying DataFrames
    and Datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will cover one of the fundamental data structures in
    Spark: The Resilient Distributed Datasets, or RDDs. We will show you how to create
    and modify these schema-less data structures using transformers and actions so
    your journey with PySpark can begin.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we do that, however, please, check the link [http://www.tomdrabas.com/site/book](http://www.tomdrabas.com/site/book)
    for the Bonus Chapter 1 where we outline instructions on how to install Spark
    locally on your machine (unless you already have it installed). Here''s a direct
    link to the manual: [https://www.packtpub.com/sites/default/files/downloads/InstallingSpark.pdf](https://www.packtpub.com/sites/default/files/downloads/InstallingSpark.pdf).'
  prefs: []
  type: TYPE_NORMAL
