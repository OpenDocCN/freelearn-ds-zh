- en: Chapter 1. Understanding Spark
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章 理解Spark
- en: Apache Spark is a powerful open source processing engine originally developed
    by Matei Zaharia as a part of his PhD thesis while at UC Berkeley. The first version
    of Spark was released in 2012\. Since then, in 2013, Zaharia co-founded and has
    become the CTO at Databricks; he also holds a professor position at Stanford,
    coming from MIT. At the same time, the Spark codebase was donated to the Apache
    Software Foundation and has become its flagship project.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark是一个强大的开源处理引擎，最初由Matei Zaharia在加州大学伯克利分校攻读博士学位时作为其博士论文的一部分开发。Spark的第一个版本于2012年发布。从那时起，在2013年，Zaharia共同创立了Databricks，并成为其CTO；他还在斯坦福大学担任教授，此前毕业于麻省理工学院。同时，Spark代码库捐赠给了Apache软件基金会，并成为其旗舰项目。
- en: Apache Spark is fast, easy to use framework, that allows you to solve a wide
    variety of complex data problems whether semi-structured, structured, streaming,
    and/or machine learning / data sciences. It also has become one of the largest
    open source communities in big data with more than 1,000 contributors from 250+
    organizations and with 300,000+ Spark Meetup community members in more than 570+
    locations worldwide.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark是一个快速、易于使用的框架，允许您解决各种复杂的数据问题，无论是半结构化、结构化、流式处理，还是机器学习/数据科学。它已经成为大数据领域最大的开源社区之一，拥有来自250多个组织的1000多名贡献者，以及全球570多个地点的30万多名Spark
    Meetup社区成员。
- en: In this chapter, we will provide a primer to understanding Apache Spark. We
    will explain the concepts behind Spark Jobs and APIs, introduce the Spark 2.0
    architecture, and explore the features of Spark 2.0.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将提供了解Apache Spark的基础。我们将解释Spark作业和API背后的概念，介绍Spark 2.0架构，并探讨Spark 2.0的功能。
- en: 'The topics covered are:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 涵盖的主题包括：
- en: What is Apache Spark?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是Apache Spark？
- en: Spark Jobs and APIs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark作业和API
- en: Review of Resilient Distributed Datasets (RDDs), DataFrames, and Datasets
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 弹性分布式数据集（RDDs）、DataFrames和Dataset综述
- en: Review of Catalyst Optimizer and Project Tungsten
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 催化剂优化器和钨项目综述
- en: Review of the Spark 2.0 architecture
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 2.0架构综述
- en: What is Apache Spark?
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是Apache Spark？
- en: 'Apache Spark is an open-source powerful distributed querying and processing
    engine. It provides flexibility and extensibility of MapReduce but at significantly
    higher speeds: Up to 100 times faster than Apache Hadoop when data is stored in
    memory and up to 10 times when accessing disk.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark是一个开源的强大分布式查询和处理引擎。它提供了MapReduce的灵活性和可扩展性，但速度显著更高：当数据存储在内存中时，速度可高达Apache
    Hadoop的100倍，当访问磁盘时，速度可高达10倍。
- en: Apache Spark allows the user to read, transform, and aggregate data, as well
    as train and deploy sophisticated statistical models with ease. The Spark APIs
    are accessible in Java, Scala, Python, R and SQL. Apache Spark can be used to
    build applications or package them up as libraries to be deployed on a cluster
    or perform *quick* analytics interactively through notebooks (like, for instance,
    Jupyter, Spark-Notebook, Databricks notebooks, and Apache Zeppelin).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark允许用户轻松地读取、转换和聚合数据，以及训练和部署复杂的统计模型。Spark API在Java、Scala、Python、R和SQL中均可访问。Apache
    Spark可用于构建应用程序或将其打包成库以在集群上部署，或通过笔记本（例如，Jupyter、Spark-Notebook、Databricks笔记本和Apache
    Zeppelin）进行交互式地快速分析。
- en: 'Apache Spark exposes a host of libraries familiar to data analysts, data scientists
    or researchers who have worked with Python''s `pandas` or R''s `data.frames` or
    `data.tables`. It is important to note that while Spark DataFrames will be *familiar*
    to `pandas` or `data.frames` / `data.tables` users, there are some differences
    so please temper your expectations. Users with more of a SQL background can use
    the language to shape their data as well. Also, delivered with Apache Spark are
    several already implemented and tuned algorithms, statistical models, and frameworks:
    MLlib and ML for machine learning, GraphX and GraphFrames for graph processing,
    and Spark Streaming (DStreams and Structured). Spark allows the user to combine
    these libraries seamlessly in the same application.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark向数据分析师、数据科学家或研究人员暴露了大量的库，这些人员曾使用过Python的`pandas`或R的`data.frames`或`data.tables`。需要注意的是，尽管Spark
    DataFrames对`pandas`或`data.frames` / `data.tables`用户来说可能很熟悉，但它们之间仍有一些差异，所以请调整您的期望。具有更多SQL背景的用户也可以使用该语言来塑造他们的数据。此外，Apache
    Spark还提供了几个已实现和调优的算法、统计模型和框架：MLlib和ML用于机器学习，GraphX和GraphFrames用于图处理，以及Spark Streaming（DStreams和Structured）。Spark允许用户在同一个应用程序中无缝地组合这些库。
- en: 'Apache Spark can easily run locally on a laptop, yet can also easily be deployed
    in standalone mode, over YARN, or Apache Mesos - either on your local cluster
    or in the cloud. It can read and write from a diverse data sources including (but
    not limited to) HDFS, Apache Cassandra, Apache HBase, and S3:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 可以轻松地在笔记本电脑上本地运行，也可以轻松地以独立模式、在 YARN 或 Apache Mesos 上部署 - 要么在本地集群中，要么在云中。它可以读取和写入多种数据源，包括但不限于
    HDFS、Apache Cassandra、Apache HBase 和 S3：
- en: '![What is Apache Spark?](img/B05793_01_01.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![什么是 Apache Spark？](img/B05793_01_01.jpg)'
- en: 'Source: Apache Spark is the smartphone of Big Data [http://bit.ly/1QsgaNj](http://bit.ly/1QsgaNj)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：Apache Spark 是大数据的智能手机 [http://bit.ly/1QsgaNj](http://bit.ly/1QsgaNj)
- en: Note
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'For more information, please refer to: Apache Spark is the Smartphone of Big
    Data at [http://bit.ly/1QsgaNj](http://bit.ly/1QsgaNj)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如需更多信息，请参阅：Apache Spark 是大数据的智能手机 [http://bit.ly/1QsgaNj](http://bit.ly/1QsgaNj)
- en: Spark Jobs and APIs
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 作业和 API
- en: In this section, we will provide a short overview of the Apache Spark Jobs and
    APIs. This provides the necessary foundation for the subsequent section on Spark
    2.0 architecture.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将简要概述 Apache Spark 作业和 API。这为 Spark 2.0 架构的后续章节提供了必要的知识基础。
- en: Execution process
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 执行过程
- en: 'Any Spark application spins off a single driver process (that can contain multiple
    jobs) on the *master* node that then directs executor processes (that contain
    multiple tasks) distributed to a number of *worker* nodes as noted in the following
    diagram:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 任何 Spark 应用都会在 *master* 节点上启动一个单独的驱动进程（可能包含多个作业），然后根据以下图中所示，将执行进程（包含多个任务）分布到多个
    *worker* 节点上：
- en: '![Execution process](img/B05793_01_02.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![执行过程](img/B05793_01_02.jpg)'
- en: The driver process determines the number and the composition of the task processes
    directed to the executor nodes based on the graph generated for the given job.
    Note, that any worker node can execute tasks from a number of different jobs.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 驱动进程根据为给定作业生成的图确定要发送到执行节点的任务进程的数量和组成。请注意，任何工作节点都可以执行来自多个不同作业的任务。
- en: 'A Spark job is associated with a chain of object dependencies organized in
    a direct acyclic graph (DAG) such as the following example generated from the
    Spark UI. Given this, Spark can optimize the scheduling (for example, determine
    the number of tasks and workers required) and execution of these tasks:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 作业与一系列对象依赖关系相关联，这些依赖关系组织在一个直接无环图（DAG）中，如下例所示，该图由 Spark UI 生成。基于此，Spark
    可以优化这些任务的调度（例如，确定所需任务和工人的数量）和执行：
- en: '![Execution process](img/B05793_01_03.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![执行过程](img/B05793_01_03.jpg)'
- en: Note
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: For more information on the DAG scheduler, please refer to [http://bit.ly/29WTiK8](http://bit.ly/29WTiK8).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 DAG 调度器的更多信息，请参阅 [http://bit.ly/29WTiK8](http://bit.ly/29WTiK8)。
- en: Resilient Distributed Dataset
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 弹性分布式数据集
- en: 'Apache Spark is built around a distributed collection of immutable Java Virtual
    Machine (JVM) objects called Resilient Distributed Datasets (RDDs for short).
    As we are working with Python, it is important to note that the Python data is
    stored within these JVM objects. More of this will be discussed in the subsequent
    chapters on RDDs and DataFrames. These objects allow any job to perform calculations
    very quickly. RDDs are calculated against, cached, and stored in-memory: a scheme
    that results in orders of magnitude faster computations compared to other traditional
    distributed frameworks like Apache Hadoop.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 是围绕一组称为弹性分布式数据集（RDD，简称）的不可变 Java 虚拟机（JVM）对象构建的。由于我们使用 Python，因此重要的是要注意
    Python 数据存储在这些 JVM 对象中。更多关于 RDD 和 DataFrame 的内容将在后续章节中讨论。这些对象允许任何作业快速执行计算。RDD
    是在它们上计算、缓存和存储在内存中的：与 Apache Hadoop 等其他传统分布式框架相比，这种方案的计算速度要快得多。
- en: At the same time, RDDs expose some coarse-grained transformations (such as `map(...)`,
    `reduce(...)`, and `filter(...)` which we will cover in greater detail in Chapter
    2, *Resilient Distributed Datasets*), keeping the flexibility and extensibility
    of the Hadoop platform to perform a wide variety of calculations. RDDs apply and
    log transformations to the data in parallel, resulting in both increased speed
    and fault-tolerance. By registering the transformations, RDDs provide data lineage
    - a form of an ancestry tree for each intermediate step in the form of a graph.
    This, in effect, guards the RDDs against data loss - if a partition of an RDD
    is lost it still has enough information to recreate that partition instead of
    simply depending on replication.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，RDDs 提供了一些粗粒度的转换（如 `map(...)`, `reduce(...)`, 和 `filter(...)`，我们将在第2章 *Resilient
    Distributed Datasets* 中更详细地介绍），保持了 Hadoop 平台的灵活性和可扩展性，以执行各种计算。RDDs 并行应用和记录转换到数据中，从而提高了速度和容错性。通过注册转换，RDDs
    提供数据血缘——以图形形式表示的中间步骤的祖先树。这实际上保护了 RDDs 防止数据丢失——如果 RDD 的一个分区丢失，它仍然有足够的信息来重新创建该分区，而不是仅仅依赖于复制。
- en: Note
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: If you want to learn more about data lineage check this link [http://ibm.co/2ao9B1t](http://ibm.co/2ao9B1t)
    .
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于数据血缘的信息，请查看这个链接 [http://ibm.co/2ao9B1t](http://ibm.co/2ao9B1t)。
- en: 'RDDs have two sets of parallel operations: *transformations* (which return
    pointers to new RDDs) and *actions* (which return values to the driver after running
    a computation); we will cover these in greater detail in later chapters.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: RDDs 有两套并行操作：*转换*（返回指向新 RDD 的指针）和*动作*（在执行计算后返回值到驱动程序）；我们将在后面的章节中更详细地介绍这些内容。
- en: Note
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: For the latest list of transformations and actions, please refer to the Spark
    Programming Guide at [http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations](http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 关于最新的转换和动作列表，请参阅 Spark 编程指南 [http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations](http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations)。
- en: 'RDD transformation operations are *lazy* in a sense that they do not compute
    their results immediately. The transformations are only computed when an action
    is executed and the results need to be returned to the driver. This delayed execution
    results in more fine-tuned queries: Queries that are optimized for performance.
    This optimization starts with Apache Spark''s DAGScheduler – the stage oriented
    scheduler that transforms using *stages* as seen in the preceding screenshot.
    By having separate RDD *transformations* and *actions*, the DAGScheduler can perform
    optimizations in the query including being able to avoid *shuffling*, the data
    (the most resource intensive task).'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: RDD 转换操作在某种程度上是 *懒加载* 的，它们不会立即计算结果。转换只有在执行动作并且需要将结果返回给驱动程序时才会进行计算。这种延迟执行导致查询更加精细：针对性能优化的查询。这种优化从
    Apache Spark 的 DAGScheduler 开始——一个以 *阶段* 为导向的调度器，如前一张截图所示使用 *阶段* 进行转换。通过将 RDD
    的 *转换* 和 *动作* 分开，DAGScheduler 可以在查询中执行优化，包括能够避免 *shuffle*，即数据（最耗资源的任务）。
- en: For more information on the DAGScheduler and optimizations (specifically around
    narrow or wide dependencies), a great reference is the *Narrow vs. Wide Transformations*
    section in *High Performance Spark* in *[Chapter 5](ch05.html "Chapter 5. Introducing
    MLlib"), Effective Transformations* ([https://smile.amazon.com/High-Performance-Spark-Practices-Optimizing/dp/1491943203](https://smile.amazon.com/High-Performance-Spark-Practices-Optimizing/dp/1491943203)).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 DAGScheduler 和优化（特别是关于窄或宽依赖）的更多信息，一个很好的参考资料是 *High Performance Spark* 中的
    *Narrow vs. Wide Transformations* 部分，位于 *[第5章](ch05.html "第5章。介绍MLlib")，有效的转换*
    ([https://smile.amazon.com/High-Performance-Spark-Practices-Optimizing/dp/1491943203](https://smile.amazon.com/High-Performance-Spark-Practices-Optimizing/dp/1491943203))。
- en: DataFrames
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DataFrames
- en: DataFrames, like RDDs, are immutable collections of data distributed among the
    nodes in a cluster. However, unlike RDDs, in DataFrames data is organized into
    named columns.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrames，像 RDDs 一样，是在集群节点间分布的数据的不可变集合。然而，与 RDDs 不同，在 DataFrames 中，数据被组织成命名的列。
- en: Note
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: If you are familiar with Python's `pandas` or R `data.frames`, this is a similar
    concept.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你熟悉 Python 的 `pandas` 或 R 的 `data.frames`，这是一个类似的概念。
- en: DataFrames were designed to make large data sets processing even easier. They
    allow developers to formalize the structure of the data, allowing higher-level
    abstraction; in that sense DataFrames resemble tables from the relational database
    world. DataFrames provide a domain specific language API to manipulate the distributed
    data and make Spark accessible to a wider audience, beyond specialized data engineers.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame 被设计得使大数据集的处理更加容易。它们允许开发者形式化数据的结构，允许更高层次的抽象；从这个意义上讲，DataFrame 类似于关系数据库世界中的表。DataFrame
    提供了一个特定领域的语言 API 来操作分布式数据，并使 Spark 对更广泛的受众，而不仅仅是专业数据工程师，变得可访问。
- en: One of the major benefits of DataFrames is that the Spark engine initially builds
    a logical execution plan and executes generated code based on a physical plan
    determined by a cost optimizer. Unlike RDDs that can be significantly slower on
    Python compared with Java or Scala, the introduction of DataFrames has brought
    performance parity across all the languages.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame 的一大优点是 Spark 引擎最初构建一个逻辑执行计划，并根据成本优化器确定的物理计划执行生成的代码。与在 Python 中与 Java
    或 Scala 相比可能显著较慢的 RDD 不同，DataFrame 的引入使得所有语言都实现了性能上的对等。
- en: Datasets
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Datasets
- en: Introduced in Spark 1.6, the goal of Spark Datasets is to provide an API that
    allows users to easily express transformations on domain objects, while also providing
    the performance and benefits of the robust Spark SQL execution engine. Unfortunately,
    at the time of writing this book Datasets are only available in Scala or Java.
    When they are available in PySpark we will cover them in future editions.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 1.6 中引入的 Spark Datasets 的目标是提供一个 API，使用户能够轻松地表达对域对象的转换，同时提供强大 Spark SQL
    执行引擎的性能和好处。不幸的是，在撰写本书时，Datasets 只在 Scala 或 Java 中可用。当它们在 PySpark 中可用时，我们将在未来的版本中介绍它们。
- en: Catalyst Optimizer
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Catalyst Optimizer
- en: 'Spark SQL is one of the most technically involved components of Apache Spark
    as it powers both SQL queries and the DataFrame API. At the core of Spark SQL
    is the Catalyst Optimizer. The optimizer is based on functional programming constructs
    and was designed with two purposes in mind: To ease the addition of new optimization
    techniques and features to Spark SQL and to allow external developers to extend
    the optimizer (for example, adding data source specific rules, support for new
    data types, and so on):'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL 是 Apache Spark 中技术含量最高的组件之一，因为它既支持 SQL 查询，也支持 DataFrame API。Spark
    SQL 的核心是 Catalyst 优化器。该优化器基于函数式编程结构，并设计有两个目的：简化新优化技术和功能添加到 Spark SQL 中，并允许外部开发者扩展优化器（例如，添加特定数据源规则，支持新数据类型等）：
- en: '![Catalyst Optimizer](img/B05793_01_04.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![Catalyst Optimizer](img/B05793_01_04.jpg)'
- en: Note
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'For more information, check out *Deep Dive into Spark SQL''s Catalyst Optimizer*
    ([http://bit.ly/271I7Dk](http://bit.ly/271I7Dk)) and *Apache Spark DataFrames:
    Simple and Fast Analysis of Structured Data* ([http://bit.ly/29QbcOV](http://bit.ly/29QbcOV))'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 更多信息，请参阅 *Spark SQL 的 Catalyst 优化器深入解析* ([http://bit.ly/271I7Dk](http://bit.ly/271I7Dk))
    和 *Apache Spark DataFrame：结构化数据的简单快速分析* ([http://bit.ly/29QbcOV](http://bit.ly/29QbcOV))
- en: Project Tungsten
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 项目 Tungsten
- en: Tungsten is the codename for an umbrella project of Apache Spark's execution
    engine. The project focuses on improving the Spark algorithms so they use memory
    and CPU more efficiently, pushing the performance of modern hardware closer to
    its limits.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Tungsten 是 Apache Spark 执行引擎的一个总称项目代号。该项目专注于改进 Spark 算法，以便它们更有效地使用内存和 CPU，将现代硬件的性能推向极限。
- en: 'The efforts of this project focus, among others, on:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 该项目的努力主要集中在以下方面：
- en: Managing memory explicitly so the overhead of JVM's object model and garbage
    collection are eliminated
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 显式管理内存，以消除 JVM 对象模型和垃圾回收的开销
- en: Designing algorithms and data structures that exploit the memory hierarchy
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计算法和数据结构以利用内存层次结构
- en: Generating code in runtime so the applications can exploit modern compliers
    and optimize for CPUs
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在运行时生成代码，以便应用程序可以利用现代编译器并针对 CPU 进行优化
- en: Eliminating virtual function dispatches so that multiple CPU calls are reduced
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消除虚函数调度，以减少多个 CPU 调用
- en: Utilizing low-level programming (for example, loading immediate data to CPU
    registers) speed up the memory access and optimizing Spark's engine to efficiently
    compile and execute simple loops
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用低级编程（例如，将即时数据加载到 CPU 寄存器中）来加速内存访问，并优化 Spark 引擎以高效地编译和执行简单的循环
- en: Note
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: For more information, please refer to
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 更多信息，请参阅
- en: '*Project Tungsten: Bringing Apache Spark Closer to Bare Metal* ([https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html](https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html))'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*Project Tungsten：将 Apache Spark 带到裸金属更近一步* ([https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html](https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html))'
- en: '*Deep Dive into Project Tungsten: Bringing Spark Closer to Bare Metal* [SSE
    2015 Video and Slides] ([https://spark-summit.org/2015/events/deep-dive-into-project-tungsten-bringing-spark-closer-to-bare-metal/](https://spark-summit.org/2015/events/deep-dive-into-project-tungsten-bringing-spark-closer-to-bare-metal/))
    and'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*深入探讨 Project Tungsten：将 Spark 带到裸金属更近一步* [SSE 2015 视频 和 幻灯片] ([https://spark-summit.org/2015/events/deep-dive-into-project-tungsten-bringing-spark-closer-to-bare-metal/](https://spark-summit.org/2015/events/deep-dive-into-project-tungsten-bringing-spark-closer-to-bare-metal/))
    以及'
- en: '*Apache Spark as a Compiler: Joining a Billion Rows per Second on a Laptop*
    ([https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html](https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html))'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '*Apache Spark 作为编译器：在笔记本电脑上每秒连接十亿行数据* ([https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html](https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html))'
- en: Spark 2.0 architecture
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 2.0 架构
- en: 'The introduction of Apache Spark 2.0 is the recent major release of the Apache
    Spark project based on the key learnings from the last two years of development
    of the platform:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 2.0 的引入是 Apache Spark 项目基于过去两年平台开发关键经验教训的最新主要版本：
- en: '![Spark 2.0 architecture](img/B05793_01_05.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![Spark 2.0 架构](img/B05793_01_05.jpg)'
- en: 'Source: Apache Spark 2.0: Faster, Easier, and Smarter [http://bit.ly/2ap7qd5](http://bit.ly/2ap7qd5)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：Apache Spark 2.0：更快、更简单、更智能 [http://bit.ly/2ap7qd5](http://bit.ly/2ap7qd5)
- en: The three overriding themes of the Apache Spark 2.0 release surround performance
    enhancements (via Tungsten Phase 2), the introduction of structured streaming,
    and unifying Datasets and DataFrames. We will describe the Datasets as they are
    part of Spark 2.0 even though they are currently only available in Scala and Java.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 2.0 版本的三个主要主题围绕着性能提升（通过 Tungsten 第二阶段），引入结构化流，以及统一 Datasets 和 DataFrames。我们将描述
    Datasets，因为它们是 Spark 2.0 的一部分，尽管它们目前仅在 Scala 和 Java 中可用。
- en: Note
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'Refer to the following presentations by key Spark committers for more information
    about Apache Spark 2.0:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 参考以下由关键 Spark 委员会成员提供的演示，以获取有关 Apache Spark 2.0 的更多信息：
- en: '*Reynold Xin''s Apache Spark 2.0: Faster, Easier, and Smarter* webinar [http://bit.ly/2ap7qd5](http://bit.ly/2ap7qd5)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*Reynold Xin 的 Apache Spark 2.0：更快、更简单、更智能* 网络研讨会 [http://bit.ly/2ap7qd5](http://bit.ly/2ap7qd5)'
- en: '*Michael Armbrust''s Structuring Spark: DataFrames, Datasets, and Streaming*
    [http://bit.ly/2ap7qd5](http://bit.ly/2ap7qd5)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '*Michael Armbrust 的 Structuring Spark：DataFrames、Datasets 和 Streaming* [http://bit.ly/2ap7qd5](http://bit.ly/2ap7qd5)'
- en: '*Tathagata Das'' A Deep Dive into Spark Streaming* [http://bit.ly/2aHt1w0](http://bit.ly/2aHt1w0)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*Tathagata Das 的深入探讨 Spark Streaming* [http://bit.ly/2aHt1w0](http://bit.ly/2aHt1w0)'
- en: '*Joseph Bradley''s Apache Spark MLlib 2.0 Preview: Data Science and Production*
    [http://bit.ly/2aHrOVN](http://bit.ly/2aHrOVN)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*Joseph Bradley 的 Apache Spark MLlib 2.0 预览：数据科学和生产* [http://bit.ly/2aHrOVN](http://bit.ly/2aHrOVN)'
- en: Unifying Datasets and DataFrames
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 统一 Datasets 和 DataFrames
- en: In the previous section, we stated out that Datasets (at the time of writing
    this book) are only available in Scala or Java. However, we are providing the
    following context to better understand the direction of Spark 2.0.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们指出（在撰写本书时）Datasets 只在 Scala 或 Java 中可用。然而，我们提供以下背景信息以更好地理解 Spark 2.0
    的方向。
- en: Datasets were introduced in 2015 as part of the Apache Spark 1.6 release. The
    goal for datasets was to provide a type-safe, programming interface. This allowed
    developers to work with semi-structured data (like JSON or key-value pairs) with
    compile time type safety (that is, production applications can be checked for
    errors before they run). Part of the reason why Python does not implement a Dataset
    API is because Python is not a type-safe language.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Datasets 于 2015 年作为 Apache Spark 1.6 版本的一部分引入。Datasets 的目标是提供一个类型安全的编程接口。这允许开发者使用半结构化数据（如
    JSON 或键值对）进行编译时类型安全（即，生产应用程序在运行之前可以检查错误）。Python 不实现 Dataset API 的一部分原因是因为 Python
    不是一个类型安全语言。
- en: Just as important, the Datasets API contain high-level domain specific language
    operations such as `sum()`, `avg()`, `join()`, and `group()`. This latter trait
    means that you have the flexibility of traditional Spark RDDs but the code is
    also easier to express, read, and write. Similar to DataFrames, Datasets can take
    advantage of Spark's catalyst optimizer by exposing expressions and data fields
    to a query planner and making use of Tungsten's fast in-memory encoding.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 同样重要的是，Dataset API 包含高级领域特定语言操作，如 `sum()`、`avg()`、`join()` 和 `group()`。这一特性意味着您具有传统
    Spark RDDs 的灵活性，但代码也更容易表达、阅读和编写。类似于 DataFrames，Dataset 可以利用 Spark 的催化剂优化器，通过向查询计划器公开表达式和数据字段以及利用
    Tungsten 的快速内存编码。
- en: 'The history of the Spark APIs is denoted in the following diagram noting the
    progression from RDD to DataFrame to Dataset:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Spark API 的历史在以下图中表示，显示了从 RDD 到 DataFrame 到 Dataset 的演变：
- en: '![Unifying Datasets and DataFrames](img/B05793_01_06.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![统一数据集和数据框](img/B05793_01_06.jpg)'
- en: 'Source: From Webinar Apache Spark 1.5: What is the difference between a DataFrame
    and a RDD? [http://bit.ly/29JPJSA](http://bit.ly/29JPJSA)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：网络研讨会 Apache Spark 1.5：DataFrame 和 RDD 之间的区别是什么？[http://bit.ly/29JPJSA](http://bit.ly/29JPJSA)
- en: 'The unification of the DataFrame and Dataset APIs has the potential of creating
    breaking changes to backwards compatibility. This was one of the main reasons
    Apache Spark 2.0 was a major release (as opposed to a 1.x minor release which
    would have minimized any breaking changes). As you can see from the following
    diagram, DataFrame and Dataset both belong to the new Dataset API introduced as
    part of Apache Spark 2.0:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame 和 Dataset API 的统一可能会对向后兼容性造成破坏性变化。这是 Apache Spark 2.0 成为重大版本（而不是 1.x
    小版本，这将最小化任何破坏性变化）的主要原因之一。正如您可以从以下图中看到的那样，DataFrame 和 Dataset 都属于 Apache Spark
    2.0 作为一部分引入的新 Dataset API：
- en: '![Unifying Datasets and DataFrames](img/B05793_01_07.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![统一数据集和数据框](img/B05793_01_07.jpg)'
- en: 'Source: A Tale of Three Apache Spark APIs: RDDs, DataFrames, and Datasets [http://bit.ly/2accSNA](http://bit.ly/2accSNA)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：三个 Apache Spark API 的故事：RDD、DataFrames 和 Datasets [http://bit.ly/2accSNA](http://bit.ly/2accSNA)
- en: 'As noted previously, the Dataset API provides a type-safe, object-oriented
    programming interface. Datasets can take advantage of the Catalyst optimizer by
    exposing expressions and data fields to the query planner and Project Tungsten''s
    Fast In-memory encoding. But with DataFrame and Dataset now unified as part of
    Apache Spark 2.0, DataFrame is now an alias for the Dataset Untyped API. More
    specifically:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Dataset API 提供了一个类型安全的面向对象编程接口。Dataset 可以通过向查询计划器公开表达式和数据字段以及利用 Project
    Tungsten 的快速内存编码来利用催化剂优化器。但是，随着 DataFrame 和 Dataset 现在作为 Apache Spark 2.0 的一部分统一，DataFrame
    现在是 Dataset Untyped API 的别名。更具体地说：
- en: '[PRE0]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Introducing SparkSession
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍 SparkSession
- en: In the past, you would potentially work with `SparkConf`, `SparkContext`, `SQLContext`,
    and `HiveContext` to execute your various Spark queries for configuration, Spark
    context, SQL context, and Hive context respectively. The `SparkSession` is essentially
    the combination of these contexts including `StreamingContext`.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去，您可能会使用 `SparkConf`、`SparkContext`、`SQLContext` 和 `HiveContext` 分别执行各种 Spark
    查询以进行配置、Spark 上下文、SQL 上下文和 Hive 上下文。`SparkSession` 实质上是这些上下文的组合，包括 `StreamingContext`。
- en: 'For example, instead of writing:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，您不再需要编写：
- en: '[PRE1]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'now you can write:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以编写：
- en: '[PRE2]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'or:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 或者：
- en: '[PRE3]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The `SparkSession` is now the entry point for reading data, working with metadata,
    configuring the session, and managing the cluster resources.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '`SparkSession` 现在是读取数据、处理元数据、配置会话和管理集群资源的入口点。'
- en: Tungsten phase 2
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Tungsten 第二阶段
- en: The fundamental observation of the computer hardware landscape when the project
    started was that, while there were improvements in *price per performance* in
    RAM memory, disk, and (to an extent) network interfaces, the *price per performance*
    advancements for CPUs were not the same. Though hardware manufacturers could put
    more cores in each socket (i.e. improve performance through parallelization),
    there were no significant improvements in the actual core speed.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 当项目开始时，对计算机硬件景观的基本观察是，尽管在 RAM 内存、磁盘和（在一定程度上）网络接口中 *性能/价格比* 有所提高，但 CPU 的 *性能/价格比*
    进步并不相同。尽管硬件制造商可以在每个插槽中放入更多的核心（即通过并行化提高性能），但实际核心速度并没有显著提高。
- en: 'Project Tungsten was introduced in 2015 to make significant changes to the
    Spark engine with the focus on improving performance. The first phase of these
    improvements focused on the following facets:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Project Tungsten于2015年推出，旨在对Spark引擎进行重大改进，重点是提高性能。这些改进的第一阶段主要集中在以下方面：
- en: '**Memory Management and Binary Processing**: Leveraging application semantics
    to manage memory explicitly and eliminate the overhead of the JVM object model
    and garbage collection'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内存管理和二进制处理**：利用应用语义显式管理内存并消除JVM对象模型和垃圾回收的开销。'
- en: '**Cache-aware computation**: Algorithms and data structures to exploit memory
    hierarchy'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缓存感知计算**：算法和数据结构来利用内存层次结构。'
- en: '**Code generation**: Using code generation to exploit modern compilers and
    CPUs'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**代码生成**：利用代码生成来利用现代编译器和CPU。'
- en: 'The following diagram is the updated Catalyst engine to denote the inclusion
    of Datasets. As you see at the right of the diagram (right of the Cost Model),
    **Code Generation** is used against the selected physical plans to generate the
    underlying RDDs:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表是更新后的Catalyst引擎，表示包含Dataset。如图所示（图表右侧，成本模型右侧），**代码生成**用于针对选定的物理计划生成底层的RDD：
- en: '![Tungsten phase 2](img/B05793_01_08.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![钨磷2阶段](img/B05793_01_08.jpg)'
- en: 'Source: Structuring Spark: DataFrames, Datasets, and Streaming [http://bit.ly/2cJ508x](http://bit.ly/2cJ508x)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：结构化Spark：DataFrame、Dataset和流 [http://bit.ly/2cJ508x](http://bit.ly/2cJ508x)
- en: 'As part of Tungsten Phase 2, there is the push into *whole-stage* code generation.
    That is, the Spark engine will now generate the byte code at compile time for
    the entire Spark stage instead of just for specific jobs or tasks. The primary
    facets surrounding these improvements include:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 作为钨磷2阶段的一部分，现在正在推动**全阶段**代码生成。也就是说，Spark引擎现在将在编译时为整个Spark阶段生成字节码，而不是只为特定的作业或任务生成。
- en: '**No virtual function dispatches**: This reduces multiple CPU calls that can
    have a profound impact on performance when dispatching billions of times'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无虚拟函数调度**：这减少了在调度数十亿次时可能对性能产生深远影响的多次CPU调用。'
- en: '**Intermediate data in memory vs CPU registers**: Tungsten Phase 2 places intermediate
    data into CPU registers. This is an order of magnitude reduction in the number
    of cycles to obtain data from the CPU registers instead of from memory'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内存中的中间数据与CPU寄存器**：钨磷2阶段将中间数据放入CPU寄存器。这是从CPU寄存器而不是从内存中获取数据周期数减少了一个数量级。'
- en: '**Loop unrolling and SIMD**: Optimize Apache Spark''s execution engine to take
    advantage of modern compilers and CPUs'' ability to efficiently compile and execute
    simple `for` loops (as opposed to complex function call graphs)'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**循环展开和SIMD**：优化Apache Spark的执行引擎，以利用现代编译器和CPU高效编译和执行简单`for`循环的能力（与复杂的函数调用图相反）。'
- en: 'For a more in-depth review of Project Tungsten, please refer to:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 要深入了解Project Tungsten，请参阅：
- en: '*Apache Spark Key Terms, Explained* [https://databricks.com/blog/2016/06/22/apache-spark-key-terms-explained.html](https://databricks.com/blog/2016/06/22/apache-spark-key-terms-explained.html)'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Apache Spark关键术语解释* [https://databricks.com/blog/2016/06/22/apache-spark-key-terms-explained.html](https://databricks.com/blog/2016/06/22/apache-spark-key-terms-explained.html)'
- en: '*Apache Spark as a Compiler: Joining a Billion Rows per Second on a Laptop*
    [https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html](https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html)'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Apache Spark作为编译器：在笔记本电脑上每秒连接十亿行* [https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html](https://databricks.com/blog/2016/05/23/apache-spark-as-a-compiler-joining-a-billion-rows-per-second-on-a-laptop.html)'
- en: '*Project Tungsten: Bringing Apache Spark Closer to Bare Metal* [https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html](https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html)'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Project Tungsten：将Apache Spark带到裸金属更近一步* [https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html](https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html)'
- en: Structured Streaming
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结构化流
- en: 'As quoted by Reynold Xin during Spark Summit East 2016:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如Reynold Xin在2016年Spark Summit East上所说：
- en: '"The simplest way to perform streaming analytics is not having to *reason*
    about streaming."'
  id: totrans-116
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “执行流式分析最简单的方法就是无需对**流**进行推理。”
- en: This is the underlying foundation for building Structured Streaming. While streaming
    is powerful, one of the key issues is that streaming can be difficult to build
    and maintain. While companies such as Uber, Netflix, and Pinterest have Spark
    Streaming applications running in production, they also have dedicated teams to
    ensure the systems are highly available.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这是构建结构化流的基础。虽然流式处理功能强大，但其中一个关键问题是流式处理可能难以构建和维护。尽管像Uber、Netflix和Pinterest这样的公司已经在生产中运行Spark
    Streaming应用程序，但他们也有专门的团队来确保系统高度可用。
- en: Note
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'For a high-level overview of Spark Streaming, please review Spark Streaming:
    What Is It and Who''s Using It? [http://bit.ly/1Qb10f6](http://bit.ly/1Qb10f6)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 关于Spark Streaming的高级概述，请参阅Spark Streaming：它是什麼以及谁在使用它？[http://bit.ly/1Qb10f6](http://bit.ly/1Qb10f6)
- en: 'As implied previously, there are many things that can go wrong when operating
    Spark Streaming (and any streaming system for that matter) including (but not
    limited to) late events, partial outputs to the final data source, state recovery
    on failure, and/or distributed reads/writes:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在操作Spark Streaming（以及任何流式系统）时可能会出现许多问题，包括但不限于迟到的事件、部分输出到最终数据源、失败时的状态恢复以及/或分布式读写：
- en: '![Structured Streaming](img/B05793_01_09.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![结构化流](img/B05793_01_09.jpg)'
- en: 'Source: A Deep Dive into Structured Streaming [http://bit.ly/2aHt1w0](http://bit.ly/2aHt1w0)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：深入解析结构化流 [http://bit.ly/2aHt1w0](http://bit.ly/2aHt1w0)
- en: Therefore, to simplify Spark Streaming, there is now a single API that addresses
    both batch and streaming within the Apache Spark 2.0 release. More succinctly,
    the high-level streaming API is now built on top of the Apache Spark SQL Engine.
    It runs the same queries as you would with Datasets/DataFrames providing you with
    all the performance and optimization benefits as well as benefits such as event
    time, windowing, sessions, sources, and sinks.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了简化Spark Streaming，现在有一个单一的API解决了Apache Spark 2.0发布中的批量和流式处理。更简洁地说，高级流式处理API现在建立在Apache
    Spark SQL引擎之上。它运行与使用Datasets/DataFrames相同的查询，为您提供所有性能和优化优势，以及诸如事件时间、窗口、会话、源和汇等好处。
- en: Continuous applications
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 持续应用程序
- en: 'Altogether, Apache Spark 2.0 not only unified DataFrames and Datasets but also
    unified streaming, interactive, and batch queries. This opens a whole new set
    of use cases including the ability to aggregate data into a stream and then serving
    it using traditional JDBC/ODBC, to change queries at run time, and/or to build
    and apply ML models in for many scenario in a variety of latency use cases:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，Apache Spark 2.0不仅统一了DataFrame和Dataset，还统一了流式处理、交互式和批量查询。这开启了一系列全新的用例，包括将数据聚合到流中，然后使用传统的JDBC/ODBC进行服务，在运行时更改查询，以及/或在不同延迟用例中构建和应用ML模型：
- en: '![Continuous applications](img/B05793_01_10.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![持续应用程序](img/B05793_01_10.jpg)'
- en: 'Source: Apache Spark Key Terms, Explained [https://databricks.com/blog/2016/06/22/apache-spark-key-terms-explained.html](https://databricks.com/blog/2016/06/22/apache-spark-key-terms-explained.html).'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：Apache Spark关键术语解释 [https://databricks.com/blog/2016/06/22/apache-spark-key-terms-explained.html](https://databricks.com/blog/2016/06/22/apache-spark-key-terms-explained.html)。
- en: Together, you can now build end-to-end **continuous applications**, in which
    you can issue the same queries to batch processing as to real-time data, perform
    ETL, generate reports, update or track specific data in the stream.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以一起构建端到端的**持续应用程序**，在其中您可以向批量处理和实时数据发出相同的查询，执行ETL，生成报告，更新或跟踪流中的特定数据。
- en: Note
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'For more information on continuous applications, please refer to Matei Zaharia''s
    blog post *Continuous Applications: Evolving Streaming in Apache Spark 2.0 - A
    foundation for end-to-end real-time applications* [http://bit.ly/2aJaSOr](http://bit.ly/2aJaSOr).'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 关于持续应用程序的更多信息，请参阅Matei Zaharia的博客文章*持续应用程序：Apache Spark 2.0中流式处理的演变 - 端到端实时应用程序的基础*
    [http://bit.ly/2aJaSOr](http://bit.ly/2aJaSOr)。
- en: Summary
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we reviewed what is Apache Spark and provided a primer on Spark
    Jobs and APIs. We also provided a primer on Resilient Distributed Datasets (RDDs),
    DataFrames, and Datasets; we will dive further into RDDs and DataFrames in subsequent
    chapters. We also discussed how DataFrames can provide faster query performance
    in Apache Spark due to the Spark SQL Engine's Catalyst Optimizer and Project Tungsten.
    Finally, we also provided a high-level overview of the Spark 2.0 architecture
    including the Tungsten Phase 2, Structured Streaming, and Unifying DataFrames
    and Datasets.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们回顾了什么是Apache Spark，并提供了Spark作业和API的入门介绍。我们还提供了关于弹性分布式数据集（RDDs）、DataFrames和Dataset的入门介绍；我们将在后续章节中进一步探讨RDDs和DataFrames。我们还讨论了由于Spark
    SQL引擎的Catalyst优化器和Project Tungsten，DataFrames如何在Apache Spark中提供更快的查询性能。最后，我们还提供了Spark
    2.0架构的高级概述，包括Tungsten Phase 2、Structured Streaming以及统一DataFrames和Dataset。
- en: 'In the next chapter, we will cover one of the fundamental data structures in
    Spark: The Resilient Distributed Datasets, or RDDs. We will show you how to create
    and modify these schema-less data structures using transformers and actions so
    your journey with PySpark can begin.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍Spark中的一个基本数据结构：弹性分布式数据集，或RDDs。我们将向您展示如何使用转换器和操作来创建和修改这些无模式的数据库结构，以便您的PySpark之旅可以开始。
- en: 'Before we do that, however, please, check the link [http://www.tomdrabas.com/site/book](http://www.tomdrabas.com/site/book)
    for the Bonus Chapter 1 where we outline instructions on how to install Spark
    locally on your machine (unless you already have it installed). Here''s a direct
    link to the manual: [https://www.packtpub.com/sites/default/files/downloads/InstallingSpark.pdf](https://www.packtpub.com/sites/default/files/downloads/InstallingSpark.pdf).'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在此之前，然而，请检查链接[http://www.tomdrabas.com/site/book](http://www.tomdrabas.com/site/book)中的Bonus
    Chapter 1，其中概述了如何在您的机器上本地安装Spark的说明（除非您已经安装了它）。以下是直接链接到手册：[https://www.packtpub.com/sites/default/files/downloads/InstallingSpark.pdf](https://www.packtpub.com/sites/default/files/downloads/InstallingSpark.pdf)。
