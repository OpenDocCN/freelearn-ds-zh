["```py\nscala> // Define the case classes for using in conjunction with DataFrames \nscala> case class Trans(accNo: String, tranAmount: Double) \ndefined class Trans \nscala> // Functions to convert the sequence of strings to objects defined by the case classes \nscala> def toTrans =  (trans: Seq[String]) => Trans(trans(0), trans(1).trim.toDouble) \ntoTrans: Seq[String] => Trans \nscala> // Creation of the list from where the RDD is going to be created \nscala> val acTransList = Array(\"SB10001,1000\", \"SB10002,1200\", \"SB10003,8000\", \"SB10004,400\", \"SB10005,300\", \"SB10006,10000\", \"SB10007,500\", \"SB10008,56\", \"SB10009,30\",\"SB10010,7000\", \"CR10001,7000\", \"SB10002,-10\") \nacTransList: Array[String] = Array(SB10001,1000, SB10002,1200, SB10003,8000, SB10004,400, SB10005,300, SB10006,10000, SB10007,500, SB10008,56, SB10009,30, SB10010,7000, CR10001,7000, SB10002,-10) \nscala> // Create the RDD \nscala> val acTransRDD = sc.parallelize(acTransList).map(_.split(\",\")).map(toTrans(_)) \nacTransRDD: org.apache.spark.rdd.RDD[Trans] = MapPartitionsRDD[2] at map at <console>:30 \nscala> // Convert RDD to DataFrame \nscala> val acTransDF = spark.createDataFrame(acTransRDD) \nacTransDF: org.apache.spark.sql.DataFrame = [accNo: string, tranAmount: double] \nscala> // Register temporary view in the DataFrame for using it in SQL \nscala> acTransDF.createOrReplaceTempView(\"trans\") \nscala> // Print the structure of the DataFrame \nscala> acTransDF.printSchema \nroot \n |-- accNo: string (nullable = true) \n |-- tranAmount: double (nullable = false) \nscala> // Show the first few records of the DataFrame \nscala> acTransDF.show \n+-------+----------+ \n|  accNo|tranAmount| \n+-------+----------+ \n|SB10001|    1000.0| \n|SB10002|    1200.0| \n|SB10003|    8000.0| \n|SB10004|     400.0| \n|SB10005|     300.0| \n|SB10006|   10000.0| \n|SB10007|     500.0| \n|SB10008|      56.0| \n|SB10009|      30.0| \n|SB10010|    7000.0| \n|CR10001|    7000.0| \n|SB10002|     -10.0| \n+-------+----------+ \nscala> // Use SQL to create another DataFrame containing the good transaction records \nscala> val goodTransRecords = spark.sql(\"SELECT accNo, tranAmount FROM trans WHERE accNo like 'SB%' AND tranAmount > 0\") \ngoodTransRecords: org.apache.spark.sql.DataFrame = [accNo: string, tranAmount: double] \nscala> // Register temporary view in the DataFrame for using it in SQL \nscala> goodTransRecords.createOrReplaceTempView(\"goodtrans\") \nscala> // Show the first few records of the DataFrame \nscala> goodTransRecords.show \n+-------+----------+ \n|  accNo|tranAmount| \n+-------+----------+ \n|SB10001|    1000.0| \n|SB10002|    1200.0| \n|SB10003|    8000.0| \n|SB10004|     400.0| \n|SB10005|     300.0| \n|SB10006|   10000.0| \n|SB10007|     500.0| \n|SB10008|      56.0| \n|SB10009|      30.0| \n|SB10010|    7000.0| \n+-------+----------+ \nscala> // Use SQL to create another DataFrame containing the high value transaction records \nscala> val highValueTransRecords = spark.sql(\"SELECT accNo, tranAmount FROM goodtrans WHERE tranAmount > 1000\") \nhighValueTransRecords: org.apache.spark.sql.DataFrame = [accNo: string, tranAmount: double] \nscala> // Show the first few records of the DataFrame \nscala> highValueTransRecords.show \n+-------+----------+ \n|  accNo|tranAmount| \n+-------+----------+ \n|SB10002|    1200.0| \n|SB10003|    8000.0| \n|SB10006|   10000.0| \n|SB10010|    7000.0| \n+-------+----------+ \nscala> // Use SQL to create another DataFrame containing the bad account records \nscala> val badAccountRecords = spark.sql(\"SELECT accNo, tranAmount FROM trans WHERE accNo NOT like 'SB%'\") \nbadAccountRecords: org.apache.spark.sql.DataFrame = [accNo: string, tranAmount: double] \nscala> // Show the first few records of the DataFrame \nscala> badAccountRecords.show \n+-------+----------+ \n|  accNo|tranAmount| \n+-------+----------+ \n|CR10001|    7000.0| \n+-------+----------+ \nscala> // Use SQL to create another DataFrame containing the bad amount records \nscala> val badAmountRecords = spark.sql(\"SELECT accNo, tranAmount FROM trans WHERE tranAmount < 0\") \nbadAmountRecords: org.apache.spark.sql.DataFrame = [accNo: string, tranAmount: double] \nscala> // Show the first few records of the DataFrame \nscala> badAmountRecords.show \n+-------+----------+ \n|  accNo|tranAmount| \n+-------+----------+ \n|SB10002|     -10.0| \n+-------+----------+ \nscala> // Do the union of two DataFrames and create another DataFrame \nscala> val badTransRecords = badAccountRecords.union(badAmountRecords) \nbadTransRecords: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [accNo: string, tranAmount: double] \nscala> // Show the first few records of the DataFrame \nscala> badTransRecords.show \n+-------+----------+ \n|  accNo|tranAmount| \n+-------+----------+ \n|CR10001|    7000.0| \n|SB10002|     -10.0| \n+-------+----------+ \nscala> // Calculate the sum \nscala> val sumAmount = spark.sql(\"SELECT sum(tranAmount) as sum FROM goodtrans\") \nsumAmount: org.apache.spark.sql.DataFrame = [sum: double] \nscala> // Show the first few records of the DataFrame \nscala> sumAmount.show \n+-------+ \n|    sum| \n+-------+ \n|28486.0| \n+-------+ \nscala> // Calculate the maximum \nscala> val maxAmount = spark.sql(\"SELECT max(tranAmount) as max FROM goodtrans\") \nmaxAmount: org.apache.spark.sql.DataFrame = [max: double] \nscala> // Show the first few records of the DataFrame \nscala> maxAmount.show \n+-------+ \n|    max| \n+-------+ \n|10000.0| \n+-------+ \nscala> // Calculate the minimum \nscala> val minAmount = spark.sql(\"SELECT min(tranAmount) as min FROM goodtrans\") \nminAmount: org.apache.spark.sql.DataFrame = [min: double] \nscala> // Show the first few records of the DataFrame \nscala> minAmount.show \n+----+ \n| min| \n+----+ \n|30.0| \n+----+ \nscala> // Use SQL to create another DataFrame containing the good account numbers \nscala> val goodAccNos = spark.sql(\"SELECT DISTINCT accNo FROM trans WHERE accNo like 'SB%' ORDER BY accNo\") \ngoodAccNos: org.apache.spark.sql.DataFrame = [accNo: string] \nscala> // Show the first few records of the DataFrame \nscala> goodAccNos.show \n+-------+ \n|  accNo| \n+-------+ \n|SB10001| \n|SB10002| \n|SB10003| \n|SB10004| \n|SB10005| \n|SB10006| \n|SB10007| \n|SB10008| \n|SB10009| \n|SB10010| \n+-------+ \nscala> // Calculate the aggregates using mixing of DataFrame and RDD like operations \nscala> val sumAmountByMixing = goodTransRecords.map(trans => trans.getAs[Double](\"tranAmount\")).reduce(_ + _) \nsumAmountByMixing: Double = 28486.0 \nscala> val maxAmountByMixing = goodTransRecords.map(trans => trans.getAs[Double](\"tranAmount\")).reduce((a, b) => if (a > b) a else b) \nmaxAmountByMixing: Double = 10000.0 \nscala> val minAmountByMixing = goodTransRecords.map(trans => trans.getAs[Double](\"tranAmount\")).reduce((a, b) => if (a < b) a else b) \nminAmountByMixing: Double = 30.0 \n\n```", "```py\n>>> from pyspark.sql import Row \n>>> # Creation of the list from where the RDD is going to be created \n>>> acTransList = [\"SB10001,1000\", \"SB10002,1200\", \"SB10003,8000\", \"SB10004,400\", \"SB10005,300\", \"SB10006,10000\", \"SB10007,500\", \"SB10008,56\", \"SB10009,30\",\"SB10010,7000\", \"CR10001,7000\", \"SB10002,-10\"] \n>>> # Create the DataFrame \n>>> acTransDF = sc.parallelize(acTransList).map(lambda trans: trans.split(\",\")).map(lambda p: Row(accNo=p[0], tranAmount=float(p[1]))).toDF() \n>>> # Register temporary view in the DataFrame for using it in SQL \n>>> acTransDF.createOrReplaceTempView(\"trans\") \n>>> # Print the structure of the DataFrame \n>>> acTransDF.printSchema() \nroot \n |-- accNo: string (nullable = true) \n |-- tranAmount: double (nullable = true) \n>>> # Show the first few records of the DataFrame \n>>> acTransDF.show() \n+-------+----------+ \n|  accNo|tranAmount| \n+-------+----------+ \n|SB10001|    1000.0| \n|SB10002|    1200.0| \n|SB10003|    8000.0| \n|SB10004|     400.0| \n|SB10005|     300.0| \n|SB10006|   10000.0| \n|SB10007|     500.0| \n|SB10008|      56.0| \n|SB10009|      30.0| \n|SB10010|    7000.0| \n|CR10001|    7000.0| \n|SB10002|     -10.0| \n+-------+----------+ \n>>> # Use SQL to create another DataFrame containing the good transaction records \n>>> goodTransRecords = spark.sql(\"SELECT accNo, tranAmount FROM trans WHERE accNo like 'SB%' AND tranAmount > 0\") \n>>> # Register temporary table in the DataFrame for using it in SQL \n>>> goodTransRecords.createOrReplaceTempView(\"goodtrans\") \n>>> # Show the first few records of the DataFrame \n>>> goodTransRecords.show() \n+-------+----------+ \n|  accNo|tranAmount| \n+-------+----------+ \n|SB10001|    1000.0| \n|SB10002|    1200.0| \n|SB10003|    8000.0| \n|SB10004|     400.0| \n|SB10005|     300.0| \n|SB10006|   10000.0| \n|SB10007|     500.0| \n|SB10008|      56.0| \n|SB10009|      30.0| \n|SB10010|    7000.0| \n+-------+----------+ \n>>> # Use SQL to create another DataFrame containing the high value transaction records \n>>> highValueTransRecords = spark.sql(\"SELECT accNo, tranAmount FROM goodtrans WHERE tranAmount > 1000\") \n>>> # Show the first few records of the DataFrame \n>>> highValueTransRecords.show() \n+-------+----------+ \n|  accNo|tranAmount| \n+-------+----------+ \n|SB10002|    1200.0| \n|SB10003|    8000.0| \n|SB10006|   10000.0| \n|SB10010|    7000.0| \n+-------+----------+ \n>>> # Use SQL to create another DataFrame containing the bad account records \n>>> badAccountRecords = spark.sql(\"SELECT accNo, tranAmount FROM trans WHERE accNo NOT like 'SB%'\") \n>>> # Show the first few records of the DataFrame \n>>> badAccountRecords.show() \n+-------+----------+ \n|  accNo|tranAmount| \n+-------+----------+ \n|CR10001|    7000.0| \n+-------+----------+ \n>>> # Use SQL to create another DataFrame containing the bad amount records \n>>> badAmountRecords = spark.sql(\"SELECT accNo, tranAmount FROM trans WHERE tranAmount < 0\") \n>>> # Show the first few records of the DataFrame \n>>> badAmountRecords.show() \n+-------+----------+ \n|  accNo|tranAmount| \n+-------+----------+ \n|SB10002|     -10.0| \n+-------+----------+ \n>>> # Do the union of two DataFrames and create another DataFrame \n>>> badTransRecords = badAccountRecords.union(badAmountRecords) \n>>> # Show the first few records of the DataFrame \n>>> badTransRecords.show() \n+-------+----------+ \n|  accNo|tranAmount| \n+-------+----------+ \n|CR10001|    7000.0| \n|SB10002|     -10.0| \n+-------+----------+ \n>>> # Calculate the sum \n>>> sumAmount = spark.sql(\"SELECT sum(tranAmount)as sum FROM goodtrans\") \n>>> # Show the first few records of the DataFrame \n>>> sumAmount.show() \n+-------+ \n|    sum| \n+-------+ \n|28486.0| \n+-------+ \n>>> # Calculate the maximum \n>>> maxAmount = spark.sql(\"SELECT max(tranAmount) as max FROM goodtrans\") \n>>> # Show the first few records of the DataFrame \n>>> maxAmount.show() \n+-------+ \n|    max| \n+-------+ \n|10000.0| \n+-------+ \n>>> # Calculate the minimum \n>>> minAmount = spark.sql(\"SELECT min(tranAmount)as min FROM goodtrans\") \n>>> # Show the first few records of the DataFrame \n>>> minAmount.show() \n+----+ \n| min| \n+----+ \n|30.0| \n+----+ \n>>> # Use SQL to create another DataFrame containing the good account numbers \n>>> goodAccNos = spark.sql(\"SELECT DISTINCT accNo FROM trans WHERE accNo like 'SB%' ORDER BY accNo\") \n>>> # Show the first few records of the DataFrame \n>>> goodAccNos.show() \n+-------+ \n|  accNo| \n+-------+ \n|SB10001| \n|SB10002| \n|SB10003| \n|SB10004| \n|SB10005| \n|SB10006| \n|SB10007| \n|SB10008| \n|SB10009| \n|SB10010| \n+-------+ \n>>> # Calculate the sum using mixing of DataFrame and RDD like operations \n>>> sumAmountByMixing = goodTransRecords.rdd.map(lambda trans: trans.tranAmount).reduce(lambda a,b : a+b) \n>>> sumAmountByMixing \n28486.0 \n>>> # Calculate the maximum using mixing of DataFrame and RDD like operations \n>>> maxAmountByMixing = goodTransRecords.rdd.map(lambda trans: trans.tranAmount).reduce(lambda a,b : a if a > b else b) \n>>> maxAmountByMixing \n10000.0 \n>>> # Calculate the minimum using mixing of DataFrame and RDD like operations \n>>> minAmountByMixing = goodTransRecords.rdd.map(lambda trans: trans.tranAmount).reduce(lambda a,b : a if a < b else b) \n>>> minAmountByMixing \n30.0 \n\n```", "```py\nscala> acTransDF.show \n+-------+----------+ \n|  accNo|tranAmount| \n+-------+----------+ \n|SB10001|    1000.0| \n|SB10002|    1200.0| \n|SB10003|    8000.0| \n|SB10004|     400.0| \n|SB10005|     300.0| \n|SB10006|   10000.0| \n|SB10007|     500.0| \n|SB10008|      56.0| \n|SB10009|      30.0| \n|SB10010|    7000.0| \n|CR10001|    7000.0| \n|SB10002|     -10.0| \n+-------+----------+ \nscala> // Create the DataFrame using API for the good transaction records \nscala> val goodTransRecords = acTransDF.filter(\"accNo like 'SB%'\").filter(\"tranAmount > 0\") \ngoodTransRecords: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [accNo: string, tranAmount: double] \nscala> // Show the first few records of the DataFrame \nscala> goodTransRecords.show \n+-------+----------+ \n|  accNo|tranAmount| \n+-------+----------+ \n|SB10001|    1000.0| \n|SB10002|    1200.0| \n|SB10003|    8000.0| \n|SB10004|     400.0| \n|SB10005|     300.0| \n|SB10006|   10000.0| \n|SB10007|     500.0| \n|SB10008|      56.0| \n|SB10009|      30.0| \n|SB10010|    7000.0| \n+-------+----------+ \nscala> // Create the DataFrame using API for the high value transaction records \nscala> val highValueTransRecords = goodTransRecords.filter(\"tranAmount > 1000\") \nhighValueTransRecords: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [accNo: string, tranAmount: double] \nscala> // Show the first few records of the DataFrame \nscala> highValueTransRecords.show \n+-------+----------+ \n|  accNo|tranAmount| \n+-------+----------+ \n|SB10002|    1200.0| \n|SB10003|    8000.0| \n|SB10006|   10000.0| \n|SB10010|    7000.0| \n+-------+----------+ \nscala> // Create the DataFrame using API for the bad account records \nscala> val badAccountRecords = acTransDF.filter(\"accNo NOT like 'SB%'\") \nbadAccountRecords: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [accNo: string, tranAmount: double] \nscala> // Show the first few records of the DataFrame \nscala> badAccountRecords.show \n+-------+----------+ \n|  accNo|tranAmount| \n+-------+----------+ \n|CR10001|    7000.0| \n+-------+----------+ \nscala> // Create the DataFrame using API for the bad amount records \nscala> val badAmountRecords = acTransDF.filter(\"tranAmount < 0\") \nbadAmountRecords: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [accNo: string, tranAmount: double] \nscala> // Show the first few records of the DataFrame \nscala> badAmountRecords.show \n+-------+----------+ \n|  accNo|tranAmount| \n+-------+----------+ \n|SB10002|     -10.0| \n+-------+----------+ \nscala> // Do the union of two DataFrames \nscala> val badTransRecords = badAccountRecords.union(badAmountRecords) \nbadTransRecords: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [accNo: string, tranAmount: double] \nscala> // Show the first few records of the DataFrame \nscala> badTransRecords.show \n+-------+----------+ \n|  accNo|tranAmount| \n+-------+----------+ \n|CR10001|    7000.0| \n|SB10002|     -10.0| \n+-------+----------+ \nscala> // Calculate the aggregates in one shot \nscala> val aggregates = goodTransRecords.agg(sum(\"tranAmount\"), max(\"tranAmount\"), min(\"tranAmount\")) \naggregates: org.apache.spark.sql.DataFrame = [sum(tranAmount): double, max(tranAmount): double ... 1 more field] \nscala> // Show the first few records of the DataFrame \nscala> aggregates.show \n+---------------+---------------+---------------+ \n|sum(tranAmount)|max(tranAmount)|min(tranAmount)| \n+---------------+---------------+---------------+ \n|        28486.0|        10000.0|           30.0| \n+---------------+---------------+---------------+ \nscala> // Use DataFrame using API for creating the good account numbers \nscala> val goodAccNos = acTransDF.filter(\"accNo like 'SB%'\").select(\"accNo\").distinct().orderBy(\"accNo\") \ngoodAccNos: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [accNo: string] \nscala> // Show the first few records of the DataFrame \nscala> goodAccNos.show \n+-------+ \n|  accNo| \n+-------+ \n|SB10001| \n|SB10002| \n|SB10003| \n|SB10004| \n|SB10005| \n|SB10006| \n|SB10007| \n|SB10008| \n|SB10009| \n|SB10010| \n+-------+ \nscala> // Persist the data of the DataFrame into a Parquet file \nscala> acTransDF.write.parquet(\"scala.trans.parquet\") \nscala> // Read the data into a DataFrame from the Parquet file \nscala> val acTransDFfromParquet = spark.read.parquet(\"scala.trans.parquet\") \nacTransDFfromParquet: org.apache.spark.sql.DataFrame = [accNo: string, tranAmount: double] \nscala> // Show the first few records of the DataFrame \nscala> acTransDFfromParquet.show \n+-------+----------+ \n|  accNo|tranAmount| \n+-------+----------+ \n|SB10002|    1200.0| \n|SB10003|    8000.0| \n|SB10005|     300.0| \n|SB10006|   10000.0| \n|SB10008|      56.0| \n|SB10009|      30.0| \n|CR10001|    7000.0| \n|SB10002|     -10.0| \n|SB10001|    1000.0| \n|SB10004|     400.0| \n|SB10007|     500.0| \n|SB10010|    7000.0| \n+-------+----------+\n\n```", "```py\n>>> acTransDF.show() \n+-------+----------+ \n|  accNo|tranAmount| \n+-------+----------+ \n|SB10001|    1000.0| \n|SB10002|    1200.0| \n|SB10003|    8000.0| \n|SB10004|     400.0| \n|SB10005|     300.0| \n|SB10006|   10000.0| \n|SB10007|     500.0| \n|SB10008|      56.0| \n|SB10009|      30.0| \n|SB10010|    7000.0| \n|CR10001|    7000.0| \n|SB10002|     -10.0| \n+-------+----------+ \n>>> # Print the structure of the DataFrame \n>>> acTransDF.printSchema() \nroot \n |-- accNo: string (nullable = true) \n |-- tranAmount: double (nullable = true) \n>>> # Create the DataFrame using API for the good transaction records \n>>> goodTransRecords = acTransDF.filter(\"accNo like 'SB%'\").filter(\"tranAmount > 0\") \n>>> # Show the first few records of the DataFrame \n>>> goodTransRecords.show() \n+-------+----------+ \n|  accNo|tranAmount| \n+-------+----------+ \n|SB10001|    1000.0| \n|SB10002|    1200.0| \n|SB10003|    8000.0| \n|SB10004|     400.0| \n|SB10005|     300.0| \n|SB10006|   10000.0| \n|SB10007|     500.0| \n|SB10008|      56.0| \n|SB10009|      30.0| \n|SB10010|    7000.0| \n+-------+----------+ \n>>> # Create the DataFrame using API for the high value transaction records \n>>> highValueTransRecords = goodTransRecords.filter(\"tranAmount > 1000\") \n>>> # Show the first few records of the DataFrame \n>>> highValueTransRecords.show() \n+-------+----------+ \n|  accNo|tranAmount| \n+-------+----------+ \n|SB10002|    1200.0| \n|SB10003|    8000.0| \n|SB10006|   10000.0| \n|SB10010|    7000.0| \n+-------+----------+ \n>>> # Create the DataFrame using API for the bad account records \n>>> badAccountRecords = acTransDF.filter(\"accNo NOT like 'SB%'\") \n>>> # Show the first few records of the DataFrame \n>>> badAccountRecords.show() \n+-------+----------+ \n|  accNo|tranAmount| \n+-------+----------+ \n|CR10001|    7000.0| \n+-------+----------+ \n>>> # Create the DataFrame using API for the bad amount records \n>>> badAmountRecords = acTransDF.filter(\"tranAmount < 0\") \n>>> # Show the first few records of the DataFrame \n>>> badAmountRecords.show() \n+-------+----------+ \n|  accNo|tranAmount| \n+-------+----------+ \n|SB10002|     -10.0| \n+-------+----------+ \n>>> # Do the union of two DataFrames and create another DataFrame \n>>> badTransRecords = badAccountRecords.union(badAmountRecords) \n>>> # Show the first few records of the DataFrame \n>>> badTransRecords.show() \n+-------+----------+ \n|  accNo|tranAmount| \n+-------+----------+ \n|CR10001|    7000.0| \n|SB10002|     -10.0| \n+-------+----------+ \n>>> # Calculate the sum \n>>> sumAmount = goodTransRecords.agg({\"tranAmount\": \"sum\"}) \n>>> # Show the first few records of the DataFrame \n>>> sumAmount.show() \n+---------------+ \n|sum(tranAmount)| \n+---------------+ \n|        28486.0| \n+---------------+ \n>>> # Calculate the maximum \n>>> maxAmount = goodTransRecords.agg({\"tranAmount\": \"max\"}) \n>>> # Show the first few records of the DataFrame \n>>> maxAmount.show() \n+---------------+ \n|max(tranAmount)| \n+---------------+ \n|        10000.0| \n+---------------+ \n>>> # Calculate the minimum \n>>> minAmount = goodTransRecords.agg({\"tranAmount\": \"min\"}) \n>>> # Show the first few records of the DataFrame \n>>> minAmount.show() \n+---------------+ \n|min(tranAmount)| \n+---------------+ \n|           30.0| \n+---------------+ \n>>> # Create the DataFrame using API for the good account numbers \n>>> goodAccNos = acTransDF.filter(\"accNo like 'SB%'\").select(\"accNo\").distinct().orderBy(\"accNo\") \n>>> # Show the first few records of the DataFrame \n>>> goodAccNos.show() \n+-------+ \n|  accNo| \n+-------+ \n|SB10001| \n|SB10002| \n|SB10003| \n|SB10004| \n|SB10005| \n|SB10006| \n|SB10007| \n|SB10008| \n|SB10009| \n|SB10010| \n+-------+ \n>>> # Persist the data of the DataFrame into a Parquet file \n>>> acTransDF.write.parquet(\"python.trans.parquet\") \n>>> # Read the data into a DataFrame from the Parquet file \n>>> acTransDFfromParquet = spark.read.parquet(\"python.trans.parquet\") \n>>> # Show the first few records of the DataFrame \n>>> acTransDFfromParquet.show() \n+-------+----------+ \n|  accNo|tranAmount| \n+-------+----------+ \n|SB10002|    1200.0| \n|SB10003|    8000.0| \n|SB10005|     300.0| \n|SB10006|   10000.0| \n|SB10008|      56.0| \n|SB10009|      30.0| \n|CR10001|    7000.0| \n|SB10002|     -10.0| \n|SB10001|    1000.0| \n|SB10004|     400.0| \n|SB10007|     500.0| \n|SB10010|    7000.0| \n+-------+----------+ \n\n```", "```py\nscala> // Define the case classes for using in conjunction with DataFrames \nscala> case class Trans(accNo: String, tranAmount: Double) \ndefined class Trans \nscala> // Functions to convert the sequence of strings to objects defined by the case classes \nscala> def toTrans =  (trans: Seq[String]) => Trans(trans(0), trans(1).trim.toDouble) \ntoTrans: Seq[String] => Trans \nscala> // Creation of the list from where the RDD is going to be created \nscala> val acTransList = Array(\"SB10001,1000\", \"SB10002,1200\",\"SB10001,8000\", \"SB10002,400\", \"SB10003,300\", \"SB10001,10000\",\"SB10004,500\",\"SB10005,56\", \"SB10003,30\",\"SB10002,7000\",\"SB10001,-100\", \"SB10002,-10\") \nacTransList: Array[String] = Array(SB10001,1000, SB10002,1200, SB10001,8000, SB10002,400, SB10003,300, SB10001,10000, SB10004,500, SB10005,56, SB10003,30, SB10002,7000, SB10001,-100, SB10002,-10) \nscala> // Create the DataFrame \nscala> val acTransDF = sc.parallelize(acTransList).map(_.split(\",\")).map(toTrans(_)).toDF() \nacTransDF: org.apache.spark.sql.DataFrame = [accNo: string, tranAmount: double] \nscala> // Show the first few records of the DataFrame \nscala> acTransDF.show \n+-------+----------+ \n|  accNo|tranAmount| \n+-------+----------+ \n|SB10001|    1000.0| \n|SB10002|    1200.0| \n|SB10001|    8000.0| \n|SB10002|     400.0| \n|SB10003|     300.0| \n|SB10001|   10000.0| \n|SB10004|     500.0| \n|SB10005|      56.0| \n|SB10003|      30.0| \n|SB10002|    7000.0| \n|SB10001|    -100.0| \n|SB10002|     -10.0| \n+-------+----------+ \nscala> // Register temporary view in the DataFrame for using it in SQL \nscala> acTransDF.createOrReplaceTempView(\"trans\") \nscala> // Use SQL to create another DataFrame containing the account summary records \nscala> val acSummary = spark.sql(\"SELECT accNo, sum(tranAmount) as TransTotal FROM trans GROUP BY accNo\") \nacSummary: org.apache.spark.sql.DataFrame = [accNo: string, TransTotal: double] \nscala> // Show the first few records of the DataFrame \nscala> acSummary.show \n+-------+----------+ \n|  accNo|TransTotal| \n+-------+----------+ \n|SB10005|      56.0| \n|SB10004|     500.0| \n|SB10003|     330.0| \n|SB10002|    8590.0| \n|SB10001|   18900.0| \n+-------+----------+ \nscala> // Create the DataFrame using API for the account summary records \nscala> val acSummaryViaDFAPI = acTransDF.groupBy(\"accNo\").agg(sum(\"tranAmount\") as \"TransTotal\") \nacSummaryViaDFAPI: org.apache.spark.sql.DataFrame = [accNo: string, TransTotal: double] \nscala> // Show the first few records of the DataFrame \nscala> acSummaryViaDFAPI.show \n+-------+----------+ \n|  accNo|TransTotal| \n+-------+----------+ \n|SB10005|      56.0| \n|SB10004|     500.0| \n|SB10003|     330.0| \n|SB10002|    8590.0| \n|SB10001|   18900.0| \n+-------+----------+\n\n```", "```py\n>>> from pyspark.sql import Row \n>>> # Creation of the list from where the RDD is going to be created \n>>> acTransList = [\"SB10001,1000\", \"SB10002,1200\", \"SB10001,8000\",\"SB10002,400\", \"SB10003,300\", \"SB10001,10000\",\"SB10004,500\",\"SB10005,56\",\"SB10003,30\",\"SB10002,7000\", \"SB10001,-100\",\"SB10002,-10\"] \n>>> # Create the DataFrame \n>>> acTransDF = sc.parallelize(acTransList).map(lambda trans: trans.split(\",\")).map(lambda p: Row(accNo=p[0], tranAmount=float(p[1]))).toDF() \n>>> # Register temporary view in the DataFrame for using it in SQL \n>>> acTransDF.createOrReplaceTempView(\"trans\") \n>>> # Use SQL to create another DataFrame containing the account summary records \n>>> acSummary = spark.sql(\"SELECT accNo, sum(tranAmount) as transTotal FROM trans GROUP BY accNo\") \n>>> # Show the first few records of the DataFrame \n>>> acSummary.show()     \n+-------+----------+ \n|  accNo|transTotal| \n+-------+----------+ \n|SB10005|      56.0| \n|SB10004|     500.0| \n|SB10003|     330.0| \n|SB10002|    8590.0| \n|SB10001|   18900.0| \n+-------+----------+ \n>>> # Create the DataFrame using API for the account summary records \n>>> acSummaryViaDFAPI = acTransDF.groupBy(\"accNo\").agg({\"tranAmount\": \"sum\"}).selectExpr(\"accNo\", \"`sum(tranAmount)` as transTotal\") \n>>> # Show the first few records of the DataFrame \n>>> acSummaryViaDFAPI.show() \n+-------+----------+ \n|  accNo|transTotal| \n+-------+----------+ \n|SB10005|      56.0| \n|SB10004|     500.0| \n|SB10003|     330.0| \n|SB10002|    8590.0| \n|SB10001|   18900.0| \n+-------+----------+\n\n```", "```py\nscala> // Define the case classes for using in conjunction with DataFrames \nscala> case class AcMaster(accNo: String, firstName: String, lastName: String) \ndefined class AcMaster \nscala> case class AcBal(accNo: String, balanceAmount: Double) \ndefined class AcBal \nscala> // Functions to convert the sequence of strings to objects defined by the case classes \nscala> def toAcMaster =  (master: Seq[String]) => AcMaster(master(0), master(1), master(2)) \ntoAcMaster: Seq[String] => AcMaster \nscala> def toAcBal =  (bal: Seq[String]) => AcBal(bal(0), bal(1).trim.toDouble) \ntoAcBal: Seq[String] => AcBal \nscala> // Creation of the list from where the RDD is going to be created \nscala> val acMasterList = Array(\"SB10001,Roger,Federer\",\"SB10002,Pete,Sampras\", \"SB10003,Rafael,Nadal\",\"SB10004,Boris,Becker\", \"SB10005,Ivan,Lendl\") \nacMasterList: Array[String] = Array(SB10001,Roger,Federer, SB10002,Pete,Sampras, SB10003,Rafael,Nadal, SB10004,Boris,Becker, SB10005,Ivan,Lendl) \nscala> // Creation of the list from where the RDD is going to be created \nscala> val acBalList = Array(\"SB10001,50000\", \"SB10002,12000\",\"SB10003,3000\", \"SB10004,8500\", \"SB10005,5000\") \nacBalList: Array[String] = Array(SB10001,50000, SB10002,12000, SB10003,3000, SB10004,8500, SB10005,5000) \nscala> // Create the DataFrame \nscala> val acMasterDF = sc.parallelize(acMasterList).map(_.split(\",\")).map(toAcMaster(_)).toDF() \nacMasterDF: org.apache.spark.sql.DataFrame = [accNo: string, firstName: string ... 1 more field] \nscala> // Create the DataFrame \nscala> val acBalDF = sc.parallelize(acBalList).map(_.split(\",\")).map(toAcBal(_)).toDF() \nacBalDF: org.apache.spark.sql.DataFrame = [accNo: string, balanceAmount: double] \nscala> // Persist the data of the DataFrame into a Parquet file \nscala> acMasterDF.write.parquet(\"scala.master.parquet\") \nscala> // Persist the data of the DataFrame into a JSON file \nscala> acBalDF.write.json(\"scalaMaster.json\") \nscala> // Read the data into a DataFrame from the Parquet file \nscala> val acMasterDFFromFile = spark.read.parquet(\"scala.master.parquet\") \nacMasterDFFromFile: org.apache.spark.sql.DataFrame = [accNo: string, firstName: string ... 1 more field] \nscala> // Register temporary view in the DataFrame for using it in SQL \nscala> acMasterDFFromFile.createOrReplaceTempView(\"master\") \nscala> // Read the data into a DataFrame from the JSON file \nscala> val acBalDFFromFile = spark.read.json(\"scalaMaster.json\") \nacBalDFFromFile: org.apache.spark.sql.DataFrame = [accNo: string, balanceAmount: double] \nscala> // Register temporary view in the DataFrame for using it in SQL \nscala> acBalDFFromFile.createOrReplaceTempView(\"balance\") \nscala> // Show the first few records of the DataFrame \nscala> acMasterDFFromFile.show \n+-------+---------+--------+ \n|  accNo|firstName|lastName| \n+-------+---------+--------+ \n|SB10001|    Roger| Federer| \n|SB10002|     Pete| Sampras| \n|SB10003|   Rafael|   Nadal| \n|SB10004|    Boris|  Becker| \n|SB10005|     Ivan|   Lendl| \n+-------+---------+--------+ \nscala> acBalDFFromFile.show \n+-------+-------------+ \n|  accNo|balanceAmount| \n+-------+-------------+ \n|SB10001|      50000.0| \n|SB10002|      12000.0| \n|SB10003|       3000.0| \n|SB10004|       8500.0| \n|SB10005|       5000.0| \n+-------+-------------+ \nscala> // Use SQL to create another DataFrame containing the account detail records \nscala> val acDetail = spark.sql(\"SELECT master.accNo, firstName, lastName, balanceAmount FROM master, balance WHERE master.accNo = balance.accNo ORDER BY balanceAmount DESC\") \nacDetail: org.apache.spark.sql.DataFrame = [accNo: string, firstName: string ... 2 more fields] \nscala> // Show the first few records of the DataFrame \nscala> acDetail.show \n+-------+---------+--------+-------------+ \n|  accNo|firstName|lastName|balanceAmount| \n+-------+---------+--------+-------------+ \n|SB10001|    Roger| Federer|      50000.0| \n|SB10002|     Pete| Sampras|      12000.0| \n|SB10004|    Boris|  Becker|       8500.0| \n|SB10005|     Ivan|   Lendl|       5000.0| \n|SB10003|   Rafael|   Nadal|       3000.0| \n+-------+---------+--------+-------------+\n\n```", "```py\nscala> // Create the DataFrame using API for the account detail records \nscala> val acDetailFromAPI = acMasterDFFromFile.join(acBalDFFromFile, acMasterDFFromFile(\"accNo\") === acBalDFFromFile(\"accNo\"), \"inner\").sort($\"balanceAmount\".desc).select(acMasterDFFromFile(\"accNo\"), acMasterDFFromFile(\"firstName\"), acMasterDFFromFile(\"lastName\"), acBalDFFromFile(\"balanceAmount\")) \nacDetailFromAPI: org.apache.spark.sql.DataFrame = [accNo: string, firstName: string ... 2 more fields] \nscala> // Show the first few records of the DataFrame \nscala> acDetailFromAPI.show \n+-------+---------+--------+-------------+ \n|  accNo|firstName|lastName|balanceAmount| \n+-------+---------+--------+-------------+ \n|SB10001|    Roger| Federer|      50000.0| \n|SB10002|     Pete| Sampras|      12000.0| \n|SB10004|    Boris|  Becker|       8500.0| \n|SB10005|     Ivan|   Lendl|       5000.0| \n|SB10003|   Rafael|   Nadal|       3000.0| \n+-------+---------+--------+-------------+ \nscala> // Use SQL to create another DataFrame containing the top 3 account detail records \nscala> val acDetailTop3 = spark.sql(\"SELECT master.accNo, firstName, lastName, balanceAmount FROM master, balance WHERE master.accNo = balance.accNo ORDER BY balanceAmount DESC\").limit(3) \nacDetailTop3: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [accNo: string, firstName: string ... 2 more fields] \nscala> // Show the first few records of the DataFrame \nscala> acDetailTop3.show \n+-------+---------+--------+-------------+ \n|  accNo|firstName|lastName|balanceAmount| \n+-------+---------+--------+-------------+ \n|SB10001|    Roger| Federer|      50000.0| \n|SB10002|     Pete| Sampras|      12000.0| \n|SB10004|    Boris|  Becker|       8500.0| \n+-------+---------+--------+-------------+\n\n```", "```py\n>>> from pyspark.sql import Row \n>>> # Creation of the list from where the RDD is going to be created \n>>> AcMaster = Row('accNo', 'firstName', 'lastName') \n>>> AcBal = Row('accNo', 'balanceAmount') \n>>> acMasterList = [\"SB10001,Roger,Federer\",\"SB10002,Pete,Sampras\", \"SB10003,Rafael,Nadal\",\"SB10004,Boris,Becker\", \"SB10005,Ivan,Lendl\"] \n>>> acBalList = [\"SB10001,50000\", \"SB10002,12000\",\"SB10003,3000\", \"SB10004,8500\", \"SB10005,5000\"] \n>>> # Create the DataFrame \n>>> acMasterDF = sc.parallelize(acMasterList).map(lambda trans: trans.split(\",\")).map(lambda r: AcMaster(*r)).toDF() \n>>> acBalDF = sc.parallelize(acBalList).map(lambda trans: trans.split(\",\")).map(lambda r: AcBal(r[0], float(r[1]))).toDF() \n>>> # Persist the data of the DataFrame into a Parquet file \n>>> acMasterDF.write.parquet(\"python.master.parquet\") \n>>> # Persist the data of the DataFrame into a JSON file \n>>> acBalDF.write.json(\"pythonMaster.json\") \n>>> # Read the data into a DataFrame from the Parquet file \n>>> acMasterDFFromFile = spark.read.parquet(\"python.master.parquet\") \n>>> # Register temporary table in the DataFrame for using it in SQL \n>>> acMasterDFFromFile.createOrReplaceTempView(\"master\") \n>>> # Register temporary table in the DataFrame for using it in SQL \n>>> acBalDFFromFile = spark.read.json(\"pythonMaster.json\") \n>>> # Register temporary table in the DataFrame for using it in SQL \n>>> acBalDFFromFile.createOrReplaceTempView(\"balance\") \n>>> # Show the first few records of the DataFrame \n>>> acMasterDFFromFile.show() \n+-------+---------+--------+ \n|  accNo|firstName|lastName| \n+-------+---------+--------+ \n|SB10001|    Roger| Federer| \n|SB10002|     Pete| Sampras| \n|SB10003|   Rafael|   Nadal| \n|SB10004|    Boris|  Becker| \n|SB10005|     Ivan|   Lendl| \n+-------+---------+--------+ \n>>> # Show the first few records of the DataFrame \n>>> acBalDFFromFile.show() \n+-------+-------------+ \n|  accNo|balanceAmount| \n+-------+-------------+ \n|SB10001|      50000.0| \n|SB10002|      12000.0| \n|SB10003|       3000.0| \n|SB10004|       8500.0| \n|SB10005|       5000.0| \n+-------+-------------+ \n>>> # Use SQL to create another DataFrame containing the account detail records \n>>> acDetail = spark.sql(\"SELECT master.accNo, firstName, lastName, balanceAmount FROM master, balance WHERE master.accNo = balance.accNo ORDER BY balanceAmount DESC\") \n>>> # Show the first few records of the DataFrame \n>>> acDetail.show() \n+-------+---------+--------+-------------+ \n|  accNo|firstName|lastName|balanceAmount| \n+-------+---------+--------+-------------+ \n|SB10001|    Roger| Federer|      50000.0| \n|SB10002|     Pete| Sampras|      12000.0| \n|SB10004|    Boris|  Becker|       8500.0| \n|SB10005|     Ivan|   Lendl|       5000.0| \n|SB10003|   Rafael|   Nadal|       3000.0| \n+-------+---------+--------+-------------+ \n>>> # Create the DataFrame using API for the account detail records \n>>> acDetailFromAPI = acMasterDFFromFile.join(acBalDFFromFile, acMasterDFFromFile.accNo == acBalDFFromFile.accNo).sort(acBalDFFromFile.balanceAmount, ascending=False).select(acMasterDFFromFile.accNo, acMasterDFFromFile.firstName, acMasterDFFromFile.lastName, acBalDFFromFile.balanceAmount) \n>>> # Show the first few records of the DataFrame \n>>> acDetailFromAPI.show() \n+-------+---------+--------+-------------+ \n|  accNo|firstName|lastName|balanceAmount| \n+-------+---------+--------+-------------+ \n|SB10001|    Roger| Federer|      50000.0| \n|SB10002|     Pete| Sampras|      12000.0| \n|SB10004|    Boris|  Becker|       8500.0| \n|SB10005|     Ivan|   Lendl|       5000.0| \n|SB10003|   Rafael|   Nadal|       3000.0| \n+-------+---------+--------+-------------+ \n>>> # Use SQL to create another DataFrame containing the top 3 account detail records \n>>> acDetailTop3 = spark.sql(\"SELECT master.accNo, firstName, lastName, balanceAmount FROM master, balance WHERE master.accNo = balance.accNo ORDER BY balanceAmount DESC\").limit(3) \n>>> # Show the first few records of the DataFrame \n>>> acDetailTop3.show() \n+-------+---------+--------+-------------+ \n|  accNo|firstName|lastName|balanceAmount| \n+-------+---------+--------+-------------+ \n|SB10001|    Roger| Federer|      50000.0| \n|SB10002|     Pete| Sampras|      12000.0| \n|SB10004|    Boris|  Becker|       8500.0| \n+-------+---------+--------+-------------+ \n\n```", "```py\nscala> // Define the case classes for using in conjunction with DataFrames and Dataset \nscala> case class Trans(accNo: String, tranAmount: Double)  \ndefined class Trans \nscala> // Creation of the list from where the Dataset is going to be created using a case class. \nscala> val acTransList = Seq(Trans(\"SB10001\", 1000), Trans(\"SB10002\",1200), Trans(\"SB10003\", 8000), Trans(\"SB10004\",400), Trans(\"SB10005\",300), Trans(\"SB10006\",10000), Trans(\"SB10007\",500), Trans(\"SB10008\",56), Trans(\"SB10009\",30),Trans(\"SB10010\",7000), Trans(\"CR10001\",7000), Trans(\"SB10002\",-10)) \nacTransList: Seq[Trans] = List(Trans(SB10001,1000.0), Trans(SB10002,1200.0), Trans(SB10003,8000.0), Trans(SB10004,400.0), Trans(SB10005,300.0), Trans(SB10006,10000.0), Trans(SB10007,500.0), Trans(SB10008,56.0), Trans(SB10009,30.0), Trans(SB10010,7000.0), Trans(CR10001,7000.0), Trans(SB10002,-10.0)) \nscala> // Create the Dataset \nscala> val acTransDS = acTransList.toDS() \nacTransDS: org.apache.spark.sql.Dataset[Trans] = [accNo: string, tranAmount: double] \nscala> acTransDS.show() \n+-------+----------+ \n|  accNo|tranAmount| \n+-------+----------+ \n|SB10001|    1000.0| \n|SB10002|    1200.0| \n|SB10003|    8000.0| \n|SB10004|     400.0| \n|SB10005|     300.0| \n|SB10006|   10000.0| \n|SB10007|     500.0| \n|SB10008|      56.0| \n|SB10009|      30.0| \n|SB10010|    7000.0| \n|CR10001|    7000.0| \n|SB10002|     -10.0| \n+-------+----------+ \nscala> // Apply filter and create another Dataset of good transaction records \nscala> val goodTransRecords = acTransDS.filter(_.tranAmount > 0).filter(_.accNo.startsWith(\"SB\")) \ngoodTransRecords: org.apache.spark.sql.Dataset[Trans] = [accNo: string, tranAmount: double] \nscala> goodTransRecords.show() \n+-------+----------+ \n|  accNo|tranAmount| \n+-------+----------+ \n|SB10001|    1000.0| \n|SB10002|    1200.0| \n|SB10003|    8000.0| \n|SB10004|     400.0| \n|SB10005|     300.0| \n|SB10006|   10000.0| \n|SB10007|     500.0| \n|SB10008|      56.0| \n|SB10009|      30.0| \n|SB10010|    7000.0| \n+-------+----------+ \nscala> // Apply filter and create another Dataset of high value transaction records \nscala> val highValueTransRecords = goodTransRecords.filter(_.tranAmount > 1000) \nhighValueTransRecords: org.apache.spark.sql.Dataset[Trans] = [accNo: string, tranAmount: double] \nscala> highValueTransRecords.show() \n+-------+----------+ \n|  accNo|tranAmount| \n+-------+----------+ \n|SB10002|    1200.0| \n|SB10003|    8000.0| \n|SB10006|   10000.0| \n|SB10010|    7000.0| \n+-------+----------+ \nscala> // The function that identifies the bad amounts \nscala> val badAmountLambda = (trans: Trans) => trans.tranAmount <= 0 \nbadAmountLambda: Trans => Boolean = <function1> \nscala> // The function that identifies bad accounts \nscala> val badAcNoLambda = (trans: Trans) => trans.accNo.startsWith(\"SB\") == false \nbadAcNoLambda: Trans => Boolean = <function1> \nscala> // Apply filter and create another Dataset of bad amount records \nscala> val badAmountRecords = acTransDS.filter(badAmountLambda) \nbadAmountRecords: org.apache.spark.sql.Dataset[Trans] = [accNo: string, tranAmount: double] \nscala> badAmountRecords.show() \n+-------+----------+ \n|  accNo|tranAmount| \n+-------+----------+ \n|SB10002|     -10.0| \n+-------+----------+ \nscala> // Apply filter and create another Dataset of bad account records \nscala> val badAccountRecords = acTransDS.filter(badAcNoLambda) \nbadAccountRecords: org.apache.spark.sql.Dataset[Trans] = [accNo: string, tranAmount: double] \nscala> badAccountRecords.show() \n+-------+----------+ \n|  accNo|tranAmount| \n+-------+----------+ \n|CR10001|    7000.0| \n+-------+----------+ \nscala> // Do the union of two Dataset and create another Dataset \nscala> val badTransRecords  = badAmountRecords.union(badAccountRecords) \nbadTransRecords: org.apache.spark.sql.Dataset[Trans] = [accNo: string, tranAmount: double] \nscala> badTransRecords.show() \n+-------+----------+ \n|  accNo|tranAmount| \n+-------+----------+ \n|SB10002|     -10.0| \n|CR10001|    7000.0| \n+-------+----------+ \nscala> // Calculate the sum \nscala> val sumAmount = goodTransRecords.map(trans => trans.tranAmount).reduce(_ + _) \nsumAmount: Double = 28486.0 \nscala> // Calculate the maximum \nscala> val maxAmount = goodTransRecords.map(trans => trans.tranAmount).reduce((a, b) => if (a > b) a else b) \nmaxAmount: Double = 10000.0 \nscala> // Calculate the minimum \nscala> val minAmount = goodTransRecords.map(trans => trans.tranAmount).reduce((a, b) => if (a < b) a else b) \nminAmount: Double = 30.0 \nscala> // Convert the Dataset to DataFrame \nscala> val acTransDF = acTransDS.toDF() \nacTransDF: org.apache.spark.sql.DataFrame = [accNo: string, tranAmount: double] \nscala> acTransDF.show() \n+-------+----------+ \n|  accNo|tranAmount| \n+-------+----------+ \n|SB10001|    1000.0| \n|SB10002|    1200.0| \n|SB10003|    8000.0| \n|SB10004|     400.0| \n|SB10005|     300.0| \n|SB10006|   10000.0| \n|SB10007|     500.0| \n|SB10008|      56.0| \n|SB10009|      30.0| \n|SB10010|    7000.0| \n|CR10001|    7000.0| \n|SB10002|     -10.0| \n+-------+----------+ \nscala> // Use Spark SQL to find out invalid transaction records \nscala> acTransDF.createOrReplaceTempView(\"trans\") \nscala> val invalidTransactions = spark.sql(\"SELECT accNo, tranAmount FROM trans WHERE (accNo NOT LIKE 'SB%') OR tranAmount <= 0\") \ninvalidTransactions: org.apache.spark.sql.DataFrame = [accNo: string, tranAmount: double] \nscala> invalidTransactions.show() \n+-------+----------+ \n|  accNo|tranAmount| \n+-------+----------+ \n|CR10001|    7000.0| \n|SB10002|     -10.0| \n+-------+----------+ \nscala> // Interoperability of RDD, DataFrame and Dataset \nscala> // Create RDD \nscala> val acTransRDD = sc.parallelize(acTransList) \nacTransRDD: org.apache.spark.rdd.RDD[Trans] = ParallelCollectionRDD[206] at parallelize at <console>:28 \nscala> // Convert RDD to DataFrame \nscala> val acTransRDDtoDF = acTransRDD.toDF() \nacTransRDDtoDF: org.apache.spark.sql.DataFrame = [accNo: string, tranAmount: double] \nscala> // Convert the DataFrame to Dataset with the type checking \nscala> val acTransDFtoDS = acTransRDDtoDF.as[Trans] \nacTransDFtoDS: org.apache.spark.sql.Dataset[Trans] = [accNo: string, tranAmount: double] \nscala> acTransDFtoDS.show() \n+-------+----------+ \n|  accNo|tranAmount| \n+-------+----------+ \n|SB10001|    1000.0| \n|SB10002|    1200.0| \n|SB10003|    8000.0| \n|SB10004|     400.0| \n|SB10005|     300.0| \n|SB10006|   10000.0| \n|SB10007|     500.0| \n|SB10008|      56.0| \n|SB10009|      30.0| \n|SB10010|    7000.0| \n|CR10001|    7000.0| \n|SB10002|     -10.0| \n+-------+----------+\n\n```", "```py\nscala> // Get the catalog object from the SparkSession object\nscala> val catalog = spark.catalog\ncatalog: org.apache.spark.sql.catalog.Catalog = org.apache.spark.sql.internal.CatalogImpl@14b8a751\nscala> // Get the list of databases\nscala> val dbList = catalog.listDatabases()\ndbList: org.apache.spark.sql.Dataset[org.apache.spark.sql.catalog.Database] = [name: string, description: string ... 1 more field]\nscala> // Display the details of the databases\nscala> dbList.select(\"name\", \"description\", \"locationUri\").show()**+-------+----------------+--------------------+**\n**| name| description| locationUri|**\n**+-------+----------------+--------------------+**\n**|default|default database|file:/Users/RajT/...|**\n**+-------+----------------+--------------------+**\nscala> // Display the details of the tables in the database\nscala> val tableList = catalog.listTables()\ntableList: org.apache.spark.sql.Dataset[org.apache.spark.sql.catalog.Table] = [name: string, database: string ... 3 more fields]\nscala> tableList.show()**+-----+--------+-----------+---------+-----------+**\n **| name|database|description|tableType|isTemporary|**\n**+-----+--------+-----------+---------+-----------+**\n**|trans| null| null|TEMPORARY| true|**\n**+-----+--------+-----------+---------+-----------+**\nscala> // The above list contains the temporary view that was created in the Dataset use case discussed in the previous section\n// The views created in the applications can be removed from the database using the Catalog APIscala> catalog.dropTempView(\"trans\")\n// List the available tables after dropping the temporary viewscala> val latestTableList = catalog.listTables()\nlatestTableList: org.apache.spark.sql.Dataset[org.apache.spark.sql.catalog.Table] = [name: string, database: string ... 3 more fields]\nscala> latestTableList.show()**+----+--------+-----------+---------+-----------+**\n**|name|database|description|tableType|isTemporary|**\n**+----+--------+-----------+---------+-----------+**\n**+----+--------+-----------+---------+-----------+** \n\n```", "```py\n>>> #Get the catalog object from the SparkSession object\n>>> catalog = spark.catalog\n>>> #Get the list of databases and their details.\n>>> catalog.listDatabases()   [Database(name='default', description='default database', locationUri='file:/Users/RajT/source-code/spark-source/spark-2.0/spark-warehouse')]\n// Display the details of the tables in the database\n>>> catalog.listTables()\n>>> []\n\n```"]