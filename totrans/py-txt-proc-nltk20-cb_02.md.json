["```py\n>>> from nltk.stem import PorterStemmer\n>>> stemmer = PorterStemmer()\n>>> stemmer.stem('cooking')\n'cook'\n>>> stemmer.stem('cookery')\n'cookeri'\n```", "```py\n>>> from nltk.stem import LancasterStemmer\n>>> stemmer = LancasterStemmer()\n>>> stemmer.stem('cooking')\n'cook'\n>>> stemmer.stem('cookery')\n'cookery'\n```", "```py\n>>> from nltk.stem import RegexpStemmer\n>>> stemmer = RegexpStemmer('ing')\n>>> stemmer.stem('cooking')\n'cook'\n>>> stemmer.stem('cookery')\n'cookery'\n>>> stemmer.stem('ingleside')\n'leside'\n```", "```py\n>>> from nltk.stem import SnowballStemmer\n>>> SnowballStemmer.languages\n('danish', 'dutch', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n>>> spanish_stemmer = SnowballStemmer('spanish')\n>>> spanish_stemmer.stem('hola')\nu'hol'\n```", "```py\n>>> from nltk.stem import WordNetLemmatizer\n>>> lemmatizer = WordNetLemmatizer()\n>>> lemmatizer.lemmatize('cooking')\n'cooking'\n>>> lemmatizer.lemmatize('cooking', pos='v')\n'cook'\n>>> lemmatizer.lemmatize('cookbooks')\n'cookbook'\n```", "```py\n>>> from nltk.stem import PorterStemmer\n>>> stemmer = PorterStemmer()\n>>> stemmer.stem('believes')\n'believ'\n>>> lemmatizer.lemmatize('believes')\n'belief'\n```", "```py\n>>> stemmer.stem('buses')\n'buse'\n>>> lemmatizer.lemmatize('buses')\n'bus'\n>>> stemmer.stem('bus')\n'bu'\n```", "```py\n>>> from nltk.misc import babelfish\n>>> babelfish.translate('cookbook', 'english', 'spanish')\n'libro de cocina'\n>>> babelfish.translate('libro de cocina', 'spanish', 'english')\n'kitchen book'\n>>> babelfish.translate('cookbook', 'english', 'german')\n'Kochbuch'\n>>> babelfish.translate('kochbuch', 'german', 'english')\n'cook book'\n```", "```py\n>>> for text in babelfish.babelize('cookbook', 'english', 'spanish'):\n...  print text\ncookbook\nlibro de cocina\nkitchen book\nlibro de la cocina\nbook of the kitchen\n```", "```py\n>>> babelfish.available_languages\n['Portuguese', 'Chinese', 'German', 'Japanese', 'French', 'Spanish', 'Russian', 'Greek', 'English', 'Korean', 'Italian']\n```", "```py\nimport re\n\nreplacement_patterns = [\n  (r'won\\'t', 'will not'),\n  (r'can\\'t', 'cannot'),\n  (r'i\\'m', 'i am'),\n  (r'ain\\'t', 'is not'),\n  (r'(\\w+)\\'ll', '\\g<1> will'),\n  (r'(\\w+)n\\'t', '\\g<1> not'),\n  (r'(\\w+)\\'ve', '\\g<1> have'),\n  (r'(\\w+)\\'s', '\\g<1> is'),\n  (r'(\\w+)\\'re', '\\g<1> are'),\n  (r'(\\w+)\\'d', '\\g<1> would')\n\n]\nclass RegexpReplacer(object):\n  def __init__(self, patterns=replacement_patterns):\n    self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n\n  def replace(self, text):\n    s = text\n    for (pattern, repl) in self.patterns:\n      (s, count) = re.subn(pattern, repl, s)\n    return s\n```", "```py\n>>> from replacers import RegexpReplacer\n>>> replacer = RegexpReplacer()\n>>> replacer.replace(\"can't is a contraction\")\n'cannot is a contraction'\n>>> replacer.replace(\"I should've done that thing I didn't do\")\n'I should have done that thing I did not do'\n```", "```py\n>>> from nltk.tokenize import word_tokenize\n>>> from replacers import RegexpReplacer\n>>> replacer = RegexpReplacer()\n>>> word_tokenize(\"can't is a contraction\")\n['ca', \"n't\", 'is', 'a', 'contraction']\n>>> word_tokenize(replacer.replace(\"can't is a contraction\"))\n['can', 'not', 'is', 'a', 'contraction']\n```", "```py\nimport re\n\nclass RepeatReplacer(object):\n  def __init__(self):\n    self.repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n    self.repl = r'\\1\\2\\3'\n\n  def replace(self, word):\n    repl_word = self.repeat_regexp.sub(self.repl, word)\n    if repl_word != word:\n      return self.replace(repl_word)\n\n    else:\n      return repl_word\n```", "```py\n>>> from replacers import RepeatReplacer\n>>> replacer = RepeatReplacer()\n>>> replacer.replace('looooove')\n'love'\n>>> replacer.replace('oooooh')\n'oh'\n>>> replacer.replace('goose')\n'gose'\n```", "```py\nimport re\nfrom nltk.corpus import wordnet\n\nclass RepeatReplacer(object):\n  def __init__(self):\n    self.repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n    self.repl = r'\\1\\2\\3'\n\n  def replace(self, word):\n    if wordnet.synsets(word):\n      return word\n    repl_word = self.repeat_regexp.sub(self.repl, word)\n\n    if repl_word != word:\n      return self.replace(repl_word)\n    else:\n      return repl_word\n```", "```py\nimport enchant\nfrom nltk.metrics import edit_distance\n\nclass SpellingReplacer(object):\n  def __init__(self, dict_name='en', max_dist=2):\n    self.spell_dict = enchant.Dict(dict_name)\n    self.max_dist = 2\n\n  def replace(self, word):\n    if self.spell_dict.check(word):\n      return word\n    suggestions = self.spell_dict.suggest(word)\n\n    if suggestions and edit_distance(word, suggestions[0]) <= self.max_dist:\n      return suggestions[0]\n    else:\n      return word\n```", "```py\n>>> from replacers import SpellingReplacer\n>>> replacer = SpellingReplacer()\n>>> replacer.replace('cookbok')\n'cookbook'\n```", "```py\n>>> import enchant\n>>> d = enchant.Dict('en')\n>>> d.suggest('languege')\n['language', 'languisher', 'languish', 'languor', 'languid']\n```", "```py\n>>> enchant.list_languages()\n['en_AU', 'en_GB', 'en_US', 'en_ZA', 'en_CA', 'en']\n```", "```py\n>>> import enchant\n>>> dUS = enchant.Dict('en_US')\n>>> dUS.check('theater')\nTrue\n>>> dGB = enchant.Dict('en_GB')\n>>> dGB.check('theater')\nFalse\n>>> from replacers import SpellingReplacer\n>>> us_replacer = SpellingReplacer('en_US')\n>>> us_replacer.replace('theater')\n'theater'\n>>> gb_replacer = SpellingReplacer('en_GB')\n>>> gb_replacer.replace('theater')\n'theatre'\n```", "```py\n>>> d = enchant.Dict('en_US')\n>>> d.check('nltk')\nFalse\n>>> d = enchant.DictWithPWL('en_US', 'mywords.txt')\n>>> d.check('nltk')\nTrue\n```", "```py\nclass CustomSpellingReplacer(SpellingReplacer):\n  def __init__(self, spell_dict, max_dist=2):\n    self.spell_dict = spell_dict\n    self.max_dist = max_dist\n```", "```py\n>>> from replacers import CustomSpellingReplacer\n>>> d = enchant.DictWithPWL('en_US', 'mywords.txt')\n>>> replacer = CustomSpellingReplacer(d)\n>>> replacer.replace('nltk')\n'nltk'\n```", "```py\nclass WordReplacer(object):\n  def __init__(self, word_map):\n    self.word_map = word_map\n  def replace(self, word):\n    return self.word_map.get(word, word)\n```", "```py\n>>> from replacers import wordReplacer\n>>> replacer = WordReplacer({'bday': 'birthday'})\n>>> replacer.replace('bday')\n'birthday'\n>>> replacer.replace('happy')\n'happy'\n```", "```py\nimport csv\n\nclass CsvWordReplacer(WordReplacer):\n  def __init__(self, fname):\n    word_map = {}\n    for line in csv.reader(open(fname)):\n      word, syn = line\n      word_map[word] = syn\n    super(CsvWordReplacer, self).__init__(word_map)\n```", "```py\n>>> from replacers import CsvWordReplacer\n>>> replacer = CsvWordReplacer('synonyms.csv')\n>>> replacer.replace('bday')\n'birthday'\n>>> replacer.replace('happy')\n'happy'\n```", "```py\nimport yaml\n\nclass YamlWordReplacer(WordReplacer):\n  def __init__(self, fname):\n    word_map = yaml.load(open(fname))\n    super(YamlWordReplacer, self).__init__(word_map)\n```", "```py\n>>> from replacers import YamlWordReplacer\n>>> replacer = YamlWordReplacer('synonyms.yaml')\n>>> replacer.replace('bday')\n'birthday'\n>>> replacer.replace('happy')\n'happy'\n```", "```py\nfrom nltk.corpus import wordnet\nclass AntonymReplacer(object):\n  def replace(self, word, pos=None):\n    antonyms = set()\n    for syn in wordnet.synsets(word, pos=pos):\n      for lemma in syn.lemmas:\n        for antonym in lemma.antonyms():\n          antonyms.add(antonym.name)\n    if len(antonyms) == 1:\n      return antonyms.pop()\n    else:\n      return None\n\n  def replace_negations(self, sent):\n    i, l = 0, len(sent)\n    words = []\n    while i < l:\n      word = sent[i]\n      if word == 'not' and i+1 < l:\n        ant = self.replace(sent[i+1])\n        if ant:\n          words.append(ant)\n          i += 2\n          continue\n      words.append(word)\n      i += 1\n    return words\n```", "```py\n>>> from replacers import AntonymReplacer\n>>> replacer = AntonymReplacer()\n>>> replacer.replace('good')\n>>> replacer.replace('uglify')\n'beautify'\n>>> sent = [\"let's\", 'not', 'uglify', 'our', 'code']\n>>> replacer.replace_negations(sent)\n[\"let's\", 'beautify', 'our', 'code']\n```", "```py\nclass AntonymWordReplacer(WordReplacer, AntonymReplacer):\n  pass\n```", "```py\n>>> from replacers import AntonymWordReplacer\n>>> replacer = AntonymWordReplacer({'evil': 'good'})\n>>> replacer.replace_negations(['good', 'is', 'not', 'evil'])\n['good', 'is', 'good']\n```"]