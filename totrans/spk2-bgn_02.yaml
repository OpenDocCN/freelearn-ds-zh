- en: Chapter 2. Spark Programming Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Extract**, **Transform**, and **Load** (**ETL**) tools proliferated along
    with the growth of the data in organizations. The need to move data from one source
    to one or more destinations, processing it on the fly before it reaches its destination,
    were all the requirements of the time. Most of the time, these ETL tools were
    supporting only a few types of data, only a few types of data sources and destinations,
    and were closed to extension to allow them to support new data types and new sources
    and destinations. Because of these stringent limitations on the tools, sometimes
    even a one-step transformation process had to be done in multiple steps. These
    convoluted approaches mandated the need to have unnecessary wastage in terms of
    manpower, as well as other computing resources. The main argument from the commercial
    ETL vendors all the time remained the same, one size doesn''t fit all. So use
    *our* suite of tools instead of the point products available on the market. Many
    organizations got into vendor lock-in because of the profuse need to process data.
    Almost all the tools introduced before the year 2005 did not make use of the real
    power of the multi-core architecture of the computers if they supported running
    their tools on the commodity hardware. So, simple but voluminous data processing
    jobs took hours and sometimes even days to complete with these tools.'
  prefs: []
  type: TYPE_NORMAL
- en: Spark became an instant hit in the market because of its ability to process
    a huge amount of data types and a growing number of data sources and data destinations.
    The most important and basic data abstraction Spark provides is the **resilient
    distributed dataset** (**RDD**). As discussed in the previous chapter, Spark supports
    distributed processing on a cluster of nodes. The moment there is a cluster of
    nodes, there is a good chance that when the data processing is going on, some
    of the nodes can die. When such failures happen, the framework should be capable
    of coming out of such failures. Spark is designed to do that and that is what
    the *resilient* part in the RDD signifies. If there is a huge amount of data to
    be processed and there are nodes available in the cluster, the framework should
    have the capability to split the big dataset into smaller chunks and distribute
    them to be processed on more than one node in a cluster, in parallel. Spark is
    capable of doing that and that is what the *distributed* part in the RDD signifies.
    In other words, Spark is designed from the ground up to have its basic dataset
    abstraction capable of getting split into smaller pieces deterministically and
    distributed to more than one node in the cluster for parallel processing, while
    elegantly handling the failures in the nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Functional programming with Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark RDD
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data transformations and actions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark programming basics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating RDDs from files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Functional programming with Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The mutation of objects at run time, and the inability to get consistent results
    from a program or function because of the side effect that the program logic creates
    makes many applications very complex. If the functions in programming languages
    start behaving exactly like mathematical functions in such a way that the output
    of the function depends only on the inputs, that gives lots of predictability
    to applications. The computer programming paradigm giving lots of emphasis to
    the process of building such functions and other elements based on that, and using
    those functions just in the way that any other data types are being used, is popularly
    known as the functional programming paradigm. Out of the JVM-based programming
    languages, Scala is one of the most important ones that has very strong functional
    programming capabilities without losing any object orientation. Spark is written
    predominantly in Scala. Because of that itself, Spark has taken lots of very good
    concepts from Scala.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Spark RDD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most important feature that Spark took from Scala is the ability to use
    functions as parameters to the Spark transformations and Spark actions. Quite
    often, the RDD in Spark behaves just like a collection object in Scala. Because
    of that, some of the data transformation method names of Scala collections are
    used in Spark RDD to do the same thing. This is a very neat approach and those
    who have expertise in Scala will find it very easy to program with RDDs. We will
    see a few important features in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Spark RDD is immutable
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are some strong rules based on which an RDD is created. Once an RDD is
    created, intentionally or unintentionally, it cannot be changed. This gives another
    insight into the construction of an RDD. Because of that, when the nodes processing
    some part of an RDD die, the driver program can recreate those parts and assign
    the task of processing it to another node, ultimately, completing the data processing
    job successfully.
  prefs: []
  type: TYPE_NORMAL
- en: Since the RDD is immutable, splitting a big one to smaller ones, distributing
    them to various worker nodes for processing, and finally compiling the results
    to produce the final result can be done safely without worrying about the underlying
    data getting changed.
  prefs: []
  type: TYPE_NORMAL
- en: Spark RDD is distributable
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If Spark is run in a cluster mode where there are multiple worker nodes available
    to take the tasks, all these nodes will have different execution contexts. The
    individual tasks are distributed and run on different JVMs. All these activities
    of a big RDD getting divided into smaller chunks, getting distributed for processing
    to the worker nodes, and finally, assembling the results back, are completely
    hidden from the users.
  prefs: []
  type: TYPE_NORMAL
- en: Spark has its own mechanism for recovering from the system faults and other
    forms of errors which occur during the data processing and hence this data abstraction
    is highly resilient.
  prefs: []
  type: TYPE_NORMAL
- en: Spark RDD lives in memory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Spark does keep all the RDDs in the memory as much as it can. Only in rare situations,
    where Spark is running out of memory or if the data size is growing beyond the
    capacity, is it written to disk. Most of the processing on RDD happens in the
    memory, and that is the reason why Spark is able to process the data at a lightning
    fast speed.
  prefs: []
  type: TYPE_NORMAL
- en: Spark RDD is strongly typed
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Spark RDD can be created using any supported data types. These data types can
    be Scala/Java supported intrinsic data types or custom created data types such
    as your own classes. The biggest advantage coming out of this design decision
    is the freedom from runtime errors. If it is going to break because of a data
    type issue, it will break during compile time.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table captures the structure of an RDD containing tuples of a
    retail bank account data. It is of the type RDD[(string, string, string, double)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **AccountNo** | **FirstName** | **LastName** | **AccountBalance** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SB001 | John | Mathew | 250.00 |'
  prefs: []
  type: TYPE_TB
- en: '| SB002 | Tracy | Mason | 450.00 |'
  prefs: []
  type: TYPE_TB
- en: '| SB003 | Paul | Thomson | 560.00 |'
  prefs: []
  type: TYPE_TB
- en: '| SB004 | Samantha | Grisham | 650.00 |'
  prefs: []
  type: TYPE_TB
- en: '| SB005 | John | Grove | 1000.00 |'
  prefs: []
  type: TYPE_TB
- en: 'Suppose this RDD is going through a process to calculate the total amount of
    all these accounts in a cluster of three nodes, N1, N2, and N3; it can be split
    and distributed for something such as parallelizing the data processing. The following
    table contains the elements of the RDD[(string, string, string, double)] distributed
    to node N1 for processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **AccountNo** | **FirstName** | **LastName** | **AccountBalance** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SB001 | John | Mathew | 250.00 |'
  prefs: []
  type: TYPE_TB
- en: '| SB002 | Tracy | Mason | 450.00 |'
  prefs: []
  type: TYPE_TB
- en: 'The following table contains the elements of the RDD[(string, string, string,
    double)] distributed to node N2 for processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **AccountNo** | **FirstName** | **LastName** | **AccountBalance** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SB003 | Paul | Thomson | 560.00 |'
  prefs: []
  type: TYPE_TB
- en: '| SB004 | Samantha | Grisham | 650.00 |'
  prefs: []
  type: TYPE_TB
- en: '| SB005 | John | Grove | 1000.00 |'
  prefs: []
  type: TYPE_TB
- en: On node N1, the summation process happens and the result is returned to the
    Spark driver program. Similarly, on node N2, the summation process happens, the
    result is returned to the Spark driver program, and the final result is computed.
  prefs: []
  type: TYPE_NORMAL
- en: Spark has very deterministic rules on splitting a big RDD into smaller chunks
    for distribution to various nodes and because of that, even if something happens
    to, say, node N1, Spark knows how to recreate exactly the chunk that was lost
    in the node N1 and continue with the data processing operation by sending the
    same payload to node N3.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1 captures the essence of the process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Spark RDD is strongly typed](img/image_02_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Spark does a lot of processing in its driver memory and in the executor memory
    on the cluster nodes. Spark has various parameters that can be configured and
    fine-tuned so that the required resources are made available before the processing
    starts.
  prefs: []
  type: TYPE_NORMAL
- en: Data transformations and actions with RDDs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Spark does the data processing using the RDDs. From the relevant data source
    such as text files and NoSQL data stores, data is read to form the RDDs. On such
    an RDD, various data transformations are performed and finally, the result is
    collected. To be precise, Spark comes with Spark transformations and Spark actions
    that act upon RDDs. Let us take the following RDD capturing a list of retail banking
    transactions, which is of the type RDD[(string, string, double)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **AccountNo** | **TranNo** | **TranAmount** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SB001 | TR001 | 250.00 |'
  prefs: []
  type: TYPE_TB
- en: '| SB002 | TR004 | 450.00 |'
  prefs: []
  type: TYPE_TB
- en: '| SB003 | TR010 | 120.00 |'
  prefs: []
  type: TYPE_TB
- en: '| SB001 | TR012 | -120.00 |'
  prefs: []
  type: TYPE_TB
- en: '| SB001 | TR015 | -10.00 |'
  prefs: []
  type: TYPE_TB
- en: '| SB003 | TR020 | 100.00 |'
  prefs: []
  type: TYPE_TB
- en: 'To calculate the account level summary of the transactions from the RDD of
    the form `(AccountNo,TranNo,TranAmount)`:'
  prefs: []
  type: TYPE_NORMAL
- en: First it has to be transformed to the form of key-value pairs `(AccountNo,TranAmount)`,
    where `AccountNo` is the key but there will be multiple elements with the same
    key.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On this key, do a summation operation on `TranAmount`, resulting in another
    RDD of the form (AccountNo,TotalAmount),where every AccountNo will have only one
    element and TotalAmount is the sum of all the TranAmount for the given AccountNo.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now sort the key-value pairs on the `AccountNo` and store the output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the whole process described, all are Spark transformations except the storing
    of the output. Storing of the output is a **Spark action**. Spark does all these
    operations on a need-to-do basis. Spark does not act when a Spark transformation
    is applied. The real act happens when the first Spark action in the chain is called.
    Then it diligently applies all the preceding Spark transformations in order, and
    then does the first encountered Spark action. This is based on the concept called
    **Lazy Evaluation**.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the programming language context of declaring and using variables, *Lazy
    Evaluation* means that a variable is evaluated only when it is first used in the
    program.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apart from this action of storing the output to disk, there are many other
    possible Spark actions including, but not limited to, some of the ones given in
    the following list:'
  prefs: []
  type: TYPE_NORMAL
- en: Collecting all the contents in the resultant RDD to an array residing in the
    driver program
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Counting the number of elements in the RDD
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Counting the number of elements for each key in the RDD element
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Taking the first element in the RDD
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Taking a given number of elements from the RDD commonly used for top N reports
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Taking a sample of elements from the RDD
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iterating through all the elements in the RDD
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this example, many transformations are done on various RDDs that get created
    on the fly till the process is completed. In other words, whenever a transformation
    is done on an RDD, a new RDD gets created. This is because RDDs are inherently
    immutable. These RDDs that are getting created at the end of each transformation
    can be saved for future reference, or they will go out of scope eventually.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, the process of creating one or more RDDs and applying transformations
    and actions on them is a very common usage pattern seen ubiquitously in Spark
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The table referred in the preceding data transformation example contains the
    values in an RDD of type the RDD[(string, string, double)]. In this RDD, there
    are multiple elements, and each one is a tuple of the type (string, string, double).
    It is very common among programmers and the user community, for easy reference
    and conveying ideas, that the term `record` is being used to refer one to element
    in the RDD. In Spark RDD there is no concept of records, rows and columns. In
    other words the term `record` is mistakenly used synonymously to an element in
    the RDD, which may be a complex data type such as a tuple or a non-scalar data
    type. In this book, this practice is highly refrained to use the correct terms.
  prefs: []
  type: TYPE_NORMAL
- en: In Spark there are a good amount of Spark transformations available. These are
    really powerful because most of these take functions as input parameters to do
    the transformation. In other words, these transformations act on the RDD based
    on the functions that are defined and supplied by the user. This becomes even
    more powerful with Spark's uniform programming model. Whether the programming
    language of choice is Scala, Java, Python, or R, the way Spark transformations
    and Spark actions are used is similar. This lets the organizations choose their
    programming language of choice.
  prefs: []
  type: TYPE_NORMAL
- en: In Spark, even though the number of Spark actions are limited in number, they
    are really powerful, and users can write their own Spark actions if there is a
    need. There are many Spark connector programs that are available in the market,
    mainly to read and write data from various data stores. These connector programs
    are designed and developed either by the user community or by the data store vendors
    themselves to have connectivity to Spark. In addition to the available Spark actions,
    they may define their own actions to supplement existing sets of Spark actions.
    For example, the Spark Cassandra Connector is used to connect to Cassandra from
    Spark. This has an action `saveToCassandra`.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring with Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous chapter covered the details of the installation and development
    tool setup that is required for developing and running data processing applications
    using Spark. In most of the real-world applications, the Spark applications can
    become very complex with a really huge **directed acyclic graph**  (**DAG**) of
    Spark transformations and Spark actions. Spark comes with really powerful monitoring
    tools for monitoring the jobs that are running in a given Spark ecosystem. The
    monitoring doesn't start automatically.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Note that this is a completely optional step for running Spark applications.
    If enabled, it will give a very good insight into the way the Spark applications
    are run. Discretion has to be used to enable this in production environments,
    as it can affect the response time of the applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, there are some configuration changes to be made. The event logging
    mechanism should be turned on. For this, take the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the preceding steps are completed, edit the newly created `spark-defaults.conf`
    file to have the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once the preceding steps are completed, make sure that the previously used log
    directory exists in the filesystem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apart from the preceding configuration file changes, there are many properties
    in that configuration file that can be changed to fine tune the Spark runtime.
    The most important among them that is used frequently is the Spark driver memory.
    If the applications are dealing with a huge amount of data, it is a good idea
    to customize this property `spark.driver.memory`to have a higher value. Then run
    the following commands to start the Spark master:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Once the preceding steps are completed, make sure that the Spark web **user
    interface** (**UI**) is starting up by going to `http://localhost:8080/`. The
    assumption here is that there is no other application running in the `8080` port.
    If for some reason, there is a need to run this application on a different port,
    the command line option `--webui-port <PORT>` can be used in the script while
    starting the web user interface.
  prefs: []
  type: TYPE_NORMAL
- en: 'The web UI should look something similar to that shown in Figure 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Monitoring with Spark](img/image_02_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2
  prefs: []
  type: TYPE_NORMAL
- en: The most important information to be noted in the preceding figure is the fully-qualified
    Spark master URL (not the REST URL). It is going to be used again and again for
    many of the hands-on exercises that are going to be discussed in this book. The
    URL can change from system to system and the DNS settings. Also note that throughout
    this book, for all the hands-on exercises, Spark standalone deployment is used,
    which is the easiest among the deployments to get started with a single computer.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: These Spark application monitoring steps are given now to make the readers familiar
    with the toolset that Spark provides. Those who are familiar with these tools
    or those who are very confident of the application behavior need not need the
    help of these tools. But to understand the concepts, debugging, and some visualizations
    of the processes, these tools definitely provide immense help.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the Spark web UI that is given in Figure 2, it can be noted that there
    are no worker nodes available to do any task, and there are no running applications.
    The following steps capture the instructions to start the worker nodes. Note how
    the Spark master URL is being used while starting the worker node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Once the worker node is started, in the Spark web UI, the newly started worker
    node is displayed. The  `$SPARK_HOME/conf/slaves.template`template captures the
    default worker nodes that will be started with the invocation of the preceding
    command.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If additional worker nodes are required, copy the `slaves.template` file to
    name it to slaves and have the entries captured in there. When a spark-shell,
    pyspark, or sparkR is started, instructions can be given to it to use a given
    Spark master. This is useful when there is a need to run Spark applications or
    statements on a remote Spark cluster or against a given Spark master. If nothing
    is given, the Spark applications will run in the local mode.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The Spark web UI will look similar to that shown in Figure 3 once a worker
    node is started successfully. After this, if an application is run with the preceding
    Spark master URL, even that application''s details will be displayed in the Spark
    web UI. A detailed coverage of the applications is to follow in this chapter.
    Use the following scripts to stop the workers and master processes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![Monitoring with Spark](img/image_02_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3
  prefs: []
  type: TYPE_NORMAL
- en: The basics of programming with Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark programming revolves around RDDs. In any Spark application, the input
    data to be processed is taken to create an appropriate RDD. To begin with, start
    with the most basic way of creating an RDD, which is from a list. The input data
    used for this `hello world` kind of application is a small collection of retail
    banking transactions. To explain the core concepts, only some very elementary
    data items have been picked up. The transaction records contain account numbers
    and transaction amounts.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In these use cases and all the upcoming use cases in the book, if the term record
    is used, that will be in the business or use case context.
  prefs: []
  type: TYPE_NORMAL
- en: 'The use cases selected for elucidating the Spark transformations and Spark
    actions here are given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The transaction records are coming as comma-separated values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Filter out only the good transaction records from the list. The account number
    should start with `SB` and the transaction amount should be greater than zero.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find all the high value transaction records with a transaction amount greater
    than 1000.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find all the transaction records where the account number is bad.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find all the transaction records where the transaction amount is less than or
    equal to zero.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find a combined list of all the bad transaction records.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the total of all the transaction amounts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the maximum of all the transaction amounts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the minimum of all the transaction amounts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find all the good account numbers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The approach that is going to be followed throughout the book for any application
    that is going to be developed begins with the Spark REPL for the appropriate language.
    Start the Scala REPL for Spark and make sure that it starts without any errors
    and the prompt is seen. For this application, we will enable monitoring to learn
    how to do that and use it along with the development process. Other than explicitly
    starting the Spark master and the slaves separately, Spark comes with a script
    that will start both of these together using a single script. Then, fire up the
    Scala REPL with the Spark master URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'At the Scala REPL prompt, try the following statements. The output of the statements
    is given in bold. Note that `scala>` is the Scala REPL prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'All the preceding statements fall into one category except the first RDD creation
    and two function value definitions. They are all Spark transformations. Here is
    the step-by-step detail capturing what has been done so far:'
  prefs: []
  type: TYPE_NORMAL
- en: The value `acTransList` is the array containing the comma separated transaction
    records.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The value `acTransRDD` is the RDD created out of the array where `sc` is the
    Spark context or the Spark driver and the RDD is created in a parallelized way
    so that the RDD elements can form a distributed dataset. In other words, an instruction
    is given to the Spark driver to form a parallel collection or RDD from the given
    collection of values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The value `goodTransRecords` is the RDD created from `acTransRDD`after filtering
    the conditions transaction amount is > 0 and the account number starts with `SB`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The value `highValueTransRecords` is the RDD created from `goodTransRecords`after
    filtering the conditions transaction amount is > 1000.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The next two statements are storing the function definitions in a Scala value
    for easy reference later.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The values `badAmountRecords` and `badAccountRecords` are RDDs created from
    `acTransRDD` to filter the bad records containing the wrong transaction amount
    and invalid account number, respectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The value `badTransRecords` contains the union of the elements of both of the
    `badAmountRecords` and `badAccountRecords` RDDs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Spark web UI for this application so far will not show anything at this
    point because only Spark transformations have been executed. The real activity
    will start only after the first Spark action is executed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following statements are the continuation of the already executed statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'All the preceding statements did one thing, which is execute a Spark action
    on the RDDs *defined* earlier. All the evaluations of the RDDs happened only when
    a Spark action was called on those RDDs. The following statements are doing some
    of the calculations on the RDDs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding numbers calculated the sum, maximum and minimum, of all transaction
    amounts from the good records. In all the preceding transformations, the transaction
    records are processed one at a time. From those records, the account number and
    transaction amount are extracted and processed. It was done like that because
    the use case requirement was like that. Now the comma-separated values in each
    transaction record are split without looking at whether it is an account number
    or a transaction amount. The resulting RDD will contain a collection with all
    these mixed up. Out of that, if the elements starting with `SB` are picked up,
    that will result in good account numbers. The following statements are going to
    do that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now at this point, if the Spark web UI is opened, unlike what is seen in Figure
    3, one difference can be noticed. Since some Spark actions have been done, an
    application entry will show up. Since the Scala REPL of Spark is still running,
    it is shown in the list of applications that are still running. The following
    Figure 4 captures that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The basics of programming with Spark](img/image_02_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4
  prefs: []
  type: TYPE_NORMAL
- en: Navigate by clicking on the application ID to see all the metrics related to
    the running applications including the DAG visualizations and many more.
  prefs: []
  type: TYPE_NORMAL
- en: 'These statements covered all the use cases discussed, and it is worth going
    through the Spark transformations covered so far. These are some of the basic
    but very important transformations that will be used in most of the applications
    again and again:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Spark transformation** | **What it does** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `filter(fn)` | **Iterates through all the elements of the RDD, applies the
    function that is passed, and picks up the elements that return true as evaluated
    by the function on the element.** |'
  prefs: []
  type: TYPE_TB
- en: '| `map(fn)` | Iterates through all the elements of the RDD, applies the function
    that is passed, and picks up the output returned by the function. |'
  prefs: []
  type: TYPE_TB
- en: '| `flatMap(fn)` | Iterates through all the elements of the RDD, applies the
    function that is passed, and picks up the output returned by the function. The
    big difference here as compared to the Spark transformation `map(fn)` is that
    the function acts on a single element and returns a flat collection of elements.
    For example, it takes one banking transaction record and splits it into multiple
    fields, resulting in a collection from a single element. |'
  prefs: []
  type: TYPE_TB
- en: '| `union(other)` | Takes the union of all the elements of this RDD and the
    other RDD. |'
  prefs: []
  type: TYPE_TB
- en: It is also worth going through the Spark actions covered so far. These are some
    of the basic ones, but more actions will be covered in due course.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Spark action** | **What it does** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `collect()` | **Collects all the elements in the RDD to an array in the Spark
    driver.** |'
  prefs: []
  type: TYPE_TB
- en: '| `reduce(fn)` | Applies the function fn on all the elements of the RDD and
    the final result is calculated as defined by the function. It should be a function
    that takes two parameters and returns one, which is also commutative and associative.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `foreach(fn)` | Applies the function fn on all the elements of the RDD. This
    is mainly used for side effects. The Spark transformation `map(fn)` applies the
    function to all the elements of the RDD and returns another RDD. But the `foreach(fn)`
    Spark transformation does not return an RDD. For example, `foreach(println)` will
    take each element from the RDD and print it onto the console. Even though it is
    not used in the use cases covered here, it is worth mentioning. |'
  prefs: []
  type: TYPE_TB
- en: The next step in the Spark learning process is to try the statements in the
    Python REPL, covering exactly the same use case. The variable definitions have
    been maintained as similar as possible in both the languages to have easy assimilation
    of ideas. There may be minor variations in the way they are used here as compared
    to the Scala way; conceptually, it is independent of the language of choice.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start the Python REPL for Spark and make sure that it starts without any errors
    and the prompt is seen. While playing around with Scala code, the monitoring was
    already enabled. Now fire up the Python REPL with the Spark master URL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'At the Python REPL prompt, try the following statements. The output of the
    statements is given in bold. Note that `>>>` is the Python REPL prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The real power of the uniform programming model of Spark is very clearly visible
    if both the Scala and Python code sets are compared. The Spark transformations
    and Spark actions are the same in both the language implementations. The way functions
    are passed into these are different because of the programming language syntax
    differences.
  prefs: []
  type: TYPE_NORMAL
- en: Before running the Python REPL for Spark, the Scala REPL was closed and this
    was done on purpose. Then the Spark web UI should look something similar to that
    shown in Figure 5\. Since the Scala REPL was closed, that is getting listed under
    the completed applications list. Since the Python REPL is still open, that is
    getting listed under the running applications list. Note the application names
    of both the Scala REPL and Python REPL of Spark in the Spark web UI. These are
    standard names. When custom applications are run from files, there are ways to
    assign custom names while defining the Spark context object for the applications
    to facilitate monitoring of the applications and for logging purposes. These details
    will be covered later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: It is a good idea to spend time with the Spark web UI, getting familiar with
    all the metrics that are being captured, and how the DAG visualization is given
    in the UI. It will help a lot while debugging complex Spark applications.
  prefs: []
  type: TYPE_NORMAL
- en: '![The basics of programming with Spark](img/image_02_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5
  prefs: []
  type: TYPE_NORMAL
- en: MapReduce
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since day one, Spark has been placed as the replacement for Hadoop MapReduce
    programs. In general, data processing jobs are done in MapReduce style if that
    job can be divided into multiple tasks and they can be executed in parallel, and
    the final results can be computed after collecting the results from all these
    distributed pieces. Unlike Hadoop MapReduce, Spark can do this even if the DAG
    of activities is more than the two stages, such as Map and Reduce. Spark is designed
    to do that and that is one of the biggest value propositions that Spark highlights.
  prefs: []
  type: TYPE_NORMAL
- en: This section is going to continue with the same retail banking application and
    pick up some of the use cases that are ideal candidates for the MapReduce kind
    of data processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'The use cases selected for elucidating the MapReduce kind of data processing
    here are given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The retail banking transaction records come with account numbers and the transaction
    amounts in comma-separated strings.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pair the transactions to have key/value pairs such as (`AccNo`, `TranAmount`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find an account level summary of all the transactions to get the account balance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'At the Scala REPL prompt, try the following statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the step-by-step detail capturing what has been done so far:'
  prefs: []
  type: TYPE_NORMAL
- en: The value `acTransList` is the array containing the comma-separated transaction
    records.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The value `acTransRDD` is the RDD created out of the array, where sc is the
    Spark context or the Spark driver and the RDD is created in a parallelized way
    so that the RDD elements can form a distributed dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transform the `acTransRDD` to `acKeyVal` to have key-value pairs of the form
    (K,V), where the account number is chosen as the key. In this set of elements
    in the RDD, there will be multiple elements with the same key.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the next step, the key-value pairs are grouped by the key and a reduction
    function has been passed, which will add the transaction amount to form key-value
    pairs containing one element for a specific key in the RDD and the total of all
    the amounts for the same key. Then sort the elements on the key before producing
    the final result.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Collect the elements to an array at the driver level.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Assuming that the RDD `acKeyVal` is partitioned into two parts and distributed
    to a cluster for processing, Figure 6 captures the essence of the processing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![MapReduce](img/image_02_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table captures the Spark actions that are introduced in this
    use case:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Spark action** | **What it does?** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `reduceByKey(fn,[noOfTasks])` | **Applies the function fn on the RDD of the
    form (K,V) and is reduced to remove duplicate keys and apply the function passed
    as the parameter to be acted on the values at the key level.** |'
  prefs: []
  type: TYPE_TB
- en: '| `sortByKey([ascending], [numTasks])` | Sorts the RDD elements if the RDD
    is of the form (K,V) by its key K |'
  prefs: []
  type: TYPE_TB
- en: 'The `reduceByKey`action deserves a special mention. In Figure 6, the grouping
    of the elements by the key is a well-known operation. But in the next step, for
    the same key, the function passed to as a parameter takes two parameters and returns
    one. It is not very intuitive to get this right and you may be wondering from
    where the two inputs are coming while iterating through the values of the (K,V)
    pair for each key. This behavior takes the concept from the Scala collection method
    `reduceLeft`. The following Figure 7, with the values of the key **SB10001** doing
    the `reduceByKey(_ + _)`operation, is an attempt to explain the concept. This
    is just for the elucidation purposes of this example and the actual Spark implementation
    to do the same may be different:'
  prefs: []
  type: TYPE_NORMAL
- en: '![MapReduce](img/image_02_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7
  prefs: []
  type: TYPE_NORMAL
- en: On the right-hand side of Figure 7, the `reduceLeft` operation of the Scala
    collection method is illustrated. That is an attempt to give some insight into
    from where the two parameters are coming for the `reduceLeft`function. As a matter
    of fact, many of the transformations that are being used on Spark RDD are adapted
    from Scala collection methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the Python REPL prompt, try the following statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: The `reduceByKey` took an input parameter, which is a function. Similar to this,
    there is another transformation that does the key-based operation in a slightly
    different way. It is `groupByKey()`. This gathers all the values of a given key
    and forms the list of values from all the individual elements.
  prefs: []
  type: TYPE_NORMAL
- en: If there is a need to do multiple levels of processing with the same value elements
    as a collection for each key, this is the suitable transformation. In other words,
    if there are many (K,V) pairs, this transformation returns (K, Iterable<V>) for
    each key.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The only thing the developer needs to be cognizant about is to make sure that
    the number of such (K,V) pairs is not really huge so that the operation doesn't
    create performance problems. There is no hard and fast rule to find this out and
    it rather depends on the use case.
  prefs: []
  type: TYPE_NORMAL
- en: In all the preceding code snippets, for extracting account numbers or any other
    field from the comma-separated transaction record, split(`,`) is used multiple
    times within the `map()` transformation. This is to demonstrate the use of array
    elements within `map()`, or any other transformation or method. A better way of
    extracting the fields of the transaction record is to transform them as a tuple
    containing the required fields and then use the fields from the tuple to employ
    them in some of the following code snippets. In this way, there is no need to
    call split (`,`) repeatedly for each field extraction.
  prefs: []
  type: TYPE_NORMAL
- en: Joins
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the **Relational Database Management Systems** (**RDBMS**) world, joining
    multiple tables rows based on a key is a very common practice. When it comes to
    the NoSQL data stores, joining multiple tables became a real problem because many
    of the NoSQL data stores don't have support for the table joins. In the NoSQL
    world, redundancy is allowed. Whether a technology supports table joins or not,
    business use cases mandate joins of datasets based on keys all the time. Because
    of this, it is imperative to have the joins done in a batch mode in many of the
    use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Spark provides transformations to join multiple RDDs based on a key. This supports
    many use cases. These days there are many NoSQL data stores having connectors
    to talk to Spark. When working with such data stores, it is very simple to construct
    RDDs of data from multiple tables, do the join from Spark, and store the results
    back into the data stores in batch mode or even in near-to-real-time mode. Spark
    transformations are available for left outer join and right outer join, as well
    as full outer join.
  prefs: []
  type: TYPE_NORMAL
- en: The use cases selected for elucidating the join of multiple datasets using a
    key are given as follows.
  prefs: []
  type: TYPE_NORMAL
- en: The first dataset contains a retail banking master records summary with an account
    number, first name, and last name. The second dataset contains the retail banking
    account balance with an account number, and balance amount. The key on both of
    the datasets is the account number. Join the two datasets and create one dataset
    containing the account number, full name, and balance amount.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the Scala REPL prompt, try the following statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'All the statements given previously must be familiar by now, except the Spark
    transformation join. Similar to this transformation, the `leftOuterJoin`, `rightOuterJoin`,
    and `fullOuterJoin`are also available with the same usage pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Spark transformation** | **What it does** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `join(other, [numTasks])` | **Joins this RDD with the other RDD, and the
    elements are joined together based on the key. Suppose the original RDD is of
    the form (K,V1) and the second RDD is of the form (K,V2), then the join operation
    will produce tuples of the form (K, (V1,V2)) with all the pairs of each key.**
    |'
  prefs: []
  type: TYPE_TB
- en: 'At the Python REPL prompt, try the following statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: More actions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far, the focus was mainly on Spark transformations. Spark actions are also
    important. To get insight into some more important Spark actions, take the following
    use cases, continuing from where it was stopped in the preceding section''s use
    cases:'
  prefs: []
  type: TYPE_NORMAL
- en: From the list containing account numbers, names, and account balances, get the
    one that has the highest account balance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From the list containing account numbers, names, and account balances, get the
    top three having the highest account balance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Count the number of balance transaction records at an account level
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Count the total number of balance transaction records
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Print the name and account balance of all the accounts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate the total of the account balance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is a very common requirement to iterate through the elements in a collection,
    do some mathematical calculation on each of the elements, and at the end of it,
    use the result. The RDD is partitioned and distributed across worker nodes. If
    any normal variable is used for storing the cumulative result while iterating
    through the RDD elements, it may not yield the correct result. In such situations,
    instead of using regular variables, use Spark provided accumulators.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the Scala REPL prompt, try the following statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The following table captures the Spark actions that are introduced in this
    use case:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Spark action** | **What it does** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `first()` | **Returns the first element in the RDD.** |'
  prefs: []
  type: TYPE_TB
- en: '| `take(n)` | Returns an array of the first `n` elements from the RDD. |'
  prefs: []
  type: TYPE_TB
- en: '| `countByKey()` | Returns the count of elements by the key. If the RDD contains
    (K,V) pairs, this will return a dictionary of `(K, numOfValues)`. |'
  prefs: []
  type: TYPE_TB
- en: '| `count()` | Returns the number of elements in the RDD. |'
  prefs: []
  type: TYPE_TB
- en: '| `foreach(fn)` | Applies the function fn to each element in the RDD. In the
    preceding use case, Spark Accumulator is being used with `foreach(fn)`. |'
  prefs: []
  type: TYPE_TB
- en: 'At the Python REPL prompt, try the following statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Creating RDDs from files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, the focus of the discussion was on the RDD functionality and programming
    with RDDs. In all the preceding use cases, the RDD creation was done from the
    collection objects. But in the real-world use cases, the data will come from files
    stored in the local filesystems, and HDFS. Quite often, the data will come from
    NoSQL data stores such as Cassandra. It is possible to create RDDs by reading
    the contents from these data sources. Once RDD is created, then all the operations
    are uniform, as given in the preceding use cases. The data files coming out of
    the filesystems may be fixed width, comma-separated, or any other format. But
    the common pattern used for reading such data files is to read the data line by
    line and split the line to have the necessary separation of data items. In the
    case of data coming from other sources, the appropriate Spark connector program
    is to be used and the appropriate API for reading data is to be used.
  prefs: []
  type: TYPE_NORMAL
- en: Many third-party libraries are available to read the contents from various types
    of text files. For example, the Spark CSV library available from GitHub is a very
    useful one for creating RDDs from CSV files.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table captures the way text files are read from various sources,
    such as local filesystems, HDFS, and so on. As discussed earlier, the processing
    of the text file is up to the use case requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **File location** | **RDD creation** | **What it does** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Local filesystem | `val textFile = sc.textFile("README.md")` | **Creates
    an RDD by reading the contents of the file named** `README.md` **from the directory
    from where the Spark shell is invoked. Here, the RDD is of the type RDD[string]
    and the elements will be lines from the file.** |'
  prefs: []
  type: TYPE_TB
- en: '| HDFS | `val textFile = sc.textFile ("hdfs://<location in HDFS>")` | Creates
    an RDD by reading the contents of the file specified in the HDFS URL |'
  prefs: []
  type: TYPE_TB
- en: The most important aspect while reading the files from the local filesystem
    is that the file should be available in all the nodes of the Spark worker nodes.
    Apart from these two file locations given in the preceding table, any supported
    filesystem URI may be used.
  prefs: []
  type: TYPE_NORMAL
- en: Just like reading the contents from files in various filesystems, it is also
    possible to write the RDD onto files using the `saveAsTextFile`(path) Spark action.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: All the Spark application use cases discussed here are run on the appropriate
    language's REPL of Spark. When writing applications, they will be written in proper
    source code files. In the case of Scala and Java, the application code files have
    to be compiled, packaged, and run with proper library dependencies, and are typically
    built using maven or sbt. This will be covered in detail when designing data processing
    applications using Spark, in the last chapter of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Spark library stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark comes with a core data processing engine and a stack of libraries working
    on top of the core engine. It is very important to understand the concept of stacking
    libraries on top of the core framework.
  prefs: []
  type: TYPE_NORMAL
- en: All these libraries that are making use of the services provided by the core
    framework support the data abstractions offered by the core framework and much
    more. Before Spark came onto market, there were lots of independent open source
    products doing what the library stack in discussion here is now doing. The biggest
    disadvantage with these point products was their interoperability. They don't
    stack together well. They were implemented in different programming languages.
    The programming language of choice supported by these products, and the lack of
    uniformity in the APIs exposed by these products, were really challenging to get
    one application done with two or more such products. That is the relevance of
    the stack of libraries that work on top of Spark. They all work together with
    the same programming model. This helps organizations to standardize on the data
    processing toolset without vendorlock-in.
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark comes with the following stack of domain-specific libraries, and Figure
    8 gives a comprehensive picture of the whole ecosystem as seen by a developer:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Spark SQL**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spark Streaming**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spark MLlib**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spark GraphX**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Understanding the Spark library stack](img/image_02_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8
  prefs: []
  type: TYPE_NORMAL
- en: In any organization, structured data is still very widely used. The most ubiquitous
    data access mechanism with structured data is SQL. Spark SQL provides the capability
    to write SQL-like queries on top of the structured data abstraction called the
    DataFrame API. DataFrame and SQL go very well and support data coming from various
    sources, such as Hive, Avro, Parquet, JSON, and many more. Once the data is loaded
    into the Spark context, they can be operated as if they are all coming from the
    same source. In other words, if required, SQL-like queries can be used to join
    data coming from different sources, such as Hive and JSON. Another big advantage
    that Spark SQL and the DataFrame API bring onto the developers table is the ease
    of use and no need-to-know functional programming methods, which is a requirement
    to do programming with RDDs.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using Spark SQL and the DataFrame API, data can be read from various data sources
    and processed as if it is all coming from a unified source. Spark transformations
    and Spark actions support uniform programming interfaces. So the data source unification,
    API unification, and ability to use multiple programming languages to write data
    processing applications help the organizations to standardize on one data processing
    framework.
  prefs: []
  type: TYPE_NORMAL
- en: The data ingestion into the organizational data sinks is increasing every day.
    At the same time, the velocity at which data is getting ingested is also increasing.
    Spark Streaming provides the library to process the data that is ingested from
    various sources at a very high velocity.
  prefs: []
  type: TYPE_NORMAL
- en: In the past, data scientists had the challenge of building their own implementations
    of the machine learning algorithms and utilities in their programming language
    of choice. Quite often, such programming languages don't interoperate with the
    data processing toolset of the organization. Spark MLlib provides the unification
    process, where it comes with a lot of machine learning algorithms and utilities
    working on top of the Spark data processing engine.
  prefs: []
  type: TYPE_NORMAL
- en: The IoT applications, especially the social media applications, mandated the
    need to have data processing capabilities where the data fits into a graph-like
    structure. For example, the connections in LinkedIn, relationship between friends
    in Facebook, workflow applications, and many such use cases, make use of the graph
    abstraction extensively. Using the graph to do various computations requires very
    high data processing capabilities and employment of sophisticated algorithms.
    Spark GraphX library comes with an API for graphs and makes use of Spark's parallel
    computing paradigm.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are many Spark libraries available that are developed by the community
    for various purposes. Many such third-party library packages are featured in the
    site [http://spark-packages.org/](http://spark-packages.org/). The number of packages
    is growing day by day as the Spark user community is growing. When developing
    data processing applications in Spark, if there is a need to have a domain-specific
    library, it would be a good idea to check this site first and see whether anybody
    has already developed it.
  prefs: []
  type: TYPE_NORMAL
- en: Reference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For more information plese visit:[https://github.com/databricks/spark-csv](https://github.com/databricks/spark-csv)
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter discussed the basic programming model of Spark with its primary
    dataset abstraction RDDs. The creation of RDDs from various data sources, and
    processing of the data in RDDs using Spark transformations and Spark actions,
    were covered using Scala and Python APIs. All the important features of the Spark
    programming model were covered with the help of real-world use cases. This chapter
    also discussed the library stack that comes with Spark and what each one is doing.
    In summary, Spark comes with a very user-friendly programming model and in turn
    provides a very powerful data processing toolset.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will discuss the Dataset API and the DataFrame API. The Dataset
    API is going to be the new way of programming with Spark, while the DataFrame
    API deals with more structured data. Spark SQL is also introduced to manipulate
    structured data and show how that can be intermixed with any Spark data processing
    application.
  prefs: []
  type: TYPE_NORMAL
