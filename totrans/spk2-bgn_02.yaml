- en: Chapter 2. Spark Programming Model
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章 Spark编程模型
- en: '**Extract**, **Transform**, and **Load** (**ETL**) tools proliferated along
    with the growth of the data in organizations. The need to move data from one source
    to one or more destinations, processing it on the fly before it reaches its destination,
    were all the requirements of the time. Most of the time, these ETL tools were
    supporting only a few types of data, only a few types of data sources and destinations,
    and were closed to extension to allow them to support new data types and new sources
    and destinations. Because of these stringent limitations on the tools, sometimes
    even a one-step transformation process had to be done in multiple steps. These
    convoluted approaches mandated the need to have unnecessary wastage in terms of
    manpower, as well as other computing resources. The main argument from the commercial
    ETL vendors all the time remained the same, one size doesn''t fit all. So use
    *our* suite of tools instead of the point products available on the market. Many
    organizations got into vendor lock-in because of the profuse need to process data.
    Almost all the tools introduced before the year 2005 did not make use of the real
    power of the multi-core architecture of the computers if they supported running
    their tools on the commodity hardware. So, simple but voluminous data processing
    jobs took hours and sometimes even days to complete with these tools.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**提取**（**Extract**）、**转换**（**Transform**）和**加载**（**Load**）（**ETL**）工具随着组织数据量的增长而迅速发展。在数据到达目的地之前，需要将其从一种源移动到一种或多种目的地，并在途中进行处理，这些都是当时的需求。大多数情况下，这些ETL工具只支持少数类型的数据，只支持少数类型的数据源和目的地，并且难以扩展以支持新的数据类型、新的源和目的地。由于这些工具的严格限制，有时甚至一个步骤的转换过程也需要分多个步骤完成。这些复杂的方法要求在人力和其他计算资源方面产生不必要的浪费。商业ETL供应商的主要论点始终相同，即一种解决方案不能适用于所有情况。因此，使用*我们的*工具套件而不是市场上可用的点产品。由于对数据处理的大量需求，许多组织陷入了供应商锁定。几乎在2005年之前推出的所有工具都没有利用计算机多核架构的真正力量，如果它们支持在通用硬件上运行其工具。因此，使用这些工具进行简单但数据量大的数据处理作业需要数小时，有时甚至需要数天才能完成。'
- en: Spark became an instant hit in the market because of its ability to process
    a huge amount of data types and a growing number of data sources and data destinations.
    The most important and basic data abstraction Spark provides is the **resilient
    distributed dataset** (**RDD**). As discussed in the previous chapter, Spark supports
    distributed processing on a cluster of nodes. The moment there is a cluster of
    nodes, there is a good chance that when the data processing is going on, some
    of the nodes can die. When such failures happen, the framework should be capable
    of coming out of such failures. Spark is designed to do that and that is what
    the *resilient* part in the RDD signifies. If there is a huge amount of data to
    be processed and there are nodes available in the cluster, the framework should
    have the capability to split the big dataset into smaller chunks and distribute
    them to be processed on more than one node in a cluster, in parallel. Spark is
    capable of doing that and that is what the *distributed* part in the RDD signifies.
    In other words, Spark is designed from the ground up to have its basic dataset
    abstraction capable of getting split into smaller pieces deterministically and
    distributed to more than one node in the cluster for parallel processing, while
    elegantly handling the failures in the nodes.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Spark能够处理大量数据类型以及不断增长的数据源和数据目的地，它在市场上迅速走红。Spark提供的重要且基本的数据抽象是**弹性分布式数据集**（**RDD**）。正如前一章所讨论的，Spark支持在节点集群上的分布式处理。一旦有节点集群，在数据处理过程中，某些节点可能会死亡。当发生此类故障时，框架应该能够从这些故障中恢复。Spark正是为此而设计的，这就是RDD中的*弹性*部分所表示的含义。如果需要处理大量数据，并且集群中有可用节点，框架应该具备将大数据集拆分成更小的数据块并将它们并行分配到集群中多个节点进行处理的
    capability。Spark能够做到这一点，这就是RDD中的*分布式*部分所表示的含义。换句话说，Spark从一开始就被设计成其基本数据抽象能够确定性地拆分成更小的部分，并分布到集群中的多个节点进行并行处理，同时优雅地处理节点故障。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Functional programming with Spark
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark进行函数式编程
- en: Spark RDD
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark RDD
- en: Data transformations and actions
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据转换和操作
- en: Spark monitoring
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark监控
- en: Spark programming basics
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark编程基础
- en: Creating RDDs from files
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从文件创建RDD
- en: Spark libraries
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark库
- en: Functional programming with Spark
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Spark进行函数式编程
- en: The mutation of objects at run time, and the inability to get consistent results
    from a program or function because of the side effect that the program logic creates
    makes many applications very complex. If the functions in programming languages
    start behaving exactly like mathematical functions in such a way that the output
    of the function depends only on the inputs, that gives lots of predictability
    to applications. The computer programming paradigm giving lots of emphasis to
    the process of building such functions and other elements based on that, and using
    those functions just in the way that any other data types are being used, is popularly
    known as the functional programming paradigm. Out of the JVM-based programming
    languages, Scala is one of the most important ones that has very strong functional
    programming capabilities without losing any object orientation. Spark is written
    predominantly in Scala. Because of that itself, Spark has taken lots of very good
    concepts from Scala.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 运行时对象的突变，以及由于程序逻辑产生的副作用而无法从程序或函数中获得一致结果，使得许多应用程序非常复杂。如果编程语言中的函数开始像数学函数那样行为，即函数的输出仅取决于输入，这将为应用程序提供很多可预测性。计算机编程范式非常重视基于此构建函数和其他元素的过程，并且像使用其他数据类型一样使用这些函数，这种范式被称为函数式编程范式。在基于JVM的编程语言中，Scala是其中最重要的之一，它具有非常强大的函数式编程能力，同时不失对象导向性。Spark主要用Scala编写。正因为如此，Spark从Scala中吸取了许多非常好的概念。
- en: Understanding Spark RDD
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解Spark RDD
- en: The most important feature that Spark took from Scala is the ability to use
    functions as parameters to the Spark transformations and Spark actions. Quite
    often, the RDD in Spark behaves just like a collection object in Scala. Because
    of that, some of the data transformation method names of Scala collections are
    used in Spark RDD to do the same thing. This is a very neat approach and those
    who have expertise in Scala will find it very easy to program with RDDs. We will
    see a few important features in the following sections.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Spark从Scala继承的最重要特性是能够将函数用作Spark转换和Spark操作的参数。在Spark中，RDD的行为常常就像Scala中的集合对象。正因为如此，Scala集合的一些数据转换方法名称在Spark
    RDD中被用来执行相同的功能。这是一个非常整洁的方法，那些精通Scala的人会发现使用RDD编程非常容易。在接下来的几节中，我们将看到一些重要特性。
- en: Spark RDD is immutable
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark RDD 是不可变的
- en: There are some strong rules based on which an RDD is created. Once an RDD is
    created, intentionally or unintentionally, it cannot be changed. This gives another
    insight into the construction of an RDD. Because of that, when the nodes processing
    some part of an RDD die, the driver program can recreate those parts and assign
    the task of processing it to another node, ultimately, completing the data processing
    job successfully.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: RDD的创建基于一些强规则。一旦RDD被创建，无论是故意还是无意，都不能对其进行更改。这为我们提供了关于RDD构建的另一个见解。正因为如此，当处理RDD某一部分的节点死亡时，驱动程序可以重新创建这些部分，并将处理这些部分的任务分配给另一个节点，最终成功完成数据处理工作。
- en: Since the RDD is immutable, splitting a big one to smaller ones, distributing
    them to various worker nodes for processing, and finally compiling the results
    to produce the final result can be done safely without worrying about the underlying
    data getting changed.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 RDD 是不可变的，因此可以将一个大RDD拆分成更小的RDD，将它们分发到各个工作节点进行处理，最后编译结果以生成最终结果，可以安全地进行，无需担心底层数据发生变化。
- en: Spark RDD is distributable
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark RDD 是可分发的
- en: If Spark is run in a cluster mode where there are multiple worker nodes available
    to take the tasks, all these nodes will have different execution contexts. The
    individual tasks are distributed and run on different JVMs. All these activities
    of a big RDD getting divided into smaller chunks, getting distributed for processing
    to the worker nodes, and finally, assembling the results back, are completely
    hidden from the users.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果Spark以集群模式运行，其中存在多个可供执行任务的worker节点，那么所有这些节点将具有不同的执行上下文。单个任务被分发并在不同的JVM上运行。一个大RDD的所有活动，包括将其分成更小的块、分发到工作节点进行处理，以及最终组装结果，对用户来说都是完全隐藏的。
- en: Spark has its own mechanism for recovering from the system faults and other
    forms of errors which occur during the data processing and hence this data abstraction
    is highly resilient.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Spark具有从系统故障和其他数据处理过程中发生的错误中恢复的机制，因此这种数据抽象具有高度的鲁棒性。
- en: Spark RDD lives in memory
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark RDD 存在于内存中
- en: Spark does keep all the RDDs in the memory as much as it can. Only in rare situations,
    where Spark is running out of memory or if the data size is growing beyond the
    capacity, is it written to disk. Most of the processing on RDD happens in the
    memory, and that is the reason why Spark is able to process the data at a lightning
    fast speed.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Spark尽可能地将所有RDD保留在内存中。只有在Spark内存不足或数据大小超出容量时，才会将其写入磁盘。RDD的大部分处理都在内存中进行，这也是Spark能够以闪电般的速度处理数据的原因。
- en: Spark RDD is strongly typed
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark RDD是强类型的
- en: Spark RDD can be created using any supported data types. These data types can
    be Scala/Java supported intrinsic data types or custom created data types such
    as your own classes. The biggest advantage coming out of this design decision
    is the freedom from runtime errors. If it is going to break because of a data
    type issue, it will break during compile time.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Spark RDD可以使用任何支持的数据类型创建。这些数据类型可以是Scala/Java支持的内建数据类型，也可以是自定义创建的数据类型，例如您自己的类。从这个设计决策中产生的最大优势是避免了运行时错误。如果因为数据类型问题而要崩溃，它将在编译时崩溃。
- en: 'The following table captures the structure of an RDD containing tuples of a
    retail bank account data. It is of the type RDD[(string, string, string, double)]:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格捕捉了一个包含零售银行账户数据的元组的RDD的结构。它属于类型RDD[(string, string, string, double)]：
- en: '| **AccountNo** | **FirstName** | **LastName** | **AccountBalance** |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| **AccountNo** | **FirstName** | **LastName** | **AccountBalance** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| SB001 | John | Mathew | 250.00 |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| SB001 | John | Mathew | 250.00 |'
- en: '| SB002 | Tracy | Mason | 450.00 |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| SB002 | Tracy | Mason | 450.00 |'
- en: '| SB003 | Paul | Thomson | 560.00 |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| SB003 | Paul | Thomson | 560.00 |'
- en: '| SB004 | Samantha | Grisham | 650.00 |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| SB004 | Samantha | Grisham | 650.00 |'
- en: '| SB005 | John | Grove | 1000.00 |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| SB005 | John | Grove | 1000.00 |'
- en: 'Suppose this RDD is going through a process to calculate the total amount of
    all these accounts in a cluster of three nodes, N1, N2, and N3; it can be split
    and distributed for something such as parallelizing the data processing. The following
    table contains the elements of the RDD[(string, string, string, double)] distributed
    to node N1 for processing:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 假设这个RDD正在通过一个过程来计算三个节点N1、N2和N3组成的集群中所有这些账户的总金额，它可以被分割并分配用于数据处理的并行化。以下表格包含了分配给节点N1进行处理的RDD[(string,
    string, string, double)]的元素：
- en: '| **AccountNo** | **FirstName** | **LastName** | **AccountBalance** |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| **AccountNo** | **FirstName** | **LastName** | **AccountBalance** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| SB001 | John | Mathew | 250.00 |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| SB001 | John | Mathew | 250.00 |'
- en: '| SB002 | Tracy | Mason | 450.00 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| SB002 | Tracy | Mason | 450.00 |'
- en: 'The following table contains the elements of the RDD[(string, string, string,
    double)] distributed to node N2 for processing:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格包含了分配给节点N2进行处理的RDD[(string, string, string, double)]的元素：
- en: '| **AccountNo** | **FirstName** | **LastName** | **AccountBalance** |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| **AccountNo** | **FirstName** | **LastName** | **AccountBalance** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| SB003 | Paul | Thomson | 560.00 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| SB003 | Paul | Thomson | 560.00 |'
- en: '| SB004 | Samantha | Grisham | 650.00 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| SB004 | Samantha | Grisham | 650.00 |'
- en: '| SB005 | John | Grove | 1000.00 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| SB005 | John | Grove | 1000.00 |'
- en: On node N1, the summation process happens and the result is returned to the
    Spark driver program. Similarly, on node N2, the summation process happens, the
    result is returned to the Spark driver program, and the final result is computed.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在节点N1上，进行求和过程，并将结果返回给Spark驱动程序。同样，在节点N2上，进行求和过程，将结果返回给Spark驱动程序，并计算最终结果。
- en: Spark has very deterministic rules on splitting a big RDD into smaller chunks
    for distribution to various nodes and because of that, even if something happens
    to, say, node N1, Spark knows how to recreate exactly the chunk that was lost
    in the node N1 and continue with the data processing operation by sending the
    same payload to node N3.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Spark在将大RDD分割成更小的块以分配到各个节点方面有非常确定的规则，因此，即使节点N1发生故障，Spark也知道如何重新创建节点N1中丢失的块，并通过向节点N3发送相同的有效载荷来继续数据处理操作。
- en: 'Figure 1 captures the essence of the process:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图1捕捉了该过程的精髓：
- en: '![Spark RDD is strongly typed](img/image_02_002.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![Spark RDD is strongly typed](img/image_02_002.jpg)'
- en: Figure 1
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图1
- en: Tip
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Spark does a lot of processing in its driver memory and in the executor memory
    on the cluster nodes. Spark has various parameters that can be configured and
    fine-tuned so that the required resources are made available before the processing
    starts.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Spark在其驱动器内存和集群节点上的执行器内存中进行大量处理。Spark有许多可配置和微调的参数，以确保在处理开始之前提供所需的资源。
- en: Data transformations and actions with RDDs
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据转换和RDD操作
- en: 'Spark does the data processing using the RDDs. From the relevant data source
    such as text files and NoSQL data stores, data is read to form the RDDs. On such
    an RDD, various data transformations are performed and finally, the result is
    collected. To be precise, Spark comes with Spark transformations and Spark actions
    that act upon RDDs. Let us take the following RDD capturing a list of retail banking
    transactions, which is of the type RDD[(string, string, double)]:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Spark使用RDD进行数据处理。从相关数据源，如文本文件和NoSQL数据存储中读取数据以形成RDD。在这样一个RDD上执行各种数据转换，最后收集结果。更准确地说，Spark自带Spark转换和作用于RDD的Spark操作。让我们看一下以下RDD，它捕获了零售银行交易列表，其类型为RDD[(string,
    string, double)]：
- en: '| **AccountNo** | **TranNo** | **TranAmount** |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| **AccountNo** | **TranNo** | **TranAmount** |'
- en: '| --- | --- | --- |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| SB001 | TR001 | 250.00 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| SB001 | TR001 | 250.00 |'
- en: '| SB002 | TR004 | 450.00 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| SB002 | TR004 | 450.00 |'
- en: '| SB003 | TR010 | 120.00 |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| SB003 | TR010 | 120.00 |'
- en: '| SB001 | TR012 | -120.00 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| SB001 | TR012 | -120.00 |'
- en: '| SB001 | TR015 | -10.00 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| SB001 | TR015 | -10.00 |'
- en: '| SB003 | TR020 | 100.00 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| SB003 | TR020 | 100.00 |'
- en: 'To calculate the account level summary of the transactions from the RDD of
    the form `(AccountNo,TranNo,TranAmount)`:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算从形式为`(AccountNo,TranNo,TranAmount)`的RDD中交易的账户级别摘要：
- en: First it has to be transformed to the form of key-value pairs `(AccountNo,TranAmount)`,
    where `AccountNo` is the key but there will be multiple elements with the same
    key.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，它必须转换为键值对的形式`(AccountNo,TranAmount)`，其中`AccountNo`是键，但将会有多个具有相同键的元素。
- en: On this key, do a summation operation on `TranAmount`, resulting in another
    RDD of the form (AccountNo,TotalAmount),where every AccountNo will have only one
    element and TotalAmount is the sum of all the TranAmount for the given AccountNo.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个键上对`TranAmount`进行求和操作，结果是一个形式为(AccountNo,TotalAmount)的RDD，其中每个AccountNo将只有一个元素，TotalAmount是给定AccountNo的所有TranAmount的总和。
- en: Now sort the key-value pairs on the `AccountNo` and store the output.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在按`AccountNo`对键值对进行排序并存储输出。
- en: In the whole process described, all are Spark transformations except the storing
    of the output. Storing of the output is a **Spark action**. Spark does all these
    operations on a need-to-do basis. Spark does not act when a Spark transformation
    is applied. The real act happens when the first Spark action in the chain is called.
    Then it diligently applies all the preceding Spark transformations in order, and
    then does the first encountered Spark action. This is based on the concept called
    **Lazy Evaluation**.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个过程中描述的，所有都是Spark转换，除了存储输出。存储输出是一个**Spark操作**。Spark根据需要执行所有这些操作。Spark在应用Spark转换时不采取行动。真正的行动发生在链中的第一个Spark操作被调用时。然后它勤奋地按顺序应用所有先前的Spark转换，然后执行第一个遇到的Spark操作。这是基于称为**延迟评估**的概念。
- en: Note
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: In the programming language context of declaring and using variables, *Lazy
    Evaluation* means that a variable is evaluated only when it is first used in the
    program.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在编程语言中声明和使用变量的上下文中，*延迟评估*意味着变量仅在程序中首次使用时进行评估。
- en: 'Apart from this action of storing the output to disk, there are many other
    possible Spark actions including, but not limited to, some of the ones given in
    the following list:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 除了将输出存储到磁盘上的操作之外，还有许多其他可能的Spark操作，包括但不限于以下列表中给出的一些：
- en: Collecting all the contents in the resultant RDD to an array residing in the
    driver program
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将结果RDD中的所有内容收集到驱动程序中的数组
- en: Counting the number of elements in the RDD
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算RDD中的元素数量
- en: Counting the number of elements for each key in the RDD element
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在RDD元素中计算每个键的元素数量
- en: Taking the first element in the RDD
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 取RDD中的第一个元素
- en: Taking a given number of elements from the RDD commonly used for top N reports
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从RDD中取出常用作Top N报告的给定数量的元素
- en: Taking a sample of elements from the RDD
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从RDD中取样本元素
- en: Iterating through all the elements in the RDD
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 遍历RDD中的所有元素
- en: In this example, many transformations are done on various RDDs that get created
    on the fly till the process is completed. In other words, whenever a transformation
    is done on an RDD, a new RDD gets created. This is because RDDs are inherently
    immutable. These RDDs that are getting created at the end of each transformation
    can be saved for future reference, or they will go out of scope eventually.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，在完成过程之前，对各种动态创建的RDD进行了许多转换。换句话说，每当对RDD进行转换时，就会创建一个新的RDD。这是因为RDD本质上是不可变的。在每个转换结束时创建的这些RDD可以保存以供将来参考，或者最终将超出作用域。
- en: To summarize, the process of creating one or more RDDs and applying transformations
    and actions on them is a very common usage pattern seen ubiquitously in Spark
    applications.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，创建一个或多个 RDD 并在它们上应用转换和动作的过程是 Spark 应用程序中普遍存在的非常常见的使用模式。
- en: Note
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The table referred in the preceding data transformation example contains the
    values in an RDD of type the RDD[(string, string, double)]. In this RDD, there
    are multiple elements, and each one is a tuple of the type (string, string, double).
    It is very common among programmers and the user community, for easy reference
    and conveying ideas, that the term `record` is being used to refer one to element
    in the RDD. In Spark RDD there is no concept of records, rows and columns. In
    other words the term `record` is mistakenly used synonymously to an element in
    the RDD, which may be a complex data type such as a tuple or a non-scalar data
    type. In this book, this practice is highly refrained to use the correct terms.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面提到的数据转换示例中，所引用的表格包含了一个类型为 RDD[(string, string, double)] 的 RDD 中的值。在这个 RDD
    中，有多个元素，每个元素都是一个类型为 (string, string, double) 的元组。在程序员和用户社区中，为了便于引用和传达思想，经常使用术语
    `record` 来指代 RDD 中的一个元素。在 Spark RDD 中，没有记录、行和列的概念。换句话说，术语 `record` 错误地被用来同义于 RDD
    中的一个元素，该元素可能是一个复杂的数据类型，如元组或非标量数据类型。在这本书中，这种做法被严格避免，以使用正确的术语。
- en: In Spark there are a good amount of Spark transformations available. These are
    really powerful because most of these take functions as input parameters to do
    the transformation. In other words, these transformations act on the RDD based
    on the functions that are defined and supplied by the user. This becomes even
    more powerful with Spark's uniform programming model. Whether the programming
    language of choice is Scala, Java, Python, or R, the way Spark transformations
    and Spark actions are used is similar. This lets the organizations choose their
    programming language of choice.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark 中，有大量的 Spark 转换可用。这些转换非常强大，因为其中大多数将函数作为输入参数来进行转换。换句话说，这些转换根据用户定义和提供的函数在
    RDD 上进行操作。结合 Spark 的一致编程模型，这种能力变得更加强大。无论选择的编程语言是 Scala、Java、Python 还是 R，Spark
    转换和 Spark 动作的使用方式都是相似的。这使得组织可以选择他们偏好的编程语言。
- en: In Spark, even though the number of Spark actions are limited in number, they
    are really powerful, and users can write their own Spark actions if there is a
    need. There are many Spark connector programs that are available in the market,
    mainly to read and write data from various data stores. These connector programs
    are designed and developed either by the user community or by the data store vendors
    themselves to have connectivity to Spark. In addition to the available Spark actions,
    they may define their own actions to supplement existing sets of Spark actions.
    For example, the Spark Cassandra Connector is used to connect to Cassandra from
    Spark. This has an action `saveToCassandra`.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark 中，尽管 Spark 动作的数量有限，但它们非常强大，如果需要，用户可以编写自己的 Spark 动作。市场上有很多 Spark 连接器程序，主要用于从各种数据存储中读取和写入数据。这些连接器程序是由用户社区或数据存储供应商设计和开发的，以便与
    Spark 连接。除了可用的 Spark 动作外，它们还可以定义自己的动作来补充现有的 Spark 动作集。例如，Spark Cassandra 连接器用于从
    Spark 连接到 Cassandra。它有一个动作 `saveToCassandra`。
- en: Monitoring with Spark
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Spark 进行监控
- en: The previous chapter covered the details of the installation and development
    tool setup that is required for developing and running data processing applications
    using Spark. In most of the real-world applications, the Spark applications can
    become very complex with a really huge **directed acyclic graph**  (**DAG**) of
    Spark transformations and Spark actions. Spark comes with really powerful monitoring
    tools for monitoring the jobs that are running in a given Spark ecosystem. The
    monitoring doesn't start automatically.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 前一章介绍了使用 Spark 开发和运行数据处理应用程序所需的安装和开发工具设置细节。在大多数实际应用中，Spark 应用程序可能会变得非常复杂，拥有一个由
    Spark 转换和 Spark 动作构成的真正巨大的**有向无环图**（**DAG**）。Spark 提供了强大的监控工具来监控给定 Spark 生态系统中的作业。监控不会自动启动。
- en: Tip
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: Note that this is a completely optional step for running Spark applications.
    If enabled, it will give a very good insight into the way the Spark applications
    are run. Discretion has to be used to enable this in production environments,
    as it can affect the response time of the applications.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这是运行 Spark 应用程序的完全可选步骤。如果启用，它将非常深入地了解 Spark 应用程序的运行方式。在生产环境中启用此功能时需要谨慎，因为它可能会影响应用程序的响应时间。
- en: 'First of all, there are some configuration changes to be made. The event logging
    mechanism should be turned on. For this, take the following steps:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，需要进行一些配置更改。事件日志机制应该开启。为此，请执行以下步骤：
- en: '[PRE0]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Once the preceding steps are completed, edit the newly created `spark-defaults.conf`
    file to have the following properties:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 完成前面的步骤后，编辑新创建的 `spark-defaults.conf` 文件，使其具有以下属性：
- en: '[PRE1]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Tip
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Once the preceding steps are completed, make sure that the previously used log
    directory exists in the filesystem.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 完成前面的步骤后，请确保之前使用的日志目录在文件系统中存在。
- en: 'Apart from the preceding configuration file changes, there are many properties
    in that configuration file that can be changed to fine tune the Spark runtime.
    The most important among them that is used frequently is the Spark driver memory.
    If the applications are dealing with a huge amount of data, it is a good idea
    to customize this property `spark.driver.memory`to have a higher value. Then run
    the following commands to start the Spark master:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 除了前面的配置文件更改之外，该配置文件中还有许多属性可以更改以微调 Spark 运行时。其中最重要的是经常使用的 Spark 驱动内存。如果应用程序处理大量数据，将此属性
    `spark.driver.memory` 定制为更高的值是一个好主意。然后运行以下命令以启动 Spark 主机：
- en: '[PRE2]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Once the preceding steps are completed, make sure that the Spark web **user
    interface** (**UI**) is starting up by going to `http://localhost:8080/`. The
    assumption here is that there is no other application running in the `8080` port.
    If for some reason, there is a need to run this application on a different port,
    the command line option `--webui-port <PORT>` can be used in the script while
    starting the web user interface.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 完成前面的步骤后，请确保 Spark 网页 **用户界面**（**UI**）正在启动，方法是通过访问 `http://localhost:8080/`。这里的假设是
    `8080` 端口上没有运行其他应用程序。如果出于某种原因需要在不同的端口上运行此应用程序，可以在启动网页用户界面时在脚本中使用命令行选项 `--webui-port
    <PORT>`。
- en: 'The web UI should look something similar to that shown in Figure 2:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 网页 UI 应该看起来与图 2 中显示的类似：
- en: '![Monitoring with Spark](img/image_02_003.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Spark 进行监控](img/image_02_003.jpg)'
- en: Figure 2
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2
- en: The most important information to be noted in the preceding figure is the fully-qualified
    Spark master URL (not the REST URL). It is going to be used again and again for
    many of the hands-on exercises that are going to be discussed in this book. The
    URL can change from system to system and the DNS settings. Also note that throughout
    this book, for all the hands-on exercises, Spark standalone deployment is used,
    which is the easiest among the deployments to get started with a single computer.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中需要注意的最重要信息是完全限定的 Spark 主机 URL（而不是 REST URL）。它将在本书中讨论的许多动手练习中反复使用。URL 可能会因系统而异，以及
    DNS 设置。此外，请注意，在本书中，对于所有动手练习，都使用 Spark 独立部署，这是在单台计算机上开始部署中最容易的。
- en: Tip
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: These Spark application monitoring steps are given now to make the readers familiar
    with the toolset that Spark provides. Those who are familiar with these tools
    or those who are very confident of the application behavior need not need the
    help of these tools. But to understand the concepts, debugging, and some visualizations
    of the processes, these tools definitely provide immense help.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在提供这些 Spark 应用程序监控步骤，以便让读者熟悉 Spark 提供的工具集。那些熟悉这些工具或对应用程序行为非常有信心的人不需要这些工具的帮助。但为了理解概念、调试以及一些过程的可视化，这些工具确实提供了巨大的帮助。
- en: 'From the Spark web UI that is given in Figure 2, it can be noted that there
    are no worker nodes available to do any task, and there are no running applications.
    The following steps capture the instructions to start the worker nodes. Note how
    the Spark master URL is being used while starting the worker node:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 从图 2 中给出的 Spark 网页 UI 可以看出，没有可用的工作节点来执行任何任务，也没有正在运行的应用程序。以下步骤记录了启动工作节点的指令。注意在启动工作节点时如何使用
    Spark 主机 URL：
- en: '[PRE3]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Once the worker node is started, in the Spark web UI, the newly started worker
    node is displayed. The  `$SPARK_HOME/conf/slaves.template`template captures the
    default worker nodes that will be started with the invocation of the preceding
    command.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦启动工作节点，在 Spark 网页界面中，将显示新启动的工作节点。`$SPARK_HOME/conf/slaves.template` 模板捕获了将使用前面命令启动的默认工作节点。
- en: Note
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: If additional worker nodes are required, copy the `slaves.template` file to
    name it to slaves and have the entries captured in there. When a spark-shell,
    pyspark, or sparkR is started, instructions can be given to it to use a given
    Spark master. This is useful when there is a need to run Spark applications or
    statements on a remote Spark cluster or against a given Spark master. If nothing
    is given, the Spark applications will run in the local mode.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要额外的工作节点，将 `slaves.template` 文件复制并命名为 `slaves`，并将条目捕获在其中。当启动 spark-shell、pyspark
    或 sparkR 时，可以给出指令让它使用指定的 Spark 主节点。这在需要运行 Spark 应用程序或语句在远程 Spark 集群或针对给定的 Spark
    主节点时非常有用。如果没有给出任何内容，Spark 应用程序将在本地模式下运行。
- en: '[PRE4]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The Spark web UI will look similar to that shown in Figure 3 once a worker
    node is started successfully. After this, if an application is run with the preceding
    Spark master URL, even that application''s details will be displayed in the Spark
    web UI. A detailed coverage of the applications is to follow in this chapter.
    Use the following scripts to stop the workers and master processes:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦成功启动工作节点，Spark 网页界面将类似于图 3 所示。在此之后，如果使用前面的 Spark 主节点 URL 运行应用程序，该应用程序的详细信息也将显示在
    Spark 网页界面中。本章将详细说明应用程序。使用以下脚本停止工作节点和主进程：
- en: '[PRE5]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![Monitoring with Spark](img/image_02_004.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Spark 进行监控](img/image_02_004.jpg)'
- en: Figure 3
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3
- en: The basics of programming with Spark
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 编程的基础知识
- en: Spark programming revolves around RDDs. In any Spark application, the input
    data to be processed is taken to create an appropriate RDD. To begin with, start
    with the most basic way of creating an RDD, which is from a list. The input data
    used for this `hello world` kind of application is a small collection of retail
    banking transactions. To explain the core concepts, only some very elementary
    data items have been picked up. The transaction records contain account numbers
    and transaction amounts.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 编程围绕 RDD 进行。在任何 Spark 应用程序中，要处理的数据的输入被用来创建一个适当的 RDD。首先，从最基本创建 RDD 的方式开始，即从一个列表开始。用于此类
    `hello world` 类型的应用程序的输入数据是一小批零售银行业务交易。为了解释核心概念，只选取了一些非常基础的数据项。交易记录包含账户号码和交易金额。
- en: Tip
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: In these use cases and all the upcoming use cases in the book, if the term record
    is used, that will be in the business or use case context.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些用例以及书中所有即将到来的用例中，如果使用了术语“记录”，那么它将处于业务或用例的上下文中。
- en: 'The use cases selected for elucidating the Spark transformations and Spark
    actions here are given as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里用于阐明 Spark 转换和 Spark 动作的用例如下所示：
- en: The transaction records are coming as comma-separated values.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 交易记录以逗号分隔值的形式出现。
- en: Filter out only the good transaction records from the list. The account number
    should start with `SB` and the transaction amount should be greater than zero.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从列表中过滤出只有好的交易记录。账户号码应以 `SB` 开头，交易金额应大于零。
- en: Find all the high value transaction records with a transaction amount greater
    than 1000.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找出所有交易金额大于 1000 的高价值交易记录。
- en: Find all the transaction records where the account number is bad.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找出所有账户号码不良的交易记录。
- en: Find all the transaction records where the transaction amount is less than or
    equal to zero.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找出所有交易金额小于或等于零的交易记录。
- en: Find a combined list of all the bad transaction records.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找出所有不良交易记录的合并列表。
- en: Find the total of all the transaction amounts.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找出所有交易金额的总和。
- en: Find the maximum of all the transaction amounts.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找出所有交易金额的最大值。
- en: Find the minimum of all the transaction amounts.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找出所有交易金额的最小值。
- en: Find all the good account numbers.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找出所有良好的账户号码。
- en: 'The approach that is going to be followed throughout the book for any application
    that is going to be developed begins with the Spark REPL for the appropriate language.
    Start the Scala REPL for Spark and make sure that it starts without any errors
    and the prompt is seen. For this application, we will enable monitoring to learn
    how to do that and use it along with the development process. Other than explicitly
    starting the Spark master and the slaves separately, Spark comes with a script
    that will start both of these together using a single script. Then, fire up the
    Scala REPL with the Spark master URL:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，对于将要开发的任何应用程序，所遵循的方法是从相应语言的 Spark REPL 开始。启动 Spark 的 Scala REPL 并确保它没有错误启动，并且可以看到提示符。对于这个应用程序，我们将启用监控以学习如何进行监控并在开发过程中使用它。除了明确启动
    Spark 主节点和从节点外，Spark 还附带一个脚本，可以使用单个脚本同时启动这两个节点。然后，使用 Spark 主节点 URL 启动 Scala REPL：
- en: '[PRE6]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'At the Scala REPL prompt, try the following statements. The output of the statements
    is given in bold. Note that `scala>` is the Scala REPL prompt:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Scala REPL 提示符下，尝试以下语句。语句的输出以粗体显示。注意 `scala>` 是 Scala REPL 的提示符：
- en: '[PRE7]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'All the preceding statements fall into one category except the first RDD creation
    and two function value definitions. They are all Spark transformations. Here is
    the step-by-step detail capturing what has been done so far:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的前述语句都属于一个类别，除了第一个 RDD 创建和两个函数值定义之外。它们都是 Spark 转换。以下是逐步详细说明到目前为止所做的工作：
- en: The value `acTransList` is the array containing the comma separated transaction
    records.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`acTransList` 的值是包含逗号分隔的交易记录的数组。'
- en: The value `acTransRDD` is the RDD created out of the array where `sc` is the
    Spark context or the Spark driver and the RDD is created in a parallelized way
    so that the RDD elements can form a distributed dataset. In other words, an instruction
    is given to the Spark driver to form a parallel collection or RDD from the given
    collection of values.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`acTransRDD` 的值是从数组中创建的 RDD，其中 `sc` 是 Spark 上下文或 Spark 驱动程序，RDD 以并行化方式创建，以便
    RDD 元素可以形成一个分布式数据集。换句话说，向 Spark 驱动程序下达了一条指令，从给定的值集合中形成一个并行集合或 RDD。'
- en: The value `goodTransRecords` is the RDD created from `acTransRDD`after filtering
    the conditions transaction amount is > 0 and the account number starts with `SB`.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`goodTransRecords` 的值是从 `acTransRDD` 中创建的 RDD，在过滤条件交易金额大于 0 且账户号码以 `SB` 开头后得到。'
- en: The value `highValueTransRecords` is the RDD created from `goodTransRecords`after
    filtering the conditions transaction amount is > 1000.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`highValueTransRecords` 的值是从 `goodTransRecords` 中创建的 RDD，在过滤条件交易金额大于 1000 后得到。'
- en: The next two statements are storing the function definitions in a Scala value
    for easy reference later.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来的两个语句是将函数定义存储在 Scala 值中，以便以后方便引用。
- en: The values `badAmountRecords` and `badAccountRecords` are RDDs created from
    `acTransRDD` to filter the bad records containing the wrong transaction amount
    and invalid account number, respectively.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`badAmountRecords` 和 `badAccountRecords` 的值是从 `acTransRDD` 中创建的 RDD，分别用于过滤包含错误交易金额和无效账户号码的坏记录。'
- en: The value `badTransRecords` contains the union of the elements of both of the
    `badAmountRecords` and `badAccountRecords` RDDs.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`badTransRecords` 包含了 `badAmountRecords` 和 `badAccountRecords` 两个 RDD 元素的并集。'
- en: The Spark web UI for this application so far will not show anything at this
    point because only Spark transformations have been executed. The real activity
    will start only after the first Spark action is executed.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，这个应用的 Spark Web UI 不会显示任何内容，因为到目前为止只执行了 Spark 转换。真正的活动只有在执行第一个 Spark 动作后才会开始。
- en: 'The following statements are the continuation of the already executed statements:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 以下语句是已执行语句的延续：
- en: '[PRE8]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'All the preceding statements did one thing, which is execute a Spark action
    on the RDDs *defined* earlier. All the evaluations of the RDDs happened only when
    a Spark action was called on those RDDs. The following statements are doing some
    of the calculations on the RDDs:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的前述语句都做了一件事情，那就是在之前定义的 RDD 上执行 Spark 动作。所有 RDD 的评估只在调用这些 RDD 上的 Spark 动作时发生。以下语句正在对
    RDD 进行一些计算：
- en: '[PRE9]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The preceding numbers calculated the sum, maximum and minimum, of all transaction
    amounts from the good records. In all the preceding transformations, the transaction
    records are processed one at a time. From those records, the account number and
    transaction amount are extracted and processed. It was done like that because
    the use case requirement was like that. Now the comma-separated values in each
    transaction record are split without looking at whether it is an account number
    or a transaction amount. The resulting RDD will contain a collection with all
    these mixed up. Out of that, if the elements starting with `SB` are picked up,
    that will result in good account numbers. The following statements are going to
    do that:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的数字计算了所有良好记录中交易金额的总和、最大值和最小值。在所有前面的转换中，交易记录是逐个处理的。从这些记录中，提取并处理账户号码和交易金额。之所以这样做，是因为用例要求是这样的。现在，在交易记录中的逗号分隔值被拆分，而不考虑它是账户号码还是交易金额。结果RDD将包含一个包含所有这些混合元素的集合。从这些元素中，如果选择以`SB`开头的元素，将得到良好的账户号码。以下语句将执行此操作：
- en: '[PRE10]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now at this point, if the Spark web UI is opened, unlike what is seen in Figure
    3, one difference can be noticed. Since some Spark actions have been done, an
    application entry will show up. Since the Scala REPL of Spark is still running,
    it is shown in the list of applications that are still running. The following
    Figure 4 captures that:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在这个时候，如果打开Spark Web UI，与图3中看到的不同，可以注意到一个差异。由于已经执行了一些Spark操作，将显示一个应用程序条目。由于Spark的Scala
    REPL仍在运行，它显示在仍在运行的应用程序列表中。以下图4捕捉了这一点：
- en: '![The basics of programming with Spark](img/image_02_005.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![Spark编程基础](img/image_02_005.jpg)'
- en: Figure 4
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图4
- en: Navigate by clicking on the application ID to see all the metrics related to
    the running applications including the DAG visualizations and many more.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 通过单击应用程序ID进行导航，以查看与运行中的应用程序相关的所有指标，包括DAG可视化等。
- en: 'These statements covered all the use cases discussed, and it is worth going
    through the Spark transformations covered so far. These are some of the basic
    but very important transformations that will be used in most of the applications
    again and again:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这些语句涵盖了所有讨论过的用例，回顾到目前为止涵盖的Spark转换是值得的。这些是一些基本但非常重要的转换，将在大多数应用程序中反复使用：
- en: '| **Spark transformation** | **What it does** |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| **Spark转换** | **它执行的操作** |'
- en: '| --- | --- |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `filter(fn)` | **Iterates through all the elements of the RDD, applies the
    function that is passed, and picks up the elements that return true as evaluated
    by the function on the element.** |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| `filter(fn)` | **遍历RDD中的所有元素，应用传入的函数，并获取函数对元素评估为true的元素。** |'
- en: '| `map(fn)` | Iterates through all the elements of the RDD, applies the function
    that is passed, and picks up the output returned by the function. |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| `map(fn)` | 遍历RDD中的所有元素，应用传入的函数，并获取函数返回的输出。 |'
- en: '| `flatMap(fn)` | Iterates through all the elements of the RDD, applies the
    function that is passed, and picks up the output returned by the function. The
    big difference here as compared to the Spark transformation `map(fn)` is that
    the function acts on a single element and returns a flat collection of elements.
    For example, it takes one banking transaction record and splits it into multiple
    fields, resulting in a collection from a single element. |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| `flatMap(fn)` | 遍历RDD中的所有元素，应用传入的函数，并获取函数返回的输出。与Spark转换`map(fn)`相比，这里的大不同在于函数作用于单个元素，并返回一个扁平化的元素集合。例如，它将一条银行交易记录拆分为多个字段，从而从一个元素生成一个集合。
    |'
- en: '| `union(other)` | Takes the union of all the elements of this RDD and the
    other RDD. |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| `union(other)` | 将此RDD和其他RDD的所有元素合并。 |'
- en: It is also worth going through the Spark actions covered so far. These are some
    of the basic ones, but more actions will be covered in due course.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，回顾到目前为止涵盖的Spark动作。这些是一些基本的动作，但将会在适当的时候涵盖更多动作。
- en: '| **Spark action** | **What it does** |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| **Spark动作** | **它执行的操作** |'
- en: '| --- | --- |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `collect()` | **Collects all the elements in the RDD to an array in the Spark
    driver.** |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| `collect()` | **将RDD中的所有元素收集到Spark驱动程序中的数组中。** |'
- en: '| `reduce(fn)` | Applies the function fn on all the elements of the RDD and
    the final result is calculated as defined by the function. It should be a function
    that takes two parameters and returns one, which is also commutative and associative.
    |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| `reduce(fn)` | 对RDD的所有元素应用函数fn，最终结果按照函数定义计算。它应该是一个接受两个参数并返回一个结果的函数，同时具有交换性和结合性。|'
- en: '| `foreach(fn)` | Applies the function fn on all the elements of the RDD. This
    is mainly used for side effects. The Spark transformation `map(fn)` applies the
    function to all the elements of the RDD and returns another RDD. But the `foreach(fn)`
    Spark transformation does not return an RDD. For example, `foreach(println)` will
    take each element from the RDD and print it onto the console. Even though it is
    not used in the use cases covered here, it is worth mentioning. |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| `foreach(fn)` | 对RDD的所有元素应用函数fn。这主要用于副作用。Spark转换`map(fn)`将函数应用于RDD的所有元素并返回另一个RDD。但`foreach(fn)`
    Spark转换不返回RDD。例如，`foreach(println)`将取RDD中的每个元素并将其打印到控制台。尽管在这里的用例中没有使用，但仍有必要提及。|'
- en: The next step in the Spark learning process is to try the statements in the
    Python REPL, covering exactly the same use case. The variable definitions have
    been maintained as similar as possible in both the languages to have easy assimilation
    of ideas. There may be minor variations in the way they are used here as compared
    to the Scala way; conceptually, it is independent of the language of choice.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Spark学习过程的下一步是尝试Python REPL中的语句，覆盖完全相同的用例。为了便于思想吸收，变量定义在两种语言中尽可能保持相似。与Scala的方式相比，它们的使用方式可能存在一些细微的差异；从概念上讲，它与选择的语言无关。
- en: 'Start the Python REPL for Spark and make sure that it starts without any errors
    and the prompt is seen. While playing around with Scala code, the monitoring was
    already enabled. Now fire up the Python REPL with the Spark master URL:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 启动Spark的Python REPL，并确保它没有错误地启动，并且可以看到提示符。在玩Scala代码时，监控已经启用。现在使用Spark主URL启动Python
    REPL：
- en: '[PRE11]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'At the Python REPL prompt, try the following statements. The output of the
    statements is given in bold. Note that `>>>` is the Python REPL prompt:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python REPL提示符下，尝试以下语句。语句的输出以粗体显示。请注意`>>>`是Python REPL提示符：
- en: '[PRE12]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The real power of the uniform programming model of Spark is very clearly visible
    if both the Scala and Python code sets are compared. The Spark transformations
    and Spark actions are the same in both the language implementations. The way functions
    are passed into these are different because of the programming language syntax
    differences.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的统一编程模型的真实力量，在将Scala和Python代码集进行比较时表现得非常明显。Spark的转换和操作在两种语言实现中都是相同的。由于编程语言语法差异，将这些函数传递给这些操作的方式不同。
- en: Before running the Python REPL for Spark, the Scala REPL was closed and this
    was done on purpose. Then the Spark web UI should look something similar to that
    shown in Figure 5\. Since the Scala REPL was closed, that is getting listed under
    the completed applications list. Since the Python REPL is still open, that is
    getting listed under the running applications list. Note the application names
    of both the Scala REPL and Python REPL of Spark in the Spark web UI. These are
    standard names. When custom applications are run from files, there are ways to
    assign custom names while defining the Spark context object for the applications
    to facilitate monitoring of the applications and for logging purposes. These details
    will be covered later in this chapter.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行Spark的Python REPL之前，已经关闭了Scala REPL，这是故意为之。然后，Spark Web UI应该看起来与图5中所示类似。由于Scala
    REPL已关闭，因此它被列在已完成的作业列表中。由于Python REPL仍然打开，因此它被列在正在运行的作业列表中。注意Spark Web UI中Scala
    REPL和Python REPL的应用程序名称。这些是标准名称。当从文件运行自定义应用程序时，有方法可以在定义Spark上下文对象时为应用程序分配自定义名称，以方便应用程序的监控和日志记录。这些细节将在本章后面进行介绍。
- en: It is a good idea to spend time with the Spark web UI, getting familiar with
    all the metrics that are being captured, and how the DAG visualization is given
    in the UI. It will help a lot while debugging complex Spark applications.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 花时间熟悉Spark Web UI中捕获的所有指标以及UI中给出的DAG可视化是一个好主意。这将在调试复杂的Spark应用程序时非常有帮助。
- en: '![The basics of programming with Spark](img/image_02_006.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![使用Spark进行编程的基础](img/image_02_006.jpg)'
- en: Figure 5
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图5
- en: MapReduce
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MapReduce
- en: Since day one, Spark has been placed as the replacement for Hadoop MapReduce
    programs. In general, data processing jobs are done in MapReduce style if that
    job can be divided into multiple tasks and they can be executed in parallel, and
    the final results can be computed after collecting the results from all these
    distributed pieces. Unlike Hadoop MapReduce, Spark can do this even if the DAG
    of activities is more than the two stages, such as Map and Reduce. Spark is designed
    to do that and that is one of the biggest value propositions that Spark highlights.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 从第一天起，Spark就被定位为Hadoop MapReduce程序的替代品。一般来说，如果数据处理作业可以分解成多个任务并且可以并行执行，并且最终结果可以在收集所有这些分布式部分的结果后计算，那么这些作业将以MapReduce风格完成。与Hadoop
    MapReduce不同，Spark即使在活动的DAG超过两个阶段（如Map和Reduce）的情况下也能做到这一点。Spark就是为了这个目的而设计的，这也是Spark强调的最大价值主张之一。
- en: This section is going to continue with the same retail banking application and
    pick up some of the use cases that are ideal candidates for the MapReduce kind
    of data processing.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将继续使用相同的零售银行业务应用，并选取一些适合MapReduce类型数据处理的使用案例。
- en: 'The use cases selected for elucidating the MapReduce kind of data processing
    here are given as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里阐述MapReduce类型数据处理所选择的使用案例如下：
- en: The retail banking transaction records come with account numbers and the transaction
    amounts in comma-separated strings.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 零售银行业务交易记录附带账户号码和以逗号分隔的交易金额。
- en: Pair the transactions to have key/value pairs such as (`AccNo`, `TranAmount`).
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将交易配对，形成如(`AccNo`, `TranAmount`)这样的键/值对。
- en: Find an account level summary of all the transactions to get the account balance.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到所有交易的账户级别摘要以获取账户余额。
- en: 'At the Scala REPL prompt, try the following statements:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala REPL提示符下，尝试以下语句：
- en: '[PRE13]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Here is the step-by-step detail capturing what has been done so far:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是逐步详细捕捉到目前为止所做的工作：
- en: The value `acTransList` is the array containing the comma-separated transaction
    records.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 值`acTransList`是包含以逗号分隔的交易记录的数组。
- en: The value `acTransRDD` is the RDD created out of the array, where sc is the
    Spark context or the Spark driver and the RDD is created in a parallelized way
    so that the RDD elements can form a distributed dataset.
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 值`acTransRDD`是由数组创建的RDD，其中sc是Spark上下文或Spark驱动程序，RDD以并行方式创建，以便RDD元素可以形成一个分布式数据集。
- en: Transform the `acTransRDD` to `acKeyVal` to have key-value pairs of the form
    (K,V), where the account number is chosen as the key. In this set of elements
    in the RDD, there will be multiple elements with the same key.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`acTransRDD`转换为`acKeyVal`以形成形式为(K,V)的键值对，其中账户号码被选为键。在这个RDD的元素集中，将会有多个具有相同键的元素。
- en: In the next step, the key-value pairs are grouped by the key and a reduction
    function has been passed, which will add the transaction amount to form key-value
    pairs containing one element for a specific key in the RDD and the total of all
    the amounts for the same key. Then sort the elements on the key before producing
    the final result.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一步中，根据键对键值对进行分组，并传递了一个减少函数，该函数将交易金额相加，形成包含RDD中特定键的一个元素和相同键的所有金额总和的键值对。然后在生成最终结果之前按键对元素进行排序。
- en: Collect the elements to an array at the driver level.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在驱动程序级别将元素收集到数组中。
- en: 'Assuming that the RDD `acKeyVal` is partitioned into two parts and distributed
    to a cluster for processing, Figure 6 captures the essence of the processing:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 假设RDD `acKeyVal`被分成两部分并分布到集群中进行处理，图6捕捉了处理的核心：
- en: '![MapReduce](img/image_02_008.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![MapReduce](img/image_02_008.jpg)'
- en: Figure 6
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图6
- en: 'The following table captures the Spark actions that are introduced in this
    use case:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格捕捉了在本用例中引入的Spark动作：
- en: '| **Spark action** | **What it does?** |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| **Spark动作** | **它做什么？** |'
- en: '| --- | --- |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `reduceByKey(fn,[noOfTasks])` | **Applies the function fn on the RDD of the
    form (K,V) and is reduced to remove duplicate keys and apply the function passed
    as the parameter to be acted on the values at the key level.** |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| `reduceByKey(fn,[noOfTasks])` | **在形式为(K,V)的RDD上应用函数fn，并减少重复键，将作为参数传递的函数应用于键级别的值。**
    |'
- en: '| `sortByKey([ascending], [numTasks])` | Sorts the RDD elements if the RDD
    is of the form (K,V) by its key K |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| `sortByKey([ascending], [numTasks])` | 如果RDD的形式为(K,V)，则按其键K对RDD元素进行排序 |'
- en: 'The `reduceByKey`action deserves a special mention. In Figure 6, the grouping
    of the elements by the key is a well-known operation. But in the next step, for
    the same key, the function passed to as a parameter takes two parameters and returns
    one. It is not very intuitive to get this right and you may be wondering from
    where the two inputs are coming while iterating through the values of the (K,V)
    pair for each key. This behavior takes the concept from the Scala collection method
    `reduceLeft`. The following Figure 7, with the values of the key **SB10001** doing
    the `reduceByKey(_ + _)`operation, is an attempt to explain the concept. This
    is just for the elucidation purposes of this example and the actual Spark implementation
    to do the same may be different:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '`reduceByKey`操作值得特别提及。在图6中，按键对元素进行分组是一个已知的操作。但在下一步中，对于相同的键，传递给参数的函数接受两个参数并返回一个。正确获取这个结果并不直观，你可能想知道在迭代每个键的(K,V)对的值时，两个输入从何而来。这种行为是从Scala集合方法`reduceLeft`的概念中来的。以下图7，使用键**SB10001**的值执行`reduceByKey(_
    + _)`操作，是为了解释这个概念。这只是为了说明这个示例，而实际的Spark实现可能不同：'
- en: '![MapReduce](img/image_02_010.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![MapReduce](img/image_02_010.jpg)'
- en: Figure 7
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图7
- en: On the right-hand side of Figure 7, the `reduceLeft` operation of the Scala
    collection method is illustrated. That is an attempt to give some insight into
    from where the two parameters are coming for the `reduceLeft`function. As a matter
    of fact, many of the transformations that are being used on Spark RDD are adapted
    from Scala collection methods.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在图7的右侧，展示了Scala集合方法的`reduceLeft`操作。这是尝试提供一些关于`reduceLeft`函数的两个参数来源的见解。实际上，Spark
    RDD上使用的许多转换都是从Scala集合方法中改编而来的。
- en: 'At the Python REPL prompt, try the following statements:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python REPL提示符下，尝试以下语句：
- en: '[PRE14]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The `reduceByKey` took an input parameter, which is a function. Similar to this,
    there is another transformation that does the key-based operation in a slightly
    different way. It is `groupByKey()`. This gathers all the values of a given key
    and forms the list of values from all the individual elements.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '`reduceByKey`接受一个输入参数，它是一个函数。与此类似，还有一个不同的转换，它以稍微不同的方式执行基于键的操作。它是`groupByKey()`。它收集给定键的所有值，并从所有单个元素形成值的列表。'
- en: If there is a need to do multiple levels of processing with the same value elements
    as a collection for each key, this is the suitable transformation. In other words,
    if there are many (K,V) pairs, this transformation returns (K, Iterable<V>) for
    each key.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要对每个键的相同值元素集合进行多级处理，这种转换是合适的。换句话说，如果有许多(K,V)对，这种转换将为每个键返回(K, Iterable<V>)。
- en: Tip
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: The only thing the developer needs to be cognizant about is to make sure that
    the number of such (K,V) pairs is not really huge so that the operation doesn't
    create performance problems. There is no hard and fast rule to find this out and
    it rather depends on the use case.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 开发者唯一需要关注的是确保这样的(K,V)对的数量不是真的很大，以免操作造成性能问题。没有硬性规则来确定这一点，这更多取决于用例。
- en: In all the preceding code snippets, for extracting account numbers or any other
    field from the comma-separated transaction record, split(`,`) is used multiple
    times within the `map()` transformation. This is to demonstrate the use of array
    elements within `map()`, or any other transformation or method. A better way of
    extracting the fields of the transaction record is to transform them as a tuple
    containing the required fields and then use the fields from the tuple to employ
    them in some of the following code snippets. In this way, there is no need to
    call split (`,`) repeatedly for each field extraction.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有前面的代码片段中，为了从逗号分隔的交易记录中提取账户号码或其他字段，在`map()`转换中多次使用了split(`,`)。这是为了展示在`map()`或任何其他转换或方法中使用数组元素。提取交易记录字段的一个更好的方法是将其转换为包含所需字段的元组，然后使用元组中的字段在以下代码片段中应用它们。这样，就不需要为每个字段提取重复调用split
    (`,`).
- en: Joins
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 连接
- en: In the **Relational Database Management Systems** (**RDBMS**) world, joining
    multiple tables rows based on a key is a very common practice. When it comes to
    the NoSQL data stores, joining multiple tables became a real problem because many
    of the NoSQL data stores don't have support for the table joins. In the NoSQL
    world, redundancy is allowed. Whether a technology supports table joins or not,
    business use cases mandate joins of datasets based on keys all the time. Because
    of this, it is imperative to have the joins done in a batch mode in many of the
    use cases.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在**关系数据库管理系统**（**RDBMS**）的世界中，基于键连接多个表行是一个非常常见的做法。当涉及到NoSQL数据存储时，连接多个表成为一个真正的问题，因为许多NoSQL数据存储不支持表连接。在NoSQL世界中，冗余是被允许的。无论技术是否支持表连接，业务用例总是要求基于键连接数据集。因此，在许多用例中，以批量模式执行连接是必不可少的。
- en: Spark provides transformations to join multiple RDDs based on a key. This supports
    many use cases. These days there are many NoSQL data stores having connectors
    to talk to Spark. When working with such data stores, it is very simple to construct
    RDDs of data from multiple tables, do the join from Spark, and store the results
    back into the data stores in batch mode or even in near-to-real-time mode. Spark
    transformations are available for left outer join and right outer join, as well
    as full outer join.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: Spark提供了基于键连接多个RDD的转换。这支持许多用例。如今，许多NoSQL数据存储都有与Spark通信的连接器。当与这样的数据存储一起工作时，构建来自多个表的数据RDD非常简单，可以在Spark中进行连接，并将结果以批量模式或近乎实时模式存储回数据存储中。Spark转换支持左外连接、右外连接以及全外连接。
- en: The use cases selected for elucidating the join of multiple datasets using a
    key are given as follows.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 用于阐明使用键连接多个数据集的用例如下。
- en: The first dataset contains a retail banking master records summary with an account
    number, first name, and last name. The second dataset contains the retail banking
    account balance with an account number, and balance amount. The key on both of
    the datasets is the account number. Join the two datasets and create one dataset
    containing the account number, full name, and balance amount.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个数据集包含一个零售银行主记录摘要，包括账户号码、名和姓。第二个数据集包含零售银行账户余额，包括账户号码和余额金额。这两个数据集的关键是账户号码。将这两个数据集连接起来，创建一个包含账户号码、全名和余额金额的数据集。
- en: 'At the Scala REPL prompt, try the following statements:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala REPL提示符下，尝试以下语句：
- en: '[PRE15]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'All the statements given previously must be familiar by now, except the Spark
    transformation join. Similar to this transformation, the `leftOuterJoin`, `rightOuterJoin`,
    and `fullOuterJoin`are also available with the same usage pattern:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，所有给出的语句都应该已经熟悉，除了Spark转换连接。与这个转换类似，`leftOuterJoin`、`rightOuterJoin`和`fullOuterJoin`也以相同的用法模式提供：
- en: '| **Spark transformation** | **What it does** |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| **Spark转换** | **它做什么** |'
- en: '| --- | --- |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `join(other, [numTasks])` | **Joins this RDD with the other RDD, and the
    elements are joined together based on the key. Suppose the original RDD is of
    the form (K,V1) and the second RDD is of the form (K,V2), then the join operation
    will produce tuples of the form (K, (V1,V2)) with all the pairs of each key.**
    |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| `join(other, [numTasks])` | **将此RDD与另一个RDD连接，元素根据键连接在一起。假设原始RDD的形式为(K,V1)，第二个RDD的形式为(K,V2)，那么连接操作将产生形式为(K,
    (V1,V2))的元组，包含每个键的所有配对。** |'
- en: 'At the Python REPL prompt, try the following statements:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python REPL提示符下，尝试以下语句：
- en: '[PRE16]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: More actions
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多动作
- en: 'So far, the focus was mainly on Spark transformations. Spark actions are also
    important. To get insight into some more important Spark actions, take the following
    use cases, continuing from where it was stopped in the preceding section''s use
    cases:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，重点主要在Spark转换上。Spark动作也同样重要。为了深入了解一些更重要的Spark动作，请看以下用例，继续从上一节用例中停止的地方：
- en: From the list containing account numbers, names, and account balances, get the
    one that has the highest account balance
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从包含账户号码、姓名和账户余额的列表中，获取账户余额最高的一个
- en: From the list containing account numbers, names, and account balances, get the
    top three having the highest account balance
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从包含账户号码、姓名和账户余额的列表中，获取账户余额最高的前三个
- en: Count the number of balance transaction records at an account level
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在账户级别计算余额交易记录数
- en: Count the total number of balance transaction records
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算总余额交易记录数
- en: Print the name and account balance of all the accounts
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 打印所有账户的名称和账户余额
- en: Calculate the total of the account balance
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算账户余额总和
- en: Tip
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: It is a very common requirement to iterate through the elements in a collection,
    do some mathematical calculation on each of the elements, and at the end of it,
    use the result. The RDD is partitioned and distributed across worker nodes. If
    any normal variable is used for storing the cumulative result while iterating
    through the RDD elements, it may not yield the correct result. In such situations,
    instead of using regular variables, use Spark provided accumulators.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 非常常见的需求是遍历集合中的元素，对每个元素进行一些数学计算，并在最后使用结果。RDD 被分区并分布到工作节点上。如果在遍历 RDD 元素时使用任何普通变量来存储累积结果，则可能不会得到正确的结果。在这种情况下，而不是使用常规变量，应使用
    Spark 提供的累加器。
- en: 'At the Scala REPL prompt, try the following statements:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Scala REPL 提示符下，尝试以下语句：
- en: '[PRE17]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The following table captures the Spark actions that are introduced in this
    use case:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 下表总结了在本用例中引入的 Spark 动作：
- en: '| **Spark action** | **What it does** |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| **Spark 动作** | **执行的操作** |'
- en: '| --- | --- |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `first()` | **Returns the first element in the RDD.** |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| `first()` | **返回 RDD 中的第一个元素。** |'
- en: '| `take(n)` | Returns an array of the first `n` elements from the RDD. |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| `take(n)` | 返回 RDD 中前 `n` 个元素的数组。 |'
- en: '| `countByKey()` | Returns the count of elements by the key. If the RDD contains
    (K,V) pairs, this will return a dictionary of `(K, numOfValues)`. |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| `countByKey()` | 返回按键的元素计数。如果 RDD 包含 (K,V) 对，则返回一个 `(K, numOfValues)` 的字典。
    |'
- en: '| `count()` | Returns the number of elements in the RDD. |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| `count()` | 返回 RDD 中的元素数量。 |'
- en: '| `foreach(fn)` | Applies the function fn to each element in the RDD. In the
    preceding use case, Spark Accumulator is being used with `foreach(fn)`. |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| `foreach(fn)` | 将函数 fn 应用到 RDD 中的每个元素。在上一个用例中，Spark Accumulator 与 `foreach(fn)`
    一起使用。 |'
- en: 'At the Python REPL prompt, try the following statements:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python REPL 提示符下，尝试以下语句：
- en: '[PRE18]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Creating RDDs from files
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从文件创建 RDD
- en: So far, the focus of the discussion was on the RDD functionality and programming
    with RDDs. In all the preceding use cases, the RDD creation was done from the
    collection objects. But in the real-world use cases, the data will come from files
    stored in the local filesystems, and HDFS. Quite often, the data will come from
    NoSQL data stores such as Cassandra. It is possible to create RDDs by reading
    the contents from these data sources. Once RDD is created, then all the operations
    are uniform, as given in the preceding use cases. The data files coming out of
    the filesystems may be fixed width, comma-separated, or any other format. But
    the common pattern used for reading such data files is to read the data line by
    line and split the line to have the necessary separation of data items. In the
    case of data coming from other sources, the appropriate Spark connector program
    is to be used and the appropriate API for reading data is to be used.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，讨论的重点是 RDD 功能和 RDD 编程。在所有前面的用例中，RDD 的创建都是从集合对象中完成的。但在现实世界的用例中，数据将来自存储在本地文件系统中的文件和
    HDFS。通常，数据将来自如 Cassandra 这样的 NoSQL 数据存储。可以通过从这些数据源读取内容来创建 RDD。一旦创建了 RDD，那么所有操作都是统一的，如前所述的用例所示。从文件系统中出来的数据文件可能是固定宽度、逗号分隔的或任何其他格式。但读取此类数据文件的常见模式是逐行读取数据并将行分割成必要的数据项分隔。在来自其他来源的数据的情况下，应使用适当的
    Spark 连接器程序和适当的读取数据 API。
- en: Many third-party libraries are available to read the contents from various types
    of text files. For example, the Spark CSV library available from GitHub is a very
    useful one for creating RDDs from CSV files.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 许多第三方库可用于从各种类型的文本文件中读取内容。例如，GitHub 上可用的 Spark CSV 库是一个非常有用的库，可以从 CSV 文件创建 RDD。
- en: 'The following table captures the way text files are read from various sources,
    such as local filesystems, HDFS, and so on. As discussed earlier, the processing
    of the text file is up to the use case requirements:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 下表总结了从各种来源（如本地文件系统、HDFS 等）读取文本文件的方式。如前所述，文本文件的处理取决于用例要求：
- en: '| **File location** | **RDD creation** | **What it does** |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| **文件位置** | **RDD 创建** | **执行的操作** |'
- en: '| --- | --- | --- |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Local filesystem | `val textFile = sc.textFile("README.md")` | **Creates
    an RDD by reading the contents of the file named** `README.md` **from the directory
    from where the Spark shell is invoked. Here, the RDD is of the type RDD[string]
    and the elements will be lines from the file.** |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 本地文件系统 | `val textFile = sc.textFile("README.md")` | **通过从Spark shell调用的目录中读取名为**
    `README.md` **的文件内容**，创建一个RDD。在这里，RDD的类型是RDD[string]，元素将是文件中的行。** |'
- en: '| HDFS | `val textFile = sc.textFile ("hdfs://<location in HDFS>")` | Creates
    an RDD by reading the contents of the file specified in the HDFS URL |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| HDFS | `val textFile = sc.textFile ("hdfs://<location in HDFS>")` | 通过读取HDFS
    URL中指定的文件内容创建一个RDD |'
- en: The most important aspect while reading the files from the local filesystem
    is that the file should be available in all the nodes of the Spark worker nodes.
    Apart from these two file locations given in the preceding table, any supported
    filesystem URI may be used.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 从本地文件系统读取文件时最重要的方面是文件应该可在所有Spark工作节点上可用。除了前面表格中给出的这两个文件位置之外，还可以使用任何支持的文件系统URI。
- en: Just like reading the contents from files in various filesystems, it is also
    possible to write the RDD onto files using the `saveAsTextFile`(path) Spark action.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 就像从各种文件系统中读取文件内容一样，也可以使用`saveAsTextFile`(path) Spark操作将RDD写入文件。
- en: Tip
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: All the Spark application use cases discussed here are run on the appropriate
    language's REPL of Spark. When writing applications, they will be written in proper
    source code files. In the case of Scala and Java, the application code files have
    to be compiled, packaged, and run with proper library dependencies, and are typically
    built using maven or sbt. This will be covered in detail when designing data processing
    applications using Spark, in the last chapter of this book.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 这里讨论的所有Spark应用程序用例都是在Spark相应语言的REPL上运行的。当编写应用程序时，它们将写入适当的源代码文件。在Scala和Java的情况下，应用程序代码文件必须编译、打包，并带有适当的库依赖项运行，通常使用maven或sbt构建。这将在本书的最后一章详细介绍设计使用Spark的数据处理应用程序时进行。
- en: Understanding the Spark library stack
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解Spark库栈
- en: Spark comes with a core data processing engine and a stack of libraries working
    on top of the core engine. It is very important to understand the concept of stacking
    libraries on top of the core framework.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: Spark附带一个核心数据处理引擎和一系列在核心引擎之上工作的库。理解在核心框架之上堆叠库的概念非常重要。
- en: All these libraries that are making use of the services provided by the core
    framework support the data abstractions offered by the core framework and much
    more. Before Spark came onto market, there were lots of independent open source
    products doing what the library stack in discussion here is now doing. The biggest
    disadvantage with these point products was their interoperability. They don't
    stack together well. They were implemented in different programming languages.
    The programming language of choice supported by these products, and the lack of
    uniformity in the APIs exposed by these products, were really challenging to get
    one application done with two or more such products. That is the relevance of
    the stack of libraries that work on top of Spark. They all work together with
    the same programming model. This helps organizations to standardize on the data
    processing toolset without vendorlock-in.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些利用核心框架提供服务的库都支持核心框架提供的数据抽象，以及更多功能。在Spark进入市场之前，有许多独立的开源产品正在做现在讨论的库栈正在做的事情。这些点产品的最大缺点是它们的互操作性。它们不能很好地堆叠在一起。它们是用不同的编程语言实现的。这些产品支持的选择的编程语言以及这些产品暴露的API的不一致性，使得使用两个或更多这样的产品完成一个应用程序变得非常具有挑战性。这就是在Spark之上工作的库栈的相关性。它们都使用相同的编程模型一起工作。这有助于组织标准化数据处理工具集，避免供应商锁定。
- en: 'Spark comes with the following stack of domain-specific libraries, and Figure
    8 gives a comprehensive picture of the whole ecosystem as seen by a developer:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: Spark附带以下特定领域的库栈，图8展示了开发者眼中整个生态系统的全面视图：
- en: '**Spark SQL**'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark SQL**'
- en: '**Spark Streaming**'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark Streaming**'
- en: '**Spark MLlib**'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark MLlib**'
- en: '**Spark GraphX**'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark GraphX**'
- en: '![Understanding the Spark library stack](img/image_02_012.jpg)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![理解Spark库栈](img/image_02_012.jpg)'
- en: Figure 8
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 图8
- en: In any organization, structured data is still very widely used. The most ubiquitous
    data access mechanism with structured data is SQL. Spark SQL provides the capability
    to write SQL-like queries on top of the structured data abstraction called the
    DataFrame API. DataFrame and SQL go very well and support data coming from various
    sources, such as Hive, Avro, Parquet, JSON, and many more. Once the data is loaded
    into the Spark context, they can be operated as if they are all coming from the
    same source. In other words, if required, SQL-like queries can be used to join
    data coming from different sources, such as Hive and JSON. Another big advantage
    that Spark SQL and the DataFrame API bring onto the developers table is the ease
    of use and no need-to-know functional programming methods, which is a requirement
    to do programming with RDDs.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何组织中，结构化数据仍然非常广泛地使用。与结构化数据最普遍的数据访问机制是SQL。Spark SQL提供了在称为DataFrame API的结构化数据抽象之上编写类似SQL查询的能力。DataFrame和SQL配合得很好，支持来自各种来源的数据，例如Hive、Avro、Parquet、JSON等。一旦数据被加载到Spark上下文中，它们就可以像它们都来自同一来源一样进行操作。换句话说，如果需要，可以使用类似SQL的查询来连接来自不同来源的数据，例如Hive和JSON。Spark
    SQL和DataFrame API带给开发者的另一个重大优势是易用性以及不需要了解函数式编程方法，这是使用RDD进行编程的要求。
- en: Tip
  id: totrans-264
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: Using Spark SQL and the DataFrame API, data can be read from various data sources
    and processed as if it is all coming from a unified source. Spark transformations
    and Spark actions support uniform programming interfaces. So the data source unification,
    API unification, and ability to use multiple programming languages to write data
    processing applications help the organizations to standardize on one data processing
    framework.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Spark SQL和DataFrame API，可以从各种数据源读取数据，并像它们都来自统一源一样进行处理。Spark转换和Spark操作支持统一的编程接口。因此，数据源统一、API统一以及使用多种编程语言编写数据处理应用程序的能力，有助于组织标准化一个数据处理框架。
- en: The data ingestion into the organizational data sinks is increasing every day.
    At the same time, the velocity at which data is getting ingested is also increasing.
    Spark Streaming provides the library to process the data that is ingested from
    various sources at a very high velocity.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 组织数据接收器的数据接收量每天都在增加。同时，数据接收的速度也在增加。Spark Streaming提供了库来处理从各种来源以极高速度接收的数据。
- en: In the past, data scientists had the challenge of building their own implementations
    of the machine learning algorithms and utilities in their programming language
    of choice. Quite often, such programming languages don't interoperate with the
    data processing toolset of the organization. Spark MLlib provides the unification
    process, where it comes with a lot of machine learning algorithms and utilities
    working on top of the Spark data processing engine.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去，数据科学家面临在所选编程语言中构建自己的机器学习算法和工具的实现挑战。通常，这样的编程语言与组织的数据处理工具集不兼容。Spark MLlib提供了统一过程，其中它自带了许多机器学习算法和工具，这些算法和工具在Spark数据处理引擎之上运行。
- en: The IoT applications, especially the social media applications, mandated the
    need to have data processing capabilities where the data fits into a graph-like
    structure. For example, the connections in LinkedIn, relationship between friends
    in Facebook, workflow applications, and many such use cases, make use of the graph
    abstraction extensively. Using the graph to do various computations requires very
    high data processing capabilities and employment of sophisticated algorithms.
    Spark GraphX library comes with an API for graphs and makes use of Spark's parallel
    computing paradigm.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 物联网应用，尤其是社交媒体应用，要求具备数据处理能力，其中数据适合于图状结构。例如，LinkedIn中的连接、Facebook中朋友之间的关系、工作流应用以及许多此类用例，广泛使用了图抽象。使用图进行各种计算需要非常高的数据处理能力和复杂的算法。Spark
    GraphX库提供了图的API，并利用了Spark的并行计算范式。
- en: Tip
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: There are many Spark libraries available that are developed by the community
    for various purposes. Many such third-party library packages are featured in the
    site [http://spark-packages.org/](http://spark-packages.org/). The number of packages
    is growing day by day as the Spark user community is growing. When developing
    data processing applications in Spark, if there is a need to have a domain-specific
    library, it would be a good idea to check this site first and see whether anybody
    has already developed it.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 社区开发了众多Spark库，用于各种目的。许多这样的第三方库包在网站[http://spark-packages.org/](http://spark-packages.org/)上有特色。随着Spark用户社区的日益增长，包的数量每天都在增加。当在Spark中开发数据处理应用程序时，如果需要特定的领域库，首先查看这个网站是个好主意，看看是否有人已经开发了它。
- en: Reference
  id: totrans-271
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: For more information plese visit:[https://github.com/databricks/spark-csv](https://github.com/databricks/spark-csv)
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 如需更多信息，请访问：[https://github.com/databricks/spark-csv](https://github.com/databricks/spark-csv)
- en: Summary
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter discussed the basic programming model of Spark with its primary
    dataset abstraction RDDs. The creation of RDDs from various data sources, and
    processing of the data in RDDs using Spark transformations and Spark actions,
    were covered using Scala and Python APIs. All the important features of the Spark
    programming model were covered with the help of real-world use cases. This chapter
    also discussed the library stack that comes with Spark and what each one is doing.
    In summary, Spark comes with a very user-friendly programming model and in turn
    provides a very powerful data processing toolset.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了Spark的基本编程模型及其主要数据抽象RDDs。本章使用Scala和Python API介绍了从各种数据源创建RDDs，以及使用Spark转换和Spark操作在RDDs中处理数据。本章通过实际案例研究涵盖了Spark编程模型的所有重要功能。本章还讨论了Spark附带库栈及其各自的功能。总之，Spark提供了一个非常用户友好的编程模型，并提供了非常强大的数据处理工具集。
- en: The next chapter will discuss the Dataset API and the DataFrame API. The Dataset
    API is going to be the new way of programming with Spark, while the DataFrame
    API deals with more structured data. Spark SQL is also introduced to manipulate
    structured data and show how that can be intermixed with any Spark data processing
    application.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将讨论数据集API和DataFrame API。数据集API将成为使用Spark编程的新方式，而DataFrame API则用于处理更结构化的数据。Spark
    SQL也被引入以操作结构化数据，并展示如何将其与任何Spark数据处理应用程序混合使用。
