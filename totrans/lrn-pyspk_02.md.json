["```py\ndata = sc.parallelize(\n    [('Amber', 22), ('Alfred', 23), ('Skye',4), ('Albert', 12), \n     ('Amber', 9)])\n```", "```py\ndata_from_file = sc.\\    \n    textFile(\n        '/Users/drabast/Documents/PySpark_Data/VS14MORT.txt.gz',\n        4)\n\n```", "```py\ndata_heterogenous = sc.parallelize([\n    ('Ferrari', 'fast'),\n    {'Porsche': 100000},\n    ['Spain','visited', 4504]\n]).collect()\n```", "```py\ndata_heterogenous[1]['Porsche']\n```", "```py\n100000\n```", "```py\ndef extractInformation(row):\n    import re\n    import numpy as np\n    selected_indices = [\n         2,4,5,6,7,9,10,11,12,13,14,15,16,17,18,\n         ...\n         77,78,79,81,82,83,84,85,87,89\n    ]\n    record_split = re\\\n        .compile(\n            r'([\\s]{19})([0-9]{1})([\\s]{40})\n            ...\n            ([\\s]{33})([0-9\\s]{3})([0-9\\s]{1})([0-9\\s]{1})')\n    try:\n        rs = np.array(record_split.split(row))[selected_indices]\n    except:\n        rs = np.array(['-99'] * len(selected_indices))\n    return rs\n```", "```py\ndata_from_file_conv = data_from_file.map(extractInformation)\n```", "```py\ndata_2014 = data_from_file_conv.map(lambda row: int(row[16]))\n```", "```py\ndata_2014_2 = data_from_file_conv.map(\n    lambda row: (row[16], int(row[16]):)\ndata_2014_2.take(5)\n```", "```py\ndata_filtered = data_from_file_conv.filter(\n    lambda row: row[16] == '2014' and row[21] == '0')\ndata_filtered.count()\n```", "```py\ndata_2014_flat = data_from_file_conv.flatMap(lambda row: (row[16], int(row[16]) + 1))\ndata_2014_flat.take(10)\n```", "```py\ndistinct_gender = data_from_file_conv.map(\n    lambda row: row[5]).distinct()\ndistinct_gender.collect()\n```", "```py\nfraction = 0.1\ndata_sample = data_from_file_conv.sample(False, fraction, 666)\n```", "```py\nprint('Original dataset: {0}, sample: {1}'\\\n.format(data_from_file_conv.count(), data_sample.count()))\n```", "```py\nrdd1 = sc.parallelize([('a', 1), ('b', 4), ('c',10)])\nrdd2 = sc.parallelize([('a', 4), ('a', 1), ('b', '6'), ('d', 15)])\nrdd3 = rdd1.leftOuterJoin(rdd2)\n```", "```py\nrdd4 = rdd1.join(rdd2)\nrdd4.collect()\n```", "```py\nrdd5 = rdd1.intersection(rdd2)\nrdd5.collect()\n```", "```py\nrdd1 = rdd1.repartition(4)\nlen(rdd1.glom().collect())\n```", "```py\ndata_first = data_from_file_conv.take(1)\n```", "```py\ndata_take_sampled = data_from_file_conv.takeSample(False, 1, 667)\n```", "```py\nrdd1.map(lambda row: row[1]).reduce(lambda x, y: x + y)\n```", "```py\ndata_reduce = sc.parallelize([1, 2, .5, .1, 5, .2], 1)\n```", "```py\nworks = data_reduce.reduce(lambda x, y: x / y)\n```", "```py\ndata_reduce = sc.parallelize([1, 2, .5, .1, 5, .2], 3)\ndata_reduce.reduce(lambda x, y: x / y)\n```", "```py\ndata_key = sc.parallelize(\n    [('a', 4),('b', 3),('c', 2),('a', 8),('d', 2),('b', 1),\n     ('d', 3)],4)\ndata_key.reduceByKey(lambda x, y: x + y).collect()\n```", "```py\ndata_reduce.count()\n```", "```py\nlen(data_reduce.collect()) # WRONG -- DON'T DO THIS!\n```", "```py\ndata_key.countByKey().items()\n```", "```py\ndata_key.saveAsTextFile(\n'/Users/drabast/Documents/PySpark_Data/data_key.txt')\n```", "```py\ndef parseInput(row):\n    import re    \n    pattern = re.compile(r'\\(\\'([a-z])\\', ([0-9])\\)')\n    row_split = pattern.split(row)\n    return (row_split[1], int(row_split[2]))\n\ndata_key_reread = sc \\\n    .textFile(\n        '/Users/drabast/Documents/PySpark_Data/data_key.txt') \\\n    .map(parseInput)\ndata_key_reread.collect()\n```", "```py\ndef f(x): \n    print(x)\n\ndata_key.foreach(f)\n```"]