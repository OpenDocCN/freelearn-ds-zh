<html><head></head><body>
        

                            
                    Exploring Text Data and Unstructured Data
                
            
            
                
<p>The need to become literate with both structured and unstructured data continues to evolve. Working with structured data has well-established techniques such as merging and uniform data types, which we have reviewed in prior chapters. However, working with unstructured data is a relatively new concept and is rapidly turning into a must-have skill in data analysis. <strong>Natural Language Processing</strong> (<strong>NLP</strong>) has evolved into an essential skill, so this chapter introduces the concepts and tools available to analyze narrative free text. As technology has advanced, using these techniques can help you to provide transparency to unstructured data, which would have been difficult to uncover only a few years ago.</p>
<p>We will cover the following topics in this chapter:</p>
<ul>
<li>Preparing to work with unstructured data</li>
<li>Tokenization explained</li>
<li>Counting words and exploring results</li>
<li>Normalizing text techniques</li>
<li>Excluding words from analysis</li>
</ul>
<h1 id="uuid-580c00c1-40d9-40cd-a982-352cfbc22e3b">Technical requirements</h1>
<p>The GitHub repository of this book is at <a href="https://github.com/PacktPublishing/Practical-Data-Analysis-using-Jupyter-Notebook/tree/master/Chapter10">https://github.com/PacktPublishing/Practical-Data-Analysis-using-Jupyter-Notebook/tree/master/Chapter10</a>.</p>
<p>You can download and install the required software from <a href="https://www.anaconda.com/products/individual" target="_blank">https://www.anaconda.com/products/individual</a>.</p>
<h1 id="uuid-5cca7dff-087c-4ee4-840a-695a950d7ac2">Preparing to work with unstructured data</h1>
<p>Today, we are living in a digital age where data is entangled into our lives in ways not technically possible or even imaginable before. From social media to mobile to the <strong>Internet of Things</strong> (<strong>IoT</strong>), humanity is living in what is commonly known as the information age. This age is where an exponentially growing of data about you is available to you instantaneously anywhere in the world. What has made this possible has been a combination of people and technology, including contributions from the <strong>Evolution of Data Analysis</strong>, which was introduced in <a href="0fa7e28f-7a30-4099-9bae-30dd3c86ee4f.xhtml">Chapter 1</a>, <em>Fundamentals of Data Analysis</em>.</p>
<p>It is commonly predicted by multiple sources that 80 percent of all of the data created around the world will be unstructured over the next few years. If you recall from <a href="0fa7e28f-7a30-4099-9bae-30dd3c86ee4f.xhtml">Chapter 1</a>, <em>Fundamentals of Data Analysis</em>., unstructured data is commonly defined as information that does not offer uniformity and pre-defined organization. Examples of unstructured data include free text, chatbots, <strong>Artificial Intelligence</strong> (<strong>AI</strong>), photos, videos, and audio files. Social media, of course, produces the highest volume, velocity, and variety of unstructured data. So, we're going to focus our examples on those data sources, but you can apply the concepts learned in this chapter to any narrative-based text data sources such as help desk ticket logs or email communications.</p>
<p>What's important to understand about working with unstructured data is that it's inherently messy. Without structure, defined data types, and conformity applied to data when it's captured, uncertainty is created. This should not prohibit or discourage you from working with the data, but just being conscious that precision is going to be a luxury in some cases. For example, words in a sentence can and will be misspelled. The meaning of a word or phrase can be misrepresented and the context is sometimes lost. With all of these shortcomings, the technology continues to be improved to a point where you will find it being used in everyday life. For example, chatbots are now replacing many customer service solutions so you may think you're talking with an agent but it is actually software with NLP algorithms installed.</p>
<p>The concepts of NLP have been around for decades. Using NLP concepts have some fundamentals that you may have already used in the past such as rule-based conditions or tags. A rule-based example would be when a specific keyword or collection of words is found in a dataset with a flag field used to identify it, similar to the following table where we have three columns with five rows including a header row to identify the purpose of each record:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/c459f02a-2498-4c02-ba87-b87887b0e0b2.png" style="width:38.58em;height:13.50em;"/></p>
<p>In the table, I have changed the font color for any variation of the keyword <kbd>balanced</kbd> to make it easier to identify. As you can observe from the table, the word does have minor variations that would seem obvious to a human but must be programmed as a rule to account for the differences. So, to identify any variation of the keyword such as whether it is capitalized or conjugated, it must be accounted for using conditional logic. In this example, a conditional field named <kbd>key_word_found_flag</kbd> is used to determine whether the phrase field contains the keyword, which is using a value of <kbd>1</kbd> to identify a true case.</p>
<p>Rule-based NLP has evolved from earlier concepts that used basic wildcard searches and ASCII character identification to machine-learned statistical models. I don't plan on going into all of the details behind the mathematical models used by NLP, but I encourage you to explore the subject. I will focus, in this chapter, on using predefined NLP libraries as is without adjusting or training the models. The intention is to help you to gain some confidence in when, where, and how it can be used. Over the last few years, I have been impressed by the level of accuracy behind solutions that leverage NLP, but I recognize it is an evolving technology. I was reminded of this when I interacted with Alexa the other day when it told me it could not understand my question regardless of how I phrased it.</p>
<p>Anytime you use or create solutions with NLP, the potential for a <strong>False Positive (FP)</strong> or <strong>False Negative (FN)</strong> exists. An FP or FN is when the NLP algorithm has incorrectly predicted the output. When the output returns true, but the actual result is false, it is considered an FP. When the model returns false or was missing but the correct result was to return true, it is called an FN. Conversely, when the algorithm or model correctly predicts the output, this is called either a <strong>True Positive (TP)</strong> or <strong>True Negative (TN)</strong>. It is common to use a confusion matrix to identify and train the model to reduce FPs and FNs.</p>
<p>I created a version of a confusion matrix found in the following screenshot to help to identify NLP predictions where the results for positive are in green and negative in red. What determines the color and designation is comparing the actual verse in the predicted output. For example, if a computer model is used to predict the outcome of testing a person for a disease, a <strong>TP</strong> or <strong>TN</strong> would indicate the model either accurately predicted that the person has the disease or verified they do not have the disease. For those cases, the green color boxes indicate a match to the actual outcomes. For any other outcome, the boxes are colored in red because the prediction inaccurately returned a false result:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/813ec9d4-abe0-40c3-b27b-e20eda1d3101.png" style="width:22.50em;height:15.42em;"/></p>
<p>Validating the results of NLP models can and should be done by a human when the data volumes are small. This will ensure trust exists with the code behind the algorithm. However, when data volumes are large, this becomes a time-consuming task, and in many cases, unrealistic to verify without alternative solutions such as crowd-sourcing. Crowd-sourcing solutions are when humans are used for data entry in the process, but at a large scale by breaking up tasks and distributing them to a population of people to validate data.</p>
<p>Typical NLP or data science models will use supervised learning techniques where training sets of data are used to teach the model to be accurate in its prediction output. The labeled data used for training will be a small population of data classified by a person or people with an accurate outcome. So, the statistical models used to accurately predict the results are based on teaching the NLP model based on trained data. Using mislabeled training data in supervised NLP models will result in more false positives and inaccurate results. If you work with a data scientist or data engineering team using supervised learning, you can showcase your data literacy knowledge by asking what process is used to retrain and re-evaluate the accuracy of the model.</p>
<p>Now that we have some fundamental understanding of working with unstructured data, let's walk through how to gather it from its source. For social media data from platforms such as Twitter, using their API connection would be an option and allows you to consume data in near real time by streaming the data into your Jupyter notebook. Using any social media's APIs will require you to set up a user account and create a secret key so you can use a REST client connection via HTTP.</p>
<p>Depending on the network connection used by your workstation, you may need to adjust your firewall or proxy settings to allow you access to consume data via APIs and REST. </p>
<p>Because there are restrictions and different API limits on exporting social media data by platform, we will be using the sample data pre-installed with the packages from the Python libraries. The <strong>Natural Language Toolkit</strong> (<strong>NLTK</strong>) package is available to help us to work with unstructured data in multiple use cases. The software is open source and contains dozens of modules to support NLP using a concept called a corpus. A corpus creates a taxonomy that can be used by linguists, computer scientists, and machine learning enthusiasts. A taxonomy is a breakdown of the language of a body of a text into its fundamental elements. Examples include the American Heritage Dictionary and Wikipedia. It also serves as the foundation for counting and searching words when using software.</p>
<h2 id="uuid-20974d7a-6821-4818-9088-6eaac30bba26">Corpus in action</h2>
<p>For our first example, we are going to use the Brown Corpus, which contains hundreds of sample language text categorized by an eclectic mix of subjects such as mystery short stories, political press releases, and religion. The original collection had over a million words defined and tagged with a part of speech such as noun, verb, and preposition. To install the Brown Corpus and use it in your Jupyter notebook, use the following steps:</p>
<ol start="1">
<li>Launch a new Jupyter notebook and name it <kbd>ch_10_exercises</kbd>.</li>
<li>Import the following libraries by adding the following command in your Jupyter notebook and running the cell. Feel free to follow along by creating your own notebook (I have placed a copy in GitHub for reference):</li>
</ol>
<pre style="padding-left: 60px">In[]: import nltk</pre>
<p>The library should already be available using Anaconda. Refer to <a href="e0fe6eb2-8f38-41f7-9dea-2b177578fd3c.xhtml">Chapter 2</a>, <em>Overview of Python and Installing Jupyter Notebook</em>, for help with setting up your environment. If you are behind a firewall, there is an <kbd>nltk.set_proxy</kbd> option available. Check the documentation at <a href="http://www.nltk.org/">http://www.nltk.org/</a> for more details.</p>
<ol start="3">
<li>Next, we download the specific corpus we want to use. Alternatively, you can download all of the packages using the <kbd>all</kbd> parameter:</li>
</ol>
<pre style="padding-left: 60px">In[]: nltk.download('brown')</pre>
<p style="padding-left: 60px">The output would look like the following screenshot where the package download is confirmed and the output is verified with <kbd>True</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/0373d61e-eb3d-4b3e-85b4-fd106c67527e.png" style="width:36.75em;height:7.17em;"/></p>
<ol start="4">
<li>To confirm the package is available in your Jupyter notebook, we can use the following command to reference the corpus using a common alias of <kbd>brown</kbd> for reference:</li>
</ol>
<pre style="padding-left: 60px">In[]: from nltk.corpus import brown</pre>
<ol start="5">
<li>To display a list of the few words available in the Brown Corpus, use the following command:</li>
</ol>
<pre style="padding-left: 60px">In[]: brown.words()</pre>
<p style="padding-left: 60px">The output would look like the following screenshot where six words in single quotes are displayed along with an ellipsis inside square brackets:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/50455bd2-1623-4811-9cee-30396d9c18da.png" style="width:34.50em;height:6.92em;"/></p>
<ol start="6">
<li>To count all of the words available, we can use the <kbd>len()</kbd> function, which counts the length of a string or the number of items in an object such as an array of values. Since our values are separated by commas, it will count all of the words available. To make it easier to format, let's assign the output to a variable called <kbd>count_of_words</kbd>, which we can use in the next step:</li>
</ol>
<pre style="padding-left: 60px">In[]: count_of_words = len(brown.words())</pre>
<ol start="7">
<li>To make the output easier to understand for the consumer of this data, we use the <kbd>print()</kbd> and <kbd>format()</kbd> functions to display the results using the following command:</li>
</ol>
<pre style="padding-left: 60px">In[]: print('Count of all the words found the Brown Corpus =',format(count_of_words,',d'))</pre>
<p style="padding-left: 60px">The output would look like the following screenshot where a sentence will appear that includes a dynamic count of all of the words assigned to the <kbd>count_of_words</kbd> variable. We also formatted the value to display with a comma:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/af69b04d-4b70-42dd-b5bb-44999c5bd05d.png" style="width:46.50em;height:6.75em;"/></p>
<p>Excellent, you have now successfully loaded your first NLP library and were able to run a few commands against a popular corpus package. Let's continue dissecting the different elements of NLP by explaining why tokenization is important.</p>
<h1 id="uuid-1ddd0647-8f32-4a5b-9ae8-9bc13d0706eb">Tokenization explained</h1>
<p>Tokenization is the process of breaking unstructured text such as paragraphs, sentences, or phrases down into a list of text values called tokens. A token is the lowest unit used by NLP functions to help to identify and work with the data. The process creates a natural hierarchy to help to identify the relationship from the highest to the lowest unit. Depending on the source data, the token could represent a word, sentence, or individual character.</p>
<p>The process to tokenize a body of text, sentence, or phrase, typically starts with breaking apart words using the white space in between them. However, to correctly identify each token accurately requires the library package to account for exceptions such as hyphens, apostrophes, and a language dictionary, to ensure the value is properly identified. Hence, tokenization requires the language of origin of the text to be known to process it. Google Translate, for example, is an NLP solution that can identify the language, but still has the option for users to define it to ensure the translation is accurate.</p>
<p>This is one of the reasons why tokenization is an evolving process. For example, as new words are added to the English language, the NLP reference libraries require an update. Handling sarcasm, dual meanings, and catchphrases may get lost in translation when using NLP solutions such as Alexa or Siri. For example, the phrase <strong>social ambassador</strong> has an obvious meaning to a human but would require the library to be trained to identify it as a token phrase.</p>
<p>There are a few techniques used by NLP to address this issue. The first is called n-grams, which is the process of combining words within the same sentence as a group, typically of two or three words, to create a pattern that is recognizable by the NLP library. Acronyms can also be used in n-grams but require identification or training to be effective. An n-gram could then be used to identify <strong>social ambassador</strong> to understand these two values can be used together.</p>
<p>Another common reference for an n-gram is a bi-gram, which only uses two words. The <strong>n</strong> denotes the number of grams so a uni-gram stands for one gram, a bi-gram is for two, a tri-gram is for three, and so on.</p>
<p>Another concept is called a <strong>bag of words</strong>, which is when a high occurrence of specific words exist in the source data. The use of a <strong>bag of words</strong> is another helpful way to identify patterns and key term searches against large text sources of data. For example, a prediction model to improve response time from system outages can use historical text logs found in help desk tickets. The <strong>bag of words</strong> technique can be used to create multiple flag fields (yes or no) as inputs into the algorithm.</p>
<p>So, the study of tokenization and NLP is a deep subject that will remain an evolving science. I recommend continuing to research the subject in more detail. The Stanford University NLP site is a fantastic source of information that I have added to the <em>Further reading</em> section.</p>
<p>Let's explore the additional features available in the NLTK library in our Jupyter notebook. Another downloadable library available is called <strong>Punkt</strong>, which is used to tokenize sentences into words. I have included a link to the downloads available from NLTK in the <em>Further reading</em> section. The code behind the algorithm requires a high volume of text so the model can be trained but the NLTK data package includes a pre-trained option in English that we can use.</p>
<h2 id="uuid-161e6e97-8e47-465b-8253-5c88a28923d4">Tokenize in action</h2>
<p>You will continue by going back into the <kbd>ch_10_exercises</kbd> notebook in Jupyter:</p>
<ol>
<li>Import the following libraries by adding the following command in your Jupyter notebook and run the cell. Feel free to follow along by creating your own notebook (I have placed a copy on GitHub for reference):</li>
</ol>
<pre style="padding-left: 60px">In[]: nltk.download('punkt')</pre>
<p style="padding-left: 60px">The output would look like the following screenshot where the package download is confirmed and the output is verified with <kbd>True</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/d462896d-7985-4963-a825-2a37bc0bacb3.png" style="width:40.33em;height:7.75em;"/></p>
<ol start="2">
<li>Next, we will create a new variable called <kbd>input_sentence</kbd> and assign it to a free-form text sentence that must be encapsulated in double quotes and on a single line input. There will be no input after you run the cell:</li>
</ol>
<pre style="padding-left: 60px">In[]: input_sentence = "Seth and Becca love to run down to the playground when the weather is nice."</pre>
<ol start="3">
<li>Next, we will use the <kbd>word_tokenize()</kbd> function that is available in the NLTK library to break up the individual words and any punctuation:</li>
</ol>
<pre style="padding-left: 60px">In[]: nltk.word_tokenize(input_sentence)</pre>
<p style="padding-left: 60px">The output would look like the following screenshot where the individual words are broken out from the sentence as an array of values with single quotes around each text value surrounded by square brackets:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/30e87028-a530-4237-b0ef-f1d6579a93fb.png" style="width:47.08em;height:21.00em;"/></p>
<ol start="4">
<li>Next, let's tokenize by sentence, which requires you to import the <kbd>sent_tokenize</kbd> option from the NLTK <kbd>tokenize</kbd> library using the following command:</li>
</ol>
<pre style="padding-left: 60px">In[]: from nltk.tokenize import sent_tokenize</pre>
<ol start="5">
<li>Now, let's assign a new variable called <kbd>input_data</kbd> to a collection of sentences that we can use later in our code. There will be no input after you run the cell:</li>
</ol>
<pre style="padding-left: 60px">In[]: input_data = "Seth and Becca love the playground.  When it sunny, they head down there to play."</pre>
<ol start="6">
<li>Then, we will pass the <kbd>input_data</kbd> variable as a parameter to the <kbd>sent_tokenize()</kbd> function, which will look at the string of text and break it down into individual token values. We wrap the output with the <kbd>print()</kbd> function to display the results cleanly in the notebook:</li>
</ol>
<pre style="padding-left: 60px">In[]: print(sent_tokenize(input_data))</pre>
<p style="padding-left: 60px">The output would look like the following screenshot where the individual sentences are broken down as an array of string values with single quotes around them surrounded by square brackets:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/9e1dda99-e155-412c-8924-f92617a105fc.png" style="width:49.42em;height:9.00em;"/></p>
<p>So, now you can see that having these NLTK library features can help you to work with unstructured data by breaking down language into foundational pieces. As a data analyst, you will be faced with free text in many different forms, so now you have some additional resources to leverage. Once you have the data tokenized, additional options are available that we will explore in the next section, such as counting the frequency of words to identify patterns within the underlying source.</p>
<h1 id="uuid-50bd1325-22c4-4ea9-b580-dc3a20bd280f">Counting words and exploring results</h1>
<p>Counting word frequency will provide initial metadata about the unstructured source text. Exposing the count of the occurrence of a word or when specific words are missing within a body of text is known as <strong>text mining</strong>. Text mining will provide analytics about the data, so a data analyst can determine the value of a data asset along with how it can be used to answer business questions. Likewise, you can identify keyword patterns that occur during unexpected outages that impact users by looking at application system logs. Once those words or phrases are identified, you can work with the developers to identify the root cause and reduce the impact on your application users.</p>
<p>A popular option available for text analysis is the use of regular expressions or <strong>regex</strong> for short. The regex concept is when you use a combination of rules and search patterns to extract features from very large, unstructured text. Regex becomes useful when reading the text line by line would be unreasonable, based on the amount of time and number of people required. Regex has a wide range of applications including how to separate emails, phone numbers, and hashtags from the source text.  If you are working with help desk ticket logs, for example, you would tag successful matches to the regex rules with a unique ticket ID so you can join the unstructured data back to the data model.</p>
<p>Regex covers a variety of techniques including wildcard character searches, pattern matching based on qualifiers, and anchors used to identify the beginning or end of textual data. Regex rules are usually combined with standard software engineering, so the code can be modular and automated when looking at high-frequency data sources such as a chatbot or system logs. For example, if you created a regex rule against any combination of the keyword <kbd>frustrated</kbd> in a customer service system, you can create a flag field named <kbd>is_customer_frustrated_yes_no</kbd> with a value of 1 for true and 0 for false. Regex rules can and should evolve over time, based on the data and validation, the rules that are accurate is important. This can be done with a random sampling of data by manually validating the conditions exist and returning the correct result. </p>
<h2 id="uuid-5b795474-c1b6-4793-ba59-ce17ae2b79f8">Counting words</h2>
<p>Before we explore those options, let's continue with the Jupyter notebook exercise and walk through how to count the frequency of words from a population of free text.</p>
<p>You will continue by going back into the <kbd>ch_10_exercises</kbd> notebook in Jupyter:</p>
<ol>
<li>Import the probability module available in the NTLK library to count the frequency of the words available in a body of text. There will be no result returned after you run the cell:</li>
</ol>
<pre style="padding-left: 60px">In[]: from nltk.probability import FreqDist</pre>
<ol start="2">
<li>Next, explore a large body of text using the Brown Corpus. To do this, we assign the population of all of the token words available by using the <kbd>FreqDist()</kbd> function and assigning it to a variable named <kbd>input_data</kbd>. To see the results of the processing of this data, we can print the variable:</li>
</ol>
<pre style="padding-left: 60px">In[]: input_data = FreqDist(brown.words())<br/>print(input_data)</pre>
<p style="padding-left: 60px">The output would look like the following screenshot where the frequency distribution results are calculated and printed using the <kbd>print()</kbd> function against the assigned variable, <kbd>input_data</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/9aa1085a-c5ca-4595-bf58-1bd75fda230d.png" style="width:33.17em;height:5.67em;"/></p>
<ol start="3">
<li>To see a list of the most common words that exist in <kbd>input_data</kbd>, we can use the <kbd>most_common()</kbd> function along with a parameter to control how many are displayed. In this case, we want to see the top 10:</li>
</ol>
<pre style="padding-left: 60px">In[]: input_data.most_common(10)</pre>
<p style="padding-left: 60px">The output would look like the following screenshot where a list of name-value pairs are displayed like a two-column table with ten rows to show each token, which can be a word, punctuation mark, or character enclosed with single quotes along with an integer value that provides the cumulative count of the times the word appears in the source data:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/b131e99b-c597-46a7-ba8d-c58a9da7b8b7.png" style="width:18.58em;height:14.58em;"/></p>
<p>Through this exercise, we can identify and extract words from a body of unstructured text, which is the foundation for creating regex rules. The next section will focus on normalizing words for consistency to improve the accuracy of NLP model predictions. </p>
<h1 id="uuid-ee6d6936-d08b-4519-9f35-07f299c08652">Normalizing text techniques</h1>
<p>In most cases, making the regex rules <strong>smarter</strong> by adding new code logic or libraries will be required. One such way to do this is by using the concepts behind normalizing your text called stemming and lemmatization. Both terms are rooted in the study of linguistics, and how they are adopted to be used in technology has exploded due to integrating NLP solutions into everything, from customer service to speech-to-text features.</p>
<p>When applied to NLP, stemming is when any word is programmatically identified to its common root form. In this process, any suffix, plural form, or synonym that exists for the word is identified. Stemmers require a reference dictionary or lookup to be accurate, so the source language is required. Lemmatization takes into account all of the variations of a word so it can be rooted back to a dictionary source. From my research, both stemming and lemmatization are used together in NLP and you can start by using the open source libraries available, which I included in the <q>Further reading</q> section. These libraries should be sufficient to cover common words but analogies or custom-defined lingo in your organization will require a new corpus. Simple examples of using stemming or a lemma include identifying when the word <strong>fishes</strong> appears, and returning <strong>fish,</strong><strong> </strong>or <strong>geese</strong> returning <strong>goose</strong>. </p>
<p>The subject for both concepts is pretty vast, so I encourage you to continue learning about it, but the bottom line is the benefits of using these concepts will help to clean and normalize data for analysis. Having the data normalized where multiple similar values are grouped together as a single value is necessary. It reduces the volume of data you are analyzing and prepares the results for deeper analytics such as creating a data science or machine learning model.</p>
<h2 id="uuid-08e7516f-e5fd-4d51-8af5-1278a4ad5701">Stemming and lemmatization in action</h2>
<p>For our exercises, we will use the Porter Stemmer, which is commonly used to help to prepare text data and normalize data in NLP.</p>
<p>Let's continue by going back to the <kbd>ch_10_exercises</kbd> notebook in Jupyter:</p>
<ol>
<li>Import the <kbd>PorterStemmer</kbd> module available in the NTLK library to normalize a word. There will be no result returned after you run the cell:</li>
</ol>
<pre style="padding-left: 60px">In[]: from nltk.stem import PorterStemmer</pre>
<ol start="2">
<li>To import an instance of this feature so it can be referenced later in the code, we use the following code. There will be no result returned after you run the cell:</li>
</ol>
<pre style="padding-left: 60px">In[]: my_word_stemmer = PorterStemmer()</pre>
<ol start="3">
<li>Now, you can pass individual words into the instance to see how the word would be normalized:</li>
</ol>
<pre style="padding-left: 60px">In[]: my_word_stemmer.stem('fishing')</pre>
<p style="padding-left: 60px">The output will look like the following screenshot where the stem of the word <kbd>fishing</kbd> will be displayed as <kbd>fish</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/0b061e67-3cb5-40a2-8100-433b2107ca16.png" style="width:23.92em;height:4.42em;"/></p>
<ol start="4">
<li>To use lemma features, we need to download the <kbd>WordNet</kbd> corpus using the following command:</li>
</ol>
<pre style="padding-left: 60px">In[]: nltk.download('wordnet')</pre>
<ol start="5">
<li>To import an instance of this feature so it can be referenced later in the code, we use the following code. There will be no result returned after you run the cell:</li>
</ol>
<pre style="padding-left: 60px">In[]: from nltk.stem import WordNetLemmatizer<br/>my_word_lemmatizer = WordNetLemmatizer()</pre>
<ol start="6">
<li>To see how the lemma would output for the same word we used a stem for earlier, we pass the same word into the <kbd>lemmatize()</kbd> function:</li>
</ol>
<pre style="padding-left: 60px">In[]: my_word_lemmatizer.lemmatize('fishing')</pre>
<p style="padding-left: 60px">The output would look like the following screenshot where the lemma of the word <kbd>fishing</kbd> will be displayed as <kbd>fishing</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/72a41c72-fe8c-4f97-9885-d0d8e49bc93e.png" style="width:27.75em;height:4.50em;"/></p>
<p style="padding-left: 60px">So, why are the results different? The algorithms used by each of the NLTK corpora apply different approaches to normalize the words. Lemmatization will adjust to the form or structure of the word as it is defined in the dictionary that is included in the WordNet corpus whereas stemming is intended to break down the word to its root. Each is a tool that is available and, depending on the use case, you may need to adjust which approach to use to normalize the data for analysis.</p>
<p style="padding-left: 60px">To take it one step forward, let's pass a list of words into each instance using a loop and print out the results to see how they compare to the original word. We will use a sample from the Brown words by limiting the results.</p>
<p>If you pass all over a million words into the loop in your Jupyter Notebook session, it will take much longer to run and take up resources (RAM and CPU) to process.</p>
<ol start="7">
<li>To create a list but limit the words to only a sample by assigning it to a variable, we use the following command. There will be no result returned after you run the cell:</li>
</ol>
<pre style="padding-left: 60px">In[]: my_list_of_words = brown.words()[:10]</pre>
<ol start="8">
<li>Now, we create a loop against each value in the list and print the results. We include some formatting to make it easier to understand the results for each row. Be sure to include a carriage return to create a new line to use the <kbd>print()</kbd> function without errors:</li>
</ol>
<pre style="padding-left: 60px">In[]: for x in my_list_of_words :    <br/>    print('word =', x, ': stem =', my_word_stemmer.stem(x), ': lemma =', my_word_lemmatizer.lemmatize(x))</pre>
<p style="padding-left: 60px">The output will look like the following screenshot where ten lines are printed with a colon delimiter used to separate the results for the original word, the stem of the word, and the lemma of the word:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/460f1849-9d73-409d-959e-66fbd21d5356.png" style="width:51.50em;height:13.08em;"/></p>
<p>Now that we learned how to normalize the words within your unstructured data, let's find out how to exclude words or phrases from your data to reduce the noise so we can focus on valuable keywords that can provide insight.</p>
<h1 id="uuid-75cf3e26-6165-42fc-ae2c-ff3466ac20e0">Excluding words from analysis</h1>
<p>Visually sifting through millions of words is impractical in data analysis because language includes many linking verbs that are repeated throughout the body of a text. Common words such as <strong>am</strong>, <strong>is</strong>, <strong>are</strong>, <strong>was</strong>, <strong>were</strong>, <strong>being</strong>, and <strong>been</strong> would be at the top of the <kbd>most_common()</kbd> list when you apply NLP against the source data even after it has been normalized. In the evolution of improving NLP libraries, a dictionary of <strong>stopwords</strong> was created to include a more comprehensive list of words that provide less value in text analytics. Example <strong>stopwords </strong>include linking verbs along with words such as <strong>the</strong>, <strong>an</strong>, <strong>a</strong>, and <strong>until</strong>. The goal is to create a subset of data that you can focus your analysis on after filtering out these stopwords from your token values.</p>
<p>NLP can require high CPU and RAM resources especially working with a large collection of words, so you may need to break up your data into logical chucks, such as alphabetically, to complete your analysis.</p>
<p>You will continue by going back to the <kbd>ch_10_exercises</kbd> notebook in Jupyter:</p>
<ol>
<li>Download the <kbd>stopwords</kbd> corpus from the NLTK library using the following command:</li>
</ol>
<pre style="padding-left: 60px">In[]: nltk.download('stopwords')</pre>
<ol start="2">
<li>Next, import the stopwords and <kbd>word_tokenize</kbd> features so they can be used later in the exercise:</li>
</ol>
<pre style="padding-left: 60px">In[]: from nltk.corpus import stopwords<br/>from nltk.tokenize import word_tokenize</pre>
<ol start="3">
<li>Now, let's assign a new variable called <kbd>input_data</kbd> to a collection of sentences that we can use later in our code. There will be no input after you run the cell:</li>
</ol>
<pre style="padding-left: 60px">In[]: input_data = “Seth and Becca love the playground.  When it's sunny, they head down there to play.”</pre>
<ol start="4">
<li>We will assign object variables called <kbd>stop_words</kbd> and <kbd>word_tokens</kbd> so they can be referenced later in the code:</li>
</ol>
<pre style="padding-left: 60px">In[]: stop_words = set(stopwords.words('english'))word_tokens = word_tokenize(input_data)</pre>
<ol start="5">
<li>Finally, we have a few lines of code that will loop through the word tokens from <kbd>input_data</kbd> and compare them to <kbd>stop_words</kbd>. If they match, they will be excluded. The final result prints the original <kbd>input_data</kbd>, which has been tokenized along with the results after the stopwords have been removed. Be sure to use the correct indentation when entering the code:</li>
</ol>
<pre style="padding-left: 60px">In[]: input_data_cleaned = [x for x in word_tokens if not x in stop_words]<br/>input_data_cleaned = []<br/><br/>for x in word_tokens:    <br/>    if x not in stop_words:        <br/>        input_data_cleaned.append(x)       <br/>print(word_tokens)<br/>print(input_data_cleaned)</pre>
<p style="padding-left: 60px">The output would look like the following screenshot where the original sentence is displayed as tokens that include all words. The second line of the output will have fewer token words because stopwords such as <kbd>the</kbd> have been removed:</p>
<p class="CDPAlignCenter CDPAlign"><img src="img/da7050f1-9b2c-4400-83d6-537c97aaf38c.png" style="width:53.83em;height:12.50em;"/></p>
<p>Excellent, we have learned now to exclude common words, which removes the noise from large volumes of text. The focus of your analysis will be on keywords and phrases to provide context within the text without reading through the entire body of unstructured data. </p>
<h1 id="uuid-d5d8cabc-2f53-46f9-97c4-2586006cd4bc">Summary</h1>
<p>Congratulations, you have successfully walked through the foundations of <strong>Natural Language Processing</strong> (<strong>NLP</strong>), along with key features that are available when working with unstructured data. We explored the <strong>Natural Language Toolkit</strong> (<strong>NLTK</strong>) Python library, which offers many options to work with free text by downloading different corpora to analyze large bodies of text. We learned how to split raw text into meaningful units called tokens so it can be interpreted and refined. We learned about regex and pattern matching using words as it applies to NLP. We also explored how to count the frequency of words in a collection of text using probability and statistical modules. Next, we learned how to normalize words using stemming and lemmatization functions, which shows how variations in words can impact your data analysis. We explained the concepts of n-grams and how to use <kbd>stopwords</kbd> to remove the noise that is common when working with large bodies of free text data.</p>
<p>In the next chapter, <a href="1a658723-79aa-4447-8dbd-74206cad9aa1.xhtml">Chapter 11</a>, <em>Practical Sentiment Analysis,</em> we will show how prediction models can be applied to unstructured data. </p>
<h1 id="uuid-dbd47ae9-3e1f-4d48-98fa-5524095bf4ce">Further reading</h1>
<p>For more information on the relative topics of this chapter, you can refer to the following links:</p>
<ul>
<li>Creating random sample data: <a href="https://www.mockaroo.com/">https://www.mockaroo.com/</a></li>
<li>The NLTK source code and documentation: <a href="https://www.nltk.org/">https://www.nltk.org/</a></li>
<li>NLP Stanford University reference: <a href="https://nlp.stanford.edu/">https://nlp.stanford.edu/</a></li>
</ul>


            

            
        
    </body></html>