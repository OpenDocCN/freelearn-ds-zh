- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Understanding Apache Spark and Its Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the advent of machine learning and data science, the world is seeing a
    paradigm shift. A tremendous amount of data is being collected every second, and
    it’s hard for computing power to keep up with this pace of rapid data growth.
    To make use of all this data, Spark has become a de facto standard for big data
    processing. Migrating data processing to Spark is not only a question of saving
    resources that will allow you to focus on your business; it’s also a means of
    modernizing your workloads to leverage the capabilities of Spark and the modern
    technology stack to create new business opportunities.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What is Apache Spark?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why choose Apache Spark?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different components of Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the Spark use cases?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Who are the Spark users?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is Apache Spark?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Spark is an open-source big data framework that is used for multiple
    big data applications. The strength of Spark lies in its superior parallel processing
    capabilities that makes it a leader in its domain.
  prefs: []
  type: TYPE_NORMAL
- en: According to its website ([https://spark.apache.org/](https://spark.apache.org/)),
    “*The most widely-used engine for* *scalable computing.*”
  prefs: []
  type: TYPE_NORMAL
- en: The history of Apache Spark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Apache Spark started as a research project at the UC Berkeley AMPLab in 2009
    and moved to an open source license in 2010\. Later, in 2013, it came under the
    Apache Software Foundation ([https://spark.apache.org/](https://spark.apache.org/)).
    It gained popularity after 2013, and today, it serves as a backbone for a large
    number of big data products across various Fortune 500 companies and has thousands
    of developers actively working on it.
  prefs: []
  type: TYPE_NORMAL
- en: Spark came into being because of limitations in the Hadoop MapReduce framework.
    MapReduce’s main premise was to read data from disk, distribute that data for
    parallel processing, apply map functions to the data, and then reduce those functions
    and save them back to disk. This back-and-forth reading and saving to disk becomes
    time-consuming and costly very quickly.
  prefs: []
  type: TYPE_NORMAL
- en: To overcome this limitation, Spark introduced the concept of in-memory computation.
    On top of that, Spark has several capabilities that came as a result of different
    research initiatives. You will read more about them in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Spark differentiators
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Spark’s foundation lies in its major capabilities such as in-memory computation,
    lazy evaluation, fault tolerance, and support for multiple languages such as Python,
    SQL, Scala, and R. We will discuss each one of them in detail in the following
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with in-memory computation.
  prefs: []
  type: TYPE_NORMAL
- en: In-memory computation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first major differentiator technology that Spark’s foundation is built on
    is that it utilizes in-memory computations. Remember when we discussed Hadoop
    MapReduce technology? One of its major limitations is to write back to disk at
    each step. Spark saw this as an opportunity for improvement and introduced the
    concept of in-memory computation. The main idea is that the data remains in memory
    as long as it is worked on. If we can work with the size of data that can be stored
    in the memory at once, we can eliminate the need to write to disk at each step.
    As a result, the complete computation cycle can be done in memory if we can work
    with all computations on that amount of data. Now, the thing to note here is that
    with the advent of big data, it’s hard to contain all the data in memory. Even
    if we look at heavyweight servers and clusters in the cloud computing world, memory
    remains finite. This is where Spark’s internal framework of parallel processing
    comes into play. Spark framework utilizes the underlying hardware resources in
    the most efficient manner. It distributes the computations across multiple cores
    and utilizes the hardware capabilities to the maximum.
  prefs: []
  type: TYPE_NORMAL
- en: This tremendously reduces the computation time, since the overhead of writing
    to disk and reading it back for the subsequent step is minimized as long as the
    data can be fit in the memory of Spark compute.
  prefs: []
  type: TYPE_NORMAL
- en: Lazy evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Generally, when we work with programming frameworks, the backend compilers
    look at each statement and execute it. While this works great for programming
    paradigms, with big data and parallel processing, we need to shift to a look-ahead
    kind of model. Spark is well known for its parallel processing capabilities. To
    achieve even better performance, Spark doesn’t execute code as it reads it, but
    once the code is there and we submit a Spark statement to execute, the first step
    is that Spark builds a logical map of the queries. Once that map is built, then
    it plans what the best path of execution is. You will read more about its intricacies
    in the Spark architecture chapters. Once the plan is established, only then will
    the execution begin. Once the execution begins, even then, Spark holds off executing
    all statements until it hits an “action” statement. There are two types of statements
    in Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: Transformations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Actions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You will learn more about the different types of Spark statements in detail
    in [*Chapter 3*](B19176_03.xhtml#_idTextAnchor053), where we discuss Spark architecture.
    Here are a few advantages of lazy evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: Efficiency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Code manageability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Query and resource optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduced complexities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resilient datasets/fault tolerance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Spark’s foundation is built on **resilient distributed datasets** (**RDDs**).
    It is an immutable distributed collection of objects that represent a set of records.
    RDDs are distributed across a number of servers, and they are computed in parallel
    across multiple cluster nodes. RDDs can be generated with code. When we read data
    from an external storage location into Spark, RDDs hold that data. This data can
    be shared across multiple clusters and can be computed in parallel, thus giving
    Spark a very efficient way of running computations on RDD data. RDDs are loaded
    in memory for processing; therefore, loading to and from memory computations is
    not required, unlike Hadoop.
  prefs: []
  type: TYPE_NORMAL
- en: RDDs are fault-tolerant. This means that if there are failures, RDDs have the
    ability to self-recover. Spark achieves that by distributing these RDDs to different
    worker nodes while keeping in view what task is performed by which worker node.
    This handling of worker nodes is done by the Spark driver. We will discuss this
    in detail in upcoming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: RDDs give a lot of power to Spark in terms of resilience and fault-tolerance.
    This capability, along with other features, makes Spark the tool of choice for
    any production-grade applications.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple language support
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Spark supports multiple languages for development such as Java, R, Scala, and
    Python. This gives users the flexibility to use any language of choice to build
    applications in Spark.
  prefs: []
  type: TYPE_NORMAL
- en: The components of Spark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s talk about the different components Spark has. As you can see in *Figure
    1**.1*, Spark Core is the backbone of operations in Spark and spans across all
    the other components that Spark has. Other components that we’re going to discuss
    in this section are Spark SQL, Spark Streaming, Spark MLlib, and GraphX.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1: Spark components](img/B19176_02_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.1: Spark components'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at the first component of Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Spark Core
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Spark Core is central to all the other components of Spark. It provides functionalities
    and core features for all the different components. Spark SQL, Spark Streaming,
    Spark MLlib, and GraphX all make use of Spark Core as their base. All the functionality
    and features of Spark are controlled by Spark Core. It provides in-memory computing
    capabilities to deliver speed, a generalized execution model to support a wide
    variety of applications, and Java, Scala, and Python APIs for ease of development.
  prefs: []
  type: TYPE_NORMAL
- en: In all of these different components, you can write queries in supported languages.
    Spark will then convert these queries to **directed acyclic graphs** (**DAGs**),
    and Spark Core has the responsibility of executing them.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key responsibilities of Spark Core are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Interacting with storage systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Task distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Task scheduling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Task monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In-memory computation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fault tolerance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark Core contains an API for RDDs which are an integral part of Spark. It
    also provides different APIs to interact and work with RDDs. All the components
    of Spark work with underlying RDDs for data manipulation and processing. RDDs
    make it possible for Spark to have a lineage for data, since they are immutable.
    This means that every time an operation is run on an RDD that requires changes
    in it, Spark will create a new RDD for it. Hence, it maintains the lineage information
    of RDDs and their corresponding operations.
  prefs: []
  type: TYPE_NORMAL
- en: Spark SQL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SQL is the most popular language for database and data warehouse applications.
    Analysts use this language for all their exploratory data analysis on relational
    databases and their counterparts in traditional data warehouses. Spark SQL adds
    this advantage to the Spark ecosystem. Spark SQL is used to query structured data
    in SQL using the DataFrame API.
  prefs: []
  type: TYPE_NORMAL
- en: As its name represents, Spark SQL gives SQL support to Spark. This means we
    can query the data present in RDDs and other external sources, such as Parquet
    files. This is a powerful capability of Spark, since it gives developers the flexibility
    to use a relational table structure on top of RDDs and other file formats and
    write SQL queries on top of it. This also adds the capabilities of using SQL where
    necessary and unifies it with analytics applications and use cases, thus providing
    unification of the platforms.
  prefs: []
  type: TYPE_NORMAL
- en: 'With Spark SQL, developers are able to do the following with ease:'
  prefs: []
  type: TYPE_NORMAL
- en: They can read data from different file formats and different sources into RDDs
    and DataFrames
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They can run SQL queries on top of the data present in DataFrames, thus giving
    flexibility to the developers to use programming languages or SQL to process data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once they’re done with the processing of the data, they have the capability
    to write RDDs and DataFrames to external sources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark SQL consists of a cost-based optimizer that optimizes queries, keeping
    in view the resources; it also has the capability to generate code for these optimizations,
    which makes these queries very fast and efficient. To support even faster query
    times, it can scale to multiple nodes with the help of Spark Core and also provides
    features such as fault tolerance and resiliency. This is known as the Catalyst
    optimizer. We will read more about it in [*Chapter 5*](B19176_05.xhtml#_idTextAnchor115).
  prefs: []
  type: TYPE_NORMAL
- en: 'The most noticeable features of Sparks SQL are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: It provides an engine for high-level structured APIs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reads/writes data to and from a large number of file formats such as Avro, Delta,
    **Comma-Separated Values** (**CSV**), and Parquet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provides **Open Database Connectivity** (**ODBC**)/**Java Database Connectivity**
    (**JDBC**) connectors to **business intelligence** (**BI**) tools such as PowerBI
    and Tableau, as well as popular **relational** **databases** (**RDBMs**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provides a way to query structured data in files as tables and views
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It supports ANSI SQL:2003-compliant commands and HiveQL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have covered SparkSQL, let’s discuss the Spark Streaming component.
  prefs: []
  type: TYPE_NORMAL
- en: Spark Streaming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have talked about the rapid growth of data in today’s times. If we were
    to divide this data into groups, there are two types of datasets in practice,
    batch and streaming:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Batch data** is when there’s a chunk of data present that you have to ingest
    and then transform all at once. Think of when you want to get a sales report of
    all the sales in a month. You would have the monthly data available as a batch
    and process it all at once.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Streaming data** is when you need output of that data in real time. To serve
    this requirement, you would have to ingest and process that data in real time.
    This means every data point can be ingested as a single data element, and we would
    not wait for it to be ingested after a block of data is collected. Think of when
    self-driving cars need to make decisions in real time based on the data they collect.
    All the data needs to be ingested and processed in real time for the car to make
    effective decisions in a given moment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are a large number of industries generating streaming data. To make use
    of this data, you need real-time ingestion, processing, and management of this
    data. It has become essential for organizations to use streaming data as it arrives
    for real-time analytics and other use cases. This gives them an edge over their
    competitors, as this allows them to make decisions in real time.
  prefs: []
  type: TYPE_NORMAL
- en: Spark Streaming enables organizations to make use of streaming data. One of
    the most important factors of Spark Streaming is its ease of use alongside batch
    data processing. You can combine batch and stream data within one framework and
    use it to augment your analytics applications. Spark Streaming also inherits Spark
    Core’s features of resilience and fault tolerance, giving it a dominant position
    in the industry. It integrates with a large number of streaming data sources such
    as HDFS, Kafka, and Flume.
  prefs: []
  type: TYPE_NORMAL
- en: The beauty of Spark Streaming is that batch data can be processed as streams
    to take advantage of built-in paradigms of streaming data and look-back capabilities.
    There are certain factors that need to be taken into consideration when we work
    with real-time data. When we work with real-time data streams, there’s a chance
    that some of the data may get missed due to system hiccups or failures altogether.
    Spark Streaming takes care of this in a seamless way. To cater to these requirements,
    it has a built-in mechanism called **checkpoints**. The purpose of these checkpoints
    is to keep track of the incoming data, knowing what was processed downstream and
    which data is still left to be processed in the next cycle. We will learn more
    about this in [*Chapter 7*](B19176_07.xhtml#_idTextAnchor183) when we discuss
    Spark Streaming in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: This makes Spark resilient to failures. If there are any failures, you need
    minimal work to reprocess old data. You can also define mechanisms and algorithms
    for missing data or late processed data. This gives a lot of flexibility to the
    data pipelines and makes them easier to maintain in large production environments.
  prefs: []
  type: TYPE_NORMAL
- en: Spark MLlib
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Spark provides a framework for distributed and scalable machine learning. It
    distributes the computations across different nodes, thus resulting in better
    performance for model training. It also distributes hyperparameter tuning. You
    will learn more about hyperparameter tuning in [*Chapter 8*](B19176_08.xhtml#_idTextAnchor220),
    where we talk about machine learning. Because Spark can scale to large datasets,
    it is the framework of choice for machine learning production pipelines. When
    you build products, execution and computation speed matter a lot. Spark gives
    you the ability to work with large amounts of data and build state-of-the-art
    machine learning models that can run very efficiently. Instead of working with
    models that take days to train, Spark reduces that time to hours. In addition,
    working with more data results in better-performing models in most cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Most of the commonly used machine learning algorithms are part of Spark’s libraries.
    There are two machine learning packages available in Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: Spark MLlib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The major difference between these two is the type of data they work with. Spark
    MLlib is built on top of RDDs while Spark ML works with DataFrames. Spark MLlib
    is the older library and has now entered maintenance mode. The more up-to-date
    library is Spark ML. You should also note that Spark ML is not the official name
    of the library itself, but it is commonly used to refer to the DataFrame-based
    API in Spark. The official name is still Spark MLlib. However, it’s important
    to know the differences.
  prefs: []
  type: TYPE_NORMAL
- en: Spark MLlib contains the most commonly used machine learning libraries for **classification**,
    **regression**, **clustering**, and **recommendation systems**. It also has some
    support for frequent pattern mining algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: When there is a need to serve these models to millions and billions of users,
    Spark is also helpful. You can distribute and parallelize both data processing
    (**Extract, Transform, Load** (**ETL**)) and model scoring with Spark.
  prefs: []
  type: TYPE_NORMAL
- en: GraphX
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: GraphX is Spark’s API for graphs and graph-parallel computation. GraphX extends
    Spark’s RDD to work with graphs and allows you to run parallel computations with
    graph objects. This speeds up the computations significantly.
  prefs: []
  type: TYPE_NORMAL
- en: Here’s a network graph that represents what a graph looks like.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2: A network graph](img/B19176_02_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.2: A network graph'
  prefs: []
  type: TYPE_NORMAL
- en: A graph is an object with vertices and edges. Properties are attached to each
    vertex and edge. There are primary graph operations that Spark supports, such
    as `subgraph` and `joinVertices`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main premise is that you can use GraphX for exploratory analysis and ETL
    and transform and join graphs with RDDs efficiently. There are two types of operator—
    `Graph` and `GraphOps`. On top of that, graph aggregation operators are also available.
    Spark also includes a number of graph algorithms that are used in common use cases.
    Some of the most popular algorithms are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: PageRank
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connected components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Label propagation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SVD++
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strongly connected components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Triangle count
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let’s discuss why we want to use Spark in our applications and what some
    of the features it provides are.
  prefs: []
  type: TYPE_NORMAL
- en: Why choose Apache Spark?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will discuss the applications of Apache Spark and its features,
    such as speed, reusability, in-memory computations, and how Spark is a unified
    platform.
  prefs: []
  type: TYPE_NORMAL
- en: Speed
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Apache Spark is one of the fastest processing frameworks for data available
    today. It beats Hadoop MapReduce by a large margin. The main reason is its in-memory
    computation capabilities and lazy evaluation. We will learn more about this when
    we discuss Spark architecture in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Reusability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Reusability is a very important consideration for large organizations making
    use of modern platforms. Spark can join batch and stream data seamlessly. Moreover,
    you can augment datasets with historical data to serve your use cases better.
    This gives a large historical view of data to run queries or build modern analytical
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: In-memory computation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With in-memory computation, all the overhead of reading and writing to disks
    is eliminated. The data is cached, and at each step, the required data is already
    present in memory. At the end of the processing, results are aggregated and sent
    back to the driver for further steps.
  prefs: []
  type: TYPE_NORMAL
- en: All of this is facilitated by the process of DAG creation that Spark performs
    inherently. Before execution, Spark creates a DAG of the necessary steps and prioritizes
    them based on its internal algorithms. We will learn more about this in the next
    chapter. These capabilities support in-memory computation, resulting in fast processing
    speeds.
  prefs: []
  type: TYPE_NORMAL
- en: A unified platform
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Spark provides a unified platform for data engineering, data science, machine
    learning, analytics, streaming, and graph processing. All of these components
    are integrated with Spark Core. The core engine is very high-speed and generalizes
    the commonly needed tasks for its other components. This gives Spark an advantage
    over other platforms because of the unification of its different components. These
    components can work in conjunction with each other, providing a unified experience
    for software applications. In modern applications, this unification makes it easy
    to use, and different parts of the application can make use of the core capabilities
    of these components without compromising on features.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you understand the benefits of using Spark, let’s talk about the different
    use cases of Spark in the industry.
  prefs: []
  type: TYPE_NORMAL
- en: What are the Spark use cases?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn about how Spark is used in the industry. There
    are various use cases of Spark prevalent today, some of which include big data
    processing, machine learning applications, near-real-time and real-time streaming,
    and using graph analytics.
  prefs: []
  type: TYPE_NORMAL
- en: Big data processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most popular use cases for Spark is big data processing. You might
    be wondering what big data is, so let’s take a look at the components that mark
    data as big data.
  prefs: []
  type: TYPE_NORMAL
- en: The first component of big data is the **volume of data**. By volume, we mean
    that the data is very large in size, often amounting to terabytes, petabytes,
    and beyond in some cases. Organizations have collected a large amount of data
    over the years. This data can be used for analysis. However, the first step in
    this activity is to process these large amounts of data. Also, it’s only recently
    that computing power has grown to now be able to process such vast volumes of
    data.
  prefs: []
  type: TYPE_NORMAL
- en: The second component of big data is the **velocity of data**. The velocity of
    data refers to the speed of its generation, ingestion, and distribution. This
    means that the speed with which this data is generated has increased manifold
    in recent years. Take, for example, data generated by your smart appliance that
    sends data every second to a server. In this process, the server also needs to
    keep up with the ingestion of this data, and then distributing this across different
    sources might be the next step.
  prefs: []
  type: TYPE_NORMAL
- en: The third component of big data is the **variety of data**. The variety of data
    refers to the different sources that generate the data. It also refers to different
    types of data that are generated. Gone are the days when data was only generated
    in structured formats that could be saved as tables in databases. Currently, data
    can be structured, semi-structured, or unstructured. The systems now have to work
    with all these different data types, and tools should be able to manipulate these
    different data types. Think of images that need to be processed or audio and video
    files that can be analyzed with advanced analytics.
  prefs: []
  type: TYPE_NORMAL
- en: Some other components can be added to the original three Vs as well, such as
    veracity and value. However, these components are out of the scope of our discussion.
  prefs: []
  type: TYPE_NORMAL
- en: Big data is too large for regular machines to process it. That’s why it’s called
    big data. Big data that is high-volume, high-velocity, and high-variety needs
    to be processed with advanced analytical tools such as Spark, which can distribute
    the workload across different machines or clusters and does processing in parallel
    to make use of all the resources available on a machine. So, instead of using
    only a single machine and loading all the data into one node, Spark gives us the
    ability to divide the data up into different parts and process them in parallel
    and across different machines. This massively speeds up the whole process and
    makes use of all the available resources.
  prefs: []
  type: TYPE_NORMAL
- en: For all of the aforementioned reasons, Spark is one of the most widely used
    big data processing technologies. Large organizations make use of Spark to analyze
    and manipulate their big data stacks. Spark serves as a backbone for big data
    processing in complex analytical use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some examples of big data use cases are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Business intelligence for reporting and dashboarding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data warehousing for complex applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Operational analytics for application monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is important to note here that working with Spark requires a mindset shift
    from single-node processing to big data-processing paradigms. You now have to
    start thinking about how to best utilize and optimize the use of large clusters
    for processing and what some of the best practices around parallel processing
    are.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As data grows, so does the need for machine learning models to make use of more
    and more data. The general understanding in the machine learning community today
    is that the more data is provided to the models, the better the models will be.
    This resulted in the need for massive amounts of data to be given to a model for
    predictive analytics. When we deal with massive amounts of data, the challenges
    for training machine learning models become more complex than data processing.
    The reason is that machine learning models crunch data and run statistical estimations
    to come to a minimum error point. To get to that minimum error, the model must
    do complex mathematical operations such as matrix multiplication. These computations
    require large amounts of data to be available in memory and then computations
    to run on it. This serves as the case for parallel processing in machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning adds an element of prediction to products. Instead of reacting
    to the changes that have already taken place, we can proactively look for ways
    to improve our products and services based on historical data and trends. Every
    aspect of an organization can make use of machine learning for predictive analytics.
    Machine learning can be applied to a number of industries, from hospitals to retail
    stores to manufacturing organizations. All of us have encountered some kind of
    machine learning algorithms when we do tasks on the internet, such as online buying
    and selling, browsing and searching for websites, and using social media platforms.
    Machine learning has become a major part of our lives knowingly or unknowingly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although there’s a large number of use cases that an organization can make
    use of in terms of machine learning, I’m highlighting only a few here:'
  prefs: []
  type: TYPE_NORMAL
- en: Personalized shopping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Website searches and ranking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fraud detection for banking and insurance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customer sentiment analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customer segmentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recommendation engines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Price optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predictive maintenance and support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text and video analytics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customer/patient 360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s move on to cover real-time streaming next.
  prefs: []
  type: TYPE_NORMAL
- en: Real-time streaming
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Real-time streaming is one of the use cases where Spark really shines. There
    are very few competing frameworks that offer the flexibility that Spark Streaming
    has.
  prefs: []
  type: TYPE_NORMAL
- en: Spark Streaming provides a mechanism to ingest data from multiple streaming
    data sources, such as Kafka and Amazon Kinesis. Once the data is ingested, it
    can be processed in real time with very efficient Spark Streaming processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a large number of real-time use cases that can make use of Spark
    Streaming. Some of them are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Self-driving cars
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-time reporting and analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Providing updates on stock market data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Internet of Things (IoT) data ingestion and processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-time news data processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-time analytics for optimization of inventory and operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-time fraud detection systems for credit cards
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-time event detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-time recommendations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Large global organizations make use of Spark Streaming to process billion and
    trillions of data rows in real time. We see some of this in action in our everyday
    life. Your credit card blocking a transaction while you’re out shopping is one
    such example of real-time fraud detection in action. Netflix and YouTube use real-time
    interactions, with the video platforms recommending users what to watch next.
  prefs: []
  type: TYPE_NORMAL
- en: As we move to a world of every device sending data back to its server for analysis,
    there’s an increased need for streaming and real-time analysis. One of the main
    advantages of using Spark Streaming for this kind of data is the built-in capabilities
    it has, for look-back and late processing of data. We discussed the usefulness
    of this approach earlier as well, and a lot of manual pipeline processing work
    is removed due to these capabilities. We will learn more about this when we discuss
    Spark Streaming in [*Chapter 7*](B19176_07.xhtml#_idTextAnchor183).
  prefs: []
  type: TYPE_NORMAL
- en: Graph analytics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Graph analytics provides a unique way of looking at data by analyzing relationships
    between different entities. The vertices of a graph represent the entities and
    the edges of a graph represent the relationship between two entities. Think of
    your social network on Facebook or Instagram. You represent one entity, and the
    people you are connected to represent another entity. The relationship (connection)
    between you and your friends is the edge. Similarly, your interests on your social
    media could all be different edges. Then, there can be a location category, for
    which all people who belong to one location would have an edge (a relationship)
    with that location, and so on. Therefore, connections can be made with any different
    type of entities. The more connected you are, the higher the chance that you are
    connected to like-minded people or interests. This is one method of measuring
    relationships between different entities. There are several uses for these kinds
    of graphs. The beauty of Spark is the distributed processing of these graphs to
    find these relationships very quickly. There can be millions and billions of connections
    for billions of entities. Spark has the capability to distribute these workloads
    and compute complex algorithms very fast.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some use cases of graph analytics:'
  prefs: []
  type: TYPE_NORMAL
- en: Social network analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fraud detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Page ranking based on relevance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weather prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Search engine optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supply chain analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding influencers on social media
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Money laundering and fraud detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With a growing number of use cases of graph analytics, this proves to be a critical
    use case in the industry today where we need to analyze networks of relationships
    among entities.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’re going to discuss who the Spark users are and what
    their typical role is within an organization.
  prefs: []
  type: TYPE_NORMAL
- en: Who are the Spark users?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As the world moves toward data-driven decision-making approaches, the role of
    data and the different types of users who can leverage it for critical business
    decisions has become paramount. There are different types of users in data who
    can leverage Spark for different purposes. I will introduce some of those different
    users in this section. This is not an exhaustive list, but it should give you
    an idea of the different roles that exist in data-driven organizations today.
    However, as the industry grows, many more new roles are coming up that are similar
    to the ones present in the following sections, although each may have its own
    separate role.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll start with the role of data analysts.
  prefs: []
  type: TYPE_NORMAL
- en: Data analysts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The more traditional role in data today is a data analyst. The data analyst
    is typically the first-tier role in data. What this means is that data analysts
    are at the core of decision making in organizations. This role spans across different
    business units in an organization, and oftentimes, data analysts have to interact
    with multiple business stakeholders to put across their requirements. This requires
    knowledge of the business domain as well as its processes. When an analyst has
    an understanding of the business and its goals, only then can they perform their
    duties best. Moreover, a lot of times, the requirement is to make current processes
    more efficient, which results in a better bottom line for the business. Therefore,
    having an understanding of not just the business goals but also how it all works
    together is one of the main requirements for this role.
  prefs: []
  type: TYPE_NORMAL
- en: 'A typical job role for a data analyst may look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: When data analysts are given a project in an organization, the first step in
    the project is to gather requirements from multiple stakeholders. Let’s work with
    an example here. Say you joined an organization as a data analyst. This organization
    makes and sells computer hardware. You are given the task of reporting on the
    revenue each month for the last 10 years. The first step for you would be to gather
    all requirements. It is possible that some stakeholders want to know how many
    units of certain products are sold each month, while others may want to know whether
    the revenues are consistently growing or not. Remember, the end users of your
    reports might work in different business units of the organization.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once you have all the requirements gathered from all the concerned stakeholders,
    then you move on to the next step, which is to look for the relevant data sources
    to answer the questions that you are tasked with. You may need to talk with database
    administrators in the organization or platform architects to know where the different
    data sources reside that have relevant information for you to extract.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once you have all the relevant sources, then you want to connect with those
    sources programmatically (in most cases) and clean and join some data together
    to come up with relevant statistics, based on your requirements. This is where
    Spark would help you connect to these different data sources and also read and
    manipulate the data most efficiently. You also want to slice and dice the data
    based on your business requirements. Once the data is clean and statistics are
    generated, you want to generate some reports based on these statistics. There
    are different tools in the market to generate reports, such as Qlik and Tableau,
    that you can work with. Once the reports are generated, you may want to share
    your results with the stakeholders. You could present your results to them or
    share the reports with them, depending on what the preferred medium is. This will
    help stakeholders make informed business-critical decisions that are data-driven
    in nature.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Collaboration across different roles also plays an important role for data analysts.
    Since organizations have been collecting data for a long time, the most important
    thing is working with all the data that has been collected over the years and
    making sense of it, helping businesses with critical decision making. Helping
    with data-driven decision making is the key to being a successful data analyst.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a summary of the steps taken in a project, as discussed in the previous
    paragraphs:'
  prefs: []
  type: TYPE_NORMAL
- en: Gather requirements from stakeholders.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Identify the relevant data sources.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Collaborate with subject matter experts (SMEs).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Slice and dice data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate reports.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Share the results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s look at data engineers next. This role is gaining a lot of traction in
    the industry today.
  prefs: []
  type: TYPE_NORMAL
- en: Data engineers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The next role that is getting more and more prevalent in the industry is a data
    engineer. This is a relatively new role but has gained immense popularity in recent
    times. The reason for this is that data is growing at tremendous levels. We have
    more data being generated per second now than in a whole month a few years ago.
    Working with all this data requires specialized skills. The data can no longer
    be contained in the modest memory of most computers, so we have to make use of
    the massive scale of cloud computing to serve this purpose. As data needs are
    becoming a lot more complex, we need complex architectures to process and use
    this data for business decision making. This is where the role of the data engineer
    comes into play. The main job of the data engineer is to prepare data for ingestion
    for different purposes. The downstream systems that leverage this prepared data
    could be dashboards that run reports based on this data, or it could be a predictive
    analytics solution that works with advanced machine learning algorithms to make
    proactive decisions based on the data.
  prefs: []
  type: TYPE_NORMAL
- en: More broadly, data engineers are responsible for creating, maintaining, optimizing,
    and monitoring data pipelines that serve different use cases in an organization.
    These pipelines are typically known as Extract, Transform, Load (ETL) pipelines.
    The major differentiator is the sheer scale of data that data engineers have to
    work with. When there are downstream needs for data for BI reporting, advanced
    analytics, and/or machine learning, that is where data pipelines come into play
    for large projects.
  prefs: []
  type: TYPE_NORMAL
- en: 'A typical job role for a data engineer in an organization may look as follows.
    When data engineers are given a task to create a data pipeline for a project,
    the first thing they need to consider is the overall architecture of an application.
    There might be data architects in some organizations to help with some of the
    architecture requirements, but that might not always be the case. So, a data engineer
    would ask questions such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: What are the different sources of data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the size of the data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Where does the data reside today?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do we need to migrate the data between different tools?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we connect to the data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What kind of transformations are required for the data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How often does the data get updated?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Should we expect a schema change in the new data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we monitor the pipelines if there are failures?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do we need to create a notification system for failures?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do we need to add a retry mechanism for failures?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the timeout strategy for failures?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we run back-dated pipelines if there are failures?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we deal with bad data?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What strategy we should follow – ETL or ELT?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can we save the costs of computation?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once they have answers to these questions, then they start working on a resilient
    architecture to build data pipelines. Once those pipelines are run and tested,
    the next step is to maintain these pipelines and make the processing more efficient
    and visible for failure detection. The goal is to build these pipelines so that
    once everything is run, the end state of data is consistent for different downstream
    use cases. Too often, data engineers have to collaborate with data analysts and
    data scientists to come up with correct data transformation requirements, based
    on the required use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s talk about data scientists now, a job that has been advertised as “*the
    sexiest job of the 21st century*” on multiple forums.
  prefs: []
  type: TYPE_NORMAL
- en: Data scientists
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Traditionally, data has been used for decision making based on what has happened
    in the past. This means that organizations have been reactive, based on the data.
    Now, there’s been a paradigm shift in advanced and predictive analytics. This
    means instead of being reactive, organizations can be proactive in their decision
    making. They achieve this with the help of all the data that is available to organizations
    now. To make effective use of this data, data scientists play a major part. They
    take analytics to the next level, where instead of just looking at what has happened
    in the past, they have sophisticated machine learning algorithms to predict what
    could take place in the future as well. All this is based on the huge amounts
    of data that is available to them.
  prefs: []
  type: TYPE_NORMAL
- en: A typical job role for a data scientist in an organization may look as follows.
  prefs: []
  type: TYPE_NORMAL
- en: The data scientist is given a problem to solve or a question to answer. The
    first task is to see what kind of data is available to them that would help them
    answer this question. They would create a few hypotheses to test with the given
    data. If the results are positive and the data is able to answer some of the problem
    statements, then they move on to experimenting with the data and seek ways to
    more effectively answer the questions at hand. For this purpose, they would join
    different datasets together, and they would also transform the data to make it
    ready for some machine learning algorithms to consume. At this stage, they would
    also need to decide what kind of machine learning problem they aim to solve.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three major types of machine learning techniques that they can use:'
  prefs: []
  type: TYPE_NORMAL
- en: Regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on the technique decided and data transformations, they would then move
    to prototype with a few machine learning algorithms to create a baseline model.
    A baseline model is a very basic model that serves to answer the original question.
    Based on this baseline model, other models can be created that would be able to
    answer the question better. In some cases, some predefined rules can also serve
    as a baseline model. What this means is that the business might already be operating
    on some predefined rules that can serve as a baseline to compare the machine learning
    model. Once the initial prototyping is done, then the data scientist moves on
    to more advanced optimizations in terms of models. They can work with different
    hyperparameters of the model or experiment with different data transformations
    and sample sizes. All of this can be done in Spark or other tools and languages,
    depending on their preference. Spark has the edge to run these algorithms in a
    parallel fashion, making the whole process very efficient. Once the data scientist
    is happy with the model results based on different metrics, they would then move
    that model to a production environment where these models can be served to customers
    solving specific problems. At this point, they would hand over these models to
    machine learning engineers to start incorporating them into the pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a summary of the steps taken in a project, as discussed in the previous
    paragraph:'
  prefs: []
  type: TYPE_NORMAL
- en: Create and test a hypothesis.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transform the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Decide on a machine learning algorithm.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prototype with different machine learning models.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a baseline model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tune the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tune the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transition models to production.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s discuss the role of machine learning engineers next.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning engineers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Like data engineers, machine learning engineers also build pipelines, but these
    pipelines are primarily built for machine learning model deployment. Machine learning
    engineers typically take prototyped models created by data scientists and build
    machine learning pipelines around them. We will discuss what machine learning
    pipelines are and what some of the questions that need to be answered to build
    these pipelines are.
  prefs: []
  type: TYPE_NORMAL
- en: 'Machine learning models are built to solve complex problems and provide advanced
    analytic methods to serve a business. After prototyping, these models need to
    run in the production environments of the organizations and be deployed to serve
    customers. For deployment, there are several considerations that need to be taken
    into account:'
  prefs: []
  type: TYPE_NORMAL
- en: How much data is there for model training?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How many customers do we plan to serve concurrently?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How often do we need to retrain the models?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How often do we expect the data to change?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we scale the pipeline up and down based on demand?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we monitor failures in model training?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do we need notifications for failures?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do we need to add a retry mechanism for failures?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the timeout strategy for failures?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we measure model performance in production?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we tackle data drift?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do we tackle model drift?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once these questions are answered, the next step is to build a pipeline around
    these models. The main purpose of the pipeline would be such that when new data
    comes in, the pre-trained models are able to answer questions based on new datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use an example to better understand these pipelines. We’ll continue with
    the first example of an organization selling computer hardware:'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose the organization wants to build a recommender system on its website
    that recommends to users which products to buy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The data scientists have built a prototype model that works well with test data.
    Now, they want to deploy it to production.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To deploy this model, the machine learning engineers would have to see how they
    can incorporate this model on the website.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They would start by getting the data ingested from the website to get the user
    information.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once they have the information, they pass it through the data pipeline to clean
    and join the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They might also want to add some precomputed features to the model, such as
    the time of the year, to get a better idea of whether it’s a holiday season and
    some special deals are going on.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, they would need a REST API endpoint to get the latest recommendations
    for each user on the website.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After that, the website needs to be connected to the REST endpoint to serve
    the actual customers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once these models are deployed on live systems (the website, in our example),
    there needs to be a monitoring system for any errors and changes in either the
    model or the data. This is known as **model drift** and **data** **drift**, respectively.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data drift
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data may change over time. In our example, people’s preferences may change with
    time, or with seasonality, data may be different. For example, during a holiday
    season, people’s preferences might slightly change because they are looking to
    get presents for their friends and family, so recommending relevant products based
    on these preferences is of paramount importance for a business. Monitoring these
    trends and changes in the data would result in better models over time and would
    ultimately benefit the business.
  prefs: []
  type: TYPE_NORMAL
- en: Model drift
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Similar to data drift, we also have the concept of model drift. This means that
    the model changes over time, and the old model that was initially built is not
    the most performant in terms of recommending items to website visitors. With changing
    data, the model also needs to be updated from time to time. To get a sense of
    when a model needs to be updated, we need to have monitoring in place for models
    as well. This monitoring would constantly compare old model results with the new
    data and see whether model performance is degrading. If that’s the case, it’s
    time to update the model.
  prefs: []
  type: TYPE_NORMAL
- en: This whole life cycle of model deployment is typically the responsibility of
    machine learning engineers. Note that the process would slightly vary for different
    problems, but the overall idea remains the same.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about the basics of Apache Spark and why Spark is
    becoming a lot more prevalent in the industry for big data applications. We also
    learned about the different components of Spark and how these components are helpful
    in terms of application development. Then, we discussed the different roles that
    are present in the industry today and who can make use of Spark’s capabilities.
    Finally, we discussed the modern-day uses of Spark in different industry use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Sample questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Although these questions are not part of the Spark certification, it’s good
    to answer these to assess your understanding of the basics of Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: What are the core components of Spark?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When do we want to use Spark Streaming?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a look-back mechanism in Spark Streaming?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are some good use cases for Spark?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which roles in an organization should use Spark?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
