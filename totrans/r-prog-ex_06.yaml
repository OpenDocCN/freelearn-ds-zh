- en: Understanding Reviews with Text Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is well known that a very large percentage of relevant information originates
    in an unstructured form, an important player being text data. Text analysis, **Natural
    Language Processing** (**NLP**), **Information Retrieval** (**IR**), and **Statistical
    Learning** (**SL**) are some areas focused on developing techniques and processes
    to deal with this data. These techniques and processes discover and present knowledge,
    facts, business rules, relationships, among others, that is otherwise locked in
    textual form, impenetrable to automated processing.
  prefs: []
  type: TYPE_NORMAL
- en: Given the explosion of textual data we see nowadays, an important skill for
    analysts such as statisticians and data scientists is to be able to work efficiently
    with this data and find the insights they are looking for. In this chapter, we
    will try to predict whether a customer is going to make repeated purchases given
    the reviews being sent to The Cake Factory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since text analysis is a very broad research area, we need to narrow the techniques
    we will look at in this chapter to the most important ones. We will take a Pareto
    approach by focusing on 20% of techniques that will be used 80% of the time when
    doing text analysis. Some of the important topics covered in this chapter are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Document feature matrices as a basic data structure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random forests for predictive modeling with text data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Term frequency-inverse document frequencies for measuring importance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: N-gram modeling to bring back order into the analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singular vector decomposition for dimensionality reduction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cosine similarity to find similar feature vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentiment analysis as an added vector feature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter's required packages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Setting up the packages for this chapter may be a bit cumbersome because some
    of the packages depend on operating system libraries which can vary from computer
    to computer. Please check  [Appendix](part0296.html#8Q96G0-f494c932c729429fb734ce52cafce730)*,
    Required Packages* for specific instructions on how to install them for your operating
    system.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Package** | **Reason** |'
  prefs: []
  type: TYPE_TB
- en: '| `lsa` | Cosine similarity computation |'
  prefs: []
  type: TYPE_TB
- en: '| `rilba` | Efficient SVD decomposition |'
  prefs: []
  type: TYPE_TB
- en: '| `caret` | Machine learning framework |'
  prefs: []
  type: TYPE_TB
- en: '| `twitteR` | Interface to Twitter''s API |'
  prefs: []
  type: TYPE_TB
- en: '| `quanteda` | Text data processing |'
  prefs: []
  type: TYPE_TB
- en: '| `sentimentr` | Text data sentiment analysis |'
  prefs: []
  type: TYPE_TB
- en: '| `randomForest` | Random forest models |'
  prefs: []
  type: TYPE_TB
- en: We will use the `rilba` package (which depends on C code) to compute a part
    of the **Singular Value Decomposition** (**SVD**) efficiently using the *Augmented
    Implicitly Restarted Lanczos Bidiagonalization Methods, by Baglama and Reichel,
    2005*, [http://www.math.uri.edu/~jbaglama/papers/paper14.pdf](http://www.math.uri.edu/~jbaglama/papers/paper14.pdf)).
  prefs: []
  type: TYPE_NORMAL
- en: We will use the `parallel` package to perform parallel processing since some
    text analysis can potentially require a lot of computations. The `parallel` package
    is the most general parallelization package in R for now, but it has been reported
    to not work correctly in some systems. Other options are `doParallel`, `doMC` and
    `doSNOW`. If you run into trouble when using one `parallel`, try switching to
    one of the other packages. The code to make them work is very similar.
  prefs: []
  type: TYPE_NORMAL
- en: Regarding text data, there are a few packages you can use in R. The most common
    ones are the `tm` package and the `quanteda` package. Both are excellent, and
    differ mostly in style. All the functionality we will see in this chapter can
    be used with either one of them, but we chose to work with the `quanteda` package.
    It is built with the `stringi` package for processing text, the `data.table` package
    for large documents, and the `Matrix` package to handle sparse objects. Therefore
    you can expect it to be very fast and handle Unicode and UTF-8 very well.
  prefs: []
  type: TYPE_NORMAL
- en: If you don't know what Unicode and UTF-8 are, I suggest you read up on them.
    Very roughly you can think of Unicode as being a standard of IDs for characters,
    while UTF-8 being a translation of this IDs into bytes computers can understand.
    During this chapter we won't worry about encodings (all the data is in UTF-8),
    but it's something that often comes up when working with text data, and is important
    to handle correctly.
  prefs: []
  type: TYPE_NORMAL
- en: What is text analysis and how does it work?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text analysis is the process of deriving information from text. Information
    is typically derived through techniques such as IR, NLP, and SL, and it involves
    structuring text, deriving patterns with the structured data, and finally evaluating
    and interpreting the output. The basic models used for text analysis are the bag-of-words
    models, the vector space model, and the semantic parsing model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The bag-of-words model is a simplified text representation in which a text
    (a review in our case) is represented as the set of its terms (words), disregarding
    grammar and word order but keeping multiplicity (hence the term bag). After transforming
    the text into a bag-of-words and structuring into a corpus (a structured collection
    of the text data), we can calculate various measures to characterize the text
    into a vector space. The bag-of-words model is commonly used in SL methods, and
    we will use it with random forests in this chapter. In practice, it is used as
    a tool of feature generation. The following image explains the bag-of-words model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00047.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Bag-of-words vs semantic parsing
  prefs: []
  type: TYPE_NORMAL
- en: The vector space model uses the bag-of-words that you extracted from the documents
    to create a feature vector for each text, where each feature is a term and the
    feature's value is a term weight. The term weight may be a binary value (1 indicating
    that the term occurred in the document and 0 indicating that it did not), a **term
    frequency** (**TF**) value (indicating how many times the term occurred in the
    document), or a **term frequency-inverse document frequency** (**TF-IDF**) value
    (indicating how important a term is to a text given its corpus). More complex
    weighting mechanisms exist which are focused on specific problems, but these are
    the most common ones, and are the ones we will focus on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given what we mentioned earlier, a text turns out to be a feature vector, and
    each feature vector corresponds to a point in a vector space. The model for this
    vector space is such that there is an axis for every term in the vocabulary, and
    so the vector space is *n*-dimensional, where *n* is the size of the vocabulary
    in all the data being analyzed (this can be huge). Sometimes, it helps to think
    about these concepts geometrically. The bag-of-words model and vector space model
    refer to different aspects of characterizing a body of text, and they complement
    each other. The following image explains the vector space model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00048.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Bag-of-words to vector space
  prefs: []
  type: TYPE_NORMAL
- en: An important weakness of the bag-of-words model is the fact that it ignores
    the terms' semantic context. More complex models exist that attempt to correct
    these deficiencies. Semantic parsing is one of them, and it's the process of mapping
    a natural-language sentence into a formal representation of its meaning. It mainly
    uses combinations of inductive logic programming and statistical learning. These
    types of techniques become more useful when dealing with complex texts. Even though
    we won't touch further on them on this book, they are a powerful tool and are
    a very interesting area of research.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, if you try to think of the representation of the following quote
    using the bag-of-words model and the semantic parsing model, you may intuitively
    think that the first one can give nonsense results while the second one can provide
    at least some understanding, and you would be correct.
  prefs: []
  type: TYPE_NORMAL
- en: '"The fish trap exists because of the fish. Once you''ve gotten the fish you
    can forget the trap. The rabbit snare exists because of the rabbit. Once you''ve
    gotten the rabbit, you can forget the snare. Words exist because of meaning. Once
    you''ve gotten the meaning, you can forget the words. Where can I find a man who
    has forgotten words so I can talk with him?"'
  prefs: []
  type: TYPE_NORMAL
- en: – The Writings of Chuang Tzu, 4th century B.C. (Original text in Chinese)
  prefs: []
  type: TYPE_NORMAL
- en: Preparing, training, and testing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As always, we will start by setting up our data. In this case, the data is
    the messages received by our fantasy company, The Cake Factory. These are in the
    `client_messages.RDS` file that we created in [Chapter 4](part0091.html#2MP360-f494c932c729429fb734ce52cafce730),
    *Simulating Sales Data and Working with Databases*. The data contains 300 observations
    for 8 variables: `SALE_ID`, `DATE`, `STARS`, `SUMMARY`, `MESSAGE`, `LAT`, `LNG`,
    and `MULT_PURCHASES`. During this chapter, we will work with the `MESSAGE` and
    `MULT_PURCHASES` variables.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will set up our seed to have reproducible results. Keep in mind that this
    should be before every function call that involves some randomization. We will
    show it just once here to save space and avoid repeating ourselves, but keep that
    in mind when you are trying to generate reproducible results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to make sure that we don''t have any missing data in the relevant
    variables. To do so, we use the `complete.cases()` function together with the
    negation (`!`) and the `sum()` function to get the total number of `NA` values''
    in each variable. As you can see, we don''t have any missing data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: If you have missing data, instead of using some imputation mechanism which is
    normally done in some data analysis scenarios, you want to remove those observations
    from this data, since it's easier to get this wrong due to the non-continuous
    characteristics of textual data.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you will often find when working on interesting real-world problems in predictive
    analysis, it''s not uncommon to work with disproportionate data. In this case,
    as can be seen with the shown code, we have around 63% of multiple purchases.
    This is not very disproportionate, but we still have to play on the safe side
    by keeping the training and testing data with similar proportions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'For data with the disproportionality problem, maintaining the same proportions
    in the testing and training sets is important to get accurate results. Therefore,
    we need to make sure our sampling method maintains these proportions. To do so,
    we will use the `createDataPartition()` function from the `caret` package to extract
    the indexes for each of the training and testing sets. It will create balanced
    data splits, and, in this case, it will use 70% of the data for training with
    a single partition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'To make sure that our proportions are being maintained, we can check each of
    them individually just as we did before with the full data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have our training and testing sets ready, we can start cleaning
    and setting up our text data as we will do in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Building the corpus with tokenization and data cleaning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first thing we need to create when working with text data is to extract
    the tokens that will be used to create our corpus. Simply, these tokens are all
    the terms found in every text in our data, put together, and removed the ordering
    or grammatical context. To create them, we use the `tokens()` function and the
    related functions from the `quanteda` package. As you can imagine, our data will
    not only contain words, but also punctuation marks, numbers, symbols, and other
    characters like hyphens. Depending on the context of the problem you''re working
    with, you may find it quite useful to remove all of them as we do here. However,
    keep in mind that in some contexts some of these special characters can be meaningful
    (for example, the hashtag symbol (#) can be relevant when analyzing Twitter data):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can imagine, there will be a huge number of tokens in the data because
    there must be one for each unique word in our data. These raw tokens will probably
    be useless (contain a low signal/noise ratio) if we don''t apply some filtering.
    We''ll start by ignoring capitalization. In our context, *something* and *something* should
    be equivalent. Therefore, we use the `tokens_tolower()` function to make all tokens
    lowercase:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, keep in mind that common words like *the*, *a*, and *to* are almost always
    the terms with highest frequency in the text and are not particularly important
    to derive insights. We should thus remove them, as we do with the `tokens_select()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Word stems allow us to reduce the number of tokens that overall share the same
    meaning. For example, the words *probability*, *probably*, and *probable* probably
    have the same meaning and their differences are mostly syntactic. Therefore, we
    could represent all of them with a single token like *probab* and reduce our feature
    space considerably. Note that all of these filters have assumptions behind them
    about the problem we''re dealing with, and you should make sure those assumptions
    are valid. In our case, they are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Instead of having to repeat this by hand every time, we want to create tokens;
    we can wrap all of these filters in a single function and make our lives a little
    bit easier down the road. The careful reader will note the `token_ngrams()` function
    and the corresponding `n_grams = 1` default parameter. We dedicate a section to
    this later, but for now, just know that `n_grams = 1` means that we want single
    words in our tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, even though we have shown the code that is used for this chapter''s example,
    we will use a smaller example (one sentence) so that you can get a visual idea
    of what''s going on in each step. You should definitely get in the habit of doing
    this yourself when exploring a problem to make sure everything is working as you
    expect it. We put the code for all the steps here and, after reading the preceding
    paragraphs, you should be able to identify the differences in each step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Document feature matrices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once we have our tokens ready, we need to create our corpus. At the most basic
    level, a **corpus** is a collection of texts that includes document-level variables
    specific to each text. The most basic *corpus* uses the bag-of-words and vector
    space models to create a matrix in which each row represents a text in our collection
    (a client message in our case), and each column represents a term. Each of the
    values in the matrix would be a 1 or a 0, indicating whether or not a specific
    term is included in a specific text. This is a very basic representation that
    we will not use. We will use a **document-feature matrix** (**DFM**), which has
    the same structure but, instead of using an indicator variable (1s and 0s), it
    will contain the number of times a term occurred within a text, using the multiplicity
    characteristic from the bag-of-words model. To create it, we use the `dfm()` function
    from the `quanteda` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'To get a visual example, here''s the DFM for the example shown in the previous
    section. We can see a couple of things here. First, it''s a special object with
    some metadata, number of documents (one sentence in our case), number of features
    (which is the number of tokens), dimensions, and the actual values for each text
    in our data. This is our corpus. If we had more than one sentence, we would see
    more than one row in it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, forget about the one-sentence example, and let''s go back to our client
    messages example. In that case the `tokens` object will be much larger. Normally,
    we tokenize and create our DFM in the same way. Therefore, we create the function
    that makes it a little easier for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can create our DFM easily with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'To get an idea of our DFM''s characteristics, you can simply print it. As you
    can see our training DFM has 212 documents (client messages) and 2,007 features
    (tokens). Clearly, most documents will not contain most of the features. Therefore
    we have a sparse structure, meaning that 98.4% of the entries in the DFM are actually
    zero. The educated reader will identify this as being the curse of dimensionality
    problem common to machine learning, and specially damaging in text analysis. As
    we will see later, this can be a computational bottleneck that we need to deal
    with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Often, tokenization requires some additional pre-processing. As you know by
    now, the tokens we find in our tokenization process end up being column (feature)
    names in our DFM. If these names contain symbols inside or start with numbers
    (for example, *something&odd* or *45pieces*), then some of our analysis algorithms
    will complain by throwing errors. We want to prevent that when we transform our
    DFM into a data frame. We can do so with the convenient `make.names()` function.
    We will also add the `MULT_PURCHASES` value (our dependent variable) to our newly
    created data frame at this point:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Again, to avoid having to repeat this boilerplate code, we can create our own
    function that packs this functionality, and easily create our data frame for analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, our training data is ready for analysis in the form of a data
    frame that can be used by our predictive models. To finish the section, if you
    want to know which are the most frequent terms in the data, you can use the `topfeatures()` function.
    In this case, most of the features can be intuitively guessed in their context.
    The only one that may require some explanation is the `br` feature. It comes from
    the fact that our data is coming from HTML pages that contain `<br>` strings that
    signal for a new line in the text (a break, hence the `br`). We could remove this
    feature if we wanted to, but we will leave for now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Training models with cross validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will efficiently train our first predictive model for this
    example and build the corresponding confusion matrix. Most of the functionality
    comes from the excellent `caret` package. You can find more information on the
    vast features within this package that we will not explore in this book in its
    documentation ([http://topepo.github.io/caret/index.html](http://topepo.github.io/caret/index.html)).
  prefs: []
  type: TYPE_NORMAL
- en: Training our first predictive model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Following best practices, we will use **Cross Validation** (**CV**) as the basis
    of our modeling process. Using CV we can create estimates of how well our model
    will do with unseen data. CV is powerful, but the downside is that it requires
    more processing and therefore more time. If you can take the computational complexity,
    you should definitely take advantage of it in your projects.
  prefs: []
  type: TYPE_NORMAL
- en: Going into the mathematics behind CV is outside of the scope of this book. If
    interested, you can find out more information on Wikipedia ([https://en.wikipedia.org/wiki/Cross-validation_(statistics)](https://en.wikipedia.org/wiki/Cross-validation_(statistics))). The
    basic idea is that the training data will be split into various parts, and each
    of these parts will be taken out of the rest of the training data one at a time,
    keeping all remaining parts together. The parts that are kept together will be
    used to train the model, while the part that was taken out will be used for testing,
    and this will be repeated by rotating the parts such that every part is taken
    out once. This allows you to test the training procedure more thoroughly, before
    doing the final testing with the testing data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use the `trainControl()` function to set our repeated CV mechanism with
    five splits and two repeats. This object will be passed to our predictive models,
    created with the `caret` package, to automatically apply this control mechanism
    within them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Our predictive models pick for this example are R*andom Forests* (RF). We will
    very briefly explain what RF are, but the interested reader is encouraged to look
    into James, Witten, Hastie, and Tibshirani's excellent "*Statistical Learning*"
    (Springer, 2013). RF are a non-linear model used to generate predictions. A *tree* is
    a structure that provides a clear path from inputs to specific outputs through
    a branching model. In predictive modeling they are used to find limited input-space
    areas that perform well when providing predictions. RF create many such trees
    and use a mechanism to aggregate the predictions provided by this trees into a
    single prediction. They are a very powerful and popular Machine Learning model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s have a look at the random forests example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00049.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Random forests aggregate trees
  prefs: []
  type: TYPE_NORMAL
- en: 'To train our model, we use the `train()` function passing a formula that signals
    R to use `MULT_PURCHASES` as the dependent variable and everything else (`~ .`)
    as the independent variables, which are the token frequencies. It also specifies
    the data, the method (`"rf"` stands for random forests), the control mechanism
    we just created, and the number of tuning scenarios to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Improving speed with parallelization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you actually executed the previous code in your computer before reading this,
    you may have found that it took a long time to finish (8.41 minutes in our case).
    As we mentioned earlier, text analysis suffers from very high dimensional structures
    which take a long time to process. Furthermore, using CV runs will take a long
    time to run. To cut down on the total execution time, use the `doParallel` package
    to allow for multi-core computers to do the training in parallel and substantially
    cut down on time.
  prefs: []
  type: TYPE_NORMAL
- en: We proceed to create the `train_model()` function, which takes the data and
    the control mechanism as parameters. It then makes a cluster object with the `makeCluster()` function
    with a number of available cores (processors) equal to the number of cores in
    the computer, detected with the `detectCores()` function. Note that if you're
    planning on using your computer to do other tasks while you train your models,
    you should leave one or two cores free to avoid choking your system (you can then
    use `makeCluster(detectCores() - 2)` to accomplish this). After that, we start
    our time measuring mechanism, train our model, print the total time, stop the
    cluster, and return the resulting model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can retrain the same model much faster. The time reduction will depend
    on your computer''s available resources. In the case of an 8-core system with
    32 GB of memory available, the total time was 3.34 minutes instead of the previous
    8.41 minutes, which implies that with parallelization, it only took 39% of the
    original time. Not bad right? We will study more the mechanics of parallelization
    and its advantages and disadvantages more in [Chapter 9](part0229.html#6QCGQ0-f494c932c729429fb734ce52cafce730),
    *Implementing An Efficient Simple Moving Average*. Let''s have look at how the
    model is trained:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Computing predictive accuracy and confusion matrices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have our trained model, we can see its results and ask it to compute
    some predictive accuracy metrics. We start by simply printing the object we get
    back from the `train()` function. As can be seen, we have some useful metadata,
    but what we are concerned with right now is the predictive accuracy, shown in
    the `Accuracy` column. From the five values we told the function to use as testing
    scenarios, the best model was reached when we used 356 out of the 2,007 available
    features (tokens). In that case, our predictive accuracy was 65.36%.
  prefs: []
  type: TYPE_NORMAL
- en: If we take into account the fact that the proportions in our data were around
    63% of cases with multiple purchases, we have made an improvement. This can be
    seen by the fact that if we just guessed the class with the most observations
    (`MULT_PURCHASES` being true) for all the observations, we would only have a 63%
    accuracy, but using our model we were able to improve toward 65%. This is a 3%
    improvement. We will try to increase this improvement as we go through this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keep in mind that this is a randomized process, and the results will be different
    every time you train these models. That''s why we want a repeated CV as well as
    various testing scenarios to make sure that our results are robust:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'To create a confusion matrix, we can use the `confusionMatrix()` function and
    send it the model''s predictions first and the real values second. This will not
    only create the confusion matrix for us, but also compute some useful metrics
    such as sensitivity and specificity. We won''t go deep into what these metrics
    mean or how to interpret them since that''s outside the scope of this book, but
    we highly encourage the reader to study them using the resources cited in this
    chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Improving our results with TF-IDF
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In general in text analysis, a high raw count for a term inside a text does
    not necessarily mean that the term is more important for the text. One of the
    most important ways to normalize the term frequencies is to weigh a term by how
    often it appears not only in a text, but also in the entire corpus.
  prefs: []
  type: TYPE_NORMAL
- en: The more a word appears inside a given text and doesn't appear too much across
    the whole corpus, it means that it's probably important for that specific text.
    However, if the term appears a lot inside a text, but also appears a lot in other
    texts in the corpus, it's probably not important for the specific text, but for
    the entire corpus, and this dilutes it's predictive power.
  prefs: []
  type: TYPE_NORMAL
- en: In IR, TF-IDF is one of the most popular term-weighting schemes and it's the
    mathematical implementation of the idea expressed in the preceding paragraph.
    The TF-IDF value increases proportionally to the number of times a word appears
    in a given text, but is diluted by the frequency of the word in the entire corpus
    (not only the given text), which helps to adjust for the fact that some words
    appear more frequently in general. It is a powerful technique for enhancing the
    information contained within a DFM.
  prefs: []
  type: TYPE_NORMAL
- en: 'The TF normalizes all documents in the corpus to be length independent. The
    **inverse document frequency** (**IDF**) accounts for the frequency of term appearance
    in all documents in the corpus. The multiplication of TF by IDF takes both of
    these concepts into account by multiplying them. The mathematical definitions
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*n(DFM):= number  of texts in DFM*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Freq(term, text):= count of term in text*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Freq(term, DFM):= count of term in DFM*'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00050.jpeg)![](img/00051.jpeg)![](img/00052.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Now we are going to show how to program this TF-IDF statistic with R functions.
    Remember that we're working with a `dfm`, so we can use vectorized operations
    to make our functions efficient and easy to program. The first three functions
    `term_frequency()`, `inverse_document_frequency()`, and `tf_idf()`, should be
    easy to understand.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `build_tf_idf()` function makes use of these functions to actually build
    the TF-IDF-weighted DFM. The idea being that we need to apply the functions we
    created to the rows or columns as necessary using the `apply()` function. We need
    to transpose the structure we get to get the texts in the rows and the features
    in the columns which is why we use the `t()` function midway through. Finally,
    we need to realize that sometimes we get NA''s for certain combinations of data
    (try to figure out these cases yourself to make sure you understand) and we need
    to substitute them with zeros:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can easily build our TF-IDF-weighted DFM using our `build_tf_df()` function
    and create the corresponding data frame as we have done earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Now the values inside our TF-IDF-weighted DFM will not only be integer values,
    which corresponded to frequency counts, but will be floating point values that
    correspond to the TF-IDF weights instead. We can train our next model using the
    same `train_model()` we used earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This time, our training process took 2.97 minutes. To see the results, simply
    print the model object. Remember that our previous predictive accuracy was of
    65.36%. Now that we have used the TF-IDF-weighted DFM, it increased to 66.48%.
    This is not a drastic improvement, and it''s due to the specific data we are working
    with. When working with other data or domains, you can expect this increase to
    be much larger:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Adding flexibility with N-grams
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The bag-of-words model takes into account isolated terms called **unigrams**.
    This looses the order of the words, which can be important in some cases. A generalization
    of the technique is called n-grams, where we use single words as well as word
    pairs or word triplets, in the case of bigrams and trigrams, respectively. The
    n-gram refers to the general case where you keep up to `n` words together in the
    data. Naturally this representation exhibits unfavorable combinatorial complexity
    characteristics and makes the data grow exponentially. When dealing with a large
    corpus this can take significant computing power.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the `sentence` object we created before to exemplify how the tokenization
    process works (it contains the sentence: `If it looks like a duck, swims like
    a duck, and quacks like a duck, then it probably is a duck`.) and the `build_dfm()` function
    we created with the `n_grams` argument, you can compare the resulting DFM with
    `n_grams = 2` to the one with `n_grams = 1`. After analyzing the features in this
    DFM, you should have a clear idea on how the tokenization process filters some
    data out and how the bigrams are created. As you can see, n-grams can potentially
    bring back some of the lost word ordering, which sometimes can be very useful:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'To retrain our full model, we will recreate our TF-IDF-weighted DFM with bigrams
    this time and its corresponding data frame. Using the function we created earlier,
    it can easily be done with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Now we will retrain the model and analyze its results. This can potentially
    take a lot of time depending on the computer you're using for training it. In
    our 8-core computer with 32 GB of memory, it took 21.62 minutes when executed
    in parallel. This is due to the large increase in the number of predictors. As
    you can see, we now have 9,366 predictors instead of the 2,007 predictors we had
    before. This huge 4x increase is due to the bigrams.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this particular case, it seems that the added complexity from the bigrams
    doesn''t increase our predictive accuracy. As a matter of fact, it decreases it.
    This can be for a couple of reasons, one of which is the increased sparsity, which
    implies a lower signal/noise ratio. In the next section, we will try to increase
    this ratio while keeping the bigrams:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Reducing dimensionality with SVD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have seen in the previous section, the dimensionality course in our data
    was amplified due to the n-gram technique. We would like to be able to use n-grams
    to bring back word ordering into our DFM, but we would like to reduce the feature
    space at the same time. To accomplish this, we can use a number of different dimensionality
    reduction techniques. In this case, we will show how to use the SVD.
  prefs: []
  type: TYPE_NORMAL
- en: The SVD helps us compress the data by using it's singular vectors instead of
    the original features. The math behind the technique is out of the scope of the
    book, but we encourage you to look at Meyer's, *Matrix Analysis & Applied Linear
    Algebra, 2000*. Basically, you can think of the singular vectors as the important
    directions in the data, so instead of using our *normal* axis, we can use these
    singular vectors in a transformed space where we have the largest signal/noise
    ratio possible. Computing the full SVD can potentially take a very large amount
    of time and we don't really need all the singular vectors. Therefore, we will
    use the `irlba` package to make use of the **Implicitly Restarted Lanczos Bidiagonalization
    Algorithm** (**IRLBA**) for a fast partial SVD, which is much faster.
  prefs: []
  type: TYPE_NORMAL
- en: 'When using the partial SVD as our DFM, we are actually working in a different
    vector space, where each feature is no longer a token, but a combination of tokens.
    These new features are not easy to comprehend and you shouldn''t try to. Treat
    it like a *black-box* model, knowing that you''re operating in a higher signal/noise
    ratio space than you started in while drastically reducing it''s dimensions. In
    our case, we will make the new space 1/4 of the original space. To do so, we will
    create a wrapper function to measure the time it takes to actually compute the
    partial SVD. The actual computation will be done with the `irlba()` function,
    sending the TF-IDF-weighted bigrams DFM and the number of singular vectors we
    want (1/4 of the possible ones) as the `nv` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Now we can easily create the partial SVD and the corresponding data frame. We
    also proceed to retrain our model. Note that even though it is conceptual, the
    `train.bigrams.svd` is our new DFM, in practice, within R, it's an object that
    contains our DFM as well as other data. Our DFM is in the `v` object within the
    `train.bigrams.svd` object, which is what we send to the `buildf_dfm_df()` function.
    Another important object within `train.bigrams.svd` is `d`,  which contains the
    singular values from the decomposition.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, our feature space was drastically reduced to only 53 features
    (which is approximately 1/4 of the 212 samples available). However, our predictive
    accuracy was not higher than our previous results either. This means that probably
    bigrams are not adding too much information for this particular problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Extending our analysis with cosine similarity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we proceed to another technique familiar in linear algebra which operates
    on a vector space. The technique is known as **cosine similarity** (**CS**), and
    its purpose is to find vectors that are similar (or different) from each other.
    The idea is to measure the direction similarity (not magnitude) among client messages,
    and try to use it to predict similar outcomes when it comes to multiple purchases.
    The cosine similarity will be between 0 and 1 when the vectors are orthogonal
    and perpendicular, respectively. However, this similarity should not be interpreted
    as percentage because the movement rate for the cosine function is not linear.
    This means that a movement from 0.2 to 0.3 does not represent a similar movement
    magnitude from 0.8 to 0.9.
  prefs: []
  type: TYPE_NORMAL
- en: Given two vectors (rows in our DFM), the cosine similarity among them is computed
    by taking the dot product between them and dividing it by the product of the Euclidian
    norms. To review what these concepts mean, take a look at Meyer's, *Matrix Analysis
    & Applied Linear Algebra, 2000*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00053.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'We create the `cosine_similarties()` function that will make use of the `cosine()` function
    from the `lsa` package. We send it a data frame and remove the first column, which
    corresponds to the dependent variable `MULT_PURCHASES`, and we use the transpose
    to make sure that we''re working with the correct orientation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we create the `mean_cosine_similarities()` function, which will take the
    cosine similarity among those texts that correspond to clients that have performed
    multiple purchases and will take the means of these similarities. We need to take
    the mean because we are computing many similarities among many vectors, and we
    want to aggregate them for each one of them. We could use other aggregation mechanisms,
    but the mean is fine for now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can use this function to generate a new DFM''s data frame that will
    be used to train a new model, which will take into account the cosine similarity
    among texts. As we saw earlier, it seems that using bigrams is not helping too
    much for this particular data. In the next section, we will try a different, very
    interesting, technique, sentiment analysis. Let''s look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Digging deeper with sentiment analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have now seen that vector space operations did not work too well regarding
    the predictive accuracy of our model. In this section, we will attempt a technique
    which is very different and is closer to the semantic parsing model we mentioned
    at the beginning of this chapter. We will try sentiment analysis.
  prefs: []
  type: TYPE_NORMAL
- en: We will not only take into account the words in a text, but we will also take
    into account shifters (that is, negators, amplifiers, de-amplifiers, and adversative
    conjunctions). A negator flips the sign of a polarized word (for example, I do
    not like it). An amplifier increases the impact of a polarized word (for example,
    I really like it.). A de-amplifier reduces the impact of a polarized word (for
    example, I hardly like it). An adversative conjunction overrules the previous
    clause containing a polarized word (for example, I like it but it's not worth
    it). This can be very powerful with some types of data.
  prefs: []
  type: TYPE_NORMAL
- en: Our sentiment analysis will produce a number, which will indicate the sentiment
    measured from the text. These numbers are unbounded and can be either positive
    or negative, corresponding to positive or negative sentiments. The larger the
    number, the stronger the inferred sentiment. To implement the technique, we will
    use the `sentimentr` package, which includes a clever algorithm to compute these
    sentiments. For the enthusiast, the details of the equation used are in its documentation
    ([https://cran.r-project.org/web/packages/sentimentr/sentimentr.pdf](https://cran.r-project.org/web/packages/sentimentr/sentimentr.pdf)).
  prefs: []
  type: TYPE_NORMAL
- en: To apply this technique, we send messages to the `sentiment_by()` function.
    This will give us back an object that contains, among other things, the `word_count` value
    and `ave_sentiment`, which is the average sentiment measured in all the sentences
    within a given text (`sentinmentr` internally splits each text into its components
    (sentences) and measures sentiment for each of them). We then add this objects
    into our DFM and proceed to train our model.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, this time we get a large increase in the predictive accuracy
    of the model up to 71.73%. This means that the sentiment feature is highly predictive
    compared to the other features we engineered in previous sections. Even though
    we could continue mixing models and exploring to see if we can get even higher
    predictive accuracy, we will stop at this point since you probably understand
    how to do these on your own at this point:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Sentiment analysis, even though it looks very easy since it did not require
    a lot of code thanks to the `sentimentr` package, is actually a very hard area
    with active research behind it. It's very important for companies to understand
    how their customers feel about them, and do so accurately. It is and will continue
    to be a very interesting area of research.
  prefs: []
  type: TYPE_NORMAL
- en: Testing our predictive model with unseen data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have our final model, we need to validate its results by testing
    it with unseen data. This will give us the confidence that our model is well trained
    and will probably produce similar results where new data is handed to use.
  prefs: []
  type: TYPE_NORMAL
- en: A careful reader should have noticed that we used the TF-IDF data frame when
    creating our sentiment analysis data, and not any of the ones we create later
    with combinations of bigrams, SVDs, and cosine similarities, which operate in
    a different semantic space due to the fact they are transformations of the original
    DFM. Therefore, before we can actually use our trained model to make predictions
    on the test data, we need to transform it into an equivalent space as our training
    data. Otherwise, we would be comparing apples and oranges, which would give us
    nonsense results.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make sure that we''re working in the same semantic space, we will apply
    the TF-IDF weights to our test data. However, if you think about it, there probably
    are a lot of terms in our test data that were not present in our training data.
    Therefore, our DFMs will have different dimensions for our training and testing
    sets. This is a problem, and we need to make sure that they are the same. To accomplish
    this, we build the DFM for the testing data and apply a filter to it which only
    keeps the terms that are present in our training DFM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Furthermore, if you think about it, the corpus-weighing part of the TF-IDF,
    that is the IDF, will also be different than these two datasets due to the change
    in the term space for the corpus. Therefore, we need to make sure that we ignore
    those terms that are new (that is, were not seen in our training data) and use
    the IDF from our training procedure to make sure that our tests are valid. To
    accomplish this, we will first compute only the IDF part of the TF-IDF for our
    training data and use that when computing the TF-IDF for our testing data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the testing data projected into the vector space of the training
    data, we can compute the sentiment analysis for the new testing DFM and compute
    our predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we are not training a new model in this case, we are just using the
    last model we created and use that to provide predictions for the testing DFM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: To know how well we predicted, we can just print a model as we did before because
    we would just be looking at the results from the training process that does not
    include predictions for the testing data. What we need to do is create a confusion
    matrix and compute the predictive accuracy metrics as we did before with the `confusionMatrix()` function.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, our results seem to be valid since we got a predictive accuracy
    of 71.91% with previously unseen data, which is very close to the predictive accuracy
    of the training data, and is 12% more than just guessing actual multiple purchases
    proportion. For text data and the problem we're dealing with, these results are
    pretty good.
  prefs: []
  type: TYPE_NORMAL
- en: If you know how to interpret the other metrics, make sure that you compare them
    to ones we had for our first model to realize how our results evolved during the
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you''re not familiar with them, we suggest you take a look at James, Witten,
    Hastie, and Tibshirani''s, *Statistical Learning, 2013*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'If you''re going to test with a DFM that was created using an SVD, you need
    to make the corresponding transformation to make sure you''re working in the correct
    semantic space before producing any predictions. If you used a procedure like
    the one shown in this chapter, you need to left-multiply the testing DFM (with
    similar transformations as your training DFM) with the vector of singular values
    adjusted by sigma, while transposing the structures accordingly. The exact transformation
    will depend on the data structures you''re using and processes you applied to
    them, but always remember to make sure that both your training and testing data
    operate in the same semantic space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Retrieving text data from Twitter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we finish this chapter, we will very briefly touch on a completely different,
    yet very sought-after topic, that is, getting data from Twitter. In case you want
    to apply predictive models, you will need to link the Twitter data to a variable
    you want to predict, which normally comes from other data. However, something
    you can easily do is measure the sentiment around a topic using the techniques
    we showed in a previous section.
  prefs: []
  type: TYPE_NORMAL
- en: The `twitteR` package actually makes it very easy for us to retrieve Twitter
    data. To do so, we will create a **Twitter App** within Twitter, which will give
    us access to the data feed. To accomplish this, we need to generate four strings
    within your Twitter account that will be the keys to using the API. These keys
    are used to validate your permissions and monitor your usage in general. Specifically,
    you need four strings, the `consumer_key` value, the `consumer_secret`, the `access_token`,
    and the `access_secret`. To retrieve them, go to the Twitter Apps website ([https://apps.twitter.com/](https://apps.twitter.com/)),
    click on Create New App, and input the information required. The name for your
    Twitter App must be unique across all of the Twitter Apps. Don't worry about picking
    a complex name, you'll never use that string again. Also make sure that you read
    the Twitter Developer Agreement and that you agree with it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once inside the dashboard for your app, go to the Keys and Access Tokens tab,
    and generate a key and access token with their corresponding secret keys. Make
    sure that you copy those strings exactly as they will grant you access to the
    data feed. Substitute them instead of the ones shown here (which no longer work
    since they were deleted after writing this book), and execute the `setup_twitter_oauth()` function.
    If everything went as expected, you should now have connected your R session to
    the data feed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'To retrieve data, we will create a wrapper function that will save us writing
    boilerplate again and again every time we want new data. The `get_twitter_data()` function
    takes a keyword we''re searching for within Twitter and the number of messages
    we want to retrieve. It then goes on to get the data from Twitter using the `searchTwitter()` function
    (in English), transform the results into a data frame with the `twListToDF()` function,
    and send that back to the user:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can easily search for messages that contain the word "*cake*" inside
    by executing the following code. As you can see, we don''t only get the messages,
    but we also get a lot of metadata, like whether or not the tweet has been favorite;
    if so, how many times, whether it was a reply, when was it created, and the coordinates
    of where the tweet was sent if they are available, among other things:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: As an exercise, get data from Twitter with the mechanism shown precedingly and
    use it to create a world map that shows where the tweets are coming from and colors
    the pin locations using the sentiment inferred from the tweet. Having a piece
    of code like that can be fun to play with and joke around with your friends as
    well as for making real business decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we showed how to perform predictive analysis using text data.
    To do so, we showed how to tokenize text to extract relevant words, how to build
    and work with **document-feature matrices** (**DFMs**), how to apply transformations
    to DFMs to explore different predictive models using term frequency-inverse document
    frequency weights, n-grams, partial singular value decompositions, and cosine
    similarities, and how to use these data structures within random forests to produce
    predictions. You learned why these techniques may be important for some problems
    and how to combine them. We also showed how to include sentiment analysis inferred
    from text to increase the predictive power of our models. Finally, we showed how
    to retrieve live data from Twitter that can be used to analyze what people are
    saying in the social network shortly after they have said it.
  prefs: []
  type: TYPE_NORMAL
- en: We encourage you to combine the knowledge from this chapter with that from previous
    chapters to try to gain deeper insights. For example, what happens if we use trigrams
    of quadgrams? What happens if we include other features in the data (for example,
    coordinates or client IDs)? And, what happens if we use other predictive models
    instead of random forests, such as support vector machines?
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at how to use what we have done during the
    last three chapters to produce reports that can be automatically generated when
    we receive new data from The Cake Factory. We will cover how to produce PDFs automatically,
    how to create presentations that can be automatically updated, as well as other
    interesting reporting techniques.
  prefs: []
  type: TYPE_NORMAL
