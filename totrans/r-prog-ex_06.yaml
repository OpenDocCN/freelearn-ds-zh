- en: Understanding Reviews with Text Analysis
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过文本分析理解评论
- en: It is well known that a very large percentage of relevant information originates
    in an unstructured form, an important player being text data. Text analysis, **Natural
    Language Processing** (**NLP**), **Information Retrieval** (**IR**), and **Statistical
    Learning** (**SL**) are some areas focused on developing techniques and processes
    to deal with this data. These techniques and processes discover and present knowledge,
    facts, business rules, relationships, among others, that is otherwise locked in
    textual form, impenetrable to automated processing.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 众所周知，很大一部分相关信息以非结构化的形式存在，其中重要的参与者是文本数据。文本分析、**自然语言处理**（**NLP**）、**信息检索**（**IR**）和**统计学习**（**SL**）是一些专注于开发处理这些数据的技术和流程的领域。这些技术和流程发现并呈现知识、事实、业务规则、关系等，否则这些内容以文本形式锁定，对自动化处理来说是不可渗透的。
- en: Given the explosion of textual data we see nowadays, an important skill for
    analysts such as statisticians and data scientists is to be able to work efficiently
    with this data and find the insights they are looking for. In this chapter, we
    will try to predict whether a customer is going to make repeated purchases given
    the reviews being sent to The Cake Factory.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于如今文本数据的爆炸式增长，对于统计学家和数据科学家等分析师来说，一项重要的技能是能够高效地处理这些数据并找到他们所寻找的见解。在本章中，我们将尝试根据发送给蛋糕工厂的评论来预测客户是否会进行重复购买。
- en: 'Since text analysis is a very broad research area, we need to narrow the techniques
    we will look at in this chapter to the most important ones. We will take a Pareto
    approach by focusing on 20% of techniques that will be used 80% of the time when
    doing text analysis. Some of the important topics covered in this chapter are
    as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 由于文本分析是一个非常广泛的研究领域，我们需要将本章中将要探讨的技术缩小到最重要的几个。我们将采用帕累托方法，专注于在文本分析中使用80%的时间的20%的技术。本章涵盖的一些重要主题如下：
- en: Document feature matrices as a basic data structure
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档特征矩阵作为基本的数据结构
- en: Random forests for predictive modeling with text data
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用文本数据预测建模的随机森林
- en: Term frequency-inverse document frequencies for measuring importance
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词语频率-逆文档频率用于衡量重要性
- en: N-gram modeling to bring back order into the analysis
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: N-gram建模将顺序带回分析
- en: Singular vector decomposition for dimensionality reduction
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于降维的奇异向量分解
- en: Cosine similarity to find similar feature vectors
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用余弦相似度查找相似的特征向量
- en: Sentiment analysis as an added vector feature
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将情感分析作为附加的向量特征
- en: This chapter's required packages
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本章所需的包
- en: Setting up the packages for this chapter may be a bit cumbersome because some
    of the packages depend on operating system libraries which can vary from computer
    to computer. Please check  [Appendix](part0296.html#8Q96G0-f494c932c729429fb734ce52cafce730)*,
    Required Packages* for specific instructions on how to install them for your operating
    system.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 由于一些包依赖于操作系统库，这些库可能因计算机而异，因此设置本章所需的包可能有些繁琐。请参阅[附录](part0296.html#8Q96G0-f494c932c729429fb734ce52cafce730)*，所需包*，获取有关如何在您的操作系统上安装它们的特定说明。
- en: '| **Package** | **Reason** |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| **包名** | **原因** |'
- en: '| `lsa` | Cosine similarity computation |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| `lsa` | 余弦相似度计算 |'
- en: '| `rilba` | Efficient SVD decomposition |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| `rilba` | 高效的SVD分解 |'
- en: '| `caret` | Machine learning framework |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| `caret` | 机器学习框架 |'
- en: '| `twitteR` | Interface to Twitter''s API |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| `twitteR` | Twitter API接口 |'
- en: '| `quanteda` | Text data processing |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| `quanteda` | 文本数据处理 |'
- en: '| `sentimentr` | Text data sentiment analysis |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| `sentimentr` | 文本数据情感分析 |'
- en: '| `randomForest` | Random forest models |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| `randomForest` | 随机森林模型 |'
- en: We will use the `rilba` package (which depends on C code) to compute a part
    of the **Singular Value Decomposition** (**SVD**) efficiently using the *Augmented
    Implicitly Restarted Lanczos Bidiagonalization Methods, by Baglama and Reichel,
    2005*, [http://www.math.uri.edu/~jbaglama/papers/paper14.pdf](http://www.math.uri.edu/~jbaglama/papers/paper14.pdf)).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`rilba`包（它依赖于C代码）通过Baglama和Reichel在2005年提出的*增强隐式重启兰索斯双带对角化方法*，有效地计算**奇异值分解**（**SVD**）的一部分。[http://www.math.uri.edu/~jbaglama/papers/paper14.pdf](http://www.math.uri.edu/~jbaglama/papers/paper14.pdf))。
- en: We will use the `parallel` package to perform parallel processing since some
    text analysis can potentially require a lot of computations. The `parallel` package
    is the most general parallelization package in R for now, but it has been reported
    to not work correctly in some systems. Other options are `doParallel`, `doMC` and
    `doSNOW`. If you run into trouble when using one `parallel`, try switching to
    one of the other packages. The code to make them work is very similar.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`parallel`包来进行并行处理，因为一些文本分析可能需要大量的计算。目前`parallel`包是R中最通用的并行化包，但据报道在某些系统中它可能无法正确工作。其他选项包括`doParallel`、`doMC`和`doSNOW`。如果你在使用其中一个`parallel`时遇到问题，尝试切换到其他包之一。使它们工作的代码非常相似。
- en: Regarding text data, there are a few packages you can use in R. The most common
    ones are the `tm` package and the `quanteda` package. Both are excellent, and
    differ mostly in style. All the functionality we will see in this chapter can
    be used with either one of them, but we chose to work with the `quanteda` package.
    It is built with the `stringi` package for processing text, the `data.table` package
    for large documents, and the `Matrix` package to handle sparse objects. Therefore
    you can expect it to be very fast and handle Unicode and UTF-8 very well.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '关于文本数据，在R中你可以使用一些包。最常见的是`tm`包和`quanteda`包。两者都非常优秀，主要区别在于风格。本章中我们将看到的所有功能都可以使用其中任何一个包来实现，但我们选择使用`quanteda`包。它是用`stringi`包来处理文本，`data.table`包来处理大量文档，以及`Matrix`包来处理稀疏对象构建的。因此，你可以期待它非常快速，并且很好地处理Unicode和UTF-8。 '
- en: If you don't know what Unicode and UTF-8 are, I suggest you read up on them.
    Very roughly you can think of Unicode as being a standard of IDs for characters,
    while UTF-8 being a translation of this IDs into bytes computers can understand.
    During this chapter we won't worry about encodings (all the data is in UTF-8),
    but it's something that often comes up when working with text data, and is important
    to handle correctly.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不知道Unicode和UTF-8是什么，我建议你了解一下。非常粗略地，你可以将Unicode视为字符的ID标准，而UTF-8则是将这些ID转换为计算机可以理解的字节。在本章中，我们不会担心编码（所有数据都在UTF-8中），但这是在处理文本数据时经常出现的问题，并且正确处理它非常重要。
- en: What is text analysis and how does it work?
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是文本分析以及它是如何工作的？
- en: Text analysis is the process of deriving information from text. Information
    is typically derived through techniques such as IR, NLP, and SL, and it involves
    structuring text, deriving patterns with the structured data, and finally evaluating
    and interpreting the output. The basic models used for text analysis are the bag-of-words
    models, the vector space model, and the semantic parsing model.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分析是从文本中提取信息的过程。信息通常通过诸如信息检索（IR）、自然语言处理（NLP）和句法分析（SL）等技术来提取，它包括对文本进行结构化、从结构化数据中推导模式，以及最终评估和解释输出。用于文本分析的基本模型包括词袋模型、向量空间模型和语义解析模型。
- en: 'The bag-of-words model is a simplified text representation in which a text
    (a review in our case) is represented as the set of its terms (words), disregarding
    grammar and word order but keeping multiplicity (hence the term bag). After transforming
    the text into a bag-of-words and structuring into a corpus (a structured collection
    of the text data), we can calculate various measures to characterize the text
    into a vector space. The bag-of-words model is commonly used in SL methods, and
    we will use it with random forests in this chapter. In practice, it is used as
    a tool of feature generation. The following image explains the bag-of-words model:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 词袋模型是一种简化的文本表示，其中文本（在我们的例子中是评论）被表示为其术语（单词）的集合，不考虑语法和单词顺序，但保持多重性（因此得名“词袋”）。在将文本转换为词袋并将其结构化为语料库（结构化文本数据集合）之后，我们可以计算各种度量来将文本特征化为向量空间。词袋模型在句法分析方法中常用，我们将在本章中使用随机森林来使用它。在实践中，它被用作特征生成工具。以下图像解释了词袋模型：
- en: '![](img/00047.jpeg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00047.jpeg)'
- en: Bag-of-words vs semantic parsing
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 词袋模型与语义解析的比较
- en: The vector space model uses the bag-of-words that you extracted from the documents
    to create a feature vector for each text, where each feature is a term and the
    feature's value is a term weight. The term weight may be a binary value (1 indicating
    that the term occurred in the document and 0 indicating that it did not), a **term
    frequency** (**TF**) value (indicating how many times the term occurred in the
    document), or a **term frequency-inverse document frequency** (**TF-IDF**) value
    (indicating how important a term is to a text given its corpus). More complex
    weighting mechanisms exist which are focused on specific problems, but these are
    the most common ones, and are the ones we will focus on.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 向量空间模型使用从文档中提取的词袋来为每个文本创建一个特征向量，其中每个特征是一个术语，特征值是术语权重。术语权重可能是一个二进制值（1表示术语在文档中出现过，0表示没有），一个**术语频率**（**TF**）值（表示术语在文档中出现的次数），或者一个**术语频率-逆文档频率**（**TF-IDF**）值（表示术语对于一个文本给定其语料库的重要性）。存在更多复杂的加权机制，它们专注于特定问题，但这些都是最常见的，也是我们将要关注的。
- en: 'Given what we mentioned earlier, a text turns out to be a feature vector, and
    each feature vector corresponds to a point in a vector space. The model for this
    vector space is such that there is an axis for every term in the vocabulary, and
    so the vector space is *n*-dimensional, where *n* is the size of the vocabulary
    in all the data being analyzed (this can be huge). Sometimes, it helps to think
    about these concepts geometrically. The bag-of-words model and vector space model
    refer to different aspects of characterizing a body of text, and they complement
    each other. The following image explains the vector space model:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们之前提到的，一个文本最终会变成一个特征向量，每个特征向量对应于向量空间中的一个点。这个向量空间模型的构建方式是，对于词汇表中的每个术语都有一个轴，因此这个向量空间是*n*-维的，其中*n*是所有被分析数据中词汇表的大小（这可以非常大）。有时，从几何角度思考这些概念会有所帮助。词袋模型和向量空间模型分别指代了表征文本的不同方面，并且它们相互补充。以下图像解释了向量空间模型：
- en: '![](img/00048.jpeg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00048.jpeg)'
- en: Bag-of-words to vector space
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 词袋到向量空间
- en: An important weakness of the bag-of-words model is the fact that it ignores
    the terms' semantic context. More complex models exist that attempt to correct
    these deficiencies. Semantic parsing is one of them, and it's the process of mapping
    a natural-language sentence into a formal representation of its meaning. It mainly
    uses combinations of inductive logic programming and statistical learning. These
    types of techniques become more useful when dealing with complex texts. Even though
    we won't touch further on them on this book, they are a powerful tool and are
    a very interesting area of research.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 词袋模型的一个重要弱点是它忽略了术语的语义上下文。存在更复杂的模型试图纠正这些不足。语义解析就是其中之一，它是将自然语言句子映射到其意义的形式表示的过程。它主要使用归纳逻辑编程和统计学习的组合。这些类型的技术在处理复杂文本时更有用。尽管我们在这本书中不会进一步探讨它们，但它们是强大的工具，并且是一个非常有意思的研究领域。
- en: As an example, if you try to think of the representation of the following quote
    using the bag-of-words model and the semantic parsing model, you may intuitively
    think that the first one can give nonsense results while the second one can provide
    at least some understanding, and you would be correct.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你尝试用词袋模型和语义解析模型来思考以下引语的表示，你可能直觉上会认为第一个可能会给出无意义的结果，而第二个至少可以提供一些理解，你的判断是正确的。
- en: '"The fish trap exists because of the fish. Once you''ve gotten the fish you
    can forget the trap. The rabbit snare exists because of the rabbit. Once you''ve
    gotten the rabbit, you can forget the snare. Words exist because of meaning. Once
    you''ve gotten the meaning, you can forget the words. Where can I find a man who
    has forgotten words so I can talk with him?"'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: “鱼网的存在是因为鱼。一旦你得到了鱼，你就可以忘记网。兔子的陷阱存在是因为兔子。一旦你得到了兔子，你就可以忘记陷阱。文字的存在是因为意义。一旦你得到了意义，你就可以忘记文字。我在哪里能找到一个已经忘记了文字的人，我可以和他交谈？”
- en: – The Writings of Chuang Tzu, 4th century B.C. (Original text in Chinese)
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: – 《庄子》著作，公元前4世纪（原文为中文）
- en: Preparing, training, and testing data
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备、训练和测试数据
- en: 'As always, we will start by setting up our data. In this case, the data is
    the messages received by our fantasy company, The Cake Factory. These are in the
    `client_messages.RDS` file that we created in [Chapter 4](part0091.html#2MP360-f494c932c729429fb734ce52cafce730),
    *Simulating Sales Data and Working with Databases*. The data contains 300 observations
    for 8 variables: `SALE_ID`, `DATE`, `STARS`, `SUMMARY`, `MESSAGE`, `LAT`, `LNG`,
    and `MULT_PURCHASES`. During this chapter, we will work with the `MESSAGE` and
    `MULT_PURCHASES` variables.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，我们将从设置数据开始。在这种情况下，数据是我们幻想公司蛋糕工厂收到的消息。这些消息在[第4章](part0091.html#2MP360-f494c932c729429fb734ce52cafce730)中创建的`client_messages.RDS`文件中，*模拟销售数据和数据库操作*。数据包含300个观测值和8个变量：`SALE_ID`、`DATE`、`STARS`、`SUMMARY`、`MESSAGE`、`LAT`、`LNG`和`MULT_PURCHASES`。在本章中，我们将处理`MESSAGE`和`MULT_PURCHASES`变量。
- en: 'We will set up our seed to have reproducible results. Keep in mind that this
    should be before every function call that involves some randomization. We will
    show it just once here to save space and avoid repeating ourselves, but keep that
    in mind when you are trying to generate reproducible results:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将设置种子以获得可重复的结果。请注意，这应该在涉及随机化的每个函数调用之前完成。我们在这里只展示一次以节省空间并避免重复，但当你尝试生成可重复结果时要记住这一点：
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we need to make sure that we don''t have any missing data in the relevant
    variables. To do so, we use the `complete.cases()` function together with the
    negation (`!`) and the `sum()` function to get the total number of `NA` values''
    in each variable. As you can see, we don''t have any missing data:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要确保相关变量中没有缺失数据。为此，我们使用`complete.cases()`函数以及否定(`!`)和`sum()`函数来获取每个变量中`NA`值的总数。正如你所看到的，我们没有缺失数据：
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If you have missing data, instead of using some imputation mechanism which is
    normally done in some data analysis scenarios, you want to remove those observations
    from this data, since it's easier to get this wrong due to the non-continuous
    characteristics of textual data.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你存在缺失数据，而不是使用通常在数据分析场景中进行的某些插补机制，你希望从这些数据中删除这些观测值，因为由于文本数据的非连续特性，这更容易出错：
- en: 'As you will often find when working on interesting real-world problems in predictive
    analysis, it''s not uncommon to work with disproportionate data. In this case,
    as can be seen with the shown code, we have around 63% of multiple purchases.
    This is not very disproportionate, but we still have to play on the safe side
    by keeping the training and testing data with similar proportions:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在预测分析中处理有趣的现实世界问题时，你经常会发现需要处理不均衡的数据。在这种情况下，正如所示代码所示，我们有大约63%的多次购买。这并不非常不均衡，但我们仍然必须保持训练和测试数据具有相似的比例，以确保安全：
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'For data with the disproportionality problem, maintaining the same proportions
    in the testing and training sets is important to get accurate results. Therefore,
    we need to make sure our sampling method maintains these proportions. To do so,
    we will use the `createDataPartition()` function from the `caret` package to extract
    the indexes for each of the training and testing sets. It will create balanced
    data splits, and, in this case, it will use 70% of the data for training with
    a single partition:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于存在不均衡问题的数据，保持测试和训练集中相同的比例对于获得准确的结果非常重要。因此，我们需要确保我们的采样方法保持这些比例。为此，我们将使用`caret`包中的`createDataPartition()`函数来提取每个训练和测试集的索引。它将创建平衡的数据分割，在这种情况下，它将使用70%的数据进行训练，并使用单个分区：
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To make sure that our proportions are being maintained, we can check each of
    them individually just as we did before with the full data:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保我们的比例保持不变，我们可以像之前对完整数据那样逐个检查它们：
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now that we have our training and testing sets ready, we can start cleaning
    and setting up our text data as we will do in the next section.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了训练和测试集，我们可以开始清理和设置我们的文本数据，就像我们在下一节将要做的那样。
- en: Building the corpus with tokenization and data cleaning
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用分词和数据清洗构建语料库
- en: 'The first thing we need to create when working with text data is to extract
    the tokens that will be used to create our corpus. Simply, these tokens are all
    the terms found in every text in our data, put together, and removed the ordering
    or grammatical context. To create them, we use the `tokens()` function and the
    related functions from the `quanteda` package. As you can imagine, our data will
    not only contain words, but also punctuation marks, numbers, symbols, and other
    characters like hyphens. Depending on the context of the problem you''re working
    with, you may find it quite useful to remove all of them as we do here. However,
    keep in mind that in some contexts some of these special characters can be meaningful
    (for example, the hashtag symbol (#) can be relevant when analyzing Twitter data):'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们处理文本数据时，需要创建的第一件事是从中提取将要用于创建语料库的标记。简单来说，这些标记是我们数据中每个文本中找到的所有术语的总和，去除了顺序或语法上下文。为了创建它们，我们使用
    `tokens()` 函数和来自 `quanteda` 包的相关函数。如您所想象，我们的数据不仅包含单词，还包括标点符号、数字、符号和其他字符，如连字符。根据您处理的问题的上下文，您可能会发现像我们这里这样做，移除所有这些特殊字符非常有用。然而，请记住，在某些上下文中，这些特殊字符可能是有意义的（例如，分析推特数据时，井号符号（#）可能是相关的）：
- en: '[PRE5]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'As you can imagine, there will be a huge number of tokens in the data because
    there must be one for each unique word in our data. These raw tokens will probably
    be useless (contain a low signal/noise ratio) if we don''t apply some filtering.
    We''ll start by ignoring capitalization. In our context, *something* and *something* should
    be equivalent. Therefore, we use the `tokens_tolower()` function to make all tokens
    lowercase:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所想象，数据中将有大量的标记，因为我们必须为数据中的每个唯一单词都有一个标记。如果我们不应用一些过滤，这些原始标记可能毫无用处（信号/噪声比低），因此，我们将首先忽略大小写。在我们的语境中，*something*
    和 *something* 应该是等效的。因此，我们使用 `tokens_tolower()` 函数将所有标记转换为小写：
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Also, keep in mind that common words like *the*, *a*, and *to* are almost always
    the terms with highest frequency in the text and are not particularly important
    to derive insights. We should thus remove them, as we do with the `tokens_select()` function:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请记住，像 *the*、*a* 和 *to* 这样的常用词几乎总是文本中频率最高的术语，并且对于得出见解并不特别重要。因此，我们应该使用 `tokens_select()`
    函数将它们移除，就像我们做的那样：
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Word stems allow us to reduce the number of tokens that overall share the same
    meaning. For example, the words *probability*, *probably*, and *probable* probably
    have the same meaning and their differences are mostly syntactic. Therefore, we
    could represent all of them with a single token like *probab* and reduce our feature
    space considerably. Note that all of these filters have assumptions behind them
    about the problem we''re dealing with, and you should make sure those assumptions
    are valid. In our case, they are as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '词根允许我们减少具有相同意义的标记数量。例如，单词 *probability*、*probably* 和 *probable* 很可能具有相同的意义，它们之间的差异主要是句法的。因此，我们可以用单个标记
    *probab* 来表示它们，从而大大减少我们的特征空间。请注意，所有这些过滤器都基于我们处理的问题的假设，您应该确保这些假设是有效的。在我们的情况下，它们如下： '
- en: '[PRE8]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Instead of having to repeat this by hand every time, we want to create tokens;
    we can wrap all of these filters in a single function and make our lives a little
    bit easier down the road. The careful reader will note the `token_ngrams()` function
    and the corresponding `n_grams = 1` default parameter. We dedicate a section to
    this later, but for now, just know that `n_grams = 1` means that we want single
    words in our tokens:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是每次都要手动重复这个过程来创建标记，我们希望创建标记；我们可以将这些过滤器包装在一个单独的函数中，使我们的工作稍微轻松一些。仔细的读者会注意到 `token_ngrams()`
    函数和相应的 `n_grams = 1` 默认参数。我们将在稍后的部分中对此进行专门介绍，但就目前而言，只需知道 `n_grams = 1` 意味着我们希望在标记中包含单个单词：
- en: '[PRE9]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now, even though we have shown the code that is used for this chapter''s example,
    we will use a smaller example (one sentence) so that you can get a visual idea
    of what''s going on in each step. You should definitely get in the habit of doing
    this yourself when exploring a problem to make sure everything is working as you
    expect it. We put the code for all the steps here and, after reading the preceding
    paragraphs, you should be able to identify the differences in each step:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，尽管我们已经展示了用于本章示例的代码，但我们将使用一个更小的示例（一句话），以便您可以直观地了解每个步骤中发生的情况。您绝对应该养成自己这样做以探索问题的习惯，以确保一切按预期工作。我们将所有步骤的代码都放在这里，在阅读前面的段落之后，您应该能够识别出每个步骤中的差异：
- en: '[PRE10]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Document feature matrices
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文档特征矩阵
- en: 'Once we have our tokens ready, we need to create our corpus. At the most basic
    level, a **corpus** is a collection of texts that includes document-level variables
    specific to each text. The most basic *corpus* uses the bag-of-words and vector
    space models to create a matrix in which each row represents a text in our collection
    (a client message in our case), and each column represents a term. Each of the
    values in the matrix would be a 1 or a 0, indicating whether or not a specific
    term is included in a specific text. This is a very basic representation that
    we will not use. We will use a **document-feature matrix** (**DFM**), which has
    the same structure but, instead of using an indicator variable (1s and 0s), it
    will contain the number of times a term occurred within a text, using the multiplicity
    characteristic from the bag-of-words model. To create it, we use the `dfm()` function
    from the `quanteda` package:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们准备好了标记，我们需要创建我们的语料库。在最基本层面上，一个 **语料库** 是一个包含每个文本特定文档级变量的文本集合。最基本的 *语料库*
    使用词袋和向量空间模型创建一个矩阵，其中每一行代表我们集合中的一个文本（在我们的例子中是一个客户消息），每一列代表一个术语。矩阵中的每个值将是一个1或0，表示一个特定的术语是否包含在特定的文本中。这是一个非常基本的表示，我们不会使用。我们将使用
    **文档-特征矩阵** （**DFM**），它具有相同的结构，但不是使用指示变量（1s和0s），而是包含一个术语在文本中出现的次数，使用词袋模型的多重特性。为了创建它，我们使用
    `quanteda` 包中的 `dfm()` 函数：
- en: '[PRE11]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'To get a visual example, here''s the DFM for the example shown in the previous
    section. We can see a couple of things here. First, it''s a special object with
    some metadata, number of documents (one sentence in our case), number of features
    (which is the number of tokens), dimensions, and the actual values for each text
    in our data. This is our corpus. If we had more than one sentence, we would see
    more than one row in it:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得一个直观的例子，这里展示了上一节中所示示例的DFM。我们可以看到一些东西。首先，这是一个具有一些元数据的特殊对象，包括文档数量（在我们的例子中是一个句子），特征数量（即标记的数量），维度以及我们数据中每个文本的实际值。这是我们语料库。如果我们有多个句子，我们会在其中看到多行：
- en: '[PRE12]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, forget about the one-sentence example, and let''s go back to our client
    messages example. In that case the `tokens` object will be much larger. Normally,
    we tokenize and create our DFM in the same way. Therefore, we create the function
    that makes it a little easier for us:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，忘记那个单句示例，让我们回到我们的客户消息示例。在这种情况下，`tokens` 对象将会更大。通常，我们以相同的方式分词并创建我们的DFM。因此，我们创建了一个使这个过程对我们来说更容易一些的函数：
- en: '[PRE13]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now we can create our DFM easily with the following:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用以下方法轻松创建我们的DFM：
- en: '[PRE14]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'To get an idea of our DFM''s characteristics, you can simply print it. As you
    can see our training DFM has 212 documents (client messages) and 2,007 features
    (tokens). Clearly, most documents will not contain most of the features. Therefore
    we have a sparse structure, meaning that 98.4% of the entries in the DFM are actually
    zero. The educated reader will identify this as being the curse of dimensionality
    problem common to machine learning, and specially damaging in text analysis. As
    we will see later, this can be a computational bottleneck that we need to deal
    with:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解我们DFM的特征，你可以简单地打印它。正如你所见，我们的训练DFM有212个文档（客户消息）和2,007个特征（标记）。显然，大多数文档不会包含大多数特征。因此，我们有一个稀疏结构，这意味着DFM中的98.4%的条目实际上是零。有经验的读者会将其识别为机器学习中常见的维度诅咒问题，在文本分析中尤其有害。正如我们稍后将会看到的，这可能会成为一个计算瓶颈，我们需要处理：
- en: '[PRE15]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Often, tokenization requires some additional pre-processing. As you know by
    now, the tokens we find in our tokenization process end up being column (feature)
    names in our DFM. If these names contain symbols inside or start with numbers
    (for example, *something&odd* or *45pieces*), then some of our analysis algorithms
    will complain by throwing errors. We want to prevent that when we transform our
    DFM into a data frame. We can do so with the convenient `make.names()` function.
    We will also add the `MULT_PURCHASES` value (our dependent variable) to our newly
    created data frame at this point:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，分词需要一些额外的预处理。正如你所知，我们在分词过程中找到的标记最终会成为我们的DFM中的列（特征）名称。如果这些名称包含符号或以数字开头（例如，*something&odd*
    或 *45pieces*），那么我们的某些分析算法会通过抛出错误来抱怨。我们希望在将我们的DFM转换为数据框时防止这种情况发生。我们可以使用方便的 `make.names()`
    函数做到这一点。在此处，我们还将添加 `MULT_PURCHASES` 值（我们的因变量）到我们新创建的数据框中：
- en: '[PRE16]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Again, to avoid having to repeat this boilerplate code, we can create our own
    function that packs this functionality, and easily create our data frame for analysis:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，为了避免重复这段样板代码，我们可以创建自己的函数来封装这个功能，并轻松地创建用于分析的数据框：
- en: '[PRE17]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'At this point, our training data is ready for analysis in the form of a data
    frame that can be used by our predictive models. To finish the section, if you
    want to know which are the most frequent terms in the data, you can use the `topfeatures()` function.
    In this case, most of the features can be intuitively guessed in their context.
    The only one that may require some explanation is the `br` feature. It comes from
    the fact that our data is coming from HTML pages that contain `<br>` strings that
    signal for a new line in the text (a break, hence the `br`). We could remove this
    feature if we wanted to, but we will leave for now:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们的训练数据已经准备好以数据框的形式进行分析，这些数据框可以被我们的预测模型使用。为了完成本节，如果你想了解数据中最频繁出现的术语，可以使用`topfeatures()`函数。在这种情况下，大多数特征可以根据其上下文直观地猜测。唯一可能需要一些解释的是`br`特征。它来自我们的数据来自包含`<br>`字符串的HTML页面，这些字符串表示文本中的新行（一个中断，因此称为`br`）。如果我们想的话，我们可以移除这个特征，但现在我们将保留它：
- en: '[PRE18]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Training models with cross validation
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用交叉验证训练模型
- en: In this section, we will efficiently train our first predictive model for this
    example and build the corresponding confusion matrix. Most of the functionality
    comes from the excellent `caret` package. You can find more information on the
    vast features within this package that we will not explore in this book in its
    documentation ([http://topepo.github.io/caret/index.html](http://topepo.github.io/caret/index.html)).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将高效地训练本例的第一个预测模型，并构建相应的混淆矩阵。大部分功能都来自优秀的`caret`包。你可以在其文档中找到更多关于这个包内丰富功能的信息，我们将在本书中不进行探索（[http://topepo.github.io/caret/index.html](http://topepo.github.io/caret/index.html)）。
- en: Training our first predictive model
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练我们的第一个预测模型
- en: Following best practices, we will use **Cross Validation** (**CV**) as the basis
    of our modeling process. Using CV we can create estimates of how well our model
    will do with unseen data. CV is powerful, but the downside is that it requires
    more processing and therefore more time. If you can take the computational complexity,
    you should definitely take advantage of it in your projects.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 按照最佳实践，我们将使用**交叉验证**（**CV**）作为建模过程的基础。使用CV，我们可以创建模型在未见数据上的表现估计。CV功能强大，但缺点是它需要更多的处理，因此需要更多的时间。如果你能够承受计算复杂性，你绝对应该在项目中利用它。
- en: Going into the mathematics behind CV is outside of the scope of this book. If
    interested, you can find out more information on Wikipedia ([https://en.wikipedia.org/wiki/Cross-validation_(statistics)](https://en.wikipedia.org/wiki/Cross-validation_(statistics))). The
    basic idea is that the training data will be split into various parts, and each
    of these parts will be taken out of the rest of the training data one at a time,
    keeping all remaining parts together. The parts that are kept together will be
    used to train the model, while the part that was taken out will be used for testing,
    and this will be repeated by rotating the parts such that every part is taken
    out once. This allows you to test the training procedure more thoroughly, before
    doing the final testing with the testing data.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 探讨CV背后的数学原理超出了本书的范围。如果你感兴趣，你可以在维基百科上找到更多信息（[https://en.wikipedia.org/wiki/Cross-validation_(statistics)](https://en.wikipedia.org/wiki/Cross-validation_(statistics))).
    基本思想是将训练数据分成各个部分，然后逐个将这些部分从其余训练数据中取出，同时保留所有剩余的部分。保留在一起的部分将用于训练模型，而被取出的部分将用于测试，并且通过旋转部分来重复这个过程，使得每个部分都被取出一次。这允许你在使用测试数据进行最终测试之前，更彻底地测试训练过程。
- en: 'We use the `trainControl()` function to set our repeated CV mechanism with
    five splits and two repeats. This object will be passed to our predictive models,
    created with the `caret` package, to automatically apply this control mechanism
    within them:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`trainControl()`函数来设置我们的重复交叉验证机制，包含五个分割和两次重复。这个对象将被传递给我们的预测模型，这些模型是用`caret`包创建的，以便在它们内部自动应用这个控制机制：
- en: '[PRE19]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Our predictive models pick for this example are R*andom Forests* (RF). We will
    very briefly explain what RF are, but the interested reader is encouraged to look
    into James, Witten, Hastie, and Tibshirani's excellent "*Statistical Learning*"
    (Springer, 2013). RF are a non-linear model used to generate predictions. A *tree* is
    a structure that provides a clear path from inputs to specific outputs through
    a branching model. In predictive modeling they are used to find limited input-space
    areas that perform well when providing predictions. RF create many such trees
    and use a mechanism to aggregate the predictions provided by this trees into a
    single prediction. They are a very powerful and popular Machine Learning model.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为这个例子选择的预测模型是R*andom Forests*（RF）。我们将非常简要地解释RF是什么，但感兴趣的读者被鼓励去查阅James, Witten,
    Hastie和Tibshirani的优秀的"*统计学习*"（Springer，2013）。RF是一种非线性模型，用于生成预测。*树*是一种结构，通过分支模型提供从输入到特定输出的清晰路径。在预测建模中，它们用于找到在提供预测时表现良好的有限输入空间区域。RF创建了许多这样的树，并使用一种机制将此树提供的预测聚合为单个预测。它们是一个非常强大且流行的机器学习模型。
- en: 'Let''s have a look at the random forests example:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看随机森林的例子：
- en: '![](img/00049.jpeg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00049.jpeg)'
- en: Random forests aggregate trees
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林聚合树
- en: 'To train our model, we use the `train()` function passing a formula that signals
    R to use `MULT_PURCHASES` as the dependent variable and everything else (`~ .`)
    as the independent variables, which are the token frequencies. It also specifies
    the data, the method (`"rf"` stands for random forests), the control mechanism
    we just created, and the number of tuning scenarios to use:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练我们的模型，我们使用`train()`函数，传递一个公式，指示R使用`MULT_PURCHASES`作为因变量，并将所有其他内容（`~ .`）作为自变量，即标记频率。它还指定了数据、方法（`"rf"`代表随机森林）、我们刚刚创建的控制机制以及要使用的调整场景数量：
- en: '[PRE20]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Improving speed with parallelization
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用并行化提高速度
- en: If you actually executed the previous code in your computer before reading this,
    you may have found that it took a long time to finish (8.41 minutes in our case).
    As we mentioned earlier, text analysis suffers from very high dimensional structures
    which take a long time to process. Furthermore, using CV runs will take a long
    time to run. To cut down on the total execution time, use the `doParallel` package
    to allow for multi-core computers to do the training in parallel and substantially
    cut down on time.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在阅读此内容之前已经在您的计算机上实际执行了前面的代码，您可能会发现它花费了很长时间才完成（在我们的例子中是8.41分钟）。正如我们之前提到的，文本分析受到非常高的维数结构的影响，这需要很长时间来处理。此外，使用交叉验证运行将需要很长时间。为了减少总执行时间，使用`doParallel`包允许多核计算机并行进行训练，从而显著减少时间。
- en: We proceed to create the `train_model()` function, which takes the data and
    the control mechanism as parameters. It then makes a cluster object with the `makeCluster()` function
    with a number of available cores (processors) equal to the number of cores in
    the computer, detected with the `detectCores()` function. Note that if you're
    planning on using your computer to do other tasks while you train your models,
    you should leave one or two cores free to avoid choking your system (you can then
    use `makeCluster(detectCores() - 2)` to accomplish this). After that, we start
    our time measuring mechanism, train our model, print the total time, stop the
    cluster, and return the resulting model.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续创建`train_model()`函数，该函数接受数据和控制机制作为参数。然后，它使用`makeCluster()`函数创建一个集群对象，其可用核心（处理器）数量等于通过`detectCores()`函数检测到的计算机中的核心数量。请注意，如果您计划在训练模型的同时使用计算机进行其他任务，您应该留出一到两个核心以避免系统过载（您可以使用`makeCluster(detectCores()
    - 2)`来完成此操作）。之后，我们开始时间测量机制，训练我们的模型，打印总时间，停止集群，并返回结果模型。
- en: '[PRE21]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now we can retrain the same model much faster. The time reduction will depend
    on your computer''s available resources. In the case of an 8-core system with
    32 GB of memory available, the total time was 3.34 minutes instead of the previous
    8.41 minutes, which implies that with parallelization, it only took 39% of the
    original time. Not bad right? We will study more the mechanics of parallelization
    and its advantages and disadvantages more in [Chapter 9](part0229.html#6QCGQ0-f494c932c729429fb734ce52cafce730),
    *Implementing An Efficient Simple Moving Average*. Let''s have look at how the
    model is trained:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Computing predictive accuracy and confusion matrices
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have our trained model, we can see its results and ask it to compute
    some predictive accuracy metrics. We start by simply printing the object we get
    back from the `train()` function. As can be seen, we have some useful metadata,
    but what we are concerned with right now is the predictive accuracy, shown in
    the `Accuracy` column. From the five values we told the function to use as testing
    scenarios, the best model was reached when we used 356 out of the 2,007 available
    features (tokens). In that case, our predictive accuracy was 65.36%.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: If we take into account the fact that the proportions in our data were around
    63% of cases with multiple purchases, we have made an improvement. This can be
    seen by the fact that if we just guessed the class with the most observations
    (`MULT_PURCHASES` being true) for all the observations, we would only have a 63%
    accuracy, but using our model we were able to improve toward 65%. This is a 3%
    improvement. We will try to increase this improvement as we go through this chapter.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: 'Keep in mind that this is a randomized process, and the results will be different
    every time you train these models. That''s why we want a repeated CV as well as
    various testing scenarios to make sure that our results are robust:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'To create a confusion matrix, we can use the `confusionMatrix()` function and
    send it the model''s predictions first and the real values second. This will not
    only create the confusion matrix for us, but also compute some useful metrics
    such as sensitivity and specificity. We won''t go deep into what these metrics
    mean or how to interpret them since that''s outside the scope of this book, but
    we highly encourage the reader to study them using the resources cited in this
    chapter:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Improving our results with TF-IDF
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In general in text analysis, a high raw count for a term inside a text does
    not necessarily mean that the term is more important for the text. One of the
    most important ways to normalize the term frequencies is to weigh a term by how
    often it appears not only in a text, but also in the entire corpus.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: The more a word appears inside a given text and doesn't appear too much across
    the whole corpus, it means that it's probably important for that specific text.
    However, if the term appears a lot inside a text, but also appears a lot in other
    texts in the corpus, it's probably not important for the specific text, but for
    the entire corpus, and this dilutes it's predictive power.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: In IR, TF-IDF is one of the most popular term-weighting schemes and it's the
    mathematical implementation of the idea expressed in the preceding paragraph.
    The TF-IDF value increases proportionally to the number of times a word appears
    in a given text, but is diluted by the frequency of the word in the entire corpus
    (not only the given text), which helps to adjust for the fact that some words
    appear more frequently in general. It is a powerful technique for enhancing the
    information contained within a DFM.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 'The TF normalizes all documents in the corpus to be length independent. The
    **inverse document frequency** (**IDF**) accounts for the frequency of term appearance
    in all documents in the corpus. The multiplication of TF by IDF takes both of
    these concepts into account by multiplying them. The mathematical definitions
    are as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '*n(DFM):= number  of texts in DFM*'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '*Freq(term, text):= count of term in text*'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '*Freq(term, DFM):= count of term in DFM*'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00050.jpeg)![](img/00051.jpeg)![](img/00052.jpeg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
- en: Now we are going to show how to program this TF-IDF statistic with R functions.
    Remember that we're working with a `dfm`, so we can use vectorized operations
    to make our functions efficient and easy to program. The first three functions
    `term_frequency()`, `inverse_document_frequency()`, and `tf_idf()`, should be
    easy to understand.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: 'The `build_tf_idf()` function makes use of these functions to actually build
    the TF-IDF-weighted DFM. The idea being that we need to apply the functions we
    created to the rows or columns as necessary using the `apply()` function. We need
    to transpose the structure we get to get the texts in the rows and the features
    in the columns which is why we use the `t()` function midway through. Finally,
    we need to realize that sometimes we get NA''s for certain combinations of data
    (try to figure out these cases yourself to make sure you understand) and we need
    to substitute them with zeros:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now we can easily build our TF-IDF-weighted DFM using our `build_tf_df()` function
    and create the corresponding data frame as we have done earlier:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now the values inside our TF-IDF-weighted DFM will not only be integer values,
    which corresponded to frequency counts, but will be floating point values that
    correspond to the TF-IDF weights instead. We can train our next model using the
    same `train_model()` we used earlier:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This time, our training process took 2.97 minutes. To see the results, simply
    print the model object. Remember that our previous predictive accuracy was of
    65.36%. Now that we have used the TF-IDF-weighted DFM, it increased to 66.48%.
    This is not a drastic improvement, and it''s due to the specific data we are working
    with. When working with other data or domains, you can expect this increase to
    be much larger:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们的训练过程耗时2.97分钟。要查看结果，只需打印模型对象。记住，我们之前的预测准确率为65.36%。现在我们使用了TF-IDF加权的DFM，它增加到了66.48%。这不是一个显著的改进，这归因于我们正在处理的具体数据。当处理其他数据或领域时，你可以预期这种增加会更大：
- en: '[PRE28]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Adding flexibility with N-grams
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过N-gram增加灵活性
- en: The bag-of-words model takes into account isolated terms called **unigrams**.
    This looses the order of the words, which can be important in some cases. A generalization
    of the technique is called n-grams, where we use single words as well as word
    pairs or word triplets, in the case of bigrams and trigrams, respectively. The
    n-gram refers to the general case where you keep up to `n` words together in the
    data. Naturally this representation exhibits unfavorable combinatorial complexity
    characteristics and makes the data grow exponentially. When dealing with a large
    corpus this can take significant computing power.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 词袋模型考虑了被称为**unigrams**的孤立术语。这丢失了单词的顺序，在某些情况下这可能很重要。这种技术的推广称为n-grams，其中我们使用单个单词以及单词对或单词三元组，在二元组和三元组的情况下分别。n-gram指的是保持数据中最多`n`个单词的一般情况。自然地，这种表示具有不利的组合复杂性特征，并使数据呈指数增长。当处理大型语料库时，这可能会消耗大量的计算能力。
- en: 'With the `sentence` object we created before to exemplify how the tokenization
    process works (it contains the sentence: `If it looks like a duck, swims like
    a duck, and quacks like a duck, then it probably is a duck`.) and the `build_dfm()` function
    we created with the `n_grams` argument, you can compare the resulting DFM with
    `n_grams = 2` to the one with `n_grams = 1`. After analyzing the features in this
    DFM, you should have a clear idea on how the tokenization process filters some
    data out and how the bigrams are created. As you can see, n-grams can potentially
    bring back some of the lost word ordering, which sometimes can be very useful:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们之前创建的`sentence`对象来举例说明标记化过程是如何工作的（它包含句子：“如果它看起来像鸭子，游泳像鸭子，嘎嘎叫像鸭子，那么它可能就是一只鸭子。”）和带有`n_grams`参数的`build_dfm()`函数，你可以比较`n_grams
    = 2`的结果DFM与`n_grams = 1`的结果。在分析这个DFM中的特征后，你应该对标记化过程如何过滤一些数据以及二元组是如何创建的有一个清晰的认识。正如你所见，n-gram有可能恢复一些丢失的单词顺序，这在某些情况下可能非常有用：
- en: '[PRE29]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'To retrain our full model, we will recreate our TF-IDF-weighted DFM with bigrams
    this time and its corresponding data frame. Using the function we created earlier,
    it can easily be done with the following code:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 为了重新训练我们的完整模型，我们将这次重新创建带有二元组的TF-IDF加权的DFM及其对应的数据框。使用我们之前创建的函数，可以通过以下代码轻松完成：
- en: '[PRE30]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Now we will retrain the model and analyze its results. This can potentially
    take a lot of time depending on the computer you're using for training it. In
    our 8-core computer with 32 GB of memory, it took 21.62 minutes when executed
    in parallel. This is due to the large increase in the number of predictors. As
    you can see, we now have 9,366 predictors instead of the 2,007 predictors we had
    before. This huge 4x increase is due to the bigrams.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将重新训练模型并分析其结果。这可能会根据你用于训练的计算机而花费大量时间。在我们的8核计算机和32GB内存上，当并行执行时，耗时21.62分钟。这是因为预测因子数量的显著增加。正如你所见，我们现在有9,366个预测因子，而不是之前的2,007个。这种巨大的4倍增加归因于二元组。
- en: 'In this particular case, it seems that the added complexity from the bigrams
    doesn''t increase our predictive accuracy. As a matter of fact, it decreases it.
    This can be for a couple of reasons, one of which is the increased sparsity, which
    implies a lower signal/noise ratio. In the next section, we will try to increase
    this ratio while keeping the bigrams:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个特定的情况下，添加的二元组复杂性并没有增加我们的预测准确率。事实上，它降低了。这可能有几个原因，其中之一是稀疏度的增加，这意味着信号/噪声比降低。在下一节中，我们将尝试提高这个比率，同时保持二元组：
- en: '[PRE31]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Reducing dimensionality with SVD
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过SVD降低维度
- en: As we have seen in the previous section, the dimensionality course in our data
    was amplified due to the n-gram technique. We would like to be able to use n-grams
    to bring back word ordering into our DFM, but we would like to reduce the feature
    space at the same time. To accomplish this, we can use a number of different dimensionality
    reduction techniques. In this case, we will show how to use the SVD.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如前文所述，由于n-gram技术，我们的数据维度发生了放大。我们希望能够使用n-gram将单词顺序带回到我们的DFM中，同时我们希望减少特征空间。为了实现这一点，我们可以使用多种不同的降维技术。在本例中，我们将展示如何使用SVD。
- en: The SVD helps us compress the data by using it's singular vectors instead of
    the original features. The math behind the technique is out of the scope of the
    book, but we encourage you to look at Meyer's, *Matrix Analysis & Applied Linear
    Algebra, 2000*. Basically, you can think of the singular vectors as the important
    directions in the data, so instead of using our *normal* axis, we can use these
    singular vectors in a transformed space where we have the largest signal/noise
    ratio possible. Computing the full SVD can potentially take a very large amount
    of time and we don't really need all the singular vectors. Therefore, we will
    use the `irlba` package to make use of the **Implicitly Restarted Lanczos Bidiagonalization
    Algorithm** (**IRLBA**) for a fast partial SVD, which is much faster.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: SVD通过使用其奇异向量而不是原始特征来压缩数据。该技术背后的数学超出了本书的范围，但我们鼓励你查阅Meyer的《矩阵分析与应用线性代数》（2000年）。基本上，你可以将奇异向量视为数据中的重要方向，因此我们可以在具有最大信号/噪声比的可能变换空间中使用这些奇异向量，而不是使用我们的*正常*轴。计算完整的SVD可能需要非常多的时间，而我们实际上并不需要所有的奇异向量。因此，我们将使用`irlba`包来利用**隐式重启兰索斯双向对角化算法**（**IRLBA**）进行快速的部分SVD，这要快得多。
- en: 'When using the partial SVD as our DFM, we are actually working in a different
    vector space, where each feature is no longer a token, but a combination of tokens.
    These new features are not easy to comprehend and you shouldn''t try to. Treat
    it like a *black-box* model, knowing that you''re operating in a higher signal/noise
    ratio space than you started in while drastically reducing it''s dimensions. In
    our case, we will make the new space 1/4 of the original space. To do so, we will
    create a wrapper function to measure the time it takes to actually compute the
    partial SVD. The actual computation will be done with the `irlba()` function,
    sending the TF-IDF-weighted bigrams DFM and the number of singular vectors we
    want (1/4 of the possible ones) as the `nv` parameter:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将部分奇异值分解（SVD）作为我们的降维方法（DFM）时，我们实际上是在一个不同的向量空间中工作，其中每个特征不再是单个标记，而是一系列标记的组合。这些新的特征不易理解，你不应该试图去理解它们。将其视为一个*黑盒*模型，知道你在比开始时更高的信号/噪声比空间中操作，同时大幅减少了其维度。在我们的案例中，我们将新空间缩小到原始空间的1/4。为此，我们将创建一个包装函数来测量实际计算部分SVD所需的时间。实际的计算将通过`irlba()`函数完成，将TF-IDF加权的二元组DFM和我们要的奇异向量数（可能的1/4）作为`nv`参数发送：
- en: '[PRE32]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Now we can easily create the partial SVD and the corresponding data frame. We
    also proceed to retrain our model. Note that even though it is conceptual, the
    `train.bigrams.svd` is our new DFM, in practice, within R, it's an object that
    contains our DFM as well as other data. Our DFM is in the `v` object within the
    `train.bigrams.svd` object, which is what we send to the `buildf_dfm_df()` function.
    Another important object within `train.bigrams.svd` is `d`,  which contains the
    singular values from the decomposition.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以轻松地创建部分SVD和相应的数据框。我们还继续重新训练我们的模型。请注意，尽管它是概念性的，但`train.bigrams.svd`是我们的新DFM，在实践中，在R中，它是一个包含我们的DFM以及其他数据的对象。我们的DFM位于`train.bigrams.svd`对象中的`v`对象内，这是我们发送给`buildf_dfm_df()`函数的内容。`train.bigrams.svd`中的另一个重要对象是`d`，它包含分解的奇异值。
- en: 'As you can see, our feature space was drastically reduced to only 53 features
    (which is approximately 1/4 of the 212 samples available). However, our predictive
    accuracy was not higher than our previous results either. This means that probably
    bigrams are not adding too much information for this particular problem:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们的特征空间被大幅减少到只有53个特征（这大约是212个样本的1/4）。然而，我们的预测准确度也没有比之前的结果更高。这意味着对于这个问题，二元组可能并没有增加太多信息：
- en: '[PRE33]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Extending our analysis with cosine similarity
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用余弦相似度扩展我们的分析
- en: Now we proceed to another technique familiar in linear algebra which operates
    on a vector space. The technique is known as **cosine similarity** (**CS**), and
    its purpose is to find vectors that are similar (or different) from each other.
    The idea is to measure the direction similarity (not magnitude) among client messages,
    and try to use it to predict similar outcomes when it comes to multiple purchases.
    The cosine similarity will be between 0 and 1 when the vectors are orthogonal
    and perpendicular, respectively. However, this similarity should not be interpreted
    as percentage because the movement rate for the cosine function is not linear.
    This means that a movement from 0.2 to 0.3 does not represent a similar movement
    magnitude from 0.8 to 0.9.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们继续探讨线性代数中的一种熟悉技术，该技术作用于向量空间。这种技术被称为**余弦相似度**（**CS**），其目的是找到彼此相似（或不同）的向量。想法是测量客户消息之间的方向相似度（而不是大小），并尝试在涉及多次购买时预测相似的结果。当向量正交时，余弦相似度为0；当向量垂直时，余弦相似度为1。然而，这种相似度不应被解释为百分比，因为余弦函数的移动速率不是线性的。这意味着从0.2到0.3的移动并不代表从0.8到0.9的相似移动幅度。
- en: Given two vectors (rows in our DFM), the cosine similarity among them is computed
    by taking the dot product between them and dividing it by the product of the Euclidian
    norms. To review what these concepts mean, take a look at Meyer's, *Matrix Analysis
    & Applied Linear Algebra, 2000*.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 给定两个向量（在我们的DFM中的行），它们之间的余弦相似度是通过计算它们的点积并将其除以欧几里得范数的乘积来计算的。为了回顾这些概念的含义，请参阅Meyer的《矩阵分析与应用线性代数，2000》。
- en: '![](img/00053.jpeg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/00053.jpeg)'
- en: 'We create the `cosine_similarties()` function that will make use of the `cosine()` function
    from the `lsa` package. We send it a data frame and remove the first column, which
    corresponds to the dependent variable `MULT_PURCHASES`, and we use the transpose
    to make sure that we''re working with the correct orientation:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建`cosine_similarties()`函数，它将使用`lsa`包中的`cosine()`函数。我们发送一个数据框，并移除第一列，这对应于因变量`MULT_PURCHASES`，并使用转置来确保我们使用的是正确的方向：
- en: '[PRE34]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now we create the `mean_cosine_similarities()` function, which will take the
    cosine similarity among those texts that correspond to clients that have performed
    multiple purchases and will take the means of these similarities. We need to take
    the mean because we are computing many similarities among many vectors, and we
    want to aggregate them for each one of them. We could use other aggregation mechanisms,
    but the mean is fine for now:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们创建`mean_cosine_similarities()`函数，它将计算对应于执行过多次购买的客户的文本之间的余弦相似度，并将这些相似度的平均值取出来。我们需要取平均值，因为我们正在计算许多向量之间的许多相似度，并且我们希望为每个向量聚合它们。我们可以使用其他聚合机制，但现在平均值就足够了：
- en: '[PRE35]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Now we can use this function to generate a new DFM''s data frame that will
    be used to train a new model, which will take into account the cosine similarity
    among texts. As we saw earlier, it seems that using bigrams is not helping too
    much for this particular data. In the next section, we will try a different, very
    interesting, technique, sentiment analysis. Let''s look at the following code:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用这个函数生成一个新的DFM数据框，该数据框将用于训练一个新的模型，该模型将考虑文本之间的余弦相似度。正如我们之前看到的，对于这种特定的数据，使用双词组并没有太大的帮助。在下一节中，我们将尝试一种不同且非常有趣的技术，即情感分析。让我们看看下面的代码：
- en: '[PRE36]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Digging deeper with sentiment analysis
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 情感分析的深入挖掘
- en: We have now seen that vector space operations did not work too well regarding
    the predictive accuracy of our model. In this section, we will attempt a technique
    which is very different and is closer to the semantic parsing model we mentioned
    at the beginning of this chapter. We will try sentiment analysis.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，在预测模型的准确性方面，向量空间操作并没有很好地工作。在本节中，我们将尝试一种非常不同的技术，它更接近于我们在本章开头提到的语义解析模型。我们将尝试情感分析。
- en: We will not only take into account the words in a text, but we will also take
    into account shifters (that is, negators, amplifiers, de-amplifiers, and adversative
    conjunctions). A negator flips the sign of a polarized word (for example, I do
    not like it). An amplifier increases the impact of a polarized word (for example,
    I really like it.). A de-amplifier reduces the impact of a polarized word (for
    example, I hardly like it). An adversative conjunction overrules the previous
    clause containing a polarized word (for example, I like it but it's not worth
    it). This can be very powerful with some types of data.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不仅会考虑文本中的单词，还会考虑位移词（即否定词、放大器、去放大器和对抗性连词）。一个否定词会翻转极化词的符号（例如，我不喜欢它）。一个放大器会增加极化词的影响（例如，我真的很喜欢它）。一个去放大器会减少极化词的影响（例如，我几乎不喜欢它）。一个对抗性连词会覆盖包含极化词的前一个子句（例如，我喜欢它，但它不值得）。这在某些类型的数据中可能非常有力量。
- en: Our sentiment analysis will produce a number, which will indicate the sentiment
    measured from the text. These numbers are unbounded and can be either positive
    or negative, corresponding to positive or negative sentiments. The larger the
    number, the stronger the inferred sentiment. To implement the technique, we will
    use the `sentimentr` package, which includes a clever algorithm to compute these
    sentiments. For the enthusiast, the details of the equation used are in its documentation
    ([https://cran.r-project.org/web/packages/sentimentr/sentimentr.pdf](https://cran.r-project.org/web/packages/sentimentr/sentimentr.pdf)).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的情感分析将产生一个数字，该数字将指示从文本中测量的情感。这些数字是无界的，可以是正数或负数，对应于积极或消极的情感。数字越大，推断出的情感越强烈。为了实现这项技术，我们将使用`sentimentr`包，该包包含一个巧妙的算法来计算这些情感。对于爱好者来说，方程式的详细信息可以在其文档中找到（[https://cran.r-project.org/web/packages/sentimentr/sentimentr.pdf](https://cran.r-project.org/web/packages/sentimentr/sentimentr.pdf)）。
- en: To apply this technique, we send messages to the `sentiment_by()` function.
    This will give us back an object that contains, among other things, the `word_count` value
    and `ave_sentiment`, which is the average sentiment measured in all the sentences
    within a given text (`sentinmentr` internally splits each text into its components
    (sentences) and measures sentiment for each of them). We then add this objects
    into our DFM and proceed to train our model.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 要应用这项技术，我们向`sentiment_by()`函数发送消息。这将返回一个包含多个值的对象，其中包括`word_count`值和`ave_sentiment`，后者是在给定文本中的所有句子中测量的平均情感（`sentinmentr`内部将每个文本分割成其组成部分（句子）并为每个部分测量情感）。然后我们将此对象添加到我们的DFM中并继续训练我们的模型。
- en: 'As you can see, this time we get a large increase in the predictive accuracy
    of the model up to 71.73%. This means that the sentiment feature is highly predictive
    compared to the other features we engineered in previous sections. Even though
    we could continue mixing models and exploring to see if we can get even higher
    predictive accuracy, we will stop at this point since you probably understand
    how to do these on your own at this point:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，这次我们的模型预测准确性大幅提高，达到71.73%。这意味着与我们在前几节中构建的其他特征相比，情感特征具有高度的预测性。尽管我们可以继续混合模型并探索，看看是否可以获得更高的预测准确性，但我们将在这一点上停止，因为您现在可能已经理解了如何自己进行这些操作：
- en: '[PRE37]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Sentiment analysis, even though it looks very easy since it did not require
    a lot of code thanks to the `sentimentr` package, is actually a very hard area
    with active research behind it. It's very important for companies to understand
    how their customers feel about them, and do so accurately. It is and will continue
    to be a very interesting area of research.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管由于`sentimentr`包的存在，情感分析看起来非常简单，因为它不需要很多代码，但实际上这是一个非常困难的领域，背后有活跃的研究。对于公司来说，了解他们的客户如何看待他们非常重要，并且需要准确地进行。这现在并将继续成为一个非常有趣的研究领域。
- en: Testing our predictive model with unseen data
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用未见数据测试我们的预测模型
- en: Now that we have our final model, we need to validate its results by testing
    it with unseen data. This will give us the confidence that our model is well trained
    and will probably produce similar results where new data is handed to use.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了最终模型，我们需要通过使用未见数据对其进行测试来验证其结果。这将使我们确信我们的模型训练良好，并且在新数据提供给我们时可能会产生类似的结果。
- en: A careful reader should have noticed that we used the TF-IDF data frame when
    creating our sentiment analysis data, and not any of the ones we create later
    with combinations of bigrams, SVDs, and cosine similarities, which operate in
    a different semantic space due to the fact they are transformations of the original
    DFM. Therefore, before we can actually use our trained model to make predictions
    on the test data, we need to transform it into an equivalent space as our training
    data. Otherwise, we would be comparing apples and oranges, which would give us
    nonsense results.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 一个细心的读者应该已经注意到，我们在创建情感分析数据时使用了TF-IDF数据框，而不是我们后来创建的任何结合了二元组、SVD和余弦相似度的数据框，因为它们在原始DFM的变换中处于不同的语义空间。因此，在我们实际上可以使用训练好的模型对测试数据进行预测之前，我们需要将其转换成与训练数据等效的空间。否则，我们就会比较苹果和橘子，这会给出荒谬的结果。
- en: 'To make sure that we''re working in the same semantic space, we will apply
    the TF-IDF weights to our test data. However, if you think about it, there probably
    are a lot of terms in our test data that were not present in our training data.
    Therefore, our DFMs will have different dimensions for our training and testing
    sets. This is a problem, and we need to make sure that they are the same. To accomplish
    this, we build the DFM for the testing data and apply a filter to it which only
    keeps the terms that are present in our training DFM:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保我们在相同的语义空间中工作，我们将TF-IDF权重应用于我们的测试数据。然而，如果你仔细想想，我们的测试数据中可能有很多术语在训练数据中并不存在。因此，我们的训练集和测试集的DFM将具有不同的维度。这是一个问题，我们需要确保它们是相同的。为了实现这一点，我们为测试数据构建DFM并对其应用一个过滤器，只保留在训练DFM中存在的术语：
- en: '[PRE38]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Furthermore, if you think about it, the corpus-weighing part of the TF-IDF,
    that is the IDF, will also be different than these two datasets due to the change
    in the term space for the corpus. Therefore, we need to make sure that we ignore
    those terms that are new (that is, were not seen in our training data) and use
    the IDF from our training procedure to make sure that our tests are valid. To
    accomplish this, we will first compute only the IDF part of the TF-IDF for our
    training data and use that when computing the TF-IDF for our testing data:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果你仔细想想，TF-IDF的语料库加权部分，即IDF，也会因为语料库术语空间的变化而与这两个数据集不同。因此，我们需要确保我们忽略那些新的术语（即在训练数据中未见过）并使用训练过程中的IDF来确保我们的测试是有效的。为了实现这一点，我们将首先只计算训练数据的TF-IDF的IDF部分，并在计算测试数据的TF-IDF时使用它：
- en: '[PRE39]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Now that we have the testing data projected into the vector space of the training
    data, we can compute the sentiment analysis for the new testing DFM and compute
    our predictions:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将测试数据映射到训练数据的向量空间中，我们可以对新的测试DFM进行情感分析并计算我们的预测结果：
- en: '[PRE40]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Note that we are not training a new model in this case, we are just using the
    last model we created and use that to provide predictions for the testing DFM:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这种情况下，我们并没有训练一个新的模型，我们只是使用我们创建的最后一个模型，并使用它来为测试DFM提供预测：
- en: '[PRE41]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: To know how well we predicted, we can just print a model as we did before because
    we would just be looking at the results from the training process that does not
    include predictions for the testing data. What we need to do is create a confusion
    matrix and compute the predictive accuracy metrics as we did before with the `confusionMatrix()` function.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 要知道我们的预测效果如何，我们只需像之前一样打印一个模型即可，因为我们只是查看训练过程中的结果，这些结果不包括测试数据的预测。我们需要做的是创建一个混淆矩阵并计算预测准确度指标，就像我们之前使用`confusionMatrix()`函数所做的那样。
- en: As you can see, our results seem to be valid since we got a predictive accuracy
    of 71.91% with previously unseen data, which is very close to the predictive accuracy
    of the training data, and is 12% more than just guessing actual multiple purchases
    proportion. For text data and the problem we're dealing with, these results are
    pretty good.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们的结果似乎有效，因为我们用71.91%的预测准确度对未见过的数据进行预测，这非常接近训练数据的预测准确度，并且比仅仅猜测实际多次购买比例高出12%。对于文本数据和我们所处理的问题，这些结果相当不错。
- en: If you know how to interpret the other metrics, make sure that you compare them
    to ones we had for our first model to realize how our results evolved during the
    chapter.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你知道如何解释其他指标，请确保将它们与我们第一个模型中的指标进行比较，以了解我们在本章中结果是如何演变的。
- en: 'If you''re not familiar with them, we suggest you take a look at James, Witten,
    Hastie, and Tibshirani''s, *Statistical Learning, 2013*:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不熟悉这些内容，我们建议您查阅James, Witten, Hastie和Tibshirani的《统计学习，2013》：
- en: '[PRE42]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'If you''re going to test with a DFM that was created using an SVD, you need
    to make the corresponding transformation to make sure you''re working in the correct
    semantic space before producing any predictions. If you used a procedure like
    the one shown in this chapter, you need to left-multiply the testing DFM (with
    similar transformations as your training DFM) with the vector of singular values
    adjusted by sigma, while transposing the structures accordingly. The exact transformation
    will depend on the data structures you''re using and processes you applied to
    them, but always remember to make sure that both your training and testing data
    operate in the same semantic space:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您打算使用通过SVD创建的DFM进行测试，您需要在生成任何预测之前进行相应的转换，以确保您在正确的语义空间中工作。如果您使用了本章中展示的程序，您需要将测试DFM（与您的训练DFM具有类似的转换）左乘以由sigma调整的奇异值向量，并相应地转置结构。确切的转换将取决于您使用的数据结构和您对其应用的过程，但请始终确保您的训练数据和测试数据在相同的语义空间中操作：
- en: '[PRE43]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Retrieving text data from Twitter
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从Twitter检索文本数据
- en: Before we finish this chapter, we will very briefly touch on a completely different,
    yet very sought-after topic, that is, getting data from Twitter. In case you want
    to apply predictive models, you will need to link the Twitter data to a variable
    you want to predict, which normally comes from other data. However, something
    you can easily do is measure the sentiment around a topic using the techniques
    we showed in a previous section.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们结束本章之前，我们将非常简要地触及一个完全不同但非常受欢迎的主题，即从Twitter获取数据。如果您想应用预测模型，您需要将Twitter数据链接到您想要预测的变量，这通常来自其他数据。然而，您可以轻松做到的是，使用我们在前一部分展示的技术来衡量一个主题周围的情感。
- en: The `twitteR` package actually makes it very easy for us to retrieve Twitter
    data. To do so, we will create a **Twitter App** within Twitter, which will give
    us access to the data feed. To accomplish this, we need to generate four strings
    within your Twitter account that will be the keys to using the API. These keys
    are used to validate your permissions and monitor your usage in general. Specifically,
    you need four strings, the `consumer_key` value, the `consumer_secret`, the `access_token`,
    and the `access_secret`. To retrieve them, go to the Twitter Apps website ([https://apps.twitter.com/](https://apps.twitter.com/)),
    click on Create New App, and input the information required. The name for your
    Twitter App must be unique across all of the Twitter Apps. Don't worry about picking
    a complex name, you'll never use that string again. Also make sure that you read
    the Twitter Developer Agreement and that you agree with it.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '`twitteR`包实际上使我们能够非常容易地检索Twitter数据。为此，我们将在Twitter内部创建一个**Twitter应用**，这将使我们能够访问数据源。为了完成此操作，我们需要在您的Twitter账户中生成四个字符串，这些字符串将是使用API的密钥。这些密钥用于验证您的权限并监控您的总体使用情况。具体来说，您需要四个字符串，即`consumer_key`值、`consumer_secret`、`access_token`和`access_secret`。要检索它们，请访问Twitter
    Apps网站([https://apps.twitter.com/](https://apps.twitter.com/))，点击创建新应用，并输入所需的信息。您的Twitter应用名称必须在所有Twitter应用中是唯一的。不用担心选择一个复杂的名称，您永远不会再次使用那个字符串。此外，请确保您阅读了Twitter开发者协议，并且您同意它。'
- en: 'Once inside the dashboard for your app, go to the Keys and Access Tokens tab,
    and generate a key and access token with their corresponding secret keys. Make
    sure that you copy those strings exactly as they will grant you access to the
    data feed. Substitute them instead of the ones shown here (which no longer work
    since they were deleted after writing this book), and execute the `setup_twitter_oauth()` function.
    If everything went as expected, you should now have connected your R session to
    the data feed:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦进入您应用的仪表板，请转到“密钥和访问令牌”选项卡，并生成一个密钥和访问令牌及其相应的密钥。确保您准确无误地复制这些字符串，因为它们将授予您访问数据源的权利。用它们替换这里显示的（由于本书编写后已删除，因此不再有效），并执行`setup_twitter_oauth()`函数。如果一切如预期进行，现在您应该已经将您的R会话连接到了数据源：
- en: '[PRE44]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'To retrieve data, we will create a wrapper function that will save us writing
    boilerplate again and again every time we want new data. The `get_twitter_data()` function
    takes a keyword we''re searching for within Twitter and the number of messages
    we want to retrieve. It then goes on to get the data from Twitter using the `searchTwitter()` function
    (in English), transform the results into a data frame with the `twListToDF()` function,
    and send that back to the user:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检索数据，我们将创建一个包装函数，这样我们就不必每次需要新数据时都重复编写样板代码。`get_twitter_data()` 函数接受我们在 Twitter
    中搜索的关键词以及我们想要检索的消息数量。然后，它继续使用 `searchTwitter()` 函数（英文）从 Twitter 获取数据，使用 `twListToDF()`
    函数将结果转换成数据框，并将数据发送回用户：
- en: '[PRE45]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Now we can easily search for messages that contain the word "*cake*" inside
    by executing the following code. As you can see, we don''t only get the messages,
    but we also get a lot of metadata, like whether or not the tweet has been favorite;
    if so, how many times, whether it was a reply, when was it created, and the coordinates
    of where the tweet was sent if they are available, among other things:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过执行以下代码轻松搜索包含单词 "*cake*" 的消息。如您所见，我们不仅获得了消息，还获得了大量元数据，例如推文是否已被收藏；如果是，收藏了多少次；是否为回复；创建时间；如果可用，推文发送的坐标，以及其他事项：
- en: '[PRE46]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: As an exercise, get data from Twitter with the mechanism shown precedingly and
    use it to create a world map that shows where the tweets are coming from and colors
    the pin locations using the sentiment inferred from the tweet. Having a piece
    of code like that can be fun to play with and joke around with your friends as
    well as for making real business decisions.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一项练习，使用前面展示的机制从 Twitter 获取数据，并使用它创建一个世界地图，显示推文的来源，并使用从推文中推断出的情感来着色推文的位置。拥有这样的代码片段可以很有趣，可以和朋友一起玩耍和开玩笑，也可以用于做出真正的商业决策。
- en: Summary
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we showed how to perform predictive analysis using text data.
    To do so, we showed how to tokenize text to extract relevant words, how to build
    and work with **document-feature matrices** (**DFMs**), how to apply transformations
    to DFMs to explore different predictive models using term frequency-inverse document
    frequency weights, n-grams, partial singular value decompositions, and cosine
    similarities, and how to use these data structures within random forests to produce
    predictions. You learned why these techniques may be important for some problems
    and how to combine them. We also showed how to include sentiment analysis inferred
    from text to increase the predictive power of our models. Finally, we showed how
    to retrieve live data from Twitter that can be used to analyze what people are
    saying in the social network shortly after they have said it.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们展示了如何使用文本数据进行预测分析。为此，我们展示了如何对文本进行分词以提取相关单词，如何构建和使用**文档-特征矩阵**（**DFMs**），如何对
    DFMs 应用转换以使用词频-逆文档频率权重、n-gram、部分奇异值分解和余弦相似度探索不同的预测模型，以及如何在随机森林中使用这些数据结构来生成预测。您学习了为什么这些技术对于某些问题可能很重要，以及如何将它们结合起来。我们还展示了如何将文本中推断出的情感分析包括进来，以增强我们模型的预测能力。最后，我们展示了如何从
    Twitter 获取实时数据，这些数据可以用来分析人们在说出他们的观点后不久在社交网络中说些什么。
- en: We encourage you to combine the knowledge from this chapter with that from previous
    chapters to try to gain deeper insights. For example, what happens if we use trigrams
    of quadgrams? What happens if we include other features in the data (for example,
    coordinates or client IDs)? And, what happens if we use other predictive models
    instead of random forests, such as support vector machines?
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们鼓励您将本章的知识与之前章节的知识结合起来，以尝试获得更深入的见解。例如，如果我们使用四元组的三角元会发生什么？如果我们将其他特征（例如，坐标或客户端
    ID）包含在数据中会发生什么？以及，如果我们使用支持向量机等而不是随机森林作为预测模型会发生什么？
- en: In the next chapter, we will look at how to use what we have done during the
    last three chapters to produce reports that can be automatically generated when
    we receive new data from The Cake Factory. We will cover how to produce PDFs automatically,
    how to create presentations that can be automatically updated, as well as other
    interesting reporting techniques.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨如何使用前三个章节中我们所做的工作来生成当从蛋糕工厂收到新数据时可以自动生成的报告。我们将介绍如何自动生成 PDF，如何创建可以自动更新的演示文稿，以及其他有趣的报告技术。
