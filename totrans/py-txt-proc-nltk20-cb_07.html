<html><head></head><body><div class="chapter" title="Chapter&#xA0;7.&#xA0;Text Classification"><div class="titlepage"><div><div><h1 class="title"><a id="ch07"/>Chapter 7. Text Classification</h1></div></div></div><p>In this chapter, we will cover:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Bag of Words feature extraction</li><li class="listitem" style="list-style-type: disc">Training a naive Bayes classifier</li><li class="listitem" style="list-style-type: disc">Training a decision tree classifier</li><li class="listitem" style="list-style-type: disc">Training a maximum entropy classifier</li><li class="listitem" style="list-style-type: disc">Measuring precision and recall of a classifier</li><li class="listitem" style="list-style-type: disc">Calculating high information words</li><li class="listitem" style="list-style-type: disc">Combining classifiers with voting</li><li class="listitem" style="list-style-type: disc">Classifying with multiple binary classifiers</li></ul></div><div class="section" title="Introduction"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec72"/>Introduction</h1></div></div></div><a id="id508" class="indexterm"/><p>
<span class="strong"><strong>Text classification</strong></span> is a way to categorize documents or pieces of text. By examining the word usage in a piece of text, classifiers can decide what <span class="emphasis"><em>class label</em></span> to assign to it. A <a id="id509" class="indexterm"/>
<span class="strong"><strong>binary classifier</strong></span> decides between two labels, such as positive or negative. The text can either be one label or the other, but not both, whereas a <span class="strong"><strong>multi-label classifier</strong></span>
<a id="id510" class="indexterm"/> can assign one or more labels to a piece of text.</p><p>Classification works by learning from<a id="id511" class="indexterm"/> <span class="emphasis"><em>labeled feature sets</em></span>, or training data, to later classify an <a id="id512" class="indexterm"/> <span class="emphasis"><em>unlabeled feature set</em></span>. A <a id="id513" class="indexterm"/>
<span class="strong"><strong>feature set</strong></span> is basically a key-value mapping of <span class="emphasis"><em>feature names</em></span> to <span class="emphasis"><em>feature values</em></span>. In the case of text classification, the feature names are usually words, and the values are all <code class="literal">True</code>. As the documents may have unknown words, and the number of possible words may be very large, words that don't occur in the text are omitted, instead of including them in a feature set with the value <code class="literal">False</code>.</p><a id="id514" class="indexterm"/><p>An <span class="strong"><strong>instance</strong></span> is a single feature set. It represents a single occurrence of a combination of features. We will use <span class="emphasis"><em>instance</em></span> and <span class="emphasis"><em>feature set</em></span> interchangeably. A <span class="emphasis"><em>labeled feature set</em></span> is an instance with a known class label that we can use for training or evaluation.</p></div></div>
<div class="section" title="Bag of Words feature extraction"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec73"/>Bag of Words feature extraction</h1></div></div></div><a id="id515" class="indexterm"/><p>Text feature extraction is the process of transforming what is essentially a list of words into a feature set that is usable by a classifier. The NLTK classifiers expect <a id="id516" class="indexterm"/>
<code class="literal">dict</code> style feature sets, so we must therefore transform our text into a <code class="literal">dict</code>. The <a id="id517" class="indexterm"/>
<span class="strong"><strong>Bag of Words</strong></span> model is the simplest method; it constructs a <span class="emphasis"><em>word presence</em></span> feature set from all the words of an instance.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec257"/>How to do it...</h2></div></div></div><p>The idea is to convert a list of words into a <code class="literal">dict</code>, where each word becomes a key with the value <code class="literal">True</code>. The <a id="id518" class="indexterm"/>
<code class="literal">bag_of_words()</code> function in <code class="literal">featx.py</code> looks like this:</p><div class="informalexample"><pre class="programlisting">def bag_of_words(words):
  return dict([(word, True) for word in words])</pre></div><p>We can use it with a list of words, in this case the tokenized sentence "the quick brown fox":</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from featx import bag_of_words
&gt;&gt;&gt; bag_of_words(['the', 'quick', 'brown', 'fox'])
{'quick': True, 'brown': True, 'the': True, 'fox': True}</pre></div><p>The resulting <code class="literal">dict</code> is known as a <span class="emphasis"><em>bag of words</em></span> because the words are not in order, and it doesn't matter where in the list of words they occurred, or how many times they occurred. All that matters is that the word is found at least once.</p></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec258"/>How it works...</h2></div></div></div><a id="id519" class="indexterm"/><p>The <code class="literal">bag_of_words()</code> function is a very simple <span class="emphasis"><em>list comprehension</em></span> that constructs a <code class="literal">dict</code> from the given words, where every word gets the value <code class="literal">True</code>.</p><p>Since we have to assign a value to each word in order to create a <code class="literal">dict</code>, <code class="literal">True</code> is a logical choice for the value to indicate word presence. If we knew the universe of all possible words, we could assign the value <code class="literal">False</code> to all the words that are not in the given list of words. But most of the time, we don't know all possible words beforehand. Plus, the <code class="literal">dict</code> that would result from assigning <code class="literal">False</code> to every possible word would be very large (assuming all words in the English language are possible). So instead, to keep feature extraction simple and use less memory, we stick with assigning the value <code class="literal">True</code> to all words that occur at least once. We don't assign the value <code class="literal">False</code> to any words since we don't know what the set of possible words are; we only know about the words we are given.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec259"/>There's more...</h2></div></div></div><p>In the default Bag of Words model, all words are treated equally. But that's not always a good idea. As we already know, some words are so common that they are practically meaningless. If you have a set of words that you want to exclude, you can use the <a id="id520" class="indexterm"/>
<code class="literal">bag_of_words_not_in_set()</code> function in <code class="literal">featx.py</code>.</p><div class="informalexample"><pre class="programlisting">def bag_of_words_not_in_set(words, badwords):
  return bag_of_words(set(words) - set(badwords))</pre></div><p>This function can be used, among other things, to filter stopwords. Here's an example where we filter the word "the" from "the quick brown fox":</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from featx import bag_of_words_not_in_set
&gt;&gt;&gt; bag_of_words_not_in_set(['the', 'quick', 'brown', 'fox'], ['the'])
{'quick': True, 'brown': True, 'fox': True}</pre></div><p>As expected, the resulting <code class="literal">dict</code> has "quick", "brown", and "fox", but not "the".</p><div class="section" title="Filtering stopwords"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec66"/>Filtering stopwords</h3></div></div></div><a id="id521" class="indexterm"/><p>Here's an example of using the <a id="id522" class="indexterm"/>
<code class="literal">bag_of_words_not_in_set()</code> function to filter all English stopwords:</p><div class="informalexample"><pre class="programlisting">from nltk.corpus import stopwords

def bag_of_non_stopwords(words, stopfile='english'):
  badwords = stopwords.words(stopfile)
  return bag_of_words_not_in_set(words, badwords)</pre></div><p>You can pass a different language filename as the<a id="id523" class="indexterm"/> <code class="literal">stopfile</code> keyword argument if you are using a language other than English. Using this function produces the same result as the previous example:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from featx import bag_of_non_stopwords
&gt;&gt;&gt; bag_of_non_stopwords(['the', 'quick', 'brown', 'fox'])
{'quick': True, 'brown': True, 'fox': True}</pre></div><p>Here, "the" is a stopword, so it is not present in the returned <code class="literal">dict</code>.</p></div><div class="section" title="Including significant bigrams"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec67"/>Including significant bigrams</h3></div></div></div><a id="id524" class="indexterm"/><p>In addition to single words, it often helps to include significant bigrams. As significant bigrams are less common than most individual words, including them in the Bag of Words can help the classifier make better decisions. We can use the <code class="literal">BigramCollocationFinder</code> covered in the <span class="emphasis"><em>Discovering word collocations</em></span> recipe of <a class="link" href="ch01.html" title="Chapter 1. Tokenizing Text and WordNet Basics">Chapter 1</a>, <span class="emphasis"><em>Tokenizing Text and WordNet Basics</em></span>, to find significant bigrams. <a id="id525" class="indexterm"/>
<code class="literal">bag_of_bigrams_words()</code> found in <code class="literal">featx.py</code> will return a <code class="literal">dict</code> of all words along with the 200 most significant bigrams.</p><div class="informalexample"><pre class="programlisting">from nltk.collocations import BigramCollocationFinder
from nltk.metrics import BigramAssocMeasures

def bag_of_bigrams_words(words, score_fn=BigramAssocMeasures.chi_sq, n=200):
  bigram_finder = BigramCollocationFinder.from_words(words)
  bigrams = bigram_finder.nbest(score_fn, n)
  return bag_of_words(words + bigrams)</pre></div><a id="id526" class="indexterm"/><p>The bigrams will be present in the returned <code class="literal">dict</code> as <code class="literal">(word1, word2)</code> and will have the value as <code class="literal">True</code>. Using the same example words as before, we get all words plus every bigram:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from featx import bag_of_bigrams_words
&gt;&gt;&gt; bag_of_bigrams_words(['the', 'quick', 'brown', 'fox'])
{'brown': True, ('brown', 'fox'): True, ('the', 'quick'): True, 'fox': True, ('quick', 'brown'): True, 'quick': True, 'the': True}</pre></div><p>You can change the maximum number of bigrams found by altering the keyword argument <code class="literal">n</code>.</p></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec260"/>See also</h2></div></div></div><p>The <span class="emphasis"><em>Discovering word collocations</em></span> recipe of <a class="link" href="ch01.html" title="Chapter 1. Tokenizing Text and WordNet Basics">Chapter 1</a>, <span class="emphasis"><em>Tokenizing Text and WordNet Basics</em></span> covers the <code class="literal">BigramCollocationFinder</code> in more detail. In the next recipe, we will train a <code class="literal">NaiveBayesClassifier</code> using feature sets created with the Bag of Words model.</p></div></div>
<div class="section" title="Training a naive Bayes classifier"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec74"/>Training a naive Bayes classifier</h1></div></div></div><a id="id527" class="indexterm"/><p>Now that we can extract features from text, we can train a classifier. The easiest classifier to get started with is the <code class="literal">NaiveBayesClassifier</code>
<a id="id528" class="indexterm"/>. <a id="id529" class="indexterm"/>It uses <span class="strong"><strong>Bayes Theorem</strong></span> to predict the probability that a given feature set belongs to a particular label. The formula is:</p><div class="informalexample"><pre class="programlisting">P(label | features) = P(label) * P(features | label) / P(features)</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">P(label)</code> <a id="id530" class="indexterm"/>is the prior probability of the label occurring, which is the same as the likelihood that a random feature set will have the label. This is based on the number of training instances with the label compared to the total number of training instances. For example, if 60/100 training instances have the label, the prior probability of the label is 60 percent.</li><li class="listitem" style="list-style-type: disc"><code class="literal">P(features | label)</code> <a id="id531" class="indexterm"/>is the prior probability of a given feature set being classified as that label. This is based on which features have occurred with each label in the training data.</li><li class="listitem" style="list-style-type: disc"><code class="literal">P(features)</code><a id="id532" class="indexterm"/> is the prior probability of a given feature set occurring. This is the likelihood of a random feature set being the same as the given feature set, and is based on the observed feature sets in the training data. For example, if the given feature set occurs twice in 100 training instances, the prior probability is 2 percent.</li><li class="listitem" style="list-style-type: disc"><code class="literal">P(label | features)</code><a id="id533" class="indexterm"/> tells us the probability that the given features should have that label. If this value is high, then we can be reasonably confident that the label is correct for the given features.</li></ul></div><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec261"/>Getting ready</h2></div></div></div><a id="id534" class="indexterm"/><p>We are going to be using the <code class="literal">movie_reviews</code> corpus for our initial classification examples. This corpus contains two categories of text: <code class="literal">pos</code> and <code class="literal">neg</code>. These categories are exclusive, which makes a classifier trained on them a <a id="id535" class="indexterm"/>
<span class="strong"><strong>binary classifier</strong></span>. Binary classifiers have only two classification labels, and will always choose one or the other.</p><p>Each file in the <code class="literal">movie_reviews</code> corpus is composed of either positive or negative movie reviews. We will be using each file as a single instance for both training and testing the classifier. Because of the nature of the text and its categories, the classification we will be doing is a form of <span class="emphasis"><em>sentiment analysis</em></span>. If the classifier returns <code class="literal">pos</code>, then the text expresses <span class="emphasis"><em>positive sentiment</em></span>, whereas if we get <code class="literal">neg</code>, then the text expresses <span class="emphasis"><em>negative sentiment</em></span>.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec262"/>How to do it...</h2></div></div></div><p>For training, we need to first create a list of labeled feature sets. This list should be of the form <code class="literal">[(featureset, label)]</code> where the <code class="literal">featureset</code> is a <code class="literal">dict</code>, and <code class="literal">label</code> is the known class label for the <code class="literal">featureset</code>. The <a id="id536" class="indexterm"/>
<code class="literal">label_feats_from_corpus()</code> function in <code class="literal">featx.py</code> takes a corpus, such as <code class="literal">movie_reviews</code>, and a <code class="literal">feature_detector</code> function, which defaults to <code class="literal">bag_of_words</code>. It then constructs and returns a mapping of the form <code class="literal">{label: [featureset]}</code>. We can use this mapping to create a list of labeled <span class="emphasis"><em>training instances</em></span> and <span class="emphasis"><em>testing instances</em></span>. The reason to do it this way is because we can get a fair sample from each label.</p><div class="informalexample"><pre class="programlisting">import collections
def label_feats_from_corpus(corp, feature_detector=bag_of_words):
  label_feats = collections.defaultdict(list)
  for label in corp.categories():
    for fileid in corp.fileids(categories=[label]):
      feats = feature_detector(corp.words(fileids=[fileid]))
      label_feats[label].append(feats)
  return label_feats</pre></div><p>Once we can get a mapping of <code class="literal">label : feature</code> sets, we want to construct a list of labeled training instances and testing instances. The function <code class="literal">split_label_feats()</code> in <code class="literal">featx.py</code> takes a mapping returned from <code class="literal">label_feats_from_corpus()</code> and splits each list of feature sets into labeled training and testing instances.</p><div class="informalexample"><pre class="programlisting">def split_label_feats(lfeats, split=0.75):
  train_feats = []
  test_feats = []
  for label, feats in lfeats.iteritems():
    cutoff = int(len(feats) * split)
    train_feats.extend([(feat, label) for feat in feats[:cutoff]])
    test_feats.extend([(feat, label) for feat in feats[cutoff:]])
  return train_feats, test_feats</pre></div><a id="id537" class="indexterm"/><p>Using these functions with the <code class="literal">movie_reviews</code> corpus gives us the lists of labeled feature sets we need to train and test a classifier.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from nltk.corpus import movie_reviews
&gt;&gt;&gt; from featx import label_feats_from_corpus, split_label_feats
&gt;&gt;&gt; movie_reviews.categories()
['neg', 'pos']
&gt;&gt;&gt; lfeats = label_feats_from_corpus(movie_reviews)
&gt;&gt;&gt; lfeats.keys()
['neg', 'pos']
&gt;&gt;&gt; train_feats, test_feats = split_label_feats(lfeats)
&gt;&gt;&gt; len(train_feats)
1500
&gt;&gt;&gt; len(test_feats)
500</pre></div><p>So there are 1,000 <code class="literal">pos</code> files, 1,000 <code class="literal">neg</code> files, and we end up with 1,500 labeled training instances and 500 labeled testing instances, each composed of equal parts <code class="literal">pos</code> and <code class="literal">neg</code>. Now we can train a <code class="literal">NaiveBayesClassifier</code> using its <code class="literal">train()</code> class method,</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from nltk.classify import NaiveBayesClassifier
&gt;&gt;&gt; nb_classifier = NaiveBayesClassifier.train(train_feats)
&gt;&gt;&gt; nb_classifier.labels()
['neg', 'pos']</pre></div><p>Let's test the classifier on a couple of made up reviews. The <code class="literal">classify()</code> method takes a single argument, which should be a feature set. We can use the same <code class="literal">bag_of_words()</code> feature detector on a made up list of words to get our feature set.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from featx import bag_of_words
&gt;&gt;&gt; negfeat = bag_of_words(['the', 'plot', 'was', 'ludicrous'])
&gt;&gt;&gt; nb_classifier.classify(negfeat)
'neg'
&gt;&gt;&gt; posfeat = bag_of_words(['kate', 'winslet', 'is', 'accessible'])
&gt;&gt;&gt; nb_classifier.classify(posfeat)
'pos'</pre></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec263"/>How it works...</h2></div></div></div><a id="id538" class="indexterm"/><a id="id539" class="indexterm"/><a id="id540" class="indexterm"/><p>The<code class="literal"> label_feats_from_corpus()</code> assumes that the corpus is categorized, and that a single file represents a single instance for feature extraction. It iterates over each category label, and extracts features from each file in that category using the <a id="id541" class="indexterm"/>
<code class="literal">feature_detector()</code> function, which defaults to <code class="literal">bag_of_words()</code>. It returns a <code class="literal">dict</code> whose keys are the category labels, and the values are lists of instances for that category.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note22"/>Note</h3><p>If we had the <a id="id542" class="indexterm"/>
<code class="literal">label_feats_from_corpus()</code> function, return a list of labeled feature sets, instead of a dict, it would be much harder to get the balanced training data. The list would be ordered by label, and if you took a slice of it, you would almost certainly be getting far more of one label than another. By returning a <code class="literal">dict</code>, you can take slices from the feature sets of each label.</p></div></div><p>Now we need to split the labeled feature sets into training and testing instances using <code class="literal">split_label_feats()</code>
<a id="id543" class="indexterm"/>. This function allows us to take a fair sample of labeled feature sets from each label, using the <code class="literal">split</code> keyword argument to determine the size of the sample. <code class="literal">split</code> defaults to <code class="literal">0.75</code>, which means the first three-fourths of the labeled feature sets for each label will be used for training, and the remaining one-fourth will be used for testing.</p><p>Once we have split up our training and testing feats, we train a classifier using the <a id="id544" class="indexterm"/>
<code class="literal">NaiveBayesClassifier.train()</code> method. This class method builds two probability distributions for calculating prior probabilities. These are passed in to the <code class="literal">NaiveBayesClassifier</code> constructor. The <code class="literal">label_probdist</code> contains <code class="literal">P(label)</code>, the prior probability for each label. The <code class="literal">feature_probdist</code> contains <code class="literal">P(feature name = feature value | label)</code>. In our case, it will store <code class="literal">P(word=True | label)</code>. Both are calculated based on the frequency of occurrence of each label, and each feature name and value in the training data.</p><p>The <code class="literal">NaiveBayesClassifier</code> inherits from <code class="literal">ClassifierI</code>, which requires subclasses to provide a <code class="literal">labels()</code> method, and at least one of the <code class="literal">classify()</code> and <code class="literal">prob_classify()</code> methods. T<a id="id545" class="indexterm"/>
<a id="id546" class="indexterm"/>he following diagram shows these and other methods, which will be covered shortly:</p><div class="mediaobject"><img src="graphics/3609OS_07_01.jpg" alt="How it works..."/></div></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec264"/>There's more...</h2></div></div></div><p>We can test the accuracy of the classifier using <code class="literal">nltk.classify.util.accuracy()</code> and the <code class="literal">test_feats</code> created previously.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from nltk.classify.util import accuracy
&gt;&gt;&gt; accuracy(nb_classifier, test_feats)
0.72799999999999998</pre></div><p>This tells us that the classifier correctly guessed the label of nearly 73 percent of the testing feature sets.</p><div class="section" title="Classification probability"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec68"/>Classification probability</h3></div></div></div><a id="id547" class="indexterm"/><a id="id548" class="indexterm"/><p>While the <code class="literal">classify()</code> method returns only a single label, you can use the <code class="literal">prob_classify()</code> method<a id="id549" class="indexterm"/> to get the classification probability of each label. This can be useful if you want to use probability thresholds greater than 50 percent for classification.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; probs = nb_classifier.prob_classify(test_feats[0][0])
&gt;&gt;&gt; probs.samples()
['neg', 'pos']
&gt;&gt;&gt; probs.max()
'pos'
&gt;&gt;&gt; probs.prob('pos')
0.99999996464309127
&gt;&gt;&gt; probs.prob('neg')
3.5356889692409258e-08</pre></div><a id="id550" class="indexterm"/><a id="id551" class="indexterm"/><p>In this case, the classifier says that the first testing instance is nearly 100 percent likely to be <code class="literal">pos</code>.</p></div><div class="section" title="Most informative features"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec69"/>Most informative features</h3></div></div></div><p>The <code class="literal">NaiveBayesClassifier</code> has two methods that are quite useful for learning about your data. Both methods take a keyword argument <code class="literal">n</code> to control how many results to show. The <code class="literal">most_informative_features()</code>
<a id="id552" class="indexterm"/> method returns a list of the form <code class="literal">[(feature name, feature value)]</code> ordered by most informative to least informative. In our case, the feature value will always be <code class="literal">True</code>.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; nb_classifier.most_informative_features(n=5)
[('magnificent', True), ('outstanding', True), ('insulting', True), ('vulnerable', True), ('ludicrous', True)]</pre></div><p>The <code class="literal">show_most_informative_features()</code>
<a id="id553" class="indexterm"/> method will print out the results from <code class="literal">most_informative_features()</code> and will also include the probability of a feature pair belonging to each label.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; nb_classifier.show_most_informative_features(n=5)
Most Informative Features

    magnificent = True    pos : neg = 15.0 : 1.0

    outstanding = True    pos : neg = 13.6 : 1.0

    insulting = True      neg : pos = 13.0 : 1.0

    vulnerable = True     pos : neg = 12.3 : 1.0

    ludicrous = True      neg : pos = 11.8 : 1.0</pre></div><a id="id554" class="indexterm"/><p>The <span class="emphasis"><em>informativeness</em></span>, or <span class="strong"><strong>information gain</strong></span>, of each feature pair is based on the prior probability of the feature pair occurring for each label. More informative features are those that occur primarily in one label and not the other. Less informative features are those that occur frequently in both labels.</p></div><div class="section" title="Training estimator"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec70"/>Training estimator</h3></div></div></div><a id="id555" class="indexterm"/><p>During training, the <code class="literal">NaiveBayesClassifier</code> constructs its probability distributions using an <code class="literal">estimator</code> parameter, which defaults to <code class="literal">nltk.probability.ELEProbDist</code>. But you can use any <code class="literal">estimator</code> you want, and there are quite a few to choose from. The only constraints are that it must inherit from <code class="literal">nltk.probability.ProbDistI</code> and its constructor must take a <code class="literal">bins</code> keyword argument. Here's an example using the <code class="literal">LaplaceProdDist</code>:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from nltk.probability import LaplaceProbDist
&gt;&gt;&gt; nb_classifier = NaiveBayesClassifier.train(train_feats, estimator=LaplaceProbDist)
&gt;&gt;&gt; accuracy(nb_classifier, test_feats)
0.71599999999999997</pre></div><a id="id556" class="indexterm"/><p>As you can see, accuracy is slightly lower, so choose your <code class="literal">estimator</code> carefully.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note23"/>Note</h3><p>You cannot use <code class="literal">nltk.probability.MLEProbDist</code> as the estimator, or any <code class="literal">ProbDistI</code> subclass that does not take the <code class="literal">bins</code> keyword argument. Training will fail with <code class="literal">TypeError: __init__() got an unexpected keyword argument 'bins'</code>.</p></div></div></div><div class="section" title="Manual training"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec71"/>Manual training</h3></div></div></div><a id="id557" class="indexterm"/><p>You don't have to use the <code class="literal">train()</code> class method to construct a <code class="literal">NaiveBayesClassifier</code>. You can instead create the <code class="literal">label_probdist</code> and <code class="literal">feature_probdist</code> manually. <code class="literal">label_probdist</code> should be an instance of <code class="literal">ProbDistI</code>, and should contain the prior probabilities for each label. <code class="literal">feature_probdist</code> should be a <code class="literal">dict</code> whose keys are tuples of the form <code class="literal">(label, feature name)</code> and whose values are instances of <code class="literal">ProbDistI</code> that have the probabilities for each feature value. In our case, each <code class="literal">ProbDistI</code> should have only one value, <code class="literal">True=1</code>. Here's a very simple example using manually constructed <code class="literal">DictionaryProbDist</code>:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from nltk.probability import DictionaryProbDist
&gt;&gt;&gt; label_probdist = DictionaryProbDist({'pos': 0.5, 'neg': 0.5})
&gt;&gt;&gt; true_probdist = DictionaryProbDist({True: 1})
&gt;&gt;&gt; feature_probdist = {('pos', 'yes'): true_probdist, ('neg', 'no'): true_probdist}
&gt;&gt;&gt; classifier = NaiveBayesClassifier(label_probdist, feature_probdist)
&gt;&gt;&gt; classifier.classify({'yes': True})
'pos'
&gt;&gt;&gt; classifier.classify({'no': True})
'neg'</pre></div></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec265"/>See also</h2></div></div></div><p>In the next recipes, we will train two more classifiers, the <code class="literal">DecisionTreeClassifier</code>, and the <code class="literal">MaxentClassifier</code>. In the <span class="emphasis"><em>Measuring precision and recall of a classifier</em></span> recipe in this chapter, we will use precision and recall instead of accuracy to evaluate the classifiers. And then in the <span class="emphasis"><em>Calculating high information words</em></span> recipe, we will see how using only the most informative features can improve classifier performance.</p><p>The <code class="literal">movie_reviews</code> corpus is an instance of <code class="literal">CategorizedPlaintextCorpusReader</code>, which is covered in the <span class="emphasis"><em>Creating a categorized text corpus</em></span> recipe in <a class="link" href="ch03.html" title="Chapter 3. Creating Custom Corpora">Chapter 3</a>, <span class="emphasis"><em>Creating Custom Corpora</em></span>.</p></div></div>
<div class="section" title="Training a decision tree classifier"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec75"/>Training a decision tree classifier</h1></div></div></div><a id="id558" class="indexterm"/><p>The <code class="literal">DecisionTreeClassifier</code> works by creating a tree structure, where each node corresponds to a feature name, and the branches correspond to the feature values. Tracing down the branches, you get to the leaves of the tree, which are the classification labels.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec266"/>Getting ready</h2></div></div></div><a id="id559" class="indexterm"/><p>For the <code class="literal">DecisionTreeClassifier</code> to work for text classification, you must use NLTK 2.0b9 or later. This is because earlier versions are unable to deal with unknown features. If the <code class="literal">DecisionTreeClassifier</code> encountered a word/feature that it hadn't seen before, then it raised an exception. This bug has now been fixed by yours truly, and is included in all NLTK versions since 2.0b9.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec267"/>How to do it...</h2></div></div></div><p>Using the same <code class="literal">train_feats</code> and <code class="literal">test_feats</code> we created from the <code class="literal">movie_reviews</code> corpus in the previous recipe, we can call the <code class="literal">DecisionTreeClassifier.train()</code> class method to get a trained classifier. We pass <code class="literal">binary=True</code> because all of our features are binary: either the word is present or it's not. For other classification use cases where you have multi-valued features, you will want to stick to the default <code class="literal">binary=False</code>.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note24"/>Note</h3><p>In this context, <code class="literal">binary</code> refers to <span class="emphasis"><em>feature values</em></span>, and is not to be confused with a <span class="emphasis"><em>binary classifier</em></span>. Our word features are binary because the value is either <code class="literal">True</code>, or the word is not present. If our features could take more than two values, we would have to use <code class="literal">binary=False</code>. A <span class="emphasis"><em>binary classifier</em></span>, on the other hand, is a classifier that only chooses between two labels. In our case, we are training a binary <code class="literal">DecisionTreeClassifier</code> on binary features. But it's also possible to have a binary classifier with non-binary features, or a non-binary classifier with binary features.</p></div></div><p>Following is the code for training and evaluating the accuracy of a <code class="literal">DecisionTreeClassifier</code>:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from nltk.classify import DecisionTreeClassifier
&gt;&gt;&gt; dt_classifier = DecisionTreeClassifier.train(train_feats, binary=True, entropy_cutoff=0.8, depth_cutoff=5, support_cutoff=30)
&gt;&gt;&gt; accuracy(dt_classifier, test_feats)
0.68799999999999994</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip03"/>Tip</h3><a id="id560" class="indexterm"/><p>The <code class="literal">DecisionTreeClassifier</code> can take much longer to train than the <code class="literal">NaiveBayesClassifier</code>. For that reason, the default parameters have been overridden so it trains faster. These parameters will be explained later.</p></div></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec268"/>How it works...</h2></div></div></div><a id="id561" class="indexterm"/><p>The <code class="literal">DecisionTreeClassifier</code>, like the <code class="literal">NaiveBayesClassifier</code>, is also an instance of <code class="literal">ClassifierI</code>. During training, the <code class="literal">DecisionTreeClassifier</code> creates a tree where the child nodes are also instances of <code class="literal">DecisionTreeClassifier</code>. The leaf nodes contain only a single label, while the intermediate child nodes contain decision mappings for each feature. These decisions map each feature value to another <code class="literal">DecisionTreeClassifier</code>, which itself may contain decisions for another feature, or it may be a final leaf node with a classification label. The <code class="literal">train()</code> class method builds this tree from the ground up, starting with the leaf nodes. It then refines itself to minimize the number of decisions needed to get to a label by putting the most informative features at the top.</p><p>To classify, the <code class="literal">DecisionTreeClassifier</code> looks at the given feature set and traces down the tree, using known feature names and values to make decisions. Because we are creating a <span class="emphasis"><em>binary tree</em></span>, each <code class="literal">DecisionTreeClassifier</code> instance also has a <span class="emphasis"><em>default</em></span> decision tree, which it uses when a known feature is not present in the feature set being classified. This is a common occurrence in text-based feature sets, and indicates that a known word was not in the text being classified. This also contributes information towards a classification decision.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec269"/>There's more...</h2></div></div></div><p>The parameters passed in to <code class="literal">DecisionTreeClassifier.train()</code> can be tweaked to improve accuracy or decrease training time. Generally, if you want to improve accuracy, you must accept a longer training time and if you want to decrease the training time, the accuracy will most likely decrease as well.</p><div class="section" title="Entropy cutoff"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec72"/>Entropy cutoff</h3></div></div></div><a id="id562" class="indexterm"/><a id="id563" class="indexterm"/><p>The <code class="literal">entropy_cutoff</code> is used during the tree refinement process. If the entropy of the probability distribution of label choices in the tree is greater than the <code class="literal">entropy_cutoff</code>, then the tree is refined further. But if the entropy is lower than the <code class="literal">entropy_cutoff</code>, then tree refinement is halted.</p><a id="id564" class="indexterm"/><p>
<span class="strong"><strong>Entropy</strong></span> is the uncertainty of the outcome. As entropy approaches 1.0, uncertainty increases and, conversely, as entropy approaches 0.0, uncertainty decreases. In other words, when you have similar probabilities, the entropy will be high as each probability has a similar likelihood (or uncertainty of occurrence). But the more the probabilities differ, the lower the entropy will be.</p><a id="id565" class="indexterm"/><a id="id566" class="indexterm"/><a id="id567" class="indexterm"/><a id="id568" class="indexterm"/><p>Entropy is calculated by giving <code class="literal">nltk.probability.entropy()</code> a <code class="literal">MLEProbDist</code> created from a <code class="literal">FreqDist</code> of label counts. Here's an example showing the entropy of various <code class="literal">FreqDist</code> values:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from nltk.probability import FreqDist, MLEProbDist, entropy
&gt;&gt;&gt; fd = FreqDist({'pos': 30, 'neg': 10})
&gt;&gt;&gt; entropy(MLEProbDist(fd))
0.81127812445913283
&gt;&gt;&gt; fd['neg'] = 25
&gt;&gt;&gt; entropy(MLEProbDist(fd))
0.99403021147695647
&gt;&gt;&gt; fd['neg'] = 30
&gt;&gt;&gt; entropy(MLEProbDist(fd))
1.0
&gt;&gt;&gt; fd['neg'] = 1
&gt;&gt;&gt; entropy(MLEProbDist(fd))
0.20559250818508304</pre></div><p>What this all means is that if the label occurrence is very skewed one way or the other, the tree doesn't need to be refined because entropy/uncertainty is low. But when the entropy is greater than <code class="literal">entropy_cutoff</code> then the tree must be refined with further decisions to reduce the uncertainty. Higher values of <code class="literal">entropy_cutoff</code> will decrease both accuracy and training time.</p></div><div class="section" title="Depth cutoff"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec73"/>Depth cutoff</h3></div></div></div><a id="id569" class="indexterm"/><a id="id570" class="indexterm"/><p>The <code class="literal">depth_cutoff</code> is also used during refinement to control the depth of the tree. The final decision tree will never be deeper than the <code class="literal">depth_cutoff</code>. The default value is <code class="literal">100</code>, which means that classification may require up to 100 decisions before reaching a leaf node. Decreasing the <code class="literal">depth_cutoff</code> will decrease the training time and most likely decrease the accuracy as well.</p></div><div class="section" title="Support cutoff"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec74"/>Support cutoff</h3></div></div></div><a id="id571" class="indexterm"/><a id="id572" class="indexterm"/><p>The <code class="literal">support_</code>
<code class="literal">cutoff</code> controls how many labeled feature sets are required to refine the tree. As the <code class="literal">DecisionTreeClassifier</code> refines itself, labeled feature sets are eliminated once they no longer provide value to the training process. When the number of labeled feature sets is less than or equal to <code class="literal">support_cutoff</code>, refinement stops, at least for that section of the tree.</p><a id="id573" class="indexterm"/><a id="id574" class="indexterm"/><p>Another way to look at it is that <code class="literal">support_cutoff</code> specifies the minimum number of instances that are required to make a decision about a feature. If <code class="literal">support_cutoff</code> is <code class="literal">20</code>, and you have less than 20 labeled feature sets with a given feature, then you don't have enough instances to make a good decision, and refinement around that feature must come to a stop.</p></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec270"/>See also</h2></div></div></div><p>The previous recipe covered the creation of training and test feature sets from the <code class="literal">movie_reviews</code> corpus. In the next recipe, we will cover training a <code class="literal">MaxentClassifier</code>, and in the <span class="emphasis"><em>Measuring precision and recall of a classifier</em></span> recipe in this chapter, we will use precision and recall to evaluate all the classifiers.</p></div></div>
<div class="section" title="Training a maximum entropy classifier"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec76"/>Training a maximum entropy classifier</h1></div></div></div><a id="id575" class="indexterm"/><p>The third classifier which we will cover is the <a id="id576" class="indexterm"/>
<code class="literal">MaxentClassifier</code>, also known as a <span class="emphasis"><em>conditional exponential classifier</em></span>. The <span class="strong"><strong>maximum entropy classifier</strong></span> converts labeled feature sets to vectors using encoding. This encoded vector is then used to calculate <span class="emphasis"><em>weights</em></span> for each feature that can then be combined to determine the most likely label for a feature set.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec271"/>Getting ready</h2></div></div></div><a id="id577" class="indexterm"/><p>The <code class="literal">MaxentClassifier</code> requires the <code class="literal">numpy</code> package, and optionally the <code class="literal">scipy</code> package. This is because the feature encodings use <code class="literal">numpy</code> arrays. Having <code class="literal">scipy</code> installed also means you will be able to use faster algorithms that consume less memory. You can find installation for both at <a class="ulink" href="http://www.scipy.org/Installing_SciPy">http://www.scipy.org/Installing_SciPy</a>.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip04"/>Tip</h3><p>Many of the algorithms can be quite memory hungry, so you may want to quit all your other programs while training a <code class="literal">MaxentClassifier</code>, just to be safe.</p></div></div></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec272"/>How to do it...</h2></div></div></div><p>We will use the same <code class="literal">train_feats</code> and <code class="literal">test_feats</code> from the <code class="literal">movie_reviews</code> corpus that we constructed before, and call the <code class="literal">MaxentClassifier.train()</code> class method. Like the <code class="literal">DecisionTreeClassifier</code>, <code class="literal">MaxentClassifier.train()</code> has its own specific parameters that have been tweaked to speed up training. These parameters will be explained in more detail later.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from nltk.classify import MaxentClassifier
&gt;&gt;&gt; me_classifier = MaxentClassifier.train(train_feats, algorithm='iis', trace=0, max_iter=1, min_lldelta=0.5)
&gt;&gt;&gt; accuracy(me_classifier, test_feats)
0.5</pre></div><p>The reason this classifier has such a low accuracy is because the parameters have been set such that it is unable to learn a more accurate model. This is due to the time required to train a suitable model using the <code class="literal">iis</code> algorithm. Higher accuracy models can be learned much faster using the <code class="literal">scipy</code> algorithms.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip05"/>Tip</h3><p>If training is taking a long time, you can usually cut it off manually by hitting <span class="emphasis"><em>Ctrl + C</em></span>. This should stop the current iteration and still return a classifier based on whatever state the model is in.</p></div></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec273"/>How it works...</h2></div></div></div><a id="id578" class="indexterm"/><a id="id579" class="indexterm"/><p>Like the previous classifiers, <code class="literal">MaxentClassifier</code> inherits from <code class="literal">ClassifierI</code>. Depending on the algorithm, <code class="literal">MaxentClassifier.train()</code> calls one of the training functions in the <code class="literal">nltk.classify.maxent</code> module. If <code class="literal">scipy</code> is not installed, the default algorithm is <code class="literal">iis</code>, and the function used is <code class="literal">train_maxent_classifier_with_iis()</code>. The other algorithm that doesn't require <code class="literal">scipy</code> is <code class="literal">gis</code>, which uses the <code class="literal">train_maxent_classifier_with_gis()</code> function. <span class="strong"><strong>gis</strong></span> <a id="id580" class="indexterm"/>stands for <span class="strong"><strong>General Iterative Scaling</strong></span>, while <span class="strong"><strong>iis</strong></span> stands for <span class="strong"><strong>Improved Iterative Scaling</strong></span>. If <code class="literal">scipy</code> is installed, the <code class="literal">train_maxent_classifier_with_scipy()</code> function is used, and the default algorithm is <code class="literal">cg</code>. If <code class="literal">megam</code> is installed and you specify the <code class="literal">megam</code> algorithm, then <code class="literal">train_maxent_classifier_with_megam()</code> is<a id="id581" class="indexterm"/> used.</p><p>The basic idea behind the maximum entropy model is to build some probability distributions that fit the observed data, then choose whichever probability distribution has the highest entropy. The <code class="literal">gis</code> and <code class="literal">iis</code> algorithms do so by iteratively improving the weights used to classify features. This is where the <code class="literal">max_iter</code> and <code class="literal">min_lldelta</code> parameters come into play.</p><p>The <code class="literal">max_iter</code> specifies the maximum number of iterations to go through and update the weights. More iterations will generally improve accuracy, but only up to a point. Eventually, the changes from one iteration to the next will hit a plateau and further iterations are useless.</p><p>The <code class="literal">min_lldelta</code> specifies the minimum change in the <span class="emphasis"><em>log likelihood</em></span> required to continue iteratively improving the weights. Before beginning training iterations, an instance of the <code class="literal">nltk.classify.util.CutoffChecker</code> is created. When its <code class="literal">check()</code> method is called, it uses functions such as <code class="literal">nltk.classify.util.log_likelihood()</code> to decide whether the cutoff limits have been reached. <a id="id582" class="indexterm"/>The <span class="strong"><strong>log </strong></span>
<a id="id583" class="indexterm"/>
<a id="id584" class="indexterm"/>
<span class="strong"><strong>likelihood</strong></span> is the log (using <code class="literal">math.log()</code>) of the average label probability of the training data (which is the log of the average likelihood of a label). As the log likelihood increases, the model improves. But it too will reach a plateau where further increases are so small that there is no point in continuing. Specifying the <code class="literal">min_lldelta</code> allows you to control how much each iteration must increase the log likelihood before stopping iterations.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec274"/>There's more...</h2></div></div></div><p>Like the <code class="literal">NaiveBayesClassifier</code>, you can see the most informative features by calling <a id="id585" class="indexterm"/>the <code class="literal">show_most_informative_features()</code> method.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; me_classifier.show_most_informative_features(n=4)
-0.740 worst==True and label is 'pos'

0.740 worst==True and label is 'neg'

0.715 bad==True and label is 'neg'

-0.715 bad==True and label is 'pos'</pre></div><p>The numbers shown are the weights for each feature. This tells us that the word <span class="emphasis"><em>worst</em></span> is <span class="emphasis"><em>negatively weighted</em></span> towards the <code class="literal">pos</code> label, and <span class="emphasis"><em>positively weighted</em></span> towards the <code class="literal">neg</code> label. In other words, if the word <span class="emphasis"><em>worst</em></span> is found in the feature set, then there's a strong possibility that the text should be classified <code class="literal">neg</code>.</p><div class="section" title="Scipy algorithms"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec75"/>Scipy algorithms</h3></div></div></div><a id="id586" class="indexterm"/><p>The algorithms available when <code class="literal">scipy</code> is installed are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>CG</strong></span> (<span class="strong"><strong>Conjugate gradient</strong></span> algorithm<a id="id587" class="indexterm"/>)—the default <code class="literal">scipy</code> algorithm</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>BFGS</strong></span> (<span class="strong"><strong>Broyden-Fletcher-Goldfarb-Shanno</strong></span> algorithm<a id="id588" class="indexterm"/>)—very memory hungry</li><li class="listitem" style="list-style-type: disc">Powell<a id="id589" class="indexterm"/></li><li class="listitem" style="list-style-type: disc">LBFGSB (limited memory version of BFGS)<a id="id590" class="indexterm"/></li><li class="listitem" style="list-style-type: disc">Nelder-Mead<a id="id591" class="indexterm"/></li></ul></div><p>Here's what happens when you use the CG algorithm:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; me_classifier = MaxentClassifier.train(train_feats, algorithm='cg', trace=0, max_iter=10)
&gt;&gt;&gt; accuracy(me_classifier, test_feats)
0.85599999999999998</pre></div><p>This is the most accurate classifier so far.</p></div><div class="section" title="Megam algorithm"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec76"/>Megam algorithm</h3></div></div></div><a id="id592" class="indexterm"/><p>If you have installed the <code class="literal">megam</code> package, then you can use the <code class="literal">megam</code> algorithm. It's a bit faster than the <code class="literal">scipy</code> algorithms and about as accurate. Installation instructions and information can be found at <a class="ulink" href="http://www.cs.utah.edu/~hal/megam/">http://www.cs.utah.edu/~hal/megam/</a>. The function <code class="literal">nltk.classify.megam.config_megam()</code> can be used to specify where the <code class="literal">megam</code> executable is found. Or, if <code class="literal">megam</code> can be found in the standard executable paths, NLTK will configure it automatically.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; me_classifier = MaxentClassifier.train(train_feats, algorithm='megam', trace=0, max_iter=10)
[Found megam: /usr/local/bin/megam]
&gt;&gt;&gt; accuracy(me_classifier, test_feats)
0.86799999999999999</pre></div><a id="id593" class="indexterm"/><p>The <code class="literal">megam</code> algorithm is highly recommended for its accuracy and speed of training.</p></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec275"/>See also</h2></div></div></div><p>The <span class="emphasis"><em>Bag of Words feature extraction</em></span> and the <span class="emphasis"><em>Training a naive Bayes classifier</em></span> recipes in this chapter show how to construct the training and testing features from the <code class="literal">movie_reviews</code> corpus. In the next recipe, we will cover how and why to evaluate a classifier using precision and recall instead of accuracy.</p></div></div>
<div class="section" title="Measuring precision and recall of a classifier"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec77"/>Measuring precision and recall of a classifier</h1></div></div></div><a id="id594" class="indexterm"/><p>In addition to accuracy, there are a number of other metrics used to evaluate classifiers. Two of the most common are <span class="emphasis"><em>precision</em></span> and <span class="emphasis"><em>recall</em></span>. To understand these two metrics, we must first understand <span class="emphasis"><em>false positives</em></span> and <span class="emphasis"><em>false negatives</em></span>. <a id="id595" class="indexterm"/>
<a id="id596" class="indexterm"/>
<span class="strong"><strong>False positives</strong></span> happen when a classifier classifies a feature set with a label it shouldn't have. <span class="strong"><strong>False negatives</strong></span> happen when a classifier doesn't assign a label to a feature set that should have it. In a <span class="emphasis"><em>binary classifier</em></span>, these errors happen at the same time.</p><p>Here's an example: the classifier classifies a movie review as <code class="literal">pos</code>, when it should have been <code class="literal">neg</code>. This counts as a <span class="emphasis"><em>false positive</em></span> for the <code class="literal">pos</code> label, and a <span class="emphasis"><em>false negative</em></span> for the <code class="literal">neg</code> label. If the classifier had correctly guessed <code class="literal">neg</code>, then it would count as a <a id="id597" class="indexterm"/>
<a id="id598" class="indexterm"/>
<span class="strong"><strong>true positive</strong></span> for the <code class="literal">neg</code> label, and a <span class="strong"><strong>true negative</strong></span> for the <code class="literal">pos</code> label.</p><p>How does this apply to precision and recall? <a id="id599" class="indexterm"/>
<span class="strong"><strong>Precision</strong></span> is the <span class="emphasis"><em>lack of false positives</em></span>, and <a id="id600" class="indexterm"/>
<span class="strong"><strong>recall</strong></span> is the <span class="emphasis"><em>lack of false negatives</em></span>. As you will see, these two metrics are often in competition: the more precise a classifier is, the lower the recall, and vice versa.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec276"/>How to do it...</h2></div></div></div><a id="id601" class="indexterm"/><p>Let's calculate the precision and recall of the <code class="literal">NaiveBayesClassifier</code> we trained in the <span class="emphasis"><em>Training a naive Bayes classifier</em></span> recipe. The <code class="literal">precision_recall()</code> function in <code class="literal">classification.py</code> looks like this:</p><div class="informalexample"><pre class="programlisting">import collections
from nltk import metrics

def precision_recall(classifier, testfeats):
  refsets = collections.defaultdict(set)
  testsets = collections.defaultdict(set)

  for i, (feats, label) in enumerate(testfeats):
    refsets[label].add(i)
    observed = classifier.classify(feats)
    testsets[observed].add(i)

  precisions = {}
  recalls = {}

  for label in classifier.labels():
    precisions[label] = metrics.precision(refsets[label], testsets[label])
    recalls[label] = metrics.recall(refsets[label], testsets[label])

  return precisions, recalls</pre></div><p>This function takes two arguments:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">The trained classifier.</li><li class="listitem">Labeled test features, also known as a gold standard.</li></ol></div><p>These are the same arguments you pass to the <a id="id602" class="indexterm"/>
<code class="literal">accuracy()</code> function. The <a id="id603" class="indexterm"/>
<code class="literal">precision_recall()</code> returns two dictionaries; the first holds the precision for each label, and the second holds the recall for each label. Here's an example usage with the <code class="literal">nb_classifier</code> and the <code class="literal">test_feats</code> we created in the <span class="emphasis"><em>Training a naive Bayes classifier</em></span> recipe earlier:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from classification import precision_recall
&gt;&gt;&gt; nb_precisions, nb_recalls = precision_recall(nb_classifier, test_feats)
&gt;&gt;&gt; nb_precisions['pos']
0.6413612565445026
&gt;&gt;&gt; nb_precisions['neg']
0.9576271186440678
&gt;&gt;&gt; nb_recalls['pos']
0.97999999999999998
&gt;&gt;&gt; nb_recalls['neg']
0.45200000000000001</pre></div><p>This tells us that while the <code class="literal">NaiveBayesClassifier</code> can correctly identify most of the <code class="literal">pos</code> feature sets (high recall), it also classifies many of the <code class="literal">neg</code> feature sets as <code class="literal">pos</code> (low precision). This behavior contributes to the high precision but low recall for the <code class="literal">neg</code> label—as the <code class="literal">neg</code> label isn't given often (low recall), and when it is, it's very likely to be correct (high precision). The conclusion could be that there are certain common words that are biased towards the <code class="literal">pos</code> label, but occur frequently enough in the <code class="literal">neg</code> feature sets to cause mis-classifications. To correct this behavior, we will use only the most informative words in the next recipe, <span class="emphasis"><em>Calculating high information words</em></span>.</p></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec277"/>How it works...</h2></div></div></div><a id="id604" class="indexterm"/><p>To calculate precision and recall, we must build two sets for each label. The first set is known as the <span class="strong"><strong>reference set</strong></span>, and contains all the correct values. The second set is called the <span class="strong"><strong>test set</strong></span>
<a id="id605" class="indexterm"/>, and contains the values guessed by the classifier. These two sets are compared to calculate the precision or recall for each label.</p><a id="id606" class="indexterm"/><p>
<span class="strong"><strong>Precision</strong></span> is defined as the size of the intersection of both sets divided by the size of the test set. In other words, the percentage of the test set that was guessed correctly. In Python, the code is <code class="literal">float(len(reference.intersection(test))) / len(test)</code>.</p><a id="id607" class="indexterm"/><p>
<span class="strong"><strong>Recall</strong></span> is the size of the intersection of both sets divided by the size of the reference set, or the percentage of the reference set that was guessed correctly. The Python code is <code class="literal">float(len(reference.intersection(test))) / len(reference)</code>.</p><a id="id608" class="indexterm"/><p>The <code class="literal">precision_recall()</code> function in <code class="literal">classification.py</code> iterates over the labeled test features and classifies each one. We store the <span class="emphasis"><em>numeric index</em></span> of the feature set (starting with <code class="literal">0</code>) in the reference set for the known training label, and also store the index in the test set for the guessed label. If the classifier guesses <code class="literal">pos</code> but the training label is <code class="literal">neg</code>, then the index is stored in the <span class="emphasis"><em>reference set</em></span> for <code class="literal">neg</code> and the <span class="emphasis"><em>test set</em></span> for <code class="literal">pos</code>.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note25"/>Note</h3><p>We use the numeric index because the feature sets aren't hashable, and we need a unique value for each feature set.</p></div></div><p>The <code class="literal">nltk.metrics</code> package contains functions for calculating both precision and recall, so all we really have to do is build the sets, then call the appropriate function.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec278"/>There's more...</h2></div></div></div><p>Let's try it with the <code class="literal">MaxentClassifier</code> we trained in the previous recipe:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; me_precisions, me_recalls = precision_recall(me_classifier, test_feats)
&gt;&gt;&gt; me_precisions['pos']
0.8801652892561983
&gt;&gt;&gt; me_precisions['neg']
0.85658914728682167
&gt;&gt;&gt; me_recalls['pos']
0.85199999999999998
&gt;&gt;&gt; me_recalls['neg']
0.88400000000000001</pre></div><p>This classifier is much more well-rounded than the <code class="literal">NaiveBayesClassifier</code>. In this case, the label bias is much less significant, and the reason is that the <code class="literal">MaxentClassifier</code> weighs its features according to its own internal model. Words that are more significant are those that occur primarily in a single label, and will get higher weights in the model. Words that are common to both labels will get lower weights, as they are less significant.</p><div class="section" title="F-measure"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec77"/>F-measure</h3></div></div></div><a id="id609" class="indexterm"/><p>The <span class="strong"><strong>F-measure</strong></span> is defined as the weighted harmonic mean of precision and recall. If <code class="literal">p</code> is the <span class="emphasis"><em>precision</em></span>, and <code class="literal">r</code> is the <span class="emphasis"><em>recall</em></span>, the formula is:</p><div class="informalexample"><pre class="programlisting">1/(alpha/p + (1-alpha)/r)</pre></div><p>where <code class="literal">alpha</code> is a weighing constant that defaults to <code class="literal">0.5</code>. You can use <code class="literal">nltk.metrics.f_measure()</code> to get the F-measure. It takes the same arguments as for the <code class="literal">precision()</code> and <code class="literal">recall()</code> functions: a reference set and a test set. It's often used instead of accuracy to measure a classifier. However, precision and recall are found to be much more useful metrics, as the F-measure can hide the kinds of imbalances we saw with the <code class="literal">NaiveBayesClassifier</code>.</p></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec279"/>See also</h2></div></div></div><p>In the <span class="emphasis"><em>Training a naive Bayes classifier</em></span> recipe, we collected training and testing feature sets, and trained the <code class="literal">NaiveBayesClassifier</code>. The <code class="literal">MaxentClassifier</code> was trained in the <span class="emphasis"><em>Training a maximum entropy classifier</em></span> recipe. In the next recipe, we will explore eliminating the less significant words, and use only the high information words to create our feature sets.</p></div></div>
<div class="section" title="Calculating high information words"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec78"/>Calculating high information words</h1></div></div></div><a id="id610" class="indexterm"/><p>A <span class="strong"><strong>high information word</strong></span> is a word that is strongly biased towards a single classification label. These are the kinds of words we saw when we called the <a id="id611" class="indexterm"/>
<code class="literal">show_most_informative_features()</code> method on both the <code class="literal">NaiveBayesClassifier</code> and the <code class="literal">MaxentClassifier</code>. Somewhat surprisingly, the top words are different for both classifiers. This discrepancy is due to how each classifier calculates the significance of each feature, and it's actually beneficial to have these different methods as they can be combined to improve accuracy, as we will see in the next recipe, <span class="emphasis"><em>Combining classifiers with voting</em></span>.</p><a id="id612" class="indexterm"/><p>The <span class="strong"><strong>low information words</strong></span> are words that are common to all labels. It may be counter-intuitive, but eliminating these words from the training data can actually improve accuracy, precision, and recall. The reason this works is that using only high information words reduces the noise and confusion of a classifier's internal model. If all the words/features are highly biased one way or the other, it's much easier for the classifier to make a correct guess.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec280"/>How to do it...</h2></div></div></div><p>First, we need to calculate the high information words in the <code class="literal">movie_review</code> corpus. We can do this using the <code class="literal">high_information_words()</code> function in <code class="literal">featx.py</code>:</p><div class="informalexample"><pre class="programlisting">from nltk.metrics import BigramAssocMeasures
from nltk.probability import FreqDist, ConditionalFreqDist

def high_information_words(labelled_words, score_fn=BigramAssocMeasures.chi_sq, min_score=5):
  word_fd = FreqDist()
  label_word_fd = ConditionalFreqDist()

  for label, words in labelled_words:
    for word in words:
      word_fd.inc(word)
      label_word_fd[label].inc(word)

  n_xx = label_word_fd.N()
  high_info_words = set()

  for label in label_word_fd.conditions():
    n_xi = label_word_fd[label].N()
    word_scores = collections.defaultdict(int)

    for word, n_ii in label_word_fd[label].iteritems():
      n_ix = word_fd[word]
      score = score_fn(n_ii, (n_ix, n_xi), n_xx)
      word_scores[word] = score

    bestwords = [word for word, score in word_scores.iteritems() if score &gt;= min_score]
    high_info_words |= set(bestwords)

  return high_info_words</pre></div><a id="id613" class="indexterm"/><p>It takes one argument , which is a list of 2-tuples of the form <code class="literal">[(label, words)]</code> where <code class="literal">label</code> is the classification label, and <code class="literal">words</code> is a list of words that occur under that label. It returns a list of the high information words, sorted from most informative to least informative.</p><p>Once we have the high information words, we use the feature detector function <code class="literal">bag_of_words_in_set()</code>, also found in <code class="literal">featx.py</code>, which will let us filter out all low information words.</p><div class="informalexample"><pre class="programlisting">def bag_of_words_in_set(words, goodwords):
  return bag_of_words(set(words) &amp; set(goodwords))</pre></div><p>With this new feature detector, we can call <code class="literal">label_feats_from_corpus()</code> and get a new <code class="literal">train_feats</code> and <code class="literal">test_feats</code> using <code class="literal">split_label_feats()</code>. These two functions were covered in the <span class="emphasis"><em>Training a naive Bayes classifier</em></span> recipe in this chapter.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from featx import high_information_words, bag_of_words_in_set
&gt;&gt;&gt; labels = movie_reviews.categories()
&gt;&gt;&gt; labeled_words = [(l, movie_reviews.words(categories=[l])) for l in labels]
&gt;&gt;&gt; high_info_words = set(high_information_words(labeled_words))
&gt;&gt;&gt; feat_det = lambda words: bag_of_words_in_set(words, high_info_words)
&gt;&gt;&gt; lfeats = label_feats_from_corpus(movie_reviews, feature_detector=feat_det)
&gt;&gt;&gt; train_feats, test_feats = split_label_feats(lfeats)</pre></div><p>Now that we have new training and testing feature sets, let's train and evaluate a <code class="literal">NaiveBayesClassifier</code>:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; nb_classifier = NaiveBayesClassifier.train(train_feats)
&gt;&gt;&gt; accuracy(nb_classifier, test_feats)
0.91000000000000003
&gt;&gt;&gt; nb_precisions, nb_recalls = precision_recall(nb_classifier, test_feats)
&gt;&gt;&gt; nb_precisions['pos']
0.89883268482490275
&gt;&gt;&gt; nb_precisions['neg']
0.92181069958847739
&gt;&gt;&gt; nb_recalls['pos']
0.92400000000000004
&gt;&gt;&gt; nb_recalls['neg']
0.89600000000000002</pre></div><p>While the <code class="literal">neg</code> precision and <code class="literal">pos</code> recall have both decreased somewhat, <code class="literal">neg</code> recall and <code class="literal">pos</code> precision have increased drastically. Accuracy is now a little higher than the <code class="literal">MaxentClassifier</code>.</p></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec281"/>How it works...</h2></div></div></div><a id="id614" class="indexterm"/><a id="id615" class="indexterm"/><p>The <code class="literal">high_information_words()</code> function starts by counting the frequency of every word, as well as the conditional frequency for each word within each label. This is why we need the words to be labelled, so we know how often each word occurs in each label.</p><p>Once we have this <code class="literal">FreqDist</code> and <code class="literal">ConditionalFreqDist</code>, we can score each word on a per-label basis. The default <code class="literal">score_fn</code> is <code class="literal">nltk.metrics.BigramAssocMeasures.chi_sq()</code>, which calculates the chi-square score for each word using the following parameters:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><code class="literal">n_ii</code>: The frequency of the word in the label.</li><li class="listitem"><code class="literal">n_ix</code>: The total frequency of the word across all labels.</li><li class="listitem"><code class="literal">n_xi</code>: The total frequency of all words that occurred in the label.</li><li class="listitem"><code class="literal">n_xx</code>: The total frequency for all words in all labels.</li></ol></div><p>The simplest way to think about these numbers is that the closer <code class="literal">n_ii</code> is to <code class="literal">n_ix</code>, the higher the score. Or, the more often a word occurs in a label, relative to its overall occurrence, the higher the score.</p><p>Once we have the scores for each word in each label, we can filter out all words whose score is below the <code class="literal">min_score</code> threshold. We keep the words that meet or exceed the threshold, and return all high scoring words in each label.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip06"/>Tip</h3><p>It is recommended to experiment with different values of <code class="literal">min_score</code> to see what happens. In some cases, less words may improve the metrics even more, while in other cases more words is better.</p></div></div></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec282"/>There's more...</h2></div></div></div><p>There are a number of other scoring functions available in the <code class="literal">BigramAssocMeasures</code> class, such as <code class="literal">phi_sq()</code> for phi-square, <code class="literal">pmi()</code> for pointwise mutual information, and <code class="literal">jaccard()</code> for using the Jaccard index. They all take the same arguments, and so can be used interchangeably with <code class="literal">chi_sq()</code>.</p><div class="section" title="MaxentClassifier with high information words"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec78"/>MaxentClassifier with high information words</h3></div></div></div><a id="id616" class="indexterm"/><p>Let's evaluate the <code class="literal">MaxentClassifier</code> using the high information words feature sets:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; me_classifier = MaxentClassifier.train(train_feats, algorithm='megam', trace=0, max_iter=10)
&gt;&gt;&gt; accuracy(me_classifier, test_feats)
0.88200000000000001
&gt;&gt;&gt; me_precisions, me_recalls = precision_recall(me_classifier, test_feats)
&gt;&gt;&gt; me_precisions['pos']
0.88663967611336036
&gt;&gt;&gt; me_precisions['neg']
0.87747035573122534
&gt;&gt;&gt; me_recalls['pos']
0.876
&gt;&gt;&gt; me_recalls['neg']
0.88800000000000001</pre></div><p>As you can see, the improvements are much more modest than with the <code class="literal">NaiveBayesClassifier</code> due to the fact that the <code class="literal">MaxentClassifier</code> already weights all features by significance. But using only the high information words still makes a positive difference compared to when we used all the words. And the precisions and recalls for each label are closer to each other, giving the <code class="literal">MaxentClassifier</code> even more well-rounded performance.</p></div><div class="section" title="DecisionTreeClassifier with high information words"><div class="titlepage"><div><div><h3 class="title"><a id="ch07lvl3sec79"/>DecisionTreeClassifier with high information words</h3></div></div></div><a id="id617" class="indexterm"/><p>Now, let's evaluate the <code class="literal">DecisionTreeClassifier</code>:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; dt_classifier = DecisionTreeClassifier.train(train_feats, binary=True, depth_cutoff=20, support_cutoff=20, entropy_cutoff=0.01)
&gt;&gt;&gt; accuracy(dt_classifier, test_feats)
0.68600000000000005
&gt;&gt;&gt; dt_precisions, dt_recalls = precision_recall(dt_classifier, test_feats)
&gt;&gt;&gt; dt_precisions['pos']
0.6741573033707865
&gt;&gt;&gt; dt_precisions['neg']
0.69957081545064381
&gt;&gt;&gt; dt_recalls['pos']
0.71999999999999997
&gt;&gt;&gt; dt_recalls['neg']
0.65200000000000002</pre></div><a id="id618" class="indexterm"/><p>The accuracy is about the same, even with a larger <code class="literal">depth_cutoff</code>, and smaller <code class="literal">support_cutoff</code> and <code class="literal">entropy_cutoff</code>. The results show that the <code class="literal">DecisionTreeClassifier</code> was already putting the high information features at the top of the tree, and it will only improve if we increase the depth significantly. But that could make training time prohibitively long.</p></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec283"/>See also</h2></div></div></div><p>We started this chapter with the <span class="emphasis"><em>Bag of Words feature extraction</em></span> recipe. The <code class="literal">NaiveBayesClassifier</code> was originally trained in the <span class="emphasis"><em>Training a naive Bayes classifier</em></span> recipe, and the <code class="literal">MaxentClassifier</code> was trained in the <span class="emphasis"><em>Training a maximum entropy classifier</em></span> recipe. Details on precision and recall can be found in the <span class="emphasis"><em>Measuring precision and recall of a classifier </em></span>recipe. We will be using only high information words in the next two recipes, where we combine classifiers.</p></div></div>
<div class="section" title="Combining classifiers with voting"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec79"/>Combining classifiers with voting</h1></div></div></div><a id="id619" class="indexterm"/><a id="id620" class="indexterm"/><p>One way to improve classification performance is to combine classifiers. The simplest way to combine multiple classifiers is to use voting, and choose whichever label gets the most votes. For this style of voting, it's best to have an odd number of classifiers so that there are no ties. This means combining at least three classifiers together. The individual classifiers should also use different algorithms; the idea is that multiple algorithms are better than one, and the combination of many can compensate for individual bias.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec284"/>Getting ready</h2></div></div></div><p>As we need to have at least three trained classifiers to combine, we are going to use a <code class="literal">NaiveBayesClassifier</code>, a <code class="literal">DecisionTreeClassifier</code>, and a <code class="literal">MaxentClassifier</code>, all trained on the highest information words of the <code class="literal">movie_reviews</code> corpus. These were all trained in the previous recipe, so we will combine these three classifiers with voting.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec285"/>How to do it...</h2></div></div></div><p>In the <code class="literal">classification.py</code> module, there is a <code class="literal">MaxVoteClassifier</code> class.</p><div class="informalexample"><pre class="programlisting">import itertools
from nltk.classify import ClassifierI
from nltk.probability import FreqDist

class MaxVoteClassifier(ClassifierI):
  def __init__(self, *classifiers):
    self._classifiers = classifiers
    self._labels = sorted(set(itertools.chain(*[c.labels() for c in classifiers])))

  def labels(self):
    return self._labels

  def classify(self, feats):
    counts = FreqDist()

    for classifier in self._classifiers:
      counts.inc(classifier.classify(feats))

    return counts.max()</pre></div><a id="id621" class="indexterm"/><a id="id622" class="indexterm"/><p>To create it, you pass in a list of classifiers that you want to combine. Once created, it works just like any other classifier. Though it may take about three times longer to classify, it should generally be at least as accurate as any individual classifier.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from classification import MaxVoteClassifier
&gt;&gt;&gt; mv_classifier = MaxVoteClassifier(nb_classifier, dt_classifier, me_classifier)
&gt;&gt;&gt; mv_classifier.labels()
['neg', 'pos']
&gt;&gt;&gt; accuracy(mv_classifier, test_feats)
0.89600000000000002
&gt;&gt;&gt; mv_precisions, mv_recalls = precision_recall(mv_classifier, test_feats)
&gt;&gt;&gt; mv_precisions['pos']
0.8928571428571429
&gt;&gt;&gt; mv_precisions['neg']
0.89919354838709675
&gt;&gt;&gt; mv_recalls['pos']
0.90000000000000002
&gt;&gt;&gt; mv_recalls['neg']
0.89200000000000002</pre></div><p>These metrics are about on par with the <code class="literal">MaxentClassifier</code> and <code class="literal">NaiveBayesClassifier</code>. Some numbers are slightly better, some worse. It's likely that a significant improvement to the <code class="literal">DecisionTreeClassifier</code> could produce some better numbers.</p></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec286"/>How it works...</h2></div></div></div><a id="id623" class="indexterm"/><p>The <code class="literal">MaxVoteClassifier</code> extends the <code class="literal">nltk.classify.ClassifierI</code> interface, which requires implementing at least two methods:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <code class="literal">labels()</code> function must return a list of possible labels. This will be the union of the <code class="literal">labels()</code> of each classifier passed in at initialization.</li><li class="listitem" style="list-style-type: disc">The <code class="literal">classify()</code> function takes a single feature set and returns a label. The <code class="literal">MaxVoteClassifier</code> iterates over its classifiers and calls <code class="literal">classify()</code> on each of them, recording their label as a vote in a <code class="literal">FreqDist</code>. The label with the most votes is returned using <code class="literal">FreqDist.max()</code>.</li></ul></div><p>While it doesn't check for this, the <code class="literal">MaxVoteClassifier</code> assumes that all the classifiers passed in at initialization use the same labels. Breaking this assumption may lead to odd behavior.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec287"/>See also</h2></div></div></div><p>In the previous recipe, we trained a <code class="literal">NaiveBayesClassifier</code>, a <code class="literal">MaxentClassifier</code>, and a <code class="literal">DecisionTreeClassifier</code> using only the highest information words. In the next recipe, we will use the <code class="literal">reuters</code> corpus and combine many binary classifiers in order to create a multi-label classifier.</p></div></div>
<div class="section" title="Classifying with multiple binary classifiers"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec80"/>Classifying with multiple binary classifiers</h1></div></div></div><p>So far we have focused on <span class="strong"><strong>binary classifiers</strong></span>, which classify with <span class="emphasis"><em>one of two possible labels</em></span>. The same techniques for training a binary classifier can also be used to create a <span class="emphasis"><em>multi-class</em></span> classifier, which is a classifier that can classify with <span class="emphasis"><em>one of many possible labels</em></span>. But there are also cases where you need to be able to classify with <span class="emphasis"><em>multiple labels</em></span>. A classifier that can return more than one label is a <span class="strong"><strong>multi-label classifier</strong></span>.</p><a id="id624" class="indexterm"/><a id="id625" class="indexterm"/><p>A common technique for creating a multi-label classifier is to combine many binary classifiers, one for each label. You train each binary classifier so that it either returns a known label, or returns something else to signal that the label does not apply. Then you can run all the binary classifiers on your feature set to collect all the applicable labels.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec288"/>Getting ready</h2></div></div></div><a id="id626" class="indexterm"/><p>The <code class="literal">reuters</code> corpus contains multi-labeled text that we can use for training and evaluation.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from nltk.corpus import reuters
&gt;&gt;&gt; len(reuters.categories())
90</pre></div><p>We will train one binary classifier per label, which means we will end up with 90 binary classifiers.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec289"/>How to do it...</h2></div></div></div><a id="id627" class="indexterm"/><p>First, we should calculate the high information words in the <code class="literal">reuters</code> corpus. This is done with the <code class="literal">reuters_high_info_words()</code> function in <code class="literal">featx.py</code>.</p><div class="informalexample"><pre class="programlisting">from nltk.corpus import reuters

def reuters_high_info_words(score_fn=BigramAssocMeasures.chi_sq):
  labeled_words = []

  for label in reuters.categories():
    labeled_words.append((label, reuters.words(categories=[label])))

  return high_information_words(labeled_words, score_fn=score_fn)</pre></div><a id="id628" class="indexterm"/><p>Then we need to get training and test feature sets based on those high information words. This is done with the <code class="literal">reuters_train_test_feats()</code>, also found in <code class="literal">featx.py</code>. It defaults to using <code class="literal">bag_of_words()</code> as its <code class="literal">feature_detector</code>, but we will be overriding this using <code class="literal">bag_of_words_in_set()</code> to use only the high information words.</p><div class="informalexample"><pre class="programlisting">def reuters_train_test_feats(feature_detector=bag_of_words):
  train_feats = []
  test_feats = []

  for fileid in reuters.fileids():
    if fileid.startswith('training'):
      featlist = train_feats
    else: # fileid.startswith('test')
      featlist = test_feats

    feats = feature_detector(reuters.words(fileid))
    labels = reuters.categories(fileid)
    featlist.append((feats, labels))

  return train_feats, test_feats</pre></div><p>We can use these two functions to get a list of multi-labeled training and testing feature sets.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from featx import reuters_high_info_words, reuters_train_test_feats
&gt;&gt;&gt; rwords = reuters_high_info_words()
&gt;&gt;&gt; featdet = lambda words: bag_of_words_in_set(words, rwords)
&gt;&gt;&gt; multi_train_feats, multi_test_feats = reuters_train_test_feats(featdet)</pre></div><a id="id629" class="indexterm"/><p>The <code class="literal">multi_train_feats</code> and <code class="literal">multi_test_feats</code> are multi-labeled feature sets. That means they have a list of labels, instead of a single label, and they look like the <code class="literal">[(featureset, [label])]</code>, as each feature set can have one or more labels. With this training data, we can train multiple binary classifiers. The <code class="literal">train_binary_classifiers()</code> function in the <code class="literal">classification.py</code> takes a training function, a list of multi-label feature sets, and a set of possible labels to return a <code class="literal">dict</code> of the <code class="literal">label : binary</code> classifier.</p><div class="informalexample"><pre class="programlisting">def train_binary_classifiers(trainf, labelled_feats, labelset):
  pos_feats = collections.defaultdict(list)
  neg_feats = collections.defaultdict(list)
  classifiers = {}

  for feat, labels in labelled_feats:
    for label in labels:
      pos_feats[label].append(feat)

    for label in labelset - set(labels):
      neg_feats[label].append(feat)

  for label in labelset:
    postrain = [(feat, label) for feat in pos_feats[label]]
    negtrain = [(feat, '!%s' % label) for feat in neg_feats[label]]
    classifiers[label] = trainf(postrain + negtrain)

  return classifiers</pre></div><p>To use this function, we need to provide a training function that takes a single argument, which is the training data. This will be a simple <code class="literal">lambda</code> wrapper around the <code class="literal">MaxentClassifier.train()</code>, so we can specify extra keyword arguments.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from classification import train_binary_classifiers
&gt;&gt;&gt; trainf = lambda train_feats: MaxentClassifier.train(train_feats, algorithm='megam', trace=0, max_iter=10)
&gt;&gt;&gt; labelset = set(reuters.categories())
&gt;&gt;&gt; classifiers = train_binary_classifiers(trainf, multi_train_feats, labelset)
&gt;&gt;&gt; len(classifiers)
90</pre></div><p>Now we can define a <code class="literal">MultiBinaryClassifier</code>, which takes a list of labeled classifiers of the form <code class="literal">[(label, classifier)]</code> where the <code class="literal">classifier</code> is assumed to be a binary classifier that either returns the <code class="literal">label</code>, or something else if the label doesn't apply.</p><div class="informalexample"><pre class="programlisting">from nltk.classify import MultiClassifierI

class MultiBinaryClassifier(MultiClassifierI):
  def __init__(self, *label_classifiers):
    self._label_classifiers = dict(label_classifiers)
    self._labels = sorted(self._label_classifiers.keys())

  def labels(self):
    return self._labels

  def classify(self, feats):
    lbls = set()

    for label, classifier in self._label_classifiers.iteritems():
      if classifier.classify(feats) == label:
        lbls.add(label)

    return lbls</pre></div><p>We can construct this class using the binary classifiers we just created.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from classification import MultiBinaryClassifier
&gt;&gt;&gt; multi_classifier = MultiBinaryClassifier(*classifiers.items())</pre></div><a id="id630" class="indexterm"/><p>To evaluate this classifier, we can use precision and recall, but not accuracy. That's because the accuracy function assumes single values, and doesn't take into account partial matches. For example, if the multi-classifier returns three labels for a feature set, and two of them are correct but the third is not, then the <code class="literal">accuracy()</code> would mark that as incorrect. <a id="id631" class="indexterm"/>So instead of using accuracy, we will use the <span class="strong"><strong>masi distance</strong></span>, which measures partial overlap between two sets. The lower the masi distance, the better the match. A lower average masi distance, therefore, means more accurate partial matches. The <code class="literal">multi_metrics()</code> function in the <code class="literal">classification.py</code> calculates the precision and recall of each label, along with the average masi distance.</p><div class="informalexample"><pre class="programlisting">import collections
from nltk import metrics

def multi_metrics(multi_classifier, test_feats):
  mds = []
  refsets = collections.defaultdict(set)
  testsets = collections.defaultdict(set)

  for i, (feat, labels) in enumerate(test_feats):
    for label in labels:
      refsets[label].add(i)

    guessed = multi_classifier.classify(feat)

    for label in guessed:
      testsets[label].add(i)

    mds.append(metrics.masi_distance(set(labels), guessed))

  avg_md = sum(mds) / float(len(mds))
  precisions = {}
  recalls = {}

  for label in multi_classifier.labels():
    precisions[label] = metrics.precision(refsets[label], testsets[label])
    recalls[label] = metrics.recall(refsets[label], testsets[label])

  return precisions, recalls, avg_md</pre></div><p>Using this with the <code class="literal">multi_classifier</code> we just created, gives us the following results:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from classification import multi_metrics
&gt;&gt;&gt; multi_precisions, multi_recalls, avg_md = multi_metrics(multi_classifier, multi_test_feats)
&gt;&gt;&gt; avg_md
0.18191264129488705</pre></div><p>So our average masi distance is fairly low, which means our multi-label classifier is usually mostly accurate. Let's take a look at a few precisions and recalls:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; multi_precisions['zinc']
1.0
&gt;&gt;&gt; multi_recalls['zinc']
0.84615384615384615
&gt;&gt;&gt; len(reuters.fileids(categories=['zinc']))
34
&gt;&gt;&gt; multi_precisions['sunseed']
0.5
&gt;&gt;&gt; multi_recalls['sunseed']
0.20000000000000001
&gt;&gt;&gt; len(reuters.fileids(categories=['sunseed']))
16
&gt;&gt;&gt; multi_precisions['rand']
None
&gt;&gt;&gt; multi_recalls['rand']
0.0
&gt;&gt;&gt; len(reuters.fileids(categories=['rand']))
3</pre></div><p>As you can see, there's quite a range of values. But, in general, the labels that have more feature sets will have higher precision and recall, and those with less feature sets will have lower performance. When there's not a lot of feature sets for a classifier to learn from, you can't expect it to perform well.</p></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec290"/>How it works...</h2></div></div></div><a id="id632" class="indexterm"/><p>The <code class="literal">reuters_high_info_words()</code> function is fairly simple; it constructs a list of <code class="literal">[(label, words)]</code> for each category of the <code class="literal">reuters</code> corpus, then passes it in to the <code class="literal">high_information_words()</code> function to return a list of the most informative words in the <code class="literal">reuters</code> corpus.</p><p>With the resulting set of words, we create a feature detector function using the <code class="literal">bag_of_words_in_set()</code>. This is then passed in to the <code class="literal">reuters_train_test_feats()</code>, which returns two lists, the first containing <code class="literal">[(feats, labels)]</code> for all the training files, and the second list has the same for all the test files.</p><p>Next, we train a binary classifier for each label using <code class="literal">train_binary_classifiers()</code>. This function constructs two lists for each label, one containing positive training feature sets, the other containing negative training feature sets. The <a id="id633" class="indexterm"/>
<a id="id634" class="indexterm"/>
<span class="strong"><strong>Positive feature sets</strong></span> are those feature sets that classify for the label. The <span class="strong"><strong>Negative feature sets</strong></span> for a label comes from the positive feature sets for all other labels. For example, a feature set that is <span class="emphasis"><em>positive</em></span> for <code class="literal">zinc</code> and <code class="literal">sunseed</code> is a <span class="emphasis"><em>negative</em></span> example for all the other 88 labels. Once we have positive and negative feature sets for each label, we can train a binary classifier for each label using the given training function.</p><p>With the resulting dictionary of binary classifiers, we create an instance of the <code class="literal">MultiBinaryClassifier</code>. This class extends the <code class="literal">nltk.classify.MultiClassifierI</code> interface, which requires at least two functions:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">The <code class="literal">labels()</code> function must return a list of possible labels.</li><li class="listitem">The <code class="literal">classify()</code> function takes a single feature set and returns a <code class="literal">set</code> of labels. To create this <code class="literal">set</code>, we iterate over the binary classifiers, and any time a call to the <code class="literal">classify()</code> returns its label, we add it to the set. If it returns something else, we continue.</li></ol></div><p>Finally, we evaluate the multi-label classifier using the <code class="literal">multi_metrics()</code> function. It is similar to the <code class="literal">precision_recall()</code> function from the <span class="emphasis"><em>Measuring precision and recall of a classifier</em></span> recipe, but in this case we know the classifier is an instance of the <code class="literal">MultiClassifierI</code> and it can therefore return multiple labels. It also keeps track of the masi distance for each set of classification labels using the <code class="literal">nltk.metrics.masi_</code>
<code class="literal">distance()</code>. The <code class="literal">multi_metrics()</code> function returns three values:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">A dictionary of precisions for each label.</li><li class="listitem">A dictionary of recalls for each label.</li><li class="listitem">The average masi distance for each feature set.</li></ol></div></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec291"/>There's more...</h2></div></div></div><a id="id635" class="indexterm"/><p>The nature of the <code class="literal">reuters</code> corpus introduces the <span class="strong"><strong>class-imbalance problem</strong></span>. This problem occurs when some labels have very few feature sets, and other labels have many. The binary classifiers that have few positive instances to train on end up with far more negative instances, and are therefore strongly biased towards the negative label. There's nothing inherently wrong about this, as the bias reflects the data, but the negative instances can overwhelm the classifier to the point where it's nearly impossible to get a positive result. There are a number of advanced techniques for overcoming this problem, but they are out of the scope of this book.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec292"/>See also</h2></div></div></div><p>The <code class="literal">MaxentClassifier</code> is covered in the <span class="emphasis"><em>Training a maximum entropy classifier</em></span> recipe in this chapter. The <span class="emphasis"><em>Measuring precision and recall of a classifier</em></span> recipe shows how to evaluate a classifier, while the <span class="emphasis"><em>Calculating high information words</em></span> recipe describes how to use only the best features.</p></div></div></body></html>