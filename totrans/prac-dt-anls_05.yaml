- en: Creating Your First pandas DataFrame
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 创建您的第一个 pandas DataFrame
- en: In this chapter, we will go through the core data analysis skills of using filesystems
    and formats. We will explore different file formats for text data using the Python
    OS and string libraries to manipulate textual and numerical data from source files,
    such as **Comma-Separated Values** (**CSV**), **Extensible Markup Language** (**XML**),
    and **JavaScript Object Notation **(**JSON**). You will learn what a pandas DataFrame
    is and how to create DataFrames from file sources for data analysis.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨使用文件系统和格式进行核心数据分析技能。我们将使用 Python OS 和字符串库探索用于操作来自源文件（如 **逗号分隔值**（**CSV**）、**可扩展标记语言**（**XML**）和
    **JavaScript 对象表示法**（**JSON**））的文本和数值数据的不同文件格式。您将了解 pandas DataFrame 是什么以及如何从文件源创建
    DataFrame 进行数据分析。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章中将涵盖以下主题：
- en: Techniques for manipulating tabular data
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作表格数据的技术
- en: Understanding pandas and DataFrames
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 pandas 和 DataFrame
- en: Handling essential data formats
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理基本数据格式
- en: Data dictionaries and data types
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据字典和数据类型
- en: Creating your first DataFrame
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建您的第一个 DataFrame
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: Here's the GitHub repository for this book: [https://github.com/PacktPublishing/Practical-Data-Analysis-using-Jupyter-Notebook/tree/master/Chapter04](https://github.com/PacktPublishing/Practical-Data-Analysis-using-Jupyter-Notebook/tree/master/Chapter04).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这是本书的 GitHub 仓库链接：[https://github.com/PacktPublishing/Practical-Data-Analysis-using-Jupyter-Notebook/tree/master/Chapter04](https://github.com/PacktPublishing/Practical-Data-Analysis-using-Jupyter-Notebook/tree/master/Chapter04)。
- en: You can download and install the required software from the following link: [https://www.anaconda.com/distribution/](https://www.anaconda.com/distribution/)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从以下链接下载并安装所需的软件：[https://www.anaconda.com/distribution/](https://www.anaconda.com/distribution/)
- en: Techniques for manipulating tabular data
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 操作表格数据的技术
- en: Now that we have a better understanding of array data structures from using
    the NumPy library in [Chapter 3](dd40977f-7c89-4933-944f-d9760d3ca217.xhtml), *Getting
    Started with NumPy*, we can now expand our data analysis expertise. We will do
    this by working with tabular data and focusing on a powerful library available
    in Python named `pandas`, which is available to use in our Jupyter notebooks.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经通过在 [第 3 章](dd40977f-7c89-4933-944f-d9760d3ca217.xhtml) “使用 NumPy 入门”
    中使用 NumPy 库更好地理解了数组数据结构，我们可以现在扩展我们的数据分析专业知识。我们将通过处理表格数据并专注于 Python 中可用的强大库 `pandas`
    来做到这一点，该库可用于我们的 Jupyter 笔记本。
- en: The `pandas` library extends our ability to analyze structured data and was
    introduced as a Python library back in 2008 by Wes McKinney. McKinney recognized
    the power of extending the Python language by using libraries and the need to
    fill the gap that existed between data preparation and data insights by *carrying
    out the entire data analysis workflow in Python without having to switch to a
    more domain-specific language such as R*.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '`pandas` 库扩展了我们分析结构化数据的能力，并于 2008 年由 Wes McKinney 作为 Python 库引入。McKinney 认识到通过使用库来扩展
    Python 语言的力量，以及通过在 Python 中执行整个数据分析工作流程（无需切换到更特定领域的语言，如 R）来填补数据准备与数据洞察之间存在的差距的必要性。'
- en: The `pandas` Python library name was taken from the term **panel data** (by McKinney)
    by shortening and combining the terms to get `pan` and `da`. Panel data is defined as
    observations that can be measured over a period of time with multiple dimensional
    values and is very common in statistical studies and research papers. I have also
    seen panel data referred to as longitudinal data, facts panel data, or cross-sectional
    time-series data. Panel data is presented in tabular form with rows and columns
    and comes in a few different types, such as balanced, unbalanced, long, short,
    fixed, and rotating.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '`pandas` Python 库的名称取自术语 **panel data**（由 McKinney 提出），通过缩短和合并术语得到 `pan` 和
    `da`。面板数据定义为可以在一段时间内用多个维度的值进行测量的观测值，在统计研究和研究论文中非常常见。我也见过面板数据被称为纵向数据、事实面板数据或横截面时间序列数据。面板数据以表格形式呈现，有行和列，并且有几种不同类型，如平衡的、不平衡的、长的、短的、固定的和旋转的。'
- en: Each of these panel data types are based on how precisely the quantity of the
    dataset is represented. The total number of observations (rows of data) is commonly
    identified using the letter `N`. The unit of the time element used, such as year,
    month, quarter, or date is typically identified using the letter `T` in either
    upper or lowercase. The dimensions or variables (columns of data) can be represented
    with specific letters for each entity, such as `x` or `z`. What is measured can
    be represented as one or more variables and is commonly assigned to `y`. You should
    be able to summarize any panel data as a representative sample in descriptive
    terms for the consumer to understand before viewing or working with it in tabular
    form.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the dataset, the dimensions may or may not change over time, so
    different letters, such as `k`, may be assigned to represent that distinction.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of panel data is a daily closing price over 3 days for three publicly
    traded companies, as in the following table:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '| **Date** | **Stock Ticker** | **Closing Price** |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
- en: '| 12/2/2019 | ABC | $50.21 |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
- en: '| 12/3/2019 | ABC | $52.22 |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
- en: '| 12/4/2019 | ABC | $51.01 |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
- en: '| 12/2/2019 | DEF | $24.22 |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
- en: '| 12/3/2019 | DEF | $26.22 |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
- en: '| 12/4/2019 | DEF | $29.50 |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
- en: '| 12/2/2019 | GHI | $61.22 |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
- en: '| 12/3/2019 | GHI | $65.33 |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
- en: '| 12/4/2019 | GHI | $75.00 |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
- en: 'Another example is the minimum, maximum, and average temperatures, in Fahrenheit,
    by ZIP code for the last three months (by month), as in the following table:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '| **Month** | **ZIP Code** | **Minimum Temperature** | **Maximum Temperature**
    | **Average Temperature** |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
- en: '| June | 19901 | 75 | 88 | 82 |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
- en: '| July | 19901 | 77 | 90 | 84 |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
- en: '| August | 19901 | 68 | 85 | 77 |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
- en: '| June | 08618 | 76 | 89 | 83 |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
- en: '| July | 08618 | 78 | 91 | 85 |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
- en: '| August | 08618 | 69 | 86 | 78 |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
- en: '| June | 18940 | 74 | 87 | 81 |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
- en: '| July | 18940 | 76 | 89 | 83 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
- en: '| August | 18940 | 67 | 84 | 76 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
- en: An unbalanced panel would be one where one or more values are missing for any
    one of the dimensional values. A balanced panel would be an inclusive dataset
    where all the elements from each dimension are included across all periods of
    time.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: We know from [Chapter 1](0fa7e28f-7a30-4099-9bae-30dd3c86ee4f.xhtml), *Fundamentals
    of Data Analysis*, that data comes in all shapes and sizes, so having data structured
    in tabular form will be the first step in the analysis process, but in many cases,
    not the final one. For example, in the following table, we have a summary pivot
    table of total sales by city over the last 3 years.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: 'This summary data can be identified as a cross table, which makes it easier
    for the consumer of this data to quickly identify the highest and lowest sales
    by City and by Year. In this case, this would be New York in 2019 with $120,000
    and Boston in 2017 with $25,000:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '| **City** | **2017** | **2018** | **2019** |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
- en: '| Philadelphia | $50,000 | $75,000 | $100,000 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
- en: '| New York | $35,000 | $65,000 | $120,000 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
- en: '| Boston | $25,000 | $40,000 | $ 90,000 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
- en: If this tabular form of data had a limited number of rows and columns, this
    would be the final step in your analysis because you can quickly answer most business
    questions without additional manipulation, such as which city has the highest
    sales. However, what if the number of records increased to display over 100 cities
    and we increased the number of years to the last 10? What if you wanted to get
    more details to better understand the sales, breaking down the amount by increasing
    the number of dimensions, such as the product, store number, date of the transaction,
    time of the day, and method of payment?
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这个数据表只有有限行和列，这将是你的分析的最终步骤，因为你可以在不进行额外操作的情况下快速回答大多数商业问题，例如哪个城市的销售额最高。然而，如果记录数量增加到显示超过100个城市，我们将年份增加到最近10年，或者你想要获取更多细节以更好地理解销售额，通过增加维度（如产品、店铺编号、交易日期、一天中的时间和支付方式）来细分金额，会怎样呢？
- en: Increasing the number of columns would make it challenging and time-consuming
    to answer a simple business question, such as what is the average sales across
    all cities across all years? Therefore, the ability to scale your analysis is
    dependent on your ability to manipulate the data beyond how the source is received.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 增加列的数量将使回答简单的商业问题变得具有挑战性和耗时，例如，所有城市所有年份的平均销售额是多少？因此，分析的可扩展性取决于你操纵数据的能力，而不仅仅是接收数据的方式。
- en: 'A best practice in data analysis is *to begin with the end in mind*. So, for
    this example, the output table we want to produce will look similar to the following
    table, where we have transposed the columns into rows to make it easier for additional
    analysis to be carried out and so that we are prepared to handle a larger volume
    of data:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析的最佳实践是*从结果出发开始分析*。因此，对于这个例子，我们想要生成的输出表将类似于以下表格，我们将列转置为行，以便更容易进行额外分析，并且我们准备好处理更大的数据量：
- en: A large scale of data volume is a subjective term but the techniques used should
    support analyzing millions or billions of rows of data. You will need additional
    infrastructure beyond the limits of your personal workstation's available RAM
    and CPU.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模的数据量是一个主观术语，但所使用的技术应该支持分析数百万或数十亿行数据。你将需要超出个人工作站可用RAM和CPU限制的额外基础设施。
- en: '| **City** | **Year** | **Sales** |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| **城市** | **年份** | **销售额** |'
- en: '| Philadelphia | 2017 | $50,000 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 费城 | 2017 | $50,000 |'
- en: '|  | 2018 | $75,000 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | 2018 | $75,000 |'
- en: '|  | 2019 | $100,000 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|  | 2019 | $100,000 |'
- en: '| New York | 2017 | $35,000 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 纽约 | 2017 | $35,000 |'
- en: '|  | 2018 | $65,000 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|  | 2018 | $65,000 |'
- en: '|  | 2019 | $120,000 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | 2019 | $120,000 |'
- en: '| Boston | 2017 | $25,000 |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 波士顿 | 2017 | $25,000 |'
- en: '|  | 2018 | $40,000 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | 2018 | $40,000 |'
- en: '|  | 2019 | $90,000 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | 2019 | $90,000 |'
- en: 'From the preceding output, we can see that:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的输出中，我们可以看到：
- en: The first advantage of having data structured similar to the way it is in the
    preceding output table is that there is a single conformed data type for each
    column, which is also known as a dimension or axis.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据结构类似于前面输出表的第一大优势是，每个列都有一个单一的数据类型，这通常被称为维度或轴。
- en: The second advantage is that it becomes much easier for statistical analysis
    to be carried out because each dimension can be treated as an independent array
    of values of the same data type where calculations can be performed using NumPy,
    as covered in [Chapter 3](dd40977f-7c89-4933-944f-d9760d3ca217.xhtml), *Getting
    Started with NumPy*.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个优势是，由于每个维度都可以被视为具有相同数据类型的独立值数组，因此统计分析变得更加容易，可以使用NumPy进行计算，如第3章所述，*NumPy入门*。
- en: The third advantage is the ability to sort by any field in the table without
    worrying about the data values in each row/tuple becoming misaligned or inconsistent.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三个优势是能够按表中的任何字段排序，而不用担心每行/元组中的数据值变得错位或不一致。
- en: Keeping the integrity of your data builds trust in your process and ensures
    your analysis will be accurate.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保持数据的完整性可以建立对分析过程的信任，并确保你的分析将是准确的。
- en: It is recommended that you break down each step during the manipulation of data
    to allow the repeatability of the process—for example, if you were asked to go
    through the process after a few months had passed or if you had to troubleshoot
    anomalies that exist in the underlining source data, which is very common.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 建议你在数据操作过程中分解每个步骤，以便使过程可重复——例如，如果你被要求在几个月后重复这个过程，或者如果你必须调试底层源数据中存在的异常，这是非常常见的。
- en: Understanding pandas and DataFrames
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解pandas和DataFrames
- en: Now that we have a better understanding of tabular data and we have provided
    some background about panel data and the origins of why the `pandas` library was
    created, let's dive into some examples using `pandas` and explain how DataFrames
    are used.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们对表格数据有了更好的理解，我们也提供了一些关于面板数据以及 `pandas` 库创建原因的背景信息，那么让我们通过一些 `pandas` 的例子来深入了解，并解释
    DataFrame 的使用方法。
- en: 'The `pandas` library is a powerful Python library used for changing and analyzing
    data. A `pandas` DataFrame is a feature available in the library and is defined
    as a two-dimensional, size-mutable, potentially heterogeneous tabular data structure
    with labeled axes (rows and columns). A DataFrame is a two-dimensional data structure—that
    is, data is aligned in a tabular fashion in rows and columns. It is commonly known
    that `pandas` DataFrame consists of three principal components: the data, the
    rows, and the columns. Being a visual learner myself, I created an example of
    this with the following diagram, which we can go through now:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`pandas` 库是一个强大的 Python 库，用于更改和分析数据。`pandas` DataFrame 是库中的一个功能，定义为具有标记轴（行和列）的二维、大小可变的、可能异构的表格数据结构。DataFrame
    是一个二维数据结构——也就是说，数据以表格形式按行和列对齐。众所周知，`pandas` DataFrame 由三个主要组件组成：数据、行和列。作为一个视觉学习者，我创建了一个以下图表的例子，我们现在可以一起看看：'
- en: '![](img/0a2ad74d-0276-47cd-a0e4-a28c7ee5d006.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0a2ad74d-0276-47cd-a0e4-a28c7ee5d006.png)'
- en: DataFrames can be compared to a spreadsheet, such as Microsoft Excel or Google
    Sheets, a single database SQL table found in any **Relational Database Management
    System** (**RDBMS**), or even a **QlikView Data** (**QVD**) file. The previous
    examples all include a common element of a **header row** that defines a label
    and alignment for your data, **rows** (with each one identified as a single record),
    **columns** that categorize the structure of each field value, and **data** that
    contains numeric and/or text values.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame 可以与电子表格进行比较，例如 Microsoft Excel 或 Google Sheets，任何 **关系数据库管理系统**（**RDBMS**）中找到的单个
    SQL 表，或者甚至是一个 **QlikView 数据**（**QVD**）文件。之前的例子都包含一个 **标题行** 的共同元素，它定义了数据的标签和对齐方式，**行**（每行被识别为一个单独的记录），**列**
    对每个字段值的结构进行分类，以及包含数值和/或文本值的 **数据**。
- en: 'In our example, each row includes an identity record of the `ID` field, but
    that is not a requirement in a `pandas` DataFrame. DataFrames are treated as objects in
    Python and support loading data from files or SQL sources directly into memory
    for additional manipulation and analysis. Some key benefits of using DataFrames
    include the following:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，每一行都包含一个 `ID` 字段的身份记录，但这在 `pandas` DataFrame 中不是必需的。DataFrame 在 Python
    中被视为对象，并支持直接将数据从文件或 SQL 源加载到内存中，以便进行额外的操作和分析。使用 DataFrame 的一些关键好处包括以下内容：
- en: It allows you to convert all source files into readable data objects for easier
    merging and analysis.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它允许您将所有源文件转换为可读的数据对象，以便更容易地进行合并和分析。
- en: It provides auto- or defined indexing to help with looking up a value or selecting
    a cross selection from your DataFrame, which is also known as a data slice.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它提供自动或定义的索引，以帮助查找值或从 DataFrame 中选择交叉选择，这通常也称为数据切片。
- en: Each column can be treated as a single NumPy array, which can collectively have
    multiple data types.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每一列都可以被视为一个单独的 NumPy 数组，这些数组可以具有多种数据类型。
- en: It really excels at fixing data alignment and missing data elements, which are
    displayed and referenced as **Not a Number **(**NaN**).
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它在修复数据对齐和缺失数据元素方面表现卓越，这些元素显示和引用为 **非数字**（**NaN**）。
- en: It allows pivoting and reshaping data without going back to the source of record
    for each dataset.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它允许在不返回每个数据集的记录源的情况下进行数据旋转和重塑。
- en: It is easy to add, remove, or change data using single Python commands to expedite
    the analysis of one or more data sources.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用单个 Python 命令添加、删除或更改数据很容易，这样可以加快对一个或多个数据源的分析。
- en: Allows aggregations, such as `Group By`, and other calculations against metrics,
    such as `sum`, `min`, `max`, can all be performed against the DataFrame.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许对指标进行聚合，如 `Group By`，以及执行 `sum`、`min`、`max` 等其他计算。
- en: Allows merging, sorting, joining, and filtering against one or more DataFrames.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许对一个或多个 DataFrame 进行合并、排序、连接和过滤。
- en: 'It is scalable to support a repeatable workflow of analysis. For example, the
    following pseudo-code steps are easy to replicate in a Jupyter notebook:'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以扩展以支持可重复的工作流程分析。例如，以下伪代码步骤在 Jupyter 笔记本中很容易复制：
- en: Import the `pandas` library.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `pandas` 库。
- en: Load the source file into a new DataFrame.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将源文件加载到新的 DataFrame 中。
- en: Create a second DataFrame that drops duplicate rows or columns from the original.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个第二个DataFrame，它会从原始DataFrame中删除重复的行或列。
- en: Create summary metrics.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建汇总指标。
- en: Save the second DataFrame as a new file.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将第二个DataFrame保存为新的文件。
- en: 'What I enjoy about using `pandas` and DataFrames is the flexibility of the
    built-in commands that are provided to you as a data analyst. Let''s walk through
    a few examples. To create a DataFrame from scratch on a limited number of records,
    you can simply add them with a few commands:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我喜欢使用`pandas`和DataFrame的原因是，作为数据分析师，内置命令的灵活性。让我们通过几个例子来了解一下。要从头开始创建一个包含有限记录的DataFrame，你可以简单地使用几个命令来添加它们：
- en: 'To load `pandas`, you just need to add the following command to your Jupyter
    notebook and run the cell. Feel free to follow along by creating your own notebook;
    I have added a copy to GitHub for reference:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要加载`pandas`，你只需将以下命令添加到你的Jupyter笔记本中并运行该单元格。你可以自由地跟随创建自己的笔记本；我已经在GitHub上添加了一个副本以供参考：
- en: '[PRE0]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we have a few products—`a`, `b`, and `c`—along with the quantity sold,
    and we assign this input data to a variable named `product_data`:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们有几个产品——`a`、`b`和`c`——以及销售数量，我们将这些输入数据分配给一个名为`product_data`的变量：
- en: '[PRE1]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: When loading data manually, note the placement of square brackets to encapsulate
    the values for each column label and how the arrays must all be of the same length.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 当手动加载数据时，请注意方括号的位置，用于封装每个列标签的值，以及数组必须具有相同的长度。
- en: 'Then, we want to load the DataFrame by calling the command using the `pd` shortcut to
    reference the `pandas` library, along with the `DataFrame()` command. We assign
    the DataFrame input data as a second variable for easier reference, called `purchase_data`.
    The `In[]` cell will look like this:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们想要通过调用命令并使用`pd`快捷方式来引用`pandas`库以及`DataFrame()`命令来加载数据框。我们将DataFrame输入数据作为第二个变量分配，以便更容易引用，称为`purchase_data`。`In[]`单元格将如下所示：
- en: '[PRE2]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To validate the results, you can run the `head()` function to display the first
    five rows of data using the following command:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了验证结果，你可以运行`head()`函数来显示前五行数据，使用以下命令：
- en: '[PRE3]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'After executing the preceding code, we can see that:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 执行前面的代码后，我们可以看到：
- en: The output would look as in the following screenshot, where the individual arrays'
    by-products have been converted into a DataFrame with a labeled header row, and
    each of the quantity sold values are aligned for easy reference.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出结果将类似于以下截图，其中各个数组的副产品已被转换为一个带有标签标题行的DataFrame，并且每个销售数量值都进行了对齐，以便于参考。
- en: 'Notice that a new index column has been created to the left of `product a`
    with assigned sequential values, starting at `0`:'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意，在`product a`左侧创建了一个新的索引列，并分配了从`0`开始的顺序值：
- en: '![](img/95c7af78-b9e4-401d-9cdb-3377f17f493d.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/95c7af78-b9e4-401d-9cdb-3377f17f493d.png)'
- en: 'Having the indexed values are useful for reference, but if we want to define
    them as we create the DataFrame, we can include a relevant command during its
    creation, as follows:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拥有索引值对于参考很有用，但如果我们希望在创建DataFrame时定义它们，我们可以在创建过程中包含一个相关命令，如下所示：
- en: '[PRE4]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, if you run the `head()` command to display the results, you will see specific
    values assigned to each indexed number, which will display as in the following
    screenshot:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，如果你运行`head()`命令来显示结果，你将看到每个索引号分配了特定的值，其显示方式如下所示：
- en: '![](img/02f922c7-2ff0-412a-aa5a-c4878054b2d2.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/02f922c7-2ff0-412a-aa5a-c4878054b2d2.png)'
- en: 'To select a specific row from the DataFrame, you use the `loc` function to
    retrieve the results by index, as follows:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要从DataFrame中选择特定的行，你使用`loc`函数通过索引检索结果，如下所示：
- en: '[PRE5]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This will have an output, as in the following screenshot, where the individual
    values from the row assigned to `Ronny` are displayed in summary format, with
    each column and value presented by a row with a final description that includes
    the name of the index with a data type of the values (`dtype:  int64`):'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '这将产生一个输出，如下面的截图所示，其中显示分配给`Ronny`的行的各个值以汇总格式呈现，每列和值都由一个包含索引名称和值的数据类型描述的行表示（`dtype:
    int64`）：'
- en: '![](img/9559a898-4514-4abf-876c-ecac8cb6257e.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9559a898-4514-4abf-876c-ecac8cb6257e.png)'
- en: Once the index is labeled, you must access the `loc[]` function with the name
    in single quotes; however, you can use the `iloc[]` or `ix[]` functions to reference
    the row index by a number, with the first row starting with `0`. So, `purchase_data.iloc[0]` or
    `purchase_data.ix[0]` will both return the same results as in the preceding screenshot.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦索引被标记，你必须使用单引号中的名称来访问`loc[]`函数；然而，你可以使用`iloc[]`或`ix[]`函数通过数字引用行索引，第一行从`0`开始。因此，`purchase_data.iloc[0]`或`purchase_data.ix[0]`都将返回与前面截图相同的结果。
- en: Handling essential data formats
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理基本数据格式
- en: With a better understanding of the power of using the `pandas` library and the
    DataFrames feature, let's explore working with multiple data formats, including
    from source files such as CSV, JSON, and XML. We briefly covered these different
    file formats as part of understanding structured data in [Chapter 1](0fa7e28f-7a30-4099-9bae-30dd3c86ee4f.xhtml),
    *Fundamentals of Data Analysis*, so let's dive deep into each source file type
    and learn some essential skills when working with them.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 通过更好地理解使用`pandas`库和数据框（DataFrames）功能的力量，让我们来探索处理多种数据格式，包括来自CSV、JSON和XML等源文件。我们在[第1章](0fa7e28f-7a30-4099-9bae-30dd3c86ee4f.xhtml)，“数据分析基础”中简要介绍了这些不同的文件格式作为理解结构化数据的一部分，因此让我们深入探讨每种源文件类型，并学习一些处理它们时的基本技能。
- en: CSV
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CSV
- en: First, we have CSV, which has been an industry standard for most of my career.
    The way to identify CSV files is typically by the `.csv` file extension; however,
    you will learn, over time, that this is not always the case, nor is the delimiter
    used to separate values always a comma within data records. CSV files are popular
    because they are portable and technologically agnostic from the source system
    that created them.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们有CSV，这在我的职业生涯的大部分时间里一直是行业标准。通常通过`.csv`文件扩展名来识别CSV文件；然而，随着时间的推移，你会发现这并不总是如此，用于分隔数据记录中值的分隔符也不一定是逗号。CSV文件之所以受欢迎，是因为它们具有可移植性，并且从创建它们的源系统中技术上是中立的。
- en: This means a CSV file could have been created with any coding language, such
    as Python, C++, or Java. Also, the same OS used to create the CSV file, such as
    Windows, Unix, Linux, or macOS, is not required to read the file. This has helped
    with its adoption for many different use cases by IT professionals because it
    helps move data between systems in and out of the organization as needed.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着CSV文件可能是由任何编程语言创建的，例如Python、C++或Java。同样，创建CSV文件的相同操作系统，如Windows、Unix、Linux或macOS，也不需要读取文件。这有助于IT专业人士在需要时在组织内外移动数据，从而帮助其被广泛采用。
- en: Because of its longevity, you will find that many different variations and standards
    have been adopted over the years. For example, records may or may not include
    a header row and the delimiter between each field/column could be a tab, a pipe
    (`|`), or any other ASCII or UTF-8 character value.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其持久性，你会发现多年来已经采用了许多不同的变体和标准。例如，记录可能包含或不包含标题行，并且字段/列之间的分隔符可以是制表符、管道（`|`）或任何其他ASCII或UTF-8字符值。
- en: '**American Standard Code for Information Interchange** (**ASCII**) is a common
    character-encoding standard used by computers to interpret keyboard values digitally. **Unicode
    Transformation Format** (**UTF-8**) is the universal character-encoding standard
    and is backward-compatible with ASCII. Both standards are popular and commonly
    used.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '**美国信息交换标准代码**（**ASCII**）是计算机用来以数字方式解释键盘值的一种常见字符编码标准。**Unicode转换格式**（**UTF-8**）是通用的字符编码标准，与ASCII向后兼容。这两个标准都很受欢迎，并且被广泛使用。'
- en: 'There are some rules defined for proper CSV format, but the more you continue
    to work with them, the more you will probably find exceptions. Some of the rules
    that were published by Y. Shafranovich in *The Internet Society* (2005) include
    the following:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为正确的CSV格式定义了一些规则，但随着你继续使用它们，你可能会发现更多例外。由Y. Shafranovich在2005年发表的《互联网协会》中发布的部分规则如下：
- en: Each record in the CSV file should be independent and include a line break (`CRLF`)
    to identify each row.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CSV文件中的每条记录应该是独立的，并包含一个换行符（`CRLF`）来标识每一行。
- en: It is optional to have a line break with the last record, but some kind of **End
    of File** (**EOF**) signifier would help with reading data between systems.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在最后一条记录后添加换行是可选的，但某种类型的**文件结束符**（**EOF**）将有助于在系统间读取数据。
- en: A header row is optional but should include the same number of fields/columns
    as the corresponding record level data.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标题行是可选的，但应包含与相应记录级别数据相同的字段/列数。
- en: Each record should have a consistent delimiter, such as a comma (`,`), a semicolon
    (`;`), a pipe (`|`), or a tab.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The inclusion of double quotes between each field value is optional but is recommended
    to avoid misalignment or confusion, especially when reading in large, descriptive
    text data that includes a comma in the field value itself, such as `Also, Stacy
    enjoys watching movies`.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leading and trailing spaces between each field are optional as well but should
    be used consistently throughout the entire file.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The size of a CSV file will vary but can be quite large and is dependent on
    the density of the data (the number of distinct values, the number of rows, and
    the number of fields).
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Depending on the OS, such as Linux, a CSV would only include a **line feed**
    (**LF**) and not a **carriage return** (**CR**) for each row.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following screenshot, I have included a few examples of samples rows
    within a CSV file that all include the exact same information but using different
    formats to delimit the fields within the file:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f0e69553-5f93-426b-85b9-08d49d9dba47.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
- en: One big advantage of consuming or producing CSV over any other format is the
    capability to share between other tools used for data analysis. For example, spreadsheet
    solutions such as Excel can easily read a CSV file without the need to convert
    the file or use a third-party extension. However, a disadvantage is the loss of
    defined data types for each column, which could lead to misrepresenting values
    in your analysis. For example, a value in a column of `1` or `0` could represent
    a Boolean flag or a user hit count from a website.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: XML
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The XML file format was introduced as a standard format in the 1990s. I still
    remember how early on in my career XML was proposed as a replacement to CSV or
    even to the use of databases as a data repository. XML is flexible as a solution
    for developers to create web applications and, similar to CSV, is used to move
    data between systems in and out of the organization. XML is open source and has
    a defined standard that is maintained by the **World Wide Web Consortium** (**WC3**)
    organization. Some key characteristics of XML file format are as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: It is most commonly identified with a file extension of `.xml`.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first line should include a declaration with encoding details and the `.xml`
    version, such as `<?xml version = "1.0" encoding="UTF-8" ?>`.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It uses tags around each element that are similar to HTML tag code, using a
    beginning tag of `<` and `>` or `/>`.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It contains elements, which are the defined fields or columns of the structured
    data.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It contains attributes, which are the data values within each defined element.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is optional but recommended to include a **Document Type Definition** (**DTD**),
    which provides details and helps define how the elements should be used, along
    with the data types
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A sample XML file is shown in the following screenshot. Here, I converted `evolution_of_data_analysis.csv`
    into XML format and displayed a few sample records:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的屏幕截图显示了示例 XML 文件。在这里，我将 `evolution_of_data_analysis.csv` 转换为 XML 格式，并显示了一些示例记录：
- en: '![](img/4470b4ad-7061-4f18-bfbe-e621d0b9ddcb.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4470b4ad-7061-4f18-bfbe-e621d0b9ddcb.png)'
- en: While a disadvantage of XML is a larger file size, due to adding tags and definitions
    to each element, an advantage of using the XML format is the ability to support
    data hierarchies and a defined schema. Let's break down those two points.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 XML 的一个缺点是文件大小较大，因为需要在每个元素中添加标签和定义，但使用 XML 格式的优点是能够支持数据层次结构和定义的架构。让我们分析这两个要点。
- en: Data hierarchy
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据层次结构
- en: Data hierarchies are defined and consistent groupings of data fields or records.
    The hierarchy can be obvious—for example, a son has a father and a mother—but
    from a data perspective, that relationship must be defined. In XML file format,
    you use a concept called an XML tree. The tree is defined by elements within the
    XML file that have a defined relationship. In the case of the `Evolution of Data
    Analysis.xml` file, each milestone has the details grouped together. Now, we can
    easily identify that the milestone event of `John von Neumann / array` was created
    in 1945, along with the rest of the supporting elements that are tagged, such
    as `<Decade>`, `<Milestone Title>`, `<Milestone Event>`, `<Why Important>`, `<Reference>`,
    and `<People Process or Technology Tag>`. This hierarchy relationship is commonly
    known as a **parent-child** relationship, where each indented element is a child to
    the *parent* element, `Evolution_of_Data_Milestone`.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 数据层次结构是数据字段或记录的明确和一致的分组。层次结构可能是显而易见的——例如，一个儿子有一个父亲和一个母亲——但从数据的角度来看，这种关系必须被定义。在
    XML 文件格式中，您使用一个称为 XML 树的概念。树是由 XML 文件中具有定义关系的元素定义的。在 `Evolution of Data Analysis.xml`
    文件的情况下，每个里程碑都有相关的详细信息分组在一起。现在，我们可以轻松地识别 `John von Neumann / array` 的里程碑事件是在 1945
    年创建的，以及所有其他标记的辅助元素，如 `<Decade>`、`<Milestone Title>`、`<Milestone Event>`、`<Why
    Important>`、`<Reference>` 和 `<People Process or Technology Tag>`。这种层次结构关系通常被称为
    **父子** 关系，其中每个缩进的元素都是父元素 `Evolution_of_Data_Milestone` 的子元素。
- en: Defined schema
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义架构
- en: A defined schema means the data elements will also include metadata (data about
    the data) to help with the conformity of each element and attribute. This concept
    was required in most RDBMSes, but XML offers the concept of a DTD file to be included
    with one or more XML files. The file extension is `.xsd`, and it should complement
    each XML file.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 定义架构意味着数据元素还将包括元数据（关于数据的资料），以帮助每个元素和属性的符合性。这个概念在大多数关系型数据库管理系统（RDBMS）中是必需的，但
    XML 提供了将 DTD 文件包含在一个或多个 XML 文件中的概念。文件扩展名是 `.xsd`，它应该与每个 XML 文件相匹配。
- en: The contents of the XSD file can be complex and very dense, depending on the
    complexity of records found in the XML file and the need to define a rigid structure
    when consuming the XML data. For example, a defined data type for each element
    would help you to better understand how to use the data during analysis. For example,
    say with `type="xs:decimal"` you know the attribute value in each element *must*
    contain numeric values and any text values should *not* exist. Another useful
    schema definition would be the definition for an element of `use="required"`,
    which means specific elements must always have a value and should *not* contain
    any null/empty attributes.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: XSD 文件的内容可能很复杂且非常密集，这取决于在 XML 文件中找到的记录的复杂性和在消费 XML 数据时定义刚性结构的需求。例如，为每个元素定义的数据类型可以帮助您更好地理解在分析期间如何使用数据。例如，如果使用
    `type="xs:decimal"`，您就知道每个元素中的属性值必须包含数值，任何文本值都不应该存在。另一个有用的架构定义是 `use="required"`
    的元素定义，这意味着特定的元素必须始终具有值，并且不应该包含任何空/空属性。
- en: There are more details on this topic available on the W3C website, which you
    can find in the *Further reading* section of this chapter.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这个主题的更多细节可以在 W3C 网站上找到，您可以在本章的“进一步阅读”部分找到。
- en: JSON
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: JSON
- en: JSON is another open source file standard for the communication of data between
    systems. It was created by Douglas Crockford around 2001 to improve communication
    between computers and web browsers using a concept called **stateless**. This
    means your computer's web browser, which is known as the **client**, doesn't have
    to wait for the **server** to respond, and vice versa. This is also known as **Representational
    State Transfer** (**REST**) architecture and is very common in web, API, and modern
    technologies because it scales to support millions of concurrent users.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: JSON是另一种用于系统间数据通信的开源文件标准。它由道格拉斯·克罗克福德于2001年左右创建，旨在通过一个称为**无状态**的概念来改善计算机和网页浏览器之间的通信。这意味着你的计算机的网页浏览器，即所谓的**客户端**，不必等待**服务器**响应，反之亦然。这也就是所谓的**表征状态转移**（**REST**）架构，在网页、API和现代技术中非常常见，因为它可以扩展以支持数百万并发用户。
- en: Once REST became a popular web architecture, the need to find a faster and more
    efficient communication protocol drove the adoption of JSON data, which can either
    be streamed or persisted as a file. Having many websites that use JavaScript and
    a JavaScript-friendly notation also increased JSON's popularity.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦REST成为流行的网络架构，寻找更快、更高效的通信协议的需求推动了JSON数据的采用，它既可以流式传输，也可以持久化为文件。由于许多网站使用JavaScript以及JavaScript友好的标记，这也增加了JSON的流行度。
- en: 'Similar to XML and CSV, JSON is readable by humans as well as many different
    types of computer systems and languages, such as Python. This also means JSON
    is not a binary file format, which means it does not require the file to be compiled
    for use by a computer. I included JSON as a milestone in the *"The evolution of
    data analysis and why it is important"* section in [Chapter 1](0fa7e28f-7a30-4099-9bae-30dd3c86ee4f.xhtml),
    *Fundamentals of Data Analysis,* because of its contributions to advancing how
    we communicate using data. A sample JSON format is shown in the following screenshot,
    which is very similar to the XML format sample from the previous *XML* section
    because you now see the data organized and grouped by record using curly brackets
    (`{` and `}`) to encapsulate each row from the original CSV file. Each grouping
    using curly brackets is identified as an object:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 与XML和CSV类似，JSON不仅对人类可读，而且对许多不同类型的计算机系统和技术，如Python，也是可读的。这也意味着JSON不是二进制文件格式，这意味着它不需要对文件进行编译以便计算机使用。我将JSON作为“数据分析的演变及其重要性”部分中的里程碑，在[第一章](0fa7e28f-7a30-4099-9bae-30dd3c86ee4f.xhtml)，“数据分析基础”中包括，因为它对推进我们使用数据通信的贡献。以下截图显示了JSON格式示例，这与之前*XML*部分中的XML格式示例非常相似，因为你现在可以看到数据通过花括号（`{`和`}`）组织并按记录分组，以封装原始CSV文件中的每一行。每个使用花括号分组的都被识别为一个对象：
- en: '![](img/311002fb-5349-4cbf-870e-70a3544c673b.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/311002fb-5349-4cbf-870e-70a3544c673b.png)'
- en: One important concept to understand with JSON data is that it evolved from XML
    but streamlines many of the complexities that could exist in XML formats. Like
    XML, it benefits from the ability to define a data hierarchy and includes a defined
    schema that supports a concept called **schema on read**.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在理解JSON数据时，一个重要的概念是它起源于XML，但简化了许多在XML格式中可能存在的复杂性。像XML一样，它受益于定义数据层次结构的能力，并包含一个定义良好的模式，支持称为**读取时模式**的概念。
- en: In traditional solutions that have a defined schema, the producer was forced to
    establish a schema before any data is loaded or transferred between systems. This
    process required expertise and extra steps during data ingestion and delayed the
    delivery of data to the consumer. With JSON and the concept of schema on read,
    the producer can send over the data along with all the metadata at the same time.
    All the details, such as field names, data types (`dtype`) for each field, and,
    in some cases, a full data dictionary, will be included. Providing this level
    of detail helps the consumer of the data to better understand the relationships
    within each element and attribute.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在具有定义良好模式的传统解决方案中，生产者在任何数据加载或系统间传输之前被迫建立模式。这个过程需要专业知识，并在数据摄入期间增加了额外步骤，延迟了数据向消费者的交付。有了JSON和读取时模式的概念，生产者可以同时发送数据以及所有元数据。所有细节，如字段名称、每个字段的`dtype`（数据类型），以及在某些情况下，完整的数据字典，都将包括在内。提供这一级别的细节有助于数据消费者更好地理解每个元素和属性之间的关系。
- en: 'You will find, in a lot of JSON-formatted data, the concept of `name: value`
    pairs, which are also used in the example in the previous screenshot. This concept
    allows values to be assigned within the identification of the field within each
    record while still maintaining the hierarchy, rather than breaking out the records
    across multiple rows. Each field name is identified to the left of the colon (`:`)
    and the value is found to the right.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '你会在很多JSON格式的数据中找到`name: value`对的概念，这在之前的屏幕截图中的示例中也使用了。这个概念允许在识别每个记录中的字段的同时分配值，而不是将记录拆分到多行中。每个字段名称位于冒号（`:`）的左侧，而值位于冒号的右侧。'
- en: 'Each `name: value` relationship is separated by a comma and many examples will
    have a unique record identity, which helps with carrying out analysis on one-to-many
    relationships. So, you can nest many different relationships deep within a JSON
    structure and still have a way to identify which record the `name: value` pair
    belongs. If an array of values are required to be stored in a JSON file, they
    use square brackets (`[` and `]`) to define the list of values.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '每个`name: value`关系由逗号分隔，许多示例将具有唯一的记录标识符，这有助于执行一对一关系的分析。因此，你可以在JSON结构中嵌套许多不同的关系，并且仍然有方法来识别`name:
    value`对属于哪个记录。如果需要在JSON文件中存储值数组，它们使用方括号（`[`和`]`）来定义值列表。'
- en: Defining a schema forces the data to have controls and context beyond what is
    observed. It removes assumptions about the data attributes and helps with interpreting
    how the data values should be used for analysis. For example, a value of `20191219`
    could be easily understood to be an integer value or could be the representation
    of the `12/19/2019` date with the format stored as `YYYYMMDD`. Without having
    a defined schema to reference, along with details about how and why that field
    is supposed to be used, your analysis of the data could be flawed.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个模式迫使数据具有超出观察到的控制项和上下文。它消除了对数据属性假设，并有助于解释数据值应该如何用于分析。例如，值`20191219`可以很容易地理解为整数值，或者可能是以`YYYYMMDD`格式存储的`12/19/2019`日期的表示。如果没有定义的模式来参考，以及关于如何以及为什么应该使用该字段的详细信息，你的数据分析可能会出现错误。
- en: Data dictionaries and data types
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据字典和数据类型
- en: Throughout the book, I will continue to re-enforce the need to have a data dictionary
    to help with the analysis of data. As with any data we have uncovered so far,
    a data dictionary will come in all shapes and sizes. This means it could be documented
    outside the source file, which is common on a help page, a wiki, or a blog or
    within the source data, as we discussed with XML and JSON files.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，我将继续强调拥有数据字典以帮助分析数据的重要性。与迄今为止我们发现的所有数据一样，数据字典将具有各种形状和大小。这意味着它可能记录在源文件之外，这在帮助页面、维基或博客中很常见，或者在我们讨论的XML和JSON文件中记录在源数据内。
- en: Having the data defined and documented will aid you in the journey to understand
    it but will not be the only method required to become a domain expert for a dataset.
    Domain expertise comes from experience with understanding how the data is used,
    along with the business or purpose behind the underlining source data. We covered
    some of these concepts in [Chapter 1](0fa7e28f-7a30-4099-9bae-30dd3c86ee4f.xhtml),
    *Fundamentals of Data Analysis*, looking at how **Know Your Data** (**KYD**) and
    having a data dictionary available aids in the effort to learn more about the
    underlying dataset.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的定义和文档化将帮助你理解数据，但不会是成为数据集领域专家所需的所有方法。领域专业知识来自于理解数据如何使用以及底层源数据的业务或目的的经验。我们在[第1章](0fa7e28f-7a30-4099-9bae-30dd3c86ee4f.xhtml)，“数据分析基础”，中讨论了这些概念，探讨了了解数据（**KYD**）和拥有数据字典如何有助于学习更多关于底层数据集的信息。
- en: The analysis of data and the KYD concept should be applied throughout the process
    of analyzing data, so be sure to check the numbers and verify that the results
    match up to how the data is defined to build trust and confidence in your insights.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析和KYD概念应该在整个数据分析过程中得到应用，所以请确保检查数字并验证结果是否符合数据的定义，以建立对洞察力的信任和信心。
- en: Data dictionaries are common for legacy systems, RDBMS, and traditional **Enterprise
    Data Warehouses** (**EDW**). It is common to have a data catalog available and,
    in many cases, they are required to build communication data pipelines between
    different systems. In some cases, a data dictionary is required as part of regulatory
    requirements or as part of a governed corporate policy.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 数据字典在遗留系统、关系数据库管理系统（RDBMS）和传统的**企业数据仓库**（**EDW**）中很常见。通常会有一个数据目录可用，在许多情况下，它们是构建不同系统之间通信数据管道所必需的。在某些情况下，数据字典是作为监管要求的一部分或作为受控企业政策的一部分所必需的。
- en: In modern systems, **Application Programming Interfaces** (**APIs**) have become
    the central repository for metadata and the de facto data dictionary because JSON
    is a popular communication vehicle where the schema is defined and should be well
    documented. However, in practice, I find that documentation is written for programmers
    by programmers, so it may not meet all the needs to fully understand the data
    and answer all the business questions during analysis.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在现代系统中，**应用程序编程接口**（**APIs**）已成为元数据的中心存储库和事实上的数据字典，因为JSON是一种流行的通信工具，其中定义了模式并且应该有良好的文档。然而，在实践中，我发现文档是由程序员为程序员编写的，因此它可能无法满足完全理解数据和在分析期间回答所有业务问题的所有需求。
- en: It is also common to version a data dictionary as part of a **Master Data Management**
    (**MDM**) or data governance solution. Within these versions, you will uncover
    details behind the *what* and the *why* for the data. For example, a field may
    be defined as inactive but still available, so it becomes sparsely populated because
    the application/system used to populate it changed.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据字典版本化作为**主数据管理**（**MDM**）或数据治理解决方案的一部分也很常见。在这些版本中，您将揭示数据背后的**是什么**和**为什么**的细节。例如，一个字段可能被定义为非活动状态但仍然可用，因此它变得稀疏，因为用于填充它的应用程序/系统已经改变。
- en: Having that level of detail may help to identify data gaps or to better understand
    how to build a data bridge by combining values from two different fields at different
    periods of time for accurate historical analysis. I worked with a client once
    who was replacing a large enterprise legacy system, which cost millions of dollars,
    with consulting hardware and software. The consulting time was calculated by the
    hour, with dozens of specialists traveling every week to the client site.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有如此详细的程度可能有助于识别数据差距或更好地理解如何通过在不同时间段结合两个不同领域的值来构建数据桥梁，以进行准确的历史分析。我曾经与一位客户合作，他正在用咨询硬件和软件替换一个耗资数百万美元的大型企业遗留系统。咨询时间按小时计算，每周有数十名专家前往客户现场。
- en: There was a pivotal moment in the project where it was determined infeasible
    and too costly to migrate all the legacy supply chain, accounting, and HR details
    from the old system to the new one. To avoid delays, we proposed an analytics
    solution where both the legacy system data and the new system data were merged
    together daily. A rolling window of time logic was built in so that after 7 years,
    the legacy data would no longer be used for analysis, but during that timeframe,
    a blend of both systems, which included different fields and records, would be
    presented for analysis.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在项目中有一个关键时刻，确定将所有遗留供应链、会计和人力资源细节从旧系统迁移到新系统是不切实际且成本过高的。为了避免延误，我们提出了一个分析解决方案，其中每天将遗留系统数据和新的系统数据合并在一起。构建了一个滚动时间窗口逻辑，因此7年后，遗留数据将不再用于分析，但在那个时间段内，包括不同字段和记录的两种系统的混合将用于分析。
- en: Having a data dictionary was a must for this type of solution, and providing
    additional documentation was required to ensure the audience understood where
    the source of the data came from depending on the time period of the reporting
    and analysis. Part of that documentation required details behind the different
    fields and variations in the data types. Some systems will allow a mix of different
    data types or, as in Python, will default to specific data types.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此类解决方案，拥有数据字典是必不可少的，并且需要提供额外的文档以确保受众了解数据来源，这取决于报告和分析的时间段。该文档的一部分需要不同字段和数据类型变化的细节。某些系统将允许不同数据类型的混合，或者在Python中，将默认为特定的数据类型。
- en: Just remember that you may need to convert a data type between multiple sources,
    especially when blending between different systems and file formats. For example,
    in JSON, a number defined as `real` would be called `float` in Python. If you
    run into issues with converting data types during the loading of data, you may
    need to go back to the data source provider and request it to be resent in a format
    that would be easier to consume.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 只需记住，你可能需要在多个来源之间转换数据类型，尤其是在不同系统和文件格式之间混合时。例如，在 JSON 中，定义为 `real` 的数字在 Python
    中被称为 `float`。如果在加载数据时遇到转换数据类型的问题，可能需要回到数据源提供者并请求以更易于消费的格式重新发送。
- en: As you continue increasing your data literacy, you need to understand that different
    technologies and data formats will lead to different data types, which will require
    translation to ensure accurate analysis of the data, especially from multiple
    sources.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 随着你继续提高你的数据素养，你需要理解不同的技术和数据格式会导致不同的数据类型，这需要转换以确保对数据的准确分析，尤其是来自多个来源的数据。
- en: Creating our first DataFrame
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建我们的第一个 DataFrame
- en: 'Before we begin with some hands-on examples, some useful commands to run in
    `pandas` are as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始一些动手示例之前，以下是一些在 `pandas` 中可以运行的实用命令：
- en: '`pd.read_csv(‘inport_filename.csv'', header=1)`: Reads data from a CSV file
    directly into a `pandas` DataFrame'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pd.read_csv(''inport_filename.csv'', header=1)`：直接从 CSV 文件读取数据到 `pandas` DataFrame'
- en: '`my_df.to_csv(‘export_filename'')`: Directly exports the DataFrame to a CSV
    file to your workstation'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`my_df.to_csv(''export_filename'')`：直接将 DataFrame 导出为 CSV 文件到您的工作站'
- en: '`my_df.shape`: Provides the number of rows and columns of your DataFrame'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`my_df.shape`：提供 DataFrame 的行数和列数'
- en: '`my_df.info()`: Provides metadata about your DataFrame, including data types
    for each column'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`my_df.info()`：提供关于 DataFrame 的元数据，包括每列的数据类型'
- en: '`my_df.describe()`: Includes statistical details with a column that includes
    the count, mean, **standard deviation** (**std**), minimum, maximum, and percentiles (25th,
    50th, and 75th) for any numeric column'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`my_df.describe()`：包括统计细节，其中包含计数、平均值、**标准差**（**std**）、最小值、最大值和百分位数（25th、50th
    和 75th）的列'
- en: '`my_df.head(2)`: Displays the first two records from the DataFrame'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`my_df.head(2)`：显示 DataFrame 的前两条记录'
- en: '`my_df.tail(2)`: Displays the last two records from the DataFrame'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`my_df.tail(2)`：显示 DataFrame 的最后两条记录'
- en: '`my_df.sort_index(1)`: Sorts by the labels along an axis—in this example, by
    the column label headers alphabetically from left to right'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`my_df.sort_index(1)`：按轴上的标签排序——在这个例子中，按列标签标题从左到右字母顺序排序'
- en: '`my_df.isnull()`: Displays a list of all rows with a `True`/`False` indicator
    if any of the values by column are null'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`my_df.isnull()`：显示所有具有 `True`/`False` 指示器的行列表，如果任何列的值是空的'
- en: 'Our first example will load data from a CSV file into a `pandas` DataFrame
    that has a pipe (`|`) delimiter and will run some of the preceding commands:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个例子将从 CSV 文件加载数据到具有管道（`|`）分隔符的 `pandas` DataFrame 中，并运行一些前面的命令：
- en: Launch Jupyter and create a new Python notebook.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动 Jupyter 并创建一个新的 Python 笔记本。
- en: To stay consistent with the best practices, be sure to rename the notebook `exploring_the_pandas_library` before
    moving forward.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了保持最佳实践的一致性，在继续之前，请确保将笔记本重命名为 `exploring_the_pandas_library`。
- en: Type `import pandas as pd` into the `In []:` cell.
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `In []:` 单元格中输入 `import pandas as pd`。
- en: Run the cell. No output will be displayed after you run the cell.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行单元格。运行单元格后不会显示任何输出。
- en: Type `my_df = pd.read_csv('evolution_of_data_analysis.csv', header=0, sep="|")` into
    the next `In []:` cell.
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一个 `In []:` 单元格中输入 `my_df = pd.read_csv('evolution_of_data_analysis.csv',
    header=0, sep="|")`。
- en: Run the cell. No output will be displayed after you run the cell.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行单元格。运行单元格后不会显示任何输出。
- en: Type `my_df.shape` into the next `In []:` cell.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一个 `In []:` 单元格中输入 `my_df.shape`。
- en: 'Verify that the output cell displays `Out []`. `(42, 7)` will be displayed,
    which tells you that there are 42 rows and 7 columns, as in the following screenshot:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确认输出单元格显示 `Out []`。将显示 `(42, 7)`，这告诉你有 42 行和 7 列，如下截图所示：
- en: '![](img/69507b5f-7127-4047-a8e2-8aa796b8335e.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![截图](img/69507b5f-7127-4047-a8e2-8aa796b8335e.png)'
- en: Type `my_df.info()` into the next `In []:` cell.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一个 `In []:` 单元格中输入 `my_df.info()`。
- en: Run the cell.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行单元格。
- en: 'Verify that the output cell displays `Out []`. There will be multiple rows,
    including data types for all seven columns, as in the following screenshot:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确认输出单元格显示 `Out []`。将会有多行，包括所有七列的数据类型，如下截图所示：
- en: '![](img/83180595-25be-40d2-8f20-cb4c3336e5e6.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![截图](img/83180595-25be-40d2-8f20-cb4c3336e5e6.png)'
- en: Type `my_df.describe()` into the next `In []:` cell.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一个 `In []:` 单元格中输入 `my_df.describe()`。
- en: Run the cell.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行单元格。
- en: 'Verify that the output cell displays `Out []`. There will be multiple rows
    of output, with one column with a header of `Year`, as in the following screenshot.
    Statistical values from the `Year` field will be displayed, including `count`,
    `mean`, and `max`:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证输出单元格显示`Out []`。将有多个输出行，其中一列的标题为`Year`，如下面的截图所示。将显示来自`Year`字段的统计值，包括`count`、`mean`和`max`：
- en: '![](img/7ae6f147-9d56-4fc6-a33f-c37987100ff2.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7ae6f147-9d56-4fc6-a33f-c37987100ff2.png)'
- en: Type `my_df.head(2)` into the next `In []:` cell and run the cell.
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`my_df.head(2)`输入到下一个`In []:`单元格中并运行该单元格。
- en: 'Verify that the output cell displays `Out []`:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证输出单元格显示`Out []`：
- en: The output should include an index in the first column with a starting row of
    `0`, as in the following screenshot.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出应包括第一列的索引，起始行为`0`，如下面的截图所示。
- en: 'All seven columns will be displayed, along with the first two rows from the
    source file:'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有七列都将显示，包括源文件的前两行：
- en: '![](img/b26064fe-3ea5-4f38-8294-1e5af795a209.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b26064fe-3ea5-4f38-8294-1e5af795a209.png)'
- en: Type `my_df.tail(2)` into the next `In []:` cell and run the cell.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`my_df.tail(2)`输入到下一个`In []:`单元格中并运行该单元格。
- en: 'Verify that the output cell displays `Out []`. The output should include an
    index in the first column with a starting row of `40`, as in the following screenshot.
    All seven columns will be displayed, along with the last two rows from the source
    file:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证输出单元格显示`Out []`。输出应包括第一列的索引，起始行为`40`，如下面的截图所示。所有七列都将显示，包括源文件中的最后两行：
- en: '![](img/df2d7257-fa2f-437f-9542-8250dd22ddf8.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/df2d7257-fa2f-437f-9542-8250dd22ddf8.png)'
- en: Type `my_df.sort_index(1)` into the next `In []:` cell and run the cell.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`my_df.sort_index(1)`输入到下一个`In []:`单元格中并运行该单元格。
- en: 'Verify that the output cell displays `Out []`. The output should include an
    index in the first column with a starting row of `0`, as in the following screenshot.
    All seven columns will be displayed, but the order of the columns has changed
    to alphabetically sort from left to right, starting with `Decade` and ending with
    `Year`:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证输出单元格显示`Out []`。输出应包括第一列的索引，起始行为`0`，如下面的截图所示。所有七列都将显示，但列的顺序已改为从左到右按字母顺序排序，从`Decade`开始，以`Year`结束：
- en: '![](img/aaaa3dee-104e-47de-b3d8-568c3954f2d6.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/aaaa3dee-104e-47de-b3d8-568c3954f2d6.png)'
- en: In the next example, let's answer a few business questions from the data by
    exploring some of the features available in `pandas`. The first question is *how
    many milestone events occurred by decade?* To answer this question, we need to
    use the `groupby` feature, so let's go through the steps to provide the answer.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个示例中，让我们通过探索`pandas`中的一些功能来回答一些来自数据的企业问题。第一个问题是*每个十年发生了多少个里程碑事件？*要回答这个问题，我们需要使用`groupby`功能，所以让我们通过以下步骤来提供答案。
- en: 'The steps to reproduce this example are as follows:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 重新生成此示例的步骤如下：
- en: Launch Jupyter and create a new Python notebook.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Jupyter并创建一个新的Python笔记本。
- en: To stay consistent with the best practices, be sure to rename the notebook `exploring_the_pandas_library_example_2`
    before moving forward.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了保持最佳实践的一致性，在继续之前，请确保将笔记本重命名为`exploring_the_pandas_library_example_2`。
- en: Type `import pandas as pd` into the `In []:` cell and run the cell.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`import pandas as pd`输入到`In []:`单元格中并运行该单元格。
- en: Type `my_df = pd.read_csv('evolution_of_data_analysis.csv', header=0, sep="|")` into
    the next `In []:` cell and run the cell.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`my_df = pd.read_csv('evolution_of_data_analysis.csv', header=0, sep="|")`输入到下一个`In
    []:`单元格中并运行该单元格。
- en: Type `my_df.head(2)` into the next `In []:` cell and run the cell.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`my_df.head(2)`输入到下一个`In []:`单元格中并运行该单元格。
- en: 'Verify that the output cell displays `Out []`:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证输出单元格显示`Out []`：
- en: The output should include an index in the first column with a starting row of
    `0`.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出应包括第一列的索引，起始行为`0`。
- en: All seven columns will be displayed, along with the first two rows from the
    source file.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有七列都将显示，包括源文件的前两行。
- en: Type `my_df.groupby(['Decade']).agg({'Year':'count'})`into the `In []:` cell
    and run the cell.
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`my_df.groupby(['Decade']).agg({'Year':'count'})`输入到`In []:`单元格中并运行该单元格。
- en: 'Verify that the output cell displays `Out []`:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证输出单元格显示`Out []`：
- en: The output will display 10 rows of data with 2 columns.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出将显示10行数据，2列。
- en: The header row in the first column will be `Decade` and will be `Year` for the
    second column.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一列的标题行将是`Decade`，第二列将是`Year`。
- en: 'The results will match the following screenshot:'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果将与以下截图匹配：
- en: '![](img/f40d7cae-2447-491e-9f33-f428151156e7.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f40d7cae-2447-491e-9f33-f428151156e7.png)'
- en: In the preceding screenshot, we followed the previous steps to load the CSV
    file as a DataFrame named `my_df`. To verify that the DataFrame loaded correctly,
    we ran the `head()` function and included the parameter of `2` to limit the number
    of rows displayed in the notebook. The last command is to run `groupby` against
    the `Decade` column and combine it with an aggregation to count the values from
    the `Milestone Event` field/column. We can now answer some questions about this
    dataset, such as that 14 milestone events occurred during the 2000s or that the
    first decade to have any milestone events was the 1940s because that is the first
    row that has any values.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的屏幕截图中，我们遵循了之前的步骤将CSV文件作为名为`my_df`的DataFrame加载。为了验证DataFrame已正确加载，我们运行了`head()`函数，并包括了参数`2`以限制在笔记本中显示的行数。最后一个命令是运行`groupby`对`Decade`列进行分组，并将其与聚合结合，以计算`Milestone
    Event`字段/列的值。我们现在可以回答一些关于这个数据集的问题，例如，在2000年代发生了14个里程碑事件，或者第一个有里程碑事件的十年是1940年代，因为这是第一个有值的行。
- en: Summary
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Congratulations, you have now created your first DataFrame using the `pandas`
    library! We started the chapter by introducing you to the concepts of structured
    tabular data and the different techniques available to manipulate it by transposing
    and pivoting the data. More importantly, we discussed the importance of why data
    should be in tabular form. We then introduced the `pandas` library and defined
    a DataFrame, and demonstrated the many benefits of this powerful feature that
    are available for you during data analysis. In the handling of essential data
    formats, we went through the different data formats available by going through
    the details of the CSV, XML, and JSON file formats. Before we ended the chapter
    by creating our first DataFrame, we discussed the importance of data dictionaries
    and how different data types improve your data literacy, as well as why they are
    important before, during, and after the data analysis workflow has completed.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜，你现在已经使用`pandas`库创建了你第一个DataFrame！我们通过介绍结构化表格数据的概念以及通过转置和交叉数据来操作它的不同技术开始了这一章。更重要的是，我们讨论了为什么数据应该以表格形式存在的重要性。然后我们介绍了`pandas`库，并定义了DataFrame，并展示了在数据分析过程中这个强大功能为您带来的许多好处。在处理基本数据格式时，我们通过CSV、XML和JSON文件格式的细节介绍了可用的不同数据格式。在我们通过创建第一个DataFrame来结束这一章之前，我们讨论了数据字典的重要性以及不同数据类型如何提高您的数据素养，以及为什么它们在数据分析工作流程完成前后都很重要。
- en: In the next chapter, [Chapter 5](bea8a62c-4469-47e3-a668-10fbb91815ea.xhtml), *Gathering
    and Loading Data in Python*, we will introduce you to how to load data from databases
    using SQL and continue working with the features available in `pandas` and DataFrames.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，[第5章](bea8a62c-4469-47e3-a668-10fbb91815ea.xhtml)，*在Python中收集和加载数据*，我们将向您介绍如何使用SQL从数据库加载数据，并继续使用`pandas`和DataFrame的功能。
- en: Further reading
  id: totrans-227
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: McKinney, W., *Data Structures for Statistical Computing in Python*, *Proceedings
    of the 9th Python in Science Conference*, Vol. 445 (2010)
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McKinney, W., 《Python中的统计计算数据结构》，*《第9届Python科学会议论文集》*，第445卷（2010年）
- en: Torres-Reyna, O., *Panel Data Analysis Fixed and Random Effects using Stata*
    (v. 4.2), *Princeton.edu*, (2007), available at [https://www.princeton.edu/~otorres/Panel101.pdf](https://www.princeton.edu/~otorres/Panel101.pdf)
    [accessed 23 Dec. 2019]
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Torres-Reyna, O.，《使用Stata进行面板数据固定效应和随机效应分析》（版本4.2），*Princeton.edu*，（2007年），可在[https://www.princeton.edu/~otorres/Panel101.pdf](https://www.princeton.edu/~otorres/Panel101.pdf)
    [访问日期：2019年12月23日]
- en: '**National Longitudinal Surveys** (**NLSes**) for examples of panel data: [https://www.bls.gov/nls/home.htm](https://www.bls.gov/nls/home.htm)'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**国家纵向调查**（**NLSes**）作为面板数据的例子：[https://www.bls.gov/nls/home.htm](https://www.bls.gov/nls/home.htm)'
- en: A definition of a `pandas` DataFrame: [https://www.geeksforgeeks.org/python-pandas-dataframe](https://www.geeksforgeeks.org/python-pandas-dataframe/)
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas` DataFrame的定义：[https://www.geeksforgeeks.org/python-pandas-dataframe](https://www.geeksforgeeks.org/python-pandas-dataframe/)'
- en: Quick details about the QVD file format: [https://help.qlik.com/en-US/sense/June2019/Subsystems/Hub/Content/Sense_Hub/Scripting/work-with-QVD-files.htm](https://help.qlik.com/en-US/sense/June2019/Subsystems/Hub/Content/Sense_Hub/Scripting/work-with-QVD-files.htm)
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于QVD文件格式的快速详情：[https://help.qlik.com/en-US/sense/June2019/Subsystems/Hub/Content/Sense_Hub/Scripting/work-with-QVD-files.htm](https://help.qlik.com/en-US/sense/June2019/Subsystems/Hub/Content/Sense_Hub/Scripting/work-with-QVD-files.htm)
- en: ASCII stands: [https://www.ansi.org/about_ansi/overview/overview?menuid=1](https://www.ansi.org/about_ansi/overview/overview?menuid=1)
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ASCII代表：[https://www.ansi.org/about_ansi/overview/overview?menuid=1](https://www.ansi.org/about_ansi/overview/overview?menuid=1)
- en: Unicode format and encoding standards: [https://home.unicode.org/](https://home.unicode.org/)
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Unicode 格式和编码标准：[https://home.unicode.org/](https://home.unicode.org/)
- en: CSV rules and standards: [https://tools.ietf.org/html/rfc4180](https://tools.ietf.org/html/rfc4180)
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CSV 规则和标准：[https://tools.ietf.org/html/rfc4180](https://tools.ietf.org/html/rfc4180)
- en: The W3C organization standards: [https://www.w3.org/](https://www.w3.org/)
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: W3C 组织标准：[https://www.w3.org/](https://www.w3.org/)
- en: The REST standards: [https://www.ics.uci.edu/~fielding/pubs/dissertation/rest_arch_style.htm#sec_5_1_3](https://www.ics.uci.edu/~fielding/pubs/dissertation/rest_arch_style.htm#sec_5_1_3)
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: REST 标准：[https://www.ics.uci.edu/~fielding/pubs/dissertation/rest_arch_style.htm#sec_5_1_3](https://www.ics.uci.edu/~fielding/pubs/dissertation/rest_arch_style.htm#sec_5_1_3)
- en: History of Unicode: [https://docs.python.org/3.4/howto/unicode.html](https://docs.python.org/3.4/howto/unicode.html)
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Unicode 历史：[https://docs.python.org/3.4/howto/unicode.html](https://docs.python.org/3.4/howto/unicode.html)
