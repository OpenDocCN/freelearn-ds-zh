- en: Creating Your First pandas DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will go through the core data analysis skills of using filesystems
    and formats. We will explore different file formats for text data using the Python
    OS and string libraries to manipulate textual and numerical data from source files,
    such as **Comma-Separated Values** (**CSV**), **Extensible Markup Language** (**XML**),
    and **JavaScript Object Notation **(**JSON**). You will learn what a pandas DataFrame
    is and how to create DataFrames from file sources for data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Techniques for manipulating tabular data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding pandas and DataFrames
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling essential data formats
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data dictionaries and data types
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating your first DataFrame
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here's the GitHub repository for this book: [https://github.com/PacktPublishing/Practical-Data-Analysis-using-Jupyter-Notebook/tree/master/Chapter04](https://github.com/PacktPublishing/Practical-Data-Analysis-using-Jupyter-Notebook/tree/master/Chapter04).
  prefs: []
  type: TYPE_NORMAL
- en: You can download and install the required software from the following link: [https://www.anaconda.com/distribution/](https://www.anaconda.com/distribution/)
  prefs: []
  type: TYPE_NORMAL
- en: Techniques for manipulating tabular data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have a better understanding of array data structures from using
    the NumPy library in [Chapter 3](dd40977f-7c89-4933-944f-d9760d3ca217.xhtml), *Getting
    Started with NumPy*, we can now expand our data analysis expertise. We will do
    this by working with tabular data and focusing on a powerful library available
    in Python named `pandas`, which is available to use in our Jupyter notebooks.
  prefs: []
  type: TYPE_NORMAL
- en: The `pandas` library extends our ability to analyze structured data and was
    introduced as a Python library back in 2008 by Wes McKinney. McKinney recognized
    the power of extending the Python language by using libraries and the need to
    fill the gap that existed between data preparation and data insights by *carrying
    out the entire data analysis workflow in Python without having to switch to a
    more domain-specific language such as R*.
  prefs: []
  type: TYPE_NORMAL
- en: The `pandas` Python library name was taken from the term **panel data** (by McKinney)
    by shortening and combining the terms to get `pan` and `da`. Panel data is defined as
    observations that can be measured over a period of time with multiple dimensional
    values and is very common in statistical studies and research papers. I have also
    seen panel data referred to as longitudinal data, facts panel data, or cross-sectional
    time-series data. Panel data is presented in tabular form with rows and columns
    and comes in a few different types, such as balanced, unbalanced, long, short,
    fixed, and rotating.
  prefs: []
  type: TYPE_NORMAL
- en: Each of these panel data types are based on how precisely the quantity of the
    dataset is represented. The total number of observations (rows of data) is commonly
    identified using the letter `N`. The unit of the time element used, such as year,
    month, quarter, or date is typically identified using the letter `T` in either
    upper or lowercase. The dimensions or variables (columns of data) can be represented
    with specific letters for each entity, such as `x` or `z`. What is measured can
    be represented as one or more variables and is commonly assigned to `y`. You should
    be able to summarize any panel data as a representative sample in descriptive
    terms for the consumer to understand before viewing or working with it in tabular
    form.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the dataset, the dimensions may or may not change over time, so
    different letters, such as `k`, may be assigned to represent that distinction.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of panel data is a daily closing price over 3 days for three publicly
    traded companies, as in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Date** | **Stock Ticker** | **Closing Price** |'
  prefs: []
  type: TYPE_TB
- en: '| 12/2/2019 | ABC | $50.21 |'
  prefs: []
  type: TYPE_TB
- en: '| 12/3/2019 | ABC | $52.22 |'
  prefs: []
  type: TYPE_TB
- en: '| 12/4/2019 | ABC | $51.01 |'
  prefs: []
  type: TYPE_TB
- en: '| 12/2/2019 | DEF | $24.22 |'
  prefs: []
  type: TYPE_TB
- en: '| 12/3/2019 | DEF | $26.22 |'
  prefs: []
  type: TYPE_TB
- en: '| 12/4/2019 | DEF | $29.50 |'
  prefs: []
  type: TYPE_TB
- en: '| 12/2/2019 | GHI | $61.22 |'
  prefs: []
  type: TYPE_TB
- en: '| 12/3/2019 | GHI | $65.33 |'
  prefs: []
  type: TYPE_TB
- en: '| 12/4/2019 | GHI | $75.00 |'
  prefs: []
  type: TYPE_TB
- en: 'Another example is the minimum, maximum, and average temperatures, in Fahrenheit,
    by ZIP code for the last three months (by month), as in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Month** | **ZIP Code** | **Minimum Temperature** | **Maximum Temperature**
    | **Average Temperature** |'
  prefs: []
  type: TYPE_TB
- en: '| June | 19901 | 75 | 88 | 82 |'
  prefs: []
  type: TYPE_TB
- en: '| July | 19901 | 77 | 90 | 84 |'
  prefs: []
  type: TYPE_TB
- en: '| August | 19901 | 68 | 85 | 77 |'
  prefs: []
  type: TYPE_TB
- en: '| June | 08618 | 76 | 89 | 83 |'
  prefs: []
  type: TYPE_TB
- en: '| July | 08618 | 78 | 91 | 85 |'
  prefs: []
  type: TYPE_TB
- en: '| August | 08618 | 69 | 86 | 78 |'
  prefs: []
  type: TYPE_TB
- en: '| June | 18940 | 74 | 87 | 81 |'
  prefs: []
  type: TYPE_TB
- en: '| July | 18940 | 76 | 89 | 83 |'
  prefs: []
  type: TYPE_TB
- en: '| August | 18940 | 67 | 84 | 76 |'
  prefs: []
  type: TYPE_TB
- en: An unbalanced panel would be one where one or more values are missing for any
    one of the dimensional values. A balanced panel would be an inclusive dataset
    where all the elements from each dimension are included across all periods of
    time.
  prefs: []
  type: TYPE_NORMAL
- en: We know from [Chapter 1](0fa7e28f-7a30-4099-9bae-30dd3c86ee4f.xhtml), *Fundamentals
    of Data Analysis*, that data comes in all shapes and sizes, so having data structured
    in tabular form will be the first step in the analysis process, but in many cases,
    not the final one. For example, in the following table, we have a summary pivot
    table of total sales by city over the last 3 years.
  prefs: []
  type: TYPE_NORMAL
- en: 'This summary data can be identified as a cross table, which makes it easier
    for the consumer of this data to quickly identify the highest and lowest sales
    by City and by Year. In this case, this would be New York in 2019 with $120,000
    and Boston in 2017 with $25,000:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **City** | **2017** | **2018** | **2019** |'
  prefs: []
  type: TYPE_TB
- en: '| Philadelphia | $50,000 | $75,000 | $100,000 |'
  prefs: []
  type: TYPE_TB
- en: '| New York | $35,000 | $65,000 | $120,000 |'
  prefs: []
  type: TYPE_TB
- en: '| Boston | $25,000 | $40,000 | $ 90,000 |'
  prefs: []
  type: TYPE_TB
- en: If this tabular form of data had a limited number of rows and columns, this
    would be the final step in your analysis because you can quickly answer most business
    questions without additional manipulation, such as which city has the highest
    sales. However, what if the number of records increased to display over 100 cities
    and we increased the number of years to the last 10? What if you wanted to get
    more details to better understand the sales, breaking down the amount by increasing
    the number of dimensions, such as the product, store number, date of the transaction,
    time of the day, and method of payment?
  prefs: []
  type: TYPE_NORMAL
- en: Increasing the number of columns would make it challenging and time-consuming
    to answer a simple business question, such as what is the average sales across
    all cities across all years? Therefore, the ability to scale your analysis is
    dependent on your ability to manipulate the data beyond how the source is received.
  prefs: []
  type: TYPE_NORMAL
- en: 'A best practice in data analysis is *to begin with the end in mind*. So, for
    this example, the output table we want to produce will look similar to the following
    table, where we have transposed the columns into rows to make it easier for additional
    analysis to be carried out and so that we are prepared to handle a larger volume
    of data:'
  prefs: []
  type: TYPE_NORMAL
- en: A large scale of data volume is a subjective term but the techniques used should
    support analyzing millions or billions of rows of data. You will need additional
    infrastructure beyond the limits of your personal workstation's available RAM
    and CPU.
  prefs: []
  type: TYPE_NORMAL
- en: '| **City** | **Year** | **Sales** |'
  prefs: []
  type: TYPE_TB
- en: '| Philadelphia | 2017 | $50,000 |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2018 | $75,000 |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2019 | $100,000 |'
  prefs: []
  type: TYPE_TB
- en: '| New York | 2017 | $35,000 |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2018 | $65,000 |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2019 | $120,000 |'
  prefs: []
  type: TYPE_TB
- en: '| Boston | 2017 | $25,000 |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2018 | $40,000 |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2019 | $90,000 |'
  prefs: []
  type: TYPE_TB
- en: 'From the preceding output, we can see that:'
  prefs: []
  type: TYPE_NORMAL
- en: The first advantage of having data structured similar to the way it is in the
    preceding output table is that there is a single conformed data type for each
    column, which is also known as a dimension or axis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second advantage is that it becomes much easier for statistical analysis
    to be carried out because each dimension can be treated as an independent array
    of values of the same data type where calculations can be performed using NumPy,
    as covered in [Chapter 3](dd40977f-7c89-4933-944f-d9760d3ca217.xhtml), *Getting
    Started with NumPy*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third advantage is the ability to sort by any field in the table without
    worrying about the data values in each row/tuple becoming misaligned or inconsistent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keeping the integrity of your data builds trust in your process and ensures
    your analysis will be accurate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is recommended that you break down each step during the manipulation of data
    to allow the repeatability of the process—for example, if you were asked to go
    through the process after a few months had passed or if you had to troubleshoot
    anomalies that exist in the underlining source data, which is very common.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding pandas and DataFrames
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have a better understanding of tabular data and we have provided
    some background about panel data and the origins of why the `pandas` library was
    created, let's dive into some examples using `pandas` and explain how DataFrames
    are used.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `pandas` library is a powerful Python library used for changing and analyzing
    data. A `pandas` DataFrame is a feature available in the library and is defined
    as a two-dimensional, size-mutable, potentially heterogeneous tabular data structure
    with labeled axes (rows and columns). A DataFrame is a two-dimensional data structure—that
    is, data is aligned in a tabular fashion in rows and columns. It is commonly known
    that `pandas` DataFrame consists of three principal components: the data, the
    rows, and the columns. Being a visual learner myself, I created an example of
    this with the following diagram, which we can go through now:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0a2ad74d-0276-47cd-a0e4-a28c7ee5d006.png)'
  prefs: []
  type: TYPE_IMG
- en: DataFrames can be compared to a spreadsheet, such as Microsoft Excel or Google
    Sheets, a single database SQL table found in any **Relational Database Management
    System** (**RDBMS**), or even a **QlikView Data** (**QVD**) file. The previous
    examples all include a common element of a **header row** that defines a label
    and alignment for your data, **rows** (with each one identified as a single record),
    **columns** that categorize the structure of each field value, and **data** that
    contains numeric and/or text values.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example, each row includes an identity record of the `ID` field, but
    that is not a requirement in a `pandas` DataFrame. DataFrames are treated as objects in
    Python and support loading data from files or SQL sources directly into memory
    for additional manipulation and analysis. Some key benefits of using DataFrames
    include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: It allows you to convert all source files into readable data objects for easier
    merging and analysis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It provides auto- or defined indexing to help with looking up a value or selecting
    a cross selection from your DataFrame, which is also known as a data slice.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each column can be treated as a single NumPy array, which can collectively have
    multiple data types.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It really excels at fixing data alignment and missing data elements, which are
    displayed and referenced as **Not a Number **(**NaN**).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It allows pivoting and reshaping data without going back to the source of record
    for each dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is easy to add, remove, or change data using single Python commands to expedite
    the analysis of one or more data sources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allows aggregations, such as `Group By`, and other calculations against metrics,
    such as `sum`, `min`, `max`, can all be performed against the DataFrame.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allows merging, sorting, joining, and filtering against one or more DataFrames.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It is scalable to support a repeatable workflow of analysis. For example, the
    following pseudo-code steps are easy to replicate in a Jupyter notebook:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Import the `pandas` library.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the source file into a new DataFrame.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a second DataFrame that drops duplicate rows or columns from the original.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create summary metrics.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Save the second DataFrame as a new file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'What I enjoy about using `pandas` and DataFrames is the flexibility of the
    built-in commands that are provided to you as a data analyst. Let''s walk through
    a few examples. To create a DataFrame from scratch on a limited number of records,
    you can simply add them with a few commands:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To load `pandas`, you just need to add the following command to your Jupyter
    notebook and run the cell. Feel free to follow along by creating your own notebook;
    I have added a copy to GitHub for reference:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we have a few products—`a`, `b`, and `c`—along with the quantity sold,
    and we assign this input data to a variable named `product_data`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: When loading data manually, note the placement of square brackets to encapsulate
    the values for each column label and how the arrays must all be of the same length.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we want to load the DataFrame by calling the command using the `pd` shortcut to
    reference the `pandas` library, along with the `DataFrame()` command. We assign
    the DataFrame input data as a second variable for easier reference, called `purchase_data`.
    The `In[]` cell will look like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'To validate the results, you can run the `head()` function to display the first
    five rows of data using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'After executing the preceding code, we can see that:'
  prefs: []
  type: TYPE_NORMAL
- en: The output would look as in the following screenshot, where the individual arrays'
    by-products have been converted into a DataFrame with a labeled header row, and
    each of the quantity sold values are aligned for easy reference.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Notice that a new index column has been created to the left of `product a`
    with assigned sequential values, starting at `0`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/95c7af78-b9e4-401d-9cdb-3377f17f493d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Having the indexed values are useful for reference, but if we want to define
    them as we create the DataFrame, we can include a relevant command during its
    creation, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, if you run the `head()` command to display the results, you will see specific
    values assigned to each indexed number, which will display as in the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/02f922c7-2ff0-412a-aa5a-c4878054b2d2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To select a specific row from the DataFrame, you use the `loc` function to
    retrieve the results by index, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This will have an output, as in the following screenshot, where the individual
    values from the row assigned to `Ronny` are displayed in summary format, with
    each column and value presented by a row with a final description that includes
    the name of the index with a data type of the values (`dtype:  int64`):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9559a898-4514-4abf-876c-ecac8cb6257e.png)'
  prefs: []
  type: TYPE_IMG
- en: Once the index is labeled, you must access the `loc[]` function with the name
    in single quotes; however, you can use the `iloc[]` or `ix[]` functions to reference
    the row index by a number, with the first row starting with `0`. So, `purchase_data.iloc[0]` or
    `purchase_data.ix[0]` will both return the same results as in the preceding screenshot.
  prefs: []
  type: TYPE_NORMAL
- en: Handling essential data formats
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With a better understanding of the power of using the `pandas` library and the
    DataFrames feature, let's explore working with multiple data formats, including
    from source files such as CSV, JSON, and XML. We briefly covered these different
    file formats as part of understanding structured data in [Chapter 1](0fa7e28f-7a30-4099-9bae-30dd3c86ee4f.xhtml),
    *Fundamentals of Data Analysis*, so let's dive deep into each source file type
    and learn some essential skills when working with them.
  prefs: []
  type: TYPE_NORMAL
- en: CSV
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, we have CSV, which has been an industry standard for most of my career.
    The way to identify CSV files is typically by the `.csv` file extension; however,
    you will learn, over time, that this is not always the case, nor is the delimiter
    used to separate values always a comma within data records. CSV files are popular
    because they are portable and technologically agnostic from the source system
    that created them.
  prefs: []
  type: TYPE_NORMAL
- en: This means a CSV file could have been created with any coding language, such
    as Python, C++, or Java. Also, the same OS used to create the CSV file, such as
    Windows, Unix, Linux, or macOS, is not required to read the file. This has helped
    with its adoption for many different use cases by IT professionals because it
    helps move data between systems in and out of the organization as needed.
  prefs: []
  type: TYPE_NORMAL
- en: Because of its longevity, you will find that many different variations and standards
    have been adopted over the years. For example, records may or may not include
    a header row and the delimiter between each field/column could be a tab, a pipe
    (`|`), or any other ASCII or UTF-8 character value.
  prefs: []
  type: TYPE_NORMAL
- en: '**American Standard Code for Information Interchange** (**ASCII**) is a common
    character-encoding standard used by computers to interpret keyboard values digitally. **Unicode
    Transformation Format** (**UTF-8**) is the universal character-encoding standard
    and is backward-compatible with ASCII. Both standards are popular and commonly
    used.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some rules defined for proper CSV format, but the more you continue
    to work with them, the more you will probably find exceptions. Some of the rules
    that were published by Y. Shafranovich in *The Internet Society* (2005) include
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Each record in the CSV file should be independent and include a line break (`CRLF`)
    to identify each row.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is optional to have a line break with the last record, but some kind of **End
    of File** (**EOF**) signifier would help with reading data between systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A header row is optional but should include the same number of fields/columns
    as the corresponding record level data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each record should have a consistent delimiter, such as a comma (`,`), a semicolon
    (`;`), a pipe (`|`), or a tab.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The inclusion of double quotes between each field value is optional but is recommended
    to avoid misalignment or confusion, especially when reading in large, descriptive
    text data that includes a comma in the field value itself, such as `Also, Stacy
    enjoys watching movies`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leading and trailing spaces between each field are optional as well but should
    be used consistently throughout the entire file.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The size of a CSV file will vary but can be quite large and is dependent on
    the density of the data (the number of distinct values, the number of rows, and
    the number of fields).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Depending on the OS, such as Linux, a CSV would only include a **line feed**
    (**LF**) and not a **carriage return** (**CR**) for each row.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following screenshot, I have included a few examples of samples rows
    within a CSV file that all include the exact same information but using different
    formats to delimit the fields within the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f0e69553-5f93-426b-85b9-08d49d9dba47.png)'
  prefs: []
  type: TYPE_IMG
- en: One big advantage of consuming or producing CSV over any other format is the
    capability to share between other tools used for data analysis. For example, spreadsheet
    solutions such as Excel can easily read a CSV file without the need to convert
    the file or use a third-party extension. However, a disadvantage is the loss of
    defined data types for each column, which could lead to misrepresenting values
    in your analysis. For example, a value in a column of `1` or `0` could represent
    a Boolean flag or a user hit count from a website.
  prefs: []
  type: TYPE_NORMAL
- en: XML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The XML file format was introduced as a standard format in the 1990s. I still
    remember how early on in my career XML was proposed as a replacement to CSV or
    even to the use of databases as a data repository. XML is flexible as a solution
    for developers to create web applications and, similar to CSV, is used to move
    data between systems in and out of the organization. XML is open source and has
    a defined standard that is maintained by the **World Wide Web Consortium** (**WC3**)
    organization. Some key characteristics of XML file format are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: It is most commonly identified with a file extension of `.xml`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first line should include a declaration with encoding details and the `.xml`
    version, such as `<?xml version = "1.0" encoding="UTF-8" ?>`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It uses tags around each element that are similar to HTML tag code, using a
    beginning tag of `<` and `>` or `/>`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It contains elements, which are the defined fields or columns of the structured
    data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It contains attributes, which are the data values within each defined element.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is optional but recommended to include a **Document Type Definition** (**DTD**),
    which provides details and helps define how the elements should be used, along
    with the data types
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A sample XML file is shown in the following screenshot. Here, I converted `evolution_of_data_analysis.csv`
    into XML format and displayed a few sample records:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4470b4ad-7061-4f18-bfbe-e621d0b9ddcb.png)'
  prefs: []
  type: TYPE_IMG
- en: While a disadvantage of XML is a larger file size, due to adding tags and definitions
    to each element, an advantage of using the XML format is the ability to support
    data hierarchies and a defined schema. Let's break down those two points.
  prefs: []
  type: TYPE_NORMAL
- en: Data hierarchy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data hierarchies are defined and consistent groupings of data fields or records.
    The hierarchy can be obvious—for example, a son has a father and a mother—but
    from a data perspective, that relationship must be defined. In XML file format,
    you use a concept called an XML tree. The tree is defined by elements within the
    XML file that have a defined relationship. In the case of the `Evolution of Data
    Analysis.xml` file, each milestone has the details grouped together. Now, we can
    easily identify that the milestone event of `John von Neumann / array` was created
    in 1945, along with the rest of the supporting elements that are tagged, such
    as `<Decade>`, `<Milestone Title>`, `<Milestone Event>`, `<Why Important>`, `<Reference>`,
    and `<People Process or Technology Tag>`. This hierarchy relationship is commonly
    known as a **parent-child** relationship, where each indented element is a child to
    the *parent* element, `Evolution_of_Data_Milestone`.
  prefs: []
  type: TYPE_NORMAL
- en: Defined schema
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A defined schema means the data elements will also include metadata (data about
    the data) to help with the conformity of each element and attribute. This concept
    was required in most RDBMSes, but XML offers the concept of a DTD file to be included
    with one or more XML files. The file extension is `.xsd`, and it should complement
    each XML file.
  prefs: []
  type: TYPE_NORMAL
- en: The contents of the XSD file can be complex and very dense, depending on the
    complexity of records found in the XML file and the need to define a rigid structure
    when consuming the XML data. For example, a defined data type for each element
    would help you to better understand how to use the data during analysis. For example,
    say with `type="xs:decimal"` you know the attribute value in each element *must*
    contain numeric values and any text values should *not* exist. Another useful
    schema definition would be the definition for an element of `use="required"`,
    which means specific elements must always have a value and should *not* contain
    any null/empty attributes.
  prefs: []
  type: TYPE_NORMAL
- en: There are more details on this topic available on the W3C website, which you
    can find in the *Further reading* section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: JSON
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: JSON is another open source file standard for the communication of data between
    systems. It was created by Douglas Crockford around 2001 to improve communication
    between computers and web browsers using a concept called **stateless**. This
    means your computer's web browser, which is known as the **client**, doesn't have
    to wait for the **server** to respond, and vice versa. This is also known as **Representational
    State Transfer** (**REST**) architecture and is very common in web, API, and modern
    technologies because it scales to support millions of concurrent users.
  prefs: []
  type: TYPE_NORMAL
- en: Once REST became a popular web architecture, the need to find a faster and more
    efficient communication protocol drove the adoption of JSON data, which can either
    be streamed or persisted as a file. Having many websites that use JavaScript and
    a JavaScript-friendly notation also increased JSON's popularity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to XML and CSV, JSON is readable by humans as well as many different
    types of computer systems and languages, such as Python. This also means JSON
    is not a binary file format, which means it does not require the file to be compiled
    for use by a computer. I included JSON as a milestone in the *"The evolution of
    data analysis and why it is important"* section in [Chapter 1](0fa7e28f-7a30-4099-9bae-30dd3c86ee4f.xhtml),
    *Fundamentals of Data Analysis,* because of its contributions to advancing how
    we communicate using data. A sample JSON format is shown in the following screenshot,
    which is very similar to the XML format sample from the previous *XML* section
    because you now see the data organized and grouped by record using curly brackets
    (`{` and `}`) to encapsulate each row from the original CSV file. Each grouping
    using curly brackets is identified as an object:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/311002fb-5349-4cbf-870e-70a3544c673b.png)'
  prefs: []
  type: TYPE_IMG
- en: One important concept to understand with JSON data is that it evolved from XML
    but streamlines many of the complexities that could exist in XML formats. Like
    XML, it benefits from the ability to define a data hierarchy and includes a defined
    schema that supports a concept called **schema on read**.
  prefs: []
  type: TYPE_NORMAL
- en: In traditional solutions that have a defined schema, the producer was forced to
    establish a schema before any data is loaded or transferred between systems. This
    process required expertise and extra steps during data ingestion and delayed the
    delivery of data to the consumer. With JSON and the concept of schema on read,
    the producer can send over the data along with all the metadata at the same time.
    All the details, such as field names, data types (`dtype`) for each field, and,
    in some cases, a full data dictionary, will be included. Providing this level
    of detail helps the consumer of the data to better understand the relationships
    within each element and attribute.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will find, in a lot of JSON-formatted data, the concept of `name: value`
    pairs, which are also used in the example in the previous screenshot. This concept
    allows values to be assigned within the identification of the field within each
    record while still maintaining the hierarchy, rather than breaking out the records
    across multiple rows. Each field name is identified to the left of the colon (`:`)
    and the value is found to the right.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Each `name: value` relationship is separated by a comma and many examples will
    have a unique record identity, which helps with carrying out analysis on one-to-many
    relationships. So, you can nest many different relationships deep within a JSON
    structure and still have a way to identify which record the `name: value` pair
    belongs. If an array of values are required to be stored in a JSON file, they
    use square brackets (`[` and `]`) to define the list of values.'
  prefs: []
  type: TYPE_NORMAL
- en: Defining a schema forces the data to have controls and context beyond what is
    observed. It removes assumptions about the data attributes and helps with interpreting
    how the data values should be used for analysis. For example, a value of `20191219`
    could be easily understood to be an integer value or could be the representation
    of the `12/19/2019` date with the format stored as `YYYYMMDD`. Without having
    a defined schema to reference, along with details about how and why that field
    is supposed to be used, your analysis of the data could be flawed.
  prefs: []
  type: TYPE_NORMAL
- en: Data dictionaries and data types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout the book, I will continue to re-enforce the need to have a data dictionary
    to help with the analysis of data. As with any data we have uncovered so far,
    a data dictionary will come in all shapes and sizes. This means it could be documented
    outside the source file, which is common on a help page, a wiki, or a blog or
    within the source data, as we discussed with XML and JSON files.
  prefs: []
  type: TYPE_NORMAL
- en: Having the data defined and documented will aid you in the journey to understand
    it but will not be the only method required to become a domain expert for a dataset.
    Domain expertise comes from experience with understanding how the data is used,
    along with the business or purpose behind the underlining source data. We covered
    some of these concepts in [Chapter 1](0fa7e28f-7a30-4099-9bae-30dd3c86ee4f.xhtml),
    *Fundamentals of Data Analysis*, looking at how **Know Your Data** (**KYD**) and
    having a data dictionary available aids in the effort to learn more about the
    underlying dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The analysis of data and the KYD concept should be applied throughout the process
    of analyzing data, so be sure to check the numbers and verify that the results
    match up to how the data is defined to build trust and confidence in your insights.
  prefs: []
  type: TYPE_NORMAL
- en: Data dictionaries are common for legacy systems, RDBMS, and traditional **Enterprise
    Data Warehouses** (**EDW**). It is common to have a data catalog available and,
    in many cases, they are required to build communication data pipelines between
    different systems. In some cases, a data dictionary is required as part of regulatory
    requirements or as part of a governed corporate policy.
  prefs: []
  type: TYPE_NORMAL
- en: In modern systems, **Application Programming Interfaces** (**APIs**) have become
    the central repository for metadata and the de facto data dictionary because JSON
    is a popular communication vehicle where the schema is defined and should be well
    documented. However, in practice, I find that documentation is written for programmers
    by programmers, so it may not meet all the needs to fully understand the data
    and answer all the business questions during analysis.
  prefs: []
  type: TYPE_NORMAL
- en: It is also common to version a data dictionary as part of a **Master Data Management**
    (**MDM**) or data governance solution. Within these versions, you will uncover
    details behind the *what* and the *why* for the data. For example, a field may
    be defined as inactive but still available, so it becomes sparsely populated because
    the application/system used to populate it changed.
  prefs: []
  type: TYPE_NORMAL
- en: Having that level of detail may help to identify data gaps or to better understand
    how to build a data bridge by combining values from two different fields at different
    periods of time for accurate historical analysis. I worked with a client once
    who was replacing a large enterprise legacy system, which cost millions of dollars,
    with consulting hardware and software. The consulting time was calculated by the
    hour, with dozens of specialists traveling every week to the client site.
  prefs: []
  type: TYPE_NORMAL
- en: There was a pivotal moment in the project where it was determined infeasible
    and too costly to migrate all the legacy supply chain, accounting, and HR details
    from the old system to the new one. To avoid delays, we proposed an analytics
    solution where both the legacy system data and the new system data were merged
    together daily. A rolling window of time logic was built in so that after 7 years,
    the legacy data would no longer be used for analysis, but during that timeframe,
    a blend of both systems, which included different fields and records, would be
    presented for analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Having a data dictionary was a must for this type of solution, and providing
    additional documentation was required to ensure the audience understood where
    the source of the data came from depending on the time period of the reporting
    and analysis. Part of that documentation required details behind the different
    fields and variations in the data types. Some systems will allow a mix of different
    data types or, as in Python, will default to specific data types.
  prefs: []
  type: TYPE_NORMAL
- en: Just remember that you may need to convert a data type between multiple sources,
    especially when blending between different systems and file formats. For example,
    in JSON, a number defined as `real` would be called `float` in Python. If you
    run into issues with converting data types during the loading of data, you may
    need to go back to the data source provider and request it to be resent in a format
    that would be easier to consume.
  prefs: []
  type: TYPE_NORMAL
- en: As you continue increasing your data literacy, you need to understand that different
    technologies and data formats will lead to different data types, which will require
    translation to ensure accurate analysis of the data, especially from multiple
    sources.
  prefs: []
  type: TYPE_NORMAL
- en: Creating our first DataFrame
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we begin with some hands-on examples, some useful commands to run in
    `pandas` are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pd.read_csv(‘inport_filename.csv'', header=1)`: Reads data from a CSV file
    directly into a `pandas` DataFrame'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`my_df.to_csv(‘export_filename'')`: Directly exports the DataFrame to a CSV
    file to your workstation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`my_df.shape`: Provides the number of rows and columns of your DataFrame'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`my_df.info()`: Provides metadata about your DataFrame, including data types
    for each column'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`my_df.describe()`: Includes statistical details with a column that includes
    the count, mean, **standard deviation** (**std**), minimum, maximum, and percentiles (25th,
    50th, and 75th) for any numeric column'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`my_df.head(2)`: Displays the first two records from the DataFrame'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`my_df.tail(2)`: Displays the last two records from the DataFrame'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`my_df.sort_index(1)`: Sorts by the labels along an axis—in this example, by
    the column label headers alphabetically from left to right'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`my_df.isnull()`: Displays a list of all rows with a `True`/`False` indicator
    if any of the values by column are null'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Our first example will load data from a CSV file into a `pandas` DataFrame
    that has a pipe (`|`) delimiter and will run some of the preceding commands:'
  prefs: []
  type: TYPE_NORMAL
- en: Launch Jupyter and create a new Python notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To stay consistent with the best practices, be sure to rename the notebook `exploring_the_pandas_library` before
    moving forward.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Type `import pandas as pd` into the `In []:` cell.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the cell. No output will be displayed after you run the cell.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Type `my_df = pd.read_csv('evolution_of_data_analysis.csv', header=0, sep="|")` into
    the next `In []:` cell.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the cell. No output will be displayed after you run the cell.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Type `my_df.shape` into the next `In []:` cell.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Verify that the output cell displays `Out []`. `(42, 7)` will be displayed,
    which tells you that there are 42 rows and 7 columns, as in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/69507b5f-7127-4047-a8e2-8aa796b8335e.png)'
  prefs: []
  type: TYPE_IMG
- en: Type `my_df.info()` into the next `In []:` cell.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the cell.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Verify that the output cell displays `Out []`. There will be multiple rows,
    including data types for all seven columns, as in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/83180595-25be-40d2-8f20-cb4c3336e5e6.png)'
  prefs: []
  type: TYPE_IMG
- en: Type `my_df.describe()` into the next `In []:` cell.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the cell.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Verify that the output cell displays `Out []`. There will be multiple rows
    of output, with one column with a header of `Year`, as in the following screenshot.
    Statistical values from the `Year` field will be displayed, including `count`,
    `mean`, and `max`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/7ae6f147-9d56-4fc6-a33f-c37987100ff2.png)'
  prefs: []
  type: TYPE_IMG
- en: Type `my_df.head(2)` into the next `In []:` cell and run the cell.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Verify that the output cell displays `Out []`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The output should include an index in the first column with a starting row of
    `0`, as in the following screenshot.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All seven columns will be displayed, along with the first two rows from the
    source file:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/b26064fe-3ea5-4f38-8294-1e5af795a209.png)'
  prefs: []
  type: TYPE_IMG
- en: Type `my_df.tail(2)` into the next `In []:` cell and run the cell.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Verify that the output cell displays `Out []`. The output should include an
    index in the first column with a starting row of `40`, as in the following screenshot.
    All seven columns will be displayed, along with the last two rows from the source
    file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/df2d7257-fa2f-437f-9542-8250dd22ddf8.png)'
  prefs: []
  type: TYPE_IMG
- en: Type `my_df.sort_index(1)` into the next `In []:` cell and run the cell.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Verify that the output cell displays `Out []`. The output should include an
    index in the first column with a starting row of `0`, as in the following screenshot.
    All seven columns will be displayed, but the order of the columns has changed
    to alphabetically sort from left to right, starting with `Decade` and ending with
    `Year`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/aaaa3dee-104e-47de-b3d8-568c3954f2d6.png)'
  prefs: []
  type: TYPE_IMG
- en: In the next example, let's answer a few business questions from the data by
    exploring some of the features available in `pandas`. The first question is *how
    many milestone events occurred by decade?* To answer this question, we need to
    use the `groupby` feature, so let's go through the steps to provide the answer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps to reproduce this example are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Launch Jupyter and create a new Python notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To stay consistent with the best practices, be sure to rename the notebook `exploring_the_pandas_library_example_2`
    before moving forward.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Type `import pandas as pd` into the `In []:` cell and run the cell.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Type `my_df = pd.read_csv('evolution_of_data_analysis.csv', header=0, sep="|")` into
    the next `In []:` cell and run the cell.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Type `my_df.head(2)` into the next `In []:` cell and run the cell.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Verify that the output cell displays `Out []`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The output should include an index in the first column with a starting row of
    `0`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All seven columns will be displayed, along with the first two rows from the
    source file.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Type `my_df.groupby(['Decade']).agg({'Year':'count'})`into the `In []:` cell
    and run the cell.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Verify that the output cell displays `Out []`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The output will display 10 rows of data with 2 columns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The header row in the first column will be `Decade` and will be `Year` for the
    second column.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The results will match the following screenshot:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/f40d7cae-2447-491e-9f33-f428151156e7.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding screenshot, we followed the previous steps to load the CSV
    file as a DataFrame named `my_df`. To verify that the DataFrame loaded correctly,
    we ran the `head()` function and included the parameter of `2` to limit the number
    of rows displayed in the notebook. The last command is to run `groupby` against
    the `Decade` column and combine it with an aggregation to count the values from
    the `Milestone Event` field/column. We can now answer some questions about this
    dataset, such as that 14 milestone events occurred during the 2000s or that the
    first decade to have any milestone events was the 1940s because that is the first
    row that has any values.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Congratulations, you have now created your first DataFrame using the `pandas`
    library! We started the chapter by introducing you to the concepts of structured
    tabular data and the different techniques available to manipulate it by transposing
    and pivoting the data. More importantly, we discussed the importance of why data
    should be in tabular form. We then introduced the `pandas` library and defined
    a DataFrame, and demonstrated the many benefits of this powerful feature that
    are available for you during data analysis. In the handling of essential data
    formats, we went through the different data formats available by going through
    the details of the CSV, XML, and JSON file formats. Before we ended the chapter
    by creating our first DataFrame, we discussed the importance of data dictionaries
    and how different data types improve your data literacy, as well as why they are
    important before, during, and after the data analysis workflow has completed.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, [Chapter 5](bea8a62c-4469-47e3-a668-10fbb91815ea.xhtml), *Gathering
    and Loading Data in Python*, we will introduce you to how to load data from databases
    using SQL and continue working with the features available in `pandas` and DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: McKinney, W., *Data Structures for Statistical Computing in Python*, *Proceedings
    of the 9th Python in Science Conference*, Vol. 445 (2010)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Torres-Reyna, O., *Panel Data Analysis Fixed and Random Effects using Stata*
    (v. 4.2), *Princeton.edu*, (2007), available at [https://www.princeton.edu/~otorres/Panel101.pdf](https://www.princeton.edu/~otorres/Panel101.pdf)
    [accessed 23 Dec. 2019]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**National Longitudinal Surveys** (**NLSes**) for examples of panel data: [https://www.bls.gov/nls/home.htm](https://www.bls.gov/nls/home.htm)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A definition of a `pandas` DataFrame: [https://www.geeksforgeeks.org/python-pandas-dataframe](https://www.geeksforgeeks.org/python-pandas-dataframe/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quick details about the QVD file format: [https://help.qlik.com/en-US/sense/June2019/Subsystems/Hub/Content/Sense_Hub/Scripting/work-with-QVD-files.htm](https://help.qlik.com/en-US/sense/June2019/Subsystems/Hub/Content/Sense_Hub/Scripting/work-with-QVD-files.htm)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ASCII stands: [https://www.ansi.org/about_ansi/overview/overview?menuid=1](https://www.ansi.org/about_ansi/overview/overview?menuid=1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unicode format and encoding standards: [https://home.unicode.org/](https://home.unicode.org/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CSV rules and standards: [https://tools.ietf.org/html/rfc4180](https://tools.ietf.org/html/rfc4180)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The W3C organization standards: [https://www.w3.org/](https://www.w3.org/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The REST standards: [https://www.ics.uci.edu/~fielding/pubs/dissertation/rest_arch_style.htm#sec_5_1_3](https://www.ics.uci.edu/~fielding/pubs/dissertation/rest_arch_style.htm#sec_5_1_3)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: History of Unicode: [https://docs.python.org/3.4/howto/unicode.html](https://docs.python.org/3.4/howto/unicode.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
