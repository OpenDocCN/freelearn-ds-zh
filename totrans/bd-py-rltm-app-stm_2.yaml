- en: Chapter 2. The Storm Anatomy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter gives a detailed view of the internal structure and processes
    of the Storm technology. We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Storm processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storm-topology-specific terminologies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interprocess communication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fault tolerance in Storm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guaranteed tuple processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallelism in Storm—scaling a distributed computation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As we advance through the chapter, you will understand Storm's processes and
    their role in detail. In this chapter, various Storm-specific terminologies will
    be explained. You will learn how Storm achieves fault tolerance for different
    types of failure. We will see what guaranteed message processing is and, most
    importantly, how to configure parallelism in Storm to achieve fast and reliable
    processing.
  prefs: []
  type: TYPE_NORMAL
- en: Storm processes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will start with Nimbus first, which is actually the entry-point daemon in
    Storm. Just to compare with Hadoop, Nimbus is actually the job tracker of Storm.
    Nimbus's job is to distribute code to all supervisor daemons of a cluster. So,
    when topology code is submitted, it actually reaches all physical machines in
    the cluster. Nimbus also monitors failure of supervisors. If a supervisor continues
    to fail, then Nimbus reassigns those workers' jobs to other workers of a different
    physical machine. The current version of Storm allows only one instance of the
    Nimbus daemon to run. Nimbus is also responsible for assigning tasks to supervisor
    nodes. If you lose Nimbus, the workers will still continue to compute. Supervisors
    will continue to restart workers as and when they die. Without Nimbus, a worker's
    task won't be reassigned to another machine worker within the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: There is no alternative Storm process that will take over if Nimbus dies, and
    no process will even try to restart it. There is nothing to worry about, however,
    since it can be restarted anytime. In a production environment, alerts can also
    be set when Nimbus dies. In future, we may see highly available Nimbus.
  prefs: []
  type: TYPE_NORMAL
- en: Supervisor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A supervisor manages all the workers of the respective machine. Distributed
    computation in Storm is possible due to the supervisor daemon, as there is one
    supervisor per machine in your cluster. The supervisor daemon listens for the
    work assigned by Nimbus to the machine that it runs, and distributes it among
    workers. Due to any runtime exception, workers can die anytime, and the supervisor
    restarts them when there is no heartbeat from dead workers. Each worker process
    executes a part of a topology. Similar to the Hadoop ecosystem, supervisor is
    a task tracker of Storm. It tracks the tasks of workers of the same machine. The
    maximum number of possible workers depends on the number of ports defined in `storm.yaml`.
  prefs: []
  type: TYPE_NORMAL
- en: Zookeeper
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to its own components, Storm relies on a Zookeeper cluster (one
    or more Zookeeper servers) to perform the coordination job between Nimbus and
    the supervisors. Apart from using Zookeeper for coordination purposes, Nimbus
    and the supervisors also store all their states in Zookeeper, and Zookeeper stores
    them on a local disk where it is running. Having more than one Zookeeper daemon
    increases the reliability of the system, because if one daemon goes down, another
    becomes the leader.
  prefs: []
  type: TYPE_NORMAL
- en: The Storm UI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Storm is also equipped with a web-based user interface. It should be started
    on a machine that also runs Nimbus. The Storm UI provides a report of the entire
    cluster, such as the sum of all active supervisor machines, the total number of
    workers available, allotted to each topology and how many remaining, and topology-level
    diagnostics such as tuples stats (how many tuples were emitted, and the ACK between
    spout to bolt or bolt to bolt). The Storm UI also shows the total number of workers,
    which is actually sum of all workers available of all supervisors' machines.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows a sample screen of the Storm UI:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The Storm UI](img/B03471_02_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Following is the explanation of Storm UI:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Topology stats**: Under **Topology stats**, you can click and see the stats
    of the last 10 minutes, 3 hours, or all time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spouts (All time)**: This displays the number of executors and tasks assigned
    for this spout, along with the stats of emitted tuples and other latency stats.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bolts (All time)**: This displays a list of all bolts, along with the assigned
    executors/tasks. When you are doing performance tuning, keep the **Capacity**
    column close to `1`. In the preceding example for **aggregatorBolt**, it is `1.500`,
    so instead of `200` executors/tasks, we can use `300`. The **Capacity** column
    helps us decide the right degree of parallelism. The idea is very simple; if the
    **Capacity** column reads more than `1`, try increasing the executors and tasks
    in the same ratio. If the value of executors/tasks is high and the **Capacity**
    column is close to zero, try reducing the number of executors/tasks. You can do
    this until you get the best configuration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storm-topology-specific terminologies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A topology is a logical separation of programming work into many small-scale
    processing units called spout and bolt, which is similar to MapReduce in Hadoop.
    A topology can be written in many languages, including Java, Python, and lot more
    supported languages. In visual depictions, a topology is shown as a graph of connecting
    spouts and bolts. Spouts and bolts execute tasks across the cluster. Storm has
    two modes of operation, called local mode and distributed mode:'
  prefs: []
  type: TYPE_NORMAL
- en: In local mode, all processes of Storm and workers run within your code development
    environment. This is good for testing and development of topologies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In distributed mode, Storm operates as a cluster of machines. When you submit
    topology code to the Nimbus, Nimbus takes care of distributing the code and allocating
    workers to run your topology based on your configuration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following figure, we have purple bolts; these receive a tuple or records
    from the spout above them. A tuple supports most of the data types available in
    the programming language in which the topology code is being written. It flows
    as an independent unit from a spout to a bolt or a bolt to another bolt. An unbounded
    flow of tuples is called a stream. In a single tuple, you can have many key-value
    pairs to pass together.
  prefs: []
  type: TYPE_NORMAL
- en: The next figure illustrates streams in more detail. A spout is connected to
    a source of tuples and generates continuous tuples for the topology as a stream.
    What you emit from the spout as a key-value pair can be received by the bolt using
    the same key.
  prefs: []
  type: TYPE_NORMAL
- en: '![Storm-topology-specific terminologies](img/B03471_02_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The worker process, executor, and task
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Storm distinguishes between the following three main entities, which are used
    to actually run a topology in a Storm cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: Worker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Executor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Task
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s say we have decided to keep two workers, one spout executor, three **Bolt1**
    executors, and two **Bolt2** executors. Assume that the ratio of the number of
    executors and tasks is the same. The total sum of executors is six for spout and
    bolt. Out of six executors, some will run within the scope of worker 1, and some
    will be in control of worker 2; this decision is taken by the supervisor. This
    is explained in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The worker process, executor, and task](img/B03471_02_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The next figure explains the position of the workers and executors within the
    scope of the supervisor that is running on a machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '![The worker process, executor, and task](img/B03471_02_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The number of executors and tasks is set while building the topology code. In
    the preceding figure, we have two workers (1 and 2), run and managed by the supervisor
    of that machine. Assume that **Executor 1** is running one task, because the ratio
    of executors to tasks is the same (for example, 10 executors means 10 tasks, which
    makes the ratio 1:1). But **Executor 2** is running two tasks sequentially, so
    the ratio of tasks to executors is 2:1 (for example, 10 executors means 20 tasks,
    which makes the ratio 2:1). Having more tasks never means higher processing speed,
    but this is true for more executors, as tasks run sequentially.
  prefs: []
  type: TYPE_NORMAL
- en: Worker processes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A single worker process executes a portion of a topology and runs on its own
    JVM. Workers are allocated during topology submission. A worker process is linked
    to a specific topology and can run one or more executors for one or more spouts
    or bolts of that topology. A running topology consists of many such workers running
    on many machines within a Storm cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Executors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An executor is a thread run within the scope of a worker's JVM. An executor
    may run one or more tasks for a spout or bolt sequentially.
  prefs: []
  type: TYPE_NORMAL
- en: 'An executor always runs on one thread for all its tasks, which means that tasks
    run serially on an executor. The number of executors can be changed after the
    topology has been started without shutdown, using the `rebalance` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A task performs data processing and runs within its parent executor's thread
    of execution. The default value of the number of tasks is the same as the number
    of executors. While building the topology, we can keep a higher number of tasks
    as well. It can help to increase the number of executors in the future, which
    keeps the scope of scaling open. Initially, we can have 10 executors and 20 tasks,
    so the ratio is 2:1\. This means two tasks per executor. A future rebalancing
    action can make 20 executors and 20 tasks, which will make the ratio 1:1.
  prefs: []
  type: TYPE_NORMAL
- en: Interprocess communication
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following figure illustrates communication between the Storm submitter (client),
    the Nimbus thrift server, Zookeeper, supervisors, workers of supervisors, executors,
    and tasks. Each worker process runs as a separate JVM.
  prefs: []
  type: TYPE_NORMAL
- en: '![Interprocess communication](img/B03471_02_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: A physical view of a Storm cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The next figure explains the physical position of each process. There can be
    only one Nimbus. However, more than one Zookeeper is there to support failover,
    and per machine, there is one supervisor.
  prefs: []
  type: TYPE_NORMAL
- en: '![A physical view of a Storm cluster](img/B03471_02_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Stream grouping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A stream grouping controls the flow of tuples between from spout to bolt or
    bolt to bolt. In Storm, we have four types of groupings. Shuffle and field grouping
    are most commonly used:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Shuffle grouping**: Tuple flow between two random tasks in this grouping'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Field grouping**: A tuple with a particular field key is always delivered
    to the same task of the downstream bolt'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**All grouping**: Sends the same tuple to all tasks of the downstream bolt'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Global grouping**: Tuples from all tasks reach one task'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The subsequent figure gives a diagrammatic explanation of all the four types
    of groupings:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Stream grouping](img/B03471_02_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Fault tolerance in Storm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Supervisor runs a synchronization thread to get assignment information (what
    part of topology I am supposed to run) from Zookeeper and write to the local disk.
    This local filesystem information helps keep the worker up to date:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Case 1**: This is the ideal case for most of the times. When the cluster
    works normally, the worker''s heartbeat goes back to the supervisors and Nimbus
    via Zookeeper.![Fault tolerance in Storm](img/B03471_02_08.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Case 2**: If a supervisor dies, processing still continues, but the assignment
    is never synchronized. Nimbus will reassign the work to another supervisor of
    a different machine. Those workers will be running, but will not receive any new
    tuples. Do set an alert to restart the supervisor or use a Unix tool that can
    restart the supervisor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Case 3**: If Nimbus dies, the topologies will continue to function normally.
    Processing will still continue, but topology life cycle operations and reassigning
    to another machine will not be possible.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Case 4**: If a worker dies (as the heartbeat stops arriving), the supervisor
    will try to restart the worker process and processing will continue. If a worker
    dies repeatedly, Nimbus will reassign the work to other nodes in the cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guaranteed tuple processing in Storm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As Storm is already equipped to deal with various process-level failures, another
    important feature is the ability to deal with failure of tuples that occurs when
    a worker dies. This is just to give an idea of bitwise XOR: the XOR of two sets
    of the same bits is 0\. This is called XOR magic, and it can help us know whether
    the delivery of a tuple to the next bolt is successful or not. Storm uses 64 bits
    to track tuples. Every tuple gets a 64-bit tuple ID. This 64-bit ID, along with
    the task ID, is kept at ACKer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next figure, ACKing and a replay case is explained:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Guaranteed tuple processing in Storm](img/B03471_02_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: XOR magic in acking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A spout tuple is not fully processed until all the tuples in the linked tuple
    tree are completed. If the tuple tree is not completed within a configured timeout
    (the default value is `topology.message.timeout.secs: 30`), the spout tuple is
    replayed.'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, the first acker gets `10101` (for simplicity of explanation,
    we are keeping 5 bits) for tuple 1 from the spout. Once **Bolt 1** receives the
    same tuple, it also ACK to acker. From both sources, acker gets `10101`. This
    means `10101` XOR `10101 = 0`. Tuple 1 is successfully received by **Bolt 1**.
    The same process repeats between bolts 1 and 2\. At last, **Bolt 2** sends ack
    to acker, and the tuple tree is completed. This creates a signal to call the spout's
    `success` function. Any failure in tuple processing can trigger the spout's `fail`
    function call, which gives an indication to send the tuple back for processing
    again.
  prefs: []
  type: TYPE_NORMAL
- en: Storm's acker tracks the completion of the tuple tree by performing XOR between
    the sender's tuple and the receiver's tuple. Each time a tuple is sent, its value
    is XORed into the checksum maintained by acker, and each time a tuple is acked,
    its value is XORed in again at acker.
  prefs: []
  type: TYPE_NORMAL
- en: If all tuples have been successfully acked, the checksum will be zero. Ackers
    are system-level executors.
  prefs: []
  type: TYPE_NORMAL
- en: In the spout, we have a choice of two emit functions.
  prefs: []
  type: TYPE_NORMAL
- en: '`emit([tuple])`: This is a simple emit'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`storm.emit([tuple], id=the_value)`: This creates a reliable spout, but only
    if you can re-emit a tuple using `the_value`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the Spout, we also have two ACK functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`fail(the_value)`: This function is called when a timeout occurs or the tuple
    fails'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ack(the_value)`: This function is called when the last bolt of the topology
    ACK the tuple tree'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This ID field should be a random and unique value to replay from the spout's
    `fail` function. Using this ID, we can re-emit it from the `fail` function. If
    successful, the jn `success` function will call and it can remove successful tuples
    from the global list or recreate from the source.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will be able to recreate the same tuple if you have a reliable spout in
    the topology. To create a reliable spout, emit a unique message ID (`the_value`)
    from the spout''s next tuple function along with the tuple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Whether a tuple is not ACKed within a configured period of time, or the programming
    code fails a tuple due to some error condition, both are valid cases of replay.
  prefs: []
  type: TYPE_NORMAL
- en: When the `fail` function is called, the code can read from the source of the
    spout using the same message ID, and when the `success` function is called, an
    action such as removing a message from the queue can be taken.
  prefs: []
  type: TYPE_NORMAL
- en: The message ID is an application-specific key that can help you recreate a tuple
    and emit it back from the spout. An example of a message ID can be a queue message
    ID, or a primary key of a table. A tuple is considered failed if a timeout occurs
    or due to any other reason.
  prefs: []
  type: TYPE_NORMAL
- en: Storm has a fault tolerance mechanism that guarantees at-least-once processing
    for all tuples emitted only from a reliable spout.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have a reliable spout in place, you can make the bolt do the linking
    between the input and output tuples, which creates a tuple tree. Once a tuple
    tree is established, acker knows any failure in the linked tree, and the original
    message ID is used to create the entire tuple tree again.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the bolt, there are two functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`emit([tuple])`: There is no tuple tree linking. We can''t track which original
    message ID was used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`storm.emit([tuple], anchors=[message_key])`: With linking in place, the original
    tuple can now be replayed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following figure explains how tuple B is generated from tuple A:'
  prefs: []
  type: TYPE_NORMAL
- en: '![XOR magic in acking](img/B03471_02_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The next figure illustrates the bolt performing **ACK**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![XOR magic in acking](img/B03471_02_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following figure illustrates the failure condition, where the signal reaches
    the spout upon failure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![XOR magic in acking](img/B03471_02_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'A successful **ACK** is demonstrated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![XOR magic in acking](img/B03471_02_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following figure illustrates a condition of a big tuple tree without a
    bolt, and there is no failure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![XOR magic in acking](img/B03471_02_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The next figure demonstrates an example of failure in a tuple tree—in the middle
    of the tuple tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '![XOR magic in acking](img/B03471_02_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Tuning parallelism in Storm – scaling a distributed computation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To explain parallelism of Storm, we will configure three parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: The number of workers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of executors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following figure gives a diagrammatic explanation of an example where we
    have a topology with just one spout and one bolt. In this case, we will set different
    values for the numbers of workers, executors, and tasks at the spout and bolt
    levels, and see how parallelism works in each case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Tuning parallelism in Storm – scaling a distributed computation](img/B03471_02_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: For this configuration, we will have two workers, which will run in separate
    JVMs (worker 1 and worker 2).
  prefs: []
  type: TYPE_NORMAL
- en: For the spout, there is one executor, and the default number of tasks is one,
    which makes the ratio 1:1 (one task per executor).
  prefs: []
  type: TYPE_NORMAL
- en: For the bolt, there are two executors and four tasks, which makes it 4/2 = two
    tasks per executor. These two executors run under worker 2, with each having two
    tasks, while the executor of worker 1 gets only one task.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be illustrated nicely using the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Tuning parallelism in Storm – scaling a distributed computation](img/B03471_02_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s change the configuration in the bolt to two executors and two tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This can be illustrated well here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Tuning parallelism in Storm – scaling a distributed computation](img/B03471_02_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The number of workers is two again. As the bolt has two executors and two tasks,
    that makes it 2/2, or one task per executor. Now you can see that both executors
    get one task each. In terms of performance, both cases are exactly the same, as
    the tasks run sequentially within the executor thread. More executors means a
    higher degree of parallelism, and more workers means using resources such as CPU
    and RAM more effectively. Memory allocation is done at the worker level using
    the `worker`.`childopts` setting. We should also monitor the maximum amount of
    memory a particular worker process is holding. This plays an important role in
    deciding the total number of workers. It can be seen using the `ps -ef` option.
    Always keep the tasks and executors in the same ratio, and derive the correct
    value for the number of executors using the capacity column of the Storm UI. As
    an important note, we should keep the shorter duration transaction in the bolt
    and try to tune it via splitting code into more bolts or reducing the batch size
    tuple. The batch size is the number of records received by the bolt in a single
    tuple delivery. Also, don't block the `nextTuple` method of the spout due to the
    longer holding transaction.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As this chapter approaches its end, you must have got a brief idea about the
    Nimbus, supervisor, UI, and Zookeeper processes. This chapter also taught you
    how to tune parallelism in Storm by playing with the number of workers, executors,
    and tasks. You became familiar with the important problem of distributing computation,
    that is, failures and overcoming failures by different kinds of fault tolerance
    available in the system. And most importantly, you learned how to write a "reliable"
    spout to achieve guaranteed message processing and linking in bolts.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will give you information about how to build a simple topology
    using a Python library called Petrel. Petrel addresses some limitations of Storm's
    built-in Python support, providing simpler and more streamlined development.
  prefs: []
  type: TYPE_NORMAL
