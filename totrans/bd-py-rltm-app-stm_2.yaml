- en: Chapter 2. The Storm Anatomy
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章. Storm 的解剖结构
- en: 'This chapter gives a detailed view of the internal structure and processes
    of the Storm technology. We will cover the following topics in this chapter:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章详细介绍了 Storm 技术的内部结构和过程。在本章中，我们将涵盖以下主题：
- en: Storm processes
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Storm 处理过程
- en: Storm-topology-specific terminologies
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Storm 拓扑特定的术语
- en: Interprocess communication
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进程间通信
- en: Fault tolerance in Storm
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Storm 中的容错性
- en: Guaranteed tuple processing
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保证元组处理
- en: Parallelism in Storm—scaling a distributed computation
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Storm 中的并行性——扩展分布式计算
- en: As we advance through the chapter, you will understand Storm's processes and
    their role in detail. In this chapter, various Storm-specific terminologies will
    be explained. You will learn how Storm achieves fault tolerance for different
    types of failure. We will see what guaranteed message processing is and, most
    importantly, how to configure parallelism in Storm to achieve fast and reliable
    processing.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们进入本章，你将详细了解 Storm 的处理过程及其在细节中的作用。在本章中，将解释各种与 Storm 相关的专业术语。你将学习 Storm 如何实现不同类型故障的容错性。我们将探讨什么是保证消息处理，最重要的是，如何配置
    Storm 的并行性以实现快速和可靠的处理。
- en: Storm processes
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Storm 处理过程
- en: We will start with Nimbus first, which is actually the entry-point daemon in
    Storm. Just to compare with Hadoop, Nimbus is actually the job tracker of Storm.
    Nimbus's job is to distribute code to all supervisor daemons of a cluster. So,
    when topology code is submitted, it actually reaches all physical machines in
    the cluster. Nimbus also monitors failure of supervisors. If a supervisor continues
    to fail, then Nimbus reassigns those workers' jobs to other workers of a different
    physical machine. The current version of Storm allows only one instance of the
    Nimbus daemon to run. Nimbus is also responsible for assigning tasks to supervisor
    nodes. If you lose Nimbus, the workers will still continue to compute. Supervisors
    will continue to restart workers as and when they die. Without Nimbus, a worker's
    task won't be reassigned to another machine worker within the cluster.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先从 Nimbus 开始，它是 Storm 的实际入口点守护进程。为了与 Hadoop 进行比较，Nimbus 实际上是 Storm 的作业跟踪器。Nimbus
    的任务是向集群中所有监督守护进程分发代码。因此，当拓扑代码提交时，它实际上会到达集群中的所有物理机器。Nimbus 还监控监督守护进程的故障。如果一个监督守护进程持续失败，那么
    Nimbus 将将这些工作者的工作重新分配给不同物理机器上的其他工作者。当前版本的 Storm 只允许运行一个 Nimbus 守护进程实例。Nimbus 还负责将任务分配给监督节点。如果你丢失了
    Nimbus，工作者仍然会继续计算。监督者将继续在工作者死亡时重启它们。没有 Nimbus，工作者的任务不会被重新分配到集群中的另一台机器上的工作者。
- en: There is no alternative Storm process that will take over if Nimbus dies, and
    no process will even try to restart it. There is nothing to worry about, however,
    since it can be restarted anytime. In a production environment, alerts can also
    be set when Nimbus dies. In future, we may see highly available Nimbus.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 Nimbus 死亡，没有其他 Storm 进程会接管，也没有任何进程会尝试重启它。然而，无需担心，因为它可以随时重启。在生产环境中，当 Nimbus
    死亡时，也可以设置警报。在未来，我们可能会看到高可用性的 Nimbus。
- en: Supervisor
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督者
- en: A supervisor manages all the workers of the respective machine. Distributed
    computation in Storm is possible due to the supervisor daemon, as there is one
    supervisor per machine in your cluster. The supervisor daemon listens for the
    work assigned by Nimbus to the machine that it runs, and distributes it among
    workers. Due to any runtime exception, workers can die anytime, and the supervisor
    restarts them when there is no heartbeat from dead workers. Each worker process
    executes a part of a topology. Similar to the Hadoop ecosystem, supervisor is
    a task tracker of Storm. It tracks the tasks of workers of the same machine. The
    maximum number of possible workers depends on the number of ports defined in `storm.yaml`.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一个监督者管理着相应机器上的所有工作者。由于你的集群中每台机器都有一个监督者，因此 Storm 中的分布式计算是可能的。监督者守护进程监听由 Nimbus
    分配给其运行的机器的工作，并将其分配给工作者。由于任何运行时异常，工作者可以随时死亡，当没有来自死亡工作者的心跳时，监督者将重启它们。每个工作者进程执行拓扑的一部分。类似于
    Hadoop 生态系统，监督者是 Storm 的任务跟踪器。它跟踪同一机器上工作者的任务。可能的最大工作者数量取决于在 `storm.yaml` 中定义的端口号数量。
- en: Zookeeper
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Zookeeper
- en: In addition to its own components, Storm relies on a Zookeeper cluster (one
    or more Zookeeper servers) to perform the coordination job between Nimbus and
    the supervisors. Apart from using Zookeeper for coordination purposes, Nimbus
    and the supervisors also store all their states in Zookeeper, and Zookeeper stores
    them on a local disk where it is running. Having more than one Zookeeper daemon
    increases the reliability of the system, because if one daemon goes down, another
    becomes the leader.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 除了其自身组件外，Storm还依赖于Zookeeper集群（一个或多个Zookeeper服务器）来在Nimbus和supervisors之间执行协调工作。除了用于协调目的外，Nimbus和supervisors还将它们的所有状态存储在Zookeeper中，而Zookeeper将它们存储在运行它的本地磁盘上。拥有多个Zookeeper守护进程可以提高系统的可靠性，因为如果一个守护进程宕机，另一个将成为领导者。
- en: The Storm UI
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Storm UI
- en: Storm is also equipped with a web-based user interface. It should be started
    on a machine that also runs Nimbus. The Storm UI provides a report of the entire
    cluster, such as the sum of all active supervisor machines, the total number of
    workers available, allotted to each topology and how many remaining, and topology-level
    diagnostics such as tuples stats (how many tuples were emitted, and the ACK between
    spout to bolt or bolt to bolt). The Storm UI also shows the total number of workers,
    which is actually sum of all workers available of all supervisors' machines.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Storm还配备了基于Web的用户界面。它应该在运行Nimbus的同一台机器上启动。Storm UI提供了整个集群的报告，例如所有活动supervisor机器的总和、可用的总工作器数量、分配给每个拓扑的数量以及剩余数量，以及拓扑级别的诊断，如元组统计（发出了多少元组，以及spout到bolt或bolt到bolt之间的ACK）。Storm
    UI还显示了总工作器数量，这实际上是所有supervisor机器上所有可用工作器的总和。
- en: 'The following screenshot shows a sample screen of the Storm UI:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的截图显示了Storm UI的一个示例屏幕：
- en: '![The Storm UI](img/B03471_02_01.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![Storm UI](img/B03471_02_01.jpg)'
- en: 'Following is the explanation of Storm UI:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是对Storm UI的解释：
- en: '**Topology stats**: Under **Topology stats**, you can click and see the stats
    of the last 10 minutes, 3 hours, or all time.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**拓扑统计**：在**拓扑统计**下，你可以点击查看过去10分钟、3小时或所有时间的统计信息。'
- en: '**Spouts (All time)**: This displays the number of executors and tasks assigned
    for this spout, along with the stats of emitted tuples and other latency stats.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spouts（所有时间）**：此部分显示分配给此spout的执行者和任务数量，以及发出的元组和其他延迟统计信息。'
- en: '**Bolts (All time)**: This displays a list of all bolts, along with the assigned
    executors/tasks. When you are doing performance tuning, keep the **Capacity**
    column close to `1`. In the preceding example for **aggregatorBolt**, it is `1.500`,
    so instead of `200` executors/tasks, we can use `300`. The **Capacity** column
    helps us decide the right degree of parallelism. The idea is very simple; if the
    **Capacity** column reads more than `1`, try increasing the executors and tasks
    in the same ratio. If the value of executors/tasks is high and the **Capacity**
    column is close to zero, try reducing the number of executors/tasks. You can do
    this until you get the best configuration.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**螺栓（所有时间）**：此部分显示所有螺栓的列表，以及分配的执行者/任务。在进行性能调整时，请将**容量**列保持在接近`1`的位置。在**aggregatorBolt**的前一个示例中，其值为`1.500`，因此我们不需要`200`个执行者/任务，而可以使用`300`个。**容量**列帮助我们决定正确的并行度。这个想法非常简单；如果**容量**列的值超过`1`，尝试以相同的比例增加执行者和任务的数量。如果执行者/任务的数量很高，而**容量**列接近零，尝试减少执行者/任务的数量。你可以这样做，直到得到最佳配置。'
- en: Storm-topology-specific terminologies
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Storm特定的术语
- en: 'A topology is a logical separation of programming work into many small-scale
    processing units called spout and bolt, which is similar to MapReduce in Hadoop.
    A topology can be written in many languages, including Java, Python, and lot more
    supported languages. In visual depictions, a topology is shown as a graph of connecting
    spouts and bolts. Spouts and bolts execute tasks across the cluster. Storm has
    two modes of operation, called local mode and distributed mode:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 拓扑是将编程工作逻辑上划分为许多小规模处理单元的过程，称为spout和bolt，这与Hadoop中的MapReduce类似。拓扑可以用多种语言编写，包括Java、Python以及更多受支持的语言。在视觉表示中，拓扑显示为连接spout和bolt的图。Spouts和bolt在集群中执行任务。Storm有两种操作模式，称为本地模式和分布式模式：
- en: In local mode, all processes of Storm and workers run within your code development
    environment. This is good for testing and development of topologies.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本地模式下，Storm和工人的所有进程都在你的代码开发环境中运行。这对于测试和拓扑的开发来说很好。
- en: In distributed mode, Storm operates as a cluster of machines. When you submit
    topology code to the Nimbus, Nimbus takes care of distributing the code and allocating
    workers to run your topology based on your configuration.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在分布式模式下，Storm作为一个机器集群运行。当你向Nimbus提交拓扑代码时，Nimbus负责分发代码并根据你的配置分配工作者来运行你的拓扑。
- en: In the following figure, we have purple bolts; these receive a tuple or records
    from the spout above them. A tuple supports most of the data types available in
    the programming language in which the topology code is being written. It flows
    as an independent unit from a spout to a bolt or a bolt to another bolt. An unbounded
    flow of tuples is called a stream. In a single tuple, you can have many key-value
    pairs to pass together.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下图中，我们看到了紫色的bolt；这些bolt从其上方的spout接收元组或记录。元组支持在编写拓扑代码的编程语言中可用的大多数数据类型。它作为一个独立的单元从spout流向bolt或从bolt流向另一个bolt。元组的无界流动称为流。在一个元组中，你可以有多个键值对一起传递。
- en: The next figure illustrates streams in more detail. A spout is connected to
    a source of tuples and generates continuous tuples for the topology as a stream.
    What you emit from the spout as a key-value pair can be received by the bolt using
    the same key.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个图更详细地说明了流。spout连接到元组的来源，并为拓扑生成连续的元组作为流。从spout作为键值对发出的内容可以通过使用相同键的bolt接收。
- en: '![Storm-topology-specific terminologies](img/B03471_02_02.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![Storm拓扑特定术语](img/B03471_02_02.jpg)'
- en: The worker process, executor, and task
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作者进程、执行者和任务
- en: 'Storm distinguishes between the following three main entities, which are used
    to actually run a topology in a Storm cluster:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Storm区分以下三个主要实体，这些实体用于在Storm集群中实际运行拓扑：
- en: Worker
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作者
- en: Executor
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行者
- en: Task
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任务
- en: 'Let''s say we have decided to keep two workers, one spout executor, three **Bolt1**
    executors, and two **Bolt2** executors. Assume that the ratio of the number of
    executors and tasks is the same. The total sum of executors is six for spout and
    bolt. Out of six executors, some will run within the scope of worker 1, and some
    will be in control of worker 2; this decision is taken by the supervisor. This
    is explained in the following figure:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们已经决定保留两个工作者，一个**Bolt1**执行者，三个**Bolt2**执行者，以及两个**Bolt2**执行者。假设执行者数量和任务数量的比例是相同的。总共有六个执行者，分别属于spout和bolt。在六个执行者中，一些将在工作者1的范围内运行，而另一些将由工作者2控制；这个决定由管理员做出。这将在以下图中解释：
- en: '![The worker process, executor, and task](img/B03471_02_03.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![工作者进程、执行者和任务](img/B03471_02_03.jpg)'
- en: 'The next figure explains the position of the workers and executors within the
    scope of the supervisor that is running on a machine:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个图解释了在工作器在机器上运行的监督器范围内的位置：
- en: '![The worker process, executor, and task](img/B03471_02_04.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![工作者进程、执行者和任务](img/B03471_02_04.jpg)'
- en: The number of executors and tasks is set while building the topology code. In
    the preceding figure, we have two workers (1 and 2), run and managed by the supervisor
    of that machine. Assume that **Executor 1** is running one task, because the ratio
    of executors to tasks is the same (for example, 10 executors means 10 tasks, which
    makes the ratio 1:1). But **Executor 2** is running two tasks sequentially, so
    the ratio of tasks to executors is 2:1 (for example, 10 executors means 20 tasks,
    which makes the ratio 2:1). Having more tasks never means higher processing speed,
    but this is true for more executors, as tasks run sequentially.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 执行者和任务的数量在构建拓扑代码时设置。在先前的图中，我们有两个工作者（1和2），由该机器的管理员运行和管理。假设**执行者1**正在运行一个任务，因为执行者与任务的比例是相同的（例如，10个执行者意味着10个任务，使得比例为1:1）。但是**执行者2**正在顺序运行两个任务，因此任务与执行者的比例为2:1（例如，10个执行者意味着20个任务，使得比例为2:1）。拥有更多的任务并不意味着更高的处理速度，但对于更多的执行者来说，这是真的，因为任务是顺序运行的。
- en: Worker processes
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作者进程
- en: A single worker process executes a portion of a topology and runs on its own
    JVM. Workers are allocated during topology submission. A worker process is linked
    to a specific topology and can run one or more executors for one or more spouts
    or bolts of that topology. A running topology consists of many such workers running
    on many machines within a Storm cluster.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 单个工作者进程执行拓扑的一部分，并在自己的JVM上运行。工作者在拓扑提交期间分配。一个工作者进程与一个特定的拓扑相关联，并可以为该拓扑的一个或多个spout或bolt运行一个或多个执行者。一个正在运行的拓扑由许多这样的工作者组成，这些工作者在Storm集群中的许多机器上运行。
- en: Executors
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 执行者
- en: An executor is a thread run within the scope of a worker's JVM. An executor
    may run one or more tasks for a spout or bolt sequentially.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 执行器是在工作者的JVM范围内运行的线程。执行器可以为spout或bolt顺序运行一个或多个任务。
- en: 'An executor always runs on one thread for all its tasks, which means that tasks
    run serially on an executor. The number of executors can be changed after the
    topology has been started without shutdown, using the `rebalance` command:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 执行器始终为所有任务在一个线程上运行，这意味着任务在执行器上是顺序运行的。可以在拓扑启动后不关闭的情况下更改执行器的数量，使用`rebalance`命令：
- en: '[PRE0]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Tasks
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 任务
- en: A task performs data processing and runs within its parent executor's thread
    of execution. The default value of the number of tasks is the same as the number
    of executors. While building the topology, we can keep a higher number of tasks
    as well. It can help to increase the number of executors in the future, which
    keeps the scope of scaling open. Initially, we can have 10 executors and 20 tasks,
    so the ratio is 2:1\. This means two tasks per executor. A future rebalancing
    action can make 20 executors and 20 tasks, which will make the ratio 1:1.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 任务执行数据处理，并在其父执行器的执行线程中运行。任务数量的默认值与执行器的数量相同。在构建拓扑时，我们还可以保持更多的任务数量。这有助于在将来增加执行器的数量，保持扩展的开放范围。最初，我们可以有10个执行器和20个任务，所以比例是2:1。这意味着每个执行器有两个任务。未来的重新平衡操作可以将20个执行器和20个任务调整为1:1的比例。
- en: Interprocess communication
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进程间通信
- en: The following figure illustrates communication between the Storm submitter (client),
    the Nimbus thrift server, Zookeeper, supervisors, workers of supervisors, executors,
    and tasks. Each worker process runs as a separate JVM.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个图展示了Storm提交者（客户端）、Nimbus thrift服务器、Zookeeper、supervisors、supervisor的工作者、执行器和任务之间的通信。每个工作进程作为一个单独的JVM运行。
- en: '![Interprocess communication](img/B03471_02_05.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![进程间通信](img/B03471_02_05.jpg)'
- en: A physical view of a Storm cluster
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Storm集群的物理视图
- en: The next figure explains the physical position of each process. There can be
    only one Nimbus. However, more than one Zookeeper is there to support failover,
    and per machine, there is one supervisor.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个图解释了每个进程的物理位置。只有一个Nimbus。然而，为了支持故障转移，有多个Zookeeper，并且每台机器有一个supervisor。
- en: '![A physical view of a Storm cluster](img/B03471_02_06.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![Storm集群的物理视图](img/B03471_02_06.jpg)'
- en: Stream grouping
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流分组
- en: 'A stream grouping controls the flow of tuples between from spout to bolt or
    bolt to bolt. In Storm, we have four types of groupings. Shuffle and field grouping
    are most commonly used:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 流分组控制元组从spout到bolt或bolt到bolt之间的流动。在Storm中，我们有四种分组类型。洗牌和字段分组是最常用的：
- en: '**Shuffle grouping**: Tuple flow between two random tasks in this grouping'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**洗牌分组**：在此分组中两个随机任务之间的元组流'
- en: '**Field grouping**: A tuple with a particular field key is always delivered
    to the same task of the downstream bolt'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**字段分组**：具有特定字段键的元组始终被发送到下游bolt的同一任务'
- en: '**All grouping**: Sends the same tuple to all tasks of the downstream bolt'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全部分组**：将相同的元组发送到下游bolt的所有任务'
- en: '**Global grouping**: Tuples from all tasks reach one task'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全局分组**：所有任务中的元组都达到一个任务'
- en: 'The subsequent figure gives a diagrammatic explanation of all the four types
    of groupings:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个图示给出了所有四种分组类型的图解：
- en: '![Stream grouping](img/B03471_02_07.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![流分组](img/B03471_02_07.jpg)'
- en: Fault tolerance in Storm
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Storm的容错性
- en: 'Supervisor runs a synchronization thread to get assignment information (what
    part of topology I am supposed to run) from Zookeeper and write to the local disk.
    This local filesystem information helps keep the worker up to date:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 管理员运行一个同步线程从Zookeeper获取分配信息（我应该运行拓扑的哪一部分）并将其写入本地磁盘。这个本地文件系统信息有助于保持工作者的最新状态：
- en: '**Case 1**: This is the ideal case for most of the times. When the cluster
    works normally, the worker''s heartbeat goes back to the supervisors and Nimbus
    via Zookeeper.![Fault tolerance in Storm](img/B03471_02_08.jpg)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**案例1**：这是大多数情况下理想的案例。当集群正常工作时，工作者的心跳通过Zookeeper返回到supervisors和Nimbus。![Storm的容错性](img/B03471_02_08.jpg)'
- en: '**Case 2**: If a supervisor dies, processing still continues, but the assignment
    is never synchronized. Nimbus will reassign the work to another supervisor of
    a different machine. Those workers will be running, but will not receive any new
    tuples. Do set an alert to restart the supervisor or use a Unix tool that can
    restart the supervisor.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**案例2**：如果一个supervisor死亡，处理仍然继续，但分配永远不会同步。Nimbus将工作重新分配给另一台机器的不同supervisor。那些工作者将继续运行，但不会收到任何新的元组。请设置一个警报来重启supervisor或使用可以重启supervisor的Unix工具。'
- en: '**Case 3**: If Nimbus dies, the topologies will continue to function normally.
    Processing will still continue, but topology life cycle operations and reassigning
    to another machine will not be possible.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**案例 3**：如果 Nimbus 死亡，拓扑将继续正常工作。处理仍将继续，但无法执行拓扑生命周期操作或将任务重新分配到另一台机器。'
- en: '**Case 4**: If a worker dies (as the heartbeat stops arriving), the supervisor
    will try to restart the worker process and processing will continue. If a worker
    dies repeatedly, Nimbus will reassign the work to other nodes in the cluster.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**案例 4**：如果工人死亡（如心跳停止到达），主管将尝试重启工人进程，处理将继续。如果工人反复死亡，Nimbus 将将工作重新分配给集群中的其他节点。'
- en: Guaranteed tuple processing in Storm
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Storm 中保证元组处理
- en: 'As Storm is already equipped to deal with various process-level failures, another
    important feature is the ability to deal with failure of tuples that occurs when
    a worker dies. This is just to give an idea of bitwise XOR: the XOR of two sets
    of the same bits is 0\. This is called XOR magic, and it can help us know whether
    the delivery of a tuple to the next bolt is successful or not. Storm uses 64 bits
    to track tuples. Every tuple gets a 64-bit tuple ID. This 64-bit ID, along with
    the task ID, is kept at ACKer.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Storm 已经能够处理各种进程级故障，另一个重要特性是处理当工人死亡时发生的元组故障的能力。这只是为了说明位运算 XOR：两个相同位集合的 XOR
    是 0。这被称为 XOR 魔法，它可以帮助我们了解元组发送到下一个 bolt 的交付是否成功。Storm 使用 64 位来跟踪元组。每个元组都得到一个 64
    位的元组 ID。这个 64 位 ID，连同任务 ID 一起，保存在 ACKer 中。
- en: 'In the next figure, ACKing and a replay case is explained:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一图中，解释了 ACK 和重放案例：
- en: '![Guaranteed tuple processing in Storm](img/B03471_02_09.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![Storm 中保证元组处理](img/B03471_02_09.jpg)'
- en: XOR magic in acking
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: XOR 魔法在 ACK 中
- en: 'A spout tuple is not fully processed until all the tuples in the linked tuple
    tree are completed. If the tuple tree is not completed within a configured timeout
    (the default value is `topology.message.timeout.secs: 30`), the spout tuple is
    replayed.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '只有当链接的元组树中的所有元组都完成时，spout 元组才被完全处理。如果元组树在配置的超时时间内未完成（默认值为 `topology.message.timeout.secs:
    30`），则 spout 元组将被重放。'
- en: In the preceding diagram, the first acker gets `10101` (for simplicity of explanation,
    we are keeping 5 bits) for tuple 1 from the spout. Once **Bolt 1** receives the
    same tuple, it also ACK to acker. From both sources, acker gets `10101`. This
    means `10101` XOR `10101 = 0`. Tuple 1 is successfully received by **Bolt 1**.
    The same process repeats between bolts 1 and 2\. At last, **Bolt 2** sends ack
    to acker, and the tuple tree is completed. This creates a signal to call the spout's
    `success` function. Any failure in tuple processing can trigger the spout's `fail`
    function call, which gives an indication to send the tuple back for processing
    again.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，第一个 acker 从 spout 获取元组 1 的 `10101`（为了解释的简便，我们保持 5 位），一旦 **Bolt 1** 收到相同的元组，它也会向
    acker ACK。从两个来源，acker 获取 `10101`。这意味着 `10101` XOR `10101` = 0。元组 1 已成功被 **Bolt
    1** 接收。相同的过程在 bolts 1 和 2 之间重复。最后，**Bolt 2** 向 acker 发送 ACK，元组树完成。这会创建一个信号来调用
    spout 的 `success` 函数。元组处理中的任何失败都可以触发 spout 的 `fail` 函数调用，这表明需要将元组发送回再次处理。
- en: Storm's acker tracks the completion of the tuple tree by performing XOR between
    the sender's tuple and the receiver's tuple. Each time a tuple is sent, its value
    is XORed into the checksum maintained by acker, and each time a tuple is acked,
    its value is XORed in again at acker.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Storm 的 acker 通过在发送者的元组和接收者的元组之间执行 XOR 来跟踪元组树的完成。每次发送元组时，其值都会与 acker 维护的校验和
    XOR，每次 ACK 元组时，其值再次在 acker 中 XOR。
- en: If all tuples have been successfully acked, the checksum will be zero. Ackers
    are system-level executors.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果所有元组都已成功 ACK，校验和将为零。Ackers 是系统级执行器。
- en: In the spout, we have a choice of two emit functions.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在 spout 中，我们有选择两个 emit 函数。
- en: '`emit([tuple])`: This is a simple emit'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`emit([tuple])`: 这是一个简单的 emit'
- en: '`storm.emit([tuple], id=the_value)`: This creates a reliable spout, but only
    if you can re-emit a tuple using `the_value`'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`storm.emit([tuple], id=the_value)`: 这创建了一个可靠的 spout，但前提是你可以使用 `the_value`
    重新发射元组'
- en: 'In the Spout, we also have two ACK functions:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spout 中，我们也有两个 ACK 函数：
- en: '`fail(the_value)`: This function is called when a timeout occurs or the tuple
    fails'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fail(the_value)`: 当发生超时或元组失败时调用此函数'
- en: '`ack(the_value)`: This function is called when the last bolt of the topology
    ACK the tuple tree'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ack(the_value)`: 当拓扑中的最后一个 bolt ACK 元组树时调用此函数'
- en: This ID field should be a random and unique value to replay from the spout's
    `fail` function. Using this ID, we can re-emit it from the `fail` function. If
    successful, the jn `success` function will call and it can remove successful tuples
    from the global list or recreate from the source.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 此ID字段应该是一个随机且唯一的值，以便从spout的`fail`函数中重新播放。使用此ID，我们可以从`fail`函数中重新发出它。如果成功，`success`函数将被调用，并且可以从中删除成功的元组或从源处重新创建。
- en: 'You will be able to recreate the same tuple if you have a reliable spout in
    the topology. To create a reliable spout, emit a unique message ID (`the_value`)
    from the spout''s next tuple function along with the tuple:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在拓扑中有一个可靠的spout，您将能够重新创建相同的元组。要创建一个可靠的spout，请从spout的下一个元组函数中发出一个唯一的消息ID（`the_value`），并附带元组：
- en: '[PRE1]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Whether a tuple is not ACKed within a configured period of time, or the programming
    code fails a tuple due to some error condition, both are valid cases of replay.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个元组在配置的期间内没有被ACK，或者编程代码由于某些错误条件而使元组失败，这两种情况都是有效的重放案例。
- en: When the `fail` function is called, the code can read from the source of the
    spout using the same message ID, and when the `success` function is called, an
    action such as removing a message from the queue can be taken.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 当调用`fail`函数时，代码可以使用相同的消息ID从spout的源读取，当调用`success`函数时，可以采取诸如从队列中删除消息之类的操作。
- en: The message ID is an application-specific key that can help you recreate a tuple
    and emit it back from the spout. An example of a message ID can be a queue message
    ID, or a primary key of a table. A tuple is considered failed if a timeout occurs
    or due to any other reason.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 消息ID是一个应用程序特定的键，可以帮助您重新创建一个元组并将其从spout重新发出。消息ID的一个例子可以是队列消息ID，或者表的主键。如果一个元组在超时或由于任何其他原因失败，则认为该元组已失败。
- en: Storm has a fault tolerance mechanism that guarantees at-least-once processing
    for all tuples emitted only from a reliable spout.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Storm有一个容错机制，它保证了只从可靠的spout发出的所有元组的至少一次处理。
- en: Once you have a reliable spout in place, you can make the bolt do the linking
    between the input and output tuples, which creates a tuple tree. Once a tuple
    tree is established, acker knows any failure in the linked tree, and the original
    message ID is used to create the entire tuple tree again.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦设置了可靠的spout，您可以让bolt在输入和输出元组之间进行链接，从而创建一个元组树。一旦建立了元组树，acker就知道链接树中的任何故障，并且使用原始消息ID重新创建整个元组树。
- en: 'In the bolt, there are two functions:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在bolt中，有两个函数：
- en: '`emit([tuple])`: There is no tuple tree linking. We can''t track which original
    message ID was used.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`emit([tuple])`：没有元组树链接。我们无法跟踪使用了哪个原始消息ID。'
- en: '`storm.emit([tuple], anchors=[message_key])`: With linking in place, the original
    tuple can now be replayed.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`storm.emit([tuple], anchors=[message_key])`：有了链接，原始元组现在可以重新播放。'
- en: 'The following figure explains how tuple B is generated from tuple A:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图解释了元组B是如何从元组A生成的：
- en: '![XOR magic in acking](img/B03471_02_10.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![XOR magic in acking](img/B03471_02_10.jpg)'
- en: 'The next figure illustrates the bolt performing **ACK**:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 下一图展示了bolt执行**ACK**的过程：
- en: '![XOR magic in acking](img/B03471_02_11.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![XOR magic in acking](img/B03471_02_11.jpg)'
- en: 'The following figure illustrates the failure condition, where the signal reaches
    the spout upon failure:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了故障条件，其中信号在故障时到达spout：
- en: '![XOR magic in acking](img/B03471_02_12.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![XOR magic in acking](img/B03471_02_12.jpg)'
- en: 'A successful **ACK** is demonstrated as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 成功的**ACK**演示如下：
- en: '![XOR magic in acking](img/B03471_02_13.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![XOR magic in acking](img/B03471_02_13.jpg)'
- en: 'The following figure illustrates a condition of a big tuple tree without a
    bolt, and there is no failure:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了没有螺栓的大元组树的状态，并且没有出现故障：
- en: '![XOR magic in acking](img/B03471_02_14.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![XOR magic in acking](img/B03471_02_14.jpg)'
- en: 'The next figure demonstrates an example of failure in a tuple tree—in the middle
    of the tuple tree:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 下一图演示了元组树中的故障示例——在元组树的中间：
- en: '![XOR magic in acking](img/B03471_02_15.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![XOR magic in acking](img/B03471_02_15.jpg)'
- en: Tuning parallelism in Storm – scaling a distributed computation
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整Storm的并行性——扩展分布式计算
- en: 'To explain parallelism of Storm, we will configure three parameters:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解释Storm的并行性，我们将配置三个参数：
- en: The number of workers
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作器的数量
- en: The number of executors
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行器的数量
- en: The number of tasks
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任务的数量
- en: 'The following figure gives a diagrammatic explanation of an example where we
    have a topology with just one spout and one bolt. In this case, we will set different
    values for the numbers of workers, executors, and tasks at the spout and bolt
    levels, and see how parallelism works in each case:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图给出了一个示例的图解，其中我们有一个只有一个spout和一个bolt的拓扑。在这种情况下，我们将设置spout和bolt级别的工作者、执行者和任务的不同值，并查看每种情况下的并行性是如何工作的：
- en: '![Tuning parallelism in Storm – scaling a distributed computation](img/B03471_02_16.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![Tuning parallelism in Storm – scaling a distributed computation](img/B03471_02_16.jpg)'
- en: '[PRE2]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: For this configuration, we will have two workers, which will run in separate
    JVMs (worker 1 and worker 2).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这种配置，我们将有两个工作者，它们将在不同的JVM（worker 1和worker 2）中运行。
- en: For the spout, there is one executor, and the default number of tasks is one,
    which makes the ratio 1:1 (one task per executor).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 对于spout，有一个执行者，默认任务数是一个，这使得比例是1:1（每个执行者一个任务）。
- en: For the bolt, there are two executors and four tasks, which makes it 4/2 = two
    tasks per executor. These two executors run under worker 2, with each having two
    tasks, while the executor of worker 1 gets only one task.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 对于螺栓，有两个执行者和四个任务，这使得每个执行者有四个任务除以两个执行者等于两个任务。这两个执行者在worker 2下运行，每个执行者有两个任务，而worker
    1的执行者只得到一个任务。
- en: 'This can be illustrated nicely using the following figure:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过以下图示很好地说明：
- en: '![Tuning parallelism in Storm – scaling a distributed computation](img/B03471_02_17.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![Tuning parallelism in Storm – scaling a distributed computation](img/B03471_02_17.jpg)'
- en: 'Let''s change the configuration in the bolt to two executors and two tasks:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们改变bolt的配置为两个执行者和两个任务：
- en: '[PRE3]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This can be illustrated well here:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这在这里可以很好地说明：
- en: '![Tuning parallelism in Storm – scaling a distributed computation](img/B03471_02_18.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![Tuning parallelism in Storm – scaling a distributed computation](img/B03471_02_18.jpg)'
- en: The number of workers is two again. As the bolt has two executors and two tasks,
    that makes it 2/2, or one task per executor. Now you can see that both executors
    get one task each. In terms of performance, both cases are exactly the same, as
    the tasks run sequentially within the executor thread. More executors means a
    higher degree of parallelism, and more workers means using resources such as CPU
    and RAM more effectively. Memory allocation is done at the worker level using
    the `worker`.`childopts` setting. We should also monitor the maximum amount of
    memory a particular worker process is holding. This plays an important role in
    deciding the total number of workers. It can be seen using the `ps -ef` option.
    Always keep the tasks and executors in the same ratio, and derive the correct
    value for the number of executors using the capacity column of the Storm UI. As
    an important note, we should keep the shorter duration transaction in the bolt
    and try to tune it via splitting code into more bolts or reducing the batch size
    tuple. The batch size is the number of records received by the bolt in a single
    tuple delivery. Also, don't block the `nextTuple` method of the spout due to the
    longer holding transaction.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 工作者的数量再次是两个。由于bolt有两个执行者和两个任务，所以是2/2，或者每个执行者一个任务。现在你可以看到两个执行者各自得到一个任务。在性能方面，这两种情况完全相同，因为任务在执行者线程中顺序运行。更多的执行者意味着更高的并行度，更多的工作者意味着更有效地使用资源，如CPU和RAM。内存分配是在工作者级别使用`worker`.`childopts`设置完成的。我们还应该监控特定工作者进程所持有的最大内存量。这在决定工作者总数方面起着重要作用。可以使用`ps
    -ef`选项查看。始终保持任务和执行者的相同比例，并使用Storm UI的容量列推导出执行者数量的正确值。作为重要提示，我们应该在bolt中保持较短的持续时间事务，并尝试通过将代码拆分成更多的bolt或减少批处理大小tuple来调整它。批处理大小是bolt在单个tuple交付中接收的记录数。另外，不要因为较长时间的事务而阻塞spout的`nextTuple`方法。
- en: Summary
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: As this chapter approaches its end, you must have got a brief idea about the
    Nimbus, supervisor, UI, and Zookeeper processes. This chapter also taught you
    how to tune parallelism in Storm by playing with the number of workers, executors,
    and tasks. You became familiar with the important problem of distributing computation,
    that is, failures and overcoming failures by different kinds of fault tolerance
    available in the system. And most importantly, you learned how to write a "reliable"
    spout to achieve guaranteed message processing and linking in bolts.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 随着本章接近尾声，你一定对Nimbus、监督器、UI和Zookeeper进程有了大致的了解。本章还教你如何通过调整工作者、执行者和任务的数量来在Storm中调整并行性。你熟悉了分布计算的重要问题，即故障和通过系统中的不同容错机制克服故障。最重要的是，你学习了如何编写一个“可靠的”spout，以实现保证的消息处理和bolt中的链接。
- en: The next chapter will give you information about how to build a simple topology
    using a Python library called Petrel. Petrel addresses some limitations of Storm's
    built-in Python support, providing simpler and more streamlined development.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将为您提供有关如何使用名为Petrel的Python库构建简单拓扑的信息。Petrel解决了Storm内置Python支持的一些局限性，提供了更简单、更流畅的开发体验。
