- en: Hadoop Design Consideration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Big data does not necessarily mean huge data. If a dataset is small, it's very
    easy to analyze it. We can load it on to an Excel spreadsheet and do the required
    calculations. But, as the volume of data gets bigger, we have to find other alternatives
    to process it. We may have to load it to an RDMBS table and run a SQL query to
    find the trend and patterns on the given structure. Further, if the dataset format
    changes to something like email, then loading to RDBMS becomes a huge challenge.
    To add more complexity to it, if the data speed changes to something like real
    time, it becomes almost impossible to analyze the given dataset with traditional
    RDBMS-based tools. In the modern world, the term *big data* can be expressed using
    the five most famous *V*s. Following is the explanation of each *V* in a nutshell.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/21dfce7d-6161-4729-94cb-0e9c871736e3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Data structure principles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing Hadoop cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring Hadoop architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing YARN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hadoop cluster composition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hadoop file formats
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding data structure principles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s go through some important data architecture principles:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data is an asset to an enterprise**: Data has a measurable value. It provides
    some real value to the enterprise. In modern times, data is treated like real
    gold.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data is shared enterprise-wide**: Data is captured only once and then used
    and analyzed many times. Multiple users access the same data for different uses
    cases and requirements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data governance**: Data is governed to ensure data quality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data management**: Data needs to be managed to attain enterprise objectives.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data access**: All users should have access to data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data security**: Data should be properly secured and protected.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data definition**: Each attribute of the data needs to be consistently defined
    enterprise-wide.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we know the basics of big data and its principles, let's get into some
    real action.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Hadoop cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following steps need to be performed in order to install Hadoop cluster.
    As the time of writing this book, Hadoop Version 2.7.3 is a stable release. We
    will install it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Check the Java version using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a Hadoop user account on all the servers, including all NameNodes and
    DataNodes with the help of the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Assume that we have four servers and we have to create a Hadoop cluster using
    all four servers. The IPs of these four servers are as follows: `192.168.11.1`,
    `192.168.11.2`, `192.168.11.3`, and `192.168.11.4`. Out of these four servers,
    we will first use a server as a master server (NameNode) and all remaining servers
    will be used as slaves (DataNodes).'
  prefs: []
  type: TYPE_NORMAL
- en: 'On both servers, NameNode and DataNodes, change the `/etc/hosts` file using
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then add the following to all files on all servers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, set up SSH on NamesNodes and DataNodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Download and install Hadoop on NameNode and all DataNodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Configuring Hadoop on NameNode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Log in to NameNode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Find and change the following properties with these values:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Filename** | **Property name** | **Property value** |'
  prefs: []
  type: TYPE_TB
- en: '| `core-site.xml` |  `fs.default.name` |  `hdfs://namenode:9000/` |'
  prefs: []
  type: TYPE_TB
- en: '|  | `dfs.permissions` |  `False` |'
  prefs: []
  type: TYPE_TB
- en: '| `hdfs-site.xml` | `dfs.data.dir` | `/opt/hadoop/hadoop/dfs/namenode/data`
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | `dfs.name.dir` | `/opt/hadoop/hadoop/dfs/namenode` |'
  prefs: []
  type: TYPE_TB
- en: '|  | `dfs.replication` | `1` |'
  prefs: []
  type: TYPE_TB
- en: '| `mapred-site.xml` | `mapred.job.tracker` | `namenode:9001 ` |'
  prefs: []
  type: TYPE_TB
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Format NameNode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following code is used to format the NameNode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Start all services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We start all the services with the following line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: For details about how to set up a Hadoop single-node and multi-node cluster,
    please use the following link: [https://hadoop.apache.org/docs/r2.7.0/hadoop-project-dist/hadoop-common/ClusterSetup.html](https://hadoop.apache.org/docs/r2.7.0/hadoop-project-dist/hadoop-common/ClusterSetup.html).
  prefs: []
  type: TYPE_NORMAL
- en: Exploring HDFS architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The HDFS architecture is based on master and slave patterns. NameNode is a master
    node and all DataNodes are SlaveNodes. Following are some important points to
    be noted about these two nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Defining NameNode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The NameNode is a master node of all DataNodes in the Hadoop cluster. It stores
    only the metadata of files and directories stored in the form of a tree. The important
    point is NameNode never stores any other data other than metadata. NameNode keeps
    track of all data written to DataNodes in the form of blocks. The default block
    size is 256 MB (which is configurable). Without the NameNode, the data on the
    DataNodes filesystem cannot be read. The metadata is stored locally on the NameNode
    using two files—filesystem namespace image file, FSImage, and edit logs. FSImage
    is the snapshot of the filesystem from the start of the NameNode edit logs—all
    the changes of the filesystem since the NameNode started, when the NameNode starts,
    it reads FSImage file and edits log files. All the transactions (edits) are merged
    into the FSImage file. The FSImage file is written to disk and a new, empty edits
    log file is created to log all the edits. Since NameNode is not restarted very
    often, the edits log file becomes very large and unmanageable. When NameNode is
    restarted, it takes a very long time to restart it as all the edits need to be
    applied to the FSImage file. In the event of NameNode crashing, all the metadata
    in the edits log file will not be written to the FSImage file and will be lost.
  prefs: []
  type: TYPE_NORMAL
- en: Secondary NameNode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The name secondary NameNode is confusing. It does not act as a NameNode. Its
    main function is to get the filesystem changes from the NameNode and merge it
    to NameNode FSImage at regular intervals. Writing edits log file changes to FSImage
    are called **commits**. Regular commits help to reduce the NameNode start time.
    The secondary NameNode is also known as the commit node.
  prefs: []
  type: TYPE_NORMAL
- en: NameNode safe mode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is a read-only mode for the HDFS cluster. Clients are not allowed any modifications
    to the filesystem or blocks. During startup, NameNode automatically starts in
    safe mode, applies edits to FSImage, disables safe mode automatically, and restarts
    in normal mode.
  prefs: []
  type: TYPE_NORMAL
- en: DataNode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DataNodes are the workhorses of the Hadoop cluster. Their main function is to
    store and retrieve data in the form of blocks. They always communicate their status
    to the NameNode in the form of heartbeats. That's how NameNode keeps track of
    any DataNodes, whether they are alive or dead. DataNodes keep three copies of
    the blocks known and the replication factor. DataNodes communicate with other
    DataNodes to copy data blocks to maintain data replication.
  prefs: []
  type: TYPE_NORMAL
- en: Data replication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: HDFS architecture supports placing very large files across the machines in a
    cluster. Each file is stored as a series of blocks. In order to ensure fault tolerance,
    each block is replicated three times to three different machines. It is known
    as a replication factor, which can be changed at the cluster level or at the individual
    file level. It is a NameNode that makes all the decisions related to block replication.
    NameNode gets heartbeat and block reports from each DataNode. Heartbeat makes
    sure that the DataNode is alive. A block report contains a list of all blocks
    on a DataNode.
  prefs: []
  type: TYPE_NORMAL
- en: Rack awareness
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'HDFS block placement will use rack awareness for fault tolerance by placing
    one block replica on a different rack, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aba679b2-4019-46ed-aea6-8d2e45428bae.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s understand the figure in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: The first replica is placed on the same rack as the initiating request DataNode,
    for example, Rack 1 and DataNode 1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second replica is placed on any DataNode of another rack, for example, Rack
    2, DataNode 2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The third replica is placed on any DataNode of the same rack, for example, Rack
    2, DataNode 3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A custom rack topology script, which contains an algorithm to select appropriate
    DataNodes, can be developed using a Unix shell, Java, or Python. It can be activated
    on the cluster by changing the `topology.script.file.name` parameter in `Core-site.xml`
    file.
  prefs: []
  type: TYPE_NORMAL
- en: HDFS WebUI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following table shows the services in the HDFS WebUI:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Service** | **Protocol** | **Port** | **URL** |'
  prefs: []
  type: TYPE_TB
- en: '| NameNode WebUI | HTTP | `50070` | `http://namenode:50070/` |'
  prefs: []
  type: TYPE_TB
- en: '| DataNode WebUI | HTTP | `50075` | `http://datanode:50075/` |'
  prefs: []
  type: TYPE_TB
- en: '| Secondary NameNode | HTTP | `50090` | `http://Snamenode:50090/` |'
  prefs: []
  type: TYPE_TB
- en: Introducing YARN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Yet Another Resource Negotiator** (**YARN**) separates the resource management,
    scheduling, and processing components. It helps to achieve 100% resource utilization
    of the cluster resources. YARN manages the CPU and memory of the cluster based
    on the Hadoop scheduler policy. YARN supports any type of application and is not
    restricted to just MapReduce. It supports applications written in any type of
    language, provided binaries can be installed on the Hadoop cluster.
  prefs: []
  type: TYPE_NORMAL
- en: YARN architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's understand the YARN architecture in detail in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Resource manager
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The resource manager is responsible for tracking the resources in a cluster
    and scheduling applications. The resource manager has two main components: the
    scheduler and the applications manager.'
  prefs: []
  type: TYPE_NORMAL
- en: Node manager
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The node manager is responsible for launching and managing containers on a node.
    Containers execute tasks as specified by the application master. It acts as a
    slave for the resource manager. Each node manager tracks the available data processing
    resources on its SlaveNode and sends regular reports to the resource manager.
    The processing resources in a Hadoop cluster are consumed in byte-size pieces
    called **containers**.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration of YARN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can perform the following steps for the configuration of YARN:'
  prefs: []
  type: TYPE_NORMAL
- en: Start Hadoop NameNode, secondary NameNode, and DataNode
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Alter `yarn-env.sh`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find corresponding XML files based on your Hadoop installation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following under the definition of `YARN_CONF_DIR`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Alter `yarn-site.xml`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Alter `mapred-site.xml`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Start the YARN services:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Configuring HDFS high availability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's take a look at the changes brought about in Hadoop over time.
  prefs: []
  type: TYPE_NORMAL
- en: During Hadoop 1.x
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hadoop 1.x started with the architecture of a single NameNode. All DataNodes
    used to send their block reports to that single NameNode. There was a secondary
    NameNode in the architecture, but its sole responsibility was to merge all edits
    to FSImage. With this architecture, the NameNode became the **single point of
    failure** (**SPOF**). Since it has all the metadata of all the DataNodes of the
    Hadoop cluster, in the event of NameNode crash, the Hadoop cluster becomes unavailable
    till the next restart of NameNode repair. If the NameNode cannot be recovered,
    then all the data in all the DataNodes would be completely lost. In the event
    of shutting down NameNode for planned maintenance, the HDFS becomes unavailable
    for normal use. Hence, it was necessary to protect the existing NameNode by taking
    frequent backups of the NameNode filesystem to minimize data loss.
  prefs: []
  type: TYPE_NORMAL
- en: During Hadoop 2.x and onwards
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to overcome HDFS **high availability** (**HA**) problems and make
    NameNode a SPOF, the architecture has changed. The new architecture provides a
    running of two redundant NameNodes in the same cluster in an active/passive configuration
    with a hot standby. This allows a fast failover to a new NameNode in the event
    of a machine crashing, or a graceful administrator-initiated failover for the
    purpose of planned maintenance. The following two architectural options are provided
    for HDFS HA:'
  prefs: []
  type: TYPE_NORMAL
- en: Using shared storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using quorum journal manager
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HDFS HA cluster using NFS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following diagram depicts the HDFS HA cluster using NFS for shared storage
    required by the NameNodes architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6b9bba22-b44f-4c0c-91b9-00fc64bd9794.png)'
  prefs: []
  type: TYPE_IMG
- en: Important architecture points
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Following are some important points to remember about the HDFS HA using shared
    storage architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the cluster, there are two separate machines: active state NameNode and
    standby state NameNode.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At any given point in time, one-and-only, one of the NameNodes is in the active
    state, and the other is in the standby state.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The active NameNode manages the requests from all client DataNodes in the cluster,
    while the standby remains a slave.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the DataNodes are configured in such a way that they send their block reports
    and heartbeats to both the active and standby NameNodes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The standby NameNode keeps its state synchronized with the active NameNode.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Active and standby nodes both have access to a filesystem on a shared storage
    device (for example, an NFS mount from a NAS)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When a client makes any filesystem change, the active NameNode makes the corresponding
    change (edits) to the edit log file residing on the network shared directory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The standby NameNode makes all the corresponding changes to its own namespace.
    That way, it remains in sync with the active NameNode.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the event of the active NameNode being unavailable, the standby NameNode
    makes sure that it absorbs all the changes (edits) from the shared network directory
    and promotes itself to an active NameNode.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Hadoop administrator should apply the fencing method to the shared storage
    to avoid a scenario that makes both the NameNodes active at a given time. In the
    event of failover, the fencing method cuts the access to the previous active NameNode
    to make any changes to the shared storage to ensure smooth failover to standby
    NameNode. After that, the standby NameNode becomes the active NameNode.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuration of HA NameNodes with shared storage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Add the following properties to the `hdfs-site.xml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Property** | **Value** |'
  prefs: []
  type: TYPE_TB
- en: '| `dfs.nameservices` | `cluster_name` |'
  prefs: []
  type: TYPE_TB
- en: '| `dfs.ha.namenodes.cluster_name` | `NN1`, `NN2` |'
  prefs: []
  type: TYPE_TB
- en: '| `dfs.namenode.rpc-address.cluster_name.NN1` | `machine1:8020` |'
  prefs: []
  type: TYPE_TB
- en: '| `dfs.namenode.rpc-address.cluster_name.NN2` | `machine2:8020` |'
  prefs: []
  type: TYPE_TB
- en: '| `dfs.namenode.http-address.cluster_name.NN1` | `machine1:50070` |'
  prefs: []
  type: TYPE_TB
- en: '| `dfs.namenode.http-address.cluster_name.NN2` | `machine2:50070` |'
  prefs: []
  type: TYPE_TB
- en: '| `dfs.namenode.shared.edits.dir` | `file:///mnt/filer1/dfs/ha-name-dir-shared`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `dfs.client.failover.proxy.provider.cluster_name` | `org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `dfs.ha.fencing.methods` | `sshfence` |'
  prefs: []
  type: TYPE_TB
- en: '| `dfs.ha.fencing.ssh.private-key-files` | `/home/myuser/.ssh/id_rsa` |'
  prefs: []
  type: TYPE_TB
- en: '| `dfs.ha.fencing.methods` | `sshfence([[username][:port]])` |'
  prefs: []
  type: TYPE_TB
- en: '| `dfs.ha.fencing.ssh.connect-timeout` | `30000` |'
  prefs: []
  type: TYPE_TB
- en: 'Add the following properties to `core-site.xml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Property** | **Value** |'
  prefs: []
  type: TYPE_TB
- en: '| `fs.defaultFS` | `hdfs://cluster_name` |'
  prefs: []
  type: TYPE_TB
- en: HDFS HA cluster using the quorum journal manager
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following diagram depicts the **quorum journal manager** (**QJM**) architecture
    to share edit logs between the active and standby NameNodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d19fd8c6-61cf-47dc-b86b-a68d32a19151.png)'
  prefs: []
  type: TYPE_IMG
- en: Important architecture points
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Following are some important points to remember about the HDFS HA using the
    QJM architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: In the cluster, there are two separate machines—the active state NameNode and
    standby state NameNode.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At any point in time, exactly one of the NameNodes is in an active state, and
    the other is in a standby state.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The active NameNode manages the requests from all client DataNodes in the cluster,
    while the standby remains a slave.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the DataNodes are configured in such a way that they send their block reports
    and heartbeats to both active and standby NameNodes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both NameNodes, active and standby, remain synchronized with each other by communicating
    with a group of separate daemons called **JournalNodes** (**JNs**).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When a client makes any filesystem change, the active NameNode durably logs
    a record of the modification to the majority of these JNs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The standby node immediately applies those changes to its own namespace by communicating
    with JNs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the event of the active NameNode being unavailable, the standby NameNode
    makes sure that it absorbs all the changes (edits) from JNs and promotes itself
    as an active NameNode.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To avoid a scenario that makes both the NameNodes active at a given time, the
    JNs will only ever allow a single NameNode to be a writer at a time. This allows
    the new active NameNode to safely proceed with failover.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuration of HA NameNodes with QJM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Add the following properties to `hdfs-site.xml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Property** | **Value** |'
  prefs: []
  type: TYPE_TB
- en: '| `dfs.nameservices` | `cluster_name` |'
  prefs: []
  type: TYPE_TB
- en: '| `dfs.ha.namenodes.cluster_name` | `NN1`, `NN2` |'
  prefs: []
  type: TYPE_TB
- en: '| `dfs.namenode.rpc-address.cluster_name.NN1` | `machine1:8020` |'
  prefs: []
  type: TYPE_TB
- en: '| `dfs.namenode.rpc-address.cluster_name.NN2` | `machine2:8020` |'
  prefs: []
  type: TYPE_TB
- en: '| `dfs.namenode.http-address.cluster_name.NN1` | `machine1:50070` |'
  prefs: []
  type: TYPE_TB
- en: '| `dfs.namenode.http-address.cluster_name.NN2` | `machine2:50070` |'
  prefs: []
  type: TYPE_TB
- en: '| `dfs.namenode.shared.edits.dir` | `qjournal://node1:8485;node2:8485;node3:8485/cluster_name`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `dfs.client.failover.proxy.provider.cluster_name` | `org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `dfs.ha.fencing.methods` | `sshfence` |'
  prefs: []
  type: TYPE_TB
- en: '| `dfs.ha.fencing.ssh.private-key-files` | `/home/myuser/.ssh/id_rsa` |'
  prefs: []
  type: TYPE_TB
- en: '| `dfs.ha.fencing.methods` | `sshfence([[username][:port]])` |'
  prefs: []
  type: TYPE_TB
- en: '| `dfs.ha.fencing.ssh.connect-timeout` | `30000` |'
  prefs: []
  type: TYPE_TB
- en: 'Add the following properties to `core-site.xml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Property** | **Value** |'
  prefs: []
  type: TYPE_TB
- en: '| `fs.defaultFS` | `hdfs://cluster_name` |'
  prefs: []
  type: TYPE_TB
- en: '| `dfs.journalnode.edits.dir` | `/path/to/journal/node/local/datat` |'
  prefs: []
  type: TYPE_TB
- en: Automatic failover
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It's very important to know that the above two architectures support only manual
    failover. In order to do automatic failover, we have to introduce two more components
    a ZooKeeper quorum, and the **ZKFailoverController** (**ZKFC**) process, and more
    configuration changes.
  prefs: []
  type: TYPE_NORMAL
- en: Important architecture points
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Each NameNode, active and standby, runs the ZKFC process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The state of the NameNode is monitored and managed by the ZKFC.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ZKFC pings its local NameNode periodically to make sure that that the NameNode
    is alive. If it doesn't get the ping back, it will mark that NameNode unhealthy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The healthy NameNode holds a special lock. If the NameNode becomes unhealthy,
    that lock will be automatically deleted.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the local NameNode is healthy, and the ZKFC sees the lock is not currently
    held by any other NameNode, it will try to acquire the lock. If it is successful
    in acquiring the lock, then it has won the election. It is now the responsibility
    of this NameNode to run a failover to make its local NameNode active.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring automatic failover
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Add the following properties to `hdfs-site.xml` to configure automatic failover:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Property** | **Value** |'
  prefs: []
  type: TYPE_TB
- en: '| `dfs.ha.automatic-failover.enabled` | `true` |'
  prefs: []
  type: TYPE_TB
- en: '| `ha.zookeeper.quorum` | `zk1:2181`, `zk2:2181`, `zk3:2181` |'
  prefs: []
  type: TYPE_TB
- en: Hadoop cluster composition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we know, a Hadoop cluster consists of master and slave servers: MasterNodes—to
    manage the infrastructure, and SlaveNodes—distributed data store and data processing.
    EdgeNodes are not a part of the Hadoop cluster. This machine is used to interact
    with the Hadoop cluster. Users are not given any permission to directly log in
    to any of the MasterNodes and DataNodes, but they can log in to the EdgeNode to
    run any jobs on the Hadoop cluster. No application data is stored on the EdgeNode.
    The data is always stored on the DataNodes on the Hadoop cluster. There can be
    more than one EdgeNode, depending on the number of users running jobs on the Hadoop
    cluster. If enough hardware is available, it''s always better to host each master
    and DataNode on a separate machine. But, in a typical Hadoop cluster, there are
    three MasterNodes.'
  prefs: []
  type: TYPE_NORMAL
- en: Please note that it is assumed that we are using HBase as a NoSQL datastore
    in our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Typical Hadoop cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Hadoop cluster composition will look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d657d3de-3c51-464f-a724-3d74f6ff3430.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following are some hardware specifications to be taken into account:'
  prefs: []
  type: TYPE_NORMAL
- en: NameNode and standby NameNodes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The memory requirement depends on the number of files and block replicas to
    be created. Typically, at least 64 GB - 96 GB memory is recommended for NameNodes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NameNodes need reliable storage to host FSImage and edit logs. It is recommended
    that these MasterNodes should have at least 4 TB - 6 TB SAS storage. It is a good
    idea to have RAID 5 - 6 storage for NameNodes. If the cluster is a HA cluster,
    then plan your Hadoop cluster in such a way that JNs should be configured on the
    master node.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As far as processors are concerned, it is recommended to have at least 2 quad
    core CPUs running at 2 GHz, to handle messaging traffic for the MasterNodes.
  prefs: []
  type: TYPE_NORMAL
- en: DataNodes/SlaveNodes should have at least 64 GB RAM per node. It is recommended
    that, typically, 2 GB - 3 GB memory is required for each Hadoop daemon, such as
    DataNode, node manager ZooKeeper, and so on; 5 GB for OS and other services; and
    5 GB - 8 GB for each MapReduce task.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DataNodes may have commodity storage with at least 8 TB - 10 TB disk storage
    with 7,200 RPM SATA drives. Hard disk configuration should be in **Just a Bunch
    Of Disks** (**JBOD**).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is recommended to have at least 8 processors—2.5 GHz cores and 24 cores CPUs
    for all DataNodes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is recommended to have 1 GbE to 10 GbE network connectivity within each RACK.
    For all slaves, 1 GB network bandwidth, and for MasterNodes, 10 GB bandwidth is
    recommended.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you plan to expand your Hadoop cluster in future, you can also add additional
    machines.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Please read the following articles from Hortonworks and Cloudera for additional
    reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://docs.hortonworks.com/HDPDocuments/HDP1/HDP-1.3.3/bk_cluster-planning-guide/content/conclusion.html](http://docs.hortonworks.com/HDPDocuments/HDP1/HDP-1.3.3/bk_cluster-planning-guide/content/conclusion.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://blog.cloudera.com/blog/2013/08/how-to-select-the-right-hardware-for-your-new-hadoop-cluster/](http://blog.cloudera.com/blog/2013/08/how-to-select-the-right-hardware-for-your-new-hadoop-cluster/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best practices Hadoop deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Following are some best practices to be followed for Hadoop deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Start small**: Like other software projects, an implementation Hadoop also
    involves risks and cost. It''s always better to set up a small Hadoop cluster
    of four nodes. This small cluster can be set up as **proof of concept** (**POC**).
    Before using any Hadoop component, it can be added to the existing Hadoop POC
    cluster as **proof of technology** (**POT**). It allows the infrastructure and
    development team to understand big data project requirements. After successful
    completion of POC and POT, additional nodes can be added to the existing cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hadoop cluster monitoring**: Proper monitoring of the NameNode and all DataNodes
    is required to understand the health of the cluster. It helps to take corrective
    actions in the event of node problems. If a service goes down, timely action can
    help avoid big problems in the future. Setting up Gangalia and Nagios are popular
    choices to configure alerts and monitoring. In the case of the Hortonworks cluster,
    Ambari monitoring, and the Cloudera cluster, Cloudera (CDH) manager monitoring
    can be an easy setup.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automated deployment**: Use of tools like Puppet or Chef is essential for
    Hadoop deployment. It becomes super easy and productive to deploy the Hadoop cluster
    with automated tools instead of manual deployment. Give importance to data analysis
    and data processing using available tools/components. Give preference to using
    Hive or Pig scripts for problem solving rather than writing heavy, custom MapReduce
    code. The goal should be to develop less and analyze more.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Implementation of HA**: While deciding about HA infrastructure and architecture,
    careful consideration should be given to any increase in demand and data growth.
    In the event of any failure or crash, the system should be able to recover itself
    or failover to another data center/site.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security**: Data needs to be protected by creating users and groups, and
    mapping users to the groups. Setting appropriate permissions and enforcing strong
    passwords should lock each user group down.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data protection**: The identification of sensitive data is critical before
    moving it to the Hadoop cluster. It''s very important to understand privacy policies
    and government regulations for the better identification and mitigation of compliance
    exposure risks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hadoop file formats
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Hadoop, there are many file formats available. A user can select any format
    based on the use case. Each format has special features in terms of storage and
    performance. Let's discuss each file format in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Text/CSV file
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text and CSV files are very common in Hadoop data processing algorithms. Each
    line in the file is treated as a new record. Typically, each line ends with the *n*
    character. These files do not support column headers. Hence, while processing,
    an extra line of the code is always required to remove column headings. CSV files
    are typically compressed using GZIP codec because they do not support block level
    compression; it adds to more processing costs. Needless to mention they do not
    support schema evolution.
  prefs: []
  type: TYPE_NORMAL
- en: JSON
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The JSON format is becoming very popular in all modern programming languages.
    These files are collection name/value pairs. The JSON format is typically used
    in data exchange applications and it is treated as an object, record, struct,
    or an array. These files are text files and support schema evolutions. It's very
    easy to add or delete attributes from a JSON file. Like text/CSV files, JSON files
    do not support block-level compression.
  prefs: []
  type: TYPE_NORMAL
- en: Sequence file
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A sequence file is a flat file consisting of binary key/value pairs. They are
    extensively used in MapReduce ([https://wiki.apache.org/hadoop/MapReduce](https://wiki.apache.org/hadoop/MapReduce))
    as input/output formats. They are mostly used for intermediate data storage within
    a sequence of MapReduce jobs. Sequence files work well as containers for small
    files. If there are too many small files in HDFS, they can be packed in a sequence
    file to make file processing efficient. There are three formats of sequence files:
    uncompressed, record compressed, and block compressed key/value records. Sequence
    files support block-level compression but do not support schema evolution.'
  prefs: []
  type: TYPE_NORMAL
- en: Avro
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Avro is a widely used file type within the Hadoop community. It is popular because
    it helps schema evolution. It contains serialized data with a binary format. An
    Avro file is splittable and supports block compression. It contains data and metadata.
    It uses a separate JSON file to define the schema format. When Avro data is stored
    in a file, its schema is stored with it so that files may be processed later by
    any program. If the program reading the data expects a different schema, this
    can be easily resolved, since both schemas are present.
  prefs: []
  type: TYPE_NORMAL
- en: Parquet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Parquet stores nested data structures in a flat columnar format. Parquet is
    more efficient in terms of storage and performance than any row-level file formats.
    Parquet stores binary data in a column-oriented way. In the Parquet format, new
    columns are added at the end of the structure. Cloudera mainly supports this format
    for Impala implementation but is aggressively becoming popular recently. This
    format is good for SQL queries, which read particular columns from a wide table
    having many columns because only selective columns are read to reduce I/O cost.
  prefs: []
  type: TYPE_NORMAL
- en: ORC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ORC files are optimized record columnar file format and are the extended version
    of RC files. These are great for compression and are best suited for Hive SQL
    performance when Hive is reading, writing, and processing data to reduce access
    time and the storage space. These files do not support true schema evolution.
    They are mainly supported by Hortonworks and are not suitable for Impala SQL processing.
  prefs: []
  type: TYPE_NORMAL
- en: Which file format is better?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The answer is: it depends on your use cases. Generally, the criteria for selecting
    a file format is based on query-read and query-write performance. Also, it depends
    on which Hadoop distribution you are using. The ORC file format is the best for
    Hive and Tez using the Hortonworks distribution and a parquet file is recommended
    for Cloudera Impala implementations. For a use case involving schema evolution,
    Avro files are best suited. If you want to import data from RDBMS using Sqoop,
    text/CSV file format is the better choice. For storing map intermediate output,
    a sequence file is the ultimate choice.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, the main objective was to learn about various Hadoop design
    alternatives. We've learned a lot when it comes to the Hadoop cluster and its
    best practices for deployment in a typical production environment. We started
    with a basic understanding about Hadoop and we proceeded to Hadoop configuration,
    installation, and HDFS architecture. We also learned about various techniques
    for achieving HDFS high availability. We also looked into YARN architecture. Finally,
    we looked at various file formats and how to choose one based on your use case.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will see how to ingest data into a newly created Hadoop
    cluster.
  prefs: []
  type: TYPE_NORMAL
