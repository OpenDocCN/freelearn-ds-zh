- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Designing Monitored Data Workﬂows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Logging code is a good practice that allows developers to debug faster and provide
    maintenance more effectively for applications or systems. There is no strict rule
    when inserting logs, but knowing when not to spam your monitoring or alerting
    tool while using it is excellent. Creating several logging messages unnecessarily
    will obfuscate the instance when something significant happens. That’s why it
    is crucial to understand the best practices when inserting logs into code.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will show how to create efficient and well-formatted logs using
    Python and PySpark for data pipelines with practical examples that can be applied
    in real-world projects.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we have the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Inserting logs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using log-level types
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating standardized logs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring our data ingest ﬁle size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logging based on data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Retrieving SparkSession metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can find the code from this chapter in the GitHub repository at [https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook).
  prefs: []
  type: TYPE_NORMAL
- en: Inserting logs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned in the introduction of this chapter, adding logging functionality
    to your applications is essential for debugging or making improvements later on.
    However, creating several log messages without necessity may generate confusion
    or even cause us to miss crucial alerts. In any case, knowing which kind of message
    to show is indispensable.
  prefs: []
  type: TYPE_NORMAL
- en: This recipe will cover how to create helpful log messages using Python and when
    to insert them.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will use only Python code. Make sure you have Python version 3.7 or above.
    You can use the following command to check it on your **command-line** **interface**
    (**CLI**):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The following code execution can be done on a Python shell or a Jupyter notebook.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To perform this exercise, we will make a function that reads and returns the
    first line of a CSV file using the best logging practices. Here is how we do it:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s import the libraries we will use and set the primary configuration
    for our `logging` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Notice that we passed a filename parameter to the `basicConfig` method. Our
    logs will be stored there.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will create a simple function to read and return a CSV file’s first
    line. Observe that `logging.info()` calls are inserted inside the functions with
    a message, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, let’s call our function, passing a CSV file as an example. Here, I will
    use the `listings.csv` file, which you can find in the GitHub repository as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – gets_csv_first_line function output](img/Figure_8.01_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – gets_csv_first_line function output
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s check the directory where we executed our Python script or Jupyter notebook.
    You should see a file named `our_application.log`. If you click on it, the result
    should be as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.2 – our_application.log content](img/Figure_8.02_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 – our_application.log content
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, we had two different outputs: one with the function results
    (*step 3*) and another that creates a file containing the log messages (*step
    4*).'
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s start understanding how the code works by looking at the first lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: After importing the built-in logging library, we called a method named `basicConfig()`,
    which sets the primary configuration for the subsequent calls in our function.
    The `filename` parameter indicates we want to save the logs into a file, and the
    `level` parameter sets the log level at which we want to start seeing messages.
    This will be covered in more detail in the *Using log-level types* recipe later
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we proceeded by creating our function and inserting the logging calls.
    Looking closely, we inserted the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: These two logs are informative and track an action or inform us as we pass through
    a part of the code or module. The best practice is to keep it as clean and objective
    as possible, so the next person (or even yourself) can identify where to start
    to look to solve a problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next log informs us of an error, as you can see here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The way to make the call for this error method is similar to the `.info()` ones.
    In this case, the best practice is to use only exception clauses and pass the
    error as a string function, as we did by passing the `e` variable in curly brackets.
    This way, even if we cannot see the Python traceback, we will store it in a file
    or monitoring application.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: It is a common practice to encapsulate the exception output in a variable, as
    in `except Exception as e`. It allows us to control how we show or get a part
    of the error message.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since our function was executed successfully, we don’t expect to see any error
    message in our `our_application.log` file, as you can see here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: If we look closely at the structure of the saved log, we will notice a pattern.
    The first word on each line, `INFO`, indicates the log level; after this, we see
    the `root` word, which indicates the logging hierarchy; and finally, we get the
    message we inserted into the code.
  prefs: []
  type: TYPE_NORMAL
- en: We can optimize and format our logs in many ways, but we won’t worry about this
    for now. We will cover the logging hierarchy in more detail in the *Formatting*
    *logs* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'See more about initiating the logs in Python in the official documentation
    here: [https://docs.python.org/3/howto/logging.xhtml#logging-to-a-file](https://docs.python.org/3/howto/logging.xhtml#logging-to-a-file)'
  prefs: []
  type: TYPE_NORMAL
- en: Using log-level types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have been introduced to how and where to insert logs, let’s understand
    log types or levels. Each log level has its own degree of relevance inside any
    system. For instance, the console output does not show debug messages by default.
  prefs: []
  type: TYPE_NORMAL
- en: We already covered how to log levels using PySpark in the *Inserting formatted
    SparkSession logs to facilitate your work recipe* in [*Chapter 6*](B19453_06.xhtml#_idTextAnchor195).
    Now we will do the same using only Python. This recipe aims to show how to set
    logging levels at the beginning of your script and insert the different levels
    inside your code to create a hierarchy of priority for your logs. With this, you
    can create a structured script that allows you or your team to monitor and identify
    errors.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will use only Python code. Make sure you have Python version 3.7 or above.
    You can use the following command on your CLI to check your version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The following code execution can be done on a Python shell or a Jupyter notebook.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s use the same example we had in the previous, *Inserting logs* recipe,
    and make some enhancements:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by importing the libraries and defining `basicConfig`. This time,
    we will set the log level to `DEBUG`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, before declaring the function, we will insert a `DEBUG` log informing
    that we are about to test this script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, as we saw in the *Inserting logs* recipe, we will build a function that
    reads a CSV file and returns the first line but with slight changes. Let’s insert
    a `DEBUG` message after the first line of the CSV is executed successfully, and
    a `CRITICAL` message if we enter the exception:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, before we make the call to the function, let’s insert a warning message
    informing that we are about to start it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After calling the function, you should see the following output in the `our_application.log`
    file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.3 – our_application.log updated with different log levels](img/Figure_8.03_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 – our_application.log updated with different log levels
  prefs: []
  type: TYPE_NORMAL
- en: It informs us the function was correctly executed and no error occurred.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now simulate an error. You should now see the following message inside
    the `our_application.log` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.4 – our_application.log showing an ERROR message](img/Figure_8.04_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 – our_application.log showing an ERROR message
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we entered the exception, and we can see the `ERROR` and `CRITICAL`
    messages.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Although it may seem irrelevant, we have made beneficial improvements to our
    function. Each log level corresponds to a different degree of criticality relating
    to what is happening. Let’s take a look at the following figure , which shows
    the weight of each level:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5 – Diagram of log level weight according to criticality](img/Figure_8.05_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 – Diagram of log level weight according to criticality
  prefs: []
  type: TYPE_NORMAL
- en: Depending on where the log level is inserted, it can prevent the script from
    continuing and creating a chain of errors since we can add different error handlers
    according to the level.
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, the Python logging library is configured to show messages only
    from the `DEBUG`, as you can see here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The purpose of showing only **WARNING** messages and above is to avoid spamming
    the console output or a log file with unnecessary system information. In the following
    figure, you can see how Python internally organizes its log levels:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.6 – Log level detailed description and when it’s best to use them
    (source: https://docs.python.org/3/howto/logging.xhtml)](img/Figure_8.06_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.6 – Log level detailed description and when it’s best to use them
    (source: [https://docs.python.org/3/howto/logging.xhtml](https://docs.python.org/3/howto/logging.xhtml))'
  prefs: []
  type: TYPE_NORMAL
- en: You can use this table as a reference when setting your log messages inside
    your code. It can also be found in the official Python documentation at [https://docs.python.org/3/howto/logging.xhtml#when-to-use-logging](https://docs.python.org/3/howto/logging.xhtml#when-to-use-logging).
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we tried to cover all the log severity levels to demonstrate
    the recommended places to insert them. Even though it may seem like simple stuff,
    knowing when each level should be used makes all the difference and brings maturity
    to your application.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Usually, each language has its structured form of logging levels. However, there
    is an *agreement* in the software engineering world about how the levels should
    be used. The following figure shows a fantastic decision diagram created by *Taco
    Jan Osinga* about the behavior of logging levels at the **Operating System** (**OS**)
    level.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.7 – Decision diagram of log levels by Taco Jan Osinga (source: https://stackoverflow.com/users/3476764/taco-jan-osinga?tab=profile)](img/Figure_8.07_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.7 – Decision diagram of log levels by Taco Jan Osinga (source: https://stackoverflow.com/users/3476764/taco-jan-osinga?tab=profile)'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For more detailed information about the foundations of Python logs, refer to
    the official documentation: [https://docs.python.org/3/howto/logging.xhtml](https://docs.python.org/3/howto/logging.xhtml)'
  prefs: []
  type: TYPE_NORMAL
- en: Creating standardized logs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we know the best practices for inserting logs and using log levels,
    we can add more relevant information to our logs to help us monitor our code.
    Information such as date and time or the module or function executed helps us
    determine where an issue occurred or where improvements are required.
  prefs: []
  type: TYPE_NORMAL
- en: Creating standardized formatting for application logs or (in our case) data
    pipeline logs makes the debugging process more manageable, and there are a variety
    of ways to do this. One way of doing it is to create `.ini` or `.conf` files that
    hold the configuration on how the logs will be formatted and applied to our wider
    Python code, for instance.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will learn how to create a configuration file that will dictate
    how the logs will be formatted across the code and shown in the execution output.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s use the same code as the previous *Using log-level types* recipe, but
    with more improvements!
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use the following code to follow the steps of this recipe in a new
    file or notebook, or reuse the function from the *Using log-level types* recipe.
    I prefer to make a copy so the first piece of code is left intact:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are the steps to perform this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: To start our exercise, let’s create a file called `logging.conf`. My recommendation
    is to store it in the same location as your Python scripts. However, feel free
    to keep it somewhere else, but do remember we will need the file’s path later.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, paste the following code inside the `logging.conf` file and save it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, insert the following `import` statements, the `config.fileConfig()` method,
    and the `logger` variable before the `gets_csv_first_line()` function, as you
    can see here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Observe that we are passing `logging.conf` as a parameter for the `config.fileConfig()`
    method. Pass the whole path if you stored it in a different directory level of
    your Python script.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s call our function by passing a CSV file. As usual, I will use the
    `listings.csv` file for this exercise:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following output in your notebook cell or Python shell:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.8 – Console output with formatted logs](img/Figure_8.08_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.8 – Console output with formatted logs
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, check your directory. You should see a file named `data_ingest.log`.
    Open it, and you should see something like the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.9 – The data_ingest.log file with formatted logs](img/Figure_8.09_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.9 – The data_ingest.log file with formatted logs
  prefs: []
  type: TYPE_NORMAL
- en: As you can observe, we created a standardized log format for both console and
    file output. Let’s now understand how we did it in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before jumping into the code, let’s first understand what a configuration file
    is. `.conf` or `.ini` file extension, offer a useful way to create customized
    applications to interact with other applications. You can find some of them inside
    your OS in the `/etc` or `/``var` directories.
  prefs: []
  type: TYPE_NORMAL
- en: Our case is no different. At the beginning of our recipe, we created a configuration
    file called `logging.conf` that holds the pattern for the Python logs we will
    apply across our application.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s take a look inside the `logging.conf` file. Looking closely, it
    is possible to see some values inside square brackets. Let’s start with the first
    three, as you can see here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'These parameters are modular components of the Python logging library, allowing
    easy customization due to their detachment from each other. In short, they represent
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Loggers are used by the code and expose the interface for itself. By default,
    there is a `root` logger used by Python. For new loggers, we use the `key` argument.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Handlers send the logs to the configured destination. In our case, we created
    two: `fileHandler` and `consoleHandler`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Formatters create a layout for the log records.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After declaring the basic parameters, we inserted two customized loggers and
    handlers, as you can observe in the following piece of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Creating a customization for the `root` `Logger` is not mandatory, but here
    we wanted to change the default log level to `DEBUG` and always send it to `fileHandler`.
    For `logger_data_ingest`, we also passed `consoleHandler`.
  prefs: []
  type: TYPE_NORMAL
- en: Speaking of handlers, they have a fundamental role here. Although they share
    the same log level and `Formatter`, they inherit different classes. The `StreamHandler`
    class catches the log records, and with `args=(sys.stdout,)` it gets all the system
    outputs for display in the console output. `FileHandler` works similarly, saving
    all the results at the `DEBUG` level and above.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, `Formatter` dictates how the log will be displayed. There are many
    ways to set the format, even passing the line of the code where the log was executed.
    You can see all the possible attributes at https://docs.python.org/3/library/logging.xhtml#logrecord-attributes.
  prefs: []
  type: TYPE_NORMAL
- en: The official Python documentation has an excellent diagram, shown in the following
    figure, that outlines the relationship between these modifiers and another one
    we didn’t cover here, called `Filter`.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.10 – Logging flow diagram (source: https://docs.python.org/3/howto/logging.xhtml#logging-flow)](img/Figure_8.10_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.10 – Logging flow diagram (source: https://docs.python.org/3/howto/logging.xhtml#logging-flow)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In our exercise, we created a simple logger handler called `data_ingest` along
    with the `gets_csv_first_line()` function. Now, imagine how it could be expanded
    through a whole application or system. Using a single configuration file, we can
    create several patterns with different applications for different scripts or ETL
    phases. Let’s take a look at the first lines of our code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '`config.fileConfig()` loads the configuration file and `logging.getLogger()`
    loads the `Logger` instance to use. It will use the `root` as default if it doesn’t
    find the proper `Logger`.'
  prefs: []
  type: TYPE_NORMAL
- en: Software engineers commonly use this best practice in a real-world application
    to avoid code redundancy and create a centralized solution.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are some other acceptable file formats with which to create log configurations.
    For example, we can use a **YAML Ain’t Markup Language** (**YAML**) file or a
    Python dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.11 – Configuration file formatting with YAML (source: https://docs.python.org/3/howto/logging.xhtml#configuring-logging)](img/Figure_8.11_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.11 – Configuration file formatting with YAML (source: https://docs.python.org/3/howto/logging.xhtml#configuring-logging)'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to know more about using the `logging.config` package to create
    improved YAML or dictionary configurations, refer to the official documentation
    here: [https://docs.python.org/3/library/logging.config.xhtml#logging-config-api](https://docs.python.org/3/library/logging.config.xhtml#logging-config-api)'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To read and understand more about how handlers work, refer to the official
    documentation here: [https://docs.python.org/3/library/logging.handlers.xhtml](https://docs.python.org/3/library/logging.handlers.xhtml)'
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring our data ingest ﬁle size
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When ingesting data, we can track a few items to ensure the incoming information
    is what we expect. One of the most important of these items is the data size we
    are ingesting, which can mean file size or the size of chunks of streaming data.
  prefs: []
  type: TYPE_NORMAL
- en: Logging the size of incoming data allows the creation of intelligent and efficient
    monitoring. If at some point the size of incoming data diverges from what is expected,
    we can take action to investigate and resolve the issue.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will create simple Python code that logs the size of ingested
    files, which is very valuable in data monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will use only Python code. Make sure you have Python version 3.7 or above.
    You can use the following command on your CLI to check your version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The following code execution can be done using a Python shell or a Jupyter notebook.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This exercise will create a simple Python function to read a file path and
    return its size in bytes by default. If we want to return the value in megabytes,
    we only need to pass the input parameter as `True`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by importing the `os` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we declare our function that requires a file path, along with an optional
    parameter to convert the size to megabytes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s use `os.stat()` to retrieve information from the file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Since it is optional, we can create an `if` condition to convert the `bytes`
    value to `megabytes`. If not flagged as `True`, we return the value in `bytes`,
    as you can see in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, let’s call our function, passing a dataset we already used:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For the `listings.csv` file, you should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.12 – File size in bytes](img/Figure_8.12_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.12 – File size in bytes
  prefs: []
  type: TYPE_NORMAL
- en: 'If we execute it by passing `s_megabytes` as `True`, we will see the following
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.13 – File size in megabytes](img/Figure_8.13_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.13 – File size in megabytes
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to test it using any file path on your machine and check whether the
    size is the same as that indicated in the console output.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A file’s size estimation is convenient when working with data. Let’s understand
    the pieces of code we used to achieve this estimation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first operation we used was the `os.stat()` method to retrieve information
    about the file, as you can see here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This method interacts directly with your OS. If we execute it in isolation,
    we will have the following output for the `listings.csv` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.14  – Attributes of the listings.csv file when using os.stat_result](img/Figure_8.14_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.14 – Attributes of the listings.csv file when using os.stat_result
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, we only need `st_size` to bring the `bytes` estimation, so we
    called it later in the `return` clause as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to know more about the other results shown, you can refer to the
    official documentation page here: [https://docs.python.org/3/library/stat.xhtml#stat.filemode](https://docs.python.org/3/library/stat.xhtml#stat.filemode)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, to provide the result in megabytes, we only need to do a simple conversion
    using the `st_size` value, where 1 KB is 1,024 bytes, and 1 MB is equal to 1,024
    KB. You can see the conversion formula here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe showed how easy it is to create a Python function that retrieves
    the file size. Unfortunately, at the time of writing, there was no straightforward
    solution to perform the same thing using PySpark.
  prefs: []
  type: TYPE_NORMAL
- en: 'Glenn Franxman, a software engineer, proposed on his GitHub a workaround solution
    using Spark internals to estimate the size of a DataFrame. You can see his code
    on his GitHub at the following link – make sure to give the proper credits if
    you do use it: [https://gist.github.com/gfranxman/4fd0719ff2618039182dd7ea1a702f8e](https://gist.github.com/gfranxman/4fd0719ff2618039182dd7ea1a702f8e)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use Glenn’s code in an example to estimate the DataFrame size and see
    how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'To execute the preceding code, you must have a SparkSession initiated. Once
    you have this and a DataFrame, execute the code and call the `estimate_df_size()`
    function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following output in bytes, depending on which DataFrame
    you are using:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.15 – DataFrame size in bytes](img/Figure_8.15_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.15 – DataFrame size in bytes
  prefs: []
  type: TYPE_NORMAL
- en: Remember that this solution will only work if you pass a DataFrame as a parameter.
    Our Python code works well for other file estimations and doesn’t have performance
    issues when estimating big files.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Unlike PySpark, Scala has a `SizeEstimator` function to return the size of
    a DataFrame. You can find more here: [https://spark.apache.org/docs/latest/api/java/index.xhtml?org/apache/spark/util/SizeEstimator.xhtml](https://spark.apache.org/docs/latest/api/java/index.xhtml?org/apache/spark/util/SizeEstimator.xhtml)'
  prefs: []
  type: TYPE_NORMAL
- en: Logging based on data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned in the *Monitoring our data ingest ﬁle size* recipe, logging our
    ingest is a good practice in the data field. There are several ways to explore
    our ingestion logs to increase the process’s reliability and our confidence in
    it. In this recipe, we will start to get into the data operations field (or **DataOps**),
    where the goal is to track the behavior of data from the source until it reaches
    its final destination.
  prefs: []
  type: TYPE_NORMAL
- en: This recipe will explore other metrics we can track to create a reliable data
    pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this exercise, let’s imagine we have two simple data ingests, one from
    a database and another from an API. Since this is a straightforward pipeline,
    let’s visualize it with the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.16 – Data ingestion phases](img/Figure_8.16_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.16 – Data ingestion phases
  prefs: []
  type: TYPE_NORMAL
- en: With this in mind, let’s explore the instances we can log to make monitoring
    efficient.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s define the essential metrics based on each layer (or step) we saw in
    the preceding diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data sources**: Let’s start with the first layer of the ingestion—the sources.
    Knowing we are handling two different data sources, we must create additional
    metrics for them. See the following figure for reference:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.17 – Database metrics to monitor](img/Figure_8.17_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.17 – Database metrics to monitor
  prefs: []
  type: TYPE_NORMAL
- en: '**Ingestion**: Now that the source is logged and monitored, let’s move on to
    the ingestion layer. As we saw previously in this chapter, we can log information
    such as errors, informative parts of the code execution, file size, and so on.
    Let’s insert more content here, such as the schema and the time taken to retrieve
    or process data. We will end up with a diagram similar to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.18 – Data ingestion metrics to monitor](img/Figure_8.18_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.18 – Data ingestion metrics to monitor
  prefs: []
  type: TYPE_NORMAL
- en: '**Staging layer**: Lastly, let’s cover the final layer after ingestion. The
    goal here is to ensure we maintain the integrity of the data, so verifying whether
    the schema still matches the data is crucial. We can also add logs to monitor
    the number of Parquet files and their sizes. See the following figure for reference:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.19 – Staging layer metrics to monitor](img/Figure_8.19_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.19 – Staging layer metrics to monitor
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have covered the essential topics to be monitored, let’s understand
    why they were chosen.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since the first recipe of this chapter, we have perpetually reinforced how logs
    are relevant to getting your system to work correctly. Here, we put it all together
    to see, albeit from a high level, how storing specific information can help us
    with monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: Starting with the data source layer, the metrics chosen were based on the response
    of the data and the availability to retrieve data. Understanding whether we can
    begin the ingestion process is fundamental, and even more important is knowing
    whether the data size is what we expect.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine the following scenario: every day, we ingest 50 MB of data from an
    API. One day, however, we received 10 KB. With proper logging and monitoring functionalities,
    we can quickly review the issue in terms of historic events. We can expand the
    data size check to the subsequent layers we covered in the recipe.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: We purposely intercalate the words “step” and “layer” when referring to the
    phases of the ingestion process since it can vary in different works of literature
    and in different companies’ internal processes.
  prefs: []
  type: TYPE_NORMAL
- en: Another way to log and monitor our data is by using schema validation. **Schema
    validation** (when applicable) guarantees that nothing has changed at the source.
    Therefore, the results for transformation or aggregation tend to be linear. We
    can also implement an auxiliary function or job to check that fields containing
    **sensitive** or **Personally Identifiable Information** (**PII**) are adequately
    anonymized.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring the *parquet file size or count* is crucial to verify that quality
    is being maintained. As seen in [*Chapter 7*](B19453_07.xhtml#_idTextAnchor227),
    the number of parquet files can interfere with other applications’ reading quality
    or even the ETL’s subsequent phases.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, it is essential to point out that we covered logs here to ensure the
    quality and reliability of our data ingestion. Remember that the best practice
    is to align the records we got from the code with the examples we saw here.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The content of this recipe is part of a more extensive subject called **data
    observability**. Data observability is a union of data operations, quality, and
    governance. The objective is to centralize everything to make the management and
    monitoring of data processes efficient and reliable, bringing health to data.
  prefs: []
  type: TYPE_NORMAL
- en: We will discuss this further in [*Chapter 12*](B19453_12.xhtml#_idTextAnchor433).
    However, if you are curious about the topic, Databand (an IBM company) has a good
    introduction at [https://databand.ai/data-observability/](https://databand.ai/data-observability/).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Find out more about monitoring ETL pipelines at the DataGaps blog page, here:
    [https://www.datagaps.com/blog/monitoring-your-etl-test-data-pipelines-in-production-dataops-suite/](https://www.datagaps.com/blog/monitoring-your-etl-test-data-pipelines-in-production-dataops-suite/)'
  prefs: []
  type: TYPE_NORMAL
- en: Retrieving SparkSession metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Until now, we created our logs to provide more information and be more useful
    for monitoring. Logging allows us to build customized metrics based on the necessity
    of our pipeline and code. However, we can also take advantage of built-in metrics
    from frameworks and programming languages.
  prefs: []
  type: TYPE_NORMAL
- en: When we create a `SparkSession`, it provides a web UI with useful metrics that
    can be used to monitor our pipelines. Using this, the following recipe shows you
    how to access and retrieve metric information from SparkSession, and use it as
    a tool when ingesting or processing a DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can execute this recipe using the PySpark command line or the Jupyter Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before exploring the Spark UI metrics, let’s create a simple `SparkSession`
    using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, let’s read a JSON file and call the `.show()` method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: I am using a dataset called `github_events.json`, which we worked with previously
    in [*Chapter 4*](B19453_04.xhtml#_idTextAnchor127). However, feel free to use
    whatever you prefer, since the objective here is not to observe the schema of
    the dataset, but to see what we can find out from the Spark UI.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With the `SparkSession` initiated as outlined in the *Getting ready* section,
    we can use the `spark` command to retrieve a link to the Spark UI, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.20 – spark command output](img/Figure_8.20_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.20 – spark command output
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on **Spark UI**, and your browser will open a new tab. You should see
    a page like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.21 – Spark UI: Jobs page view](img/Figure_8.21_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.21 – Spark UI: Jobs page view'
  prefs: []
  type: TYPE_NORMAL
- en: If you are not using the Jupyter Notebook, you can access this interface by
    pointing your browser to [http://localhost:4040/](http://localhost:4040/).
  prefs: []
  type: TYPE_NORMAL
- en: My page looks more crowded because I expanded **Event Timeline** and **Completed
    Jobs** – you can do the same by clicking on them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s explore the first completed job further. Click on **showString
    at NativeMethodAccessorImpl.java:0** and you should see the following page:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.22 – Spark UI: Stage page view for a specific job](img/Figure_8.22_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.22 – Spark UI: Stage page view for a specific job'
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see the task status of this job in more detail, covering things
    such as how much memory it used, the time taken to execute it, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Note also that it switched to the **Stages** tab at the top menu.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, click on the **Executors** button at the top of the page. You should see
    a page similar to this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.23 – Spark UI: Executors page view](img/Figure_8.23_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.23 – Spark UI: Executors page view'
  prefs: []
  type: TYPE_NORMAL
- en: All the metrics here are related to the Spark drivers and nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, click on the **SQL** button in the top menu. You should see the following
    page:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.24 – Spark UI: SQL page view](img/Figure_8.24_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.24 – Spark UI: SQL page view'
  prefs: []
  type: TYPE_NORMAL
- en: On this page, it is possible to see the queries executed by Spark internally.
    If we used an explicit query in our code, we would see here how it was performed
    internally.
  prefs: []
  type: TYPE_NORMAL
- en: You don’t need to worry about the `.``show()` method.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have explored Spark UI, let’s understand how each tab is organized
    and some of the steps we did with them.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *step 2*, we had a first glance at the interface. This interface makes it
    possible to see an event timeline with information about when the driver was created
    and executed. Also, we can observe the jobs marked on the timeline, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.25 – Detailed view of the Event Timeline expanded menu](img/Figure_8.25_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.25 – Detailed view of the Event Timeline expanded menu
  prefs: []
  type: TYPE_NORMAL
- en: We can observe how they interact when working with bigger jobs and more complex
    parallel tasks. Unfortunately, we would need a dedicated project and several datasets
    to simulate this, but you now know where to look for future reference.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we selected **showString at NativeMethodAccessorImpl.java:0**, which redirected
    us to the **Stages** page. This page offers more detailed information about Spark’s
    tasks, whether the task was successful or not.
  prefs: []
  type: TYPE_NORMAL
- en: 'An excellent metric and visualization tool is **DAG Visualization** (referring
    to directed acyclic graphs), which can be expanded and will show something like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.26 – DAG Visualization of a job](img/Figure_8.26_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.26 – DAG Visualization of a job
  prefs: []
  type: TYPE_NORMAL
- en: This offers an excellent overview of each step performed at each stage. We can
    also consult this to understand which part was problematic in the event of an
    error based on the traceback message.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we selected a specific task (or job), it showed its stages and details.
    However, we can display all the steps executed if we go directly to **Stages**.
    Doing so, you should see something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.27 – Spark UI: Stages overview with all jobs executed](img/Figure_8.27_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.27 – Spark UI: Stages overview with all jobs executed'
  prefs: []
  type: TYPE_NORMAL
- en: Although the description messages are not straightforward, we can get the gist
    of what each of them is doing. `Stage id 0` refers to the reading JSON function,
    and `Stage id 1` with the `showString` message refers to the `.show()` method
    call.
  prefs: []
  type: TYPE_NORMAL
- en: The **Executors** page shows the metrics related to the core of Spark and how
    it is performing. You can use this information to understand your cluster’s behavior
    and whether any tuning is needed. For more detailed information about each field,
    refer to the Spark official documentation at https://spark.apache.org/docs/latest/monitoring.xhtml#executor-metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Last but not least, we saw the **SQL** page, where it was possible to see how
    Spark internally shuffles and aggregates the data behind the scenes, like **Stages**,
    taking advantage of a more visual form of execution, as you can see in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.28 – Flow diagram of the SQL query internally executed](img/Figure_8.28_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.28 – Flow diagram of the SQL query internally executed
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that the query is related to the `.show()` method. There is
    helpful information inside it, including the number of output rows, the files
    read, and their sizes.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Even though Spark metrics are handy, you might wonder how to use them when hosting
    your PySpark jobs on cloud providers such as AWS or Google Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: '**AWS** provides a simple solution to enable Spark UI when using **AWS Glue**.
    You can find out more about it at [https://docs.aws.amazon.com/glue/latest/dg/monitor-spark-ui-jobs.xhtml](https://docs.aws.amazon.com/glue/latest/dg/monitor-spark-ui-jobs.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Google Data Proc** provides a web interface for its cluster, where you can
    also see metrics for **Hadoop** and **YARN**. Since Spark runs on top of YARN,
    you won’t find a link directly for Spark UI, but you can use the YARN interface
    to access it. You can find out more here: [https://cloud.google.com/dataproc/docs/concepts/accessing/cluster-web-interfaces](https://cloud.google.com/dataproc/docs/concepts/accessing/cluster-web-interfaces).'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Towards Data Science* has a fantastic article about Spark metrics: https://towardsdatascience.com/monitoring-of-spark-applications-3ca0c271c4e0'
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: https://spark.apache.org/docs/latest/monitoring.xhtml#executor-task-metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://developer.here.com/documentation/metrics-and-logs/user_guide/topics/spark-ui.xhtml](https://developer.here.com/documentation/metrics-and-logs/user_guide/topics/spark-ui.xhtml)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/LucaCanali/Miscellaneous/blob/master/Spark_Notes/Spark_TaskMetrics.md](https://github.com/LucaCanali/Miscellaneous/blob/master/Spark_Notes/Spark_TaskMetrics.md)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.python.org/3/howto/logging.xhtml](https://docs.python.org/3/howto/logging.xhtml)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[datadoghq.com/blog/python-logging-best-practices/](http://datadoghq.com/blog/python-logging-best-practices/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://coralogix.com/blog/python-logging-best-practices-tips/](https://coralogix.com/blog/python-logging-best-practices-tips/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
