- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Using PySpark with Deﬁned and Non-Deﬁned Schemas
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 PySpark 与定义和非定义模式
- en: Generally, schemas are forms used to create or apply structures to data. As
    someone who works or will work with large volumes of data, it is essential to
    understand how to manipulate DataFrames and apply structure when it is necessary
    to bring more context to the information involved.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，模式是用来创建或应用结构到数据的形式。作为一个处理或将要处理大量数据的人，理解如何操作 DataFrame 并在需要为涉及的信息提供更多上下文时应用结构是至关重要的。
- en: However, as seen in the previous chapters, data can come from different sources
    or be present without a well-defined structure, and applying a schema can be challenging.
    Here, we will see how to create schemas and standard formats using PySpark with
    structured and unstructured data.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如前几章所见，数据可能来自不同的来源或以未定义的结构存在，应用模式可能具有挑战性。在这里，我们将看到如何使用 PySpark 创建模式和标准格式，无论是结构化数据还是非结构化数据。
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下食谱：
- en: Applying schemas to data ingestion
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用模式到数据摄入
- en: Importing structured data using a well-deﬁned schema
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用已定义模式导入结构化数据
- en: Importing unstructured data with an undefined schema
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用未定义模式的非结构化数据导入
- en: Ingesting unstructured data with a well-deﬁned schema and format
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用已定义和非定义模式导入结构化数据
- en: Inserting formatted SparkSession logs to facilitate your work
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 插入格式化的 SparkSession 日志以方便你的工作
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'You can also find the code for this chapter in the GitHub repository here:
    [https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在这个 GitHub 仓库中找到本章的代码：[https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook)。
- en: 'Using **Jupyter Notebook** is not mandatory but can help you see how the code
    works interactively. Since we will execute Python and PySpark code, it can help
    us understand the scripts better. Once you have it installed, you can execute
    Jupyter using the following line:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 **Jupyter Notebook** 不是强制性的，但它可以帮助你交互式地查看代码的工作方式。由于我们将执行 Python 和 PySpark
    代码，这可以帮助我们更好地理解脚本。一旦安装完成，你可以使用以下命令来运行 Jupyter：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: It is recommended to create a separate folder to store the Python files or Notebooks
    we will cover in this chapter; however, feel free to organize the files in the
    best way that fits you.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 建议创建一个单独的文件夹来存储本章中将要涵盖的 Python 文件或 Notebooks；然而，你可以自由地以最适合你的方式组织文件。
- en: 'In this chapter, all recipes will need a `SparkSession` instance initialized,
    and you can use the same session for all of them. You can use the following code
    to create your session:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，所有食谱都需要初始化一个 `SparkSession` 实例，并且你可以为它们使用相同的会话。你可以使用以下代码来创建你的会话：
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: A `WARN` message as output is expected in some cases, especially if you are
    using WSL on Windows, so you don’t need to worry.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，预期输出会有 `WARN` 消息，特别是如果你在 Windows 上使用 WSL，所以你不需要担心。
- en: Applying schemas to data ingestion
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用模式到数据摄入
- en: The application of schemas is common practice when ingesting data, and PySpark
    natively supports applying them to DataFrames. To define and apply schemas to
    our DataFrames, we need to understand some concepts of Spark.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理数据时，应用模式是一种常见的做法，PySpark 本地支持将这些模式应用到 DataFrame 中。为了定义并应用模式到我们的 DataFrame，我们需要了解一些
    Spark 的概念。
- en: This recipe introduces the basic concept of working with schemas using PySpark
    and its best practices so that we can later apply them to structured and unstructured
    data.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱介绍了使用 PySpark 处理模式的基本概念及其最佳实践，以便我们可以在以后将它们应用到结构化和非结构化数据中。
- en: Getting ready
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Make sure PySpark is installed and working on your machine for this recipe.
    You can run the following code on your command line to check this requirement:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 确保你的机器上安装并运行了 PySpark，你可以通过在命令行中运行以下代码来检查此要求：
- en: '[PRE2]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You should see the following output:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下输出：
- en: '![Figure 6.1 – PySpark version console output](img/Figure_6.1_B19453.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.1 – PySpark 版本控制台输出](img/Figure_6.1_B19453.jpg)'
- en: Figure 6.1 – PySpark version console output
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1 – PySpark 版本控制台输出
- en: If don’t have PySpark installed on your local machine, please refer to the *Installing
    PySpark* recipe in [*Chapter 1*](B19453_01.xhtml#_idTextAnchor022).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有在本地机器上安装 PySpark，请参考 [*第 1 章*](B19453_01.xhtml#_idTextAnchor022) 中的 *安装
    PySpark* 食谱。
- en: 'I will use Jupyter Notebook to execute the code to make it more interactive.
    You can use this link and follow the instructions on the screen to install it:
    [https://jupyter.org/install](https://jupyter.org/install).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我将使用 Jupyter Notebook 来执行代码，使其更具交互性。你可以使用此链接并遵循屏幕上的说明来安装它：[https://jupyter.org/install](https://jupyter.org/install)。
- en: 'If you already have it installed, check the version using the following code
    on your command line:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经安装了它，请使用以下命令行代码检查版本：
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The following screenshot shows the expected output:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了预期的输出：
- en: '![Figure 6.2 – Jupyter package versions](img/Figure_6.2_B19453.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.2 – Jupyter 包版本](img/Figure_6.2_B19453.jpg)'
- en: Figure 6.2 – Jupyter package versions
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2 – Jupyter 包版本
- en: As you can see, the Notebook version was `6.4.4` at the time this book was written.
    Make sure to always use the latest version.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，本书编写时 Notebook 版本是 `6.4.4`。请确保始终使用最新版本。
- en: How to do it…
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点…
- en: 'Here are the steps to carry out the recipe:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是执行食谱的步骤：
- en: '**Creating mock data**: Before applying the schema to our DataFrame, we need
    to create a simple set containing simulated data of people’s information in this
    format—ID, name, last name, age, and gender:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**创建模拟数据**：在将模式应用到我们的 DataFrame 之前，我们需要创建一个包含模拟数据的简单集合，这些数据以以下格式表示人的信息——ID、姓名、姓氏、年龄和性别：'
- en: '[PRE4]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '**Importing and structuring the schema**: The next step is to import the types
    and create the structure of our schema:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**导入和构建模式**：下一步是导入类型并创建我们模式的结构：'
- en: '[PRE5]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '**Creating the DataFrame**: Then, we make the DataFrame, applying the schema
    we have created:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**创建 DataFrame**：然后，我们创建 DataFrame，应用我们创建的模式：'
- en: '[PRE6]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'When printing our DataFrame schema using the `.printSchema()` method, this
    is the expected output:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 `.printSchema()` 方法打印我们的 DataFrame 模式时，这是预期的输出：
- en: '![Figure 6.3 – The DataFrame schema](img/Figure_6.3_B19453.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.3 – DataFrame 模式](img/Figure_6.3_B19453.jpg)'
- en: Figure 6.3 – The DataFrame schema
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.3 – DataFrame 模式
- en: How it works…
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: Before understanding the methods in *step 2*, let’s step back a bit and understand
    the concept of a DataFrame.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在理解 *步骤 2* 中的方法之前，让我们稍微回顾一下 DataFrame 的概念。
- en: DataFrame is like a table with data stored and organized in a two-dimensional
    array, which resembles a table from a relational database such as MySQL or Postgres.
    Each line corresponds to a record, and libraries such as pandas and PySpark, by
    default, assign a record number internally to each line (or index).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame 类似于一个存储和组织在二维数组中的数据表，它类似于 MySQL 或 Postgres 等关系型数据库中的表。每一行对应一个记录，pandas
    和 PySpark 等库默认情况下会为每一行（或索引）分配一个内部记录号。
- en: '![Figure 6.4 – GeeksforGeeks DataFrame explanation](img/Figure_6.4_B19453.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.4 – GeeksforGeeks DataFrame 解释](img/Figure_6.4_B19453.jpg)'
- en: Figure 6.4 – GeeksforGeeks DataFrame explanation
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4 – GeeksforGeeks DataFrame 解释
- en: Note
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Speaking of the Pandas library, it is common to refer to a column in a Pandas
    DataFrame as a series and expect it to behave like a Python list. It makes it
    easier to manipulate data for analysis and visualization.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 谈到 Pandas 库，通常将 Pandas DataFrame 中的列称为序列，并期望它像 Python 列表一样表现。这使得分析数据和处理可视化变得更加容易。
- en: The objective of using a DataFrame is to utilize the several optimizations for
    data processing under the hood that Spark brings, which are directly linked to
    parallel processing.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 DataFrame 的目的是利用 Spark 在底层提供的多个数据处理的优化，这些优化与并行处理直接相关。
- en: 'Back to the schema definition in our `schema` variable, let’s take a look at
    the code we created:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 回到 `schema` 变量中的模式定义，让我们看看我们创建的代码：
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The first object we declare is the `StructType` class. This class will create
    an object of collection or our rows. Next, we declare a `StructField` instance,
    representing our column with its name, data type, whether it is nullable, and
    its metadata when applicable. `StructField` must be in the same order as the columns
    in the DataFrame; otherwise, it can generate errors due to the incompatibility
    of a data type (for example, the column has string values, and we are setting
    it as an integer) or the presence of null values. Defining `StructField` is an
    excellent opportunity to standardize the name of the DataFrame and, therefore,
    the analytical data.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们声明的第一个对象是 `StructType` 类。这个类将创建一个集合或我们行对象的实例。接下来，我们声明一个 `StructField` 实例，它代表我们的列，包括其名称、数据类型、是否可空以及适用时的元数据。`StructField`
    必须按照 DataFrame 中列的顺序排列；否则，由于数据类型不兼容（例如，列具有字符串值，而我们将其设置为整数）或存在空值，可能会产生错误。定义 `StructField`
    是标准化 DataFrame 名称以及因此分析数据的绝佳机会。
- en: Finally, `StringType` and `IntegerType` are the methods that will cast the data
    type into the respective columns. They were imported at the beginning of the recipe
    and derived from the SQL types inside PySpark. In our mock data example, we defined
    the `id` and `age` columns as `IntegerType` since we expect no other kind of data
    in them. However, there are many situations where the `id` column is referred
    to as a string type, usually when the data comes from different systems.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`StringType`和`IntegerType`是将数据类型转换为相应列的方法。它们在菜谱开始时导入，并来自PySpark内部的SQL类型。在我们的模拟数据示例中，我们将`id`和`age`列定义为`IntegerType`，因为我们预计其中不会有其他类型的数据。然而，在许多情况下，`id`列被称为字符串类型，通常当数据来自不同的系统时。
- en: 'Here we used `StringType` and `IntegerType`, but many others can be used to
    create context and standardize our data. Refer to the following figure:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了`StringType`和`IntegerType`，但还可以使用许多其他类型来创建上下文并标准化我们的数据。请参考以下图表：
- en: '![Figure 6.5 – SparkbyExample table of data types in Spark](img/Figure_6.5_B19453.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图6.5 – SparkbyExample中Spark的数据类型表](img/Figure_6.5_B19453.jpg)'
- en: Figure 6.5 – SparkbyExample table of data types in Spark
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 – SparkbyExample中Spark的数据类型表
- en: 'You can understand more about how to apply the **Spark data types** in the
    Spark official documentation here: [https://spark.apache.org/docs/latest/sql-ref-datatypes.xhtml](https://spark.apache.org/docs/latest/sql-ref-datatypes.xhtml).'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在Spark官方文档中了解如何应用**Spark数据类型**：[https://spark.apache.org/docs/latest/sql-ref-datatypes.xhtml](https://spark.apache.org/docs/latest/sql-ref-datatypes.xhtml)。
- en: There’s more…
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: When handling terminology, there needs to be a common understanding about using
    a dataset or DataFrame, especially if you are a newcomer to the data world.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理术语时，需要对使用数据集或DataFrame有一个共同的理解，尤其是如果您是数据世界的新手。
- en: A dataset is a collection of data containing rows and columns (for example,
    relational data) or documents and files (for example, non-relational data). It
    comes from a source and is available in different file formats.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集是一组包含行和列（例如，关系型数据）或文档和文件（例如，非关系型数据）的数据集合。它来自一个源，并以不同的文件格式提供。
- en: On the other hand, a DataFrame is derived from a dataset, presenting the data
    in a tabular form even if that is not the primary format. The DataFrame can transform
    a MongoDB document collection into a tabular organization based on the configurations
    set when it is created.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，DataFrame是从数据集派生出来的，即使不是主要格式，也会以表格形式呈现数据。DataFrame可以根据创建时设置的配置将MongoDB文档集合转换为表格组织。
- en: See also
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'More examples on the *SparkbyExample* site can be found here: [https://sparkbyexamples.com/pyspark/pyspark-sql-types-datatype-with-examples/](https://sparkbyexamples.com/pyspark/pyspark-sql-types-datatype-with-examples/).'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于*SparkbyExample*网站的示例可以在以下链接找到：[https://sparkbyexamples.com/pyspark/pyspark-sql-types-datatype-with-examples/](https://sparkbyexamples.com/pyspark/pyspark-sql-types-datatype-with-examples/)。
- en: Importing structured data using a well-deﬁned schema
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用定义良好的模式导入结构化数据
- en: As seen in the previous chapter, *Ingesting Data from Structured and Unstructured
    Databases*, structured data has a standard format presented in rows and columns
    and is often stored inside a database.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一章所述，在“从结构化和非结构化数据库中摄取数据”中，结构化数据具有标准的行和列格式，通常存储在数据库中。
- en: Due to its format, the application of a DataFrame schema tends to be less complex
    and has several benefits, such as ensuring the ingested information is the same
    as the data source or follows a rule.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其格式，DataFrame模式的适用性通常较为简单，并且具有几个优点，例如确保摄取的信息与数据源相同或遵循规则。
- en: In this recipe, we will ingest data from a structured file such as a CSV file
    and apply a DataFrame schema to understand better how it is used in a real-world
    scenario.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将从结构化文件（如CSV文件）中摄取数据，并应用DataFrame模式以更好地理解它在现实世界场景中的应用。
- en: Getting ready
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: This exercise requires the `listings.csv` file found inside the GitHub repository
    for this book. Also, make sure your `SparkSession` is initialized.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 此练习需要GitHub存储库中这本书的`listings.csv`文件。同时，请确保您的`SparkSession`已初始化。
- en: All the code in this recipe can be executed in Jupyter Notebook cells or a PySpark
    shell.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 此菜谱中的所有代码都可以在Jupyter Notebook单元格或PySpark shell中执行。
- en: How to do it…
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做…
- en: 'Here are the steps to perform this recipe:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此菜谱的步骤如下：
- en: '`StringType` and `IntegerType`, we will include two more data types in our
    import, `FloatType` and `DateType`, shown as follows:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`StringType`和`IntegerType`，我们将在导入中包含两种更多数据类型，即`FloatType`和`DateType`，如下所示：'
- en: '[PRE8]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '`StructField` and assign them to a respective data type:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`StructField`分配给相应的数据类型：
- en: '[PRE9]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '`listings.csv` file with the `.options()` configurations and add the `.schema()`
    method with the `schema` variable seen in *step 2*.'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`.options()`配置和`.schema()`方法与*步骤2*中看到的`schema`变量一起添加到`listings.csv`文件中。
- en: '[PRE10]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: If everything is well set, you should see no output from this execution.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切设置正确，你应该不会看到此执行的任何输出。
- en: '**Checking the read DataFrame**: We can check the schema of our DataFrame by
    executing the following code:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**检查读取的DataFrame**：我们可以通过执行以下代码来检查我们DataFrame的模式：'
- en: '[PRE11]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'You should see the following output:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下输出：
- en: '![Figure 6.6 – listings.csv DataFrame schema](img/Figure_6.6_B19453.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图6.6 – listings.csv DataFrame模式](img/Figure_6.6_B19453.jpg)'
- en: Figure 6.6 – listings.csv DataFrame schema
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 – listings.csv DataFrame模式
- en: How it works…
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: We made a few additions in this exercise that differ from the last recipe, *Applying
    schemas to* *data ingestion.*
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '在这个练习中，我们做了一些与上一个食谱*应用模式到数据摄取*不同的添加。 '
- en: 'As usual, we started by importing the required methods to make the script work.
    We added three more data types: float, double, and date. The choice was made based
    on what the CSV file contained. Let’s look at the first lines of our file, as
    you can see in the following screenshot:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如同往常，我们首先导入使脚本工作的必需方法。我们添加了三种更多数据类型：float、double和date。选择是基于CSV文件的内容。让我们看看我们文件的第一个行，如下面的截图所示：
- en: '![Figure 6.7 – listings.csv view from Microsoft Excel](img/Figure_6.7_B19453.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图6.7 – 从Microsoft Excel查看listings.csv](img/Figure_6.7_B19453.jpg)'
- en: Figure 6.7 – listings.csv view from Microsoft Excel
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 – 从Microsoft Excel查看listings.csv
- en: 'We can observe different types of numerical fields; some require more decimal
    places, and `last_review` is in a date format. Because of this, we made additional
    library imports, as you can see in the following piece of code:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以观察到不同类型的数值字段；一些需要更多的小数位数，而`last_review`是日期格式。因此，我们添加了额外的库导入，如下面的代码片段所示：
- en: '[PRE12]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '*Step 2* is similar to what we did before, where we attributed the column names
    and their respective data types. It is in *step 3* that we made the schema attribution
    using the `schema()` method from the `SparkSession` class. If the schema contains
    the same number of columns as the file, we should expect no output here; otherwise,
    this message will appear:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤2*与之前我们所做的是相似的，其中我们分配了列名及其相应的数据类型。在*步骤3*中，我们使用`SparkSession`类的`schema()`方法进行了模式分配。如果模式包含与文件相同的列数，我们应该在这里看不到任何输出；否则，将出现此消息：'
- en: '![Figure 6.8 – Output warning message when the schema does not match](img/Figure_6.8_B19453.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图6.8 – 当模式不匹配时输出警告信息](img/Figure_6.8_B19453.jpg)'
- en: Figure 6.8 – Output warning message when the schema does not match
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 – 当模式不匹配时输出警告信息
- en: The content is important, even if it is a `WARN` log message. Looking closely,
    it says the schema does not match the number of columns in the file. This could
    be a problem later in the ETL pipeline when loading data into a data warehouse
    or any other analytical database.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是`WARN`日志消息，内容也很重要。仔细观察，它说模式与文件中的列数不匹配。这可能在ETL管道的后期加载数据到数据仓库或任何其他分析数据库时成为问题。
- en: There’s more…
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多……
- en: 'If you go back to [*Chapter 4*](B19453_04.xhtml#_idTextAnchor127), you will
    observe that one of our CSV readings contains an `inferSchema` parameter inserted
    into the `options()` method. Refer to the following code:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你回到[*第4章*](B19453_04.xhtml#_idTextAnchor127)，你会注意到我们CSV读取中的一个`inferSchema`参数被插入到`options()`方法中。参考以下代码：
- en: '[PRE13]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This parameter tells Spark to infer the data types based on the traits of the
    rows. For example, if a row has a value without quotation marks and is a number,
    there is a good chance this is an integer. However, if it contains a quotation
    mark, Spark can interpret it as a string, and any numerical operation will break.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 此参数告诉Spark根据行的特征推断数据类型。例如，如果一行没有引号且是数字，那么这很可能是一个整数。然而，如果它包含引号，Spark可以将其解释为字符串，任何数值操作都会失败。
- en: 'In the recipe, if we use `inferSchema`, we will see a very similar `printSchema`
    output from the schema we defined, except for some fields that were interpreted
    as `DoubleType` and that we declared as `FloatType` or `DateType`. This is shown
    in the following screenshot:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在食谱中，如果我们使用`inferSchema`，我们将看到与我们所定义的模式非常相似的`printSchema`输出，除了某些被解释为`DoubleType`的字段，而我们将其声明为`FloatType`或`DateType`。这如下面的截图所示：
- en: '![Figure 6.9 – Schema comparison when using a schema with inferSchema](img/Figure_6.9_B19453.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图6.9 – 使用带有inferSchema模式的模式比较输出](img/Figure_6.9_B19453.jpg)'
- en: Figure 6.9 – Schema comparison when using a schema with inferSchema
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9 – 使用带有inferSchema模式的模式比较输出
- en: Even though it seems like a tiny detail, when dealing with streaming or a large
    dataset and few computational resources, it can make a difference. Float types
    have a small range and bring in high processing power. Double data types bring
    more precision and are used to decrease mathematical errors or values being rounded
    by decimal data types.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 即使这看起来像是一个微不足道的细节，但在处理流数据或大数据集以及有限的计算资源时，它可能会产生影响。浮点类型具有较小的范围，并带来较高的处理能力。双精度数据类型提供了更多的精度，并用于减少数学误差或避免十进制数据类型四舍五入的值。
- en: See also
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'Read more about float and double types on the *Hackr IO* website here: [https://hackr.io/blog/float-vs-double](https://hackr.io/blog/float-vs-double).'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在*Hackr IO*网站上了解更多关于浮点型和双精度类型的信息：[https://hackr.io/blog/float-vs-double](https://hackr.io/blog/float-vs-double)。
- en: Importing unstructured data without a schema
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在没有模式的情况下导入非结构化数据
- en: As seen before, unstructured data or **NoSQL** is a group of information that
    does not follow a format, such as relational or tabular data. It can be presented
    as an image, video, metadata, transcripts, and so on. The data ingestion process
    usually involves a JSON file or a document collection, as we previously saw when
    ingesting data from **MongoDB**.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，非结构化数据或**NoSQL**是一组不遵循格式（如关系型或表格数据）的信息。它可以表示为图像、视频、元数据、转录等。数据摄取过程通常涉及JSON文件或文档集合，正如我们之前在从**MongoDB**摄取数据时所见。
- en: In this recipe, we will read a JSON file and transform it into a DataFrame without
    a schema. Although unstructured data is supposed to have a more flexible design,
    we will see some implications of not having any schema or structure in our DataFrame.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将读取一个JSON文件并将其转换为没有模式的DataFrame。尽管非结构化数据应该具有更灵活的设计，但我们将看到在没有模式或结构的情况下DataFrame的一些影响。
- en: Getting ready…
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作...
- en: 'Here, we will use the `holiday_brazil.json` file to create the DataFrame. You
    can find it in the GitHub repository here: [https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook).'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用`holiday_brazil.json`文件来创建DataFrame。你可以在GitHub仓库中找到它：[https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook)。
- en: We will use `SparkSession` to read the JSON file and create a DataFrame to ensure
    the session is up and running.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`SparkSession`来读取JSON文件并创建一个DataFrame，以确保会话处于运行状态。
- en: All code can be executed in a Jupyter Notebook or at PySpark shell.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 所有代码都可以在Jupyter Notebook或PySpark shell中执行。
- en: How to do it…
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Let’s now read our `holiday_brazil.json` file, observing how Spark handles
    it:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在读取我们的`holiday_brazil.json`文件，观察Spark是如何处理它的：
- en: '`multiline` as a parameter to the `options()` method. We will also let PySpark
    infer the data types in it:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`multiline`作为`options()`方法的参数。我们还将让PySpark推断其中的数据类型：
- en: '[PRE14]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: If all goes right, you should see no output.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，你应该看不到任何输出。
- en: '`printSchema()` method, we can see how PySpark interpreted the data types of
    each key:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`printSchema()`方法，我们可以看到PySpark如何解释每个键的数据类型：
- en: '[PRE15]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The following screenshot is the expected output:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是预期的输出：
- en: '![Figure 6.10 – holiday_brazil.json DataFrame schema](img/Figure_6.10_B19453.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![图6.10 – holiday_brazil.json DataFrame模式](img/Figure_6.10_B19453.jpg)'
- en: Figure 6.10 – holiday_brazil.json DataFrame schema
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10 – holiday_brazil.json DataFrame模式
- en: '`toPandas()` function to visualize our DataFrame better:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`toPandas()`函数来更好地可视化我们的DataFrame：
- en: '[PRE16]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'You should see this output:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下输出：
- en: '![Figure 6.11 – Output of toPandas() vision from the DataFrame](img/Figure_6.11_B19453.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![图6.11 – 使用带有inferSchema模式的模式比较输出](img/Figure_6.11_B19453.jpg)'
- en: Figure 6.11 – Output of toPandas() vision from the DataFrame
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11 – 从DataFrame中toPandas()视图的输出
- en: How it works…
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'Let’s take a look at the output from *step 2*:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下*步骤2*的输出：
- en: '![Figure 6.12 – Holiday_brasil.json DataFrame schema](img/Figure_6.12_B19453.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图6.12 – Holiday_brasil.json DataFrame模式](img/Figure_6.12_B19453.jpg)'
- en: Figure 6.12 – Holiday_brasil.json DataFrame schema
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.12 – Holiday_brasil.json DataFrame模式
- en: As we can observe, Spark only brought four columns, the first four keys in the
    JSON file, and ignored the rest of the other nested keys by keeping them inside
    the main four ones. This happens because Spark needs to handle the parameter better
    by flattening values from nested fields, even though we passed `multiline` as
    a parameter in the `options()` configuration.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所观察到的，Spark只带来了四个列，JSON文件中的前四个键，并且通过将它们保留在主要的四个键中忽略了其他嵌套键。这是因为Spark需要通过展平嵌套字段中的值来更好地处理参数，尽管我们在`options()`配置中传递了`multiline`作为参数。
- en: 'Another important point is that data types inferred for the `numeric` keys
    inside the `weekday` array are string values and should be integers. This happened
    because those values have quotation marks, as you can see in the following figure:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要点是，对`weekday`数组内`numeric`键推断的数据类型是字符串值，应该是整数。这是因为这些值有引号，正如你在以下图中可以看到的：
- en: '![Figure 6.13 – weekday objects](img/Figure_6.13_B19453.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图6.13 – 星期对象](img/Figure_6.13_B19453.jpg)'
- en: Figure 6.13 – weekday objects
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.13 – 星期对象
- en: Applying this to a real-world scenario where it is crucial to have a qualitative
    evaluation, these unformatted and schemaless DataFrame can create problems later
    when uploaded to a data warehouse. If a field suddenly changes its name or is
    unavailable in the source, it can lead to data inconsistency. There are some ways
    to solve this, and we will cover this further in the following recipe, *Ingesting
    unstructured data with a well-deﬁned schema* *and format.*
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 将此应用于一个需要定性评估的现实世界场景，这些未格式化和无模式的DataFrame在上传到数据仓库时可能会引起问题。如果一个字段突然更改了名称或在源中不可用，它可能导致数据不一致。有一些方法可以解决这个问题，我们将在下一个食谱*使用定义良好的模式和格式导入非结构化数据*中进一步介绍。
- en: However, there are plenty of other scenarios where a schema is not needed to
    be defined or unstructured data doesn’t need to be standardized. An excellent
    example is application logs or metadata, where data is often linked to the application’s
    or system’s availability to send the information. In that case, solutions such
    as **ElasticSearch**, **DynamoDB**, and many others are good storage options that
    provide query support. In other words, most of the issues here will be more inclined
    to generate a quantitative output.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，还有许多其他场景中不需要定义模式或非结构化数据不需要标准化。一个很好的例子是应用程序日志或元数据，其中数据通常与应用程序或系统的可用性相关联，以发送信息。在这种情况下，像**ElasticSearch**、**DynamoDB**和许多其他提供查询支持的存储选项都是好的选择。换句话说，这里的大部分问题将更倾向于生成定量输出。
- en: Ingesting unstructured data with a well-deﬁned schema and format
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用定义良好的模式和格式导入非结构化数据
- en: In the previous recipe, *Importing unstructured data without schema*, we read
    a JSON file without any schema or formatting application. This led us to an odd
    output, which could bring confusion and require additional work later in the data
    pipeline. While this example pertains specifically to a JSON file, it also applies
    to all other NoSQL or unstructured data that needs to be converted into analytical
    data.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个食谱中，*无模式导入非结构化数据*，我们读取了一个没有任何模式或格式化应用的JSON文件。这导致了一个奇怪的结果，可能会引起混淆，并在数据管道的后续阶段需要额外的工作。虽然这个例子具体涉及到一个JSON文件，但它也适用于所有其他需要转换为分析数据的NoSQL或非结构化数据。
- en: The objective is to continue the last recipe and apply a schema and standard
    to our data, making it more legible and easy to process in the subsequent phases
    of **ETL**.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是继续上一个食谱，并应用一个模式和标准到我们的数据中，使其在后续的**ETL**阶段更易于阅读和处理。
- en: Getting ready
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: This recipe has the exact same requirements as the *Importing unstructured data
    without a* *schema* recipe.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这个食谱与*无模式导入无* *模式*食谱有完全相同的要求。
- en: How to do it…
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点…
- en: 'We will perform the following steps to perform this recipe:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将执行以下步骤来完成这个食谱：
- en: '**Importing data types**: As usual, let’s start by importing our data types
    from the PySpark library:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**导入数据类型**：像往常一样，让我们从PySpark库中导入我们的数据类型：'
- en: '[PRE17]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '**Structuring the JSON schema**: Next, we set the schema based on how the JSON
    is structured:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**结构化JSON模式**：接下来，我们根据JSON的结构设置模式：'
- en: '[PRE18]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '`schema()` method to apply the schema created in the previous step:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用上一个步骤中创建的模式应用`schema()`方法：
- en: '[PRE19]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '`explode()` method, let’s broaden the fields inside the `holidays` column:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`explode()`方法，让我们扩展`holidays`列内的字段：
- en: '[PRE20]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '**Expanding more columns**: This is an optional step, but if needed, we can
    keep growing and flattening the other columns with nested content inside:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**扩展更多列**：这是一个可选步骤，但如果需要，我们可以继续增长并展平包含嵌套内容的其他列：'
- en: '[PRE21]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '`toPandas()` function, we can better view what our DataFrame looks like now:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `toPandas()` 函数，我们可以更好地查看我们的 DataFrame 现在的样子：
- en: '[PRE22]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'You should see the following output:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下输出：
- en: '![Figure 6.14 – The DataFrame with expanded columns](img/Figure_6.14_B19453.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.14 – 扩展列的 DataFrame](img/Figure_6.14_B19453.jpg)'
- en: Figure 6.14 – The DataFrame with expanded columns
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.14 – 扩展列的 DataFrame
- en: The final DataFrame can be saved as a Parquet file, which can make the next
    steps in the data pipeline easier to handle.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的 DataFrame 可以保存为 Parquet 文件，这将使数据管道中的下一步更容易处理。
- en: How it works…
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: As you can observe, this JSON file has some complexity due to the number of
    nested objects. When handling a file such as this, we can use several approaches.
    When coding, there are many approaches to reaching a solution. Let’s understand
    how this recipe works.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这个 JSON 文件由于嵌套对象的数量而具有一定的复杂性。当处理这样的文件时，我们可以使用多种方法。在编码时，有许多达到解决方案的方法。让我们了解这个食谱是如何工作的。
- en: In *step 1*, two new data types were imported—`ArrayType` and `MapType`. Even
    though there is some confusion when using each type, it is somewhat simple to
    understand when looking at the JSON structure.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *步骤 1* 中，导入了两种新的数据类型—`ArrayType` 和 `MapType`。尽管在使用每种类型时有些困惑，但当我们查看 JSON 结构时，它相对简单易懂。
- en: 'We used `ArrayType` for the `holiday` key because its structure looks like
    this:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `ArrayType` 为 `holiday` 键，因为它的结构看起来像这样：
- en: '[PRE23]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'It is an array (or a list if we use Python) of objects, each representing a
    holiday in Brazil. When `ArrayType` has other objects inside it, we need to re-use
    `StructType` to inform Spark of the structure of objects that reside inside. That’s
    why in *step 2*, our schema started to look like this:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 它是一个对象数组（如果我们使用 Python 则为列表），每个对象代表巴西的一个节日。当 `ArrayType` 包含其他对象时，我们需要重新使用 `StructType`
    来告知 Spark 内部对象的架构。这就是为什么在 *步骤 2* 中，我们的模式开始看起来是这样的：
- en: '[PRE24]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The following new data type is `MapType`. This type refers to an object with
    other objects inside. If we are only using Python, it can be referred to as a
    dictionary. PySpark extends this data type from a **superclass** in Spark, and
    you can read more about that here: [https://spark.apache.org/docs/2.0.1/api/java/index.xhtml?org/apache/spark/sql/types/MapType.xhtml](https://spark.apache.org/docs/2.0.1/api/java/index.xhtml?org/apache/spark/sql/types/MapType.xhtml).'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 以下新的数据类型是 `MapType`。这种类型指的是包含其他对象的对象。如果我们只使用 Python，它可以被称为字典。PySpark 从 Spark
    的 **超类** 中扩展了这种数据类型，你可以在这里了解更多信息：[https://spark.apache.org/docs/2.0.1/api/java/index.xhtml?org/apache/spark/sql/types/MapType.xhtml](https://spark.apache.org/docs/2.0.1/api/java/index.xhtml?org/apache/spark/sql/types/MapType.xhtml)。
- en: 'The syntax of `MapType` requires the key-value type, the value type, and whether
    it accepts null values. We used this type in the `weekday` field, as you can see
    here:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '`MapType` 的语法需要键值类型、值类型以及它是否接受空值。我们在 `weekday` 字段中使用了这种类型，正如你所见：'
- en: '[PRE25]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Maybe this is the most complex structure we have seen so far, and that was
    due to how JSON gets structured through it:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是我们迄今为止见过的最复杂的结构，这归因于 JSON 通过它的结构：
- en: '[PRE26]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The structure in our schema created `MapType` for `weekday` and the subsequent
    keys, `day` and `observed`.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模式中的结构为 `weekday` 以及随后的键 `day` 和 `observed` 创建了 `MapType`。
- en: 'Once our schema is defined and applied to our DataFrame, we can make a preview
    of it using `df_json.show()`. If the schema does not match the JSON structure,
    you should see this output:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们的模式被定义并应用到我们的 DataFrame 上，我们可以使用 `df_json.show()` 来预览它。如果模式与 JSON 结构不匹配，你应该看到以下输出：
- en: '![Figure 6.15 – df_json print](img/Figure_6.15_B19453.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.15 – df_json 打印](img/Figure_6.15_B19453.jpg)'
- en: Figure 6.15 – df_json print
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.15 – df_json 打印
- en: This demonstrates that Spark was unable to create the DataFrame correctly. The
    best action in this situation is to apply the schema step by step until the problem
    is resolved.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明 Spark 无法正确创建 DataFrame。在这种情况下，最好的行动是逐步应用模式，直到问题得到解决。
- en: 'To flatten our fields inside the `holidays` column, we need to use a function
    from PySpark called `explode`, as you can see here:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展平 `holidays` 列内的字段，我们需要使用 PySpark 中的一个函数 `explode`，正如你所见：
- en: '[PRE27]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'To apply the changes, we need to attribute the results to a variable where
    it will create a new DataFrame:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 要应用这些更改，我们需要将结果分配给一个变量，这将创建一个新的 DataFrame：
- en: '[PRE28]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Note
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Although it seems redundant, preventing the original DataFrame from being modified
    is a good practice. If anything goes wrong, we don’t need to reread the file because
    we have the intact state of the original DataFrame.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然看起来有些多余，但防止原始 DataFrame 被修改是一个好习惯。如果出了问题，我们不需要重新读取文件，因为我们有原始 DataFrame 的完整状态。
- en: Using the `select()` method, we select the column that will remain intact and
    expand the desired one. We do so because Spark requires at least one column flattened
    as a reference. As you can observe, the other columns regarding the status of
    the **API** ingestion were removed from this schema.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `select()` 方法，我们选择将保持不变的列并展开所需的列。我们这样做是因为 Spark 至少需要一个展开的列作为参考。正如你所观察到的，关于
    **API** 数据摄取状态的其它列已从该模式中移除。
- en: The second parameter of `select()` is the `explode()` method, where we pass
    the `holiday` column and attribute an alias. The second chain, `select()`, will
    only retrieve the `holidaysExplode` column. *Step 5* follows the same process
    but for the `weekdays` column.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '`select()` 方法的第二个参数是 `explode()` 方法，其中我们传递 `holiday` 列并赋予一个别名。第二个链式调用 `select()`
    将仅检索 `holidaysExplode` 列。*步骤 5* 遵循相同的流程，但针对的是 `weekdays` 列。'
- en: There’s more…
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容…
- en: 'As we previously discussed, there are many ways of flattening a JSON and applying
    a schema. You can see an example from Thomas at his *Medium blog* here: [https://medium.com/@thomaspt748/how-to-flatten-json-files-dynamically-using-apache-pyspark-c6b1b5fd4777](mailto:https://medium.com/@thomaspt748/how-to-flatten-json-files-dynamically-using-apache-pyspark-c6b1b5fd4777).'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，有许多方法可以展平 JSON 并应用模式。你可以在托马斯的 *Medium 博客* 上看到一个例子：[https://medium.com/@thomaspt748/how-to-flatten-json-files-dynamically-using-apache-pyspark-c6b1b5fd4777](https://medium.com/@thomaspt748/how-to-flatten-json-files-dynamically-using-apache-pyspark-c6b1b5fd4777).
- en: He uses Python functions to decouple the nested fields and then apply the PySpark
    code.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 他使用 Python 函数来解耦嵌套字段，然后应用 PySpark 代码。
- en: '*Towards Data Science* also offers a solution using Python’s lambda function.
    You can see it here: [https://towardsdatascience.com/flattening-json-records-using-pyspark-b83137669def](https://towardsdatascience.com/flattening-json-records-using-pyspark-b83137669def).'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '*数据科学之路* 也提供了一个使用 Python 的 lambda 函数的解决方案。你可以在这里看到它：[https://towardsdatascience.com/flattening-json-records-using-pyspark-b83137669def](https://towardsdatascience.com/flattening-json-records-using-pyspark-b83137669def).'
- en: Flattening the JSON file can be a fantastic approach but requires more knowledge
    of complex Python functions. Again, there is no right way of doing it; it is important
    to bring a solution that you and your team can support.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 展平 JSON 文件可以是一个极好的方法，但需要更多关于复杂 Python 函数的知识。同样，没有正确的方法来做这件事；重要的是要提供一个你和你团队都能支持的解决方案。
- en: See also
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'You can read more about the PySpark data structures here: [https://sparkbyexamples.com/pyspark/pyspark-structtype-and-structfield/](https://sparkbyexamples.com/pyspark/pyspark-structtype-and-structfield/).'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里了解更多关于 PySpark 数据结构的信息：[https://sparkbyexamples.com/pyspark/pyspark-structtype-and-structfield/](https://sparkbyexamples.com/pyspark/pyspark-structtype-and-structfield/).
- en: Inserting formatted SparkSession logs to facilitate your work
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 插入格式化的 SparkSession 日志以方便你的工作
- en: A commonly underestimated best practice is how to create valuable logs. Applications
    that log information and small code files can save a significant amount of debugging
    time. This is also true when ingesting or processing data.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常被低估的最佳实践是如何创建有价值的日志。记录信息和小型代码文件的应用程序可以节省大量的调试时间。这在摄取或处理数据时也是如此。
- en: This recipe approaches the best practice of logging events in our PySpark scripts.
    The examples here will give a more generic overview, which can be applied to any
    other piece of code and will even be used later in this book.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配方探讨了在 PySpark 脚本中记录事件的最佳实践。这里提供的示例将给出一个更通用的概述，它可以应用于任何其他代码片段，甚至会在本书的后续部分使用。
- en: Getting ready
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will use the `listings.csv` file to execute the `read` method from Spark.
    You can find this dataset inside the GitHub repository for this book. Make sure
    your `SparkSession` is up and running.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 `listings.csv` 文件来执行 Spark 的 `read` 方法。你可以在本书的 GitHub 仓库中找到这个数据集。确保你的
    `SparkSession` 正在运行。
- en: How to do it…
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点…
- en: 'Here are the steps to perform this recipe:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此配方的步骤如下：
- en: '`sparkContext`, we will assign the log level:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`sparkContext`，我们将分配日志级别：'
- en: '[PRE29]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '`getLogger()` method:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`getLogger()` 方法：'
- en: '[PRE30]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '`getLogger()` instantiated, we can now call the internal methods representing
    the log levels, such as `ERROR` or `INFO`:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`getLogger()` 实例化后，我们现在可以调用表示日志级别的内部方法，例如 `ERROR` 或 `INFO`：'
- en: '[PRE31]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This gives us the following output:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们以下输出：
- en: '![Figure 6.16 – Log message output example from Spark](img/Figure_6.16_B19453.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![图6.16 – Spark日志消息输出示例](img/Figure_6.16_B19453.jpg)'
- en: Figure 6.16 – Log message output example from Spark
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.16 – Spark日志消息输出示例
- en: '**Creating a DataFrame**: Now, let’s create a DataFrame using a file we have
    already seen in this chapter and observe the output:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**创建DataFrame**：现在，让我们使用本章中已经看到的一个文件来创建一个DataFrame，并观察输出：'
- en: '[PRE32]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'You should see the following output:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下输出：
- en: '![Figure 6.17 – PySpark logs output](img/Figure_6.17_B19453.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![图6.17 – PySpark日志输出](img/Figure_6.17_B19453.jpg)'
- en: Figure 6.17 – PySpark logs output
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.17 – PySpark日志输出
- en: Don’t worry if more lines are showing in your console; the image was cut to
    make it more readable.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的控制台显示了更多行，不要担心；图片被裁剪以使其更易于阅读。
- en: How it works…
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: Let’s explore a bit of what was done in this recipe. Spark has a native library
    called `log4j`, or `rootLogger`. `log4j` is the default logging mechanism Spark
    uses to throw all log messages such as `TRACE`, `DEBUG`, `INFO`, `WARN`, `ERROR`,
    and `FATAL`. The severity of the message increases with each level. By default,
    Spark has logs at the `WARN` level, so new messages started to appear when we
    set it to `INFO`, such as memory store information.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索一下这个配方中做了些什么。Spark有一个名为`log4j`或`rootLogger`的本地库。`log4j`是Spark使用的默认日志机制，用于抛出所有日志消息，如`TRACE`、`DEBUG`、`INFO`、`WARN`、`ERROR`和`FATAL`。消息的严重性随着每个级别的增加而增加。默认情况下，Spark的日志级别为`WARN`，因此当我们将其设置为`INFO`时，开始出现新的消息，例如内存存储信息。
- en: We will observe more `INFO` logs when we execute a `spark-submit` command to
    run PySpark (we will cover this later in [*Chapter 11*](B19453_11.xhtml#_idTextAnchor402)).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们执行`spark-submit`命令来运行PySpark时，我们将观察到更多的`INFO`日志（我们将在[第11章](B19453_11.xhtml#_idTextAnchor402)中稍后介绍）。
- en: 'Depending on how big our script is and which environment it belongs to, it
    is a good practice to set it only to show `ERROR` messages. We can change the
    log level with the following code:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的脚本大小和所属的环境，只将其设置为显示`ERROR`消息是一个好习惯。我们可以使用以下代码更改日志级别：
- en: '[PRE33]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'As you can observe, we used `sparkContext` to set the log level. `sparkContext`
    is a crucial component in Spark applications. It manages the cluster resources,
    coordinates the execution of tasks, and provides an interface for interacting
    with distributed datasets and performing computations in a distributed and parallel
    manner. Defining the log level of our code will prevent levels below `ERROR` from
    appearing on the console, making it cleaner:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们使用`sparkContext`来设置日志级别。`sparkContext`是Spark应用程序中的一个关键组件。它管理集群资源，协调任务执行，并为与分布式数据集交互以及以分布式和并行方式执行计算提供接口。定义我们代码的日志级别将防止`ERROR`级别以下的级别出现在控制台上，使其更干净：
- en: '[PRE34]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Next, we retrieved our `log4j` module and its `Logger` class from the instantiated
    session. This class has a method to show logs already formatted as Spark does.
    The `__name__` parameter will retrieve the name of the current module from the
    Python internals; in our case, it is `main`.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们从实例化的会话中检索了`log4j`模块及其`Logger`类。这个类有一个方法可以显示像Spark那样格式化的日志。`__name__`参数将从Python内部检索当前模块的名称；在我们的例子中，它是`main`。
- en: 'With this, we can create customized logs as follows:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们可以创建如下自定义日志：
- en: '[PRE35]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'And, of course, you can ally the PySpark logs with your Python output for a
    complete solution using a `try…except` exception-handling closure. Let’s simulate
    an error by passing the wrong filename to our reading function as follows:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你也可以将PySpark日志与Python输出结合起来，使用`try...except`异常处理闭包来获得完整的解决方案。让我们通过将错误的文件名传递给我们的读取函数来模拟一个错误，如下所示：
- en: '[PRE36]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'You should see the following output message:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到以下输出消息：
- en: '![Figure 6.18 – Error message formatted by log4j](img/Figure_6.18_B19453.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![图6.18 – 由log4j格式化的错误消息](img/Figure_6.18_B19453.jpg)'
- en: Figure 6.18 – Error message formatted by log4j
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.18 – 由log4j格式化的错误消息
- en: There’s more…
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多内容…
- en: Many more customizations are available in `log4j`, but this might require a
    little more work. For example, you can change some `WARN` messages to appear as
    `ERROR` messages and prevent the script from continuing to be processed. A practical
    example in this chapter would be the *Importing structured data using a well-deﬁned
    schema* recipe, when the number of columns in the schema does not match the file.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在`log4j`中还有许多可用的自定义选项，但这可能需要一点更多的工作。例如，你可以将一些`WARN`消息更改为`ERROR`消息，以防止脚本继续被处理。本章的一个实际例子是使用**预定义的架构**导入结构化数据时，架构中的列数与文件不匹配。
- en: 'You can read more about it on *Ivan Trusov*’s blog page here: [https://polarpersonal.medium.com/writing-pyspark-logs-in-apache-spark-and-databricks-8590c28d1d51](https://polarpersonal.medium.com/writing-pyspark-logs-in-apache-spark-and-databricks-8590c28d1d51).'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 *Ivan Trusov* 的博客页面上了解更多相关信息：[https://polarpersonal.medium.com/writing-pyspark-logs-in-apache-spark-and-databricks-8590c28d1d51](https://polarpersonal.medium.com/writing-pyspark-logs-in-apache-spark-and-databricks-8590c28d1d51).
- en: See also
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另请参阅
- en: 'You can find more about the PySpark best practices here: [https://climbtheladder.com/10-pyspark-logging-best-practices/](https://climbtheladder.com/10-pyspark-logging-best-practices/).'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以在这里找到更多关于 PySpark 最佳实践的介绍：[https://climbtheladder.com/10-pyspark-logging-best-practices/](https://climbtheladder.com/10-pyspark-logging-best-practices/).
- en: 'Read more about `sparkContext` and how it works in the Spark official documentation
    here: [https://spark.apache.org/docs/3.2.0/api/java/org/apache/spark/SparkContext.xhtml](https://spark.apache.org/docs/3.2.0/api/java/org/apache/spark/SparkContext.xhtml).'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Spark 官方文档中了解更多关于 `sparkContext` 及其工作原理的信息：[https://spark.apache.org/docs/3.2.0/api/java/org/apache/spark/SparkContext.xhtml](https://spark.apache.org/docs/3.2.0/api/java/org/apache/spark/SparkContext.xhtml).
- en: Further reading
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '[https://www.tibco.com/reference-center/what-is-structured-data](https://www.tibco.com/reference-center/what-is-structured-data)'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.tibco.com/reference-center/what-is-structured-data](https://www.tibco.com/reference-center/what-is-structured-data)'
- en: '[https://spark.apache.org/docs/latest/sql-programming-guide.xhtml](https://spark.apache.org/docs/latest/sql-programming-guide.xhtml)'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://spark.apache.org/docs/latest/sql-programming-guide.xhtml](https://spark.apache.org/docs/latest/sql-programming-guide.xhtml)'
- en: '[https://mungingdata.com/pyspark/schema-structtype-structfield/](https://mungingdata.com/pyspark/schema-structtype-structfield/)'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://mungingdata.com/pyspark/schema-structtype-structfield/](https://mungingdata.com/pyspark/schema-structtype-structfield/)'
- en: '[https://sparkbyexamples.com/pyspark/pyspark-select-nested-struct-columns/](https://sparkbyexamples.com/pyspark/pyspark-select-nested-struct-columns/)'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://sparkbyexamples.com/pyspark/pyspark-select-nested-struct-columns/](https://sparkbyexamples.com/pyspark/pyspark-select-nested-struct-columns/)'
- en: '[https://benalexkeen.com/using-pyspark-to-read-and-flatten-json-data-using-an-enforced-schema/](https://benalexkeen.com/using-pyspark-to-read-and-flatten-json-data-using-an-enforced-schema/)'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://benalexkeen.com/using-pyspark-to-read-and-flatten-json-data-using-an-enforced-schema/](https://benalexkeen.com/using-pyspark-to-read-and-flatten-json-data-using-an-enforced-schema/)'
- en: '[https://towardsdatascience.com/json-in-databricks-and-pyspark-26437352f0e9](https://towardsdatascience.com/json-in-databricks-and-pyspark-26437352f0e9)'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://towardsdatascience.com/json-in-databricks-and-pyspark-26437352f0e9](https://towardsdatascience.com/json-in-databricks-and-pyspark-26437352f0e9)'
