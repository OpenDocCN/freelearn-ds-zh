- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using PySpark with Deﬁned and Non-Deﬁned Schemas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generally, schemas are forms used to create or apply structures to data. As
    someone who works or will work with large volumes of data, it is essential to
    understand how to manipulate DataFrames and apply structure when it is necessary
    to bring more context to the information involved.
  prefs: []
  type: TYPE_NORMAL
- en: However, as seen in the previous chapters, data can come from different sources
    or be present without a well-defined structure, and applying a schema can be challenging.
    Here, we will see how to create schemas and standard formats using PySpark with
    structured and unstructured data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Applying schemas to data ingestion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Importing structured data using a well-deﬁned schema
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Importing unstructured data with an undefined schema
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ingesting unstructured data with a well-deﬁned schema and format
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inserting formatted SparkSession logs to facilitate your work
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can also find the code for this chapter in the GitHub repository here:
    [https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using **Jupyter Notebook** is not mandatory but can help you see how the code
    works interactively. Since we will execute Python and PySpark code, it can help
    us understand the scripts better. Once you have it installed, you can execute
    Jupyter using the following line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: It is recommended to create a separate folder to store the Python files or Notebooks
    we will cover in this chapter; however, feel free to organize the files in the
    best way that fits you.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, all recipes will need a `SparkSession` instance initialized,
    and you can use the same session for all of them. You can use the following code
    to create your session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: A `WARN` message as output is expected in some cases, especially if you are
    using WSL on Windows, so you don’t need to worry.
  prefs: []
  type: TYPE_NORMAL
- en: Applying schemas to data ingestion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The application of schemas is common practice when ingesting data, and PySpark
    natively supports applying them to DataFrames. To define and apply schemas to
    our DataFrames, we need to understand some concepts of Spark.
  prefs: []
  type: TYPE_NORMAL
- en: This recipe introduces the basic concept of working with schemas using PySpark
    and its best practices so that we can later apply them to structured and unstructured
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Make sure PySpark is installed and working on your machine for this recipe.
    You can run the following code on your command line to check this requirement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – PySpark version console output](img/Figure_6.1_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – PySpark version console output
  prefs: []
  type: TYPE_NORMAL
- en: If don’t have PySpark installed on your local machine, please refer to the *Installing
    PySpark* recipe in [*Chapter 1*](B19453_01.xhtml#_idTextAnchor022).
  prefs: []
  type: TYPE_NORMAL
- en: 'I will use Jupyter Notebook to execute the code to make it more interactive.
    You can use this link and follow the instructions on the screen to install it:
    [https://jupyter.org/install](https://jupyter.org/install).'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you already have it installed, check the version using the following code
    on your command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the expected output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Jupyter package versions](img/Figure_6.2_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – Jupyter package versions
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the Notebook version was `6.4.4` at the time this book was written.
    Make sure to always use the latest version.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are the steps to carry out the recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Creating mock data**: Before applying the schema to our DataFrame, we need
    to create a simple set containing simulated data of people’s information in this
    format—ID, name, last name, age, and gender:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Importing and structuring the schema**: The next step is to import the types
    and create the structure of our schema:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Creating the DataFrame**: Then, we make the DataFrame, applying the schema
    we have created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When printing our DataFrame schema using the `.printSchema()` method, this
    is the expected output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – The DataFrame schema](img/Figure_6.3_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – The DataFrame schema
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before understanding the methods in *step 2*, let’s step back a bit and understand
    the concept of a DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: DataFrame is like a table with data stored and organized in a two-dimensional
    array, which resembles a table from a relational database such as MySQL or Postgres.
    Each line corresponds to a record, and libraries such as pandas and PySpark, by
    default, assign a record number internally to each line (or index).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – GeeksforGeeks DataFrame explanation](img/Figure_6.4_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – GeeksforGeeks DataFrame explanation
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Speaking of the Pandas library, it is common to refer to a column in a Pandas
    DataFrame as a series and expect it to behave like a Python list. It makes it
    easier to manipulate data for analysis and visualization.
  prefs: []
  type: TYPE_NORMAL
- en: The objective of using a DataFrame is to utilize the several optimizations for
    data processing under the hood that Spark brings, which are directly linked to
    parallel processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Back to the schema definition in our `schema` variable, let’s take a look at
    the code we created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The first object we declare is the `StructType` class. This class will create
    an object of collection or our rows. Next, we declare a `StructField` instance,
    representing our column with its name, data type, whether it is nullable, and
    its metadata when applicable. `StructField` must be in the same order as the columns
    in the DataFrame; otherwise, it can generate errors due to the incompatibility
    of a data type (for example, the column has string values, and we are setting
    it as an integer) or the presence of null values. Defining `StructField` is an
    excellent opportunity to standardize the name of the DataFrame and, therefore,
    the analytical data.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, `StringType` and `IntegerType` are the methods that will cast the data
    type into the respective columns. They were imported at the beginning of the recipe
    and derived from the SQL types inside PySpark. In our mock data example, we defined
    the `id` and `age` columns as `IntegerType` since we expect no other kind of data
    in them. However, there are many situations where the `id` column is referred
    to as a string type, usually when the data comes from different systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here we used `StringType` and `IntegerType`, but many others can be used to
    create context and standardize our data. Refer to the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – SparkbyExample table of data types in Spark](img/Figure_6.5_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 – SparkbyExample table of data types in Spark
  prefs: []
  type: TYPE_NORMAL
- en: 'You can understand more about how to apply the **Spark data types** in the
    Spark official documentation here: [https://spark.apache.org/docs/latest/sql-ref-datatypes.xhtml](https://spark.apache.org/docs/latest/sql-ref-datatypes.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When handling terminology, there needs to be a common understanding about using
    a dataset or DataFrame, especially if you are a newcomer to the data world.
  prefs: []
  type: TYPE_NORMAL
- en: A dataset is a collection of data containing rows and columns (for example,
    relational data) or documents and files (for example, non-relational data). It
    comes from a source and is available in different file formats.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, a DataFrame is derived from a dataset, presenting the data
    in a tabular form even if that is not the primary format. The DataFrame can transform
    a MongoDB document collection into a tabular organization based on the configurations
    set when it is created.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'More examples on the *SparkbyExample* site can be found here: [https://sparkbyexamples.com/pyspark/pyspark-sql-types-datatype-with-examples/](https://sparkbyexamples.com/pyspark/pyspark-sql-types-datatype-with-examples/).'
  prefs: []
  type: TYPE_NORMAL
- en: Importing structured data using a well-deﬁned schema
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As seen in the previous chapter, *Ingesting Data from Structured and Unstructured
    Databases*, structured data has a standard format presented in rows and columns
    and is often stored inside a database.
  prefs: []
  type: TYPE_NORMAL
- en: Due to its format, the application of a DataFrame schema tends to be less complex
    and has several benefits, such as ensuring the ingested information is the same
    as the data source or follows a rule.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will ingest data from a structured file such as a CSV file
    and apply a DataFrame schema to understand better how it is used in a real-world
    scenario.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This exercise requires the `listings.csv` file found inside the GitHub repository
    for this book. Also, make sure your `SparkSession` is initialized.
  prefs: []
  type: TYPE_NORMAL
- en: All the code in this recipe can be executed in Jupyter Notebook cells or a PySpark
    shell.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are the steps to perform this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '`StringType` and `IntegerType`, we will include two more data types in our
    import, `FloatType` and `DateType`, shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`StructField` and assign them to a respective data type:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`listings.csv` file with the `.options()` configurations and add the `.schema()`
    method with the `schema` variable seen in *step 2*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If everything is well set, you should see no output from this execution.
  prefs: []
  type: TYPE_NORMAL
- en: '**Checking the read DataFrame**: We can check the schema of our DataFrame by
    executing the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – listings.csv DataFrame schema](img/Figure_6.6_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 – listings.csv DataFrame schema
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We made a few additions in this exercise that differ from the last recipe, *Applying
    schemas to* *data ingestion.*
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, we started by importing the required methods to make the script work.
    We added three more data types: float, double, and date. The choice was made based
    on what the CSV file contained. Let’s look at the first lines of our file, as
    you can see in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7 – listings.csv view from Microsoft Excel](img/Figure_6.7_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 – listings.csv view from Microsoft Excel
  prefs: []
  type: TYPE_NORMAL
- en: 'We can observe different types of numerical fields; some require more decimal
    places, and `last_review` is in a date format. Because of this, we made additional
    library imports, as you can see in the following piece of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '*Step 2* is similar to what we did before, where we attributed the column names
    and their respective data types. It is in *step 3* that we made the schema attribution
    using the `schema()` method from the `SparkSession` class. If the schema contains
    the same number of columns as the file, we should expect no output here; otherwise,
    this message will appear:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8 – Output warning message when the schema does not match](img/Figure_6.8_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 – Output warning message when the schema does not match
  prefs: []
  type: TYPE_NORMAL
- en: The content is important, even if it is a `WARN` log message. Looking closely,
    it says the schema does not match the number of columns in the file. This could
    be a problem later in the ETL pipeline when loading data into a data warehouse
    or any other analytical database.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you go back to [*Chapter 4*](B19453_04.xhtml#_idTextAnchor127), you will
    observe that one of our CSV readings contains an `inferSchema` parameter inserted
    into the `options()` method. Refer to the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This parameter tells Spark to infer the data types based on the traits of the
    rows. For example, if a row has a value without quotation marks and is a number,
    there is a good chance this is an integer. However, if it contains a quotation
    mark, Spark can interpret it as a string, and any numerical operation will break.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the recipe, if we use `inferSchema`, we will see a very similar `printSchema`
    output from the schema we defined, except for some fields that were interpreted
    as `DoubleType` and that we declared as `FloatType` or `DateType`. This is shown
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.9 – Schema comparison when using a schema with inferSchema](img/Figure_6.9_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.9 – Schema comparison when using a schema with inferSchema
  prefs: []
  type: TYPE_NORMAL
- en: Even though it seems like a tiny detail, when dealing with streaming or a large
    dataset and few computational resources, it can make a difference. Float types
    have a small range and bring in high processing power. Double data types bring
    more precision and are used to decrease mathematical errors or values being rounded
    by decimal data types.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Read more about float and double types on the *Hackr IO* website here: [https://hackr.io/blog/float-vs-double](https://hackr.io/blog/float-vs-double).'
  prefs: []
  type: TYPE_NORMAL
- en: Importing unstructured data without a schema
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As seen before, unstructured data or **NoSQL** is a group of information that
    does not follow a format, such as relational or tabular data. It can be presented
    as an image, video, metadata, transcripts, and so on. The data ingestion process
    usually involves a JSON file or a document collection, as we previously saw when
    ingesting data from **MongoDB**.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will read a JSON file and transform it into a DataFrame without
    a schema. Although unstructured data is supposed to have a more flexible design,
    we will see some implications of not having any schema or structure in our DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here, we will use the `holiday_brazil.json` file to create the DataFrame. You
    can find it in the GitHub repository here: [https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook).'
  prefs: []
  type: TYPE_NORMAL
- en: We will use `SparkSession` to read the JSON file and create a DataFrame to ensure
    the session is up and running.
  prefs: []
  type: TYPE_NORMAL
- en: All code can be executed in a Jupyter Notebook or at PySpark shell.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s now read our `holiday_brazil.json` file, observing how Spark handles
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '`multiline` as a parameter to the `options()` method. We will also let PySpark
    infer the data types in it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If all goes right, you should see no output.
  prefs: []
  type: TYPE_NORMAL
- en: '`printSchema()` method, we can see how PySpark interpreted the data types of
    each key:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following screenshot is the expected output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.10 – holiday_brazil.json DataFrame schema](img/Figure_6.10_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.10 – holiday_brazil.json DataFrame schema
  prefs: []
  type: TYPE_NORMAL
- en: '`toPandas()` function to visualize our DataFrame better:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see this output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.11 – Output of toPandas() vision from the DataFrame](img/Figure_6.11_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.11 – Output of toPandas() vision from the DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s take a look at the output from *step 2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.12 – Holiday_brasil.json DataFrame schema](img/Figure_6.12_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.12 – Holiday_brasil.json DataFrame schema
  prefs: []
  type: TYPE_NORMAL
- en: As we can observe, Spark only brought four columns, the first four keys in the
    JSON file, and ignored the rest of the other nested keys by keeping them inside
    the main four ones. This happens because Spark needs to handle the parameter better
    by flattening values from nested fields, even though we passed `multiline` as
    a parameter in the `options()` configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another important point is that data types inferred for the `numeric` keys
    inside the `weekday` array are string values and should be integers. This happened
    because those values have quotation marks, as you can see in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.13 – weekday objects](img/Figure_6.13_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.13 – weekday objects
  prefs: []
  type: TYPE_NORMAL
- en: Applying this to a real-world scenario where it is crucial to have a qualitative
    evaluation, these unformatted and schemaless DataFrame can create problems later
    when uploaded to a data warehouse. If a field suddenly changes its name or is
    unavailable in the source, it can lead to data inconsistency. There are some ways
    to solve this, and we will cover this further in the following recipe, *Ingesting
    unstructured data with a well-deﬁned schema* *and format.*
  prefs: []
  type: TYPE_NORMAL
- en: However, there are plenty of other scenarios where a schema is not needed to
    be defined or unstructured data doesn’t need to be standardized. An excellent
    example is application logs or metadata, where data is often linked to the application’s
    or system’s availability to send the information. In that case, solutions such
    as **ElasticSearch**, **DynamoDB**, and many others are good storage options that
    provide query support. In other words, most of the issues here will be more inclined
    to generate a quantitative output.
  prefs: []
  type: TYPE_NORMAL
- en: Ingesting unstructured data with a well-deﬁned schema and format
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipe, *Importing unstructured data without schema*, we read
    a JSON file without any schema or formatting application. This led us to an odd
    output, which could bring confusion and require additional work later in the data
    pipeline. While this example pertains specifically to a JSON file, it also applies
    to all other NoSQL or unstructured data that needs to be converted into analytical
    data.
  prefs: []
  type: TYPE_NORMAL
- en: The objective is to continue the last recipe and apply a schema and standard
    to our data, making it more legible and easy to process in the subsequent phases
    of **ETL**.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe has the exact same requirements as the *Importing unstructured data
    without a* *schema* recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will perform the following steps to perform this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Importing data types**: As usual, let’s start by importing our data types
    from the PySpark library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Structuring the JSON schema**: Next, we set the schema based on how the JSON
    is structured:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`schema()` method to apply the schema created in the previous step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`explode()` method, let’s broaden the fields inside the `holidays` column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Expanding more columns**: This is an optional step, but if needed, we can
    keep growing and flattening the other columns with nested content inside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`toPandas()` function, we can better view what our DataFrame looks like now:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.14 – The DataFrame with expanded columns](img/Figure_6.14_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.14 – The DataFrame with expanded columns
  prefs: []
  type: TYPE_NORMAL
- en: The final DataFrame can be saved as a Parquet file, which can make the next
    steps in the data pipeline easier to handle.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you can observe, this JSON file has some complexity due to the number of
    nested objects. When handling a file such as this, we can use several approaches.
    When coding, there are many approaches to reaching a solution. Let’s understand
    how this recipe works.
  prefs: []
  type: TYPE_NORMAL
- en: In *step 1*, two new data types were imported—`ArrayType` and `MapType`. Even
    though there is some confusion when using each type, it is somewhat simple to
    understand when looking at the JSON structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'We used `ArrayType` for the `holiday` key because its structure looks like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'It is an array (or a list if we use Python) of objects, each representing a
    holiday in Brazil. When `ArrayType` has other objects inside it, we need to re-use
    `StructType` to inform Spark of the structure of objects that reside inside. That’s
    why in *step 2*, our schema started to look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The following new data type is `MapType`. This type refers to an object with
    other objects inside. If we are only using Python, it can be referred to as a
    dictionary. PySpark extends this data type from a **superclass** in Spark, and
    you can read more about that here: [https://spark.apache.org/docs/2.0.1/api/java/index.xhtml?org/apache/spark/sql/types/MapType.xhtml](https://spark.apache.org/docs/2.0.1/api/java/index.xhtml?org/apache/spark/sql/types/MapType.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The syntax of `MapType` requires the key-value type, the value type, and whether
    it accepts null values. We used this type in the `weekday` field, as you can see
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Maybe this is the most complex structure we have seen so far, and that was
    due to how JSON gets structured through it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The structure in our schema created `MapType` for `weekday` and the subsequent
    keys, `day` and `observed`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once our schema is defined and applied to our DataFrame, we can make a preview
    of it using `df_json.show()`. If the schema does not match the JSON structure,
    you should see this output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.15 – df_json print](img/Figure_6.15_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.15 – df_json print
  prefs: []
  type: TYPE_NORMAL
- en: This demonstrates that Spark was unable to create the DataFrame correctly. The
    best action in this situation is to apply the schema step by step until the problem
    is resolved.
  prefs: []
  type: TYPE_NORMAL
- en: 'To flatten our fields inside the `holidays` column, we need to use a function
    from PySpark called `explode`, as you can see here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'To apply the changes, we need to attribute the results to a variable where
    it will create a new DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Although it seems redundant, preventing the original DataFrame from being modified
    is a good practice. If anything goes wrong, we don’t need to reread the file because
    we have the intact state of the original DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Using the `select()` method, we select the column that will remain intact and
    expand the desired one. We do so because Spark requires at least one column flattened
    as a reference. As you can observe, the other columns regarding the status of
    the **API** ingestion were removed from this schema.
  prefs: []
  type: TYPE_NORMAL
- en: The second parameter of `select()` is the `explode()` method, where we pass
    the `holiday` column and attribute an alias. The second chain, `select()`, will
    only retrieve the `holidaysExplode` column. *Step 5* follows the same process
    but for the `weekdays` column.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we previously discussed, there are many ways of flattening a JSON and applying
    a schema. You can see an example from Thomas at his *Medium blog* here: [https://medium.com/@thomaspt748/how-to-flatten-json-files-dynamically-using-apache-pyspark-c6b1b5fd4777](mailto:https://medium.com/@thomaspt748/how-to-flatten-json-files-dynamically-using-apache-pyspark-c6b1b5fd4777).'
  prefs: []
  type: TYPE_NORMAL
- en: He uses Python functions to decouple the nested fields and then apply the PySpark
    code.
  prefs: []
  type: TYPE_NORMAL
- en: '*Towards Data Science* also offers a solution using Python’s lambda function.
    You can see it here: [https://towardsdatascience.com/flattening-json-records-using-pyspark-b83137669def](https://towardsdatascience.com/flattening-json-records-using-pyspark-b83137669def).'
  prefs: []
  type: TYPE_NORMAL
- en: Flattening the JSON file can be a fantastic approach but requires more knowledge
    of complex Python functions. Again, there is no right way of doing it; it is important
    to bring a solution that you and your team can support.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can read more about the PySpark data structures here: [https://sparkbyexamples.com/pyspark/pyspark-structtype-and-structfield/](https://sparkbyexamples.com/pyspark/pyspark-structtype-and-structfield/).'
  prefs: []
  type: TYPE_NORMAL
- en: Inserting formatted SparkSession logs to facilitate your work
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A commonly underestimated best practice is how to create valuable logs. Applications
    that log information and small code files can save a significant amount of debugging
    time. This is also true when ingesting or processing data.
  prefs: []
  type: TYPE_NORMAL
- en: This recipe approaches the best practice of logging events in our PySpark scripts.
    The examples here will give a more generic overview, which can be applied to any
    other piece of code and will even be used later in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use the `listings.csv` file to execute the `read` method from Spark.
    You can find this dataset inside the GitHub repository for this book. Make sure
    your `SparkSession` is up and running.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are the steps to perform this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sparkContext`, we will assign the log level:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`getLogger()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`getLogger()` instantiated, we can now call the internal methods representing
    the log levels, such as `ERROR` or `INFO`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This gives us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.16 – Log message output example from Spark](img/Figure_6.16_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.16 – Log message output example from Spark
  prefs: []
  type: TYPE_NORMAL
- en: '**Creating a DataFrame**: Now, let’s create a DataFrame using a file we have
    already seen in this chapter and observe the output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.17 – PySpark logs output](img/Figure_6.17_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.17 – PySpark logs output
  prefs: []
  type: TYPE_NORMAL
- en: Don’t worry if more lines are showing in your console; the image was cut to
    make it more readable.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s explore a bit of what was done in this recipe. Spark has a native library
    called `log4j`, or `rootLogger`. `log4j` is the default logging mechanism Spark
    uses to throw all log messages such as `TRACE`, `DEBUG`, `INFO`, `WARN`, `ERROR`,
    and `FATAL`. The severity of the message increases with each level. By default,
    Spark has logs at the `WARN` level, so new messages started to appear when we
    set it to `INFO`, such as memory store information.
  prefs: []
  type: TYPE_NORMAL
- en: We will observe more `INFO` logs when we execute a `spark-submit` command to
    run PySpark (we will cover this later in [*Chapter 11*](B19453_11.xhtml#_idTextAnchor402)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on how big our script is and which environment it belongs to, it
    is a good practice to set it only to show `ERROR` messages. We can change the
    log level with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can observe, we used `sparkContext` to set the log level. `sparkContext`
    is a crucial component in Spark applications. It manages the cluster resources,
    coordinates the execution of tasks, and provides an interface for interacting
    with distributed datasets and performing computations in a distributed and parallel
    manner. Defining the log level of our code will prevent levels below `ERROR` from
    appearing on the console, making it cleaner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Next, we retrieved our `log4j` module and its `Logger` class from the instantiated
    session. This class has a method to show logs already formatted as Spark does.
    The `__name__` parameter will retrieve the name of the current module from the
    Python internals; in our case, it is `main`.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this, we can create customized logs as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'And, of course, you can ally the PySpark logs with your Python output for a
    complete solution using a `try…except` exception-handling closure. Let’s simulate
    an error by passing the wrong filename to our reading function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following output message:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.18 – Error message formatted by log4j](img/Figure_6.18_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.18 – Error message formatted by log4j
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many more customizations are available in `log4j`, but this might require a
    little more work. For example, you can change some `WARN` messages to appear as
    `ERROR` messages and prevent the script from continuing to be processed. A practical
    example in this chapter would be the *Importing structured data using a well-deﬁned
    schema* recipe, when the number of columns in the schema does not match the file.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can read more about it on *Ivan Trusov*’s blog page here: [https://polarpersonal.medium.com/writing-pyspark-logs-in-apache-spark-and-databricks-8590c28d1d51](https://polarpersonal.medium.com/writing-pyspark-logs-in-apache-spark-and-databricks-8590c28d1d51).'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can find more about the PySpark best practices here: [https://climbtheladder.com/10-pyspark-logging-best-practices/](https://climbtheladder.com/10-pyspark-logging-best-practices/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Read more about `sparkContext` and how it works in the Spark official documentation
    here: [https://spark.apache.org/docs/3.2.0/api/java/org/apache/spark/SparkContext.xhtml](https://spark.apache.org/docs/3.2.0/api/java/org/apache/spark/SparkContext.xhtml).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[https://www.tibco.com/reference-center/what-is-structured-data](https://www.tibco.com/reference-center/what-is-structured-data)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://spark.apache.org/docs/latest/sql-programming-guide.xhtml](https://spark.apache.org/docs/latest/sql-programming-guide.xhtml)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://mungingdata.com/pyspark/schema-structtype-structfield/](https://mungingdata.com/pyspark/schema-structtype-structfield/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://sparkbyexamples.com/pyspark/pyspark-select-nested-struct-columns/](https://sparkbyexamples.com/pyspark/pyspark-select-nested-struct-columns/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://benalexkeen.com/using-pyspark-to-read-and-flatten-json-data-using-an-enforced-schema/](https://benalexkeen.com/using-pyspark-to-read-and-flatten-json-data-using-an-enforced-schema/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://towardsdatascience.com/json-in-databricks-and-pyspark-26437352f0e9](https://towardsdatascience.com/json-in-databricks-and-pyspark-26437352f0e9)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
