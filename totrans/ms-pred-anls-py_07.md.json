["```py\n>>> from tensorflow.examples.tutorials.mnist import input_data\n>>> mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n\n```", "```py\n>>> len(mnist.train.images)\n>>> len(mnist.test.images)\n\n```", "```py\n>>> from skimage import io\n>> io.imshow(np.reshape(mnist.train.images[0],(28,28))) \n\n```", "```py\n>>> mnist.train.labels[0]\n\n```", "```py\n>>> def weight_variable(dimensions,stddev):\n …     return tf.Variable(tf.truncated_normal(dimensions, stddev=stddev))\n\n>>> def bias_variable(dimensions,constant):\n…      return tf.Variable(tf.constant(constant, shape=dimensions))\n\n>>> def two_dimensional_convolutional_layer(x, W, strides, padding):\n …     return tf.nn.conv2d(x, W, strides=strides, padding=padding)\n\n>>> def max_pooling(x,strides,ksize,padding):\n…     return tf.nn.max_pool(x, ksize=ksize,strides=strides, padding=padding)\n\n>>> def generate_network(weight_variables,\\\n bias_variables,\\\n relu_layers,\\\n pooling_layers,\\\n fully_connected_layers,\\\n inputs,\\\n conv_strides,\\\n pool_stries,\\\n ksize,\\\n output_channels,\\\n conv_field_sizes,\\\n conv_field_depths,\\\n sd_weights\\\n ,bias_mean,\\\n padding,\\\n conv_layers,\\\n fc_layers,\\\n fc_shape,\\\n keep_prob,\\\n class_num,\\\n dropouts):\n\n # add convolutional layers\n >>> for k in range(conv_layers):\n …      weight_variables.append(weight_variable([conv_field_sizes[k], conv_field_sizes[k], conv_field_depths[k],output_channels[k]],sd_weights))\n bias_variables.append(bias_variable([output_channels[k]],bias_mean))\n relu_layers.append(tf.nn.relu(two_dimensional_convolutional_layer(inputs[k],weight_variables[k],\\\n conv_strides,padding) + bias_variables[k]))\n pooling_layers.append(max_pooling(relu_layers[k],pool_strides,ksize,padding))\n inputs.append(pooling_layers[k])\n\n # finally, add fully connected layers at end with dropout\n >>> for r in range(fc_layers):\n weight_variables.append(weight_variable(fc_shape,sd_weights))\n bias_variables.append(bias_variable([fc_shape[1]],bias_mean))\n pooling_layers.append(tf.reshape(pooling_layers[-1],[-1,fc_shape[0]]))\n fully_connected_layers.append(tf.nn.relu(tf.matmul(pooling_layers[-1], weight_variables[-1]) + bias_variables[-1]))\n dropouts.append(tf.nn.dropout(fully_connected_layers[-1], keep_prob))\n\n # output layer\n weight_variables.append(weight_variable([fc_shape[1],class_num],sd_weights))\n bias_variables.append(bias_variable([class_num],bias_mean))\n return tf.nn.softmax(tf.matmul(dropouts[-1],weight_variables[-1])+bias_variables[-1])\n\n```", "```py\n >>> X = tf.placeholder(\"float\", shape=[None, 784])\n>>> observed = tf.placeholder(\"float\", shape=[None, 10])\n>>> images = tf.reshape(X, [-1,28,28,1])\n\n# shape variables\n>>> sd_weights = 0.1\n>>> bias_mean = 0.1\n>>> padding = 'SAME'\n>>> conv_strides = [1,1,1,1]\n>>> pool_strides = [1,2,2,1]\n>>> ksize = [1,2,2,1]\n>>> output_channels = [32,64]\n>>> conv_field_sizes = [5,5]\n>>> conv_field_depths = [1,32]\n>>>fc_shape = [7*7*64,1024]\n>>> keep_prob = tf.placeholder(\"float\")\n>>> class_num = 10\n>>> conv_layers = 2\n>>> fc_layers = 1\n\n# layers variables\n>>> weight_variables = []\n>>> bias_variables = []\n>>> relu_layers = []\n>>> pooling_layers = []\n>>> inputs = [images]\n>>> fully_connected_layers = []\n>>> dropouts = []\n\n>>> prediction = generate_network(weight_variables,\\\n bias_variables,\\\n relu_layers,\\\n pooling_layers,\\\n fully_connected_layers,\\\n inputs,\\\n conv_strides,\\\n pool_strides,\\\n ksize,\\\n output_channels,\\\n conv_field_sizes,\\\n conv_field_depths,\\\n sd_weights\\\n ,bias_mean,\\\n padding,\\\n conv_layers,\\\n fc_layers,\\\n fc_shape,\\\n keep_prob,\\\n class_num,\\\n dropouts)\n\n```", "```py\n>>> my_session = tf.InteractiveSession()\n>>> squared_error = tf.reduce_sum(tf.pow(tf.reduce_sum(tf.sub(observed,prediction)),[2]))\n>>> train_step = tf.train.GradientDescentOptimizer(0.01).minimize(squared_error)\n>>> correct_prediction = tf.equal(tf.argmax(prediction,1), tf.argmax(observed,1))\n>>> accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n>>> my_session.run(tf.initialize_all_variables())\n\n>>>for i in range(20000):\n…    batch = mnist.train.next_batch(50)\n …   if i%1000 == 0:\n…      train_accuracy = accuracy.eval(feed_dict={X: batch[0], observed: batch[1], keep_prob: 1.0})\n …     print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n…        train_step.run(feed_dict={X: batch[0], observed: batch[1], keep_prob: 0.5})\n…        print(\"test accuracy %g\"%accuracy.eval(feed_dict={X: mnist.test.images, observed: mnist.test.labels, keep_prob: 1.0}))\n\n```"]