- en: Chapter 1. Tokenizing Text and WordNet Basics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizing text into sentences
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tokenizing sentences into words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tokenizing sentences using regular expressions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Filtering stopwords in a tokenized sentence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Looking up synsets for a word in WordNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Looking up lemmas and synonyms in WordNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculating WordNet synset similarity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discovering word collocations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**NLTK** is the **Natural Language Toolkit**, a comprehensive Python library
    for natural language processing and text analytics. Originally designed for teaching,
    it has been adopted in the industry for research and development due to its usefulness
    and breadth of coverage.'
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will cover the basics of tokenizing text and using WordNet. **Tokenization**
    is a method of breaking up a piece of text into many pieces, and is an essential
    first step for recipes in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: '**WordNet** is a dictionary designed for programmatic access by natural language
    processing systems. NLTK includes a WordNet corpus reader, which we will use to
    access and explore WordNet. We''ll be using WordNet again in later chapters, so
    it''s important to familiarize yourself with the basics first.'
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizing text into sentences
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tokenization is the process of splitting a string into a list of pieces, or
    *tokens*. We'll start by splitting a paragraph into a list of sentences.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Installation instructions for NLTK are available at [http://www.nltk.org/download](http://www.nltk.org/download)
    and the latest version as of this writing is 2.0b9\. NLTK requires Python 2.4
    or higher, but is **not compatible with Python 3.0**. The **recommended Python
    version is 2.6**.
  prefs: []
  type: TYPE_NORMAL
- en: Once you've installed NLTK, you'll also need to install the data by following
    the instructions at [http://www.nltk.org/data](http://www.nltk.org/data). We recommend
    installing everything, as we'll be using a number of corpora and pickled objects.
    The data is installed in a data directory, which on Mac and Linux/Unix is usually
    `/usr/share/nltk_data`, or on Windows is `C:\nltk_data`. Make sure that `tokenizers/punkt.zip`
    is in the data directory and has been unpacked so that there's a file at `tokenizers/punkt/english.pickle`.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, to run the code examples, you'll need to start a Python console. Instructions
    on how to do so are available at [http://www.nltk.org/getting-started](http://www.nltk.org/getting-started).
    For Mac with Linux/Unix users, you can open a terminal and type **python**.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once NLTK is installed and you have a Python console running, we can start
    by creating a paragraph of text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Now we want to split `para` into sentences. First we need to import the sentence
    tokenization function, and then we can call it with the paragraph as an argument.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: So now we have a list of sentences that we can use for further processing.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`sent_tokenize` uses an instance of `PunktSentenceTokenizer` from the `nltk.tokenize.punkt`
    module. This instance has already been trained on and works well for many European
    languages. So it knows what punctuation and characters mark the end of a sentence
    and the beginning of a new sentence.'
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The instance used in `sent_tokenize()` is actually loaded on demand from a pickle
    file. So if you're going to be tokenizing a lot of sentences, it's more efficient
    to load the `PunktSentenceTokenizer` once, and call its `tokenize()` method instead.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Other languages
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you want to tokenize sentences in languages other than English, you can
    load one of the other pickle files in `tokenizers/punkt` and use it just like
    the English sentence tokenizer. Here''s an example for Spanish:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the next recipe, we'll learn how to split sentences into individual words.
    After that, we'll cover how to use regular expressions for tokenizing text.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizing sentences into words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we'll split a sentence into individual words. The simple task
    of creating a list of words from a string is an essential part of all text processing.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Basic word tokenization is very simple: use the `word_tokenize()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`word_tokenize()` is a wrapper function that calls `tokenize()` on an instance
    of the `TreebankWordTokenizer` . It''s equivalent to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: It works by separating words using spaces and punctuation. And as you can see,
    it does not discard the punctuation, allowing you to decide what to do with it.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Ignoring the obviously named `WhitespaceTokenizer` and `SpaceTokenizer` , there
    are two other word tokenizers worth looking at: `PunktWordTokenizer` and `WordPunctTokenizer`.
    These differ from the `TreebankWordTokenizer` by how they handle punctuation and
    contractions, but they all inherit from `TokenizerI`. The inheritance tree looks
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![There''s more...](img/3609OS_01_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Contractions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`TreebankWordTokenizer` uses conventions found in the Penn Treebank corpus,
    which we''ll be using for training in [Chapter 4](ch04.html "Chapter 4. Part-of-Speech
    Tagging"), *Part-of-Speech Tagging* and [Chapter 5](ch05.html "Chapter 5. Extracting
    Chunks"), *Extracting Chunks*. One of these conventions is to separate contractions.
    For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: If you find this convention unacceptable, then read on for alternatives, and
    see the next recipe for tokenizing with regular expressions.
  prefs: []
  type: TYPE_NORMAL
- en: PunktWordTokenizer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An alternative word tokenizer is the `PunktWordTokenizer`. It splits on punctuation,
    but keeps it with the word instead of creating separate tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: WordPunctTokenizer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another alternative word tokenizer is `WordPunctTokenizer`. It splits all punctuations
    into separate tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For more control over word tokenization, you'll want to read the next recipe
    to learn how to use regular expressions and the `RegexpTokenizer` for tokenization.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizing sentences using regular expressions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regular expression can be used if you want complete control over how to tokenize
    text. As regular expressions can get complicated very quickly, we only recommend
    using them if the word tokenizers covered in the previous recipe are unacceptable.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First you need to decide how you want to tokenize a piece of text, as this
    will determine how you construct your regular expression. The choices are:'
  prefs: []
  type: TYPE_NORMAL
- en: Match on the tokens
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Match on the separators, or gaps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We'll start with an example of the first, matching alphanumeric tokens plus
    single quotes so that we don't split up contractions.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We'll create an instance of the `RegexpTokenizer`, giving it a regular expression
    string to use for matching tokens.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: There's also a simple helper function you can use in case you don't want to
    instantiate the class.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Now we finally have something that can treat contractions as whole words, instead
    of splitting them into tokens.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `RegexpTokenizer` works by compiling your pattern, then calling `re.findall()`
    on your text. You could do all this yourself using the `re` module, but the `RegexpTokenizer`
    implements the `TokenizerI` interface, just like all the word tokenizers from
    the previous recipe. This means it can be used by other parts of the NLTK package,
    such as corpus readers, which we'll cover in detail in [Chapter 3](ch03.html "Chapter 3. Creating
    Custom Corpora"), *Creating Custom Corpora*. Many corpus readers need a way to
    tokenize the text they're reading, and can take optional keyword arguments specifying
    an instance of a `TokenizerI` subclass. This way, you have the ability to provide
    your own tokenizer instance if the default tokenizer is unsuitable.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`RegexpTokenizer` can also work by matching the gaps, instead of the tokens.
    Instead of using `re.findall()`, the `RegexpTokenizer` will use `re.split()`.
    This is how the `BlanklineTokenizer` in `nltk.tokenize` is implemented.'
  prefs: []
  type: TYPE_NORMAL
- en: Simple whitespace tokenizer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here''s a simple example of using the `RegexpTokenizer` to tokenize on whitespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Notice that punctuation still remains in the tokens.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For simpler word tokenization, see the previous recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Filtering stopwords in a tokenized sentence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Stopwords** are common words that generally do not contribute to the meaning
    of a sentence, at least for the purposes of information retrieval and natural
    language processing. Most search engines will filter stopwords out of search queries
    and documents in order to save space in their index.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: NLTK comes with a stopwords corpus that contains word lists for many languages.
    Be sure to unzip the datafile so NLTK can find these word lists in `nltk_data/corpora/stopwords/`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We're going to create a set of all English stopwords, then use it to filter
    stopwords from a sentence.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The stopwords corpus is an instance of `nltk.corpus.reader.WordListCorpusReader`.
    As such, it has a `words()` method that can take a single argument for the file
    ID, which in this case is `'english'`, referring to a file containing a list of
    English stopwords. You could also call `stopwords.words()` with no argument to
    get a list of all stopwords in every language available.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can see the list of all English stopwords using `stopwords.words(''english'')`
    or by examining the word list file at `nltk_data/corpora/stopwords/english`. There
    are also stopword lists for many other languages. You can see the complete list
    of languages using the `fileids()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Any of these `fileids` can be used as an argument to the `words()` method to
    get a list of stopwords for that language.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you'd like to create your own stopwords corpus, see the *Creating a word
    list corpus* recipe in [Chapter 3](ch03.html "Chapter 3. Creating Custom Corpora"),
    *Creating Custom Corpora*, to learn how to use the `WordListCorpusReader`. We'll
    also be using stopwords in the *Discovering word collocations* recipe, later in
    this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Looking up synsets for a word in WordNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: WordNet is a lexical database for the English language. In other words, it's
    a dictionary designed specifically for natural language processing.
  prefs: []
  type: TYPE_NORMAL
- en: NLTK comes with a simple interface for looking up words in WordNet. What you
    get is a list of **synset** instances, which are groupings of synonymous words
    that express the same concept. Many words have only one synset, but some have
    several. We'll now explore a single synset, and in the next recipe, we'll look
    at several in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Be sure you've unzipped the `wordnet` corpus in `nltk_data/corpora/wordnet`.
    This will allow the `WordNetCorpusReader` to access it.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now we're going to lookup the `synset` for `cookbook`, and explore some of the
    properties and methods of a synset.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can look up any word in WordNet using `wordnet.synsets(word)` to get a list
    of synsets. The list may be empty if the word is not found. The list may also
    have quite a few elements, as some words can have many possible meanings and therefore
    many synsets.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Each synset in the list has a number of attributes you can use to learn more
    about it. The `name` attribute will give you a unique name for the synset, which
    you can use to get the synset directly.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The `definition` attribute should be self-explanatory. Some synsets also have
    an `examples` attribute, which contains a list of phrases that use the word in
    context.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Hypernyms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Synsets are organized in a kind of inheritance tree. More abstract terms are
    known as **hypernyms** and more specific terms are **hyponyms** . This tree can
    be traced all the way up to a root hypernym.
  prefs: []
  type: TYPE_NORMAL
- en: Hypernyms provide a way to categorize and group words based on their similarity
    to each other. The synset similarity recipe details the functions used to calculate
    similarity based on the distance between two words in the hypernym tree.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, `reference book` is a *hypernym* of `cookbook`, but `cookbook`
    is only one of many *hyponyms* of `reference book`. All these types of books have
    the same root hypernym, `entity`, one of the most abstract terms in the English
    language. You can trace the entire path from `entity` down to `cookbook` using
    the `hypernym_paths()` method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This method returns a list of lists, where each list starts at the root hypernym
    and ends with the original `Synset`. Most of the time you'll only get one nested
    list of synsets.
  prefs: []
  type: TYPE_NORMAL
- en: Part-of-speech (POS)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can also look up a simplified part-of-speech tag.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: There are four common POS found in WordNet.
  prefs: []
  type: TYPE_NORMAL
- en: '| Part-of-speech | Tag |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Noun | n |'
  prefs: []
  type: TYPE_TB
- en: '| Adjective | a |'
  prefs: []
  type: TYPE_TB
- en: '| Adverb | r |'
  prefs: []
  type: TYPE_TB
- en: '| Verb | v |'
  prefs: []
  type: TYPE_TB
- en: These POS tags can be used for looking up specific `synsets` for a word. For
    example, the word `great` can be used as a noun or an adjective. In WordNet, `great`
    has one noun synset and six adjective synsets.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: These POS tags will be referenced more in the *Using WordNet for Tagging* recipe
    of [Chapter 4](ch04.html "Chapter 4. Part-of-Speech Tagging"), *Part-of-Speech
    Tagging*.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the next two recipes, we'll explore lemmas and how to calculate synset similarity.
    In [Chapter 2](ch02.html "Chapter 2. Replacing and Correcting Words"), *Replacing
    and Correcting Words*, we'll use WordNet for lemmatization, synonym replacement,
    and then explore the use of antonyms.
  prefs: []
  type: TYPE_NORMAL
- en: Looking up lemmas and synonyms in WordNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building on the previous recipe, we can also look up lemmas in WordNet to find
    **synonyms** of a word. A **lemma** (in linguistics) is the canonical form, or
    morphological form, of a word.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the following block of code, we''ll find that there are two lemmas for the
    `cookbook synset` by using the `lemmas` attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you can see, `cookery_book` and `cookbook` are two distinct `lemmas` in the
    same `synset`. In fact, a lemma can only belong to a single synset. In this way,
    a synset represents a group of lemmas that all have the same meaning, while a
    lemma represents a distinct word form.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since lemmas in a synset all have the same meaning, they can be treated as
    synonyms. So if you wanted to get all synonyms for a `synset`, you could do:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: All possible synonyms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As mentioned before, many words have multiple `synsets` because the word can
    have different meanings depending on the context. But let's say you didn't care
    about the context, and wanted to get all possible synonyms for a word.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, there appears to be 38 possible synonyms for the word `book`.
    But in fact, some are verb forms, and many are just different usages of `book`.
    Instead, if we take the set of synonyms, there are fewer unique words.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Antonyms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Some lemmas also have **antonyms**. The word `good`, for example, has 27 `synsets`,
    five of which have `lemmas` with antonyms.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The `antonyms()` method returns a list of `lemmas`. In the first case here,
    we see that the second `synset` for `good` as a noun is defined as `moral excellence`,
    and its first antonym is `evil`, defined as `morally wrong`. In the second case,
    when `good` is used as an adjective to describe positive qualities, the first
    antonym is `bad`, which describes negative qualities.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the next recipe, we'll learn how to calculate `synset` similarity. Then in
    [Chapter 2](ch02.html "Chapter 2. Replacing and Correcting Words"), *Replacing
    and Correcting Words*, we'll revisit lemmas for lemmatization, synonym replacement,
    and antonym replacement.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating WordNet synset similarity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Synsets are organized in a *hypernym* tree. This tree can be used for reasoning
    about the similarity between the synsets it contains. Two synsets are more similar,
    the closer they are in the tree.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you were to look at all the hyponyms of `reference book` (which is the hypernym
    of `cookbook`) you'd see that one of them is `instruction_book`. These seem intuitively
    very similar to `cookbook`, so let's see what WordNet similarity has to say about
    it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: So they are over 91% similar!
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`wup_similarity` is short for *Wu-Palmer Similarity*, which is a scoring method
    based on how similar the word senses are and where the synsets occur relative
    to each other in the hypernym tree. One of the core metrics used to calculate
    similarity is the shortest path distance between the two synsets and their common
    hypernym.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: So `cookbook` and `instruction book` must be very similar, because they are
    only one step away from the same hypernym, `reference book`, and therefore only
    two steps away from each other.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's look at two dissimilar words to see what kind of score we get. We'll compare
    `dog` with `cookbook`, two seemingly very different words.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Wow, `dog` and `cookbook` are apparently 38% similar! This is because they share
    common hypernyms farther up the tree.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Comparing verbs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The previous comparisons were all between nouns, but the same can be done for
    verbs as well.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The previous synsets were obviously handpicked for demonstration, and the reason
    is that the hypernym tree for verbs has a lot more breadth and a lot less depth.
    While most nouns can be traced up to `object`, thereby providing a basis for similarity,
    many verbs do not share common hypernyms, making WordNet unable to calculate similarity.
    For example, if you were to use the `synset` for `bake.v.01` here, instead of
    `bake.v.02`, the return value would be `None`. This is because the root hypernyms
    of the two synsets are different, with no overlapping paths. For this reason,
    you also cannot calculate similarity between words with different parts of speech.
  prefs: []
  type: TYPE_NORMAL
- en: Path and LCH similarity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Two other similarity comparisons are the path similarity and **Leacock Chodorow
    (LCH)** similarity.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the number ranges are very different for these scoring methods,
    which is why we prefer the `wup_similarity()` method.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The recipe on *Looking up synsets for a word in WordNet,* discussed earlier
    in this chapter, has more details about hypernyms and the hypernym tree.
  prefs: []
  type: TYPE_NORMAL
- en: Discovering word collocations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Collocations** are two or more words that tend to appear frequently together,
    such as "United States". Of course, there are many other words that can come after
    "United", for example "United Kingdom", "United Airlines", and so on. As with
    many aspects of natural language processing, context is very important, and for
    collocations, context is everything!'
  prefs: []
  type: TYPE_NORMAL
- en: In the case of collocations, the context will be a document in the form of a
    list of words. Discovering collocations in this list of words means that we'll
    find common phrases that occur frequently throughout the text. For fun, we'll
    start with the script for *Monty Python and the Holy Grail*.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The script for *Monty Python and the Holy Grail* is found in the `webtext` corpus,
    so be sure that it's unzipped in `nltk_data/corpora/webtext/`.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We're going to create a list of all lowercased words in the text, and then produce
    a `BigramCollocationFinder`, which we can use to find **bigrams**, which are pairs
    of words. These bigrams are found using association measurement functions found
    in the `nltk.metrics` package.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Well that's not very useful! Let's refine it a bit by adding a word filter to
    remove punctuation and stopwords.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Much better—we can clearly see four of the most common bigrams in *Monty Python
    and the Holy Grail*. If you'd like to see more than four, simply increase the
    number to whatever you want, and the collocation finder will do its best.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `BigramCollocationFinder` constructs two frequency distributions: one for
    each word, and another for bigrams. A **frequency distribution**, or `FreqDist`
    in NLTK, is basically an enhanced dictionary where the keys are what''s being
    counted, and the values are the counts. Any filtering functions that are applied,
    reduce the size of these two `FreqDists` by eliminating any words that don''t
    pass the filter. By using a filtering function to eliminate all words that are
    one or two characters, and all English stopwords, we can get a much cleaner result.
    After filtering, the collocation finder is ready to accept a generic scoring function
    for finding collocations. Additional scoring functions are covered in the *Scoring
    functions* section further in this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to `BigramCollocationFinder`, there's also `TrigramCollocationFinder`,
    for finding triples instead of pairs. This time, we'll look for **trigrams** in
    Australian singles ads.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Now, we don't know whether people are looking for a long-term relationship or
    not, but clearly it's an important topic. In addition to the stopword filter,
    we also applied a frequency filter which removed any trigrams that occurred less
    than three times. This is why only one result was returned when we asked for four—because
    there was only one result that occurred more than twice.
  prefs: []
  type: TYPE_NORMAL
- en: Scoring functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are many more scoring functions available besides `likelihood_ratio()`.
    But other than `raw_freq()`, you may need a bit of a statistics background to
    understand how they work. Consult the NLTK API documentation for `NgramAssocMeasures`
    in the `nltk.metrics` package, to see all the possible scoring functions.
  prefs: []
  type: TYPE_NORMAL
- en: Scoring ngrams
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In addition to the `nbest()` method, there are two other ways to get **ngrams**
    (a generic term for describing *bigrams* and *trigrams*) from a collocation finder.
  prefs: []
  type: TYPE_NORMAL
- en: '`above_score(score_fn, min_score)` can be used to get all ngrams with scores
    that are at least `min_score`. The `min_score` that you choose will depend heavily
    on the `score_fn` you use.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`score_ngrams(score_fn)` will return a list with tuple pairs of `(ngram, score)`.
    This can be used to inform your choice for `min_score` in the previous step.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `nltk.metrics` module will be used again in [Chapter 7](ch07.html "Chapter 7. Text
    Classification"), *Text Classification*.
  prefs: []
  type: TYPE_NORMAL
