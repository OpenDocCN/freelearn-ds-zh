<html><head></head><body><div id="sbo-rt-content"><div>
			<div id="_idContainer085" class="Content">
			</div>
		</div>
		<div id="_idContainer086" class="Content">
			<h1 id="_idParaDest-120"><a id="_idTextAnchor124"/>4. A Deep Dive into Data Wrangling with Python</h1>
		</div>
		<div id="_idContainer162" class="Content">
			<p class="callout-heading"><a id="_idTextAnchor125"/>Overview</p>
			<p class="callout">This chapter will cover pandas DataFrames in depth, thus teaching you how to perform subsetting, filtering, and grouping on DataFrames. You will be able to apply Boolean filtering and indexing to a DataFrame to choose specific elements from it. Later on in the chapter, you will learn how to perform JOIN operations in pandas that are analogous to the SQL command. By the end of this chapter you will be able to apply imputation techniques to identify missing or corrupted data and choose to drop it.</p>
			<h1 id="_idParaDest-121"><a id="_idTextAnchor126"/>Introduction</h1>
			<p>In the previous chapter, we learned how to use the pa<strong class="source-inline">ndas</strong>, <strong class="source-inline">numpy</strong>, and <strong class="source-inline">matplotlib</strong> libraries while handling various datatypes. In this chapter, we will learn about several advanced operations involving <strong class="source-inline">pandas</strong> DataFrames and <strong class="source-inline">numpy</strong> arrays. We will be working with several powerful DataFrame operations, including subsetting, filtering grouping, checking uniqueness, and even dealing with missing data, among others. These techniques are extremely useful when working with data in any way. When we want to look at a portion of the data, we must subset, filter, or group the data. <strong class="source-inline">Pandas</strong> contains the functionality to create descriptive statistics of the dataset. These methods will allow us to start shaping our perception of the data. Ideally, when we have a dataset, we want it to be complete, but in reality, there is often missing or corrupt data. This can happen for a variety of reasons that we can't control, such as user error and sensor malfunction. Pandas has built-in functionalities to deal with such kinds of missing data within our dataset.</p>
			<h1 id="_idParaDest-122"><a id="_idTextAnchor127"/>Subsetting, Filtering, and Grouping</h1>
			<p>One of the most important aspects of data wrangling is to curate the data carefully from the deluge of streaming data that pours into an organization or business entity from various sources. Lots of data is not always a good thing; rather, data needs to be useful and of high quality to be effectively used in downstream activities of a data science pipeline, such as machine learning and predictive model building. Moreover, one data source can be used for multiple purposes, and this often requires different subsets of data to be processed by a data wrangling module. This is then passed on to separate analytics modules.</p>
			<p>For example, let's say you are doing data wrangling on US state-level economic output. It is a fairly common scenario that one machine learning model may require data for large and populous states (such as California and Texas), while another model demands processed data for small and sparsely populated states (such as Montana or North Dakota). As the frontline of the data science process, it is the responsibility of the data wrangling module to satisfy the requirements of both these machine learning models. Therefore, as a data wrangling engineer, you have to filter and group data accordingly (based on the population of the state) before processing them and producing separate datasets as the final output for separate machine learning models.</p>
			<p>Also, in some cases, data sources may be biased, or the measurement may corrupt the incoming data occasionally. It is a good idea to try to filter only the error-free, good data for downstream modeling. From these examples and discussions, it is clear that filtering and grouping/bucketing data is an essential skill to have for any engineer that's engaged in the task of data wrangling. Let's proceed to learn about a few of these skills with pandas.</p>
			<h2 id="_idParaDest-123"><a id="_idTextAnchor128"/>Exercise 4.01: Examining the Superstore Sales Data in an Excel File</h2>
			<p>In this exercise, we will read and examine an Excel file called <strong class="source-inline">Sample-Superstore.xls</strong> and will check all the columns to check if they are useful for analysis. We'll use the <strong class="source-inline">drop</strong> method to delete the columns that are unnecessary from the <strong class="source-inline">.xls</strong> file. Then, we'll use the <strong class="source-inline">shape</strong> function to check the number of rows and columns in the dataset. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">The <strong class="source-inline">superstore</strong> dataset file can be found here: <a href="https://packt.live/3dcVnMs">https://packt.live/3dcVnMs</a>.</p>
			<p>To do so, perform the following steps: </p>
			<ol>
				<li>To read an Excel file into <strong class="source-inline">pandas</strong>, you will need a small package called <strong class="source-inline">xlrd</strong> to be installed on your system. Use the following code to install the <strong class="source-inline">xlrd</strong> package:<p class="source-code">!pip install xlrd</p><p class="callout-heading">Note</p><p class="callout">The <strong class="source-inline">!</strong> notation tells the Jupyter Notebook that the cell should be treated as a shell command. </p></li>
				<li>Read the Excel file from GitHub into a <strong class="source-inline">pandas</strong> DataFrame using the <strong class="source-inline">read_excel</strong> method in <strong class="source-inline">pandas</strong>:<p class="source-code">import numpy as np</p><p class="source-code">import pandas as pd</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">df = pd.read_excel("<strong class="bold">../datasets/Sample - Superstore.xls</strong>")</p><p class="source-code">df.head()</p><p class="callout-heading">Note</p><p class="callout">The highlighted path must be changed based on the location of the file on your system. </p><p>The output (partially shown) is as follows:</p><div id="_idContainer087" class="IMG---Figure"><img src="Images/B15780_04_01.jpg" alt="Figure 4.1: Partial output of the Excel file in a DataFrame&#13;&#10;" width="1010" height="625"/></div><p class="figure-caption">Figure 4.1: Partial output of the Excel file in a DataFrame</p><p>On examining the file, we can see that the first column, called <strong class="source-inline">Row ID</strong>, is not very useful because we already have a row index on the far left. This is a common occurrence in <strong class="source-inline">pandas</strong> and can be resolved in a few ways, most importantly by removing the <strong class="source-inline">rowid</strong> column.</p></li>
				<li>Drop this column altogether from the DataFrame by using the <strong class="source-inline">drop</strong> method:<p class="source-code">df.drop('Row ID',axis=1,inplace=True)</p><p class="source-code">df.head()</p><p>The output is as follows:</p><div id="_idContainer088" class="IMG---Figure"><img src="Images/B15780_04_02.jpg" alt="Figure 4.2: Partial output of the Superstore dataset after dropping the 'Row ID' column&#13;&#10;" width="1186" height="631"/></div><p class="figure-caption">Figure 4.2: Partial output of the Superstore dataset after dropping the 'Row ID' column</p></li>
				<li>Check the number of rows and columns in the newly created dataset. We will use the <strong class="source-inline">shape</strong> function here:<p class="source-code">df.shape</p><p>The output is as follows:</p><p class="source-code">(9994, 20)</p></li>
			</ol>
			<p>In this exercise, we can see that the dataset has <strong class="source-inline">9,994</strong> rows and <strong class="source-inline">20</strong> columns. We have now seen that a simple way to remove unwanted columns such as a row count is simple with <strong class="source-inline">pandas</strong>. Think about how hard this would be if, instead of <strong class="source-inline">pandas</strong>, we used a list of dictionaries? We would have to write a loop to remove the <strong class="source-inline">rowid</strong> element from each dictionary in the list. <strong class="source-inline">pandas</strong> makes this functionality simple and easy.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2Y9ZTXW">https://packt.live/2Y9ZTXW</a>. </p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2N4dVUO">https://packt.live/2N4dVUO</a>.</p>
			<p>In the next section, we'll discuss how to subset a DataFrame.</p>
			<h2 id="_idParaDest-124"><a id="_idTextAnchor129"/>Subsetting the DataFrame</h2>
			<p><strong class="bold">Subsetting</strong> involves the extraction of partial data based on specific columns and rows, as per business needs. Let's pretend we are creating a report on our customers at the superstore. Suppose we are interested only in the following information from this dataset: <strong class="source-inline">Customer ID</strong>, <strong class="source-inline">Customer Name</strong>, <strong class="source-inline">City</strong>, <strong class="source-inline">Postal Code</strong>, and <strong class="source-inline">Sales</strong>. For demonstration purposes, let's assume that we are only interested in <strong class="source-inline">5</strong> records – rows <strong class="source-inline">5-9</strong>. We can subset the DataFrame to extract only this much information using a single line of Python code.</p>
			<p>We can use the <strong class="source-inline">loc</strong> method to index the <strong class="source-inline">Sample Superstore</strong> dataset by the names of the columns and the indexes of the rows, as shown in the following code:</p>
			<p class="source-code">df_subset = df.loc[</p>
			<p class="source-code">    <strong class="bold">[i for i in range(5,10)],</strong></p>
			<p class="source-code">    ['Customer ID','Customer Name','City','Postal Code','Sales']]</p>
			<p class="source-code">df_subset</p>
			<p>The output is as follows:</p>
			<div>
				<div id="_idContainer089" class="IMG---Figure">
					<img src="Images/B15780_04_03.jpg" alt="Figure 4.3: Partial data of the DataFrame indexed by the names of the columns&#13;&#10;" width="1192" height="395"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.3: Partial data of the DataFrame indexed by the names of the columns</p>
			<p>We need to pass on two arguments to the <strong class="source-inline">loc</strong> method – one for indicating the rows, and another for indicating the columns. When passing more than one value, you must pass them as a list for a row or column.</p>
			<p>For the rows, we have to pass a list, that is, <strong class="source-inline">[5,6,7,8,9]</strong>, but instead of writing that explicitly, we use a list comprehension, that is, <strong class="source-inline">[i for i in range(5,10)]</strong>.</p>
			<p>Because the columns we are interested in are not continuous and we cannot just put in a continuous range, we need to pass on a list containing the specific names. So, the second argument is just a simple list with specific column names. The dataset shows the fundamental concepts of the process of <strong class="bold">subsetting</strong> a DataFrame based on business requirements.</p>
			<p>Let's look at an example use case and practice subsetting a bit more.</p>
			<h2 id="_idParaDest-125"><a id="_idTextAnchor130"/>An Example Use Case – Determining Statistics on Sales and Profit</h2>
			<p>Let's take a look at a typical use case of subsetting. Suppose we want to calculate descriptive statistics (mean, median, standard deviation, and so on) of records <strong class="source-inline">100-199</strong> for sales and profit in the <strong class="source-inline">SuperStore</strong> dataset. The following code shows how subsetting helps us achieve that:</p>
			<p class="source-code">df_subset = df.loc[[i for i in range(100,199)],['Sales','Profit']]</p>
			<p class="source-code">df_subset.describe()</p>
			<p>The output is as follows:</p>
			<div>
				<div id="_idContainer090" class="IMG---Figure">
					<img src="Images/B15780_04_04.jpg" alt="Figure 4.4: Output of descriptive statistics of data&#13;&#10;" width="1134" height="581"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.4: Output of descriptive statistics of data</p>
			<p>We simply extract records <strong class="source-inline">100-199</strong> and run the <strong class="source-inline">describe</strong> function on them because we don't want to process all the data. For this particular business question, we are only interested in sales and profit numbers, and therefore we should not take the easy route and run a <strong class="source-inline">describe</strong> function on all the data. For a dataset that's being used in machine learning analysis, the number of rows and columns could often be in the millions, and we don't want to compute anything that is not asked for in the data wrangling task. We always aim to subset the exact data that needs to be processed and run statistical or plotting functions on that partial data. One of the most intuitive ways to try and understand the data is through charting. This can be a critical component of data wrangling. </p>
			<p>To better understand sales and profit, let's create a box plot of the data using <strong class="source-inline">matplotlib</strong>:</p>
			<p class="source-code">import matplotlib as plt</p>
			<p class="source-code">boxplot = df_subset.boxplot()</p>
			<p>The output is as follows:</p>
			<div>
				<div id="_idContainer091" class="IMG---Figure">
					<img src="Images/B15780_04_05.jpg" alt="Figure 4.5: Box plot of sales and profit&#13;&#10;" width="927" height="437"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.5: Box plot of sales and profit</p>
			<p>As we can see from the preceding box plot, there are some outliers for profit. Now, they could be normal outliers, or they could be <strong class="source-inline">NaN</strong> values. At this point, we can't speculate, but this could cause some further analysis to see how we want to treat those outliers in profit. In some cases, outliers are fine, but for some predictive modeling techniques such as regression, outliers can have unwanted effects. </p>
			<p>Before continuing further with filtering methods, let's take a quick detour and explore a super useful function called <strong class="source-inline">unique</strong>. As its name suggests, this function is used to scan through the data quickly and extract only the unique values in a column or row.</p>
			<h2 id="_idParaDest-126"><a id="_idTextAnchor131"/>Exercise 4.02: The unique Function</h2>
			<p>In the superstore sales data, you will notice that there are columns such as <strong class="source-inline">Country</strong>, <strong class="source-inline">State</strong>, and <strong class="source-inline">City</strong>. A natural question will be to ask how many <strong class="source-inline">countries/states/cities</strong> are present in the dataset. In this exercise, we'll use the <strong class="source-inline">unique</strong> function to find the number of unique <strong class="source-inline">countries/states/cities</strong> in the dataset. Let's go through the following steps:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The <strong class="source-inline">superstore</strong> dataset file can be found here: <a href="https://packt.live/3dcVnMs">https://packt.live/3dcVnMs</a>.</p>
			<ol>
				<li value="1">Import the necessary libraries and read the file from GitHub by using the <strong class="source-inline">read_excel</strong> method in <strong class="source-inline">pandas</strong> into a DataFrame:<p class="source-code">import numpy as np</p><p class="source-code">import pandas as pd</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">df = pd.read_excel("<strong class="bold">../datasets/Sample - Superstore.xls</strong>")</p><p class="callout-heading">Note</p><p class="callout">The highlighted path must be changed based on the location of the file on your system. </p></li>
				<li>Extract <strong class="source-inline">countries/states/cities</strong> for which the information is in the database, with one simple line of code, as follows:<p class="source-code">df['State'].unique()</p><p>The output is as follows:</p><div id="_idContainer092" class="IMG---Figure"><img src="Images/B15780_04_06.jpg" alt="Figure 4.6: Different states present in the dataset&#13;&#10;" width="865" height="283"/></div><p class="figure-caption">Figure 4.6: Different states present in the dataset</p><p>You will see a list of all the states whose data is present in the dataset.</p></li>
				<li>Use the <strong class="source-inline">nunique</strong> method to count the number of unique values in the <strong class="source-inline">State</strong> column, like so:<p class="source-code">df['State'].nunique()</p><p>The output is as follows:</p><p class="source-code">49</p><p>This returns <strong class="source-inline">49</strong> for this dataset. So, one out of <strong class="source-inline">50</strong> states in the US does not appear in this dataset. Therefore, we can conclude that there's one repetition in the <strong class="source-inline">State</strong> column.</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2NaBkUB">https://packt.live/2NaBkUB</a>. </p><p class="callout">You can also run this example online at <a href="https://packt.live/2N7NHkf">https://packt.live/2N7NHkf</a>.</p></li>
			</ol>
			<p>Similarly, if we run this function on the <strong class="source-inline">Country</strong> column, we get an array with only one element, <strong class="source-inline">United States</strong>. Immediately, we can see that we don't need to keep the country column at all because there is no useful information in that column, except that all the entries are the same. This is how a simple function helped us to decide about dropping a column altogether – that is, removing <strong class="source-inline">9,994</strong> pieces of unnecessary data.</p>
			<h2 id="_idParaDest-127"><a id="_idTextAnchor132"/>Conditional Selection and Boolean Filtering</h2>
			<p>Often, we don't want to process the whole dataset and would like to select only a partial dataset whose contents satisfy a particular condition. This is probably the most common use case of any data wrangling task. In the context of our <strong class="source-inline">superstore sales</strong> dataset, think of these common questions that may arise from the daily activities of the business analytics team:</p>
			<ul>
				<li>What are the average sales and profit figures in California?</li>
				<li>Which states have the highest and lowest total sales?</li>
				<li>What consumer segment has the most variance in sales/profit?</li>
				<li>Among the top five states in sales, which shipping mode and product category are the most popular choices?</li>
			</ul>
			<p>Countless examples can be given where the business analytics team or the executive management wants to glean insight from a particular subset of data that meets certain criteria.</p>
			<p>If you have any prior experience with SQL, you will know that these kinds of questions require fairly complex SQL query writing. Remember the <strong class="source-inline">WHERE</strong> clause?</p>
			<p>We will show you how to use conditional subsetting and boolean filtering to answer such questions.</p>
			<p>First, we need to understand the critical concept of boolean indexing. This process essentially accepts a conditional expression as an argument and returns a dataset of booleans in which the <strong class="source-inline">TRUE</strong> value appears in places where the condition was satisfied. A simple example is shown in the following code. For demonstration purposes, we're subsetting a small dataset of <strong class="source-inline">10</strong> records and <strong class="source-inline">3</strong> columns:</p>
			<p class="source-code">df_subset = df.loc[[i for i in range (10)],\</p>
			<p class="source-code">                   ['Ship Mode','State','Sales']]</p>
			<p class="source-code">df_subset</p>
			<p>The output is as follows:</p>
			<div>
				<div id="_idContainer093" class="IMG---Figure">
					<img src="Images/B15780_04_07.jpg" alt="Figure 4.7: Sample dataset&#13;&#10;" width="1052" height="578"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.7: Sample dataset</p>
			<p>Now, if we just want to know the records with sales higher than <strong class="source-inline">$100</strong>, then we can write the following:</p>
			<p class="source-code">df_subset['Sales'] &gt; 100</p>
			<p>This produces the following <strong class="source-inline">boolean</strong> DataFrame:</p>
			<div>
				<div id="_idContainer094" class="IMG---Figure">
					<img src="Images/B15780_04_08.jpg" alt="Figure 4.8 Records with sales higher than $100&#13;&#10;" width="1026" height="586"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.8 Records with sales higher than $100</p>
			<p>Let's take a look at the <strong class="source-inline">True</strong> and <strong class="source-inline">False</strong> entries in the <strong class="source-inline">Sales</strong> column. The values in the <strong class="source-inline">Ship Mode</strong> and <strong class="source-inline">State</strong> columns were not impacted by this code because the comparison was with a numerical quantity, and the only numeric column in the original DataFrame was <strong class="source-inline">Sales</strong>.</p>
			<p>Now, let's see what happens if we pass this <strong class="source-inline">boolean</strong> DataFrame as an index to the original DataFrame:</p>
			<p class="source-code">df_subset[df_subset['Sales']&gt;100]</p>
			<p>The output is as follows:</p>
			<div>
				<div id="_idContainer095" class="IMG---Figure">
					<img src="Images/B15780_04_09.jpg" alt="Figure 4.9: Results after passing the boolean DataFrame as an index &#13;&#10;to the original DataFrame&#13;&#10;" width="1288" height="399"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.9: Results after passing the boolean DataFrame as an index to the original DataFrame</p>
			<p>We are not limited to conditional expressions involving numeric quantities only. Let's try to extract high sales values (<strong class="source-inline">&gt;$100</strong>) for entries that do not involve <strong class="source-inline">California</strong>.</p>
			<p>We can write the following code to accomplish this:</p>
			<p class="source-code">df_subset[(df_subset['State']!='California') \</p>
			<p class="source-code">          &amp; (df_subset['Sales']&gt;100)]</p>
			<p>Note the use of a conditional involving string. In this expression, we are joining two conditionals by an <strong class="source-inline">&amp;</strong> operator. Both conditions must be wrapped inside parentheses.</p>
			<p>The first conditional expression simply matches the entries in the <strong class="source-inline">State</strong> column to the <strong class="source-inline">California</strong> string and assigns <strong class="source-inline">TRUE</strong>/<strong class="source-inline">FALSE</strong> accordingly. The second conditional is the same as before. Together, joined by the <strong class="source-inline">&amp;</strong> operator, they extract only those rows for which <strong class="source-inline">State</strong> is <em class="italic">not</em> <strong class="source-inline">California</strong> and <strong class="source-inline">Sales</strong> is <strong class="source-inline">&gt; $100</strong>. We get the following result:</p>
			<div>
				<div id="_idContainer096" class="IMG---Figure">
					<img src="Images/B15780_04_10.jpg" alt="Figure 4.10: Results, where State is not California and Sales, is higher than $100&#13;&#10;" width="1368" height="327"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.10: Results, where State is not California and Sales, is higher than $100</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Although, in theory, there is no limit to how complex a conditional you can build using individual expressions and the <strong class="source-inline">&amp;</strong> (<strong class="source-inline">LOGICAL AND</strong>) and <strong class="source-inline">|</strong> (<strong class="source-inline">LOGICAL OR</strong>) operators, it is advisable to create intermediate boolean DataFrames with limited conditional expressions and build your final DataFrame step by step. This keeps the code legible and scalable.</p>
			<p>In the following exercise, we'll look at a few different methods we can use to manipulate the DataFrame. </p>
			<h2 id="_idParaDest-128"><a id="_idTextAnchor133"/>Exercise 4.03: Setting and Resetting the Index</h2>
			<p>In this exercise, we will create a pandas DataFrame and set and reset the index. We'll also add a new column and set it as the new index of this DataFrame. To do so, let's go through the following steps:</p>
			<ol>
				<li value="1">Import the <strong class="source-inline">numpy</strong> library:<p class="source-code">import numpy as np</p></li>
				<li>Create the <strong class="source-inline">matrix_data</strong>, <strong class="source-inline">row_labels</strong>, and <strong class="source-inline">column_headings</strong> functions using the following commands:<p class="source-code">matrix_data = np.matrix('22,66,140;42,70,148;\</p><p class="source-code">                        30,62,125;35,68,160;25,62,152')</p><p class="source-code">row_labels = ['A','B','C','D','E']</p><p class="source-code">column_headings = ['Age', 'Height', 'Weight']</p></li>
				<li>Import the <strong class="source-inline">pandas</strong> library and then create a DataFrame using the <strong class="source-inline">matrix_data</strong>, <strong class="source-inline">row_labels</strong>, and <strong class="source-inline">column_headings</strong> functions:<p class="source-code">import pandas as pd</p><p class="source-code">df1 = pd.DataFrame(data=matrix_data,\</p><p class="source-code">                   index=row_labels,\</p><p class="source-code">                   columns=column_headings)</p><p class="source-code">print("\nThe DataFrame\n",'-'*25, sep='')</p><p class="source-code">df1</p><p>The output is as follows:</p><div id="_idContainer097" class="IMG---Figure"><img src="Images/B15780_04_11.jpg" alt="Figure 4.11: The original DataFrame&#13;&#10;" width="1053" height="442"/></div><p class="figure-caption">Figure 4.11: The original DataFrame</p></li>
				<li>Reset the index, as follows:<p class="source-code">print("\nAfter resetting index\n",'-'*35, sep='')</p><p class="source-code">df1.reset_index()</p><p>The output is as follows:</p><div id="_idContainer098" class="IMG---Figure"><img src="Images/B15780_04_12.jpg" alt="Figure 4.12: DataFrame after resetting the index&#13;&#10;" width="1315" height="538"/></div><p class="figure-caption">Figure 4.12: DataFrame after resetting the index</p></li>
				<li>Reset the index with <strong class="source-inline">drop</strong> set to <strong class="source-inline">True</strong>, as follows:<p class="source-code">print("\nAfter resetting index with 'drop' option TRUE\n",\</p><p class="source-code">      '-'*45, sep='')</p><p class="source-code">df1.reset_index(drop=True)</p><p>The output is as follows:</p><div id="_idContainer099" class="IMG---Figure"><img src="Images/B15780_04_13.jpg" alt="Figure 4.13: DataFrame after resetting the index with the drop option set to true&#13;&#10;" width="1248" height="547"/></div><p class="figure-caption">Figure 4.13: DataFrame after resetting the index with the drop option set to true</p></li>
				<li>Add a new column using the following command:<p class="source-code">print("\nAdding a new column 'Profession'\n",\</p><p class="source-code">      '-'*45, sep='')</p><p class="source-code">df1['Profession'] = "Student Teacher Engineer Doctor Nurse"\</p><p class="source-code">                    .split()</p><p class="source-code">df1</p><p>The output is as follows:</p><div id="_idContainer100" class="IMG---Figure"><img src="Images/B15780_04_14.jpg" alt="Figure 4.14: DataFrame after adding a new column called Profession&#13;&#10;" width="1444" height="538"/></div><p class="figure-caption">Figure 4.14: DataFrame after adding a new column called Profession</p></li>
				<li>Now, set the <strong class="source-inline">Profession</strong> column as an <strong class="source-inline">index</strong> using the following code:<p class="source-code">print("\nSetting 'Profession' column as index\n",\</p><p class="source-code">      '-'*45, sep='')</p><p class="source-code">df1.set_index('Profession')</p><p>The output is as follows:</p><div id="_idContainer101" class="IMG---Figure"><img src="Images/B15780_04_15.jpg" alt="Figure 4.15: DataFrame after setting the Profession column as an index&#13;&#10;" width="1427" height="611"/></div></li>
			</ol>
			<p class="figure-caption">Figure 4.15: DataFrame after setting the Profession column as an index</p>
			<p>As we can see, the new data was added at the end of the table.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/30QknH2">https://packt.live/30QknH2</a>. </p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/37CdM4o">https://packt.live/37CdM4o</a>.</p>
			<h2 id="_idParaDest-129"><a id="_idTextAnchor134"/>The GroupBy Method</h2>
			<p><strong class="bold">GroupBy</strong> refers to a process involving one or more of the following steps:</p>
			<ul>
				<li>Splitting the data into groups based on some criteria</li>
				<li>Applying a function to each group independently</li>
				<li>Combining the results into a data structure</li>
			</ul>
			<p>In many situations, we can split the dataset into groups and do something with those groups. In the apply step, we may wish to do one of the following:</p>
			<ul>
				<li><strong class="bold">Aggregation</strong>: Compute a summary statistic (or statistics) for each group – sum, mean, and so on</li>
				<li><strong class="bold">Transformation</strong>: Perform a group-specific computation and return a like-indexed object – z-transformation or filling missing data with a value</li>
				<li><strong class="bold">Filtration</strong>: Discard a few groups, according to a group-wise computation that evaluates <strong class="source-inline">TRUE</strong> or <strong class="source-inline">FALSE</strong></li>
			</ul>
			<p>There is, of course, a describe method for this <strong class="source-inline">GroupBy</strong> object, which produces the summary statistics in the form of a DataFrame.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The name GroupBy should be quite familiar to those who have used a SQL-based tool before.</p>
			<p>GroupBy is not limited to a single variable. If you pass on multiple variables (as a list), then you will get a structure essentially similar to a Pivot Table (from Excel). The following exercise shows an example of where we group together all the states and cities from the whole dataset (the snapshot is only a partial view).</p>
			<h2 id="_idParaDest-130"><a id="_idTextAnchor135"/>Exercise 4.04: The GroupBy Method</h2>
			<p>In this exercise, we're going to create a subset from a dataset. We will use the <strong class="source-inline">groupBy</strong> object to filter the dataset and calculate the mean of that filtered dataset. To do so, let's go through the following steps:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The <strong class="source-inline">superstore</strong> dataset file can be found here: <a href="https://packt.live/3dcVnMs">https://packt.live/3dcVnMs</a>.</p>
			<ol>
				<li value="1">Import the necessary Python modules and read the Excel file from GitHub by using the <strong class="source-inline">read_excel</strong> method in <strong class="source-inline">pandas</strong>:<p class="source-code">import numpy as np</p><p class="source-code">import pandas as pd</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">df = pd.read_excel("<strong class="bold">../datasets/Sample - Superstore.xls</strong>")</p><p class="source-code">df.head()</p><p>The output (partially shown) is as follows:</p><div id="_idContainer102" class="IMG---Figure"><img src="Images/B15780_04_16.jpg" alt="Figure 4.16: Partial output of the DataFrame&#13;&#10;" width="1074" height="738"/></div><p class="figure-caption">Figure 4.16: Partial output of the DataFrame</p><p class="callout-heading">Note</p><p class="callout">The highlighted path must be changed based on the location of the file on your system. </p></li>
				<li>Create a 10-record subset using the following command:<p class="source-code">df_subset = df.loc[[i for i in range (10)],\</p><p class="source-code">                   ['Ship Mode','State','Sales']]</p><p class="source-code">df_subset</p><p>The output will be as follows:</p><div id="_idContainer103" class="IMG---Figure"><img src="Images/B15780_04_17.jpg" alt="Figure 4.17: 10-Record Subset&#13;&#10;" width="897" height="557"/></div><p class="figure-caption">Figure 4.17: 10-Record Subset</p></li>
				<li>Create a <strong class="source-inline">pandas</strong> DataFrame using the <strong class="source-inline">groupby</strong> method, as follows:<p class="source-code">byState = df_subset.groupby('State')</p><p class="source-code">byState</p><p>The output will be similar to:</p><p class="source-code">&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x00000202FB931B08&gt;</p></li>
				<li>Calculate the mean sales figure by <strong class="source-inline">State</strong> by using the following command:<p class="source-code">print("\nGrouping by 'State' column and listing mean sales\n",\</p><p class="source-code">      '-'*50, sep='')</p><p class="source-code">byState.mean()</p><p>The output is as follows:</p><div id="_idContainer104" class="IMG---Figure"><img src="Images/B15780_04_18.jpg" alt="Figure 4.18: Output after grouping the state with the listing mean sales&#13;&#10;" width="978" height="377"/></div><p class="figure-caption">Figure 4.18: Output after grouping the state with the listing mean sales</p></li>
				<li>Calculate the total sales figure by <strong class="source-inline">State</strong> by using the following command:<p class="source-code">print("\nGrouping by 'State' column and listing total "\</p><p class="source-code">      "sum of sales\n", '-'*50, sep='')</p><p class="source-code">byState.sum()</p><p>The output is as follows:</p><div id="_idContainer105" class="IMG---Figure"><img src="Images/B15780_04_19.jpg" alt="Figure 4.19: The output after grouping the state with the listing sum of sales&#13;&#10;" width="1029" height="388"/></div><p class="figure-caption">Figure 4.19: The output after grouping the state with the listing sum of sales</p></li>
				<li>Subset that DataFrame for a particular state and show the statistics:<p class="source-code">pd.DataFrame(byState.describe().loc['California'])</p><p>The output is as follows:</p><div id="_idContainer106" class="IMG---Figure"><img src="Images/B15780_04_20.jpg" alt="Figure 4.20: Checking the statistics of a particular state&#13;&#10;" width="875" height="457"/></div><p class="figure-caption">Figure 4.20: Checking the statistics of a particular state</p></li>
				<li>Perform a similar summarization by using the <strong class="source-inline">Ship Mode</strong> attribute:<p class="source-code">df_subset.groupby('Ship Mode').describe()\</p><p class="source-code">.loc[['Second Class','Standard Class']]</p><p>The output will be as follows:</p><div id="_idContainer107" class="IMG---Figure"><img src="Images/B15780_04_21.jpg" alt="Figure 4.21: Checking the sales by summarizing the Ship Mode attribute&#13;&#10;" width="1059" height="257"/></div><p class="figure-caption">Figure 4.21: Checking the sales by summarizing the Ship Mode attribute</p></li>
				<li>Display the complete summary statistics of sales by every city in each state – all with two lines of code – by using the following command:<p class="source-code">byStateCity=df.groupby(['State','City'])</p><p class="source-code">byStateCity.describe()['Sales']</p><p>The output (partially shown) is as follows:</p><div id="_idContainer108" class="IMG---Figure"><img src="Images/B15780_04_22.jpg" alt="Figure 4.22: Partial output while checking the summary statistics of sales&#13;&#10;" width="1269" height="406"/></div></li>
			</ol>
			<p class="figure-caption">Figure 4.22: Partial output while checking the summary statistics of sales</p>
			<p>Note how <strong class="source-inline">pandas</strong> has grouped the data by <strong class="source-inline">State</strong> first and then by cities under each state.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2Cm9eUl">https://packt.live/2Cm9eUl</a>. </p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3fxK43c">https://packt.live/3fxK43c</a>.</p>
			<p>We now understand how to use <strong class="source-inline">pandas</strong> to group our dataset and then find aggregate values such as the mean sales return of our top employees. We also looked at how pandas will display descriptive statistics about our data for us. Both of these techniques can be used to perform analysis on our superstore data.</p>
			<h1 id="_idParaDest-131"><a id="_idTextAnchor136"/>Detecting Outliers and Handling Missing Values</h1>
			<p>Outlier detection and handling missing values fall under the subtle art of data quality checking. A modeling or data mining process is fundamentally a complex series of computations whose output quality largely depends on the quality and consistency of the input data being fed. The responsibility of maintaining and gatekeeping that quality often falls on the shoulders of a data wrangling team.</p>
			<p>Apart from the obvious issue of poor-quality data, missing data can sometimes wreak havoc with the <strong class="bold">Machine Learning</strong> (<strong class="bold">ML</strong>) model downstream. A few ML models, such as Bayesian learning, are inherently robust to outliers and missing data, but common techniques such as Decision Trees and Random Forest have an issue with missing data because the fundamental splitting strategy employed by these techniques depends on an individual piece of data and not a cluster. Therefore, it is almost always imperative to impute missing data before handing it over to such an ML model.</p>
			<p>Outlier detection is a subtle art. Often, there is no universally agreed definition of an outlier. In a statistical sense, a data point that falls outside a certain range may often be classified as an outlier, but to apply that definition, you need to have a fairly high degree of certainty about the assumption of the nature and parameters of the inherent statistical distribution about the data. It takes a lot of data to build that statistical certainty and even after that, an outlier may not be just unimportant noise but a clue to something deeper. Let's look at an example with some fictitious sales data from an American fast-food chain restaurant. If we want to model the daily sales data as a time series, we will observe an unusual spike in the data somewhere around mid-April:</p>
			<div>
				<div id="_idContainer109" class="IMG---Figure">
					<img src="Images/B15780_04_23.jpg" alt="Figure 4.23: Fictitious sales data of an American fast-food chain restaurant&#13;&#10;" width="1405" height="556"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.23: Fictitious sales data of an American fast-food chain restaurant</p>
			<p>A good data scientist or data wrangler should develop curiosity about this data point rather than just rejecting it just because it falls outside the statistical range. In the actual anecdote, the sales figure spiked that day because of an unusual reason. So, the data was real. But just because it was real does not mean it is useful. In the final goal of building a smoothly varying time series model, this one point should not matter and should be rejected. In this chapter, however, we're going to look at ways of handling outliers instead of rejecting them.</p>
			<p>Therefore, the key to outliers is their systematic and timely detection in an incoming stream of millions of data or while reading data from cloud-based storage. In this section, we will quickly go over some basic statistical tests for detecting outliers and some basic imputation techniques for filling up missing data.</p>
			<h2 id="_idParaDest-132"><a id="_idTextAnchor137"/>Missing Values in Pandas</h2>
			<p>One of the most useful functions for detecting missing values is <strong class="source-inline">isnull</strong>. We'll use this function on a DataFrame called <strong class="source-inline">df_missing</strong> (based on the Superstore DataFrame we are working with), which, as the name suggests, will contain some missing values. You can create this DataFrame using the following command:</p>
			<p class="source-code">df_missing=pd.read_excel("<strong class="bold">../datasets/Sample - Superstore.xls</strong>",\</p>
			<p class="source-code">                         sheet_name="Missing")</p>
			<p class="source-code">df_missing</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Don't forget to change the path (highlighted) based on the location of the file on your system. </p>
			<p>The output will be as follows:</p>
			<div>
				<div id="_idContainer110" class="IMG---Figure">
					<img src="Images/B15780_04_24.jpg" alt="Figure 4.24: DataFrame with missing values&#13;&#10;" width="1665" height="1212"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.24: DataFrame with missing values</p>
			<p>We can see that the missing values are denoted by <strong class="source-inline">NaN</strong>. Now let's use the <strong class="source-inline">isnull</strong> function on the same DataFrame and observe the results:</p>
			<p class="source-code">df_missing.isnull()</p>
			<p>The output is as follows:</p>
			<p> </p>
			<div>
				<div id="_idContainer111" class="IMG---Figure">
					<img src="Images/B15780_04_25.jpg" alt="Figure 4.25: Output highlighting the missing values&#13;&#10;" width="938" height="634"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.25: Output highlighting the missing values</p>
			<p>As you can see, the missing values are indicated by the Boolean value <strong class="source-inline">True</strong>. Now, let's see how we can use the <strong class="source-inline">isnull</strong> function to deliver results that are a bit more user friendly. Here is an example of some very simple code to detect, count, and print out missing values in every column of a DataFrame:</p>
			<p class="source-code">for c in df_missing.columns:</p>
			<p class="source-code">    miss = df_missing[c].isnull().sum()</p>
			<p class="source-code">    if miss&gt;0:</p>
			<p class="source-code">        print("{} has {} missing value(s)".format(c,miss))</p>
			<p class="source-code">    else:</p>
			<p class="source-code">        print("{} has NO missing value!".format(c))</p>
			<p>This code scans every column of the DataFrame, calls the <strong class="source-inline">isnull</strong> function, and sums up the returned object (a <strong class="source-inline">pandas</strong> Series object, in this case) to count the number of missing values. If the missing value is greater than zero, it prints out the message accordingly. The output looks as follows:</p>
			<p> </p>
			<div>
				<div id="_idContainer112" class="IMG---Figure">
					<img src="Images/B15780_04_26.jpg" alt="Figure 4.26: Output of counting the missing values&#13;&#10;" width="1461" height="328"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.26: Output of counting the missing values</p>
			<p>As we can see from the preceding output, the missing values were detected from the <strong class="source-inline">Superstore</strong> dataset. </p>
			<p>To handle missing values, you should look for ways not to drop them altogether but to fill them somehow. The <strong class="source-inline">fillna</strong> method is a useful function for performing this task on <strong class="source-inline">pandas</strong> DataFrames. The <strong class="source-inline">fillna</strong> method may work for string data, but not for numerical columns such as sales or profits. So, we should restrict ourselves in regard to this fixed string replacement being used on non-numeric text-based columns only. The <strong class="source-inline">Pad</strong> or <strong class="source-inline">ffill</strong> function is used to fill forward the data, that is, copy it from the preceding data of the series. Forward fill is a technique where the missing value is filled with the previous value. On the other hand, backward fill or <strong class="source-inline">bfill</strong> uses the next value to fill in any missing data. Let's practice this with the following exercise.</p>
			<h2 id="_idParaDest-133"><a id="_idTextAnchor138"/>Exercise 4.05: Filling in the Missing Values Using the fillna Method</h2>
			<p>In this exercise, we are going to perform four techniques in order to deal with the missing values in a dataset. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">The <strong class="source-inline">superstore</strong> dataset file can be found here: <a href="https://packt.live/3dcVnMs">https://packt.live/3dcVnMs</a>.</p>
			<p>Firstly, we are going to replace the missing values with static values using the <strong class="source-inline">fillna</strong> method. Then, we will use the <strong class="source-inline">ffill</strong> and <strong class="source-inline">bfill</strong> methods to replace the missing values. Lastly, we will calculate the average of a column and replace the missing value with that. To do so, let's go through the following steps:</p>
			<ol>
				<li value="1">Import the necessary Python modules and read the Excel file from GitHub by using the <strong class="source-inline">read_excel</strong> method in <strong class="source-inline">pandas</strong>:<p class="source-code">import numpy as np</p><p class="source-code">import pandas as pd</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">df_missing = pd.read_excel("<strong class="bold">../datasets/Sample - Superstore.xls</strong>",\</p><p class="source-code">                           sheet_name="Missing")</p><p class="source-code">df_missing.head()</p><p class="callout-heading">Note</p><p class="callout">The highlighted path must be changed based on the location of the file on your system. </p><p>The output is as follows:</p><div id="_idContainer113" class="IMG---Figure"><img src="Images/B15780_04_27.jpg" alt="Figure 4.27: Snapshot of the dataset&#13;&#10;" width="893" height="317"/></div><p class="figure-caption">Figure 4.27: Snapshot of the dataset</p></li>
				<li>Fill in all the missing values with the <strong class="source-inline">FILL</strong> string by using the following command:<p class="source-code">df_missing.fillna('FILL')</p><p>The output is as follows:</p><div id="_idContainer114" class="IMG---Figure"><img src="Images/B15780_04_28.jpg" alt="Figure 4.28: Missing values replaced with FILL&#13;&#10;" width="1026" height="706"/></div><p class="figure-caption">Figure 4.28: Missing values replaced with FILL</p></li>
				<li>Fill in the specified columns with the <strong class="source-inline">FILL</strong> string by using the following command:<p class="source-code">df_missing[['Customer','Product']].fillna('FILL')</p><p>The output is as follows:</p><div id="_idContainer115" class="IMG---Figure"><img src="Images/B15780_04_29.jpg" alt="Figure 4.29: Specified columns replaced with FILL&#13;&#10;" width="1143" height="706"/></div><p class="figure-caption">Figure 4.29: Specified columns replaced with FILL</p><p class="callout-heading">Note</p><p class="callout">In all of these cases, the function works on a copy of the original DataFrame. So, if you want to make the changes permanent, you have to assign the DataFrames that are returned by these functions to the original DataFrame object.</p></li>
				<li>Fill in the values using <strong class="source-inline">ffill</strong> or forward fill by using the following command on the <strong class="source-inline">Sales</strong> column:<p class="source-code">df_missing['Sales'].fillna(method='ffill')</p><p>The output is as follows:</p><div id="_idContainer116" class="IMG---Figure"><img src="Images/B15780_04_30.jpg" alt="Figure 4.30: Sales column using the forward fill&#13;&#10;" width="763" height="301"/></div><p class="figure-caption">Figure 4.30: Sales column using the forward fill</p></li>
				<li>Use <strong class="source-inline">bfill</strong> to fill backward, that is, copy from the next data in the series:<p class="source-code">df_missing['Sales'].fillna(method='bfill')</p><p>The output is as follows:</p><div id="_idContainer117" class="IMG---Figure"><img src="Images/B15780_04_31.jpg" alt="Figure 4.31: Sales column using the backward fill&#13;&#10;" width="675" height="305"/></div><p class="figure-caption">Figure 4.31: Sales column using the backward fill</p><p>Let's compare these two series and see what happened in each case:</p><div id="_idContainer118" class="IMG---Figure"><img src="Images/B15780_04_32.jpg" alt="Figure 4.32: Using forward fill and backward fill to fill in missing data&#13;&#10;" width="1065" height="306"/></div><p class="figure-caption">Figure 4.32: Using forward fill and backward fill to fill in missing data</p><p>You can also fill by using a function average of DataFrames. For example, we may want to fill the missing values in <strong class="source-inline">Sales</strong> by the average sales amount. </p></li>
				<li>Fill the missing values in <strong class="source-inline">Sales</strong> by the average sales amount:<p class="source-code">df_missing['Sales'].fillna(df_missing.mean()['Sales'])</p><p>The output is as follows:</p><div id="_idContainer119" class="IMG---Figure"><img src="Images/B15780_04_33.jpg" alt="Figure 4.33: Sales column with average sales amount&#13;&#10;" width="713" height="313"/></div></li>
			</ol>
			<p class="figure-caption">Figure 4.33: Sales column with average sales amount</p>
			<p>The following screenshot shows what happened in the preceding code:</p>
			<div>
				<div id="_idContainer120" class="IMG---Figure">
					<img src="Images/B15780_04_34.jpg" alt="Figure 4.34: Using average to fill in missing data&#13;&#10;" width="1011" height="405"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.34: Using average to fill in missing data</p>
			<p>Here, we can observe that the missing value in the cell was filled by the average sales amount.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2ACDYjp">https://packt.live/2ACDYjp</a>. </p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2YNZnhh">https://packt.live/2YNZnhh</a>.</p>
			<p>With this, we have now seen how to replace missing values within a <strong class="source-inline">pandas</strong> DataFrame using four methods: static value, forward fill, backward fill, and the average. These are the fundamental techniques when cleaning data with missing values.</p>
			<h2 id="_idParaDest-134"><a id="_idTextAnchor139"/>The dropna Method</h2>
			<p>This function is used to simply drop the rows or columns that contain <strong class="source-inline">NaN</strong> or missing values. However, there is some choice involved.</p>
			<p>The following is the syntax of the <strong class="source-inline">dropna()</strong> method:</p>
			<p class="source-code">DataFrameName.dropna(axis=0, how='any', \</p>
			<p class="source-code">                     thresh=None, subset=None, \</p>
			<p class="source-code">                     inplace=False)</p>
			<p>If the <strong class="source-inline">axis</strong> parameter of a <strong class="source-inline">dropna()</strong> method is set to <strong class="source-inline">0</strong>, then rows containing missing values are dropped; if the axis parameter is set to <strong class="source-inline">1</strong>, then columns containing missing values are dropped. These are useful if we don't want to drop a particular row/column if the <strong class="source-inline">NaN</strong> values do not exceed a certain percentage.</p>
			<p>Two arguments that are useful for the <strong class="source-inline">dropna()</strong> method are as follows:</p>
			<ul>
				<li>The <strong class="source-inline">how</strong> argument determines if a row or column is removed from a DataFrame when we have at least one <strong class="source-inline">NaN</strong> value or all <strong class="source-inline">NaN</strong> values.</li>
				<li>The <strong class="source-inline">thresh</strong> argument requires that many non-<strong class="source-inline">NaN</strong> values to keep the row/ column.</li>
			</ul>
			<p>We'll practice using the <strong class="source-inline">dropna()</strong> method in the following exercise.</p>
			<h2 id="_idParaDest-135"><a id="_idTextAnchor140"/>Exercise 4.06: Dropping Missing Values with dropna</h2>
			<p>In this exercise, we will remove the cells in a dataset that don't contain data. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">The <strong class="source-inline">superstore</strong> dataset file can be found here: <a href="https://packt.live/3dcVnMs">https://packt.live/3dcVnMs</a>.</p>
			<p>We are going to use the <strong class="source-inline">dropna</strong> method in order to remove missing cells in a dataset. To do so, let's go through the following steps:</p>
			<ol>
				<li value="1">Import the necessary Python libraries and read the Excel file from GitHub by using the <strong class="source-inline">read_excel</strong> method in <strong class="source-inline">pandas</strong>:<p class="source-code">import numpy as np</p><p class="source-code">import pandas as pd</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">df_missing = pd.read_excel("<strong class="bold">../datasets/Sample - Superstore.xls",\</strong></p><p class="source-code"><strong class="bold">                           sheet_name="Missing</strong>")</p><p class="source-code">df_missing.head()</p><p class="callout-heading">Note</p><p class="callout">The highlighted path must be changed based on the location of the file on your system. </p><p>The output is as follows:</p><div id="_idContainer121" class="IMG---Figure"><img src="Images/B15780_04_35.jpg" alt="Figure 4.35: Superstore dataset&#13;&#10;" width="1349" height="401"/></div><p class="figure-caption">Figure 4.35: Superstore dataset</p><p class="callout-heading">Note</p><p class="callout">The outputs you get will vary from the ones shown in this exercise.</p></li>
				<li>To set the <strong class="source-inline">axis</strong> parameter to <strong class="source-inline">zero</strong> and drop all missing rows, use the following command:<p class="source-code">df_missing.dropna(axis=0)</p><p>The output is as follows:</p><div id="_idContainer122" class="IMG---Figure"><img src="Images/B15780_04_36.jpg" alt="Figure 4.36: Dropping all the missing rows&#13;&#10;" width="1408" height="485"/></div><p class="figure-caption">Figure 4.36: Dropping all the missing rows</p></li>
				<li>To set the <strong class="source-inline">axis</strong> parameter to <strong class="source-inline">1</strong> and drop all missing rows, use the following command:<p class="source-code">df_missing.dropna(axis=1)</p><p>The output is as follows:</p><div id="_idContainer123" class="IMG---Figure"><img src="Images/B15780_04_37.jpg" alt="Figure 4.37: Dropping rows or columns to handle missing data&#13;&#10;" width="1034" height="637"/></div><p class="figure-caption">Figure 4.37: Dropping rows or columns to handle missing data</p></li>
				<li>Drop the values with <strong class="source-inline">axis</strong> set to <strong class="source-inline">1</strong> and <strong class="source-inline">thresh</strong> set to <strong class="source-inline">10</strong>:<p class="source-code">df_missing.dropna(axis=1,thresh=10)</p><p>The output is as follows:</p><div id="_idContainer124" class="IMG---Figure"><img src="Images/B15780_04_38.jpg" alt="Figure 4.38: DataFrame with values dropped with axis=1 and thresh=10&#13;&#10;" width="1034" height="647"/></div></li>
			</ol>
			<p class="figure-caption">Figure 4.38: DataFrame with values dropped with axis=1 and thresh=10</p>
			<p>As you can see, some <strong class="source-inline">NaN</strong> values still exist, but because of the minimum threshold, those rows were kept in place.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2Ybvx7t">https://packt.live/2Ybvx7t</a>. </p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/30RNCsY">https://packt.live/30RNCsY</a>.</p>
			<p>In this exercise, we looked at dropping missing values rows and columns. This is a useful technique for a variety of cases, including when working with machine learning. Some machine learning models do not handle missing data well and removing them ahead of time can be best practices. </p>
			<h2 id="_idParaDest-136"><a id="_idTextAnchor141"/>Outlier Detection Using a Simple Statistical Test</h2>
			<p>As we've already discussed, outliers in a dataset can occur due to many factors and in many ways:</p>
			<ul>
				<li>Data entry errors</li>
				<li>Experimental errors (data extraction related)</li>
				<li>Measurement errors due to noise or instrumental failure</li>
				<li>Data processing errors (data manipulation or mutations due to coding errors)</li>
				<li>Sampling errors (extracting or mixing data from wrong or various sources)</li>
			</ul>
			<p>It is impossible to pinpoint one universal method for outlier detection. Here, we will show you some simple tricks for numeric data using standard statistical tests.</p>
			<p>Box plots may show unusual values. We can corrupt two sales values by assigning negatives, as follows:</p>
			<p class="source-code">df_sample = df[['Customer Name','State','Sales','Profit']]\</p>
			<p class="source-code">               .sample(n=50).copy()</p>
			<p class="source-code">df_sample['Sales'].iloc[5]=-1000.0</p>
			<p class="source-code">df_sample['Sales'].iloc[15]=-500.0</p>
			<p>To plot the box plot, use the following code:</p>
			<p class="source-code">df_sample.plot.box()</p>
			<p class="source-code">plt.title("Boxplot of sales and profit", fontsize=15)</p>
			<p class="source-code">plt.xticks(fontsize=15)</p>
			<p class="source-code">plt.yticks(fontsize=15)</p>
			<p class="source-code">plt.grid(True)</p>
			<p>The output (which will vary with each run) is as follows:</p>
			<div>
				<div id="_idContainer125" class="IMG---Figure">
					<img src="Images/B15780_04_39.jpg" alt="Figure 4.39: Box plot of sales and profit&#13;&#10;" width="1665" height="687"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.39: Box plot of sales and profit</p>
			<p>We can create simple box plots to check for any unusual/nonsensical values. For example, in the preceding example, we intentionally corrupted two sales values so that they were negative, and they were readily caught in a box plot.</p>
			<p>Note that profit may be negative, so those negative points are generally not suspicious. But sales cannot be negative in general, so they are detected as outliers.</p>
			<p>We can create a distribution of a numerical quantity and check for values that lie at the extreme end to see if they are truly part of the data or outlier. For example, if a distribution is almost normal, then any value more than four or five standard deviations away may be a suspect:</p>
			<div>
				<div id="_idContainer126" class="IMG---Figure">
					<img src="Images/B15780_04_40.jpg" alt="Figure 4.40: Value away from the main outliers&#13;&#10;" width="862" height="355"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.40: Value away from the main outliers</p>
			<h1 id="_idParaDest-137"><a id="_idTextAnchor142"/>Concatenating, Merging, and Joining</h1>
			<p>Merging and joining tables or datasets are highly common operations in the day-to-day job of a data wrangling professional. These operations are akin to the <strong class="source-inline">JOIN</strong> query in SQL for relational database tables. Often, the key data is present in multiple tables, and those records need to be brought into one combined table that matches on that common key. This is an extremely common operation in any type of sales or transactional data, and therefore must be mastered by a data wrangler. The <strong class="source-inline">pandas</strong> library offers nice and intuitive built-in methods to perform various types of <strong class="source-inline">JOIN</strong> queries involving multiple DataFrame objects.</p>
			<h2 id="_idParaDest-138"><a id="_idTextAnchor143"/>Exercise 4.07: Concatenation in Datasets</h2>
			<p>In this exercise, we will concatenate DataFrames along various axes (rows or columns).</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The <strong class="source-inline">superstore</strong> dataset file can be found here: <a href="https://packt.live/3dcVnMs">https://packt.live/3dcVnMs</a>.</p>
			<p>This is a very useful operation as it allows you to grow a DataFrame as the new data comes in or new feature columns need to be inserted into the table. To do so, let's go through the following steps:</p>
			<ol>
				<li value="1">Read the Excel file from GitHub by using the <strong class="source-inline">read_excel</strong> method in <strong class="source-inline">pandas</strong>:<p class="source-code">import numpy as np</p><p class="source-code">import pandas as pd</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">df = pd.read_excel("<strong class="bold">../datasets/Sample - Superstore.xls</strong>")</p><p class="source-code">df.head()</p><p class="callout-heading">Note</p><p class="callout">The highlighted path must be changed based on the location of the file on your system. </p><p>The output (partially shown) will be as follows:</p><div id="_idContainer127" class="IMG---Figure"><img src="Images/B15780_04_41.jpg" alt="Figure 4.41: Partial output of the DataFrame&#13;&#10;" width="1266" height="842"/></div><p class="figure-caption">Figure 4.41: Partial output of the DataFrame</p></li>
				<li>Sample <strong class="source-inline">4</strong> records each to create three DataFrames at random from the original sales dataset we are working with:<p class="source-code">df_1 = df[['Customer Name','State',\</p><p class="source-code">           'Sales','Profit']].sample(n=4)</p><p class="source-code">df_2 = df[['Customer Name','State',\</p><p class="source-code">           'Sales','Profit']].sample(n=4)</p><p class="source-code">df_3 = df[['Customer Name','State',\</p><p class="source-code">           'Sales','Profit']].sample(n=4)</p></li>
				<li>Create a combined DataFrame with all the rows concatenated by using the following code:<p class="source-code">df_cat1 = pd.concat([df_1,df_2,df_3], axis=0)</p><p class="source-code">df_cat1</p><p>The output (partially shown) is as follows:</p><div id="_idContainer128" class="IMG---Figure"><img src="Images/B15780_04_42.jpg" alt="Figure 4.42: Partial output after concatenating the DataFrames&#13;&#10;" width="885" height="579"/></div><p class="figure-caption">Figure 4.42: Partial output after concatenating the DataFrames</p><p>As you can see, concatenation will vertically combine multiple DataFrames. You can also try concatenating along the columns, although that does not make any practical sense for this particular example. However, <strong class="source-inline">pandas</strong> fills in the unavailable values with <strong class="source-inline">NaN</strong> for that operation.</p><p class="callout-heading">Note</p><p class="callout">The outputs you get will vary from the ones shown in this exercise.</p></li>
				<li>Create a combined DataFrame with all the columns concatenated by using the following code:<p class="source-code">df_cat2 = pd.concat([df_1,df_2,df_3], axis=1)</p><p class="source-code">df_cat2</p><p>The output (partially shown) is as follows:</p><div id="_idContainer129" class="IMG---Figure"><img src="Images/B15780_04_43.jpg" alt="Figure 4.43: Partial output after concatenating the DataFrames&#13;&#10;" width="984" height="731"/></div></li>
			</ol>
			<p class="figure-caption">Figure 4.43: Partial output after concatenating the DataFrames</p>
			<p>As we can observe, the cells in the dataset that do not contain any values are replaced with <strong class="source-inline">NaN</strong> values.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3epn5aB">https://packt.live/3epn5aB</a>. </p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3edUPrh">https://packt.live/3edUPrh</a>.</p>
			<h2 id="_idParaDest-139"><a id="_idTextAnchor144"/>Merging by a Common Key</h2>
			<p>Merging by a common key is an extremely common operation for data tables as it allows you to rationalize multiple sources of data in one master database – that is, if they have some common features/keys.</p>
			<p>When joining and merging two DataFrames, we use two separate types: <strong class="bold">inner</strong> and <strong class="bold">outer {left|right}</strong>. Let's take a look at them:</p>
			<ul>
				<li><strong class="bold">Inner</strong>: A combining method that uses a column or key to be compared on each dataset. Rows that share the same column or key will be present after the join.</li>
				<li><strong class="bold">Outer</strong>: A way to combine datasets such as inner, but all data on the right or left (depending on which is chosen) is kept, and matching data from the opposite side is combined.</li>
			</ul>
			<p>This is often the first step in building a large database for machine learning tasks where daily incoming data may be put into separate tables. However, at the end of the day, the most recent table needs to be merged with the master data table so that it can be fed into the backend machine learning server, which will then update the model and its prediction capacity. Merge is a way to combine DataFrames vertically, using a column to compare on. The functionality of merge and join are very similar; their capabilities are the same.</p>
			<h2 id="_idParaDest-140"><a id="_idTextAnchor145"/>Exercise 4.08: Merging by a Common Key</h2>
			<p>In this exercise, we'll create two DataFrames with the <strong class="source-inline">Customer Name</strong> common key from the Superstore dataset. Then, we will use the inner and outer joins to merge or combine these DataFrames. To do so, let's go through the following steps:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The Superstore file can be found here: <a href="https://packt.live/3dcVnMs">https://packt.live/3dcVnMs</a>.</p>
			<ol>
				<li value="1">Import the necessary Python libraries and read the Excel file from GitHub by using the <strong class="source-inline">read_excel</strong> method in <strong class="source-inline">pandas</strong>:<p class="source-code">import numpy as np</p><p class="source-code">import pandas as pd</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">df = pd.read_excel("<strong class="bold">../datasets/Sample - Superstore.xls</strong>")</p><p class="source-code">df.head()</p><p class="callout-heading">Note</p><p class="callout">The highlighted path must be changed based on the location of the file on your system. </p><p>The partial output of the preceding step is as follows:</p><div id="_idContainer130" class="IMG---Figure"><img src="Images/B15780_04_44.jpg" alt="Figure 4.44: Partial output of the Data frame&#13;&#10;" width="1276" height="868"/></div><p class="figure-caption">Figure 4.44: Partial output of the Data frame</p><p>One DataFrame, <strong class="source-inline">df_1</strong>, will have shipping information associated with the customer name, and another table, <strong class="source-inline">df_2</strong>, will have the product information tabulated.</p></li>
				<li>Create the <strong class="source-inline">df1</strong> DataFrame with the <strong class="source-inline">Customer Name</strong> common key:<p class="source-code">df_1=df[['Ship Date','Ship Mode','Customer Name']][0:4]</p><p class="source-code">df_1</p><p>The output of the first DataFrame is as follows:</p><div id="_idContainer131" class="IMG---Figure"><img src="Images/B15780_04_45.jpg" alt="Figure 4.45: Entries in table df_1&#13;&#10;" width="1216" height="346"/></div><p class="figure-caption">Figure 4.45: Entries in table df_1</p></li>
				<li>Create the second DataFrame, <strong class="source-inline">df2</strong>, with the <strong class="source-inline">Customer Name</strong> common key, as follows:<p class="source-code">df_2=df[['Customer Name','Product Name','Quantity']][0:4]</p><p class="source-code">df_2</p><p>The output is as follows:</p><div id="_idContainer132" class="IMG---Figure"><img src="Images/B15780_04_46.jpg" alt="Figure 4.46: Entries in table df_2&#13;&#10;" width="1282" height="348"/></div><p class="figure-caption">Figure 4.46: Entries in table df_2</p></li>
				<li>Join these two tables with an inner join by using the following command:<p class="source-code">pd.merge(df_1,df_2,on='Customer Name',how='inner')</p><p>The output is as follows:</p><div id="_idContainer133" class="IMG---Figure"><img src="Images/B15780_04_47.jpg" alt="Figure 4.47: Inner join on table df_1 and table df_2&#13;&#10;" width="1247" height="560"/></div><p class="figure-caption">Figure 4.47: Inner join on table df_1 and table df_2</p></li>
				<li>Drop the duplicates by using the following command:<p class="source-code">pd.merge(df_1,df_2,on='Customer Name',\</p><p class="source-code">         how='inner').drop_duplicates()</p><p>The output is as follows:</p><div id="_idContainer134" class="IMG---Figure"><img src="Images/B15780_04_48.jpg" alt="Figure 4.48: Inner join on table df_1 and table df_2 after dropping the duplicates&#13;&#10;" width="1265" height="429"/></div><p class="figure-caption">Figure 4.48: Inner join on table df_1 and table df_2 after dropping the duplicates</p></li>
				<li>Extract another small table called <strong class="source-inline">df_3</strong> to show the concept of an outer join:<p class="source-code">df_3=df[['Customer Name','Product Name','Quantity']][2:6]</p><p class="source-code">df_3</p><p>The output is as follows:</p><div id="_idContainer135" class="IMG---Figure"><img src="Images/B15780_04_49.jpg" alt="Figure 4.49: Creating table df_3&#13;&#10;" width="1209" height="360"/></div><p class="figure-caption">Figure 4.49: Creating table df_3</p></li>
				<li>Perform an inner join on <strong class="source-inline">df_1</strong> and <strong class="source-inline">df_3</strong> by using the following command:<p class="source-code">pd.merge(df_1,df_3,on='Customer Name',\</p><p class="source-code">         how='inner').drop_duplicates()</p><p>The output is as follows:</p><div id="_idContainer136" class="IMG---Figure"><img src="Images/B15780_04_50.jpg" alt="Figure 4.50: Merging table df_1 and table df_3 and dropping duplicates&#13;&#10;" width="1564" height="411"/></div><p class="figure-caption">Figure 4.50: Merging table df_1 and table df_3 and dropping duplicates</p></li>
				<li>Perform an outer join on <strong class="source-inline">df_1</strong> and <strong class="source-inline">df_3</strong> by using the following command:<p class="source-code">pd.merge(df_1,df_3,on='Customer Name',\</p><p class="source-code">         how='outer').drop_duplicates()</p><p>The output is as follows:</p><div id="_idContainer137" class="IMG---Figure"><img src="Images/B15780_04_51.jpg" alt="Figure 4.51: Outer join on table df_1 and table df_3 and dropping the duplicates&#13;&#10;" width="1221" height="491"/></div></li>
			</ol>
			<p class="figure-caption">Figure 4.51: Outer join on table df_1 and table df_3 and dropping the duplicates</p>
			<p>Notice how some <strong class="source-inline">NaN</strong> and <strong class="source-inline">NaT</strong> values are inserted automatically because no corresponding entries could be found for those records, as those are the entries with unique customer names from their respective tables. <strong class="source-inline">NaT</strong> represents a <strong class="source-inline">Not a Time</strong> object, as the objects in the <strong class="source-inline">Ship Date</strong> column are timestamped objects.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2Y8G5UW">https://packt.live/2Y8G5UW</a>. </p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/30RNUA4">https://packt.live/30RNUA4</a>.</p>
			<p>With this, we have gone over how to use the <strong class="source-inline">merge</strong> method to do inner and outer joins. </p>
			<h2 id="_idParaDest-141"><a id="_idTextAnchor146"/>The join Method</h2>
			<p>Joining is performed based on <strong class="bold">index keys</strong> and is done by combining the columns of two potentially differently indexed DataFrames into a single one. It offers a faster way to accomplish merging by row indices. This is useful if the records in different tables are indexed differently but represent the same inherent data and you want to merge them into a single table:</p>
			<h2 id="_idParaDest-142"><a id="_idTextAnchor147"/>Exercise 4.09: The join Method</h2>
			<p>In this exercise, we will create two DataFrames and perform the different kind of joins on these DataFrames.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The Superstore file can be found here: <a href="https://packt.live/3dcVnMs">https://packt.live/3dcVnMs</a>.</p>
			<p>To complete this exercise, perform the following steps:</p>
			<ol>
				<li value="1">Import the Python libraries and load the file from GitHub by using the <strong class="source-inline">read_excel</strong> method in <strong class="source-inline">pandas</strong>:<p class="source-code">import numpy as np</p><p class="source-code">import pandas as pd</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">df = pd.read_excel("<strong class="bold">../datasets/Sample - Superstore.xls</strong>")</p><p class="source-code">df.head()</p><p class="callout-heading">Note</p><p class="callout">The highlighted path must be changed based on the location of the file on your system. </p><p>The partial output of the code is as follows:</p><div id="_idContainer138" class="IMG---Figure"><img src="Images/B15780_04_52.jpg" alt="Figure 4.52: Partial output of the DataFrame&#13;&#10;" width="1255" height="889"/></div><p class="figure-caption">Figure 4.52: Partial output of the DataFrame</p></li>
				<li>Create <strong class="source-inline">df1</strong> with <strong class="source-inline">Customer Name</strong> as the index by using the following command:<p class="source-code">df_1=df[['Customer Name','Ship Date','Ship Mode']][0:4]</p><p class="source-code">df_1.set_index(['Customer Name'],inplace=True)</p><p class="source-code">df_1</p><p>The output is as follows:</p><div id="_idContainer139" class="IMG---Figure"><img src="Images/B15780_04_53.jpg" alt="Figure 4.53: DataFrame df_1&#13;&#10;" width="1523" height="472"/></div><p class="figure-caption">Figure 4.53: DataFrame df_1</p></li>
				<li>Create <strong class="source-inline">df2</strong> with <strong class="source-inline">Customer Name</strong> as the index by using the following command:<p class="source-code">df_2=df[['Customer Name','Product Name','Quantity']][2:6]</p><p class="source-code">df_2.set_index(['Customer Name'],inplace=True) </p><p class="source-code">df_2</p><p>The output is as follows:</p><div id="_idContainer140" class="IMG---Figure"><img src="Images/B15780_04_54.jpg" alt="Figure 4.54: DataFrame df_2&#13;&#10;" width="1389" height="471"/></div><p> </p><p class="figure-caption">Figure 4.54: DataFrame df_2</p></li>
				<li>Perform a left join on <strong class="source-inline">df_1</strong> and <strong class="source-inline">df_2</strong> by using the following command:<p class="source-code">df_1.join(df_2,how='left').drop_duplicates()</p><p>The output is as follows:</p><div id="_idContainer141" class="IMG---Figure"><img src="Images/B15780_04_55.jpg" alt="Figure 4.55: Left join on table df_1 and table df_2 after dropping the duplicates&#13;&#10;" width="1568" height="419"/></div><p class="figure-caption">Figure 4.55: Left join on table df_1 and table df_2 after dropping the duplicates</p></li>
				<li>Perform a right join on <strong class="source-inline">df_1</strong> and <strong class="source-inline">df_2</strong> by using the following command:<p class="source-code">df_1.join(df_2,how='right').drop_duplicates()</p><p>The output is as follows:</p><div id="_idContainer142" class="IMG---Figure"><img src="Images/B15780_04_56.jpg" alt="Figure 4.56: Right join on table df_1 and table df_2 after dropping the duplicates&#13;&#10;" width="1240" height="467"/></div><p class="figure-caption">Figure 4.56: Right join on table df_1 and table df_2 after dropping the duplicates</p></li>
				<li>Perform an inner join on <strong class="source-inline">df_1</strong> and <strong class="source-inline">df_2</strong> by using the following command:<p class="source-code">df_1.join(df_2,how='inner').drop_duplicates()</p><p>The output is as follows:</p><div id="_idContainer143" class="IMG---Figure"><img src="Images/B15780_04_57.jpg" alt="Figure 4.57: Inner join on table df_1 and table df_2 after dropping the duplicates&#13;&#10;" width="1235" height="291"/></div><p class="figure-caption">Figure 4.57: Inner join on table df_1 and table df_2 after dropping the duplicates</p></li>
				<li>Perform an outer join on <strong class="source-inline">df_1</strong> and <strong class="source-inline">df_2</strong> by using the following command:<p class="source-code">df_1.join(df_2,how='outer').drop_duplicates()</p><p>The output is as follows:</p><div id="_idContainer144" class="IMG---Figure"><img src="Images/B15780_04_58.jpg" alt="Figure 4.58: Outer join on table df_1 and table df_2 after dropping the duplicates&#13;&#10;" width="1250" height="520"/></div></li>
			</ol>
			<p class="figure-caption">Figure 4.58: Outer join on table df_1 and table df_2 after dropping the duplicates</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/30S9nZH">https://packt.live/30S9nZH</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2NbDweg">https://packt.live/2NbDweg</a>.</p>
			<p>We have now gone through the basic functionality of <strong class="source-inline">pandas</strong> DataFrame joining. We used inner and out joining and showed you how we can use indexes to perform a join and how it can help in analysis. </p>
			<h1 id="_idParaDest-143"><a id="_idTextAnchor148"/>Useful Methods of Pandas</h1>
			<p>In this section, we will discuss some small utility functions that are offered by <strong class="source-inline">pandas</strong> so that we can work efficiently with DataFrames. They don't fall under any particular group of functions, so they are mentioned here under the Miscellaneous category. Let's discuss these miscellaneous methods in detail.</p>
			<h2 id="_idParaDest-144"><a id="_idTextAnchor149"/>Randomized Sampling</h2>
			<p>In this section, we will discuss random sampling data from our DataFrames. This is a very common task in a variety of pipelines, one of which is machine learning. Sampling is often used in machine learning data-wrangling pipelines when choosing which data to train and which data to test against. Sampling a random fraction of a big DataFrame is often very useful so that we can practice other methods on them and test our ideas. If you have a database table of 1 million records, then it is not computationally effective to run your test scripts on the full table.</p>
			<p>However, you may also not want to extract only the first 100 elements as the data may have been sorted by a particular key and you may get an uninteresting table back, which may not represent the full statistical diversity of the parent database.</p>
			<p>In these situations, the <strong class="source-inline">sample</strong> method comes in super handy so that we can randomly choose a controlled fraction of the DataFrame.</p>
			<h2 id="_idParaDest-145"><a id="_idTextAnchor150"/>Exercise 4.10: Randomized Sampling</h2>
			<p>In this exercise, we are going to randomly take five samples from the Superstore dataset and calculate a definite fraction of the data to be sampled. To do so, let's go through the following steps:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The Superstore file can be found here: <a href="https://packt.live/3dcVnMs">https://packt.live/3dcVnMs</a>.</p>
			<ol>
				<li value="1">Import the necessary Python modules and read them from GitHub by using the <strong class="source-inline">read_excel</strong> method in <strong class="source-inline">pandas</strong>:<p class="source-code">import numpy as np</p><p class="source-code">import pandas as pd</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">df = pd.read_excel("<strong class="bold">../datasets/Sample - Superstore.xls</strong>")</p><p class="source-code">df.head()</p><p class="callout-heading">Note</p><p class="callout">The highlighted path must be changed based on the location of the file on your system. </p><p>The partial output will be:</p><div id="_idContainer145" class="IMG---Figure"><img src="Images/B15780_04_59.jpg" alt="Figure 4.59: Partial output of the DataFrame&#13;&#10;" width="1253" height="885"/></div><p class="figure-caption">Figure 4.59: Partial output of the DataFrame</p></li>
				<li>Specify the number of samples that we require from the DataFrame by using the following command:<p class="source-code">df.sample(n=5)</p><p>The random output (partially shown) is as follows:</p><div id="_idContainer146" class="IMG---Figure"><img src="Images/B15780_04_60.jpg" alt="Figure 4.60: DataFrame with five samples&#13;&#10;" width="1128" height="476"/></div><p class="figure-caption">Figure 4.60: DataFrame with five samples</p><p class="callout-heading">Note</p><p class="callout">The outputs you get will vary from the ones shown in this exercise.</p></li>
				<li>Specify a definite fraction (percentage) of the data to be sampled by using the following command:<p class="source-code">df.sample(frac=0.1)</p><p>The output is as follows:</p><div id="_idContainer147" class="IMG---Figure"><img src="Images/B15780_04_61.jpg" alt="Figure 4.61: Partial output of a DataFrame with 0.1% data sampled&#13;&#10;" width="803" height="673"/></div><p class="figure-caption">Figure 4.61: Partial output of a DataFrame with 0.1% data sampled</p><p>You can also choose if sampling is done with replacement, that is, whether the same record can be chosen more than once. The default <strong class="source-inline">replace</strong> choice is <strong class="source-inline">FALSE</strong>, that is, no repetition and sampling will try to choose new elements only.</p></li>
				<li>Choose the sampling by using the following command:<p class="source-code">df.sample(frac=0.1, replace=True)</p><p>The output is as follows:</p><div id="_idContainer148" class="IMG---Figure"><img src="Images/B15780_04_62.jpg" alt="Figure 4.62: DataFrame with 0.1% data sampled and repetition enabled&#13;&#10;" width="849" height="617"/></div></li>
			</ol>
			<p class="figure-caption">Figure 4.62: DataFrame with 0.1% data sampled and repetition enabled</p>
			<p>Here, as you can see, we have encouraged repetitions in the sampled data by setting the <strong class="source-inline">replace</strong> parameter to <strong class="source-inline">True</strong>. Therefore, the same elements could be chosen again while performing random sampling.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2N7fWzt">https://packt.live/2N7fWzt</a>. </p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2YLTt0f">https://packt.live/2YLTt0f</a>.</p>
			<h2 id="_idParaDest-146"><a id="_idTextAnchor151"/>The value_counts Method</h2>
			<p>We discussed the <strong class="source-inline">unique</strong> method previously, which finds and counts the unique records from a DataFrame. Another useful function in a similar vein is <strong class="source-inline">value_counts</strong>. This function returns an object containing counts of unique values. In the object that is returned, the first element is the most frequently used object. The elements are arranged in descending order.</p>
			<p>Let's consider a practical application of this method to illustrate its utility. Suppose your manager asks you to list the top 10 customers from the big sales database that you have. So, the business question is: which 10 customers' names occur the most frequently in the sales table? You can achieve this with a SQL query if the data is in an RDBMS, but in pandas, this can be done by using one simple function:</p>
			<p class="source-code">df['Customer Name'].value_counts()[:10]</p>
			<p>The output is as follows:</p>
			<p> </p>
			<div>
				<div id="_idContainer149" class="IMG---Figure">
					<img src="Images/B15780_04_63.jpg" alt="Figure 4.63: List of top 10 customers&#13;&#10;" width="1253" height="478"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.63: List of top 10 customers</p>
			<p>The <strong class="source-inline">value_counts</strong> method returns a series of counts of all unique customer names sorted by the frequency of the count. By asking for only the first 10 elements of that list, this code returns a series of the most frequently occurring top 10 customer names.</p>
			<h2 id="_idParaDest-147"><a id="_idTextAnchor152"/>Pivot Table Functionality</h2>
			<p>Similar to group by, pandas also offer pivot table functionality, which works the same as a Pivot Table in spreadsheet programs such as MS Excel. For example, in this sales database, you want to know the average sales, profit, and quantity sold by Region and State (two levels of index).</p>
			<p>We can extract this information by using one simple piece of code (we sample 100 records first to keep the computation fast and then apply the code):</p>
			<p class="source-code">df_sample = df.sample(n=100)</p>
			<p class="source-code">df_sample.pivot_table(values=['Sales','Quantity','Profit'],\</p>
			<p class="source-code">                      index=['Region','State'],aggfunc='mean')</p>
			<p>The output is as follows (note that your specific output may be different due to random sampling):</p>
			<p> </p>
			<div>
				<div id="_idContainer150" class="IMG---Figure">
					<img src="Images/B15780_04_64.jpg" alt="Figure 4.64: Sample of 100 records&#13;&#10;" width="980" height="607"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.64: Sample of 100 records</p>
			<p>Sorting a table by a particular column is one of the most frequently used operations in the daily work of an analyst. Sorting can help you understand your data better while presenting it in a specific view of the data. When training a machine learning model, the way data is sorted can impact the performance of a model based on the sampling that's being done. Not surprisingly, <strong class="source-inline">pandas</strong> provide a simple and intuitive method for sorting called the <strong class="source-inline">sort_values</strong> method. We'll practice using this in the following exercise. </p>
			<h2 id="_idParaDest-148"><a id="_idTextAnchor153"/>Exercise 4.11: Sorting by Column Values – the sort_values Method</h2>
			<p>In this exercise, we will take a random sample of <strong class="source-inline">15</strong> records from the Superstore dataset. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">The Superstore file can be found here: <a href="https://packt.live/3dcVnMs">https://packt.live/3dcVnMs</a>.</p>
			<p>We will sort the column values in the dataset with respect to column names using the <strong class="source-inline">sort_values</strong> method. To do so, let's go through the following steps: </p>
			<ol>
				<li value="1">Import the necessary Python modules and read the Excel file from GitHub by using the <strong class="source-inline">read_excel</strong> method in <strong class="source-inline">pandas</strong>:<p class="source-code">import numpy as np</p><p class="source-code">import pandas as pd</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">df = pd.read_excel("<strong class="bold">../datasets/Sample - Superstore.xls</strong>")</p><p class="source-code">df.head()</p><p class="callout-heading">Note</p><p class="callout">The highlighted path must be changed based on the location of the file on your system. </p><p>The output (partially shown) will be as follows:</p><div id="_idContainer151" class="IMG---Figure"><img src="Images/B15780_04_65.jpg" alt="Figure 4.65: Partial output of the DataFrame&#13;&#10;" width="1064" height="743"/></div><p class="figure-caption">Figure 4.65: Partial output of the DataFrame</p></li>
				<li>Take a random sample of <strong class="source-inline">15</strong> records and then sort by the <strong class="source-inline">Sales</strong> column and then by both the <strong class="source-inline">Sales</strong> and <strong class="source-inline">State</strong> columns together:<p class="source-code">df_sample=df[['Customer Name','State',\</p><p class="source-code">              'Sales','Quantity']].sample(n=15)</p><p class="source-code">df_sample</p><p>The output is as follows:</p><div id="_idContainer152" class="IMG---Figure"><img src="Images/B15780_04_66.jpg" alt="Figure 4.66: Sample of 15 records&#13;&#10;" width="754" height="636"/></div><p class="figure-caption">Figure 4.66: Sample of 15 records</p><p class="callout-heading">Note</p><p class="callout">The outputs you get will vary from the ones shown in this exercise.</p></li>
				<li>Sort the values with respect to <strong class="source-inline">Sales</strong> by using the following command:<p class="source-code">df_sample.sort_values(by='Sales')</p><p>The output is as follows:</p><p> </p><div id="_idContainer153" class="IMG---Figure"><img src="Images/B15780_04_67.jpg" alt="Figure 4.67: DataFrame with the Sales value sorted&#13;&#10;" width="778" height="642"/></div><p class="figure-caption">Figure 4.67: DataFrame with the Sales value sorted</p></li>
				<li>Sort the values with respect to <strong class="source-inline">Sales</strong> and <strong class="source-inline">State</strong>:<p class="source-code">df_sample.sort_values(by=['State','Sales'])</p><p>The output is as follows:</p><p> </p><div id="_idContainer154" class="IMG---Figure"><img src="Images/B15780_04_68.jpg" alt="Figure 4.68: DataFrame sorted with respect to Sales and State&#13;&#10;" width="740" height="644"/></div></li>
			</ol>
			<p class="figure-caption">Figure 4.68: DataFrame sorted with respect to Sales and State</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3dcWNXi">https://packt.live/3dcWNXi</a>. </p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/30UqwSn">https://packt.live/30UqwSn</a>.</p>
			<p>The <strong class="source-inline">pandas</strong> library provides great flexibility for working with user-defined functions of arbitrary complexity through the <strong class="source-inline">apply</strong> method. Much like the native Python <strong class="source-inline">apply</strong> function, this method accepts a user-defined function and additional arguments and returns a new column after applying the function on a particular column elementwise.</p>
			<p>As an example, suppose we want to create a column of categorical features such as high/medium/low based on the sales price column. Note that this is a conversion from a numeric value into a categorical factor (string) based on certain conditions (threshold values of sales).</p>
			<h2 id="_idParaDest-149"><a id="_idTextAnchor154"/>Exercise 4.12: Flexibility of User-Defined Functions with the apply Method</h2>
			<p>In this exercise, we will create a user-defined function called <strong class="source-inline">categorize_sales</strong> that categorizes Sales data based on price. If the <strong class="source-inline">price</strong> is less than <strong class="source-inline">50</strong>, it is classified as <strong class="source-inline">Low</strong>, if the <strong class="source-inline">price</strong> is less than <strong class="source-inline">200</strong>, it is classified as <strong class="source-inline">Medium</strong>, or <strong class="source-inline">High</strong> if the <strong class="source-inline">price</strong> doesn't fall under either of these categories. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">The Superstore file can be found here: <a href="https://packt.live/3dcVnMs">https://packt.live/3dcVnMs</a>.</p>
			<p>We'll then take 100 random samples from the <strong class="source-inline">superstore</strong> dataset and use the <strong class="source-inline">apply</strong> method on the <strong class="source-inline">categorize_sales</strong> function in order to create a new column to store the values returned by the function. To do so, perform the following steps: </p>
			<ol>
				<li value="1">Import the necessary Python modules and read the Excel file from GitHub by using the <strong class="source-inline">read_excel</strong> method in <strong class="source-inline">pandas</strong>:<p class="source-code">import numpy as np</p><p class="source-code">import pandas as pd</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">df = pd.read_excel("<strong class="bold">../datasets/Sample - Superstore.xls</strong>")</p><p class="source-code">df.head()</p><p class="callout-heading">Note</p><p class="callout">The highlighted path must be changed based on the location of the file on your system. </p><p>The output (partially shown) will be:</p><div id="_idContainer155" class="IMG---Figure"><img src="Images/B15780_04_69.jpg" alt="Figure 4.69: Partial output of the DataFrame&#13;&#10;" width="1053" height="737"/></div><p class="figure-caption">Figure 4.69: Partial output of the DataFrame</p></li>
				<li>Create a user-defined function, as follows:<p class="source-code">def categorize_sales(price):</p><p class="source-code">    if price &lt; 50:</p><p class="source-code">        return "Low"</p><p class="source-code">    elif price &lt; 200:</p><p class="source-code">        return "Medium"</p><p class="source-code">    else:</p><p class="source-code">        return "High"</p></li>
				<li>Sample <strong class="source-inline">100</strong> records randomly from the database:<p class="source-code">df_sample=df[['Customer Name',\</p><p class="source-code">              'State','Sales']].sample(n=100)</p><p class="source-code">df_sample.head(10)</p><p>The output is as follows:</p><p> </p><div id="_idContainer156" class="IMG---Figure"><img src="Images/B15780_04_70.jpg" alt="Figure 4.70: 100 sample records from the database&#13;&#10;" width="945" height="562"/></div><p class="figure-caption">Figure 4.70: 100 sample records from the database</p><p class="callout-heading">Note</p><p class="callout">The outputs you get will vary from the ones shown in this exercise.</p></li>
				<li>Use the <strong class="source-inline">apply</strong> method to apply the categorization function to the <strong class="source-inline">Sales</strong> column. We need to create a new column to store the category string values that are returned by the function:<p class="source-code">df_sample['Sales Price Category']=df_sample['Sales']\</p><p class="source-code">                                  .apply(categorize_sales)</p><p class="source-code">df_sample.head(10)</p><p>The output is as follows:</p><p> </p><div id="_idContainer157" class="IMG---Figure"><img src="Images/B15780_04_71.jpg" alt="Figure 4.71: DataFrame with 10 rows after using the apply function on the Sales column&#13;&#10;" width="977" height="600"/></div><p class="figure-caption">Figure 4.71: DataFrame with 10 rows after using the apply function on the Sales column</p><p>The <strong class="source-inline">apply</strong> method also works with the built-in native Python functions. </p></li>
				<li>For practice, let's create another column for storing the length of the name of the customer. We can do this using the familiar <strong class="source-inline">len</strong> function:<p class="source-code">df_sample['Customer Name Length']=df_sample['Customer Name']\</p><p class="source-code">                                  .apply(len)</p><p class="source-code">df_sample.head(10)</p><p>The output is as follows:</p><p> </p><div id="_idContainer158" class="IMG---Figure"><img src="Images/B15780_04_72.jpg" alt="Figure 4.72: DataFrame with a new column&#13;&#10;" width="1186" height="580"/></div><p class="figure-caption">Figure 4.72: DataFrame with a new column</p><p>Instead of writing out a separate function, we can even insert <em class="italic">lambda expressions</em> directly into the <strong class="source-inline">apply</strong> method for short functions. For example, let's say we are promoting our product and want to show the discounted sales price if the original price is <em class="italic">&gt; $200</em>. </p></li>
				<li>Use a <strong class="source-inline">lambda</strong> function and the <strong class="source-inline">apply</strong> method to do so:<p class="source-code">df_sample['Discounted Price']=df_sample['Sales']\</p><p class="source-code">                              .apply(lambda x:0.85*x if x&gt;200 \</p><p class="source-code">                              else x)</p><p class="source-code">df_sample.head(10)</p><p>The output is as follows:</p><p> </p><div id="_idContainer159" class="IMG---Figure"><img src="Images/B15780_04_73.jpg" alt="Figure 4.73: Lambda function&#13;&#10;" width="1252" height="501"/></div></li>
			</ol>
			<p class="figure-caption">Figure 4.73: Lambda function</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The lambda function contains a conditional, and a discount is applied to those records where the original sales price is <strong class="source-inline">&gt;$200</strong>.</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3ddJYwa">https://packt.live/3ddJYwa</a>. </p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3d63D0Y">https://packt.live/3d63D0Y</a>.</p>
			<p>After going through this exercise, we know how to apply a function to a column in a DataFrame. This method is very useful for going beyond the basic functions that are present in <strong class="source-inline">pandas</strong>. </p>
			<h2 id="_idParaDest-150"><a id="_idTextAnchor155"/>Activity 4.01: Working with the Adult Income Dataset (UCI)</h2>
			<p>In this activi<a id="_idTextAnchor156"/>ty, we will detect outliers in the Adult Income Dataset from the UCI machine learning portal <a href="https://packt.live/2N9lRUU">https://packt.live/2N9lRUU</a>.</p>
			<p>You can find a description of the dataset <a href="https://packt.live/2N9lRUU">https://packt.live/2N9lRUU</a>. We will use the concepts we've learned throughout this chapter, such as subsetting, applying user-defined functions, summary statistics, visualizations, boolean indexing, and group by to find a whole group of outliers in a dataset. We will create a bar plot to plot this group of outliers. Finally, we will merge two datasets by using a common key.  </p>
			<p>These are the steps that will help you solve this activity:</p>
			<ol>
				<li value="1">Load the necessary libraries.</li>
				<li>Read the adult income dataset from the following URL: <a href="https://packt.live/2N9lRUU">https://packt.live/2N9lRUU</a>.</li>
				<li>Create a script that will read a text file line by line.</li>
				<li>Add a name of <strong class="source-inline">Income</strong> for the response variable to the dataset.</li>
				<li>Find the missing values.</li>
				<li>Create a DataFrame with only age, education, and occupation by using subsetting.</li>
				<li>Plot a histogram of age with a bin size of <strong class="source-inline">20</strong>.</li>
				<li>Create a function to strip the whitespace characters.</li>
				<li>Use the <strong class="source-inline">apply</strong> method to apply this function to all the columns with string values, create a new column, copy the values from this new column to the old column, and drop the new column.</li>
				<li>Find the number of people who are aged between <strong class="source-inline">30</strong> and <strong class="source-inline">50</strong>.</li>
				<li>Group the records based on age and education to find how the mean age is distributed.</li>
				<li>Group by occupation and show the summary statistics of age. Find which profession has the oldest workers on average and which profession has its largest share of the workforce above the 75th percentile.</li>
				<li>Use <strong class="source-inline">subset</strong> and <strong class="source-inline">groupBy</strong> to find the outliers.</li>
				<li>Plot the outlier values on a bar chart. It should look something like this:<div id="_idContainer160" class="IMG---Figure"><img src="Images/B15780_04_74.jpg" alt="Figure 4.74: Bar plot displaying the outliers&#13;&#10;" width="1465" height="686"/></div><p class="figure-caption">Figure 4.74: Bar plot displaying the outliers</p></li>
				<li>Merge the two DataFrames using common keys to drop duplicate values.<p>The output should look like this:</p><div id="_idContainer161" class="IMG---Figure"><img src="Images/B15780_04_75.jpg" alt="Figure 4.75: Merged DataFrame&#13;&#10;" width="846" height="267"/></div></li>
			</ol>
			<p class="figure-caption">Figure 4.75: Merged DataFrame</p>
			<p class="callout-heading">Note </p>
			<p class="callout">The solution for this activity can be found via <a href="B15780_Solution_Final_RK.xhtml#_idTextAnchor314">this link</a>.</p>
			<p>As you can see, we now have a single DataFrame because we have merged two DataFrames into one.</p>
			<p>With that, we conclude this activity and the chapter.</p>
			<h1 id="_idParaDest-151"><a id="_idTextAnchor157"/>Summary</h1>
			<p>In this chapter, we deep-dived into the <strong class="source-inline">pandas</strong> library to learn advanced data wrangling techniques. We started with some advanced subsetting and filtering on DataFrames and rounded this off by learning about boolean indexing and conditionally selecting a subset of data. We also covered how to set and reset the index of a DataFrame, especially while initializing.</p>
			<p>Next, we learned about a particular topic that has a deep connection with traditional relational database systems – the <strong class="source-inline">groupBy</strong> method. Then, we deep-dived into an important skill for data wrangling – checking for and handling missing data. We showed you how pandas helps in handling missing data using various imputation techniques. We also discussed methods for dropping missing values. Furthermore, methods and usage examples of concatenation and merging DataFrame objects were shown. We saw the <strong class="source-inline">join</strong> method and how it compares to a similar operation in SQL.</p>
			<p>Lastly, miscellaneous useful methods on DataFrames, such as randomized sampling, <strong class="source-inline">unique</strong>, <strong class="source-inline">value_count</strong>, <strong class="source-inline">sort_values</strong>, and pivot table functionality were covered. We also showed an example of running an arbitrary user-defined function on a DataFrame using the <strong class="source-inline">apply</strong> method.</p>
			<p>After learning about the basic and advanced data wrangling techniques with the <strong class="source-inline">numpy</strong> and <strong class="source-inline">pandas</strong> libraries, the natural question of data acquisition rises. In the next chapter, we will show you how to work with a wide variety of data sources; that is, you will learn how to read data in tabular format in <strong class="source-inline">pandas</strong> from different sources.</p>
		</div>
		<div>
			<div id="_idContainer163" class="Content">
			</div>
		</div>
	</div></body></html>