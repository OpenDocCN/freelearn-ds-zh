<html><head></head><body><div><div><div></div>
		</div>
		<div><h1 id="_idParaDest-120"><a id="_idTextAnchor124"/>4. A Deep Dive into Data Wrangling with Python</h1>
		</div>
		<div><p class="callout-heading"><a id="_idTextAnchor125"/>Overview</p>
			<p class="callout">This chapter will cover pandas DataFrames in depth, thus teaching you how to perform subsetting, filtering, and grouping on DataFrames. You will be able to apply Boolean filtering and indexing to a DataFrame to choose specific elements from it. Later on in the chapter, you will learn how to perform JOIN operations in pandas that are analogous to the SQL command. By the end of this chapter you will be able to apply imputation techniques to identify missing or corrupted data and choose to drop it.</p>
			<h1 id="_idParaDest-121"><a id="_idTextAnchor126"/>Introduction</h1>
			<p>In the previous chapter, we learned how to use the pa<code>ndas</code>, <code>numpy</code>, and <code>matplotlib</code> libraries while handling various datatypes. In this chapter, we will learn about several advanced operations involving <code>pandas</code> DataFrames and <code>numpy</code> arrays. We will be working with several powerful DataFrame operations, including subsetting, filtering grouping, checking uniqueness, and even dealing with missing data, among others. These techniques are extremely useful when working with data in any way. When we want to look at a portion of the data, we must subset, filter, or group the data. <code>Pandas</code> contains the functionality to create descriptive statistics of the dataset. These methods will allow us to start shaping our perception of the data. Ideally, when we have a dataset, we want it to be complete, but in reality, there is often missing or corrupt data. This can happen for a variety of reasons that we can't control, such as user error and sensor malfunction. Pandas has built-in functionalities to deal with such kinds of missing data within our dataset.</p>
			<h1 id="_idParaDest-122"><a id="_idTextAnchor127"/>Subsetting, Filtering, and Grouping</h1>
			<p>One of the most important aspects of data wrangling is to curate the data carefully from the deluge of streaming data that pours into an organization or business entity from various sources. Lots of data is not always a good thing; rather, data needs to be useful and of high quality to be effectively used in downstream activities of a data science pipeline, such as machine learning and predictive model building. Moreover, one data source can be used for multiple purposes, and this often requires different subsets of data to be processed by a data wrangling module. This is then passed on to separate analytics modules.</p>
			<p>For example, let's say you are doing data wrangling on US state-level economic output. It is a fairly common scenario that one machine learning model may require data for large and populous states (such as California and Texas), while another model demands processed data for small and sparsely populated states (such as Montana or North Dakota). As the frontline of the data science process, it is the responsibility of the data wrangling module to satisfy the requirements of both these machine learning models. Therefore, as a data wrangling engineer, you have to filter and group data accordingly (based on the population of the state) before processing them and producing separate datasets as the final output for separate machine learning models.</p>
			<p>Also, in some cases, data sources may be biased, or the measurement may corrupt the incoming data occasionally. It is a good idea to try to filter only the error-free, good data for downstream modeling. From these examples and discussions, it is clear that filtering and grouping/bucketing data is an essential skill to have for any engineer that's engaged in the task of data wrangling. Let's proceed to learn about a few of these skills with pandas.</p>
			<h2 id="_idParaDest-123"><a id="_idTextAnchor128"/>Exercise 4.01: Examining the Superstore Sales Data in an Excel File</h2>
			<p>In this exercise, we will read and examine an Excel file called <code>Sample-Superstore.xls</code> and will check all the columns to check if they are useful for analysis. We'll use the <code>drop</code> method to delete the columns that are unnecessary from the <code>.xls</code> file. Then, we'll use the <code>shape</code> function to check the number of rows and columns in the dataset. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">The <code>superstore</code> dataset file can be found here: <a href="https://packt.live/3dcVnMs">https://packt.live/3dcVnMs</a>.</p>
			<p>To do so, perform the following steps: </p>
			<ol>
				<li>To read an Excel file into <code>pandas</code>, you will need a small package called <code>xlrd</code> to be installed on your system. Use the following code to install the <code>xlrd</code> package:<pre>!pip install xlrd</pre><p class="callout-heading">Note</p><p class="callout">The <code>!</code> notation tells the Jupyter Notebook that the cell should be treated as a shell command. </p></li>
				<li>Read the Excel file from GitHub into a <code>pandas</code> DataFrame using the <code>read_excel</code> method in <code>pandas</code>:<pre>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
df = pd.read_excel("<code>Row ID</code>, is not very useful because we already have a row index on the far left. This is a common occurrence in <code>pandas</code> and can be resolved in a few ways, most importantly by removing the <code>rowid</code> column.</p></li>
				<li>Drop this column altogether from the DataFrame by using the <code>drop</code> method:<pre>df.drop('Row ID',axis=1,inplace=True)
df.head()</pre><p>The output is as follows:</p><div><img src="img/B15780_04_02.jpg" alt="Figure 4.2: Partial output of the Superstore dataset after dropping the 'Row ID' column&#13;&#10;" width="1186" height="631"/></div><p class="figure-caption">Figure 4.2: Partial output of the Superstore dataset after dropping the 'Row ID' column</p></li>
				<li>Check the number of rows and columns in the newly created dataset. We will use the <code>shape</code> function here:<pre>df.shape</pre><p>The output is as follows:</p><pre>(9994, 20)</pre></li>
			</ol>
			<p>In this exercise, we can see that the dataset has <code>9,994</code> rows and <code>20</code> columns. We have now seen that a simple way to remove unwanted columns such as a row count is simple with <code>pandas</code>. Think about how hard this would be if, instead of <code>pandas</code>, we used a list of dictionaries? We would have to write a loop to remove the <code>rowid</code> element from each dictionary in the list. <code>pandas</code> makes this functionality simple and easy.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2Y9ZTXW">https://packt.live/2Y9ZTXW</a>. </p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2N4dVUO">https://packt.live/2N4dVUO</a>.</p>
			<p>In the next section, we'll discuss how to subset a DataFrame.</p>
			<h2 id="_idParaDest-124"><a id="_idTextAnchor129"/>Subsetting the DataFrame</h2>
			<p><code>Customer ID</code>, <code>Customer Name</code>, <code>City</code>, <code>Postal Code</code>, and <code>Sales</code>. For demonstration purposes, let's assume that we are only interested in <code>5</code> records – rows <code>5-9</code>. We can subset the DataFrame to extract only this much information using a single line of Python code.</p>
			<p>We can use the <code>loc</code> method to index the <code>Sample Superstore</code> dataset by the names of the columns and the indexes of the rows, as shown in the following code:</p>
			<pre>df_subset = df.loc[
    <strong class="bold">[i for i in range(5,10)],</strong>
    ['Customer ID','Customer Name','City','Postal Code','Sales']]
df_subset</pre>
			<p>The output is as follows:</p>
			<div><div><img src="img/B15780_04_03.jpg" alt="Figure 4.3: Partial data of the DataFrame indexed by the names of the columns&#13;&#10;" width="1192" height="395"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.3: Partial data of the DataFrame indexed by the names of the columns</p>
			<p>We need to pass on two arguments to the <code>loc</code> method – one for indicating the rows, and another for indicating the columns. When passing more than one value, you must pass them as a list for a row or column.</p>
			<p>For the rows, we have to pass a list, that is, <code>[5,6,7,8,9]</code>, but instead of writing that explicitly, we use a list comprehension, that is, <code>[i for i in range(5,10)]</code>.</p>
			<p>Because the columns we are interested in are not continuous and we cannot just put in a continuous range, we need to pass on a list containing the specific names. So, the second argument is just a simple list with specific column names. The dataset shows the fundamental concepts of the process of <strong class="bold">subsetting</strong> a DataFrame based on business requirements.</p>
			<p>Let's look at an example use case and practice subsetting a bit more.</p>
			<h2 id="_idParaDest-125"><a id="_idTextAnchor130"/>An Example Use Case – Determining Statistics on Sales and Profit</h2>
			<p>Let's take a look at a typical use case of subsetting. Suppose we want to calculate descriptive statistics (mean, median, standard deviation, and so on) of records <code>100-199</code> for sales and profit in the <code>SuperStore</code> dataset. The following code shows how subsetting helps us achieve that:</p>
			<pre>df_subset = df.loc[[i for i in range(100,199)],['Sales','Profit']]
df_subset.describe()</pre>
			<p>The output is as follows:</p>
			<div><div><img src="img/B15780_04_04.jpg" alt="Figure 4.4: Output of descriptive statistics of data&#13;&#10;" width="1134" height="581"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.4: Output of descriptive statistics of data</p>
			<p>We simply extract records <code>100-199</code> and run the <code>describe</code> function on them because we don't want to process all the data. For this particular business question, we are only interested in sales and profit numbers, and therefore we should not take the easy route and run a <code>describe</code> function on all the data. For a dataset that's being used in machine learning analysis, the number of rows and columns could often be in the millions, and we don't want to compute anything that is not asked for in the data wrangling task. We always aim to subset the exact data that needs to be processed and run statistical or plotting functions on that partial data. One of the most intuitive ways to try and understand the data is through charting. This can be a critical component of data wrangling. </p>
			<p>To better understand sales and profit, let's create a box plot of the data using <code>matplotlib</code>:</p>
			<pre>import matplotlib as plt
boxplot = df_subset.boxplot()</pre>
			<p>The output is as follows:</p>
			<div><div><img src="img/B15780_04_05.jpg" alt="Figure 4.5: Box plot of sales and profit&#13;&#10;" width="927" height="437"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.5: Box plot of sales and profit</p>
			<p>As we can see from the preceding box plot, there are some outliers for profit. Now, they could be normal outliers, or they could be <code>NaN</code> values. At this point, we can't speculate, but this could cause some further analysis to see how we want to treat those outliers in profit. In some cases, outliers are fine, but for some predictive modeling techniques such as regression, outliers can have unwanted effects. </p>
			<p>Before continuing further with filtering methods, let's take a quick detour and explore a super useful function called <code>unique</code>. As its name suggests, this function is used to scan through the data quickly and extract only the unique values in a column or row.</p>
			<h2 id="_idParaDest-126"><a id="_idTextAnchor131"/>Exercise 4.02: The unique Function</h2>
			<p>In the superstore sales data, you will notice that there are columns such as <code>Country</code>, <code>State</code>, and <code>City</code>. A natural question will be to ask how many <code>countries/states/cities</code> are present in the dataset. In this exercise, we'll use the <code>unique</code> function to find the number of unique <code>countries/states/cities</code> in the dataset. Let's go through the following steps:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The <code>superstore</code> dataset file can be found here: <a href="https://packt.live/3dcVnMs">https://packt.live/3dcVnMs</a>.</p>
			<ol>
				<li value="1">Import the necessary libraries and read the file from GitHub by using the <code>read_excel</code> method in <code>pandas</code> into a DataFrame:<pre>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
df = pd.read_excel("<strong class="bold">../datasets/Sample - Superstore.xls</strong>")</pre><p class="callout-heading">Note</p><p class="callout">The highlighted path must be changed based on the location of the file on your system. </p></li>
				<li>Extract <code>countries/states/cities</code> for which the information is in the database, with one simple line of code, as follows:<pre>df['State'].unique()</pre><p>The output is as follows:</p><div><img src="img/B15780_04_06.jpg" alt="Figure 4.6: Different states present in the dataset&#13;&#10;" width="865" height="283"/></div><p class="figure-caption">Figure 4.6: Different states present in the dataset</p><p>You will see a list of all the states whose data is present in the dataset.</p></li>
				<li>Use the <code>nunique</code> method to count the number of unique values in the <code>State</code> column, like so:<pre>df['State'].nunique()</pre><p>The output is as follows:</p><pre>49</pre><p>This returns <code>49</code> for this dataset. So, one out of <code>50</code> states in the US does not appear in this dataset. Therefore, we can conclude that there's one repetition in the <code>State</code> column.</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2NaBkUB">https://packt.live/2NaBkUB</a>. </p><p class="callout">You can also run this example online at <a href="https://packt.live/2N7NHkf">https://packt.live/2N7NHkf</a>.</p></li>
			</ol>
			<p>Similarly, if we run this function on the <code>Country</code> column, we get an array with only one element, <code>United States</code>. Immediately, we can see that we don't need to keep the country column at all because there is no useful information in that column, except that all the entries are the same. This is how a simple function helped us to decide about dropping a column altogether – that is, removing <code>9,994</code> pieces of unnecessary data.</p>
			<h2 id="_idParaDest-127"><a id="_idTextAnchor132"/>Conditional Selection and Boolean Filtering</h2>
			<p>Often, we don't want to process the whole dataset and would like to select only a partial dataset whose contents satisfy a particular condition. This is probably the most common use case of any data wrangling task. In the context of our <code>superstore sales</code> dataset, think of these common questions that may arise from the daily activities of the business analytics team:</p>
			<ul>
				<li>What are the average sales and profit figures in California?</li>
				<li>Which states have the highest and lowest total sales?</li>
				<li>What consumer segment has the most variance in sales/profit?</li>
				<li>Among the top five states in sales, which shipping mode and product category are the most popular choices?</li>
			</ul>
			<p>Countless examples can be given where the business analytics team or the executive management wants to glean insight from a particular subset of data that meets certain criteria.</p>
			<p>If you have any prior experience with SQL, you will know that these kinds of questions require fairly complex SQL query writing. Remember the <code>WHERE</code> clause?</p>
			<p>We will show you how to use conditional subsetting and boolean filtering to answer such questions.</p>
			<p>First, we need to understand the critical concept of boolean indexing. This process essentially accepts a conditional expression as an argument and returns a dataset of booleans in which the <code>TRUE</code> value appears in places where the condition was satisfied. A simple example is shown in the following code. For demonstration purposes, we're subsetting a small dataset of <code>10</code> records and <code>3</code> columns:</p>
			<pre>df_subset = df.loc[[i for i in range (10)],\
                   ['Ship Mode','State','Sales']]
df_subset</pre>
			<p>The output is as follows:</p>
			<div><div><img src="img/B15780_04_07.jpg" alt="Figure 4.7: Sample dataset&#13;&#10;" width="1052" height="578"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.7: Sample dataset</p>
			<p>Now, if we just want to know the records with sales higher than <code>$100</code>, then we can write the following:</p>
			<pre>df_subset['Sales'] &gt; 100</pre>
			<p>This produces the following <code>boolean</code> DataFrame:</p>
			<div><div><img src="img/B15780_04_08.jpg" alt="Figure 4.8 Records with sales higher than $100&#13;&#10;" width="1026" height="586"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.8 Records with sales higher than $100</p>
			<p>Let's take a look at the <code>True</code> and <code>False</code> entries in the <code>Sales</code> column. The values in the <code>Ship Mode</code> and <code>State</code> columns were not impacted by this code because the comparison was with a numerical quantity, and the only numeric column in the original DataFrame was <code>Sales</code>.</p>
			<p>Now, let's see what happens if we pass this <code>boolean</code> DataFrame as an index to the original DataFrame:</p>
			<pre>df_subset[df_subset['Sales']&gt;100]</pre>
			<p>The output is as follows:</p>
			<div><div><img src="img/B15780_04_09.jpg" alt="Figure 4.9: Results after passing the boolean DataFrame as an index &#13;&#10;to the original DataFrame&#13;&#10;" width="1288" height="399"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.9: Results after passing the boolean DataFrame as an index to the original DataFrame</p>
			<p>We are not limited to conditional expressions involving numeric quantities only. Let's try to extract high sales values (<code>&gt;$100</code>) for entries that do not involve <code>California</code>.</p>
			<p>We can write the following code to accomplish this:</p>
			<pre>df_subset[(df_subset['State']!='California') \
          &amp; (df_subset['Sales']&gt;100)]</pre>
			<p>Note the use of a conditional involving string. In this expression, we are joining two conditionals by an <code>&amp;</code> operator. Both conditions must be wrapped inside parentheses.</p>
			<p>The first conditional expression simply matches the entries in the <code>State</code> column to the <code>California</code> string and assigns <code>TRUE</code>/<code>FALSE</code> accordingly. The second conditional is the same as before. Together, joined by the <code>&amp;</code> operator, they extract only those rows for which <code>State</code> is <em class="italic">not</em> <code>California</code> and <code>Sales</code> is <code>&gt; $100</code>. We get the following result:</p>
			<div><div><img src="img/B15780_04_10.jpg" alt="Figure 4.10: Results, where State is not California and Sales, is higher than $100&#13;&#10;" width="1368" height="327"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.10: Results, where State is not California and Sales, is higher than $100</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Although, in theory, there is no limit to how complex a conditional you can build using individual expressions and the <code>&amp;</code> (<code>LOGICAL AND</code>) and <code>|</code> (<code>LOGICAL OR</code>) operators, it is advisable to create intermediate boolean DataFrames with limited conditional expressions and build your final DataFrame step by step. This keeps the code legible and scalable.</p>
			<p>In the following exercise, we'll look at a few different methods we can use to manipulate the DataFrame. </p>
			<h2 id="_idParaDest-128"><a id="_idTextAnchor133"/>Exercise 4.03: Setting and Resetting the Index</h2>
			<p>In this exercise, we will create a pandas DataFrame and set and reset the index. We'll also add a new column and set it as the new index of this DataFrame. To do so, let's go through the following steps:</p>
			<ol>
				<li value="1">Import the <code>numpy</code> library:<pre>import numpy as np</pre></li>
				<li>Create the <code>matrix_data</code>, <code>row_labels</code>, and <code>column_headings</code> functions using the following commands:<pre>matrix_data = np.matrix('22,66,140;42,70,148;\
                        30,62,125;35,68,160;25,62,152')
row_labels = ['A','B','C','D','E']
column_headings = ['Age', 'Height', 'Weight']</pre></li>
				<li>Import the <code>pandas</code> library and then create a DataFrame using the <code>matrix_data</code>, <code>row_labels</code>, and <code>column_headings</code> functions:<pre>import pandas as pd
df1 = pd.DataFrame(data=matrix_data,\
                   index=row_labels,\
                   columns=column_headings)
print("\nThe DataFrame\n",'-'*25, sep='')
df1</pre><p>The output is as follows:</p><div><img src="img/B15780_04_11.jpg" alt="Figure 4.11: The original DataFrame&#13;&#10;" width="1053" height="442"/></div><p class="figure-caption">Figure 4.11: The original DataFrame</p></li>
				<li>Reset the index, as follows:<pre>print("\nAfter resetting index\n",'-'*35, sep='')
df1.reset_index()</pre><p>The output is as follows:</p><div><img src="img/B15780_04_12.jpg" alt="Figure 4.12: DataFrame after resetting the index&#13;&#10;" width="1315" height="538"/></div><p class="figure-caption">Figure 4.12: DataFrame after resetting the index</p></li>
				<li>Reset the index with <code>drop</code> set to <code>True</code>, as follows:<pre>print("\nAfter resetting index with 'drop' option TRUE\n",\
      '-'*45, sep='')
df1.reset_index(drop=True)</pre><p>The output is as follows:</p><div><img src="img/B15780_04_13.jpg" alt="Figure 4.13: DataFrame after resetting the index with the drop option set to true&#13;&#10;" width="1248" height="547"/></div><p class="figure-caption">Figure 4.13: DataFrame after resetting the index with the drop option set to true</p></li>
				<li>Add a new column using the following command:<pre>print("\nAdding a new column 'Profession'\n",\
      '-'*45, sep='')
df1['Profession'] = "Student Teacher Engineer Doctor Nurse"\
                    .split()
df1</pre><p>The output is as follows:</p><div><img src="img/B15780_04_14.jpg" alt="Figure 4.14: DataFrame after adding a new column called Profession&#13;&#10;" width="1444" height="538"/></div><p class="figure-caption">Figure 4.14: DataFrame after adding a new column called Profession</p></li>
				<li>Now, set the <code>Profession</code> column as an <code>index</code> using the following code:<pre>print("\nSetting 'Profession' column as index\n",\
      '-'*45, sep='')
df1.set_index('Profession')</pre><p>The output is as follows:</p><div><img src="img/B15780_04_15.jpg" alt="Figure 4.15: DataFrame after setting the Profession column as an index&#13;&#10;" width="1427" height="611"/></div></li>
			</ol>
			<p class="figure-caption">Figure 4.15: DataFrame after setting the Profession column as an index</p>
			<p>As we can see, the new data was added at the end of the table.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/30QknH2">https://packt.live/30QknH2</a>. </p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/37CdM4o">https://packt.live/37CdM4o</a>.</p>
			<h2 id="_idParaDest-129"><a id="_idTextAnchor134"/>The GroupBy Method</h2>
			<p><strong class="bold">GroupBy</strong> refers to a process involving one or more of the following steps:</p>
			<ul>
				<li>Splitting the data into groups based on some criteria</li>
				<li>Applying a function to each group independently</li>
				<li>Combining the results into a data structure</li>
			</ul>
			<p>In many situations, we can split the dataset into groups and do something with those groups. In the apply step, we may wish to do one of the following:</p>
			<ul>
				<li><strong class="bold">Aggregation</strong>: Compute a summary statistic (or statistics) for each group – sum, mean, and so on</li>
				<li><strong class="bold">Transformation</strong>: Perform a group-specific computation and return a like-indexed object – z-transformation or filling missing data with a value</li>
				<li><code>TRUE</code> or <code>FALSE</code></li>
			</ul>
			<p>There is, of course, a describe method for this <code>GroupBy</code> object, which produces the summary statistics in the form of a DataFrame.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The name GroupBy should be quite familiar to those who have used a SQL-based tool before.</p>
			<p>GroupBy is not limited to a single variable. If you pass on multiple variables (as a list), then you will get a structure essentially similar to a Pivot Table (from Excel). The following exercise shows an example of where we group together all the states and cities from the whole dataset (the snapshot is only a partial view).</p>
			<h2 id="_idParaDest-130"><a id="_idTextAnchor135"/>Exercise 4.04: The GroupBy Method</h2>
			<p>In this exercise, we're going to create a subset from a dataset. We will use the <code>groupBy</code> object to filter the dataset and calculate the mean of that filtered dataset. To do so, let's go through the following steps:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The <code>superstore</code> dataset file can be found here: <a href="https://packt.live/3dcVnMs">https://packt.live/3dcVnMs</a>.</p>
			<ol>
				<li value="1">Import the necessary Python modules and read the Excel file from GitHub by using the <code>read_excel</code> method in <code>pandas</code>:<pre>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
df = pd.read_excel("<strong class="bold">../datasets/Sample - Superstore.xls</strong>")
df.head()</pre><p>The output (partially shown) is as follows:</p><div><img src="img/B15780_04_16.jpg" alt="Figure 4.16: Partial output of the DataFrame&#13;&#10;" width="1074" height="738"/></div><p class="figure-caption">Figure 4.16: Partial output of the DataFrame</p><p class="callout-heading">Note</p><p class="callout">The highlighted path must be changed based on the location of the file on your system. </p></li>
				<li>Create a 10-record subset using the following command:<pre>df_subset = df.loc[[i for i in range (10)],\
                   ['Ship Mode','State','Sales']]
df_subset</pre><p>The output will be as follows:</p><div><img src="img/B15780_04_17.jpg" alt="Figure 4.17: 10-Record Subset&#13;&#10;" width="897" height="557"/></div><p class="figure-caption">Figure 4.17: 10-Record Subset</p></li>
				<li>Create a <code>pandas</code> DataFrame using the <code>groupby</code> method, as follows:<pre>byState = df_subset.groupby('State')
byState</pre><p>The output will be similar to:</p><pre>&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x00000202FB931B08&gt;</pre></li>
				<li>Calculate the mean sales figure by <code>State</code> by using the following command:<pre>print("\nGrouping by 'State' column and listing mean sales\n",\
      '-'*50, sep='')
byState.mean()</pre><p>The output is as follows:</p><div><img src="img/B15780_04_18.jpg" alt="Figure 4.18: Output after grouping the state with the listing mean sales&#13;&#10;" width="978" height="377"/></div><p class="figure-caption">Figure 4.18: Output after grouping the state with the listing mean sales</p></li>
				<li>Calculate the total sales figure by <code>State</code> by using the following command:<pre>print("\nGrouping by 'State' column and listing total "\
      "sum of sales\n", '-'*50, sep='')
byState.sum()</pre><p>The output is as follows:</p><div><img src="img/B15780_04_19.jpg" alt="Figure 4.19: The output after grouping the state with the listing sum of sales&#13;&#10;" width="1029" height="388"/></div><p class="figure-caption">Figure 4.19: The output after grouping the state with the listing sum of sales</p></li>
				<li>Subset that DataFrame for a particular state and show the statistics:<pre>pd.DataFrame(byState.describe().loc['California'])</pre><p>The output is as follows:</p><div><img src="img/B15780_04_20.jpg" alt="Figure 4.20: Checking the statistics of a particular state&#13;&#10;" width="875" height="457"/></div><p class="figure-caption">Figure 4.20: Checking the statistics of a particular state</p></li>
				<li>Perform a similar summarization by using the <code>Ship Mode</code> attribute:<pre>df_subset.groupby('Ship Mode').describe()\
.loc[['Second Class','Standard Class']]</pre><p>The output will be as follows:</p><div><img src="img/B15780_04_21.jpg" alt="Figure 4.21: Checking the sales by summarizing the Ship Mode attribute&#13;&#10;" width="1059" height="257"/></div><p class="figure-caption">Figure 4.21: Checking the sales by summarizing the Ship Mode attribute</p></li>
				<li>Display the complete summary statistics of sales by every city in each state – all with two lines of code – by using the following command:<pre>byStateCity=df.groupby(['State','City'])
byStateCity.describe()['Sales']</pre><p>The output (partially shown) is as follows:</p><div><img src="img/B15780_04_22.jpg" alt="Figure 4.22: Partial output while checking the summary statistics of sales&#13;&#10;" width="1269" height="406"/></div></li>
			</ol>
			<p class="figure-caption">Figure 4.22: Partial output while checking the summary statistics of sales</p>
			<p>Note how <code>pandas</code> has grouped the data by <code>State</code> first and then by cities under each state.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2Cm9eUl">https://packt.live/2Cm9eUl</a>. </p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3fxK43c">https://packt.live/3fxK43c</a>.</p>
			<p>We now understand how to use <code>pandas</code> to group our dataset and then find aggregate values such as the mean sales return of our top employees. We also looked at how pandas will display descriptive statistics about our data for us. Both of these techniques can be used to perform analysis on our superstore data.</p>
			<h1 id="_idParaDest-131"><a id="_idTextAnchor136"/>Detecting Outliers and Handling Missing Values</h1>
			<p>Outlier detection and handling missing values fall under the subtle art of data quality checking. A modeling or data mining process is fundamentally a complex series of computations whose output quality largely depends on the quality and consistency of the input data being fed. The responsibility of maintaining and gatekeeping that quality often falls on the shoulders of a data wrangling team.</p>
			<p>Apart from the obvious issue of poor-quality data, missing data can sometimes wreak havoc with the <strong class="bold">Machine Learning</strong> (<strong class="bold">ML</strong>) model downstream. A few ML models, such as Bayesian learning, are inherently robust to outliers and missing data, but common techniques such as Decision Trees and Random Forest have an issue with missing data because the fundamental splitting strategy employed by these techniques depends on an individual piece of data and not a cluster. Therefore, it is almost always imperative to impute missing data before handing it over to such an ML model.</p>
			<p>Outlier detection is a subtle art. Often, there is no universally agreed definition of an outlier. In a statistical sense, a data point that falls outside a certain range may often be classified as an outlier, but to apply that definition, you need to have a fairly high degree of certainty about the assumption of the nature and parameters of the inherent statistical distribution about the data. It takes a lot of data to build that statistical certainty and even after that, an outlier may not be just unimportant noise but a clue to something deeper. Let's look at an example with some fictitious sales data from an American fast-food chain restaurant. If we want to model the daily sales data as a time series, we will observe an unusual spike in the data somewhere around mid-April:</p>
			<div><div><img src="img/B15780_04_23.jpg" alt="Figure 4.23: Fictitious sales data of an American fast-food chain restaurant&#13;&#10;" width="1405" height="556"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.23: Fictitious sales data of an American fast-food chain restaurant</p>
			<p>A good data scientist or data wrangler should develop curiosity about this data point rather than just rejecting it just because it falls outside the statistical range. In the actual anecdote, the sales figure spiked that day because of an unusual reason. So, the data was real. But just because it was real does not mean it is useful. In the final goal of building a smoothly varying time series model, this one point should not matter and should be rejected. In this chapter, however, we're going to look at ways of handling outliers instead of rejecting them.</p>
			<p>Therefore, the key to outliers is their systematic and timely detection in an incoming stream of millions of data or while reading data from cloud-based storage. In this section, we will quickly go over some basic statistical tests for detecting outliers and some basic imputation techniques for filling up missing data.</p>
			<h2 id="_idParaDest-132"><a id="_idTextAnchor137"/>Missing Values in Pandas</h2>
			<p>One of the most useful functions for detecting missing values is <code>isnull</code>. We'll use this function on a DataFrame called <code>df_missing</code> (based on the Superstore DataFrame we are working with), which, as the name suggests, will contain some missing values. You can create this DataFrame using the following command:</p>
			<pre>df_missing=pd.read_excel("<strong class="bold">../datasets/Sample - Superstore.xls</strong>",\
                         sheet_name="Missing")
df_missing</pre>
			<p class="callout-heading">Note</p>
			<p class="callout">Don't forget to change the path (highlighted) based on the location of the file on your system. </p>
			<p>The output will be as follows:</p>
			<div><div><img src="img/B15780_04_24.jpg" alt="Figure 4.24: DataFrame with missing values&#13;&#10;" width="1665" height="1212"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.24: DataFrame with missing values</p>
			<p>We can see that the missing values are denoted by <code>NaN</code>. Now let's use the <code>isnull</code> function on the same DataFrame and observe the results:</p>
			<pre>df_missing.isnull()</pre>
			<p>The output is as follows:</p>
			<p> </p>
			<div><div><img src="img/B15780_04_25.jpg" alt="Figure 4.25: Output highlighting the missing values&#13;&#10;" width="938" height="634"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.25: Output highlighting the missing values</p>
			<p>As you can see, the missing values are indicated by the Boolean value <code>True</code>. Now, let's see how we can use the <code>isnull</code> function to deliver results that are a bit more user friendly. Here is an example of some very simple code to detect, count, and print out missing values in every column of a DataFrame:</p>
			<pre>for c in df_missing.columns:
    miss = df_missing[c].isnull().sum()
    if miss&gt;0:
        print("{} has {} missing value(s)".format(c,miss))
    else:
        print("{} has NO missing value!".format(c))</pre>
			<p>This code scans every column of the DataFrame, calls the <code>isnull</code> function, and sums up the returned object (a <code>pandas</code> Series object, in this case) to count the number of missing values. If the missing value is greater than zero, it prints out the message accordingly. The output looks as follows:</p>
			<p> </p>
			<div><div><img src="img/B15780_04_26.jpg" alt="Figure 4.26: Output of counting the missing values&#13;&#10;" width="1461" height="328"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.26: Output of counting the missing values</p>
			<p>As we can see from the preceding output, the missing values were detected from the <code>Superstore</code> dataset. </p>
			<p>To handle missing values, you should look for ways not to drop them altogether but to fill them somehow. The <code>fillna</code> method is a useful function for performing this task on <code>pandas</code> DataFrames. The <code>fillna</code> method may work for string data, but not for numerical columns such as sales or profits. So, we should restrict ourselves in regard to this fixed string replacement being used on non-numeric text-based columns only. The <code>Pad</code> or <code>ffill</code> function is used to fill forward the data, that is, copy it from the preceding data of the series. Forward fill is a technique where the missing value is filled with the previous value. On the other hand, backward fill or <code>bfill</code> uses the next value to fill in any missing data. Let's practice this with the following exercise.</p>
			<h2 id="_idParaDest-133"><a id="_idTextAnchor138"/>Exercise 4.05: Filling in the Missing Values Using the fillna Method</h2>
			<p>In this exercise, we are going to perform four techniques in order to deal with the missing values in a dataset. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">The <code>superstore</code> dataset file can be found here: <a href="https://packt.live/3dcVnMs">https://packt.live/3dcVnMs</a>.</p>
			<p>Firstly, we are going to replace the missing values with static values using the <code>fillna</code> method. Then, we will use the <code>ffill</code> and <code>bfill</code> methods to replace the missing values. Lastly, we will calculate the average of a column and replace the missing value with that. To do so, let's go through the following steps:</p>
			<ol>
				<li value="1">Import the necessary Python modules and read the Excel file from GitHub by using the <code>read_excel</code> method in <code>pandas</code>:<pre>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
df_missing = pd.read_excel("<strong class="bold">../datasets/Sample - Superstore.xls</strong>",\
                           sheet_name="Missing")
df_missing.head()</pre><p class="callout-heading">Note</p><p class="callout">The highlighted path must be changed based on the location of the file on your system. </p><p>The output is as follows:</p><div><img src="img/B15780_04_27.jpg" alt="Figure 4.27: Snapshot of the dataset&#13;&#10;" width="893" height="317"/></div><p class="figure-caption">Figure 4.27: Snapshot of the dataset</p></li>
				<li>Fill in all the missing values with the <code>FILL</code> string by using the following command:<pre>df_missing.fillna('FILL')</pre><p>The output is as follows:</p><div><img src="img/B15780_04_28.jpg" alt="Figure 4.28: Missing values replaced with FILL&#13;&#10;" width="1026" height="706"/></div><p class="figure-caption">Figure 4.28: Missing values replaced with FILL</p></li>
				<li>Fill in the specified columns with the <code>FILL</code> string by using the following command:<pre>df_missing[['Customer','Product']].fillna('FILL')</pre><p>The output is as follows:</p><div><img src="img/B15780_04_29.jpg" alt="Figure 4.29: Specified columns replaced with FILL&#13;&#10;" width="1143" height="706"/></div><p class="figure-caption">Figure 4.29: Specified columns replaced with FILL</p><p class="callout-heading">Note</p><p class="callout">In all of these cases, the function works on a copy of the original DataFrame. So, if you want to make the changes permanent, you have to assign the DataFrames that are returned by these functions to the original DataFrame object.</p></li>
				<li>Fill in the values using <code>ffill</code> or forward fill by using the following command on the <code>Sales</code> column:<pre>df_missing['Sales'].fillna(method='ffill')</pre><p>The output is as follows:</p><div><img src="img/B15780_04_30.jpg" alt="Figure 4.30: Sales column using the forward fill&#13;&#10;" width="763" height="301"/></div><p class="figure-caption">Figure 4.30: Sales column using the forward fill</p></li>
				<li>Use <code>bfill</code> to fill backward, that is, copy from the next data in the series:<pre>df_missing['Sales'].fillna(method='bfill')</pre><p>The output is as follows:</p><div><img src="img/B15780_04_31.jpg" alt="Figure 4.31: Sales column using the backward fill&#13;&#10;" width="675" height="305"/></div><p class="figure-caption">Figure 4.31: Sales column using the backward fill</p><p>Let's compare these two series and see what happened in each case:</p><div><img src="img/B15780_04_32.jpg" alt="Figure 4.32: Using forward fill and backward fill to fill in missing data&#13;&#10;" width="1065" height="306"/></div><p class="figure-caption">Figure 4.32: Using forward fill and backward fill to fill in missing data</p><p>You can also fill by using a function average of DataFrames. For example, we may want to fill the missing values in <code>Sales</code> by the average sales amount. </p></li>
				<li>Fill the missing values in <code>Sales</code> by the average sales amount:<pre>df_missing['Sales'].fillna(df_missing.mean()['Sales'])</pre><p>The output is as follows:</p><div><img src="img/B15780_04_33.jpg" alt="Figure 4.33: Sales column with average sales amount&#13;&#10;" width="713" height="313"/></div></li>
			</ol>
			<p class="figure-caption">Figure 4.33: Sales column with average sales amount</p>
			<p>The following screenshot shows what happened in the preceding code:</p>
			<div><div><img src="img/B15780_04_34.jpg" alt="Figure 4.34: Using average to fill in missing data&#13;&#10;" width="1011" height="405"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.34: Using average to fill in missing data</p>
			<p>Here, we can observe that the missing value in the cell was filled by the average sales amount.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2ACDYjp">https://packt.live/2ACDYjp</a>. </p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2YNZnhh">https://packt.live/2YNZnhh</a>.</p>
			<p>With this, we have now seen how to replace missing values within a <code>pandas</code> DataFrame using four methods: static value, forward fill, backward fill, and the average. These are the fundamental techniques when cleaning data with missing values.</p>
			<h2 id="_idParaDest-134"><a id="_idTextAnchor139"/>The dropna Method</h2>
			<p>This function is used to simply drop the rows or columns that contain <code>NaN</code> or missing values. However, there is some choice involved.</p>
			<p>The following is the syntax of the <code>dropna()</code> method:</p>
			<pre>DataFrameName.dropna(axis=0, how='any', \
                     thresh=None, subset=None, \
                     inplace=False)</pre>
			<p>If the <code>axis</code> parameter of a <code>dropna()</code> method is set to <code>0</code>, then rows containing missing values are dropped; if the axis parameter is set to <code>1</code>, then columns containing missing values are dropped. These are useful if we don't want to drop a particular row/column if the <code>NaN</code> values do not exceed a certain percentage.</p>
			<p>Two arguments that are useful for the <code>dropna()</code> method are as follows:</p>
			<ul>
				<li>The <code>how</code> argument determines if a row or column is removed from a DataFrame when we have at least one <code>NaN</code> value or all <code>NaN</code> values.</li>
				<li>The <code>thresh</code> argument requires that many non-<code>NaN</code> values to keep the row/ column.</li>
			</ul>
			<p>We'll practice using the <code>dropna()</code> method in the following exercise.</p>
			<h2 id="_idParaDest-135"><a id="_idTextAnchor140"/>Exercise 4.06: Dropping Missing Values with dropna</h2>
			<p>In this exercise, we will remove the cells in a dataset that don't contain data. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">The <code>superstore</code> dataset file can be found here: <a href="https://packt.live/3dcVnMs">https://packt.live/3dcVnMs</a>.</p>
			<p>We are going to use the <code>dropna</code> method in order to remove missing cells in a dataset. To do so, let's go through the following steps:</p>
			<ol>
				<li value="1">Import the necessary Python libraries and read the Excel file from GitHub by using the <code>read_excel</code> method in <code>pandas</code>:<pre>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
df_missing = pd.read_excel("<strong class="bold">../datasets/Sample - Superstore.xls",\</strong>
<strong class="bold">                           sheet_name="Missing</strong>")
df_missing.head()</pre><p class="callout-heading">Note</p><p class="callout">The highlighted path must be changed based on the location of the file on your system. </p><p>The output is as follows:</p><div><img src="img/B15780_04_35.jpg" alt="Figure 4.35: Superstore dataset&#13;&#10;" width="1349" height="401"/></div><p class="figure-caption">Figure 4.35: Superstore dataset</p><p class="callout-heading">Note</p><p class="callout">The outputs you get will vary from the ones shown in this exercise.</p></li>
				<li>To set the <code>axis</code> parameter to <code>zero</code> and drop all missing rows, use the following command:<pre>df_missing.dropna(axis=0)</pre><p>The output is as follows:</p><div><img src="img/B15780_04_36.jpg" alt="Figure 4.36: Dropping all the missing rows&#13;&#10;" width="1408" height="485"/></div><p class="figure-caption">Figure 4.36: Dropping all the missing rows</p></li>
				<li>To set the <code>axis</code> parameter to <code>1</code> and drop all missing rows, use the following command:<pre>df_missing.dropna(axis=1)</pre><p>The output is as follows:</p><div><img src="img/B15780_04_37.jpg" alt="Figure 4.37: Dropping rows or columns to handle missing data&#13;&#10;" width="1034" height="637"/></div><p class="figure-caption">Figure 4.37: Dropping rows or columns to handle missing data</p></li>
				<li>Drop the values with <code>axis</code> set to <code>1</code> and <code>thresh</code> set to <code>10</code>:<pre>df_missing.dropna(axis=1,thresh=10)</pre><p>The output is as follows:</p><div><img src="img/B15780_04_38.jpg" alt="Figure 4.38: DataFrame with values dropped with axis=1 and thresh=10&#13;&#10;" width="1034" height="647"/></div></li>
			</ol>
			<p class="figure-caption">Figure 4.38: DataFrame with values dropped with axis=1 and thresh=10</p>
			<p>As you can see, some <code>NaN</code> values still exist, but because of the minimum threshold, those rows were kept in place.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2Ybvx7t">https://packt.live/2Ybvx7t</a>. </p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/30RNCsY">https://packt.live/30RNCsY</a>.</p>
			<p>In this exercise, we looked at dropping missing values rows and columns. This is a useful technique for a variety of cases, including when working with machine learning. Some machine learning models do not handle missing data well and removing them ahead of time can be best practices. </p>
			<h2 id="_idParaDest-136"><a id="_idTextAnchor141"/>Outlier Detection Using a Simple Statistical Test</h2>
			<p>As we've already discussed, outliers in a dataset can occur due to many factors and in many ways:</p>
			<ul>
				<li>Data entry errors</li>
				<li>Experimental errors (data extraction related)</li>
				<li>Measurement errors due to noise or instrumental failure</li>
				<li>Data processing errors (data manipulation or mutations due to coding errors)</li>
				<li>Sampling errors (extracting or mixing data from wrong or various sources)</li>
			</ul>
			<p>It is impossible to pinpoint one universal method for outlier detection. Here, we will show you some simple tricks for numeric data using standard statistical tests.</p>
			<p>Box plots may show unusual values. We can corrupt two sales values by assigning negatives, as follows:</p>
			<pre>df_sample = df[['Customer Name','State','Sales','Profit']]\
               .sample(n=50).copy()
df_sample['Sales'].iloc[5]=-1000.0
df_sample['Sales'].iloc[15]=-500.0</pre>
			<p>To plot the box plot, use the following code:</p>
			<pre>df_sample.plot.box()
plt.title("Boxplot of sales and profit", fontsize=15)
plt.xticks(fontsize=15)
plt.yticks(fontsize=15)
plt.grid(True)</pre>
			<p>The output (which will vary with each run) is as follows:</p>
			<div><div><img src="img/B15780_04_39.jpg" alt="Figure 4.39: Box plot of sales and profit&#13;&#10;" width="1665" height="687"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.39: Box plot of sales and profit</p>
			<p>We can create simple box plots to check for any unusual/nonsensical values. For example, in the preceding example, we intentionally corrupted two sales values so that they were negative, and they were readily caught in a box plot.</p>
			<p>Note that profit may be negative, so those negative points are generally not suspicious. But sales cannot be negative in general, so they are detected as outliers.</p>
			<p>We can create a distribution of a numerical quantity and check for values that lie at the extreme end to see if they are truly part of the data or outlier. For example, if a distribution is almost normal, then any value more than four or five standard deviations away may be a suspect:</p>
			<div><div><img src="img/B15780_04_40.jpg" alt="Figure 4.40: Value away from the main outliers&#13;&#10;" width="862" height="355"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.40: Value away from the main outliers</p>
			<h1 id="_idParaDest-137"><a id="_idTextAnchor142"/>Concatenating, Merging, and Joining</h1>
			<p>Merging and joining tables or datasets are highly common operations in the day-to-day job of a data wrangling professional. These operations are akin to the <code>JOIN</code> query in SQL for relational database tables. Often, the key data is present in multiple tables, and those records need to be brought into one combined table that matches on that common key. This is an extremely common operation in any type of sales or transactional data, and therefore must be mastered by a data wrangler. The <code>pandas</code> library offers nice and intuitive built-in methods to perform various types of <code>JOIN</code> queries involving multiple DataFrame objects.</p>
			<h2 id="_idParaDest-138"><a id="_idTextAnchor143"/>Exercise 4.07: Concatenation in Datasets</h2>
			<p>In this exercise, we will concatenate DataFrames along various axes (rows or columns).</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The <code>superstore</code> dataset file can be found here: <a href="https://packt.live/3dcVnMs">https://packt.live/3dcVnMs</a>.</p>
			<p>This is a very useful operation as it allows you to grow a DataFrame as the new data comes in or new feature columns need to be inserted into the table. To do so, let's go through the following steps:</p>
			<ol>
				<li value="1">Read the Excel file from GitHub by using the <code>read_excel</code> method in <code>pandas</code>:<pre>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
df = pd.read_excel("<strong class="bold">../datasets/Sample - Superstore.xls</strong>")
df.head()</pre><p class="callout-heading">Note</p><p class="callout">The highlighted path must be changed based on the location of the file on your system. </p><p>The output (partially shown) will be as follows:</p><div><img src="img/B15780_04_41.jpg" alt="Figure 4.41: Partial output of the DataFrame&#13;&#10;" width="1266" height="842"/></div><p class="figure-caption">Figure 4.41: Partial output of the DataFrame</p></li>
				<li>Sample <code>4</code> records each to create three DataFrames at random from the original sales dataset we are working with:<pre>df_1 = df[['Customer Name','State',\
           'Sales','Profit']].sample(n=4)
df_2 = df[['Customer Name','State',\
           'Sales','Profit']].sample(n=4)
df_3 = df[['Customer Name','State',\
           'Sales','Profit']].sample(n=4)</pre></li>
				<li>Create a combined DataFrame with all the rows concatenated by using the following code:<pre>df_cat1 = pd.concat([df_1,df_2,df_3], axis=0)
df_cat1</pre><p>The output (partially shown) is as follows:</p><div><img src="img/B15780_04_42.jpg" alt="Figure 4.42: Partial output after concatenating the DataFrames&#13;&#10;" width="885" height="579"/></div><p class="figure-caption">Figure 4.42: Partial output after concatenating the DataFrames</p><p>As you can see, concatenation will vertically combine multiple DataFrames. You can also try concatenating along the columns, although that does not make any practical sense for this particular example. However, <code>pandas</code> fills in the unavailable values with <code>NaN</code> for that operation.</p><p class="callout-heading">Note</p><p class="callout">The outputs you get will vary from the ones shown in this exercise.</p></li>
				<li>Create a combined DataFrame with all the columns concatenated by using the following code:<pre>df_cat2 = pd.concat([df_1,df_2,df_3], axis=1)
df_cat2</pre><p>The output (partially shown) is as follows:</p><div><img src="img/B15780_04_43.jpg" alt="Figure 4.43: Partial output after concatenating the DataFrames&#13;&#10;" width="984" height="731"/></div></li>
			</ol>
			<p class="figure-caption">Figure 4.43: Partial output after concatenating the DataFrames</p>
			<p>As we can observe, the cells in the dataset that do not contain any values are replaced with <code>NaN</code> values.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3epn5aB">https://packt.live/3epn5aB</a>. </p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3edUPrh">https://packt.live/3edUPrh</a>.</p>
			<h2 id="_idParaDest-139"><a id="_idTextAnchor144"/>Merging by a Common Key</h2>
			<p>Merging by a common key is an extremely common operation for data tables as it allows you to rationalize multiple sources of data in one master database – that is, if they have some common features/keys.</p>
			<p>When joining and merging two DataFrames, we use two separate types: <strong class="bold">inner</strong> and <strong class="bold">outer {left|right}</strong>. Let's take a look at them:</p>
			<ul>
				<li><strong class="bold">Inner</strong>: A combining method that uses a column or key to be compared on each dataset. Rows that share the same column or key will be present after the join.</li>
				<li><strong class="bold">Outer</strong>: A way to combine datasets such as inner, but all data on the right or left (depending on which is chosen) is kept, and matching data from the opposite side is combined.</li>
			</ul>
			<p>This is often the first step in building a large database for machine learning tasks where daily incoming data may be put into separate tables. However, at the end of the day, the most recent table needs to be merged with the master data table so that it can be fed into the backend machine learning server, which will then update the model and its prediction capacity. Merge is a way to combine DataFrames vertically, using a column to compare on. The functionality of merge and join are very similar; their capabilities are the same.</p>
			<h2 id="_idParaDest-140"><a id="_idTextAnchor145"/>Exercise 4.08: Merging by a Common Key</h2>
			<p>In this exercise, we'll create two DataFrames with the <code>Customer Name</code> common key from the Superstore dataset. Then, we will use the inner and outer joins to merge or combine these DataFrames. To do so, let's go through the following steps:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The Superstore file can be found here: <a href="https://packt.live/3dcVnMs">https://packt.live/3dcVnMs</a>.</p>
			<ol>
				<li value="1">Import the necessary Python libraries and read the Excel file from GitHub by using the <code>read_excel</code> method in <code>pandas</code>:<pre>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
df = pd.read_excel("<code>df_1</code>, will have shipping information associated with the customer name, and another table, <code>df_2</code>, will have the product information tabulated.</p></li>
				<li>Create the <code>df1</code> DataFrame with the <code>Customer Name</code> common key:<pre>df_1=df[['Ship Date','Ship Mode','Customer Name']][0:4]
df_1</pre><p>The output of the first DataFrame is as follows:</p><div><img src="img/B15780_04_45.jpg" alt="Figure 4.45: Entries in table df_1&#13;&#10;" width="1216" height="346"/></div><p class="figure-caption">Figure 4.45: Entries in table df_1</p></li>
				<li>Create the second DataFrame, <code>df2</code>, with the <code>Customer Name</code> common key, as follows:<pre>df_2=df[['Customer Name','Product Name','Quantity']][0:4]
df_2</pre><p>The output is as follows:</p><div><img src="img/B15780_04_46.jpg" alt="Figure 4.46: Entries in table df_2&#13;&#10;" width="1282" height="348"/></div><p class="figure-caption">Figure 4.46: Entries in table df_2</p></li>
				<li>Join these two tables with an inner join by using the following command:<pre>pd.merge(df_1,df_2,on='Customer Name',how='inner')</pre><p>The output is as follows:</p><div><img src="img/B15780_04_47.jpg" alt="Figure 4.47: Inner join on table df_1 and table df_2&#13;&#10;" width="1247" height="560"/></div><p class="figure-caption">Figure 4.47: Inner join on table df_1 and table df_2</p></li>
				<li>Drop the duplicates by using the following command:<pre>pd.merge(df_1,df_2,on='Customer Name',\
         how='inner').drop_duplicates()</pre><p>The output is as follows:</p><div><img src="img/B15780_04_48.jpg" alt="Figure 4.48: Inner join on table df_1 and table df_2 after dropping the duplicates&#13;&#10;" width="1265" height="429"/></div><p class="figure-caption">Figure 4.48: Inner join on table df_1 and table df_2 after dropping the duplicates</p></li>
				<li>Extract another small table called <code>df_3</code> to show the concept of an outer join:<pre>df_3=df[['Customer Name','Product Name','Quantity']][2:6]
df_3</pre><p>The output is as follows:</p><div><img src="img/B15780_04_49.jpg" alt="Figure 4.49: Creating table df_3&#13;&#10;" width="1209" height="360"/></div><p class="figure-caption">Figure 4.49: Creating table df_3</p></li>
				<li>Perform an inner join on <code>df_1</code> and <code>df_3</code> by using the following command:<pre>pd.merge(df_1,df_3,on='Customer Name',\
         how='inner').drop_duplicates()</pre><p>The output is as follows:</p><div><img src="img/B15780_04_50.jpg" alt="Figure 4.50: Merging table df_1 and table df_3 and dropping duplicates&#13;&#10;" width="1564" height="411"/></div><p class="figure-caption">Figure 4.50: Merging table df_1 and table df_3 and dropping duplicates</p></li>
				<li>Perform an outer join on <code>df_1</code> and <code>df_3</code> by using the following command:<pre>pd.merge(df_1,df_3,on='Customer Name',\
         how='outer').drop_duplicates()</pre><p>The output is as follows:</p><div><img src="img/B15780_04_51.jpg" alt="Figure 4.51: Outer join on table df_1 and table df_3 and dropping the duplicates&#13;&#10;" width="1221" height="491"/></div></li>
			</ol>
			<p class="figure-caption">Figure 4.51: Outer join on table df_1 and table df_3 and dropping the duplicates</p>
			<p>Notice how some <code>NaN</code> and <code>NaT</code> values are inserted automatically because no corresponding entries could be found for those records, as those are the entries with unique customer names from their respective tables. <code>NaT</code> represents a <code>Not a Time</code> object, as the objects in the <code>Ship Date</code> column are timestamped objects.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2Y8G5UW">https://packt.live/2Y8G5UW</a>. </p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/30RNUA4">https://packt.live/30RNUA4</a>.</p>
			<p>With this, we have gone over how to use the <code>merge</code> method to do inner and outer joins. </p>
			<h2 id="_idParaDest-141"><a id="_idTextAnchor146"/>The join Method</h2>
			<p>Joining is performed based on <strong class="bold">index keys</strong> and is done by combining the columns of two potentially differently indexed DataFrames into a single one. It offers a faster way to accomplish merging by row indices. This is useful if the records in different tables are indexed differently but represent the same inherent data and you want to merge them into a single table:</p>
			<h2 id="_idParaDest-142"><a id="_idTextAnchor147"/>Exercise 4.09: The join Method</h2>
			<p>In this exercise, we will create two DataFrames and perform the different kind of joins on these DataFrames.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The Superstore file can be found here: <a href="https://packt.live/3dcVnMs">https://packt.live/3dcVnMs</a>.</p>
			<p>To complete this exercise, perform the following steps:</p>
			<ol>
				<li value="1">Import the Python libraries and load the file from GitHub by using the <code>read_excel</code> method in <code>pandas</code>:<pre>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
df = pd.read_excel("<strong class="bold">../datasets/Sample - Superstore.xls</strong>")
df.head()</pre><p class="callout-heading">Note</p><p class="callout">The highlighted path must be changed based on the location of the file on your system. </p><p>The partial output of the code is as follows:</p><div><img src="img/B15780_04_52.jpg" alt="Figure 4.52: Partial output of the DataFrame&#13;&#10;" width="1255" height="889"/></div><p class="figure-caption">Figure 4.52: Partial output of the DataFrame</p></li>
				<li>Create <code>df1</code> with <code>Customer Name</code> as the index by using the following command:<pre>df_1=df[['Customer Name','Ship Date','Ship Mode']][0:4]
df_1.set_index(['Customer Name'],inplace=True)
df_1</pre><p>The output is as follows:</p><div><img src="img/B15780_04_53.jpg" alt="Figure 4.53: DataFrame df_1&#13;&#10;" width="1523" height="472"/></div><p class="figure-caption">Figure 4.53: DataFrame df_1</p></li>
				<li>Create <code>df2</code> with <code>Customer Name</code> as the index by using the following command:<pre>df_2=df[['Customer Name','Product Name','Quantity']][2:6]
df_2.set_index(['Customer Name'],inplace=True) 
df_2</pre><p>The output is as follows:</p><div><img src="img/B15780_04_54.jpg" alt="Figure 4.54: DataFrame df_2&#13;&#10;" width="1389" height="471"/></div><p> </p><p class="figure-caption">Figure 4.54: DataFrame df_2</p></li>
				<li>Perform a left join on <code>df_1</code> and <code>df_2</code> by using the following command:<pre>df_1.join(df_2,how='left').drop_duplicates()</pre><p>The output is as follows:</p><div><img src="img/B15780_04_55.jpg" alt="Figure 4.55: Left join on table df_1 and table df_2 after dropping the duplicates&#13;&#10;" width="1568" height="419"/></div><p class="figure-caption">Figure 4.55: Left join on table df_1 and table df_2 after dropping the duplicates</p></li>
				<li>Perform a right join on <code>df_1</code> and <code>df_2</code> by using the following command:<pre>df_1.join(df_2,how='right').drop_duplicates()</pre><p>The output is as follows:</p><div><img src="img/B15780_04_56.jpg" alt="Figure 4.56: Right join on table df_1 and table df_2 after dropping the duplicates&#13;&#10;" width="1240" height="467"/></div><p class="figure-caption">Figure 4.56: Right join on table df_1 and table df_2 after dropping the duplicates</p></li>
				<li>Perform an inner join on <code>df_1</code> and <code>df_2</code> by using the following command:<pre>df_1.join(df_2,how='inner').drop_duplicates()</pre><p>The output is as follows:</p><div><img src="img/B15780_04_57.jpg" alt="Figure 4.57: Inner join on table df_1 and table df_2 after dropping the duplicates&#13;&#10;" width="1235" height="291"/></div><p class="figure-caption">Figure 4.57: Inner join on table df_1 and table df_2 after dropping the duplicates</p></li>
				<li>Perform an outer join on <code>df_1</code> and <code>df_2</code> by using the following command:<pre>df_1.join(df_2,how='outer').drop_duplicates()</pre><p>The output is as follows:</p><div><img src="img/B15780_04_58.jpg" alt="Figure 4.58: Outer join on table df_1 and table df_2 after dropping the duplicates&#13;&#10;" width="1250" height="520"/></div></li>
			</ol>
			<p class="figure-caption">Figure 4.58: Outer join on table df_1 and table df_2 after dropping the duplicates</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/30S9nZH">https://packt.live/30S9nZH</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2NbDweg">https://packt.live/2NbDweg</a>.</p>
			<p>We have now gone through the basic functionality of <code>pandas</code> DataFrame joining. We used inner and out joining and showed you how we can use indexes to perform a join and how it can help in analysis. </p>
			<h1 id="_idParaDest-143"><a id="_idTextAnchor148"/>Useful Methods of Pandas</h1>
			<p>In this section, we will discuss some small utility functions that are offered by <code>pandas</code> so that we can work efficiently with DataFrames. They don't fall under any particular group of functions, so they are mentioned here under the Miscellaneous category. Let's discuss these miscellaneous methods in detail.</p>
			<h2 id="_idParaDest-144"><a id="_idTextAnchor149"/>Randomized Sampling</h2>
			<p>In this section, we will discuss random sampling data from our DataFrames. This is a very common task in a variety of pipelines, one of which is machine learning. Sampling is often used in machine learning data-wrangling pipelines when choosing which data to train and which data to test against. Sampling a random fraction of a big DataFrame is often very useful so that we can practice other methods on them and test our ideas. If you have a database table of 1 million records, then it is not computationally effective to run your test scripts on the full table.</p>
			<p>However, you may also not want to extract only the first 100 elements as the data may have been sorted by a particular key and you may get an uninteresting table back, which may not represent the full statistical diversity of the parent database.</p>
			<p>In these situations, the <code>sample</code> method comes in super handy so that we can randomly choose a controlled fraction of the DataFrame.</p>
			<h2 id="_idParaDest-145"><a id="_idTextAnchor150"/>Exercise 4.10: Randomized Sampling</h2>
			<p>In this exercise, we are going to randomly take five samples from the Superstore dataset and calculate a definite fraction of the data to be sampled. To do so, let's go through the following steps:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The Superstore file can be found here: <a href="https://packt.live/3dcVnMs">https://packt.live/3dcVnMs</a>.</p>
			<ol>
				<li value="1">Import the necessary Python modules and read them from GitHub by using the <code>read_excel</code> method in <code>pandas</code>:<pre>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
df = pd.read_excel("<strong class="bold">../datasets/Sample - Superstore.xls</strong>")
df.head()</pre><p class="callout-heading">Note</p><p class="callout">The highlighted path must be changed based on the location of the file on your system. </p><p>The partial output will be:</p><div><img src="img/B15780_04_59.jpg" alt="Figure 4.59: Partial output of the DataFrame&#13;&#10;" width="1253" height="885"/></div><p class="figure-caption">Figure 4.59: Partial output of the DataFrame</p></li>
				<li>Specify the number of samples that we require from the DataFrame by using the following command:<pre>df.sample(n=5)</pre><p>The random output (partially shown) is as follows:</p><div><img src="img/B15780_04_60.jpg" alt="Figure 4.60: DataFrame with five samples&#13;&#10;" width="1128" height="476"/></div><p class="figure-caption">Figure 4.60: DataFrame with five samples</p><p class="callout-heading">Note</p><p class="callout">The outputs you get will vary from the ones shown in this exercise.</p></li>
				<li>Specify a definite fraction (percentage) of the data to be sampled by using the following command:<pre>df.sample(frac=0.1)</pre><p>The output is as follows:</p><div><img src="img/B15780_04_61.jpg" alt="Figure 4.61: Partial output of a DataFrame with 0.1% data sampled&#13;&#10;" width="803" height="673"/></div><p class="figure-caption">Figure 4.61: Partial output of a DataFrame with 0.1% data sampled</p><p>You can also choose if sampling is done with replacement, that is, whether the same record can be chosen more than once. The default <code>replace</code> choice is <code>FALSE</code>, that is, no repetition and sampling will try to choose new elements only.</p></li>
				<li>Choose the sampling by using the following command:<pre>df.sample(frac=0.1, replace=True)</pre><p>The output is as follows:</p><div><img src="img/B15780_04_62.jpg" alt="Figure 4.62: DataFrame with 0.1% data sampled and repetition enabled&#13;&#10;" width="849" height="617"/></div></li>
			</ol>
			<p class="figure-caption">Figure 4.62: DataFrame with 0.1% data sampled and repetition enabled</p>
			<p>Here, as you can see, we have encouraged repetitions in the sampled data by setting the <code>replace</code> parameter to <code>True</code>. Therefore, the same elements could be chosen again while performing random sampling.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2N7fWzt">https://packt.live/2N7fWzt</a>. </p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2YLTt0f">https://packt.live/2YLTt0f</a>.</p>
			<h2 id="_idParaDest-146"><a id="_idTextAnchor151"/>The value_counts Method</h2>
			<p>We discussed the <code>unique</code> method previously, which finds and counts the unique records from a DataFrame. Another useful function in a similar vein is <code>value_counts</code>. This function returns an object containing counts of unique values. In the object that is returned, the first element is the most frequently used object. The elements are arranged in descending order.</p>
			<p>Let's consider a practical application of this method to illustrate its utility. Suppose your manager asks you to list the top 10 customers from the big sales database that you have. So, the business question is: which 10 customers' names occur the most frequently in the sales table? You can achieve this with a SQL query if the data is in an RDBMS, but in pandas, this can be done by using one simple function:</p>
			<pre>df['Customer Name'].value_counts()[:10]</pre>
			<p>The output is as follows:</p>
			<p> </p>
			<div><div><img src="img/B15780_04_63.jpg" alt="Figure 4.63: List of top 10 customers&#13;&#10;" width="1253" height="478"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.63: List of top 10 customers</p>
			<p>The <code>value_counts</code> method returns a series of counts of all unique customer names sorted by the frequency of the count. By asking for only the first 10 elements of that list, this code returns a series of the most frequently occurring top 10 customer names.</p>
			<h2 id="_idParaDest-147"><a id="_idTextAnchor152"/>Pivot Table Functionality</h2>
			<p>Similar to group by, pandas also offer pivot table functionality, which works the same as a Pivot Table in spreadsheet programs such as MS Excel. For example, in this sales database, you want to know the average sales, profit, and quantity sold by Region and State (two levels of index).</p>
			<p>We can extract this information by using one simple piece of code (we sample 100 records first to keep the computation fast and then apply the code):</p>
			<pre>df_sample = df.sample(n=100)
df_sample.pivot_table(values=['Sales','Quantity','Profit'],\
                      index=['Region','State'],aggfunc='mean')</pre>
			<p>The output is as follows (note that your specific output may be different due to random sampling):</p>
			<p> </p>
			<div><div><img src="img/B15780_04_64.jpg" alt="Figure 4.64: Sample of 100 records&#13;&#10;" width="980" height="607"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.64: Sample of 100 records</p>
			<p>Sorting a table by a particular column is one of the most frequently used operations in the daily work of an analyst. Sorting can help you understand your data better while presenting it in a specific view of the data. When training a machine learning model, the way data is sorted can impact the performance of a model based on the sampling that's being done. Not surprisingly, <code>pandas</code> provide a simple and intuitive method for sorting called the <code>sort_values</code> method. We'll practice using this in the following exercise. </p>
			<h2 id="_idParaDest-148"><a id="_idTextAnchor153"/>Exercise 4.11: Sorting by Column Values – the sort_values Method</h2>
			<p>In this exercise, we will take a random sample of <code>15</code> records from the Superstore dataset. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">The Superstore file can be found here: <a href="https://packt.live/3dcVnMs">https://packt.live/3dcVnMs</a>.</p>
			<p>We will sort the column values in the dataset with respect to column names using the <code>sort_values</code> method. To do so, let's go through the following steps: </p>
			<ol>
				<li value="1">Import the necessary Python modules and read the Excel file from GitHub by using the <code>read_excel</code> method in <code>pandas</code>:<pre>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
df = pd.read_excel("<strong class="bold">../datasets/Sample - Superstore.xls</strong>")
df.head()</pre><p class="callout-heading">Note</p><p class="callout">The highlighted path must be changed based on the location of the file on your system. </p><p>The output (partially shown) will be as follows:</p><div><img src="img/B15780_04_65.jpg" alt="Figure 4.65: Partial output of the DataFrame&#13;&#10;" width="1064" height="743"/></div><p class="figure-caption">Figure 4.65: Partial output of the DataFrame</p></li>
				<li>Take a random sample of <code>15</code> records and then sort by the <code>Sales</code> column and then by both the <code>Sales</code> and <code>State</code> columns together:<pre>df_sample=df[['Customer Name','State',\
              'Sales','Quantity']].sample(n=15)
df_sample</pre><p>The output is as follows:</p><div><img src="img/B15780_04_66.jpg" alt="Figure 4.66: Sample of 15 records&#13;&#10;" width="754" height="636"/></div><p class="figure-caption">Figure 4.66: Sample of 15 records</p><p class="callout-heading">Note</p><p class="callout">The outputs you get will vary from the ones shown in this exercise.</p></li>
				<li>Sort the values with respect to <code>Sales</code> by using the following command:<pre>df_sample.sort_values(by='Sales')</pre><p>The output is as follows:</p><p> </p><div><img src="img/B15780_04_67.jpg" alt="Figure 4.67: DataFrame with the Sales value sorted&#13;&#10;" width="778" height="642"/></div><p class="figure-caption">Figure 4.67: DataFrame with the Sales value sorted</p></li>
				<li>Sort the values with respect to <code>Sales</code> and <code>State</code>:<pre>df_sample.sort_values(by=['State','Sales'])</pre><p>The output is as follows:</p><p> </p><div><img src="img/B15780_04_68.jpg" alt="Figure 4.68: DataFrame sorted with respect to Sales and State&#13;&#10;" width="740" height="644"/></div></li>
			</ol>
			<p class="figure-caption">Figure 4.68: DataFrame sorted with respect to Sales and State</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3dcWNXi">https://packt.live/3dcWNXi</a>. </p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/30UqwSn">https://packt.live/30UqwSn</a>.</p>
			<p>The <code>pandas</code> library provides great flexibility for working with user-defined functions of arbitrary complexity through the <code>apply</code> method. Much like the native Python <code>apply</code> function, this method accepts a user-defined function and additional arguments and returns a new column after applying the function on a particular column elementwise.</p>
			<p>As an example, suppose we want to create a column of categorical features such as high/medium/low based on the sales price column. Note that this is a conversion from a numeric value into a categorical factor (string) based on certain conditions (threshold values of sales).</p>
			<h2 id="_idParaDest-149"><a id="_idTextAnchor154"/>Exercise 4.12: Flexibility of User-Defined Functions with the apply Method</h2>
			<p>In this exercise, we will create a user-defined function called <code>categorize_sales</code> that categorizes Sales data based on price. If the <code>price</code> is less than <code>50</code>, it is classified as <code>Low</code>, if the <code>price</code> is less than <code>200</code>, it is classified as <code>Medium</code>, or <code>High</code> if the <code>price</code> doesn't fall under either of these categories. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">The Superstore file can be found here: <a href="https://packt.live/3dcVnMs">https://packt.live/3dcVnMs</a>.</p>
			<p>We'll then take 100 random samples from the <code>superstore</code> dataset and use the <code>apply</code> method on the <code>categorize_sales</code> function in order to create a new column to store the values returned by the function. To do so, perform the following steps: </p>
			<ol>
				<li value="1">Import the necessary Python modules and read the Excel file from GitHub by using the <code>read_excel</code> method in <code>pandas</code>:<pre>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
df = pd.read_excel("<strong class="bold">../datasets/Sample - Superstore.xls</strong>")
df.head()</pre><p class="callout-heading">Note</p><p class="callout">The highlighted path must be changed based on the location of the file on your system. </p><p>The output (partially shown) will be:</p><div><img src="img/B15780_04_69.jpg" alt="Figure 4.69: Partial output of the DataFrame&#13;&#10;" width="1053" height="737"/></div><p class="figure-caption">Figure 4.69: Partial output of the DataFrame</p></li>
				<li>Create a user-defined function, as follows:<pre>def categorize_sales(price):
    if price &lt; 50:
        return "Low"
    elif price &lt; 200:
        return "Medium"
    else:
        return "High"</pre></li>
				<li>Sample <code>100</code> records randomly from the database:<pre>df_sample=df[['Customer Name',\
              'State','Sales']].sample(n=100)
df_sample.head(10)</pre><p>The output is as follows:</p><p> </p><div><img src="img/B15780_04_70.jpg" alt="Figure 4.70: 100 sample records from the database&#13;&#10;" width="945" height="562"/></div><p class="figure-caption">Figure 4.70: 100 sample records from the database</p><p class="callout-heading">Note</p><p class="callout">The outputs you get will vary from the ones shown in this exercise.</p></li>
				<li>Use the <code>apply</code> method to apply the categorization function to the <code>Sales</code> column. We need to create a new column to store the category string values that are returned by the function:<pre>df_sample['Sales Price Category']=df_sample['Sales']\
                                  .apply(categorize_sales)
df_sample.head(10)</pre><p>The output is as follows:</p><p> </p><div><img src="img/B15780_04_71.jpg" alt="Figure 4.71: DataFrame with 10 rows after using the apply function on the Sales column&#13;&#10;" width="977" height="600"/></div><p class="figure-caption">Figure 4.71: DataFrame with 10 rows after using the apply function on the Sales column</p><p>The <code>apply</code> method also works with the built-in native Python functions. </p></li>
				<li>For practice, let's create another column for storing the length of the name of the customer. We can do this using the familiar <code>len</code> function:<pre>df_sample['Customer Name Length']=df_sample['Customer Name']\
                                  .apply(len)
df_sample.head(10)</pre><p>The output is as follows:</p><p> </p><div><img src="img/B15780_04_72.jpg" alt="Figure 4.72: DataFrame with a new column&#13;&#10;" width="1186" height="580"/></div><p class="figure-caption">Figure 4.72: DataFrame with a new column</p><p>Instead of writing out a separate function, we can even insert <em class="italic">lambda expressions</em> directly into the <code>apply</code> method for short functions. For example, let's say we are promoting our product and want to show the discounted sales price if the original price is <em class="italic">&gt; $200</em>. </p></li>
				<li>Use a <code>lambda</code> function and the <code>apply</code> method to do so:<pre>df_sample['Discounted Price']=df_sample['Sales']\
                              .apply(lambda x:0.85*x if x&gt;200 \
                              else x)
df_sample.head(10)</pre><p>The output is as follows:</p><p> </p><div><img src="img/B15780_04_73.jpg" alt="Figure 4.73: Lambda function&#13;&#10;" width="1252" height="501"/></div></li>
			</ol>
			<p class="figure-caption">Figure 4.73: Lambda function</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The lambda function contains a conditional, and a discount is applied to those records where the original sales price is <code>&gt;$200</code>.</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3ddJYwa">https://packt.live/3ddJYwa</a>. </p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3d63D0Y">https://packt.live/3d63D0Y</a>.</p>
			<p>After going through this exercise, we know how to apply a function to a column in a DataFrame. This method is very useful for going beyond the basic functions that are present in <code>pandas</code>. </p>
			<h2 id="_idParaDest-150"><a id="_idTextAnchor155"/>Activity 4.01: Working with the Adult Income Dataset (UCI)</h2>
			<p>In this activi<a id="_idTextAnchor156"/>ty, we will detect outliers in the Adult Income Dataset from the UCI machine learning portal <a href="https://packt.live/2N9lRUU">https://packt.live/2N9lRUU</a>.</p>
			<p>You can find a description of the dataset <a href="https://packt.live/2N9lRUU">https://packt.live/2N9lRUU</a>. We will use the concepts we've learned throughout this chapter, such as subsetting, applying user-defined functions, summary statistics, visualizations, boolean indexing, and group by to find a whole group of outliers in a dataset. We will create a bar plot to plot this group of outliers. Finally, we will merge two datasets by using a common key.  </p>
			<p>These are the steps that will help you solve this activity:</p>
			<ol>
				<li value="1">Load the necessary libraries.</li>
				<li>Read the adult income dataset from the following URL: <a href="https://packt.live/2N9lRUU">https://packt.live/2N9lRUU</a>.</li>
				<li>Create a script that will read a text file line by line.</li>
				<li>Add a name of <code>Income</code> for the response variable to the dataset.</li>
				<li>Find the missing values.</li>
				<li>Create a DataFrame with only age, education, and occupation by using subsetting.</li>
				<li>Plot a histogram of age with a bin size of <code>20</code>.</li>
				<li>Create a function to strip the whitespace characters.</li>
				<li>Use the <code>apply</code> method to apply this function to all the columns with string values, create a new column, copy the values from this new column to the old column, and drop the new column.</li>
				<li>Find the number of people who are aged between <code>30</code> and <code>50</code>.</li>
				<li>Group the records based on age and education to find how the mean age is distributed.</li>
				<li>Group by occupation and show the summary statistics of age. Find which profession has the oldest workers on average and which profession has its largest share of the workforce above the 75th percentile.</li>
				<li>Use <code>subset</code> and <code>groupBy</code> to find the outliers.</li>
				<li>Plot the outlier values on a bar chart. It should look something like this:<div><img src="img/B15780_04_74.jpg" alt="Figure 4.74: Bar plot displaying the outliers&#13;&#10;" width="1465" height="686"/></div><p class="figure-caption">Figure 4.74: Bar plot displaying the outliers</p></li>
				<li>Merge the two DataFrames using common keys to drop duplicate values.<p>The output should look like this:</p><div><img src="img/B15780_04_75.jpg" alt="Figure 4.75: Merged DataFrame&#13;&#10;" width="846" height="267"/></div></li>
			</ol>
			<p class="figure-caption">Figure 4.75: Merged DataFrame</p>
			<p class="callout-heading">Note </p>
			<p class="callout">The solution for this activity can be found via <a href="B15780_Solution_Final_RK.xhtml#_idTextAnchor314">this link</a>.</p>
			<p>As you can see, we now have a single DataFrame because we have merged two DataFrames into one.</p>
			<p>With that, we conclude this activity and the chapter.</p>
			<h1 id="_idParaDest-151"><a id="_idTextAnchor157"/>Summary</h1>
			<p>In this chapter, we deep-dived into the <code>pandas</code> library to learn advanced data wrangling techniques. We started with some advanced subsetting and filtering on DataFrames and rounded this off by learning about boolean indexing and conditionally selecting a subset of data. We also covered how to set and reset the index of a DataFrame, especially while initializing.</p>
			<p>Next, we learned about a particular topic that has a deep connection with traditional relational database systems – the <code>groupBy</code> method. Then, we deep-dived into an important skill for data wrangling – checking for and handling missing data. We showed you how pandas helps in handling missing data using various imputation techniques. We also discussed methods for dropping missing values. Furthermore, methods and usage examples of concatenation and merging DataFrame objects were shown. We saw the <code>join</code> method and how it compares to a similar operation in SQL.</p>
			<p>Lastly, miscellaneous useful methods on DataFrames, such as randomized sampling, <code>unique</code>, <code>value_count</code>, <code>sort_values</code>, and pivot table functionality were covered. We also showed an example of running an arbitrary user-defined function on a DataFrame using the <code>apply</code> method.</p>
			<p>After learning about the basic and advanced data wrangling techniques with the <code>numpy</code> and <code>pandas</code> libraries, the natural question of data acquisition rises. In the next chapter, we will show you how to work with a wide variety of data sources; that is, you will learn how to read data in tabular format in <code>pandas</code> from different sources.</p>
		</div>
		<div><div></div>
		</div>
	</div></body></html>