<html><head></head><body><div class="chapter" title="Chapter&#xA0;6.&#xA0;Sentiment Analysis &#x2013; Categorizing Hotel Reviews"><div class="titlepage"><div><div><h1 class="title"><a id="ch06"/>Chapter 6. Sentiment Analysis – Categorizing Hotel Reviews</h1></div></div></div><p>People talk about a lot of things online. There are forums and communities for almost everything under the sun, and some of them may be about your product or service. People may complain, or they may praise, and you would want to know which of the two they're doing.</p><p>This is where sentiment analysis helps. It can automatically track whether the reviews and discussions are positive or negative overall, and it can pull out items from either category to make them easier to respond to or draw attention to.</p><p>Over the course of this chapter, we'll cover a lot of ground. Some of it will be a little hazy, but in general, here's what we'll cover:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Exploring and preparing the data</li><li class="listitem" style="list-style-type: disc">Understanding the classifiers</li><li class="listitem" style="list-style-type: disc">Running the experiment</li><li class="listitem" style="list-style-type: disc">Examining the error rates</li></ul></div><p>Before we go any further, let's learn what sentiment analysis is.</p><div class="section" title="Understanding sentiment analysis"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec38"/>Understanding sentiment analysis</h1></div></div></div><p>Sentiment analysis<a id="id378" class="indexterm"/> is a form of text categorization that works on opinions instead of topics. Often, texts are categorized according to the subject they discuss. For example, sentiment analysis attempts to categorize texts according to the opinions or emotions of the writers, whether the text is about cars or pets. Often, these are cast in binary terms: good or bad, like or dislike, positive or negative, and so on. Does this person love Toyotas or hate them? Are Pugs the best or German Shepherds? Would they go back to this restaurant? Questions like these have proven to be an important area of research, simply because so many companies want to know what people say about their goods and services online. This provides a way for companies' marketing departments to monitor people's opinions about their products or services as they talk on Twitter and other online public forums. They can reach out to unhappy customers to provide better, more proactive customer service or reach out to satisfied ones to strengthen their relationships and opinions.</p><p>As you can imagine, categorizing based on opinion than on topics is much more difficult. Even basic words tend to take on multiple meanings that are very dependent on their contexts.</p><p>For example, take the <a id="id379" class="indexterm"/>word <span class="emphasis"><em>good</em></span>. In a review, I can say that something is <span class="emphasis"><em>good</em></span>. I can also say that it's not good, no good, or so far from good that It can almost see it on a clear day. On the other hand, I can say that something's <span class="emphasis"><em>bad</em></span>. Or can I say that it's <span class="emphasis"><em>not bad</em></span>. Or, if I'm stuck in the '80s, I can say that "I love it, it's so bad."</p><p>This is a very important and interesting problem, so people have been working on it for a number of years. An early paper on this topic came in 2002,<span class="emphasis"><em> Thumbs up? Sentiment classification using machine learning techniques</em></span>, published by <span class="emphasis"><em>Bo Pang</em></span>, <span class="emphasis"><em>Lillian Lee</em></span>, and <span class="emphasis"><em>Shivakumar Vaithyanathan</em></span>. In this paper, they compared movie reviews using naive Bayes' maximum entropy and support vector machines to categorize movie reviews into positive and negative. They also compared a variety of feature types such as unigrams, bigrams, and other combinations. In general, they found that support vector machines with single tokens performed best, although the difference wasn't usually huge.</p><p>Together and separately, <span class="emphasis"><em>Bo Pang</em></span>, <span class="emphasis"><em>Lillian Lee</em></span>, and many others have extended sentiment analysis in interesting ways. They've attempted to go beyond simple binary classifications toward predicting finer-grained sentiments. For example, they've worked on systems to predict from a document the number of stars the author of the review would give the reviewed service or object on a four-star or five-star rating system.</p><p>Part of what makes this interesting is that the baseline is how well the system explicitly agrees with the judgment of the human raters. However, in research, human raters only agree about 79 percent of the time, so a system that agrees with human raters 60 or 70 percent of the time is doing pretty well.</p></div></div>
<div class="section" title="Getting hotel review data"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec39"/>Getting hotel review data</h1></div></div></div><p>For this chapter, we'll<a id="id380" class="indexterm"/> look at the<a id="id381" class="indexterm"/> <span class="strong"><strong>OpinRank Review</strong></span> dataset (<a class="ulink" href="http://archive.ics.uci.edu/ml/datasets/OpinRank+Review+Dataset">http://archive.ics.uci.edu/ml/datasets/OpinRank+Review+Dataset</a>). This is a dataset that contains almost 260,000 reviews for hotels (<a class="ulink" href="http://tripadvisor.com/">http://tripadvisor.com/</a>) from around the world on <span class="strong"><strong>TripAdvisor</strong></span><a id="id382" class="indexterm"/> as well as more than 42,000 car reviews (<a class="ulink" href="http://edmunds.com/">http://edmunds.com/</a>) from 2007, 2008, and 2009 on <a id="id383" class="indexterm"/>
<span class="strong"><strong>Edmunds</strong></span>.</p></div>
<div class="section" title="Exploring the data"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec40"/>Exploring the data</h1></div></div></div><p>If we look at some <a id="id384" class="indexterm"/>of these reviews, we can see just how difficult categorizing the reviews as positive or negative is, even for humans.</p><p>For instance, some words are used in ways that aren't associated with their straightforward meaning. For example, look at the use of the term <span class="emphasis"><em>greatest</em></span> in the following quote from a review for a Beijing hotel:</p><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>"Not the greatest area but no problems, even at 3:00 AM."</em></span></p></blockquote></div><p>Also, many reviews recount both good and bad aspects of the hotel that they're discussing, even if the final review decidedly comes down one way or the other. This review of a London hotel starts off listing the positives, but then it pivots:</p><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>"… These are the only real positives. Everything else was either average or below average...."</em></span></p></blockquote></div><p>Another reason why reviews are difficult to classify is that many reviews just don't wholeheartedly endorse whatever it is they're reviewing. Instead, the review will be tepid, or the reviewers qualify their conclusions as they did in this review for a Las Vegas hotel:</p><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>"It's faded, but it's fine. If you're on a budget and want to stay on the Strip, this is the place. But for a really great inexpensive experience, try the Main Street Station downtown."</em></span></p></blockquote></div><p>All of these factors contribute toward making this task more difficult than standard document classification problems.</p></div>
<div class="section" title="Preparing the data"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec41"/>Preparing the data</h1></div></div></div><p>For this experiment, I've <a id="id385" class="indexterm"/>randomly selected 500 hotel reviews and classified them manually. A better option might be to use Amazon's <a id="id386" class="indexterm"/>Mechanical Turk (<a class="ulink" href="https://www.mturk.com/mturk/">https://www.mturk.com/mturk/</a>) to get more reviews classified than any one person might be able to do easily. Really, a few hundred is about the minimum that we'd like to use as both the training and test sets need to come from this. I made sure that the sample contained an equal number of positive and negative reviews. (You can find the sample in the <code class="literal">data</code> directory of the code download.)</p><p>The data files are <a id="id387" class="indexterm"/>
<span class="strong"><strong>tab-separated values</strong></span> (<span class="strong"><strong>TSV</strong></span>). After being manually classified, each line had four fields: the classification as a <code class="literal">+</code> or <code class="literal">-</code> sign, the date of the review, the title of the review, and the review itself. Some of the reviews are quite long.</p><p>After <a id="id388" class="indexterm"/>tagging the files, we'll take those files and create feature vectors from the vocabulary of the title and create a review for each one. For this chapter, we'll see what works best: unigrams (single tokens), bigrams, trigrams, or part-of-speech annotated unigrams. These features comprise several common ways to extract features from the text:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Unigrams <a id="id389" class="indexterm"/>are single tokens, for example, features from the preceding sentence</li><li class="listitem" style="list-style-type: disc">Bigrams <a id="id390" class="indexterm"/>are two tokens next to each other, for example, <span class="emphasis"><em>features comprise</em></span></li><li class="listitem" style="list-style-type: disc">Trigrams are three <a id="id391" class="indexterm"/>tokens next to each other, for example, <span class="emphasis"><em>features comprise several</em></span></li><li class="listitem" style="list-style-type: disc">Part-of-speech annotated unigrams <a id="id392" class="indexterm"/>would look something like <code class="literal">features_N</code>, which just means that the unigram features is a noun.</li></ul></div><p>We'll also use these features to train a variety of classifiers on the reviews. Just like <span class="emphasis"><em>Bo Pang</em></span> and <span class="emphasis"><em>Lillian Lee</em></span> did, we'll try experiments with naive Bayes maximum entropy classifiers. To compare how well each of these does, we'll use cross validation to train and test our classifier multiple times.</p><div class="section" title="Tokenizing"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec33"/>Tokenizing</h2></div></div></div><p>Before we get <a id="id393" class="indexterm"/>started on the <a id="id394" class="indexterm"/>code for this chapter, note that the Leiningen 2 <code class="literal">project.clj</code> file looks like the following code:</p><div class="informalexample"><pre class="programlisting">(defproject sentiment "0.1.0-SNAPSHOT"
:plugins [[lein-cljsbuild "0.3.2"]]
:dependencies [[org.clojure/clojure "1.5.1"]
                 [org.clojure/data.csv "0.1.2"]
                 [org.clojure/data.json "0.2.3"]
                 [org.apache.opennlp/opennlp-tools "1.5.3"]
                 [nz.ac.waikato.cms.weka/weka-dev "3.7.7"]]
:jvm-opts ["-Xmx4096m"])</pre></div><p>First, let's create some functions to handle tokenization. Under the cover's, we'll use methods from the<a id="id395" class="indexterm"/> <span class="strong"><strong>OpenNLP</strong></span> library (<a class="ulink" href="http://opennlp.apache.org/">http://opennlp.apache.org/</a>) to process the next methods from the <span class="strong"><strong>Weka machine learning</strong></span> library<a id="id396" class="indexterm"/> (<a class="ulink" href="http://www.cs.waikato.ac.nz/ml/weka/">http://www.cs.waikato.ac.nz/ml/weka/</a>) to perform the sentiment analysis. However, we'll wrap these to provide a more natural, Clojure-like interface.</p><p>Let's start in the <code class="literal">src/sentiment/tokens.clj</code> file, which will begin in the following way:</p><div class="informalexample"><pre class="programlisting">(ns sentiment.tokens
  (:require [clojure.string :as str]
            [clojure.java.io :as io])
  (:import [opennlp.tools.tokenizeSimpleTokenizer]
           [opennlp.tools.postagPOSModelPOSTaggerME]))</pre></div><p>Our tokenizer will use <code class="literal">SimpleTokenizer</code> from the OpenNLP library and normalize all characters to lowercase:</p><div class="informalexample"><pre class="programlisting">(defn tokenize [s]
  (map (memfn toLowerCase)
       (seq
         (.tokenize SimpleTokenizer/INSTANCE s))))</pre></div><p>I've aliased <a id="id397" class="indexterm"/>the <code class="literal">sentiment.tokens</code> <a id="id398" class="indexterm"/>namespace to <code class="literal">t</code> in the REPL. This function is used to break an input string into a sequence of token substrings:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>user=&gt; (t/tokenize "How would this be TOKENIZED?")</strong></span>
<span class="strong"><strong>("how" "would" "this" "be" "tokenized" "?")</strong></span>
</pre></div><p>Next, we'll take the token streams and create feature vectors from them.</p></div><div class="section" title="Creating feature vectors"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec34"/>Creating feature vectors</h2></div></div></div><p>A feature vector <a id="id399" class="indexterm"/>is a vector that summarizes an observation or document. Each vector contains the values associated with each variable or feature. The values may be boolean, indicating the presence or absence with 0 or 1, they may be raw counts, or they may be proportions scaled by the size of the overall document. As much of machine learning is based on linear algebra, vectors and matrices are very convenient data structures.</p><p>In order<a id="id400" class="indexterm"/> to <a id="id401" class="indexterm"/>maintain consistent indexes for each feature, we have to maintain a mapping from feature to indexes. Whenever we encounter a new feature, we need to assign it to a new index.</p><p>For example, the following table traces the steps to create a feature vector based on token frequencies from the phrase <span class="emphasis"><em>the cat in the hat</em></span>.</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Step</p>
</th><th style="text-align: left" valign="bottom">
<p>Feature</p>
</th><th style="text-align: left" valign="bottom">
<p>Index</p>
</th><th style="text-align: left" valign="bottom">
<p>Feature Vector</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>the</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>[1]</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>2</p>
</td><td style="text-align: left" valign="top">
<p>cat</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>[1, 1]</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>3</p>
</td><td style="text-align: left" valign="top">
<p>in</p>
</td><td style="text-align: left" valign="top">
<p>2</p>
</td><td style="text-align: left" valign="top">
<p>[1, 1, 1]</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>4</p>
</td><td style="text-align: left" valign="top">
<p>the</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>[2, 1, 1]</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>5</p>
</td><td style="text-align: left" valign="top">
<p>hat</p>
</td><td style="text-align: left" valign="top">
<p>3</p>
</td><td style="text-align: left" valign="top">
<p>[2, 1, 1, 1]</p>
</td></tr></tbody></table></div><p>So, the final feature vector for <span class="emphasis"><em>the cat in the hat</em></span> would be <code class="literal">[2, 1, 1, 1]</code>. In this case, we're counting the features. In other applications, we might use a bag-of-words approach that only tests the presence of the features. In that case, the feature vector would be <code class="literal">[1, 1, 1, 1]</code>.</p><p>We'll include the code to<a id="id402" class="indexterm"/> do this in the <code class="literal">sentiment.tokens</code> namespace. First, we'll create a function that increments the value of a feature in the feature vector. It looks up the index of the feature in the vector from the feature index (<code class="literal">f-index</code>). If the feature hasn't been seen yet, this function also allocates an index for it:</p><div class="informalexample"><pre class="programlisting">(defn inc-feature [f-index f-vec feature]
  (if-let [i (f-index feature)]
    [f-index, (assoc f-veci (inc (nth f-veci)))]
    (let [i (count f-index)]
      [(assoc f-index feature i), (assoc f-veci 1)])))</pre></div><p>We can use this function to convert a feature sequence into a feature vector. This function initially creates a vector of zeroes for the feature sequence, and then it reduces over the features, updating the feature index and vector as necessary:</p><div class="informalexample"><pre class="programlisting">(defn -&gt;feature-vec [f-index features]
  (reduce #(inc-feature (first %1) (second %1) %2)
          [f-index (vec (repeat (count f-index) 0))]
features))</pre></div><p>Finally, for this task, <a id="id403" class="indexterm"/>we have several functions that we'll look at together. The first function, <code class="literal">accum-features</code>, builds the index and the list of feature vectors. Each time it's called, it takes the sequence of features passed to it and creates a feature vector. It appends this to the collection of feature vectors also passed into it. The next function, <code class="literal">pad-to</code>, makes sure that the feature vector has the same number of elements as the feature index. This makes it slightly easier to work with the feature vectors later on. The final function takes a list of feature vectors and returns the feature index and vectors for this data:</p><div class="informalexample"><pre class="programlisting">(defnaccum-features [state features]
  (let [[index accum] state
        [new-index feature] (-&gt;feature-vec index features)]
    [new-index (conj accum feature)]))

(defn pad-to [f-index f-vec]
(vec (take (count f-index) (concat f-vec (repeat 0)))))

(defn -&gt;features [feature-seq]
  (let [[f-index f-vecs]
        (reduce accum-features [{} []] feature-seq)]
    [f-index (map #(pad-to f-index %) f-vecs)]))</pre></div><p>We can use these functions to build up a matrix of feature vectors from a set of input sentences. Let's see how this works in the first few sentences of an <span class="emphasis"><em>Emily Dickinson</em></span> poem:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>user=&gt; (def f-out</strong></span>
<span class="strong"><strong>         (t/-&gt;features</strong></span>
<span class="strong"><strong>           (map set</strong></span>
<span class="strong"><strong>             (map t/tokenize ["I'm nobody."</strong></span>
<span class="strong"><strong>                              "Who are you?"</strong></span>
<span class="strong"><strong>                              "Are you nobody too?"]))))</strong></span>
<span class="strong"><strong>#'user/f-out</strong></span>
<span class="strong"><strong>user=&gt; (first f-out)</strong></span>
<span class="strong"><strong>{"nobody" 0, "'" 1, "i" 2, "m" 3, "." 4, "too" 9, "are" 5,</strong></span>
<span class="strong"><strong> "who" 6, "you" 7, "?" 8}</strong></span>
<span class="strong"><strong>user=&gt; (print (second f-out))</strong></span>
<span class="strong"><strong>([1 1 111 0 0000] [0 0 000 1 111 0]</strong></span>
<span class="strong"><strong> [1 0 000 1 0 1 11])</strong></span>
</pre></div><p>Notice that after<a id="id404" class="indexterm"/> tokenizing each document, we created a set of <a id="id405" class="indexterm"/>the tokens. This changes the system here to use a bag-of-words approach. We're only looking at the presence or absence of a feature, not its frequency. This does put the tokens out of order, <code class="literal">nobody</code> was evidently the first token indexed, but this doesn't matter.</p><p>Now, by inverting the feature index, we can look up the words in a document from the features that it contains. This allows us to recreate a frequency map for each document as well as to recreate the tokens in each document. In this case, we'll look up the words from the first feature vector, <code class="literal">I'm nobody</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>user=&gt; (def index (map first (sort-by second (first f-out))))</strong></span>
<span class="strong"><strong>#'user/index</strong></span>
<span class="strong"><strong>user=&gt; index</strong></span>
<span class="strong"><strong>("nobody" "'" "i" "m" "." "are" "who" "you" "?" "too")</strong></span>
<span class="strong"><strong>user=&gt; (-&gt;&gt; f-out</strong></span>
<span class="strong"><strong>second</strong></span>
<span class="strong"><strong>first</strong></span>
<span class="strong"><strong>         (map-indexed vector)</strong></span>
<span class="strong"><strong>         (remove #(zero? (second %)))</strong></span>
<span class="strong"><strong>         (map first)</strong></span>
<span class="strong"><strong>         (map #(nth index %)))</strong></span>
<span class="strong"><strong>("nobody" "'" "i" "m" ".")</strong></span>
</pre></div><p>This block of code gets the indexes for each position in the feature vector, removes the features that didn't occur, and then looks up the index in the inverted feature index. This provides us with the <a id="id406" class="indexterm"/>sequence of features that occurred in that<a id="id407" class="indexterm"/> document. Notice that they're out of order. This is to be expected because neither the input sequence of features (in this case a set) nor the feature vector itself preserves the order of the features.</p></div><div class="section" title="Creating feature vector functions and POS tagging"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec35"/>Creating feature vector functions and POS tagging</h2></div></div></div><p>We'll also include<a id="id408" class="indexterm"/> some functions to turn a list of <a id="id409" class="indexterm"/>tokens into a list of features. By wrapping these <a id="id410" class="indexterm"/>into functions, we make it easier to compose pipelines of<a id="id411" class="indexterm"/> processing functions and experiment with different feature sets.</p><p>The simplest and probably the most common type of feature is the unigram or a single token. As the <code class="literal">tokenize</code> function already outputs single functions, the <code class="literal">unigram</code> function is very simple to implement:</p><div class="informalexample"><pre class="programlisting">(def unigrams identity)</pre></div><p>Another way to construct features is to use a number of consecutive tokens. In the abstract, these are called n-grams. Bigrams (two tokens) and trigrams (three tokens) are common instances of this type of function. We'll define all of these as functions:</p><div class="informalexample"><pre class="programlisting">(defn n-grams [n coll]
  (map #(str/join " " %) (partition n 1 coll)))
(defn bigrams [coll] (n-grams 2 coll))
(defn trigrams [coll] (n-grams 3 coll))</pre></div><p>There are a number of different features we could create and experiment with, but we won't show them all here. However, before we move on, here's one more common type of feature: the token tagged with its <span class="strong"><strong>part of speech</strong></span> (<span class="strong"><strong>POS</strong></span>). POS<a id="id412" class="indexterm"/> is the category for words, which determines their range of uses in sentences. You probably remember these from elementary school. Nouns are people, places, and things. Verbs are actions.</p><p>To get this information, we'll use OpenNLP's trained POS tagger. This takes a word and associates it with a part of speech. In order to use this, we need to download the training model file. You can find it at <a class="ulink" href="http://opennlp.sourceforge.net/models-1.5/">http://opennlp.sourceforge.net/models-1.5/</a>. Download <span class="strong"><strong>en POS tagger</strong></span> (English) with a description of <span class="strong"><strong>Maxent model with tag dictionary</strong></span>. The file itself is named <code class="literal">en-pos-maxent.bin</code>, and I put it into the <code class="literal">data</code> directory of my project.</p><p>This tagger uses the POS tags defined by the Penn Treebank (<a class="ulink" href="http://www.cis.upenn.edu/~treebank/">http://www.cis.upenn.edu/~treebank/</a>). It uses a trained, probabilistic tagger to associate tags with each token from a sentence. For example, it might associate the token things with the <code class="literal">NNS</code> tag, which is the abbreviation for plural nouns. We'll create the string for this feature by putting these two together so that this feature would look like <code class="literal">things_NNS</code>.</p><p>Once we have the data file, we<a id="id413" class="indexterm"/> need to<a id="id414" class="indexterm"/> load it into a POS model. We'll write a function to do this and return the tagger object:</p><div class="informalexample"><pre class="programlisting">(defn read-me-tagger [filename]
  (-&gt;&gt;filename
io/input-stream
POSModel.
POSTaggerME.))</pre></div><p>Using the tagger is <a id="id415" class="indexterm"/>pretty easy. We just call its <a id="id416" class="indexterm"/>tag method as follows:</p><div class="informalexample"><pre class="programlisting">(defn with-pos [model coll]
  (map #(str/join "_" [%1 %2])
coll
       (.tag model (into-array coll))))</pre></div><p>Now that we have these functions ready, let's take a short sentence and generate the features for it. For this set of examples, we'll use the clauses, <code class="literal">Time flies like an arrow; fruit flies like a banana</code>. To begin with, we'll define the input data and load the POS tagger.</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>user=&gt; (def data</strong></span>
<span class="strong"><strong>         "Time flies like an arrow; fruit flies like a banana.")</strong></span>
<span class="strong"><strong>user=&gt; (def tagger (t/read-me-tagger "data/en-pos-maxent.bin"))</strong></span>
<span class="strong"><strong>user=&gt; (def tokens (t/tokenize data))</strong></span>
<span class="strong"><strong>user=&gt; (t/unigrams tokens)</strong></span>
<span class="strong"><strong>("time" "flies" "like" "an" "arrow" ";" "fruit" "flies" "like" "a"</strong></span>
<span class="strong"><strong> "banana" ".")</strong></span>
<span class="strong"><strong>user=&gt; (t/bigrams tokens)</strong></span>
<span class="strong"><strong>("time flies" "flies like" "like an" "an arrow" "arrow ;"</strong></span>
<span class="strong"><strong> "; fruit" "fruit flies" "flies like" "like a" "a banana"</strong></span>
<span class="strong"><strong> "banana .")</strong></span>
<span class="strong"><strong>user=&gt; (t/trigrams tokens)</strong></span>
<span class="strong"><strong>("time flies like" "flies like an" "like an arrow" "an arrow ;"</strong></span>
<span class="strong"><strong> "arrow ; fruit" "; fruit flies" "fruit flies like" "flies like a"</strong></span>
<span class="strong"><strong> "like a banana" "a banana .")</strong></span>
<span class="strong"><strong>user=&gt; (t/with-pos tagger tokens)</strong></span>
<span class="strong"><strong>("time_NN" "flies_VBZ" "like_IN" "an_DT" "arrow_NN" ";_:"</strong></span>
<span class="strong"><strong> "fruit_NN" "flies_NNS" "like_IN" "a_DT" "banana_NN" "._.")</strong></span>
</pre></div><p>In the last output, the words are associated with part-of-speech tags. This output uses the tags from the Penn Treebank (<a class="ulink" href="http://www.cis.upenn.edu/~treebank/">http://www.cis.upenn.edu/~treebank/</a>). You can look at it for more information, but briefly, here are the tags used in the preceding code snippet:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">NN</code> means noun;</li><li class="listitem" style="list-style-type: disc"><code class="literal">VBZ</code> means the present tense verb, third person, singular;</li><li class="listitem" style="list-style-type: disc"><code class="literal">IN</code> means and, the preposition or subordinating conjunction </li><li class="listitem" style="list-style-type: disc"><code class="literal">DT</code> means the determiner.</li></ul></div><p>So we can see<a id="id417" class="indexterm"/> that the POS-tagged features provide <a id="id418" class="indexterm"/>the most data on the single tokens; however, the<a id="id419" class="indexterm"/> n-grams (bigrams and trigrams) provide <a id="id420" class="indexterm"/>more information about the context around each word. Later on, we'll see which one gets better results.</p><p>Now that we have the preprocessing out of way, let's turn our attention to the documents and how we want to structure the rest of the experiment.</p></div></div>
<div class="section" title="Cross-validating the results"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec42"/>Cross-validating the results</h1></div></div></div><p>As I've <a id="id421" class="indexterm"/>already <a id="id422" class="indexterm"/>mentioned, the dataset for this chapter is a manually coded group of 500 hotel reviews taken from the OpinRank dataset. For this experiment, we'll break these into 10 chunks of 50 reviews each.</p><p>These chunks will allow us to use <span class="strong"><strong>K-fold cross validation</strong></span><a id="id423" class="indexterm"/> to test how our system is doing. Cross validation is a way of checking your algorithm and procedures by splitting your data up into equally sized chunks. You then train your data on all of the chunks but one; that is the training set. You calculate the error after running the trained system on the validation set. Then, you use the next chunk as a validation set and start over again. Finally, we can average the error for all of the trials.</p><p>For example, the validation procedure uses four folds, A, B, C, and D. For the first run, A, B, and C would be the training set, and D would be the test set. Next, A, B, and D would be the training set, and C would be the test set. This would continue until every fold is used as the test set once.</p><p>This may seem like a lot of work, but it helps us makes sure that we didn't just get lucky with our choice of training or validation data. It provides a much more robust way of estimating the error rates and accuracy of our classifier.</p><p>The main trick in implementing cross validation is that Clojure's native partitioning functions (<code class="literal">partition</code> and <code class="literal">partition-all</code>) don't handle extra items exactly the way we'd like. The <code class="literal">partition</code> function<a id="id424" class="indexterm"/> just throws the extras away, and <code class="literal">partition-all</code> sticks all <a id="id425" class="indexterm"/>of the extras to the end in a smaller group. What we'd like is to include the extras in the previous chunks. Each chunk should have one extra until all of the remainders are exhausted. To handle this, we'll define a function named<a id="id426" class="indexterm"/> <code class="literal">partition-spread</code>. It will partition the first part of the collection into larger chunks and the second part into smaller chunks.</p><p>Unfortunately, we'll <a id="id427" class="indexterm"/>need to <a id="id428" class="indexterm"/>know the size of the input collection. To do this, we must hold the entire collection in the memory at once, so this algorithm isn't good for very large sequences:</p><div class="informalexample"><pre class="programlisting">(defn partition-spread [k coll]
  (let [get-mod (fn [i x]
                  [(mod i k) x])
map-second #(map second (second %))]
    (-&gt;&gt;coll
      (map-indexed get-mod)
      (group-by first)
      (map map-second))))</pre></div><p>We can now see how these partitioning functions differ:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>user=&gt; (partition 4 (range 10))</strong></span>
<span class="strong"><strong>((0 1 2 3) (4 5 6 7))</strong></span>
<span class="strong"><strong>user=&gt; (partition-all 4 (range 10))</strong></span>
<span class="strong"><strong>((0 1 2 3) (4 5 6 7) (8 9))</strong></span>
<span class="strong"><strong>user=&gt; (xv/partition-spread 4 (range 10))</strong></span>
<span class="strong"><strong>((0 4 8) (1 5 9) (2 6) (3 7))</strong></span>
<span class="strong"><strong>user=&gt; (xv/partition-spread 3 (range 10))</strong></span>
<span class="strong"><strong>((0 3 6 9) (1 4 7) (2 5 8))</strong></span>
</pre></div><p>We can also see that the semantics of the first parameter have changed. Instead of indicating the size of the partitions, it specifies the number of partitions. Now the partitions are all of a roughly equal size.</p><p>Next, we'll create a couple of functions that pull out each chunk to use as the validation set and concatenates all the other chunks.</p><div class="informalexample"><pre class="programlisting">(defn step-folds-seq [folds steps]
  (lazy-seq
    (when-let [[s &amp;ss] (seq steps)]
      (let [[prefix [validation &amp; suffix]] (split-at s folds)
training (flatten (concat prefix suffix))
current [validation training]]
        (cons current (step-folds-seq folds ss))))))
(defn step-folds [folds]
  (step-folds-seq folds (range (count folds))))</pre></div><p>Now, by partitioning into chunks with one element each, we can clearly see just how the K-fold partitioning works. Each time, a new chunk is selected as the validation set (the first item), and the rest of the chunks are concatenated into the training set (the second item):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>user=&gt; (xv/step-folds (xv/partition-spread 10 (range 10)))</strong></span>
<span class="strong"><strong>([(0) (1 2 3 4 5 6 7 8 9)] [(1) (0 2 3 4 5 6 7 8 9)]</strong></span>
<span class="strong"><strong> [(2) (0 1 3 4 5 6 7 8 9)] [(3) (0 1 2 4 5 6 7 8 9)]</strong></span>
<span class="strong"><strong> [(4) (0 1 2 3 5 6 7 8 9)] [(5) (0 1 2 3 4 6 7 8 9)]</strong></span>
<span class="strong"><strong> [(6) (0 1 2 3 4 5 7 8 9)] [(7) (0 1 2 3 4 5 6 8 9)]</strong></span>
<span class="strong"><strong> [(8) (0 1 2 3 4 5 6 7 9)] [(9) (0 1 2 3 4 5 6 7 8)])</strong></span>
</pre></div><p>Now we can define a <a id="id429" class="indexterm"/>function that controls the K-fold validation process. It takes the training and error steps as function parameters, and it just handles partitioning the data into groups, calling the<a id="id430" class="indexterm"/> training and error functions, and combining their output into one result:</p><div class="informalexample"><pre class="programlisting">(defn k-fold
  ([train error combine data]
   (k-fold train error combine 10 data))
  ([train error combine k input-data]
   (-&gt;&gt; input-data
shuffle
     (partition-spread k)
step-folds
     (map (fn [[v t]] [v (train t)]))
     (map (fn [[v t]] [err (error t v)]
                        (println :error err)
err)))
     (reduce combine (combine)))))</pre></div><p>Now we need to decide what constitutes an error and how we'll compute it.</p></div>
<div class="section" title="Calculating error rates"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec43"/>Calculating error rates</h1></div></div></div><p>To calculate the <a id="id431" class="indexterm"/>error rates on classification algorithms, <a id="id432" class="indexterm"/>we'll keep count of several things. We'll track how many positives are correctly and incorrectly identified as well as how many negatives are correctly and incorrectly identified. These values are usually called true positives, false positives, true negatives, and false negatives. The relationship of these values to the expected values and the classifier's outputs and to each other can be seen in the following diagram:</p><div class="mediaobject"><img src="graphics/4139OS_06_01.jpg" alt="Calculating error rates"/></div><p>From these <a id="id433" class="indexterm"/>numbers, we'll first calculate<a id="id434" class="indexterm"/> the precision of the algorithm. This is the ratio of true positives to the number of all identified positives (both true and false positives). This tells us how many of the items that it identified as positives actually are positives.</p><p>We'll then calculate the recall. This is the ratio of true positives to all actual positives (true positives and false negatives). This gives us an idea of how many positives it's missing.</p><p>To calculate this, we'll use a standard <code class="literal">reduce</code> loop. First, we'll write the accumulator function for it. This will take a mapping of the counts that we need to tally and a pair of ratings, the expected and the actual. Depending on what they are and whether they match, we'll increment one of the counts as follows:</p><div class="informalexample"><pre class="programlisting">(defnaccum-error [error-counts pair]
  (let [get-key {["+" "+"] :true-pos
                 ["-" "-"] :true-neg
                 ["+" "-"] :false-neg
                 ["-" "+"] :false-pos}
k (get-key pair)]
    (assoc error-counts k (inc (error-counts k)))))</pre></div><p>Once we have the counts for a test set, we'll need to summarize these counts into the figure for precision and recall:</p><div class="informalexample"><pre class="programlisting">(defn summarize-error [error-counts]
  (let [{:keys [true-pos false-pos true-neg false-neg]}
error-counts]
    {:precision (float (/ true-pos (+ true-pos false-pos))),
:recall (float (/ true-pos (+ true-pos false-neg)))}))</pre></div><p>With these two defined, the function to actually calculate the error is standard Clojure:</p><div class="informalexample"><pre class="programlisting">(defn compute-error [expecteds actuals]
  (let [start {:true-neg 0, :false-neg 0, :true-pos 0,
:false-pos 0}]
    (summarize-error
      (reduceaccum-error start (map vector expecteds actuals)))))</pre></div><p>We can do <a id="id435" class="indexterm"/>something similar to determine the <a id="id436" class="indexterm"/>mean error of a collection of precision/recall mappings. We could simply figure the value for each key separately, but rather than walking over the collection multiple times, we will do something more complicated and walk over it once while calculating the sums for each key:</p><div class="informalexample"><pre class="programlisting">(defn mean-error [coll]
  (let [start {:precision 0, :recall 0}
accum (fn [a b]
                {:precision (+ (:precision a) (:precision b))
:recall (+ (:recall a) (:recall b))})
summarize (fn [n a]
                    {:precision (/ (:precision a) n)
:recall (/ (:recall a) n)})]
    (summarize (count coll) (reduce accum start coll))))</pre></div><p>These functions will give us a good grasp of the performance of our classifiers and how well they do at identifying the sentiments expressed in the data.</p></div>
<div class="section" title="Using the Weka machine learning library"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec44"/>Using the Weka machine learning library</h1></div></div></div><p>We're going to test a <a id="id437" class="indexterm"/>couple of machine learning algorithms that are commonly used for sentiment analysis. Some of them are implemented in the OpenNLP library. However, they do not have anything for others algorithms. So instead, we'll use the Weka machine learning <a id="id438" class="indexterm"/>library (<a class="ulink" href="http://www.cs.waikato.ac.nz/ml/weka/">http://www.cs.waikato.ac.nz/ml/weka/</a>). This doesn't have the classes to tokenize or segment the data that an application in a natural language processing requires, but it does have a more complete palette of machine learning algorithms.</p><p>All of the classes in the Weka library also have a standard, consistent interface. These classes are really designed to be used from the command line, so each takes its options as an array of strings with a command-line-like syntax. For example, the array for a naive Bayesian classifier may have a flag to indicate that it should use the kernel density estimator rather than the normal distribution. This would be indicated by the <code class="literal">-K</code> flag being included in the option array. Other options may include a parameter that would follow the option in the array. For example, the logistic regression classifier can take a parameter to indicate the maximum number of iterations it should run. This would include the items <code class="literal">-M</code> and <code class="literal">1000</code> (say) in the options array.</p><p>The Clojure interface<a id="id439" class="indexterm"/> functions for these classes are very regular. In fact, they're almost boilerplate. Unfortunately, they're also a little redundant. Option names are repeated in the functions' parameter list, the default values for those parameters, and where the parameters are fed into the options array. It would be better to have one place for a specification of each option, its name, its flag, its semantics, and its default value.</p><p>This is a perfect application of Clojure's macro system. The data to create the functions can be transformed into the function definition, which is then compiled into the interface function.</p><p>The final product of this is the <code class="literal">defanalysis</code> macro, which takes the name of the function, the class, the method it's based on, and the options it accepts. We'll see several uses of it later in this chapter.</p><p>Unfortunately, at almost 40 lines, this system is a little long and disruptive to include here, however interesting it may be. You can find this in the <code class="literal">src/sentiment/weka.clj</code> file in the code download, and I have discussed it in a bit more length in <span class="emphasis"><em>Clojure Data Analysis Cookbook</em></span>, <span class="emphasis"><em>Packt Publishing</em></span>.</p><p>We do still need to convert the <code class="literal">HotelReview</code> records that we loaded earlier into a <code class="literal">WekaInstances</code> collection. We'll need to do this several times as we train and test the classifiers, and this will provide us with a somewhat shorter example of interacting with Weka.</p><p>To store a data matrix, Weka uses an <code class="literal">Instances</code> object. This implements a number of standard Java collection interfaces, and it holds objects that implement the <code class="literal">Instance</code> interface, such as <code class="literal">DenseInstance</code> or <code class="literal">SparseInstance</code>.</p><p>Instances also keep track of which fields each item has in its collection of <code class="literal">Attribute</code> objects. To create these, we'll populate <code class="literal">ArrayList</code> with all of the features that we accumulated in the feature index. We'll also create a feature for the ratings and add it to <code class="literal">ArrayList</code>. We'll return both the full collection of the attributes and the single attribute for the review's rating:</p><div class="informalexample"><pre class="programlisting">(defn instances-attributes [f-index]
  (let [attrs (-&gt;&gt; f-index
                (sort-by second)
                (map #(Attribute. (first %)))
ArrayList.)
review (Attribute. "review-rating"
                           (ArrayList. ["+" "-"]))]
    (.add attrs review)
    [attrs review]))</pre></div><p>(At this point, we're hardcoding the markers for the sentiments as a plus sign and a negative sign. However, these could easily be made into parameters for a more flexible system.)</p><p>Each hotel review <a id="id440" class="indexterm"/>itself can be converted separately. As most documents will only have a fraction of the full number of features, we'll use <code class="literal">SparseInstance</code>. Sparse vectors are more memory efficient if most of the values in the instance are zero. If the feature is nonzero in the feature vector, we'll set it in <code class="literal">Instance</code>. Finally, we'll also set the rating attribute as follows:</p><div class="informalexample"><pre class="programlisting">(defn review-&gt;instance [attrs review]
  (let [i (SparseInstance. (.size attrs))]
    (doseq [[attr value] (map vector attrs (:feature-vec review))]
      (when-not (zero? value)
        (.setValueiattr (double value))))
    (.setValuei (last attrs) (:rating review))
i))</pre></div><p>With these, we can populate <code class="literal">Instances</code> with the data from the <code class="literal">HotelReview</code> records:</p><div class="informalexample"><pre class="programlisting">(defn -&gt;instances
  ([f-index review-coll]
   (-&gt;instances f-index review-coll "hotel-reviews"))
  ([f-index review-coll name]
   (let [[attrs review] (instances-attributes f-index)
instances (Instances. name attrs (count review-coll))]
     (doseq [review review-coll]
       (let [i (review-&gt;instance attrs review)]
         (.add instances i)))
     (.setClass instances review)
instances)))</pre></div><p>Now we can define some functions to sit between the cross-validation functions we defined earlier and the Weka interface functions.</p><div class="section" title="Connecting Weka and cross-validation"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec36"/>Connecting Weka and cross-validation</h2></div></div></div><p>The first of these functions <a id="id441" class="indexterm"/>will classify an instance and determine which rating symbol it is classified by (<code class="literal">+</code> or <code class="literal">-</code>), given the distribution of probabilities for each category. This function is used to run the classifier on all data in an <code class="literal">Instances</code> object:</p><div class="informalexample"><pre class="programlisting">(defn run-instance [classifier instances instance]
  (let [dist (.distributionForInstance classifier instance)
i (first (apply max-key second
                       (map vector (range) dist)))]
    (.. instances classAttribute (value i))))
(defn run-classifier [classifier instances]
  (map #(run-instance classifier instances %) instances))</pre></div><p>The next function defines the cross-validation procedure for a group of <code class="literal">HotelReview</code> records. This function actually takes a training function and returns a function that takes the feature index and collection of <code class="literal">HotelReview</code> records and actually performs the cross validation. This will allow us to create some wrapper functions for each type of classifier:</p><div class="informalexample"><pre class="programlisting">(defn run-k-fold [trainer]
  (fn [f-index coll]
    (let [do-train (fn [xs]
                     (let [is (w/-&gt;instances f-index xs)]
                       (trainer is)))
do-test (fn [classifier xs]
                    (-&gt;&gt;xs
                      (w/-&gt;instances f-index)
w/filter-class-index
                      (run-classifier classifier)
                      (xv/compute-error (map :rating xs))
vector))]
      (xv/k-fold do-train do-test concat 10 coll))))</pre></div><p>When executed, this function will return a list of ten of whatever the <code class="literal">do-test</code> function returns. In this case, that means a list of ten precision and recall mappings. We can average the output of this to get a summary of each classifier's performance.</p><p>Now we can start actually defining and testing classifiers.</p></div><div class="section" title="Understanding maximum entropy classifiers"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec37"/>Understanding maximum entropy classifiers</h2></div></div></div><p>Maximum entropy (maxent) classifiers<a id="id442" class="indexterm"/> are, in a sense, very conservative classifiers. They assume nothing about hidden variables and base their classifications strictly upon the evidence they've been trained on. They are consistent with the facts that they've seen, but all other distributions are assumed to be completely uniform otherwise. What does this mean?</p><p>Let's say that we have a set of reviews and positive or negative ratings, and we wish to be able to predict the value of ratings when the ratings are unavailable, given the tokens or other features in the reviews. The probability that a rating is positive would be p(+). Initially, before we see any actual evidence, we may intuit that this probability would be uniform across all possible features. So, for a set of five features, before training, we might expect the probability function to return these values:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><tbody><tr><td style="text-align: left" valign="top">
<p>p(+)</p>
</td><td style="text-align: left" valign="top">
<p> ½</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>p(-)</p>
</td><td style="text-align: left" valign="top">
<p> ½</p>
</td></tr></tbody></table></div><p>This is perfectly uniform but not very useful. We have to make observations from the data in order to train the classifier.</p><p>The process of training involves observing the features in each document and its rating and determining the probability of any given feature that is found in a document with a given rating. We'll denote this as p(x, y) or the probability as feature x and rating y.</p><p>These features impose constraints on our model. As we gather more and more constraints, figuring a consistent and uniform distribution for the non-constrained probabilities in the model becomes increasingly difficult.</p><p>Essentially, <a id="id443" class="indexterm"/>this is the maxent algorithm's job. It takes into account all of the constraints imposed by the probabilities found in the training data, but it maintains a uniform distribution on everything that's unconstrained. This provides a more consistent, stronger algorithm overall, and it still performs very well, usually. Also, cross validation can help us evaluate its performance.</p><p>Another benefit is that maxent doesn't make any assumptions about the relationships between different features. In a bit, we'll look at a naive Bayesian classifier, and it does make an assumption about the relationships between the features, an often unrealistic assumption. Because maxent does not make that assumption, it can better match the data involved.</p><p>For this chapter, we'll use the maxent classifier found in the Weka class, <code class="literal">weka.classifiers.functions.Logistic</code> (maxent is equivalent to the logistic regression, which attempts to classify data based on a binary categorical label, which is based on one or more features). We'll use the <code class="literal">defanalysis</code> macro to define a utility function that cross validates a logistic regression classifier as follows:</p><div class="informalexample"><pre class="programlisting">(w/defanalysis train-logistic Logistic buildClassifier
  [["-D" debugging false :flag-true]
   ["-R" ridge nil :not-nil]
   ["-M" max-iterations -1]])
(def k-fold-logistic (run-k-fold train-logistic))</pre></div><p>Now let's define something similar for a naive Bayesian classifier.</p></div><div class="section" title="Understanding naive Bayesian classifiers"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec38"/>Understanding naive Bayesian classifiers</h2></div></div></div><p>A common, generally well-performing classifier is the<a id="id444" class="indexterm"/> naive Bayesian classifier. It's naive because it makes an assumption about that data and the features; it assumes that the features are independent of each other. That is, the probability of, say, <span class="emphasis"><em>good</em></span> occurring in a document is not influenced at all by the probability of any other token or feature, such as, say, <span class="emphasis"><em>not</em></span>. Unfortunately, language doesn't work this way, and there are dependencies all through the features of any linguistic dataset.</p><p>Fortunately, even when the data and features are not completely independent, this classifier often still performs quite well in practice. For example, in <span class="emphasis"><em>An analysis of data characteristics that affect naive Bayes performance</em></span> by <span class="emphasis"><em>Irina Rish</em></span>, <span class="emphasis"><em>Joseph Hellerstein</em></span>, and <span class="emphasis"><em>Jayram Thathachar</em></span>, it was found that Bayesian classifiers perform best with features that are completely independent or functionally dependent.</p><p>This classifier works by knowing several probabilities and then using Bayes' theorem to turn them around to predict the classification of the document. The following are the probabilities that it needs to know:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">It needs to know the probability for each feature in the training set. We'll call this <span class="strong"><strong>p(F)</strong></span>. Say the word <span class="emphasis"><em>good</em></span> occurs in 40 percent of the documents. This is the evidence of the classification.</li><li class="listitem" style="list-style-type: disc">It needs to know the probability that a document will be part of a classification. We'll call this <span class="strong"><strong>p(C)</strong></span>. Say that the rate of positive ratings in the corpus of reviews is 80 percent. This is the prior distribution.</li><li class="listitem" style="list-style-type: disc">Now it needs to know the probability that the good feature is in the document if the document is rated positively. This is <span class="strong"><strong>p(F|C)</strong></span>. For this hypothetical example, say that <span class="emphasis"><em>good</em></span> appears in 40 percent of the positive reviews. This is the likelihood.</li></ul></div><p>Bayes theorem allows us to turn this around and compute the probability that a document is positively rated, if it contains the feature <span class="emphasis"><em>good</em></span>.</p><div class="mediaobject"><img src="graphics/4139OS_06_02.jpg" alt="Understanding naive Bayesian classifiers"/></div><p>For this example, this turns out to be <code class="literal">(0.8)(0.4) / 0.4</code>, or 0.8 (80 percent). So, if the document contains the feature <span class="emphasis"><em>good</em></span>, it is very likely to be positively rated.</p><p>Of course, things begin to get more and more interesting as we start to track more and more features. If the document contains both <span class="emphasis"><em>not</em></span> and <span class="emphasis"><em>good</em></span>, for instance, the probability that the review is positive may change drastically.</p><p>The <a id="id445" class="indexterm"/>Weka implementation of a naive Bayesian classifier is found in <code class="literal">weka.classifiers.bayes.NaiveBayes</code>, and we'll wrap it in a manner that is similar to the one we used for the maxent classifier:</p><div class="informalexample"><pre class="programlisting">(w/defanalysis train-naive-bayesNaiveBayesbuildClassifier
  [["-K" kernel-density false :flag-true]
   ["-D" discretization false :flag-true]])
(def k-fold-naive-bayes (run-k-fold train-naive-bayes))</pre></div><p>Now that we have both the classifiers in place, let's look again at the features we'll use and how we'll compare everything.</p></div></div>
<div class="section" title="Running the experiment"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec45"/>Running the experiment</h1></div></div></div><p>Remember, earlier we <a id="id446" class="indexterm"/>defined functions to break a sequence of tokens into features of various sorts: unigrams, bigrams, trigrams, and POS-tagged unigrams. We can take these and automatically test both the classifiers against all of these types of features. Let's see how.</p><p>First, we'll define some top-level variables that associate label keywords with the functions that we want to test at that point in the process (that is, classifiers or feature-generators):</p><div class="informalexample"><pre class="programlisting">(def classifiers
  {:naive-bayes a/k-fold-naive-bayes
:maxent a/k-fold-logistic})
(def feature-factories
  {:unigram t/unigrams
:bigram t/bigrams
:trigram t/trigrams
:pos (let [pos-model 
              (t/read-me-tagger "data/en-pos-maxent.bin")]
          (fn [ts] (t/with-pos pos-model ts)))})</pre></div><p>We can now iterate over both of these hash maps and cross-validate these classifiers on these features. We'll average the error information (the precision and recall) for all of them and return the averages. Once we've executed that, we can spend some time looking at the results.</p><p>For the inner-most loop of this process, we'll take a collection of features and a classifier and cross validate them. This is pretty straightforward; it simply constructs an identifying key out of the keywords for the feature generator and the classifier, runs the cross validation, and averages the output error information as follows:</p><div class="informalexample"><pre class="programlisting">(defn do-class [f-key f-index features c-info]
  (let [[c-key c] c-info, k [c-key f-key]]
    (println k)
    [k (x/mean-error (c f-index features))]))</pre></div><p>Now, given a set of features, we'll call <code class="literal">do-class</code> on each classifier one loop up. Constructing the loop this way by generating the features and then looping on the classifiers keeps us from needing to regenerate the same set of features multiple times:</p><div class="informalexample"><pre class="programlisting">(defn do-features [docs classifiers f-info]
  (let [[f-key f] f-info
        [f-index features] (d/add-features f docs)]
    (map #(do-class f-key f-index features %) classifiers)))</pre></div><p>The controlling <a id="id447" class="indexterm"/>function for this process simply calls <code class="literal">do-features</code> on each set of feature-generating functions and stores all the outputs into a hash map:</p><div class="informalexample"><pre class="programlisting">(defn test-suite [docs]
  (into {} (mapcat #(do-features docs classifiers %)
feature-factories)))</pre></div><p>This takes a while to execute:</p><div class="informalexample"><pre class="programlisting">user=&gt; (def reviews (-&gt;&gt; "data/hotels-sample" d/read-data
d/sample vals flatten))
#'user/reviews
user=&gt; (c/test-suite reviews)
[:naive-bayes :unigram]
:error [{:precision 0.5185185, :recall 0.5}]
:error [{:precision 0.6, :recall 0.5769231}]
:error [{:precision 0.5185185, :recall 0.6666667}]</pre></div><p>Now we can start looking at the data in more detail.</p></div>
<div class="section" title="Examining the results"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec46"/>Examining the results</h1></div></div></div><p>First, let's examine the <a id="id448" class="indexterm"/>precision<a id="id449" class="indexterm"/> of the classifiers. Remember that the precision is how well the classifiers do at only returning positive reviews. This indicates the percentage of reviews that each classifier has identified as being positive is actually positive in the test set:</p><div class="mediaobject"><img src="graphics/4139OS_06_03.jpg" alt="Examining the results"/></div><p>We need to remember a <a id="id450" class="indexterm"/>couple of things while looking at this graph. First, <a id="id451" class="indexterm"/>sentiment analysis is difficult, compared to other categorization tasks. Most importantly, human raters only agree about 80 percent of the time. So, the bar seen in the preceding figure that almost reaches 65 percent is actually decent, if not great. Still, we can see that the naive Bayesian classifier generally outperforms the maxent one for this dataset, especially when using unigram features. It performed less well for the bigram and trigram features, and slightly lesser for the POS-tagged unigrams.</p><p>We didn't try tagging the bigram and trigrams with POS information, but that might have been an interesting experiment. Based on what we can see here, these feature generators would not get better results than what we've already tested, but it would be good to know that more definitively.</p><p>It's interesting to see that maxent performed best with trigrams. Generally, compared to unigrams, trigrams pack more information into each feature, as they encode some implicit syntactical information into each feature. However, each feature also occurs fewer times, which makes performing some statistical processes on it more difficult. Remember that recall is the percentage of positives in the test set that were correctly identified by each classifier. Now let's look at the recall of these classifiers:</p><div class="mediaobject"><img src="graphics/4139OS_06_04.jpg" alt="Examining the results"/></div><p>First, while the naive <a id="id452" class="indexterm"/>Bayesian classifier still outperforms the maxent <a id="id453" class="indexterm"/>classifier, this time the bigram and trigram get much better results than the unigram or POS-tagged features.</p><p>Also, the recall numbers on these two tests are better than any of the values for the precision. The best part is that the naive Bayes bigram test had a recall of just over 90 percent.</p><p>In fact, just looking at the results, there appeared to be an inverse relationship between the precision and the recall, as there typically is. Tests with high precision tended to have lower recall numbers and vice versa. This makes intuitive sense. A classifier can get a high recall number by marking more reviews as positive, but that negatively impacts its precision. Or, a classifier can have better precision by being more selective in what it marks as positive but also noting that will drag down its recall.</p><div class="section" title="Combining the error rates"><div class="titlepage"><div><div><h2 class="title"><a id="ch06lvl2sec39"/>Combining the error rates</h2></div></div></div><p>We can combine these two<a id="id454" class="indexterm"/> into a single metric using the harmonic mean of the precision and recall, also known as the F-measure. We'll compute this with the following function:</p><div class="informalexample"><pre class="programlisting">(defn f-score [error]
  (let [{:keys [precision recall]} error]
    (* 2 (/ (* precision recall) (+ precision recall)))))</pre></div><p>This gives us a way to combine the precision and recall in a rational, meaningful manner. Let's see what values it gives for the F-measure:</p><div class="mediaobject"><img src="graphics/4139OS_06_05.jpg" alt="Combining the error rates"/></div><p>So, as we've already <a id="id455" class="indexterm"/>noticed, the naive Bayesian classifier performed better than the maxent classifier in general, and on balance, the bigram features worked best for this classifier.</p><p>While this gives us a good starting point, we'll also want to consider why we're looking for this information, how we'll use it, and what penalties are involved. If it's vitally important that we get all the positive reviews, then we will definitely want to use the naive Bayesian classifier with the bigram features. However, if the cost of missing some isn't so high but the cost of having to sort through too many false results is high, then we'll probably want to use unigram features, which would minimize the number of false results we have to manually sort through later.</p></div></div>
<div class="section" title="Improving the results"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec47"/>Improving the results</h1></div></div></div><p>What could we <a id="id456" class="indexterm"/>do to<a id="id457" class="indexterm"/> improve these results?</p><p>First, we should improve the test and training sets. It would be good to have multiple raters, say, have each review independently reviewed three times and use the rating that was chosen two or three times.</p><p>Most importantly, we'd like to have a larger and better test set and training set. For this type of problem, having 500 observations is really on the low end of what you can do anything useful with, and you can expect the results to improve with more observations. However, I do need to stress on the fact that more training data doesn't necessarily imply better results. It could help, but there are no guarantees.</p><p>We could also look at improving the features. We could select them more carefully, because having too many useless or unneeded features can make the classifier perform poorly. We could also select different features such as dates or information about the informants; if we had any data on them, it might be useful.</p><p>There has also been more<a id="id458" class="indexterm"/> recent work in moving beyond polarity <a id="id459" class="indexterm"/>classification, such as looking at emotional classification. Another way of being more fine grained than binary categorization is to classify the documents on a scale. For instance, instead of positive or negative, these classifiers could try to predict how the user would rate the product on a five-star scale, such as what has become popular on <span class="strong"><strong>Amazon</strong></span><a id="id460" class="indexterm"/> and many websites that include user ratings and reviews.</p><p>Once we have identified the positive or negative reviews, we can apply other analyses separately to those reviews, whether its topic modeling, named entity recognition, or something else.</p></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch06lvl1sec48"/>Summary</h1></div></div></div><p>In the end, sentiment analysis is a simple tool to analyze documents according to two complex, possibly ill-defined categories. Although language is used in complex ways, modern sentiment analysis techniques can do almost as well as humans, which, admittedly, isn't particularly efficient.</p><p>What's most powerful about these techniques is that they can provide answers to questions that cannot be answered in other ways. As such, they're an important part of the data analyst's toolbox.</p><p>In the next chapter, we'll look at null hypothesis testing, which is a standard and foundational technique of traditional statistics. This informs how we approach many experiments and how we frame the questions that we're asking. By following these guides, we can make sure that our results are more valid and generalizable.</p></div></body></html>