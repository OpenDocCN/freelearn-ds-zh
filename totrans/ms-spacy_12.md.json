["```py\npip install transformers\npip install \"tensorflow>=2.0.0\"\n```", "```py\nplaying  play, ##ing\nplayed   play, ##ed\ngoing    go, ##ing\nvocabulary = [play,go, ##ing, ##ed]\n```", "```py\n[CLS] Yesterday I [MASK] my friend at [MASK] house [SEP] \n```", "```py\n[CLS] A man robbed a [MASK] yesterday [MASK] 8 o'clock [SEP] He [MASK] the bank with 6 million dollars [SEP]\nLabel = IsNext\n```", "```py\n[CLS] Rabbits like to [MASK] carrots and [MASK] leaves [SEP] [MASK] Schwarzenegger is elected as the governor of [MASK] [SEP]\nLabel= NotNext\n```", "```py\nfrom transformers import BertTokenizer\nbtokenizer =\\\nBertTokenizer.from_pretrained('bert-base-uncased')\ntokens = btokenizer.tokenize(sentence)\ntokens\n['he', 'lived', 'characteristic', '##ally', 'idle', 'and', 'romantic', '.']\nids = btokenizer.convert_tokens_to_ids(tokens)\nids\n[2002, 2973, 8281, 3973, 18373, 1998, 6298, 1012]\n```", "```py\nfrom transformers import BertTokenizer\nbtokenizer =\\\nBertTokenizer.from_pretrained('bert-base-uncased')\nsentence = \"He lived characteristically idle and romantic.\"\nids = btokenizer.encode(sentence)\nids\n[101, 2002, 2973, 8281, 3973, 18373, 1998, 6298, 1012, 102]\n```", "```py\nfrom transformers import BertTokenizer\nbtokenizer =\\\nBertTokenizer.from_pretrained('bert-base-uncased')\nsentence = \"He lived characteristically idle and romantic.\"\nencoded = btokenizer.encode_plus(\n        text=sentence,\n        add_special_tokens=True,\n        max_length=12,\n        pad_to_max_length=True,\n        return_tensors=\"tf\"\n) \ntoken_ids = encoded[\"input_ids\"]\nprint(token_ids)\ntf.Tensor([[  101  2002  2973  8281  3973 18373  1998  6298  1012   102     0     0]], shape=(1, 12), dtype=int32)\n```", "```py\nfrom transformers import BertTokenizer, TFBertModel\nbtokenizer =\\\n BertTokenizer.from_pretrained('bert-base-uncased')\nbmodel = TFBertModel.from_pretrained(\"bert-base-uncased\")\nsentence = \"He was idle.\"\nencoded = btokenizer.encode_plus(\n        text=sentence,\n        add_special_tokens=True,\n        max_length=10,\n        pad_to_max_length=True,\n        return_attention_mask=True,\n        return_tensors=\"tf\"\n)\ninputs = encoded[\"input_ids\"] \noutputs = bmodel(inputs)\n```", "```py\noutputs[0].shape\n(1, 10, 768)\noutputs[1].shape\n(1, 768)\n```", "```py\nfrom transformers import BertTokenizer, TFBertModel\nbert_tokenizer =\\\nBertTokenizer.from_pretrained(\"bert-base-uncased\")\nbmodel = TFBertModel.from_pretrained(\"bert-base-uncased\")\n```", "```py\nimport numpy as np\nimport tensorflow\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.models import Model\n```", "```py\ninput_ids=[]\n\nfor sent in sentences:\n   bert_inp=bert_tokenizer.encode_plus(sent,add_special_tokens = True,max_length =64,pad_to_max_length = True,return_attention_mask = True)\n   input_ids.append(bert_inp['input_ids'])\ninput_ids=np.asarray(input_ids)\nlabels=np.array(labels)\n```", "```py\ninputs = Input(shape=(64,), dtype=\"int32\")\nbert = bmodel(inputs)\nbert = bert[1]\noutputs = Dense(units=1, activation=\"sigmoid\")(bert)\nmodel = Model(inputs, outputs)\n```", "```py\nfrom transformers import pipeline\nnlp = pipeline(\"sentiment-analysis\")\n\nsent1 = \"I hate you so much right now.\"\nsent2 = \"I love fresh air and exercising.\"\nresult1 = nlp(sent1)\nresult2 = nlp(sent2)\n```", "```py\nresult1\n[{'label': 'NEGATIVE', 'score': 0.9984998}]\nresult2\n[{'label': 'POSITIVE', 'score': 0.99987185}]\n```", "```py\nfrom transformers import pipeline\nnlp = pipeline(\"question-answering\")\n\nres = nlp({\n    'question': 'What is the name of this book?',\n    'context': \"I'll publish my new book Mastering spaCy soon.\"\n})\nprint(res)\n{'score': 0.0007240351873990664, 'start': 25, 'end': 40, 'answer': 'Mastering spaCy'}\n```", "```py\npython3 -m spacy download en_core_web_trf\n```", "```py\nCollecting en-core-web-trf==3.0.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.0.0/en_core_web_trf-3.0.0-py3-none-any.whl (459.7 MB)\n     |████████████████████████████████| 459.7 MB 40 kB/s \nRequirement already satisfied: spacy<3.1.0,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from en-core-web-trf==3.0.0) (3.0.5)\n```", "```py\nSuccessfully installed en-core-web-trf-3.0.0 spacy-alignments-0.8.3 spacy-transformers-1.0.2 tokenizers-0.10.2 transformers-4.5.1\n Download and installation successful\nYou can now load the package via spacy.load('en_core_web_trf')\n```", "```py\nimport spacy\nnlp = spacy.load(\"en_core_web_trf\")\n```", "```py\ndoc = nlp(\"I visited my friend Betty at her house.\")\ndoc.ents\n(Betty,)\nfor word in doc:\n    print(word.pos_, word.lemma_)\n... \nPRON I\nVERB visit\nPRON my\nNOUN friend\nPROPN Betty\nADP at\nPRON her\nNOUN house\nPUNCT .\n```", "```py\ndoc = nlp(\"It went there unwillingly.\")\ndoc._.trf_data.wordpieces\nWordpieceBatch(strings=[['<s>', 'It', 'Gwent', 'Gthere', 'Gunw', 'ill', 'ingly', '.', '</s>']], input_ids=array([[    0,   243,   439,    89, 10963,  1873,  7790,     4,     2]]), attention_mask=array([[1, 1, 1, 1, 1, 1, 1, 1, 1]]), lengths=[9], token_type_ids=None)\n```", "```py\n<s>\nIt\nGwent\nGthere\nGunw\nIll\ningly\n.\n</s>\n```", "```py\ndoc._.trf_data.tensors[0].shape\n(1, 9, 768)\ndoc._.trf_data.tensors[1].shape\n(1, 768)\n```"]