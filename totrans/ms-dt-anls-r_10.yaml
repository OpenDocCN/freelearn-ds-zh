- en: Chapter 10. Classification and Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we concentrated on how to compress information found
    in a number of continuous variables into a smaller set of numbers, but these statistical
    methods are somewhat limited when we are dealing with categorized data, for example
    when analyzing surveys.
  prefs: []
  type: TYPE_NORMAL
- en: Although some methods try to convert discrete variables into numeric ones, such
    as by using a number of dummy or indicator variables, in most cases it's simply
    better to think about our research design goals instead of trying to forcibly
    use previously learned methods in the analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can replace a categorical variable with a number of dummy variables by creating
    a new variable for each label of the original discrete variable, and then assign
    *1* to the related column and *0* to all the others. Such values can be used as
    numeric variables in statistical analysis, especially with regression models.
  prefs: []
  type: TYPE_NORMAL
- en: When we analyze a sample and target population via categorical variables, usually
    we are not interested in individual cases, but instead in similar elements and
    groups. Similar elements can be defined as rows in a dataset with similar values
    in the columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will discuss different *supervised* and *unsupervised*
    ways to identify similar cases in a dataset, such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-means clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some machine learning algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Latent class model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discriminant analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cluster analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Clustering** is an unsupervised data analysis method that is used in diverse
    fields, such as pattern recognition, social sciences, and pharmacy. The aim of
    cluster analysis is to make homogeneous subgroups called clusters, where the objects
    in the same cluster are similar, and the clusters differ from each other.'
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Cluster analysis is one of the most well known and popular pattern recognition
    methods; thus, there are many clustering models and algorithms analyzing the distribution,
    density, possible center points, and so on in the dataset. In this section we
    are going to examine some hierarchical clustering methods.
  prefs: []
  type: TYPE_NORMAL
- en: '**Hierarchical clustering** can be either agglomerative or divisive. In agglomerative
    methods every case starts out as an individual cluster, then the closest clusters
    are merged together in an iterative manner, until finally they merge into one
    single cluster, which includes all elements of the original dataset. The biggest
    problem with this approach is that distances between clusters have to be recalculated
    at each iteration, which makes it extremely slow on large data. I''d rather not
    suggest trying to run the following commands on the `hflights` dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Divisive methods on the other hand take a top-down approach. They start from
    a single cluster, which is then iteratively divided into smaller groups until
    they are all singletons.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `stats` package contains the `hclust` function for hierarchical clustering
    that takes a distance matrix as an input. To see how it works, let''s use the
    `mtcars` dataset that we already analyzed in [Chapter 3](ch03.html "Chapter 3. Filtering
    and Summarizing Data"), *Filtering and Summarizing Data* and [Chapter 9](ch09.html
    "Chapter 9. From Big to Small Data"), *From Big to Smaller Data*. The `dist` function
    is also familiar from the latter chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Well, this is a way too brief output and only shows that our distance matrix
    included 32 elements and the clustering method. A visual representation of the
    results will be a lot more useful for such a small dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![Hierarchical clustering](img/2028OS_10_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: By plotting this `hclust` object, we obtained a *dendrogram*, which shows how
    the clusters are formed. It can be useful for determining the number of clusters,
    although in datasets with numerous cases it becomes difficult to interpret. A
    horizontal line can be drawn to any given height on the *y* axis so that the *n*
    number of intersections with the line provides a n-cluster solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'R can provide very convenient ways of visualizing the clusters on the *dendrogram*.
    In the following plot, the red boxes show the cluster membership of a three-cluster
    solution on top of the previous plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![Hierarchical clustering](img/2028OS_10_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Although this graph looks nice and it is extremely useful to have similar elements
    grouped together, for bigger datasets, it becomes hard to see through. Instead,
    we might be rather interested in the actual cluster membership represented in
    a vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'And the number of elements in the resulting clusters as a frequency table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'It seems that *Cluster 1*, the third cluster on the preceding plot, has the
    most elements. Can you guess how this group differs from the other two clusters?
    Well, those readers who are familiar with car names might be able to guess the
    answer, but let''s see what the numbers actually show:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Please note that we use the `round` function in the following examples to suppress
    the number of decimal places to 1 or 4 in the code output to fit the page width.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: There's a really spectacular difference in the average performance and gas consumption
    between the clusters! What about the standard deviation inside the groups?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'These values are pretty low compared to the standard deviations in the original
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'And the same applies when compared to the standard deviation between the groups
    as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This means that we achieved our original goal to identify similar elements of
    our data and organize those in groups that differ from each other. But why did
    we split the original data into exactly three artificially defined groups? Why
    not two, four, or even more?
  prefs: []
  type: TYPE_NORMAL
- en: Determining the ideal number of clusters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `NbClust` package offers a very convenient way to do some exploratory data
    analysis on our data before running the actual cluster analysis. The main function
    of the package can compute 30 different indices, all designed to determine the
    ideal number of groups. These include:'
  prefs: []
  type: TYPE_NORMAL
- en: Single link
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Average
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Complete link
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: McQuitty
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Centroid (cluster center)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Median
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-means
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ward
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After loading the package, let''s start with a visual method representing the
    possible number of clusters in our data—on a knee plot, which might be familiar
    from [Chapter 9](ch09.html "Chapter 9. From Big to Small Data"), *From Big to
    Smaller Data*, where you can also find some more information about the following
    elbow-rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![Determining the ideal number of clusters](img/2028OS_10_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding plots, we traditionally look for the *elbow*, but the second
    differences plot on the right might be more straightforward for most readers.
    There we are interested in where the most significant peak can be found, which
    suggests that choosing three groups would be ideal when clustering the `mtcars`
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, running all `NbClust` methods fails on such a small dataset.
    Thus, for demonstrational purposes, we are now running only a few standard methods
    and filtering the results for the suggested number of clusters via the related
    list element:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Both the Hartigan and Krzanowski-Lai indexes suggest sticking to three clusters.
    Let''s view the `iris` dataset as well, which includes a lot more cases with fewer
    numeric columns, so we can run all available methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The output summarizes that the ideal number of clusters is three based on the
    13 methods returning that number, five further methods suggest four clusters,
    and a few other cluster numbers were also computed by a much smaller number of
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: These methods are not only useful with the previously discussed hierarchical
    clustering, but generally used with k-means clustering as well, where the number
    of clusters is to be defined before running the analysis—unlike the hierarchical
    method, where we cut the dendogram after the heavy computations have already been
    run.
  prefs: []
  type: TYPE_NORMAL
- en: K-means clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**K-means clustering** is a non-hierarchical method first described by MacQueen
    in 1967\. Its big advantage over hierarchical clustering is its great performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unlike hierarchical cluster analysis, k-means clustering requires you to determine
    the number of clusters before running the actual analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm runs the following steps in a nutshell:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize a predefined (*k*) number of randomly chosen centroids in space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assign each object to the cluster with the closest centroid.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Recalculate centroids.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat the second and third steps until convergence.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We are going to use the `kmeans` function from the `stats` package. As k-means
    clustering requires a prior decision on the number of clusters, we can either
    use the `NbClust` function described previously, or we can come up with an arbitrary
    number that fits the goals of the analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'According to the previously defined optimal cluster number in the previous
    section, we are going to stick to three groups, where the within-cluster sum of
    squares ceases to drop significantly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The cluster means show some really important characteristics for each cluster,
    which we generated manually for the hierarchical clusters in the previous section.
    We can see that, in the first cluster, the cars have high mpg (low gas consumption),
    on average four cylinders (in contrast to six or eight), rather low performance
    and so on. The output also automatically reveals the actual cluster numbers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s compare these to the clusters defined by the hierarchical method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The results seem to be pretty stable, right?
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The cluster numbers have no meaning and their order is arbitrary. In other words,
    the cluster membership is a nominal variable. Based on this, the preceding R command
    might return `FALSE` instead of `TRUE` when the cluster numbers were allocated
    in a different order, but comparing the actual cluster membership will verify
    that we have found the very same groups. See for example `cbind(cn, k$cluster)`
    to generate a table including both cluster memberships.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing clusters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Plotting these clusters is also a great way to understand groupings. To this
    end, we will use the `clusplot` function from the `cluster` package. For easier
    understanding, this function reduces the number of dimensions to two, in a similar
    way to when we are conducting a PCA or MDS (described in [Chapter 9](ch09.html
    "Chapter 9. From Big to Small Data"), *From Big to Smaller Data*):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![Visualizing clusters](img/2028OS_10_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, after the dimension reduction, the two components explain 84.17
    percent of variance, so this small information loss is a great trade-off in favor
    of an easier understanding of the clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the relative density of the ellipses with the `shade` parameter
    can also help us realize how similar the elements of the same groups are. And
    we used the labels argument to show both the points and cluster labels as well.
    Be sure to stick to the default of *0* (no labels) or *4* (only ellipse labels)
    when visualizing large number of elements.
  prefs: []
  type: TYPE_NORMAL
- en: Latent class models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Latent Class Analysis** (**LCA**) is a method for identifying latent variables
    among polychromous outcome variables. It is similar to factor analysis, but can
    be used with discrete/categorical data. To this end, LCA is mostly used when analyzing
    surveys.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we are going to use the `poLCA` function from the `poLCA` package.
    It uses expectation-maximization and Newton-Raphson algorithms for finding the
    maximum likelihood for the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `poLCA` function requires the data to be coded as integers starting from
    one or as a factor, otherwise it will produce an error message. To this end, let''s
    transform some of the variables in the `mtcars` dataset to factors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The preceding command will overwrite the `mtcars` dataset in your current R
    session. To revert to the original dataset for other examples, please delete this
    updated dataset from the session by `rm(mtcars)` if needed.
  prefs: []
  type: TYPE_NORMAL
- en: Latent Class Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that the data is in an appropriate format, we can conduct the LCA. The
    related function comes with a number of important arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we have to define a formula that describes the model. Depending on the
    formula, we can define LCA (similar to clustering but with discrete variables)
    or **Latent Class Regression** (**LCR**) model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `nclass` argument specifies the number of latent classes assumed in the
    model, which is 2 by default. Based on the previous examples in this chapter,
    we will override this to 3.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can use the `maxiter`, `tol`, `probs.start`, and `nrep` parameters to fine-tune
    the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `graphs` argument can display or suppress the parameter estimates.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s start with basic LCA of three latent classes defined by all the available
    discrete variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The first part of the output (which can be also accessed via the `probs` element
    of the preceding saved `poLCA` list) summarizes the probabilities of the outcome
    variables by each latent class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'From these probabilities, we can see that all 8-cylinder cars belong to the
    third class, the first one only includes cars with automatic transmission, one
    carburetor, three gears, and so on. The exact same values can be plotted as well
    by setting the graph parameter to `TRUE` in the function call, or by calling the
    plot function directly afterwards:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Latent Class Analysis](img/2028OS_10_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The plot is also useful in highlighting that the first latent class includes
    only a few elements compared to the other classes (also known as "Estimated class
    population shares"):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The `poLCA` object can also reveal a bunch of other important information about
    the results. Just to name a few, let''s see the named list parts of the object,
    which can be extracted via the standard `$` operator:'
  prefs: []
  type: TYPE_NORMAL
- en: The `predclass` returns the most likely class memberships
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the other hand, the posterior element is a matrix containing the class membership
    probabilities of each case
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Akaike Information Criterion** (`aic`), **Bayesian Information Criterion**
    (`bic`), **deviance** (`Gsq`), and `Chisq` values represent different measures
    of goodness of fit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LCR models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: On the other hand, the LCR model is a supervised method, where we are not mainly
    interested in the latent variables explaining our observations at the exploratory
    data analysis scale, but instead we are using training data from which one or
    more covariates predict the probability of the latent class membership.
  prefs: []
  type: TYPE_NORMAL
- en: Discriminant analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Discriminant Function Analysis** (**DA**) refers to the process of determining
    which continuous independent (predictor) variables discriminate between a discrete
    dependent (response) variable''s categories, which can be considered as a reversed
    **Multivariate Analysis of Variance** (**MANOVA**).'
  prefs: []
  type: TYPE_NORMAL
- en: 'This suggests that DA is very similar to logistic regression (see [Chapter
    6](ch06.html "Chapter 6. Beyond the Linear Trend Line (authored by Renata Nemeth
    and Gergely Toth)"), *Beyond the Linear Trend Line (authored by Renata Nemeth
    and Gergely Toth)* and the following section), which is more generally used because
    of its flexibility. While logistic regression can handle both categorical and
    continuous data, DA requires numeric independent variables and has a few further
    requirements that logistic regression does not have:'
  prefs: []
  type: TYPE_NORMAL
- en: Normal distribution is assumed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Outliers should be eliminated
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No two variables should be highly correlated (multi-collinearity)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The sample size of the smallest category should be higher than the number of
    predictor values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of independent variables should not exceed the sample size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are two different types of DA, and we will use `lda` from the `MASS` package
    for the linear discriminant function, and `qda` for the quadratic discriminant
    function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us start with the dependent variable being the number of gears, and we
    will use all the other numeric values as independent variables. To make sure that
    we start with a standard `mtcars` dataset not overwritten in the preceding examples,
    let''s clear the namespace and update the gear column to include categories instead
    of the actual numeric values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Due to the low number of observations (and as we have already discussed the
    related options in [Chapter 9](ch09.html "Chapter 9. From Big to Small Data"),
    *From Big to Smaller Data*), we can now set aside conducting the normality and
    other tests. Let's proceed with the actual analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'We call the `lda` function, setting **cross validation** (**CV**) to `TRUE`,
    so that we can test the accuracy of the prediction. The dot in the formula refers
    to all variables except the explicitly mentioned gear:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'So now we can check the accuracy of the predictions by comparing them to the
    original values via the confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'To present relative percentages instead of the raw numbers, we can do some
    quick transformations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'And we can also compute the percentage of missed predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'After all, around 84 percent of the cases got classified into their most likely
    respective classes, which were made up from the actual probabilities that can
    be extracted by the `posterior` element of the list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can run `lda` again without cross validation to see the actual discriminants
    and how the different categories of `gear` are structured:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '![Discriminant analysis](img/2028OS_10_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The numbers in the preceding plot stand for the cars in the `mtcars` dataset
    presented by the actual number of gears. It is really straightforward that the
    elements rendered by the two discriminants highlight the similarity of cars with
    the same number of gears and the difference between those with unequal values
    in the `gear` column.
  prefs: []
  type: TYPE_NORMAL
- en: 'These discriminants can be also extracted from the `d` object by calling `predict`,
    or can directly be rendered on a histogram to see the distribution of this continuous
    variable by the categories of the independent variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '![Discriminant analysis](img/2028OS_10_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although logistic regression was partly covered in [Chapter 6](ch06.html "Chapter 6. Beyond
    the Linear Trend Line (authored by Renata Nemeth and Gergely Toth)"), *Beyond
    the Linear Trend Line (authored by Renata Nemeth and Gergely Toth)*, as it's often
    used to solve classification problems we will revisit this topic again with some
    related examples and some notes on—for example—the multinomial version of logistic
    regression, which was not introduced in the previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Our data often does not meet the requirements of the *discriminant analysis*.
    In such cases, using logistic, logit, or probit regression can be a reasonable
    choice, as these methods are not sensitive to non-normal distribution and unequal
    variances within each group; on the other hand, they require much larger sample
    sizes. For small sample sizes, discriminant analysis is much more reliable.
  prefs: []
  type: TYPE_NORMAL
- en: As a rule of thumb, you should have at least 50 observations for each independent
    variable, which means that, if we want to build a logistic regression model for
    the `mtcars` dataset as earlier, we will need at least 500 observations—but we
    have only 32.
  prefs: []
  type: TYPE_NORMAL
- en: 'To this end, we will restrict this section to one or two quick examples on
    how to conduct a logit regression—for example, to estimate whether a car has automatic
    or manual transmission based on the performance and weight of the automobile:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The most important table from the preceding output is the coefficients table,
    which describes whether the model and the independent variables significantly
    contribute to the value of the independent variable. We can conclude that:'
  prefs: []
  type: TYPE_NORMAL
- en: A 1-unit increase of horsepower increases the log odds of having a manual transmission
    (at least back in 1974, when the data was collected)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A 1-unit increase of weight (in pounds), on the other hand, decreases the same
    log odds by 8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It seems that, despite (or rather due to) the low sample size, the model fits
    the data very well, and the horsepower and weight of the cars can explain whether
    a car has an automatic transmission or manual shift:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'But running the preceding command on the number of gears instead of transmission
    would fail, as logit regression by default expects a dichotomous variable. We
    can overcome this by fitting multiple models on the data, such as verifying whether
    a car has 3/4/5 gears or not with dummy variables, or by fitting a multinomial
    logistic regression. The `nnet` package has a very convenient function to do so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'As expected, it returns a highly fitted model to our small dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'However, due to the small sample size, this model is extremely limited. Before
    proceeding to the next examples, please remove the updated `mtcars` dataset from
    the current R session to avoid unexpected errors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Machine learning algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Machine learning** (**ML**) is a collection of data-driven algorithms that
    work without being explicitly programmed for a specific task. Unlike non-ML algorithms,
    they require (and learn by) the training data. ML algorithms are classified into
    supervised and unsupervised types.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Supervised learning** means that the training data consists of input vectors
    and their corresponding output value as well. This means that the task is to establish
    relationships between inputs and outputs in a historical database, called the
    training set, and thus make it possible to predict outputs for future input values.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, banks have vast databases on previous loan transaction details.
    The input vector is comprised of personal information—such as age, salary, marital
    status and so on—while the output (target) variable shows whether the payment
    deadlines were kept or not. In this case, a supervised algorithm may detect different
    groups of people who may be prone to not being able to keep the deadlines, which
    may serve as a screening of applicants.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning has different goals. As the output values are not available
    in the historical dataset, the aim is to identify underlying correlations between
    the inputs, and define arbitrary groups of cases.
  prefs: []
  type: TYPE_NORMAL
- en: The K-Nearest Neighbors algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**K-Nearest Neighbors** (**k-NN**), unlike the hierarchical or k-means clustering,
    is a supervised classification algorithm. Although it is often confused with k-means
    clustering, k-NN classification is a completely different method. It is mostly
    used in pattern recognition and business analytics. A big advantage of k-NN is
    that it is not sensitive to outliers, and the usage is extremely straightforward—just
    like with most machine learning algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: The main idea of k-NN is that it identifies the *k* number of nearest neighbors
    of the observation in the historical dataset, then it defines the class of the
    observation to match the majority of the neighbors mentioned earlier.
  prefs: []
  type: TYPE_NORMAL
- en: As a sample analysis, we are going to use the `knn` function from the `class`
    package. The `knn` function takes 4 parameters, where `train` and `test` are the
    training and test datasets respectively, `cl` is the class membership of the training
    data, and `k` is the number of neighbors to take into account when classifying
    the elements of the test dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The default value of `k` is `1`, which always works without a problem—although
    usually with a rather low accuracy. When defining a higher number of neighbors
    to be used in the analysis for improved accuracy, it's wise to select an integer
    that is not a multiple of the number of classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s split the `mtcars` dataset into two parts: training and test data. For
    the sake of simplicity, half of the cars will belong to the training set, and
    the other half to the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We used `set.seed` to configure the random generator''s state to a (well) known
    number for the sake of reproducibility: so that the exact same *random* numbers
    will be generated on all machines.'
  prefs: []
  type: TYPE_NORMAL
- en: 'So we sampled 16 integers between 1 and 32 to select 50 percent of the rows
    from the `mtcars` dataset. Some might consider the following `dplyr` (discussed
    in [Chapter 3](ch03.html "Chapter 3. Filtering and Summarizing Data"), *Filtering
    and Summarizing Data* and in [Chapter 4](ch04.html "Chapter 4. Restructuring Data"),
    *Restructuring Data*) code snippet more appealing for the task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Then let''s select the rest of the rows with the difference of the newly created
    `data.frame` compared to the original data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Now we have to define the class memberships of the observations in the training
    data, what we would like to predict in the test dataset in the means of classification.
    To this end, we might use what we have learned in the previous section and, instead
    of an already known characteristic of the cars, we could run a clustering method
    to define the class membership of each element in the training data—but that's
    not something we should do for instructional purposes. You could also run the
    clustering algorithm on your test data as well, right? The major difference between
    the supervised and unsupervised methods is that we have empirical data with the
    former methods to feed the classification models.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, instead, let''s use the number of gears in the cars as the class membership
    and, based on the information found in the training data, let''s predict the number
    of gears in the test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The test cases have just got classified into the preceding classes. We can
    check the accuracy of the classification, for example, by calculating the correlation
    coefficient between the real and predicted number of gears:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Well, this might have been a lot better, especially if the training data had
    been a lot larger. Machine learning algorithms typically ­use millions of rows
    from historical databases, as opposed to our meager dataset with only 16 cases.
    But let''s see where the model failed to provide accurate predictions by computing
    the confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'So it seems that the k-NN classification algorithm could predict the number
    of gears very accurately (one miss out of 13) for all those cars with three or
    four gears, but it ultimately failed with the ones with five gears. This can be
    explained by the number of related cars in the original dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Well, the training data had only two cars with 5 gears, which is indeed really
    tight when it comes to building a model providing accurate predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Classification trees
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An alternative ML method for supervised classification is the use of recursive
    partitioning via decision trees. The great advantage of this method is that visualizing
    decision rules can significantly improve understanding of the underlying data,
    and running the algorithm can be extremely easy in most cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s load the `rpart` package and build a classification tree with the response
    variable being the `gear` function again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: The resulting object is a rather simple *decision tree*—despite the fact that
    we have specified an extremely low `minsplit` parameter, to be able to generate
    more than one node. Running the preceding call without this argument would not
    even result in a decision tree, as the 16 cases of our train data would fit in
    a single node due to the default minimum value of 20 elements per node.
  prefs: []
  type: TYPE_NORMAL
- en: 'But we have built a decision tree where the most important rule to determine
    the number of gears is the rear axle ratio and whether the car has automatic or
    manual transmission:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '![Classification trees](img/2028OS_10_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'To translate this into plain and simple English:'
  prefs: []
  type: TYPE_NORMAL
- en: A car with a high rear axle ratio has four gears
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All other cars with automatic transmission have three gears
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cars with manual shift have five gears
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Well, this rule is indeed very basic due to the low number of cases and the
    confusion matrix also reveals the serious limitation of the model, namely that
    it cannot successfully identify cars with 5 gears:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: But 13 out of 16 cars were classified perfectly, which is quite impressive and
    a bit better than the previous k-NN example!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s improve the preceding code, rather minimalist graph a bit by either
    calling the `main` function from the `rpart.plot` package on the preceding object,
    or loading the `party` package, which provides a very neat plotting function for
    `party` objects. One option might be to call `as.party` on the previously computed
    `ct` object via the `partykit` package; alternatively, we can recreate the classification
    tree with its `ctree` function. Based on the previous experiences, let''s pass
    only the preceding highlighted variables to the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '![Classification trees](img/2028OS_10_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'It seems that this model decides on the number of gears solely based on the
    rear axle ratio with a lot lower accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Now let's see which additional ML algorithms can provide more accurate and/or
    reliable models!
  prefs: []
  type: TYPE_NORMAL
- en: Random forest
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The main idea behind **random forest** is that, instead of building a deep decision
    tree with an ever-growing number of nodes that might risk overfitting the data,
    we instead generate multiple trees to minimize the variance instead of maximizing
    the accuracy. This way the results are expected to be noisier compared to a well-trained
    decision tree, but on average these results are more reliable.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be achieved in a similar way to the preceding examples in R, via for
    example the `randomForest` package, which provides very user-friendly access to
    the classical random forest algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'This function is very convenient to use: it automatically returns the confusion
    matrix and also computes the estimated error rate—although we can of course, generate
    our own based on the other subset of `mtcars`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'But this time, the plotting function returns something new:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '![Random forest](img/2028OS_10_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We see how the mean squared error of the model changes over time as we generate
    more and more decision trees on random subsamples of the training data, where
    the error rate does not seem to change after a while, and there's not much sense
    in generating more than a given number of random samples.
  prefs: []
  type: TYPE_NORMAL
- en: Well, this is really straightforward for such small example, as the combination
    of the possible subsamples is limited. It's also worth mentioning that the error
    rate of cars with five gears (blue line) did not change at all over time, which
    highlights again the main limitation of our training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Other algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Although it would be great to continue discussing the wide variety of related
    ML algorithms (for example, the ID3 and Gradient Boosting algorithms from the
    `gbm` or `xgboost` packages) and how to call, say, Weka from the R console to
    use C4.5, in this chapter I can focus on only one last practical example on how
    to use a general interface for all these algorithms via the `caret` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: This package bundles some really useful functions and methods, which can be
    used as general, algorithm-independent tools for predictive models. This means
    that all the previous models could be run without actually calling the `rpart`,
    `ctree`, or `randomForest` functions, and we can simply rely on the `train` function
    of caret, which takes the algorithm definition as an argument.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a quick example, let''s see how the improved version and open-source implementation
    of C4.5 performs with our training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'This output seems extremely compelling as the error rate is exactly zero, which
    means that we have just created a model that perfectly fits out training data
    with three simple rules:'
  prefs: []
  type: TYPE_NORMAL
- en: Cars with a large rear axle ratio have four gears
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The others have either three (manual shift) or five (automatic transmission)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Well, a second look at the results reveals that we have not found the Holy
    Grail yet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: So the overall performance of this algorithm with our test dataset resulted
    in 12 hits out of the 16 cars, which is a good example of how a single decision
    tree might over-fit the training data.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter introduced a wide variety of ways to cluster and classify data,
    discussed which analysis procedures and models are very important, and generally
    used elements of a data scientist's toolbox. In the next chapter, we will focus
    on a less general, but still important, field— how to analyze graphs and network
    data.
  prefs: []
  type: TYPE_NORMAL
