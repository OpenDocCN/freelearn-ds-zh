- en: Chapter 10. Classification and Clustering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章 分类与聚类
- en: In the previous chapter, we concentrated on how to compress information found
    in a number of continuous variables into a smaller set of numbers, but these statistical
    methods are somewhat limited when we are dealing with categorized data, for example
    when analyzing surveys.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们关注了如何将多个连续变量中找到的信息压缩成更小的数字集，但当处理分类数据时，例如分析调查数据时，这些统计方法就有些局限了。
- en: Although some methods try to convert discrete variables into numeric ones, such
    as by using a number of dummy or indicator variables, in most cases it's simply
    better to think about our research design goals instead of trying to forcibly
    use previously learned methods in the analysis.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管一些方法试图将离散变量转换为数值变量，例如通过使用多个虚拟变量或指示变量，但在大多数情况下，简单地考虑我们的研究设计目标而不是试图强行在分析中使用先前学到的方
    法会更好。
- en: Note
  id: totrans-3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: We can replace a categorical variable with a number of dummy variables by creating
    a new variable for each label of the original discrete variable, and then assign
    *1* to the related column and *0* to all the others. Such values can be used as
    numeric variables in statistical analysis, especially with regression models.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过为原始离散变量的每个标签创建一个新变量来用多个虚拟变量替换一个分类变量，然后为相关列分配*1*，为所有其他列分配*0*。这些值可以用作统计分析中的数值变量，尤其是在回归模型中。
- en: When we analyze a sample and target population via categorical variables, usually
    we are not interested in individual cases, but instead in similar elements and
    groups. Similar elements can be defined as rows in a dataset with similar values
    in the columns.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们通过分类变量分析样本和目标总体时，我们通常对单个案例不感兴趣，而是对相似元素和组感兴趣。相似元素可以定义为数据集中具有相似列值的行。
- en: 'In this chapter, we will discuss different *supervised* and *unsupervised*
    ways to identify similar cases in a dataset, such as:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论不同的*监督*和*无监督*方法来识别数据集中的相似案例，例如：
- en: Hierarchical clustering
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层次聚类
- en: K-means clustering
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K-均值聚类
- en: Some machine learning algorithms
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些机器学习算法
- en: Latent class model
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐含类模型
- en: Discriminant analysis
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 判别分析
- en: Logistic regression
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: Cluster analysis
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类分析
- en: '**Clustering** is an unsupervised data analysis method that is used in diverse
    fields, such as pattern recognition, social sciences, and pharmacy. The aim of
    cluster analysis is to make homogeneous subgroups called clusters, where the objects
    in the same cluster are similar, and the clusters differ from each other.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**聚类**是一种无监督数据分析方法，在多个领域得到应用，如模式识别、社会科学和药学。聚类分析的目标是创建同质子组，称为簇，其中同一簇中的对象相似，而簇之间相互不同。'
- en: Hierarchical clustering
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 层次聚类
- en: Cluster analysis is one of the most well known and popular pattern recognition
    methods; thus, there are many clustering models and algorithms analyzing the distribution,
    density, possible center points, and so on in the dataset. In this section we
    are going to examine some hierarchical clustering methods.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类分析是已知的最著名和最受欢迎的图案识别方法之一；因此，有许多聚类模型和算法在数据集中分析分布、密度、可能的中心点等。在本节中，我们将探讨一些层次聚类方法。
- en: '**Hierarchical clustering** can be either agglomerative or divisive. In agglomerative
    methods every case starts out as an individual cluster, then the closest clusters
    are merged together in an iterative manner, until finally they merge into one
    single cluster, which includes all elements of the original dataset. The biggest
    problem with this approach is that distances between clusters have to be recalculated
    at each iteration, which makes it extremely slow on large data. I''d rather not
    suggest trying to run the following commands on the `hflights` dataset.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '**层次聚类**可以是聚合的或划分的。在聚合方法中，每个案例最初都是一个单独的簇，然后通过迭代方式将最近的簇合并在一起，直到最终合并成一个包含原始数据集所有元素的单一簇。这种方法的最大问题是每次迭代都必须重新计算簇之间的距离，这使得在大数据集上非常慢。我宁愿不推荐尝试在`hflights`数据集上运行以下命令。'
- en: Divisive methods on the other hand take a top-down approach. They start from
    a single cluster, which is then iteratively divided into smaller groups until
    they are all singletons.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，划分方法采用自上而下的方法。它们从一个单一簇开始，然后迭代地将其划分为更小的组，直到它们都是单例。
- en: 'The `stats` package contains the `hclust` function for hierarchical clustering
    that takes a distance matrix as an input. To see how it works, let''s use the
    `mtcars` dataset that we already analyzed in [Chapter 3](ch03.html "Chapter 3. Filtering
    and Summarizing Data"), *Filtering and Summarizing Data* and [Chapter 9](ch09.html
    "Chapter 9. From Big to Small Data"), *From Big to Smaller Data*. The `dist` function
    is also familiar from the latter chapter:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '`stats`包包含用于层次聚类的`hclust`函数，它接受距离矩阵作为输入。为了了解它是如何工作的，让我们使用已经分析过的`mtcars`数据集，它在[第3章](ch03.html
    "第3章。过滤和汇总数据")，*过滤和汇总数据*和[第9章](ch09.html "第9章。从大数据到小数据")，*从大数据到小数据*中已经分析过。`dist`函数在后面的章节中也是熟悉的：'
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Well, this is a way too brief output and only shows that our distance matrix
    included 32 elements and the clustering method. A visual representation of the
    results will be a lot more useful for such a small dataset:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，这是一个过于简略的输出，仅表明我们的距离矩阵包含了32个元素以及聚类方法。对于如此小的数据集，结果的视觉表示将更有用：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![Hierarchical clustering](img/2028OS_10_01.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![层次聚类](img/2028OS_10_01.jpg)'
- en: By plotting this `hclust` object, we obtained a *dendrogram*, which shows how
    the clusters are formed. It can be useful for determining the number of clusters,
    although in datasets with numerous cases it becomes difficult to interpret. A
    horizontal line can be drawn to any given height on the *y* axis so that the *n*
    number of intersections with the line provides a n-cluster solution.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 通过绘制这个`hclust`对象，我们得到了一个*树状图*，它显示了聚类是如何形成的。这有助于确定簇的数量，尽管在包含大量案例的数据集中，它变得难以解释。可以在*y*轴的任何给定高度上画一条水平线，这样与线的交点数量*n*就提供了一个n簇的解决方案。
- en: 'R can provide very convenient ways of visualizing the clusters on the *dendrogram*.
    In the following plot, the red boxes show the cluster membership of a three-cluster
    solution on top of the previous plot:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: R可以提供非常方便的方式来在*树状图*上可视化簇。在下面的图表中，红色方框显示了在前面图表之上的三个簇解决方案的簇成员资格：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![Hierarchical clustering](img/2028OS_10_02.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![层次聚类](img/2028OS_10_02.jpg)'
- en: 'Although this graph looks nice and it is extremely useful to have similar elements
    grouped together, for bigger datasets, it becomes hard to see through. Instead,
    we might be rather interested in the actual cluster membership represented in
    a vector:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个图表看起来很漂亮，并且将相似元素分组在一起非常有用，但对于更大的数据集，它变得难以看透。相反，我们可能更感兴趣的是在向量中实际表示的簇成员资格：
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'And the number of elements in the resulting clusters as a frequency table:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 以及结果簇中元素数量的频率表：
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'It seems that *Cluster 1*, the third cluster on the preceding plot, has the
    most elements. Can you guess how this group differs from the other two clusters?
    Well, those readers who are familiar with car names might be able to guess the
    answer, but let''s see what the numbers actually show:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来，*簇1*，在前面图表中的第三个簇，包含的元素最多。你能猜出这个组与其他两个簇有什么不同吗？好吧，那些熟悉汽车名称的读者可能能够猜出答案，但让我们看看数字实际上显示了什么：
- en: Note
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Please note that we use the `round` function in the following examples to suppress
    the number of decimal places to 1 or 4 in the code output to fit the page width.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在以下示例中，我们使用`round`函数将代码输出中的小数位数限制为1或4，以适应页面宽度。
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: There's a really spectacular difference in the average performance and gas consumption
    between the clusters! What about the standard deviation inside the groups?
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在簇之间的平均性能和油耗之间有一个非常显著的区别！那么组内的标准差如何呢？
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'These values are pretty low compared to the standard deviations in the original
    dataset:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这些值与原始数据集中的标准差相比相当低：
- en: '[PRE7]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'And the same applies when compared to the standard deviation between the groups
    as well:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，当比较组之间的标准差时也是如此：
- en: '[PRE8]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This means that we achieved our original goal to identify similar elements of
    our data and organize those in groups that differ from each other. But why did
    we split the original data into exactly three artificially defined groups? Why
    not two, four, or even more?
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们实现了我们的原始目标，即识别数据中的相似元素并将它们组织成彼此不同的组。但为什么我们要将原始数据分成恰好三个人为定义的组？为什么不是两个、四个，甚至更多？
- en: Determining the ideal number of clusters
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 确定理想簇的数量
- en: 'The `NbClust` package offers a very convenient way to do some exploratory data
    analysis on our data before running the actual cluster analysis. The main function
    of the package can compute 30 different indices, all designed to determine the
    ideal number of groups. These include:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '`NbClust` 包提供了一种非常方便的方法，在运行实际的聚类分析之前，可以对我们的数据进行一些探索性数据分析。该包的主要功能可以计算 30 种不同的指标，所有这些指标都是为了确定理想的组数。这些包括：'
- en: Single link
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单链接
- en: Average
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均值
- en: Complete link
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完全链接
- en: McQuitty
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McQuitty
- en: Centroid (cluster center)
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 质心（聚类中心）
- en: Median
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中位数
- en: K-means
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K-means
- en: Ward
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ward
- en: 'After loading the package, let''s start with a visual method representing the
    possible number of clusters in our data—on a knee plot, which might be familiar
    from [Chapter 9](ch09.html "Chapter 9. From Big to Small Data"), *From Big to
    Smaller Data*, where you can also find some more information about the following
    elbow-rule:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载包之后，让我们从一种表示数据中可能聚类数量的可视化方法开始——膝形图，这可能在[第 9 章](ch09.html "第 9 章。从大数据到小数据")中很熟悉，*从大数据到小数据*，在那里你还可以找到关于以下肘部规则的更多信息：
- en: '[PRE9]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![Determining the ideal number of clusters](img/2028OS_10_03.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![确定理想聚类数量](img/2028OS_10_03.jpg)'
- en: In the preceding plots, we traditionally look for the *elbow*, but the second
    differences plot on the right might be more straightforward for most readers.
    There we are interested in where the most significant peak can be found, which
    suggests that choosing three groups would be ideal when clustering the `mtcars`
    dataset.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图表中，我们传统上寻找 *肘部*，但右侧的第二差分图可能对大多数读者来说更直接。在那里，我们感兴趣的是最显著的峰值在哪里，这表明在聚类 `mtcars`
    数据集时选择三个组将是理想的。
- en: 'Unfortunately, running all `NbClust` methods fails on such a small dataset.
    Thus, for demonstrational purposes, we are now running only a few standard methods
    and filtering the results for the suggested number of clusters via the related
    list element:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，在如此小的数据集上运行所有 `NbClust` 方法都失败了。因此，为了演示目的，我们现在只运行一些标准方法，并通过相关列表元素过滤结果以建议的聚类数量：
- en: '[PRE10]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Both the Hartigan and Krzanowski-Lai indexes suggest sticking to three clusters.
    Let''s view the `iris` dataset as well, which includes a lot more cases with fewer
    numeric columns, so we can run all available methods:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Hartigan 和 Krzanowski-Lai 指数都建议坚持三个聚类。让我们也查看 `iris` 数据集，它包含更多案例且数值列较少，因此我们可以运行所有可用方法：
- en: '[PRE11]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The output summarizes that the ideal number of clusters is three based on the
    13 methods returning that number, five further methods suggest four clusters,
    and a few other cluster numbers were also computed by a much smaller number of
    methods.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 输出总结表明，基于返回该数字的 13 种方法，有五种进一步的方法建议四个聚类，还有一些其他聚类数量也由更少的方法计算得出。
- en: These methods are not only useful with the previously discussed hierarchical
    clustering, but generally used with k-means clustering as well, where the number
    of clusters is to be defined before running the analysis—unlike the hierarchical
    method, where we cut the dendogram after the heavy computations have already been
    run.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法不仅适用于之前讨论过的层次聚类，而且通常也用于 k-means 聚类分析，其中在运行分析之前需要定义聚类数量——与层次方法不同，在重计算已经完成之后我们才切割树状图。
- en: K-means clustering
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: K-means 聚类分析
- en: '**K-means clustering** is a non-hierarchical method first described by MacQueen
    in 1967\. Its big advantage over hierarchical clustering is its great performance.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**K-means 聚类分析**是一种非层次方法，最早由 MacQueen 在 1967 年描述。它相对于层次聚类的最大优势是其出色的性能。'
- en: Note
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Unlike hierarchical cluster analysis, k-means clustering requires you to determine
    the number of clusters before running the actual analysis.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 与层次聚类分析不同，k-means 聚类分析要求你在实际分析运行之前确定聚类数量。
- en: 'The algorithm runs the following steps in a nutshell:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 算法简要运行以下步骤：
- en: Initialize a predefined (*k*) number of randomly chosen centroids in space.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在空间中初始化一个预定义的（*k*）数量随机选择的质心。
- en: Assign each object to the cluster with the closest centroid.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个对象分配到最近的质心的聚类中。
- en: Recalculate centroids.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新计算质心。
- en: Repeat the second and third steps until convergence.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复第二步和第三步，直到收敛。
- en: We are going to use the `kmeans` function from the `stats` package. As k-means
    clustering requires a prior decision on the number of clusters, we can either
    use the `NbClust` function described previously, or we can come up with an arbitrary
    number that fits the goals of the analysis.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`stats`包中的`kmeans`函数。由于k均值聚类需要对簇的数量做出先前的决定，我们可以使用之前描述的`NbClust`函数，或者我们可以提出一个符合分析目标的任意数字。
- en: 'According to the previously defined optimal cluster number in the previous
    section, we are going to stick to three groups, where the within-cluster sum of
    squares ceases to drop significantly:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上一节中定义的最佳簇数量，我们将坚持三个组，其中簇内平方和不再显著下降：
- en: '[PRE12]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The cluster means show some really important characteristics for each cluster,
    which we generated manually for the hierarchical clusters in the previous section.
    We can see that, in the first cluster, the cars have high mpg (low gas consumption),
    on average four cylinders (in contrast to six or eight), rather low performance
    and so on. The output also automatically reveals the actual cluster numbers.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 簇均值显示了每个簇的一些非常重要的特征，我们在上一节中手动为层次聚类生成了这些特征。我们可以看到，在第一个簇中，汽车的平均油耗（低耗油量）很高，平均有四个汽缸（与六个或八个汽缸相比），性能相对较低，等等。输出还自动揭示了实际的簇编号。
- en: 'Let''s compare these to the clusters defined by the hierarchical method:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们比较这些与层次方法定义的簇：
- en: '[PRE13]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The results seem to be pretty stable, right?
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 结果似乎相当稳定，对吧？
- en: Tip
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: The cluster numbers have no meaning and their order is arbitrary. In other words,
    the cluster membership is a nominal variable. Based on this, the preceding R command
    might return `FALSE` instead of `TRUE` when the cluster numbers were allocated
    in a different order, but comparing the actual cluster membership will verify
    that we have found the very same groups. See for example `cbind(cn, k$cluster)`
    to generate a table including both cluster memberships.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 簇编号没有意义，它们的顺序是任意的。换句话说，簇成员资格是一个名义变量。基于此，当簇编号以不同的顺序分配时，前面的R命令可能会返回`FALSE`而不是`TRUE`，但比较实际的簇成员资格将验证我们已经找到了完全相同的群体。例如，查看`cbind(cn,
    k$cluster)`以生成包括簇成员资格的表格。
- en: Visualizing clusters
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化簇
- en: 'Plotting these clusters is also a great way to understand groupings. To this
    end, we will use the `clusplot` function from the `cluster` package. For easier
    understanding, this function reduces the number of dimensions to two, in a similar
    way to when we are conducting a PCA or MDS (described in [Chapter 9](ch09.html
    "Chapter 9. From Big to Small Data"), *From Big to Smaller Data*):'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制这些簇也是理解分组的一种很好的方式。为此，我们将使用`clusplot`函数，该函数来自`cluster`包。为了更容易理解，此函数将维度数量减少到两个，类似于我们在进行PCA或MDS（在第9章中描述，*从大数据到小数据*）时的情况：
- en: '[PRE14]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![Visualizing clusters](img/2028OS_10_04.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![可视化簇](img/2028OS_10_04.jpg)'
- en: As you can see, after the dimension reduction, the two components explain 84.17
    percent of variance, so this small information loss is a great trade-off in favor
    of an easier understanding of the clusters.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，在降维后，两个成分解释了84.17%的方差，因此这种小的信息损失是易于理解簇的一个很好的权衡。
- en: Visualizing the relative density of the ellipses with the `shade` parameter
    can also help us realize how similar the elements of the same groups are. And
    we used the labels argument to show both the points and cluster labels as well.
    Be sure to stick to the default of *0* (no labels) or *4* (only ellipse labels)
    when visualizing large number of elements.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`shade`参数可视化椭圆的相对密度也可以帮助我们了解同一组元素之间的相似性。我们还使用了标签参数来显示点和簇标签。在可视化大量元素时，请务必坚持默认的*0*（无标签）或*4*（仅椭圆标签）。
- en: Latent class models
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 潜在类别模型
- en: '**Latent Class Analysis** (**LCA**) is a method for identifying latent variables
    among polychromous outcome variables. It is similar to factor analysis, but can
    be used with discrete/categorical data. To this end, LCA is mostly used when analyzing
    surveys.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**潜在类别分析**（**LCA**）是一种识别多色结果变量中潜在变量的方法。它与因子分析类似，但可以用于离散/分类数据。为此，LCA主要在分析调查时使用。'
- en: In this section, we are going to use the `poLCA` function from the `poLCA` package.
    It uses expectation-maximization and Newton-Raphson algorithms for finding the
    maximum likelihood for the parameters.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用`poLCA`包中的`poLCA`函数。它使用期望最大化算法和牛顿-拉夫森算法来寻找参数的最大似然值。
- en: 'The `poLCA` function requires the data to be coded as integers starting from
    one or as a factor, otherwise it will produce an error message. To this end, let''s
    transform some of the variables in the `mtcars` dataset to factors:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '`poLCA`函数要求数据编码为从一开始的整数或因子，否则将产生错误信息。为此，让我们将`mtcars`数据集中的某些变量转换为因子：'
- en: '[PRE15]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Tip
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: The preceding command will overwrite the `mtcars` dataset in your current R
    session. To revert to the original dataset for other examples, please delete this
    updated dataset from the session by `rm(mtcars)` if needed.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令将覆盖当前R会话中的`mtcars`数据集。要恢复到本章其他示例中的原始数据集，请通过`rm(mtcars)`删除此更新后的数据集，如果需要的话。
- en: Latent Class Analysis
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 潜在类别分析
- en: 'Now that the data is in an appropriate format, we can conduct the LCA. The
    related function comes with a number of important arguments:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据已处于适当的格式，我们可以进行LCA。相关的函数附带了许多重要的参数：
- en: First, we have to define a formula that describes the model. Depending on the
    formula, we can define LCA (similar to clustering but with discrete variables)
    or **Latent Class Regression** (**LCR**) model.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们必须定义一个描述模型的公式。根据公式，我们可以定义LCA（类似于聚类，但使用离散变量）或**潜在类别回归**（**LCR**）模型。
- en: The `nclass` argument specifies the number of latent classes assumed in the
    model, which is 2 by default. Based on the previous examples in this chapter,
    we will override this to 3.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nclass`参数指定模型中假设的潜在类别数量，默认为2。根据本章前面的示例，我们将将其覆盖为3。'
- en: We can use the `maxiter`, `tol`, `probs.start`, and `nrep` parameters to fine-tune
    the model.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用`maxiter`、`tol`、`probs.start`和`nrep`参数来微调模型。
- en: The `graphs` argument can display or suppress the parameter estimates.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`graphs`参数可以显示或抑制参数估计。'
- en: 'Let''s start with basic LCA of three latent classes defined by all the available
    discrete variables:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从由所有可用离散变量定义的三个潜在类的基本LCA开始：
- en: '[PRE16]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The first part of the output (which can be also accessed via the `probs` element
    of the preceding saved `poLCA` list) summarizes the probabilities of the outcome
    variables by each latent class:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的第一部分（也可以通过先前保存的`poLCA`列表的`probs`元素访问）总结了每个潜在类别对结果变量的概率：
- en: '[PRE17]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'From these probabilities, we can see that all 8-cylinder cars belong to the
    third class, the first one only includes cars with automatic transmission, one
    carburetor, three gears, and so on. The exact same values can be plotted as well
    by setting the graph parameter to `TRUE` in the function call, or by calling the
    plot function directly afterwards:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些概率中，我们可以看到所有8缸汽车都属于第三类，第一类只包括自动变速、一个化油器、三个档位等的汽车。通过在函数调用中将图形参数设置为`TRUE`，或者直接在调用后调用绘图函数，也可以绘制出完全相同的值：
- en: '![Latent Class Analysis](img/2028OS_10_05.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![潜在类别分析](img/2028OS_10_05.jpg)'
- en: 'The plot is also useful in highlighting that the first latent class includes
    only a few elements compared to the other classes (also known as "Estimated class
    population shares"):'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 该图也很有用，可以突出显示与其他类别相比，第一个潜在类别只包含少数几个元素（也称为“估计类别人口份额”）：
- en: '[PRE18]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The `poLCA` object can also reveal a bunch of other important information about
    the results. Just to name a few, let''s see the named list parts of the object,
    which can be extracted via the standard `$` operator:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`poLCA`对象还可以揭示关于结果的其他许多重要信息。仅举几例，让我们看看对象的命名列表部分，可以通过标准的`$`运算符提取：'
- en: The `predclass` returns the most likely class memberships
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`predclass`返回最可能的类别成员资格'
- en: On the other hand, the posterior element is a matrix containing the class membership
    probabilities of each case
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一方面，后验元素是一个矩阵，包含每个案例的类别成员概率
- en: The **Akaike Information Criterion** (`aic`), **Bayesian Information Criterion**
    (`bic`), **deviance** (`Gsq`), and `Chisq` values represent different measures
    of goodness of fit
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**赤池信息准则**（`aic`）、**贝叶斯信息准则**（`bic`）、**偏差**（`Gsq`）和`Chisq`值代表不同的拟合优度度量'
- en: LCR models
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LCR模型
- en: On the other hand, the LCR model is a supervised method, where we are not mainly
    interested in the latent variables explaining our observations at the exploratory
    data analysis scale, but instead we are using training data from which one or
    more covariates predict the probability of the latent class membership.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，LCR模型是一种监督方法，在探索性数据分析尺度上，我们主要不感兴趣的是解释我们观察到的潜在变量，而是使用训练数据，其中一个或多个协变量预测潜在类别成员的概率。
- en: Discriminant analysis
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 判别分析
- en: '**Discriminant Function Analysis** (**DA**) refers to the process of determining
    which continuous independent (predictor) variables discriminate between a discrete
    dependent (response) variable''s categories, which can be considered as a reversed
    **Multivariate Analysis of Variance** (**MANOVA**).'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '**判别函数分析**（**DA**）指的是确定哪些连续的独立（预测）变量可以区分离散的依赖（响应）变量的类别，这可以被视为反向的**多元方差分析**（**MANOVA**）。'
- en: 'This suggests that DA is very similar to logistic regression (see [Chapter
    6](ch06.html "Chapter 6. Beyond the Linear Trend Line (authored by Renata Nemeth
    and Gergely Toth)"), *Beyond the Linear Trend Line (authored by Renata Nemeth
    and Gergely Toth)* and the following section), which is more generally used because
    of its flexibility. While logistic regression can handle both categorical and
    continuous data, DA requires numeric independent variables and has a few further
    requirements that logistic regression does not have:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明DA与逻辑回归非常相似（见[第6章](ch06.html "第6章. 超越线性趋势线 (由Renata Nemeth和Gergely Toth撰写)"),
    *超越线性趋势线 (由Renata Nemeth和Gergely Toth撰写)*以及以下章节），由于其灵活性而更广泛地使用。虽然逻辑回归可以处理分类和连续数据，但DA需要数值独立变量，并且有一些逻辑回归没有的进一步要求：
- en: Normal distribution is assumed
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设正态分布
- en: Outliers should be eliminated
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应消除异常值
- en: No two variables should be highly correlated (multi-collinearity)
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个变量不应高度相关（多重共线性）
- en: The sample size of the smallest category should be higher than the number of
    predictor values
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小类别的样本量应高于预测值的数量
- en: The number of independent variables should not exceed the sample size
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 独立变量的数量不应超过样本量
- en: There are two different types of DA, and we will use `lda` from the `MASS` package
    for the linear discriminant function, and `qda` for the quadratic discriminant
    function.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种不同的DA类型，我们将使用`MASS`包中的`lda`进行线性判别函数，以及`qda`进行二次判别函数。
- en: 'Let us start with the dependent variable being the number of gears, and we
    will use all the other numeric values as independent variables. To make sure that
    we start with a standard `mtcars` dataset not overwritten in the preceding examples,
    let''s clear the namespace and update the gear column to include categories instead
    of the actual numeric values:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从依赖变量是齿轮数量开始，并将所有其他数值作为独立变量。为了确保我们从标准`mtcars`数据集开始，该数据集在前面的示例中没有覆盖，让我们清除命名空间并更新齿轮列，以包含类别而不是实际的数值：
- en: '[PRE19]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Due to the low number of observations (and as we have already discussed the
    related options in [Chapter 9](ch09.html "Chapter 9. From Big to Small Data"),
    *From Big to Smaller Data*), we can now set aside conducting the normality and
    other tests. Let's proceed with the actual analysis.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 由于观察值数量较少（并且我们已经讨论了[第9章](ch09.html "第9章. 从大数据到小数据"), *从大数据到小数据*中的相关选项），我们现在可以暂时搁置进行正态性和其他测试。让我们继续实际分析。
- en: 'We call the `lda` function, setting **cross validation** (**CV**) to `TRUE`,
    so that we can test the accuracy of the prediction. The dot in the formula refers
    to all variables except the explicitly mentioned gear:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们调用`lda`函数，将**交叉验证**（**CV**）设置为`TRUE`，以便我们可以测试预测的准确性。公式中的点代表所有变量，除了明确提到的齿轮：
- en: '[PRE20]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'So now we can check the accuracy of the predictions by comparing them to the
    original values via the confusion matrix:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在我们可以通过比较混淆矩阵来检查预测的准确性：
- en: '[PRE21]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'To present relative percentages instead of the raw numbers, we can do some
    quick transformations:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 要表示相对百分比而不是原始数字，我们可以进行一些快速转换：
- en: '[PRE22]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'And we can also compute the percentage of missed predictions:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以计算未预测的百分比：
- en: '[PRE23]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'After all, around 84 percent of the cases got classified into their most likely
    respective classes, which were made up from the actual probabilities that can
    be extracted by the `posterior` element of the list:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，大约84%的案例被分类到最有可能的相应类别中，这些类别由列表中可以提取的实际概率组成：
- en: '[PRE24]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now we can run `lda` again without cross validation to see the actual discriminants
    and how the different categories of `gear` are structured:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以再次运行`lda`而不进行交叉验证，以查看实际的判别函数以及不同类别的`gear`是如何结构的：
- en: '[PRE25]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![Discriminant analysis](img/2028OS_10_06.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![判别分析](img/2028OS_10_06.jpg)'
- en: The numbers in the preceding plot stand for the cars in the `mtcars` dataset
    presented by the actual number of gears. It is really straightforward that the
    elements rendered by the two discriminants highlight the similarity of cars with
    the same number of gears and the difference between those with unequal values
    in the `gear` column.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 前面图表中的数字代表`mtcars`数据集中由实际档位数表示的汽车。两个判别因子所渲染的元素非常直观地突出了具有相同档位数的汽车之间的相似性，以及`gear`列中值不等的汽车之间的差异。
- en: 'These discriminants can be also extracted from the `d` object by calling `predict`,
    or can directly be rendered on a histogram to see the distribution of this continuous
    variable by the categories of the independent variable:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这些判别因子也可以通过调用`predict`从`d`对象中提取，或者可以直接在直方图上直接渲染，以查看独立变量类别的连续变量的分布：
- en: '[PRE26]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '![Discriminant analysis](img/2028OS_10_07.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![判别分析](img/2028OS_10_07.jpg)'
- en: Logistic regression
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Logistic回归
- en: Although logistic regression was partly covered in [Chapter 6](ch06.html "Chapter 6. Beyond
    the Linear Trend Line (authored by Renata Nemeth and Gergely Toth)"), *Beyond
    the Linear Trend Line (authored by Renata Nemeth and Gergely Toth)*, as it's often
    used to solve classification problems we will revisit this topic again with some
    related examples and some notes on—for example—the multinomial version of logistic
    regression, which was not introduced in the previous chapters.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管logistic回归在[第6章](ch06.html "第6章。超越线性趋势线（由Renata Nemeth和Gergely Toth撰写)")中有所涉及，*超越线性趋势线（由Renata
    Nemeth和Gergely Toth撰写)*，因为它常用于解决分类问题，我们将再次通过一些相关示例和一些注意事项来回顾这个主题，例如logistic回归的多项式版本，这在之前的章节中没有介绍。
- en: Our data often does not meet the requirements of the *discriminant analysis*.
    In such cases, using logistic, logit, or probit regression can be a reasonable
    choice, as these methods are not sensitive to non-normal distribution and unequal
    variances within each group; on the other hand, they require much larger sample
    sizes. For small sample sizes, discriminant analysis is much more reliable.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据通常不符合**判别分析**的要求。在这种情况下，使用logistic、logit或probit回归可以是一个合理的选择，因为这些方法对非正态分布和每个组内不等方差不敏感；另一方面，它们需要更大的样本量。对于小样本量，判别分析要可靠得多。
- en: As a rule of thumb, you should have at least 50 observations for each independent
    variable, which means that, if we want to build a logistic regression model for
    the `mtcars` dataset as earlier, we will need at least 500 observations—but we
    have only 32.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 按照惯例，你应该至少有50个观测值每个自变量，这意味着，如果我们想为之前的`mtcars`数据集建立logistic回归模型，我们至少需要500个观测值——但我们只有32个。
- en: 'To this end, we will restrict this section to one or two quick examples on
    how to conduct a logit regression—for example, to estimate whether a car has automatic
    or manual transmission based on the performance and weight of the automobile:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将本节限制在一两个快速示例上，说明如何进行logit回归——例如，根据汽车的性能和重量来估计汽车是自动变速箱还是手动变速箱：
- en: '[PRE27]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The most important table from the preceding output is the coefficients table,
    which describes whether the model and the independent variables significantly
    contribute to the value of the independent variable. We can conclude that:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 前面输出中最重要的表格是系数表，它描述了模型和自变量是否对自变量的值有显著贡献。我们可以得出以下结论：
- en: A 1-unit increase of horsepower increases the log odds of having a manual transmission
    (at least back in 1974, when the data was collected)
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 马力每增加1单位，拥有手动变速箱的对数几率就会增加（至少在1974年数据收集时是这样的）
- en: A 1-unit increase of weight (in pounds), on the other hand, decreases the same
    log odds by 8
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一方面，重量（以磅为单位）每增加1单位，相同的对数几率就会减少8
- en: 'It seems that, despite (or rather due to) the low sample size, the model fits
    the data very well, and the horsepower and weight of the cars can explain whether
    a car has an automatic transmission or manual shift:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来，尽管（或者更确切地说，正因为）样本量较小，模型与数据拟合得非常好，汽车的马力与重量可以解释汽车是自动变速箱还是手动换挡：
- en: '[PRE28]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'But running the preceding command on the number of gears instead of transmission
    would fail, as logit regression by default expects a dichotomous variable. We
    can overcome this by fitting multiple models on the data, such as verifying whether
    a car has 3/4/5 gears or not with dummy variables, or by fitting a multinomial
    logistic regression. The `nnet` package has a very convenient function to do so:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 但在齿轮数量而不是变速器上运行前面的命令会失败，因为默认情况下，逻辑回归期望的是二元变量。我们可以通过在数据上拟合多个模型来克服这个问题，例如使用虚拟变量来验证一辆车是否有3/4/5个齿轮，或者通过拟合多项式逻辑回归。`nnet`包有一个非常方便的函数来完成这个任务：
- en: '[PRE29]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'As expected, it returns a highly fitted model to our small dataset:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，它返回了一个高度拟合我们小数据集的模型：
- en: '[PRE30]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'However, due to the small sample size, this model is extremely limited. Before
    proceeding to the next examples, please remove the updated `mtcars` dataset from
    the current R session to avoid unexpected errors:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于样本量小，这个模型非常有限。在继续下一个示例之前，请从当前的R会话中删除更新的`mtcars`数据集，以避免意外错误：
- en: '[PRE31]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Machine learning algorithms
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习算法
- en: '**Machine learning** (**ML**) is a collection of data-driven algorithms that
    work without being explicitly programmed for a specific task. Unlike non-ML algorithms,
    they require (and learn by) the training data. ML algorithms are classified into
    supervised and unsupervised types.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器学习**（**ML**）是一组数据驱动算法，它们在没有为特定任务明确编程的情况下工作。与无机器学习算法不同，它们需要（并通过）训练数据来学习。机器学习算法分为监督学习和无监督学习两种类型。'
- en: '**Supervised learning** means that the training data consists of input vectors
    and their corresponding output value as well. This means that the task is to establish
    relationships between inputs and outputs in a historical database, called the
    training set, and thus make it possible to predict outputs for future input values.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '**监督学习**意味着训练数据包括输入向量和它们对应的输出值。这意味着任务是建立历史数据库中输入和输出之间的关系，称为训练集，从而能够预测未来输入值的输出。'
- en: For example, banks have vast databases on previous loan transaction details.
    The input vector is comprised of personal information—such as age, salary, marital
    status and so on—while the output (target) variable shows whether the payment
    deadlines were kept or not. In this case, a supervised algorithm may detect different
    groups of people who may be prone to not being able to keep the deadlines, which
    may serve as a screening of applicants.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，银行拥有大量关于以前贷款交易详情的数据库。输入向量包括个人信息，如年龄、薪水、婚姻状况等，而输出（目标）变量显示是否按时支付了款项。在这种情况下，监督算法可能会检测到不同的人群，这些人可能无法按时支付，这可能作为申请人的筛选。
- en: Unsupervised learning has different goals. As the output values are not available
    in the historical dataset, the aim is to identify underlying correlations between
    the inputs, and define arbitrary groups of cases.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习有不同的目标。由于历史数据集中没有输出值，目标是识别输入之间的潜在相关性，并定义任意案例组。
- en: The K-Nearest Neighbors algorithm
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: K-Nearest Neighbors算法
- en: '**K-Nearest Neighbors** (**k-NN**), unlike the hierarchical or k-means clustering,
    is a supervised classification algorithm. Although it is often confused with k-means
    clustering, k-NN classification is a completely different method. It is mostly
    used in pattern recognition and business analytics. A big advantage of k-NN is
    that it is not sensitive to outliers, and the usage is extremely straightforward—just
    like with most machine learning algorithms.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '**K-Nearest Neighbors**（**k-NN**），与层次聚类或k-means聚类不同，是一种监督分类算法。尽管它经常与k-means聚类混淆，但k-NN分类是一种完全不同的方法。它主要用于模式识别和商业分析。k-NN的一个大优点是它对异常值不敏感，使用起来极其简单——就像大多数机器学习算法一样。'
- en: The main idea of k-NN is that it identifies the *k* number of nearest neighbors
    of the observation in the historical dataset, then it defines the class of the
    observation to match the majority of the neighbors mentioned earlier.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: k-NN的主要思想是它识别历史数据集中观察到的k个最近邻，然后定义观察到的类别以匹配前面提到的多数邻居。
- en: As a sample analysis, we are going to use the `knn` function from the `class`
    package. The `knn` function takes 4 parameters, where `train` and `test` are the
    training and test datasets respectively, `cl` is the class membership of the training
    data, and `k` is the number of neighbors to take into account when classifying
    the elements of the test dataset.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 作为样本分析，我们将使用来自 `class` 包的 `knn` 函数。`knn` 函数接受 4 个参数，其中 `train` 和 `test` 分别是训练集和测试集，`cl`
    是训练数据的类别成员资格，而 `k` 是在分类测试数据集中的元素时考虑的邻居数量。
- en: The default value of `k` is `1`, which always works without a problem—although
    usually with a rather low accuracy. When defining a higher number of neighbors
    to be used in the analysis for improved accuracy, it's wise to select an integer
    that is not a multiple of the number of classes.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '`k` 的默认值是 `1`，这通常没有问题——尽管通常准确性相当低。当定义用于分析的更高数量的邻居以提高准确性时，选择一个不是类别数量倍数的整数是明智的。'
- en: 'Let''s split the `mtcars` dataset into two parts: training and test data. For
    the sake of simplicity, half of the cars will belong to the training set, and
    the other half to the test set:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将 `mtcars` 数据集分成两部分：训练数据和测试数据。为了简单起见，一半的汽车将属于训练集，另一半将属于测试集：
- en: '[PRE32]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Tip
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'We used `set.seed` to configure the random generator''s state to a (well) known
    number for the sake of reproducibility: so that the exact same *random* numbers
    will be generated on all machines.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `set.seed` 配置随机生成器的状态为一个（已知）的数字，以便于可重复性：这样，所有机器上都会生成完全相同的 *随机* 数字。
- en: 'So we sampled 16 integers between 1 and 32 to select 50 percent of the rows
    from the `mtcars` dataset. Some might consider the following `dplyr` (discussed
    in [Chapter 3](ch03.html "Chapter 3. Filtering and Summarizing Data"), *Filtering
    and Summarizing Data* and in [Chapter 4](ch04.html "Chapter 4. Restructuring Data"),
    *Restructuring Data*) code snippet more appealing for the task:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们采样了 16 个介于 1 和 32 之间的整数，以从 `mtcars` 数据集中选择 50% 的行。有些人可能会认为以下 `dplyr`（在第
    3 章[过滤和汇总数据](ch03.html "第 3 章。过滤和汇总数据")和第 4 章[重构数据](ch04.html "第 4 章。重构数据")中讨论）代码片段更适合这项任务：
- en: '[PRE33]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Then let''s select the rest of the rows with the difference of the newly created
    `data.frame` compared to the original data:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，让我们选择与原始数据相比新创建的 `data.frame` 差异的其他行：
- en: '[PRE34]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Now we have to define the class memberships of the observations in the training
    data, what we would like to predict in the test dataset in the means of classification.
    To this end, we might use what we have learned in the previous section and, instead
    of an already known characteristic of the cars, we could run a clustering method
    to define the class membership of each element in the training data—but that's
    not something we should do for instructional purposes. You could also run the
    clustering algorithm on your test data as well, right? The major difference between
    the supervised and unsupervised methods is that we have empirical data with the
    former methods to feed the classification models.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们必须定义训练数据中观测值的类别成员资格，这是我们希望在测试数据集中通过分类进行预测的内容。为此，我们可以使用我们在上一节中学到的知识，而不是汽车已经知道的特征，我们可以运行聚类方法来定义训练数据中每个元素的类别成员资格——但这不是我们应该用于教学目的的事情。你也可以在你的测试数据上运行聚类算法，对吧？监督学习和无监督方法之间的主要区别在于，前者方法我们有经验数据来喂养分类模型。
- en: 'So, instead, let''s use the number of gears in the cars as the class membership
    and, based on the information found in the training data, let''s predict the number
    of gears in the test dataset:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们使用汽车中的齿轮数作为类别成员资格，并根据训练数据中找到的信息，预测测试数据集中的齿轮数：
- en: '[PRE35]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The test cases have just got classified into the preceding classes. We can
    check the accuracy of the classification, for example, by calculating the correlation
    coefficient between the real and predicted number of gears:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 测试用例刚刚被分类到前面的类别中。我们可以通过计算实际齿轮数与预测齿轮数之间的相关系数来检查分类的准确性：
- en: '[PRE36]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Well, this might have been a lot better, especially if the training data had
    been a lot larger. Machine learning algorithms typically ­use millions of rows
    from historical databases, as opposed to our meager dataset with only 16 cases.
    But let''s see where the model failed to provide accurate predictions by computing
    the confusion matrix:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，这可能要好得多，特别是如果训练数据量很大。机器学习算法通常使用历史数据库中的数百万行数据，而我们的数据集只有 16 个案例。但让我们通过计算混淆矩阵来看看模型在哪里未能提供准确的预测：
- en: '[PRE37]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'So it seems that the k-NN classification algorithm could predict the number
    of gears very accurately (one miss out of 13) for all those cars with three or
    four gears, but it ultimately failed with the ones with five gears. This can be
    explained by the number of related cars in the original dataset:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，似乎k-NN分类算法可以非常准确地预测具有三个或四个档位的所有汽车的档位数（13个中只有一个错误），但对于具有五个档位的汽车最终失败了。这可以通过原始数据集中相关汽车的数量来解释：
- en: '[PRE38]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Well, the training data had only two cars with 5 gears, which is indeed really
    tight when it comes to building a model providing accurate predictions.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，训练数据中只有两辆汽车有5个档位，这在构建一个提供准确预测的模型时确实非常紧张。
- en: Classification trees
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分类树
- en: An alternative ML method for supervised classification is the use of recursive
    partitioning via decision trees. The great advantage of this method is that visualizing
    decision rules can significantly improve understanding of the underlying data,
    and running the algorithm can be extremely easy in most cases.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种用于监督分类的机器学习方法是通过决策树进行递归分区。这种方法的一个巨大优势是，可视化决策规则可以显著提高对底层数据的理解，并且在大多数情况下运行算法可以非常简单。
- en: 'Let''s load the `rpart` package and build a classification tree with the response
    variable being the `gear` function again:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们加载`rpart`包并再次使用响应变量`gear`函数构建一个分类树：
- en: '[PRE39]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The resulting object is a rather simple *decision tree*—despite the fact that
    we have specified an extremely low `minsplit` parameter, to be able to generate
    more than one node. Running the preceding call without this argument would not
    even result in a decision tree, as the 16 cases of our train data would fit in
    a single node due to the default minimum value of 20 elements per node.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的对象是一个相当简单的*决策树*——尽管我们指定了一个非常低的`minsplit`参数，以便能够生成多个节点。在没有这个参数的情况下运行前面的调用甚至不会产生决策树，因为我们的训练数据中的16个案例会由于节点默认的最小值是20个元素而适合在一个节点中。
- en: 'But we have built a decision tree where the most important rule to determine
    the number of gears is the rear axle ratio and whether the car has automatic or
    manual transmission:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们已经构建了一个决策树，其中确定档位数的最重要的规则是后轴比以及汽车是否有自动或手动变速箱：
- en: '[PRE40]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '![Classification trees](img/2028OS_10_08.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![分类树](img/2028OS_10_08.jpg)'
- en: 'To translate this into plain and simple English:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 将其翻译成简单明了的英语：
- en: A car with a high rear axle ratio has four gears
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高后轴比的汽车有四个档位
- en: All other cars with automatic transmission have three gears
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有其他自动挡汽车有三个档位
- en: Cars with manual shift have five gears
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手动换挡的汽车有五个档位
- en: 'Well, this rule is indeed very basic due to the low number of cases and the
    confusion matrix also reveals the serious limitation of the model, namely that
    it cannot successfully identify cars with 5 gears:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，由于案例数量较少，这个规则确实非常基础，混淆矩阵也揭示了模型的严重局限性，即它无法成功识别具有5个档位的汽车：
- en: '[PRE41]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: But 13 out of 16 cars were classified perfectly, which is quite impressive and
    a bit better than the previous k-NN example!
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 但有13辆中的16辆被完美分类，这相当令人印象深刻，并且比之前的k-NN示例要好一些！
- en: 'Let''s improve the preceding code, rather minimalist graph a bit by either
    calling the `main` function from the `rpart.plot` package on the preceding object,
    or loading the `party` package, which provides a very neat plotting function for
    `party` objects. One option might be to call `as.party` on the previously computed
    `ct` object via the `partykit` package; alternatively, we can recreate the classification
    tree with its `ctree` function. Based on the previous experiences, let''s pass
    only the preceding highlighted variables to the model:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们改进前面代码，稍微改进一下这个非常简约的图形，可以通过从`rpart.plot`包中调用前面的`main`函数，或者加载`party`包来实现，该包为`party`对象提供了非常整洁的绘图函数。一个选项可能是通过`partykit`包在先前计算的`ct`对象上调用`as.party`；或者，我们可以使用其`ctree`函数重新创建分类树。根据之前的经验，让我们只将前面突出显示的变量传递给模型：
- en: '[PRE42]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '![Classification trees](img/2028OS_10_09.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![分类树](img/2028OS_10_09.jpg)'
- en: 'It seems that this model decides on the number of gears solely based on the
    rear axle ratio with a lot lower accuracy:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来这个模型完全基于后轴比来决定档位数，准确度相当低：
- en: '[PRE43]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Now let's see which additional ML algorithms can provide more accurate and/or
    reliable models!
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看哪些额外的机器学习算法可以提供更准确和/或更可靠的模型！
- en: Random forest
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机森林
- en: The main idea behind **random forest** is that, instead of building a deep decision
    tree with an ever-growing number of nodes that might risk overfitting the data,
    we instead generate multiple trees to minimize the variance instead of maximizing
    the accuracy. This way the results are expected to be noisier compared to a well-trained
    decision tree, but on average these results are more reliable.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林背后的主要思想是，我们不是构建一个具有不断增长节点数量的深度决策树，这可能会风险过度拟合数据，而是生成多个树来最小化方差而不是最大化准确率。这样，预期的结果与训练良好的决策树相比可能会更嘈杂，但平均而言，这些结果更可靠。
- en: 'This can be achieved in a similar way to the preceding examples in R, via for
    example the `randomForest` package, which provides very user-friendly access to
    the classical random forest algorithm:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过与R中先前的示例类似的方式实现，例如通过`randomForest`包，该包提供了对经典随机森林算法非常用户友好的访问：
- en: '[PRE44]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'This function is very convenient to use: it automatically returns the confusion
    matrix and also computes the estimated error rate—although we can of course, generate
    our own based on the other subset of `mtcars`:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数非常方便使用：它自动返回混淆矩阵，并计算估计的错误率——尽管我们当然可以根据`mtcars`的其他子集生成自己的：
- en: '[PRE45]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'But this time, the plotting function returns something new:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 但这次，绘图函数返回了一些新内容：
- en: '[PRE46]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '![Random forest](img/2028OS_10_10.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![随机森林](img/2028OS_10_10.jpg)'
- en: We see how the mean squared error of the model changes over time as we generate
    more and more decision trees on random subsamples of the training data, where
    the error rate does not seem to change after a while, and there's not much sense
    in generating more than a given number of random samples.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，当我们对训练数据的随机子样本生成越来越多的决策树时，模型的均方误差是如何随时间变化的，经过一段时间后，错误率似乎没有变化，生成超过给定数量的随机样本似乎没有太多意义。
- en: Well, this is really straightforward for such small example, as the combination
    of the possible subsamples is limited. It's also worth mentioning that the error
    rate of cars with five gears (blue line) did not change at all over time, which
    highlights again the main limitation of our training dataset.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，对于如此小的例子来说，这确实非常直接，因为可能的子样本组合是有限的。还值得一提的是，具有五个档位（蓝色线）的汽车的错误率在时间上没有任何变化，这再次突出了我们训练数据集的主要限制。
- en: Other algorithms
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他算法
- en: 'Although it would be great to continue discussing the wide variety of related
    ML algorithms (for example, the ID3 and Gradient Boosting algorithms from the
    `gbm` or `xgboost` packages) and how to call, say, Weka from the R console to
    use C4.5, in this chapter I can focus on only one last practical example on how
    to use a general interface for all these algorithms via the `caret` package:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管继续讨论广泛的关联机器学习算法（例如来自`gbm`或`xgboost`包的ID3和梯度提升算法）以及如何从R控制台调用Weka来使用C4.5会很棒，但在本章中，我只能专注于最后一个实际示例，即如何通过`caret`包使用这些算法的通用接口：
- en: '[PRE47]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: This package bundles some really useful functions and methods, which can be
    used as general, algorithm-independent tools for predictive models. This means
    that all the previous models could be run without actually calling the `rpart`,
    `ctree`, or `randomForest` functions, and we can simply rely on the `train` function
    of caret, which takes the algorithm definition as an argument.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 这个包捆绑了一些非常有用的函数和方法，可以作为预测模型的通用、算法无关的工具使用。这意味着所有之前的模型都可以在不实际调用`rpart`、`ctree`或`randomForest`函数的情况下运行，我们只需简单地依赖caret的`train`函数，该函数将算法定义作为参数。
- en: 'For a quick example, let''s see how the improved version and open-source implementation
    of C4.5 performs with our training data:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 为了快速举例，让我们看看改进版和开源的C4.5实现在我们训练数据上的表现：
- en: '[PRE48]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'This output seems extremely compelling as the error rate is exactly zero, which
    means that we have just created a model that perfectly fits out training data
    with three simple rules:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 这个输出看起来非常令人信服，因为错误率正好为零，这意味着我们刚刚创建了一个模型，它仅用三条简单规则就完美地拟合了我们的训练数据：
- en: Cars with a large rear axle ratio have four gears
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 车辆后轴比大的有四个档位
- en: The others have either three (manual shift) or five (automatic transmission)
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他车辆要么有三个（手动变速）要么有五个（自动变速）
- en: 'Well, a second look at the results reveals that we have not found the Holy
    Grail yet:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，再次审视结果揭示，我们还没有找到圣杯：
- en: '[PRE49]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: So the overall performance of this algorithm with our test dataset resulted
    in 12 hits out of the 16 cars, which is a good example of how a single decision
    tree might over-fit the training data.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这个算法在我们测试数据集上的整体性能结果是16辆车中有12次命中，这是一个单棵决策树可能过度拟合训练数据的良好例子。
- en: Summary
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter introduced a wide variety of ways to cluster and classify data,
    discussed which analysis procedures and models are very important, and generally
    used elements of a data scientist's toolbox. In the next chapter, we will focus
    on a less general, but still important, field— how to analyze graphs and network
    data.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了多种聚类和分类数据的方法，讨论了哪些分析程序和模型非常重要，并通常使用了数据科学家工具箱的元素。在下一章中，我们将关注一个不那么通用但仍然重要的领域——如何分析图和网络数据。
