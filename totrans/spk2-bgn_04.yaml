- en: Chapter 4. Spark Programming with R
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: R is a popular statistical computing programming language used by many and freely
    available under the **General Public License** (**GNU**). R originated from the
    programming language S, created by John Chambers. R was developed by Ross Ihaka
    and Robert Gentleman. Many data scientists use R for their computing needs. R
    has inherent support for many statistical functions and many scalar data types,
    and has composite data structures for vectors, matrices, data frames, and more,
    for statistical computation. R is highly extensible and for that, external packages
    can be created. Once an external package is created, it has to be installed and
    loaded for any program to use it. A collection of such packages under a directory
    forms an R library. In other words, R comes with a set of base packages and additional
    packages that can be installed on top of it to form the required library for the
    desired computing needs. In addition to functions, datasets can also be packaged
    in R packages.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: The need for SparkR
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Essentials of R
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataframes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aggregations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-datasource joins with SparkR
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The need for SparkR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A plain base R installation cannot interact with Spark. The **SparkR** package
    exposes all the required objects and functions for R to talk to the Spark ecosystem.
    Compared to Scala, Java, and Python, the Spark programming in R is different and
    the SparkR package mainly exposes R API for DataFrame-based Spark SQL programming.
    At the moment, R cannot be used to manipulate the RDDs of Spark directly. So for
    all practical purposes, the R API for Spark has access to only Spark SQL abstractions.
    The Spark **MLlib** can also be programmed using R because Spark MLlib uses DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: How is SparkR going to help the data scientists to do better data processing?
    The base R installation mandates that all the data to be stored (or accessible)
    on the computer where R is installed. The data processing occurs on the single
    computer on which the R installation is available. Moreover, if the data size
    is more than the main memory available on the computer, R will not be able to
    do the required processing. With the SparkR package, there is access to a whole
    new world of a cluster of nodes for data storage and for doing data processing.
    With the help of SparkR package, R can be used to access the Spark DataFrames
    as well as R DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: It is very important to know the distinction between the two types of data frames,
    R Dataframes and Spark Dataframes. An R DataFrame is completely local and a data
    structure of the R language. A Spark DataFrame is a parallel collection of structured
    data managed by the Spark infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: An R DataFrame can be converted to a Spark DataFrame and a Spark DataFrame can
    be converted to an R DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: When a Spark DataFrame is converted to an R DataFrame, it should fit in the
    available memory of the computer. This conversion is a great feature and there
    is a need to do so. By converting an R DataFrame to a Spark DataFrame, the data
    can be distributed and processed in parallel. By converting a Spark DataFrame
    to an R DataFrame, a lot of computations, charting, and plotting that is done
    by other R functions can be done. In a nutshell, the SparkR package brings the
    power of distributed and parallel computing capabilities to R.
  prefs: []
  type: TYPE_NORMAL
- en: Often, when performing data processing with R, because of the sheer size of
    the data and the need to fit it into the main memory of the computer, the data
    processing is done in multiple batches and the results are consolidated to compute
    the final results. This kind of multi-batch processing can be completely avoided
    if Spark with R is used to process the data.
  prefs: []
  type: TYPE_NORMAL
- en: Often, reporting, charting, and plotting are done on the aggregated and summarized
    raw data. The raw data size can be huge and need not fit into one computer. In
    such cases, Spark with R can be used to process the entire raw data and finally,
    the aggregated and summarized data can be used to produce the reports, charts,
    or plots.
  prefs: []
  type: TYPE_NORMAL
- en: Because of the inability to process huge amounts of data and for doing data
    analysis with R, many times, ETL tools are made to use for doing the pre-processing
    or transformations on the raw data, and only in the final stage is the data analysis
    done using R. Because of Spark's ability to process data at scale, Spark with
    R can replace the entire ETL pipeline and do the desired data analysis with R.
  prefs: []
  type: TYPE_NORMAL
- en: Many R users use the **dplyr **R package for manipulating datasets in R. This
    package provides fast data manipulation capabilities with R DataFrames. Just like
    manipulating local R DataFrames, it can access data from some of the RDBMS tables
    too. Apart from these primitive data manipulation capabilities, it lacks many
    of the data processing features available in Spark. So Spark with R is a good
    alternative to packages such as dplyr.
  prefs: []
  type: TYPE_NORMAL
- en: The SparkR package is yet another R package, but that is not stopping anybody
    from using any of the R packages that are already being used. At the same time,
    it supplements the data processing capability of R manifold by making use of the
    huge data processing capabilities of Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Basics of the R language
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is not in any way a guide to R programming. But, it is important to touch
    upon the basics of R as a language very briefly for the benefit of those who are
    not familiar with R to appreciate what is being covered in this chapter. A very
    basic introduction to the language features is covered here.
  prefs: []
  type: TYPE_NORMAL
- en: R comes with a few built-in data types to hold numerical values, character values,
    and boolean values. There are composite data structures available and the most
    important ones are, namely, vectors, lists, matrices, and data frames. A vector
    consists of ordered collection of values of a given type. A list is an ordered
    collection of elements that can be of different types. For example, a list can
    hold two vectors, of which one is a vector containing numerical values and the
    the other is a vector containing boolean values. A matrix is a two-dimensional
    data structure holding numerical values in rows and columns. A data frame is a
    two-dimensional data structure containing rows and columns, where columns can
    have different data types but a single column cannot hold different data types.
  prefs: []
  type: TYPE_NORMAL
- en: 'Code samples of using a variable (a special case of vector), a numeric vector,
    a character vector, a list, a matrix, a data frame, and assigning column names
    to a data frame are as follows. The variable names are given as self-descriptive
    as possible for the reader to understand without the help of additional explanation.
    The following code snippet run on a regular R REPL gives an idea of the data structures
    of R:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The main topic of discussion here is going to be revolving around data frames.
    Some of the functions that are commonly used with data frames are demonstrated
    here. All these commands are to be executed on the regular R REPL as a continuation
    of the session that executed the preceding code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: DataFrames in R and Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When working with Spark using R, it is very easy to get confused with the DataFrame
    data structure. As mentioned earlier, it is there in R and in Spark SQL. The following
    code snippet deals with converting an R DataFrame to a Spark DataFrame and vice
    versa. This is going to be a very common operation when programming Spark with
    R. The following code snippet is to be executed in the R REPL of Spark. From now
    on, all the references to the R REPL are with respect to the R REPL of Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: There is no complete compatibility and interoperability between an R DataFrame
    and a Spark DataFrame in terms of the supported functions.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As a good practice, it is better to name the R DataFrame and Spark DataFrame
    with agreed  conventions in R programs in order to have a distinction between
    the two different types. Not all the functions that are supported on R DataFrames
    are not supported on Spark DataFrames and vice versa. Always refer to the right
    version of the R API for Spark.
  prefs: []
  type: TYPE_NORMAL
- en: 'Those who use a lot of charting and plotting have to be extra careful while
    dealing with R DataFrames in conjunction with Spark DataFrames. The charting and
    plotting of R works with only R DataFrames. If there is a need to produce charts
    or plots with the data processed by Spark and available in Spark DataFrame, it
    has to be converted to an R DataFrame to proceed with the charting and plotting.
    The following code snippet will give an idea this. We will use the faithful dataset
    again for elucidation purposes in the R REPL of Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The figure here is used jut to demonstrate that the Spark DataFrame cannot
    be used to do charting and R DataFrame has to be used for the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '![DataFrames in R and Spark](img/image_04_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1
  prefs: []
  type: TYPE_NORMAL
- en: The charting and plotting library, when used with Spark DataFrame, gave an error
    because of the incompatibility of the data types.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The most important aspect to have in mind is that an R DataFrame is an in-memory
    resident data structure, while a Spark DataFrame is a parallel collection of datasets
    distributed across a cluster of nodes. So, all the functions that use R DataFrames
    need not work with Spark DataFrames and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s revisit the bigger picture again, as given in *Figure 2*, to set the
    context and see what is being discussed here before getting into and taking up
    the use cases. In the previous chapter, the same subject was introduced by using
    the programming languages Scala and Python. In this chapter, the same set of use
    cases used in the Spark SQL programming will be implemented using R:'
  prefs: []
  type: TYPE_NORMAL
- en: '![DataFrames in R and Spark](img/image_04_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2
  prefs: []
  type: TYPE_NORMAL
- en: The use cases that are going to be discussed here will be demonstrating the
    ability to mix SQL queries with Spark programs in R. Multiple data sources will
    be chosen, data will be read from those sources using DataFrame, and uniform data
    access will be demonstrated.
  prefs: []
  type: TYPE_NORMAL
- en: Spark DataFrame programming with R
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The use cases selected for elucidating the Spark SQL way of programming with
    DataFrame are given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The transaction records are comma-separated values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Filter out only the good transaction records from the list. The account number
    should start with `SB` and the transaction amount should be greater than zero.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find all the high value transaction records with a transaction amount greater
    than 1000.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find all the transaction records where the account number is bad.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find all the transaction records where the transaction amount is less than or
    equal to zero.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find a combined list of all the bad transaction records.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find the total of all the transaction amounts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find the maximum of all the transaction amounts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find the minimum of all the transaction amounts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find all the good account numbers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is exactly the same set of use cases that were used in the previous chapter,
    but here, the programming model is totally different. Here, the programming is
    done in R. Using this set of use cases, two types of programming model are demonstrated
    here. One is using the SQL queries and other is using DataFrame APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The data files needed for running the following code snippets are available
    from the same directory where the R code is kept.
  prefs: []
  type: TYPE_NORMAL
- en: In the following code snippets, data is read from files located in the filesystem.
    Since all these code snippets are executed from the R REPL of Spark, all the data
    files are to be kept in the `$SPARK_HOME` directory.
  prefs: []
  type: TYPE_NORMAL
- en: Programming with SQL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At the R REPL prompt, try the following statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The retail banking transaction records come with account number, transaction
    amount are processed using SparkSQL to get the desired results of the use cases.
    Here is the summary of what the preceding script did:'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike other programming languages supported with Spark, R doesn't have an RDD
    programming capability. So, instead of going with the construction of RDD from
    collections, the data is read from the JSON file containing the transaction records.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Spark DataFrame is created from the JSON file.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A table is registered with the DataFrame with a name. This registered name of
    the table can be used in SQL statements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, all the other activities are issuing SQL statements using the SQL function
    from the SparkR package.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The result of all these SQL statements is stored as Spark DataFrames, and showDF
    function is used to extract the values to the calling R program.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The aggregate value calculations are also done through the SQL statements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The DataFrame contents are displayed in table format using the the `showDF`
    function of SparkR.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A detailed view of the structure of the DataFrame is displayed using the print
    function. This is akin to the describe command of the database tables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the preceding R code, the style of programming is different as compared to
    the Scala code. That is because it is an R program. Using the SparkR library,
    the Spark features are being used. But the functions and other abstractions are
    not in a really different style.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Throughout this chapter, there will be instances where DataFrames are used.
    It is very easy to get confused by which is the R DataFrame and which is the Spark
    DataFrame. Hence, care is taken to specifically mention by qualifying the DataFrame,
    such as R DataFrame and Spark DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Programming with R DataFrame API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, the code snippets will be run in the same R REPL. Like the
    preceding code snippets, initially, some DataFrame-specific basic commands are
    given. These are used regularly to see the contents and for doing some sanity
    tests on the DataFrame and its contents. These are commands that are typically
    used in the exploratory stage of the data analysis quite often to get more insight
    into the structure and contents of the underlying data.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the R REPL prompt, try the following statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the summary of what the preceding script did from a DataFrame API perspective:'
  prefs: []
  type: TYPE_NORMAL
- en: The DataFrame containing the superset of data used in the preceding section
    is used here.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Filtering of the records is demonstrated next. Here, the most important aspect
    to notice is that the filter predicate is to be given exactly like the predicates
    in the SQL statements. Filters can not be chained.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The aggregation methods are calculated next.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The final statements in this set are doing the selection, filtering, choosing
    distinct records, and ordering.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the transaction records are persisted in Parquet format, read from
    the Parquet store, and created a Spark DataFrame. More details on the persistence
    formats have been covered in the previous chapter and the concepts remain the
    same. Only the DataFrame API syntax is different.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this code snippet, the Parquet format data is stored in the current directory,
    from where the corresponding REPL is invoked. When it is run as a Spark program,
    the directory again will be the current directory from where the Spark submit
    is invoked.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The last few statements are about the persisting of the DataFrame contents into
    the media. If this is compared with the persistence mechanisms in the previous
    chapter with Scala and Python, here also it is done in similar ways.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding aggregations in Spark R
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In SQL, aggregation of data is very flexible. The same thing is true in Spark
    SQL too. Instead of running SQL statements on a single data source located in
    a single machine, here, Spark SQL can do the same on distributed data sources.
    In the chapter where RDD-based programming is covered, a MapReduce use case was
    discussed to do data aggregation and the same is being used here to demonstrate
    the aggregation capabilities of Spark SQL. In this section also, the use cases
    are approached in the SQL query way as well as in the DataFrame API way.
  prefs: []
  type: TYPE_NORMAL
- en: 'The use cases selected for elucidating the MapReduce kind of data processing
    are given here:'
  prefs: []
  type: TYPE_NORMAL
- en: The retail banking transaction records come with account number and transaction
    amount in comma-separated strings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find an account level summary of all the transactions to get the account balance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At the R REPL prompt, try the following statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In the R DataFrame API, there are some syntax differences as compared to its
    Scala or Python counterparts, mainly because this is a purely API-based programming
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding multi-datasource joins with SparkR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, the joining of multiple DataFrames based on the key
    has been discussed. In this section, the same use case is implemented using R
    API of Spark SQL. The use cases selected for elucidating the join of multiple
    datasets using a key are given in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: The first dataset contains a retail banking master records summary with the
    account number, first name, and last name. The second dataset contains the retail
    banking account balance with account number and balance amount. The key on both
    of the datasets is the account number. Join the two datasets and create one dataset
    containing the account number, first name, last name, and balance amount. From
    this report, pick up the top three accounts in terms of the balance amount.
  prefs: []
  type: TYPE_NORMAL
- en: The Spark DataFrames are created from persisted JSON files. Instead of the JSON
    files, it can be any supported data files. Then they are read from the disk to
    form the DataFrames and they are joined together.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the R REPL prompt, try the following statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Continuing from the same R REPL session, the following lines of code get the
    same result through the DataFrame API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The join type selected in the preceding section of the code is inner join. Instead
    of that, any other type of join can be used, either through the SQL query way
    or through the DataFrame API way. One word of caution before using the join using
    DataFrame API is that the column names of both the Spark DataFrames have to be
    different to avoid ambiguity in the resultant Spark DataFrame. In this particular
    use case, it can be seen that the DataFrame API is becoming a bit difficult to
    deal with, while the SQL query way is looking very straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding sections, the R API for Spark SQL has been covered. In general,
    if possible, it is better to write the code using the SQL query way as much as
    possible. The DataFrame API is getting better, but it is not as flexible as in
    the other languages, such as Scala or Python.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the other chapters in this book, this is a self-contained one to introduce
    Spark to R programmers. All the use cases that are discussed in this chapter are
    run in the R REPL of Spark. But in real-world applications, this method is not
    ideal. The R commands have to be organized in script files and to be submitted
    to a Spark cluster to run. The easiest way is to use the already existing `$SPARK_HOME/bin/spark-submit
    <path to the R script file>` script, where the fully-qualified R filename is given
    with respect to the current directory from where the command is being invoked.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information refer to: [https://spark.apache.org/docs/latest/api/R/index.html](https://spark.apache.org/docs/latest/api/R/index.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A whirlwind tour of the R language was covered in this chapter, followed by
    a special mention about the need to have a distinction of understanding the difference
    between an R DataFrame and a Spark DataFrame. Then, basic Spark programming with
    R was covered using the same use cases of the previous chapters. R API for Spark
    was covered, and the use cases have been implemented using the SQL query way and
    DataFrame API way. This chapter helps data scientists understand the power of
    Spark and use it in their R applications, using the SparkR package that comes
    with Spark. This opens up the door of big data processing, using Spark with R
    to process structured data.
  prefs: []
  type: TYPE_NORMAL
- en: The subject of Spark-based data processing in various languages has been discussed,
    and it is time to focus on some data analysis with charting and plotting. Python
    comes with a lot of charting and plotting libraries that produce publication quality
    pictures. The next chapter will discuss charting and plotting with the data processed
    by Spark.
  prefs: []
  type: TYPE_NORMAL
