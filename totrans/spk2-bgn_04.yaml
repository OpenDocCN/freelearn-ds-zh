- en: Chapter 4. Spark Programming with R
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 4 章. 使用 R 语言进行 Spark 编程
- en: R is a popular statistical computing programming language used by many and freely
    available under the **General Public License** (**GNU**). R originated from the
    programming language S, created by John Chambers. R was developed by Ross Ihaka
    and Robert Gentleman. Many data scientists use R for their computing needs. R
    has inherent support for many statistical functions and many scalar data types,
    and has composite data structures for vectors, matrices, data frames, and more,
    for statistical computation. R is highly extensible and for that, external packages
    can be created. Once an external package is created, it has to be installed and
    loaded for any program to use it. A collection of such packages under a directory
    forms an R library. In other words, R comes with a set of base packages and additional
    packages that can be installed on top of it to form the required library for the
    desired computing needs. In addition to functions, datasets can also be packaged
    in R packages.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: R 是一种流行的统计计算编程语言，被许多人使用，并且可以在 **通用公共许可证**（**GNU**）下免费获得。R 语言起源于由 John Chambers
    创建的编程语言 S，由 Ross Ihaka 和 Robert Gentleman 开发。许多数据科学家使用 R 来满足他们的计算需求。R 语言具有许多内置的统计函数和许多标量数据类型，并为向量、矩阵、数据框等提供了复合数据结构，用于统计计算。R
    语言高度可扩展，因此可以创建外部包。一旦创建了外部包，就必须安装和加载它，以便任何程序可以使用它。这些包的集合在目录下形成了一个 R 库。换句话说，R 语言自带了一套基础包，以及可以安装在其上的附加包，以形成满足所需计算需求所需的库。除了函数外，数据集也可以打包在
    R 包中。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: The need for SparkR
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SparkR 的需求
- en: Essentials of R
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R 语言基础
- en: Dataframes
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据框
- en: Aggregations
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚合
- en: Multi-datasource joins with SparkR
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 SparkR 的多数据源连接
- en: The need for SparkR
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SparkR 的需求
- en: A plain base R installation cannot interact with Spark. The **SparkR** package
    exposes all the required objects and functions for R to talk to the Spark ecosystem.
    Compared to Scala, Java, and Python, the Spark programming in R is different and
    the SparkR package mainly exposes R API for DataFrame-based Spark SQL programming.
    At the moment, R cannot be used to manipulate the RDDs of Spark directly. So for
    all practical purposes, the R API for Spark has access to only Spark SQL abstractions.
    The Spark **MLlib** can also be programmed using R because Spark MLlib uses DataFrames.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 纯 R 语言基础安装无法与 Spark 交互。**SparkR** 包暴露了 R 与 Spark 生态系统通信所需的所有对象和函数。与 Scala、Java
    和 Python 相比，R 语言的 Spark 编程有所不同，SparkR 包主要暴露了基于 DataFrame 的 Spark SQL 编程的 R API。目前，R
    无法直接操作 Spark 的 RDD。因此，从实际应用的角度来看，R API 对 Spark 的访问仅限于 Spark SQL 抽象。Spark **MLlib**
    也可以使用 R 进行编程，因为 Spark MLlib 使用 DataFrame。
- en: How is SparkR going to help the data scientists to do better data processing?
    The base R installation mandates that all the data to be stored (or accessible)
    on the computer where R is installed. The data processing occurs on the single
    computer on which the R installation is available. Moreover, if the data size
    is more than the main memory available on the computer, R will not be able to
    do the required processing. With the SparkR package, there is access to a whole
    new world of a cluster of nodes for data storage and for doing data processing.
    With the help of SparkR package, R can be used to access the Spark DataFrames
    as well as R DataFrames.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: SparkR 如何帮助数据科学家更好地进行数据处理？基础 R 安装要求所有要存储（或可访问）的数据都在安装 R 的计算机上。数据处理发生在可访问 R 安装的单一计算机上。此外，如果数据量超过计算机上的主内存，R
    将无法进行所需的处理。使用 SparkR 包，可以访问一个全新的节点集群，用于数据存储和数据处理。借助 SparkR 包，可以使用 R 访问 Spark DataFrame
    以及 R DataFrame。
- en: It is very important to know the distinction between the two types of data frames,
    R Dataframes and Spark Dataframes. An R DataFrame is completely local and a data
    structure of the R language. A Spark DataFrame is a parallel collection of structured
    data managed by the Spark infrastructure.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 了解两种数据框类型的区别非常重要，即 R 数据框和 Spark 数据框。R 数据框是完全局部的，是 R 语言的数结构。Spark 数据框是由 Spark
    基础设施管理的结构化数据的并行集合。
- en: An R DataFrame can be converted to a Spark DataFrame and a Spark DataFrame can
    be converted to an R DataFrame.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: R 数据框可以转换为 Spark 数据框，Spark 数据框也可以转换为 R 数据框。
- en: When a Spark DataFrame is converted to an R DataFrame, it should fit in the
    available memory of the computer. This conversion is a great feature and there
    is a need to do so. By converting an R DataFrame to a Spark DataFrame, the data
    can be distributed and processed in parallel. By converting a Spark DataFrame
    to an R DataFrame, a lot of computations, charting, and plotting that is done
    by other R functions can be done. In a nutshell, the SparkR package brings the
    power of distributed and parallel computing capabilities to R.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当 Spark DataFrame 转换为 R DataFrame 时，它应该适合计算机可用的内存。这种转换是一个很好的特性，并且有必要这样做。通过将
    R DataFrame 转换为 Spark DataFrame，数据可以分布式并行处理。通过将 Spark DataFrame 转换为 R DataFrame，可以使用其他
    R 函数执行的大量计算、图表和绘图操作。简而言之，SparkR 包将分布式和并行计算能力引入 R。
- en: Often, when performing data processing with R, because of the sheer size of
    the data and the need to fit it into the main memory of the computer, the data
    processing is done in multiple batches and the results are consolidated to compute
    the final results. This kind of multi-batch processing can be completely avoided
    if Spark with R is used to process the data.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 R 进行数据处理时，由于数据量巨大以及需要将其放入计算机的主内存中，数据处理通常在多个批次中完成，并将结果合并以计算最终结果。如果使用 Spark
    与 R 处理数据，可以完全避免这种多批次处理。
- en: Often, reporting, charting, and plotting are done on the aggregated and summarized
    raw data. The raw data size can be huge and need not fit into one computer. In
    such cases, Spark with R can be used to process the entire raw data and finally,
    the aggregated and summarized data can be used to produce the reports, charts,
    or plots.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，报告、图表和绘图是在汇总和总结的原始数据上完成的。原始数据的大小可能很大，不一定适合在一个计算机中。在这种情况下，可以使用 Spark 与 R 处理整个原始数据，最后，使用汇总和总结的数据来生成报告、图表或绘图。
- en: Because of the inability to process huge amounts of data and for doing data
    analysis with R, many times, ETL tools are made to use for doing the pre-processing
    or transformations on the raw data, and only in the final stage is the data analysis
    done using R. Because of Spark's ability to process data at scale, Spark with
    R can replace the entire ETL pipeline and do the desired data analysis with R.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 由于无法处理大量数据以及使用 R 进行数据分析，很多时候，ETL 工具被用来在原始数据上执行预处理或转换，并且仅在最终阶段使用 R 进行数据分析。由于
    Spark 能够大规模处理数据，Spark 与 R 可以替代整个 ETL 管道，并使用 R 执行所需的数据分析。
- en: Many R users use the **dplyr **R package for manipulating datasets in R. This
    package provides fast data manipulation capabilities with R DataFrames. Just like
    manipulating local R DataFrames, it can access data from some of the RDBMS tables
    too. Apart from these primitive data manipulation capabilities, it lacks many
    of the data processing features available in Spark. So Spark with R is a good
    alternative to packages such as dplyr.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 许多 R 用户使用 **dplyr** R 包来操作 R 中的数据集。这个包提供了与 R DataFrames 一起快速的数据操作能力。就像操作本地 R
    DataFrame 一样，它还可以访问一些 RDBMS 表中的数据。除了这些原始的数据操作能力之外，它还缺少 Spark 中可用的许多数据处理功能。因此，Spark
    与 R 是 dplyr 等包的良好替代品。
- en: The SparkR package is yet another R package, but that is not stopping anybody
    from using any of the R packages that are already being used. At the same time,
    it supplements the data processing capability of R manifold by making use of the
    huge data processing capabilities of Spark.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 'SparkR 包是另一个 R 包，但这并没有阻止任何人使用已经使用的任何 R 包。同时，它通过利用 Spark 的巨大数据处理能力，补充了 R 的数据处理能力。 '
- en: Basics of the R language
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: R 语言基础
- en: This is not in any way a guide to R programming. But, it is important to touch
    upon the basics of R as a language very briefly for the benefit of those who are
    not familiar with R to appreciate what is being covered in this chapter. A very
    basic introduction to the language features is covered here.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是任何形式的 R 编程指南。但是，为了使不熟悉 R 的人能够欣赏本章所涵盖的内容，简要地介绍 R 语言的基本知识是很重要的。这里涵盖了语言特性的非常基本的介绍。
- en: R comes with a few built-in data types to hold numerical values, character values,
    and boolean values. There are composite data structures available and the most
    important ones are, namely, vectors, lists, matrices, and data frames. A vector
    consists of ordered collection of values of a given type. A list is an ordered
    collection of elements that can be of different types. For example, a list can
    hold two vectors, of which one is a vector containing numerical values and the
    the other is a vector containing boolean values. A matrix is a two-dimensional
    data structure holding numerical values in rows and columns. A data frame is a
    two-dimensional data structure containing rows and columns, where columns can
    have different data types but a single column cannot hold different data types.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: R 随带一些内置数据类型来存储数值、字符和布尔值。有复合数据结构可用，其中最重要的有，即向量、列表、矩阵和数据框。向量是由给定类型的值按顺序排列的集合。列表是元素按顺序排列的集合，这些元素可以是不同类型的。例如，列表可以包含两个向量，其中一个向量包含数值，另一个向量包含布尔值。矩阵是一个二维数据结构，在行和列中存储数值。数据框是一个二维数据结构，包含行和列，其中列可以有不同的数据类型，但单个列不能包含不同的数据类型。
- en: 'Code samples of using a variable (a special case of vector), a numeric vector,
    a character vector, a list, a matrix, a data frame, and assigning column names
    to a data frame are as follows. The variable names are given as self-descriptive
    as possible for the reader to understand without the help of additional explanation.
    The following code snippet run on a regular R REPL gives an idea of the data structures
    of R:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些使用变量（向量的特例）、数值向量、字符向量、列表、矩阵、数据框以及为数据框分配列名的代码示例。变量名尽可能具有自描述性，以便读者在没有额外解释的情况下理解。以下在常规
    R REPL 上运行的代码片段给出了 R 的数据结构概念：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The main topic of discussion here is going to be revolving around data frames.
    Some of the functions that are commonly used with data frames are demonstrated
    here. All these commands are to be executed on the regular R REPL as a continuation
    of the session that executed the preceding code snippet:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这里讨论的主要话题将围绕数据框展开。这里展示了与数据框常用的一些函数。所有这些命令都应在常规 R REPL 中执行，作为执行前一个代码片段的会话的延续：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: DataFrames in R and Spark
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: R 和 Spark 中的 DataFrames
- en: 'When working with Spark using R, it is very easy to get confused with the DataFrame
    data structure. As mentioned earlier, it is there in R and in Spark SQL. The following
    code snippet deals with converting an R DataFrame to a Spark DataFrame and vice
    versa. This is going to be a very common operation when programming Spark with
    R. The following code snippet is to be executed in the R REPL of Spark. From now
    on, all the references to the R REPL are with respect to the R REPL of Spark:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 R 与 Spark 一起工作时，很容易对 DataFrame 数据结构感到困惑。如前所述，它在 R 和 Spark SQL 中都存在。以下代码片段处理将
    R DataFrame 转换为 Spark DataFrame 以及相反操作。当使用 R 编程 Spark 时，这将是一个非常常见的操作。以下代码片段应在
    Spark 的 R REPL 中执行。从现在开始，所有关于 R REPL 的引用都是指 Spark 的 R REPL：
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: There is no complete compatibility and interoperability between an R DataFrame
    and a Spark DataFrame in terms of the supported functions.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在支持的功能方面，R DataFrame 和 Spark DataFrame 之间没有完全的兼容性和互操作性。
- en: Tip
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: As a good practice, it is better to name the R DataFrame and Spark DataFrame
    with agreed  conventions in R programs in order to have a distinction between
    the two different types. Not all the functions that are supported on R DataFrames
    are not supported on Spark DataFrames and vice versa. Always refer to the right
    version of the R API for Spark.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种良好的实践，在 R 程序中最好使用约定的命名约定来命名 R DataFrame 和 Spark DataFrame，以便在两种不同类型之间有所区分。并非所有在
    R DataFrame 上支持的功能都在 Spark DataFrame 上支持，反之亦然。始终参考 Spark 的正确版本 R API。
- en: 'Those who use a lot of charting and plotting have to be extra careful while
    dealing with R DataFrames in conjunction with Spark DataFrames. The charting and
    plotting of R works with only R DataFrames. If there is a need to produce charts
    or plots with the data processed by Spark and available in Spark DataFrame, it
    has to be converted to an R DataFrame to proceed with the charting and plotting.
    The following code snippet will give an idea this. We will use the faithful dataset
    again for elucidation purposes in the R REPL of Spark:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 那些大量使用图表和绘图的人在使用R DataFrame与Spark DataFrame结合时必须格外小心。R的图表和绘图仅与R DataFrame一起工作。如果需要使用Spark
    DataFrame中处理的数据生成图表或绘图，则必须将其转换为R DataFrame才能进行图表和绘图。以下代码片段将给出一个想法。我们将再次使用faithful数据集，在Spark的R
    REPL中进行阐明：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The figure here is used jut to demonstrate that the Spark DataFrame cannot
    be used to do charting and R DataFrame has to be used for the same:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 此图仅用于演示Spark DataFrame不能用于图表和绘图，而必须使用R DataFrame进行相同的操作：
- en: '![DataFrames in R and Spark](img/image_04_002.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![R和Spark中的DataFrame](img/image_04_002.jpg)'
- en: Figure 1
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图1
- en: The charting and plotting library, when used with Spark DataFrame, gave an error
    because of the incompatibility of the data types.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当与Spark DataFrame一起使用时，由于数据类型的不兼容，图表和绘图库出现了错误。
- en: Tip
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: The most important aspect to have in mind is that an R DataFrame is an in-memory
    resident data structure, while a Spark DataFrame is a parallel collection of datasets
    distributed across a cluster of nodes. So, all the functions that use R DataFrames
    need not work with Spark DataFrames and vice versa.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 需要牢记的最重要的一点是，R DataFrame是一个内存驻留的数据结构，而Spark DataFrame是跨节点集群分布的数据集的并行集合。因此，所有使用R
    DataFrame的函数不必与Spark DataFrame一起工作，反之亦然。
- en: 'Let''s revisit the bigger picture again, as given in *Figure 2*, to set the
    context and see what is being discussed here before getting into and taking up
    the use cases. In the previous chapter, the same subject was introduced by using
    the programming languages Scala and Python. In this chapter, the same set of use
    cases used in the Spark SQL programming will be implemented using R:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次回顾一下更大的图景，如图2所示，以设置上下文并了解在这里讨论的内容，然后再进入并处理使用案例。在前一章中，使用Scala和Python编程语言介绍了相同主题。在这一章中，将使用与Spark
    SQL编程中使用的相同的一组使用案例，但使用R来实现：
- en: '![DataFrames in R and Spark](img/image_04_004.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![R和Spark中的DataFrame](img/image_04_004.jpg)'
- en: Figure 2
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图2
- en: The use cases that are going to be discussed here will be demonstrating the
    ability to mix SQL queries with Spark programs in R. Multiple data sources will
    be chosen, data will be read from those sources using DataFrame, and uniform data
    access will be demonstrated.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这里将要讨论的使用案例将展示在R中混合SQL查询与Spark程序的能力。将选择多个数据源，使用DataFrame从这些源读取数据，并演示统一的数据访问。
- en: Spark DataFrame programming with R
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用R进行Spark DataFrame编程
- en: 'The use cases selected for elucidating the Spark SQL way of programming with
    DataFrame are given as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 用于阐明使用DataFrame进行Spark SQL编程的使用案例如下：
- en: The transaction records are comma-separated values.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交易记录是逗号分隔值。
- en: Filter out only the good transaction records from the list. The account number
    should start with `SB` and the transaction amount should be greater than zero.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从列表中过滤出仅包含良好交易记录。账户号码应以`SB`开头，交易金额应大于零。
- en: Find all the high value transaction records with a transaction amount greater
    than 1000.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找所有交易金额大于1000的高价值交易记录。
- en: Find all the transaction records where the account number is bad.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找所有账户号码错误的交易记录。
- en: Find all the transaction records where the transaction amount is less than or
    equal to zero.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找所有交易金额小于或等于零的交易记录。
- en: Find a combined list of all the bad transaction records.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找所有不良交易记录的合并列表。
- en: Find the total of all the transaction amounts.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找所有交易金额的总和。
- en: Find the maximum of all the transaction amounts.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找所有交易金额的最大值。
- en: Find the minimum of all the transaction amounts.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找所有交易金额的最小值。
- en: Find all the good account numbers.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找所有账户号码良好的记录。
- en: This is exactly the same set of use cases that were used in the previous chapter,
    but here, the programming model is totally different. Here, the programming is
    done in R. Using this set of use cases, two types of programming model are demonstrated
    here. One is using the SQL queries and other is using DataFrame APIs.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是上一章中使用的一组用例，但在这里，编程模型完全不同。在这里，编程是在R中完成的。使用这组用例，展示了两种类型的编程模型。一种是使用SQL查询，另一种是使用DataFrame
    API。
- en: Tip
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: The data files needed for running the following code snippets are available
    from the same directory where the R code is kept.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 运行以下代码片段所需的数据文件与R代码所在的同一目录中可用。
- en: In the following code snippets, data is read from files located in the filesystem.
    Since all these code snippets are executed from the R REPL of Spark, all the data
    files are to be kept in the `$SPARK_HOME` directory.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码片段中，数据是从文件系统中读取的。由于所有这些代码片段都是在Spark的R REPL中执行的，因此所有数据文件都必须保存在`$SPARK_HOME`目录中。
- en: Programming with SQL
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用SQL进行编程
- en: 'At the R REPL prompt, try the following statements:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在R REPL提示符下，尝试以下语句：
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The retail banking transaction records come with account number, transaction
    amount are processed using SparkSQL to get the desired results of the use cases.
    Here is the summary of what the preceding script did:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 零售银行交易记录包含账户号码和交易金额，使用SparkSQL进行处理以获得用例的预期结果。以下是前面脚本所做操作的摘要：
- en: Unlike other programming languages supported with Spark, R doesn't have an RDD
    programming capability. So, instead of going with the construction of RDD from
    collections, the data is read from the JSON file containing the transaction records.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与Spark支持的其他编程语言不同，R没有RDD编程能力。因此，不是从集合中构建RDD，而是从包含交易记录的JSON文件中读取数据。
- en: A Spark DataFrame is created from the JSON file.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从JSON文件创建Spark DataFrame。
- en: A table is registered with the DataFrame with a name. This registered name of
    the table can be used in SQL statements.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用DataFrame注册一个带有名称的表。这个注册的表名可以在SQL语句中使用。
- en: Then, all the other activities are issuing SQL statements using the SQL function
    from the SparkR package.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，所有其他活动都是通过SparkR包中的SQL函数发出SQL语句。
- en: The result of all these SQL statements is stored as Spark DataFrames, and showDF
    function is used to extract the values to the calling R program.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有这些SQL语句的结果都存储为Spark DataFrame，并使用showDF函数将值提取到调用R程序中。
- en: The aggregate value calculations are also done through the SQL statements.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚合值计算也是通过SQL语句完成的。
- en: The DataFrame contents are displayed in table format using the the `showDF`
    function of SparkR.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DataFrame的内容使用SparkR的`showDF`函数以表格格式显示。
- en: A detailed view of the structure of the DataFrame is displayed using the print
    function. This is akin to the describe command of the database tables.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用打印函数可以显示DataFrame的结构视图。这类似于数据库表的describe命令。
- en: In the preceding R code, the style of programming is different as compared to
    the Scala code. That is because it is an R program. Using the SparkR library,
    the Spark features are being used. But the functions and other abstractions are
    not in a really different style.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的R代码中，编程风格与Scala代码不同，因为它是一个R程序。使用SparkR库，正在使用Spark功能。但函数和其他抽象并没有真正不同的风格。
- en: Note
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Throughout this chapter, there will be instances where DataFrames are used.
    It is very easy to get confused by which is the R DataFrame and which is the Spark
    DataFrame. Hence, care is taken to specifically mention by qualifying the DataFrame,
    such as R DataFrame and Spark DataFrame.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，将会有使用DataFrame的实例。很容易混淆哪个是R DataFrame，哪个是Spark DataFrame。因此，特别注意通过限定DataFrame来具体说明，例如R
    DataFrame和Spark DataFrame。
- en: Programming with R DataFrame API
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用R DataFrame API进行编程
- en: In this section, the code snippets will be run in the same R REPL. Like the
    preceding code snippets, initially, some DataFrame-specific basic commands are
    given. These are used regularly to see the contents and for doing some sanity
    tests on the DataFrame and its contents. These are commands that are typically
    used in the exploratory stage of the data analysis quite often to get more insight
    into the structure and contents of the underlying data.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，代码片段将在相同的R REPL中运行。与前面的代码片段一样，最初给出了一些DataFrame特定的基本命令。这些命令被经常使用，用于查看内容并对DataFrame及其内容进行一些基本测试。这些是在数据分析的探索阶段经常使用的命令，以获得更多对底层数据结构和内容的洞察。
- en: 'At the R REPL prompt, try the following statements:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在R的REPL提示符下，尝试以下语句：
- en: '[PRE5]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here is the summary of what the preceding script did from a DataFrame API perspective:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是从DataFrame API角度对前面脚本所做操作的总结：
- en: The DataFrame containing the superset of data used in the preceding section
    is used here.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本节使用的是包含上一节中使用的数据集的超集的DataFrame。
- en: Filtering of the records is demonstrated next. Here, the most important aspect
    to notice is that the filter predicate is to be given exactly like the predicates
    in the SQL statements. Filters can not be chained.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来演示记录的过滤。这里，需要注意的最重要方面是过滤谓词必须与SQL语句中的谓词完全相同。过滤器不能链式使用。
- en: The aggregation methods are calculated next.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来计算聚合方法。
- en: The final statements in this set are doing the selection, filtering, choosing
    distinct records, and ordering.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本集合中的最后几个语句正在进行选择、过滤、选择不同的记录以及排序操作。
- en: Finally, the transaction records are persisted in Parquet format, read from
    the Parquet store, and created a Spark DataFrame. More details on the persistence
    formats have been covered in the previous chapter and the concepts remain the
    same. Only the DataFrame API syntax is different.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，事务记录以Parquet格式持久化，从Parquet存储中读取，并创建了一个Spark DataFrame。关于持久化格式的更多细节已在上一章中介绍，概念保持不变。只是DataFrame
    API的语法有所不同。
- en: In this code snippet, the Parquet format data is stored in the current directory,
    from where the corresponding REPL is invoked. When it is run as a Spark program,
    the directory again will be the current directory from where the Spark submit
    is invoked.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这个代码片段中，Parquet格式数据存储在当前目录中，从该目录调用相应的REPL。当它作为一个Spark程序运行时，目录再次将是Spark提交调用的当前目录。
- en: The last few statements are about the persisting of the DataFrame contents into
    the media. If this is compared with the persistence mechanisms in the previous
    chapter with Scala and Python, here also it is done in similar ways.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 最后几个语句是关于将DataFrame内容持久化到媒体中的。如果与上一章中基于Scala和Python的持久化机制进行比较，这里也是以类似的方式进行。
- en: Understanding aggregations in Spark R
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解Spark R中的聚合
- en: In SQL, aggregation of data is very flexible. The same thing is true in Spark
    SQL too. Instead of running SQL statements on a single data source located in
    a single machine, here, Spark SQL can do the same on distributed data sources.
    In the chapter where RDD-based programming is covered, a MapReduce use case was
    discussed to do data aggregation and the same is being used here to demonstrate
    the aggregation capabilities of Spark SQL. In this section also, the use cases
    are approached in the SQL query way as well as in the DataFrame API way.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在SQL中，数据的聚合非常灵活。在Spark SQL中也是如此。在这里，Spark SQL可以在分布式数据源上执行与在单台机器上的单个数据源上运行SQL语句相同的事情。在介绍基于RDD的编程的章节中，讨论了一个MapReduce用例来进行数据聚合，这里同样使用它来展示Spark
    SQL的聚合能力。在本节中，使用SQL查询方式以及DataFrame API方式来处理用例。
- en: 'The use cases selected for elucidating the MapReduce kind of data processing
    are given here:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 下面给出用于阐明MapReduce类型数据处理的使用案例：
- en: The retail banking transaction records come with account number and transaction
    amount in comma-separated strings
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 零售银行交易记录包含以逗号分隔的账户号码和交易金额字符串
- en: Find an account level summary of all the transactions to get the account balance
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到所有交易的账户级别摘要以获取账户余额
- en: 'At the R REPL prompt, try the following statements:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在R的REPL提示符下，尝试以下语句：
- en: '[PRE6]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In the R DataFrame API, there are some syntax differences as compared to its
    Scala or Python counterparts, mainly because this is a purely API-based programming
    model.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在R DataFrame API中，与Scala或Python的对应版本相比，存在一些语法差异，主要是因为这是一个纯API编程模型。
- en: Understanding multi-datasource joins with SparkR
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解SparkR中的多数据源连接
- en: In the previous chapter, the joining of multiple DataFrames based on the key
    has been discussed. In this section, the same use case is implemented using R
    API of Spark SQL. The use cases selected for elucidating the join of multiple
    datasets using a key are given in the following section.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，已经讨论了基于键的多个DataFrame的连接。在本节中，使用Spark SQL的R API实现了相同的用例。用于阐明使用键连接多个数据集的用例将在以下章节中给出。
- en: The first dataset contains a retail banking master records summary with the
    account number, first name, and last name. The second dataset contains the retail
    banking account balance with account number and balance amount. The key on both
    of the datasets is the account number. Join the two datasets and create one dataset
    containing the account number, first name, last name, and balance amount. From
    this report, pick up the top three accounts in terms of the balance amount.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个数据集包含一个零售银行主记录摘要，包括账户号码、名和姓。第二个数据集包含零售银行账户余额，包括账户号码和余额金额。这两个数据集的关键是账户号码。将这两个数据集连接起来，创建一个包含账户号码、名、姓和余额金额的单一数据集。从这份报告中，挑选出余额金额最高的前三个账户。
- en: The Spark DataFrames are created from persisted JSON files. Instead of the JSON
    files, it can be any supported data files. Then they are read from the disk to
    form the DataFrames and they are joined together.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Spark DataFrame是从持久化的JSON文件创建的。除了JSON文件，还可以是任何支持的数据文件。然后它们从磁盘读取以形成DataFrame，并将它们连接在一起。
- en: 'At the R REPL prompt, try the following statements:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在R交互式命令行提示符下，尝试以下语句：
- en: '[PRE7]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Continuing from the same R REPL session, the following lines of code get the
    same result through the DataFrame API:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 从相同的R交互式命令行会话继续，以下代码行通过DataFrame API得到相同的结果：
- en: '[PRE8]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The join type selected in the preceding section of the code is inner join. Instead
    of that, any other type of join can be used, either through the SQL query way
    or through the DataFrame API way. One word of caution before using the join using
    DataFrame API is that the column names of both the Spark DataFrames have to be
    different to avoid ambiguity in the resultant Spark DataFrame. In this particular
    use case, it can be seen that the DataFrame API is becoming a bit difficult to
    deal with, while the SQL query way is looking very straightforward.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码的前一部分中选择的连接类型是内连接。而不是这样，可以使用任何其他类型的连接，无论是通过SQL查询方式还是通过DataFrame API方式。在使用DataFrame
    API进行连接之前有一个注意事项是，两个Spark DataFrame的列名必须不同，以避免在结果Spark DataFrame中产生歧义。在这个特定的用例中，可以看到DataFrame
    API处理起来有点困难，而SQL查询方式看起来非常直接。
- en: In the preceding sections, the R API for Spark SQL has been covered. In general,
    if possible, it is better to write the code using the SQL query way as much as
    possible. The DataFrame API is getting better, but it is not as flexible as in
    the other languages, such as Scala or Python.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，已经介绍了Spark SQL的R API。一般来说，如果可能的话，最好尽可能多地使用SQL查询方式编写代码。DataFrame API正在变得更好，但它不如Scala或Python等其他语言灵活。
- en: Unlike the other chapters in this book, this is a self-contained one to introduce
    Spark to R programmers. All the use cases that are discussed in this chapter are
    run in the R REPL of Spark. But in real-world applications, this method is not
    ideal. The R commands have to be organized in script files and to be submitted
    to a Spark cluster to run. The easiest way is to use the already existing `$SPARK_HOME/bin/spark-submit
    <path to the R script file>` script, where the fully-qualified R filename is given
    with respect to the current directory from where the command is being invoked.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 与本书中的其他章节不同，这是一个独立的章节，旨在向R程序员介绍Spark。本章中讨论的所有用例都是在Spark的R交互式命令行中运行的。但在现实世界的应用中，这种方法并不理想。R命令必须组织在脚本文件中，并提交到Spark集群以运行。最简单的方法是使用已经存在的`$SPARK_HOME/bin/spark-submit
    <R脚本文件路径>`脚本，其中完全限定的R文件名是根据命令执行的当前目录给出的。
- en: References
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'For more information refer to: [https://spark.apache.org/docs/latest/api/R/index.html](https://spark.apache.org/docs/latest/api/R/index.html)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 更多信息请参阅：[https://spark.apache.org/docs/latest/api/R/index.html](https://spark.apache.org/docs/latest/api/R/index.html)
- en: Summary
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: A whirlwind tour of the R language was covered in this chapter, followed by
    a special mention about the need to have a distinction of understanding the difference
    between an R DataFrame and a Spark DataFrame. Then, basic Spark programming with
    R was covered using the same use cases of the previous chapters. R API for Spark
    was covered, and the use cases have been implemented using the SQL query way and
    DataFrame API way. This chapter helps data scientists understand the power of
    Spark and use it in their R applications, using the SparkR package that comes
    with Spark. This opens up the door of big data processing, using Spark with R
    to process structured data.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了R语言的快速浏览，随后特别提到了需要区分理解R DataFrame与Spark DataFrame之间的差异。接着，使用与之前章节相同的用例，介绍了基于R的基本Spark编程。本章涵盖了Spark的R
    API，并使用SQL查询方式和DataFrame API方式实现了用例。这有助于数据科学家理解Spark的强大功能，并使用SparkR包（Spark附带）将其应用于R应用程序中。这开启了使用Spark与R处理结构化数据的大数据处理之门。
- en: The subject of Spark-based data processing in various languages has been discussed,
    and it is time to focus on some data analysis with charting and plotting. Python
    comes with a lot of charting and plotting libraries that produce publication quality
    pictures. The next chapter will discuss charting and plotting with the data processed
    by Spark.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在各种语言中基于Spark的数据处理主题已经被讨论，现在是时候专注于一些带有图表和绘图的数据分析了。Python附带了许多图表和绘图库，可以生成高质量的图片。下一章将讨论使用Spark处理的数据进行图表和绘图。
