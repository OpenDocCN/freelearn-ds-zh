- en: Chapter 9. Analyzing Social Data Participation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Social networks and websites have revolutionized the Internet. Most people online
    participate in some social network, either it's **Facebook**, **Twitter**, **Pinterest**,
    **GitHub**, **StackOverflow**, or any of the zillion other social networking websites
    that have sprung up. They're an important way for people to connect and stay in
    contact, but they're also a major source of data about people's relationships
    and activities.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing this data is important for a number of reasons. Of course, advertisers
    and marketers want to squeeze as much information out of the data as they can.
    But if you're running the social network, you'll want to analyze the data to figure
    out what's working and what's falling flat. You want to ask yourself constantly
    what you can do to engage users better and to make your social network a more
    compelling, enjoyable, or useful experience for your users.
  prefs: []
  type: TYPE_NORMAL
- en: Over the course of this chapter, we'll get an open data dump from the **StackExchange**
    ([http://stackexchange.com](http://stackexchange.com)) website. This includes
    StackOverflow ([http://stackoverflow.com/](http://stackoverflow.com/)) and a host
    of other question-and-answer sites. We'll analyze this in a number of different
    ways and try to learn both about how people interact and generate content on those
    sites and about what makes a good answer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the topics we are going to cover in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the analyses we can perform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding patterns of participation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing askers and answerers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding participation patterns over time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding up-voted answers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tagging questions automatically
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up the project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we get started, let''s set up the project. I''ve done this using Leiningen
    2 ([http://leiningen.org/](http://leiningen.org/)) and Stuart Sierra''s reloaded
    project template ([https://github.com/stuartsierra/reloaded](https://github.com/stuartsierra/reloaded)).
    I named the project `social-so` by running the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Of course, we''ll need more dependencies. The following is the `project.clj`
    file for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The highlights here are that we'll use `org.clojure/data.xml` to read XML files,
    `org.clojure/data.json` to read JSON, and `org.jsoup/jsoup` to clean up HTML.
    If you're still using Java 6, you'll need `jsr166y` to provide concurrency with
    the reducers library. And we'll use `cc.mallet/mallet` to handle some Naïve Bayesian
    classification.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the analyses
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have the infrastructure out of the way, let's step back and think
    about what kind of data we have and what we can do with it.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding social network data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Social networks come in two broad kinds:'
  prefs: []
  type: TYPE_NORMAL
- en: There are **networking-oriented** social networks. These include the usual subjects,
    such as Facebook ([http://facebook.com](http://facebook.com)), LinkedIn ([http://linkedin.com](http://linkedin.com)),
    Twitter ([http://twitter.com/](http://twitter.com/)), or Sina Weibo ([http://weibo.com](http://weibo.com)).
    These focus on allowing people to connect with each other, build relationships,
    and post updates about themselves.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There are **knowledge-sharing-oriented** social networks. These include the
    StackExchange ([http://stackexchange.com](http://stackexchange.com)) family of
    social networks, including StackOverflow ([http://stackoverflow.com](http://stackoverflow.com))
    or Quora ([https://www.quora.com/](https://www.quora.com/)). These focus on allowing
    people to exchange information and knowledge. Usually, they are more structured
    and focused on questions and answers than web forums or wikis.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Obviously, these networks enable entirely different kinds of interactions and
    have different features and produce different kinds of data. Different kinds of
    analyses are appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding knowledge-based social networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In knowledge-based social networks, people come together to share information.
    Often, these are question-and-answer forums, such as the StackExchange network
    of sites, but also on **Quora** ([https://www.quora.com/](https://www.quora.com/)),
    **Yahoo Answers** ([http://answers.yahoo.com/](http://answers.yahoo.com/)), and
    so on.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, in this genre of social network, some users post questions. Others
    answer them. There's usually some kind of in-site economy, whether it's represented
    by badges, points, or some combination. This encourages people to keep both questions
    and answers on topic, to answer questions, and to set and maintain the community's
    tone. Sometimes, there are moderators, sometimes the communities are self-moderated,
    and sometimes there's a combination of the two.
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at the front page of StackOverflow, we can see the basic elements of
    the social network. Look at the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Understanding knowledge-based social networks](img/4139OS_09_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding screenshot depicts a very interesting layout. You can easily
    notice the following two things:'
  prefs: []
  type: TYPE_NORMAL
- en: There's a button labeled **Ask Question** at the upper right-hand side for posting
    a question. This isn't as large as you might expect, since those asking questions
    are presumably motivated to find the button and are willing to click through into
    another page to do it. This is different than the posting boxes found on sites
    such as Facebook or Twitter, where they try to reduce the friction for posting
    new statuses in order to encourage people to do it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is also a list of recently posted questions. This can be filtered to display
    tags that you're interested in. For example, you can use this to find either questions
    that you are qualified to answer or those you are interested in learning about
    yourself.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So we can see immediately that the primary interactions of the site are accessible
    from the front page. Also, the site's design makes it easier to do the more difficult
    interaction (answering questions).
  prefs: []
  type: TYPE_NORMAL
- en: We can already guess that most users will join to post only one or two questions
    and never participate on the site again. This group may potentially be quite large.
    Who those users are and how to motivate them to answer questions may be one critical
    question that StackExchange has.
  prefs: []
  type: TYPE_NORMAL
- en: There may also be a similar dynamic among the users who do answer questions.
    There are probably a small number of users who answer most of the questions. StackExchange
    may be interested in how to get contributions more evenly from all users, not
    just from a few *power users*.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the 80/20 rule
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In fact, both of these observations are examples of the general principle of
    social networks. This has been called the **80/20** rule. This simply states that
    approximately 80 percent of the content will be created by 20 percent of the users.
    It's also known as the **Pareto Principle**, which states more generally that
    80 percent of the effects come from 20 percent of the causes. Although in different
    social networks, the details may be off—for example, 15 percent of the users may
    create 95 percent of the content—in general this observation is surprisingly robust.
    One of the things that we'll look at in this chapter is exactly how the 80/20
    rule applies to the StackOverflow data.
  prefs: []
  type: TYPE_NORMAL
- en: With this in mind, let's get the data so we can start looking at it.
  prefs: []
  type: TYPE_NORMAL
- en: Getting the data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this chapter, we're going to focus on knowledge-based social networks, and
    in particular, we'll work with the StackExchange sites. For some time, StackExchange
    has made public a periodic data dump of their sites ([http://blog.stackexchange.com/category/cc-wiki-dump/](http://blog.stackexchange.com/category/cc-wiki-dump/)).
    This will provide a great test bed for working with a social network site's data.
  prefs: []
  type: TYPE_NORMAL
- en: The data dump is made available through the **Internet Archive** ([https://archive.org/](https://archive.org/)).
    The webpage for that is currently at [https://archive.org/details/stackexchange](https://archive.org/details/stackexchange).
    You can download the entire dump using a **BitTorrent** client ([http://www.bittorrent.com/](http://www.bittorrent.com/))
    such as **μTorrent** ([http://www.utorrent.com/](http://www.utorrent.com/)). However,
    we're only interested in the StackOverflow posts and comments, so if you'd like,
    you can just download those two archives. Of course, combined they're about 6
    GB, so the torrent may make the most sense, anyway.
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting the data](img/4139OS_09_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The archived files are compressed using the 7z format. Windows users can get
    a utility for extracting this from the **7-zip** site ([http://www.7-zip.org/](http://www.7-zip.org/)).
    That site's download page also has links to some unofficial binaries for Mac OS
    X and Linux ([http://www.7-zip.org/download.html](http://www.7-zip.org/download.html)).
    Both of these platforms also have command-line binaries available. For example,
    **Homebrew** ([http://brew.sh/](http://brew.sh/)) has a recipe for this named
    **p7zip**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Extract this data into your project directory, into a subdirectory named `data`,
    using the following lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Now we're ready to start digging into the data and see what surprises it has
    for us.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the amount of data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First, we need to see how much data there will be. The raw archive for this
    part of the data is about 6 GB. Not insignificant, but it's not petabytes, either.
  prefs: []
  type: TYPE_NORMAL
- en: So the compressed file is almost 5 GB, and the expanded file is 23 GB! We have
    a lot of data to look at.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the data format
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: All of the files are in XML format. The file labeled `Posts` contains both the
    questions and the answers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The format of the data is fairly simple (but for a full description, see the
    `README.txt` file). The following is the first entry:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As we can see from `README.txt`, this post represents a question (the `PostTypeId`
    field is `1`). We can see its body, its tags, and its accepted answer, as well
    as a lot of the metadata about this post. This should give us plenty to go on.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we look at the third entry, we''ll see one of the accepted answers for this
    post, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: So, for answers (the `PostTypeId` field is 2), we can get their parents, their
    body texts, and their scores. Their parents indicate which child was accepted.
    This should be enough to help us analyze their content.
  prefs: []
  type: TYPE_NORMAL
- en: In both cases, we also have `OwnerUserId`, and this will help us understand
    how people interact with the site and with each other.
  prefs: []
  type: TYPE_NORMAL
- en: The text-field attributes allow rich content (`Body` and `Title`), and these
    are handled by encoding HTML into the fields. We'll need to escape those and probably
    scrub out the tags. That won't be a problem, but we'll need to keep it in mind.
  prefs: []
  type: TYPE_NORMAL
- en: Defining and loading the data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can trace out some of the data that we'll need to use throughout this chapter.
    We can put these into the `src/social_so/data.clj` file.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll use two record types. The `CountRank` type will hold together a raw
    count and its rank in the list of frequencies, and the `UserInfo` type will store
    the user and the frequencies and ranks for the different types of posts that they''ve
    made. Look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The `post`, `q`, and `a` fields will keep track of the frequencies and rank
    by all posts, by question posts, and by answer posts.
  prefs: []
  type: TYPE_NORMAL
- en: Together, these record structures should help us get a start in understanding
    this data and some of the patterns of participation.
  prefs: []
  type: TYPE_NORMAL
- en: 'For loading the data, let''s move to a new file, named `src/social_so/xml.clj`,
    and let''s give it the following namespace declaration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We'll use the functions in this namespace to read the XML file and build the
    records containing the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the most basic level, we need to be able to read the post elements from
    the XML file. Look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll also need to access a little bit of data from each element. The following
    are some getter functions for an identifier for the user and for the post type
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In this snippet, `el` represents the XML element being processed, and we're
    using a custom function to lowercase the string (`social-so.utils`/`to-lower`)
    to be a little defensive about being passed `null` values.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the data from the XML files will take place in two stages. First, we'll
    get the raw frequencies, and then we'll sort the data several different ways and
    assign ranks to the data.
  prefs: []
  type: TYPE_NORMAL
- en: Counting frequencies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The way that we'll count the frequencies is to walk over the posts in the XML
    file. We'll maintain an index of the users with their `UserInfo` records. The
    first time each user is found, they will get a new `UserInfo` object. Subsequently,
    their `UserInfo` record will be updated with new counts.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how this works in practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first function, `update-user-info`, operates on the level of a single record.
    It takes a `UserInfo` record and updates it based on the type of post currently
    being processed. If the record is nil, then a new one is created follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The next function operates at the level of the index from `user-id` to `UserInfo`
    records. It takes an XML post, and it gets the user''s information from it. It
    tries to retrieve the `UserInfo` record for that user from the index, and it uses
    `update-user-info` to increment the counts in that record. Look at the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, `load-user-infos` opens the XML file, reads in the posts, and counts
    the raw frequencies of the posts for each user. Finally, it forces the result
    with `doall`, because we''re working inside of a `with-open` block, so we''ll
    want to have the results fully realized before we close the file. Look at the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Now we're ready to walk over these, multiple times, and assign ranks based on
    the various counts.
  prefs: []
  type: TYPE_NORMAL
- en: Sorting and ranking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Currently, we''re storing the raw frequency under the `UserInfo` records''
    fields. However, we want to move the frequencies into a `CountRank` record and
    store the rank alongside it. We will achieve this by performing the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll find the rank using the `rank-on` function. This sorts by one of the
    properties of the `UserInfo` records (`:post`, `:q`, or `:a`) and then associates
    each instance with a rank by containing both within a vector pair. Look at the
    following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The function `update-rank` will then take the rank-and-user pair from `rank-on`
    and associate it with the appropriate property, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The next function, `add-rank-data`, coordinates this process by calling these
    functions on all users. And the function controlling this process, `add-all-ranks`,
    does this on each user as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can combine reading the XML file and counting the posts with sorting and
    ranking the users. Look at the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'All of these functions make it simple to load the XML file and to assign the
    ranks. Look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Now we have the information that we'll need to perform the first round of analyses.
  prefs: []
  type: TYPE_NORMAL
- en: Finding the patterns of participation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have some data loaded, let's roll up our sleeves and see what we
    can learn from it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we do, however, it would be nice to have some way to generate reports
    of which users are the most active for each type of post. Look at the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: This allows us to create tables listing the top 10 (or so) users for each post
    type.
  prefs: []
  type: TYPE_NORMAL
- en: '| Rank | User | All Posts |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 22656 | 28166 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 29407 | 20342 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 157882 | 15444 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 17034 | 13287 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 34397 | 13209 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 23354 | 12312 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 115145 | 11806 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 20862 | 10455 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 57695 | 9730 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 19068 | 9560 |'
  prefs: []
  type: TYPE_TB
- en: Based on this table, we can see that some users are *very* active. The top user
    has almost 8,000 more posts than the second most active user, who is still very,
    very active.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following graph, of the post counts of the top 1,000 users, shows how quickly
    the activity falls off and how much the top users dominate the conversation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Finding the patterns of participation](img/4139OS_09_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We can break this down further, however. I would expect the people who post
    questions to behave differently than the people who post answers.
  prefs: []
  type: TYPE_NORMAL
- en: Matching the 80/20 rule
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Previously, we talked about the 80/20 rule: that 80 percent of the content
    is created by 20 percent of the users. That''s obviously a rough estimate, but
    it does provide a good intuition for the dynamics of these networks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To find the break-down, we need to perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Sort the users in descending order by the count that we're interested in.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Partition them into **quintiles**, that is, five equally sized buckets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sum the counts for each bucket.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To implement this, we can use a function named `quantile-on`, which sorts a
    collection and breaks it into the buckets. Look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we just need to pull out the appropriate fields and sum their values, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: So the top 20 percent of users actually produce more than 85 percent of the
    content. The quintiles drop off rapidly from there.
  prefs: []
  type: TYPE_NORMAL
- en: '![Matching the 80/20 rule](img/4139OS_09_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: And we can pull these into a graph to be able to see the distribution of contributors
    more easily.
  prefs: []
  type: TYPE_NORMAL
- en: Looking for the 20 percent of questioners
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While finding those who post questions, we can also see who are most active
    in asking questions.
  prefs: []
  type: TYPE_NORMAL
- en: '| Rank | User | Question Posts |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 39677 | 1858 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 4653 | 1605 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 34537 | 1604 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 179736 | 1327 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 117700 | 1327 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 149080 | 1261 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 84201 | 1177 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 434051 | 1107 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 325418 | 1074 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 146780 | 1055 |'
  prefs: []
  type: TYPE_TB
- en: When we run this, it gives us a very different set of frequencies. These are
    more than an order of magnitude less than the frequencies for all posts.
  prefs: []
  type: TYPE_NORMAL
- en: We can also get the numbers for the distribution of questioners.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we use `quantile-on` and `sum-count` again, we can also see the break-down
    by quintile. Look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'And the following is the graph for this group:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Looking for the 20 percent of questioners](img/4139OS_09_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Interesting. So the lowest quintile doesn't contribute anything. Presumably,
    those are the users who answer questions. We'll look at this group more in a minute.
    But overall, the distribution of those asking questions follows what we'd expect
    from the 80/20 rule.
  prefs: []
  type: TYPE_NORMAL
- en: Looking for the 20 percent of respondents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Because most of the posts are answers, we can expect that these frequencies
    will be closer to the aggregate frequencies. We'll find this similarly to how
    we found the frequencies of those asking questions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Look at the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Rank | User | Answer Posts |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 22656 | 28137 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 29407 | 20310 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 157882 | 15431 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 17034 | 13285 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 34397 | 13157 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 23354 | 12270 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 115145 | 11784 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 20862 | 10447 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 57695 | 9711 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 19068 | 9557 |'
  prefs: []
  type: TYPE_TB
- en: 'We can see that there is a lot of similarity between this set of numbers and
    the first one. And in fact there is a lot of similarity between the distribution
    of this set of posters and the last set, as we can see in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Looking for the 20 percent of respondents](img/4139OS_09_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Looking at the distribution overall, however, we can see that the question answerers
    are even more lopsided than the question askers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us try to break down the contributors who answer the questions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'And the following is the graph for this data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Looking for the 20 percent of respondents](img/4139OS_09_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: So almost half of the users never post an answer! However, the top 20 percent
    of users post 95 percent of the answers. So answering questions appears to be
    dominated by a few users, while question asking is (marginally) more widespread.
  prefs: []
  type: TYPE_NORMAL
- en: Combining ranks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can see even more similarity if we compare the ranks. This table shows the
    rank for each category of post for all of the top 10 users in any category. (Note
    that the ranks begin at 0, not 1.)
  prefs: []
  type: TYPE_NORMAL
- en: '| User ID | All Post Rank | Question Post Rank | Answer Post Rank |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 4653 | 423 | 1 | 21342 |'
  prefs: []
  type: TYPE_TB
- en: '| 17034 | 3 | 602037 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| 19068 | 9 | 420772 | 9 |'
  prefs: []
  type: TYPE_TB
- en: '| 20862 | 7 | 169469 | 7 |'
  prefs: []
  type: TYPE_TB
- en: '| 22656 | 0 | 37889 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 23354 | 5 | 22760 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| 29407 | 1 | 33177 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 34397 | 4 | 16478 | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| 34537 | 358 | 2 | 8024 |'
  prefs: []
  type: TYPE_TB
- en: '| 39677 | 345 | 0 | 151684 |'
  prefs: []
  type: TYPE_TB
- en: '| 57695 | 8 | 65071 | 8 |'
  prefs: []
  type: TYPE_TB
- en: '| 84201 | 631 | 6 | 10521 |'
  prefs: []
  type: TYPE_TB
- en: '| 115145 | 6 | 54339 | 6 |'
  prefs: []
  type: TYPE_TB
- en: '| 117700 | 595 | 4 | 29654 |'
  prefs: []
  type: TYPE_TB
- en: '| 146780 | 923 | 9 | 123737 |'
  prefs: []
  type: TYPE_TB
- en: '| 149080 | 682 | 5 | 56862 |'
  prefs: []
  type: TYPE_TB
- en: '| 157882 | 2 | 101282 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| 179736 | 605 | 3 | 36463 |'
  prefs: []
  type: TYPE_TB
- en: '| 325418 | 523 | 8 | 3502 |'
  prefs: []
  type: TYPE_TB
- en: '| 434051 | 858 | 7 | 164416 |'
  prefs: []
  type: TYPE_TB
- en: 'The data in this table makes certain points clear:'
  prefs: []
  type: TYPE_NORMAL
- en: The top-ranked users asking questions are a very different set than the top-ranked
    users answering questions. The top questioner was ranked 141,674th as an answerer,
    and the top answerer was ranked 37,887th as a questioner.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once we're beyond the top posters, neither subgroup correlates well with the
    aggregate of all posts. All of these users rank within the top 1,000 for all types
    of posts. This just indicates that the question answerers don't completely dominate
    the questioners.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These observations confirm what we found looking at the quintiles and the graphs.
    Both of these groups look very different from each other, and from the aggregate
    of the two.
  prefs: []
  type: TYPE_NORMAL
- en: Let's break down the groups into those who only post questions, those who only
    post answers, and those who do both. That should give us more insight into the
    types of participation.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at those who only post questions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can get the users who only post answers fairly easily, and then running
    the previous analyses on that subset is also not difficult. Let''s see how this
    will work. Look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: So first we filtered for only users who post no answers. This leaves us with
    49 percent of the total number of users, so this is really an extremely large
    group of StackOverflow users.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the interesting part is that their distribution is more uniform. The
    most active quintile has posted less than two-thirds of the questions. The following
    graph makes that clear:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Looking at those who only post questions](img/4139OS_09_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The ratios for these are much different than we've seen so far. This group is
    much less driven by a few users. But when you think about it, this makes sense.
    Many people who come to StackOverflow only post one question, and that's the extent
    of their interaction. In fact, the bottom three quintiles only post one question
    and no answers. That's almost 33 percent of the total number of users.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how this compares to those who post only answers.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at those who only post answers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Getting the users who only post answers will be almost exactly like the process
    we just went through. However, this time we''ll switch questions and answers,
    of course. Look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: This time, we're working with a group roughly half the size of those who post
    only questions, roughly a quarter of the entire group of users. And the distribution
    of the top quintile is much closer to what we'd expect from the 80/20 rule.
  prefs: []
  type: TYPE_NORMAL
- en: Again, notice that the last few quintiles appear to have users who have only
    posted one answer. In fact, about 16 percent of the total number of users have
    posted no questions and only one answer. This seems to be one of the most curious
    groups, and trying to get more interaction out of them would be a priority (as
    I'm sure it has been for StackExchange).
  prefs: []
  type: TYPE_NORMAL
- en: The graph for this, shown following this paragraph, is somewhat between the
    last graph (for those who ask only questions) and the first few graphs. The first
    quintile is about 80 percent, but the rest don't taper off as much as they sometimes
    do.
  prefs: []
  type: TYPE_NORMAL
- en: '![Looking at those who only post answers](img/4139OS_09_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Now let's look at the breakdown for the rest of the users, those who've posted
    both questions and answers.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at those who post both questions and answers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The predicate needed to select those users who have posted both questions and
    answers will be slightly different than what we've seen in the last two sections.
    However, once we have those users, the analysis will be the same. The only wrinkle
    will be that we'll get the distribution for both questions and answers.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll get the users who answer both using a slightly more complicated predicate,
    which we will page to remove. Look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we''ll also need to compute the values for both the questions and the answers.
    First, let''s see what the questions look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The graph that follows makes clear that the ratios on this are more like the
    distribution that we''d expect:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Looking at those who post both questions and answers](img/4139OS_09_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Looking at the numbers for the answers from this group, we again seem to be
    following a very rough approximation of the 80/20 rule. Look at the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'And the following is the graph for this data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Looking at those who post both questions and answers](img/4139OS_09_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: So the group who has posted both questions and answers seem to be more balanced
    and have a more typical interaction with the website.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another way of looking at this data is to look at how the number of questions
    each user posts by the number of answers each posts. This gives us an indication
    of how active users are in each type of activity. Look at the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Looking at those who post both questions and answers](img/4139OS_09_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This graph makes clear that typically users engage in one type of activity
    or another, and there''s not as much cross-over as you might have expected. Also,
    the scales of the axes are a bit deceiving: the *y* axis is over 16 times larger
    than the *x* axis.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a better understanding of the way that users are interacting
    with StackOverflow and the ways that they're generating content, let's look at
    that content and see if we can figure out what makes a good answer and what doesn't.
  prefs: []
  type: TYPE_NORMAL
- en: Finding the up-voted answers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Answers can be rated in a couple of different ways. The community can vote an
    answer up or down, and the original poster can accept an answer. For the purposes
    of this demonstration, we'll look at accepted answers; however, both metrics might
    be useful and interesting to explore.
  prefs: []
  type: TYPE_NORMAL
- en: We'll look at how we might automatically recognize answers that will be accepted.
  prefs: []
  type: TYPE_NORMAL
- en: On the one hand, this would be very useful to do. If the original poster forgets
    to accept an answer, the website could prompt them with a possible solution. Also,
    the site could send the poster an e-mail when someone posts an answer that should
    be considered.
  prefs: []
  type: TYPE_NORMAL
- en: But on the other hand, acceptable answers probably don't share common linguistic
    features that any kind of algorithm could latch on to in order to identify potential
    solutions. I'm doubtful that we'll be able to train an algorithm to identify acceptable
    answers.
  prefs: []
  type: TYPE_NORMAL
- en: Still, let's try and see how well we can actually do.
  prefs: []
  type: TYPE_NORMAL
- en: Processing the answers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are over 18 million posts at this point. We can always work on the full
    data set eventually, but to get started, let's pull out a sample of the data.
    To make things easier, I've uploaded a random sample of 100,000 answers on [http://www.ericrochester.com/mastering-clj-data/data/post-sample-100000.json.gz](http://www.ericrochester.com/mastering-clj-data/data/post-sample-100000.json.gz).
    These have been transformed into the data structure that we'll use everywhere.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can download these with `curl` and decompress it with `gzip` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll put the code for this section into the `src/social_so/post.clj` file,
    and at the top we''ll add the following namespace declaration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'To represent the data that we''ll work with, we''ll use the `PostInfo` record
    type. Look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The interesting fields here are `body-text`, which contains the answer's text,
    stripped of HTML, and `accepted-for`, which is nil if the post isn't accepted
    or contains the question's ID, if the post was accepted for its question.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a flat data record, so it''s easy to load the JSON data into these
    structures. Look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'And now we can read the data on the REPL, assuming we''ve aliased this namespace
    to `p`. Look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Looking at these briefly, we can see that just over 20 percent of the posts
    in the sample were accepted.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting the accepted answer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that the data is in a usable form, let's turn our attention to categorizing
    the posts. To do this, we'll use **MALLET** ([http://mallet.cs.umass.edu/](http://mallet.cs.umass.edu/)).
    We saw MALLET before in [Chapter 3](ch03.html "Chapter 3. Topic Modeling – Changing
    Concerns in the State of the Union Addresses"), *Topic Modeling – Changing Concerns
    in the State of the Union Addresses*, on topic modeling. That's usually the task
    that this library is used for. However, it also provides an implementation of
    a number of classification algorithms, and we'll use one of those now.
  prefs: []
  type: TYPE_NORMAL
- en: Over the course of this chapter, we'll categorize the posts as what MALLET calls
    `instances`. This will divide them into categories based on features, or clues
    within each instance. We'll use MALLET to take each post, create an instance from
    it, identify its features and categories, and finally use those to train a classifier.
    We can later use this classifier on new posts.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ll use a new namespace for this code. Open the file `src/social_so/nlp.clj`
    and add the following namespace declaration to the top of the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: That's a lot of imports. But really, that's the most complicated that this code
    will be. MALLET does a lot of the heavy lifting for us.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the code to come, we''ll refer to this in the REPL with the `n` prefix.
    To make this available, execute the following line (after the **user=>** prompt)
    in your REPL environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Now we're ready to start filling in the blanks.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the InstanceList object
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: MALLET represents each input as an `Instance` object. `Instance` objects contain
    their data, a target label, name, and other metadata.
  prefs: []
  type: TYPE_NORMAL
- en: MALLET works on collections of `Instance` objects as an `InstanceList`, which
    is just a collection of `Instance` objects. All the instances in the list are
    processed using the same pipe of transformations.
  prefs: []
  type: TYPE_NORMAL
- en: Each step in the pipe changes one property of each `Instance` object in the
    list. For example, one pipe (`CharSequence2TokenSequence`) tokenizes the input,
    and another (`Target2Label`) creates an index of the target labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure illustrates this process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating the InstanceList object](img/4139OS_09_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: For the features in the documents and their labels, the `InstanceList` also
    maintains **alphabets**. These are indexes from the input strings to integers.
    The integers serve to distinguish the inputs and also act as indexes in arrays.
    This allows MALLET to work with frequencies as an array, which saves both space
    and processing time. The trick is that all of the `Instance` objects being processed
    must share the same alphabet. The `InstanceList` makes sure that they do.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our processing, to make clear what transformations we''ll use, we''ll define
    them all in a function named `make-pipe`. The following is that function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: What's happening here? Let's take the steps apart.
  prefs: []
  type: TYPE_NORMAL
- en: '`Target2Label` builds the alphabet for the `Instance` objects'' target property.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Input2CharSequence` reads the input from a string, file, or URL named in the
    data property and replaces it with the resource''s content.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`CharSequence2TokenSequence` tokenizes the string in the data property.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`TokenSequenceLowercase` lowercases the tokens in the data property.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`TokenSequenceRemoveStopwords` filters out stop words (common words) from the
    tokens in the data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`TokenSequence2FeatureSequence` creates the alphabet of tokens and convert
    the sequence of tokens to a sequence of feature indexes.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`FeatureSequence2FeatureVector` converts the sequence of feature indexes to
    a vector for a bag-of-words approach.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: So, at the end of this pipeline, each document will be represented by a vector
    indicating how many times each feature appears in that document. Features can
    be almost anything, but usually they are words that appear in the document or
    metadata (author, date, tags) associated with that document. This is the format
    that the classifiers—as well as many other machine learning and natural language
    algorithms—expect.
  prefs: []
  type: TYPE_NORMAL
- en: This handles the processing of the `Instance` objects, but before we can do
    that, we'll need to convert the `PostInfo` objects into `Instance` objects.
  prefs: []
  type: TYPE_NORMAL
- en: MALLET has a number of classes that do this from more primitive data types.
    They take some kind of collection of primitive inputs and iterate over the `Instance`
    objects they represent. In our case, we'll use `ArrayDataAndTargetIterator`. This
    iterates over two string arrays. One contains each input's data, and the other
    contains each input's target.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll wrap creating this with the function `post-info-iterator`. This uses
    the `accepted-tag` function to decide whether the post was accepted or not and
    tag it appropriately. Look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have these functions, we can use them to populate an `InstanceList`
    that will run all the documents through the transformation pipeline that we defined
    earlier. Look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we''re ready. Let''s pick up with the `PostInfo` sample that we extracted
    earlier and bound to `s` in the *Processing the answers* section. This contains
    a sequence of `PostInfo` instances. In the REPL, we can create an `InstanceList`
    using the functions that we just defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Now we can think about how we're going to use these documents to train and test
    a classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Training sets and Test sets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we've processed our data into a format that MALLET can use, we'll divide
    the input corpus into a test set and a training set. We can use different relative
    sizes for each, but often we'll train on more documents than we'll test on.
  prefs: []
  type: TYPE_NORMAL
- en: MALLET's `InstanceList` class has a `split` method, but we'll define a thin
    wrapper over it to make it easier to use. MALLET's `split` method takes an array
    listing the proportions of the total for each group. Since we only want two groups
    that completely divide the input, we can pass in the proportion for one group
    and compute the value of the other group's proportion. We'll also return a hash-map
    indicating which of the output lists is for testing and which is for training.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the `split-sets` function that acts as the wrapper:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'And we can use the following on the command line to divide the input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Now we're ready to use these for training and testing.
  prefs: []
  type: TYPE_NORMAL
- en: Training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we can use a classifier, we have to train it on the training set that
    we just created.
  prefs: []
  type: TYPE_NORMAL
- en: For this example, we'll create a Naive Bayesian classifier. Again, we'll just
    create a thin wrapper function. In this case, it's not that significantly simpler
    than creating the Bayesian trainer and calling it on the data. However, it will
    allow us to use the trainer without having to tease out which MALLET packages
    we have to import. This allows us to require our `social-so.nlp` namespace into
    the REPL and execute the entire process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the wrapper:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'And we can use the following in the REPL to get a trained Bayesian classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: That's it. We've trained our classifier. Now let's see how to use it.
  prefs: []
  type: TYPE_NORMAL
- en: Testing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To test the classifier, MALLET provides a `Trial` class that encapsulates running
    a classifier over some inputs that are already tagged. It provides counts over
    how accurate the classifier is and calculates statistics to show how well it does.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make it easier to load the development environment and to use this class,
    we''ll create a factory function for it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'And now let''s use this function at the REPL as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Well, great. Now what can we do with this?
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the outcome
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are several factors to consider when evaluating a classifier's ability
    to identify inputs as belonging to category *X*. Let's consider what they are
    and see how we can get them from the `Trial` object.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to think about the classifier''s **precision or positive predictive
    value** (**PPV**). This takes into account how many items the classifier incorrectly
    included in category *X*, and it''s given by the ratio of the true positives with
    all labeled positive. In our case, that means the number of items that were correctly
    identified as `accepted` divided by all of those identified as `accepted`. You
    can get this using the `getPrecision` method, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: So we can see that it correctly identified the accepted rows rather poorly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next number that we need to consider is the classifier''s **recall**. This
    is sometimes referred to as its **sensitivity** or **true positive rate** (**TPR**).
    This is the percentage of all positives that it found, and it''s found by dividing
    the true positives by the true positives and the false negatives. MALLET exposes
    this with the `getRecall` method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: In this case, the recall is actually worse than the precision, which is *saying*
    something.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we''ll consider the **accuracy** (**ACC**) of the classifier. This is
    the ratio of true classifications, both positive and negative, to the total number
    of items. This is rendered from the `getAccuracy` method. Look at the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Presumably, this classifier''s precision and recall are better when identifying
    not-accepted answers. Let''s test that as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: These results aren't great. A simple baseline that classifies everything as
    *not accepted* would actually score slightly better than this (80 percent). The
    criteria involved in accepting an answer don't appear to be captured by the simple
    token features that we've used here.
  prefs: []
  type: TYPE_NORMAL
- en: The performance here is likely because `not accepted` is the default state for
    an answer, and the status of most answers.
  prefs: []
  type: TYPE_NORMAL
- en: 'There''s one final number that we want to pay attention to. The F1 score is
    a measure of the classifier''s accuracy. This combines the precision and recall
    into a single number ranging from 0 (poor) to 1 (perfect). Look at the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Again, we can see that the classifier doesn't do a good job.
  prefs: []
  type: TYPE_NORMAL
- en: We can get more detailed information about how well the classifier did by looking
    at its **confusion** matrix. This is a matrix that breaks down how many items
    are predicated to be in both categories, and how many actually are.
  prefs: []
  type: TYPE_NORMAL
- en: 'MALLET has a `ConfusionMatrix` class for displaying these charts. We''ll again
    wrap that class in a function to make it easier to call. Look at the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'This generates the confusion matrix and returns it as a string. Let''s call
    this and print the matrix out as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Now we can see where the numbers came from. Rows are the actual classes, and
    columns are how the classifier predicted the classes. The true positives in the
    lower right-hand side are much smaller than the total number of positives in the
    bottom row. On the other hand, the true negatives in the upper left-hand side
    are the majority of the total number of negatives in the top row.
  prefs: []
  type: TYPE_NORMAL
- en: Naive Bayesian classifiers don't perform well here, but it's possible (although
    unlikely) that other classifiers might. It's also possible that adding more metadata
    as features might help. For example, the length of time between when the question
    was posted and when the answer was posted might be fruitful. Other features that
    might help are the length of the answer or the answerer's reputation. Reputation
    is the points awarded for accepted answers, though, so including them introduces
    circularity – we'd be training directly on what we're attempting to classify.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: However the results of the last experiment, we can see that there's a lot of
    information embedded in social networks. Depending on the nature of the network,
    we can have different kinds of interactions and different kinds of data in the
    network.
  prefs: []
  type: TYPE_NORMAL
- en: In the final chapter, which is next, we'll look at whether analyzing financial
    data and using machine learning to examine news documents help predict the future
    of stock prices.
  prefs: []
  type: TYPE_NORMAL
