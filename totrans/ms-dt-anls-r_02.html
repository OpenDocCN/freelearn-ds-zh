<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch02"/>Chapter 2. Getting Data from the Web</h1></div></div></div><p>It happens pretty often that we want to use data in a project that is not yet available in our databases or on our disks, but can be found on the Internet. In such situations, one option might be to get the IT department or a data engineer at our company to extend our data warehouse to <a class="indexterm" id="id167"/>scrape, process, and load the data into our database as shown in the following diagram:</p><div><img alt="Getting Data from the Web" src="img/2028OS_02_01.jpg"/></div><p>On the other hand, if we have no <em>ETL</em> system (<em>to Extract, Transform, and Load data</em>) or simply just cannot wait a few weeks for the IT department to implement our request, we are on our own. This is pretty standard for the data scientist, as most of the time we are developing prototypes that can be later transformed into products by software developers. To this end, a variety of skills are required in the daily round, including the following topics that we will cover in this chapter:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Downloading data programmatically from the Web</li><li class="listitem" style="list-style-type: disc">Processing XML and JSON formats</li><li class="listitem" style="list-style-type: disc">Scraping and parsing data from raw HTML sources</li><li class="listitem" style="list-style-type: disc">Interacting with APIs</li></ul></div><p>Although being a <em>data scientist</em> was referred to as the sexiest job of the 21st century (Source: <a class="ulink" href="https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century/">https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century/</a>), most data science tasks have nothing to do with data analysis. Worse, sometimes the job seems to be boring, or the daily routine requires just basic IT skills and no machine learning at all. Hence, I prefer to call this role a <em>data hacker</em> instead of <em>data scientist</em>, which also means that we often have to get our hands dirty.</p><p>For instance, scraping and scrubbing data is the least sexy part of the analysis process for sure, but it's one of the most important steps; it is also said, that around 80 percent of data analysis is spent cleaning data. There is no sense in running the most advanced machine learning algorithm on junk data, so be sure to take your time to get useful and tidy data from your sources.</p><div><div><h3 class="title"><a id="note18"/>Note</h3><p>This chapter will also depend on extensive usage of Internet browser debugging tools with some R packages. These include Chrome <code class="literal">DevTools</code> or <code class="literal">FireBug</code> in Firefox. Although the steps to use these tools will be straightforward and also shown on screenshots, it's definitely worth mastering these tools for future usage; therefore, I suggest checking out a few tutorials on these tools if you are into fetching data from online sources. Some starting points are listed in the <em>References</em> section of the <em>Appendix</em> at the end of the book.</p></div></div><p>For a quick overview and a collection of relevant R packages for scraping data from the Web and to interact with <a class="indexterm" id="id168"/>Web services, see the <em>Web Technologies and Services CRAN Task View</em> <a class="indexterm" id="id169"/>at <a class="ulink" href="http://cran.r-project.org/web/views/WebTechnologies.html">http://cran.r-project.org/web/views/WebTechnologies.html</a>.</p><div><div><div><div><h1 class="title"><a id="ch02lvl1sec15"/>Loading datasets from the Internet</h1></div></div></div><p>The most obvious task is to download<a class="indexterm" id="id170"/> datasets from the Web and load those into our <a class="indexterm" id="id171"/>R session in two manual steps:</p><div><ol class="orderedlist arabic"><li class="listitem">Save the datasets to disk.</li><li class="listitem">Read those with standard functions, such as <code class="literal">read.table</code> or for example <code class="literal">foreign::read.spss</code>, to import <code class="literal">sav</code> files.</li></ol></div><p>But we can often save some time by skipping the first step and loading the flat text data files directly from the URL. The following example fetches a comma-separated file from the <strong>Americas Open Geocode</strong> (<strong>AOG</strong>) database<a class="indexterm" id="id172"/> at <a class="ulink" href="http://opengeocode.org">http://opengeocode.org</a>, which contains the government, national statistics, geological information, and post office websites for the countries of the world:</p><div><pre class="programlisting"><strong>&gt; str(read.csv('http://opengeocode.org/download/CCurls.txt'))</strong>
<strong>'data.frame':  249 obs. of  5 variables:</strong>
<strong> $ ISO.3166.1.A2                  : Factor w/ 248 levels "AD" ...</strong>
<strong> $ Government.URL                 : Factor w/ 232 levels ""  ...</strong>
<strong> $ National.Statistics.Census..URL: Factor w/ 213 levels ""  ...</strong>
<strong> $ Geological.Information.URL     : Factor w/ 116 levels ""  ...</strong>
<strong> $ Post.Office.URL                : Factor w/ 156 levels ""  ...</strong>
</pre></div><p>In this example, we passed a hyperlink to the <code class="literal">file</code> argument of <code class="literal">read.table</code>, which actually downloaded the text file before processing. The <code class="literal">url</code> function, used by <code class="literal">read.table</code> in the background, supports HTTP and<a class="indexterm" id="id173"/> FTP protocols, and can also handle proxies, but it has its own limitations. For example <code class="literal">url</code> does not support <strong>Hypertext Transfer Protocol Secure</strong> (<strong>HTTPS</strong>) <a class="indexterm" id="id174"/>except for a few exceptions on Windows, which is often a must to access <a class="indexterm" id="id175"/>Web services that handle sensitive data.</p><div><div><h3 class="title"><a id="note19"/>Note</h3><p>HTTPS is not a separate protocol alongside HTTP, but instead HTTP over an encrypted SSL/TLS connection. While HTTP is considered to be insecure due to the unencrypted packets travelling between the client and server, HTTPS does not let third-parties discover sensitive information with the help of signed and trusted certificates.</p></div></div><p>In such situations, it's wise, and used to be the only reasonable option, to install and use the<a class="indexterm" id="id176"/> <code class="literal">RCurl</code> package, which is an R client interface to curl: <a class="ulink" href="http://curl.haxx.se">http://curl.haxx.se</a>. Curl<a class="indexterm" id="id177"/> supports a wide variety of protocols and URI schemes and handles cookies, authentication, redirects, timeouts, and even more.</p><p>For example, let's check the U.S. Government's open data catalog at <a class="ulink" href="http://catalog.data.gov/dataset">http://catalog.data.gov/dataset</a>. Although the general site can be accessed without SSL, most of the generated download URLs follow the HTTPS URI scheme. In the following example, we will fetch the <strong>Comma Separated Values</strong> (<strong>CSV</strong>) file<a class="indexterm" id="id178"/> of the Consumer Complaint Database from the Consumer Financial Protection Bureau, which can be accessed at <a class="ulink" href="http://catalog.data.gov/dataset/consumer-complaint-database">http://catalog.data.gov/dataset/consumer-complaint-database</a>.</p><div><div><h3 class="title"><a id="note20"/>Note</h3><p>This CSV file contains metadata on around a quarter of a million of complaints about financial products and services since 2011. Please note that the file is around 35-40 megabytes, so downloading it might take some time, and you would probably not want to reproduce the following example on mobile or limited Internet. If the <code class="literal">getURL</code> function fails with a certificate error (this might happen on Windows), please provide the path of the certificate manually by <code class="literal">options(RCurlOptions = list(cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl")))</code> or try the more recently <a class="indexterm" id="id179"/>published <code class="literal">curl</code> package by Jeroen Ooms<a class="indexterm" id="id180"/> or <code class="literal">httr</code> (<code class="literal">RCurl</code> front-end) by Hadley Wickham—see later.</p></div></div><p>Let's see the<a class="indexterm" id="id181"/> distribution of these complaints by product type <a class="indexterm" id="id182"/>after fetching and loading the CSV file directly from R:</p><div><pre class="programlisting"><strong>&gt; library(RCurl)</strong>
<strong>Loading required package: bitops</strong>
<strong>&gt; url &lt;- 'https://data.consumerfinance.gov/api/views/x94z-ydhh/rows.csv?accessType=DOWNLOAD'</strong>
<strong>&gt; df  &lt;- read.csv(text = getURL(url))</strong>
<strong>&gt; str(df)</strong>
<strong>'data.frame':  236251 obs. of  14 variables:</strong>
<strong> $ Complaint.ID        : int  851391 851793 ...</strong>
<strong> $ Product             : Factor w/ 8 levels ...</strong>
<strong> $ Sub.product         : Factor w/ 28 levels ...</strong>
<strong> $ Issue               : Factor w/ 71 levels "Account opening ...</strong>
<strong> $ Sub.issue           : Factor w/ 48 levels "Account status" ...</strong>
<strong> $ State               : Factor w/ 63 levels "","AA","AE",,..</strong>
<strong> $ ZIP.code            : int  14220 64119 ...</strong>
<strong> $ Submitted.via       : Factor w/ 6 levels "Email","Fax" ...</strong>
<strong> $ Date.received       : Factor w/ 897 levels  ...</strong>
<strong> $ Date.sent.to.company: Factor w/ 847 levels "","01/01/2013" ...</strong>
<strong> $ Company             : Factor w/ 1914 levels ...</strong>
<strong> $ Company.response    : Factor w/ 8 levels "Closed" ...</strong>
<strong> $ Timely.response.    : Factor w/ 2 levels "No","Yes" ...</strong>
<strong> $ Consumer.disputed.  : Factor w/ 3 levels "","No","Yes" ...</strong>
<strong>&gt; sort(table(df$Product))</strong>

<strong>      Money transfers         Consumer loan              Student loan </strong>
<strong>                  965                  6564                      7400 </strong>
<strong>      Debt collection      Credit reporting   Bank account or service </strong>
<strong>                24907                 26119                     30744 </strong>
<strong>          Credit card              Mortgage </strong>
<strong>                34848                104704</strong>
</pre></div><p>Although it's nice to know that most complaints were received about mortgages, the point here was to use curl to download the CSV file with a HTTPS URI and then pass the content to the <code class="literal">read.csv</code> function (or any other parser we discussed in the previous chapter) as text.</p><div><div><h3 class="title"><a id="note21"/>Note</h3><p>Besides <code class="literal">GET</code> requests, you can easily interact with RESTful API endpoints via <code class="literal">POST</code>, <code class="literal">DELETE</code>, or <code class="literal">PUT</code> requests as well by using the <code class="literal">postForm</code> function from the <code class="literal">RCurl</code> package or the <code class="literal">httpDELETE</code>, <code class="literal">httpPUT</code>, or <code class="literal">httpHEAD</code> functions— see details about the <code class="literal">httr</code> package<a class="indexterm" id="id183"/> later.</p></div></div><p>Curl can also <a class="indexterm" id="id184"/>help to download data from a secured site that<a class="indexterm" id="id185"/> requires authorization. The easiest way to do so is to login to the homepage in a browser, save the cookie to a text file, and then pass the path of that to <code class="literal">cookiefile</code> in <code class="literal">getCurlHandle</code>. You can also specify <code class="literal">useragent</code> among other options. Please see <a class="ulink" href="http://www.omegahat.org/RCurl/RCurlJSS.pdf">http://www.omegahat.org/RCurl/RCurlJSS.pdf</a> for more details and an overall (and very useful) overview on the most important RCurl features.</p><p>Although curl is extremely powerful, the syntax and the numerous options with the technical details might be way too complex for those without a decent IT background. The <code class="literal">httr</code> package is a simplified wrapper around <code class="literal">RCurl</code> with some sane defaults and much simpler configuration options for common operations and everyday actions.</p><p>For example, cookies are handled automatically by sharing the same connection across all requests to the same website; error handling is much improved, which means easier debugging if something goes wrong; the package comes with various helper functions to, for instance, set headers, use proxies, and easily issue <code class="literal">GET</code>, <code class="literal">POST</code>, <code class="literal">PUT</code>, <code class="literal">DELETE</code>, and other methods. Even more, it also handles authentication in a much more user-friendly way—along with OAuth support.</p><div><div><h3 class="title"><a id="note22"/>Note</h3><p>OAuth<a class="indexterm" id="id186"/> is the open standard for authorization with the help of intermediary service providers. This simply means that the user does not have to share actual credentials, but can rather delegate rights to access some of the stored information at the service providers. For example, one can authorize Google to share the real name, e-mail address, and so on with a third-party without disclosing any other sensitive information or any need for passwords. Most generally, OAuth is used for password-less login to various <a class="indexterm" id="id187"/>Web services and APIs. For more information, please see the <a class="link" href="ch14.html" title="Chapter 14. Analyzing the R Community">Chapter 14</a>, <em>Analyzing the R Community</em>, where we will use OAuth with Twitter to authorize the R session for fetching data.</p></div></div><p>But what if the data is not available to be downloaded as CSV files?</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch02lvl1sec16"/>Other popular online data formats</h1></div></div></div><p>Structured data is<a class="indexterm" id="id188"/> often available in XML or JSON formats on the Web. The high popularity of these two formats is due to the fact that both are human-readable, easy to handle from a programmatic point of view, and can manage any type of hierarchical data structure, not just a simple tabular design, as CSV files are.</p><div><div><h3 class="title"><a id="note23"/>Note</h3><p>JSON<a class="indexterm" id="id189"/> is originally derived from <em>JavaScript Object Notation</em>, which recently became one of the top, most-used standards for human-readable data exchange format. JSON is considered to be a low-overhead alternative to XML with attribute-value pairs, although it also supports a wide variety of object types such as number, string, boolean, ordered lists, and associative arrays. JSON is highly used in Web applications, services, and APIs.</p></div></div><p>Of course, R also supports loading (and saving) data in JSON. Let's demonstrate that by fetching some data from the previous example via the Socrata API (more on that later in the <em>R packages to interact with data source APIs</em> section of this chapter), provided by the Consumer Financial Protection Bureau. The full documentation of the API is available at <a class="ulink" href="http://www.consumerfinance.gov/complaintdatabase/technical-documentation">http://www.consumerfinance.gov/complaintdatabase/technical-documentation</a>.</p><p>The endpoint of the API is a URL where we can query the background database without authentication is <a class="ulink" href="http://data.consumerfinance.gov/api/views">http://data.consumerfinance.gov/api/views</a>. To get an overall picture on the structure of the data, the following is the returned JSON list opened in a browser:</p><div><img alt="Other popular online data formats" src="img/2028OS_02_02.jpg"/></div><p>As JSON is extremely<a class="indexterm" id="id190"/> easy to read, it's often very helpful to skim through the structure manually before parsing. Now let's load that tree list into R with the<a class="indexterm" id="id191"/> <code class="literal">rjson</code> package:</p><div><pre class="programlisting"><strong>&gt; library(rjson)</strong>
<strong>&gt; u &lt;- 'http://data.consumerfinance.gov/api/views'</strong>
<strong>&gt; fromJSON(file = u)</strong>
<strong>[[1]]</strong>
<strong>[[1]]$id</strong>
<strong>[1] "25ei-6bcr"</strong>

<strong>[[1]]$name</strong>
<strong>[1] "Credit Card Complaints"</strong>

<strong>[[1]]$averageRating</strong>
<strong>[1] 0</strong>
<strong>…</strong>
</pre></div><p>Well, it does not seem to be the same data we have seen before in the comma-separated values file! After a closer look at the documentation, it's clear that the endpoint of the API returns metadata on the available views instead of the raw tabular data that we saw in the CSV file. So let's see the view with the ID of <code class="literal">25ei-6bcr</code> now for the first five rows by opening the related URL in a browser:</p><div><img alt="Other popular online data formats" src="img/2028OS_02_03.jpg"/></div><p>The structure of the resulting <a class="indexterm" id="id192"/>JSON list has changed for sure. Now let's read that hierarchical list into R:</p><div><pre class="programlisting"><strong>&gt; res &lt;- fromJSON(file = paste0(u,'/25ei-6bcr/rows.json?max_rows=5'))</strong>
<strong>&gt; names(res)</strong>
<strong>[1] "meta" "data"</strong>
</pre></div><p>We managed to fetch the data along with some further meta-information on the view, columns, and so on, which is not something that we are interested in at the moment. As <code class="literal">fromJSON</code> returned a <code class="literal">list</code> object, we can simply drop the metadata and work with the <code class="literal">data</code> rows from now on:</p><div><pre class="programlisting"><strong>&gt; res &lt;- res$data</strong>
<strong>&gt; class(res)</strong>
<strong>[1] "list"</strong>
</pre></div><p>This is still a <code class="literal">list</code>, which we usually want to transform into a <code class="literal">data.frame</code> instead. So we have <code class="literal">list</code> with five elements, each holding 19 nested children. Please note that one of those, the 13th sub element, is list again with 5-5 vectors. This means that transforming the tree list into tabular format is not straightforward, even less so when we realize that one of those vectors holds multiple values in an unprocessed JSON format. So, for the sake of simplicity and proof of a concept demo, let's simply ditch the location-related values now and transform all other values to <code class="literal">data.frame</code>:</p><div><pre class="programlisting"><strong>&gt; df &lt;- as.data.frame(t(sapply(res, function(x) unlist(x[-13]))))</strong>
<strong>&gt; str(df)</strong>
<strong>'data.frame':  5 obs. of  18 variables:</strong>
<strong> $ V1 : Factor w/ 5 levels "16756","16760",..: 3 5 ...</strong>
<strong> $ V2 : Factor w/ 5 levels "F10882C0-23FC-4064-979C-07290645E64B" ...</strong>
<strong> $ V3 : Factor w/ 5 levels "16756","16760",..: 3 5 ...</strong>
<strong> $ V4 : Factor w/ 1 level "1364270708": 1 1 ...</strong>
<strong> $ V5 : Factor w/ 1 level "403250": 1 1 ...</strong>
<strong> $ V6 : Factor w/ 5 levels "1364274327","1364274358",..: 5 4 ...</strong>
<strong> $ V7 : Factor w/ 1 level "546411": 1 1 ...</strong>
<strong> $ V8 : Factor w/ 1 level "{\n}": 1 1 ...</strong>
<strong> $ V9 : Factor w/ 5 levels "2083","2216",..: 1 2 ...</strong>
<strong> $ V10: Factor w/ 1 level "Credit card": 1 1 ...</strong>
<strong> $ V11: Factor w/ 2 levels "Referral","Web": 1 1 ...</strong>
<strong> $ V12: Factor w/ 1 level "2011-12-01T00:00:00": 1 1 ...</strong>
<strong> $ V13: Factor w/ 5 levels "Application processing delay",..: 5 1 ...</strong>
<strong> $ V14: Factor w/ 3 levels "2011-12-01T00:00:00",..: 1 1 ...</strong>
<strong> $ V15: Factor w/ 5 levels "Amex","Bank of America",..: 2 5 ...</strong>
<strong> $ V16: Factor w/ 1 level "Closed without relief": 1 1 ...</strong>
<strong> $ V17: Factor w/ 1 level "Yes": 1 1 ...</strong>
<strong> $ V18: Factor w/ 2 levels "No","Yes": 1 1 ...</strong>
</pre></div><p>So we applied a simple<a class="indexterm" id="id193"/> function that drops location information from each element of the list (by removing the 13th element of each <em>x</em>), automatically simplified to <code class="literal">matrix</code> (by using <code class="literal">sapply</code> instead of <code class="literal">lapply</code> to iterate though each element of the list), transposed it (via <code class="literal">t</code>), and then coerced the resulting object to <code class="literal">data.frame</code>.</p><p>Well, we can also use some helper functions instead of manually tweaking all the list elements, as earlier. The<a class="indexterm" id="id194"/> <code class="literal">plyr</code> package (please find more details in <a class="link" href="ch03.html" title="Chapter 3. Filtering and Summarizing Data">Chapter 3</a>, <em>Filtering and Summarizing Data</em> and <a class="link" href="ch04.html" title="Chapter 4. Restructuring Data">Chapter 4</a>, <em>Restructuring Data</em>) includes some extremely useful functions to split and combine data:</p><div><pre class="programlisting"><strong>&gt; library(plyr)</strong>
<strong>&gt; df &lt;- ldply(res, function(x) unlist(x[-13]))</strong>
</pre></div><p>It looks a lot more familiar now, although we miss the variable names, and all values were converted to character vectors or factors—even the dates that were stored as UNIX timestamps. We can easily fix these problems with the help of the provided metadata (<code class="literal">res$meta</code>): for example, let's set the variable names by extracting (via the <code class="literal">[</code> operator) the name field of all columns except for the dropped (13th) location data:</p><div><pre class="programlisting"><strong>&gt; names(df) &lt;- sapply(res$meta$view$columns, `[`, 'name')[-13]</strong>
</pre></div><p>One might also identify the object classes with the help of the provided metadata. For example, the <code class="literal">renderTypeName</code> field would be a good start to check, and using <code class="literal">as.numeric</code> for number and <code class="literal">as.POSIXct</code> for all <code class="literal">calendar_date</code> fields would resolve most of the preceding issues.</p><p>Well, did you ever hear that around 80 percent of data analysis is spent on data preparation?</p><p>Parsing and restructuring JSON and XML to <code class="literal">data.frame</code> can take a long time, especially when you are dealing with hierarchical lists primarily. The<a class="indexterm" id="id195"/> <code class="literal">jsonlite</code> package tries to overcome this issue by transforming R objects into a conventional JSON data structure and vice-versa instead of raw conversion. This means from a practical point of view that <code class="literal">jsonlite::fromJSON</code> will result in <code class="literal">data.frame</code> instead of raw list if possible, and it makes the interchange data format even more seamless. Unfortunately, we cannot always transform lists to a tabular format; in such cases, the list <a class="indexterm" id="id196"/>transformations can be speeded up by for example the<a class="indexterm" id="id197"/> <code class="literal">rlist</code> package. Please find more details on list manipulations in <a class="link" href="ch14.html" title="Chapter 14. Analyzing the R Community">Chapter 14</a>, <em>Analyzing the R Community</em>.</p><div><div><h3 class="title"><a id="note24"/>Note</h3><p>
<strong>Extensible Markup Language</strong> (<strong>XML</strong>)<a class="indexterm" id="id198"/> was originally developed by the World Wide Web Consortium in 1996 to store documents in a both human-readable and machine-readable format. This popular syntax is used in for example the Microsoft Office Open XML and Open/LibreOffice OpenDocument file formats, in RSS feeds, and in various configuration files. As the format is also highly used for the interchange of data over the Internet, data is often available in XML as the only option—especially with some older APIs.</p></div></div><p>Let us also see how we can handle another popular online data interchange format besides JSON. The XML API can be used in a similar way, but we must define the desired output format in the endpoint URL: <a class="ulink" href="http://data.consumerfinance.gov/api/views.xml">http://data.consumerfinance.gov/api/views.xml</a>, as you should be able to see in the following screenshot:</p><div><img alt="Other popular online data formats" src="img/2028OS_02_04.jpg"/></div><p>It seems that the<a class="indexterm" id="id199"/> XML output of the API differs from what we have seen in the JSON format, and it simply includes the rows that we are interested in. This way, we can simply parse the XML document and extract the rows from the response then transform them to <code class="literal">data.frame</code>:</p><div><pre class="programlisting"><strong>&gt; library(XML)</strong>
<strong>&gt; doc &lt;- xmlParse(paste0(u, '/25ei-6bcr/rows.xml?max_rows=5'))</strong>
<strong>&gt; df  &lt;- xmlToDataFrame(nodes = getNodeSet(doc,"//response/row/row"))</strong>
<strong>&gt; str(df)</strong>
<strong>'data.frame':  5 obs. of  11 variables:</strong>
<strong> $ complaint_id        : Factor w/ 5 levels "2083","2216",..: 1 2 ...</strong>
<strong> $ product             : Factor w/ 1 level "Credit card": 1 1 ...</strong>
<strong> $ submitted_via       : Factor w/ 2 levels "Referral","Web": 1 1 ...</strong>
<strong> $ date_recieved       : Factor w/ 1 level "2011-12-01T00:00:00" ...</strong>
<strong> $ zip_code            : Factor w/ 1 level "": 1 1 ...</strong>
<strong> $ issue               : Factor w/ 5 levels  ...</strong>
<strong> $ date_sent_to_company: Factor w/ 3 levels "2011-12-01T00:00:00" ...</strong>
<strong> $ company             : Factor w/ 5 levels "Amex" ....</strong>
<strong> $ company_response    : Factor w/ 1 level "Closed without relief"...</strong>
<strong> $ timely_response     : Factor w/ 1 level "Yes": 1 1 ...</strong>
<strong> $ consumer_disputed   : Factor w/ 2 levels "No","Yes": 1 1 ...</strong>
</pre></div><p>Although we could manually set the desired classes of the variables in the <code class="literal">colClasses</code> argument passed to <code class="literal">xmlToDataFrame</code>, just like in <code class="literal">read.tables</code> we can also fix this issue afterwards with a quick <code class="literal">helper</code> function:</p><div><pre class="programlisting"><strong>&gt; is.number &lt;- function(x)</strong>
<strong>+     all(!is.na(suppressWarnings(as.numeric(as.character(x)))))</strong>
<strong>&gt; for (n in names(df))</strong>
<strong>+     if (is.number(df[, n]))</strong>
<strong>+         df[, n] &lt;- as.numeric(as.character(df[, n]))</strong>
</pre></div><p>So we tried to guess if a column includes only numbers, and convert those to <code class="literal">numeric</code> if our helper function returns <code class="literal">TRUE</code>. Please note that we first convert the <code class="literal">factor</code> to <code class="literal">character</code> before transforming to number, as a direct conversion from <code class="literal">factor</code> to <code class="literal">numeric</code> would return the <code class="literal">factor</code> order instead of the real value. One might also try to resolve this issue with the <code class="literal">type.convert</code> function, which is used by default in <code class="literal">read.table</code>.</p><div><div><h3 class="title"><a id="note25"/>Note</h3><p>To test similar <a class="indexterm" id="id200"/>APIs and JSON or XML resources, you may find it interesting to check out the API of Twitter, GitHub, or probably any other online service provider. On the other hand, there is also another open-source service based on R that can return XML, JSON, or CSV files from any R code. Please<a class="indexterm" id="id201"/> find more details at <a class="ulink" href="http://www.opencpu.org">http://www.opencpu.org</a>.</p></div></div><p>So now we can process structured data from various kinds of downloadable data formats but, as there are still some other data source options to master, I promise you it's worth it to keep reading.</p></div>
<div><div><div><div><h1 class="title"><a id="ch02lvl1sec17"/>Reading data from HTML tables</h1></div></div></div><p>According to the traditional document<a class="indexterm" id="id202"/> formats on the World Wide Web, most texts and <a class="indexterm" id="id203"/>data are served in HTML pages. We can often find interesting pieces of information in for example HTML tables, from which it's pretty easy to copy and paste data into an Excel spreadsheet, save that to disk, and load it to R afterwards. But it takes time, it's boring, and can be automated anyway.</p><p>Such HTML tables can be easily generated with the help of the aforementioned API of the Customer Compliant Database. If we do not set the required output format for which we used XML or JSON earlier, then the browser returns a HTML table instead, as you should be able to see in the following screenshot:</p><div><img alt="Reading data from HTML tables" src="img/2028OS_02_05.jpg"/></div><p>Well, in the R console it's a bit more complicated as the browser sends some non-default HTTP headers<a class="indexterm" id="id204"/> while using curl, so the preceding URL would simply return a <a class="indexterm" id="id205"/>JSON list. To get HTML, let the server know <a class="indexterm" id="id206"/>that we expect HTML output. To do so, simply set the appropriate HTTP header of the query:</p><div><pre class="programlisting"><strong>&gt; doc &lt;- getURL(paste0(u, '/25ei-6bcr/rows?max_rows=5'),</strong>
<strong>+   httpheader = c(Accept = "text/html"))</strong>
</pre></div><p>The <code class="literal">XML</code> package <a class="indexterm" id="id207"/>provides an extremely easy way to parse all the HTML tables from a document or specific nodes with the help of the <code class="literal">readHTMLTable</code> function, which returns a <code class="literal">list</code> of <code class="literal">data.frames</code> by default:</p><div><pre class="programlisting"><strong>&gt; res &lt;- readHTMLTable(doc)</strong>
</pre></div><p>To get only the first table on the<a class="indexterm" id="id208"/> page, we can filter <code class="literal">res</code> afterwards or pass the <code class="literal">which</code> <a class="indexterm" id="id209"/>argument to <code class="literal">readHTMLTable</code>. The following two R expressions have the very same results:</p><div><pre class="programlisting"><strong>&gt; df &lt;- res[[1]]</strong>
<strong>&gt; df &lt;- readHTMLTable(doc, which = 1)</strong>
</pre></div><div><div><div><div><h2 class="title"><a id="ch02lvl2sec17"/>Reading tabular data from static Web pages</h2></div></div></div><p>Okay, so far we have seen a <a class="indexterm" id="id210"/>bunch of variations on the same theme, but<a class="indexterm" id="id211"/> what if we do not find a downloadable dataset in any popular data format? For example, one might be interested in the available R packages hosted at CRAN, whose list is available at <a class="ulink" href="http://cran.r-project.org/web/packages/available_packages_by_name.html">http://cran.r-project.org/web/packages/available_packages_by_name.html</a>. How do we <a class="indexterm" id="id212"/>scrape that? No need to call <code class="literal">RCurl</code> or to specify custom headers, still less do we have to download the file first; it's enough to pass the URL to <code class="literal">readHTMLTable</code>:</p><div><pre class="programlisting"><strong>&gt; res &lt;- readHTMLTable('http://cran.r-project.org/Web/packages/available_packages_by_name.html')</strong>
</pre></div><p>So <code class="literal">readHTMLTable</code> can directly fetch HTML pages, then it extracts all the HTML tables to <code class="literal">data.frame</code> R objects, and returns a <code class="literal">list</code> of those. In the preceding example, we got a <code class="literal">list</code> of only one <code class="literal">data.frame</code> with all the package names and descriptions as columns.</p><p>Well, this amount of textual information is not really informative with the <code class="literal">str</code> function. For a quick example of processing and visualizing this type of raw data, and to present the plethora of available features by means of R packages at CRAN, now we can create a word cloud of the package descriptions with some nifty functions<a class="indexterm" id="id213"/> from the <code class="literal">wordcloud</code> and the<a class="indexterm" id="id214"/> <code class="literal">tm</code> packages:</p><div><pre class="programlisting"><strong>&gt; library(wordcloud)</strong>
<strong>Loading required package: Rcpp</strong>
<strong>Loading required package: RColorBrewer</strong>
<strong>&gt; wordcloud(res[[1]][, 2])</strong>
<strong>Loading required package: tm</strong>
</pre></div><p>This short command results in the following screenshot, which shows the most frequent words found in the R package descriptions. The position of the words has no special meaning, but the larger the font size, the higher the frequency. Please see the technical description of the plot following the screenshot:</p><div><img alt="Reading tabular data from static Web pages" src="img/2028OS_02_06.jpg"/></div><p>So we simply passed all the <a class="indexterm" id="id215"/>strings from the second column of the first <code class="literal">list</code> <a class="indexterm" id="id216"/>element to the <code class="literal">wordcloud</code> function, which <a class="indexterm" id="id217"/>automatically runs a few text-mining scripts from the <code class="literal">tm</code> package on the text. You can find more details on this topic in <a class="link" href="ch07.html" title="Chapter 7. Unstructured Data">Chapter 7</a>, <em>Unstructured Data</em>. Then, it renders the words with a relative size weighted by the number of occurrences in the package descriptions. It seems that R packages are indeed primarily targeted at building models and applying multivariate tests on data.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch02lvl1sec18"/>Scraping data from other online sources</h1></div></div></div><p>Although the <code class="literal">readHTMLTable</code> function is very useful, sometimes the data is not structured in<a class="indexterm" id="id218"/> tables, but rather it's available only as HTML lists. Let's<a class="indexterm" id="id219"/> demonstrate such a data format by checking all the R packages listed in the relevant CRAN Task View at <a class="ulink" href="http://cran.r-project.org/web/views/WebTechnologies.html">http://cran.r-project.org/web/views/WebTechnologies.html</a>, as you can see in the following screenshot:</p><div><img alt="Scraping data from other online sources" src="img/2028OS_02_07.jpg"/></div><p>So we see a HTML list of the package names along with a URL pointing to the CRAN, or in some cases to the GitHub repositories. To proceed, first we have to get acquainted a bit with the HTML sources to see how we can parse them. You can do that easily either in Chrome or Firefox: just right-click on the <strong>CRAN</strong> packages heading at the top of the list, and choose <strong>Inspect Element</strong>, as you can see in the following screenshot:</p><div><img alt="Scraping data from other online sources" src="img/2028OS_02_08.jpg"/></div><p>So we have the list of related R packages in an <code class="literal">ul</code> (unordered list) HTML tag, just after the <code class="literal">h3</code> (level 3 heading) tag holding the <code class="literal">CRAN packages</code> string.</p><p>In short:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">We have to <a class="indexterm" id="id220"/>parse this HTML file</li><li class="listitem" style="list-style-type: disc">Look for the <a class="indexterm" id="id221"/>third-level heading holding the search term </li><li class="listitem" style="list-style-type: disc">Get all the list elements from the subsequent unordered HTML list</li></ul></div><p>This can be done by, for example, the XML Path Language, which has a special syntax to select nodes in XML/HTML documents via queries.</p><div><div><h3 class="title"><a id="note26"/>Note</h3><p>For more details and R-driven examples, see <em>Chapter 4</em>, <em>XPath, XPointer, and XInclude</em> of the book <em>XML and Web Technologies for Data Sciences with R</em> written by Deborah Nolan and Duncan Temple Lang in the Use R! series from Springer. Please see more references in the <em>Appendix</em> at the end of the book.</p></div></div><p>XPath can be rather ugly and complex at first glance. For example, the preceding list can be described with:</p><div><pre class="programlisting"><strong>//h3[text()='CRAN packages:']/following-sibling::ul[1]/li</strong>
</pre></div><p>Let me elaborate a bit on this:</p><div><ol class="orderedlist arabic"><li class="listitem">We are looking for a <code class="literal">h3</code> tag which has <code class="literal">CRAN packages</code> as its text, so we are searching for a specific node in the whole document with these attributes.</li><li class="listitem">Then the <code class="literal">following-siblings</code> expression stands for all the subsequent nodes at the same hierarchy level as the chosen <code class="literal">h3</code> tag.</li><li class="listitem">Filter to find only <code class="literal">ul</code> HTML tags.</li><li class="listitem">As we have several of those, we select only the first of the further siblings with the index <code class="literal">(1)</code> between the brackets.</li><li class="listitem">Then we <a class="indexterm" id="id222"/>simply select all <code class="literal">li</code> tags (the list elements) inside <a class="indexterm" id="id223"/>that.</li></ol></div><p>Let's try it in R:</p><div><pre class="programlisting"><strong>&gt; page &lt;- htmlParse(file = </strong>
<strong>+   'http://cran.r-project.org/Web/views/WebTechnologies.html')</strong>
<strong>&gt; res  &lt;- unlist(xpathApply(doc = page, path =</strong>
<strong>+   "//h3[text()='CRAN packages:']/following-sibling::ul[1]/li",</strong>
<strong>+   fun  = xmlValue))</strong>
</pre></div><p>And we have the character vector of the related 118 R packages:</p><div><pre class="programlisting"><strong>&gt; str(res)</strong>
<strong> chr [1:118] "acs" "alm" "anametrix" "AWS.tools" "bigml" ...</strong>
</pre></div><p>XPath<a class="indexterm" id="id224"/> is really powerful for selecting and searching for nodes in HTML documents, so is <code class="literal">xpathApply</code>. The latter is the R wrapper around most of the XPath functionality in <code class="literal">libxml</code>, which makes the process rather quick and efficient. One might rather use the <code class="literal">xpathSApply</code> instead, which tries to simplify the returned list of elements, just like <code class="literal">sapply</code> does compared to the <code class="literal">lapply</code> function. So we can also update our previous code to save the <code class="literal">unlist</code> call:</p><div><pre class="programlisting"><strong>&gt; res &lt;- xpathSApply(page, path =</strong>
<strong>+ "//h3[text()='CRAN packages:']/following-sibling::ul[1]/li",  </strong>
<strong>+   fun  = xmlValue)</strong>
</pre></div><p>The attentive reader must have noticed that the returned list was a simple character vector, while the original HTML list also included the URLs of the aforementioned packages. Where and why did those vanish?</p><p>We can blame <code class="literal">xmlValue</code> for this result, which we called instead of the default <code class="literal">NULL</code> as the evaluating function to extract the nodes from the original document at the <code class="literal">xpathSApply</code> call. This function simply extracts the raw text content of each leaf node without any children, which explains this behavior. What if we are rather interested in the package URLs?</p><p>Calling <code class="literal">xpathSapply</code> without a specified fun returns all the raw child nodes, which is of no direct help, and we shouldn't try to apply some regular expressions on those. The help page of <code class="literal">xmlValue</code> can point us to some similar functions that can be very handy with such tasks. Here we definitely want to use <code class="literal">xmlAttrs</code>:</p><div><pre class="programlisting"><strong>&gt; xpathSApply(page,</strong>
<strong>+   "//h3[text()='CRAN packages:']/following-sibling::ul[1]/li/a",</strong>
<strong>+   xmlAttrs, 'href')</strong>
</pre></div><p>Please note that an<a class="indexterm" id="id225"/> updated path was used here, where now we<a class="indexterm" id="id226"/> selected all the <code class="literal">a</code> tags instead of the <code class="literal">li</code> parents. And, instead of the previously introduced <code class="literal">xmlValue</code>, now we called <code class="literal">xmlAttrs</code> with the <code class="literal">'href'</code> extra argument. This simply extracts all the <code class="literal">href</code> arguments of all the related <code class="literal">a</code> nodes.</p><p>With these primitives, you will be able to fetch any publicly available data from online sources, although sometimes the implementation can end up being rather complex.</p><div><div><h3 class="title"><a id="note27"/>Note</h3><p>On the other hand, please be sure to always consult the terms and conditions and other legal documents of all potential data sources, as fetching data is often prohibited by the copyright owner.</p><p>Beside the legal issues, it's also wise to think of fetching and crawling data from the technical point of view of the service provider. If you start to send a plethora of queries to a server without consulting with their administrators beforehand, this action might be construed as a network attack and/or might result in an unwanted load on the servers. To keep it simple, always use a sane delay between your queries. This should be for example, a 2-second pause between queries at a minimum, but it's better to check the <em>Crawl-delay</em> directive set in the site's <em>robot.txt</em>, which can be found in the root path if available. This file also contains other directives if crawling is allowed or limited. Most of the data provider sites also have some technical documentation on data crawling; please be sure to search for Rate limits and throttling.</p></div></div><p>And sometimes we are just simply lucky in that someone else has already written the tricky XPath selectors or other interfaces, so we can load data from<a class="indexterm" id="id227"/> Web services and homepages with the help of native R packages.</p></div>
<div><div><div><div><h1 class="title"><a id="ch02lvl1sec19"/>R packages to interact with data source APIs</h1></div></div></div><p>Although it's great that we can read <a class="indexterm" id="id228"/>HTML tables, CSV<a class="indexterm" id="id229"/> files and JSON and XML data, and even parse raw HTML documents to store some parts of those in a dataset, there is no sense in spending too much time developing custom tools until we have no other option. First, always start with a quick look on the Web Technologies and Services CRAN Task View; also search R-bloggers, StackOverflow, and GitHub for any possible solution before getting your hands dirty with custom XPath selectors and JSON list magic.</p><div><div><div><div><h2 class="title"><a id="ch02lvl2sec18"/>Socrata Open Data API</h2></div></div></div><p>Let's do this for our previous<a class="indexterm" id="id230"/> examples by searching for Socrata, the Open Data Application Program Interface of the Consumer Financial Protection Bureau. Yes, there is a package for that:</p><div><pre class="programlisting"><strong>&gt; library(RSocrata)</strong>
<strong>Loading required package: httr</strong>
<strong>Loading required package: RJSONIO</strong>

<strong>Attaching package: 'RJSONIO'</strong>

<strong>The following objects are masked from 'package:rjson':</strong>

<strong>    fromJSON, toJSON</strong>
</pre></div><p>As a matter of fact, the<a class="indexterm" id="id231"/> <code class="literal">RSocrata</code> package uses the same JSON sources (or CSV files), as we did before. Please note the warning message, which says that <code class="literal">RSocrata</code> depends on another JSON parser R package rather than the one we used, so some function names are conflicting. It's probably wise to <code class="literal">detach('package:rjson')</code> before automatically loading the<a class="indexterm" id="id232"/> <code class="literal">RJSONIO</code> package.</p><p>Loading the Customer Complaint Database by the given URL is pretty easy with <code class="literal">RSocrata</code>:</p><div><pre class="programlisting"><strong>&gt; df &lt;- read.socrata(paste0(u, '/25ei-6bcr'))</strong>
<strong>&gt; str(df)</strong>
<strong>'data.frame':  18894 obs. of  11 variables:</strong>
<strong> $ Complaint.ID        : int  2240 2243 2260 2254 2259 2261 ...</strong>
<strong> $ Product             : chr  "Credit card" "Credit card" ...</strong>
<strong> $ Submitted.via       : chr  "Web" "Referral" "Referral" ...</strong>
<strong> $ Date.received       : chr  "12/01/2011" "12/01/2011" ...</strong>
<strong> $ ZIP.code            : chr  ...</strong>
<strong> $ Issue               : chr  ...</strong>
<strong> $ Date.sent.to.company: POSIXlt, format: "2011-12-19" ...</strong>
<strong> $ Company             : chr  "Citibank" "HSBC" ...</strong>
<strong> $ Company.response    : chr  "Closed without relief" ...</strong>
<strong> $ Timely.response.    : chr  "Yes" "Yes" "No" "Yes" ...</strong>
<strong> $ Consumer.disputed.  : chr  "No" "No" "" "No" ...</strong>
</pre></div><p>We got <code class="literal">numeric</code> values for numbers, and the dates are also automatically processed to <code class="literal">POSIXlt</code>!</p><p>Similarly, the Web <a class="indexterm" id="id233"/>Technologies and Services CRAN Task View contains more than a hundred R packages to interact with data sources on the Web in natural sciences such as ecology, genetics, chemistry, weather, finance, economics, and marketing, but we can also find R packages to fetch texts, bibliography resources, Web analytics, news, and map and social media data besides some other topics. Due to page limitations, here we will only focus on the most-used packages.</p></div><div><div><div><div><h2 class="title"><a id="ch02lvl2sec19"/>Finance APIs</h2></div></div></div><p>Yahoo! and Google <a class="indexterm" id="id234"/>Finance are pretty standard free data sources for all those working in the industry. Fetching for example stock, metal, or foreign exchange prices is extremely easy with the<a class="indexterm" id="id235"/> <code class="literal">quantmod</code> package and the aforementioned service providers. For example, let us see the most recent stock prices for Agilent Technologies with the <code class="literal">A</code> ticker symbol:</p><div><pre class="programlisting"><strong>&gt; library(quantmod)</strong>
<strong>Loading required package: Defaults</strong>
<strong>Loading required package: xts</strong>
<strong>Loading required package: zoo</strong>

<strong>Attaching package: 'zoo'</strong>

<strong>The following objects are masked from 'package:base':</strong>

<strong>    as.Date, as.Date.numeric</strong>

<strong>Loading required package: TTR</strong>
<strong>Version 0.4-0 included new data defaults. See ?getSymbols.</strong>
<strong>&gt; tail(getSymbols('A', env = NULL))</strong>
<strong>           A.Open A.High A.Low A.Close A.Volume A.Adjusted</strong>
<strong>2014-05-09  55.26  55.63 54.81   55.39  1287900      55.39</strong>
<strong>2014-05-12  55.58  56.62 55.47   56.41  2042100      56.41</strong>
<strong>2014-05-13  56.63  56.98 56.40   56.83  1465500      56.83</strong>
<strong>2014-05-14  56.78  56.79 55.70   55.85  2590900      55.85</strong>
<strong>2014-05-15  54.60  56.15 53.75   54.49  5740200      54.49</strong>
<strong>2014-05-16  54.39  55.13 53.92   55.03  2405800      55.03</strong>
</pre></div><p>By default, <code class="literal">getSymbols</code> assigns the fetched results to the <code class="literal">parent.frame</code> (usually the global) environment with the name of the symbols, while specifying <code class="literal">NULL</code> as the desired environment simply returns the fetched results as an <code class="literal">xts</code> time-series object, as seen earlier.</p><p>Foreign exchange rates can be fetched just as easily:</p><div><pre class="programlisting"><strong>&gt; getFX("USD/EUR")</strong>
<strong>[1] "USDEUR"</strong>
<strong>&gt; tail(USDEUR)</strong>
<strong>           USD.EUR</strong>
<strong>2014-05-13  0.7267</strong>
<strong>2014-05-14  0.7281</strong>
<strong>2014-05-15  0.7293</strong>
<strong>2014-05-16  0.7299</strong>
<strong>2014-05-17  0.7295</strong>
<strong>2014-05-18  0.7303</strong>
</pre></div><p>The returned string of <code class="literal">getSymbols</code> refers to the R variable in which the data was saved inside <code class="literal">.GlobalEnv</code>. To see all the available data sources, let's query the related S3 methods:</p><div><pre class="programlisting"><strong>&gt; methods(getSymbols)</strong>
<strong> [1] getSymbols.csv    getSymbols.FRED   getSymbols.google</strong>
<strong> [4] getSymbols.mysql  getSymbols.MySQL  getSymbols.oanda </strong>
<strong> [7] getSymbols.rda    getSymbols.RData  getSymbols.SQLite</strong>
<strong>[10] getSymbols.yahoo</strong>
</pre></div><p>So besides some offline data sources, we can <a class="indexterm" id="id236"/>query Google, Yahoo!, and OANDA for recent financial information. To see the full list of available symbols, the already loaded <code class="literal">TTR</code> package <a class="indexterm" id="id237"/>might help:</p><div><pre class="programlisting"><strong>&gt; str(stockSymbols())</strong>
<strong>Fetching AMEX symbols...</strong>
<strong>Fetching NASDAQ symbols...</strong>
<strong>Fetching NYSE symbols...</strong>
<strong>'data.frame':  6557 obs. of  8 variables:</strong>
<strong> $ Symbol   : chr  "AAMC" "AA-P" "AAU" "ACU" ...</strong>
<strong> $ Name     : chr  "Altisource Asset Management Corp" ...</strong>
<strong> $ LastSale : num  841 88.8 1.3 16.4 15.9 ...</strong>
<strong> $ MarketCap: num  1.88e+09 0.00 8.39e+07 5.28e+07 2.45e+07 ...</strong>
<strong> $ IPOyear  : int  NA NA NA 1988 NA NA NA NA NA NA ...</strong>
<strong> $ Sector   : chr  "Finance" "Capital Goods" ...</strong>
<strong> $ Industry : chr  "Real Estate" "Metal Fabrications" ...</strong>
<strong> $ Exchange : chr  "AMEX" "AMEX" "AMEX" "AMEX" ...</strong>
</pre></div><div><div><h3 class="title"><a id="note28"/>Note</h3><p>Find more information on how to handle and analyze similar datasets in <a class="link" href="ch12.html" title="Chapter 12. Analyzing Time-series">Chapter 12</a>, <em>Analyzing Time-series</em>.</p></div></div></div><div><div><div><div><h2 class="title"><a id="ch02lvl2sec20"/>Fetching time series with Quandl</h2></div></div></div><p>Quandl provides access to<a class="indexterm" id="id238"/> millions of similar time-series data in a standard <a class="indexterm" id="id239"/>format, via a custom API, from around 500 data sources. In R, the<a class="indexterm" id="id240"/> <code class="literal">Quandl</code> package provides easy access to all these open data in various industries all around the world. Let us see for example the dividends paid by Agilent Technologies published by the U.S. Securities and Exchange Commission. To do so, simply search for "Agilent Technologies" at the <a class="ulink" href="http://www.quandl.com">http://www.quandl.com</a> homepage, and provide the code of the desired data from the search results to the <code class="literal">Quandl</code> function:</p><div><pre class="programlisting"><strong>&gt; library(Quandl)</strong>
<strong>&gt; Quandl('SEC/DIV_A')</strong>
<strong>        Date Dividend</strong>
<strong>1 2013-12-27    0.132</strong>
<strong>2 2013-09-27    0.120</strong>
<strong>3 2013-06-28    0.120</strong>
<strong>4 2013-03-28    0.120</strong>
<strong>5 2012-12-27    0.100</strong>
<strong>6 2012-09-28    0.100</strong>
<strong>7 2012-06-29    0.100</strong>
<strong>8 2012-03-30    0.100</strong>
<strong>9 2006-11-01    2.057</strong>
<strong>Warning message:</strong>
<strong>In Quandl("SEC/DIV_A") :</strong>
<strong>  It would appear you aren't using an authentication token. Please visit http://www.quandl.com/help/r or your usage may be limited.</strong>
</pre></div><p>As you can see, the API is rather limited without a valid authentication token, which can be redeemed at the <code class="literal">Quandl</code> homepage for free. To set your token, simply pass that to the <code class="literal">Quandl.auth</code> function.</p><p>This package also lets you:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Fetch filtered data by time</li><li class="listitem" style="list-style-type: disc">Perform some transformations of the data on the server side—such as cumulative sums and the first differential</li><li class="listitem" style="list-style-type: disc">Sort the data</li><li class="listitem" style="list-style-type: disc">Define the desired class of the returning object—such as <code class="literal">ts</code>, <code class="literal">zoo</code>, and <code class="literal">xts</code></li><li class="listitem" style="list-style-type: disc">Download some meta-information on the data source</li></ul></div><p>The latter is saved as <code class="literal">attributes</code> of the returning R object. So, for example, to see the frequency of the <a class="indexterm" id="id241"/>queried<a class="indexterm" id="id242"/> dataset, call:</p><div><pre class="programlisting"><strong>&gt; attr(Quandl('SEC/DIV_A', meta = TRUE), 'meta')$frequency</strong>
<strong>[1] "quarterly"</strong>
</pre></div></div><div><div><div><div><h2 class="title"><a id="ch02lvl2sec21"/>Google documents and analytics</h2></div></div></div><p>You might however be more<a class="indexterm" id="id243"/> interested in loading your own or custom data from Google Docs, to which end the<a class="indexterm" id="id244"/> <code class="literal">RGoogleDocs</code> package is a great help and is available for download at the <a class="ulink" href="http://www.omegahat.org/">http://www.omegahat.org/</a> homepage. It provides authenticated access to Google spreadsheets with both read and write access.</p><p>Unfortunately, this package is rather outdated and uses some deprecated API functions, so you might be better trying some newer alternatives, such as the recently<a class="indexterm" id="id245"/> released <code class="literal">googlesheets</code> package, which can manage Google Spreadsheets (but not other documents) from R.</p><p>Similar packages are also available to interact with Google Analytics or Google Adwords for all those, who would like to analyze page visits or ad performance in R.</p></div><div><div><div><div><h2 class="title"><a id="ch02lvl2sec22"/>Online search trends</h2></div></div></div><p>On the other hand, we<a class="indexterm" id="id246"/> interact with APIs to download public data. Google also provides access to some public data of the World Bank, IMF, US Census Bureau, and so on at <a class="ulink" href="http://www.google.com/publicdata/directory">http://www.google.com/publicdata/directory</a> and also some of their own internal data in the form of search trends at <a class="ulink" href="http://google.com/trends">http://google.com/trends</a>.</p><p>The latter can be queried extremely easily with<a class="indexterm" id="id247"/> the <code class="literal">GTrendsR</code> package, which is not yet available on CRAN, but we can at least practice how to install R packages from other sources. The <code class="literal">GTrendR</code> code repository can be found on <code class="literal">BitBucket</code>, from where it's really convenient to install it with <a class="indexterm" id="id248"/>the <code class="literal">devtools</code> package:</p><div><div><h3 class="title"><a id="tip04"/>Tip</h3><p>To make sure you install the same version of <code class="literal">GTrensR</code> as used in the following, you can specify the <code class="literal">branch</code>, <code class="literal">commit</code>, or other reference in the <code class="literal">ref</code> argument of the <code class="literal">install_bitbucket</code> (or <code class="literal">install_github</code>) function. Please see the <em>References </em>section in the <em>Appendix</em> at the end of the book for the commit hash.</p></div></div><div><pre class="programlisting"><strong>&gt; library(devtools)</strong>
<strong>&gt; install_bitbucket('GTrendsR', 'persican', quiet = TRUE)</strong>
<strong>Installing bitbucket repo(s) GTrendsR/master from persican</strong>
<strong>Downloading master.zip from https://bitbucket.org/persican/gtrendsr/get/master.zip</strong>
<strong>arguments 'minimized' and 'invisible' are for Windows only </strong>
</pre></div><p>So installing R packages from BitBucket or GitHub is as easy as providing the name of the code repository and author's username and allowing <code class="literal">devtools</code> to do the rest: downloading the sources and compiling them.</p><p>Windows users should install <code class="literal">Rtools</code> prior<a class="indexterm" id="id249"/> to compiling packages from the source: <a class="ulink" href="http://cran.r-project.org/bin/windows/Rtools/">http://cran.r-project.org/bin/windows/Rtools/</a>. We also enabled the quiet mode, to suppress compilation logs and the boring details.</p><p>After the package has been installed, we can load it in the traditional way:</p><div><pre class="programlisting"><strong>&gt; library(GTrendsR)</strong>
</pre></div><p>First, we have to <a class="indexterm" id="id250"/>authenticate with a valid Google username and password before being able to query the Google Trends database. Our search term will be "how to install R":</p><div><div><h3 class="title"><a id="tip05"/>Tip</h3><p>Please make sure you provide a valid username and password; otherwise the following query will fail.</p></div></div><div><pre class="programlisting"><strong>&gt; conn &lt;- gconnect('some Google username', 'some Google password')</strong>
<strong>&gt; df   &lt;- gtrends(conn, query = 'how to install R')</strong>
<strong>&gt; tail(df$trend)</strong>
<strong>         start        end how.to.install.r</strong>
<strong>601 2015-07-05 2015-07-11               86</strong>
<strong>602 2015-07-12 2015-07-18               70</strong>
<strong>603 2015-07-19 2015-07-25              100</strong>
<strong>604 2015-07-26 2015-08-01               75</strong>
<strong>605 2015-08-02 2015-08-08               73</strong>
<strong>606 2015-08-09 2015-08-15               94 </strong>
</pre></div><p>The returned dataset includes weekly metrics on the relative amount of search queries on R installation. The data shows that the highest activity was recorded in the middle of July, while only around 75 percent of those search queries were triggered at the beginning of the next month. So Google do not publish raw search query statistics, but rather comparative studies can be done with different search terms and time periods.</p></div><div><div><div><div><h2 class="title"><a id="ch02lvl2sec23"/>Historical weather data</h2></div></div></div><p>There are also various packages<a class="indexterm" id="id251"/> providing access to data sources for all R users in Earth Science. For example, the<a class="indexterm" id="id252"/> <code class="literal">RNCEP</code> package can download historical weather data from the National Centers for Environmental Prediction for more than one hundred years in six hourly resolutions. The <code class="literal">weatherData</code> package <a class="indexterm" id="id253"/>provides direct access to <a class="ulink" href="http://wunderground.com">http://wunderground.com</a>. For a quick example, let us download the daily temperature averages for the last seven days in London:</p><div><pre class="programlisting"><strong>&gt; library(weatherData)</strong>
<strong>&gt; getWeatherForDate('London', start_date = Sys.Date()-7, end_date = Sys.Date())</strong>
<strong>Retrieving from: http://www.wunderground.com/history/airport/London/2014/5/12/CustomHistory.html?dayend=19&amp;monthend=5&amp;yearend=2014&amp;req_city=NA&amp;req_state=NA&amp;req_statename=NA&amp;format=1 </strong>
<strong>Checking Summarized Data Availability For London</strong>
<strong>Found 8 records for 2014-05-12 to 2014-05-19</strong>
<strong>Data is Available for the interval.</strong>
<strong>Will be fetching these Columns:</strong>
<strong>[1] "Date"              "Max_TemperatureC"  "Mean_TemperatureC"</strong>
<strong>[4] "Min_TemperatureC" </strong>
<strong>        Date Max_TemperatureC Mean_TemperatureC Min_TemperatureC</strong>
<strong>1 2014-05-12               18                13                9</strong>
<strong>2 2014-05-13               16                12                8</strong>
<strong>3 2014-05-14               19                13                6</strong>
<strong>4 2014-05-15               21                14                8</strong>
<strong>5 2014-05-16               23                16                9</strong>
<strong>6 2014-05-17               23                17               11</strong>
<strong>7 2014-05-18               23                18               12</strong>
<strong>8 2014-05-19               24                19               13</strong>
</pre></div><p>Please note that an<a class="indexterm" id="id254"/> unimportant part of the preceding output was suppressed, but what happened here is quite straightforward: the package fetched the specified URL, which is a CSV file by the way, then parsed that with some additional information. Setting <code class="literal">opt_detailed</code> to <code class="literal">TRUE</code> would also return intraday data with a 30-minute resolution.</p></div><div><div><div><div><h2 class="title"><a id="ch02lvl2sec24"/>Other online data sources</h2></div></div></div><p>Of course, this short <a class="indexterm" id="id255"/>chapter cannot provide an overview of querying all the available online data sources and R implementations, but please consult the Web Technologies and Services CRAN Task View, R-bloggers, StackOverflow, and the resources in the <em>References</em> chapter at the end of the book to look for any already existing R packages or helper functions before creating your own crawler R scripts.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch02lvl1sec20"/>Summary</h1></div></div></div><p>This chapter focused on how to fetch and process data directly from the Web, including some problems with downloading files, processing XML and JSON formats, parsing HTML tables, applying XPath selectors to extract data from HTML pages, and interacting with RESTful APIs.</p><p>Although some examples in this chapter might appear to have been an idle struggle with the Socrata API, it turned out that the <code class="literal">RSocrata</code> package provides production-ready access to all those data. However, please bear in mind that you will face some situations without ready-made R packages; thus, as a data hacker, you will have to get your hands dirty with all the JSON, HTML and XML sources.</p><p>In the next chapter, we will discover how to filter and aggregate the already acquired and loaded data with the top, most-used methods for reshaping and restructuring data.</p></div></body></html>