<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch03"/>Chapter 3. Understanding OpenCL Data Types</h1></div></div></div><p>In this chapter, we are going to cover the following recipes:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Initializing the OpenCL scalar data types</li><li class="listitem" style="list-style-type: disc">Initializing the OpenCL vector data types</li><li class="listitem" style="list-style-type: disc">Using OpenCL scalar types</li><li class="listitem" style="list-style-type: disc">Understanding OpenCL vector types</li><li class="listitem" style="list-style-type: disc">Vector and scalar address spaces</li><li class="listitem" style="list-style-type: disc">Configuring your OpenCL projects to enable the double data type</li></ul></div><div><div><div><div><h1 class="title"><a id="ch03lvl1sec24"/>Introduction</h1></div></div></div><p>OpenCL supports a wide range of data types derived from the C programming language. They are widely classified into two groups called scalars and vectors. Scalars are basically elemental values, whereas vectors are a collection of elemental values and a good thing about vectors is that many OpenCL SDK vendors have provided automated vectorization which allows the values to be loaded into wide, that is, 128-bit, 256-bit, or 512-bit registers for consumption.</p><p>OpenCL scalar<a id="id178" class="indexterm"/> integral data types consists of the signed and unsigned types of <code class="literal">bool</code>, <code class="literal">char</code>, <code class="literal">short</code>, <code class="literal">int</code>, <code class="literal">long</code>, <code class="literal">uchar</code>, <code class="literal">ushort</code>, <code class="literal">uint</code>, and <code class="literal">ulong</code> respectively; for floating-point values there are <code class="literal">float</code>, <code class="literal">half</code>, and <code class="literal">double</code>. To represent those types in your host program, you have to just prepend the letters <code class="literal">cl_</code> to each type, which the OpenCL compiler will understand.</p><p>OpenCL vector data types<a id="id179" class="indexterm"/> consists of a multiple of scalar data integral and floating-point data types and they are <code class="literal">char&lt;N&gt;</code>, <code class="literal">short&lt;N&gt;</code>, <code class="literal">int&lt;N&gt;</code>, <code class="literal">long&lt;N&gt;</code>, <code class="literal">uchar&lt;N&gt;</code>, <code class="literal">ushort&lt;N&gt;</code>, <code class="literal">uint&lt;N&gt;</code>, <code class="literal">ulong&lt;N&gt;</code>, and <code class="literal">float&lt;N&gt;</code> where <code class="literal">&lt;N&gt;</code> represents a value of 2, 3, 4, 8, or 16. Similarly, you will represent those types in your host program by prepending the letters <code class="literal">cl_</code> to the data types.</p><p>In both the cases, if you prefer the explicit form of an <code class="literal">unsigned</code> type, then you can replace the letter <code class="literal">u</code> in the data types with the keyword <code class="literal">unsigned</code>.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch03lvl1sec25"/>Initializing the OpenCL scalar data types</h1></div></div></div><p>In this recipe, we are <a id="id180" class="indexterm"/>going to demonstrate various ways to initialize <a id="id181" class="indexterm"/>scalar types, and most of the techniques will make a lot of sense if you already have programmed using the C programming language.</p><div><div><div><div><h2 class="title"><a id="ch03lvl2sec49"/>Getting ready</h2></div></div></div><p>In addition to the regular data types defined in C which works in OpenCL, the standard have added a few more data types in addition to the ones we have mentioned in the previous section, and the following table illustrates them:</p><div><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Type</p>
</th><th style="text-align: left" valign="bottom">
<p>Description</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p><code class="literal">half</code><a id="id182" class="indexterm"/></p>
</td><td style="text-align: left" valign="top">
<p>It is a<a id="id183" class="indexterm"/> 16-bit floating-point. The <code class="literal">half</code> data type must conform to the IEEE 754-2008 <code class="literal">half precision</code> storage format.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">bool</code><a id="id184" class="indexterm"/></p>
</td><td style="text-align: left" valign="top">
<p>It is a <a id="id185" class="indexterm"/>conditional data type that evaluates to true or false. The value true expands to an integer 1 while false expands to 0.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">size_t</code><a id="id186" class="indexterm"/></p>
</td><td style="text-align: left" valign="top">
<p>It is the<a id="id187" class="indexterm"/> unsigned integer type of the result of the sizeof operator. This can be a 32-bit or 64-bit unsigned integer.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">ptrdiff_t</code><a id="id188" class="indexterm"/></p>
</td><td style="text-align: left" valign="top">
<p>It is a <a id="id189" class="indexterm"/>32-bit or 64-bit signed integer and usually it is used to represent the result of subtracting two points</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">intptr_t</code><a id="id190" class="indexterm"/></p>
</td><td style="text-align: left" valign="top">
<p>It is a <a id="id191" class="indexterm"/>32-bit or 64-bit signed integer with the property that any valid point to avoid can be converted to this type, and then converted back to point to void and the result will compare equal to the original pointer.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">uintptr_t</code><a id="id192" class="indexterm"/></p>
</td><td style="text-align: left" valign="top">
<p>It is a <a id="id193" class="indexterm"/>32-bit or 64-bit unsigned integer that has got the same property as <code class="literal">intptr_t</code>.</p>
</td></tr></tbody></table></div><p>OpenCL allows the<a id="id194" class="indexterm"/> following data types to be used interchangeably in<a id="id195" class="indexterm"/> your source codes:</p><div><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Type in OpenCL</p>
</th><th style="text-align: left" valign="bottom">
<p>Type in application</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p><code class="literal">bool</code><a id="id196" class="indexterm"/></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">n/a</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">char</code><a id="id197" class="indexterm"/></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">cl_char</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">unsigned char</code><a id="id198" class="indexterm"/>
<code class="literal">, uchar</code><a id="id199" class="indexterm"/></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">cl_uchar</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">short</code><a id="id200" class="indexterm"/></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">cl_short</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">unsigned short</code><a id="id201" class="indexterm"/>
<code class="literal">, ushort</code><a id="id202" class="indexterm"/></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">cl_ushort</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">int</code><a id="id203" class="indexterm"/></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">cl_int</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">unsigned int</code><a id="id204" class="indexterm"/>
<code class="literal">, uint</code><a id="id205" class="indexterm"/></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">cl_uint</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">long</code><a id="id206" class="indexterm"/></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">cl_long</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">unsigned long</code><a id="id207" class="indexterm"/>
<code class="literal">, ulong</code><a id="id208" class="indexterm"/></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">cl_ulong</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">float</code><a id="id209" class="indexterm"/></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">cl_float</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">double</code><a id="id210" class="indexterm"/></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">cl_double</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">half</code><a id="id211" class="indexterm"/></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">cl_half</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">size_t</code><a id="id212" class="indexterm"/></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">n/a</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">ptrdiff_t</code><a id="id213" class="indexterm"/></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">n/a</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">intptr_t</code><a id="id214" class="indexterm"/></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">n/a</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">uintptr_t</code><a id="id215" class="indexterm"/></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">n/a</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">void</code><a id="id216" class="indexterm"/></p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">void</code></p>
</td></tr></tbody></table></div><p>So following are a <a id="id217" class="indexterm"/>few examples on how you can possibly declare and <a id="id218" class="indexterm"/>define scalar data types in your source code in the kernels and host:</p><div><pre class="programlisting">float f = 1.0f;                  // In the OpenCL kernel
char c = 'a';                    // In the OpenCL kernel
const char* cs = "hello world\n";
cl_char c1 = 'b';                // in the host program
cl_float f1 = 1.0f;              // in the host program
const cl_char* css = "hello world\n";</pre></div><p>In the previous chapter, <em>Understanding OpenCL Data Transfer and Partitioning</em>, we spent some time discussing about data types and how alignment works or in other words, how data misalignment can affect the performance. Scalar data types are always aligned to the size of the data type in bytes. Built-in data types whose sizes are not a power of two must be aligned to the next larger power of two. That is, a <code class="literal">char</code> variable will be aligned a 1-byte boundary, a <code class="literal">float</code> variable will be aligned to a 4-byte boundary.</p></div><div><div><div><div><h2 class="title"><a id="ch03lvl2sec50"/>How to do it…</h2></div></div></div><p>If your application needs user-defined data types, then you need to place <code class="literal">__attribute__((aligned))</code> to those types; refer to the <a class="link" href="ch02.html" title="Chapter 2. Understanding OpenCL Data Transfer and Partitioning">Chapter 2</a>, <em>Understanding OpenCL Data Transfer and Partitioning</em> for more details.</p><p>In OpenCL, several operators convert operand values from one type to another, and this is commonly referred to as implicit conversions; another way is to apply a cast operation on operands or on the result of a binary operation. Implicit conversions between scalar built-in types are supported with the exception of <code class="literal">void</code> and <code class="literal">half</code> data types. What it means is shown in the following code:</p><div><pre class="programlisting">cl_int x = 9;
cl_float y = x; // y will get the value 9.0</pre></div><p>Or</p><div><pre class="programlisting">int x = 9;
float y = x;  // y will get the value 9.0</pre></div><p>You can use both forms in your application code. You can coerce a data type to another data type in OpenCL too, just as you can do in the C programming language. Refer to the following example:</p><div><pre class="programlisting">float f = 1.0f;
int i = (int) f; // i would receive the value of 1</pre></div><p>You can also <a id="id219" class="indexterm"/>coerce a scalar data type to a vector data type in OpenCL <a id="id220" class="indexterm"/>with the following code:</p><div><pre class="programlisting">float f = 1.0f;
float4 vf = (float4)f; // vf is a vector with elements (1.0, 1.0, 1.0, 1.0)
uchar4 vtrue = (uchar4)true; // vtrue is a vector with elements(true, true, true, true)
             // which is actually (0xff, 0xff, 0xff, 0xff)</pre></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch03lvl1sec26"/>Initializing the OpenCL vector data types</h1></div></div></div><p>Vectors are extremely <a id="id221" class="indexterm"/>powerful to an OpenCL programmer because it <a id="id222" class="indexterm"/>allows the hardware to bulk load/store data to/from memory; such computations typically take advantage of the algorithms spatial and temporal locality properties. In this recipe, we are going to familiarize ourselves with creating various types of vectors.</p><div><div><div><div><h2 class="title"><a id="ch03lvl2sec51"/>Getting ready</h2></div></div></div><p>You can initialize a vector in two primary manners and they are as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Vector literal</li><li class="listitem" style="list-style-type: disc">Vector composition</li></ul></div><p>Creating a vector literal simply means that you can construct your vector of whatever type you wish as shown in the following code:</p><div><pre class="programlisting">float a = 1.0f;
float b = 2.0f;
float c = 3.0f;
float d = 4.0f;
float4 vf = (float4) (a, b, c, d);
//vf will store (1.0f, 2.0f, 3.0f, 4.0f)</pre></div><p>Another way to initialize a vector is to do it via a scalar value as shown in the following code:</p><div><pre class="programlisting">uint4 ui4 = (uint4)(9); // ui4 will store (9, 9, 9, 9)</pre></div><p>You can also create vectors in the following fashion:</p><div><pre class="programlisting">float4 f = (float4) ((float2) (1.1f, 2.2f), 
                     (float2) (3.3f, 4.4f));
float4 f2 = (float4) (1.1f, (float2) (2.2f, 3.3f), 4.4f);</pre></div><p>The data type on the left-hand-side and right-hand-side must be same or the OpenCL compiler will issue a complaint.</p></div><div><div><div><div><h2 class="title"><a id="ch03lvl2sec52"/>How to do it…</h2></div></div></div><p>Vectors <a id="id223" class="indexterm"/>have another remarkable property, and that is, you <a id="id224" class="indexterm"/>can access the individual components through indexes, that is to say if you wish to access each component of a <code class="literal">float4</code> vector, <code class="literal">v</code>, then you would do so via <code class="literal">v.x</code>, <code class="literal">v.y</code>, <code class="literal">v.z</code>, <code class="literal">v.w</code> respectively, and for larger vectors of 8 or 16 elements we would access those individual elements via <code class="literal">v.s0</code>, <code class="literal">v.s1</code> through to <code class="literal">v.s7</code>, and <code class="literal">v.s0</code>, <code class="literal">v.s1</code>, <code class="literal">v.sa</code> through to <code class="literal">v.sf</code> respectively. Hence, vectors of type <code class="literal">char2</code>, <code class="literal">uchar2</code>, <code class="literal">short2</code>, <code class="literal">ushort2</code>, <code class="literal">int2</code>, <code class="literal">uint2</code>, <code class="literal">long2</code>, <code class="literal">ulong2</code>, and <code class="literal">float2</code> can access their <code class="literal">.xy</code> elements.</p><p>Following is another way of creating vectors and that is through composition:</p><div><pre class="programlisting">float4 c;
c.xyzw = (float4) (1.0f, 2.0f, 3.0f, 4.0f);
float4 d;
d.x = c.x;
d.y = c.y;
d.z = c.z;
d.w = c.w; // d stores (1.0f, 2.0f, 3.0f, 4.0f)</pre></div></div><div><div><div><div><h2 class="title"><a id="ch03lvl2sec53"/>How it works…</h2></div></div></div><p>On a similar note, you can use numerical indexes to reference the components in your vector and create vectors in turn. The following table shows a list of indexes for the various vector data types:</p><div><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Vector components</p>
</th><th style="text-align: left" valign="bottom">
<p>Numeric indexes that can be used</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>2-component</p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0, 1</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>3-component</p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0, 1, 2</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>4-component</p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0, 1, 2, 3</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>8-component</p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0, 1, 2, 3, 4, 5, 6, 7</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>16-component</p>
</td><td style="text-align: left" valign="top">
<p><code class="literal">0, 1, 2, 3, 4, 5, 6, 7, 8, 9, a, A, b, B, c, C, d, D, e, E, f, F</code></p>
</td></tr></tbody></table></div><p>To use these numerical indexes, you have to precede by the letter <em>s</em> or <em>S</em>, and following are a few quick examples on how to create vectors:</p><div><pre class="programlisting">float4 pos = (float4)(1.0f, 2.0f, 3.0f, 4.0f); 
float4 swiz= pos.wzyx; // swiz = (4.0f, 3.0f, 2.0f, 1.0f) 
float4 dup = pos.xxyy; // dup = (1.0f, 1.0f, 2.0f, 2.0f)
float4 f, a, b;
f.xyzw = a.s0123 + b.s0123;</pre></div></div><div><div><div><div><h2 class="title"><a id="ch03lvl2sec54"/>There's more…</h2></div></div></div><p>Lastly, vector <a id="id225" class="indexterm"/>data types can use the <code class="literal">.lo</code> (or <code class="literal">.even</code>) and <code class="literal">.hi</code> (or <code class="literal">.odd</code>) suffixes to compose <a id="id226" class="indexterm"/>new vector types, or to combine smaller vector types to a larger vector type. Multiple levels of <code class="literal">.lo</code> (or <code class="literal">.even</code>) and <code class="literal">.hi</code> (or <code class="literal">.odd</code>) suffixes can be used until they refer to a scalar term. The <code class="literal">.lo</code> and <code class="literal">.hi</code> suffix refers to the lower and upper halves of a vector. The <code class="literal">.even</code> and <code class="literal">.odd</code> suffixes of a vector refer to the even and odd elements of a vector. Following are the examples of vector creation via composition:</p><div><pre class="programlisting">float4 vf = (float4) (1.0f, 2.0f, 3.0f, 4.0f);
float2 low = vf.lo; // returns vf.xy
float2 high = vf.hi; // returns vf.zw
float4 vf4 = (float4) (low, high);// returns (1.0f, 2.0f, 3.0f, 4.0f)</pre></div><p>Vectors are disallowed from implicit conversions so you cannot perform the following operation:</p><div><pre class="programlisting">float4 vf4, wf4;
int4 if4;
wf4 = vf4; // illegal
if4 = wf4; // illegal</pre></div><p>Explicit casts between vector types are also disallowed, and in fact the only form of explicit cast to a vector type is when you're initializing a vector with a scalar:</p><div><pre class="programlisting">float f = 4.4f;
float4 va = (float4) (f); // va stores ( 4.4f, 4.4f, 4.4f, 4.4f)</pre></div><p>If you were to extract components of a 3-component vector type via the suffixes <code class="literal">.lo</code> (or <code class="literal">.even</code>), <code class="literal">.hi</code> (or <code class="literal">.odd</code>), then the 3-component vector type would behave as if it is a 4-component vector type with the exception that the <code class="literal">w</code> component would be undefined.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch03lvl1sec27"/>Using OpenCL scalar types</h1></div></div></div><p>The scalar data <a id="id227" class="indexterm"/>types are quite similar to what you would expect if you were <a id="id228" class="indexterm"/>programming in the C language. However, two topics deserve more attention and we'll touch on that in this recipe; we'll look at the <code class="literal">half</code> data type and examine how OpenCL devices might order their data.</p><div><div><div><div><h2 class="title"><a id="ch03lvl2sec55"/>Getting ready</h2></div></div></div><p>Many of the OpenCL compliant devices are actually little-endian architectures, and developers need to ensure that their kernels are tested on both big-endian and little-endian devices to ensure source compatibility with current and future devices. Let's use a simple example to illustrate endian-ness.</p></div><div><div><div><div><h2 class="title"><a id="ch03lvl2sec56"/>How to do it…</h2></div></div></div><p>Consider a variable <code class="literal">x</code> that holds the value <code class="literal">0x01234567</code> and the address of <code class="literal">x</code> starts at <code class="literal">0x100</code>. In computer architecture terminology, the value <code class="literal">0x01</code> is the <a id="id229" class="indexterm"/>
<strong>most significant byte</strong> ( <strong>MSB</strong>) and <code class="literal">0x67</code> is the<a id="id230" class="indexterm"/> <strong>least significant byte</strong> ( <strong>LSB</strong>). Big-endian storage scheme stores the MSB first till it meets the LSB and little-endian storage schemes stores the LSB first till it<a id="id231" class="indexterm"/> meets the <a id="id232" class="indexterm"/>MSB.</p><p>Big-endian</p><div><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><tbody><tr><td style="text-align: left" valign="bottom">
<p><strong>Address</strong></p>
</td><td style="text-align: left" valign="top">
<p>0x100</p>
</td><td style="text-align: left" valign="top">
<p>0x101</p>
</td><td style="text-align: left" valign="top">
<p>0x102</p>
</td><td style="text-align: left" valign="top">
<p>0x103</p>
</td><td style="text-align: left" valign="top"> </td></tr><tr><td style="text-align: left" valign="bottom">
<p><strong>Values</strong></p>
</td><td style="text-align: left" valign="top">
<p>0x01</p>
</td><td style="text-align: left" valign="top">
<p>0x23</p>
</td><td style="text-align: left" valign="top">
<p>0x45</p>
</td><td style="text-align: left" valign="top">
<p>0x67</p>
</td><td style="text-align: left" valign="top">
<p>…</p>
</td></tr></tbody></table></div><p>Little-endian</p><div><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><tbody><tr><td style="text-align: left" valign="bottom">
<p><strong>Address</strong></p>
</td><td style="text-align: left" valign="top">
<p>0x100</p>
</td><td style="text-align: left" valign="top">
<p>0x101</p>
</td><td style="text-align: left" valign="top">
<p>0x102</p>
</td><td style="text-align: left" valign="top">
<p>0x103</p>
</td><td style="text-align: left" valign="top"> </td></tr><tr><td style="text-align: left" valign="bottom">
<p><strong>Values</strong></p>
</td><td style="text-align: left" valign="top">
<p>0x67</p>
</td><td style="text-align: left" valign="top">
<p>0x45</p>
</td><td style="text-align: left" valign="top">
<p>0x23</p>
</td><td style="text-align: left" valign="top">
<p>0x01</p>
</td><td style="text-align: left" valign="top">
<p>…</p>
</td></tr></tbody></table></div><div><div><h3 class="title"><a id="note08"/>Note</h3><p>Review the full code listed in <code class="literal">Ch3/byte_ordering/show_bytes.c</code>, compile the code by running the commands <code class="literal">cmake</code> and <code class="literal">make</code> in that order; that will generate a binary named <code class="literal">ShowBytes</code>, and then run that program to see its output. This code will print out a series of output, and depending on the endian-ness of the architecture, you will notice different byte orderings.</p></div></div><p>Refer to the following code:</p><div><pre class="programlisting">#include &lt;stdio.h&gt;
typedef unsigned char* byte_pointer;
void show_bytes(byte_pointer start, int len) {
  int i;
  for(i = 0; i &lt; len; i++)
    printf(" %.2x", start[i]);
  printf("\n");
}
void show_int(int x) {
  show_bytes((byte_pointer) &amp;x, sizeof(int));
}
void show_float(float x) {
  show_bytes((byte_pointer) &amp;x, sizeof(float));
}
void show_pointer(void* x) {
  show_bytes((byte_pointer) &amp;x, sizeof(void*));
}
void test_show_bytes(int val ) {
  int ival = val;
  float fval = (float) ival;
  int* pval = &amp;ival;
  show_int(ival);
  show_float(fval);
  show_pointer(pval);
}</pre></div><p>Since you've <a id="id233" class="indexterm"/>understood how byte ordering affects the way data (scalar) is <a id="id234" class="indexterm"/>being read and written; let's take a look at how the ordering affects vector data types in OpenCL. With vector data types, both, the order of the bytes within each value and the order of the values with respect to one another are reversed. Using an example of a <code class="literal">uint4</code> vector which contains the values <code class="literal">0x000102030405060708090A0B0C0D0E0F</code>, at address <code class="literal">0x100</code>, following table shows how a little-endian storage scheme would look:</p><div><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><tbody><tr><td style="text-align: left" valign="top">
<p>0x100</p>
</td><td style="text-align: left" valign="top">
<p>0x104</p>
</td><td style="text-align: left" valign="top">
<p>0x108</p>
</td><td style="text-align: left" valign="top">
<p>0x1b0</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>0x0F0E0D0C</p>
</td><td style="text-align: left" valign="top">
<p>0x0B0A0908</p>
</td><td style="text-align: left" valign="top">
<p>0x07060504</p>
</td><td style="text-align: left" valign="top">
<p>0x3020100</p>
</td></tr></tbody></table></div><p>Awareness of this fact is important if you are working with data compression and computer-imaging algorithms since these two classes of algorithms have a significant amount of byte-level operations and you don't want to be bitten by these issues.</p></div><div><div><div><div><h2 class="title"><a id="ch03lvl2sec57"/>How it works…</h2></div></div></div><p>The <code class="literal">half-precision</code> data type<a id="id235" class="indexterm"/>, conveniently called <code class="literal">half</code> actually has half the storage and precision of a regular <code class="literal">float</code> type. The <code class="literal">half</code> type is IEEE754-2008 compliant and was first introduced by NVIDIA, and Industrial Light and Magic. The only thing you can do with this type is to declare a pointer to a buffer that contains <code class="literal">half</code> values; those values must be finite and normal numbers, de-normalized numbers, infinities, and NaN.</p><p>You can choose to use the vector load and store functions such as <code class="literal">vload_half</code>, <code class="literal">vload_halfn</code>, <code class="literal">vstore_half</code>, and so on. However, bear in mind that the load/store operation will create an intermediate floating -point value.</p><div><div><h3 class="title"><a id="note09"/>Note</h3><p>The <code class="literal">load</code> function read the <code class="literal">half</code> values from memory and converts it to a regular <code class="literal">float</code> value. The <code class="literal">store</code> functions take a <code class="literal">float</code> as an input, convert it to a <code class="literal">half</code> value and store that value into memory.</p></div></div><p>To determine if <a id="id236" class="indexterm"/>your device <a id="id237" class="indexterm"/>supports this, you can run the program in <code class="literal">Ch2/device_extension/device_extensions</code>, and the output should contain <code class="literal">cl_khr_fp16</code>; alternatively you can query the device by passing the parameter <code class="literal">CL_DEVICE_EXTENSIONS</code> to <code class="literal">clGetDeviceInfo</code>. Following is the code snippet from <code class="literal">Ch2/device_extensions/device_extensions.c</code>:</p><div><pre class="programlisting">/* --- file: device_extensions.c --- */
displayDeviceDetails( devices[i], CL_DEVICE_EXTENSIONS, "CL_DEVICE_EXTENSIONS");
void displayDeviceDetails(cl_device_id id,
                          cl_device_info param_name,
                          const char* paramNameAsStr) {
  cl_int error = 0;
  size_t paramSize = 0;
  error = clGetDeviceInfo( id, param_name, 0, NULL, &amp;paramSize );
  if (error != CL_SUCCESS ) {
    perror("Unable to obtain device info for param\n");
    return;
  }
  /* the cl_device_info are preprocessor directives defined in cl.h */
  switch (param_name) {
// code omitted
    case CL_DEVICE_EXTENSIONS : {
// beware of buffer overflow; alternatively use the OpenCL C++ //bindings
      char* extension_info[4096];
      error = clGetDeviceInfo( id, CL_DEVICE_EXTENSIONS, sizeof(extension_info), extension_info, NULL);
      printf("\tSupported extensions: %s\n", extension_info);
    }break;
  } //end of switch</pre></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch03lvl1sec28"/>Understanding OpenCL vector types</h1></div></div></div><p>When you start <a id="id238" class="indexterm"/>working through your OpenCL project you are inevitably<a id="id239" class="indexterm"/> going to use both the scalar and vector data types to model the algorithm. Scalars work like any variable declaration/definition you may have come across in most of the programming languages, and you should think of vectors as a wide container that can deliver all items in that container in parallel, and the one thing that differentiates scalars and vectors is the fact that when an operation is applied to a scalar, it affects just a single value while the same operation applied to a vector affects all items in it in parallel.</p><p>In the modern processors, there exist a specialized hardware unit that processes more data per cycle and they are often termed as <strong>Single Instruction Multiple Data</strong> (<strong>SIMD</strong>)<a id="id240" class="indexterm"/> or known as<a id="id241" class="indexterm"/> <strong>Streaming SIMD Extensions</strong> (<strong>SSE</strong>) which is intel's implementation of SIMD. The advantage that SIMD instructions provide is that they allow multiple values to be operated upon in a large register in a cycle; quite often there are many such units, thus increasing performance of the program. We should be clear that SIMD describes a mechanism that allows parallelism to occur gleaned from Flynn's taxonomy, while SSE describes how two CPU processor manufacturers namely, Intel and AMD implemented SIMD.</p><p>The first part of the story is to tell you how OpenCL kernels<a id="id242" class="indexterm"/> run on the CPUs before we reveal how it would work on the GPU, and for now we place our attention on the Intel CPU architecture. On these architectures, OpenCL sees a single device with multiple compute units and if you are guessing each core is a compute unit then you're correct and hence, your kernels run on all compute units unless you are using the device fission extension, which is new in OpenCL 1.2.</p><div><div><h3 class="title"><a id="note10"/>Note</h3><p>Device fission <a id="id243" class="indexterm"/>(<code class="literal">cl_khr_device_fission</code>) which is new in OpenCL 1.2 is currently supported by multicore CPUs by Intel, AMD, and IBM Cell Broadband. GPUs are currently not supported.</p></div></div><p>The next part of the story is to describe, how OpenCL kernels would run on GPUs manufactured by AMD, and we focus on the AMD GPU we used for this book which is based on AMD's Southern Island Architecture which includes their Radeon HD 7900, 7800, and 7700 GPUs; on a side note, you might wish to consult NVIDIA's website for more product details pertaining to their GPUs at <a class="ulink" href="http://www.nvidia.com">www.nvidia.com</a>.</p><p>Kernels basically<a id="id244" class="indexterm"/> execute instructions that are either scalar-based or vector-based, and<a id="id245" class="indexterm"/> work is assigned to a compute unit in blocks of 64 work items, which is termed as wavefront. A wavefront has a single program counter, and is considered as a small unit of work and what that means is that they execute in lock-step.</p><p>When your application passes workloads to the GPU, it must first compile the kernel and load it into memory. It must also bind buffers for the source and result data, and finally it would decide how to execute the given workload on the GPU. When the workload is to be executed, the GPU divides the input domain into blocks of 64 threads aka wavefronts and dispatches them to the <a id="id246" class="indexterm"/>
<strong>compute unit</strong> (<strong>CU</strong>). The kernel is next fetched into the instruction cache and the compute unit begins dispatching instructions to the execution units; each compute unit can work on multiple wavefronts in parallel, simultaneously processing vector and scalar ALU computations, as well as memory accesses. The wavefront continues executing until the end of the kernel is reached, when the wavefront is terminated and a new one can take its place on the GPU.</p><p>Taking into account the fact that memory accesses by wavefronts happens in parallel, you will expect some sort of latency to occur and the processor is pretty clever in dealing with that situation, and what it does is executing many wavefronts in parallel and it works such that if one wavefront is waiting for results from the memory, other wavefronts can issue memory requests, and they can execute ALU operations in parallel with outstanding memory requests if and only if they are independent calculations. Factors that increase the amount of parallelism that can be extracted from the program varies, but one of them would be the actual number of hardware units available for parallel computation and in OpenCL terminology, it is known as the CU and in both CPUs and GPUs they are basically the processor.</p><p>A compute unit is the basis of parallel <a id="id247" class="indexterm"/>computation, and in the Southern Island <a id="id248" class="indexterm"/>Architecture which hosts other products, the number of compute units varies and each compute unit basically contains the following:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Scalar ALU and scalar <strong>GPRs</strong> (<strong>General-Purpose Registers</strong>)<a id="id249" class="indexterm"/> aka <strong>SGPRs</strong></li><li class="listitem" style="list-style-type: disc">Four SIMDs, each consisting of a vector ALU and vector GPRs, aka VGPRs</li><li class="listitem" style="list-style-type: disc">Local memory</li><li class="listitem" style="list-style-type: disc">Read/write access to vector memory through a Level-1 cache</li><li class="listitem" style="list-style-type: disc">Instruction cache, which is shared by four CUs, that is, compute units</li><li class="listitem" style="list-style-type: disc">Constant cache, which is shared by four CUs, that is, compute units</li></ul></div><p>Now we will focus on the vector operations on GPUs, which include ALU and memory operations. Each of the four SIMDs contains a vector-ALU that operates on wavefronts over four cycles; each SIMD also can host ten wavefronts in flight, that is, one CU can have forty wavefronts executing in parallel. In the AMD GPU based on the Southern Island Architecture used for this book which is the AMD HD 7870, we have 20 compute units and we know now that each CU holds four SIMDs and each SIMD would execute a wavefront means that we can have 20 x 4 x 10 x 64 = 51,200 work items at any one time, and if you were to imagine that each work item is at the stage of executing vector operations then the parallelism offered by GPUs is considerably larger than that of the CPU; the specific CPU we are referring to is the Intel Xeon Phi which has 60 cores and each core hosts 4 work items which provides 60 x 4 = 240 work items; be aware that we're not stating that GPUs are superior to CPUs since each device has its niche but we illustrate these numbers to demonstrate a simple fact that GPU has a higher throughput than the CPU.</p><p>Having said all that, we are going to see an example soon but first recall that vector operations are component-wise and that vectors can be accessed via numeric indexes, and each index can be combined into larger group of indices to perform a store/load to/from memory. Refer to the following code:</p><div><pre class="programlisting">float4 v, u;
float f;
v = u + f;
// equivalent to 
// v.x = u.x + f
// v.y = u.y + f
// v.z = u.z + f
// v.w = u.w + f
float4 a, b, c;
c = a + b
// equivalent to 
// c.x = a.x + b.x
// c.y = a.y + b.y
// c.z = a.z + b.z
// c.w = a.w + b.w</pre></div><p>The component-wise manner in which vectors can be aggregated to perform an operation without code verbosity actually helps the programmer in their daily work and increases productivity. Next, we can a dive into how vector types are translated to utilize your hardware.</p><div><div><div><div><h2 class="title"><a id="ch03lvl2sec58"/>Getting ready</h2></div></div></div><p>The <a id="id250" class="indexterm"/>demonstration we are going to describe has two parts in it. First, we are going to use the Intel OpenCL compiler on Windows to demonstrate the implicit vectorization of the kernel code; secondly, we are going to demonstrate how to enable native <a id="id251" class="indexterm"/>vector type notation in your code to express the desire to generate vectorized code using the AMD APP SDK v2.7 or v2.8 on Linux.</p><p>We combined these two approaches with the intention to solve the problem of transferring a large input array from one part of the device memory to another part of the device memory, and finally we extract and compare them for equality. As before, we would prepare the data structures for transfers on the host code and write a suitable OpenCL kernel to actually transfer the memory contents across. The source can be found in <code class="literal">Ch3/vectorization</code>, and we build the program using the AMD APP SDK.</p><div><div><h3 class="title"><a id="note11"/>Note</h3><p>Readers who are interested in the OpenCL code generation for AMD CPU and GPU platforms should consult the <code class="literal">AMD CodeXL</code> product as the AMD APP Kernel Analyzer has been retired. You may wish to consult the AMD Intermediate Language Manual in conjunction when you study the intermediate language output.</p></div></div><p>Implicit vectorization<a id="id252" class="indexterm"/> is a required feature that is supported by all the compliant OpenCL<a id="id253" class="indexterm"/> compiler implementations, and the reason we chose to demonstrate this feature with the Intel OpenCL compiler is the fact that the generated SIMD instructions are more likely to be recognized by the reader than would the intermediate code generated by other compiler implementations such as AMD or NVIDIA's. The kernel code we have for you can be found in <code class="literal">Ch3/vectorization/vectorization.cl</code>, and we reveal it as in the following code:</p><div><pre class="programlisting">__kernel void copyNPaste(__global float* in, __global float8* out) {
  size_t id = get_global_id(0);
  size_t index = id*sizeof(float8);
  float8 t = vload8(index, in);
  out[index].s0 = t.s0;
  out[index].s1 = t.s1;
  out[index].s2 = t.s2;
  out[index].s3 = t.s3;
  out[index].s4 = t.s4;
  out[index].s5 = t.s5;
  out[index].s6 = t.s6;
  out[index].s7 = t.s7;
}</pre></div><p>This <a id="id254" class="indexterm"/>kernel's main action is to transfer the contents from one place to another, and it does this by transporting it in parallel using two vectors of eight floats each and you will notice that we use the vector component notation to state these memory transfers explicitly.</p><p>In the next demonstration, we swing from the kernel code back to the host code assuming that the developer has a desire to control the code generation in a more explicit manner; and this can be done through the native vector type notation.</p><p>We ask the reader to refer to the section <em>There's more…</em> for details, but the demonstration here rests on the assumption that the developer would like to hand tune the procedure that handles data validation once the memory transfers have been completed in the device, and this function can be found in <code class="literal">Ch3/vectorization/vectorization.c</code> named <code class="literal">valuesOK</code> and the following code is how it is implemented:</p><div><pre class="programlisting">#ifdef __CL_FLOAT4__
int valuesOK(cl_float8* to, cl_float8* from, size_t length) {
#ifdef DEBUG
  printf("Checking data of size: %lu\n", length);
#endif
  for(int i = 0; i &lt; length; ++i) {
#ifdef __SSE__
    __cl_float4 __toFirstValue = to-&gt;v4[0];
    __cl_float4 __toSecondValue = to-&gt;v4[1];
    __cl_float4 __fromFirstValue = from-&gt;v4[0];
    __cl_float4 __fromSecondValue = from-&gt;v4[1];
    __m128i vcmp = (__m128i) _mm_cmpneq_ps(__toFirstValue, __fromFirstValue);
    uint16_t test = _mm_movemask_epi8(vcmp);
    __m128i vcmp_2 = (__m128i) _mm_cmpneq_ps(__toSecondValue, __fromSecondValue);
    uint16_t test_2 = _mm_movemask_epi8(vcmp_2);
    if( (test|test_2) != 0 ) return 0; // indicative that the result failed
#else
    #error "SSE not supported, which is required for example code to work!"
#endif
  }
return 1;
}
#endif</pre></div></div><div><div><div><div><h2 class="title"><a id="ch03lvl2sec59"/>How to do it…</h2></div></div></div><p>Implicit vectorization<a id="id255" class="indexterm"/> through the Intel OpenCL compiler is relatively easy and the purpose of this simple <a id="id256" class="indexterm"/>example, we have chosen to install it on the <a id="id257" class="indexterm"/>Windows operating system. You can download the compiler from <a class="ulink" href="http://software.intel.com/en-us/vcsource/tools/opencl">http://software.intel.com/en-us/vcsource/tools/opencl</a>.</p><p>To witness how the implicit vectorization can be achieved through this compiler, you would copy and paste the kernel code (the previous code) into the editor pane of the GUI and start the compilation. Once compiled, you would be able to view the generated code by clicking on the <strong>ASM</strong> or <strong>LLVM</strong> buttons on the GUI. An example of this is shown in the following screenshot:</p><div><img src="img/4520OT_03_01.jpg" alt="How to do it…"/></div><p>The next<a id="id258" class="indexterm"/> item is to hand-tune our data validation code, <code class="literal">valuesOK</code>, to exhibit vectorization. This example is only meant to illustrate how one<a id="id259" class="indexterm"/> would go about accomplishing something similar to this and you don't have to do anything besides invoking <code class="literal">make</code> in the directory <code class="literal">Ch3/vectorization</code>, and an executable vectorization will be dropped into the filesystem to which we'll next dissect it.</p><div><div><h3 class="title"><a id="note12"/>Note</h3><p>If you are running OpenCL 1.1 on Mac OSX 10.7, then passing the flag <code class="literal">–cl-auto-vectorizer-enable</code> to <code class="literal">clBuildProgram</code> as a build option will vectorize the kernels that will execute on the CPU. The SIMD instructions will be similar to the ones you see in this recipe.</p></div></div><p>Hand-tuning your code in such a manner basically turns implicit vectorization off, and you will need to judge for your scenario whether the effort justifies with respects to the complexity of the issue. To view the generated SIMD code, it would be best to put the program under a debugger, and on Linux the best debugger will be the GNU GDB. You basically load the program into the debugger and issue the command <code class="literal">disassemble /m valuesOK</code> to verify that the SIMD instructions were indeed generated. Following is a sample <code class="literal">gdb</code> <a id="id260" class="indexterm"/>session <a id="id261" class="indexterm"/>where the disassembly is interleaved with the source code:</p><div><pre class="programlisting">$ gdb ./Vectorization 
GNU gdb (GDB) 7.5-ubuntu
Copyright (C) 2012 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later sa&lt;http://gnu.org/licenses/gpl.html&gt;
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law. Type "show copying" and "show warranty" for details.
This GDB was configured as "x86_64-linux-gnu".
For bug reporting instructions, please see:
&lt;http://www.gnu.org/software/gdb/bugs/&gt;...
Reading symbols from /home/tayboonl/PACKT_OpenCL_Book/src/Ch3/vectorization/Vectorization...done.
(gdb) disassemble /m valuesOK 
Dump of assembler code for function valuesOK:
warning: Source file is more recent than executable.
31 int valuesOK(cl_float8* to, cl_float8* from, size_t length) {
  0x000000000040117c &lt;+0&gt;: push  %rbp
  0x000000000040117d &lt;+1&gt;: mov  %rsp,%rbp
  0x0000000000401180 &lt;+4&gt;: sub  $0xf0,%rsp
  0x0000000000401187 &lt;+11&gt;: mov  %rdi,-0xd8(%rbp)
  0x000000000040118e &lt;+18&gt;: mov  %rsi,-0xe0(%rbp)
  0x0000000000401195 &lt;+25&gt;: mov  %rdx,-0xe8(%rbp)
32 #ifdef DEBUGf
33 printf("Checking data of size: %lu\n", length);
  0x000000000040119c &lt;+32&gt;: mov  -0xe8(%rbp),%rax
  0x00000000004011a3 &lt;+39&gt;: mov  %rax,%rsi
  0x00000000004011a6 &lt;+42&gt;: mov  $0x4020a8,%edi
  0x00000000004011ab &lt;+47&gt;: mov  $0x0,%eax
  0x00000000004011b0 &lt;+52&gt;: callq  0x400f20 &lt;printf@plt&gt;
34 #endif
35 for(int i = 0; i &lt; length; ++i) {
  0x00000000004011b5 &lt;+57&gt;: movl  $0x0,-0xc4(%rbp)
  0x00000000004011bf &lt;+67&gt;: jmpq  0x4012a9 &lt;valuesOK+301&gt;
  0x00000000004012a2 &lt;+294&gt;: addl  $0x1,-0xc4(%rbp)
  0x00000000004012a9 &lt;+301&gt;: mov  -0xc4(%rbp),%eax
  0x00000000004012af &lt;+307&gt;: cltq  
  0x00000000004012b1 &lt;+309&gt;: cmp  -0xe8(%rbp),%rax
  0x00000000004012b8 &lt;+316&gt;: jb  0x4011c4 &lt;valuesOK+72&gt;
36 #ifdef __SSE__
37    __cl_float4 __toFirstValue = to-&gt;v4[0];
  0x00000000004011c4 &lt;+72&gt;: mov  -0xd8(%rbp),%rax
  0x00000000004011cb &lt;+79&gt;: movaps (%rax),%xmm0
  0x00000000004011ce &lt;+82&gt;: movaps %xmm0,-0xc0(%rbp)
38    __cl_float4 __toSecondValue = to-&gt;v4[1];
  0x00000000004011d5 &lt;+89&gt;: mov  -0xd8(%rbp),%rax
  0x00000000004011dc &lt;+96&gt;: movaps 0x10(%rax),%xmm0
  0x00000000004011e0 &lt;+100&gt;: movaps %xmm0,-0xb0(%rbp)

39    __cl_float4 __fromFirstValue = from-&gt;v4[0];
  0x00000000004011e7 &lt;+107&gt;: mov  -0xe0(%rbp),%rax
  0x00000000004011ee &lt;+114&gt;: movaps (%rax),%xmm0
  x00000000004011f1 &lt;+117&gt;: movaps %xmm0,-0xa0(%rbp)
40    __cl_float4 __fromSecondValue = from-&gt;v4[1];
  0x00000000004011f8 &lt;+124&gt;: mov  -0xe0(%rbp),%rax
  0x00000000004011ff &lt;+131&gt;: movaps 0x10(%rax),%xmm0
  0x0000000000401203 &lt;+135&gt;: movaps %xmm0,-0x90(%rbp)
  0x000000000040120a &lt;+142&gt;: movaps -0xc0(%rbp),%xmm0
  0x0000000000401211 &lt;+149&gt;: movaps %xmm0,-0x60(%rbp)
---Type &lt;return&gt; to continue, or q &lt;return&gt; to quit---
  0x0000000000401215 &lt;+153&gt;: movaps -0xa0(%rbp),%xmm0
  0x000000000040121c &lt;+160&gt;: movaps %xmm0,-0x50(%rbp)
41    __m128i vcmp = (__m128i) _mm_cmpneq_ps(__toFirstValue, __fromFirstValue);
  0x0000000000401229 &lt;+173&gt;: movdqa %xmm0,-0x80(%rbp)
  0x000000000040122e &lt;+178&gt;: movdqa -0x80(%rbp),%xmm0
  0x0000000000401233 &lt;+183&gt;: movdqa %xmm0,-0x40(%rbp)
42    uint16_t test = _mm_movemask_epi8(vcmp);
  0x0000000000401241 &lt;+197&gt;: mov  %ax,-0xc8(%rbp)
  0x0000000000401248 &lt;+204&gt;: movaps -0xb0(%rbp),%xmm0
  0x000000000040124f &lt;+211&gt;: movaps %xmm0,-0x30(%rbp)
  0x0000000000401253 &lt;+215&gt;: movaps -0x90(%rbp),%xmm0
  0x000000000040125a &lt;+222&gt;: movaps %xmm0,-0x20(%rbp)
43    __m128i vcmp_2 = (__m128i) _mm_cmpneq_ps(__toSecondValue, __fromSecondValue);
  0x0000000000401267 &lt;+235&gt;: movdqa %xmm0,-0x70(%rbp)
  0x000000000040126c &lt;+240&gt;: movdqa -0x70(%rbp),%xmm0
  0x0000000000401271 &lt;+245&gt;: movdqa %xmm0,-0x10(%rbp)
44    uint16_t test_2 = _mm_movemask_epi8(vcmp_2);
  0x000000000040127f &lt;+259&gt;: mov  %ax,-0xc6(%rbp)
45    if( (test|test_2) != 0 ) return 0; // indicative that the result failed
  0x0000000000401286 &lt;+266&gt;: movzwl -0xc6(%rbp),%eax
  0x000000000040128d &lt;+273&gt;: movzwl -0xc8(%rbp),%edx
  0x0000000000401294 &lt;+280&gt;: or  %edx,%eax
  0x0000000000401296 &lt;+282&gt;: test  %ax,%ax
  0x0000000000401299 &lt;+285&gt;: je  0x4012a2 &lt;valuesOK+294&gt;
  0x000000000040129b &lt;+287&gt;: mov  $0x0,%eax
  0x00000000004012a0 &lt;+292&gt;: jmp  0x4012c3 &lt;valuesOK+327&gt;
46 #else
47    #error "SSE not supported, which is required for example code to work!"  
48 #endif
49  }
50 return 1;
  0x00000000004012be &lt;+322&gt;: mov  $0x1,%eax
51  }
  0x00000000004012c3 &lt;+327&gt;: leaveq
  0x00000000004012c4 &lt;+328&gt;: retq
End of assembler dump
(gdb)</pre></div></div><div><div><div><div><h2 class="title"><a id="ch03lvl2sec60"/>How it works…</h2></div></div></div><p>Implicit vectorization is a piece of complicated software written into the compiler provided by the implementation, and is definitely hardware dependent and often represented by an <strong>intermediate language</strong> (<strong>IL</strong>)<a id="id262" class="indexterm"/> that's proprietary to the processor manufacturer and to our disappointment, not very well documented so<a id="id263" class="indexterm"/> we like to focus on how native vector type notation works in more detail.</p><div><div><h3 class="title"><a id="note13"/>Note</h3><p>The interested reader is however invited to explore the ILs developed by AMD and NVIDIA, which are known as AMD IL, and NVIDIA's PTX respectively.</p></div></div><p>This method of <a id="id264" class="indexterm"/>hand-tuning allows the developer to reference the built-in vector data type of the platform they're working on instead of relying on the OpenCL compiler to auto-vectorize the code, and may bring about performance benefits. The manner in which it is being done in OpenCL so far is to abstract these differences into platform dependent macros in the file <code class="literal">cl_platform.h</code>. Let's work out how this would work in our example.</p><p>The example, we saw previously, was tested on the Ubuntu Linux 12.04 operating system with an Intel Core i7 CPU and an AMD Radeon HD 7870 GPU, but since our example focuses on explicit vectorization on the host code, it implies that we need to know the width of SIMD vectors based on the Intel instruction set. We know this to be 128-bits and what this means is as follows:</p><div><pre class="programlisting">float4 a,b;
float4 c = a + b;</pre></div><p>The preceding code gets translated to the following C code snippet:</p><div><pre class="programlisting">__m128 a, b;
__m128 c = __mm_add_ps(a, b);</pre></div><p>The function <code class="literal">__mm_add_ps</code> is the SIMD function for adding two vectors by adding their single precision floating-point values component-wise in this manner and at first hand, it will look like syntax sugar but this is one of the many ways in which OpenCL provides cross platform compatibility and removes the pain of delivering customized vectorized code for various processor architectures so in this way a façade is actually a good thing.</p><p>Coming back to the problem we are trying to solve, which is to vectorize the procedure for performing data validation for the input and output arrays. In our example, we chose arrays or rather vectors that can contain 8 floats and what we will like to do is to examine them and compare them for equality. Using the native vector type notation in OpenCL, we know that the vector-of-8 can be decomposed into vector-of-4 elements because, OpenCL stipulates that if a platform can support a native vector type then the macro is identified in the<code class="literal"> cl_platform.h</code> file by the name <code class="literal">__CL_&lt;TYPEN&gt;</code>, where <code class="literal">&lt;TYPEN&gt;</code> can be <code class="literal">UCHAR16</code>,<code class="literal"> CHAR16</code>, <code class="literal">INT4</code>, <code class="literal">FLOAT4</code>, that is, the vectorized primitive types and in general, you can access the native components using the <code class="literal">.v&lt;N&gt;</code> subvector notation where <code class="literal">&lt;N&gt;</code> is the number of<a id="id265" class="indexterm"/> elements in the subvector.</p><p>Using this<a id="id266" class="indexterm"/> newly found information, we can dissect the program we saw previously with the fact that the memory content of the original host memory is represented by the <code class="literal">cl_float8 *</code> to while the copied memory contents from host to device are held by the <code class="literal">cl_float8*</code> from:</p><div><pre class="programlisting">int valuesOK(cl_float8* to, cl_float8* from, size_t length) {
// code omitted
for(int i = 0; i &lt; length; ++i) {</pre></div><p>We need to iterate through the vectors in both input and output arrays and proceed to extract the first and second vector-of-4s from the host pointer as follows:</p><div><pre class="programlisting">    __cl_float4 __hostFirstValue = to-&gt;v4[0];
    __cl_float4 __hostSecondValue = to-&gt;v4[1];</pre></div><p>Then we extract the first and second vector-of-4s from the device pointer as follows:</p><div><pre class="programlisting">    __cl_float4 __deviceFirstValue = from-&gt;v4[0];
    __cl_float4 __deviceSecondValue = from-&gt;v4[1];</pre></div><p>Now, we compare each of the halves by using the SSE API <code class="literal">__mm_cmp_neq_ps</code>, and keep the result of each test into the variables test and test2 as shown in the following code:</p><div><pre class="programlisting">    __m128i vcmp = (__m128i) _mm_cmpneq_ps(__hostFirstValue, __deviceFirstValue);
    uint16_t test = _mm_movemask_epi8(vcmp);
    __m128i vcmp_2 = (__m128i) _mm_cmpneq_ps(__hostSecondValue, __deviceSecondValue);
    uint16_t test_2 = _mm_movemask_epi8(vcmp_2);</pre></div><p>Finally, we compare those results as follows:</p><div><pre class="programlisting">    if( (test|test_2) != 0 ) return 0; // indicative that the result failed
#else</pre></div></div><div><div><div><div><h2 class="title"><a id="ch03lvl2sec61"/>There's more…</h2></div></div></div><p>Another part of the vectorization story that we wanted to tell you is that you, the developer, has the option of controlling the auto-vectorization by providing an explicit compiler hint to the kernel code. This can be useful if you want to hand-tune the vectorization of your code.</p><p>The compiler hint we are referring to is the <code class="literal">vec_type_hint(&lt;type&gt;)</code> where <code class="literal">&lt;type&gt;</code> is any of the built-in scalar or vector data types we mentioned previously. The attribute <code class="literal">vec_type_hint(&lt;type&gt;)</code> represents the computation width of the kernel and if it's not specified, the kernel is assumed to have the <code class="literal">vec_type_hint(int)</code> qualifier applied to the kernel, that is, 4-bytes wide. The following code snippets illustrate how the computation width of the kernel changes from 16-bytes to 8-bytes and finally to 4-bytes which happens to be the default:</p><div><pre class="programlisting">// autovectoize assuming float4 as computation width
__kernel __attribute__((vec_type_hint(float4)))
void computeThis(__global float4*p ) {…}
// autovectorize assuming double as computation width
__kernel __attribute__((vec_type_hint(double)))
void computeThis(__global float4*p ) {…}
// autovectorize assuming int (default) as computation width
__kernel __attribute__((vec_type_hint(int)))
void computeThis(__global float4*p ) {…}</pre></div><p>For you, the<a id="id267" class="indexterm"/> developer, to be able to use this, you will need to <a id="id268" class="indexterm"/>know the width of the vector units in your platform which could be running on a CPU or GPU. In the next diagram, we have two scenarios where we assume that both the <code class="literal">__kernel</code> functions are declared with <code class="literal">__attribute_((vec_type_hint(float4)))</code> and <code class="literal">__attribute_((vec_type_hint(char4)))</code> respectively. Furthermore, we assumed that the kernel is running on 256-bit wide registers and how the auto-vectorizer might choose to run one or more work items so that the register's usage is maximized; this is of course dependent on the compiler's implementation. The following figure is a conceptual view of how the OpenCL compiler might generate work items to consume the data in the wide registers:</p><div><img src="img/4520OT_03_02.jpg" alt="There's more…"/></div><p>In the native <a id="id269" class="indexterm"/>vector type notation method for explicit vectorization, we mentioned that native vector types are identified in <code class="literal">cl_platform.h</code> by the <code class="literal">__CL_&lt;TYPEN&gt;__</code> preprocessor symbols aka C macros but, we haven't told you how we came to use the SSE instructions in the code example. Let's now find out why, and we need to reference the <code class="literal">cl_platform.h</code> defined by the OpenCL 1.2 standard (which you can download from <a class="ulink" href="http://www.khronos.org/registry/cl/api/1.2/cl_platform.h">http://www.khronos.org/registry/cl/api/1.2/cl_platform.h</a>)</p><p>The code example <a id="id270" class="indexterm"/>was tested on the Ubuntu Linux 12.04 64-bit operating system with an Intel Core i7 CPU and a AMD Radeon HD 7870 GPU, and we should ignore the presence of the GPU as it has no relevance other than to inform you the machine setup.</p><p>What this setup tells us is that we have a SSE-capable instruction set and as a convention adopted by the UNIX and GCC community in general, is to look for the <code class="literal">__SSE__</code> preprocessor symbol and we indeed do that as follows:</p><div><pre class="programlisting">#if defined(__SSE__)
#if defined(__MINGW64__)
#include &lt;intrin.h&gt;
#else
#include &lt;xmmintrin.h&gt; 
#endif
#if defined(__GNUC__)
typedef float __cl_float4 __attribute__((vector_size(16)));
#else
typedef __m128 __cl_float4;// statement 1
#endif
#define __CL_FLOAT4__ 1// statement 2
#endif</pre></div><p>From the <a id="id271" class="indexterm"/>preceding code snippet, we know we should be<a id="id272" class="indexterm"/> focusing on the statement 1 as it has provided us the indicative width of the SIMD vectors, and we also know that by convention <code class="literal">__m128</code> indicates that its vector's width is 128-bits; other values includes 64-bits and 256-bits. We should also be careful to contain the explicit vectorization within the preprocessor guard, as a best practice, that is, <code class="literal">#ifdef __CL_FLOAT4__</code>.Using this understanding, we can proceed to search for the appropriate SSE APIs that allows us to manipulate data values of the desired width. The interested reader is invited to check the Intel Developer Manuals and AMD Developer Manuals, and explore how these ISAs compare and most importantly where they differ.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch03lvl1sec29"/>Vector and scalar address spaces</h1></div></div></div><p>Now that we have understood how to use scalars and vectors in OpenCL, it's time to examine the OpenCL's defined <a id="id273" class="indexterm"/>four <a id="id274" class="indexterm"/>address spaces: <code class="literal">__global</code>, <code class="literal">__local</code>, <code class="literal">__constant</code>, and <code class="literal">__private</code> in which vectors and scalars can exist in. These spaces are mapped to the memory units and hence, limited by the actual resource on the device and define how work items can access memory.</p><div><div><div><div><h2 class="title"><a id="ch03lvl2sec62"/>Getting ready</h2></div></div></div><p>Following is a conceptual diagram of the various memory domains:</p><div><img src="img/4520OT_03_03.jpg" alt="Getting ready"/></div><p>The <strong>Global Memory</strong><a id="id275" class="indexterm"/> and <a id="id276" class="indexterm"/>
<strong>Constant Memory</strong> found in the lower-half of the preceding diagram corresponds to the <code class="literal">__global</code> and <code class="literal">__constant</code> domain. The <strong>Local Memory</strong> <a id="id277" class="indexterm"/>associated with each <a id="id278" class="indexterm"/>compute unit in OpenCL (that executes the kernel code) will have a memory space that's shared by all work items in the block which corresponds to the<code class="literal"> __local</code> memory space<a id="id279" class="indexterm"/> while each processing element will have its own namespace to store data and, it is represented by the <code class="literal">__private</code> memory space<a id="id280" class="indexterm"/>. Be aware that there is no way in which a work item can access the (<code class="literal">__private</code>) <a id="id281" class="indexterm"/>memory space of another work item regardless of <a id="id282" class="indexterm"/>whether they're in the same work group or not, the same can be said of shared memory, that is, <code class="literal">__local</code> memory where no two work groups can inspect the other's memory.</p><p>Each compute unit in the device has a certain number of processing elements which executes work items and the compute unit as a whole would access the local, constant, or global memory space as determined by the computation. Each processing element (work group or work item), stores its own private variables in its private memory space.</p></div><div><div><div><div><h2 class="title"><a id="ch03lvl2sec63"/>How to do it…</h2></div></div></div><p>The <code class="literal">__global</code> address space name<a id="id283" class="indexterm"/> is used to refer to memory objects allocated from the global memory pool. To determine the actual amount of resources available on the device, you need to pass the parameter <code class="literal">CL_DEVICE_GLOBAL_MEM_SIZE</code> to <code class="literal">clGetDeviceInfo</code>. The following snippet is drawn from <code class="literal">Ch2/device_details/device_details.c</code>:</p><div><pre class="programlisting">displayDeviceDetails( devices[i], CL_DEVICE_GLOBAL_MEM_SIZE, "CL_DEVICE_GLOBAL_MEM_SIZE");
void displayDeviceDetails(cl_device_id id,
                          cl_device_info param_name,
                          const char* paramNameAsStr) {
  cl_int error = 0;
  size_t paramSize = 0;
  error = clGetDeviceInfo( id, param_name, 0, NULL, &amp;paramSize );
  if (error != CL_SUCCESS ) {
    perror("Unable to obtain device info for param\n");
    return;
  }
  /* the cl_device_info are preprocessor directives defined in cl.h */
  switch (param_name) { 
    case CL_DEVICE_GLOBAL_MEM_SIZE:
    case CL_DEVICE_MAX_MEM_ALLOC_SIZE: {
      cl_ulong* size = (cl_ulong*) alloca(sizeof(cl_ulong) * paramSize);
      error = clGetDeviceInfo( id, param_name, paramSize, size, NULL );
      if (error != CL_SUCCESS ) {
        perror("Unable to obtain device name/vendor info for param\n");
        return;
      }</pre></div><p>The <code class="literal">__local</code> address space name is used to <a id="id284" class="indexterm"/>describe variables that need to be allocated in <a id="id285" class="indexterm"/>the local memory and shared by all work items of a work group. You can determine the maximum size of this space by passing the parameter <code class="literal">CL_DEVICE_MAX_LOCAL_MEM_SIZE</code> to <code class="literal">clGetDeviceInfo</code>.</p><p>The <code class="literal">__constant</code> address space name<a id="id286" class="indexterm"/> is used to describe non-mutable variables that need to be allocated as read-only in global memory, and can be read by all work items during the kernel's execution. You can determine the maximum size of this space by passing the parameter <code class="literal">CL_DEVICE_MAX_CONSTANT_BUFFER_SIZE</code> to <code class="literal">clGetDeviceInfo</code>. This address space is useful if there is a specific value that does not change and is needed by the kernel functions.</p><p>The <code class="literal">__private</code> address space is used to describe objects private-only distinct work items; hence work items cannot inspect one another's variables if they were marked by <code class="literal">__private</code>. By default, variables inside a kernel function not declared with any address space qualifiers such as: <code class="literal">__global</code>, <code class="literal">__local</code>, or <code class="literal">__constant</code> are marked <code class="literal">__private</code>; this includes all variables in the non-kernel functions and function arguments. The following kernel code from <code class="literal">Ch3/vectorization/vectorization.cl</code> will illustrate the global and private memory spaces whereby the variables <code class="literal">id</code>, <code class="literal">index</code>, and <code class="literal">t</code> are in the private memory space and hence not visible <a id="id287" class="indexterm"/>across other work items, therefore, free from interference, whereas the<a id="id288" class="indexterm"/> variables <code class="literal">in</code> and <code class="literal">out</code> exist in the global memory space and are visible by all work items:</p><div><pre class="programlisting">__kernel void copyNPaste(__global float* in, __global float8* out) {
    size_t id = get_global_id(0);
    size_t index = id*sizeof(float8);
    float8 t = vload8(index, in);
    out[index].s0 = t.s0;
  //code omitted
  out[index].s7 = t.s7;
}</pre></div></div><div><div><div><div><h2 class="title"><a id="ch03lvl2sec64"/>How it works…</h2></div></div></div><p>The following diagram illustrates the OpenCL programming model:</p><div><img src="img/4520OT_03_04.jpg" alt="How it works…"/></div><p>Let's use the <a id="id289" class="indexterm"/>preceding diagram to understand how your <a id="id290" class="indexterm"/>kernel will function in OpenCL. Imagine you <a id="id291" class="indexterm"/>have a kernel named <code class="literal">doCompute</code> that takes several arguments that reference the global, constant, local, or private memory spaces. Work and data is divided among the kernels across the compute units represented by the W<sub>0…4</sub>; they would represent either work groups (collection of work items) or work items.</p><p>Typically, computing in OpenCL often either involves individual work items performing the computation independently via the global, private, or constant spaces, or collecting these work items to form a work group so that they can load and store data more efficiently via utilizing the local memory space since that space allows sharing of data across all work items in the work group hence, preventing multiple memory loads from device memory.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch03lvl1sec30"/>Configuring your OpenCL projects to enable the double data type</h1></div></div></div><p>Today's modern<a id="id292" class="indexterm"/> processors from Intel, AMD, and ARM have their floating-point units (FPUs) IEEE 754 compliant; however, ARM has <a id="id293" class="indexterm"/>both hardware and software support for half-precision numbers in addition to single-precision and double-precision numbers. Hence this implies that your OpenCL programs can actually utilize half-precision on ARM-based processors and this raise a question on how can one determine what sort of floating-point support does the device have.</p><p>The answer to that question is to query the device via the <code class="literal">clGetDeviceInfo</code> API and passing in any of the following parameters: <code class="literal">CL_DEVICE_SINGLE_FP_CONFIG</code>, <code class="literal">CL_DEVICE_DOUBLE_FP_CONFIG</code>, and <code class="literal">CL_DEVICE_HALF_FP_CONFIG</code> which identifies whether the device supports single-precision, double-precision, or half-precision number operations.</p><div><div><h3 class="title"><a id="tip13"/>Tip</h3><p><code class="literal">CL_DEVICE_HALF_FP_CONFIG</code> and <code class="literal">CL_DEVICE_DOUBLE_FP_CONFIG</code> are not supported on Mac OSX 10.6 for OpenCL 1.0.</p></div></div><p>The result of API invocation returns an object of <code class="literal">cl_device_fp_config</code> type.</p><div><div><h3 class="title"><a id="tip14"/>Tip</h3><p>At the time of this writing, <code class="literal">CL_FP_SOFT_FLOAT</code> was not available on Mac OSX 10.6, but available on AMD APP SDK v2.7 and Intel OpenCL SDK.</p></div></div><p>In the case of the double-precision floating-point values, the OpenCL device extension, <code class="literal">cl_khr_fp64</code>, needs to be present before you can utilize the <code class="literal">double</code> data type in the kernel. As of OpenCL 1.2, the developer no longer has to query the device's extensions to verify the existence of the double-precision floating-point support, and we'll explain what you'll need to do in this case in the later part of this recipe.</p><div><div><h3 class="title"><a id="tip15"/>Tip</h3><p>As of OpenCL 1.1, the working committee does not mandate the support of the <code class="literal">double</code> data type except through the OpenCL 1.1 device extension <code class="literal">cl_khr_fp64</code>. If you are using AMD devices, you should know that AMD provides an extension that implements a subset of <code class="literal">cl_khr_fp64</code> and is known as <code class="literal">cl_amd_fp64</code>.</p></div></div><p>Let's understand this with a simple example.</p><div><div><div><div><h2 class="title"><a id="ch03lvl2sec65"/>Getting ready</h2></div></div></div><p>In the upcoming example, the goal of the example is to illustrate the use of a <code class="literal">double</code> data type to hold the intermediate result of adding two <code class="literal">floats</code>, after which we send this <code class="literal">double</code> to be stored as a <code class="literal">float</code> in a result array. Take note that you cannot use the <code class="literal">double</code> type in the kernel code if the extension <code class="literal">cl_khr_fp64</code> or <code class="literal">cl_amd_fp64</code> (for AMD devices) is enabled.</p><p>The two test machines involved have<code class="literal"> cl_khr_fp64</code> supported on the Intel Core i7 processor and a NVIDIA GPU but the ATI 6870x2 GPU doesn't support <code class="literal">cl_khr_fp64</code> or <code class="literal">cl_amd_fp64</code>.</p></div><div><div><div><div><h2 class="title"><a id="ch03lvl2sec66"/>How to do it…</h2></div></div></div><p>Following is the <a id="id294" class="indexterm"/>code excerpt <a id="id295" class="indexterm"/>from <code class="literal">Ch3/double_support/double_support.cl</code>, which illustrates the kernel code:</p><div><pre class="programlisting">#ifdef fp64
#pragma OPENCL EXTENSION cl_khr_fp64 : enable
#endif
__kernel void add3(__global float* a, __global float* b, __global float* out) {
  int id = get_global_id(0);
#ifdef fp64
  double d = (double)a[id] + (double)b[id];
  out[id] = d;
#else
  out[id] = a[id] + b[id];
#endif
}</pre></div><p>Next, is the code snippet from <code class="literal">Ch3/double_support/double_support.c</code>, where it shows how to set the kernel arguments to the function <code class="literal">add3</code>:</p><div><pre class="programlisting">// memobj1 &amp; memobj2 refers to float arrays for consumption
// outObj refers to the output float array
error = clSetKernelArg(kernels[j], 0, sizeof(cl_mem), &amp;memobj1);
error = clSetKernelArg(kernels[j], 1, sizeof(cl_mem), &amp;memobj2);
error = clSetKernelArg(kernels[j], 2, sizeof(cl_mem), &amp;outObj);
if (error != CL_SUCCESS) { 
  perror("Unable to set buffer object in kernel arguments");
  exit(1);
}
/* Enqueue the kernel to the command queue */
size_t local[1] = {1};
size_t global[1] = {64};
error = clEnqueueNDRangeKernel(cQ, kernels[j], 1, NULL, global, local, 0, NULL, NULL);
if (error != CL_SUCCESS) {
  perror("Unable to enqueue task to command-queue");
  exit(1);}</pre></div><p>To build the program with <code class="literal">CMake</code>, navigate to the directory <code class="literal">Ch3/double_support</code>, and enter <code class="literal">make</code>. It should drop a nice binary named <code class="literal">DoubleSupport</code> upon which you can execute it to observe the results. On both the test machines, the results for a small run, that is, 64-floating-point values are good with the runs on CPU and GPU.</p><div><pre class="programlisting">
<strong>Number of OpenCL platforms found: 1</strong>
<strong>Number of detected OpenCL devices: 2</strong>
<strong>Kernel name: add3 with arity: 3</strong>
<strong>About to create command queue and enqueue this kernel...</strong>
<strong>Task has been enqueued successfully!</strong>
<strong>Checking data of size: 64</strong>
<strong>Check passed!</strong>
<strong>Kernel name: add3 with arity: 3</strong>
<strong>About to create command queue and enqueue this kernel...</strong>
<strong>Task has been enqueued successfully!</strong>
<strong>Checking data of size: 64</strong>
<strong>Check passed!</strong>
</pre></div><p>The code in this example was constructed in such a manner that even if <code class="literal">double</code> wasn't supported the program will run. Upon inspecting the code, you will realize that its use case was to hold the result of adding two <code class="literal">float</code> values (which by intention will not overflow) but in other <a id="id296" class="indexterm"/>situations, you might want to use <code class="literal">double</code>s, and the conditional-directives, that is, <code class="literal">#ifdef</code>, <code class="literal">#else</code>, and <code class="literal">#endif</code> used to check for the presence of double floating-point support for the device and it is a standard technique.</p></div><div><div><div><div><h2 class="title"><a id="ch03lvl2sec67"/>How it works...</h2></div></div></div><p>The type, <code class="literal">cl_device_fp_config</code> is actually composed of several values (shown in the following table) and you can <a id="id297" class="indexterm"/>determine whether a particular feature is supported or not by performing a bitwise-AND operation and for example, if we wish to determine which rounding modes are supported in double-precision operations then, we will have the following code:</p><div><pre class="programlisting">cl_device_fp_config config;
clGetDeviceInfo( deviceId, CL_DEVICE_DOUBLE_FP_CONFIG, sizeof(config), &amp;config, NULL);
if (config &amp; CL_FP_ROUND_TO_NEAREST) printf("Round to nearest is supported on the device!");</pre></div><div><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Parameter</p>
</th><th style="text-align: left" valign="bottom">
<p>float</p>
</th><th style="text-align: left" valign="bottom">
<p>double</p>
</th><th style="text-align: left" valign="bottom">
<p>half</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p><code class="literal">CL_FP_DENORM</code></p>
</td><td style="text-align: left" valign="top">
<p>Optional</p>
</td><td style="text-align: left" valign="top">
<p>Supported</p>
</td><td style="text-align: left" valign="top">
<p>Optional</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">CL_FP_INF_NAN</code></p>
</td><td style="text-align: left" valign="top">
<p>Supported</p>
</td><td style="text-align: left" valign="top">
<p>Supported</p>
</td><td style="text-align: left" valign="top">
<p>Supported</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">CL_FP_ROUND_TO_NEAREST</code></p>
</td><td style="text-align: left" valign="top">
<p>Supported</p>
</td><td style="text-align: left" valign="top">
<p>Supported</p>
</td><td style="text-align: left" valign="top">
<p>Optional</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">CL_FP_ROUND_TO_ZERO</code></p>
</td><td style="text-align: left" valign="top">
<p>Optional</p>
</td><td style="text-align: left" valign="top">
<p>Supported</p>
</td><td style="text-align: left" valign="top">
<p>Supported</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">CL_FP_ROUND_TO_INF</code></p>
</td><td style="text-align: left" valign="top">
<p>Optional</p>
</td><td style="text-align: left" valign="top">
<p>Supported</p>
</td><td style="text-align: left" valign="top">
<p>Supported</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">CL_FP_FMA</code></p>
</td><td style="text-align: left" valign="top">
<p>Optional</p>
</td><td style="text-align: left" valign="top">
<p>Supported</p>
</td><td style="text-align: left" valign="top">
<p>Optional</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">CL_FP_SOFT_FLOAT</code></p>
</td><td style="text-align: left" valign="top">
<p>Optional</p>
</td><td style="text-align: left" valign="top">
<p>Optional</p>
</td><td style="text-align: left" valign="top">
<p>Optional</p>
</td></tr></tbody></table></div><p>For those who are <a id="id298" class="indexterm"/>inclined to use OpenCL 1.2, the specification has made double-precision an optional feature instead of an extension, and this means that instead of checking for the existence of the extensions <code class="literal">cl_khr_fp64</code> or <code class="literal">cl_amd_fp64</code> in the device, you will simply check that the returned value of the call to <code class="literal">clGetDeviceInfo</code> when passed the parameter <code class="literal">CL_DEVICE_PREFERRED_VECTOR_WIDTH_DOUBLE</code> and <code class="literal">CL_DEVICE_NATIVE_VECTOR_WIDTH</code> must be equal to <code class="literal">1</code> if the device were to support double-precision. The<a id="id299" class="indexterm"/> following code snippet illustrates how to check for the preferred native vector width size for built-in scalar types that can be put into vectors:</p><div><pre class="programlisting">cl_uint vectorWidth;
size_t returned_size;
clGetDeviceInfo( deviceId, CL_DEVICE_PREFERRED_VECTOR_WIDTH_DOUBLE,sizeof(cl_uint), &amp;vectorWidth, &amp;returned_size);
if(vectorWidth &gt; 0) printf("Vectors of size %d for 'double' are:", vectorWidth);</pre></div></div></div></body></html>