<html><head></head><body>
  <div id="sbo-rt-content"><div class="chapter" title="Chapter 9. Reporting and Testing – Iterating on Analytic Systems"><div class="titlepage"><div><div><h1 class="title"><a id="ch09"/>Chapter 9. Reporting and Testing – Iterating on Analytic Systems</h1></div></div></div><p>In previous chapters we have considered many components of an analytical application, from the input data set to the choice of algorithm and tuning parameters, and even illustrated a potential deployment strategy using a web server. In this process, we considered parameters such as scalability, interpretability, and flexibility in making our applications robust to both later refinements of an algorithm and changing requirements of scale. However, these sorts of details miss the most important element of this application: your business partners who hope to derive insight from the model and the continuing needs of the organization. What metrics should we gather on the performance of a model to make the case for its impact? How can we iterate on an initial model to optimize its use for a business application? How can these results be articulated to stakeholders? These sorts of questions are key in conveying the benefit of building analytical applications for your organization.</p><p>Just as we can use increasingly larger data sets to build predictive models, automated analysis packages and "big data" systems are making it easier to gather substantial amounts of information about the behavior of algorithms. Thus, the challenge becomes not so much if we can collect data on an algorithm or how to measure this performance, but to choose what statistics are most relevant to demonstrate value in the context of a business analysis. In order to equip you with the skills to better monitor the health of your predictive applications, improve them through iterative milestones and explain these techniques to others in this chapter, we will:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Review common model diagnostics and performance indicators.</li><li class="listitem" style="list-style-type: disc">Describe how A/B testing may be used to iteratively improve upon a model.</li><li class="listitem" style="list-style-type: disc">Summarize ways in which predictive insights from predictive models can be communicated in reports.</li></ul></div><div class="section" title="Checking the health of models with diagnostics"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec48"/>Checking the health of models with diagnostics</h1></div></div></div><p>Throughout the previous chapters, we have primarily focused on the initial steps of predictive modeling, from data preparation and feature extraction to optimization of parameters. However, it is unlikely that our customers or business will remain unchanging, so predictive models must typically adapt as well. We can use a number of diagnostics to check the <a id="id560" class="indexterm"/>performance of models over time, which serve as a useful benchmark to evaluate the health of our algorithms.</p><div class="section" title="Evaluating changes in model performance"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec86"/>Evaluating changes in model performance</h2></div></div></div><p>Let us consider a <a id="id561" class="indexterm"/>scenario in which we train a predictive model on customer data and evaluate its performance on a set of new records each day for a month afterward. If this were a classification model, such as predicting whether a customer will cancel their subscription<a id="id562" class="indexterm"/> in the next pay period, we could use a metric such as the <span class="strong"><strong>Area Under the Curve</strong></span> (<span class="strong"><strong>AUC</strong></span>) of the <a id="id563" class="indexterm"/>
<span class="strong"><strong>Receiver-Operator-Characteristic</strong></span> (<span class="strong"><strong>ROC</strong></span>) curve that we saw previously in <a class="link" href="ch05.html" title="Chapter 5. Putting Data in its Place – Classification Methods and Analysis">Chapter 5</a>, <span class="emphasis"><em>Putting Data in its Place – Classification Methods and Analysis</em></span>. Alternatively, in the case of a regression model, such as predicting average customer spend, we can use the <span class="emphasis"><em>R<sup>2</sup></em></span> value or average squared error:</p><div class="mediaobject"><img src="images/B04881_09_Formula1.jpg" alt="Evaluating changes in model performance"/></div><p>to quantify performance over time. If we observe a drop in one of these statistics, how can we analyze further what the root cause may be?</p><div class="mediaobject"><img src="images/B04881_09_01.jpg" alt="Evaluating changes in model performance"/></div><p>In the graph above, we show such a scenario, where we measured the AUC for a hypothetical ad-targeting algorithm for 30 days after initial training by quantifying how many of the targeted users clicked on an ad sent in an e-mail and visited the website for our <a id="id564" class="indexterm"/>company. We see AUC begin to dip on day 18, but because the AUC is an overall measure of accuracy, it is not clear whether all observations are being poorly predicted or only a subpopulation is leading to this drop in performance. Thus, in addition to measuring overall AUC, we might think of calculating the AUC for subsets of data defined by the input features. In addition to providing a way of identifying problematic new data (and suggest when the model needs to be retrained), such reports provide a way of identifying the overall business impact of our model.</p><div class="mediaobject"><img src="images/B04881_09_02.jpg" alt="Evaluating changes in model performance"/></div><p>As an example, for our ad-targeting algorithm, we might look at overall performance and compare that to one of the labels in our data set: whether the user is a current subscriber or not. It may not be surprising that the performance on subscribers is usually higher, as these users were already likely to visit our site. The non-subscribers, who may never have visited our site before, represent the real opportunity in this scenario. In the preceding graph, we see that, indeed, the performance on non-subscribers dropped on day 18. However, it is also worth noting that this does not <a id="id565" class="indexterm"/>necessarily tell the whole story of why the performance dropped. We still do not know why the performance on new members is lower. We can subset the data again and look for a correlated variable. For example, if we looked along a number of ad IDs (which correspond to different images displayed to a customer in an e-mail), we might find that the performance dip is due to one particular ad (please refer to the following graph). Following up with our business stakeholders, we might find that this particular ad was for a seasonal product and is only shown every 12 months. Therefore, the product was familiar to subscribers, who may have seen this product before, but not to non-members, who thus were unfamiliar with the item and did not click on it. We might be able to confirm this hypothesis by looking at subscriber data and seeing whether performance of the model also dips for subscribers with tenure less than 12 months.</p><p> </p><div class="mediaobject"><img src="images/B04881_09_03.jpg" alt="Evaluating changes in model performance"/></div><p>This sort of investigation can then begin to ask how to optimize this particular advertisement for new members, but can also indicate ways to improve our model training. In this scenario, it is likely that we trained the algorithm on a simple random sample of data that was biased for current subscribers, as we have more data on these customers if we took a simple random sample of event data: subscribers are more active, and thus both produce more impressions (as they may have registered for promotional e-mails) and are more likely to have clicked on ads. To improve our model, we might want to balance our training data between subscribers and non-subscribers to<a id="id566" class="indexterm"/> compensate for this bias.</p><p>In this simple example, we were able to diagnose the problem by examining performance of the model in only a small number of sub-segments of data. However, we cannot guarantee that this will always be the case, and manually searching through hundreds of variables will be inefficient. Thus, we might consider using a predictive model to help narrow down the search space. For example, consider using a <a id="id567" class="indexterm"/>
<span class="strong"><strong>gradient boosted machine</strong></span> (<span class="strong"><strong>GBM</strong></span>) from <a class="link" href="ch05.html" title="Chapter 5. Putting Data in its Place – Classification Methods and Analysis">Chapter 5</a>, <span class="emphasis"><em>Putting Data in its Place – Classification Methods and Analysis</em></span>, with the inputs being the same data we used to train our predictive model, and the outputs being the misclassification (either a label of 1, 0 for a categorical model, or a continuous error such as squared error or log loss in a regression model). We now have a model that predicts errors in the first model. Using a method such as a GBM allows us to examine systematically a large number of potential variables and use the resulting variable importance to pinpoint a smaller number of hypotheses.</p><p>Of course, the success of any these approaches hinges on the fact that the variable causing the drop in performance is a part of our training set, and that the issue has to do with the underlying algorithm or data. It is also certainly possible to imagine other cases where there is an additional variable we are not using to construct our data set for training which is causing the problem, such as poor connections on a given Internet service provider that are preventing users from clicking through an ad to our webpage, or a system problem such as failure in e-mail delivery.</p><p>Looking at performance by segments can also help us determine if the algorithm is functioning as intended when we make changes. For example, if we reweighted our training data to emphasize non-subscribers, we would hope that performance of AUC on these customers would improve. If we only examined overall performance, we might observe increases that are improvements on existing customers, but not the effect we actually wished to achieve.</p></div><div class="section" title="Changes in feature importance"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec87"/>Changes in feature importance</h2></div></div></div><p>Besides examining the accuracy of models over time, we might also want to examine changes in the importance of different input data. In a regression model, we might examine<a id="id568" class="indexterm"/> the important coefficients as judged by magnitude and statistical significance, while in a decision-tree-based algorithm such as a Random Forest or GBM, we can look at measures of variable importance. Even if the model is performing at the same level as described by evaluation statistics discussed previously, shifts in the underlying variables may signal issues in data logging or real changes in the underlying data that are of business significance.</p><p>Let us consider a churn model, where we input a number of features for a user account (such as zip code, income level, gender, and engagement metrics such as hours spent per week on our website) and try to predict whether a given user will cancel his/her subscription at the end of each billing period. While it is useful to have a score predicting the likelihood of churn, as we would target these users with additional promotional campaigns or targeted messaging, the underlying features that contribute to this prediction may provide insight for more specific action.</p><p>In this example, we generate a report of the 10 most important features in the predictive model each week. Historically, this list has been consistent, with the customer's profession and income being the top variables. However, in one week, we find that income is no longer in this list, and that instead zip code has replaced it. When we check the data flowing into the model, we find that the income variable is no longer being logged correctly; thus, zip code, which is correlated with income, becomes a substitute for this feature in the model, and our regular analysis of variable importance helped us detect a significant data issue.</p><p>What if, instead the income variable was being logged correctly? In this case, it seems unlikely that zip code is more powerful a predictor than income if the underlying feature both are capturing is a customer's finances. Thus, we might examine whether there are particular zip codes for which churn has changed significantly over the past week. Upon doing so, we find that a competitor recently launched a site with a lower price in certain zip codes, letting us both understand the reason for the rise in zip code as a predictor (customers with the lower price option are more likely to abandon our site) and indicate market dynamics that are of larger interest.</p><p>This second scenario also suggests another variable we might monitor: the correlation between variables in our data set. While it is both computationally difficult and practically restrictive to comprehensively consider every pair of variables in large<a id="id569" class="indexterm"/> data sets, we can use dimensionality reduction techniques such as Principal Components Analysis described in <a class="link" href="ch06.html" title="Chapter 6. Words and Pixels – Working with Unstructured Data">Chapter 6</a>, <span class="emphasis"><em>Words and Pixels – Working with Unstructured Data</em></span>, to provide a high-level summary of the correlations between variables. This reduces the task of monitoring such correlations to examination of a few diagrams of the important components, which, in turn, can alert us to changes in the underlying structure of the data.</p></div><div class="section" title="Changes in unsupervised model performance"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec88"/>Changes in unsupervised model performance</h2></div></div></div><p>The examples <a id="id570" class="indexterm"/>we looked at previously all concern a supervised model where we have a target to predict, and we measure performance by looking at AUC or similar metrics. In the case of the unsupervised models we examined in <a class="link" href="ch03.html" title="Chapter 3. Finding Patterns in the Noise – Clustering and Unsupervised Learning">Chapter 3</a>, <span class="emphasis"><em>Finding Patterns in the Noise – Clustering and Unsupervised Learning</em></span>, our outcome is a cluster membership rather than a target. What sort of diagnostics can we look at in this scenario?</p><p>In cases where we have a gold-standard label, such as a human-annotated label of spam versus non-spam messages if we are clustering e-mail documents, we can examine whether the messages end up in distinct clusters or are mixed. In a sense, this is comparable to looking at classification accuracy. However, for unsupervised models, we might frequently not have any known label, and the clustering is purely an exploratory tool. We might still use human-annotated examples as a guide, but this becomes prohibitive for larger data sets. In other scenarios, such as sentiment in online media, remain subjective enough that human labels may not significantly enrich labels derived from automated methods such as the LDA topic model we discussed in <a class="link" href="ch06.html" title="Chapter 6. Words and Pixels – Working with Unstructured Data">Chapter 6</a>, <span class="emphasis"><em>Words and Pixels – Working with Unstructured Data</em></span>. In this case, how can we judge the quality of the clustering over time?</p><p>In cases where the number of groups is determined dynamically, such as through the Affinity Propagation Clustering algorithm described in <a class="link" href="ch03.html" title="Chapter 3. Finding Patterns in the Noise – Clustering and Unsupervised Learning">Chapter 3</a>, <span class="emphasis"><em>Finding Patterns in the Noise – Clustering and Unsupervised Learning</em></span>, we examine whether the number of clusters remains fixed over time. In most cases we previously examined, though, the number of clusters remains fixed. Thus, we could envision one diagnostic in which we examine the distance between the centers of the nearest clusters between training cycles: for example, with a k-means model with 20 clusters, assign each cluster in week 1 its closest match in week 2 and compare the distribution of the 20 distances. If the clustering remains stable, then the distribution of these distances should as well. Changes could indicate that 20 is no longer a good number to fit the data or that the composition of the 20 clusters is significantly changing over time. We might also examine a value such as the sum of squares error in k-means clustering over time to see if the quality of the obtained clusters is significantly varying.</p><p>Another quality metric that is agnostic to a specific clustering algorithm is Silhouette analysis (Rousseeuw, Peter J. "Silhouettes: a graphical aid to the interpretation and validation of cluster analysis." <span class="emphasis"><em>Journal of computational and applied mathematics 20</em></span> (1987): 53-65). For each data point <span class="strong"><strong>i</strong></span> in the set, we ask how dissimilar (as judged by the distance metric used in the clustering algorithm) on average it is to other points in its cluster, giving a value <span class="emphasis"><em>d(i)</em></span>. If the point <span class="strong"><strong>i</strong></span> is appropriately assigned, then <span class="emphasis"><em>d(i)</em></span> is near 0, as the average dissimilarity between <span class="strong"><strong>i</strong></span> and other points in its cluster is low. We could also calculate the same average dissimilarity value for <span class="strong"><strong>i</strong></span> for other clusters, and the second lowest value (the second best cluster assignment for <span class="strong"><strong>i</strong></span>) is given by <span class="emphasis"><em>d'(i)</em></span>. We then <a id="id571" class="indexterm"/>obtain a silhouette score between –1 and 1 using the formula:</p><div class="mediaobject"><img src="images/B04881_09_Formula2.jpg" alt="Changes in unsupervised model performance"/></div><p>If a data point is well assigned to its cluster, then it is much more dissimilar on average to other clusters. Thus, <code class="literal">d'(i)</code> (the 'second best cluster for i') is larger than <code class="literal">d(i)</code>, and the ratio in the silhouette score formula is near 1. Conversely, if the point is poorly assigned to its cluster, then the value of <code class="literal">d'(i)</code> could be less than <code class="literal">d(i)</code>, giving a negative value in the numerator of the silhouette score formula. Values near zero suggest the point could be reasonably assigned in the two clusters equally well. By looking at the distribution of silhouette scores over a data set, we can get a sense of how well points are being clustered over time.</p><p>Finally, we might use a bootstrap approach, where we rerun the clustering algorithm many times and ask how often two points end up in the same cluster. The distribution of these cluster co-occurrences (between 0 and 1) can also give a sense of how stable the assignment is over time.</p><p>Like clustering models, dimensionality reduction techniques also do not lend themselves easily to a gold standard by which to judge model quality over time. However, we can take values such as the principal components vectors of a data set and examine their pairwise dissimilarity (for example, using the cosine score described in <a class="link" href="ch03.html" title="Chapter 3. Finding Patterns in the Noise – Clustering and Unsupervised Learning">Chapter 3</a>, <span class="emphasis"><em>Finding Patterns in the Noise – Clustering and Unsupervised Learning</em></span>) to determine if they are changing significantly. In the case of matrix decomposition techniques, we could also look at the reconstruction error (for example, averaged squared difference over all matrix elements) between the original matrix and the product of the factored elements (such as the <span class="emphasis"><em>W</em></span> and <span class="emphasis"><em>H</em></span> matrices in nonnegative matrix factorization).</p></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Iterating on models through A/B testing"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec49"/>Iterating on models through A/B testing</h1></div></div></div><p>In the examples above and in the previous chapters of this volume, we have primarily examined analytical systems in terms of their predictive ability. However, these measures do <a id="id572" class="indexterm"/>not necessarily ultimately quantify the kinds of outcomes that are meaningful to the <a id="id573" class="indexterm"/>business, such as revenue and user engagement. In some cases, this shortcoming is overcome by converting the performance statistics of a model into other units that are more readily understood for a business application. For example, in our preceding churn model, we might multiply our prediction of 'cancel' or 'not-cancel' to generate a predicted dollar amount lost through subscriber cancellation.</p><p>In other scenarios, we are fundamentally unable to measure a business outcome using historical data. For example, in trying to optimize a search model, we can measure whether a user clicked a recommendation and whether they ended up purchasing anything after clicking. Through such retrospective analysis, we can only optimize the order of the recommendations the user was actually presented on a web page. However, it might be that with a better search model, we would have presented the user a completely different set of recommendations, which would have also led to greater click-through rates and revenue. However, we cannot quantify this hypothetical scenario, meaning we need alternative methods to assess algorithms as we improve them.</p><p>One way to do so is through the process of experimentation, or A/B testing, which takes its name from the concept of comparing outcomes from test subjects (for example, customers) randomly assigned to treatment (for example, a search recommendation algorithm) A and B to determine which method generates the best result. In practice, there may be many more than two treatments, and the experiment can be randomized at the level of users, sessions (such as periods between login and logout on a website), products, or other units. While a truly comprehensive discussion of A/B testing is outside the scope of this chapter, we refer interested readers to more extensive references (Bailey, Rosemary A. <span class="emphasis"><em>Design of comparative experiments</em></span>. Vol. 25. Cambridge University Press, 2008; Eisenberg, Bryan, and John Quarto-vonTivadar. <span class="emphasis"><em>Always be testing: The complete guide to Google website optimizer</em></span>. John Wiley &amp; Sons, 2009; Finger, Lutz, and Soumitra Dutta. <span class="emphasis"><em>Ask, Measure, Learn: Using Social Media Analytics to Understand and Influence Customer Behavior</em></span>. " O'Reilly Media, Inc.", 2014).</p><div class="section" title="Experimental allocation – assigning customers to experiments"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec89"/>Experimental allocation – assigning customers to experiments</h2></div></div></div><p>You have an algorithm <a id="id574" class="indexterm"/>that you wish to improve—how can you compare its performance improving a metric (such as revenue, retention, engagement) in comparison to an existing model (or no predictive model at all)? In this comparison, we want to make sure to remove all potential confounding factors other than the two (or more) models themselves. This idea underlies the concept of experimental randomization: if we randomly assign customers (for example) to receive search recommendations from two different models, any variation in customer demographics such as age, income, and subscription tenure should be roughly the same between the groups. Thus, when we compare the performance of models over time between groups following this random allocation, differences in performance of the algorithms can be attributed to the models themselves, as we have already accounted for other potential sources of variation through this randomization.</p><p>How can we guarantee that users are assigned to experimental groups randomly? One possibility is to assign each member a random number between 0 and 1, and split them based on whether this number is greater than 0.5 or not. However, this method might have the downside that it will be difficult to replicate our analysis since the random number assigned to a user could change. Alternatively, we often will have user IDs, random numbers assigned to a given account. Assuming the form of this number is sufficiently randomized, we could take the modulus of this number (the remainder when divided by a fixed denominator, such as 2) and assign users to the two groups based on the modulus (for example, if 2, this would be 0 or 1 based on if the account ID is even or odd).  Thus, users are randomly allocated to the two groups, but we can easily recreate this assignment in the future.</p><p>We might also consider whether we always want a simple random stratification. In the ad-targeting example discussed previously, we are actually more concerned with the performance of the algorithm on nonsubscribers, rather than the existing users who would comprise most of a random allocation in our example. Thus, depending upon our objective, we may want to consider randomly allocating stratified samples in which we oversampled some accounts to compensate for inherent skew in the data. For example, we would enforce a roughly equal number of accounts per country to offset geographical bias toward a more populous region, or equal numbers of teenage and adult users for a service with primarily younger users.</p><p>In addition to randomly assigning users to receive an experience (such as search recommendations or ads sent through e-mail) dictated by a particular algorithm, we often need a control, a baseline to which to compare the results. In some cases, the control might be the outcome expected with no predictive model used at all. In others, we are comparing the old predictive model to a new version.</p></div><div class="section" title="Deciding a sample size"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec90"/>Deciding a sample size</h2></div></div></div><p>Now that we know what we are trying to test and have a way to randomly assign users, how should <a id="id575" class="indexterm"/>we determine how many to allocate to an experiment? If we have a control and several experimental conditions, how many users should we assign to each group? If our predictive model relies upon user interaction (for example, gauging the performance of a search model requires the user to visit a website) that may not be guaranteed to occur for every member of the experimental population, how many activities (for example, searches) do we need to accumulate to judge the success of our experiment? These questions all concern making estimates of effect size and experimental power.</p><p>As you may recall from statistics, in a controlled experiment we are trying to determine whether the differences in outcomes between two populations (for example, the revenue generated from groups of users in our experimental evaluation of different prediction algorithms for ad targeting) are more likely due to random change or actual differences in the performance of an algorithm. These two options are also known as the null hypothesis, often represented by <span class="emphasis"><em>H0</em></span> (that is, there is no difference between the two groups) and the alternative represented by <span class="emphasis"><em>H1</em></span>. To determine whether an effect (for example, the difference in revenue between the two groups) is explained by random chance, we compare this effect to a distribution (frequently the t-distribution, for reasons we will discuss below) and ask what is the likelihood of observing this effect or greater if the true effect is <span class="strong"><strong>0</strong></span>. This value—the cumulative probability of an effect greater than or equal to the observed given an assumption of no effect—is known as the p-value, to which we often apply a threshold such as 0.05 (in the example below, this is indicated by the shaded region on the left side of the standard normal distribution).</p><p> </p><div class="mediaobject"><img src="images/B04881_09_06.jpg" alt="Deciding a sample size"/></div><p>When we are assessing this statistical significance, we may encounter two kinds of errors because of the fact that any measurement of effect is subject to uncertainty. We never really know the true value of an effect, but rather measure this real, unknown effect with some error. First, we could inaccurately declare a result to be statistically significant when it is not. This is known as type I error (false positive). Secondly, we could fail to declare a result statistically significant when it actually is (also known as Type II error, or false negative).</p><p>We can ask the <a id="id576" class="indexterm"/>question how many samples we need to declare a particular effect (for example, revenue difference) significant, if there really were a difference between our two populations. While exact applications may vary, we will assume for illustration that the two groups are sufficiently large and that any difference between measured average values (such as revenue or click through rate) follows a normal distribution, which is due to the <span class="strong"><strong>Law of Large Numbers</strong></span>. We can then evaluate this difference using the t-distribution, which approximate the standard normal distribution for large samples but does not require that we know the population mean and variance, just the mean and variance of a sample. Then, calculating the necessary number of samples requires just using the following formula (for a t-test between samples of whose variance is potentially unequal, also known as Welch's t-test):</p><div class="mediaobject"><img src="images/B04881_09_Formula3.jpg" alt="Deciding a sample size"/></div><p>Here, <span class="emphasis"><em>Y</em></span> is the average effect (for example, revenue per customer) of each group, and S (the standard deviation) is given by the following equation:</p><div class="mediaobject"><img src="images/B04881_09_Formula4.jpg" alt="Deciding a sample size"/></div><p>Here, <span class="emphasis"><em>S<sub>1</sub></em></span> and <span class="emphasis"><em>S<sub>2</sub></em></span> are<a id="id577" class="indexterm"/> the sample variances, and <span class="emphasis"><em>n<sub>1</sub></em></span> and <span class="emphasis"><em>n<sub>2</sub></em></span> are the sample sizes of the two groups. So if we want to be able to detect a difference of 10, for example, with a p-value of 0.05, we solve for the sample size at which the t-statistic under the null yields a false positive 5% of the time (for which we use the normal approximation of 1.64, which is the value at which the cumulative distribution function of the standard normal distribution assumes the value of 0.05). We can solve:</p><div class="mediaobject"><img src="images/B04881_09_Formula5.jpg" alt="Deciding a sample size"/></div><p>Thus, given values for the variance of the groups in our experiment, we can plug in different values of <span class="emphasis"><em>n</em></span> for the two groups and see if they are sufficient to fulfill the inequality. For this application, we might estimate the variance by looking at historical data for revenue among users of a given sample size.</p><p>If you look at the right-hand side of the preceding equation carefully, you will see that (assuming reasonably similar sample variances, which is not an unreasonable assumption in many large scale experiments such as those conducted on consumer website) this value will be determined by the smaller of <span class="emphasis"><em>n<sub>1</sub>,n<sub>2</sub></em></span>, since as we increase one sample size the term containing it tends toward 0. Thus, we often achieve optimal power by assigning equal sample sizes to both groups. This fact is important in considering how to decide the relative sizes of our control and experimental cells. Take an example in which we have three version of an ad-targeting algorithm, along with no algorithm at all as a control, and measure the resulting click through rate. Based on the preceding calculation, we need to decide what our main question is. If we want to know if any algorithm is better than no algorithm, we should assign users evenly between control and any of the three algorithm variants. However, if we instead want to decide which algorithm is best compared to control, we want equal numbers of users in all four cells, so that control and each treatment are of approximately equal size.</p><p>Note that the preceding calculation assumes we are interested in a fixed difference of 10 in response between the two groups. We could also just ask whether there is any difference at all (for example, the difference is not zero). The choice depends whether any lift represented by the algorithm is valuable or whether a fixed improvement is necessary to <a id="id578" class="indexterm"/>achieve the business goal at hand.</p></div><div class="section" title="Multiple hypothesis testing"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec91"/>Multiple hypothesis testing</h2></div></div></div><p>The last topic we will cover is somewhat subtle, but important due to the fact that with models <a id="id579" class="indexterm"/>with numerous tunable parameters and algorithm variations, we may often be performing a large number of hypothesis tests within a single A/B experiment. While we might evaluate each test at a significance of 0.05, if we perform 20 such evaluations, we have a probability of 20*0.05 = 1 (or almost certainty) of finding some significant result, even if it is in truth random noise. This issue, known as <span class="emphasis"><em>Multiple Hypothesis Testing</em></span>, requires that we may need to recalibrate our significance threshold. The simplest way to do so is to divide the p-value threshold we use (for example, 0.05) by the number of tests performed (20) to obtain a new threshold for significance. This is known as Bonferonni Correction (Dunn, Olive Jean. "Estimation of the medians for dependent variables." <span class="emphasis"><em>The Annals of Mathematical Statistics</em></span> (1959): 192-197; Dunnett, Charles W. "A multiple comparison procedure for comparing several treatments with a control." <span class="emphasis"><em>Journal of the American Statistical Association</em></span> 50.272 (1955): 1096-1121) and, while correct, may be overly conservative in some scenarios. It assumes that we want a type I (false positive) rate of zero. However, in exploratory analyses, we often can accept some nonzero false positive rate as long as we are reasonably sure that a majority of the significant results are replicable. In this scenario, a <a id="id580" class="indexterm"/>
<span class="strong"><strong>familywise error rate</strong></span> (<span class="strong"><strong>FWER</strong></span>) approach may be preferable. While a discussion of FWER is outside the scope of this chapter, we refer the interested reader to references on the subject (Shaffer, Juliet Popper. "Multiple hypothesis testing." <span class="emphasis"><em>Annual review of psychology</em></span> 46 (1995): 561; Toothaker, Larry E. <span class="emphasis"><em>Multiple comparison procedures</em></span>. No. 89. Sage, 1993).</p></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Guidelines for communication"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec50"/>Guidelines for communication</h1></div></div></div><p>Now that we have <a id="id581" class="indexterm"/>covered debugging, monitoring and iterative testing of predictive models, we close with a few notes on communicating results of algorithms to a more general audience.</p><div class="section" title="Translate terms to business values"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec92"/>Translate terms to business values</h2></div></div></div><p>In this text, we frequently discuss evaluation statistics or coefficients whose interpretations are<a id="id582" class="indexterm"/> not immediately obvious, nor the difference in numerical variation for these values. What does it mean for a coefficient to be larger or smaller? What does an AUC mean in terms of customer interactions predicted? In any of these scenarios, it is useful to translate the underlying value into a business metric in explaining their significance to non-technical colleagues: for example, coefficients in a linear model represent the unit change in an outcome (such as revenue) for a 1-unit change in particular input variable. For transformed variables, it may be useful to relate values such as the log-odds (from logistic regression) to a value such as doubling the probability of an event. Additionally, as discussed previously, we may need to translate the outcome we predict (such as a cancelation) into a financial amount to make its implication clear. This sort of conversion is useful not only in communicating the impact of a predictive algorithm, but also in clarifying priorities in planning. If the development time for an algorithm (whose cost might be approximated by the salaries of the employees involved) is not offset by the estimated benefit of its performance, then this suggests it is not a useful application from a business perspective.</p></div><div class="section" title="Visualizing results"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec93"/>Visualizing results</h2></div></div></div><p>While not all algorithms <a id="id583" class="indexterm"/>we have discussed are amenable to visualization, many have elements that may be plotted for clarity. For example, regression coefficients can be compared using a barplot, and tree models may be represented visually by the branching decision points leading to a particular outcome. Such graphics help to turn inherently mathematical objects into more understandable results as well as provide ongoing insight into the performance of models, as detailed previously.</p><p>As a practical example of building such a service, this chapter's case study will walk through the generation of a custom dashboard as an extension of the prediction service we built in <a class="link" href="ch08.html" title="Chapter 8. Sharing Models with Prediction Services">Chapter 8</a>, <span class="emphasis"><em>Sharing Models with Prediction Services</em></span>.</p><div class="section" title="Case Study: building a reporting service"><div class="titlepage"><div><div><h3 class="title"><a id="ch09lvl3sec03"/>Case Study: building a reporting service</h3></div></div></div><p>In <a class="link" href="ch08.html" title="Chapter 8. Sharing Models with Prediction Services">Chapter 8</a>, <span class="emphasis"><em>Sharing Models with Prediction Services</em></span>, we created a prediction service that uses MongoDB as a <a id="id584" class="indexterm"/>backend database to store model data and predictions. We can use this same database as a source to create a reporting service. Like the separation of concerns between the CherryPy server and the modeling service application that we described in <a class="link" href="ch08.html" title="Chapter 8. Sharing Models with Prediction Services">Chapter 8</a>, <span class="emphasis"><em>Sharing Models with Prediction Services</em></span>, a reporting service can be written without any knowledge of how the information in the database is generated, making it possible to generate a flexible reporting infrastructure as the modeling code may change over time. Like the prediction service, our reporting service has a few key components.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The server that will receive requests for the output of the reporting service.</li><li class="listitem" style="list-style-type: disc">The reporting application run by the server, which receive requests from the server and routes them to display the correct data.</li><li class="listitem" style="list-style-type: disc">The database from which we retrieve the information required to make a plot.</li><li class="listitem" style="list-style-type: disc">Charting systems that render the plots we are interested in for the end user.</li></ul></div><p>Let us walk through an example of each component, which will illustrate how they fit together.</p></div></div><div class="section" title="The report server"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec94"/>The report server</h2></div></div></div><p>Our server code is <a id="id585" class="indexterm"/>very similar to the <code class="literal">CherryPy</code> server we used in <a class="link" href="ch08.html" title="Chapter 8. Sharing Models with Prediction Services">Chapter 8</a>, <span class="emphasis"><em>Sharing Models with Prediction Services</em></span>.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note14"/>Note</h3><p>This example was inspired by the code available at <a class="ulink" href="https://github.com/adilmoujahid/DonorsChoose_Visualization">https://github.com/adilmoujahid/DonorsChoose_Visualization</a>.</p></div></div><p>The only difference is that instead of starting the modelservice application, we use the server to start the reportservice, as you can see in the <code class="literal">main</code> method:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; if __name__ == "__main__":</strong></span>
<span class="strong"><strong>…      service = reportservice()</strong></span>
<span class="strong"><strong>…    run_server(service)</strong></span>
</pre></div><p>We can test this server by simple running the following on the command line:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>python report_server.py</strong></span>
</pre></div><p>You should see the server begin to log information to the console as we observed previously for the modelserver.</p></div><div class="section" title="The report application"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec95"/>The report application</h2></div></div></div><p>In the application <a id="id586" class="indexterm"/>code, which is also a Flask application like the model service we built in <a class="link" href="ch08.html" title="Chapter 8. Sharing Models with Prediction Services">Chapter 8</a>, <span class="emphasis"><em>Sharing Models with Prediction Services</em></span>, we need a few additional pieces of information that we didn't use previously. The first is path variable to specify the location of the JavaScript and CSS files that we will need when we construct our charts, which are specified using the commands:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; static_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'templates/assets')</strong></span>
</pre></div><p>We also need to specify where to find the HTML pages that we render to the user containing our charts with the argument:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; tmpl_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'templates')</strong></span>
</pre></div><p>When we initialize our application, we will pass both of these as variables to the constructor:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; app = Flask(__name__,template_folder=tmpl_dir,static_folder=static_dir)</strong></span>
</pre></div><p>To return this application when called by the server, we simply return app in the <code class="literal">reportservice</code> function:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; def reportservice():</strong></span>
<span class="strong"><strong>…    return app</strong></span>
</pre></div><p>We now just need to specify the response of the application to requests forwarded by the server. The first is simply to render a page containing our charts:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; @app.route("/")</strong></span>
<span class="strong"><strong>…   def index():</strong></span>
<span class="strong"><strong>…     return render_template("layouts/hero-thirds/index.html")</strong></span>
</pre></div><p>The template in this example is taken from <a class="ulink" href="https://%20github.com/keen/dashboards">https:// github.com/keen/dashboards</a>, an open source project that provides reusable templates for generating quick dashboards.</p><p>The second route will allow us to retrieve the data we will use to populate the chart. This is not meant to be exposed to the end user (though you would see a text dump of all the JSONS in our collection if you navigated to this endpoint in your browser): rather it is used by the client-side JavaScript code to retrieve the information to populate the charts. First we need to start the mongodb application in another terminal window using:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt; mongod</strong></span>
</pre></div><p>Next, in our code, we need to specify the MongoDB parameters to use in accessing our data. While we could have passed these as parameters in our URL, for simplicity in this example, we will just hard-code them at the top of the reportservice code to point to the <a id="id587" class="indexterm"/>results of bulk scoring the bank dataset we used to train our Spark Logistic Regression Model in <a class="link" href="ch08.html" title="Chapter 8. Sharing Models with Prediction Services">Chapter 8</a>, <span class="emphasis"><em>Sharing Models with Prediction Services</em></span>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; FIELDS = {'score': True, \</strong></span>
<span class="strong"><strong>…          'value': True, \</strong></span>
<span class="strong"><strong>…          '_id': False}</strong></span>
<span class="strong"><strong>… MONGODB_HOST = 'localhost'</strong></span>
<span class="strong"><strong>… MONGODB_PORT = 27017</strong></span>
<span class="strong"><strong>… DBS_NAME = 'datasets'</strong></span>
<span class="strong"><strong>… COLLECTION_NAME = 'bankResults'</strong></span>
</pre></div><p>Note that we could just as easily have pointed to a remote data source, rather than one running on our machine, by changing the <code class="literal">MONGODB_HOST</code> parameter. Recall that when we stored the results of bulk scoring, we saved records with two elements, the score and the original data row. In order to plot our results, we will need to extract the original data row and present it along with the score using the following code:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; @app.route("/report_dashboard")</strong></span>
<span class="strong"><strong>…  def run_report():</strong></span>
<span class="strong"><strong>…    connection = MongoClient(MONGODB_HOST, MONGODB_PORT)</strong></span>
<span class="strong"><strong>…    collection = connection[DBS_NAME][COLLECTION_NAME]</strong></span>
<span class="strong"><strong>…    data = collection.find(projection=FIELDS)</strong></span>
<span class="strong"><strong>…    records = []</strong></span>
<span class="strong"><strong>…    for record in data:</strong></span>
<span class="strong"><strong>…        tmp_record = {}</strong></span>
<span class="strong"><strong>…        tmp_record = record['value']</strong></span>
<span class="strong"><strong>…        tmp_record['score'] = record['score']</strong></span>
<span class="strong"><strong>…        records.append(tmp_record)</strong></span>
<span class="strong"><strong>…    records = json.dumps(records, default=json_util.default)</strong></span>
<span class="strong"><strong>…    connection.close()</strong></span>
</pre></div><p>Now that we have all of our scored records in a single array of json strings, we can plot them using a bit of JavaScript and HTML.</p></div><div class="section" title="The visualization layer"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec96"/>The visualization layer</h2></div></div></div><p>The final piece we <a id="id588" class="indexterm"/>will need is the client-side JavaScript code used to populate the charts, and some modifications to the <code class="literal">index.html</code> file to make use of the charting code. Let us look at each of these in turn.</p><p>The chart generating code is a JavaScript function contained in the file <code class="literal">report.js</code> that you can find under <code class="literal">templates/assets/js</code> in the project directory for <a class="link" href="ch09.html" title="Chapter 9. Reporting and Testing – Iterating on Analytic Systems">Chapter 9</a>, <span class="emphasis"><em>Reporting and Testing – Iterating on Analytic Systems</em></span>. We begin this function by calling for the data we need and waiting for it to be retrieved using the asynchronous function <code class="literal">d3.queue()</code>:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; d3_queue.queue()     </strong></span>
<span class="strong"><strong>… .defer(d3.json, "/report_dashboard")</strong></span>
<span class="strong"><strong>… .await(runReport);</strong></span>
</pre></div><p>Notice that this <a id="id589" class="indexterm"/>URL is the same endpoint that we specified earlier in the report application to retrieve the data from MongoDB. The <code class="literal">d3_queue</code> function calls this endpoint and waits for the data to be returned before running the <code class="literal">runReport</code> function. While a more extensive discussion is outside the scope of this text, <code class="literal">d3_queue</code> is a member of the <code class="literal">d3</code> library (<a class="ulink" href="https://d3js.org/">https://d3js.org/</a>), a popular visualization framework for the javascript language.</p><p>Once we have retrieved the data from our database, we need to specify how to plot it using the <code class="literal">runReport</code> function. First we will declare the data associated with the function:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; function runReport(error, recordsJson) {     </strong></span>
<span class="strong"><strong>…  var reportData = recordsJson;      </strong></span>
<span class="strong"><strong>…  var cf = crossfilter(reportData);</strong></span>
</pre></div><p>Though  it will not be apparent until we visually examine the resulting chart, the <code class="literal">crossfilter</code> library (<a class="ulink" href="http://square.github.io/crossfilter/">http://square.github.io/crossfilter/</a>) allows us to highlight a subset of data in one plot and simultaneously highlight the corresponding data in another plot, even if the dimensions plotted are different. For example, imagine we had a histogram of ages for particular <code class="literal">account_ids</code> in our system, and a scatterplot of click-through-rate versus <code class="literal">account_id</code> for a particular ad campaign. The <code class="literal">Crossfilter</code>  function would allow us to select a subset of the scatterplot points using our cursor and, at the same time, filter the histogram to only those ages that correspond to the points we have selected. This kind of filtering is very useful for drilling down on particular sub-segments of data. Next we will generate the dimensions we will use when plotting:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt;  var ageDim = cf.dimension(function(d) { return d["age"]; });</strong></span>
<span class="strong"><strong>…  var jobDim = cf.dimension(function(d) { return d["job"]; });</strong></span>
<span class="strong"><strong>…  var maritalDim = cf.dimension(function(d) { return d["marital"]; });</strong></span>
</pre></div><p>Each of these functions takes the input data and returns the requested data field. The dimension contains all the data points in a column and forms the superset from which we will filter when examining subsets of data. Using these dimensions, we construct groups of unique values that we can use, for example, in plotting histograms:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt;  var ageDimGroup = ageDim.group();</strong></span>
<span class="strong"><strong>…  var jobDimGroup = jobDim.group();</strong></span>
<span class="strong"><strong>…  var maritalDimGroup = maritalDim.group();</strong></span>
</pre></div><p>For some of our <a id="id590" class="indexterm"/>dimensions, we want to add values representing that maximum or minimum, which we use in plotting ranges of numerical data:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; var minAge = ageDim.bottom(1)[0]["age"];</strong></span>
<span class="strong"><strong>… var maxAge = ageDim.top(1)[0]["age"];</strong></span>
<span class="strong"><strong>… var minBalance = balanceDim.bottom(1)[0]["balance"];</strong></span>
<span class="strong"><strong>… var maxBalance = balanceDim.top(1)[0]["balance"];</strong></span>
</pre></div><p>Finally, we can specify our chart objects using <code class="literal">dc</code> (<a class="ulink" href="https://dc-js.github.io/dc.js/">https://dc-js.github.io/dc.js/</a>), a charting library that uses <code class="literal">d3</code> and crossfilter to create interactive visualizations. The <code class="literal">#</code> tag given to each chart constructor specifies the ID we will use to reference it when we insert it into the HTML template later. We construct the charts using the following code:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt;  var ageChart = dc.barChart("#age-chart"); 	</strong></span>
<span class="strong"><strong>…  var jobChart = dc.rowChart("#job-chart"); 	</strong></span>
<span class="strong"><strong>…  var maritalChart = dc.rowChart("#marital-chart"); 	</strong></span>
<span class="strong"><strong>…</strong></span>
</pre></div><p>Finally, we specify the dimension and axes of these charts:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt;  ageChart</strong></span>
<span class="strong"><strong>…  .width(750)     </strong></span>
<span class="strong"><strong>…  .height(210)     </strong></span>
<span class="strong"><strong>…  .dimension(ageDim)     </strong></span>
<span class="strong"><strong>…  .group(ageDimGroup)     </strong></span>
<span class="strong"><strong>…  .x(d3_scale.scaleLinear()</strong></span>
<span class="strong"><strong>…  .domain([minAge, maxAge]))</strong></span>
<span class="strong"><strong>…  .xAxis().ticks(4);     	</strong></span>

<span class="strong"><strong>&gt;&gt;&gt;  jobChart     </strong></span>
<span class="strong"><strong>…  .width(375)     </strong></span>
<span class="strong"><strong>…  .height(210)     	</strong></span>
<span class="strong"><strong>…  .dimension(jobDim)     </strong></span>
<span class="strong"><strong>…  .group(jobDimGroup)     </strong></span>
<span class="strong"><strong>…  .xAxis().ticks(4);</strong></span>
</pre></div><p>We just need a call to render in order to display the result:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt;  dc.renderAll();</strong></span>
</pre></div><p>Finally, we need to<a id="id591" class="indexterm"/> modify our <code class="literal">index.html</code> file in order to display our charts. If you open this file in a text editor, you will notice several places where we have a <code class="literal">&lt;div&gt;</code> tag such as:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt;  &lt;div class="chart-stage"&gt;</strong></span>
<span class="strong"><strong>…</strong></span>
<span class="strong"><strong>…       &lt;/div&gt;</strong></span>
</pre></div><p>This is where we need to place our charts using the following IDs that we specified in the preceding JavaScript code:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt;  &lt;div id="age-chart"&gt;</strong></span>
<span class="strong"><strong>…         &lt;/div&gt;</strong></span>
</pre></div><p>Finally, in order to render the charts, we need to include our <code class="literal">javascript</code> code in the <code class="literal">&lt;script&gt;</code> arguments at the bottom of the HTML document:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; &lt;script type="text/javascript" … src="../../assets/js/report.js"&gt;&lt;/script&gt;</strong></span>
</pre></div><p>Now, you should be able to navigate to the URL to which the <code class="literal">CherryPy</code> server points, <code class="literal">localhost:5000</code>, should now display the charts like this:</p><div class="mediaobject"><img src="images/B04881_09_07.jpg" alt="The visualization layer"/><div class="caption"><p>Crossfilter chart highlighting other dimensions of subset of users in a given age range.</p></div></div><p>The data is drawn from the bank default example we used to train our model service in <a class="link" href="ch08.html" title="Chapter 8. Sharing Models with Prediction Services">Chapter 8</a>, <span class="emphasis"><em>Sharing Models with Prediction Services</em></span>. You can see that by selecting a subset of data points in the age distribution, we highlight the distribution of occupations, bank balance, and educations for these same users. This kind of visualization is very<a id="id592" class="indexterm"/> useful for drill-down diagnosis of problems points (as may be the case, for example, if a subset of data points is poorly classified by a model). Using these few basic ingredients you can now not only scale model training using the prediction service in <a class="link" href="ch08.html" title="Chapter 8. Sharing Models with Prediction Services">Chapter 8</a>, <span class="emphasis"><em>Sharing Models with Prediction Services</em></span>, but also visualize its behavior for end users using a reporting layer.</p></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec51"/>Summary</h1></div></div></div><p>In this chapter, we have learned several strategies for monitoring the performance of predictive models following initial design and looked at a number of scenarios where the performance or components of the model change over time. As part of the process of refining models, we examined A/B testing strategies and illustrated how to perform basic random allocation and estimate the sample sizes needed to measure improvement. We also demonstrated how to leverage the infrastructure from our prediction service to create dashboard visualizations for monitoring, which can easily be extended for other use cases.</p></div></div>
</body></html>