["```py\ntar xvf confluent-oss-4.0.0-2.11.tar.gz\ncd /opt/confluent-4.0.0/etc/kafka\nvi server.properties\n```", "```py\n$ ./bin/confluent start schema-registry\n```", "```py\nzookeeper is [UP]\n```", "```py\nkafka is [UP]\n```", "```py\nschema-registry is [UP]\nA4774045:confluent-4.0.0 m046277$\n```", "```py\n/opt/confluent-4.0.0\nbin/kafka-topics --list --zookeeper localhost:2181\n_schemas\n```", "```py\nbin/kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 3 --topic my-first-topic\n\nCreated topic \"my-first-topic\"\n```", "```py\nbin/kafka-topics --list --zookeeper localhost:2181\n_schemas\nmy-first-topic\n```", "```py\nbin/kafka-console-producer --broker-list localhost:9092 --topic my-first-topic\ntest1\ntest2\ntest3\n```", "```py\n$ bin/kafka-console-consumer --bootstrap-server localhost:9092 --topic my-first-topic --from-beginning\ntest3\ntest2\ntest1\n```", "```py\ntest4\n```", "```py\ncd /opt/confluent-4.0.0/etc/kafka\nvi connect-file-test-source.properties\nname=local-file-source\nconnector.class=FileStreamSource\ntasks.max=1\nfile=/opt/kafka_2.10-0.10.2.1/source-file.txt\ntopic=my-first-topic\nvi connect-file-test-sink.properties\nname=local-file-sink\nconnector.class=FileStreamSink\ntasks.max=1\nfile=/opt/kafka_2.10-0.10.2.1/target-file.txt\ntopics=my-first-topic\n```", "```py\ncd /opt/confluent-4.0.0\n$ ./bin/connect-standalone config/connect-standalone.properties config/connect-file-test-source.properties config/connect-file-test-sink.properties\n\necho 'test-kafka-connect-1' >> source-file.txt\necho 'test-kafka-connect-2' >> source-file.txt\necho 'test-kafka-connect-3' >> source-file.txt\necho 'test-kafka-connect-4' >> source-file.txt\n```", "```py\n$ ./bin/kafka-console-consumer.sh --zookeeper localhost:2181 --from-beginning --topic my-first-topic\n\ntest3\ntest1\ntest4\n\n{\"schema\":{\"type\":\"string\",\"optional\":false},\"payload\":\"test-kafka-connect-1\"}\n{\"schema\":{\"type\":\"string\",\"optional\":false},\"payload\":\"test-kafka-connect-2\"}\n{\"schema\":{\"type\":\"string\",\"optional\":false},\"payload\":\"test-kafka-connect-3\"}\n{\"schema\":{\"type\":\"string\",\"optional\":false},\"payload\":\"test-kafka-connect-4\"}\n\ntest2\n```", "```py\n$ cat target-file.txt\n\n{\"schema\":{\"type\":\"string\",\"optional\":false},\"payload\":\"test-kafka-connect-1\"}\n{\"schema\":{\"type\":\"string\",\"optional\":false},\"payload\":\"test-kafka-connect-2\"}\n{\"schema\":{\"type\":\"string\",\"optional\":false},\"payload\":\"test-kafka-connect-3\"}\n{\"schema\":{\"type\":\"string\",\"optional\":false},\"payload\":\"test-kafka-connect-4\"}\n```", "```py\n$ sqlite3 firstdb.db\n\nSQLite version 3.16.0 2016-11-04 19:09:39\nEnter \".help\" for usage hints.\n\nsqlite>\nsqlite> CREATE TABLE customer(cust_id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL, cust_name VARCHAR(255));\nsqlite> INSERT INTO customer(cust_id,cust_name) VALUES(1,'Jon');\nsqlite> INSERT INTO customer(cust_id,cust_name) VALUES(2,'Harry');\nsqlite> INSERT INTO customer(cust_id,cust_name) VALUES(3,'James');\nsqlite> select * from customer;\n\n1|Jon\n2|Harry\n3|James\n```", "```py\ncd /opt/confluent-4.0.0\nvi ./etc/kafka-connect-jdbc/source-quickstart-sqlite.properties\nname=test-sqlite-jdbc-autoincrement\nconnector.class=io.confluent.connect.jdbc.JdbcSourceConnector\ntasks.max=1\nconnection.url=jdbc:sqlite:firstdb.db\nmode=incrementing\nincrementing.column.name=cust_id\ntopic.prefix=test-sqlite-jdbc-\n```", "```py\ncd /opt/confluent-4.0.0\nvi etc/kafka/connect-file-sink.properties\nname=local-file-sink\nconnector.class=FileStreamSink\ntasks.max=1\nfile=/opt/confluent-4.0.0/test.sink.txt\ntopics=test-sqlite-jdbc-customer\n```", "```py\n./bin/connect-standalone ./etc/schema-registry/connect-avro-standalone.properties ./etc/kafka-connect-jdbc/source-quickstart-sqlite.properties ./etc/kafka/connect-file-sink.properties\n```", "```py\n$ ./bin/kafka-avro-console-consumer --new-consumer --bootstrap-server localhost:9092 --topic test-sqlite-jdbc-customer --from-beginning\n```", "```py\n{\"cust_id\":1,\"cust_name\":{\"string\":\"Jon\"}}\n{\"cust_id\":2,\"cust_name\":{\"string\":\"Harry\"}}\n{\"cust_id\":3,\"cust_name\":{\"string\":\"James\"}}\n```", "```py\ntail -f /opt/confluent-4.0.0/test.sink.txt\n\nStruct{cust_id=1,cust_name=Jon}\nStruct{cust_id=2,cust_name=Harry}\nStruct{cust_id=3,cust_name=James}\n```", "```py\nsqlite> INSERT INTO customer(cust_id,cust_name) VALUES(4,'Susan');\nsqlite> INSERT INTO customer(cust_id,cust_name) VALUES(5,'Lisa');\n```", "```py\ntail -f /opt/confluent-4.0.0/test.sink.txt\n```", "```py\n$ java -version\n```", "```py\nopenjdk version \"1.8.0_141\"\nOpenJDK Runtime Environment (build 1.8.0_141-b16)\nOpenJDK 64-Bit Server VM (build 25.141-b16, mixed mod\n```", "```py\n$ mkdir /opt/storm\n$ cd storm\n```", "```py\n$ mkdir /usr/local/zookeeper/data\n$ mkdir /usr/local/storm/data\n```", "```py\n$ wget http://apache.osuosl.org/zookeeper/stable/zookeeper-3.4.10.tar.gz\n$ gunzip zookeeper-3.4.10.tar.gz\n$ tar -xvf zookeeper-3.4.10.tar\n$ wget http://mirrors.ibiblio.org/apache/storm/apache-storm-1.0.5/apache-storm-1.0.5.tar.gz\n$ gunzip apache-storm-1.0.5.tar.gz\n$ tar -xvf apache-storm-1.0.5.tar\n```", "```py\n$ cd zookeeper-3.4.10\n$ vi con/zoo.cfg\ntickTime = 2000\ndataDir = /usr/local/zookeeper/data\nclientPort = 2181\n```", "```py\n$ cd /opt/ apache-storm-1.0.5\n$ vi conf/storm.yaml\n```", "```py\nstorm.zookeeper.servers:\n - \"127.0.0.1\"\n nimbus.host: \"127.0.0.1\"\n storm.local.dir: \"/usr/local/storm/data\"\n supervisor.slots.ports:\n - 6700\n - 6701\n - 6702\n - 6703\n```", "```py\n$ cd /opt/zookeeper-3.4.10\n$ bin/zkServer.sh start &amp;amp;\n```", "```py\n$ cd /opt/ apache-storm-1.0.5\n$ bin/storm nimbus &amp;amp;\n```", "```py\n$ bin/storm supervisor &amp;amp;\n```", "```py\nhttp://127.0.0.1:8080\n```", "```py\npackage com.StormMysql;\nimport java.sql.Connection;\nimport java.sql.DriverManager;\npublic class MysqlConnection {\nprivate String server_name;\n private String database_name;\n private String user_name;\n private String password;\n private Connection connection;\n\npublic MysqlConnection(String server_name, String database_name, String user_name, String password)\n {\n this.server_name=server_name;\n this.database_name=database_name;\n this.user_name=user_name;\n this.password=password;\n }\n\npublic Connection getConnection()\n {\n return connection;\n }\n\npublic boolean open()\n {\n boolean successful=true;\n try{\n Class.*forName*(\"com.mysql.jdbc.Driver\");\n connection = DriverManager.*getConnection*(\"jdbc:mysql://\"+server_name+\"/\"+database_name+\"?\"+\"user=\"+user_name+\"&amp;amp;password=\"+password);\n }catch(Exception ex)\n {\n successful=false;\n ex.printStackTrace();\n }\n return successful;\n }\n\npublic boolean close()\n {\n if(connection==null)\n {\n return false;\n }\n\nboolean successful=true;\n try{\n connection.close();\n }catch(Exception ex)\n {\n successful=false;\n ex.printStackTrace();\n }\n\nreturn successful;\n }\n }\n```", "```py\npackage com.StormMysql;\nimport org.apache.storm.tuple.Tuple;\nimport java.sql.PreparedStatement;\npublic class MySqlPrepare {\n private MysqlConnection conn;\n\npublic MySqlPrepare(String server_name, String database_name, String user_name, String password)\n {\n conn = new MysqlConnection(server_name, database_name, user_name, password);\n conn.open();\n }\n\npublic void persist(Tuple tuple)\n {\n PreparedStatement statement=null;\n try{\n statement = conn.getConnection().prepareStatement(\"insert into customer (cust_id,cust_firstname, cust_lastname) values (default, ?,?)\");\n statement.setString(1, tuple.getString(0));\n\nstatement.executeUpdate();\n }catch(Exception ex)\n {\n ex.printStackTrace();\n }finally {\n if(statement != null)\n {\n try{\n statement.close();\n }catch(Exception ex)\n {\n ex.printStackTrace();\n }\n }\n }\n }\n\npublic void close()\n {\n conn.close();\n }\n }\n```", "```py\npackage com.StormMysql;\n\nimport java.util.Map;\n\nimport org.apache.storm.topology.BasicOutputCollector;\n import org.apache.storm.topology.OutputFieldsDeclarer;\n import org.apache.storm.topology.base.BaseBasicBolt;\n import org.apache.storm.tuple.Fields;\n import org.apache.storm.tuple.Tuple;\n import org.apache.storm.tuple.Values;\n import org.apache.storm.task.TopologyContext;\n import java.util.Map;\n\npublic class MySqlBolt extends BaseBasicBolt {\n\nprivate static final long *serialVersionUID* = 1L;\n private MySqlPrepare mySqlPrepare;\n\n@Override\n public void prepare(Map stormConf, TopologyContext context)\n {\n mySqlPrepare=new MySqlPrepare(\"localhost\", \"sales\",\"root\",\"\");\n }\n\npublic void execute(Tuple input, BasicOutputCollector collector) {\n *//* *TODO Auto-generated method stub* mySqlPrepare.persist(input);\n *//System.out.println(input);* }\n@Override\n public void cleanup() {\n mySqlPrepare.close();\n }\n}\n```", "```py\npackage com.StormMysql;\nimport org.apache.storm.Config;\n import org.apache.storm.spout.SchemeAsMultiScheme;\n import org.apache.storm.topology.TopologyBuilder;\n import org.apache.storm.kafka.*;\n import org.apache.storm.LocalCluster;\n import org.apache.storm.generated.AlreadyAliveException;\n import org.apache.storm.generated.InvalidTopologyException;\npublic class KafkaMySQLTopology\n {\n public static void main( String[] args ) throws AlreadyAliveException, InvalidTopologyException\n {\n ZkHosts zkHosts=new ZkHosts(\"localhost:2181\");\nString topic=\"mysql-topic\";\n String consumer_group_id=\"id7\";\nSpoutConfig kafkaConfig=new SpoutConfig(zkHosts, topic, \"\", consumer_group_id);\nkafkaConfig.scheme=new SchemeAsMultiScheme(new StringScheme());\nKafkaSpout kafkaSpout=new KafkaSpout(kafkaConfig);\nTopologyBuilder builder=new TopologyBuilder();\n builder.setSpout(\"KafkaSpout\", kafkaSpout);\n builder.setBolt(\"MySqlBolt\", new MySqlBolt()).globalGrouping(\"KafkaSpout\");\nLocalCluster cluster=new LocalCluster();\nConfig config=new Config();\ncluster.submitTopology(\"KafkaMySQLTopology\", config, builder.createTopology());\ntry{\n Thread.*sleep*(10000);\n }catch(InterruptedException ex)\n {\n ex.printStackTrace();\n }\n// cluster.killTopology(\"KafkaMySQLTopology\");\n // cluster.shutdown();\n}\n }\n```", "```py\npackage com.stormhdfs;\nimport org.apache.storm.Config;\n import org.apache.storm.LocalCluster;\n import org.apache.storm.generated.AlreadyAliveException;\n import org.apache.storm.generated.InvalidTopologyException;\n import org.apache.storm.hdfs.bolt.HdfsBolt;\n import org.apache.storm.hdfs.bolt.format.DefaultFileNameFormat;\n import org.apache.storm.hdfs.bolt.format.DelimitedRecordFormat;\n import org.apache.storm.hdfs.bolt.format.RecordFormat;\n import org.apache.storm.hdfs.bolt.rotation.FileRotationPolicy;\n import org.apache.storm.hdfs.bolt.rotation.FileSizeRotationPolicy;\n import org.apache.storm.hdfs.bolt.sync.CountSyncPolicy;\n import org.apache.storm.hdfs.bolt.sync.SyncPolicy;\n import org.apache.storm.kafka.KafkaSpout;\n import org.apache.storm.kafka.SpoutConfig;\n import org.apache.storm.kafka.StringScheme;\n import org.apache.storm.kafka.ZkHosts;\n import org.apache.storm.spout.SchemeAsMultiScheme;\n import org.apache.storm.topology.TopologyBuilder;\npublic class KafkaTopology {\n public static void main(String[] args) throws AlreadyAliveException, InvalidTopologyException {\n// zookeeper hosts for the Kafka clusterZkHosts zkHosts = new ZkHosts(\"localhost:2181\");\n// Create the KafkaSpout configuartion\n // Second argument is the topic name\n // Third argument is the zookeeper root for Kafka\n // Fourth argument is consumer group id\nSpoutConfig kafkaConfig = new SpoutConfig(zkHosts,\n \"data-pipleline-topic\", \"\", \"id7\");\n// Specify that the kafka messages are String\nkafkaConfig.scheme = new SchemeAsMultiScheme(new StringScheme());\n// We want to consume all the first messages in the topic everytime\n // we run the topology to help in debugging. In production, this\n // property should be false\nkafkaConfig.startOffsetTime = kafka.api.OffsetRequest.*EarliestTime*();\nRecordFormat format = new DelimitedRecordFormat().withFieldDelimiter(\"|\");\n SyncPolicy syncPolicy = new CountSyncPolicy(1000);\nFileRotationPolicy rotationPolicy = new FileSizeRotationPolicy(1.0f,FileSizeRotationPolicy.Units.*MB*);\nDefaultFileNameFormat fileNameFormat = new DefaultFileNameFormat();\n\nfileNameFormat.withPath(\"/user/storm-data\");\n\nfileNameFormat.withPrefix(\"records-\");\n\nfileNameFormat.withExtension(\".txt\");\n\nHdfsBolt bolt =\n new HdfsBolt().withFsUrl(\"hdfs://127.0.0.1:8020\")\n .withFileNameFormat(fileNameFormat)\n .withRecordFormat(format)\n .withRotationPolicy(rotationPolicy)\n .withSyncPolicy(syncPolicy);\n\n// Now we create the topology\nTopologyBuilder builder = new TopologyBuilder();\n\n// set the kafka spout class\nbuilder.setSpout(\"KafkaSpout\", new KafkaSpout(kafkaConfig), 1);\n\n// configure the bolts\n // builder.setBolt(\"SentenceBolt\", new SentenceBolt(), 1).globalGrouping(\"KafkaSpout\");\n // builder.setBolt(\"PrinterBolt\", new PrinterBolt(), 1).globalGrouping(\"SentenceBolt\");\nbuilder.setBolt(\"HDFS-Bolt\", bolt ).globalGrouping(\"KafkaSpout\");\n\n// create an instance of LocalCluster class for executing topology in local mode.\nLocalCluster cluster = new LocalCluster();\n Config conf = new Config();\n\n// Submit topology for execution\ncluster.submitTopology(\"KafkaTopology\", conf, builder.createTopology());\n\ntry {\n // Wait for some time before exiting\nSystem.out.println(\"Waiting to consume from kafka\");\n Thread.sleep(10000);\n } catch (Exception exception) {\n System.out.println(\"Thread interrupted exception : \" + exception);\n }\n\n// kill the KafkaTopology\n //cluster.killTopology(\"KafkaTopology\");\n\n// shut down the storm test cluster\n // cluster.shutdown();\n}\n }\n```", "```py\nCREATE EXTERNAL TABLE IF NOT EXISTS customer (\ncustomer_id INT,\ncustomer_firstname String,\ncustomer_lastname String))\nCOMMENT 'customer table'\nROW FORMAT DELIMITED\nFIELDS TERMINATED BY '|'\nSTORED AS TEXTFILE\nlocation '/user/storm-data';\n$ hive > select * from customer;\n```"]