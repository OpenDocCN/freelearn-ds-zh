<html><head></head><body>
  <div id="sbo-rt-content"><div class="chapter" title="Chapter 3. DataFrames"><div class="titlepage"><div><div><h1 class="title"><a id="ch03"/>Chapter 3. DataFrames</h1></div></div></div><p>A DataFrame is an immutable distributed collection of data that is organized into named columns analogous to a table in a relational database. Introduced as an experimental feature within Apache Spark 1.0 as <code class="literal">SchemaRDD</code>, they were renamed to <code class="literal">DataFrames</code> as part of the Apache Spark 1.3 release. For readers who are familiar with Python Pandas <code class="literal">DataFrame</code> or R <code class="literal">DataFrame</code>, a Spark DataFrame is a similar concept in that it allows users to easily work with structured data (for example, data tables); there are some differences as well so please temper your expectations.</p><p>By imposing a structure onto a distributed collection of data, this allows Spark users to query structured data in Spark SQL or using expression methods (instead of lambdas). In this chapter, we will include code samples using both methods. By structuring your data, this allows the Apache Spark engine – specifically, the Catalyst Optimizer – to significantly improve the performance of Spark queries. In earlier APIs of Spark (that is, RDDs), executing queries in Python could be significantly slower due to communication overhead between the Java JVM and Py4J.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note24"/>Note</h3><p>If you are familiar with working with DataFrames in previous versions of Spark (that is Spark 1.x), you will notice that in Spark 2.0 we are using SparkSession instead of <code class="literal">SQLContext</code>. The various Spark contexts: <code class="literal">HiveContext</code>, <code class="literal">SQLContext</code>, <code class="literal">StreamingContext</code>, and <code class="literal">SparkContext</code> have merged together in SparkSession<a id="id116" class="indexterm"/>. This way you will be working with this session only as an entry point for reading data, working with metadata, configuration, and cluster resource management.</p><p>For more information, please refer to <span class="emphasis"><em>How to use SparkSession in Apache Spark 2.0</em></span>(<a class="ulink" href="http://bit.ly/2br0Fr1">http://bit.ly/2br0Fr1</a>).</p></div></div><p>In this chapter, you will learn about the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Python to RDD communications</li><li class="listitem" style="list-style-type: disc">A quick refresh of Spark's Catalyst Optimizer</li><li class="listitem" style="list-style-type: disc">Speeding up PySpark with DataFrames</li><li class="listitem" style="list-style-type: disc">Creating DataFrames</li><li class="listitem" style="list-style-type: disc">Simple DataFrame queries</li><li class="listitem" style="list-style-type: disc">Interoperating with RDDs</li><li class="listitem" style="list-style-type: disc">Querying with the DataFrame API</li><li class="listitem" style="list-style-type: disc">Querying with Spark SQL</li><li class="listitem" style="list-style-type: disc">Using DataFrames for an on-time flight performance</li></ul></div><div class="section" title="Python to RDD communications"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec17"/>Python to RDD communications</h1></div></div></div><p>Whenever a PySpark program is executed using RDDs, there is a potentially large overhead to<a id="id117" class="indexterm"/> execute the job. As noted in the following diagram, in the<a id="id118" class="indexterm"/> PySpark driver, the <code class="literal">Spark Context</code> uses <code class="literal">Py4j</code> to launch a JVM using the <code class="literal">JavaSparkContext</code>. Any RDD transformations are initially mapped to <code class="literal">PythonRDD</code> objects in Java.</p><p>Once these tasks are pushed out to the Spark Worker(s), <code class="literal">PythonRDD</code> objects launch Python <code class="literal">subprocesses</code> using pipes to send <span class="emphasis"><em>both code and data</em></span> to be processed within Python:</p><div class="mediaobject"><img src="images/B05793_03_01.jpg" alt="Python to RDD communications"/></div><p>While this approach allows PySpark to distribute the processing of the data to multiple Python subprocesses on<a id="id119" class="indexterm"/> multiple workers, as you can see, there is a lot of context switching and communications overhead<a id="id120" class="indexterm"/> between Python and the JVM.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note25"/>Note</h3><p>An excellent<a id="id121" class="indexterm"/> resource on PySpark performance is Holden Karau's <span class="emphasis"><em>Improving PySpark Performance: Spark performance beyond the JVM</em></span>: <a class="ulink" href="http://bit.ly/2bx89bn">http://bit.ly/2bx89bn</a>.</p></div></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Catalyst Optimizer refresh"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec18"/>Catalyst Optimizer refresh</h1></div></div></div><p>As noted in <a class="link" href="ch01.html" title="Chapter 1. Understanding Spark">Chapter 1</a>, <span class="emphasis"><em>Understanding Spark</em></span>, one of the primary reasons the Spark SQL engine is so fast is because of the <span class="strong"><strong>Catalyst Optimizer</strong></span>. For readers with a database background, this diagram<a id="id122" class="indexterm"/> looks similar to the logical/physical planner and cost model/cost-based optimization of a <span class="strong"><strong>relational database management system</strong></span> (<span class="strong"><strong>RDBMS</strong></span>):</p><div class="mediaobject"><img src="images/B05793_03_02.jpg" alt="Catalyst Optimizer refresh"/></div><p>The significance of this is that, as opposed to immediately processing the query, the Spark engine's Catalyst Optimizer compiles and optimizes a logical plan and has a cost optimizer that<a id="id123" class="indexterm"/> determines the most efficient physical plan generated.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note26"/>Note</h3><p>As noted in earlier chapters, while the Spark SQL Engine has both rules-based and cost-based optimizations that include (but are not limited to) predicate push down and<a id="id124" class="indexterm"/> column pruning. Targeted for the Apache Spark 2.2 release, the jira item <span class="emphasis"><em>[SPARK-16026] Cost-based Optimizer Framework</em></span> at <a class="ulink" href="https://issues.apache.org/jira/browse/SPARK-16026">https://issues.apache.org/jira/browse/SPARK-16026</a> is an umbrella ticket to implement a cost-based optimizer framework beyond broadcast join selection. For more information, please refer to the <span class="emphasis"><em>Design Specification of Spark Cost-Based Optimization</em></span> at <a class="ulink" href="http://bit.ly/2li1t4T">http://bit.ly/2li1t4T</a>.</p></div></div><p>As part of <span class="strong"><strong>Project Tungsten</strong></span>, there are further improvements to performance by generating<a id="id125" class="indexterm"/> byte code (code generation or <code class="literal">codegen</code>) instead of interpreting each row of data. Find more details on Tungsten in the <span class="emphasis"><em>Project Tungsten</em></span> section in <a class="link" href="ch01.html" title="Chapter 1. Understanding Spark">Chapter 1</a>, <span class="emphasis"><em>Understanding Spark</em></span>.</p><p>As previously noted, the optimizer is based on functional programming constructs and was designed with two purposes in mind: to ease the adding of new optimization techniques and features<a id="id126" class="indexterm"/> to Spark SQL, and to allow external developers to extend the optimizer (for example, adding data-source-specific rules, support for new data types, and so on).</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note27"/>Note</h3><p>For more information, please refer<a id="id127" class="indexterm"/> to Michael Armbrust's excellent presentation, <span class="emphasis"><em>Structuring Spark: SQL DataFrames, Datasets, and Streaming</em></span> at <a class="ulink" href="http://bit.ly/2cJ508x">http://bit.ly/2cJ508x</a>.</p><p>For further<a id="id128" class="indexterm"/> understanding of the <span class="emphasis"><em>Catalyst Optimizer</em></span>, please refer to <span class="emphasis"><em>Deep Dive into Spark SQL's Catalyst Optimizer</em></span> at <a class="ulink" href="http://bit.ly/2bDVB1T">http://bit.ly/2bDVB1T</a>.</p><p>Also, for more<a id="id129" class="indexterm"/> information on <span class="emphasis"><em>Project Tungsten</em></span>, please refer to <span class="emphasis"><em>Project Tungsten: Bringing Apache Spark Closer to Bare Metal</em></span> at <a class="ulink" href="http://bit.ly/2bQIlKY">http://bit.ly/2bQIlKY</a>, and <span class="emphasis"><em>Apache Spark as a Compiler: Joining a Billion Rows per Second on a Laptop</em></span> at <a class="ulink" href="http://bit.ly/2bDWtnc">http://bit.ly/2bDWtnc</a>.</p></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Speeding up PySpark with DataFrames"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec19"/>Speeding up PySpark with DataFrames</h1></div></div></div><p>The significance of DataFrames and the <span class="emphasis"><em>Catalyst Optimizer</em></span> (and <span class="emphasis"><em>Project Tungsten</em></span>) is the increase in<a id="id130" class="indexterm"/> performance of PySpark queries when<a id="id131" class="indexterm"/> compared to non-optimized RDD queries. As shown in the following figure, prior to the introduction of DataFrames, Python query speeds were often twice as slow as the same Scala queries using RDD. Typically, this slowdown in query performance was due to the communications overhead between Python and the JVM:</p><div class="mediaobject"><img src="images/B05793_03_03.jpg" alt="Speeding up PySpark with DataFrames"/><div class="caption"><p>Source: <span class="emphasis"><em>Introducing DataFrames in Apache-spark for Large Scale Data Science </em></span>at <a class="ulink" href="http://bit.ly/2blDBI1">http://bit.ly/2blDBI1</a>
</p></div></div><p>With DataFrames, not only was there a significant improvement in Python performance, there is now<a id="id132" class="indexterm"/> performance parity between Python, Scala, SQL, and R.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip09"/>Tip</h3><p>It is important to note that while, with DataFrames, PySpark is often significantly faster, there are some exceptions. The most prominent one is the use of Python UDFs, which results in round-trip communication between Python and the JVM. Note, this would be the worst-case scenario which would be similar if the compute was done on RDDs.</p></div></div><p>Python can take advantage of the performance optimizations in Spark even while the codebase for<a id="id133" class="indexterm"/> the Catalyst Optimizer is written in Scala. Basically, it is a Python wrapper of approximately 2,000 lines of code that allows<a id="id134" class="indexterm"/> PySpark DataFrame queries to be significantly faster.</p><p>Altogether, Python DataFrames (as well as SQL, Scala DataFrames, and R DataFrames) are all able to make use of the Catalyst Optimizer (as per the following updated diagram):</p><div class="mediaobject"><img src="images/B05793_03_04.jpg" alt="Speeding up PySpark with DataFrames"/></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note28"/>Note</h3><p>For more information, please refer to the blog post <span class="emphasis"><em>Introducing DataFrames in Apache Spark for Large Scale Data Science </em></span>at <a class="ulink" href="http://bit.ly/2blDBI1">http://bit.ly/2blDBI1</a>, as well as<a id="id135" class="indexterm"/> Reynold Xin's Spark Summit 2015 presentation, <span class="emphasis"><em>From DataFrames to Tungsten: A Peek into Spark's Future </em></span>at <a class="ulink" href="http://bit.ly/2bQN92T">http://bit.ly/2bQN92T</a>.</p></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Creating DataFrames"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec20"/>Creating DataFrames</h1></div></div></div><p>Typically, you will<a id="id136" class="indexterm"/> create DataFrames by importing data using SparkSession (or calling <code class="literal">spark</code> in the PySpark shell).</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip10"/>Tip</h3><p>In Spark 1.x versions, you typically had to use <code class="literal">sqlContext</code>.</p></div></div><p>In future chapters, we will discuss how to import data into your local file system, <span class="strong"><strong>Hadoop Distributed File System</strong></span> (<span class="strong"><strong>HDFS</strong></span>), or other cloud storage systems (for example, S3 or WASB). For this chapter, we will<a id="id137" class="indexterm"/> focus on generating your own DataFrame data directly within Spark or utilizing the data sources already available within Databricks Community Edition.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note29"/>Note</h3><p>For instructions on how to sign up for the Community Edition of Databricks, see the bonus chapter, <span class="emphasis"><em>Free Spark Cloud Offering</em></span>.</p></div></div><p>First, instead of accessing the file system, we will create a DataFrame by generating the data. In this case, we'll first create the <code class="literal">stringJSONRDD</code> RDD and then convert it into a DataFrame. This code snippet<a id="id138" class="indexterm"/> creates an RDD comprised of swimmers (their ID, name, age, and eye color) in JSON format.</p><div class="section" title="Generating our own JSON data"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec33"/>Generating our own JSON data</h2></div></div></div><p>Below, we will generate<a id="id139" class="indexterm"/> initially generate the <code class="literal">stringJSONRDD</code> RDD:</p><div class="informalexample"><pre class="programlisting">stringJSONRDD = sc.parallelize(("""
  { "id": "123",
"name": "Katie",
"age": 19,
"eyeColor": "brown"
  }""",
"""{
"id": "234",
"name": "Michael",
"age": 22,
"eyeColor": "green"
  }""", 
"""{
"id": "345",
"name": "Simone",
"age": 23,
"eyeColor": "blue"
  }""")
)</pre></div><p>Now that we have created the RDD, we will convert this into a DataFrame by using the SparkSession <code class="literal">read.json</code> method (that is, <code class="literal">spark.read.json(...)</code>). We will also create a temporary table by using the <code class="literal">.createOrReplaceTempView</code> method.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note30"/>Note</h3><p>In Spark 1.x, this method was<code class="literal">.registerTempTable</code>, which is being deprecated as part of Spark 2.x.</p></div></div></div><div class="section" title="Creating a DataFrame"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec34"/>Creating a DataFrame</h2></div></div></div><p>Here is the<a id="id140" class="indexterm"/> code to create a DataFrame:</p><div class="informalexample"><pre class="programlisting">swimmersJSON = spark.read.json(stringJSONRDD)</pre></div></div><div class="section" title="Creating a temporary table"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec35"/>Creating a temporary table</h2></div></div></div><p>Here is the code for creating a temporary table:</p><div class="informalexample"><pre class="programlisting">swimmersJSON.createOrReplaceTempView("swimmersJSON")</pre></div><p>As noted in the<a id="id141" class="indexterm"/> previous chapters, many RDD operations are transformations, which are not executed until an action operation is executed. For example, in the preceding code snippet, the <code class="literal">sc.parallelize</code> is a transformation that is executed when converting from an RDD to a DataFrame by using <code class="literal">spark.read.json</code>. Notice that, in the screenshot of this code snippet notebook (near the bottom left), the Spark job is not executed until the second cell containing the <code class="literal">spark.read.json</code> operation.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip11"/>Tip</h3><p>These are screenshots from Databricks Community Edition, but all the code samples and Spark UI screenshots can be executed/viewed in any flavor of Apache Spark 2.x.</p></div></div><p>To further emphasize the point, in the right pane of the following figure, we present the DAG graph of execution.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note31"/>Note</h3><p>A great resource to better understand the Spark UI DAG visualization is the blog post <span class="emphasis"><em>Understanding Your Apache Spark Application Through Visualization</em></span> at <a class="ulink" href="http://bit.ly/2cSemkv">http://bit.ly/2cSemkv</a>.</p></div></div><p>In the following screenshot, you can see the Spark job' s<code class="literal">parallelize</code> operation is from the first cell generating the RDD <code class="literal">stringJSONRDD</code>, while the <code class="literal">map</code> and <code class="literal">mapPartitions</code> operations are the operations required to create the DataFrame:</p><div class="mediaobject"><img src="images/B05793_03_05.jpg" alt="Creating a temporary table"/><div class="caption"><p>Spark UI of the DAG visualization of the spark.read.json(stringJSONRDD) job.</p></div></div><p>In the following screenshot, you can see the <span class="emphasis"><em>stages</em></span> for the <code class="literal">parallelize</code> operation are from the<a id="id142" class="indexterm"/> first cell generating the RDD <code class="literal">stringJSONRDD</code>, while the <code class="literal">map</code> and <code class="literal">mapPartitions</code> operations are the operations required to create the DataFrame:</p><div class="mediaobject"><img src="images/B05793_03_06.jpg" alt="Creating a temporary table"/><div class="caption"><p>Spark UI of the DAG visualization of the stages within the spark.read.json(stringJSONRDD) job.</p></div></div><p>It is important to note that <code class="literal">parallelize</code>, <code class="literal">map</code>, and <code class="literal">mapPartitions</code> are all RDD <span class="emphasis"><em>transformations</em></span>. Wrapped within the DataFrame operation, <code class="literal">spark.read.json</code> (in this case), are not only<a id="id143" class="indexterm"/> the RDD transformations, but also the <span class="emphasis"><em>action</em></span> which converts the RDD into a DataFrame. This is an important call out, because even though you are executing DataFrame<span class="emphasis"><em> operations</em></span>, to debug your operations you will need to remember that you will be making sense of <span class="emphasis"><em>RDD operations</em></span> within the Spark UI.</p><p>Note that creating the temporary table is a DataFrame transformation and not executed until a DataFrame action is executed (for example, in the SQL query to be executed in the following section).</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note32"/>Note</h3><p>DataFrame transformations and actions are similar to RDD transformations and actions in that there<a id="id144" class="indexterm"/> is a set of operations that are lazy (transformations). But, in comparison to RDDs, DataFrames operations are not as lazy, primarily due to the Catalyst Optimizer. For more information, please refer to Holden Karau and Rachel Warren's book <span class="emphasis"><em>High Performance Spark</em></span>, <a class="ulink" href="http://highperformancespark.com/">http://highperformancespark.com/</a>.</p></div></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Simple DataFrame queries"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec21"/>Simple DataFrame queries</h1></div></div></div><p>Now that you<a id="id145" class="indexterm"/> have created the <code class="literal">swimmersJSON</code> DataFrame, we will be able to run the DataFrame API, as well as SQL queries against it. Let's start with a simple query showing all the rows within the DataFrame.</p><div class="section" title="DataFrame API query"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec36"/>DataFrame API query</h2></div></div></div><p>To do this<a id="id146" class="indexterm"/> using the DataFrame API, you can use the <code class="literal">show(&lt;n&gt;)</code> method, which prints the first <code class="literal">n</code> rows to the console:</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip12"/>Tip</h3><p>Running the<code class="literal">.show()</code> method will default to present the first 10 rows.</p></div></div><div class="informalexample"><pre class="programlisting"># DataFrame API
swimmersJSON.show()</pre></div><p>This gives the following output:</p><div class="mediaobject"><img src="images/B05793_03_07.jpg" alt="DataFrame API query"/></div></div><div class="section" title="SQL query"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec37"/>SQL query</h2></div></div></div><p>If you prefer<a id="id147" class="indexterm"/> writing SQL statements, you can write the following query:</p><div class="informalexample"><pre class="programlisting">spark.sql("select * from swimmersJSON").collect()</pre></div><p>This will give the following output:</p><div class="mediaobject"><img src="images/B05793_03_08.jpg" alt="SQL query"/></div><p>We are using the <code class="literal">.collect()</code> method, which returns all the records as a list of <span class="strong"><strong>Row</strong></span> objects. Note that you<a id="id148" class="indexterm"/> can use either the <code class="literal">collect()</code> or <code class="literal">show()</code> method for<a id="id149" class="indexterm"/> both DataFrames and SQL queries. Just make sure that if you use <code class="literal">.collect()</code>, this is for a small DataFrame, since it will return all of the rows in the DataFrame and move them back from the executors to the driver. You can instead use <code class="literal">take(&lt;n&gt;)</code> or <code class="literal">show(&lt;n&gt;)</code>, which allow you to limit the number of rows returned by specifying <code class="literal">&lt;n&gt;</code>:</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip13"/>Tip</h3><p>Note that, if you are using Databricks, you can use the <code class="literal">%sql</code> command and run your SQL statement directly within a notebook cell, as noted.</p></div></div><div class="mediaobject"><img src="images/B05793_03_09.jpg" alt="SQL query"/></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Interoperating with RDDs"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec22"/>Interoperating with RDDs</h1></div></div></div><p>There are two<a id="id150" class="indexterm"/> different methods for converting existing RDDs to DataFrames (or Datasets[T]): inferring the schema using reflection, or programmatically specifying the schema. The former allows you to write more concise code (when your Spark application already knows the schema), while the latter allows you to construct DataFrames when the columns and their data types are only revealed at run time. Note, <span class="strong"><strong>reflection</strong></span> is in reference to <span class="emphasis"><em>schema reflection</em></span> as opposed to Python <code class="literal">reflection</code>.</p><div class="section" title="Inferring the schema using reflection"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec38"/>Inferring the schema using reflection</h2></div></div></div><p>In the process of<a id="id151" class="indexterm"/> building the DataFrame and running the queries, we skipped over the fact that the schema for this DataFrame was automatically defined. Initially, row objects are constructed by passing a list of key/value pairs as <code class="literal">**kwargs</code> to the row class. Then, Spark SQL converts this RDD of row objects into a DataFrame, where the keys are the columns and the data types are inferred by sampling the data.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip14"/>Tip</h3><p>The <code class="literal">**kwargs</code> construct allows you to pass a variable number of parameters to a method at runtime.</p></div></div><p>Going back to the code, after initially creating the <code class="literal">swimmersJSON</code> DataFrame, without specifying the schema, you will notice the schema definition by using the <code class="literal">printSchema()</code> method:</p><div class="informalexample"><pre class="programlisting"># Print the schema
swimmersJSON.printSchema()</pre></div><p>This gives the following output:</p><div class="mediaobject"><img src="images/B05793_03_10.jpg" alt="Inferring the schema using reflection"/></div><p>But what if we<a id="id152" class="indexterm"/> want to specify the schema because, in this example, we know that the <code class="literal">id</code> is actually a <code class="literal">long</code> instead of a <code class="literal">string</code>?</p></div><div class="section" title="Programmatically specifying the schema"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec39"/>Programmatically specifying the schema</h2></div></div></div><p>In this case, let's programmatically<a id="id153" class="indexterm"/> specify the schema by bringing in Spark SQL data types (<code class="literal">pyspark.sql.types</code>) and generate some <code class="literal">.csv</code> data for this example:</p><div class="informalexample"><pre class="programlisting"># Import types
from pyspark.sql.types import *

# Generate comma delimited data
stringCSVRDD = sc.parallelize([
(123, 'Katie', 19, 'brown'), 
(234, 'Michael', 22, 'green'), 
(345, 'Simone', 23, 'blue')
])</pre></div><p>First, we will encode the schema as a string, per the <code class="literal">[schema]</code> variable below. Then we will define the schema using <code class="literal">StructType</code> and <code class="literal">StructField</code>:</p><div class="informalexample"><pre class="programlisting"># Specify schema
schema = StructType([
StructField("id", LongType(), True),    
StructField("name", StringType(), True),
StructField("age", LongType(), True),
StructField("eyeColor", StringType(), True)
])</pre></div><p>Note, the <code class="literal">StructField</code> class is broken down in terms of:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">name</code>: The name of this field</li><li class="listitem" style="list-style-type: disc"><code class="literal">dataType</code>: The data type of this field</li><li class="listitem" style="list-style-type: disc"><code class="literal">nullable</code>: Indicates whether values of this field can be null</li></ul></div><p>Finally, we will apply the schema (<code class="literal">schema</code>) we created to the <code class="literal">stringCSVRDD</code> RDD (that is, the generated<code class="literal">.csv</code> data) and create a temporary view so we can query it using SQL:</p><div class="informalexample"><pre class="programlisting"># Apply the schema to the RDD and Create DataFrame
swimmers = spark.createDataFrame(stringCSVRDD, schema)

# Creates a temporary view using the DataFrame
swimmers.createOrReplaceTempView("swimmers")</pre></div><p>With this example, we have<a id="id154" class="indexterm"/> finer-grain control over the schema and can specify that <code class="literal">id</code> is a <code class="literal">long</code> (as opposed to a string in the previous section):</p><div class="informalexample"><pre class="programlisting">swimmers.printSchema()</pre></div><p>This gives the following output:</p><div class="mediaobject"><img src="images/B05793_03_11.jpg" alt="Programmatically specifying the schema"/></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip15"/>Tip</h3><p>In many cases, the schema can be inferred (as per the previous section) and you do not need to specify the schema, as in this preceding example.</p></div></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Querying with the DataFrame API"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec23"/>Querying with the DataFrame API</h1></div></div></div><p>As noted in<a id="id155" class="indexterm"/> the previous section, you can start off by using <code class="literal">collect()</code>, <code class="literal">show()</code>, or <code class="literal">take()</code> to view the data within your DataFrame (with the last two including the option to limit the number of returned rows).</p><div class="section" title="Number of rows"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec40"/>Number of rows</h2></div></div></div><p>To get the number<a id="id156" class="indexterm"/> of rows within your DataFrame, you can use the <code class="literal">count()</code> method:</p><div class="informalexample"><pre class="programlisting">swimmers.count()</pre></div><p>This gives the following output:</p><div class="informalexample"><pre class="programlisting">Out[13]: 3</pre></div></div><div class="section" title="Running filter statements"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec41"/>Running filter statements</h2></div></div></div><p>To run<a id="id157" class="indexterm"/> a filter statement, you can use the <code class="literal">filter</code> clause; in the following code snippet, we are using the <code class="literal">select</code> clause to specify the columns to be returned as well:</p><div class="informalexample"><pre class="programlisting"># Get the id, age where age = 22
swimmers.select("id", "age").filter("age = 22").show()

# Another way to write the above query is below
swimmers.select(swimmers.id, swimmers.age).filter(swimmers.age == 22).show()</pre></div><p>The output of this query is to choose only the <code class="literal">id</code> and <code class="literal">age</code> columns, where <code class="literal">age</code> = <code class="literal">22</code>:</p><div class="mediaobject"><img src="images/B05793_03_12.jpg" alt="Running filter statements"/></div><p>If we only want to get back the name of the swimmers who have an eye color that begins with the letter <code class="literal">b</code>, we can use a SQL-like syntax, <code class="literal">like</code>, as shown in the following code:</p><div class="informalexample"><pre class="programlisting"># Get the name, eyeColor where eyeColor like 'b%'
swimmers.select("name", "eyeColor").filter("eyeColor like 'b%'").show()</pre></div><p>The output is as follows:</p><div class="mediaobject"><img src="images/B05793_03_13.jpg" alt="Running filter statements"/></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Querying with SQL"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec24"/>Querying with SQL</h1></div></div></div><p>Let's run<a id="id158" class="indexterm"/> the same queries, except this time, we will do so using SQL queries against the same DataFrame. Recall that this DataFrame is accessible because we executed the <code class="literal">.createOrReplaceTempView</code> method for <code class="literal">swimmers</code>.</p><div class="section" title="Number of rows"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec42"/>Number of rows</h2></div></div></div><p>The following<a id="id159" class="indexterm"/> is the code snippet to get the number of rows within your DataFrame using SQL:</p><div class="informalexample"><pre class="programlisting">spark.sql("select count(1) from swimmers").show()</pre></div><p>The output is as follows:</p><div class="mediaobject"><img src="images/B05793_03_14.jpg" alt="Number of rows"/></div></div><div class="section" title="Running filter statements using the where Clauses"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec43"/>Running filter statements using the where Clauses</h2></div></div></div><p>To run a<a id="id160" class="indexterm"/> filter statement using SQL, you can<a id="id161" class="indexterm"/> use the <code class="literal">where</code> clause, as noted in the following code snippet:</p><div class="informalexample"><pre class="programlisting"># Get the id, age where age = 22 in SQL
spark.sql("select id, age from swimmers where age = 22").show()</pre></div><p>The output of this query is to choose only the <code class="literal">id</code> and <code class="literal">age</code> columns where <code class="literal">age</code> = <code class="literal">22</code>:</p><div class="mediaobject"><img src="images/B05793_03_15.jpg" alt="Running filter statements using the where Clauses"/></div><p>As with the DataFrame API querying, if we want to get back the name of the swimmers who have an eye color that begins with the letter <code class="literal">b</code> only, we can use the <code class="literal">like</code> syntax as well:</p><div class="informalexample"><pre class="programlisting">spark.sql(
"select name, eyeColor from swimmers where eyeColor like 'b%'").show()</pre></div><p>The output is as follows:</p><div class="mediaobject"><img src="images/B05793_03_16.jpg" alt="Running filter statements using the where Clauses"/></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip16"/>Tip</h3><p>For more information, please refer to the <span class="emphasis"><em>Spark SQL, DataFrames, and Datasets Guide</em></span> at <a class="ulink" href="http://bit.ly/2cd1wyx">http://bit.ly/2cd1wyx</a>.</p></div></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note33"/>Note</h3><p>An important<a id="id162" class="indexterm"/> note when working<a id="id163" class="indexterm"/> with Spark SQL and DataFrames is that while it is easy to work with CSV, JSON, and a variety of data formats, the most<a id="id164" class="indexterm"/> common storage format for Spark SQL analytics queries is the <span class="emphasis"><em>Parquet</em></span> file format. It is a columnar format that is supported by many other data processing systems and Spark SQL supports both reading and writing Parquet files that automatically preserves the schema of the original data. For more information, please refer to the latest <span class="emphasis"><em>Spark SQL Programming Guide &gt; Parquet Files</em></span> at: <a class="ulink" href="http://spark.apache.org/docs/latest/sql-programming-guide.html#parquet-files">http://spark.apache.org/docs/latest/sql-programming-guide.html#parquet-files</a>. Also, there are many performance optimizations that pertain to Parquet, including (but not limited to) <span class="emphasis"><em>Automatic Partition Discovery and Schema Migration for Parquet</em></span> at <a class="ulink" href="https://databricks.com/blog/2015/03/24/spark-sql-graduates-from-alpha-in-spark-1-3.html">https://databricks.com/blog/2015/03/24/spark-sql-graduates-from-alpha-in-spark-1-3.html</a> and <span class="emphasis"><em>How Apache Spark performs a fast count using the parquet metadata</em></span> at <a class="ulink" href="https://github.com/dennyglee/databricks/blob/master/misc/parquet-count-metadata-explanation.md">https://github.com/dennyglee/databricks/blob/master/misc/parquet-count-metadata-explanation.md</a>.</p></div></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="DataFrame scenario – on-time flight performance"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec25"/>DataFrame scenario – on-time flight performance</h1></div></div></div><p>To showcase the<a id="id165" class="indexterm"/> types of queries you can do with DataFrames, let's look at the use case of on-time flight performance. We will analyze the <span class="emphasis"><em>Airline On-Time Performance and Causes of Flight Delays: On-Time Data</em></span> (<a class="ulink" href="http://bit.ly/2ccJPPM">http://bit.ly/2ccJPPM</a>), and join this with the airports dataset, obtained from the <span class="emphasis"><em>Open Flights Airport, airline, and route data</em></span> (<a class="ulink" href="http://bit.ly/2ccK5hw">http://bit.ly/2ccK5hw</a>), to better understand the variables associated with flight delays.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip17"/>Tip</h3><p>For this section, we will<a id="id166" class="indexterm"/> be using Databricks Community Edition (a free offering of the Databricks product), which you can get at <a class="ulink" href="https://databricks.com/try-databricks">https://databricks.com/try-databricks</a>. We will be using visualizations and pre-loaded datasets within Databricks to make it easier for you to focus on writing the code and analyzing the results.</p><p>If you would<a id="id167" class="indexterm"/> prefer to run this on your own environment, you can find the datasets available in our GitHub repository for this book at <a class="ulink" href="https://github.com/drabastomek/learningPySpark">https://github.com/drabastomek/learningPySpark</a>.</p></div></div><div class="section" title="Preparing the source datasets"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec44"/>Preparing the source datasets</h2></div></div></div><p>We will first<a id="id168" class="indexterm"/> process the source airports and flight performance datasets by specifying their file path location and importing them using SparkSession:</p><div class="informalexample"><pre class="programlisting"># Set File Paths
flightPerfFilePath = 
"/databricks-datasets/flights/departuredelays.csv"
airportsFilePath = 
"/databricks-datasets/flights/airport-codes-na.txt"

# Obtain Airports dataset
airports = spark.read.csv(airportsFilePath, header='true', inferSchema='true', sep='\t')
airports.createOrReplaceTempView("airports")

# Obtain Departure Delays dataset
flightPerf = spark.read.csv(flightPerfFilePath, header='true')
flightPerf.createOrReplaceTempView("FlightPerformance")

# Cache the Departure Delays dataset 
flightPerf.cache()</pre></div><p>Note that we're importing the data using the CSV reader (<code class="literal">com.databricks.spark.csv</code>), which works for any specified delimiter (note that the airports data<a id="id169" class="indexterm"/> is tab-delimited, while the flight performance data is comma-delimited). Finally, we cache the flight dataset so subsequent queries will be faster.</p></div><div class="section" title="Joining flight performance and airports"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec45"/>Joining flight performance and airports</h2></div></div></div><p>One of the<a id="id170" class="indexterm"/> more common<a id="id171" class="indexterm"/> tasks with DataFrames/SQL is to join two different datasets; it is often one of the more demanding operations (from a performance perspective). With DataFrames, a lot of the performance optimizations for these joins are included by default:</p><div class="informalexample"><pre class="programlisting"># Query Sum of Flight Delays by City and Origin Code 
# (for Washington State)
spark.sql("""
select a.City, 
f.origin, 
sum(f.delay) as Delays 
from FlightPerformance f 
join airports a 
on a.IATA = f.origin
where a.State = 'WA'
group by a.City, f.origin
order by sum(f.delay) desc"""
).show()</pre></div><p>In our scenario, we are<a id="id172" class="indexterm"/> querying the total delays by city and origin code for the state of Washington. This will require joining the flight performance data with the airports data by <span class="strong"><strong>International Air Transport Association </strong></span>(<span class="strong"><strong>IATA</strong></span>) code. The output of the query is as follows:</p><div class="mediaobject"><img src="images/B05793_03_17.jpg" alt="Joining flight performance and airports"/></div><p>Using notebooks (such as Databricks, iPython, Jupyter, and Apache Zeppelin), you can more easily execute<a id="id173" class="indexterm"/> and visualize your<a id="id174" class="indexterm"/> queries. In the following examples, we will be using the Databricks notebook. Within our Python notebook, we can use the <code class="literal">%sql</code> function to execute SQL statements within that notebook cell:</p><div class="informalexample"><pre class="programlisting">%sql
-- Query Sum of Flight Delays by City and Origin Code (for Washington State)
select a.City, f.origin, sum(f.delay) as Delays
  from FlightPerformance f
    join airports a
      on a.IATA = f.origin
 where a.State = 'WA'
 group by a.City, f.origin
 order by sum(f.delay) desc</pre></div><p>This is the same as the previous query, but due to formatting, easier to read. In our Databricks notebook example, we can quickly visualize this data into a bar chart:</p><div class="mediaobject"><img src="images/B05793_03_18.jpg" alt="Joining flight performance and airports"/></div></div><div class="section" title="Visualizing our flight-performance data"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec46"/>Visualizing our flight-performance data</h2></div></div></div><p>Let's continue<a id="id175" class="indexterm"/> visualizing our data, but broken down by all states in the continental US:</p><div class="informalexample"><pre class="programlisting">%sql
-- Query Sum of Flight Delays by State (for the US)
select a.State, sum(f.delay) as Delays
  from FlightPerformance f
    join airports a
      on a.IATA = f.origin
 where a.Country = 'USA'
 group by a.State</pre></div><p>The output bar chart is as follows:</p><div class="mediaobject"><img src="images/B05793_03_19.jpg" alt="Visualizing our flight-performance data"/></div><p>But, it would be<a id="id176" class="indexterm"/> cooler to view this data as a map; click on the bar chart icon at the bottom-left of the chart, and you can choose from many different native navigations, including a map:</p><div class="mediaobject"><img src="images/B05793_03_20.jpg" alt="Visualizing our flight-performance data"/></div><p>One of the key benefits of DataFrames is that the information is structured similar to a table. Therefore, whether you are using notebooks or your favorite BI tool, you will be able to quickly<a id="id177" class="indexterm"/> visualize your data.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip18"/>Tip</h3><p>You can<a id="id178" class="indexterm"/> find the full list of <code class="literal">pyspark.sql.DataFrame</code> methods at <a class="ulink" href="http://bit.ly/2bkUGnT">http://bit.ly/2bkUGnT</a>.</p><p>You can<a id="id179" class="indexterm"/> find the full list of <code class="literal">pyspark.sql.functions</code> at <a class="ulink" href="http://bit.ly/2bTAzLT">http://bit.ly/2bTAzLT</a>.</p></div></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Spark Dataset API"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec26"/>Spark Dataset API</h1></div></div></div><p>After this discussion about Spark DataFrames, let's have a quick recap of the Spark Dataset API. Introduced in  Apache Spark 1.6, the goal of Spark Datasets was to provide an API that allows users<a id="id180" class="indexterm"/> to easily express transformations on domain objects, while also providing the performance and benefits of the robust Spark SQL execution engine. As part of the Spark 2.0 release (and as noted in the diagram below), the DataFrame APIs is merged into the Dataset API thus unifying data processing capabilities across all libraries. Because of this unification, developers now have fewer concepts to learn or remember, and work with a single high-level and <span class="emphasis"><em>type-safe</em></span> API – called Dataset:</p><div class="mediaobject"><img src="images/B05793_03_21.jpg" alt="Spark Dataset API"/></div><p>Conceptually, the Spark DataFrame is an <span class="emphasis"><em>alias</em></span> for a collection of generic objects Dataset[Row], where a Row is a generic <span class="emphasis"><em>untyped</em></span> JVM object. Dataset, by contrast, is a collection of <span class="emphasis"><em>strongly-typed</em></span> JVM objects, dictated by a case class you define, in Scala or Java. This last point is particularly important as this means that the Dataset API is <span class="emphasis"><em>not supported</em></span> by PySpark<a id="id181" class="indexterm"/> due to the lack of benefit from the type enhancements. Note, for the parts of the Dataset API that are not available in PySpark, they can<a id="id182" class="indexterm"/> be accessed by converting to an RDD or by using UDFs. For more information, please refer to the jira [SPARK-13233]: Python Dataset at <a class="ulink" href="http://bit.ly/2dbfoFT">http://bit.ly/2dbfoFT</a>.</p></div></div>


  <div id="sbo-rt-content"><div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec27"/>Summary</h1></div></div></div><p>With Spark DataFrames, Python developers can make use of a simpler abstraction layer that is also potentially significantly faster. One of the main reasons Python is initially slower within Spark is due to the communication layer between Python sub-processes and the JVM. For Python DataFrame users, we have a Python wrapper around Scala DataFrames that avoids the Python sub-process/JVM communication overhead. Spark DataFrames has many performance enhancements through the Catalyst Optimizer and Project Tungsten which we have reviewed in this chapter. In this chapter, we also reviewed how to work with Spark DataFrames and worked on an on-time flight performance scenario using DataFrames.</p><p>In this chapter, we created and worked with DataFrames by generating the data or making use of existing datasets.</p><p>In the next chapter, we will discuss how to transform and understand your own data.</p></div></div>
</body></html>