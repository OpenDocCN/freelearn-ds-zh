- en: Chapter 3. Spark SQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most businesses deal with quite a lot of structured data all the time. Even
    if there are too many ways to deal with unstructured data, many application use
    cases still have to have structured data. What is the major difference between
    processing structured data and unstructured data? If the data source is structured,
    and if the data processing engine knows the data structure a priori, the data
    processing engine can do lots of optimizations while processing the data, or even
    beforehand. This is very crucial when the data processing volume is huge and the
    turn around time is very critical.
  prefs: []
  type: TYPE_NORMAL
- en: Proliferation of enterprise data mandated the need to empower the end users
    to query and process the data in simple and easy to use application user interfaces.
    The RDBMS vendors united and the **structured query language** (**SQL**) came
    about as a solution for this. Over the last couple of decades, everyone who deals
    with data became familiar with SQL if not power users.
  prefs: []
  type: TYPE_NORMAL
- en: The large scale Internet applications in the social networking and microblogging
    spaces, to name a few, produced data beyond the consumption of many traditional
    data processing tools. When dealing with such a sea of data, picking and choosing
    the right piece of data from it became even more important. Spark was a highly
    prevalent data processing platform and its RDD-based programming model reduced
    the data processing effort as compared to the Hadoop MapReduce data processing
    framework. But, the initial versions of Spark's RDD-based programming model remained
    elusive on making end users, such as data scientists, data analysts, and business
    analysts from using Spark. The main reason why they could not make use of RDD
    based Spark programming model is because it requires some amount of functional
    programming. The solution to this problem is Spark SQL. Spark SQL is a library
    built on top of Spark. It exposes SQL interface and DataFrame API. DataFrame API
    supports programming languages Scala, Java, Python, and R.
  prefs: []
  type: TYPE_NORMAL
- en: If the structure of the data is known in advance, if the data fits into the
    model of rows and columns, it doesn't matter from where the data is coming and
    Spark SQL can use all of it together and process it as if all the data is coming
    from a single source. Moreover, the querying dialect is the ubiquitous SQL.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Structure of data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark SQL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aggregations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-datasource joins
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data catalog
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the structure of data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The structure of the data that is being discussed here needs some more elucidation.
    What do we mean by the structure of the data? The data stored in RDBMS has a way
    of storing the data in rows/columns or records/fields. Every field has a data
    type and every record is a collection of fields of the same or different data
    types. In the early days of RDBMS, the data types of the fields were scalar and
    in the recent versions, it expanded to include collection data types or composite
    data types as well. So, whether the record contains scalar data types or composite
    data types, the important point to note here is that there is a structure to the
    underlying data. Many of the data processing paradigms have adopted the concept
    of mirroring the underlying data structure persisted in the RDBMS or other stores
    in memory to make the data processing easy.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, if the data in an RDBMS table is being processed by a data processing
    application, if the same table-like data structure is available in memory to the
    programs, for the end users and programmers it is easy to model the applications
    and query the data. For example, suppose there is a set of comma-separated data
    items with a fixed number of values in each row having a specific data type for
    the values coming in the specific position in all the rows. This is a structured
    data file. It is a data table and is very similar to an RDBMS table.
  prefs: []
  type: TYPE_NORMAL
- en: In programming languages such as R, there is a data frame abstraction used to
    store data tables in memory. The Python data analysis library, named Pandas, also
    has a similar data frame concept. Once that data structure is available in memory,
    the programs can extract the data and slice and dice it as per the need. The same
    data table concept is extended to Spark, known as DataFrame, built on top of RDD,
    and there is a very comprehensive API known as DataFrame API in Spark SQL to process
    the data in the DataFrame. A SQL-like query language is also developed on top
    of the DataFrame abstraction, catering to the needs of the end users to query
    and process the underlying structured data. In summary, DataFrame is a distributed
    data table organized in rows and columns and having names for each column.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Spark SQL library built on top of Spark is developed based on the research
    paper titled *"Spark SQL: Relational Data Processing in Spark"*. It talks about
    four goals for Spark SQL and they are reproduced verbatim as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Support relational processing both within Spark programs (on native RDDs) and
    on external data sources using a programmer-friendly API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide high performance using established DBMS techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easily support new data sources, including semi-structured data and external
    databases amenable to query federation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enable extension with advanced analytics algorithms such as graph processing
    and machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DataFrame holds structured data, and it is distributed. It allows selection,
    filtering, and aggregation of data. Sounding very similar to RDD? The key difference
    between RDD and DataFrame is that DataFrame stores much more information about
    the structure of the data, such as the data types and names of the columns, than
    RDD. This allows the DataFrame to optimize the processing much more effectively
    than Spark transformations and Spark actions doing processing on RDD. The other
    most important aspect to mention here is that all the supported programming languages
    of Spark can be used to develop applications using the DataFrame API of Spark
    SQL. For all practical purposes, Spark SQL is a distributed SQL engine.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Those who have worked earlier to Spark 1.3 must be familiar with SchemaRDD,
    and the concept of DataFrame is exactly built on top of SchemaRDD with API-level
    compatibility.
  prefs: []
  type: TYPE_NORMAL
- en: Why Spark SQL?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is no doubt that SQL is the lingua franca for doing data analysis and
    Spark SQL is the answer from the Spark family of toolsets to do data analysis.
    So, what does it provide? It provides the ability to run SQL on top of Spark.
    Whether the data is coming from CSV, Avro, Parquet, Hive, NoSQL data stores such
    as Cassandra, or even RDBMS, Spark SQL can be used to analyze data and mix in
    with Spark programs. Many of the data sources mentioned here are supported intrinsically
    by Spark SQL and many others are supported by external packages. The most important
    aspect to highlight here is the ability of Spark SQL to deal with data from a
    very wide variety of data sources. Once it is available as a DataFrame in Spark,
    Spark SQL can process data in a completely distributed way, combining the DataFrames
    coming from various data sources to process and query as if the entire dataset
    were coming from a single source.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, the RDD was discussed in detail and introduced as the
    Spark programming model. Is the DataFrames API and the usage of SQL dialects in
    Spark SQL replacing RDD-based programming model? Definitely not! The RDD-based
    programming model is the generic and the basic data processing model in Spark.
    RDD-based programming requires the use of real programming techniques. The Spark
    transformations and Spark actions use a lot of functional programming constructs.
    Even though the amount of code that is required to be written in the RDD-based
    programming model is less compared to Hadoop MapReduce or any other paradigm,
    there is still a need to write some amount of functional code. This is a barrier
    for many data scientists, data analysts, and business analysts, who may perform
    major exploratory kinds of data analysis or do some prototyping with the data.
    Spark SQL completely removes those constraints. Simple and easy-to-use **domain
    specific language** (**DSL**) based methods to read and write data from data sources,
    SQL-like language to select, filter, and aggregate, and the capability to read
    data from a wide variety of data sources, make it easy for anybody who knows the
    data structure to use it.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: What is the best use case to use RDD and which is the best use case to use Spark
    SQL? The answer is very simple. If the data is structured, if it can be arranged
    in tables, and if each column can be given a name, then use Spark SQL. This doesn't
    mean that the RDD and DataFrame are two disparate entities. They interoperate
    very well. Conversions from RDD to DataFrame and vice versa are very much possible.
    Many of the Spark transformations and Spark actions that are typically applied
    on RDDs can also be applied on DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, during the application design phase, business analysts generally
    do lots of analysis with the application data using SQL, and that is fed to the
    application requirements and testing artifacts. While designing big data applications,
    the same thing is needed, and in such situations, apart from business analysts,
    data scientists will also be there in the team. In a Hadoop-based ecosystem, Hive
    is used extensively for data analysis with big data. Now Spark SQL brings that
    capability to any platform with support for a whole lot of data sources. If there
    is a standalone Spark installation on commodity hardware, lots of these kinds
    of activities can be done to analyze the data. A basic Spark installation deployed
    in standalone mode on commodity hardware is enough to play around with a whole
    lot of data.
  prefs: []
  type: TYPE_NORMAL
- en: The SQL-on-Hadoop strategy has introduced many applications, such as Hive and
    Impala to name a few, providing a SQL-like interface to the underlying big data
    stored in the **Hadoop Distributed File System** (**HDFS**). Where does Spark
    SQL fit in that space? Before jumping into that, it is a good idea to touch upon
    Hive and Impala. Hive is a MapReduce-based data warehousing technology and, because
    of the use of MapReduce to process queries, Hive queries require lots of I/O operations
    before completing a query. Impala came up with a brilliant solution by doing the
    in-memory processing while making use of the Hive meta store that describes the
    data. Spark SQL uses SQLContext to do all the operations with data. But it can
    also use HiveContext, which is much more feature rich and advanced than SQLContext.
    HiveContext can do all that SQLContext can do and, on top of that, it can read
    from Hive meta store and tables, and can access Hive user-defined functions as
    well. The only requirement to use HiveContext is obviously that there should be
    an already existing Hive setup readily available. In this way, Spark SQL can easily
    co-exist with Hive.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: From Spark 2.0 onwards, SparkSession is the new starting point for Spark SQL-based
    applications, which are a combination of SQLContext and HiveContext while supporting
    backward compatibility with SQLContext and HiveContext.
  prefs: []
  type: TYPE_NORMAL
- en: Spark SQL can process the data from Hive tables faster than Hive using its Hive
    Query Language. Another very interesting feature of Spark SQL is that it can read
    data from different versions of Hive, which is a great feature enabling data source
    consolidation for data processing.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The library that exposes Spark SQL and DataFrame API provides interfaces that
    can be accessed through JDBC/ODBC. This opens up a whole new world of data analysis.
    For example, a **business intelligence** (**BI**) tool that connects to data sources
    using JDBC/ODBC can use a whole lot of data sources supported by Spark SQL. Moreover,
    the BI tools can push down the processor intensive join aggregation operations
    to a huge cluster of worker nodes in the Spark infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Anatomy of Spark SQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Interaction with Spark SQL library is done mainly through two methods. One is
    through SQL-like queries and the other is through DataFrame API. Before getting
    into the details of how DataFrame-based programs work, it is a good idea to see
    how the RDD-based programs work.
  prefs: []
  type: TYPE_NORMAL
- en: The Spark transformations and Spark actions are converted into Java functions
    and they act on top of RDDs, which are nothing but Java objects acting upon data.
    Since RDD is a pure Java object, there is no way, at compile time or at run time,
    to know about what data is going to process. There is no metadata available to
    the execution engine beforehand to optimize the Spark transformations or Spark
    actions. There are no multiple execution paths or query plans available in advance
    to process that data and so, evaluation of the efficacy of various paths of execution
    is not available.
  prefs: []
  type: TYPE_NORMAL
- en: Here, there is no optimized query plan executed because there is no schema associated,
    with data. In the case of DataFrame, the structure is well-known in advance. Because
    of this the queries can be optimized and data cache can be built beforehand.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following *Figure 1* gives an idea about the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Anatomy of Spark SQL](img/image_03_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1
  prefs: []
  type: TYPE_NORMAL
- en: The SQL-like queries and DataFrame API calls made against DataFrame are converted
    to language-neutral expressions. The language-neutral expression corresponding
    to a SQL query or DataFrame API is called an unresolved logical plan.
  prefs: []
  type: TYPE_NORMAL
- en: The unresolved logical plan is converted to a logical plan by doing validations
    on column names from the metadata of the DataFrame. The logical plan is further
    optimized by applying standard rules such as simplification of expressions, evaluations
    of expressions, and other optimization rules, to form an optimized logical plan.
    The optimized logical plan is converted to multiple physical plans. The physical
    plans are created by using Spark-specific operators in the logical plan. The best
    physical plan is chosen and the resultant queries are pushed down to RDDs to act
    on the data. Because the SQL queries and DataFrame API calls are converted to
    language-neutral query expressions, the performance of these queries is consistent
    across all the supported languages. That is the same reason why the DataFrame
    API is supported by all the Spark supported languages such as Scala, Java, Python,
    and R. In the future, there is a good chance that many more languages are going
    to be supporting DataFrame API and Spark SQL because of this reason.
  prefs: []
  type: TYPE_NORMAL
- en: The query planning and optimizations of Spark SQL are worth mentioning here
    too. Any query operation done on a DataFrame through SQL queries or through DataFrame
    API is highly optimized before the corresponding operations are physically applied
    on the underlying base RDD. There are many processes in between before the real
    action happening on the RDD.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 2* gives some idea about the whole query optimization process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Anatomy of Spark SQL](img/image_03_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2
  prefs: []
  type: TYPE_NORMAL
- en: Two types of queries can be called against a DataFrame. They are SQL queries
    or DataFrame API calls. They go through a proper analysis to come up with a logical
    query plan of execution. Then, optimizations are applied on the logical query
    plans to arrive at an optimized logical query plan. From the final optimized logical
    query plan, one or more physical query plans are made. For each of the physical
    query plans, cost models are worked out, and based on the optimal cost, an appropriate
    physical query plan is selected, and highly optimized code is generated and run
    against the RDDs. This is the reason behind the consistent performance of queries
    of any type on DataFrame. This is the same reason why the DataFrame API calls
    from all these different languages, Scala, Java, Python, and R, give consistent
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s revisit the bigger picture once again, as given in *Figure 3*, to set
    the context and see what is being discussed here before getting into and taking
    up the use cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Anatomy of Spark SQL](img/image_03_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3
  prefs: []
  type: TYPE_NORMAL
- en: The use cases that are going to be discussed here will demonstrate the ability
    to mix SQL queries with Spark programs. Multiple data sources will be chosen,
    data will be read from those sources using DataFrame, and uniform data access
    will be demonstrated. The programming languages used to demonstrate are still
    Scala and Python. The usage of R to manipulate DataFrames is on the agenda of
    the book and a whole chapter is dedicated to the same.
  prefs: []
  type: TYPE_NORMAL
- en: DataFrame programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The use cases selected for elucidating the Spark SQL way of programming with
    DataFrame are given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The transaction records come as comma-separated values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Filter out only the good transaction records from the list. The account number
    should start with `SB` and the transaction amount should be greater than zero.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find all the high-value transaction records with a transaction amount greater
    than 1000.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find all the transaction records where the account number is bad.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find all the transaction records where the transaction amount is less than or
    equal to zero.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find a combined list of all the bad transaction records.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find the total of all the transaction amounts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find the maximum of all the transaction amounts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find the minimum of all the transaction amounts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find all the good account numbers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is exactly the same set of use cases that were used in the previous chapter
    as well, but here the programming model is totally different. Using this set of
    use cases, two types of programming models are demonstrated here. One is using
    the SQL queries and the other is using DataFrame APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Programming with SQL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At the Scala REPL prompt, try the following statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The retail banking transaction records come with account number, transaction
    amount and are processed using SparkSQL to get the desired results of the use
    cases. Here is the summary of what the preceding script did:'
  prefs: []
  type: TYPE_NORMAL
- en: A Scala case class is defined to describe the structure of the transaction record
    to be fed into the DataFrame.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An array is defined with the necessary transaction records.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An RDD is made from the array, split the comma-separated values, mapped it to
    create objects using the Scala case class that was defined as the first step in
    the scripts, and the RDD is converted to a DataFrame. This is one use case of
    interoperability between RDD and DataFrame.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A table is registered with the DataFrame with a name. This registered name of
    the table can be used in SQL statements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, all the other activities are just issuing SQL statements using the `spark.sql`
    method. Here the object spark is of type the SparkSession.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The result of all these SQL statements is stored as DataFrames and, just like
    the RDD's `collect` action, DataFrame's show method is used to extract the values
    to the Spark driver program.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The aggregate value calculations are done in two different ways. One is in the
    SQL statement way, which is the easiest way. The other is using the regular RDD-style
    Spark transformations and Spark actions. This is to show that even DataFrame can
    be operated like an RDD, and Spark transformations and Spark actions can be applied
    on top of DataFrame.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At times, it is easy to do some data manipulation activities through the functional
    style operations using functions. So, there is a flexibility here to mix SQL,
    RDD, and DataFrame to have a very convenient programming model to process data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The DataFrame contents are displayed in table format using the `show` method
    of the DataFrame.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A detailed view of the structure of the DataFrame is displayed using the `printSchema`
    method. This is akin to the `describe` command of the database tables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At the Python REPL prompt, try the following statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding Python code snippet, except for a few language-specific constructs
    such as importing libraries and the definition of lambda functions, the style
    of programming is almost the same, most of the time, as the Scala code. This is
    the advantage of Spark's uniform programming model. As discussed earlier, when
    business analysts or data analysts provide the SQL for data access, it is very
    easy to integrate that along with the data processing code in Spark. This uniform
    programming style of coding is very useful for organizations to use the language
    of their choice for developing data processing applications in Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'On DataFrames, if applicable Spark transformations are applied, then a Dataset
    is returned instead of a DataFrame. The concept of Dataset is introduced toward
    the end of this chapter. There is a very strong relationship between DataFrame
    and Dataset, and that is explained in the section covering Datasets. While developing
    applications, caution must be used in this kind of situation. For example, in
    the preceding code snippets, if the following transformation is tried in Scala
    REPL, it will return a dataset: `val amount = goodTransRecords.map(trans => trans.getAs[Double]("tranAmount"))amount:
    org.apache.spark.sql.Dataset[Double] = [value: double]`'
  prefs: []
  type: TYPE_NORMAL
- en: Programming with DataFrame API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, the code snippets will be run in the appropriate language REPLs
    as a continuation of the previous section so that the setup of the data and other
    initializations are not repeated. Like the preceding code snippets, initially,
    some DataFrame-specific basic commands are given. These are used regularly to
    see the contents and for doing some sanity tests on the DataFrame and its contents.
    These are commands that are typically used in the exploratory stage of the data
    analysis, quite often to get more insight into the structure and contents of the
    underlying data.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the Scala REPL prompt, try the following statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the summary of what the preceding script did from a DataFrame API perspective:'
  prefs: []
  type: TYPE_NORMAL
- en: The DataFrame containing the superset of data used in the preceding section
    is used here.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Filtering of the records is demonstrated next. Here, the most important aspect
    to notice is that the filter predicate is to be given exactly like the predicates
    in the SQL statements. Filters can be chained.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The aggregation methods are calculated in one go as three columns in the resultant
    DataFrame.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The final statements in this set are doing the selection, filtering, choosing
    distinct records, and ordering in one single chained statement.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the transaction records are persisted in Parquet format, read from
    the Parquet store and create a DataFrame. More details on the persistence formats
    is coming in the following section.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this code snippet, the Parquet format data is stored in the current directory
    from where the corresponding REPL is invoked. When it is run as a Spark program,
    the directory again will be the current directory from where the Spark submit
    is invoked.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At the Python REPL prompt, try the following statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding Python code snippet, except for a very few variations in the
    aggregation calculations, the programming constructs are almost similar to its
    Scala counterpart.
  prefs: []
  type: TYPE_NORMAL
- en: The last few statements of the preceding Scala and Python sections are about
    the persisting of the DataFrame contents into the media. The writing and reading
    operations are very much required in any kind of data processing operations, but
    most of the tools don't have a uniform way of writing and reading. Spark SQL is
    different. The DataFrame API comes with a rich set of persistence mechanisms.
    It is very easy to write contents of a DataFrame into many supported persistence
    stores. All these writing and reading operations have very simple DSL style interfaces.
    Here are some of the built-in formats in which DataFrames can be written to and
    read from.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apart from these, there are so many other external data sources supported through
    third-party packages:'
  prefs: []
  type: TYPE_NORMAL
- en: JSON
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parquet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MySQL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PostgreSQL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HDFS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Plain Text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon S3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ORC
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JDBC
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The write and read of DataFrame into and from Parquet has been demonstrated
    in the preceding code snippets. All the preceding inherently supported data stores
    have very simple DSL style syntax for persistence and reading back, which makes
    the programming style uniform once again. The DataFrame API reference is a great
    source to know about the details of dealing with each of these data stores.
  prefs: []
  type: TYPE_NORMAL
- en: The sample code in this chapter persists data in Parquet and JSON formats. The
    data store location names chosen are `python.trans.parquet`, `scala.trans.parquet`,
    and so on. This is just to give an indication of which programming language is
    used and which is the format of the data. This is not a proper convention but
    a convenience. When one run of the program is completed, these directories would
    have been created. Next time the same program is run, it will try to create the
    same and will result in an error. The workaround is to remove the directories
    manually, before the subsequent runs, and proceed. Proper error handling mechanisms
    and other nuances of fine programming are going to dilute the focus and hence
    are deliberately left out of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Aggregations in Spark SQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In SQL, aggregation of data is very flexible. The same thing is true in Spark
    SQL too. Instead of running SQL statements on a single data source located in
    a single machine, here Spark SQL can do the same on distributed data sources.
    In the previous chapter, a MapReduce use case was discussed to do data aggregation
    and the same is being used here to demonstrate the aggregation capabilities of
    Spark SQL. In this section also, the use cases are approached in the SQL query
    way as well as in the DataFrame API way.
  prefs: []
  type: TYPE_NORMAL
- en: 'The use cases selected for elucidating the MapReduce kind of data processing
    here are given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The retail banking transaction records come with account number and transaction
    amount in comma-separated strings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find an account level summary of all the transactions to get the account balance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At the Scala REPL prompt, try the following statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In this code snippet, everything is very similar to the preceding section's
    code. The only difference is that, here, aggregations are used in the SQL queries
    as well as in the DataFrame API.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the Python REPL prompt, try the following statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In the DataFrame API for Python, there are some minor syntax differences as
    compared to its Scala counterpart.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding multi-datasource joining with SparkSQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, the joining of multiple RDDs based on the key has been
    discussed. In this section, the same use case is implemented using Spark SQL.
    The use cases selected for elucidating the joining of multiple datasets using
    a key are given here.
  prefs: []
  type: TYPE_NORMAL
- en: The first dataset contains a retail banking master records summary with account
    number, first name, and last name. The second dataset contains the retail banking
    account balance with account number and balance amount. The key on both of the
    datasets is account number. Join the two datasets and create one dataset containing
    account number, first name, last name, and balance amount. From this report, pick
    up the top three accounts in terms of the balance amount.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, the concept of joining data from multiple data sources is also
    demonstrated. First the DataFrames are created from two arrays. They are persisted
    in Parquet and JSON formats. Then they are read from the disk to form the DataFrames,
    and they are joined together.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the Scala REPL prompt, try the following statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Continuing from the same Scala REPL session, the following lines of code get
    the same result through the DataFrame API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The join type selected in the preceding section of the code is inner join. Instead
    of that, any other type of join can be used, either through the SQL query way
    or through the DataFrame API way. In this particular use case, it can be seen
    that the DataFrame API is becoming a bit clunky, while the SQL query looks very
    straightforward. The point here is that depending on the situation, in the application
    code, the SQL query way and the DataFrame API way can be mixed to produce the
    desired result. The DataFrame `acDetailTop3` given in the following scripts is
    an example of that.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the Python REPL prompt, try the following statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding sections, application of the RDD operations on DataFrame has
    been demonstrated. This shows the capability of Spark SQL to interoperate with
    the RDDs and vice versa. In the same way, SQL queries and the DataFrame API can
    be mixed in to have flexibility to use the easiest method of computation when
    solving real-world use cases in the applications.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Spark programming paradigm has many abstractions to choose from when it
    comes to developing data processing applications. The fundamentals of Spark programming
    start with RDDs that can easily deal with unstructured, semi-structured, and structured
    data. The Spark SQL library offers highly optimized performance when processing
    structured data. This makes the basic RDDs look inferior in terms of performance.
    To fill this gap, from Spark 1.6 onwards, a new abstraction, named Dataset, was
    introduced that complements the RDD-based Spark programming model. It works pretty
    much the same way as RDD when it comes to Spark transformations and Spark actions,
    and at the same time, it is highly optimized like the Spark SQL. Dataset API provides
    strong compile-time type safety when it comes to writing programs and, because
    of that, the Dataset API is available only in Scala and Java.
  prefs: []
  type: TYPE_NORMAL
- en: The transaction banking use case discussed in the chapter covering the Spark
    programming model is taken up again here to elucidate the dataset-based programming
    model, because this programming model has a very close resemblance to RDD-based
    programming. The use case mainly deals with a set of banking transaction records
    and various processing done on those records to extract various information from
    it. The use case descriptions are not repeated here and it is not difficult to
    understand by looking at the comments and the code.
  prefs: []
  type: TYPE_NORMAL
- en: The following code snippet demonstrates the methods used to create Dataset,
    along with its usage, conversion of RDD to DataFrame, and conversion of DataFrame
    to dataset. The RDD to DataFrame conversion has already been discussed, but is
    captured here again to to keep the concepts in context. This is mainly to prove
    that various programming models in Spark and the data abstractions are highly
    interoperable.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the Scala REPL prompt, try the following statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: It is very clear that the dataset-based programming has good applicability in
    many of the data processing use cases; at the same time, it has got high interoperability
    with other data processing abstractions within Spark itself.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the preceding code snippet, the DataFrame was converted to Dataset with
    a type specification `acTransRDDToDF.as[Trans]`. This type of conversion is really
    required when reading data from external data sources such as JSON, Avro, or Parquet
    files. That is when strong type checking is needed. Typically, structured data
    is read into DataFrame, and that can be converted to DataSet with strong type
    safety check like this in one shot: `spark.read.json("/transaction.json").as[Trans]`'
  prefs: []
  type: TYPE_NORMAL
- en: If the Scala code snippets throughout this chapter are examined, when some of
    the methods are called on a DataFrame, instead of returning a DataFrame object,
    an object of type `org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]` is
    returned. This is the important relationship between DataFrame and dataset. In
    other words, DataFrame is a dataset of type `org.apache.spark.sql.Row`. If required,
    this object of type `org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]` can
    be explicitly converted to DataFrame using the `toDF()` method.
  prefs: []
  type: TYPE_NORMAL
- en: Too many choices confuse everybody. Here in the Spark programming model, the
    same problem is seen. But it is not as confusing as in many other programming
    paradigms. Whenever there is a need to process any kind of data with very high
    flexibility in terms of the data processing requirements and having the lowest
    API level control such as library development, the RDD-based programming model
    is ideal. Whenever there is a need to process structured data with flexibility
    for accessing and processing data with optimized performance across all the supported
    programming languages, the DataFrame-based Spark SQL programming model is ideal.
  prefs: []
  type: TYPE_NORMAL
- en: Whenever there is a need to process unstructured data with optimized performance
    requirements as well as compile-time type safety but not very complex Spark transformations
    and Spark actions usage requirements, the dataset-based programming model is ideal.
    At a data processing application development level, if the programming language
    of choice permits, it is better to use dataset and DataFrame to have better performance.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Data Catalogs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous sections of this chapter covered programming models with DataFrames
    and datasets. Both of these programming models can deal with structured data.
    The structured data comes with metadata or the data describing the structure of
    the data. Spark SQL provides a minimalist API known as Catalog API for data processing
    applications to query and use the metadata in the applications. The Catalog API
    exposes a catalog abstraction with many databases in it. For the regular SparkSession,
    it will have only one database, namely default. But if Spark is used with Hive,
    then the entire Hive meta store will be available through the Catalog API. The
    following code snippets demonstrate the usage of Catalog API in Scala and Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing from the same Scala REPL prompt, try the following statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, the Catalog API can be used from Python code as well. Since the
    dataset example was not applicable in Python, the table list will be empty. At
    the Python REPL prompt, try the following statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The Catalog API is very handy when writing data processing applications with
    the ability to process data dynamically, based on the contents in the meta store,
    especially when using it in conjunction with Hive.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information you can refer to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://amplab.cs.berkeley.edu/wp-content/uploads/2015/03/SparkSQLSigmod2015.pdf](https://amplab.cs.berkeley.edu/wp-content/uploads/2015/03/SparkSQLSigmod2015.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://pandas.pydata.org/](http://pandas.pydata.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark SQL is a very highly useful library on top of the Spark core infrastructure.
    This library makes the Spark programming more inclusive to a wider group of programmers
    who are well versed with the imperative style of programming but not as competent
    in functional programming. Apart from this, Spark SQL is the best library to process
    structured data in the Spark family of data processing libraries. Spark SQL-based
    data processing application programs can be written with SQL-like queries or DSL
    style imperative programs of DataFrame API. This chapter has also demonstrated
    various strategies of mixing RDD and DataFrames, mixing SQL-like queries and DataFrame
    API. This gives amazing flexibility for the application developers to write data
    processing programs in the way they are most comfortable with, or that is more
    appropriate to the use cases, and at the same time, without compromising performance.
  prefs: []
  type: TYPE_NORMAL
- en: The Dataset API is as the next generation of programming model based on dataset
    in Spark, providing optimized performance and compile-time type safety.
  prefs: []
  type: TYPE_NORMAL
- en: The Catalog API comes as a very handy tool to process data dynamically, based
    on the contents of the meta store.
  prefs: []
  type: TYPE_NORMAL
- en: R is the language of data scientists. Till the support of R as a programming
    language in Spark SQL was available, major distributed data processing was not
    easy for them. Now, using R as a language of choice, they can seamlessly write
    distributed data processing applications as if they are using an R data frame
    from their individual machines. The next chapter is going to discuss the use of
    R to do data processing in Spark SQL.
  prefs: []
  type: TYPE_NORMAL
