- en: Chapter 3. Spark SQL
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章. Spark SQL
- en: Most businesses deal with quite a lot of structured data all the time. Even
    if there are too many ways to deal with unstructured data, many application use
    cases still have to have structured data. What is the major difference between
    processing structured data and unstructured data? If the data source is structured,
    and if the data processing engine knows the data structure a priori, the data
    processing engine can do lots of optimizations while processing the data, or even
    beforehand. This is very crucial when the data processing volume is huge and the
    turn around time is very critical.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数企业始终在处理大量的结构化数据。即使处理非结构化数据的方法有很多，许多应用场景仍然需要结构化数据。处理结构化数据和非结构化数据之间主要的区别是什么？如果数据源是结构化的，并且数据处理引擎事先知道数据结构，数据处理引擎在处理数据时可以进行很多优化，甚至在处理之前。这在数据处理量巨大且周转时间非常关键时非常关键。
- en: Proliferation of enterprise data mandated the need to empower the end users
    to query and process the data in simple and easy to use application user interfaces.
    The RDBMS vendors united and the **structured query language** (**SQL**) came
    about as a solution for this. Over the last couple of decades, everyone who deals
    with data became familiar with SQL if not power users.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 企业数据的激增要求赋予最终用户在简单且易于使用的应用程序用户界面中查询和处理数据的能力。关系数据库管理系统（RDBMS）供应商联合起来，**结构化查询语言**（**SQL**）应运而生，作为解决这一问题的方案。在过去的几十年里，所有处理数据的人如果还不是高级用户，也熟悉了
    SQL。
- en: The large scale Internet applications in the social networking and microblogging
    spaces, to name a few, produced data beyond the consumption of many traditional
    data processing tools. When dealing with such a sea of data, picking and choosing
    the right piece of data from it became even more important. Spark was a highly
    prevalent data processing platform and its RDD-based programming model reduced
    the data processing effort as compared to the Hadoop MapReduce data processing
    framework. But, the initial versions of Spark's RDD-based programming model remained
    elusive on making end users, such as data scientists, data analysts, and business
    analysts from using Spark. The main reason why they could not make use of RDD
    based Spark programming model is because it requires some amount of functional
    programming. The solution to this problem is Spark SQL. Spark SQL is a library
    built on top of Spark. It exposes SQL interface and DataFrame API. DataFrame API
    supports programming languages Scala, Java, Python, and R.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 社交网络和微博等大型互联网应用产生了超出许多传统数据处理工具消费能力的数据。在处理如此庞大的数据海洋时，从其中挑选和选择正确的数据变得更加重要。Spark
    是一个高度流行的数据处理平台，其基于 RDD 的编程模型与 Hadoop MapReduce 数据处理框架相比，降低了数据处理工作量。但是，Spark 基于
    RDD 的编程模型的初始版本在让最终用户，如数据科学家、数据分析师和业务分析师使用 Spark 方面仍然难以捉摸。他们无法利用基于 RDD 的 Spark
    编程模型的主要原因是因为它需要一定程度的函数式编程。解决这个问题的方法是 Spark SQL。Spark SQL 是建立在 Spark 之上的库。它公开了
    SQL 接口和 DataFrame API。DataFrame API 支持编程语言 Scala、Java、Python 和 R。
- en: If the structure of the data is known in advance, if the data fits into the
    model of rows and columns, it doesn't matter from where the data is coming and
    Spark SQL can use all of it together and process it as if all the data is coming
    from a single source. Moreover, the querying dialect is the ubiquitous SQL.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 如果提前知道数据的结构，如果数据符合行和列的模型，那么数据的来源并不重要，Spark SQL 可以将其全部一起使用，并像所有数据都来自单一来源一样进行处理。此外，查询方言是通用的
    SQL。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Structure of data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据结构
- en: Spark SQL
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark SQL
- en: Aggregations
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚合
- en: Multi-datasource joins
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多数据源连接
- en: Dataset
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集
- en: Data catalog
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据目录
- en: Understanding the structure of data
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解数据结构
- en: The structure of the data that is being discussed here needs some more elucidation.
    What do we mean by the structure of the data? The data stored in RDBMS has a way
    of storing the data in rows/columns or records/fields. Every field has a data
    type and every record is a collection of fields of the same or different data
    types. In the early days of RDBMS, the data types of the fields were scalar and
    in the recent versions, it expanded to include collection data types or composite
    data types as well. So, whether the record contains scalar data types or composite
    data types, the important point to note here is that there is a structure to the
    underlying data. Many of the data processing paradigms have adopted the concept
    of mirroring the underlying data structure persisted in the RDBMS or other stores
    in memory to make the data processing easy.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这里所讨论的数据结构需要进一步阐明。我们所说的数据结构是什么意思？存储在关系型数据库管理系统（RDBMS）中的数据以行/列或记录/字段的方式存储。每个字段都有一个数据类型，每个记录都是相同或不同数据类型的字段集合。在RDBMS的早期阶段，字段的数据类型是标量型的，而在最近版本中，它扩展到包括集合数据类型或复合数据类型。因此，无论记录包含标量数据类型还是复合数据类型，这里要强调的重要一点是，底层数据是有结构的。许多数据处理范式都采用了在内存中镜像RDBMS或其他存储中持久化的底层数据结构的概念，以简化数据处理。
- en: In other words, if the data in an RDBMS table is being processed by a data processing
    application, if the same table-like data structure is available in memory to the
    programs, for the end users and programmers it is easy to model the applications
    and query the data. For example, suppose there is a set of comma-separated data
    items with a fixed number of values in each row having a specific data type for
    the values coming in the specific position in all the rows. This is a structured
    data file. It is a data table and is very similar to an RDBMS table.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，如果一个关系型数据库表中的数据正在被数据处理应用程序处理，如果相同的表样数据结构在内存中对程序、最终用户和程序员可用，那么建模应用程序和查询数据对它们来说就很容易了。例如，假设有一组以逗号分隔的数据项，每行有固定数量的值，这些值在所有行中的特定位置具有特定的数据类型。这是一个结构化数据文件。它是一个数据表，非常类似于RDBMS表。
- en: In programming languages such as R, there is a data frame abstraction used to
    store data tables in memory. The Python data analysis library, named Pandas, also
    has a similar data frame concept. Once that data structure is available in memory,
    the programs can extract the data and slice and dice it as per the need. The same
    data table concept is extended to Spark, known as DataFrame, built on top of RDD,
    and there is a very comprehensive API known as DataFrame API in Spark SQL to process
    the data in the DataFrame. A SQL-like query language is also developed on top
    of the DataFrame abstraction, catering to the needs of the end users to query
    and process the underlying structured data. In summary, DataFrame is a distributed
    data table organized in rows and columns and having names for each column.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在R等编程语言中，有一个用于在内存中存储数据表的DataFrame抽象。Python数据分析库Pandas也有类似的数据框概念。一旦这种数据结构在内存中可用，程序就可以提取数据，并根据需要对其进行切片和切块。相同的数据表概念在Spark中得到了扩展，称为DataFrame，它建立在RDD之上，Spark
    SQL中有一个非常全面的API称为DataFrame API，用于处理DataFrame中的数据。在DataFrame抽象之上还开发了一种类似SQL的查询语言，以满足最终用户查询和处理底层结构化数据的需求。总之，DataFrame是一个按行和列组织的数据表，并为每个列命名。
- en: 'The Spark SQL library built on top of Spark is developed based on the research
    paper titled *"Spark SQL: Relational Data Processing in Spark"*. It talks about
    four goals for Spark SQL and they are reproduced verbatim as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '基于Spark构建的Spark SQL库是基于名为 *"Spark SQL: Relational Data Processing in Spark"*
    的研究论文开发的。它讨论了Spark SQL的四个目标，以下为原文照搬：'
- en: Support relational processing both within Spark programs (on native RDDs) and
    on external data sources using a programmer-friendly API
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持在Spark程序内部（在本地RDD上）以及使用程序员友好的API在外部数据源上进行关系处理
- en: Provide high performance using established DBMS techniques
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用成熟的数据库管理系统（DBMS）技术提供高性能
- en: Easily support new data sources, including semi-structured data and external
    databases amenable to query federation
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 便于支持新的数据源，包括半结构化数据和适用于查询联合的外部数据库
- en: Enable extension with advanced analytics algorithms such as graph processing
    and machine learning
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许使用高级分析算法进行扩展，例如图处理和机器学习
- en: DataFrame holds structured data, and it is distributed. It allows selection,
    filtering, and aggregation of data. Sounding very similar to RDD? The key difference
    between RDD and DataFrame is that DataFrame stores much more information about
    the structure of the data, such as the data types and names of the columns, than
    RDD. This allows the DataFrame to optimize the processing much more effectively
    than Spark transformations and Spark actions doing processing on RDD. The other
    most important aspect to mention here is that all the supported programming languages
    of Spark can be used to develop applications using the DataFrame API of Spark
    SQL. For all practical purposes, Spark SQL is a distributed SQL engine.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame 存储结构化数据，并且是分布式的。它允许选择、过滤和聚合数据。听起来很像是 RDD 吗？RDD 和 DataFrame 之间的关键区别在于，DataFrame
    存储了比 RDD 更多的关于数据结构的信息，例如列的数据类型和名称。这使得 DataFrame 能够比 Spark 对 RDD 进行处理的转换和操作更有效地优化处理。在这里需要提到的另一个最重要的方面是，所有
    Spark 支持的编程语言都可以用来开发使用 Spark SQL DataFrame API 的应用程序。从所有实际应用的角度来看，Spark SQL 是一个分布式
    SQL 引擎。
- en: Tip
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: Those who have worked earlier to Spark 1.3 must be familiar with SchemaRDD,
    and the concept of DataFrame is exactly built on top of SchemaRDD with API-level
    compatibility.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 之前在 Spark 1.3 版本中工作过的人一定熟悉 SchemaRDD，DataFrame 的概念正是建立在 SchemaRDD 之上，并且保持了 API
    级别的兼容性。
- en: Why Spark SQL?
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么选择 Spark SQL？
- en: There is no doubt that SQL is the lingua franca for doing data analysis and
    Spark SQL is the answer from the Spark family of toolsets to do data analysis.
    So, what does it provide? It provides the ability to run SQL on top of Spark.
    Whether the data is coming from CSV, Avro, Parquet, Hive, NoSQL data stores such
    as Cassandra, or even RDBMS, Spark SQL can be used to analyze data and mix in
    with Spark programs. Many of the data sources mentioned here are supported intrinsically
    by Spark SQL and many others are supported by external packages. The most important
    aspect to highlight here is the ability of Spark SQL to deal with data from a
    very wide variety of data sources. Once it is available as a DataFrame in Spark,
    Spark SQL can process data in a completely distributed way, combining the DataFrames
    coming from various data sources to process and query as if the entire dataset
    were coming from a single source.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 毫无疑问，SQL 是进行数据分析的通用语言，而 Spark SQL 是 Spark 工具集家族中用于数据分析的解决方案。那么，它提供了什么？它提供了在
    Spark 上运行 SQL 的能力。无论数据来自 CSV、Avro、Parquet、Hive，还是来自 Cassandra 这样的 NoSQL 数据存储，甚至是
    RDBMS，Spark SQL 都可以用来分析数据，并与 Spark 程序混合使用。这里提到的许多数据源都由 Spark SQL 内置支持，而许多其他数据源则由外部包支持。在这里需要强调的最重要的一点是
    Spark SQL 处理来自非常广泛的数据源的能力。一旦数据作为 Spark 中的 DataFrame 可用，Spark SQL 就可以以完全分布式的方式处理数据，将来自各种数据源的数据帧组合起来进行处理和查询，就像整个数据集都来自单一来源一样。
- en: In the previous chapter, the RDD was discussed in detail and introduced as the
    Spark programming model. Is the DataFrames API and the usage of SQL dialects in
    Spark SQL replacing RDD-based programming model? Definitely not! The RDD-based
    programming model is the generic and the basic data processing model in Spark.
    RDD-based programming requires the use of real programming techniques. The Spark
    transformations and Spark actions use a lot of functional programming constructs.
    Even though the amount of code that is required to be written in the RDD-based
    programming model is less compared to Hadoop MapReduce or any other paradigm,
    there is still a need to write some amount of functional code. This is a barrier
    for many data scientists, data analysts, and business analysts, who may perform
    major exploratory kinds of data analysis or do some prototyping with the data.
    Spark SQL completely removes those constraints. Simple and easy-to-use **domain
    specific language** (**DSL**) based methods to read and write data from data sources,
    SQL-like language to select, filter, and aggregate, and the capability to read
    data from a wide variety of data sources, make it easy for anybody who knows the
    data structure to use it.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们已经详细讨论了RDD，并将其介绍为Spark编程模型。Spark SQL的DataFrame API和SQL方言的使用是否正在取代基于RDD的编程模型？当然不是！基于RDD的编程模型是Spark中通用的和基本的数据处理模型。基于RDD的编程需要使用真正的编程技术。Spark的转换和操作使用了大量的函数式编程结构。尽管与Hadoop
    MapReduce或其他任何范式相比，基于RDD的编程模型所需的代码量较少，但仍然需要编写一些函数式代码。这对许多数据科学家、数据分析师和业务分析师来说是一个障碍，他们可能需要进行大量的探索性数据分析或使用数据进行原型设计。Spark
    SQL完全消除了这些限制。基于简单易用的**领域特定语言**（**DSL**）的方法来从数据源读取和写入数据，类似于SQL的语言来选择、过滤和聚合，以及从广泛的数据源读取数据的能力，使得任何了解数据结构的人都能轻松使用它。
- en: Note
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: What is the best use case to use RDD and which is the best use case to use Spark
    SQL? The answer is very simple. If the data is structured, if it can be arranged
    in tables, and if each column can be given a name, then use Spark SQL. This doesn't
    mean that the RDD and DataFrame are two disparate entities. They interoperate
    very well. Conversions from RDD to DataFrame and vice versa are very much possible.
    Many of the Spark transformations and Spark actions that are typically applied
    on RDDs can also be applied on DataFrames.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 使用RDD的最佳用例是什么，使用Spark SQL的最佳用例又是什么？答案非常简单。如果数据是有结构的，如果它可以被安排在表格中，并且如果每一列都可以被赋予一个名称，那么就使用Spark
    SQL。这并不意味着RDD和DataFrame是两个截然不同的实体。它们可以很好地交互。从RDD到DataFrame以及相反的转换都是可能的。许多通常应用于RDD的Spark转换和操作也可以应用于DataFrame。
- en: Typically, during the application design phase, business analysts generally
    do lots of analysis with the application data using SQL, and that is fed to the
    application requirements and testing artifacts. While designing big data applications,
    the same thing is needed, and in such situations, apart from business analysts,
    data scientists will also be there in the team. In a Hadoop-based ecosystem, Hive
    is used extensively for data analysis with big data. Now Spark SQL brings that
    capability to any platform with support for a whole lot of data sources. If there
    is a standalone Spark installation on commodity hardware, lots of these kinds
    of activities can be done to analyze the data. A basic Spark installation deployed
    in standalone mode on commodity hardware is enough to play around with a whole
    lot of data.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在设计应用阶段，业务分析师通常使用SQL对应用数据进行大量分析，并将其用于应用需求和测试工件。在设计大数据应用时，也需要同样的东西，在这种情况下，除了业务分析师外，数据科学家也会在团队中。在基于Hadoop的生态系统中，Hive被广泛用于大数据的数据分析。现在Spark
    SQL将这种能力带到了任何支持大量数据源的平台。如果有一个在通用硬件上的独立Spark安装，就可以进行大量此类活动来分析数据。在通用硬件上以独立模式部署的基本Spark安装就足以处理大量数据。
- en: The SQL-on-Hadoop strategy has introduced many applications, such as Hive and
    Impala to name a few, providing a SQL-like interface to the underlying big data
    stored in the **Hadoop Distributed File System** (**HDFS**). Where does Spark
    SQL fit in that space? Before jumping into that, it is a good idea to touch upon
    Hive and Impala. Hive is a MapReduce-based data warehousing technology and, because
    of the use of MapReduce to process queries, Hive queries require lots of I/O operations
    before completing a query. Impala came up with a brilliant solution by doing the
    in-memory processing while making use of the Hive meta store that describes the
    data. Spark SQL uses SQLContext to do all the operations with data. But it can
    also use HiveContext, which is much more feature rich and advanced than SQLContext.
    HiveContext can do all that SQLContext can do and, on top of that, it can read
    from Hive meta store and tables, and can access Hive user-defined functions as
    well. The only requirement to use HiveContext is obviously that there should be
    an already existing Hive setup readily available. In this way, Spark SQL can easily
    co-exist with Hive.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: SQL-on-Hadoop 策略引入了许多应用程序，例如 Hive 和 Impala 等，为存储在 Hadoop 分布式文件系统（HDFS）中的底层大数据提供了类似
    SQL 的接口。Spark SQL 在这个空间中处于什么位置？在深入探讨这个问题之前，先简要提及 Hive 和 Impala。Hive 是一种基于 MapReduce
    的数据仓库技术，由于查询处理使用了 MapReduce，因此 Hive 查询在完成查询之前需要进行大量的 I/O 操作。Impala 通过在内存中处理数据并利用描述数据的
    Hive 元存储提出了一个绝妙的解决方案。Spark SQL 使用 SQLContext 来执行所有数据操作。但它也可以使用 HiveContext，HiveContext
    比 SQLContext 功能更丰富、更先进。HiveContext 可以执行 SQLContext 可以执行的所有操作，并且在此基础上，它还可以从 Hive
    元存储和表中读取数据，还可以访问 Hive 用户定义的函数。显然，使用 HiveContext 的唯一要求是应该有一个已经存在的 Hive 设置 readily
    available。这样，Spark SQL 可以轻松与 Hive 共存。
- en: Note
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: From Spark 2.0 onwards, SparkSession is the new starting point for Spark SQL-based
    applications, which are a combination of SQLContext and HiveContext while supporting
    backward compatibility with SQLContext and HiveContext.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 从 Spark 2.0 开始，SparkSession 成为基于 Spark SQL 的应用程序的新起点，它是 SQLContext 和 HiveContext
    的组合，同时支持与 SQLContext 和 HiveContext 的向后兼容。
- en: Spark SQL can process the data from Hive tables faster than Hive using its Hive
    Query Language. Another very interesting feature of Spark SQL is that it can read
    data from different versions of Hive, which is a great feature enabling data source
    consolidation for data processing.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL 可以使用其 Hive 查询语言比 Hive 更快地处理 Hive 表中的数据。Spark SQL 的另一个非常有趣的功能是它可以读取不同版本的
    Hive 数据，这是一个非常棒的功能，使得数据源整合在数据处理中成为可能。
- en: Note
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The library that exposes Spark SQL and DataFrame API provides interfaces that
    can be accessed through JDBC/ODBC. This opens up a whole new world of data analysis.
    For example, a **business intelligence** (**BI**) tool that connects to data sources
    using JDBC/ODBC can use a whole lot of data sources supported by Spark SQL. Moreover,
    the BI tools can push down the processor intensive join aggregation operations
    to a huge cluster of worker nodes in the Spark infrastructure.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 提供了 Spark SQL 和 DataFrame API 的库提供了可以通过 JDBC/ODBC 访问的接口。这开启了一个全新的数据分析世界。例如，一个通过
    JDBC/ODBC 连接到数据源的 **商业智能**（BI）工具可以使用 Spark SQL 支持的许多数据源。此外，BI 工具可以将计算密集型的连接聚合操作推送到
    Spark 基础设施中巨大的工作节点集群。
- en: Anatomy of Spark SQL
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark SQL 的结构
- en: Interaction with Spark SQL library is done mainly through two methods. One is
    through SQL-like queries and the other is through DataFrame API. Before getting
    into the details of how DataFrame-based programs work, it is a good idea to see
    how the RDD-based programs work.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Spark SQL 库的交互主要通过两种方法进行。一种是通过类似 SQL 的查询，另一种是通过 DataFrame API。在深入了解基于 DataFrame
    的程序的工作原理之前，先看看基于 RDD 的程序是如何工作的是个好主意。
- en: The Spark transformations and Spark actions are converted into Java functions
    and they act on top of RDDs, which are nothing but Java objects acting upon data.
    Since RDD is a pure Java object, there is no way, at compile time or at run time,
    to know about what data is going to process. There is no metadata available to
    the execution engine beforehand to optimize the Spark transformations or Spark
    actions. There are no multiple execution paths or query plans available in advance
    to process that data and so, evaluation of the efficacy of various paths of execution
    is not available.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Spark转换和Spark操作被转换为Java函数，并在RDD上执行，RDD实际上就是Java对象对数据进行操作。由于RDD是一个纯Java对象，在编译时或运行时都无法知道将要处理什么数据。在执行引擎之前没有可用的元数据来优化Spark转换或Spark操作。没有预先可用的多个执行路径或查询计划来处理这些数据，因此无法评估各种执行路径的有效性。
- en: Here, there is no optimized query plan executed because there is no schema associated,
    with data. In the case of DataFrame, the structure is well-known in advance. Because
    of this the queries can be optimized and data cache can be built beforehand.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，因为没有与数据关联的模式，所以没有执行优化的查询计划。在DataFrame的情况下，结构是预先知道的。正因为如此，查询可以提前优化，数据缓存也可以提前建立。
- en: 'The following *Figure 1* gives an idea about the same:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的*图1*给出了关于同一内容的想法：
- en: '![Anatomy of Spark SQL](img/image_03_002.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![Spark SQL的解剖结构](img/image_03_002.jpg)'
- en: Figure 1
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图1
- en: The SQL-like queries and DataFrame API calls made against DataFrame are converted
    to language-neutral expressions. The language-neutral expression corresponding
    to a SQL query or DataFrame API is called an unresolved logical plan.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对DataFrame进行的类似SQL的查询和DataFrame API调用被转换为语言无关的表达式。对应于SQL查询或DataFrame API的语言无关表达式称为未解析的逻辑计划。
- en: The unresolved logical plan is converted to a logical plan by doing validations
    on column names from the metadata of the DataFrame. The logical plan is further
    optimized by applying standard rules such as simplification of expressions, evaluations
    of expressions, and other optimization rules, to form an optimized logical plan.
    The optimized logical plan is converted to multiple physical plans. The physical
    plans are created by using Spark-specific operators in the logical plan. The best
    physical plan is chosen and the resultant queries are pushed down to RDDs to act
    on the data. Because the SQL queries and DataFrame API calls are converted to
    language-neutral query expressions, the performance of these queries is consistent
    across all the supported languages. That is the same reason why the DataFrame
    API is supported by all the Spark supported languages such as Scala, Java, Python,
    and R. In the future, there is a good chance that many more languages are going
    to be supporting DataFrame API and Spark SQL because of this reason.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对DataFrame元数据中的列名进行验证，将未解析的逻辑计划转换为逻辑计划。通过应用标准规则，如表达式简化、表达式评估和其他优化规则，进一步优化逻辑计划，形成优化的逻辑计划。优化的逻辑计划被转换为多个物理计划。这些物理计划是通过在逻辑计划中使用Spark特定的操作来创建的。选择最佳的物理计划，并将结果查询推送到RDD以对数据进行操作。由于SQL查询和DataFrame
    API调用被转换为语言无关的查询表达式，因此这些查询的性能在所有支持的语言中都是一致的。这也是为什么DataFrame API被所有Spark支持的语言（如Scala、Java、Python和R）支持的原因。在未来，由于这个原因，许多更多的语言可能会支持DataFrame
    API和Spark SQL。
- en: The query planning and optimizations of Spark SQL are worth mentioning here
    too. Any query operation done on a DataFrame through SQL queries or through DataFrame
    API is highly optimized before the corresponding operations are physically applied
    on the underlying base RDD. There are many processes in between before the real
    action happening on the RDD.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里需要提及的是Spark SQL的查询计划和优化。在DataFrame上通过SQL查询或通过DataFrame API进行的任何查询操作，在物理上对底层的base
    RDD应用相应的操作之前，都经过了高度优化。在真正的RDD操作发生之前，有许多过程。
- en: '*Figure 2* gives some idea about the whole query optimization process:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2*给出了整个查询优化过程的一些想法：'
- en: '![Anatomy of Spark SQL](img/image_03_004.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![Spark SQL的解剖结构](img/image_03_004.jpg)'
- en: Figure 2
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图2
- en: Two types of queries can be called against a DataFrame. They are SQL queries
    or DataFrame API calls. They go through a proper analysis to come up with a logical
    query plan of execution. Then, optimizations are applied on the logical query
    plans to arrive at an optimized logical query plan. From the final optimized logical
    query plan, one or more physical query plans are made. For each of the physical
    query plans, cost models are worked out, and based on the optimal cost, an appropriate
    physical query plan is selected, and highly optimized code is generated and run
    against the RDDs. This is the reason behind the consistent performance of queries
    of any type on DataFrame. This is the same reason why the DataFrame API calls
    from all these different languages, Scala, Java, Python, and R, give consistent
    performance.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 可以对 DataFrame 调用两种类型的查询。它们是 SQL 查询或 DataFrame API 调用。它们经过适当的分析，以得出逻辑查询执行计划。然后，在逻辑查询计划上应用优化，以得出优化的逻辑查询计划。从最终的优化逻辑查询计划中，生成一个或多个物理查询计划。对于每个物理查询计划，都会计算出成本模型，并根据最优成本选择合适的物理查询计划，并生成高度优化的代码，针对
    RDD 运行。这就是 DataFrame 上任何类型查询性能一致的原因。这也是为什么来自所有这些不同语言的 DataFrame API 调用（Scala、Java、Python
    和 R）都能提供一致性能的原因。
- en: 'Let''s revisit the bigger picture once again, as given in *Figure 3*, to set
    the context and see what is being discussed here before getting into and taking
    up the use cases:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次回顾一下更大的图景，如*图 3*所示，以设定上下文并在我们进入和讨论用例之前了解这里正在讨论的内容：
- en: '![Anatomy of Spark SQL](img/image_03_006.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![Spark SQL 的解剖结构](img/image_03_006.jpg)'
- en: Figure 3
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3
- en: The use cases that are going to be discussed here will demonstrate the ability
    to mix SQL queries with Spark programs. Multiple data sources will be chosen,
    data will be read from those sources using DataFrame, and uniform data access
    will be demonstrated. The programming languages used to demonstrate are still
    Scala and Python. The usage of R to manipulate DataFrames is on the agenda of
    the book and a whole chapter is dedicated to the same.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这里将要讨论的用例将展示混合 SQL 查询与 Spark 程序的能力。将选择多个数据源，使用 DataFrame 从这些源读取数据，并展示统一的数据访问。演示中使用的编程语言仍然是
    Scala 和 Python。使用 R 操作 DataFrame 的用法将在本书的议程上，并有一个专门的章节介绍。
- en: DataFrame programming
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DataFrame 编程
- en: 'The use cases selected for elucidating the Spark SQL way of programming with
    DataFrame are given as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 用于阐明使用 DataFrame 进行 Spark SQL 编程的用例如下：
- en: The transaction records come as comma-separated values.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交易记录以逗号分隔值的形式出现。
- en: Filter out only the good transaction records from the list. The account number
    should start with `SB` and the transaction amount should be greater than zero.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从列表中过滤出只有有效的交易记录。账户号码应以 `SB` 开头，交易金额应大于零。
- en: Find all the high-value transaction records with a transaction amount greater
    than 1000.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到所有交易金额大于 1000 的高价值交易记录。
- en: Find all the transaction records where the account number is bad.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到所有账户号码无效的交易记录。
- en: Find all the transaction records where the transaction amount is less than or
    equal to zero.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到所有交易金额小于或等于零的交易记录。
- en: Find a combined list of all the bad transaction records.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到所有无效交易记录的合并列表。
- en: Find the total of all the transaction amounts.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到所有交易金额的总和。
- en: Find the maximum of all the transaction amounts.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到所有交易金额的最大值。
- en: Find the minimum of all the transaction amounts.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到所有交易金额的最小值。
- en: Find all the good account numbers.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到所有有效的账户号码。
- en: This is exactly the same set of use cases that were used in the previous chapter
    as well, but here the programming model is totally different. Using this set of
    use cases, two types of programming models are demonstrated here. One is using
    the SQL queries and the other is using DataFrame APIs.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是前一章中使用的相同的一组用例，但在这里编程模型完全不同。使用这组用例，这里展示了两种编程模型。一种是使用 SQL 查询，另一种是使用 DataFrame
    API。
- en: Programming with SQL
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 SQL 进行编程
- en: 'At the Scala REPL prompt, try the following statements:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Scala REPL 提示符下，尝试以下语句：
- en: '[PRE0]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The retail banking transaction records come with account number, transaction
    amount and are processed using SparkSQL to get the desired results of the use
    cases. Here is the summary of what the preceding script did:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 零售银行交易记录包含账户号码、交易金额，并使用 SparkSQL 处理以获得用例所需的预期结果。以下是前面脚本所做操作的摘要：
- en: A Scala case class is defined to describe the structure of the transaction record
    to be fed into the DataFrame.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义了一个Scala案例类来描述要输入到DataFrame中的交易记录的结构。
- en: An array is defined with the necessary transaction records.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用必要的交易记录定义了一个数组。
- en: An RDD is made from the array, split the comma-separated values, mapped it to
    create objects using the Scala case class that was defined as the first step in
    the scripts, and the RDD is converted to a DataFrame. This is one use case of
    interoperability between RDD and DataFrame.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RDD是从数组中生成的，将逗号分隔的值拆分，使用在脚本的第一步中定义的Scala案例类映射创建对象，并将RDD转换为DataFrame。这是RDD和DataFrame之间互操作性的一个用例。
- en: A table is registered with the DataFrame with a name. This registered name of
    the table can be used in SQL statements.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用名称注册了一个表与DataFrame。这个注册的表名可以在SQL语句中使用。
- en: Then, all the other activities are just issuing SQL statements using the `spark.sql`
    method. Here the object spark is of type the SparkSession.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，所有其他活动只是使用`spark.sql`方法发出SQL语句。在这里，spark对象是SparkSession类型。
- en: The result of all these SQL statements is stored as DataFrames and, just like
    the RDD's `collect` action, DataFrame's show method is used to extract the values
    to the Spark driver program.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有这些SQL语句的结果存储为DataFrame，就像RDD的`collect`操作一样，DataFrame的`show`方法用于将值提取到Spark驱动程序中。
- en: The aggregate value calculations are done in two different ways. One is in the
    SQL statement way, which is the easiest way. The other is using the regular RDD-style
    Spark transformations and Spark actions. This is to show that even DataFrame can
    be operated like an RDD, and Spark transformations and Spark actions can be applied
    on top of DataFrame.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚合值计算以两种不同的方式进行。一种是在SQL语句方式中，这是最简单的方式。另一种是使用常规的RDD风格的Spark转换和Spark操作。这表明DataFrame也可以像RDD一样操作，Spark转换和Spark操作可以应用于DataFrame之上。
- en: At times, it is easy to do some data manipulation activities through the functional
    style operations using functions. So, there is a flexibility here to mix SQL,
    RDD, and DataFrame to have a very convenient programming model to process data.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有时，通过函数式风格的运算使用函数进行一些数据处理活动是很简单的。因此，这里有一个灵活性，可以混合SQL、RDD和DataFrame，以获得一个非常方便的编程模型来处理数据。
- en: The DataFrame contents are displayed in table format using the `show` method
    of the DataFrame.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用DataFrame的`show`方法以表格格式显示DataFrame的内容。
- en: A detailed view of the structure of the DataFrame is displayed using the `printSchema`
    method. This is akin to the `describe` command of the database tables.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`printSchema`方法显示了DataFrame的结构视图。这类似于数据库表的`describe`命令。
- en: 'At the Python REPL prompt, try the following statements:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python交互式解释器提示符下，尝试以下语句：
- en: '[PRE1]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In the preceding Python code snippet, except for a few language-specific constructs
    such as importing libraries and the definition of lambda functions, the style
    of programming is almost the same, most of the time, as the Scala code. This is
    the advantage of Spark's uniform programming model. As discussed earlier, when
    business analysts or data analysts provide the SQL for data access, it is very
    easy to integrate that along with the data processing code in Spark. This uniform
    programming style of coding is very useful for organizations to use the language
    of their choice for developing data processing applications in Spark.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的Python代码片段中，除了导入库和lambda函数定义等一些语言特定的结构之外，编程风格几乎与Scala代码相同，大多数时候都是如此。这是Spark统一编程模型的优势。如前所述，当业务分析师或数据分析师提供数据访问的SQL时，很容易将其与Spark中的数据处理代码集成。这种统一的编程风格对组织使用所选语言在Spark中开发数据处理应用程序非常有用。
- en: Tip
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'On DataFrames, if applicable Spark transformations are applied, then a Dataset
    is returned instead of a DataFrame. The concept of Dataset is introduced toward
    the end of this chapter. There is a very strong relationship between DataFrame
    and Dataset, and that is explained in the section covering Datasets. While developing
    applications, caution must be used in this kind of situation. For example, in
    the preceding code snippets, if the following transformation is tried in Scala
    REPL, it will return a dataset: `val amount = goodTransRecords.map(trans => trans.getAs[Double]("tranAmount"))amount:
    org.apache.spark.sql.Dataset[Double] = [value: double]`'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '在 DataFrame 上，如果适用 Spark 转换，则返回 Dataset 而不是 DataFrame。Dataset 的概念在本章末尾介绍。DataFrame
    和 Dataset 之间有非常紧密的联系，这一点在介绍 Dataset 的章节中解释。在开发应用程序时，必须小心处理这种情况。例如，在 Scala REPL
    中尝试前面的代码片段中的以下转换时，它将返回一个数据集：`val amount = goodTransRecords.map(trans => trans.getAs[Double]("tranAmount"))amount:
    org.apache.spark.sql.Dataset[Double] = [value: double]`'
- en: Programming with DataFrame API
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 DataFrame API 编程
- en: In this section, the code snippets will be run in the appropriate language REPLs
    as a continuation of the previous section so that the setup of the data and other
    initializations are not repeated. Like the preceding code snippets, initially,
    some DataFrame-specific basic commands are given. These are used regularly to
    see the contents and for doing some sanity tests on the DataFrame and its contents.
    These are commands that are typically used in the exploratory stage of the data
    analysis, quite often to get more insight into the structure and contents of the
    underlying data.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，代码片段将在适当的语言 REPL 中运行，作为上一节的延续，这样就不需要重复设置数据和其它初始化。与前面的代码片段类似，最初给出一些 DataFrame
    特定的基本命令。这些命令被经常使用，用于查看内容并对 DataFrame 及其内容进行一些基本测试。这些是在数据分析的探索阶段通常使用的命令，常常用于深入了解底层数据的结构和内容。
- en: 'At the Scala REPL prompt, try the following statements:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Scala REPL 提示符下，尝试以下语句：
- en: '[PRE2]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Here is the summary of what the preceding script did from a DataFrame API perspective:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是从 DataFrame API 视角对前面脚本所做操作的总结：
- en: The DataFrame containing the superset of data used in the preceding section
    is used here.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本处使用的是包含前面章节中使用的数据超集的 DataFrame。
- en: Filtering of the records is demonstrated next. Here, the most important aspect
    to notice is that the filter predicate is to be given exactly like the predicates
    in the SQL statements. Filters can be chained.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来演示了记录的过滤。这里，需要注意的最重要的一点是，过滤谓词必须与 SQL 语句中的谓词完全相同。过滤器可以串联使用。
- en: The aggregation methods are calculated in one go as three columns in the resultant
    DataFrame.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚合方法一次性计算为结果 DataFrame 中的三个列。
- en: The final statements in this set are doing the selection, filtering, choosing
    distinct records, and ordering in one single chained statement.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本组中的最后几个语句在一个单链语句中执行选择、过滤、选择不同记录和排序操作。
- en: Finally, the transaction records are persisted in Parquet format, read from
    the Parquet store and create a DataFrame. More details on the persistence formats
    is coming in the following section.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，事务记录以 Parquet 格式持久化，从 Parquet 存储中读取并创建一个 DataFrame。关于持久化格式的更多细节将在下一节中介绍。
- en: In this code snippet, the Parquet format data is stored in the current directory
    from where the corresponding REPL is invoked. When it is run as a Spark program,
    the directory again will be the current directory from where the Spark submit
    is invoked.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在此代码片段中，Parquet 格式的数据存储在从相应 REPL 调用的当前目录中。当它作为一个 Spark 程序运行时，目录再次将是从该目录调用 Spark
    submit 的当前目录。
- en: 'At the Python REPL prompt, try the following statements:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python REPL 提示符下，尝试以下语句：
- en: '[PRE3]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In the preceding Python code snippet, except for a very few variations in the
    aggregation calculations, the programming constructs are almost similar to its
    Scala counterpart.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的 Python 代码片段中，除了聚合计算中的一些细微差异外，编程结构几乎与 Scala 对应版本相似。
- en: The last few statements of the preceding Scala and Python sections are about
    the persisting of the DataFrame contents into the media. The writing and reading
    operations are very much required in any kind of data processing operations, but
    most of the tools don't have a uniform way of writing and reading. Spark SQL is
    different. The DataFrame API comes with a rich set of persistence mechanisms.
    It is very easy to write contents of a DataFrame into many supported persistence
    stores. All these writing and reading operations have very simple DSL style interfaces.
    Here are some of the built-in formats in which DataFrames can be written to and
    read from.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的 Scala 和 Python 部分的最后几行是关于将 DataFrame 内容持久化到媒体中的。在任何类型的数据处理操作中，写入和读取操作都是非常必要的，但大多数工具都没有统一的写入和读取方式。Spark
    SQL 是不同的。DataFrame API 提供了一套丰富的持久化机制。将 DataFrame 的内容写入许多支持的持久化存储非常简单。所有这些写入和读取操作都有非常简单的
    DSL 风格接口。以下是一些 DataFrame 可以写入和读取的内置格式。
- en: 'Apart from these, there are so many other external data sources supported through
    third-party packages:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些，还有许多其他通过第三方包支持的外部数据源：
- en: JSON
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JSON
- en: Parquet
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parquet
- en: Hive
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hive
- en: MySQL
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MySQL
- en: PostgreSQL
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PostgreSQL
- en: HDFS
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HDFS
- en: Plain Text
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 纯文本
- en: Amazon S3
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amazon S3
- en: ORC
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ORC
- en: JDBC
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JDBC
- en: The write and read of DataFrame into and from Parquet has been demonstrated
    in the preceding code snippets. All the preceding inherently supported data stores
    have very simple DSL style syntax for persistence and reading back, which makes
    the programming style uniform once again. The DataFrame API reference is a great
    source to know about the details of dealing with each of these data stores.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中已经演示了 DataFrame 到 Parquet 以及从 Parquet 读写的过程。所有之前内在支持的数据存储都有非常简单的 DSL
    风格语法用于持久化和读取，这使得编程风格再次统一。DataFrame API 参考是了解如何处理每个数据存储细节的绝佳来源。
- en: The sample code in this chapter persists data in Parquet and JSON formats. The
    data store location names chosen are `python.trans.parquet`, `scala.trans.parquet`,
    and so on. This is just to give an indication of which programming language is
    used and which is the format of the data. This is not a proper convention but
    a convenience. When one run of the program is completed, these directories would
    have been created. Next time the same program is run, it will try to create the
    same and will result in an error. The workaround is to remove the directories
    manually, before the subsequent runs, and proceed. Proper error handling mechanisms
    and other nuances of fine programming are going to dilute the focus and hence
    are deliberately left out of this book.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的示例代码以 Parquet 和 JSON 格式持久化数据。所选的数据存储位置名称为 `python.trans.parquet`、`scala.trans.parquet`
    等。这只是为了表明使用了哪种编程语言以及数据的格式。这并不是一个正确的约定，而是一种便利。当程序的一次运行完成后，这些目录就会被创建。下次运行相同的程序时，它将尝试创建相同的目录，并导致错误。解决方案是在后续运行之前手动删除这些目录，然后继续。适当的错误处理机制和精细编程的其他细微之处可能会分散注意力，因此故意从本书中省略。
- en: Understanding Aggregations in Spark SQL
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 Spark SQL 中的聚合
- en: In SQL, aggregation of data is very flexible. The same thing is true in Spark
    SQL too. Instead of running SQL statements on a single data source located in
    a single machine, here Spark SQL can do the same on distributed data sources.
    In the previous chapter, a MapReduce use case was discussed to do data aggregation
    and the same is being used here to demonstrate the aggregation capabilities of
    Spark SQL. In this section also, the use cases are approached in the SQL query
    way as well as in the DataFrame API way.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在 SQL 中，数据的聚合非常灵活。在 Spark SQL 中也是如此。在这里，Spark SQL 可以在分布式数据源上执行与在单个机器上的单个数据源上运行
    SQL 语句相同的事情。在前一章中，讨论了一个 MapReduce 用例来进行数据聚合，这里同样使用它来展示 Spark SQL 的聚合能力。在本节中，用例也是以
    SQL 查询方式和 DataFrame API 方式来处理的。
- en: 'The use cases selected for elucidating the MapReduce kind of data processing
    here are given as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在此处阐述 MapReduce 类型的数据处理所选择的用例如下：
- en: The retail banking transaction records come with account number and transaction
    amount in comma-separated strings
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 零售银行交易记录包含账户号码和以逗号分隔的交易金额字符串
- en: Find an account level summary of all the transactions to get the account balance
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到所有交易的账户级别摘要以获取账户余额
- en: 'At the Scala REPL prompt, try the following statements:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Scala REPL 提示符下，尝试以下语句：
- en: '[PRE4]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In this code snippet, everything is very similar to the preceding section's
    code. The only difference is that, here, aggregations are used in the SQL queries
    as well as in the DataFrame API.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码片段中，一切与前面章节的代码非常相似。唯一的区别是，在这里，SQL 查询以及 DataFrame API 中都使用了聚合操作。
- en: 'At the Python REPL prompt, try the following statements:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python REPL 提示符下，尝试以下语句：
- en: '[PRE5]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In the DataFrame API for Python, there are some minor syntax differences as
    compared to its Scala counterpart.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 的 DataFrame API 中，与 Scala 的对应版本相比，有一些微小的语法差异。
- en: Understanding multi-datasource joining with SparkSQL
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 SparkSQL 中的多数据源连接
- en: In the previous chapter, the joining of multiple RDDs based on the key has been
    discussed. In this section, the same use case is implemented using Spark SQL.
    The use cases selected for elucidating the joining of multiple datasets using
    a key are given here.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，已经讨论了基于键的多个 RDD 的连接。在本节中，使用 Spark SQL 实现了相同的用例。这里给出的用于阐明使用键连接多个数据集的用例已选定。
- en: The first dataset contains a retail banking master records summary with account
    number, first name, and last name. The second dataset contains the retail banking
    account balance with account number and balance amount. The key on both of the
    datasets is account number. Join the two datasets and create one dataset containing
    account number, first name, last name, and balance amount. From this report, pick
    up the top three accounts in terms of the balance amount.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个数据集包含一个零售银行主记录摘要，包括账户号码、名和姓。第二个数据集包含零售银行账户余额，包括账户号码和余额金额。这两个数据集的关键是账户号码。将这两个数据集连接起来，创建一个包含账户号码、名、姓和余额金额的数据集。从这个报告中，挑选出余额金额最高的前三个账户。
- en: In this section, the concept of joining data from multiple data sources is also
    demonstrated. First the DataFrames are created from two arrays. They are persisted
    in Parquet and JSON formats. Then they are read from the disk to form the DataFrames,
    and they are joined together.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，还演示了从多个数据源连接数据的概念。首先，从两个数组创建 DataFrame。它们以 Parquet 和 JSON 格式持久化。然后，从磁盘读取它们以形成
    DataFrame，并将它们连接起来。
- en: 'At the Scala REPL prompt, try the following statements:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Scala REPL 提示符下，尝试以下语句：
- en: '[PRE6]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Continuing from the same Scala REPL session, the following lines of code get
    the same result through the DataFrame API:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 继续使用相同的 Scala REPL 会话，以下代码行通过 DataFrame API 获取相同的结果：
- en: '[PRE7]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The join type selected in the preceding section of the code is inner join. Instead
    of that, any other type of join can be used, either through the SQL query way
    or through the DataFrame API way. In this particular use case, it can be seen
    that the DataFrame API is becoming a bit clunky, while the SQL query looks very
    straightforward. The point here is that depending on the situation, in the application
    code, the SQL query way and the DataFrame API way can be mixed to produce the
    desired result. The DataFrame `acDetailTop3` given in the following scripts is
    an example of that.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码的前一部分中选定的连接类型是内连接。而不是这样，可以通过 SQL 查询方式或 DataFrame API 方式使用任何其他类型的连接。在这个特定用例中，可以看到
    DataFrame API 变得有点笨拙，而 SQL 查询看起来非常直接。这里的要点是，根据具体情况，在应用程序代码中，可以混合使用 SQL 查询方式和 DataFrame
    API 方式来产生所需的结果。以下脚本中给出的 DataFrame `acDetailTop3` 是一个例子。
- en: 'At the Python REPL prompt, try the following statements:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python REPL 提示符下，尝试以下语句：
- en: '[PRE8]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In the preceding sections, application of the RDD operations on DataFrame has
    been demonstrated. This shows the capability of Spark SQL to interoperate with
    the RDDs and vice versa. In the same way, SQL queries and the DataFrame API can
    be mixed in to have flexibility to use the easiest method of computation when
    solving real-world use cases in the applications.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，已经展示了在 DataFrame 上应用 RDD 操作。这显示了 Spark SQL 与 RDDs 交互以及反之亦然的能力。同样，SQL
    查询和 DataFrame API 可以混合使用，以便在解决应用程序中的实际用例时，能够灵活地使用计算的最简单方法。
- en: Introducing datasets
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍数据集
- en: The Spark programming paradigm has many abstractions to choose from when it
    comes to developing data processing applications. The fundamentals of Spark programming
    start with RDDs that can easily deal with unstructured, semi-structured, and structured
    data. The Spark SQL library offers highly optimized performance when processing
    structured data. This makes the basic RDDs look inferior in terms of performance.
    To fill this gap, from Spark 1.6 onwards, a new abstraction, named Dataset, was
    introduced that complements the RDD-based Spark programming model. It works pretty
    much the same way as RDD when it comes to Spark transformations and Spark actions,
    and at the same time, it is highly optimized like the Spark SQL. Dataset API provides
    strong compile-time type safety when it comes to writing programs and, because
    of that, the Dataset API is available only in Scala and Java.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到开发数据处理应用程序时，Spark编程范式提供了许多抽象供选择。Spark编程的基础始于可以轻松处理非结构化、半结构化和结构化数据的RDDs。Spark
    SQL库在处理结构化数据时提供了高度优化的性能。这使得基本的RDDs在性能方面看起来有些不足。为了填补这一差距，从Spark 1.6版本开始，引入了一种新的抽象，名为Dataset，它补充了基于RDD的Spark编程模型。它在Spark转换和Spark操作方面几乎与RDD相同，同时，它像Spark
    SQL一样高度优化。Dataset API在编写程序时提供了强大的编译时类型安全性，因此，Dataset API仅在Scala和Java中可用。
- en: The transaction banking use case discussed in the chapter covering the Spark
    programming model is taken up again here to elucidate the dataset-based programming
    model, because this programming model has a very close resemblance to RDD-based
    programming. The use case mainly deals with a set of banking transaction records
    and various processing done on those records to extract various information from
    it. The use case descriptions are not repeated here and it is not difficult to
    understand by looking at the comments and the code.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在涵盖Spark编程模型的章节中讨论的交易银行业务案例在此再次提出，以阐明基于dataset的编程模型，因为这种编程模型与基于RDD的编程非常相似。该案例主要涉及一组银行交易记录以及在这些记录上执行的各种处理，以从中提取各种信息。案例描述在此不再重复，通过查看注释和代码不难理解。
- en: The following code snippet demonstrates the methods used to create Dataset,
    along with its usage, conversion of RDD to DataFrame, and conversion of DataFrame
    to dataset. The RDD to DataFrame conversion has already been discussed, but is
    captured here again to to keep the concepts in context. This is mainly to prove
    that various programming models in Spark and the data abstractions are highly
    interoperable.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段演示了创建Dataset所使用的方法，以及它的使用、将RDD转换为DataFrame以及将DataFrame转换为dataset的过程。RDD到DataFrame的转换已经讨论过，但在此再次捕获以保持概念的一致性。这主要是为了证明Spark中的各种编程模型和数据抽象具有高度的互操作性。
- en: 'At the Scala REPL prompt, try the following statements:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scala REPL提示符下，尝试以下语句：
- en: '[PRE9]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: It is very clear that the dataset-based programming has good applicability in
    many of the data processing use cases; at the same time, it has got high interoperability
    with other data processing abstractions within Spark itself.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，基于dataset的编程在许多数据处理用例中具有良好的适用性；同时，它与其他Spark内部的数据处理抽象具有高度的互操作性。
- en: Tip
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'In the preceding code snippet, the DataFrame was converted to Dataset with
    a type specification `acTransRDDToDF.as[Trans]`. This type of conversion is really
    required when reading data from external data sources such as JSON, Avro, or Parquet
    files. That is when strong type checking is needed. Typically, structured data
    is read into DataFrame, and that can be converted to DataSet with strong type
    safety check like this in one shot: `spark.read.json("/transaction.json").as[Trans]`'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，DataFrame通过类型指定`acTransRDDToDF.as[Trans]`转换为Dataset。这种类型的转换在从外部数据源（如JSON、Avro或Parquet文件）读取数据时确实是必需的。那时就需要强类型检查。通常，结构化数据被读取到DataFrame中，然后可以通过以下方式一次性转换为具有强类型安全检查的DataSet：`spark.read.json("/transaction.json").as[Trans]`
- en: If the Scala code snippets throughout this chapter are examined, when some of
    the methods are called on a DataFrame, instead of returning a DataFrame object,
    an object of type `org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]` is
    returned. This is the important relationship between DataFrame and dataset. In
    other words, DataFrame is a dataset of type `org.apache.spark.sql.Row`. If required,
    this object of type `org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]` can
    be explicitly converted to DataFrame using the `toDF()` method.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如果检查本章中的 Scala 代码片段，当在 DataFrame 上调用某些方法时，返回的不是 DataFrame 对象，而是一个 `org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]`
    类型的对象。这是 DataFrame 和 dataset 之间的重要关系。换句话说，DataFrame 是一个 `org.apache.spark.sql.Row`
    类型的 dataset。如果需要，可以使用 `toDF()` 方法显式地将此 `org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]`
    类型的对象转换为 DataFrame。
- en: Too many choices confuse everybody. Here in the Spark programming model, the
    same problem is seen. But it is not as confusing as in many other programming
    paradigms. Whenever there is a need to process any kind of data with very high
    flexibility in terms of the data processing requirements and having the lowest
    API level control such as library development, the RDD-based programming model
    is ideal. Whenever there is a need to process structured data with flexibility
    for accessing and processing data with optimized performance across all the supported
    programming languages, the DataFrame-based Spark SQL programming model is ideal.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 选项过多会让人感到困惑。在这里的 Spark 编程模型中，也存在同样的问题。但相较于许多其他编程范式，它并不那么令人困惑。每当需要以非常高的灵活性处理各种数据，并且需要最低级别的
    API 控制如库开发时，基于 RDD 的编程模型是理想的。每当需要以灵活的方式处理结构化数据，并且在整个支持的编程语言中具有优化性能时，基于 DataFrame
    的 Spark SQL 编程模型是理想的。
- en: Whenever there is a need to process unstructured data with optimized performance
    requirements as well as compile-time type safety but not very complex Spark transformations
    and Spark actions usage requirements, the dataset-based programming model is ideal.
    At a data processing application development level, if the programming language
    of choice permits, it is better to use dataset and DataFrame to have better performance.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 当需要以优化性能要求以及编译时类型安全，但不是非常复杂的 Spark 转换和 Spark 动作使用要求来处理非结构化数据时，基于 dataset 的编程模型是理想的。在数据处理应用程序开发层面，如果选择的编程语言允许，最好使用
    dataset 和 DataFrame 以获得更好的性能。
- en: Understanding Data Catalogs
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解数据目录
- en: The previous sections of this chapter covered programming models with DataFrames
    and datasets. Both of these programming models can deal with structured data.
    The structured data comes with metadata or the data describing the structure of
    the data. Spark SQL provides a minimalist API known as Catalog API for data processing
    applications to query and use the metadata in the applications. The Catalog API
    exposes a catalog abstraction with many databases in it. For the regular SparkSession,
    it will have only one database, namely default. But if Spark is used with Hive,
    then the entire Hive meta store will be available through the Catalog API. The
    following code snippets demonstrate the usage of Catalog API in Scala and Python.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的前几节介绍了 DataFrame 和 dataset 的编程模型。这两个编程模型都可以处理结构化数据。结构化数据包含元数据或描述数据结构的描述性数据。Spark
    SQL 为数据处理应用程序提供了一个名为 Catalog API 的最小化 API，用于查询和应用程序中的元数据。Catalog API 提供了一个包含许多数据库的目录抽象。对于常规的
    SparkSession，它将只有一个数据库，即默认数据库。但如果 Spark 与 Hive 一起使用，则整个 Hive 元存储将通过 Catalog API
    可用。以下代码片段展示了 Scala 和 Python 中 Catalog API 的使用示例。
- en: 'Continuing from the same Scala REPL prompt, try the following statements:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 从相同的 Scala REPL 提示符继续，尝试以下语句：
- en: '[PRE10]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Similarly, the Catalog API can be used from Python code as well. Since the
    dataset example was not applicable in Python, the table list will be empty. At
    the Python REPL prompt, try the following statements:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，Catalog API 也可以从 Python 代码中使用。由于在 Python 中 dataset 示例不适用，因此表列表将为空。在 Python
    REPL 提示符下，尝试以下语句：
- en: '[PRE11]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The Catalog API is very handy when writing data processing applications with
    the ability to process data dynamically, based on the contents in the meta store,
    especially when using it in conjunction with Hive.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 当编写数据处理应用程序时，Catalog API 非常方便，它可以根据元存储中的内容动态处理数据，尤其是在与 Hive 结合使用时。
- en: References
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'For more information you can refer to:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如需更多信息，您可以参考：
- en: '[https://amplab.cs.berkeley.edu/wp-content/uploads/2015/03/SparkSQLSigmod2015.pdf](https://amplab.cs.berkeley.edu/wp-content/uploads/2015/03/SparkSQLSigmod2015.pdf)'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://amplab.cs.berkeley.edu/wp-content/uploads/2015/03/SparkSQLSigmod2015.pdf](https://amplab.cs.berkeley.edu/wp-content/uploads/2015/03/SparkSQLSigmod2015.pdf)'
- en: '[http://pandas.pydata.org/](http://pandas.pydata.org/)'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[http://pandas.pydata.org/](http://pandas.pydata.org/)'
- en: Summary
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Spark SQL is a very highly useful library on top of the Spark core infrastructure.
    This library makes the Spark programming more inclusive to a wider group of programmers
    who are well versed with the imperative style of programming but not as competent
    in functional programming. Apart from this, Spark SQL is the best library to process
    structured data in the Spark family of data processing libraries. Spark SQL-based
    data processing application programs can be written with SQL-like queries or DSL
    style imperative programs of DataFrame API. This chapter has also demonstrated
    various strategies of mixing RDD and DataFrames, mixing SQL-like queries and DataFrame
    API. This gives amazing flexibility for the application developers to write data
    processing programs in the way they are most comfortable with, or that is more
    appropriate to the use cases, and at the same time, without compromising performance.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL是Spark核心基础设施之上的一个非常有用的库。这个库使得Spark编程对那些熟悉命令式编程风格但不太擅长函数式编程的程序员更加包容。除此之外，Spark
    SQL是Spark数据处理库家族中处理结构化数据的最佳库。基于Spark SQL的数据处理应用程序可以使用类似SQL的查询或DataFrame API的命令式程序风格进行编写。本章还演示了混合RDD和DataFrame、混合类似SQL的查询和DataFrame
    API的各种策略。这为应用程序开发者提供了极大的灵活性，让他们可以以最舒适的方式或更符合用例的方式编写数据处理程序，同时不牺牲性能。
- en: The Dataset API is as the next generation of programming model based on dataset
    in Spark, providing optimized performance and compile-time type safety.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集API是Spark中基于数据集的下一代编程模型，提供优化的性能和编译时类型安全。
- en: The Catalog API comes as a very handy tool to process data dynamically, based
    on the contents of the meta store.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 目录API是一个非常实用的工具，可以根据元存储的内容动态处理数据。
- en: R is the language of data scientists. Till the support of R as a programming
    language in Spark SQL was available, major distributed data processing was not
    easy for them. Now, using R as a language of choice, they can seamlessly write
    distributed data processing applications as if they are using an R data frame
    from their individual machines. The next chapter is going to discuss the use of
    R to do data processing in Spark SQL.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: R是数据科学家的语言。在Spark SQL支持R作为编程语言之前，对于他们来说，主要的分布式数据处理并不容易。现在，使用R作为首选语言，他们可以无缝地编写分布式数据处理应用程序，就像他们使用个人机器上的R数据框一样。下一章将讨论在Spark
    SQL中使用R进行数据处理。
