<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch02"/>Chapter 2. Building Batch and Streaming Apps with Spark</h1></div></div></div><p>The objective of the book is to teach you about PySpark and the PyData libraries by building an app that analyzes the Spark community's interactions on social networks. We will gather information on Apache Spark from GitHub, check the relevant tweets on Twitter, and get a feel for the buzz around Spark in the broader open source software communities<a id="id109" class="indexterm"/> using <strong>Meetup</strong>.</p><p>In this chapter, we will outline the various sources of data and information. We will get an understanding of their structure. We will outline the data processing pipeline, from collection to batch and streaming processing.</p><p>In this section, we will cover the following points:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Outline data processing pipelines from collection to batch and stream processing, effectively depicting the architecture of the app we are planning to build.</li><li class="listitem" style="list-style-type: disc">Check out the various data sources (GitHub, Twitter, and Meetup), their data structure (JSON, structured information, unstructured text, geo-location, time series data, and so on), and their complexities. We also discuss the tools to connect to three different APIs, so you can build your own data mashups. The book will focus on Twitter in the following chapters.</li></ul></div><div><div><div><div><h1 class="title"><a id="ch02lvl1sec16"/>Architecting data-intensive apps</h1></div></div></div><p>We <a id="id110" class="indexterm"/>defined the data-intensive app framework architecture blueprint in the previous chapter. Let's put back in context the various software components we are going to use throughout the book in our original framework. Here's an illustration of the various components of software mapped in the data-intensive architecture framework:</p><div><img src="img/B03986_02_01.jpg" alt="Architecting data-intensive apps"/></div><p>Spark is an<a id="id111" class="indexterm"/> extremely efficient, distributed computing framework. In order to exploit its full power, we need to architect our solution accordingly. For performance reasons, the overall solution needs to also be aware of its usage in terms of CPU, storage, and network.</p><p>These imperatives drive the architecture of our solution:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Latency</strong>: This architecture combines slow and fast processing. Slow processing<a id="id112" class="indexterm"/> is done on historical data in batch mode. This is also called data at rest. This phase builds precomputed models and data patterns that will be used by the fast processing arm once live continuous data is fed into the system. Fast processing of data or real-time analysis of streaming data refers to data in motion. Data at rest is essentially processing data in batch mode with a longer latency. Data in motion refers to the streaming computation of data ingested in real time.</li><li class="listitem" style="list-style-type: disc"><strong>Scalability</strong>: Spark<a id="id113" class="indexterm"/> is natively linearly scalable through its distributed in-memory computing framework. Databases and data stores interacting with Spark need to be also able to scale linearly as data volume grows.</li><li class="listitem" style="list-style-type: disc"><strong>Fault tolerance</strong>: When a failure occurs due to hardware, software, or network <a id="id114" class="indexterm"/>reasons, the architecture should be resilient enough and provide availability at all times.</li><li class="listitem" style="list-style-type: disc"><strong>Flexibility</strong>: The data pipelines put in place in this architecture can be adapted and<a id="id115" class="indexterm"/> retrofitted very quickly depending on the use case.</li></ul></div><p>Spark is <a id="id116" class="indexterm"/>unique as it allows batch processing and streaming analytics on the same unified platform.</p><p>We will consider two data processing pipelines:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The first one handles data at rest and is focused on putting together the pipeline for batch analysis of the data</li><li class="listitem" style="list-style-type: disc">The second one, data in motion, targets real-time data ingestion and delivering insights based on precomputed models and data patterns</li></ul></div><div><div><div><div><h2 class="title"><a id="ch02lvl2sec20"/>Processing data at rest</h2></div></div></div><p>Let's get an<a id="id117" class="indexterm"/> understanding of the data at rest or batch processing pipeline. The objective in this pipeline is to ingest the various datasets from Twitter, GitHub, and Meetup; prepare the data for Spark MLlib, the machine learning engine; and derive the base models that will be applied for insight generation in batch mode or in real time.</p><p>The following diagram illustrates the data pipeline in order to enable processing data at rest:</p><div><img src="img/B03986_02_02.jpg" alt="Processing data at rest"/></div></div><div><div><div><div><h2 class="title"><a id="ch02lvl2sec21"/>Processing data in motion</h2></div></div></div><p>Processing<a id="id118" class="indexterm"/> data in motion introduces a new level of complexity, as we are introducing a new possibility of failure. If we want to scale, we need to consider bringing in distributed message queue systems such as Kafka. We will dedicate a subsequent chapter to understanding streaming analytics.</p><p>The following diagram depicts a data pipeline for processing data in motion:</p><div><img src="img/B03986_02_03.jpg" alt="Processing data in motion"/></div></div><div><div><div><div><h2 class="title"><a id="ch02lvl2sec22"/>Exploring data interactively</h2></div></div></div><p>Building a<a id="id119" class="indexterm"/> data-intensive app is not as straightforward as exposing a database to a web interface. During the setup of both the data at rest and data in motion processing, we will capitalize on Spark's ability to analyse data interactively and refine the data richness and quality required for the machine learning and streaming activities. Here, we will go through an iterative cycle of data collection, refinement, and investigation in order to get to the dataset of interest for our apps.</p></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch02lvl1sec17"/>Connecting to social networks</h1></div></div></div><p>Let's <a id="id120" class="indexterm"/>delve into the first steps of the data-intensive app architecture's integration layer. We are going to focus on harvesting the data, ensuring its integrity and preparing for batch and streaming data processing by Spark at the next stage. This phase is described in the five process steps: <em>connect</em>, <em>correct</em>, <em>collect</em>, <em>compose</em>, and <em>consume</em>. These are iterative steps of data exploration that will get us acquainted with the data and help us refine the data structure for further processing.</p><p>The following diagram depicts the iterative process of data acquisition and refinement for consumption:</p><div><img src="img/B03986_02_04.jpg" alt="Connecting to social networks"/></div><p>We connect<a id="id121" class="indexterm"/> to the social networks of interest: Twitter, GitHub, and Meetup. We will discuss the mode of access to the <strong>APIs</strong> (short for <strong>Application Programming Interface</strong>) and how to create a RESTful connection with those <a id="id122" class="indexterm"/>services while respecting the rate <a id="id123" class="indexterm"/>limitation imposed by the social networks. <strong>REST</strong> (short for <strong>Representation State Transfer</strong>) is the most widely adopted architectural style on the Internet in order to enable scalable web services. It relies <a id="id124" class="indexterm"/>on exchanging messages predominantly in <strong>JSON</strong> (short for <strong>JavaScript Object Notation</strong>). RESTful APIs and web services implement the four most prevalent verbs <code class="literal">GET</code>, <code class="literal">PUT</code>, <code class="literal">POST</code>, and <code class="literal">DELETE</code>. <code class="literal">GET</code> is used to retrieve an element or a collection from a given <code class="literal">URI</code>. <code class="literal">PUT</code> updates a collection with a new one. <code class="literal">POST</code> allows the creation of a new entry, while <code class="literal">DELETE</code> eliminates a collection.</p><div><div><div><div><h2 class="title"><a id="ch02lvl2sec23"/>Getting Twitter data</h2></div></div></div><p>Twitter<a id="id125" class="indexterm"/> allows access to registered users to its search and streaming tweet services under an authorization protocol called OAuth that allows API applications to securely act on a user's behalf. In order to create the connection, the <a id="id126" class="indexterm"/>first step is to create an application with Twitter at <a class="ulink" href="https://apps.twitter.com/app/new">https://apps.twitter.com/app/new</a>.</p><div><img src="img/B03986_02_05.jpg" alt="Getting Twitter data"/></div><p>Once the <a id="id127" class="indexterm"/>application has been created, Twitter will issue the four codes that will allow it to tap into the Twitter hose:</p><div><pre class="programlisting">CONSUMER_KEY = 'GetYourKey@Twitter'
CONSUMER_SECRET = ' GetYourKey@Twitter'
OAUTH_TOKEN = ' GetYourToken@Twitter'
OAUTH_TOKEN_SECRET = ' GetYourToken@Twitter'</pre></div><p>If you wish to get a feel for the various RESTful queries offered, you can explore the Twitter <a id="id128" class="indexterm"/>API on the dev console at <a class="ulink" href="https://dev.twitter.com/rest/tools/console">https://dev.twitter.com/rest/tools/console</a>:</p><div><img src="img/B03986_02_06.jpg" alt="Getting Twitter data"/></div><p>We will <a id="id129" class="indexterm"/>make a programmatic connection on Twitter using the following code, which will activate our OAuth access and allows us to tap into the Twitter API under the rate limitation. In the streaming mode, the limitation is for a GET request.</p></div><div><div><div><div><h2 class="title"><a id="ch02lvl2sec24"/>Getting GitHub data</h2></div></div></div><p>GitHub <a id="id130" class="indexterm"/>uses a similar authentication process to Twitter. Head to the developer site and retrieve your credentials after duly registering <a id="id131" class="indexterm"/>with GitHub at <a class="ulink" href="https://developer.github.com/v3/">https://developer.github.com/v3/</a>:</p><div><img src="img/B03986_02_07.jpg" alt="Getting GitHub data"/></div></div><div><div><div><div><h2 class="title"><a id="ch02lvl2sec25"/>Getting Meetup data</h2></div></div></div><p>Meetup can<a id="id132" class="indexterm"/> be accessed using the token issued in the developer resources to members of Meetup.com. The necessary token or OAuth credential <a id="id133" class="indexterm"/>for Meetup API access can be obtained on their developer's website at <a class="ulink" href="https://secure.meetup.com/meetup_api">https://secure.meetup.com/meetup_api</a>:</p><div><img src="img/B03986_02_08.jpg" alt="Getting Meetup data"/></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch02lvl1sec18"/>Analyzing the data</h1></div></div></div><p>Let's get <a id="id134" class="indexterm"/>a first feel for the data extracted from each of the social networks and get an understanding of the data structure from each these sources.</p><div><div><div><div><h2 class="title"><a id="ch02lvl2sec26"/>Discovering the anatomy of tweets</h2></div></div></div><p>In <a id="id135" class="indexterm"/>this section, we are going to establish connection with the Twitter API. Twitter offers two connection modes: the REST API, which allows us to search historical tweets for a given search term or hashtag, and the streaming API, which delivers real-time tweets under the rate limit in place.</p><p>In order to get a better understanding of how to operate with the Twitter API, we will go through the following steps:</p><div><ol class="orderedlist arabic"><li class="listitem">Install the Twitter Python library.</li><li class="listitem">Establish a connection programmatically via OAuth, the authentication required for Twitter.</li><li class="listitem">Search<a id="id136" class="indexterm"/> for recent tweets for the query <em>Apache Spark</em> and explore the results obtained.</li><li class="listitem">Decide on the key attributes of interest and retrieve the information from the JSON output.</li></ol></div><p>Let's go through it step-by-step:</p><div><ol class="orderedlist arabic"><li class="listitem">Install the Python Twitter library. In order to install it, you need to write <code class="literal">pip install twitter</code> from the command line:<div><pre class="programlisting">
<strong>$ pip install twitter</strong>
</pre></div></li><li class="listitem">Create the Python Twitter API class and its base methods for authentication, searching, and parsing the results. <code class="literal">self.auth</code> gets the credentials from Twitter. It then creates a registered API as <code class="literal">self.api</code>. We have implemented two methods: the first one to search Twitter with a given query and the second one to parse the output to retrieve relevant information such as the tweet ID, the tweet text, and the tweet author. The code is as follows:<div><pre class="programlisting">import twitter
import urlparse
from pprint import pprint as pp

class TwitterAPI(object):
    """
    TwitterAPI class allows the Connection to Twitter via OAuth
    once you have registered with Twitter and receive the 
    necessary credentiials 
    """

# initialize and get the twitter credentials
     def __init__(self): 
        consumer_key = 'Provide your credentials'
        consumer_secret = 'Provide your credentials'
        access_token = 'Provide your credentials'
        access_secret = 'Provide your credentials'
     
        self.consumer_key = consumer_key
        self.consumer_secret = consumer_secret
        self.access_token = access_token
        self.access_secret = access_secret

#
# authenticate credentials with Twitter using OAuth
        self.auth = twitter.oauth.OAuth(access_token, access_secret, consumer_key, consumer_secret)
    # creates registered Twitter API
        self.api = twitter.Twitter(auth=self.auth)
#
# search Twitter with query q (i.e. "ApacheSpark") and max. result
    def searchTwitter(self, q, max_res=10,**kwargs):
        search_results = self.api.search.tweets(q=q, count=10, **kwargs)
        statuses = search_results['statuses']
        max_results = min(1000, max_res)

        for _ in range(10): 
            try:
                next_results = search_results['search_metadata']['next_results']
            except KeyError as e: 
                break

            next_results = urlparse.parse_qsl(next_results[1:])
            kwargs = dict(next_results)
            search_results = self.api.search.tweets(**kwargs)
            statuses += search_results['statuses']

            if len(statuses) &gt; max_results: 
                break
        return statuses
#
# parse tweets as it is collected to extract id, creation 
# date, user id, tweet text
    def parseTweets(self, statuses):
        return [ (status['id'], 
                  status['created_at'], 
                  status['user']['id'],
                  status['user']['name'], 
                  status['text'], url['expanded_url']) 
                        for status in statuses 
                            for url in status['entities']['urls'] ]</pre></div></li><li class="listitem">Instantiate<a id="id137" class="indexterm"/> the class with the required authentication:<div><pre class="programlisting">t= TwitterAPI()</pre></div></li><li class="listitem">Run a search on the query term <em>Apache Spark</em>:<div><pre class="programlisting">q="ApacheSpark"
tsearch = t.searchTwitter(q)</pre></div></li><li class="listitem">Analyze<a id="id138" class="indexterm"/> the JSON output:<div><pre class="programlisting">pp(tsearch[1])

{u'contributors': None,
 u'coordinates': None,
 u'created_at': u'Sat Apr 25 14:50:57 +0000 2015',
 u'entities': {u'hashtags': [{u'indices': [74, 86], u'text': u'sparksummit'}],
               u'media': [{u'display_url': u'pic.twitter.com/WKUMRXxIWZ',
                           u'expanded_url': u'http://twitter.com/bigdata/status/591976255831969792/photo/1',
                           u'id': 591976255156715520,
                           u'id_str': u'591976255156715520',
                           u'indices': [143, 144],
                           u'media_url': 
...(snip)... 
 u'text': u'RT @bigdata: Enjoyed catching up with @ApacheSpark users &amp;amp; leaders at #sparksummit NYC: video clips are out http://t.co/qrqpP6cG9s http://t\u2026',
 u'truncated': False,
 u'user': {u'contributors_enabled': False,
           u'created_at': u'Sat Apr 04 14:44:31 +0000 2015',
           u'default_profile': True,
           u'default_profile_image': True,
           u'description': u'',
           u'entities': {u'description': {u'urls': []}},
           u'favourites_count': 0,
           u'follow_request_sent': False,
           u'followers_count': 586,
           u'following': False,
           u'friends_count': 2,
           u'geo_enabled': False,
           u'id': 3139047660,
           u'id_str': u'3139047660',
           u'is_translation_enabled': False,
           u'is_translator': False,
           u'lang': u'zh-cn',
           u'listed_count': 749,
           u'location': u'',
           u'name': u'Mega Data Mama',
           u'notifications': False,
           u'profile_background_color': u'C0DEED',
           u'profile_background_image_url': u'http://abs.twimg.com/images/themes/theme1/bg.png',
           u'profile_background_image_url_https': u'https://abs.twimg.com/images/themes/theme1/bg.png',
           ...(snip)... 
           u'screen_name': u'MegaDataMama',
           u'statuses_count': 26673,
           u'time_zone': None,
           u'url': None,
           u'utc_offset': None,
           u'verified': False}}</pre></div></li><li class="listitem">Parse<a id="id139" class="indexterm"/> the Twitter output to retrieve key information of interest:<div><pre class="programlisting">tparsed = t.parseTweets(tsearch)
pp(tparsed)

[(591980327784046592,
  u'Sat Apr 25 15:01:23 +0000 2015',
  63407360,
  u'Jos\xe9 Carlos Baquero',
  u'Big Data systems are making a difference in the fight against cancer. #BigData #ApacheSpark http://t.co/pnOLmsKdL9',
  u'http://tmblr.co/ZqTggs1jHytN0'),
 (591977704464875520,
  u'Sat Apr 25 14:50:57 +0000 2015',
  3139047660,
  u'Mega Data Mama',
  u'RT @bigdata: Enjoyed catching up with @ApacheSpark users &amp;amp; leaders at #sparksummit NYC: video clips are out http://t.co/qrqpP6cG9s http://t\u2026',
  u'http://goo.gl/eF5xwK'),
 (591977172589539328,
  u'Sat Apr 25 14:48:51 +0000 2015',
  2997608763,
  u'Emma Clark',
  u'RT @bigdata: Enjoyed catching up with @ApacheSpark users &amp;amp; leaders at #sparksummit NYC: video clips are out http://t.co/qrqpP6cG9s http://t\u2026',
  u'http://goo.gl/eF5xwK'),
 ... (snip)...  
 (591879098349268992,
  u'Sat Apr 25 08:19:08 +0000 2015',
  331263208,
  u'Mario Molina',
  u'#ApacheSpark speeds up big data decision-making http://t.co/8hdEXreNfN',
  u'http://www.computerweekly.com/feature/Apache-Spark-speeds-up-big-data-decision-making')]</pre></div></li></ol></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch02lvl1sec19"/>Exploring the GitHub world</h1></div></div></div><p>In order<a id="id140" class="indexterm"/> to get a better understanding on how to operate with the GitHub API, we will go through the following steps:</p><div><ol class="orderedlist arabic"><li class="listitem">Install the GitHub Python library.</li><li class="listitem">Access the API by using the token provided when we registered in the developer website.</li><li class="listitem">Retrieve some key facts on the Apache foundation that is hosting the spark repository.</li></ol></div><p>Let's go through the process step-by-step:</p><div><ol class="orderedlist arabic"><li class="listitem">Install the Python PyGithub library. In order to install it, you need to <code class="literal">pip install PyGithub</code> from the command line:<div><pre class="programlisting">pip install PyGithub</pre></div></li><li class="listitem">Programmatically create a client to instantiate the GitHub API:<div><pre class="programlisting">from github import Github

# Get your own access token

ACCESS_TOKEN = 'Get_Your_Own_Access_Token'

# We are focusing our attention to User = apache and Repo = spark

USER = 'apache'
REPO = 'spark'

g = Github(ACCESS_TOKEN, per_page=100)
user = g.get_user(USER)
repo = user.get_repo(REPO)</pre></div></li><li class="listitem">Retrieve key facts from the Apache User. There are 640 active Apache repositories in GitHub:<div><pre class="programlisting">repos_apache = [repo.name for repo in g.get_user('apache').get_repos()]
len(repos_apache)
640</pre></div></li><li class="listitem">Retrieve<a id="id141" class="indexterm"/> key facts from the Spark repository, The programing languages used in the Spark repo are given here under:<div><pre class="programlisting">pp(repo.get_languages())

{u'C': 1493,
 u'CSS': 4472,
 u'Groff': 5379,
 u'Java': 1054894,
 u'JavaScript': 21569,
 u'Makefile': 7771,
 u'Python': 1091048,
 u'R': 339201,
 u'Scala': 10249122,
 u'Shell': 172244}</pre></div></li><li class="listitem">Retrieve a few key participants of the wide Spark GitHub repository network. There are 3,738 stargazers in the Apache Spark repository at the time of writing. The network is immense. The first stargazer is <em>Matei Zaharia</em>, the cofounder of the Spark project when he was doing his PhD in Berkeley.<div><pre class="programlisting">stargazers = [ s for s in repo.get_stargazers() ]
print "Number of stargazers", len(stargazers)
Number of stargazers 3738

[stargazers[i].login for i in range (0,20)]
[u'mateiz',
 u'beyang',
 u'abo',
 u'CodingCat',
 u'andy327',
 u'CrazyJvm',
 u'jyotiska',
 u'BaiGang',
 u'sundstei',
 u'dianacarroll',
 u'ybotco',
 u'xelax',
 u'prabeesh',
 u'invkrh',
 u'bedla',
 u'nadesai',
 u'pcpratts',
 u'narkisr',
 u'Honghe',
 u'Jacke']</pre></div></li></ol></div><div><div><div><div><h2 class="title"><a id="ch02lvl2sec27"/>Understanding the community through Meetup</h2></div></div></div><p>In order <a id="id142" class="indexterm"/>to get a better understanding of how to operate with the Meetup API, we will go through the following steps:</p><div><ol class="orderedlist arabic"><li class="listitem">Create a Python program to call the Meetup API using an authentication token.</li><li class="listitem">Retrieve information of past events for meetup groups such as <em>London Data Science</em>.</li><li class="listitem">Retrieve the profile of the meetup members in order to analyze their participation in similar meetup groups.</li></ol></div><p>Let's go through the process step-by-step:</p><div><ol class="orderedlist arabic"><li class="listitem">As there is no reliable Meetup API Python library, we will programmatically create a client to instantiate the Meetup API:<div><pre class="programlisting">import json
import mimeparse
import requests
import urllib
from pprint import pprint as pp

MEETUP_API_HOST = 'https://api.meetup.com'
EVENTS_URL = MEETUP_API_HOST + '/2/events.json'
MEMBERS_URL = MEETUP_API_HOST + '/2/members.json'
GROUPS_URL = MEETUP_API_HOST + '/2/groups.json'
RSVPS_URL = MEETUP_API_HOST + '/2/rsvps.json'
PHOTOS_URL = MEETUP_API_HOST + '/2/photos.json'
GROUP_URLNAME = 'London-Machine-Learning-Meetup'
# GROUP_URLNAME = 'London-Machine-Learning-Meetup' # 'Data-Science-London'

class Mee
tupAPI(object):
    """
    Retrieves information about meetup.com
    """
    def __init__(self, api_key, num_past_events=10, http_timeout=1,
                 http_retries=2):
        """
        Create a new instance of MeetupAPI
        """
        self._api_key = api_key
        self._http_timeout = http_timeout
        self._http_retries = http_retries
        self._num_past_events = num_past_events

    def get_past_events(self):
        """
        Get past meetup events for a given meetup group
        """
        params = {'key': self._api_key,
                  'group_urlname': GROUP_URLNAME,
                  'status': 'past',
                  'desc': 'true'}
        if self._num_past_events:
            params['page'] = str(self._num_past_events)

        query = urllib.urlencode(params)
        url = '{0}?{1}'.format(EVENTS_URL, query)
        response = requests.get(url, timeout=self._http_timeout)
        data = response.json()['results']
        return data

    def get_members(self):
        """
        Get meetup members for a given meetup group
        """
        params = {'key': self._api_key,
                  'group_urlname': GROUP_URLNAME,
                  'offset': '0',
                  'format': 'json',
                  'page': '100',
                  'order': 'name'}
        query = urllib.urlencode(params)
        url = '{0}?{1}'.format(MEMBERS_URL, query)
        response = requests.get(url, timeout=self._http_timeout)
        data = response.json()['results']
        return data

    def get_groups_by_member(self, member_id='38680722'):
        """
        Get meetup groups for a given meetup member
        """
        params = {'key': self._api_key,
                  'member_id': member_id,
                  'offset': '0',
                  'format': 'json',
                  'page': '100',
                  'order': 'id'}
        query = urllib.urlencode(params)
        url = '{0}?{1}'.format(GROUPS_URL, query)
        response = requests.get(url, timeout=self._http_timeout)
        data = response.json()['results']
        return data</pre></div></li><li class="listitem">Then, we <a id="id143" class="indexterm"/>will retrieve past events from a given Meetup group:<div><pre class="programlisting">m = MeetupAPI(api_key='Get_Your_Own_Key')
last_meetups = m.get_past_events()
pp(last_meetups[5])

{u'created': 1401809093000,
 u'description': u"&lt;p&gt;We are hosting a joint meetup between Spark London and Machine Learning London. Given the excitement in the machine learning community around Spark at the moment a joint meetup is in order!&lt;/p&gt; &lt;p&gt;Michael Armbrust from the Apache Spark core team will be flying over from the States to give us a talk in person.\xa0Thanks to our sponsors, Cloudera, MapR and Databricks for helping make this happen.&lt;/p&gt; &lt;p&gt;The first part of the talk will be about MLlib, the machine learning library for Spark,\xa0and the second part, on\xa0Spark SQL.&lt;/p&gt; &lt;p&gt;Don't sign up if you have already signed up on the Spark London page though!&lt;/p&gt; &lt;p&gt;\n\n\nAbstract for part one:&lt;/p&gt; &lt;p&gt;In this talk, we\u2019ll introduce Spark and show how to use it to build fast, end-to-end machine learning workflows. Using Spark\u2019s high-level API, we can process raw data with familiar libraries in Java, Scala or Python (e.g. NumPy) to extract the features for machine learning. Then, using MLlib, its built-in machine learning library, we can run scalable versions of popular algorithms. We\u2019ll also cover upcoming development work including new built-in algorithms and R bindings.&lt;/p&gt; &lt;p&gt;\n\n\n\nAbstract for part two:\xa0&lt;/p&gt; &lt;p&gt;In this talk, we'll examine Spark SQL, a new Alpha component that is part of the Apache Spark 1.0 release. Spark SQL lets developers natively query data stored in both existing RDDs and external sources such as Apache Hive. A key feature of Spark SQL is the ability to blur the lines between relational tables and RDDs, making it easy for developers to intermix SQL commands that query external data with complex analytics. In addition to Spark SQL, we'll explore the Catalyst optimizer framework, which allows Spark SQL to automatically rewrite query plans to execute more efficiently.&lt;/p&gt;",
 u'event_url': u'http://www.meetup.com/London-Machine-Learning-Meetup/events/186883262/',
 u'group': {u'created': 1322826414000,
            u'group_lat': 51.52000045776367,
            u'group_lon': -0.18000000715255737,
            u'id': 2894492,
            u'join_mode': u'open',
            u'name': u'London Machine Learning Meetup',
            u'urlname': u'London-Machine-Learning-Meetup',
            u'who': u'Machine Learning Enthusiasts'},
 u'headcount': 0,
 u'id': u'186883262',
 u'maybe_rsvp_count': 0,
 u'name': u'Joint Spark London and Machine Learning Meetup',
 u'rating': {u'average': 4.800000190734863, u'count': 5},
 u'rsvp_limit': 70,
 u'status': u'past',
 u'time': 1403200800000,
 u'updated': 1403450844000,
 u'utc_offset': 3600000,
 u'venue': {u'address_1': u'12 Errol St, London',
            u'city': u'EC1Y 8LX',
            u'country': u'gb',
            u'id': 19504802,
            u'lat': 51.522533,
            u'lon': -0.090934,
            u'name': u'Royal Statistical Society',
            u'repinned': False},
 u'visibility': u'public',
 u'waitlist_count': 84,
 u'yes_rsvp_count': 70}</pre></div></li><li class="listitem">Get information about the Meetup members:<div><pre class="programlisting">members = m.get_members()

{u'city': u'London',
  u'country': u'gb',
  u'hometown': u'London',
  u'id': 11337881,
  u'joined': 1421418896000,
  u'lat': 51.53,
  u'link': u'http://www.meetup.com/members/11337881',
  u'lon': -0.09,
  u'name': u'Abhishek Shivkumar',
  u'other_services': {u'twitter': {u'identifier': u'@abhisemweb'}},
  u'photo': {u'highres_link': u'http://photos3.meetupstatic.com/photos/member/9/6/f/3/highres_10898643.jpeg',
             u'photo_id': 10898643,
             u'photo_link': u'http://photos3.meetupstatic.com/photos/member/9/6/f/3/member_10898643.jpeg',
             u'thumb_link': u'http://photos3.meetupstatic.com/photos/member/9/6/f/3/thumb_10898643.jpeg'},
  u'self': {u'common': {}},
  u'state': u'17',
  u'status': u'active',
  u'topics': [{u'id': 1372, u'name': u'Semantic Web', u'urlkey': u'semweb'},
              {u'id': 1512, u'name': u'XML', u'urlkey': u'xml'},
              {u'id': 49585,
               u'name': u'Semantic Social Networks',
               u'urlkey': u'semantic-social-networks'},
              {u'id': 24553,
               u'name': u'Natural Language Processing',
...(snip)...
               u'name': u'Android Development',
               u'urlkey': u'android-developers'}],
  u'visited': 1429281599000}</pre></div></li></ol></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch02lvl1sec20"/>Previewing our app</h1></div></div></div><p>Our<a id="id144" class="indexterm"/> challenge is to make sense of the data retrieved from these social networks, finding the key relationships and deriving insights. Some of the elements of interest are as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Visualizing the top influencers: Discover the top influencers in the community:<div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Heavy Twitter users on <em>Apache Spark</em></li><li class="listitem" style="list-style-type: disc">Committers in GitHub</li><li class="listitem" style="list-style-type: disc">Leading Meetup presentations</li></ul></div></li><li class="listitem" style="list-style-type: disc">Understanding the Network: Network graph of GitHub committers, watchers, and stargazers</li><li class="listitem" style="list-style-type: disc">Identifying the Hot Locations: Locating the most active location for Spark</li></ul></div><p>The following <a id="id145" class="indexterm"/>screenshot provides a preview of our app:</p><div><img src="img/B03986_02_09.jpg" alt="Previewing our app"/></div></div>
<div><div><div><div><h1 class="title"><a id="ch02lvl1sec21"/>Summary</h1></div></div></div><p>In this chapter, we laid out the overall architecture of our app. We explained the two main paradigms of processing data: batch processing, also called data at rest, and streaming analytics, referred to as data in motion. We proceeded to establish connections to three social networks of interest: Twitter, GitHub, and Meetup. We sampled the data and provided a preview of what we are aiming to build. The remainder of the book will focus on the Twitter dataset. We provided here the tools and API to access three social networks, so you can at a later stage create your own data mashups. We are now ready to investigate the data collected, which will be the topic of the next chapter.</p><p>In the next chapter, we will delve deeper into data analysis, extracting the key attributes of interest for our purposes and managing the storage of the information for batch and stream processing.</p></div></body></html>