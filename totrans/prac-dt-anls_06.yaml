- en: Gathering and Loading Data in Python
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will explain what SQL is and why it is important for data analysis
    by teaching you how to use and access databases using SQLite for our examples.
    An overview of relational database technology will be provided along with insightful
    information on database systems to help to improve your data literacy when communicating
    with experts. You will also learn how to run SQL `SELECT` queries from the Jupyter
    Notebook and how to load them into DataFrames. Basic statistics, data lineage,
    and metadata (data about data) will be explained using the `pandas` library.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to SQL and relational databases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From SQL to pandas DataFrames
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data about your data explained
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The importance of data lineage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here's the GitHub repository of this book: [https://github.com/PacktPublishing/Practical-Data-Analysis-using-Jupyter-Notebook/tree/master/Chapter05](https://github.com/PacktPublishing/Practical-Data-Analysis-using-Jupyter-Notebook/tree/master/Chapter05).
  prefs: []
  type: TYPE_NORMAL
- en: You can download and install the required software from the following link: [https://www.anaconda.com/products/individual](https://www.anaconda.com/products/individual).
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to SQL and relational databases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are now at a point in this book where my professional career started, working
    with databases and SQL. **Structured Query Language** (**SQL**) was created decades
    ago as a means to communicate with structured data stored in tables. Over the
    years, SQL has evolved from multiple variations that were specific to the underlining
    database technology. For example, IBM, Oracle, and Sybase all had variations in
    their SQL commands, which built loyalty in their customers but required changes
    when switching vendors. The adoption of the **International Organization for Standardization **(**ISO**)
    and **American National Standards Institute** (**ANSI**) standards helped to define
    what is commonly used today.
  prefs: []
  type: TYPE_NORMAL
- en: 'So far in this book, all of the examples of structured data focused on one
    table or file. Relational databases solve the problem of storing data together
    in multiple tables while keeping consistency across them using the concept of
    a primary and foreign key:'
  prefs: []
  type: TYPE_NORMAL
- en: A primary key is the unique value (typically an integer) used to represent a
    single distinct record or tuple in each table.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A foreign key would be a field in one table that references the primary key
    from another.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This relationship defines integrity between one or more tables for consistency
    for all of the data. Since the concept of storing and joining the data is abstract,
    this allows it to be applied to many different data subjects. For example, you
    can create a database to store sales from a manufacturing company, user hits from
    a website, or stock purchases in a financial services company. Because of this
    versatility, SQL remains a top programming language and a must-have skill for
    data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: SQL was created to communicate with data stored in database tables that have
    a defined schema. A database schema is like a blueprint that defines a structure
    for storing data before the data is loaded. This definition includes rules, conditions,
    and specific data types for each field in a table. The foundation for the database
    technology was created by Dr. EF Codd back in 1970 and was a milestone of the *Evolution
    of Data Analysis*, which I defined in [Chapter 1](0fa7e28f-7a30-4099-9bae-30dd3c86ee4f.xhtml), *Fundamentals
    of Data Analysis*. The concept of persisting data in defined columns and rows
    as tables in a structured relationship showcases the legacy of Dr. Codd's contribution
    to this technology and data. His contributions to the technology along with others
    such as Ralph Kimball and Bill Inmon has created new industries and careers over
    the years. If you come across an **Enterprise Data Warehouse** (**EDW**), you
    can bet money it uses the Kimball or Inmon methods as a standard. Their influence, which
    defined new skills to work with data, cannot be understated. I have immense gratitude
    for the people who have evolved technologies and concepts supporting all things
    data.
  prefs: []
  type: TYPE_NORMAL
- en: What is defined as a relational database is a vast subject so I'm going to focus
    on what is relevant for building your data literacy and the analysis of data from
    consuming data using SQL. The key concepts to focus on behind working with any
    **Relational Database Management System** (**RDBMS**) begin with how to communicate
    with the system or servers that host the database. Most of them support using
    an ODBC driver, which handles authentication and communication over a network.
    **Open Database Connectivity** (**ODBC**) is a common standard used to send and
    receive data between your analysis tool, for example, the Jupyter Notebook, and
    where the data is stored. Most large-scale, enterprise relational database systems
    support ODBC connections to communicate with the database.
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally, this would be known as a client-server architecture, where your
    local computer is known as the client and the location of the database would be
    managed by one or more servers. When I was a consultant, the most common enterprise
    RDBMSes I worked with were Microsoft SQL Server, Oracle, IBM DB2, MySQL, and PostgreSQL.
  prefs: []
  type: TYPE_NORMAL
- en: An ODBC driver may need to be installed and configured on your workstation to
    communicate with a client-server architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Today, other flavors of both open source and vendor database products exist
    but many do and should support SQL or a variation of it. For example, Apache's
    HiveQL is very similar to ASCI SQL but runs on top of the **Hadoop Distributed
    File System** (**HDFS**) instead of a database. For our examples, we will be using
    SQLite, which is a file-based database you can install locally or connect with
    via ODBC. SQLite is open source and cross-platform, which means we can install
    it on any operating system, and it is touted as the *most widely deployed and
    used database engine in the world*according to their download page, which you
    can find in the *Further reading* section.
  prefs: []
  type: TYPE_NORMAL
- en: Once a connection has been established, a user ID and password are commonly
    required, which control what actions you can perform and which tables you can
    access. If you installed the database yourself, you are the owner of the database
    and probably have system administrator rights, which gives you full access to
    create, delete, and read any table. If you are a client, the **Database Administrator**
    (**DBA**) would be responsible for setting up access and permission for your user
    ID.
  prefs: []
  type: TYPE_NORMAL
- en: I find what makes the SQL a popular language even today is the learning curve
    required to use it. In my experience, many business users and data analysts find
    the syntax intuitively obvious even without a background in computer science.
    SQL code is easy to read and it's quickly understood what the expected results are.
    It also supports instant gratification where a few commands can produce results
    in less than one second even with large volumes of data once it's been optimized
    for performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let''s say I want to know the highest closing stock price of Apple
    stock in all of 2018\. Even without really understanding all of the details behind
    how or where that data is stored, the syntax for this one line of code is easy
    to interpret:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s walk through this code and break out the key components:'
  prefs: []
  type: TYPE_NORMAL
- en: First, I capitalized the reserved words, which are universal across any RDBMS
    that supports ISO standard/ASCI SQL.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `SELECT` command instructs the code to retrieve data in the form of rows
    and columns from a table defined after the `FROM` statement.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Between the `SELECT` and the `FROM` reserved words is the `max(closing_price)` command.
    This is using the `max()` function that is available in SQL to retrieve the maximum
    or largest value from the `closing_price` field. The max function will only return
    one row and one value regardless of whether duplicate values exist in the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `FROM` section of the code lets the SQL interpreter know a table or object
    is being referenced immediately afterward. For this example, we are looking for
    records from the `tbl_stock_price` table.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `WHERE` clause from the `SELECT` SQL statement restricts the data by reducing
    the number of rows to a specific condition, which is defined by a specific field
    of `year` and `value` to the right of the equals sign of `2018`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SELECT` is the most common SQL command and has many different use cases and
    levels of complexity. We are just scratching the surface but you can find more
    resources in the *Further reading* section.'
  prefs: []
  type: TYPE_NORMAL
- en: SQL is not case sensitive but the tables and fields referenced might be, depending
    on which RDBMS is being used. Spaces are important between reserve words but you
    typically won't find spaces in the table or field names. Rather, underscores or
    dashes are common.
  prefs: []
  type: TYPE_NORMAL
- en: From SQL to pandas DataFrames
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have some background on SQL and relational databases, let's download
    a local copy of an SQLite database file, set up a connection, and load some data
    into a `pandas` DataFrame. For this example, I have provided the database file
    named `customer_sales.db` so be sure to download it from the GitHub repository
    beforehand.
  prefs: []
  type: TYPE_NORMAL
- en: 'To give you some context about this database file and support the **Know Your
    Data** (**KYD**) concept that we learned in [Chapter 1](0fa7e28f-7a30-4099-9bae-30dd3c86ee4f.xhtml),
    *Fundamentals* *of Data Analysis*, we have three tables named `tbl_customers`,
    `tbl_products`, and `tbl_sales`. This would be a simple example of any company
    that has customers who purchase products that generate sales over any period of
    time. A visual representation of how the data is stored and joined together, which
    is commonly known as an **ERD** (short for **Entity Relationship Diagram**), is
    shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/349bdfe8-02e1-43dd-9926-6fc1bbe4084b.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding diagram, we have a visual of three tables with the column name
    defined on the left side of each box and the data type of each column immediately
    to the right. The primary key for each table is identified with a suffix in the
    name of `_ID`, along with bolding the text in the first row of each table. The primary
    key commonly has a data type of integer, which is also the case here.
  prefs: []
  type: TYPE_NORMAL
- en: The `tbl_sales` table includes two of those fields, `Customer_ID` and `Product_ID`, which
    means they are classified as foreign keys. The lines between the tables reinforce
    the relationship between them, which also indicates how to join them together.
    The small lines that look like *crow's feet* tell the consumer these tables are
    defined with a one-to-many relationship. In this example, `tbl_sales` will have
    many customers and many products but a record in `tbl_customers` will only have
    one value assigned per `Customer_ID` and `tbl_products` will only have one value
    assigned per `Product_ID`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have more information about the data, let''s launch a new Jupyter
    notebook and name it `retrieve_sql_and_create_dataframe`. To create a connection
    and use SQLite, we have to import a new library using code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To load an SQLite database connection, you just need to add the following command
    in your Jupyter notebook and run the cell. Feel free to follow along by creating
    your own notebook (I have placed a copy in GitHub for reference):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The `sqlite3` module comes with the Anaconda distribution installed. Refer to
    [Chapter 2](e0fe6eb2-8f38-41f7-9dea-2b177578fd3c.xhtml), *Overview of Python and
    Installing Jupyter Notebook*, for help with setting up your environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to assign a connection to a variable named `conn` and point to
    the location of the database file, which is named `customer_sales.db`. Since we
    already imported the `sqlite3` library in the prior `In[]` line, we can use this
    built-in function to communicate with the database:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Be sure to copy the `customer_sales.db` file to the correct Jupyter folder directory
    to avoid errors with the connection.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next library to import should be very familiar, which allows us to use
    `pandas` so the code will be as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'To run a SQL statement and assign the results to a DataFrame, we have to run
    this one line of code. The `pandas` library includes a `read_sql_query()` function
    to make it easier to communicate with databases using SQL. It requires a connection
    parameter, which we named `conn` in the previous steps. We assign the results
    to a new DataFrame as `df_sales` to make it easier to identify:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the results in a DataFrame, we can use all of the available
    `pandas` library commands against this data without going back to the database.
    To view the results, we can just run the `head()` command against this DataFrame
    using this code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look like the following screenshot where the `tbl_sales` table has
    been loaded into a DataFrame with a labeled header row with the index column to
    the left starting with a value of `0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c32c6615-5457-4e20-8630-ffcfbdc6d11e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To sort the values in the DataFrame, we can use the `sort_values()` function
    and include a parameter of the field name, which will default to ascending order.
    Let''s begin by sorting the results by date to see when the first sale was recorded
    in the database by using this command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The output would look like the following screenshot where the DataFrame output
    is now sorted by the `Sale_Date` field from `1/15/2015` to `6/9/2019`. Notice
    the difference in `Sale_ID`, which is out of sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9169e662-b2d9-4523-8f1a-c531c5104c27.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To limit the data displayed, we can use the `DataFrame.loc` command to isolate
    specific rows or columns based on how it is labeled by the header row. To retrieve
    the first row available, we simply run this command against our DataFrame and
    reference the index value, which begins with `0`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The output would look like the following screenshot where a single record is
    displayed as a series with the rows transposed from multiple columns to multiple
    rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4ded9bc9-f8e9-4e4c-8cb4-17bd6eb75751.png)'
  prefs: []
  type: TYPE_IMG
- en: Using this method, you must know which specific record you are looking for by
    index, which reflects how the data was loaded from the SQL statement. To ensure
    consistency between the database tables, you may want to include an `ORDER BY`
    command when loading the data into the DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'To restrict the data displayed, we can use a nested command to isolate specific
    rows based on a condition. A business task you could address using this data would
    be to *identify customers with high sales so we can thank them personally*. To
    do this, we can filter the sales by a specific value and display only the rows
    that meet or exceed that condition. For this example, we assigned `high` to an
    arbitrary number so any `Sales_Amount` over 100 will be displayed using this command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The output would look like the following screenshot where a single record is
    displayed based on the condition because there is only one record where `Sales_Amount`
    is greater than `100`, which is `Sale_ID` equal to `4`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0d0d28d3-e4f3-4bb9-934d-ba765dd9a288.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Another example of how to restrict results would be looking for a specific
    value assigned to a specific field in the DataFrame. If we wanted to better understand
    this data, we could do so by looking at the `Sales_Quantity` field and seeing
    which records only had one product purchased:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The output would look like the following screenshot, where multiple records
    are displayed based on the condition where `Sales_Quantity` is equal to `1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/07b0a5ba-380a-40ac-a8a7-25996b2f9da3.png)'
  prefs: []
  type: TYPE_IMG
- en: The steps define a best practice for an analysis workflow. Retrieving SQL results,
    storing them in one or more DataFrames, and then performing analysis in your notebook
    is common and encouraged. Migrating data between sources (from database to Jupyter
    Notebook) can take high compute resources depending on the volume of data, so
    be conscious of how much memory you have available and how large the databases
    you are working with are.
  prefs: []
  type: TYPE_NORMAL
- en: Data about your data explained
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have a better understanding of how to work with SQL sourced data
    using Python and pandas, let's explore some fundamental statistics along with
    practical usage for data analysis. So far, we have focused on descriptive statistics
    versus predictive statistics. However, I recommend not proceeding with any data
    science predictive analytics without a firm understanding of descriptive analytics
    first.
  prefs: []
  type: TYPE_NORMAL
- en: Fundamental statistics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Descriptive analytics is based on what has already happened in the past by analyzing
    the digital footprint of data to gain insights, analyze trends, and identify patterns.
    Using SQL to read data from one or more tables supports this effort, which should
    include basic statistics and arithmetic. Having the data structured and conformed,
    which includes defined data types per column, makes this type of analysis easier
    once you understand some key concepts and commands.There are many statistical
    functions available in both SQL and Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'I have summarized a few that are fundamental to your data analysis in this
    table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Statistical Measure** | **Description** | **Best For/Use Case** | **SQL
    Syntax** | **pandas Function** |'
  prefs: []
  type: TYPE_TB
- en: '| Count | The number of occurrences of a value regardless of data type | Finding
    out the size of a table/number of records | `SELECT Count(*) FROM table_name`
    | `df.count()` |'
  prefs: []
  type: TYPE_TB
- en: '| Count Distinct | The number of distinct occurrences of a value regardless
    of data type | Removing duplicate values/verify distinct values used for categories
    of data | `SELECT Count(distinct field_name) FROM table_name` | `df.nunique()`
    |'
  prefs: []
  type: TYPE_TB
- en: '| Sum | The aggregation of values as a whole or total against numeric data
    types | Finding the total population or measuring the amount of money | `SELECT
    Sum(field_name) FROM table_name` | `df.sum()` |'
  prefs: []
  type: TYPE_TB
- en: '| Mean | The arithmetic average from a set of two or more numeric data types
    | Sum of values divided by the count of values | `SELECT AVG(field_name) FROM
    table_name` | `df.mean()` |'
  prefs: []
  type: TYPE_TB
- en: '| Min | The lowest numeric value of a value in a field | Finding the lowest
    value  | `SELECT MIN(field_name) FROM table_name` | `df.min()` |'
  prefs: []
  type: TYPE_TB
- en: '| Max | The highest numeric value of a value in a field | Finding the highest
    value  | `SELECT MAX(field_name) FROM table_name` | `df.max()` |'
  prefs: []
  type: TYPE_TB
- en: 'The most common statistical measure I use in SQL is *Count* where you are counting
    the number of records per table. Using this function helps to validate that the
    volume of data you are working with is in line with the source system, producers
    of data, and business sponsors. For example, you are told by the business sponsor
    that they use a database to store customers, products, and sales and they have
    over 30,000 customers. Let''s say you run the following SQL query:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'There are 90,000 results. Why is there such a dramatic difference? The first
    question would be: are you using the correct table? Any database is flexible so
    it can be organized by the DBA to manage relationships based on application and
    business needs, so active customers (customers who purchased a product and created
    sales data) could be stored in a different table, such as `active_customers`.
    Another question would be: is there a field used to identify whether the record
    is active or not? If so, that field should be included in the `WHERE` section
    of your `SELECT` statement, for example, `SELECT count(*) from customers where
    active_flag = true`.'
  prefs: []
  type: TYPE_NORMAL
- en: A second advantage of using the `count()` function for analysis is to set expectations
    for yourself as to how much time it takes for each query to return results. If
    you run a `count(*)` on products, customers, and sales tables, the amount of time
    taken to retrieve the results will vary depending on the volume of data and how
    the DBA has optimized the performance. Tables have shapes, which means the number
    of rows and columns will vary between them. They also can grow or shrink depending
    on their intended use. A table such as `sales` is transactional so the number
    of rows will dramatically increase over time. We can classify transaction tables
    as deep because the number of columns is minimal, but the number of rows will
    grow. Tables such as `customers` and `products` are known as reference tables,
    which are wide in shape because they could have dozens of columns with significantly
    fewer rows compared to transaction tables.
  prefs: []
  type: TYPE_NORMAL
- en: Tables with high numbers of rows and columns and densely-populated distinct
    values take up more disk space and require more memory and CPU to process. If
    the `sales` table has billons of rows, counting the number of rows could take
    hours waiting for the response from `SELECT count(*) from sales` and would be
    discouraged by the administrators/IT support team. I worked with a data engineering
    team that was able to retrieve SQL results in less than 10 seconds against a 100
    billion record table. That kind of response time requires developer expertise
    and administrative access to configure the table to support a super-fast response
    time.
  prefs: []
  type: TYPE_NORMAL
- en: Another valid point when dealing with the `count()` function is knowing the
    difference between frequency versus distinct values. Depending on which table
    you are performing a counting function against, you may be just counting the number
    of occurrences, or frequency of records. For the 30,000 customers example, if
    there is a difference in the results between `count(customer_id)` and `count(distinct
    customer_id)`, we know counting the records includes duplicate customers. This
    may not be an issue depending on the analysis you are performing. If you wanted
    to know how often a customer buys any product, then `counting(customer_id)` will
    answer that question. If you wanted to know how many customers are buying each
    product, using `distinct` would provide more accurate information.
  prefs: []
  type: TYPE_NORMAL
- en: The `sum()` function, which is short for summation, is another common measure
    used for statistical analysis in descriptive analytics. One key difference between
    counting versus summing would be that sum requires a number value to calculate
    accurate results whereas counting can be done against any data type. For example,
    you cannot and should not sum the `customer_name` field in the `customers` table
    because the data type is defined as a string. You can technically sum the `customer_id`
    field if it's defined as an integer, however, that would give you misleading information
    because that is not the intended use of the field. Like `count`, `sum` is an aggregate
    measure used to add together all of the values found in a specific field such
    as `sales_amount` or quantity from a `sales` table.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use the `sum()` function in SQL is easy. If you want to know the sum for
    all time with no constraints or conditions, use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: You can then add a condition such as only active customers by including the
    `WHERE` clause with the `flag` field, which has the following syntax: `SELECT
    sum(field_name) from table_name WHERE active_flg = TRUE`.
  prefs: []
  type: TYPE_NORMAL
- en: We will uncover more advanced features such as aggregation using SQL in [Chapter
    8](9bdac090-8534-480e-8154-a854115c0b7a.xhtml), *Understanding Joins, Relationships,
    and Aggregates*.
  prefs: []
  type: TYPE_NORMAL
- en: The mean or average function is another common statistical function very useful
    for data analysis, and it's easy to write the command using SQL. average is the
    sum of all values divided by the count with the syntax of `SELECT avg(field_name)
    from table_name`.
  prefs: []
  type: TYPE_NORMAL
- en: The denominator of counting values is using the frequency/number of occurrences
    versus distinct values so you should understand how the table is populated before
    running the SQL command. For example, a sales table is transaction-based with
    many customers and products so the average would be different from the average
    against the product or customer table because each record would be distinct.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `min` and `max` functions are also useful and easy to interpret using SQL.
    The built-in functions are `min()` and `max()`, which return the minimum numeric
    value from a population of data along with the maximum or highest value. A good
    business question to understand from your table would be what is the lowest and
    highest sales amount for 2018? The syntax in SQL would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This information would be useful to know to understand the range of sales per
    customer and product across all periods of time.
  prefs: []
  type: TYPE_NORMAL
- en: An important factor to recognize when running these statistical functions against
    your data is to understand when values are blank or what is commonly known as
    null. In SQL, `NULL` represents nothing and the nonexistence of a value. In RDBMS,
    null values are a rule when a DBA defines the schema for each table. During that
    process of creating columns by defining the data type for each field, there is
    an option to allow null values. The reasons vary by use case whether to allow
    nulls during the design of a database table, but what's important to understand
    for analysis is whether they exist in your data and how they should be treated.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start with an example from our `customers` table where one of the fields
    such as the second address line allows `NULL`, which is common. Why is this common?
    Because a second address field is optional and is not even used in many cases,
    but what if you are a company that needs to physically mail marketing materials
    or invoices to customers? If the data entry always required a value, it would
    unnecessarily populate a value in that second address field in the database, which
    is inefficient because it takes more time to enter a value for each customer and
    takes more storage space. In most cases, forcing a value in large-scale enterprise
    systems creates poor data quality, which then requires time to fix the data or
    creates confusion working with the data, especially working with millions of customers.
  prefs: []
  type: TYPE_NORMAL
- en: Metadata explained
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Metadata is commonly known as descriptive information about the data source.
    A key concept exposed in metadata analysis is related to understanding that nulls
    exist in databases. From a data analysis perspective, we need to make sure we
    understand how it impacts our analysis. In Python and other coding languages such
    as Java, you may see the word `NaN` returned. This is an acronym for *Not a Number*
    and helps you to understand that you may not be able to perform statistical calculations
    or functions against those values. In other cases such as Python, `NaN` values
    will have special functions to handle them, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: In NumPy, use the `nansum()` function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use pandas with the `isnull()` function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In SQL, use `is null` or `isnull` depending on the RDBMS you are working with
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since you are testing for a condition to exist, you can also include the keyword
    of `NOT` to test for the opposite, for example, `Select * from customer_table
    where customer_name is NOT null`.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding nulls and `NaN` boils down to KYD and metadata about the source
    datasets you are working with. If you don't have access to the database system
    to see the metadata and underlining schema, we can use pandas and DataFrames to
    gain some insights about SQL data. Let's walk through an example, by loading a
    single table from the database into a DataFrame in a notebook and run some metadata
    functions to gain more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin, create a new Jupyter notebook and name it `test_for_nulls_using_sql_and_pandas`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to the prior example, to load an SQLite database connection, you just
    need to add the following command in your Jupyter notebook and run the cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to assign a connection to a variable named `conn` and point to
    the location of the database file, which is named `customer_sales.db`. Since we
    already imported the `sqlite3` library in the prior `In[]` line, we can use this
    built-in function to communicate with the database:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the `pandas` library as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the `read_sql_query()` function, we assign the results to a new DataFrame
    as `df_customers` to make it easier to identify:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'To view the results, we can just run the `head()` command against this DataFrame
    using this code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The output would look like the following screenshot where the `tbl_customers` table has
    been loaded into a DataFrame with a labeled header row with the index column to
    the left starting with a value of `0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d95da4e6-ed39-443c-94bc-0895a5548000.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can profile the DataFrame and easily identify any `NULL` values using the
    following command. The `isnull()` pandas function tests for null values across
    the entire DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The output would look like the following screenshot where the DataFrame will
    return a `True` or `False` value rather than the actual value by the cell for
    each row and column:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b9fa1c24-fe8c-4668-aaa6-a5f90b5b4c7e.png)'
  prefs: []
  type: TYPE_IMG
- en: With a few commands, we learned how to communicate with databases and identify
    some important metadata about the data stored in tables. To continue improving
    our data literacy, let's understand how the data was populated into the database
    by understanding data lineage.
  prefs: []
  type: TYPE_NORMAL
- en: The importance of data lineage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Data lineage** is the ability to trace back the source of a dataset to how
    it was created. It is a fun topic for me because it typically requires investigating
    the history of how systems generate data, identifying how it was processed, and
    working with the people who produce and consume the data. This process helps to
    improve your data literacy, which is the ability to read, write, analyze, and
    argue with data because you can learn how the data impacts the organization. Is
    the data critical to business functions such as generating sales or was it created
    for compliance purposes? These types of questions should be answered by learning
    more about the lineage of the data.'
  prefs: []
  type: TYPE_NORMAL
- en: From experience, this process of tracing data lineage involves working sessions
    directly with the people who are responsible for the data and uncovering any documentation
    like an ERD demonstrated in the *From SQL to pandas DataFrames* section or help
    guides. In many cases, the documentation available for enterprise systems that
    have matured over time will not reflect the nuances that you will see when analyzing
    the data. For example, if a new field was created on an existing table that is
    populated from a web form that did not exist before, historical data will have
    `NULL` or `NaN` values until the point in time when the data entry started.
  prefs: []
  type: TYPE_NORMAL
- en: Data lineage can quickly become complex, which takes time to unwind and multiple
    resources to expose the details when not properly documented. When multiple systems
    are involved, working with **Subject Matter Experts** (**SMEs**) will fast track
    the process so you don't have to reverse engineer all of the steps in the data
    flow.
  prefs: []
  type: TYPE_NORMAL
- en: Data flow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Data flow** is a subset of data lineage that is typically part of a larger
    data governance strategy within large organizations so there may be existing tools
    or systems already in place that visually represent how the data is processed,
    which is commonly known as data flow. A hypothetical example of a data flow diagram
    would be the following diagram where we look at some of the data we have been
    working with in our exercises so far. In this diagram, we have a logical representation
    of how the `tbl_customers` table is populated from our SQLite database. I have
    documented the inputs and outputs as stages one to four:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/41e59c35-0b8b-4de8-8e0a-47bbee0751b7.png)'
  prefs: []
  type: TYPE_IMG
- en: The input stage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First, we have the **Input** stage, which is identified as the **Mobile App**,
    **Web App,** and **Client PC** systems. These systems have created feeds out into
    multiple file formats. In our example, this data is batch processed, where the
    data files are saved and sent out for the next stage.
  prefs: []
  type: TYPE_NORMAL
- en: The data ingestion stage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **Data Ingestion** stage is where multiple files such as `customers.json` and
    `customers.xml` are processed. Because this is a logical diagram rather than a
    highly technical one, the details behind what technologies are used to process
    the data ingestion are omitted. Data ingestion is also known as **ETL**, which
    is an acronym for **Extract, Transform, and Load**, which is automated and maintained
    by data engineering teams or developers.
  prefs: []
  type: TYPE_NORMAL
- en: We can see an intermediary step called `tbl_stage_customers` during this ETL,
    which is a landing table for processing the data between the source files and
    the target table in the database. Also included in this stage is an `ODBC` connection
    where the **Client PC** system has direct access to insert, update, and delete
    records from the `tbl_customers` table.
  prefs: []
  type: TYPE_NORMAL
- en: During the process of learning more about the data flow, be sure to ask whether
    the tables are defined with logical delete versus the physical deleting of rows.
    In most cases, the direct removal of rows in a table is not supported, so Boolean
    data type columns are used to indicate whether the record is active or flagged
    for deletion by the system or user.
  prefs: []
  type: TYPE_NORMAL
- en: The data source stage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The third stage is named **Data Source**, which is defined as the `tbl_customers` table.
    Some questions to ask the developer or DBA are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: What is your retention policy for this data/how long is the data preserved?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the average daily volume of records being populated in this table?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can they provide some metadata such as how many rows, columns, and data types
    for each field?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the dependencies/joins to this table including primary and foreign
    keys?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How often is this table backed up and is there system downtime we should be
    aware of that would impact analysis?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does a data dictionary exist for this table/database?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data target stage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The fourth stage, named **Data Target**, helps a data analyst to understand
    downstream dependencies from the source table. In this example, we have a **Sales
    Report**, the `compliance_feed.json` file, and **Jupyter Notebook**. Some useful
    information to uncover would be the frequency of how often that compliance feed
    is sent and who the consumers of that data are.
  prefs: []
  type: TYPE_NORMAL
- en: This may become important if the timing of your analysis is not in line with
    data feeds from the **Data Source** stage. Trust in your analysis and the ability
    to argue that your analysis is complete and accurate comes from understanding
    timing issues and your ability to reconcile and match counts between multiple
    data-target outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Business rules
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another important point about data lineage is to uncover business rules, lookup
    values, or mapping reference sources. A business rule is an abstract concept that
    helps you to understand software code that is applied during data processing.
    An example would be when the user of the **Mobile App** clicks a **Submit** button,
    a new `customers.json` file is created. Business rules can also be more complex,
    such as `tbl_stage_customers` table does not populate records in the `tbl_customers` until
    all source files are received and a batch process runs at 12 A.M. EST daily. Business
    rules may be explicitly defined in the database during the creation of a database
    table such as the rule to define a primary key on a column, coded on a web form
    or mobile application.
  prefs: []
  type: TYPE_NORMAL
- en: Documenting these business rules should be included in your methodology to support
    your analysis. This helps you to argue insights from your data analysis by either
    verifying the existence of the business rule or identifying outliers that contradict
    assumptions made about the source data. For example, if you were told a database
    table was created to not allow `NULL` in specific fields but you end up finding
    it, you can review your findings with the DBA to uncover how this occurred. It
    could have easily been a business exception that was created or that the enforcement
    of the business rule was implemented after the table was already populated.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding business rules helps to identify data gaps and verifies accuracy
    during analysis. If the average daily volume of records for this table drops to
    zero records for multiple consecutive days, there might be an issue in stage 2
    during the **Data Ingestion** or it just might be a holiday where no customer
    records were received and processed.
  prefs: []
  type: TYPE_NORMAL
- en: In either case, learning how to ask these questions of the subject matter experts
    and verifying the data lineage will build confidence in your analysis and trust
    with both producers and consumers of the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you understand all of the concepts, let''s walk through the data lineage
    of the data we are working with in this chapter—`customer_sales.db`:'
  prefs: []
  type: TYPE_NORMAL
- en: In the **Input** stage for this database, three source CSV files were manually
    created for example purposes. Each source table has a one-for-one match with a
    CSV file named `tbl_customers`, `tbl_products`, and `tbl_sales`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the **Data Ingestion** stage, each file was imported using a few SQL commands,
    which created the schema for each table (the field names, defined data types,
    and join relationships). This process is commonly known as an ETL where the source
    data is ingested and persisted as tables in the database. If any changes between
    the source files and the target database table are required, a business rule should
    be documented to help to provide transparency between the producers and consumers
    of the data. For this example, the source and target match.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **Data Source** stage in this example would be `customer_sales.db`. This
    now becomes the golden copy for data flowing out of the database for analysis
    and any reporting.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The **Target** stage in our example would be the Jupyter notebook and the creation
    of DataFrames for analysis.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: While this is a small example with only a few steps, the concepts apply to large-scale
    enterprise solutions with many more data sources and technologies used to automate
    the data flow. I commonly sketch out the stages for data lineage before doing
    any data analysis to ensure I understand the complete process. This helps to communicate
    with stakeholders and SMEs to ensure accuracy in the insights you gain from data
    sources.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have covered a few key topics in this chapter to help you to improve your
    data literacy by learning about working with databases and using SQL. We learned
    about the history of SQL and the people who created the foundation for storing
    structured data in databases. We walked through some examples and how to insert
    records from a SQL `SELECT` statement into a `pandas` DataFrame for analysis.
  prefs: []
  type: TYPE_NORMAL
- en: By using the `pandas` library, we learned about how to sort, limit, and restrict
    data along with fundamental statistical functions such as counting, summing, and
    average. We covered how to identify and work with `NaN` (that is, nulls) in datasets
    along with the importance of data lineage during analysis.
  prefs: []
  type: TYPE_NORMAL
- en: In our next chapter, we will explore time series data and learn how to visualize
    your data using additional Python libraries to help to improve your data literacy
    skills.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are some links that you can refer to for more information on the relative
    topics of this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Historical details about how SQL was created: [http://www.contrib.andrew.cmu.edu/~shadow/sql/sql1992.txt ](http://www.contrib.andrew.cmu.edu/~shadow/sql/sql1992.txt)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling `NULL` values: [https://codeburst.io/understanding-null-undefined-and-nan-b603cb74b44c](https://codeburst.io/understanding-null-undefined-and-nan-b603cb74b44c)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling duplicate values with pandas: [https://www.python-course.eu/dealing_with_NaN_in_python.php](https://www.python-course.eu/dealing_with_NaN_in_python.php)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: About SQLite databases: [https://www.sqlite.org/about.html](https://www.sqlite.org/about.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data modeling techniques: [https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/](https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: pandas DataFrame functions: [https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
