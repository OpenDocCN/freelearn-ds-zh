- en: '*Chapter 14*: Data Processing with Apache Spark'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, you learned how to add streaming data to your data
    pipelines. Using Python or Apache NiFi, you can extract, transform, and load streaming
    data. However, to perform transformations on large amounts of streaming data,
    data engineers turn to tools such as Apache Spark. Apache Spark is faster than
    most other methods – such as MapReduce on non-trivial transformations – and it
    allows distributed data processing.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Installing and running Spark
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing and configuring PySpark
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing data with PySpark
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing and running Spark
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Apache Spark is a distributed data processing engine that can handle both streams
    and batch data, and even graphs. It has a core set of components and other libraries
    that are used to add functionality. A common depiction of the Spark ecosystem
    is shown in the following diagram:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.1 – The Apache Spark ecosystem'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_14.1_B15739.jpg)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.1 – The Apache Spark ecosystem
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: To run Spark as a cluster, you have several options. Spark can run in a standalone
    mode, which uses a simple cluster manager provided by Spark. It can also run on
    an Amazon EC2 instance, using YARN, Mesos, or Kubernetes. In a production environment
    with a significant workload, you would probably not want to run in standalone
    mode; however, this is how we will stand up our cluster in this chapter. The principles
    will be the same, but the standalone cluster provides the fastest way to get you
    up and running without needing to dive into more complicated infrastructure.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 'To install Apache Spark, take the following steps:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Browse to the website at [http://spark.apache.org](http://spark.apache.org).
    From there, you can keep up to date with new versions of Apache Spark, read the
    documentation, learn about the libraries, and find code examples:![Figure 14.2
    – The Apache Spark website
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_14.2_B15739.jpg)'
  id: totrans-14
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 14.2 – The Apache Spark website
  id: totrans-15
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: From the website, select the **Download** menu option. Choose the version of
    Spark you want to use – at the time of writing, the newest version is 3.0.0\.
    You will be asked to choose a package type. We will not be using Hadoop, but have
    to select a version or provide our own. I have chosen **Pre-built for Apache Hadoop
    2.7**. On Windows, you may need to trick the operating system into thinking Hadoop
    is installed by setting environment variables, but on Linux and macOS, this should
    not be an issue. The download options are shown in the following screenshot:![Figure
    14.3 – Downloading Apache Spark for Hadoop 2.7
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_14.3_B15739.jpg)'
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 14.3 – Downloading Apache Spark for Hadoop 2.7
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'After downloading the file, you will extract it, then move it to the home directory
    in a directory named `spark3`. You can do this using the following commands:'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, you will need to make a cluster. As you did with Kafka, you will make
    a copy of the Spark directory on the same machine and make it act as another node.
    If you have another machine, you could also put another copy of Spark on that
    server. Copy the directory and rename it `spark-node`, as shown:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To run the Spark cluster, you can use the provided scripts. The scripts to
    run the cluster use outdated terminology – `master` and `slave`. This language
    is common in the technology space; however, there have been many voices opposed
    to it for a long time. Finally, it appears that there is some traction being made
    in removing this language as GitHub will remove `master` from the branch names.
    I too have renamed the scripts, using the terms `head` and `node`. To do this,
    use the following commands:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To start the cluster, you can now run the scripts as shown:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You can pass parameters to the scripts, and in the preceding command, you pass
    the port flag (`-p`) to tell the script which port you want the node to run on.
    You can also pass the following:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'a) `-h, --host`: The hostname to run on. The `i, -ip` flag has been deprecated.'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b) `-p, --port`: The port to listen on.'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'c) `--webui-port`: The port for the web GUI, which defaults to `8080`.'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'd) `-c, --cores`: The number of cores to use.'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'e) `-m, --memory`: The amount of memory to use. By default, it is 1 gigabyte
    less than your full memory.'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'f) `-d, --work-dir`: The scratch space directory for the worker only.'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'g) `--properties-file`: This is where you can specify several of these flags
    in a `spark.conf` file.'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The cluster will take a minute to load, and when it has finished, you can browse
    to the web UI at [http://localhost:8080/](http://localhost:8080/). You will see
    the details of your cluster and it will look as in the following screenshot:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.4 – Spark cluster web UI'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_14.4_B15739.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.4 – Spark cluster web UI
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: With the cluster up and running, you will need to set up the Python environment
    so that you can code against it. The next section will walk you through those
    steps.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Installing and configuring PySpark
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'PySpark is installed with Spark. You can see it in the `~/spark3/bin` directory,
    as well as other libraries and tools. To configure PySpark to run, you need to
    export environment variables. The variables are shown here:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The preceding command set the `SPARK_HOME` variable. This will be where you
    installed Spark. I have pointed the variable to the head of the Spark cluster
    because the node would really be on another machine. Then, it adds `SPARK_HOME`
    to your path. This means that when you type a command, the operating system will
    look for it in the directories specified in your path, so now it will search `~/spark3/bin`,
    which is where PySpark lives.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: 'Running the preceding commands in a terminal will allow Spark to run while
    the terminal is open. You will have to rerun these commands every time. To make
    them permanent, you can add the commands to your `~/.bashrc` file. After saving
    the `.bashrc` file, you need to reload it. You can do that by running the following
    command:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You should now be able to open a terminal and run PySpark, and the result will
    be the PySpark interactive shell, as shown:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.5 – The interactive Spark shell'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_14.5_B15739.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.5 – The interactive Spark shell
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: 'If you see the preceding screenshot, congratulations, you have PySpark configured.
    In this chapter, the examples will use PySpark in Jupyter notebooks. There are
    two ways to configure PySpark to work with Jupyter:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '`PYSPARK_DRIVER_PYTHON` environment variable and the `_OPTS` variable to the
    Jupyter Notebook using the following commands (add this to `~/.bashrc` if you
    want it to be permanent):'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`findspark` library and add code to your Jupyter notebook that gets the Spark
    information as it runs. The examples in this chapter will use this method. You
    can install `findspark` using `pip`, as shown:'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: import findspark
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: findspark.init()
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: You can now run PySpark code on your Spark cluster. The next section will walk
    you through some basic PySpark examples.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Processing data with PySpark
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before processing data with PySpark, let''s run one of the samples to show
    how Spark works. Then, we will skip the boilerplate in later examples and focus
    on data processing. The Jupyter notebook for the **Pi Estimation** example from
    the Spark website at [http://spark.apache.org/examples.html](http://spark.apache.org/examples.html)
    is shown in the following screenshot:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.6 – The Pi Estimation example in a Jupyter notebook'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_14.6_B15739.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.6 – The Pi Estimation example in a Jupyter notebook
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: 'The example from the website will not run without some modifications. In the
    following points, I will walk through the cells:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: 'The first cell imports `findspark` and runs the `init()` method. This was explained
    in the preceding section as the preferred method to include PySpark in Jupyter
    notebooks. The code is as follows:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The next cell imports the `pyspark` library and `SparkSession`. It then creates
    the session by passing the head node of the Spark cluster. You can get the URL
    from the Spark web UI – you also used it to start the worker node:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Running the first two cells, you can browse to the Spark GUI and see that there
    is a task running. The running task is shown in the following screenshot – notice
    the name of the worker is `Pi-Estimation`, which is the `appName` parameter in
    the preceding code:![Figure 14.7 – The Spark web UI with a running session and
    two completed sessions
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_14.7_B15739.jpg)'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 14.7 – The Spark web UI with a running session and two completed sessions
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The preceding code will be used in all of your Spark code. It is the boilerplate
    code.
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The next cell contains the work. The code that follows will estimate the value
    of pi. The details of the code are not important, but notice that the `count`
    variable uses `sparkContext` and parallelizes a task on the cluster. After the
    boilerplate, your Spark code will execute a task and get the results:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Lastly, stop the session:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Once the session has been stopped, it will show up as a completed application
    in the web UI. The next section will use Spark with DataFrames and send the data
    to Kafka.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Spark for data engineering
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The previous section showed the structure of a Spark application: we used `findspark`
    to get the paths, imported the libraries, created a session, did something, and
    stopped the session. When you do something, it will most likely involve a Spark
    DataFrame. This section will provide a brief overview of how Spark DataFrames
    work – it is slightly different than `pandas`.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing you have to do is use `findspark` to set up the environment.
    Then, you can import the required libraries. Then, create the session. The following
    code shows the boilerplate to get set up:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now you are connected to a session on the Spark cluster. You can read CSV and
    JSON data just like you did with DataFrames in [*Chapter 3*](B15739_03_ePub_AM.xhtml#_idTextAnchor039)*,
    Reading and Writing Files* and [*Chapter 4*](B15739_04_ePub_AM.xhtml#_idTextAnchor049)*,
    Working with Databases*, with some slight modifications. When you read in the
    data, you can use `read.csv` instead of `read_csv` in `pandas`. Another difference
    between Spark and `pandas` is the use of `.show()` in Spark to see the DataFrame.
    In `pandas`, you can view `dtypes` of a DataFrame, and in Spark, you can do the
    same using `printSchema()`. The following code reads the `data.csv` file and prints
    the top five rows and the schema:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output will be a DataFrame like the one shown in the following screenshot:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.8 – DataFrame from CSV with schema'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_14.8_B15739.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.8 – DataFrame from CSV with schema
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: 'You will notice that the headers are the first row and there are default `_c0`
    column names. The output also shows that all the columns are strings. You can
    specify a schema and pass it as a parameter; however, you can also tell Spark
    to infer the schema. The following code passes that there are headers and tells
    Spark to infer the schema:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The results are what you expected: a DataFrame with the correct types. The
    following screenshot shows the results:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.9 – DataFrame with headers and correct types'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_14.9_B15739.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.9 – DataFrame with headers and correct types
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: 'You can select a column by using `select()` and passing the column name as
    a parameter. Don''t forget to add `.show()` or it will return a DataFrame and
    not display it:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'You will notice that in `pandas`, you would have used `[]` and a column name
    and also did not need the `select` method. In `pandas`, you could also filter
    a DataFrame by using the `df[(df[''field'']< value)]` format. In Spark, you can
    use `select` and `filter` to do the same, the difference being that a `select`
    method returns `True` and `False` for a condition, and `filter` will return the
    DataFrame for that condition. With `filter`, you can also add a `select` method
    and pass an array of columns to return. The code is shown as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到在 `pandas` 中，你会使用 `[]` 和列名，并且不需要 `select` 方法。在 `pandas` 中，你也可以使用 `df[(df['field']<
    value)]` 格式来过滤 DataFrame。在 Spark 中，你可以使用 `select` 和 `filter` 来做同样的事情，区别在于 `select`
    方法对于一个条件返回 `True` 和 `False`，而 `filter` 将返回该条件的 DataFrame。使用 `filter`，你还可以添加一个
    `select` 方法并传递一个列名的数组来返回。代码如下所示：
- en: '[PRE17]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Notice that in the last line, you didn''t use `df[''age'']` but were able to
    just pass the column name. When you want to iterate through a DataFrame, you could
    use `iterrows` in `pandas`. In Spark, you create an array of rows using `collect()`.
    The following code will use a `filter` method to get all people under 40 and print
    the array:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在最后一行中，你没有使用 `df['age']`，而是直接传递了列名。当你想要遍历 DataFrame 时，你可以在 `pandas` 中使用 `iterrows`。在
    Spark 中，你使用 `collect()` 创建一个行数组。以下代码将使用 `filter` 方法获取所有 40 岁以下的人并打印数组：
- en: '[PRE18]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'To get a single row, you can just pass the index. You can convert the row into
    different formats, and in this example, I converted it into a dictionary. As a
    dictionary, you can select any value by specifying the key. The code is shown
    as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取单行，你只需传递索引。你可以将行转换为不同的格式，在这个例子中，我将它转换成了字典。作为字典，你可以通过指定键来选择任何值。代码如下所示：
- en: '[PRE19]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The output of the preceding code is a `Row` object, a dictionary, and a string
    of the value for the key name, as shown:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码的输出是一个 `Row` 对象、一个字典和键名的值字符串，如下所示：
- en: '[PRE20]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'To iterate through the DataFrame in Spark, you call `collect()`, and then iterate
    through the array using a `for` loop. You can then convert each of the rows into
    a dictionary and do what you need for processing. The following code snippet prints
    the dictionary:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 Spark 中遍历 DataFrame，你调用 `collect()`，然后使用 `for` 循环遍历数组。然后你可以将每一行转换为字典，并对其进行处理。以下代码片段打印了字典：
- en: '[PRE21]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'If you are more comfortable with SQL, you can filter a DataFrame using `spark.sql`.
    To use SQL, you must first create a view, then you can query it with SQL, as shown
    in the following code:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你更熟悉 SQL，你可以使用 `spark.sql` 来过滤 DataFrame。要使用 SQL，你必须首先创建一个视图，然后你可以用 SQL 查询它，如下面的代码所示：
- en: '[PRE22]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The results will be the same DataFrame as the `filter` method, but just a different
    method by which to achieve the same result.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将与 `filter` 方法相同，但只是达到相同结果的不同方法。
- en: 'There are several functions to perform modifications or analysis on columns
    or data in a DataFrame. In Spark, you can use `describe()` to get a basic summary
    of the data in a column. The following code uses it on the `age` column:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个函数可以对 DataFrame 中的列或数据进行修改或分析。在 Spark 中，你可以使用 `describe()` 来获取列中数据的简要概述。以下代码在
    `age` 列上使用它：
- en: '[PRE23]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The output is the common descriptive statistics of `count`, `mean`, `standard
    deviation`, `min`, and `max`. These five statistics give you a good overview of
    the data.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是常见的描述性统计，包括 `count`、`mean`、`standard deviation`、`min` 和 `max`。这五个统计量为你提供了数据的良好概述。
- en: 'You can also group and aggregate your data just like in `pandas`. To group
    the counts of states, you can use `groupBy()`, as shown:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以像在 `pandas` 中一样对数据进行分组和聚合。要按州分组计数，你可以使用 `groupBy()`，如下所示：
- en: '[PRE24]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Aggregation allows you to pass in a dictionary of the field and a method. To
    calculate the mean of the `age` column, you would use the following code:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合允许你传入一个字段和方法的字典。要计算 `age` 列的平均值，你会使用以下代码：
- en: '[PRE25]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'For both `groupBy` and `agg`, you can use `mean`, `max`, `min`, `sum`, and
    other methods that you can read about in the documentation. There is a large number
    of other functions you can use that require you to import the `pyspark.sql.functions`
    module. The following code imports it as `f` and demonstrates some useful functions.
    Again, for more information on all of the functions, you can read the Python API
    documents at [https://spark.apache.org/docs/latest/api/python/pyspark.sql.html](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html):'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 `groupBy` 和 `agg`，您可以使用 `mean`、`max`、`min`、`sum` 以及您可以在文档中阅读到的其他方法。您可以使用大量其他函数，这些函数需要您导入
    `pyspark.sql.functions` 模块。以下代码将其导入为 `f` 并演示了一些有用的函数。同样，有关所有函数的更多信息，您可以在 [https://spark.apache.org/docs/latest/api/python/pyspark.sql.html](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html)
    阅读Python API 文档：
- en: '[PRE26]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'When you have finished your data processing, stop the session using `stop()`,
    as shown:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 当您完成数据处理后，使用 `stop()` 停止会话，如下所示：
- en: '[PRE27]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Congratulations! You have successfully processed data with PySpark.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！您已成功使用 PySpark 处理数据。
- en: Summary
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned the basics of working with Apache Spark. First,
    you downloaded and installed Spark and configured PySpark to run in Jupyter notebooks.
    You also learned how to scale Spark horizontally by adding nodes. Spark uses DataFrames
    similar to those used in `pandas`. The last section taught you the basics of manipulating
    data in Spark.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您学习了使用 Apache Spark 的基础知识。首先，您下载并安装了 Spark，并配置 PySpark 在 Jupyter 笔记本中运行。您还学习了如何通过添加节点来水平扩展
    Spark。Spark 使用类似于 `pandas` 中使用的 DataFrame。最后一节教您了在 Spark 中操作数据的基本方法。
- en: In the next chapter, you will use Spark with Apache MiNiFi to move data at the
    edge or on Internet-of-Things devices.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，您将使用 Spark 与 Apache MiNiFi 一起在边缘或物联网设备上移动数据。
