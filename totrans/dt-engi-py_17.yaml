- en: '*Chapter 14*: Data Processing with Apache Spark'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第 14 章*：使用 Apache Spark 进行数据处理'
- en: In the previous chapter, you learned how to add streaming data to your data
    pipelines. Using Python or Apache NiFi, you can extract, transform, and load streaming
    data. However, to perform transformations on large amounts of streaming data,
    data engineers turn to tools such as Apache Spark. Apache Spark is faster than
    most other methods – such as MapReduce on non-trivial transformations – and it
    allows distributed data processing.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，您学习了如何将流数据添加到您的数据管道中。使用 Python 或 Apache NiFi，您可以提取、转换和加载数据。然而，为了对大量流数据进行转换，数据工程师转向
    Apache Spark 等工具。Apache Spark 比大多数其他方法（如非平凡转换上的 MapReduce）都要快，并且它允许分布式数据处理。
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Installing and running Spark
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装和运行 Spark
- en: Installing and configuring PySpark
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装和配置 PySpark
- en: Processing data with PySpark
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 PySpark 处理数据
- en: Installing and running Spark
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装和运行 Spark
- en: 'Apache Spark is a distributed data processing engine that can handle both streams
    and batch data, and even graphs. It has a core set of components and other libraries
    that are used to add functionality. A common depiction of the Spark ecosystem
    is shown in the following diagram:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark 是一个可以处理流和批量数据，甚至图数据的分布式数据处理引擎。它有一组核心组件和其他用于添加功能的库。以下图表显示了 Spark
    生态系统的常见表示：
- en: '![Figure 14.1 – The Apache Spark ecosystem'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 14.1 – Apache Spark 生态系统'
- en: '](img/Figure_14.1_B15739.jpg)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_14.1_B15739.jpg)'
- en: Figure 14.1 – The Apache Spark ecosystem
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.1 – Apache Spark 生态系统
- en: To run Spark as a cluster, you have several options. Spark can run in a standalone
    mode, which uses a simple cluster manager provided by Spark. It can also run on
    an Amazon EC2 instance, using YARN, Mesos, or Kubernetes. In a production environment
    with a significant workload, you would probably not want to run in standalone
    mode; however, this is how we will stand up our cluster in this chapter. The principles
    will be the same, but the standalone cluster provides the fastest way to get you
    up and running without needing to dive into more complicated infrastructure.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 要以集群模式运行 Spark，您有几个选项。Spark 可以在独立模式下运行，这使用 Spark 提供的简单集群管理器。它也可以在 Amazon EC2
    实例上运行，使用 YARN、Mesos 或 Kubernetes。在生产环境中，如果您的工作负载很大，您可能不想在独立模式下运行；然而，在本章中，我们将这样建立我们的集群。原则将是相同的，但独立集群提供了最快的启动和运行方式，无需深入研究更复杂的基础设施。
- en: 'To install Apache Spark, take the following steps:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装 Apache Spark，请按照以下步骤操作：
- en: Browse to the website at [http://spark.apache.org](http://spark.apache.org).
    From there, you can keep up to date with new versions of Apache Spark, read the
    documentation, learn about the libraries, and find code examples:![Figure 14.2
    – The Apache Spark website
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 访问 [http://spark.apache.org](http://spark.apache.org) 网站浏览。从这里，您可以了解 Apache
    Spark 的新版本，阅读文档，了解库，并查找代码示例：![图 14.2 – Apache Spark 网站
- en: '](img/Figure_14.2_B15739.jpg)'
  id: totrans-14
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/Figure_14.2_B15739.jpg)'
- en: Figure 14.2 – The Apache Spark website
  id: totrans-15
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 14.2 – Apache Spark 网站
- en: From the website, select the **Download** menu option. Choose the version of
    Spark you want to use – at the time of writing, the newest version is 3.0.0\.
    You will be asked to choose a package type. We will not be using Hadoop, but have
    to select a version or provide our own. I have chosen **Pre-built for Apache Hadoop
    2.7**. On Windows, you may need to trick the operating system into thinking Hadoop
    is installed by setting environment variables, but on Linux and macOS, this should
    not be an issue. The download options are shown in the following screenshot:![Figure
    14.3 – Downloading Apache Spark for Hadoop 2.7
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从网站中选择**下载**菜单选项。选择您想要使用的 Spark 版本 – 在撰写本文时，最新版本是 3.0.0。您将被要求选择一个包类型。我们不会使用
    Hadoop，但必须选择一个版本或提供自己的。我选择了**为 Apache Hadoop 2.7 预构建**。在 Windows 上，您可能需要通过设置环境变量来欺骗操作系统认为已安装了
    Hadoop，但在 Linux 和 macOS 上，这应该不会是问题。下载选项如下所示：![图 14.3 – 为 Hadoop 2.7 下载 Apache
    Spark
- en: '](img/Figure_14.3_B15739.jpg)'
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/Figure_14.3_B15739.jpg)'
- en: Figure 14.3 – Downloading Apache Spark for Hadoop 2.7
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 14.3 – 为 Hadoop 2.7 下载 Apache Spark
- en: 'After downloading the file, you will extract it, then move it to the home directory
    in a directory named `spark3`. You can do this using the following commands:'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载文件后，您将提取它，然后将其移动到名为 `spark3` 的目录中的主目录。您可以使用以下命令完成此操作：
- en: '[PRE0]'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, you will need to make a cluster. As you did with Kafka, you will make
    a copy of the Spark directory on the same machine and make it act as another node.
    If you have another machine, you could also put another copy of Spark on that
    server. Copy the directory and rename it `spark-node`, as shown:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，您需要创建一个集群。就像您对Kafka所做的那样，您将在同一台机器上复制Spark目录，并使其作为另一个节点运行。如果您还有另一台机器，您也可以在该服务器上放置Spark的另一个副本。复制目录并将其重命名为`spark-node`，如下所示：
- en: '[PRE1]'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To run the Spark cluster, you can use the provided scripts. The scripts to
    run the cluster use outdated terminology – `master` and `slave`. This language
    is common in the technology space; however, there have been many voices opposed
    to it for a long time. Finally, it appears that there is some traction being made
    in removing this language as GitHub will remove `master` from the branch names.
    I too have renamed the scripts, using the terms `head` and `node`. To do this,
    use the following commands:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要运行Spark集群，您可以使用提供的脚本。运行集群的脚本使用过时的术语 – `master`和`slave`。这种语言在技术空间中很常见；然而，长期以来，很多人反对使用这些术语。最终，似乎有一些进展正在被做出，GitHub将从分支名称中移除`master`。我也已将脚本重命名，使用术语`head`和`node`。要这样做，请使用以下命令：
- en: '[PRE2]'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To start the cluster, you can now run the scripts as shown:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要启动集群，您现在可以像下面这样运行脚本：
- en: '[PRE3]'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You can pass parameters to the scripts, and in the preceding command, you pass
    the port flag (`-p`) to tell the script which port you want the node to run on.
    You can also pass the following:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以向脚本传递参数，在先前的命令中，您通过传递端口标志（`-p`）来告诉脚本您希望节点在哪个端口上运行。您还可以传递以下参数：
- en: 'a) `-h, --host`: The hostname to run on. The `i, -ip` flag has been deprecated.'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'a) `-h, --host`: 要运行的计算机名。`i, -ip`标志已被弃用。'
- en: 'b) `-p, --port`: The port to listen on.'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'b) `-p, --port`: 要监听的端口。'
- en: 'c) `--webui-port`: The port for the web GUI, which defaults to `8080`.'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'c) `--webui-port`: Web GUI的端口，默认为`8080`。'
- en: 'd) `-c, --cores`: The number of cores to use.'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'd) `-c, --cores`: 要使用的核心数。'
- en: 'e) `-m, --memory`: The amount of memory to use. By default, it is 1 gigabyte
    less than your full memory.'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'e) `-m, --memory`: 要使用的内存量。默认情况下，它比您的全部内存少1吉字节。'
- en: 'f) `-d, --work-dir`: The scratch space directory for the worker only.'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'f) `-d, --work-dir`: 工作节点的临时空间目录。'
- en: 'g) `--properties-file`: This is where you can specify several of these flags
    in a `spark.conf` file.'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'g) `--properties-file`: 您可以在`spark.conf`文件中指定这些标志中的几个。'
- en: 'The cluster will take a minute to load, and when it has finished, you can browse
    to the web UI at [http://localhost:8080/](http://localhost:8080/). You will see
    the details of your cluster and it will look as in the following screenshot:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 集群将花费一分钟来加载，加载完成后，您可以浏览到Web UI，网址为[http://localhost:8080/](http://localhost:8080/)。您将看到集群的详细信息，它将如下截图所示：
- en: '![Figure 14.4 – Spark cluster web UI'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '![图14.4 – Spark集群Web UI'
- en: '](img/Figure_14.4_B15739.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_14.4_B15739.jpg)'
- en: Figure 14.4 – Spark cluster web UI
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.4 – Spark集群Web UI
- en: With the cluster up and running, you will need to set up the Python environment
    so that you can code against it. The next section will walk you through those
    steps.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 当集群启动并运行时，您需要设置Python环境，以便您可以针对它进行编码。下一节将指导您完成这些步骤。
- en: Installing and configuring PySpark
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装和配置PySpark
- en: 'PySpark is installed with Spark. You can see it in the `~/spark3/bin` directory,
    as well as other libraries and tools. To configure PySpark to run, you need to
    export environment variables. The variables are shown here:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark与Spark一起安装。您可以在`~/spark3/bin`目录中看到它，以及其他库和工具。要配置PySpark以运行，您需要导出环境变量。变量如下所示：
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The preceding command set the `SPARK_HOME` variable. This will be where you
    installed Spark. I have pointed the variable to the head of the Spark cluster
    because the node would really be on another machine. Then, it adds `SPARK_HOME`
    to your path. This means that when you type a command, the operating system will
    look for it in the directories specified in your path, so now it will search `~/spark3/bin`,
    which is where PySpark lives.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 先前的命令设置了`SPARK_HOME`变量。这将是指定Spark安装的位置。我已经将变量指向Spark集群的头部，因为节点实际上将位于另一台机器上。然后，它将`SPARK_HOME`添加到您的路径中。这意味着当您输入命令时，操作系统将在您路径中指定的目录中查找它，因此现在它将搜索`~/spark3/bin`，这是PySpark所在的位置。
- en: 'Running the preceding commands in a terminal will allow Spark to run while
    the terminal is open. You will have to rerun these commands every time. To make
    them permanent, you can add the commands to your `~/.bashrc` file. After saving
    the `.bashrc` file, you need to reload it. You can do that by running the following
    command:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在终端中运行前面的命令将允许 Spark 在终端打开时运行。您每次都必须重新运行这些命令。要使它们永久，可以将命令添加到您的 `~/.bashrc` 文件中。保存
    `.bashrc` 文件后，您需要重新加载它。您可以通过运行以下命令来完成此操作：
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You should now be able to open a terminal and run PySpark, and the result will
    be the PySpark interactive shell, as shown:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在应该能够打开一个终端并运行 PySpark，结果将是 PySpark 交互式 shell，如下所示：
- en: '![Figure 14.5 – The interactive Spark shell'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 14.5 – 交互式 Spark shell'
- en: '](img/Figure_14.5_B15739.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_14.5_B15739.jpg)'
- en: Figure 14.5 – The interactive Spark shell
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.5 – 交互式 Spark shell
- en: 'If you see the preceding screenshot, congratulations, you have PySpark configured.
    In this chapter, the examples will use PySpark in Jupyter notebooks. There are
    two ways to configure PySpark to work with Jupyter:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您看到了前面的截图，恭喜您，您已经配置好了 PySpark。在本章中，示例将使用 Jupyter notebooks 中的 PySpark。有两种方法可以配置
    PySpark 以与 Jupyter 一起工作：
- en: '`PYSPARK_DRIVER_PYTHON` environment variable and the `_OPTS` variable to the
    Jupyter Notebook using the following commands (add this to `~/.bashrc` if you
    want it to be permanent):'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用以下命令将 `PYSPARK_DRIVER_PYTHON` 环境变量和 `_OPTS` 变量添加到 Jupyter Notebook 中（如果您想使其永久，请将其添加到
    `~/.bashrc`）：
- en: '[PRE6]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`findspark` library and add code to your Jupyter notebook that gets the Spark
    information as it runs. The examples in this chapter will use this method. You
    can install `findspark` using `pip`, as shown:'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `findspark` 库并将代码添加到您的 Jupyter notebook 中，以在运行时获取 Spark 信息。本章中的示例将使用此方法。您可以使用
    `pip` 安装 `findspark`，如下所示：
- en: '[PRE7]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: import findspark
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 导入 findspark
- en: findspark.init()
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: findspark.init()
- en: '[PRE8]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: You can now run PySpark code on your Spark cluster. The next section will walk
    you through some basic PySpark examples.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以在您的 Spark 集群上运行 PySpark 代码。下一节将带您了解一些基本的 PySpark 示例。
- en: Processing data with PySpark
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 PySpark 处理数据
- en: 'Before processing data with PySpark, let''s run one of the samples to show
    how Spark works. Then, we will skip the boilerplate in later examples and focus
    on data processing. The Jupyter notebook for the **Pi Estimation** example from
    the Spark website at [http://spark.apache.org/examples.html](http://spark.apache.org/examples.html)
    is shown in the following screenshot:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 PySpark 处理数据之前，让我们运行一个示例来展示 Spark 的工作原理。然后，在后续示例中我们将跳过模板代码，专注于数据处理。以下截图显示了
    Spark 网站上 [http://spark.apache.org/examples.html](http://spark.apache.org/examples.html)
    的 **Pi 估计** 示例的 Jupyter notebook：
- en: '![Figure 14.6 – The Pi Estimation example in a Jupyter notebook'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 14.6 – Jupyter notebook 中的 Pi 估计示例'
- en: '](img/Figure_14.6_B15739.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_14.6_B15739.jpg)'
- en: Figure 14.6 – The Pi Estimation example in a Jupyter notebook
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.6 – Jupyter notebook 中的 Pi 估计示例
- en: 'The example from the website will not run without some modifications. In the
    following points, I will walk through the cells:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 网站上的示例未经修改将无法运行。在以下要点中，我将带您浏览单元格：
- en: 'The first cell imports `findspark` and runs the `init()` method. This was explained
    in the preceding section as the preferred method to include PySpark in Jupyter
    notebooks. The code is as follows:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一个单元格导入 `findspark` 并运行 `init()` 方法。这在前一节中已解释为在 Jupyter notebooks 中包含 PySpark
    的首选方法。代码如下：
- en: '[PRE9]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The next cell imports the `pyspark` library and `SparkSession`. It then creates
    the session by passing the head node of the Spark cluster. You can get the URL
    from the Spark web UI – you also used it to start the worker node:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一个单元格导入 `pyspark` 库和 `SparkSession`。然后通过传递 Spark 集群的头部节点来创建会话。您可以从 Spark web
    UI 获取 URL – 您也用它来启动工作节点：
- en: '[PRE10]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Running the first two cells, you can browse to the Spark GUI and see that there
    is a task running. The running task is shown in the following screenshot – notice
    the name of the worker is `Pi-Estimation`, which is the `appName` parameter in
    the preceding code:![Figure 14.7 – The Spark web UI with a running session and
    two completed sessions
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行前两个单元格，您可以浏览到 Spark GUI 并看到有一个任务正在运行。运行的任务如下截图所示 – 注意工作节点的名称是 `Pi-Estimation`，这是前面代码中的
    `appName` 参数：![图 14.7 – 带有运行会话和两个完成会话的 Spark web UI
- en: '](img/Figure_14.7_B15739.jpg)'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/Figure_14.7_B15739.jpg)'
- en: Figure 14.7 – The Spark web UI with a running session and two completed sessions
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 14.7 – 带有运行会话和两个完成会话的 Spark web UI
- en: The preceding code will be used in all of your Spark code. It is the boilerplate
    code.
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前面的代码将在您的所有 Spark 代码中使用。这是模板代码。
- en: 'The next cell contains the work. The code that follows will estimate the value
    of pi. The details of the code are not important, but notice that the `count`
    variable uses `sparkContext` and parallelizes a task on the cluster. After the
    boilerplate, your Spark code will execute a task and get the results:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一个单元包含工作内容。下面的代码将估算 π 的值。代码的细节并不重要，但请注意，`count` 变量使用 `sparkContext` 并在集群上并行化一个任务。在样板代码之后，您的
    Spark 代码将执行一个任务并获取结果：
- en: '[PRE11]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Lastly, stop the session:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，停止会话：
- en: '[PRE12]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Once the session has been stopped, it will show up as a completed application
    in the web UI. The next section will use Spark with DataFrames and send the data
    to Kafka.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦会话停止，它将在 web UI 中显示为一个已完成的应用程序。下一节将使用 Spark 和 DataFrame 将数据发送到 Kafka。
- en: Spark for data engineering
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据工程中的 Spark
- en: 'The previous section showed the structure of a Spark application: we used `findspark`
    to get the paths, imported the libraries, created a session, did something, and
    stopped the session. When you do something, it will most likely involve a Spark
    DataFrame. This section will provide a brief overview of how Spark DataFrames
    work – it is slightly different than `pandas`.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 上一节展示了 Spark 应用的结构：我们使用 `findspark` 获取路径，导入库，创建会话，执行一些操作，然后停止会话。当您执行某些操作时，它很可能涉及到
    Spark DataFrame。本节将简要概述 Spark DataFrame 的工作原理——它与 `pandas` 略有不同。
- en: 'The first thing you have to do is use `findspark` to set up the environment.
    Then, you can import the required libraries. Then, create the session. The following
    code shows the boilerplate to get set up:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 您必须做的第一件事是使用 `findspark` 设置环境。然后，您可以导入所需的库。然后，创建会话。以下代码显示了设置所需的样板代码：
- en: '[PRE13]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now you are connected to a session on the Spark cluster. You can read CSV and
    JSON data just like you did with DataFrames in [*Chapter 3*](B15739_03_ePub_AM.xhtml#_idTextAnchor039)*,
    Reading and Writing Files* and [*Chapter 4*](B15739_04_ePub_AM.xhtml#_idTextAnchor049)*,
    Working with Databases*, with some slight modifications. When you read in the
    data, you can use `read.csv` instead of `read_csv` in `pandas`. Another difference
    between Spark and `pandas` is the use of `.show()` in Spark to see the DataFrame.
    In `pandas`, you can view `dtypes` of a DataFrame, and in Spark, you can do the
    same using `printSchema()`. The following code reads the `data.csv` file and prints
    the top five rows and the schema:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您已连接到 Spark 集群上的一个会话。您可以像在 [*第 3 章*](B15739_03_ePub_AM.xhtml#_idTextAnchor039)*，*读取和写入文件*
    和 [*第 4 章*](B15739_04_ePub_AM.xhtml#_idTextAnchor049)*，*与数据库交互* 中使用 DataFrame
    一样读取 CSV 和 JSON 数据，有一些细微的修改。当读取数据时，您可以使用 `read.csv` 而不是 `pandas` 中的 `read_csv`。Spark
    和 `pandas` 之间的另一个区别是 Spark 中使用 `.show()` 来查看 DataFrame。在 `pandas` 中，您可以查看 DataFrame
    的 `dtypes`，而在 Spark 中，您可以使用 `printSchema()` 做到同样的事情。以下代码读取 `data.csv` 文件并打印前五行和模式：
- en: '[PRE14]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output will be a DataFrame like the one shown in the following screenshot:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于以下截图所示的 DataFrame：
- en: '![Figure 14.8 – DataFrame from CSV with schema'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 14.8 – 带有模式和 CSV 的 DataFrame'
- en: '](img/Figure_14.8_B15739.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/Figure_14.8_B15739.jpg)'
- en: Figure 14.8 – DataFrame from CSV with schema
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.8 – 带有模式和 CSV 的 DataFrame
- en: 'You will notice that the headers are the first row and there are default `_c0`
    column names. The output also shows that all the columns are strings. You can
    specify a schema and pass it as a parameter; however, you can also tell Spark
    to infer the schema. The following code passes that there are headers and tells
    Spark to infer the schema:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 您会注意到标题是第一行，并且有默认的 `_c0` 列名。输出还显示所有列都是字符串类型。您可以指定一个模式并将其作为参数传递；然而，您也可以告诉 Spark
    推断模式。以下代码传递存在标题并告诉 Spark 推断模式：
- en: '[PRE15]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The results are what you expected: a DataFrame with the correct types. The
    following screenshot shows the results:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 结果正如您所预期的：一个具有正确类型的 DataFrame。以下截图显示了结果：
- en: '![Figure 14.9 – DataFrame with headers and correct types'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 14.9 – 带有标题和正确类型的 DataFrame'
- en: '](img/Figure_14.9_B15739.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/Figure_14.9_B15739.jpg)'
- en: Figure 14.9 – DataFrame with headers and correct types
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.9 – 带有标题和正确类型的 DataFrame
- en: 'You can select a column by using `select()` and passing the column name as
    a parameter. Don''t forget to add `.show()` or it will return a DataFrame and
    not display it:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过使用 `select()` 函数并传递列名作为参数来选择一列。别忘了添加 `.show()`，否则它将返回一个 DataFrame 而不会显示：
- en: '[PRE16]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'You will notice that in `pandas`, you would have used `[]` and a column name
    and also did not need the `select` method. In `pandas`, you could also filter
    a DataFrame by using the `df[(df[''field'']< value)]` format. In Spark, you can
    use `select` and `filter` to do the same, the difference being that a `select`
    method returns `True` and `False` for a condition, and `filter` will return the
    DataFrame for that condition. With `filter`, you can also add a `select` method
    and pass an array of columns to return. The code is shown as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到在 `pandas` 中，你会使用 `[]` 和列名，并且不需要 `select` 方法。在 `pandas` 中，你也可以使用 `df[(df['field']<
    value)]` 格式来过滤 DataFrame。在 Spark 中，你可以使用 `select` 和 `filter` 来做同样的事情，区别在于 `select`
    方法对于一个条件返回 `True` 和 `False`，而 `filter` 将返回该条件的 DataFrame。使用 `filter`，你还可以添加一个
    `select` 方法并传递一个列名的数组来返回。代码如下所示：
- en: '[PRE17]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Notice that in the last line, you didn''t use `df[''age'']` but were able to
    just pass the column name. When you want to iterate through a DataFrame, you could
    use `iterrows` in `pandas`. In Spark, you create an array of rows using `collect()`.
    The following code will use a `filter` method to get all people under 40 and print
    the array:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在最后一行中，你没有使用 `df['age']`，而是直接传递了列名。当你想要遍历 DataFrame 时，你可以在 `pandas` 中使用 `iterrows`。在
    Spark 中，你使用 `collect()` 创建一个行数组。以下代码将使用 `filter` 方法获取所有 40 岁以下的人并打印数组：
- en: '[PRE18]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'To get a single row, you can just pass the index. You can convert the row into
    different formats, and in this example, I converted it into a dictionary. As a
    dictionary, you can select any value by specifying the key. The code is shown
    as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取单行，你只需传递索引。你可以将行转换为不同的格式，在这个例子中，我将它转换成了字典。作为字典，你可以通过指定键来选择任何值。代码如下所示：
- en: '[PRE19]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The output of the preceding code is a `Row` object, a dictionary, and a string
    of the value for the key name, as shown:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码的输出是一个 `Row` 对象、一个字典和键名的值字符串，如下所示：
- en: '[PRE20]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'To iterate through the DataFrame in Spark, you call `collect()`, and then iterate
    through the array using a `for` loop. You can then convert each of the rows into
    a dictionary and do what you need for processing. The following code snippet prints
    the dictionary:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 Spark 中遍历 DataFrame，你调用 `collect()`，然后使用 `for` 循环遍历数组。然后你可以将每一行转换为字典，并对其进行处理。以下代码片段打印了字典：
- en: '[PRE21]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'If you are more comfortable with SQL, you can filter a DataFrame using `spark.sql`.
    To use SQL, you must first create a view, then you can query it with SQL, as shown
    in the following code:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你更熟悉 SQL，你可以使用 `spark.sql` 来过滤 DataFrame。要使用 SQL，你必须首先创建一个视图，然后你可以用 SQL 查询它，如下面的代码所示：
- en: '[PRE22]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The results will be the same DataFrame as the `filter` method, but just a different
    method by which to achieve the same result.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将与 `filter` 方法相同，但只是达到相同结果的不同方法。
- en: 'There are several functions to perform modifications or analysis on columns
    or data in a DataFrame. In Spark, you can use `describe()` to get a basic summary
    of the data in a column. The following code uses it on the `age` column:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个函数可以对 DataFrame 中的列或数据进行修改或分析。在 Spark 中，你可以使用 `describe()` 来获取列中数据的简要概述。以下代码在
    `age` 列上使用它：
- en: '[PRE23]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The output is the common descriptive statistics of `count`, `mean`, `standard
    deviation`, `min`, and `max`. These five statistics give you a good overview of
    the data.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是常见的描述性统计，包括 `count`、`mean`、`standard deviation`、`min` 和 `max`。这五个统计量为你提供了数据的良好概述。
- en: 'You can also group and aggregate your data just like in `pandas`. To group
    the counts of states, you can use `groupBy()`, as shown:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以像在 `pandas` 中一样对数据进行分组和聚合。要按州分组计数，你可以使用 `groupBy()`，如下所示：
- en: '[PRE24]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Aggregation allows you to pass in a dictionary of the field and a method. To
    calculate the mean of the `age` column, you would use the following code:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合允许你传入一个字段和方法的字典。要计算 `age` 列的平均值，你会使用以下代码：
- en: '[PRE25]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'For both `groupBy` and `agg`, you can use `mean`, `max`, `min`, `sum`, and
    other methods that you can read about in the documentation. There is a large number
    of other functions you can use that require you to import the `pyspark.sql.functions`
    module. The following code imports it as `f` and demonstrates some useful functions.
    Again, for more information on all of the functions, you can read the Python API
    documents at [https://spark.apache.org/docs/latest/api/python/pyspark.sql.html](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html):'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 `groupBy` 和 `agg`，您可以使用 `mean`、`max`、`min`、`sum` 以及您可以在文档中阅读到的其他方法。您可以使用大量其他函数，这些函数需要您导入
    `pyspark.sql.functions` 模块。以下代码将其导入为 `f` 并演示了一些有用的函数。同样，有关所有函数的更多信息，您可以在 [https://spark.apache.org/docs/latest/api/python/pyspark.sql.html](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html)
    阅读Python API 文档：
- en: '[PRE26]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'When you have finished your data processing, stop the session using `stop()`,
    as shown:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 当您完成数据处理后，使用 `stop()` 停止会话，如下所示：
- en: '[PRE27]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Congratulations! You have successfully processed data with PySpark.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！您已成功使用 PySpark 处理数据。
- en: Summary
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned the basics of working with Apache Spark. First,
    you downloaded and installed Spark and configured PySpark to run in Jupyter notebooks.
    You also learned how to scale Spark horizontally by adding nodes. Spark uses DataFrames
    similar to those used in `pandas`. The last section taught you the basics of manipulating
    data in Spark.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您学习了使用 Apache Spark 的基础知识。首先，您下载并安装了 Spark，并配置 PySpark 在 Jupyter 笔记本中运行。您还学习了如何通过添加节点来水平扩展
    Spark。Spark 使用类似于 `pandas` 中使用的 DataFrame。最后一节教您了在 Spark 中操作数据的基本方法。
- en: In the next chapter, you will use Spark with Apache MiNiFi to move data at the
    edge or on Internet-of-Things devices.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，您将使用 Spark 与 Apache MiNiFi 一起在边缘或物联网设备上移动数据。
