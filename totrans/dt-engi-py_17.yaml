- en: '*Chapter 14*: Data Processing with Apache Spark'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, you learned how to add streaming data to your data
    pipelines. Using Python or Apache NiFi, you can extract, transform, and load streaming
    data. However, to perform transformations on large amounts of streaming data,
    data engineers turn to tools such as Apache Spark. Apache Spark is faster than
    most other methods – such as MapReduce on non-trivial transformations – and it
    allows distributed data processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Installing and running Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing and configuring PySpark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing data with PySpark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing and running Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Apache Spark is a distributed data processing engine that can handle both streams
    and batch data, and even graphs. It has a core set of components and other libraries
    that are used to add functionality. A common depiction of the Spark ecosystem
    is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.1 – The Apache Spark ecosystem'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_14.1_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.1 – The Apache Spark ecosystem
  prefs: []
  type: TYPE_NORMAL
- en: To run Spark as a cluster, you have several options. Spark can run in a standalone
    mode, which uses a simple cluster manager provided by Spark. It can also run on
    an Amazon EC2 instance, using YARN, Mesos, or Kubernetes. In a production environment
    with a significant workload, you would probably not want to run in standalone
    mode; however, this is how we will stand up our cluster in this chapter. The principles
    will be the same, but the standalone cluster provides the fastest way to get you
    up and running without needing to dive into more complicated infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install Apache Spark, take the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Browse to the website at [http://spark.apache.org](http://spark.apache.org).
    From there, you can keep up to date with new versions of Apache Spark, read the
    documentation, learn about the libraries, and find code examples:![Figure 14.2
    – The Apache Spark website
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_14.2_B15739.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 14.2 – The Apache Spark website
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: From the website, select the **Download** menu option. Choose the version of
    Spark you want to use – at the time of writing, the newest version is 3.0.0\.
    You will be asked to choose a package type. We will not be using Hadoop, but have
    to select a version or provide our own. I have chosen **Pre-built for Apache Hadoop
    2.7**. On Windows, you may need to trick the operating system into thinking Hadoop
    is installed by setting environment variables, but on Linux and macOS, this should
    not be an issue. The download options are shown in the following screenshot:![Figure
    14.3 – Downloading Apache Spark for Hadoop 2.7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_14.3_B15739.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 14.3 – Downloading Apache Spark for Hadoop 2.7
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'After downloading the file, you will extract it, then move it to the home directory
    in a directory named `spark3`. You can do this using the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, you will need to make a cluster. As you did with Kafka, you will make
    a copy of the Spark directory on the same machine and make it act as another node.
    If you have another machine, you could also put another copy of Spark on that
    server. Copy the directory and rename it `spark-node`, as shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To run the Spark cluster, you can use the provided scripts. The scripts to
    run the cluster use outdated terminology – `master` and `slave`. This language
    is common in the technology space; however, there have been many voices opposed
    to it for a long time. Finally, it appears that there is some traction being made
    in removing this language as GitHub will remove `master` from the branch names.
    I too have renamed the scripts, using the terms `head` and `node`. To do this,
    use the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To start the cluster, you can now run the scripts as shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can pass parameters to the scripts, and in the preceding command, you pass
    the port flag (`-p`) to tell the script which port you want the node to run on.
    You can also pass the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'a) `-h, --host`: The hostname to run on. The `i, -ip` flag has been deprecated.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b) `-p, --port`: The port to listen on.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'c) `--webui-port`: The port for the web GUI, which defaults to `8080`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'd) `-c, --cores`: The number of cores to use.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'e) `-m, --memory`: The amount of memory to use. By default, it is 1 gigabyte
    less than your full memory.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'f) `-d, --work-dir`: The scratch space directory for the worker only.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'g) `--properties-file`: This is where you can specify several of these flags
    in a `spark.conf` file.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The cluster will take a minute to load, and when it has finished, you can browse
    to the web UI at [http://localhost:8080/](http://localhost:8080/). You will see
    the details of your cluster and it will look as in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.4 – Spark cluster web UI'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_14.4_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.4 – Spark cluster web UI
  prefs: []
  type: TYPE_NORMAL
- en: With the cluster up and running, you will need to set up the Python environment
    so that you can code against it. The next section will walk you through those
    steps.
  prefs: []
  type: TYPE_NORMAL
- en: Installing and configuring PySpark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'PySpark is installed with Spark. You can see it in the `~/spark3/bin` directory,
    as well as other libraries and tools. To configure PySpark to run, you need to
    export environment variables. The variables are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command set the `SPARK_HOME` variable. This will be where you
    installed Spark. I have pointed the variable to the head of the Spark cluster
    because the node would really be on another machine. Then, it adds `SPARK_HOME`
    to your path. This means that when you type a command, the operating system will
    look for it in the directories specified in your path, so now it will search `~/spark3/bin`,
    which is where PySpark lives.
  prefs: []
  type: TYPE_NORMAL
- en: 'Running the preceding commands in a terminal will allow Spark to run while
    the terminal is open. You will have to rerun these commands every time. To make
    them permanent, you can add the commands to your `~/.bashrc` file. After saving
    the `.bashrc` file, you need to reload it. You can do that by running the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'You should now be able to open a terminal and run PySpark, and the result will
    be the PySpark interactive shell, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.5 – The interactive Spark shell'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_14.5_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.5 – The interactive Spark shell
  prefs: []
  type: TYPE_NORMAL
- en: 'If you see the preceding screenshot, congratulations, you have PySpark configured.
    In this chapter, the examples will use PySpark in Jupyter notebooks. There are
    two ways to configure PySpark to work with Jupyter:'
  prefs: []
  type: TYPE_NORMAL
- en: '`PYSPARK_DRIVER_PYTHON` environment variable and the `_OPTS` variable to the
    Jupyter Notebook using the following commands (add this to `~/.bashrc` if you
    want it to be permanent):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`findspark` library and add code to your Jupyter notebook that gets the Spark
    information as it runs. The examples in this chapter will use this method. You
    can install `findspark` using `pip`, as shown:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: import findspark
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: findspark.init()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You can now run PySpark code on your Spark cluster. The next section will walk
    you through some basic PySpark examples.
  prefs: []
  type: TYPE_NORMAL
- en: Processing data with PySpark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before processing data with PySpark, let''s run one of the samples to show
    how Spark works. Then, we will skip the boilerplate in later examples and focus
    on data processing. The Jupyter notebook for the **Pi Estimation** example from
    the Spark website at [http://spark.apache.org/examples.html](http://spark.apache.org/examples.html)
    is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.6 – The Pi Estimation example in a Jupyter notebook'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_14.6_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.6 – The Pi Estimation example in a Jupyter notebook
  prefs: []
  type: TYPE_NORMAL
- en: 'The example from the website will not run without some modifications. In the
    following points, I will walk through the cells:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first cell imports `findspark` and runs the `init()` method. This was explained
    in the preceding section as the preferred method to include PySpark in Jupyter
    notebooks. The code is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The next cell imports the `pyspark` library and `SparkSession`. It then creates
    the session by passing the head node of the Spark cluster. You can get the URL
    from the Spark web UI – you also used it to start the worker node:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Running the first two cells, you can browse to the Spark GUI and see that there
    is a task running. The running task is shown in the following screenshot – notice
    the name of the worker is `Pi-Estimation`, which is the `appName` parameter in
    the preceding code:![Figure 14.7 – The Spark web UI with a running session and
    two completed sessions
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/Figure_14.7_B15739.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 14.7 – The Spark web UI with a running session and two completed sessions
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The preceding code will be used in all of your Spark code. It is the boilerplate
    code.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The next cell contains the work. The code that follows will estimate the value
    of pi. The details of the code are not important, but notice that the `count`
    variable uses `sparkContext` and parallelizes a task on the cluster. After the
    boilerplate, your Spark code will execute a task and get the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Lastly, stop the session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Once the session has been stopped, it will show up as a completed application
    in the web UI. The next section will use Spark with DataFrames and send the data
    to Kafka.
  prefs: []
  type: TYPE_NORMAL
- en: Spark for data engineering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The previous section showed the structure of a Spark application: we used `findspark`
    to get the paths, imported the libraries, created a session, did something, and
    stopped the session. When you do something, it will most likely involve a Spark
    DataFrame. This section will provide a brief overview of how Spark DataFrames
    work – it is slightly different than `pandas`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing you have to do is use `findspark` to set up the environment.
    Then, you can import the required libraries. Then, create the session. The following
    code shows the boilerplate to get set up:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you are connected to a session on the Spark cluster. You can read CSV and
    JSON data just like you did with DataFrames in [*Chapter 3*](B15739_03_ePub_AM.xhtml#_idTextAnchor039)*,
    Reading and Writing Files* and [*Chapter 4*](B15739_04_ePub_AM.xhtml#_idTextAnchor049)*,
    Working with Databases*, with some slight modifications. When you read in the
    data, you can use `read.csv` instead of `read_csv` in `pandas`. Another difference
    between Spark and `pandas` is the use of `.show()` in Spark to see the DataFrame.
    In `pandas`, you can view `dtypes` of a DataFrame, and in Spark, you can do the
    same using `printSchema()`. The following code reads the `data.csv` file and prints
    the top five rows and the schema:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be a DataFrame like the one shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.8 – DataFrame from CSV with schema'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_14.8_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.8 – DataFrame from CSV with schema
  prefs: []
  type: TYPE_NORMAL
- en: 'You will notice that the headers are the first row and there are default `_c0`
    column names. The output also shows that all the columns are strings. You can
    specify a schema and pass it as a parameter; however, you can also tell Spark
    to infer the schema. The following code passes that there are headers and tells
    Spark to infer the schema:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are what you expected: a DataFrame with the correct types. The
    following screenshot shows the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.9 – DataFrame with headers and correct types'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_14.9_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.9 – DataFrame with headers and correct types
  prefs: []
  type: TYPE_NORMAL
- en: 'You can select a column by using `select()` and passing the column name as
    a parameter. Don''t forget to add `.show()` or it will return a DataFrame and
    not display it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'You will notice that in `pandas`, you would have used `[]` and a column name
    and also did not need the `select` method. In `pandas`, you could also filter
    a DataFrame by using the `df[(df[''field'']< value)]` format. In Spark, you can
    use `select` and `filter` to do the same, the difference being that a `select`
    method returns `True` and `False` for a condition, and `filter` will return the
    DataFrame for that condition. With `filter`, you can also add a `select` method
    and pass an array of columns to return. The code is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that in the last line, you didn''t use `df[''age'']` but were able to
    just pass the column name. When you want to iterate through a DataFrame, you could
    use `iterrows` in `pandas`. In Spark, you create an array of rows using `collect()`.
    The following code will use a `filter` method to get all people under 40 and print
    the array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'To get a single row, you can just pass the index. You can convert the row into
    different formats, and in this example, I converted it into a dictionary. As a
    dictionary, you can select any value by specifying the key. The code is shown
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is a `Row` object, a dictionary, and a string
    of the value for the key name, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'To iterate through the DataFrame in Spark, you call `collect()`, and then iterate
    through the array using a `for` loop. You can then convert each of the rows into
    a dictionary and do what you need for processing. The following code snippet prints
    the dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'If you are more comfortable with SQL, you can filter a DataFrame using `spark.sql`.
    To use SQL, you must first create a view, then you can query it with SQL, as shown
    in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The results will be the same DataFrame as the `filter` method, but just a different
    method by which to achieve the same result.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several functions to perform modifications or analysis on columns
    or data in a DataFrame. In Spark, you can use `describe()` to get a basic summary
    of the data in a column. The following code uses it on the `age` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The output is the common descriptive statistics of `count`, `mean`, `standard
    deviation`, `min`, and `max`. These five statistics give you a good overview of
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also group and aggregate your data just like in `pandas`. To group
    the counts of states, you can use `groupBy()`, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Aggregation allows you to pass in a dictionary of the field and a method. To
    calculate the mean of the `age` column, you would use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'For both `groupBy` and `agg`, you can use `mean`, `max`, `min`, `sum`, and
    other methods that you can read about in the documentation. There is a large number
    of other functions you can use that require you to import the `pyspark.sql.functions`
    module. The following code imports it as `f` and demonstrates some useful functions.
    Again, for more information on all of the functions, you can read the Python API
    documents at [https://spark.apache.org/docs/latest/api/python/pyspark.sql.html](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'When you have finished your data processing, stop the session using `stop()`,
    as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Congratulations! You have successfully processed data with PySpark.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned the basics of working with Apache Spark. First,
    you downloaded and installed Spark and configured PySpark to run in Jupyter notebooks.
    You also learned how to scale Spark horizontally by adding nodes. Spark uses DataFrames
    similar to those used in `pandas`. The last section taught you the basics of manipulating
    data in Spark.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will use Spark with Apache MiNiFi to move data at the
    edge or on Internet-of-Things devices.
  prefs: []
  type: TYPE_NORMAL
