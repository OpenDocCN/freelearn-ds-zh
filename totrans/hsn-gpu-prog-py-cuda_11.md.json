["```py\nfrom __future__ import division\nimport numpy as np\nfrom pycuda.compiler import DynamicSourceModule\nimport pycuda.autoinit\n```", "```py\nDynamicParallelismCode='''\n__global__ void dynamic_hello_ker(int depth)\n{\n printf(\"Hello from thread %d, recursion depth %d!\\\\n\", threadIdx.x, depth);\n if (threadIdx.x == 0 && blockIdx.x == 0 && blockDim.x > 1)\n {\n  printf(\"Launching a new kernel from depth %d .\\\\n\", depth);\n  printf(\"-----------------------------------------\\\\n\");\n  dynamic_hello_ker<<< 1, blockDim.x - 1 >>>(depth + 1);\n }\n}'''\n```", "```py\ndp_mod = DynamicSourceModule(DynamicParallelismCode)\nhello_ker = dp_mod.get_function('dynamic_hello_ker')\nhello_ker(np.int32(0), grid=(1,1,1), block=(4,1,1))\n```", "```py\nfrom __future__ import division\nimport numpy as np\nfrom pycuda.compiler import DynamicSourceModule\nimport pycuda.autoinit\nfrom pycuda import gpuarray\nfrom random import shuffle\n```", "```py\nDynamicQuicksortCode='''\n__device__ int partition(int * a, int lo, int hi)\n{\n int i = lo;\n int pivot = a[hi];\n int temp;\n\n for (int k=lo; k<hi; k++)\n {\n  if (a[k] < pivot)\n  {\n   temp = a[k];\n   a[k] = a[i];\n   a[i] = temp;\n   i++;\n  }\n }\n\n a[hi] = a[i];\n a[i] = pivot;\n\n return i;\n}\n```", "```py\n__global__ void quicksort_ker(int *a, int lo, int hi)\n{\n\n cudaStream_t s_left, s_right; \n cudaStreamCreateWithFlags(&s_left, cudaStreamNonBlocking);\n cudaStreamCreateWithFlags(&s_right, cudaStreamNonBlocking);\n\n int mid = partition(a, lo, hi);\n\n if(mid - 1 - lo > 0)\n   quicksort_ker<<< 1, 1, 0, s_left >>>(a, lo, mid - 1);\n if(hi - (mid + 1) > 0)\n   quicksort_ker<<< 1, 1, 0, s_right >>>(a, mid + 1, hi);\n\n cudaStreamDestroy(s_left);\n cudaStreamDestroy(s_right);\n\n}\n'''\n```", "```py\nqsort_mod = DynamicSourceModule(DynamicQuicksortCode)\n\nqsort_ker = qsort_mod.get_function('quicksort_ker')\n\nif __name__ == '__main__':\n    a = range(100)\n    shuffle(a)\n\n    a = np.int32(a)\n\n    d_a = gpuarray.to_gpu(a)\n\n    print 'Unsorted array: %s' % a\n\n    qsort_ker(d_a, np.int32(0), np.int32(a.size - 1), grid=(1,1,1), block=(1,1,1))\n\n    a_sorted = list(d_a.get())\n\n    print 'Sorted array: %s' % a_sorted\n```", "```py\nfrom __future__ import division\nimport numpy as np\nfrom pycuda.compiler import SourceModule\nimport pycuda.autoinit\nfrom pycuda import gpuarray\n\nVecCode='''\n__global__ void vec_ker(int *ints, double *doubles) { \n\n int4 f1, f2;\n\n f1 = *reinterpret_cast<int4*>(ints);\n f2 = *reinterpret_cast<int4*>(&ints[4]);\n\n printf(\"First int4: %d, %d, %d, %d\\\\n\", f1.x, f1.y, f1.z, f1.w);\n printf(\"Second int4: %d, %d, %d, %d\\\\n\", f2.x, f2.y, f2.z, f2.w);\n\n double2 d1, d2;\n\n d1 = *reinterpret_cast<double2*>(doubles);\n d2 = *reinterpret_cast<double2*>(&doubles[2]);\n\n printf(\"First double2: %f, %f\\\\n\", d1.x, d1.y);\n printf(\"Second double2: %f, %f\\\\n\", d2.x, d2.y);\n\n}'''\n```", "```py\nfrom __future__ import division\nimport numpy as np\nfrom pycuda.compiler import SourceModule\nimport pycuda.autoinit\nfrom pycuda import gpuarray\nimport pycuda.driver as drv\n\nAtomicCode='''\n__global__ void atomic_ker(int *add_out, int *max_out) \n{\n\n int tid = blockIdx.x*blockDim.x + threadIdx.x;\n\n atomicExch(add_out, 0);\n```", "```py\n __syncthreads();\n```", "```py\n atomicAdd(add_out, 1);\n```", "```py\n atomicMax(max_out, tid);\n\n}\n'''\n```", "```py\natomic_mod = SourceModule(AtomicCode)\natomic_ker = atomic_mod.get_function('atomic_ker')\n\nadd_out = gpuarray.empty((1,), dtype=np.int32)\nmax_out = gpuarray.empty((1,), dtype=np.int32)\n\natomic_ker(add_out, max_out, grid=(1,1,1), block=(100,1,1))\n\nprint 'Atomic operations test:'\nprint 'add_out: %s' % add_out.get()[0]\nprint 'max_out: %s' % max_out.get()[0]\n```", "```py\nfrom __future__ import division\nimport numpy as np\nfrom pycuda.compiler import SourceModule\nimport pycuda.autoinit\nfrom pycuda import gpuarray\n\nShflCode='''\n__global__ void shfl_xor_ker(int *input, int * output) {\n\nint temp = input[threadIdx.x];\n\ntemp = __shfl_xor (temp, 1, blockDim.x);\n\noutput[threadIdx.x] = temp;\n\n}'''\n```", "```py\nshfl_mod = SourceModule(ShflCode)\nshfl_ker = shfl_mod.get_function('shfl_xor_ker')\n\ndinput = gpuarray.to_gpu(np.int32(range(32)))\ndoutout = gpuarray.empty_like(dinput)\n\nshfl_ker(dinput, doutout, grid=(1,1,1), block=(32,1,1))\n\nprint 'input array: %s' % dinput.get()\nprint 'array after __shfl_xor: %s' % doutout.get()\n```", "```py\nfrom __future__ import division\nimport numpy as np\nfrom pycuda.compiler import SourceModule\nimport pycuda.autoinit\nfrom pycuda import gpuarray\n\nShflSumCode='''\n__global__ void shfl_sum_ker(int *input, int *out) {\n\n int temp = input[threadIdx.x];\n\n for (int i=1; i < 32; i *= 2)\n     temp += __shfl_down (temp, i, 32);\n\n if (threadIdx.x == 0)\n     *out = temp;\n\n}'''\n\nshfl_mod = SourceModule(ShflSumCode)\nshfl_sum_ker = shfl_mod.get_function('shfl_sum_ker')\n\narray_in = gpuarray.to_gpu(np.int32(range(32)))\nout = gpuarray.empty((1,), dtype=np.int32)\n\nshfl_sum_ker(array_in, out, grid=(1,1,1), block=(32,1,1))\n\nprint 'Input array: %s' % array_in.get()\nprint 'Summed value: %s' % out.get()[0]\nprint 'Does this match with Python''s sum? : %s' % (out.get()[0] == sum(array_in.get()) )\n```", "```py\nfrom __future__ import division\nimport numpy as np\nfrom pycuda.compiler import SourceModule\nimport pycuda.autoinit\nfrom pycuda import gpuarray\n\nPtxCode='''\n```", "```py\n__device__ void set_to_zero(int &x)\n{\n asm(\"mov.s32 %0, 0;\" : \"=r\"(x));\n}\n```", "```py\n__device__ void add_floats(float &out, float in1, float in2)\n{\n asm(\"add.f32 %0, %1, %2 ;\" : \"=f\"(out) : \"f\"(in1) , \"f\"(in2));\n}\n```", "```py\n__device__ void plusplus(int &x)\n{\n asm(\"add.s32 %0, %0, 1;\" : \"+r\"(x));\n}\n```", "```py\n__device__ int laneid()\n{\n int id; \n asm(\"mov.u32 %0, %%laneid; \" : \"=r\"(id)); \n return id;\n}\n```", "```py\n__device__ void split64(double val, int & lo, int & hi)\n{\n asm volatile(\"mov.b64 {%0, %1}, %2; \":\"=r\"(lo),\"=r\"(hi):\"d\"(val));\n}\n\n__device__ void combine64(double &val, int lo, int hi)\n{\n asm volatile(\"mov.b64 %0, {%1, %2}; \":\"=d\"(val):\"r\"(lo),\"r\"(hi));\n}\n```", "```py\n__global__ void ptx_test_ker() { \n\n int x=123;\n\n printf(\"x is %d \\\\n\", x);\n\n set_to_zero(x);\n\n printf(\"x is now %d \\\\n\", x);\n\n plusplus(x);\n\n printf(\"x is now %d \\\\n\", x);\n\n float f;\n\n add_floats(f, 1.11, 2.22 );\n\n printf(\"f is now %f \\\\n\", f);\n\n printf(\"lane ID: %d \\\\n\", laneid() );\n\n double orig = 3.1415;\n\n int t1, t2;\n\n split64(orig, t1, t2);\n\n double recon;\n\n combine64(recon, t1, t2);\n\n printf(\"Do split64 / combine64 work? : %s \\\\n\", (orig == recon) ? \"true\" : \"false\"); \n\n}'''\n\nptx_mod = SourceModule(PtxCode)\nptx_test_ker = ptx_mod.get_function('ptx_test_ker')\nptx_test_ker(grid=(1,1,1), block=(1,1,1))\n```", "```py\nfrom __future__ import division\nimport numpy as np\nfrom pycuda.compiler import SourceModule\nimport pycuda.autoinit\nfrom pycuda import gpuarray\nimport pycuda.driver as drv\nfrom timeit import timeit\n\nSumCode='''\n__device__ void __inline__ laneid(int & id)\n{\n asm(\"mov.u32 %0, %%laneid; \" : \"=r\"(id)); \n}\n```", "```py\n__device__ void __inline__ split64(double val, int & lo, int & hi)\n{\n asm volatile(\"mov.b64 {%0, %1}, %2; \":\"=r\"(lo),\"=r\"(hi):\"d\"(val));\n}\n\n__device__ void __inline__ combine64(double &val, int lo, int hi)\n{\n asm volatile(\"mov.b64 %0, {%1, %2}; \":\"=d\"(val):\"r\"(lo),\"r\"(hi));\n}\n```", "```py\n__global__ void sum_ker(double *input, double *out) \n{\n\n int id;\n laneid(id);\n\n double2 vals = *reinterpret_cast<double2*> ( &input[(blockDim.x*blockIdx.x + threadIdx.x) * 2] );\n```", "```py\n double sum_val = vals.x + vals.y;\n\n double temp;\n\n int s1, s2;\n```", "```py\n for (int i=1; i < 32; i *= 2)\n {\n\n     // use PTX assembly to split\n     split64(sum_val, s1, s2);\n\n     // shuffle to transfer data\n     s1 = __shfl_down (s1, i, 32);\n     s2 = __shfl_down (s2, i, 32);\n\n     // PTX assembly to combine\n     combine64(temp, s1, s2);\n     sum_val += temp;\n }\n```", "```py\n if (id == 0)\n     atomicAdd(out, sum_val);\n\n}'''\n```", "```py\nsum_mod = SourceModule(SumCode)\nsum_ker = sum_mod.get_function('sum_ker')\n\na = np.float64(np.random.randn(10000*2*32))\na_gpu = gpuarray.to_gpu(a)\nout = gpuarray.zeros((1,), dtype=np.float64)\n\nsum_ker(a_gpu, out, grid=(int(np.ceil(a.size/64)),1,1), block=(32,1,1))\ndrv.Context.synchronize()\n\nprint 'Does sum_ker produces the same value as NumPy\\'s sum (according allclose)? : %s' % np.allclose(np.sum(a) , out.get()[0])\n\nprint 'Performing sum_ker / PyCUDA sum timing tests (20 each)...'\n\nsum_ker_time = timeit('''from __main__ import sum_ker, a_gpu, out, np, drv \\nsum_ker(a_gpu, out, grid=(int(np.ceil(a_gpu.size/64)),1,1), block=(32,1,1)) \\ndrv.Context.synchronize()''', number=20)\npycuda_sum_time = timeit('''from __main__ import gpuarray, a_gpu, drv \\ngpuarray.sum(a_gpu) \\ndrv.Context.synchronize()''', number=20)\n\nprint 'sum_ker average time duration: %s, PyCUDA\\'s gpuarray.sum average time duration: %s' % (sum_ker_time, pycuda_sum_time)\nprint '(Performance improvement of sum_ker over gpuarray.sum: %s )' % (pycuda_sum_time / sum_ker_time)\n```"]