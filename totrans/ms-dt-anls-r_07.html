<html><head></head><body><div class="chapter" title="Chapter&#xA0;7.&#xA0;Unstructured Data"><div class="titlepage"><div><div><h1 class="title"><a id="ch07"/>Chapter 7. Unstructured Data</h1></div></div></div><p>In the previous chapter, we looked at different ways of building and fitting models on structured data. Unfortunately, these otherwise extremely useful methods are of no use (yet) when dealing with, for example, a pile of PDF documents. Hence, the following pages will focus on methods to deal with non-tabular data, such as:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Extracting metrics from a collection of text documents</li><li class="listitem" style="list-style-type: disc">Filtering and parsing<a class="indexterm" id="id517"/> <span class="strong"><strong>natural language texts</strong></span> (<span class="strong"><strong>NLP</strong></span>)</li><li class="listitem" style="list-style-type: disc">Visualizing unstructured data in a structured way</li></ul></div><p>Text mining is<a class="indexterm" id="id518"/> the process of analyzing natural language text; in most cases from online content, such as emails and social media streams (Twitter or Facebook). In this chapter, we are going to cover the most used methods of<a class="indexterm" id="id519"/> the <code class="literal">tm</code> package—although, there is a variety of further types of unstructured data, such as text, image, audio, video, non-digital contents, and so on, which we cannot discuss for the time being.</p><div class="section" title="Importing the corpus"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec45"/>Importing the corpus</h1></div></div></div><p>A corpus is basically a<a class="indexterm" id="id520"/> collection of text documents that you want to include in the <a class="indexterm" id="id521"/>analytics. Use the <code class="literal">getSources</code> function to see the available options to import a corpus with the <code class="literal">tm</code> package:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; library(tm)</strong></span>
<span class="strong"><strong>&gt; getSources()</strong></span>
<span class="strong"><strong>[1] "DataframeSource" "DirSource"  "ReutersSource"   "URISource"</strong></span>
<span class="strong"><strong>[2] "VectorSource"  </strong></span>
</pre></div><p>So, we can import text documents from a <code class="literal">data.frame</code>, a <code class="literal">vector</code>, or directly from a uniform resource identifier with the <code class="literal">URISource</code> function. The latter stands for a collection of hyperlinks or file paths, although this is somewhat easier to handle with <code class="literal">DirSource</code>, which imports all the textual documents found in the referenced directory on our hard drive. By calling the <code class="literal">getReaders</code> function in the R console, you can see the supported text file formats:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; getReaders()</strong></span>
<span class="strong"><strong>[1] "readDOC"                 "readPDF"                </strong></span>
<span class="strong"><strong>[3] "readPlain"               "readRCV1"               </strong></span>
<span class="strong"><strong>[5] "readRCV1asPlain"         "readReut21578XML"       </strong></span>
<span class="strong"><strong>[7] "readReut21578XMLasPlain" "readTabular"            </strong></span>
<span class="strong"><strong>[9] "readXML"    </strong></span>
</pre></div><p>So, there are some nifty<a class="indexterm" id="id522"/> functions to read and parse MS Word, PDFs, plain text, or XML files among a few other file formats. The previous <code class="literal">Reut</code> reader stands for the Reuters demo corpus that is bundled with the<a class="indexterm" id="id523"/> <code class="literal">tm</code> package.</p><p>But let's not stick to some factory default demo files! You can see the package examples in the vignette or reference manual. As we have already fetched some textual data in <a class="link" href="ch02.html" title="Chapter 2. Getting Data from the Web">Chapter 2</a>, <span class="emphasis"><em>Getting Data from the Web</em></span>, let's see how we can process and analyze that content:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; res &lt;- XML::readHTMLTable(paste0('http://cran.r-project.org/',</strong></span>
<span class="strong"><strong>+                   'web/packages/available_packages_by_name.html'),</strong></span>
<span class="strong"><strong>+               which = 1)</strong></span>
</pre></div><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="tip09"/>Tip</h3><p>The preceding command requires a live Internet connection and could take 15-120 seconds to download and parse the referenced HTML page. Please note that the content of the downloaded HTML file might be different from what is shown in this chapter, so please be prepared for slightly different outputs in your R session, as compared to what we published in this book.</p></div></div><p>So, now we have a <code class="literal">data.frame</code> with more than 5,000 R package names and short descriptions. Let's build a corpus from the vector source of package descriptions, so that we can parse those further and see the most important trends in package development:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; v &lt;- Corpus(VectorSource(res$V2))</strong></span>
</pre></div><p>We have just created a <code class="literal">VCorpus</code> (in-memory) object, which currently holds 5,880 package descriptions:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; v</strong></span>
<span class="strong"><strong>&lt;&lt;VCorpus (documents: 5880, metadata (corpus/indexed): 0/0)&gt;&gt;</strong></span>
</pre></div><p>As the default <code class="literal">print</code> method (see the preceding output) shows a concise overview on the corpus, we will need to use another function to inspect the actual content:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; inspect(head(v, 3))</strong></span>
<span class="strong"><strong>&lt;&lt;VCorpus (documents: 3, metadata (corpus/indexed): 0/0)&gt;&gt;</strong></span>

<span class="strong"><strong>[[1]]</strong></span>
<span class="strong"><strong>&lt;&lt;PlainTextDocument (metadata: 7)&gt;&gt;</strong></span>
<span class="strong"><strong>A3: Accurate, Adaptable, and Accessible Error Metrics for</strong></span>
<span class="strong"><strong>Predictive Models</strong></span>

<span class="strong"><strong>[[2]]</strong></span>
<span class="strong"><strong>&lt;&lt;PlainTextDocument (metadata: 7)&gt;&gt;</strong></span>
<span class="strong"><strong>Tools for Approximate Bayesian Computation (ABC)</strong></span>

<span class="strong"><strong>[[3]]</strong></span>
<span class="strong"><strong>&lt;&lt;PlainTextDocument (metadata: 7)&gt;&gt;</strong></span>
<span class="strong"><strong>ABCDE_FBA: A-Biologist-Can-Do-Everything of Flux Balance</strong></span>
<span class="strong"><strong>Analysis with this package</strong></span>
</pre></div><p>Here, we can see the<a class="indexterm" id="id524"/> first three documents in the corpus, along with some metadata. Until now, we have not done much more than when in the <a class="link" href="ch02.html" title="Chapter 2. Getting Data from the Web">Chapter 2</a>, <span class="emphasis"><em>Getting Data from the Web</em></span>, we visualized a wordcloud of the expression used in the package descriptions. But that's exactly where<a class="indexterm" id="id525"/> the journey begins with text mining!</p></div></div>
<div class="section" title="Cleaning the corpus"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec46"/>Cleaning the corpus</h1></div></div></div><p>One of the nicest features<a class="indexterm" id="id526"/> of the <code class="literal">tm</code> package<a class="indexterm" id="id527"/> is the variety of bundled transformations to be applied on corpora (corpuses). The <code class="literal">tm_map</code> function provides a convenient way of running the transformations on the corpus to filter out all the data that is irrelevant in the actual research. To see the list of available transformation methods, simply call the <code class="literal">getTransformations</code> function:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; getTransformations()</strong></span>
<span class="strong"><strong>[1] "as.PlainTextDocument" "removeNumbers"</strong></span>
<span class="strong"><strong>[3] "removePunctuation"    "removeWords"</strong></span>
<span class="strong"><strong>[5] "stemDocument"         "stripWhitespace"     </strong></span>
</pre></div><p>We should usually start with removing the most frequently used, so <a class="indexterm" id="id528"/>called stopwords from the corpus. These are the most common, short function terms, which usually carry less important meanings than the other expressions in the corpus, especially the keywords. The package already includes such lists of words in different languages:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; stopwords("english")</strong></span>
<span class="strong"><strong>  [1] "i"          "me"         "my"         "myself"     "we"        </strong></span>
<span class="strong"><strong>  [6] "our"        "ours"       "ourselves"  "you"        "your"      </strong></span>
<span class="strong"><strong> [11] "yours"      "yourself"   "yourselves" "he"         "him"       </strong></span>
<span class="strong"><strong> [16] "his"        "himself"    "she"        "her"        "hers"      </strong></span>
<span class="strong"><strong> [21] "herself"    "it"         "its"        "itself"     "they"      </strong></span>
<span class="strong"><strong> [26] "them"       "their"      "theirs"     "themselves" "what"      </strong></span>
<span class="strong"><strong> [31] "which"      "who"        "whom"       "this"       "that"      </strong></span>
<span class="strong"><strong> [36] "these"      "those"      "am"         "is"         "are"       </strong></span>
<span class="strong"><strong> [41] "was"        "were"       "be"         "been"       "being"     </strong></span>
<span class="strong"><strong> [46] "have"       "has"        "had"        "having"     "do"        </strong></span>
<span class="strong"><strong> [51] "does"       "did"        "doing"      "would"      "should"    </strong></span>
<span class="strong"><strong> [56] "could"      "ought"      "i'm"        "you're"     "he's"      </strong></span>
<span class="strong"><strong> [61] "she's"      "it's"       "we're"      "they're"    "i've"      </strong></span>
<span class="strong"><strong> [66] "you've"     "we've"      "they've"    "i'd"        "you'd"     </strong></span>
<span class="strong"><strong> [71] "he'd"       "she'd"      "we'd"       "they'd"     "i'll"      </strong></span>
<span class="strong"><strong> [76] "you'll"     "he'll"      "she'll"     "we'll"      "they'll"   </strong></span>
<span class="strong"><strong> [81] "isn't"      "aren't"     "wasn't"     "weren't"    "hasn't"    </strong></span>
<span class="strong"><strong> [86] "haven't"    "hadn't"     "doesn't"    "don't"      "didn't"    </strong></span>
<span class="strong"><strong> [91] "won't"      "wouldn't"   "shan't"     "shouldn't"  "can't"     </strong></span>
<span class="strong"><strong> [96] "cannot"     "couldn't"   "mustn't"    "let's"      "that's"    </strong></span>
<span class="strong"><strong>[101] "who's"      "what's"     "here's"     "there's"    "when's"    </strong></span>
<span class="strong"><strong>[106] "where's"    "why's"      "how's"      "a"          "an"        </strong></span>
<span class="strong"><strong>[111] "the"        "and"        "but"        "if"         "or"        </strong></span>
<span class="strong"><strong>[116] "because"    "as"         "until"      "while"      "of"        </strong></span>
<span class="strong"><strong>[121] "at"         "by"         "for"        "with"       "about"     </strong></span>
<span class="strong"><strong>[126] "against"    "between"    "into"       "through"    "during"    </strong></span>
<span class="strong"><strong>[131] "before"     "after"      "above"      "below"      "to"        </strong></span>
<span class="strong"><strong>[136] "from"       "up"         "down"       "in"         "out"       </strong></span>
<span class="strong"><strong>[141] "on"         "off"        "over"       "under"      "again"     </strong></span>
<span class="strong"><strong>[146] "further"    "then"       "once"       "here"       "there"     </strong></span>
<span class="strong"><strong>[151] "when"       "where"      "why"        "how"        "all"       </strong></span>
<span class="strong"><strong>[156] "any"        "both"       "each"       "few"        "more"      </strong></span>
<span class="strong"><strong>[161] "most"       "other"      "some"       "such"       "no"        </strong></span>
<span class="strong"><strong>[166] "nor"        "not"        "only"       "own"        "same"      </strong></span>
<span class="strong"><strong>[171] "so"         "than"       "too"        "very"       </strong></span>
</pre></div><p>Skimming through this list verifies that removing these rather unimportant words will not really modify the meaning of the R package descriptions. Although there are some rare cases in which<a class="indexterm" id="id529"/> removing the <a class="indexterm" id="id530"/>stopwords is not a good idea at all! Carefully examine the output of the following R command:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; removeWords('to be or not to be', stopwords("english"))</strong></span>
<span class="strong"><strong>[1] "     "</strong></span>
</pre></div><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note55"/>Note</h3><p>This does not suggest that the memorable quote from Shakespeare is meaningless, or that we can ignore any of the stopwords in all cases. Sometimes, these words have a very important role in the context, where replacing the words with a space is not useful, but rather deteriorative. Although I would suggest, that in most cases, removing the <a class="indexterm" id="id531"/>stopwords is highly practical for keeping the number of words to process at a low level.</p></div></div><p>To iteratively apply the previous call on each document in our corpus, the <code class="literal">tm_map</code> function is extremely useful:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; v &lt;- tm_map(v, removeWords, stopwords("english"))</strong></span>
</pre></div><p>Simply pass the corpus and the transformation function, along with its parameters, to <code class="literal">tm_map</code>, which<a class="indexterm" id="id532"/> takes and returns a corpus of any number of documents:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; inspect(head(v, 3))</strong></span>
<span class="strong"><strong>&lt;&lt;VCorpus (documents: 3, metadata (corpus/indexed): 0/0)&gt;&gt;</strong></span>

<span class="strong"><strong>[[1]]</strong></span>
<span class="strong"><strong>&lt;&lt;PlainTextDocument (metadata: 7)&gt;&gt;</strong></span>
<span class="strong"><strong>A3 Accurate Adaptable Accessible Error Metrics Predictive Models</strong></span>

<span class="strong"><strong>[[2]]</strong></span>
<span class="strong"><strong>&lt;&lt;PlainTextDocument (metadata: 7)&gt;&gt;</strong></span>
<span class="strong"><strong>Tools Approximate Bayesian Computation ABC</strong></span>

<span class="strong"><strong>[[3]]</strong></span>
<span class="strong"><strong>&lt;&lt;PlainTextDocument (metadata: 7)&gt;&gt;</strong></span>
<span class="strong"><strong>ABCDEFBA ABiologistCanDoEverything Flux Balance Analysis package</strong></span>
</pre></div><p>We can see that the most common function words and a few special characters are now gone from the package descriptions. But what happens if someone starts the description with uppercase stopwords? This is <a class="indexterm" id="id533"/>shown in the following example:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; removeWords('To be or not to be.', stopwords("english"))</strong></span>
<span class="strong"><strong>[1] "To     ."</strong></span>
</pre></div><p>It's clear that the uppercase version of the <code class="literal">to</code> common word was not removed from the sentence, and the trailing dot was also preserved. For this end, usually, we should simply transform the uppercase letters to lowercase, and replace the punctuations with a space to keep the clutter among the keywords at a minimal level:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; v &lt;- tm_map(v, content_transformer(tolower))</strong></span>
<span class="strong"><strong>&gt; v &lt;- tm_map(v, removePunctuation)</strong></span>
<span class="strong"><strong>&gt; v &lt;- tm_map(v, stripWhitespace)</strong></span>
<span class="strong"><strong>&gt; inspect(head(v, 3))</strong></span>
<span class="strong"><strong>&lt;&lt;VCorpus (documents: 3, metadata (corpus/indexed): 0/0)&gt;&gt;</strong></span>

<span class="strong"><strong>[[1]]</strong></span>
<span class="strong"><strong>[1] a3 accurate adaptable accessible error metrics predictive models</strong></span>

<span class="strong"><strong>[[2]]</strong></span>
<span class="strong"><strong>[1] tools approximate bayesian computation abc</strong></span>

<span class="strong"><strong>[[3]]</strong></span>
<span class="strong"><strong>[1] abcdefba abiologistcandoeverything flux balance analysis package</strong></span>
</pre></div><p>So, we first called the <code class="literal">tolower</code> function from the <code class="literal">base</code> package to transform all characters from upper to lower case. Please note that we had to wrap the <code class="literal">tolower</code> function in the <code class="literal">content_transformer</code> function, so that our transformation really complies with the<a class="indexterm" id="id534"/> <code class="literal">tm</code> package's object structure. This is usually required when using a transformation function outside of the <code class="literal">tm</code> package.</p><p>Then, we removed all the punctuation marks from the text with the help of the <code class="literal">removePunctutation</code> function. The punctuations marks are the ones referred to as <code class="literal">[:punct:]</code> in regular<a class="indexterm" id="id535"/> expressions, including the following characters: <code class="literal">!</code> <code class="literal">"</code> <code class="literal">#</code> <code class="literal">$</code> <code class="literal">%</code> <code class="literal">&amp;</code> <code class="literal">'</code> <code class="literal">( )</code> <code class="literal">*</code> <code class="literal">+</code> <code class="literal">,</code> <code class="literal">-</code> <code class="literal">.</code> <code class="literal">/</code> <code class="literal">:</code> <code class="literal">;</code> <code class="literal">&lt;</code> <code class="literal">=</code> <code class="literal">&gt;</code> <code class="literal">?</code> <code class="literal">@</code> <code class="literal">[</code> <code class="literal">\</code> <code class="literal">]</code> <code class="literal">^</code> <code class="literal">_</code> <code class="literal">`</code> <code class="literal">{</code> <code class="literal">|</code> <code class="literal">}</code> <code class="literal">~'</code>. Usually, it's safe to remove these separators, especially when we analyze the words on their own and not their relations.</p><p>And we also removed the multiple whitespace characters from the document, so that we find only one space between the filtered words.</p></div>
<div class="section" title="Visualizing the most frequent words in the corpus"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec47"/>Visualizing the most frequent words in the corpus</h1></div></div></div><p>Now that we<a class="indexterm" id="id536"/> have cleared up our corpus a bit, we can generate a much more<a class="indexterm" id="id537"/> useful wordcloud, as compared to the proof-of-concept demo we generated in <a class="link" href="ch02.html" title="Chapter 2. Getting Data from the Web">Chapter 2</a>, <span class="emphasis"><em>Getting Data from the Web</em></span>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; wordcloud::wordcloud(v)</strong></span>
</pre></div><div class="mediaobject"><img alt="Visualizing the most frequent words in the corpus" src="graphics/2028OS_07_01.jpg"/></div></div>
<div class="section" title="Further cleanup"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec48"/>Further cleanup</h1></div></div></div><p>There are still some <a class="indexterm" id="id538"/>small disturbing glitches in the wordlist. Maybe, we do not really want to keep numbers in the package descriptions at all (or we might want to replace all numbers with a placeholder text, such as <code class="literal">NUM</code>), and there are some frequent technical words that can be ignored as well, for example, <code class="literal">package</code>. Showing the plural version of nouns is also redundant. Let's improve our corpus with some further tweaks, step by step!</p><p>Removing the numbers from the package descriptions is fairly straightforward, as based on the previous examples:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; v &lt;- tm_map(v, removeNumbers)</strong></span>
</pre></div><p>To remove some frequent domain-specific words with less important meanings, let's see the most common words in the documents. For this end, first we have to compute the <code class="literal">TermDocumentMatrix</code> function that can be passed later to the <code class="literal">findFreqTerms</code> function to identify the most popular terms in the corpus, based on frequency:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; tdm &lt;- TermDocumentMatrix(v)</strong></span>
</pre></div><p>This object is<a class="indexterm" id="id539"/> basically a matrix which includes the words in the rows and the documents in the columns, where the cells show the number of occurrences. For example, let's take a look at the first 5 words' occurrences in the first 20 documents:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; inspect(tdm[1:5, 1:20])</strong></span>
<span class="strong"><strong>&lt;&lt;TermDocumentMatrix (terms: 5, documents: 20)&gt;&gt;</strong></span>
<span class="strong"><strong>Non-/sparse entries: 5/95</strong></span>
<span class="strong"><strong>Sparsity           : 95%</strong></span>
<span class="strong"><strong>Maximal term length: 14</strong></span>
<span class="strong"><strong>Weighting          : term frequency (tf)</strong></span>

<span class="strong"><strong>                Docs</strong></span>
<span class="strong"><strong>Terms            1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20</strong></span>
<span class="strong"><strong>  aalenjohansson 0 0 0 0 0 0 0 0 0  0  0  0  0  0  0  0  0  0  0  0</strong></span>
<span class="strong"><strong>  abc            0 1 0 1 1 0 1 0 0  0  0  0  0  0  0  0  0  0  0  0</strong></span>
<span class="strong"><strong>  abcdefba       0 0 1 0 0 0 0 0 0  0  0  0  0  0  0  0  0  0  0  0</strong></span>
<span class="strong"><strong>  abcsmc         0 0 0 0 0 0 0 0 0  0  0  0  0  0  0  0  0  0  0  0</strong></span>
<span class="strong"><strong>  aberrations    0 0 0 0 0 0 0 0 0  0  0  0  0  0  0  0  0  0  0  0</strong></span>
</pre></div><p>Extracting the overall number of occurrences for each word is fairly easy. In theory, we could compute the <code class="literal">rowSums</code> function of this sparse matrix. But let's simply call the <code class="literal">findFreqTerms</code> function, which does exactly what we were up to. Let's show all those terms that show up in the descriptions at least a 100 times:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; findFreqTerms(tdm, lowfreq = 100)</strong></span>
<span class="strong"><strong> [1] "analysis"     "based"        "bayesian"     "data"        </strong></span>
<span class="strong"><strong> [5] "estimation"   "functions"    "generalized"  "inference"   </strong></span>
<span class="strong"><strong> [9] "interface"    "linear"       "methods"      "model"       </strong></span>
<span class="strong"><strong>[13] "models"       "multivariate" "package"      "regression"  </strong></span>
<span class="strong"><strong>[17] "series"       "statistical"  "test"         "tests"       </strong></span>
<span class="strong"><strong>[21] "time"         "tools"        "using"       </strong></span>
</pre></div><p>Manually reviewing this list suggests ignoring the <code class="literal">based</code> and <code class="literal">using</code> words, besides the previously suggested <code class="literal">package</code> term:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; myStopwords &lt;- c('package', 'based', 'using')</strong></span>
<span class="strong"><strong>&gt; v &lt;- tm_map(v, removeWords, myStopwords)</strong></span>
</pre></div><div class="section" title="Stemming words"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec46"/>Stemming words</h2></div></div></div><p>Now, let's get rid of the plural forms of the nouns, which also occur in the preceding top 20 lists of the most common words! This is not as easy as it sounds. We might apply some regular expressions to cut the trailing <code class="literal">s</code> from the words, but this method has many drawbacks, such as not taking into account some evident English grammar rules.</p><p>But we can, instead, use<a class="indexterm" id="id540"/> some<a class="indexterm" id="id541"/> stemming algorithms, especially Porter's stemming algorithm, which is available in the <a class="indexterm" id="id542"/>
<code class="literal">SnowballC</code> package. The <code class="literal">wordStem</code> function supports 16 languages (take a look at the <code class="literal">getStemLanguages</code> for details), and can identify the stem of a character vector as easily as calling the function:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; library(SnowballC)</strong></span>
<span class="strong"><strong>&gt; wordStem(c('cats', 'mastering', 'modelling', 'models', 'model'))</strong></span>
<span class="strong"><strong>[1] "cat"    "master" "model"  "model"  "model"</strong></span>
</pre></div><p>The only penalty here is the fact that Porter's algorithm does not provide real English words in all cases:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; wordStem(c('are', 'analyst', 'analyze', 'analysis'))</strong></span>
<span class="strong"><strong>[1] "ar"      "analyst" "analyz"  "analysi"</strong></span>
</pre></div><p>So later, we will have to tweak the results further; to reconstruct the words with the help of a language lexicon database. The easiest way to construct such a database is copying the words of the already existing corpus:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; d &lt;- v</strong></span>
</pre></div><p>Then, let's stem all the words in the documents:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; v &lt;- tm_map(v, stemDocument, language = "english")</strong></span>
</pre></div><p>Now, we called the <code class="literal">stemDocument</code> function, which is a wrapper around the <code class="literal">SnowballC</code> package's <code class="literal">wordStem</code> function. We specified only one parameter, which sets the language of the <a class="indexterm" id="id543"/>stemming algorithm. And now, let's call the <code class="literal">stemCompletion</code> function on our previously defined directory, and let's formulate each stem to the shortest relevant word found in the database.</p><p>Unfortunately, it's not as straightforward as the previous examples, as the <code class="literal">stemCompletion</code> function takes a character vector of words instead of documents that we have in our corpus. So thus, we have to write our own transformation function with the previously used <code class="literal">content_transformer</code> helper. The basic idea is to split each documents into words by a space, apply the <code class="literal">stemCompletion</code> function, and then concatenate the words into sentences again:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; v &lt;- tm_map(v, content_transformer(function(x, d) {</strong></span>
<span class="strong"><strong>+         paste(stemCompletion(</strong></span>
<span class="strong"><strong>+                 strsplit(stemDocument(x), ' ')[[1]],</strong></span>
<span class="strong"><strong>+                 d),</strong></span>
<span class="strong"><strong>+         collapse = ' ')</strong></span>
<span class="strong"><strong>+       }), d)</strong></span>
</pre></div><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="tip10"/>Tip</h3><p>The preceding example is rather resource hungry, so please be prepared for high CPU usage for around 30 to 60 minutes on a standard PC. As you can (technically) run the forthcoming code samples without actually performing this step, you may feel free to skip to the next code chunk, if in a hurry.</p></div></div><p>It took some time, huh? Well, we had to iterate through all the words in each document found in the corpus , but it's <a class="indexterm" id="id544"/>well worth the trouble! Let's see the top used terms in the cleaned corpus:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; tdm &lt;- TermDocumentMatrix(v)</strong></span>
<span class="strong"><strong>&gt; findFreqTerms(tdm, lowfreq = 100)</strong></span>
<span class="strong"><strong> [1] "algorithm"     "analysing"     "bayesian"      "calculate"    </strong></span>
<span class="strong"><strong> [5] "cluster"       "computation"   "data"          "distributed"  </strong></span>
<span class="strong"><strong> [9] "estimate"      "fit"           "function"      "general"      </strong></span>
<span class="strong"><strong>[13] "interface"     "linear"        "method"        "model"        </strong></span>
<span class="strong"><strong>[17] "multivariable" "network"       "plot"          "random"       </strong></span>
<span class="strong"><strong>[21] "regression"    "sample"        "selected"      "serial"       </strong></span>
<span class="strong"><strong>[25] "set"           "simulate"      "statistic"     "test"         </strong></span>
<span class="strong"><strong>[29] "time"          "tool"          "variable"     </strong></span>
</pre></div><p>While previously the very same command returned 23 terms, out of which we removed 3, now we see more than 30 words occurring more than 100 times in the corpus. We got rid of the plural versions of the nouns and a few other similar variations of the same terms, so the density of the document term matrix also increased:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; tdm</strong></span>
<span class="strong"><strong>&lt;&lt;TermDocumentMatrix (terms: 4776, documents: 5880)&gt;&gt;</strong></span>
<span class="strong"><strong>Non-/sparse entries: 27946/28054934</strong></span>
<span class="strong"><strong>Sparsity           : 100%</strong></span>
<span class="strong"><strong>Maximal term length: 35</strong></span>
<span class="strong"><strong>Weighting          : term frequency (tf)</strong></span>
</pre></div><p>We not only decreased the number of different words to be indexed in the next steps, but we also identified a few new terms that are to be ignored in our further analysis, for example, <code class="literal">set</code> does not seem to be an important word in the package descriptions.</p></div><div class="section" title="Lemmatisation"><div class="titlepage"><div><div><h2 class="title"><a id="ch07lvl2sec47"/>Lemmatisation</h2></div></div></div><p>While<a class="indexterm" id="id545"/> stemming terms, we started to remove characters from the end of words in the hope of finding the stem, which is a heuristic process often resulting in not-existing words, as<a class="indexterm" id="id546"/> we have seen previously. We tried to overcome this issue by completing these<a class="indexterm" id="id547"/> stems to the shortest meaningful words by using a dictionary, which might result in derivation in the meaning of the term, for example, removing the <code class="literal">ness</code> suffix.</p><p>Another way to reduce the number of inflectional forms of different terms, instead of deconstructing and then trying to rebuild the words, is morphological analysis with the help of a dictionary. This <a class="indexterm" id="id548"/>process is called lemmatisation, which looks for<a class="indexterm" id="id549"/> lemma (the canonical form of a word) instead of stems.</p><p>The Stanford NLP Group created and maintains a Java-based NLP tool <a class="indexterm" id="id550"/>called Stanford CoreNLP, which <a class="indexterm" id="id551"/>supports lemmatization besides many other NLP algorithms such as tokenization, sentence splitting, POS tagging, and syntactic parsing.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="tip11"/>Tip</h3><p>You can use CoreNLP from R via the <code class="literal">rJava</code> package<a class="indexterm" id="id552"/>, or you might install the <a class="indexterm" id="id553"/>
<code class="literal">coreNLP</code> package, which includes some wrapper functions around the <code class="literal">CoreNLP</code> Java library, which are meant for providing easy access to, for example, lammatisation. Please note that after installing the R package, you have to use the <code class="literal">downloadCoreNLP</code> function to actually install and make accessible the features of the Java library.</p></div></div></div></div>
<div class="section" title="Analyzing the associations among terms"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec49"/>Analyzing the associations among terms</h1></div></div></div><p>The previously <a class="indexterm" id="id554"/>computed <code class="literal">TermDocumentMatrix</code>, can also be used to identify the association between the cleaned terms found in the corpus. This simply suggests the correlation coefficient computed on the joint occurrence of term-pairs in the same document, which can be queried easily with the <code class="literal">findAssocs</code> function.</p><p>Let's see which words are associated with <code class="literal">data</code>:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; findAssocs(tdm, 'data', 0.1)</strong></span>
<span class="strong"><strong>             data</strong></span>
<span class="strong"><strong>set          0.17</strong></span>
<span class="strong"><strong>analyzing    0.13</strong></span>
<span class="strong"><strong>longitudinal 0.11</strong></span>
<span class="strong"><strong>big          0.10</strong></span>
</pre></div><p>Only four terms seem to have a higher correlation coefficient than 0.1, and it's not surprising at all that <code class="literal">analyzing</code> is among the top associated words. Probably, we can ignore the <code class="literal">set</code> term, but it seems that <code class="literal">longitudinal</code> and <code class="literal">big</code> data are pretty frequent idioms in package descriptions. So, what other <code class="literal">big</code> terms do we have?</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; findAssocs(tdm, 'big', 0.1)</strong></span>
<span class="strong"><strong>               big</strong></span>
<span class="strong"><strong>mpi           0.38</strong></span>
<span class="strong"><strong>pbd           0.33</strong></span>
<span class="strong"><strong>program       0.32</strong></span>
<span class="strong"><strong>unidata       0.19</strong></span>
<span class="strong"><strong>demonstration 0.17</strong></span>
<span class="strong"><strong>netcdf        0.15</strong></span>
<span class="strong"><strong>forest        0.13</strong></span>
<span class="strong"><strong>packaged      0.13</strong></span>
<span class="strong"><strong>base          0.12</strong></span>
<span class="strong"><strong>data          0.10</strong></span>
</pre></div><p>Checking the original corpus reveals that there are several R packages starting with <span class="strong"><strong>pbd</strong></span>, which stands for <a class="indexterm" id="id555"/>
<span class="strong"><strong>Programming with Big Data</strong></span>. The <code class="literal">pbd</code> packages are usually tied to Open MPI, which <a class="indexterm" id="id556"/>pretty well explains the high association between these terms.</p></div>
<div class="section" title="Some other metrics"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec50"/>Some other metrics</h1></div></div></div><p>And, of course, we can use the<a class="indexterm" id="id557"/> standard data analysis tools as well after quantifying our package descriptions a bit. Let's see, for example, the length of the documents in the corpus:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; vnchar &lt;- sapply(v, function(x) nchar(x$content))</strong></span>
<span class="strong"><strong>&gt; summary(vnchar)</strong></span>
<span class="strong"><strong>   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. </strong></span>
<span class="strong"><strong>   2.00   27.00   37.00   39.85   50.00  168.00</strong></span>
</pre></div><p>So, the average package description consists of around 40 characters, while there is a package with only two characters in the description. Well, two characters after removing numbers, punctuations, and the common words. To see which package has this very short description, we might simply call the <code class="literal">which.min</code> function:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; (vm &lt;- which.min(vnchar))</strong></span>
<span class="strong"><strong>[1] 221</strong></span>
</pre></div><p>And this is what's strange about it:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; v[[vm]]</strong></span>
<span class="strong"><strong>&lt;&lt;PlainTextDocument (metadata: 7)&gt;&gt;</strong></span>
<span class="strong"><strong>NA</strong></span>
<span class="strong"><strong>&gt; res[vm, ]</strong></span>
<span class="strong"><strong>    V1   V2</strong></span>
<span class="strong"><strong>221    &lt;NA&gt;</strong></span>
</pre></div><p>So, this is not a real package after all, but rather an empty row in the original table. Let's visually inspect the overall number of characters in the package descriptions:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; hist(vnchar, main = 'Length of R package descriptions',</strong></span>
<span class="strong"><strong>+     xlab = 'Number of characters')</strong></span>
</pre></div><div class="mediaobject"><img alt="Some other metrics" src="graphics/2028OS_07_02.jpg"/></div><p>The histogram<a class="indexterm" id="id558"/> suggests that most packages have a rather short description with no more than one sentence, based on the fact that an average English sentence includes around 15-20 words with 75-100 characters.</p></div>
<div class="section" title="The segmentation of documents"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec51"/>The segmentation of documents</h1></div></div></div><p>To identify the<a class="indexterm" id="id559"/> different groups of cleaned terms, based on the frequency and association of the terms in the documents of the corpus, one might directly use our <code class="literal">tdm</code> matrix to run, for example, the<a class="indexterm" id="id560"/> classic hierarchical cluster algorithm.</p><p>On the other hand, if you would rather like to cluster the R packages based on their description, we should compute a new matrix with <code class="literal">DocumentTermMatrix</code>, instead of the previously used <code class="literal">TermDocumentMatrix</code>. Then, calling the clustering algorithm on this matrix would result in the segmentation of the packages.</p><p>For more details on the available methods, algorithms, and guidance on choosing the appropriate functions for clustering, please see <a class="link" href="ch10.html" title="Chapter 10. Classification and Clustering">Chapter 10</a>, <span class="emphasis"><em>Classification and Clustering</em></span>. For now, we will fall back to the traditional <code class="literal">hclust</code> function, which provides a built-in way of running hierarchical clustering on distance matrices. For a quick demo, let's demonstrate this on the so-called <code class="literal">Hadleyverse</code>, which describes a useful collection of R packages developed by Hadley Wickham:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; hadleyverse &lt;- c('ggplot2', 'dplyr', 'reshape2', 'lubridate',</strong></span>
<span class="strong"><strong>+   'stringr', 'devtools', 'roxygen2', 'tidyr')</strong></span>
</pre></div><p>Now, let's identify<a class="indexterm" id="id561"/> which elements of the <code class="literal">v</code> corpus hold the cleaned terms of the previously listed packages:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; (w &lt;- which(res$V1 %in% hadleyverse))</strong></span>
<span class="strong"><strong>[1] 1104 1230 1922 2772 4421 4658 5409 5596</strong></span>
</pre></div><p>And then, we can simply compute the (dis)similarity matrix of the used terms:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; plot(hclust(dist(DocumentTermMatrix(v[w]))),</strong></span>
<span class="strong"><strong>+   xlab = 'Hadleyverse packages')</strong></span>
</pre></div><div class="mediaobject"><img alt="The segmentation of documents" src="graphics/2028OS_07_03.jpg"/></div><p>Besides the <code class="literal">reshape2</code> and <code class="literal">tidyr</code> packages that we covered in <a class="link" href="ch04.html" title="Chapter 4. Restructuring Data">Chapter 4</a>, <span class="emphasis"><em>Restructuring Data</em></span>, we can see two separate clusters in the previous plot (the highlighted terms in the following list are copied from the package descriptions):</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Packages that <span class="emphasis"><em>make</em></span> things a bit <span class="emphasis"><em>easier</em></span></li><li class="listitem" style="list-style-type: disc">Others dealing with the language, <span class="emphasis"><em>documentation</em></span> and <span class="emphasis"><em>grammar</em></span></li></ul></div><p>To verify this, you might be interested in the cleansed terms for each package:</p><div class="informalexample"><pre class="programlisting"><span class="strong"><strong>&gt; sapply(v[w], function(x) structure(content(x),</strong></span>
<span class="strong"><strong>+   .Names = meta(x, 'id')))</strong></span>
<span class="strong"><strong>                                  devtools </strong></span>
<span class="strong"><strong>     "tools make developing r code easier" </strong></span>
<span class="strong"><strong>                                     dplyr </strong></span>
<span class="strong"><strong>             "a grammar data manipulation" </strong></span>
<span class="strong"><strong>                                   ggplot2 </strong></span>
<span class="strong"><strong>      "an implementation grammar graphics" </strong></span>
<span class="strong"><strong>                                 lubridate </strong></span>
<span class="strong"><strong>        "make dealing dates little easier" </strong></span>
<span class="strong"><strong>                                  reshape2 </strong></span>
<span class="strong"><strong>   "flexibly reshape data reboot reshape " </strong></span>
<span class="strong"><strong>                                  roxygen2 </strong></span>
<span class="strong"><strong>                "insource documentation r" </strong></span>
<span class="strong"><strong>                                   stringr </strong></span>
<span class="strong"><strong>                "make easier work strings" </strong></span>
<span class="strong"><strong>                                     tidyr </strong></span>
<span class="strong"><strong>"easily tidy data spread gather functions"</strong></span>
</pre></div><p>An alternative and probably more appropriate, long-term approach for clustering documents based on NLP algorithms, would be fitting<a class="indexterm" id="id562"/> topic models, for example, via the <code class="literal">topicmodels</code> <a class="indexterm" id="id563"/>package. This R package comes with a detailed and very useful vignette, which includes some theoretical background as well as some hands-on examples. But for a quick start, you might simply try to run the <code class="literal">LDA</code> or <code class="literal">CTM</code> functions on our previously created <code class="literal">DocumentTermMatrix</code>, and specify the number of topics for the models to be built. A good start, based on our previous clustering example, might be <code class="literal">k=3</code>.</p></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch07lvl1sec52"/>Summary</h1></div></div></div><p>The preceding examples and quick theoretical background introduced text mining algorithms to structure plain English texts into numbers for further analysis. In the next chapter, we will concentrate on some similarly important methods in the process of data analysis, such as how to polish this kind of data in the means of identifying outliers, extreme values, and how to handle missing data.</p></div></body></html>