["```py\n> curl <address>\n\n```", "```py\n>>> r = requests.get(url,params)\n\n```", "```py\n>>> print(r.url)\n\n```", "```py\n>>> r.json()\n>>> r.text\n\n```", "```py\n>>> r.status_code\n\n```", "```py\n> curl –x POST  -d  <data> <url>\n\n```", "```py\n>>> r = requests.post(url,data)\n\n```", "```py\n> curl –i –X HEAD <url>\n\n```", "```py\n>>>  requests.head(url)\n\n```", "```py\n> curl -X PUT -d key1=value1 -d key2=value2 <url>\n\n```", "```py\n>>>  r = requests.put(url,data)\n\n```", "```py\n> curl -X DELETE -d key1=value1 -d key2=value2 <url>\n\n```", "```py\n>>>  r = requests.delete(url,data)\n\n```", "```py\n$ mongodb\n\n```", "```py\n$ mongoimport -d datasets -c bank --type csv --file bank-full.csv —headerline\n\n```", "```py\n$ mongo\n\n```", "```py\n$ use datasets\n$ show collections\n\n```", "```py\n$ db.bank.findOne()\n\n```", "```py\n$ db.bank.findOne({},{key:value,..})\n\n```", "```py\n>>> from pymongo import MongoClient\n>>> MONGODB_HOST = 'localhost'\n>>> MONGODB_PORT = 27017\n>>> DBS_NAME = 'datasets'\n>>> COLLECTION_NAME = 'bank'\n>>> connection = MongoClient(MONGODB_HOST, MONGODB_PORT)\n>>> collection = connection[DBS_NAME][COLLECTION_NAME]\n>>> customers = collection.find(projection=FIELDS)\n\n```", "```py\n > redis-server\n\n```", "```py\n>>> import redis\n>>> r = redis.StrictRedis(host='localhost', port=6379, db=1)\n>>> r.get(key)\n>>> r.set(key,value)\n\n```", "```py\n>>>if __name__ == \"__main__\":\n\n modelparameters = json.loads(open(sys.argv[1]).readline())\n\n service = modelservice(modelparameters)\n\n run_server(service)\n\n```", "```py\n>>>  def run_server(app):\n import paste\n from paste.translogger import TransLogger\n app_ = TransLogger(app)\n cherrypy.tree.graft(app_, '/')\n cherrypy.config.update({\n 'engine.autoreload.on': True,\n 'log.screen': True,\n 'server.socket_port': 5000,\n 'server.socket_host': '0.0.0.0'\n })\n cherrypy.engine.start()\n cherrypy.engine.block()\n\n```", "```py\n> python modelserver.py parameters.json\n\n```", "```py\n>>> def modelservice(model_parameters):\n …return app\nWhat is the definition of app? If we examine the beginning of the modelservice.py file, we see that app is defined using the Flask library:\n>>> app = Flask(__name__)\n… app.config.update(CELERY_BROKER_URL='redis://localhost:6379',CELERY_RESULT_BACKEND='redis://localhost:6379')\n… celery = Celery(app.import_name, backend=app.config['CELERY_RESULT_BACKEND'],broker=app.config['CELERY_BROKER_URL'])\n… celery.conf.update(app.config)\n\n```", "```py\n> celery worker -A modelservice.celery\n\n```", "```py\ncurl -X POST http://0.0.0.0:5000/train/ -d @job.json --header \"Content-Type: application/json\"\n\n```", "```py\n>>> @app.route(\"/train/\",methods=[\"POST\"])\n… def train():\n…    try:\n …       parsed_parameters = request.json\n …   trainTask = train_task.apply_async(args=[parsed_parameters])\n …   return json.dumps( {\"job_id\": trainTask.id } )\n except:\n …    print(traceback.format_exc())\n\n```", "```py\n>>> @celery.task(bind=True)\n… def train_task(self,parameters):\n…   try: \n …       spark_conf = start_conf(parameters)\n…        model.set_model(parameters)\n…        messagehandler = MessageHandler(self)\n …       model.train(parameters,messagehandler=messagehandler,sc=spark_conf)\n …   except:\n…        messagehandler.update('FAILURE',traceback.format_exc())\n\n```", "```py\n>>> class MessageHandler:\n …\n …def __init__(self,parent):\n …    self.parent = parent\n …   self.task_id = parent.request.id\n …\n … def update(self,state,message):\n …    self.parent.update_state(state=state,meta={\"message\": message})\n …\n …def get_id(self):\n …return self.task_id\n\n```", "```py\n>>> @app.route('/training/status/<task_id>')\n… def training_status(task_id):\n…    try: \n…        task = train_task.AsyncResult(task_id)\n…        message = \"\"\n…        if task.state == 'PENDING':\n …           response = {\n …               'status': task.status,\n …               'message': \"waiting for job {0} to start\".format(task_id)\n …           }\n …       elif task.state != 'FAILED':\n …           if task.info is not None:\n …               message = task.info.get('message','no message')\n …           response = {\n …               'status': task.status,\n …               'message': message\n …           }\n …       else:\n…            if task.info is not None:\n …               message = task.info.get('message','no message')\n …           response = {\n …              'status': task.status,\n …              'message': message \n …           }\n …       return json.dumps(response)\n …   except:\n …       print(traceback.format_exc())\n\n```", "```py\n>>> def start_conf(jobparameters):\n … conf = SparkConf().setAppName(\"prediction-service\")\n … conf.set(\"spark.driver.allowMultipleContexts\",True)\n …conf.set(\"spark.mongodb.input.uri\",jobparameters.get('inputCollection',\\\n …     \"mongodb://127.0.0.1/datasets.bank?readPreference=primaryPreferred\"))\n … conf.set(\"spark.mongodb.output.uri\",jobparameters.get('outputCollection',\\\n …    \"mongodb://127.0.0.1/datasets.bankResults\"))\n …return conf\n\n```", "```py\nexport PYSPARK_SUBMIT_ARGS=\"--packages org.mongodb.spark:mongo-spark-connector_2.10:1.0.0 pyspark-shell\"\n\n```", "```py\n>>> model = ModelFactory()\n\n```", "```py\n>>> class ModelFactory:\n\n...  def __init__(self):\n…    self._model = None\n\n…  def set_model(self,modelparameters):\n…    module = importlib.import_module(modelparameters.get('name'))\n…    model_class = getattr(module, modelparameters.get('name'))\n…    self._model = model_class(modelparameters)\n\n…  def get_model(self,modelparameters,modelkey):\n…    module = importlib.import_module(modelparameters.get('name'))\n…    model_class = getattr(module, modelparameters.get('name'))\n…    self._model = model_class(modelparameters)\n…    self._model.get_model(modelkey)\n\n…  def train(self,parameters,messagehandler,sc):\n…    self._model.train(parameters,messagehandler,sc)\n\n…  def predict(self,parameters,input_data):\n…    return self._model.predict(parameters,input_data)\n\n…  def predict_all(self,parameters,messagehandler,sc):\n…    self._model.predict_all(parameters,messagehandler,sc)\n\n```", "```py\n>>> def train(self,parameters,messagehandler,spark_conf):\n…        try:\n…            sc = SparkContext(conf=spark_conf, pyFiles=['modelfactory.py', 'modelservice.py'])\n…            sqlContext = SQLContext(sc)\n…            iterations = parameters.get('iterations',None)\n…            weights = parameters.get('weights',None)\n…           intercept = parameters.get('intercept',False)\n…            regType = parameters.get('regType',None)\n …           data = sqlContext.\\\n …               createDataFrame(\\\n …               sqlContext.read.format(\"com.mongodb.spark.sql.DefaultSource\").\\\n …               load().\\\n …               map(lambda x: DataParser(parameters).parse_line(x)))\n …           lr = LogisticRegression()\n …           pipeline = Pipeline(stages=[lr])\n …           paramGrid = ParamGridBuilder()\\\n …               .addGrid(lr.regParam, [0.1]) \\\n …               .build()\n\n …           crossval = CrossValidator(estimator=pipeline,\\\n …                 estimatorParamMaps=paramGrid,\\\n …                 evaluator=BinaryClassificationEvaluator(),\\\n …                 numFolds=2)\n …           messagehandler.update(\"SUBMITTED\",\"submitting training job\")\n …           crossvalModel = crossval.fit(data)\n …           self._model = crossvalModel.bestModel.stages[-1]\n …           self._model.numFeatures = len(data.take(1)[0]['features'])\n …           self._model.numClasses = len(data.select('label').distinct().collect())\n …          r = redis.StrictRedis(host='localhost', port=6379, db=1)\n …          r.set( messagehandler.get_id(), self.serialize(self._model) )\n …          messagehandler.update(\"COMPLETED\",\"completed training job\")\n …          sc.stop()\n …      except:\n …          print(traceback.format_exc())\n ….          messagehandler.update(\"FAILED\",traceback.format_exc())\n\n```", "```py\n>>> def parse_line(self,input,train=True):\n…        try:\n…            if train:\n…               if self.schema_dict.get('label').get('values',None) is not None:\n …                   label = self.schema_dict.\\\n …                   get('label').\\\n …                   get('values').\\\n …                   get(input[self.schema_dict.\\\n …                   get('label').\\\n…                    get('key')])\n …               else:\n …                   label = input[self.schema_dict.\\\n …                   get('label').\\\n …                   get('key')]\n …           features = []\n …           for f in self.schema_dict['features']:\n …               if f.get('values',None) is not None:\n …                   cat_feature = [ 0 ] * len(f['values'].keys())\n …                  if len(f['values'].keys()) > 1: # 1 hot encoding\n …                       cat_feature[f['values'][str(input[f.get('key')])]] = 1\n …                   features += cat_feature # numerical\n …               else:\n …                   features += [ input[f.get('key')] ]\n\n …           if train:\n …               Record = Row(\"features\", \"label\")\n …               return Record(Vectors.dense(features),label)\n …           else:\n …               return Vectors.dense(features)\n\n…        except:\n…            print(traceback.format_exc())\n…            pass\n\n```", "```py\n>>> def serialize(self,model):\n…        try:\n…            model_dict = {}\n …           model_dict['weights'] = model.weights.tolist()\n…            model_dict['intercept'] = model.intercept\n …           model_dict['numFeatures'] = model.numFeatures\n…            model_dict['numClasses'] = model.numClasses\n …           return json.dumps(model_dict)\n…        except:\n …           raise Exception(\"failed serializing model: {0}\".format(traceback.format_exc()))\n\n```", "```py\n>>> @app.route(\"/predict/\",methods=['POST'])\n… def predict():\n…    try:\n…        parsed_parameters = request.json\n …       model.get_model(parsed_parameters,parsed_parameters.get('modelkey'))\n…        score = model.predict(parsed_parameters,parsed_parameters.get('record'))\n…        return json.dumps(score)\n…    except:\n …       print(traceback.format_exc())\n\n```", "```py\n>>> def get_model(self,modelkey):\n…        try:\n…            r = redis.StrictRedis(host='localhost', port=6379, db=1)\n…            model_dict = json.loads(r.get(modelkey))\n …           self._model = LogisticRegressionModel(weights=Vectors.dense(model_dict['weights']),\\\n …               intercept=model_dict['intercept'],\\\n …               numFeatures=model_dict['numFeatures'],\\\n …               numClasses=model_dict['numClasses']\n …               )\n …       except:\n …          raise Exception(\"couldn't load model {0}: {1}\".format(modelkey,traceback.format_exc()))\n\n```", "```py\n>>> def predict(self,parameters,input_data):\n…        try:\n…            if self._model is not None:\n …               return self._model.predict(DataParser(parameters).parse_line(input_data,train=False))\n …           else:\n …               return \"Error, no model is trained to give predictions\"\n …       except:\n …           print(traceback.format_exc())\n\n```", "```py\n>>> @app.route(\"/predictall/\",methods=[\"POST\"])\n… def predictall():\n…    try:\n…       parsed_parameters = request.json\n\n…        predictTask = predict_task.apply_async(args=[parsed_parameters])\n…        return json.dumps( {\"job_id\": predictTask.id } )\n…    except:\n…        print(traceback.format_exc())\n\n```", "```py\n>>> @celery.task(bind=True)\n… def predict_task(self,parameters):\n…    try: \n…        spark_conf = start_conf(parameters)\n …       messagehandler = MessageHandler(self)\n…        model.get_model(parameters,parameters.get('modelkey'))\n…        print(model._model._model)\n …       model.predict_all(parameters,messagehandler=messagehandler,sc=spark_conf)\n…    except:\n…        messagehandler.update('FAILURE',traceback.format_exc())\n\n```", "```py\n>>> def predict_all(self,parameters,messagehandler,spark_conf):\n…        try:\n…            sc = SparkContext(conf=spark_conf, pyFiles=['modelfactory.py', 'modelservice.py'])\n…            sqlContext = SQLContext(sc)\n…            Record = Row(\"score\",\"value\")\n…           scored_data = sqlContext.\\\n…                createDataFrame(\\\n…                sqlContext.read.format(\"com.mongodb.spark.sql.DefaultSource\").\\\n…                load().\\\n…                map(lambda x: Record(self._model.predict(DataParser(parameters).parse_line(x,train=False)),x)))\n…           messagehandler.update(\"SUBMITTED\",\"submitting scoring job\")\n… scored_data.write.format(\"com.mongodb.spark.sql.DefaultSource\").mode(\"overwrite\").save()\n…            sc.stop()\n…        except:\n…         messagehander.update(\"FAILED\",traceback.format_exc())\n\n```"]