- en: '*Chapter 5*: Cleaning, Transforming, and Enriching Data'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第五章*：清理、转换和丰富数据'
- en: 'In the previous two chapters, you learned how to build data pipelines that
    could read and write from files and databases. In many instances, these skills
    alone will enable you to build production data pipelines. For example, you will
    read files from a data lake and insert them into a database. You now have the
    skills to accomplish this. Sometimes, however, you will need to do something with
    the data after extraction but prior to loading. What you will need to do is clean
    the data. Cleaning is a vague term. More specifically, you will need to check
    the validity of the data and answer questions such as the following: Is it complete?
    Are the values within the proper ranges? Are the columns the proper type? Are
    all the columns useful?'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两章中，你学习了如何构建能够从文件和数据库中读取和写入的数据管道。在许多情况下，仅这些技能就足以让你构建生产级的数据管道。例如，你将读取数据湖中的文件并将它们插入到数据库中。你现在有了完成这项任务的能力。然而，有时在提取数据之后但在加载之前，你可能需要对数据进行一些操作。你需要做的是清理数据。清理是一个模糊的术语。更具体地说，你需要检查数据的有效性，并回答以下问题：它是否完整？值是否在适当的范围内？列的类型是否正确？所有列是否都有用？
- en: In this chapter, you will learn the basic skills needed to perform exploratory
    data analysis. Once you have an understanding of the data, you will use that knowledge
    to fix common data problems that you have discovered – such as dropping columns
    and replacing nulls. You will learn many useful methods available in the `pandas`
    library for Python. These skills will allow you to quickly perform exploratory
    data analysis and allow you to clean the data, all within Python. These skills
    will become the tools for the transform stage of the ETL data engineering process.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习执行探索性数据分析所需的基本技能。一旦你对数据有了了解，你将利用这些知识来解决你发现的一些常见数据问题——例如删除列和替换空值。你将学习`pandas`库中许多有用的方法。这些技能将使你能够快速执行探索性数据分析，并允许你在Python中清理数据。这些技能将成为ETL数据工程流程转换阶段的工具。
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Performing exploratory data analysis in Python
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Python中执行探索性数据分析
- en: Handling common data issues using pandas
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用pandas处理常见数据问题
- en: Cleaning data using Airflow
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Airflow清理数据
- en: Performing exploratory data analysis in Python
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Python中执行探索性数据分析
- en: Before you can clean your data, you need to know what your data looks like.
    As a data engineer, you are not the domain expert and are not the end user of
    the data, but you should know what the data will be used for and what valid data
    would look like. For example, you do not need to be a demographer to know that
    an `age` field should not be negative, and the frequency of values over 100 should
    be low.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在你清理数据之前，你需要了解你的数据看起来是什么样子。作为一个数据工程师，你并不是领域专家，也不是数据的最终用户，但你应该知道数据将用于什么，以及有效的数据应该是什么样子。例如，你不需要是人口统计学家就能知道一个`年龄`字段不应该为负数，并且超过100的值的频率应该很低。
- en: Downloading the data
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下载数据
- en: In this chapter, you will use real e-scooter data from the City of Albuquerque.
    The data contains trips taken using e-scooters from May to July 22, 2019\. You
    will need to download the e-scooter data from [https://github.com/PaulCrickard/escooter/blob/master/scooter.csv](https://github.com/PaulCrickard/escooter/blob/master/scooter.csv).
    The repository also contains the original Excel file as well as some other summary
    files provided by the City of Albuquerque.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将使用来自阿尔伯克基市的真实电动滑板车数据。这些数据包含从2019年5月到7月22日使用电动滑板车进行的行程。你需要从[https://github.com/PaulCrickard/escooter/blob/master/scooter.csv](https://github.com/PaulCrickard/escooter/blob/master/scooter.csv)下载电动滑板车数据。该存储库还包含原始的Excel文件以及阿尔伯克基市提供的其他一些总结文件。
- en: Basic data exploration
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基本数据探索
- en: Before you can clean your data, you have to know what your data looks like.
    The process of understanding your data is called **exploratory data analysis**
    (**EDA**). You will look at the shape of your data, the number of rows and columns,
    as well as the data types in the columns, and the ranges of values. You can perform
    a much more in-depth analysis, such as the distribution of the data, or the skewness,
    but for this section, you will learn how to quickly understand your data so that
    in the next section, you can clean it.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在你清理数据之前，你必须知道你的数据看起来是什么样子。理解你的数据的过程被称为 **探索性数据分析** （**EDA**）。你将查看数据的形状、行数和列数，以及列中的数据类型和值的范围。你可以执行更深入的分析，例如数据的分布或偏度，但在这个部分，你将学习如何快速理解你的数据，以便在下一部分中你可以清理它。
- en: In the two previous chapters, you learned how to import files and databases
    into pandas DataFrames. That knowledge will be expanded in this section, as DataFrames
    will be the tool used for the EDA.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两个章节中，你学习了如何将文件和数据库导入 pandas DataFrame。这部分知识将在本节中扩展，因为 DataFrame 将是用于 EDA
    的工具。
- en: 'To begin with, you will need to import `pandas` and read the `.csv` file:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要导入 `pandas` 并读取 `.csv` 文件：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: With the data in a DataFrame, you can now explore it, and then analyze it.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在 DataFrame 中有了数据后，你现在可以探索它，然后分析它。
- en: Exploring the data
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索数据
- en: 'Now you can start to look at the data. The first thing you will probably want
    to do is print it out. But before you get to that, take a look at the columns
    and the data types using `columns` and `dtypes`:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以开始查看数据了。你可能会做的第一件事就是打印它。但在你做到这一点之前，看看列和数据类型，使用 `columns` 和 `dtypes`：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You will see that you have eleven columns, five of which are integers (all of
    the columns with ID in their name) and the rest are objects. Objects are what
    a DataFrame uses as `dtype` when there are mixed types. Also, `DURATION` should
    stand out because it is the only column name in all capitals. In the next section,
    you will fix common errors, such as the column cases are not uniform (all lowercase
    or uppercase) and make the `dtypes` object proper types, such as `strings` for
    text data and `datetimes` for dates and times.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到你有 11 列，其中 5 列是整数（所有名称中包含 ID 的列）其余的是对象。对象是 DataFrame 在存在混合类型时用作 `dtype`
    的内容。此外，`DURATION` 应该很突出，因为它是所有大写字母的唯一列名。在下一节中，你将修复常见的错误，例如列的大小写不统一（全部小写或大写）以及将
    `dtypes` 对象转换为适当的类型，例如 `strings` 用于文本数据，`datetimes` 用于日期和时间。
- en: 'Now you know what you have for columns and types, let''s look at the data.
    You can print out the first five records using `head()`:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经知道了列和类型，让我们来看看数据。你可以使用 `head()` 打印出前五条记录：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The opposite of `head()` is `tail()`. Both of these methods default to showing
    5 rows. However, you can pass an integer as a parameter that specifies how many
    rows to show. For example, you could pass `head(10)` to see the first 10 rows.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '`head()` 的对立面是 `tail()`。这两个方法默认显示 5 行。然而，你可以传递一个整数作为参数来指定要显示的行数。例如，你可以传递 `head(10)`
    来查看前 10 行。'
- en: 'Notice in both the `head()` and `tail()` output that the third column is `...`,
    and then there are two more columns after this. The display is cropping out the
    columns in the middle. If you were to print the entire DataFrame, the same thing
    would happen with the rows as well. To display all the columns, you can change
    the number of columns to show using the `set_options` method:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 注意在 `head()` 和 `tail()` 输出中，第三列是 `...`，然后是此之后的两个更多列。显示正在裁剪中间的列。如果你要打印整个 DataFrame，行也会发生相同的事情。要显示所有列，你可以使用
    `set_options` 方法更改要显示的列数：
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now, when you use `head()`, you will see all the column. However, depending
    on the width of your display, the output may be wrapped to fit.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当你使用 `head()` 时，你会看到所有列。然而，根据你显示器的宽度，输出可能会被换行以适应。
- en: 'The `head` and `tail` methods display all the columns, but if you are only
    interested in a single column, you can specify it like you would in a Python dictionary.
    The following code prints the `DURATION` column:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '`head` 和 `tail` 方法显示所有列，但如果你只对单个列感兴趣，你可以像在 Python 字典中一样指定它。以下代码打印了 `DURATION`
    列：'
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Again, notice that the output is cropped with `...`, but this time for the rows.
    The result is the combination of `head()` and `tail()`. You could change this
    using the `display_max_rows` option as you did earlier with columns, but for this
    exploration, it is unnecessary.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 再次注意，输出被 `...` 截断，但这次是针对行的。结果是 `head()` 和 `tail()` 的组合。你可以使用 `display_max_rows`
    选项来改变它，就像你之前改变列一样，但在这个探索中，这是不必要的。
- en: 'Just like you can display a single column, you can display a list of columns
    using double `[]`, as shown in the following code block:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 就像你可以显示单个列一样，你可以使用双 `[]` 显示列的列表，如下面的代码块所示：
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You can also pull a sample from your data using `sample()`. The sample methods
    allow you to specify how many rows you would like to pull. The results are shown
    in the following code block:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以使用 `sample()` 从你的数据中抽取样本。样本方法允许你指定你想要抽取多少行。结果如下面的代码块所示：
- en: '[PRE6]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Notice that the index of the rows is not incremental, but rather it jumps around.
    It should, as it is a sample.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，行的索引不是递增的，而是跳跃式的。它应该是这样的，因为它是一个样本。
- en: 'You can also slice the data. Slicing takes the format of `[start:end]`, where
    a blank is the first or last row depending on which position is blank. To slice
    the first 10 rows, you can use the following notation:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以切片数据。切片的格式为 `[start:end]`，其中空白表示第一行或最后一行，具体取决于哪个位置是空的。要切片前10行，你可以使用以下表示法：
- en: '[PRE7]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Likewise, to grab the rows from 10 to the end (34,225), you can use the following
    notation:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，要获取从第10行到结束（34,225）的行，你可以使用以下表示法：
- en: '[PRE8]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'You can also slice the frame starting on the third row and ending before nine,
    as shown in the following code block:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以从第三行开始切片，到第九行之前结束，如下面的代码块所示：
- en: '[PRE9]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Sometimes, you know the exact row you want, and instead of slicing it, you
    can select it using `loc()`. The `loc` method takes the index name, which, in
    this example, is an integer. The following code and output show a single row selected
    with `loc()`:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，你知道你想要的确切行，而不是切片，你可以使用 `loc()` 来选择它。`loc` 方法接受索引名称，在这个例子中是一个整数。下面的代码和输出显示了使用
    `loc()` 选择的单行：
- en: '[PRE10]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Using `at()`, with the position, as you did in the slicing examples, and a
    column name, you can select a single value. For example, this can be done to know
    the duration of the trip in the second row:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `at()`，与切片示例中的位置一样，以及列名，你可以选择单个值。例如，这可以用来知道第二行的行程时长：
- en: '[PRE11]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Slicing and using `loc()` and `at()` pull data based on position, but you can
    also use DataFrames to select rows based on some condition. Using the `where`
    method, you can pass a condition, as shown in the following code block:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 切片和使用 `loc()` 和 `at()` 是基于位置拉取数据，但你也可以使用 DataFrames 根据某些条件选择行。使用 `where` 方法，你可以传递一个条件，如下面的代码块所示：
- en: '[PRE12]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The preceding code and results show the results of `where` with the condition
    of the user ID being equal to `8417864`. The results replace values that do not
    meet the criteria as `NaN`. This will be covered in the next section.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码和结果显示了 `where` 方法在用户ID等于 `8417864` 的条件下的结果。结果将不符合标准的数据替换为 `NaN`。这将在下一节中介绍。
- en: 'You can get the same results similar to the preceding example with the exception
    of using a different notation, and this method will not include the `NaN` rows.
    You can pass the condition into the DataFrame as you did with column names. The
    following example shows you how:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用与前面示例类似的结果，只是使用不同的表示法，并且这种方法将不包括 `NaN` 行。你可以将条件传递到 DataFrame 中，就像你传递列名一样。以下示例展示了如何操作：
- en: '[PRE13]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The results of the preceding code is the same as the `where()` example, but
    without the `NaN` rows, so the DataFrame will only have four rows.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的结果与 `where()` 示例相同，但没有 `NaN` 行，因此 DataFrame 将只有四行。
- en: 'Using both notations, you can combine conditional statements. By using the
    same user ID condition, you can add a trip ID condition. The following example
    shows you how:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这两种表示法，你可以组合条件语句。通过使用相同的用户ID条件，你可以添加行程ID条件。以下示例展示了如何操作：
- en: '[PRE14]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Using the second notation, the output is as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 使用第二种表示法，输出如下：
- en: '[PRE15]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In the preceding examples, the conditions were assigned to a variable and combined
    in both the `where` and secondary notation, generating the expected results.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，条件被分配给一个变量，并在 `where` 和二级表示法中结合，生成了预期的结果。
- en: Analyzing the data
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分析数据
- en: 'Now that you have seen the data, you can start to analyze it. By using the
    `describe` method, you can see a series of statistics pertaining to your data.
    In statistics, there is a set of statistics referred to as the five-number summary,
    and `describe()` is a variant of that:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经看到了数据，你可以开始分析它。通过使用 `describe` 方法，你可以看到一系列与你的数据相关的统计数据。在统计学中，有一组被称为五数摘要的统计数据，`describe()`
    是其变体：
- en: '[PRE16]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The `describe` method is not very useful unless you have numeric data. If you
    were looking at ages for example, it would quickly show you the distribution of
    ages, and you would be able to quickly see errors such as negative ages or too
    many ages over 100\.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '`describe` 方法在没有数值数据的情况下并不很有用。例如，如果你在查看年龄，它会快速显示年龄分布，你能够快速看到错误，如负年龄或超过 100
    岁的年龄过多。'
- en: 'Using `describe()` on a single column is sometimes more helpful. Let''s try
    looking at the `start_location_name` column. The code and results are shown in
    the following code block:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在单个列上使用 `describe()` 有时更有帮助。让我们尝试查看 `start_location_name` 列。代码和结果如下所示：
- en: '[PRE17]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The data is not numeric, so we get a different set of statistics, but these
    provide some insight. Of the `34220` starting locations, there are actually `2972`
    unique locations. The top location (`1898 Mountain Rd NW`) accounts for `1210`
    trip starting locations. Later, you will geocode this data — add coordinates to
    the address — and knowing the unique values means you only have to geocode those
    2,972 and not the full 34,220.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 数据不是数值型的，所以我们得到一组不同的统计数据，但这些提供了一些见解。在 `34220` 个起始位置中，实际上有 `2972` 个独特的位置。最热门的位置（`1898
    Mountain Rd NW`）占 `1210` 个行程的起始位置。稍后，你将对这些数据进行地理编码——为地址添加坐标——知道唯一值意味着你只需要地理编码这
    2,972 个，而不是全部 34,220 个。
- en: 'Another method that allows you to see details about your data is `value_counts`.
    The `value_counts` method will give you the value and count for all unique values.
    We need to call it to a single column, which is done in the following snippet:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种允许你查看数据细节的方法是 `value_counts`。`value_counts` 方法将为你提供所有唯一值的值和计数。我们需要将其调用到一个单列上，如下面的代码片段所示：
- en: '[PRE18]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'From this method, you can see that `0:04:00` is at the top with a frequency
    of `825` — which you could have found out with `describe()` — but you can also
    see the frequency of all the other values. To see the frequency as a percentage,
    you can pass the normalize parameter (which is `False` by default):'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个方法中，你可以看到 `0:04:00` 是频率最高的，频率为 `825` —— 你可以用 `describe()` 找到这个信息——但你也可以看到其他所有值的频率。要查看频率作为百分比，你可以传递
    `normalize` 参数（默认值为 `False`）：
- en: '[PRE19]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: You will notice that no single value makes up a significant percentage of the
    duration.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到没有任何单个值占用了显著的比例。
- en: 'You can also pass the `dropna` parameter. By default, `value_counts()` sets
    it to `True` and you will not see them. Setting it to `False`, you can see that
    `end_location_name` is missing `2070` entries:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以传递 `dropna` 参数。默认情况下，`value_counts()` 将其设置为 `True`，你将看不到它们。将其设置为 `False`，你可以看到
    `end_location_name` 缺少 `2070` 个条目：
- en: '[PRE20]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The best way to find out how many missing values you have in your columns is
    to use the `isnull()` method. The following code combines `isnull()` with `sum()`
    to get the counts:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 要找出你列中缺失值的数量，最好的方法是使用 `isnull()` 方法。以下代码将 `isnull()` 与 `sum()` 结合起来以获取计数：
- en: '[PRE21]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Another parameter of `value_counts()` is `bins`. The scooter dataset does not
    have a good column for this, but using a numeric column, you would get results
    like the following:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '`value_counts()` 的另一个参数是 `bins`。滑板车数据集没有很好的列来表示这个参数，但使用数值列，你会得到以下结果：'
- en: '[PRE22]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: These results are fairly meaningless, but if it is used on a column such as
    `age`, it would come in handy as you could create age groups quickly and get an
    idea of the distribution.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果相当没有意义，但如果它用于像 `age` 这样的列，它将非常有用，因为你能够快速创建年龄组并了解分布情况。
- en: Now that you have explored and analyzed the data, you should have an understanding
    of what the data is and what the issues are — for example, nulls, improper `dtypes`,
    combined, and fields. With that knowledge, you can start to clean the data. The
    next section will walk you through how to fix common data problems.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经探索并分析了数据，你应该了解数据是什么以及存在的问题——例如，空值、不正确的 `dtypes`、组合字段等。有了这些知识，你就可以开始清理数据。下一节将指导你如何修复常见的数据问题。
- en: Handling common data issues using pandas
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 pandas 处理常见数据问题
- en: Your data may feel special, it is unique, you have created the world's best
    systems for collecting it, and you have done everything you can to ensure it is
    clean and accurate. Congratulations! But your data will almost certainly have
    some problems, and these problems are not special, or unique, and are probably
    a result of your systems or data entry. The e-scooter dataset is collected using
    GPS with little to no human input, yet there are end locations that are missing.
    How is it possible that a scooter was rented, ridden, and stopped, yet the data
    doesn't know where it stopped? Seems strange, yet here we are. In this section,
    you will learn how to deal with common data problems using the e-scooter dataset.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 你的数据可能感觉特别，它是独特的，你已经创建了世界上最好的系统来收集它，并且你已经尽了一切努力确保它是干净和准确的。恭喜！但你的数据几乎肯定会有一些问题，这些问题并不特殊或独特，可能是你的系统或数据输入的结果。电动滑板车数据集是通过GPS收集的，几乎没有人工输入，但仍有结束位置缺失。一辆滑板车被租用、骑行并停止，但数据却不知道它停在哪里？这似乎很奇怪，但我们就在这里。在本节中，你将学习如何使用电动滑板车数据集来处理常见的数据问题。
- en: Drop rows and columns
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 删除行和列
- en: Before you modify any fields in your data, you should first decide whether you
    are going to use all the fields. Looking at the e-scooter data, there is a field
    named `region_id`. This field is a code used by the vendor to label Albuquerque.
    Since we are only using the Albuquerque data, we don't need this field as it adds
    nothing to the data.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在你修改数据中的任何字段之前，你应该首先决定你是否将使用所有字段。查看电动滑板车数据，有一个名为`region_id`的字段。这个字段是供应商用来标记阿尔伯克基的代码。由于我们只使用阿尔伯克基的数据，我们不需要这个字段，因为它对数据没有任何贡献。
- en: 'You can drop columns using the `drop` method. The method will allow you to
    specify whether to drop a row or a column. Rows are the default, so we will specify
    `columns`, as shown in the following code block:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用`drop`方法来删除列。该方法将允许你指定是删除行还是列。默认情况下是行，所以我们将指定`columns`，如下面的代码块所示：
- en: '[PRE23]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Specifying the columns to drop, you also need to add `inplace` to make it modify
    the original DataFrame.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 指定要删除的列时，你还需要添加`inplace`以使其修改原始DataFrame。
- en: 'To drop a row, you only need to specify `index` instead of `columns`. To drop
    the row with the index of `34225`, you need to use the following code:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 要删除一行，你只需要指定`index`而不是`columns`。要删除索引为`34225`的行，你需要使用以下代码：
- en: '[PRE24]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The preceding code works when you want to drop an entire column or row, but
    what if you wanted to drop them based on conditions?
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码在你想要删除整个列或行时有效，但如果你想要根据条件删除它们怎么办？
- en: The first condition you may want to consider is where there are nulls. If you
    are missing data, the column and row may not be useful, or may distort the data.
    To handle this, you can use `dropna()`.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能首先想要考虑的第一个条件是存在null值。如果你缺失数据，列和行可能没有用，或者可能扭曲数据。为了处理这种情况，你可以使用`dropna()`。
- en: 'By using `dropna()`, you can pass `axis`, `how`, `thresh`, `subset`, and `inplace`
    as parameters:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用`dropna()`，你可以将`axis`、`how`、`thresh`、`subset`和`inplace`作为参数传递：
- en: '`axis` specifies rows or columns with indexes or columns (0 or 1). It defaults
    to rows.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`axis`指定具有索引或列的行或列（0或1）。默认为行。'
- en: '`how` specifies whether to drop rows or columns if all the values are null
    or if any value is null (all or any). It defaults to `any`.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`how`指定如果所有值都是null或任何值是null（全部或任何）时，是删除行还是列。默认为`any`。'
- en: '`thresh` allows more control than allowing you to specify an integer value
    of how many nulls must be present.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`thresh`允许你比指定必须存在的null值的整数数量有更多的控制。'
- en: '`subset` allows you to specify a list of rows or columns to search.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`subset`允许你指定要搜索的行或列的列表。'
- en: '`inplace` allows you to modify the existing DataFrame. It defaults to `False`.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`inplace`允许你修改现有的DataFrame。默认为`False`。'
- en: 'Looking at the e-scooter data, there are six rows with no start location name:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 观察电动滑板车数据，有六行没有起始位置名称：
- en: '[PRE25]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'To drop these rows, you can use `dropna` on `axis=0` with `how=any`, which
    are the defaults. This will, however, delete rows where other nulls exist, such
    as `end_location_name`. So, you will need to specify the column name as a subset,
    as shown in the following code block:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 要删除这些行，你可以使用`dropna`在`axis=0`上，`how=any`，这是默认设置。然而，这将会删除存在其他null值的行，例如`end_location_name`。因此，你需要指定列名作为子集，如下面的代码块所示：
- en: '[PRE26]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Then, when you select nulls in the `start_location_name` field as in the preceding
    code block, you will get an empty series:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，当你像前面代码块中那样在`start_location_name`字段中选择null值时，你将得到一个空序列：
- en: '[PRE27]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Dropping an entire column based on missing values may only make sense if a
    certain percentage of rows are null. For example, if more than 25% of the rows
    are null, you may want to drop it. You could specify this in the threshold by
    using something like the following code for the `thresh` parameter:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 基于缺失值删除整个列可能只有在一定百分比的行是空的情况下才有意义。例如，如果超过25%的行是空的，您可能想删除它。您可以通过使用以下代码为`thresh`参数指定这个阈值：
- en: '[PRE28]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Before showing more advanced filters for dropping rows, you may not want to
    drop nulls. You may want to fill them with a value. You can use `fillna()` to
    fill either null columns or rows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在展示更高级的删除行的过滤器之前，您可能不想删除空值。您可能想用值填充它们。您可以使用`fillna()`来填充空列或行：
- en: '[PRE29]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: What if you want to use `fillna()` but use different values depending on the
    column? You would not want to have to specify a column every time and run `fillna()`
    multiple times. You can specify an object to map to the DataFrame and pass it
    as the `value` parameter.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想使用`fillna()`但根据列使用不同的值怎么办？您可能不想每次都指定一个列并多次运行`fillna()`。您可以指定一个映射到DataFrame的对象，并将其作为`value`参数传递。
- en: 'In the following code, we will copy the rows where both the start and end location
    are null. Then, we will create a `value` object that assigns a street name to
    the `start_location_name` field and a different street address to the `end_location_name`
    field. Using `fillna()`, we pass the value to the `value` parameter, and then
    print those two columns in the DataFrame by showing the change:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，我们将复制起始位置和结束位置都为空的行。然后，我们将创建一个`value`对象，将街道名称分配给`start_location_name`字段，并将不同的街道地址分配给`end_location_name`字段。使用`fillna()`，我们将值传递给`value`参数，然后通过显示变化在DataFrame中打印这两个列：
- en: '[PRE30]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'You can drop rows based on more advanced filters; for example, what if you
    want to drop all the rows where the month was May? You could iterate through the
    DataFrame and check the month, and then drop it if it is May. Or, a much better
    way would be to filter out the rows, and then pass the index to the `drop` method.
    You can filter the DataFrame and pass it to a new one, as shown in the following
    code block:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以根据更高级的过滤器删除行；例如，如果您想删除所有月份为五月的行怎么办？您可以遍历DataFrame并检查月份，如果是五月就删除它。或者，一个更好的方法是对行进行过滤，然后将索引传递给`drop`方法。您可以对DataFrame进行过滤并将其传递给一个新的DataFrame，如下面的代码块所示：
- en: '[PRE31]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Then you can use `drop()` on the original DataFrame and pass the index for
    the rows in the `may` DataFrame, as shown:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以在原始DataFrame上使用`drop()`并传递`may` DataFrame中行的索引，如下所示：
- en: '[PRE32]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now, if you look at the months in the original DataFrame, you will see that
    May is missing:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果您查看原始DataFrame中的月份，您会看到五月是缺失的：
- en: '[PRE33]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Now that you have removed the rows and columns that you either do not need,
    or that were unusable on account of missing data, it is time to format them.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经删除了您不需要的行和列，或者由于缺失数据而无法使用的行和列，是时候对它们进行格式化了。
- en: Creating and modifying columns
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建和修改列
- en: 'The first thing that stood out in the preceding section was that there was
    a single column, duration, that was all in capital letters. Capitalization is
    a common problem. You will often find columns with all capitals, or with title
    case — where the first letter of every word is capitalized — and if a coder wrote
    it, you may find camel case — where the first letter is lowercase and the first
    letter of the next word is capital with no spaces, as in **camelCase**. The following
    code will make all the columns lowercase:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的部分中，最引人注目的是有一个单独的列，即持续时间，它全部是大写字母。大写是一个常见问题。您经常会发现全大写的列，或者首字母大写的列——每个单词的首字母都大写——如果是一个程序员编写的，您可能会发现驼峰式命名——首字母小写，下一个单词的首字母大写，没有空格，就像**camelCase**。下面的代码将所有列转换为小写：
- en: '[PRE34]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The preceding code is a condensed version of a `for` loop. What happens in
    the loop comes before the `for` loop. The preceding code says that for every item
    in `df.columns`, make it lowercase, and assign it back to `df.columns`. You can
    also use `capitalize()`, which is titlecase, or `upper()` as shown:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码是`for`循环的简化版本。循环中的内容在`for`循环之前发生。上述代码表示对于`df.columns`中的每个项目，将其转换为小写，并将其赋值回`df.columns`。您也可以使用`capitalize()`，它是标题大小写，或者使用`upper()`，如下所示：
- en: '[PRE35]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'You could also make the `DURATION` field lowercase using the `rename` method,
    as shown:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以使用`rename`方法将`DURATION`字段转换为小写，如下所示：
- en: '[PRE36]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: You will notice an `inplace` parameter set to `True`. When you used psycopg2
    to modify databases, you need to use `conn.commit()` to make it permanent, and
    you need to do the same with DataFrames. When you modify a DataFrame, the result
    is returned. You can store that new DataFrame (result) in a variable, and the
    original DataFrame is left unchanged. If you want to modify the original DataFrame
    and not assign it to another variable, you must use the `inplace` parameter.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到一个设置为 `True` 的 `inplace` 参数。当你使用 psycopg2 修改数据库时，你需要使用 `conn.commit()`
    来使其永久，你也需要对 DataFrame 做同样的事情。当你修改 DataFrame 时，结果会被返回。你可以将那个新的 DataFrame（结果）存储在一个变量中，而原始
    DataFrame 保持不变。如果你想修改原始 DataFrame 而不是将其分配给另一个变量，你必须使用 `inplace` 参数。
- en: 'The `rename` method works for fixing the case of column names but is not the
    best choice. It is better used for actually changing multiple column names. You
    can pass an object with multiple column name remapping. For example, you can remove
    the underscore in `region_id` using `rename`. In the following code snippet, we
    change the `DURATION` column to lowercase and remove the underscore in `region_id`:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '`rename` 方法适用于修复列名的大小写，但不是最佳选择。它更适合用于实际更改多个列名。你可以传递一个具有多个列名重映射的对象。例如，你可以使用
    `rename` 来移除 `region_id` 中的下划线。在以下代码片段中，我们将 `DURATION` 列转换为小写并移除 `region_id` 中的下划线：'
- en: '[PRE37]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'It is good to know different ways to accomplish the same task, and you can
    decide which makes the most sense for your use case. Now that you have applied
    changes to the column names, you can apply these functions to the values in the
    columns as well. Instead of using `df.columns`, you will specify which column
    to modify, and then whether to make it `upper()`, `lower()`, or `capitalize()`.
    In the following code snippet, we have made the `month` column all capitals:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 了解完成同一任务的不同方法是有好处的，你可以决定哪种方法对你的用例最有意义。现在你已经对列名应用了更改，你也可以将这些函数应用到列的值上。你将不再使用
    `df.columns`，而是指定要修改的列，然后决定是否将其设置为 `upper()`、`lower()` 或 `capitalize()`。在以下代码片段中，我们将
    `month` 列全部转换为大写：
- en: '[PRE38]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: It may not matter what the capitalization is on your column names or the values.
    However, it is best to be consistent. In the case of the scooter data, having
    one column name in all capitals, while the rest were all lower, would become confusing.
    Imagine a data scientist querying data from multiple databases or your data warehouse
    and having to remember that all their queries needed to account for the `duration`
    field being in all caps, and when they forgot, their code failed.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 列名或值的大小写可能并不重要。然而，保持一致性是最好的。在电动滑板车数据的情况下，如果有一个列名全部大写，而其余的都是小写，可能会造成混淆。想象一下数据科学家从多个数据库或你的数据仓库查询数据，并需要记住所有他们的查询都需要考虑到
    `duration` 字段是大写的，当他们忘记时，他们的代码就会失败。
- en: You can add data to the DataFrame by creating columns using the `df['new column
    name']=value` format.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过使用 `df['new column name']=value` 格式创建列来向 DataFrame 添加数据。
- en: 'The preceding format would create a new column and assign the value to every
    row. You could iterate through a DataFrame and add a value based on a condition,
    for example:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 上述格式将创建一个新列并将值分配给每一行。你可以遍历 DataFrame 并根据条件添加值，例如：
- en: '[PRE39]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Iterating through DataFrames works but can be very slow. To accomplish the
    same thing as the preceding example, but more efficiently, you can use `loc()`
    and pass the condition, the column name, and then the value. The following example
    shows the code and the results:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 DataFrame 迭代操作可行，但可能会非常慢。为了更高效地完成与前面示例相同的事情，你可以使用 `loc()` 并传递条件、列名和值。以下示例展示了代码和结果：
- en: '[PRE40]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Another way to create columns is by splitting the data and then inserting it
    into the DataFrame. You can use `str.split()` on a series to split text on any
    separator, or a `(expand=True)`. If you do not set `expand` to `True`, you will
    get a list in the column, which is the default. Furthermore, if you do not specify
    a separator, whitespace will be used. The defaults are shown:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 创建列的另一种方法是分割数据，然后将它插入到 DataFrame 中。你可以在一个序列上使用 `str.split()` 来分割文本，使用任何分隔符，或者
    `(expand=True)`。如果你不将 `expand` 设置为 `True`，你将得到一个列表在列中，这是默认的。此外，如果你没有指定分隔符，将使用空白字符。默认值如下所示：
- en: '[PRE41]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'You can expand the data and pass it to a new variable. Then you can assign
    the columns to a column in the original DataFrame. For example, if you wanted
    to create a `date` and a `time` column, you could do the following:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以扩展数据并将其传递给新变量。然后你可以将列分配到原始 DataFrame 的列中。例如，如果你想创建 `date` 和 `time` 列，你可以这样做：
- en: '[PRE42]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'If you recall from the *Exploring the data* section, the data had several `dtypes`
    objects. The `started_at` column is an object and, looking at it, it should be
    clear that it is a `datetime` object. If you try to filter on the `started_at`
    field using a date, it will return all rows, as shown:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还记得在*探索数据*部分，数据有几个`dtypes`对象。`started_at`列是一个对象，从外观上看，很明显它是一个`datetime`对象。如果你尝试使用日期对`started_at`字段进行过滤，它将返回所有行，如下所示：
- en: '[PRE43]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The length of the entire DataFrame is `34226`, so the filter returned all the
    rows. That is not what we wanted. Using `to_datetime()`, you can specify the column
    and the format. You can assign the result to the same column or specify a new
    one. In the following example, the `started_at` column is replaced with the new
    `datetime` data type:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 整个DataFrame的长度是`34226`，所以过滤返回了所有行。这并不是我们想要的。使用`to_datetime()`，你可以指定列和格式。你可以将结果分配给同一个列或指定一个新的列。在以下示例中，`started_at`列被替换为新的`datetime`数据类型：
- en: '[PRE44]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Now, the `started_at` column is a `datetime` data type and not an object. You
    can now run queries using dates, as we attempted earlier on the `full` DataFrame
    and failed:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，`started_at`列是`datetime`数据类型，而不是对象。你现在可以使用日期运行查询，就像我们之前在`full` DataFrame上尝试的那样，但失败了：
- en: '[PRE45]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The rest of the rows were all on 2019-05-21, so we got the results we expected.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 其余的行都是在2019-05-21，所以我们得到了预期的结果。
- en: Now that you can add and remove rows and columns, replace nulls, and create
    columns, in the next section, you will learn how to enrich your data with external
    sources.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经可以添加和删除行和列，替换空值，并创建列，在下一节中，你将学习如何通过外部来源丰富你的数据。
- en: Enriching data
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据丰富
- en: The e-scooter data is geographic data — it contains locations — but it lacks
    coordinates. If you want to map, or perform spatial queries on this data, you
    will need coordinates. You can get coordinates by geocoding the location. As luck
    would have it, the City of Albuquerque has a public geocoder that we can use.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 电动滑板车数据是地理数据——它包含位置——但它缺少坐标。如果你想将数据映射到地图上，或对此数据进行空间查询，你需要坐标。你可以通过地理编码位置来获取坐标。幸运的是，阿尔伯克基市有一个我们可以使用的公共地理编码器。
- en: 'For this example, we will take a subset of the data. We will use the top five
    most frequent starting locations. We will then put them in a DataFrame using the
    following code:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，我们将取数据的一个子集。我们将使用最频繁的前五个起始位置。然后我们将使用以下代码将它们放入一个DataFrame中：
- en: '[PRE46]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The `address` field has more information than we need to geocode. We only need
    the street address. You will also notice that the second record is an intersection
    – `Central @ Tingley`. The geocoder will want the word *and* between the streets.
    Let''s clean the data and put it in its own column:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '`address`字段包含的信息比我们用于地理编码所需的多。我们只需要街道地址。你也会注意到第二个记录是一个交叉路口——`Central @ Tingley`。地理编码器希望街道之间有单词*和*。让我们清理数据并将其放入自己的列中：'
- en: '[PRE47]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Now you can iterate through the DataFrame and geocode the street field. For
    this section, you will use another CSV and join it to the DataFrame.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以遍历DataFrame并对街道字段进行地理编码。在这个部分，你将使用另一个CSV文件并将其与DataFrame连接。
- en: 'You can enrich data by combining it with other data sources. Just like you
    can join data from two tables in a database, you can do the same with a pandas
    DataFrame. You can download the `geocodedstreet.csv` file from the book''s GitHub
    repository. Load the data using `pd.read_csv()` and you will have a DataFrame
    with a `street` column, as well as a column for the `x` and `y` coordinates. The
    result is shown as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过结合其他数据源来丰富数据。就像你可以在数据库中的两个表中连接数据一样，你可以在pandas DataFrame中做同样的事情。你可以从书籍的GitHub仓库中下载`geocodedstreet.csv`文件。使用`pd.read_csv()`加载数据，你将有一个包含`street`列以及`x`和`y`坐标列的DataFrame。结果如下所示：
- en: '[PRE48]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'To enrich the original DataFrame with this new data, you can either join or
    merge the DataFrames. Using a join, you can start with a DataFrame and then add
    the other as a parameter. You can pass how to join using `left`, `right`, or `inner`,
    just like you would in SQL. You can add a `left` and `right` suffix so the columns
    that overlap have a way to determine where they came from. We have joined the
    two DataFrames in the following example:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 要用这些新数据丰富原始DataFrame，你可以连接或合并DataFrame。使用连接，你可以从一个DataFrame开始，然后添加另一个作为参数。你可以通过`left`、`right`或`inner`来传递如何连接，就像在SQL中一样。你可以添加`left`和`right`后缀，这样重叠的列就有一种确定它们来源的方法。以下示例中，我们连接了两个DataFrame：
- en: '[PRE49]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: The `street` column is duplicated and has a `left` and `right` suffix. This
    works but is unnecessary, and we would end up dropping one column and renaming
    the remaining column, which is just extra work.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '`street`列被重复，并带有`left`和`right`后缀。这可以工作，但不是必需的，我们最终会删除一个列并将剩余的列重命名，这只是一些额外的工作。'
- en: 'You can use merge to join the DataFrames on a column and not have the duplicates.
    Merge allows you to pass the DataFrames to merge as well as the field to join
    on, as shown:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用合并来在列上连接DataFrame，而不需要重复。合并允许你传递要合并的DataFrame以及要连接的字段，如下所示：
- en: '[PRE50]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Notice how the new fields `x` and `y` came over in to the new DataFrame, but
    there is only a single `street` column. This is much cleaner. In either case,
    `joined` or `merged`, you can only use the index if you have it set on both DataFrames.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 注意新的字段`x`和`y`如何被引入到新的DataFrame中，但只有一个`street`列。这要干净得多。在`joined`或`merged`的情况下，只有当你在两个DataFrame上都设置了索引时，你才能使用索引。
- en: Now that you know how to clean, transform, and enrich data, it is time to put
    these skills together and build a data pipeline using this newfound knowledge.
    The next two sections will show you how to use Airflow and NiFi to build a data
    pipeline.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经知道了如何清理、转换和丰富数据，是时候将这些技能结合起来，利用新获得的知识构建数据管道了。接下来的两个部分将展示如何使用Airflow和NiFi构建数据管道。
- en: Cleaning data using Airflow
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Airflow清理数据
- en: Now that you can clean your data in Python, you can create functions to perform
    different tasks. By combining the functions, you can create a data pipeline in
    Airflow. The following example will clean data, and then filter it and write it
    out to disk.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以在Python中清理你的数据，你可以创建执行不同任务的函数。通过组合这些函数，你可以在Airflow中创建数据管道。以下示例将清理数据，然后过滤它并将其写入磁盘。
- en: 'Starting with the same Airflow code you have used in the previous examples,
    set up the imports and the default arguments, as shown:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 从与之前示例中相同的Airflow代码开始，设置导入和默认参数，如下所示：
- en: '[PRE51]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Now you can write the functions that will perform the cleaning tasks. First,
    you need to read the file, then you can drop the region ID, convert the columns
    to lowercase, and change the `started_at` field to a `datetime` data type. Lastly,
    write the changes to a file. The following is the code:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以编写执行清理任务的函数了。首先，你需要读取文件，然后你可以删除区域ID，将列转换为小写，并将`started_at`字段更改为`datetime`数据类型。最后，将更改写入文件。以下是对应的代码：
- en: '[PRE52]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Next, the pipeline will read in the cleaned data and filter based on a start
    and end date. The code is as follows:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，管道将读取清理后的数据，并根据开始和结束日期进行过滤。代码如下：
- en: '[PRE53]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'These two functions should look familiar as the code is line for line the same
    as in the preceding examples, just regrouped. Next, you need to define the operators
    and tasks. You will use `PythonOperator` and point it to your functions. Create
    the DAG and the tasks as shown:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个函数看起来应该很熟悉，因为代码与前面的示例完全相同，只是重新分组了。接下来，你需要定义操作符和任务。你将使用`PythonOperator`并将其指向你的函数。创建DAG和任务，如下所示：
- en: '[PRE54]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'In this example, we will add in another task using `BashOperator` again. If
    you recall, you used it in [*Chapter 3*](B15739_03_ePub_AM.xhtml#_idTextAnchor039)*,
    Reading and Writing Files*, just to print a message to the terminal. This time,
    you will use it to move the file from the `selectData` task and copy it to the
    desktop. The code is as follows:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将再次使用`BashOperator`添加另一个任务。如果你还记得，你在[*第3章*](B15739_03_ePub_AM.xhtml#_idTextAnchor039)*，读取和写入文件*中使用过它，只是为了在终端打印一条消息。这次，你将使用它将文件从`selectData`任务移动到桌面。以下是代码：
- en: '[PRE55]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The preceding command just uses the Linux copy command to make a copy of the
    file. When working with files, you need to be careful that your tasks can access
    them. If multiple processes attempt to touch the same file or a user tries to
    access it, you could break your pipeline. Lastly, specify the order of the tasks
    — create the direction of the DAG as shown:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的命令只是使用了Linux复制命令来复制文件。当处理文件时，你需要小心确保你的任务可以访问它们。如果有多个进程尝试访问同一文件或用户尝试访问它，你可能会破坏你的管道。最后，指定任务的顺序——创建DAG的方向，如下所示：
- en: '[PRE56]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Now you have a completed DAG. Copy this file to your `$AIRFLOW_HOME/dags` folder.
    Then, start Airflow with the following command:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经完成了一个DAG。将此文件复制到你的`$AIRFLOW_HOME/dags`文件夹。然后，使用以下命令启动Airflow：
- en: '[PRE57]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Now you can browse to `http://localhost:8080/admin` to view the GUI. Select
    your new DAG and click the **Tree View** tab. You will see your DAG and you can
    turn it on and run it. In the following screenshot, you will see the DAG and the
    runs of each task:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以通过 `http://localhost:8080/admin` 浏览到 GUI。选择您的新 DAG，然后点击 **树形视图** 选项卡。您将看到您的
    DAG，并可以将其开启并运行。在下面的屏幕截图中，您将看到 DAG 以及每个任务的运行情况：
- en: '![Figure 5.1 – Running the DAG'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.1 – 运行 DAG'
- en: '](img/Figure_5.1_B15739.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_5.1_B15739.jpg)'
- en: Figure 5.1 – Running the DAG
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1 – 运行 DAG
- en: You will see that the DAG has two failed runs. This was a result of the file
    not being present when a task ran. I had used `move` instead of `copy` in `BashOperator`,
    hence the warning about being careful when handling files in Airflow.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 您会看到 DAG 有两次失败的运行。这是由于任务运行时文件不存在导致的。我在 `BashOperator` 中使用了 `move` 而不是 `copy`，因此出现了关于在
    Airflow 中处理文件时要小心谨慎的警告。
- en: Congratulations! You have successfully completed this chapter.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！您已成功完成本章内容。
- en: Summary
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned how to perform basic EDA with an eye toward finding
    errors or problems within your data. You then learned how to clean your data and
    fix common data issues. With this set of skills, you built a data pipeline in
    Apache Airflow.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您学习了如何进行基本的 EDA，目的是在您的数据中寻找错误或问题。然后您学习了如何清理您的数据并修复常见的数据问题。通过这些技能，您在 Apache
    Airflow 中构建了一个数据管道。
- en: In the next chapter, you will walk through a project, building a 311 data pipeline
    and dashboard in Kibana. This project will utilize all of the skills you have
    acquired up to this point and will introduce a number of new skills – such as
    building dashboards and making API calls.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，您将进行一个项目，构建一个在 Kibana 中的 311 数据管道和仪表板。这个项目将利用您到目前为止所掌握的所有技能，并介绍一些新技能——例如构建仪表板和进行
    API 调用。
