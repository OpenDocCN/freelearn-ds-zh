- en: '*Chapter 5*: Cleaning, Transforming, and Enriching Data'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous two chapters, you learned how to build data pipelines that
    could read and write from files and databases. In many instances, these skills
    alone will enable you to build production data pipelines. For example, you will
    read files from a data lake and insert them into a database. You now have the
    skills to accomplish this. Sometimes, however, you will need to do something with
    the data after extraction but prior to loading. What you will need to do is clean
    the data. Cleaning is a vague term. More specifically, you will need to check
    the validity of the data and answer questions such as the following: Is it complete?
    Are the values within the proper ranges? Are the columns the proper type? Are
    all the columns useful?'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will learn the basic skills needed to perform exploratory
    data analysis. Once you have an understanding of the data, you will use that knowledge
    to fix common data problems that you have discovered – such as dropping columns
    and replacing nulls. You will learn many useful methods available in the `pandas`
    library for Python. These skills will allow you to quickly perform exploratory
    data analysis and allow you to clean the data, all within Python. These skills
    will become the tools for the transform stage of the ETL data engineering process.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Performing exploratory data analysis in Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling common data issues using pandas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cleaning data using Airflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing exploratory data analysis in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before you can clean your data, you need to know what your data looks like.
    As a data engineer, you are not the domain expert and are not the end user of
    the data, but you should know what the data will be used for and what valid data
    would look like. For example, you do not need to be a demographer to know that
    an `age` field should not be negative, and the frequency of values over 100 should
    be low.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, you will use real e-scooter data from the City of Albuquerque.
    The data contains trips taken using e-scooters from May to July 22, 2019\. You
    will need to download the e-scooter data from [https://github.com/PaulCrickard/escooter/blob/master/scooter.csv](https://github.com/PaulCrickard/escooter/blob/master/scooter.csv).
    The repository also contains the original Excel file as well as some other summary
    files provided by the City of Albuquerque.
  prefs: []
  type: TYPE_NORMAL
- en: Basic data exploration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before you can clean your data, you have to know what your data looks like.
    The process of understanding your data is called **exploratory data analysis**
    (**EDA**). You will look at the shape of your data, the number of rows and columns,
    as well as the data types in the columns, and the ranges of values. You can perform
    a much more in-depth analysis, such as the distribution of the data, or the skewness,
    but for this section, you will learn how to quickly understand your data so that
    in the next section, you can clean it.
  prefs: []
  type: TYPE_NORMAL
- en: In the two previous chapters, you learned how to import files and databases
    into pandas DataFrames. That knowledge will be expanded in this section, as DataFrames
    will be the tool used for the EDA.
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin with, you will need to import `pandas` and read the `.csv` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: With the data in a DataFrame, you can now explore it, and then analyze it.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now you can start to look at the data. The first thing you will probably want
    to do is print it out. But before you get to that, take a look at the columns
    and the data types using `columns` and `dtypes`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: You will see that you have eleven columns, five of which are integers (all of
    the columns with ID in their name) and the rest are objects. Objects are what
    a DataFrame uses as `dtype` when there are mixed types. Also, `DURATION` should
    stand out because it is the only column name in all capitals. In the next section,
    you will fix common errors, such as the column cases are not uniform (all lowercase
    or uppercase) and make the `dtypes` object proper types, such as `strings` for
    text data and `datetimes` for dates and times.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you know what you have for columns and types, let''s look at the data.
    You can print out the first five records using `head()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The opposite of `head()` is `tail()`. Both of these methods default to showing
    5 rows. However, you can pass an integer as a parameter that specifies how many
    rows to show. For example, you could pass `head(10)` to see the first 10 rows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice in both the `head()` and `tail()` output that the third column is `...`,
    and then there are two more columns after this. The display is cropping out the
    columns in the middle. If you were to print the entire DataFrame, the same thing
    would happen with the rows as well. To display all the columns, you can change
    the number of columns to show using the `set_options` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Now, when you use `head()`, you will see all the column. However, depending
    on the width of your display, the output may be wrapped to fit.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `head` and `tail` methods display all the columns, but if you are only
    interested in a single column, you can specify it like you would in a Python dictionary.
    The following code prints the `DURATION` column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Again, notice that the output is cropped with `...`, but this time for the rows.
    The result is the combination of `head()` and `tail()`. You could change this
    using the `display_max_rows` option as you did earlier with columns, but for this
    exploration, it is unnecessary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just like you can display a single column, you can display a list of columns
    using double `[]`, as shown in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also pull a sample from your data using `sample()`. The sample methods
    allow you to specify how many rows you would like to pull. The results are shown
    in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the index of the rows is not incremental, but rather it jumps around.
    It should, as it is a sample.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also slice the data. Slicing takes the format of `[start:end]`, where
    a blank is the first or last row depending on which position is blank. To slice
    the first 10 rows, you can use the following notation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Likewise, to grab the rows from 10 to the end (34,225), you can use the following
    notation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also slice the frame starting on the third row and ending before nine,
    as shown in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Sometimes, you know the exact row you want, and instead of slicing it, you
    can select it using `loc()`. The `loc` method takes the index name, which, in
    this example, is an integer. The following code and output show a single row selected
    with `loc()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Using `at()`, with the position, as you did in the slicing examples, and a
    column name, you can select a single value. For example, this can be done to know
    the duration of the trip in the second row:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Slicing and using `loc()` and `at()` pull data based on position, but you can
    also use DataFrames to select rows based on some condition. Using the `where`
    method, you can pass a condition, as shown in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code and results show the results of `where` with the condition
    of the user ID being equal to `8417864`. The results replace values that do not
    meet the criteria as `NaN`. This will be covered in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can get the same results similar to the preceding example with the exception
    of using a different notation, and this method will not include the `NaN` rows.
    You can pass the condition into the DataFrame as you did with column names. The
    following example shows you how:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The results of the preceding code is the same as the `where()` example, but
    without the `NaN` rows, so the DataFrame will only have four rows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using both notations, you can combine conditional statements. By using the
    same user ID condition, you can add a trip ID condition. The following example
    shows you how:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the second notation, the output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding examples, the conditions were assigned to a variable and combined
    in both the `where` and secondary notation, generating the expected results.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that you have seen the data, you can start to analyze it. By using the
    `describe` method, you can see a series of statistics pertaining to your data.
    In statistics, there is a set of statistics referred to as the five-number summary,
    and `describe()` is a variant of that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The `describe` method is not very useful unless you have numeric data. If you
    were looking at ages for example, it would quickly show you the distribution of
    ages, and you would be able to quickly see errors such as negative ages or too
    many ages over 100\.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using `describe()` on a single column is sometimes more helpful. Let''s try
    looking at the `start_location_name` column. The code and results are shown in
    the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The data is not numeric, so we get a different set of statistics, but these
    provide some insight. Of the `34220` starting locations, there are actually `2972`
    unique locations. The top location (`1898 Mountain Rd NW`) accounts for `1210`
    trip starting locations. Later, you will geocode this data — add coordinates to
    the address — and knowing the unique values means you only have to geocode those
    2,972 and not the full 34,220.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another method that allows you to see details about your data is `value_counts`.
    The `value_counts` method will give you the value and count for all unique values.
    We need to call it to a single column, which is done in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'From this method, you can see that `0:04:00` is at the top with a frequency
    of `825` — which you could have found out with `describe()` — but you can also
    see the frequency of all the other values. To see the frequency as a percentage,
    you can pass the normalize parameter (which is `False` by default):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: You will notice that no single value makes up a significant percentage of the
    duration.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also pass the `dropna` parameter. By default, `value_counts()` sets
    it to `True` and you will not see them. Setting it to `False`, you can see that
    `end_location_name` is missing `2070` entries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The best way to find out how many missing values you have in your columns is
    to use the `isnull()` method. The following code combines `isnull()` with `sum()`
    to get the counts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Another parameter of `value_counts()` is `bins`. The scooter dataset does not
    have a good column for this, but using a numeric column, you would get results
    like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: These results are fairly meaningless, but if it is used on a column such as
    `age`, it would come in handy as you could create age groups quickly and get an
    idea of the distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have explored and analyzed the data, you should have an understanding
    of what the data is and what the issues are — for example, nulls, improper `dtypes`,
    combined, and fields. With that knowledge, you can start to clean the data. The
    next section will walk you through how to fix common data problems.
  prefs: []
  type: TYPE_NORMAL
- en: Handling common data issues using pandas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Your data may feel special, it is unique, you have created the world's best
    systems for collecting it, and you have done everything you can to ensure it is
    clean and accurate. Congratulations! But your data will almost certainly have
    some problems, and these problems are not special, or unique, and are probably
    a result of your systems or data entry. The e-scooter dataset is collected using
    GPS with little to no human input, yet there are end locations that are missing.
    How is it possible that a scooter was rented, ridden, and stopped, yet the data
    doesn't know where it stopped? Seems strange, yet here we are. In this section,
    you will learn how to deal with common data problems using the e-scooter dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Drop rows and columns
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before you modify any fields in your data, you should first decide whether you
    are going to use all the fields. Looking at the e-scooter data, there is a field
    named `region_id`. This field is a code used by the vendor to label Albuquerque.
    Since we are only using the Albuquerque data, we don't need this field as it adds
    nothing to the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can drop columns using the `drop` method. The method will allow you to
    specify whether to drop a row or a column. Rows are the default, so we will specify
    `columns`, as shown in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Specifying the columns to drop, you also need to add `inplace` to make it modify
    the original DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'To drop a row, you only need to specify `index` instead of `columns`. To drop
    the row with the index of `34225`, you need to use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code works when you want to drop an entire column or row, but
    what if you wanted to drop them based on conditions?
  prefs: []
  type: TYPE_NORMAL
- en: The first condition you may want to consider is where there are nulls. If you
    are missing data, the column and row may not be useful, or may distort the data.
    To handle this, you can use `dropna()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'By using `dropna()`, you can pass `axis`, `how`, `thresh`, `subset`, and `inplace`
    as parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`axis` specifies rows or columns with indexes or columns (0 or 1). It defaults
    to rows.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`how` specifies whether to drop rows or columns if all the values are null
    or if any value is null (all or any). It defaults to `any`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`thresh` allows more control than allowing you to specify an integer value
    of how many nulls must be present.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`subset` allows you to specify a list of rows or columns to search.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inplace` allows you to modify the existing DataFrame. It defaults to `False`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Looking at the e-scooter data, there are six rows with no start location name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'To drop these rows, you can use `dropna` on `axis=0` with `how=any`, which
    are the defaults. This will, however, delete rows where other nulls exist, such
    as `end_location_name`. So, you will need to specify the column name as a subset,
    as shown in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, when you select nulls in the `start_location_name` field as in the preceding
    code block, you will get an empty series:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Dropping an entire column based on missing values may only make sense if a
    certain percentage of rows are null. For example, if more than 25% of the rows
    are null, you may want to drop it. You could specify this in the threshold by
    using something like the following code for the `thresh` parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Before showing more advanced filters for dropping rows, you may not want to
    drop nulls. You may want to fill them with a value. You can use `fillna()` to
    fill either null columns or rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: What if you want to use `fillna()` but use different values depending on the
    column? You would not want to have to specify a column every time and run `fillna()`
    multiple times. You can specify an object to map to the DataFrame and pass it
    as the `value` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we will copy the rows where both the start and end location
    are null. Then, we will create a `value` object that assigns a street name to
    the `start_location_name` field and a different street address to the `end_location_name`
    field. Using `fillna()`, we pass the value to the `value` parameter, and then
    print those two columns in the DataFrame by showing the change:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'You can drop rows based on more advanced filters; for example, what if you
    want to drop all the rows where the month was May? You could iterate through the
    DataFrame and check the month, and then drop it if it is May. Or, a much better
    way would be to filter out the rows, and then pass the index to the `drop` method.
    You can filter the DataFrame and pass it to a new one, as shown in the following
    code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Then you can use `drop()` on the original DataFrame and pass the index for
    the rows in the `may` DataFrame, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, if you look at the months in the original DataFrame, you will see that
    May is missing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Now that you have removed the rows and columns that you either do not need,
    or that were unusable on account of missing data, it is time to format them.
  prefs: []
  type: TYPE_NORMAL
- en: Creating and modifying columns
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first thing that stood out in the preceding section was that there was
    a single column, duration, that was all in capital letters. Capitalization is
    a common problem. You will often find columns with all capitals, or with title
    case — where the first letter of every word is capitalized — and if a coder wrote
    it, you may find camel case — where the first letter is lowercase and the first
    letter of the next word is capital with no spaces, as in **camelCase**. The following
    code will make all the columns lowercase:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code is a condensed version of a `for` loop. What happens in
    the loop comes before the `for` loop. The preceding code says that for every item
    in `df.columns`, make it lowercase, and assign it back to `df.columns`. You can
    also use `capitalize()`, which is titlecase, or `upper()` as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'You could also make the `DURATION` field lowercase using the `rename` method,
    as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: You will notice an `inplace` parameter set to `True`. When you used psycopg2
    to modify databases, you need to use `conn.commit()` to make it permanent, and
    you need to do the same with DataFrames. When you modify a DataFrame, the result
    is returned. You can store that new DataFrame (result) in a variable, and the
    original DataFrame is left unchanged. If you want to modify the original DataFrame
    and not assign it to another variable, you must use the `inplace` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `rename` method works for fixing the case of column names but is not the
    best choice. It is better used for actually changing multiple column names. You
    can pass an object with multiple column name remapping. For example, you can remove
    the underscore in `region_id` using `rename`. In the following code snippet, we
    change the `DURATION` column to lowercase and remove the underscore in `region_id`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'It is good to know different ways to accomplish the same task, and you can
    decide which makes the most sense for your use case. Now that you have applied
    changes to the column names, you can apply these functions to the values in the
    columns as well. Instead of using `df.columns`, you will specify which column
    to modify, and then whether to make it `upper()`, `lower()`, or `capitalize()`.
    In the following code snippet, we have made the `month` column all capitals:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: It may not matter what the capitalization is on your column names or the values.
    However, it is best to be consistent. In the case of the scooter data, having
    one column name in all capitals, while the rest were all lower, would become confusing.
    Imagine a data scientist querying data from multiple databases or your data warehouse
    and having to remember that all their queries needed to account for the `duration`
    field being in all caps, and when they forgot, their code failed.
  prefs: []
  type: TYPE_NORMAL
- en: You can add data to the DataFrame by creating columns using the `df['new column
    name']=value` format.
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding format would create a new column and assign the value to every
    row. You could iterate through a DataFrame and add a value based on a condition,
    for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Iterating through DataFrames works but can be very slow. To accomplish the
    same thing as the preceding example, but more efficiently, you can use `loc()`
    and pass the condition, the column name, and then the value. The following example
    shows the code and the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Another way to create columns is by splitting the data and then inserting it
    into the DataFrame. You can use `str.split()` on a series to split text on any
    separator, or a `(expand=True)`. If you do not set `expand` to `True`, you will
    get a list in the column, which is the default. Furthermore, if you do not specify
    a separator, whitespace will be used. The defaults are shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'You can expand the data and pass it to a new variable. Then you can assign
    the columns to a column in the original DataFrame. For example, if you wanted
    to create a `date` and a `time` column, you could do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'If you recall from the *Exploring the data* section, the data had several `dtypes`
    objects. The `started_at` column is an object and, looking at it, it should be
    clear that it is a `datetime` object. If you try to filter on the `started_at`
    field using a date, it will return all rows, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The length of the entire DataFrame is `34226`, so the filter returned all the
    rows. That is not what we wanted. Using `to_datetime()`, you can specify the column
    and the format. You can assign the result to the same column or specify a new
    one. In the following example, the `started_at` column is replaced with the new
    `datetime` data type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the `started_at` column is a `datetime` data type and not an object. You
    can now run queries using dates, as we attempted earlier on the `full` DataFrame
    and failed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: The rest of the rows were all on 2019-05-21, so we got the results we expected.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you can add and remove rows and columns, replace nulls, and create
    columns, in the next section, you will learn how to enrich your data with external
    sources.
  prefs: []
  type: TYPE_NORMAL
- en: Enriching data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The e-scooter data is geographic data — it contains locations — but it lacks
    coordinates. If you want to map, or perform spatial queries on this data, you
    will need coordinates. You can get coordinates by geocoding the location. As luck
    would have it, the City of Albuquerque has a public geocoder that we can use.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this example, we will take a subset of the data. We will use the top five
    most frequent starting locations. We will then put them in a DataFrame using the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The `address` field has more information than we need to geocode. We only need
    the street address. You will also notice that the second record is an intersection
    – `Central @ Tingley`. The geocoder will want the word *and* between the streets.
    Let''s clean the data and put it in its own column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Now you can iterate through the DataFrame and geocode the street field. For
    this section, you will use another CSV and join it to the DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can enrich data by combining it with other data sources. Just like you
    can join data from two tables in a database, you can do the same with a pandas
    DataFrame. You can download the `geocodedstreet.csv` file from the book''s GitHub
    repository. Load the data using `pd.read_csv()` and you will have a DataFrame
    with a `street` column, as well as a column for the `x` and `y` coordinates. The
    result is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'To enrich the original DataFrame with this new data, you can either join or
    merge the DataFrames. Using a join, you can start with a DataFrame and then add
    the other as a parameter. You can pass how to join using `left`, `right`, or `inner`,
    just like you would in SQL. You can add a `left` and `right` suffix so the columns
    that overlap have a way to determine where they came from. We have joined the
    two DataFrames in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: The `street` column is duplicated and has a `left` and `right` suffix. This
    works but is unnecessary, and we would end up dropping one column and renaming
    the remaining column, which is just extra work.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use merge to join the DataFrames on a column and not have the duplicates.
    Merge allows you to pass the DataFrames to merge as well as the field to join
    on, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Notice how the new fields `x` and `y` came over in to the new DataFrame, but
    there is only a single `street` column. This is much cleaner. In either case,
    `joined` or `merged`, you can only use the index if you have it set on both DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know how to clean, transform, and enrich data, it is time to put
    these skills together and build a data pipeline using this newfound knowledge.
    The next two sections will show you how to use Airflow and NiFi to build a data
    pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning data using Airflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you can clean your data in Python, you can create functions to perform
    different tasks. By combining the functions, you can create a data pipeline in
    Airflow. The following example will clean data, and then filter it and write it
    out to disk.
  prefs: []
  type: TYPE_NORMAL
- en: 'Starting with the same Airflow code you have used in the previous examples,
    set up the imports and the default arguments, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you can write the functions that will perform the cleaning tasks. First,
    you need to read the file, then you can drop the region ID, convert the columns
    to lowercase, and change the `started_at` field to a `datetime` data type. Lastly,
    write the changes to a file. The following is the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, the pipeline will read in the cleaned data and filter based on a start
    and end date. The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'These two functions should look familiar as the code is line for line the same
    as in the preceding examples, just regrouped. Next, you need to define the operators
    and tasks. You will use `PythonOperator` and point it to your functions. Create
    the DAG and the tasks as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, we will add in another task using `BashOperator` again. If
    you recall, you used it in [*Chapter 3*](B15739_03_ePub_AM.xhtml#_idTextAnchor039)*,
    Reading and Writing Files*, just to print a message to the terminal. This time,
    you will use it to move the file from the `selectData` task and copy it to the
    desktop. The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command just uses the Linux copy command to make a copy of the
    file. When working with files, you need to be careful that your tasks can access
    them. If multiple processes attempt to touch the same file or a user tries to
    access it, you could break your pipeline. Lastly, specify the order of the tasks
    — create the direction of the DAG as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you have a completed DAG. Copy this file to your `$AIRFLOW_HOME/dags` folder.
    Then, start Airflow with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you can browse to `http://localhost:8080/admin` to view the GUI. Select
    your new DAG and click the **Tree View** tab. You will see your DAG and you can
    turn it on and run it. In the following screenshot, you will see the DAG and the
    runs of each task:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – Running the DAG'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_5.1_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.1 – Running the DAG
  prefs: []
  type: TYPE_NORMAL
- en: You will see that the DAG has two failed runs. This was a result of the file
    not being present when a task ran. I had used `move` instead of `copy` in `BashOperator`,
    hence the warning about being careful when handling files in Airflow.
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have successfully completed this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to perform basic EDA with an eye toward finding
    errors or problems within your data. You then learned how to clean your data and
    fix common data issues. With this set of skills, you built a data pipeline in
    Apache Airflow.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will walk through a project, building a 311 data pipeline
    and dashboard in Kibana. This project will utilize all of the skills you have
    acquired up to this point and will introduce a number of new skills – such as
    building dashboards and making API calls.
  prefs: []
  type: TYPE_NORMAL
