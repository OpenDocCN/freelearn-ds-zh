<html><head></head><body><div><div><h1 id="_idParaDest-281"><em class="italics"><a id="_idTextAnchor336"/>Appendix</em></h1>
		</div>
		<div><div></div>
		</div>
		<div><h2>About</h2>
			<p>This section is included to assist the students to perform the activities in the book. It includes detailed steps that are to be performed by the students to achieve the objectives of the activities.</p>
		</div>
		<div><h3 id="_idParaDest-282"><a id="_idTextAnchor337"/>Solution of Activity 1: Handling Lists</h3>
			<p>These are the steps to complete this activity:</p>
			<ol>
				<li>Import the <code>random</code> library:<pre>import random</pre></li>
				<li>Set the maximum number of random numbers:<pre>LIMIT = 100</pre></li>
				<li>Use the <code>randint</code> function from the <code>random</code> library to create 100 random numbers. Tip: try getting a list with the least number of duplicates:<pre>random_number_list = [random.randint(0, LIMIT) for x in range(0, LIMIT)]</pre></li>
				<li>Print <code>random_number_list</code>:<pre>random_number_list</pre><p>The sample output is as follows:</p><div><img src="img/C11065_01_16.jpg" alt="Figure 1.16: Section of output for random_number_list" width="828" height="597"/></div><h6>Figure 1.16: Section of output for random_number_list</h6></li>
				<li>Create a <code>list_with_divisible_by_3</code> list from <code>random_number_list</code>, which will contain only numbers that are divisible by <code>3</code>:<pre>list_with_divisible_by_3 = [a for a in random_number_list if a % 3 == 0]
list_with_divisible_by_3</pre><p>The sample output is as follows:</p><div><img src="img/C11065_01_17.jpg" alt="Figure 1.17: Section of output for random_number_list divisible by 3" width="644" height="582"/></div><h6>Figure 1.17: Section of output for random_number_list divisible by 3</h6></li>
				<li>Use the <code>len</code> function to measure the length of the first list and the second list, and store them in two different variables, <code>length_of_random_list</code> and <code>length_of_3_divisible_list</code>. Calculate the difference in length in a variable called <code>difference</code>:<pre>length_of_random_list = len(random_number_list)
length_of_3_divisible_list = len(list_with_divisible_by_3)
difference = length_of_random_list - length_of_3_divisible_list
difference</pre><p>The sample output is as follows:</p><pre>62</pre></li>
				<li>Combine the tasks we have performed so far and add a while loop to it. Run the loop 10 times and add the values of the difference variables to a list:<pre>NUMBER_OF_EXPERIMENTS = 10
difference_list = []
for i in range(0, NUMBER_OF_EXPERIEMENTS):
    random_number_list = [random.randint(0, LIMIT) for x in range(0, LIMIT)]
    list_with_divisible_by_3 = [a for a in random_number_list if a % 3 == 0]
    
    length_of_random_list = len(random_number_list)
    length_of_3_divisible_list = len(list_with_divisible_by_3)
    difference = length_of_random_list - length_of_3_divisible_list
    
    difference_list.append(difference)
difference_list</pre><p>The sample output is as follows:</p><pre>[64, 61, 67, 60, 73, 66, 66, 75, 70, 61]</pre></li>
				<li>Then, calculate the arithmetic mean (common average) for the differences in the lengths that you have: <pre>avg_diff = sum(difference_list) / float(len(difference_list))
avg_diff</pre><p>The sample output is as follows:</p><pre>66.3</pre></li>
			</ol>
			<h3 id="_idParaDest-283"><a id="_idTextAnchor338"/>Solution of Activity 2: Analyze a Multiline String and Generate the Unique Word Count</h3>
			<p>These are the steps to complete this activity:</p>
			<ol>
				<li value="1">Create a string called <code>multiline_text</code> and copy the text present in the first chapter of <em class="italics">Pride and Prejudice</em>. Use <em class="italics">Ctrl</em> <em class="italics">+</em> <em class="italics">A</em> to select the entire text and then <em class="italics">Ctrl</em> <em class="italics">+</em> <em class="italics">C</em> to copy it and paste the text you just copied into it:<div><img src="img/C11065_01_18.jpg" alt="Figure 1.18: Initializing the mutliline_text string" width="1003" height="473"/></div><h6>Figure 1.18: Initializing the mutliline_text string</h6></li>
				<li>Find the type of the string using the <code>type</code> function:<pre>type(multiline_text)</pre><p>The output is as follows:</p><pre>str</pre></li>
				<li>Now, find the length of the string, using the <code>len</code> function:<pre>len(multiline_text)</pre><p>The output is as follows:</p><pre>4475</pre></li>
				<li>Use string methods to get rid of all the new lines (<code>\n</code> or <code>\r</code>),and symbols. Remove all new lines by replacing them with this: <pre>multiline_text = multiline_text.replace('\n', "")</pre><p>Then, we will print and check the output:</p><pre>multiline_text</pre><p>The output is as follows:</p><div><img src="img/C11065_01_19.jpg" alt="Figure 1.19: The multiline_text string after removing the new lines" width="981" height="397"/></div><h6>Figure 1.19: The multiline_text string after removing the new lines</h6></li>
				<li>Removing the special characters and punctuation:<pre># remove special chars, punctuation etc.
cleaned_multiline_text = ""
for char in multiline_text:
    if char == " ":
        cleaned_multiline_text += char
    elif char.isalnum():  # using the isalnum() method of strings.
        cleaned_multiline_text += char
    else:
        cleaned_multiline_text += " "</pre></li>
				<li>Check the content of <code>cleaned_multiline_text</code>:<pre>cleaned_multiline_text</pre><p>The output is as follows:</p><div><img src="img/C11065_01_20.jpg" alt="Figure 1.20: The cleaned_multiline_text string" width="990" height="392"/></div><h6>Figure 1.20: The cleaned_multiline_text string</h6></li>
				<li>Generate a list of all the words from the cleaned string using the following command:<pre>list_of_words = cleaned_multiline_text.split()
list_of_words</pre><p>The output is as follows:</p><div><img src="img/C11065_01_21.jpg" alt="Figure 1.21: The section of output displaying the list_of_words" width="682" height="326"/></div><h6>Figure 1.21: The section of output displaying the list_of_words</h6></li>
				<li>Find the number of words:<pre>len(list_of_words)</pre><p>The output is <code>852</code>.</p></li>
				<li>Create a list from the list you just created, which includes only unique words:  <pre>unique_words_as_dict = dict.fromkeys(list_of_words)
len(list(unique_words_as_dict.keys()))</pre><p>The output is <code>340</code>.</p></li>
				<li>Count the number of times each of the unique words appeared in the cleaned text: <pre>for word in list_of_words:
    if unique_words_as_dict[word] is None:
        unique_words_as_dict[word] = 1
    else:
        unique_words_as_dict[word] += 1
unique_words_as_dict</pre><p>The output is as follows:</p><div><img src="img/C11065_01_22.jpg" alt="Figure 1.22: Section of output showing unique_words_as_dict" width="667" height="326"/></div><h6>Figure 1.22: Section of output showing unique_words_as_dict</h6><p>You just created, step by step, a unique word counter using all the neat tricks that you just learned.</p></li>
				<li>Find the top 25 words from the <code>unique_words_as_dict</code>.<pre>top_words = sorted(unique_words_as_dict.items(), key=lambda key_val_tuple: key_val_tuple[1], reverse=True)
top_words[:25]</pre><p>These are the steps to complete this activity:</p></li>
			</ol>
			<div><div><img src="img/C11065_01_23.jpg" alt="Figure 1.23: Top 25 unique words from multiline_text" width="629" height="432"/>
				</div>
			</div>
			<h6>Figure 1.23: Top 25 unique words from multiline_text</h6>
			<h3 id="_idParaDest-284"><a id="_idTextAnchor339"/>Solution of Activity 3: Permutation, Iterator, Lambda, List</h3>
			<p>These are the steps to solve this activity:</p>
			<ol>
				<li value="1">Look up the definition of <code>permutations</code> and <code>dropwhile</code> from <code>itertools</code>. There is a way to look up the definition of a function inside Jupyter itself. Just type the function name, followed by <em class="italics">?</em>, and press <em class="italics">Shift +</em> <em class="italics">Enter</em>:<pre>from itertools import permutations, dropwhile
permutations?
dropwhile?</pre><p>You will see a long list of definitions after each <code>?</code>. We will skip it here.</p></li>
				<li>Write an expression to generate all the possible three-digit numbers using 1, 2, and 3:<pre>permutations(range(3))</pre><p>The output is as follows:</p><pre>&lt;itertools.permutations at 0x7f6c6c077af0&gt;</pre></li>
				<li>Loop over the iterator expression you generated before. Use print to print each element returned by the iterator. Use <code>assert</code> and <code>isinstance</code> to make sure that the elements are tuples:<pre>for number_tuple in permutations(range(3)):
    print(number_tuple)
    assert isinstance(number_tuple, tuple)</pre><p>The output is as follows:</p><pre>(0, 1, 2)
(0, 2, 1)
(1, 0, 2)
(1, 2, 0)
(2, 0, 1)
(2, 1, 0)</pre></li>
				<li>Write the loop again. But this time, use <code>dropwhile</code> with a lambda expression to drop any leading zeros from the tuples. As an example, <code>(0, 1, 2)</code> will become <code>[0, 2]</code>. Also, cast the output of the <code>dropwhile</code> to a list.<p>An extra task can be to check the actual type that <code>dropwhile</code> returns without casting:</p><pre>for number_tuple in permutations(range(3)):
    print(list(dropwhile(lambda x: x &lt;= 0, number_tuple)))</pre><p>The output is as follows:</p><pre>[1, 2]
[2, 1]
[1, 0, 2]
[1, 2, 0]
[2, 0, 1]
[2, 1, 0]</pre></li>
				<li>Write all the logic you wrote before, but this time write a separate function where you will be passing the list generated from <code>dropwhile</code>, and the function will return the whole number contained in the list. As an example, if you pass <code>[1, 2]</code> to the function, it will return <code>12</code>. Make sure that the return type is indeed a number and not a string. Although this task can be achieved using other tricks, we require that you treat the incoming list as a stack in the function and generate the number there:<pre>import math
def convert_to_number(number_stack):
    final_number = 0
    for i in range(0, len(number_stack)):
        final_number += (number_stack.pop() * (math.pow(10, i)))
    return final_number
for number_tuple in permutations(range(3)):
    number_stack = list(dropwhile(lambda x: x &lt;= 0, number_tuple))
    print(convert_to_number(number_stack))</pre><p>The output is as follows:</p><pre>12.0
21.0
102.0
120.0
201.0
210.0</pre></li>
			</ol>
			<h3 id="_idParaDest-285"><a id="_idTextAnchor340"/>Solution of Activity 4: Design Your Own CSV Parser</h3>
			<p>These are the steps to complete this activity:</p>
			<ol>
				<li value="1">Import <code>zip_longest</code> from <code>itertools</code>:<pre>from itertools import zip_longest</pre></li>
				<li>Define the <code>return_dict_from_csv_line</code> function so that it contains <code>header</code>, <code>line</code>, and <code>fillvalue</code> as <code>None</code>, and add it to a <code>dict</code>:<pre>def return_dict_from_csv_line(header, line):
    # Zip them
    zipped_line = zip_longest(header, line, fillvalue=None)
    # Use dict comprehension to generate the final dict
    ret_dict = {kv[0]: kv[1] for kv in zipped_line}
    return ret_dict</pre></li>
				<li>Open the accompanying <code>sales_record.csv</code> file using <code>r</code> mode inside a with block. First, check that it is opened, read the first line, and use string methods to generate a list of all the column names with <code>open("sales_record.csv", "r") as fd</code>. When you read each line, pass that line to a function along with the list of the headers. The work of the function is to construct a dict out of these two and fill up the <code>key:values</code>. Keep in mind that a missing value should result in a <code>None</code>:<pre>    first_line = fd.readline()
    header = first_line.replace("\n", "").split(",")
    for i, line in enumerate(fd):
        line = line.replace("\n", "").split(",")
        d = return_dict_from_csv_line(header, line)
        print(d)
        if i &gt; 10:
            break</pre><p>The output is as follows:</p></li>
			</ol>
			<div><div><img src="img/C11065_02_10.jpg" alt="Figure 2.10: Section of code" width="1096" height="136"/>
				</div>
			</div>
			<h6>Figure 2.10: Section of output</h6>
			<h3 id="_idParaDest-286"><a id="_idTextAnchor341"/>Solution of Activity 5: Generating Statistics from a CSV File</h3>
			<p>These are the steps to complete this activity:</p>
			<ol>
				<li value="1">Load the necessary libraries:<pre>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt</pre></li>
				<li>Read in the Boston housing dataset (given as a <code>.csv</code> file) from the local direction:<pre># Hint: The Pandas function for reading a CSV file is 'read_csv'.
# Don't forget that all functions in Pandas can be accessed by syntax like pd.{function_name} 
df=pd.read_csv("Boston_housing.csv")</pre></li>
				<li>Check the first 10 records:<pre>df.head(10)</pre><p>The output is as follows:</p><div><img src="img/C11065_03_23.jpg" alt="Figure 3.23: Output displaying the first 10 records" width="1449" height="645"/></div><h6>Figure 3.23: Output displaying the first 10 records</h6></li>
				<li>Find the total number of records:<pre>df.shape</pre><p>The output is as follows:</p><pre>(506, 14)</pre></li>
				<li>Create a smaller DataFrame with columns that do not include <code>CHAS</code>, <code>NOX</code>, <code>B</code>, and <code>LSTAT</code>:<pre>df1=df[['CRIM','ZN','INDUS','RM','AGE','DIS','RAD','TAX','PTRATIO','PRICE']]</pre></li>
				<li>Check the last 7 records of the new DataFrame you just created:<pre>df1.tail(7)</pre><p>The output is as follows:</p><div><img src="img/C11065_03_24.jpg" alt="Figure 3.24: Last seven records of the DataFrame" width="1088" height="477"/></div><h6>Figure 3.24: Last seven records of the DataFrame</h6></li>
				<li>Plot histograms of all the variables (columns) in the new DataFrame by using a <code>for</code> loop:<pre>for c in df1.columns:
    plt.title("Plot of "+c,fontsize=15)
    plt.hist(df1[c],bins=20)
    plt.show()</pre><p>The output is as follows:</p><div><img src="img/C11065_03_25.jpg" alt="Figure 3.25: Plot of all variables using a for loop" width="916" height="1111"/></div><h6>Figure 3.25: Plot of all variables using a for loop</h6></li>
				<li>Crime rate could be an indicator of house price (people don't want to live in high-crime areas). Create a scatter plot of crime rate versus price:<pre>plt.scatter(df1['CRIM'],df1['PRICE'])
plt.show()</pre><p>The output is as follows:</p><div><img src="img/C11065_03_26.jpg" alt="Figure 3.26: Scatter plot of crime rate versus price" width="614" height="252"/></div><h6>Figure 3.26: Scatter plot of crime rate versus price</h6><p>We can understand the relationship better if we plot log10(crime) versus price.</p></li>
				<li>Create that plot of log10(crime) versus price:<pre>plt.scatter(np.log10(df1['CRIM']),df1['PRICE'],c='red')
plt.title("Crime rate (Log) vs. Price plot", fontsize=18)
plt.xlabel("Log of Crime rate",fontsize=15)
plt.ylabel("Price",fontsize=15)
plt.grid(True)
plt.show()</pre><p>The output is as follows:</p><div><img src="img/C11065_03_27.jpg" alt="Figure 3.27: Scatter plot of crime rate (Log) versus price" width="669" height="287"/></div><h6>Figure 3.27: Scatter plot of crime rate (Log) versus price</h6></li>
				<li>Calculate the mean rooms per dwelling:<pre>df1['RM'].mean()</pre><p>The output is <code>6.284634387351788</code>.</p></li>
				<li>Calculate the median age:<pre>df1['AGE'].median()</pre><p>The output is <code>77.5</code>.</p></li>
				<li>Calculate the average (mean) distances to five Boston employment centers:<pre>df1['DIS'].mean()</pre><p>The output is <code>3.795042687747034</code>.</p></li>
				<li>Calculate the percentage of houses with low price <em class="italics">(&lt; $20,000):</em><pre># Create a Pandas series and directly compare it with 20
# You can do this because Pandas series is basically NumPy array and you have seen how to filter NumPy array
low_price=df1['PRICE']&lt;20
# This creates a Boolean array of True, False
print(low_price)
# True = 1, False = 0, so now if you take an average of this NumPy array, you will know how many 1's are there.
# That many houses are priced below 20,000. So that is the answer. 
# You can convert that into percentage by multiplying with 100
pcnt=low_price.mean()*100
print("\nPercentage of house with &lt;20,000 price is: ",pcnt)</pre><p>The output is as follows:</p><pre>0      False
1      False
2      False
3      False
4      False
5      False
6      False
7      False
8       True
9       True
10      True
…
500     True
501    False
502    False
503    False
504    False
505     True
Name: PRICE, Length: 506, dtype: bool
Percentage of house with &lt;20,000 price is:  41.50197628458498</pre></li>
			</ol>
			<h3 id="_idParaDest-287"><a id="_idTextAnchor342"/>Solution of Activity 6: Working with the Adult Income Dataset (UCI)</h3>
			<p>These are the steps to complete this activity:</p>
			<ol>
				<li value="1">Load the necessary libraries:<pre>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt</pre></li>
				<li>Read in the adult income dataset (given as a <code>.csv</code> file) from the local directory and check the first 5 records:<pre>df = pd.read_csv("adult_income_data.csv")
df.head()</pre><p>The output is as follows:</p><div><img src="img/C11065_04_61.jpg" alt="Figure 4.61: DataFrame displaying the first five records from the .csv file" width="719" height="210"/></div><h6>Figure 4.61: DataFrame displaying the first five records from the .csv file</h6></li>
				<li>Create a script that will read a text file line by line and extracts the first line, which is the header of the .csv file:<pre>names = []
with open('adult_income_names.txt','r') as f:
    for line in f:
        f.readline()
        var=line.split(":")[0]
        names.append(var)
names</pre><p>The output is as follows:</p><div><img src="img/C11065_04_62.jpg" alt="Figure 4.62: Names of the columns in the database" width="495" height="214"/></div><h6>Figure 4.62: Names of the columns in the database</h6></li>
				<li>Add a name of <code>Income</code> for the response variable (last column) to the dataset by using the <code>append</code> command:<pre>names.append('Income')</pre></li>
				<li>Read the new file again using the following command:<pre>df = pd.read_csv("adult_income_data.csv",names=names)
df.head()</pre><p>The output is as follows:</p><div><img src="img/C11065_04_63.jpg" alt="" width="735" height="279"/></div><h6>Figure 4.63: DataFrame with the income column added</h6></li>
				<li>Use the <code>describe</code> command to get the statistical summary of the dataset:<pre>df.describe()</pre><p>The output is as follows:</p><div><img src="img/C11065_04_64.jpg" alt="" width="931" height="367"/></div><h6>Figure 4.64: Statistical summary of the dataset</h6><p>Note that only a small number of columns are included. Many variables in the dataset have multiple factors or classes.</p></li>
				<li>Make a list of all the variables in the classes by using the following command:<pre># Make a list of all variables with classes
vars_class = ['workclass','education','marital-status',
              'occupation','relationship','sex','native-country']</pre></li>
				<li>Create a loop to count and print them by using the following command:<pre>for v in vars_class:
    classes=df[v].unique()
    num_classes = df[v].nunique()
    print("There are {} classes in the \"{}\" column. They are: {}".format(num_classes,v,classes))
    print("-"*100)</pre><p>The output is as follows:</p><div><img src="img/C11065_04_65.jpg" alt="Figure 4.65: Output of different factors or classes" width="935" height="526"/></div><h6>Figure 4.65: Output of different factors or classes</h6></li>
				<li>Find the missing values by using the following command:<pre>df.isnull().sum()</pre><p>The output is as follows:</p><div><img src="img/C11065_04_66.jpg" alt="Figure 4.66: Finding the missing values" width="536" height="241"/></div><h6>Figure 4.66: Finding the missing values</h6></li>
				<li>Create a DataFrame with only age, education, and occupation by using subsetting:<pre>df_subset = df[['age','education','occupation']]
df_subset.head()</pre><p>The output is as follows:</p><div><img src="img/C11065_04_67.jpg" alt="Fig 4.67: Subset DataFrame" width="531" height="142"/></div><h6>Figure 4.67: Subset DataFrame</h6></li>
				<li>Plot a histogram of age with a bin size of 20:<pre>df_subset['age'].hist(bins=20)</pre><p>The output is as follows:</p><pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x19dea8d0&gt;</pre><div><img src="img/C11065_04_68.jpg" alt="Figure 4.68: Histogram of age with a bin size of 20" width="525" height="252"/></div><h6>Figure 4.68: Histogram of age with a bin size of 20</h6></li>
				<li>Plot boxplots for <code>age</code> grouped by <code>education</code> (use a long figure size 25x10 and make x ticks font size 15):<pre>df_subset.boxplot(column='age',by='education',figsize=(25,10))
plt.xticks(fontsize=15)
plt.xlabel("Education",fontsize=20)
plt.show()</pre><p>The output is as follows:</p><div><img src="img/C11065_04_69.jpg" alt="Figure 4.69: Boxplot of age grouped by education" width="1482" height="656"/></div><h6>Figure 4.69: Boxplot of age grouped by education</h6><p>Before doing any further operations, we need to use the <code>apply</code> method we learned in this chapter. It turns out that when reading the dataset from the CSV file, all the strings came with a whitespace character in front. So, we need to remove that whitespace from all the strings.</p></li>
				<li>Create a function to strip the whitespace characters:<pre>def strip_whitespace(s):
    return s.strip()</pre></li>
				<li>Use the <code>apply</code> method to apply this function to all the columns with string values, create a new column, copy the values from this new column to the old column, and drop the new column. This is the preferred method so that you don't accidentally delete valuable data. Most of the time, you will need to create a new column with a desired operation and then copy it back to the old column if necessary. Ignore any warning messages that are printed:<pre># Education column
df_subset['education_stripped']=df['education'].apply(strip_whitespace)
df_subset['education']=df_subset['education_stripped']
df_subset.drop(labels=['education_stripped'],axis=1,inplace=True)
# Occupation column
df_subset['occupation_stripped']=df['occupation'].apply(strip_whitespace)
df_subset['occupation']=df_subset['occupation_stripped']
df_subset.drop(labels=['occupation_stripped'],axis=1,inplace=True)</pre><p>This is the sample warning message, which you should ignore:</p><div><img src="img/C11065_04_70.jpg" alt="Figure 4.70: Warning message to be ignored" width="786" height="347"/></div><h6>Figure 4.70: Warning message to be ignored</h6></li>
				<li>Find the number of people who are aged between 30 and 50 (inclusive) by using the following command:<pre># Conditional clauses and join them by &amp; (AND) 
df_filtered=df_subset[(df_subset['age']&gt;=30) &amp; (df_subset['age']&lt;=50)]</pre><p>Check the contents of the new dataset:</p><pre>df_filtered.head()</pre><p>The output is as follows:</p><div><img src="img/C11065_04_71.jpg" alt="Figure 4.71: Contents of new DataFrame" width="844" height="254"/></div><h6>Figure 4.71: Contents of new DataFrame</h6></li>
				<li>Find the <code>shape</code> of the filtered DataFrame and specify the index of the tuple as 0 to return the first element:<pre>answer_1=df_filtered.shape[0]
answer_1</pre><p>The output is as follows:</p><pre>1630</pre></li>
				<li>Print the number of black people aged between 30 and 50 using the following command:<pre>print("There are {} people of age between 30 and 50 in this dataset.".format(answer_1))</pre><p>The output is as follows:</p><pre>There are 1630 black of age between 30 and 50 in this dataset.</pre></li>
				<li>Group the records based on occupation to find how the mean age is distributed:<pre>df_subset.groupby('occupation').describe()['age']</pre><p>The output is as follows:</p><div><img src="img/C11065_04_72.jpg" alt="Figure 4.72: DataFrame with data grouped by age and education" width="615" height="386"/></div><h6>Figure 4.72: DataFrame with data grouped by age and education</h6><p>The code returns <code>79 rows × 1 columns.</code></p></li>
				<li>Group by occupation and show the summary statistics of age. Find which profession has the oldest workers on average and which profession has its largest share of workforce above the 75th percentile:<pre>df_subset.groupby('occupation').describe()['age']</pre><p>The output is as follows:</p><div><img src="img/C11065_04_73.jpg" alt="Figure 4.73: DataFrame showing summary statistics of age" width="668" height="376"/></div><h6>Figure 4.73: DataFrame showing summary statistics of age</h6><p>Is there a particular occupation group that has very low representation? Perhaps we should remove those pieces of data because with very low data, the group won't be useful in analysis. Actually, just by looking at the preceding table, you should be able to see that the <code>barh</code> function is the index of the DataFrame, which is the summary stats of the occupation groups. We can see that the <strong class="bold">Armed-Forces</strong> group has almost no data. This exercise teaches you that, sometimes, the outlier is not just a value, but can be a whole group. The data of this group is fine, but it is too small to be useful for any analysis. So, it can be treated as an outlier in this case. But always use your business knowledge and engineering judgement for such outlier detection and how to process them.</p></li>
				<li>Use subset and groupby to find the outliers:<pre>occupation_stats= df_subset.groupby(
    'occupation').describe()['age']</pre></li>
				<li>Plot the values on a bar chart:<pre>plt.figure(figsize=(15,8))
plt.barh(y=occupation_stats.index,
         width=occupation_stats['count'])
plt.yticks(fontsize=13)
plt.show()</pre><p>The output is as follows:</p><div><img src="img/C11065_04_74.jpg" alt="Figure 4.74: Bar chart displaying occupation statistics" width="981" height="469"/></div><h6>Figure 4.74: Bar chart displaying occupation statistics</h6></li>
				<li>Practice merging by common keys. Suppose you are given two datasets where the common key is <code>occupation</code>. First, create two such disjoint datasets by taking random samples from the full dataset and then try merging. Include at least two other columns, along with the common key column for each dataset. Notice how the resulting dataset, after merging, may have more data points than either of the two starting datasets if your common key is not unique:<pre>df_1 = df[['age',
           'workclass',
           'occupation']].sample(5,random_state=101)
df_1.head()</pre><p>The output is as follows:</p></li>
			</ol>
			<div><div><img src="img/C11065_04_75.jpg" alt="" width="453" height="144"/>
				</div>
			</div>
			<h6>Figure 4.75: Output after merging the common keys</h6>
			<p>The second dataset is as follows:</p>
			<pre>df_2 = df[['education',
           'occupation']].sample(5,random_state=101)
df_2.head()</pre>
			<p>The output is as follows:</p>
			<div><div><img src="img/C11065_04_76.jpg" alt="" width="515" height="148"/>
				</div>
			</div>
			<h6>Figure 4.76: Output after merging the common keys</h6>
			<p>Merging the two datasets together:</p>
			<pre>df_merged = pd.merge(df_1,df_2,
                     on='occupation',
                     how='inner').drop_duplicates()
df_merged</pre>
			<p>The output is as follows:</p>
			<div><div><img src="img/C11065_04_77.jpg" alt="" width="501" height="150"/>
				</div>
			</div>
			<h6>Figure 4.77: Output of distinct occupation values</h6>
			<h3 id="_idParaDest-288"><a id="_idTextAnchor343"/>Solution of Activity 7: Reading Tabular Data from a Web Page and Creating DataFrames</h3>
			<p>These are the steps to complete this activity:</p>
			<ol>
				<li value="1">Import BeautifulSoup and load the data by using the following command:<pre>from bs4 import BeautifulSoup
import pandas as pd</pre></li>
				<li>Open the Wikipedia file by using the following command:<pre>fd = open("List of countries by GDP (nominal) - Wikipedia.htm", "r")
soup = BeautifulSoup(fd)
fd.close()</pre></li>
				<li>Calculate the tables by using the following command:<pre>all_tables = soup.find_all("table")
print("Total number of tables are {} ".format(len(all_tables)))</pre><p>There are 9 tables in total.</p></li>
				<li>Find the right table using the class attribute by using the following command:<pre>data_table = soup.find("table", {"class": '"wikitable"|}'})
print(type(data_table))</pre><p>The output is as follows:</p><pre>&lt;class 'bs4.element.Tag'&gt;</pre></li>
				<li>Separate the source and the actual data by using the following command:<pre>sources = data_table.tbody.findAll('tr', recursive=False)[0]
sources_list = [td for td in sources.findAll('td')]
print(len(sources_list))</pre><p>The output is as follows:</p><pre>Total number of tables are <code>3.</code></pre></li>
				<li>Use <code>findAll</code> function to find the data from the <code>data_table</code>'s <code>body</code> tag, using the following command:<pre>data = data_table.tbody.findAll('tr', recursive=False)[1].findAll('td', recursive=False)</pre></li>
				<li>Use the <code>findAll</code> function to find the data from the <code>data_table</code> <code>td</code> tag by using the following command:<pre>data_tables = []
for td in data:
    data_tables.append(td.findAll('table'))</pre></li>
				<li>Find the length of <code>data_tables</code> by using the following command:<pre>len(data_tables)</pre><p>The output is as follows:</p><pre>3</pre></li>
				<li>Check how to get the source names by using the following command:<pre>source_names = [source.findAll('a')[0].getText() for source in sources_list]
print(source_names)</pre><p>The output is as follows:</p><pre> ['International Monetary Fund', 'World Bank', 'United Nations']</pre></li>
				<li>Separate the header and data for the first source:<pre>header1 = [th.getText().strip() for th in data_tables[0][0].findAll('thead')[0].findAll('th')]
header1</pre><p>The output is as follows:</p><pre> ['Rank', 'Country', 'GDP(US$MM)']</pre></li>
				<li>Find the rows from <code>data_tables</code> using <code>findAll</code>:<pre>rows1 = data_tables[0][0].findAll('tbody')[0].findAll('tr')[1:]</pre></li>
				<li>Find the data from <code>rows1</code> using the <code>strip</code> function for each <code>td</code> tag:<pre>data_rows1 = [[td.get_text().strip() for td in tr.findAll('td')] for tr in rows1]</pre></li>
				<li>Find the DataFrame:<pre>df1 = pd.DataFrame(data_rows1, columns=header1)
df1.head()</pre><p>The output is as follows:</p><div><img src="img/C11065_05_351.jpg" alt="Figure 5.35: DataFrame" width="568" height="184"/></div><h6>Figure 5.35: DataFrame created from Web page</h6></li>
				<li>Do the same for the other two sources by using the following command:<pre>header2 = [th.getText().strip() for th in data_tables[1][0].findAll('thead')[0].findAll('th')]
header2</pre><p>The output is as follows:</p><pre> <code>['Rank', 'Country', 'GDP(US$MM)']</code></pre></li>
				<li>Find the rows from <code>data_tables</code> using <code>findAll</code> by using the following command:<pre>rows2 = data_tables[1][0].findAll('tbody')[0].findAll('tr')[1:]</pre></li>
				<li>Define <code>find_right_text</code> using the <code>strip</code> function by using the following command:<pre>def find_right_text(i, td):
    if i == 0:
        return td.getText().strip()
    elif i == 1:
        return td.getText().strip()
    else:
        index = td.text.find("♠")
        return td.text[index+1:].strip()</pre></li>
				<li>Find the rows from <code>data_rows</code> using <code>find_right_text</code> by using the following command:<pre>data_rows2 = [[find_right_text(i, td) for i, td in enumerate(tr.findAll('td'))] for tr in rows2]</pre></li>
				<li>Calculate the <code>df2</code> DataFrame by using the following command:<pre>df2 = pd.DataFrame(data_rows2, columns=header2)
df2.head()</pre><p>The output is as follows:</p><div><img src="img/C11065_05_44.jpg" alt="Figure 5.36: Output of the DataFrame&#13;&#10;" width="623" height="182"/></div><h6>Figure 5.36: Output of the DataFrame</h6></li>
				<li>Now, perform the same operations for the third DataFrame by using the following command:<pre>header3 = [th.getText().strip() for th in data_tables[2][0].findAll('thead')[0].findAll('th')]
header3</pre><p>The output is as follows:</p><pre><code>['Rank', 'Country', 'GDP(US$MM)']</code></pre></li>
				<li>Find the rows from <code>data_tables</code> using <code>findAll</code> by using the following command:<pre>rows3 = data_tables[2][0].findAll('tbody')[0].findAll('tr')[1:]</pre></li>
				<li>Find the rows from <code>data_rows3</code> by using <code>find_right_text</code>:<pre>data_rows3 = [[find_right_text(i, td) for i, td in enumerate(tr.findAll('td'))] for tr in rows2]</pre></li>
				<li>Calculate the <code>df3</code> DataFrame by using the following command:<pre>df3 = pd.DataFrame(data_rows3, columns=header3)
df3.head()</pre><p>The output is as follows:</p></li>
			</ol>
			<div><div><img src="img/C11065_05_55.jpg" alt="Figure 5.37: The third DataFrame&#13;&#10;" width="535" height="181"/>
				</div>
			</div>
			<h6>Figure 5.37: The third DataFrame</h6>
			<h3 id="_idParaDest-289"><a id="_idTextAnchor344"/>Solution of Activity 8: Handling Outliers and Missing Data </h3>
			<p>These are the steps to complete this activity:</p>
			<ol>
				<li value="1">Load the data:<pre>import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline</pre></li>
				<li>Read the .csv file:<pre>df = pd.read_csv("visit_data.csv")</pre></li>
				<li>Print the data from the DataFrame:<pre>df.head()</pre><p>The output is as follows:</p><div><img src="img/C11065_06_10.jpg" alt="Figure 6.10: The contents of the CSV file" width="538" height="152"/></div><h6>Figure 6.10: The contents of the CSV file</h6><p>As we can see, there is data where some values are missing, and if we examine this, we will see some outliers.</p></li>
				<li>Check for duplicates by using the following command:<pre>print("First name is duplicated - {}".format(any(df.first_name.duplicated())))
print("Last name is duplicated - {}".format(any(df.last_name.duplicated())))
print("Email is duplicated - {}".format(any(df.email.duplicated())))</pre><p>The output is as follows:</p><pre>First name is duplicated - True
Last name is duplicated - True
Email is duplicated - False</pre><p>There are duplicates in both the first and last names, which is normal. However, as we can see, there is no duplicate in email. That's good.</p></li>
				<li>Check if any essential column contains <code>NaN</code>:<pre># Notice that we have different ways to format boolean values for the % operator
print("The column Email contains NaN - %r " % df.email.isnull().values.any())
print("The column IP Address contains NaN - %s " % df.ip_address.isnull().values.any())
print("The column Visit contains NaN - %s " % df.visit.isnull().values.any())</pre><p>The output is as follows:</p><pre>The column Email contains NaN - False 
The column IP Address contains NaN - False 
The column Visit contains NaN - True </pre><p>The column visit contains some None values. Given that the final task at hand will probably be predicting the number of visits, we cannot do anything with rows that do not have that information. They are a type of outlier. Let's get rid of them.</p></li>
				<li>Get rid of the outliers:<pre># There are various ways to do this. This is just one way. We encourage you to explore other ways.
# But before that we need to store the previous size of the data set and we will compare it with the new size
size_prev = df.shape
df = df[np.isfinite(df['visit'])] #This is an inplace operation. After this operation the original DataFrame is lost.
size_after = df.shape</pre></li>
				<li>Report the size difference:<pre># Notice how parameterized format is used and then the indexing is working inside the quote marks
print("The size of previous data was - {prev[0]} rows and the size of the new one is - {after[0]} rows".
format(prev=size_prev, after=size_after))</pre><p>The output is as follows:</p><pre>The size of previous data was - 1000 rows and the size of the new one is - 974 rows</pre></li>
				<li>Plot a boxplot to find if the data has outliers.<pre>plt.boxplot(df.visit, notch=True)</pre><p>The output is as follows:</p><pre>{'whiskers': [&lt;matplotlib.lines.Line2D at 0x7fa04cc08668&gt;,
  &lt;matplotlib.lines.Line2D at 0x7fa04cc08b00&gt;],
 'caps': [&lt;matplotlib.lines.Line2D at 0x7fa04cc08f28&gt;,
  &lt;matplotlib.lines.Line2D at 0x7fa04cc11390&gt;],
 'boxes': [&lt;matplotlib.lines.Line2D at 0x7fa04cc08518&gt;],
 'medians': [&lt;matplotlib.lines.Line2D at 0x7fa04cc117b8&gt;],
 'fliers': [&lt;matplotlib.lines.Line2D at 0x7fa04cc11be0&gt;],
 'means': []}</pre><p>The boxplot is as follows:</p><div><img src="img/C11065_06_43.jpg" alt="Figure 6.43: Boxplot using the data" width="528" height="240"/></div><h6>Figure 6.43: Boxplot using the data</h6><p>As we can see, we have data in this column in the interval (0, 3000). However, the main concentration of the data is between ~700 and ~2300. </p></li>
				<li>Get rid of values beyond 2900 and below 100 – these are outliers for us. We need to get rid of them:<pre>df1 = df[(df['visit'] &lt;= 2900) &amp; (df['visit'] &gt;= 100)]  # Notice the powerful &amp; operator
# Here we abuse the fact the number of variable can be greater than the number of replacement targets
print("After getting rid of outliers the new size of the data is - {}".format(*df1.shape))</pre><p>After getting rid of the outliers, the new size of the data is <code>923.</code></p><p>This is the end of the activity for this chapter.</p></li>
			</ol>
			<h3 id="_idParaDest-290"><a id="_idTextAnchor345"/>Solution of Activity 9: Extracting the Top 100 eBooks from Gutenberg</h3>
			<p>These are the steps to complete this activity:</p>
			<ol>
				<li value="1">Import the necessary libraries, including <code>regex</code> and <code>beautifulsoup:</code><pre>import urllib.request, urllib.parse, urllib.error
import requests
from bs4 import BeautifulSoup
import ssl
import re</pre></li>
				<li>Check the SSL certificate:<pre># Ignore SSL certificate errors
ctx = ssl.create_default_context()
ctx.check_hostname = False
ctx.verify_mode = ssl.CERT_NONE</pre></li>
				<li>Read the HTML from the URL:<pre># Read the HTML from the URL and pass on to BeautifulSoup
top100url = 'https://www.gutenberg.org/browse/scores/top'
response = requests.get(top100url)</pre></li>
				<li>Write a small function to check the status of the web request:<pre>def status_check(r):
    if r.status_code==200:
        print("Success!")
        return 1
    else:
        print("Failed!")
        return -1</pre></li>
				<li>Check the status of <code>response</code>:<pre>status_check(response)</pre><p>The output is as follows:</p><pre>Success!
1</pre></li>
				<li>Decode the response and pass it on to <code>BeautifulSoup</code> for HTML parsing:<pre>contents = response.content.decode(response.encoding)
soup = BeautifulSoup(contents, 'html.parser')</pre></li>
				<li>Find all the <code>href</code> tags and store them in the list of links. Check what the list looks like – print the first 30 elements:<pre># Empty list to hold all the http links in the HTML page
lst_links=[]
# Find all the href tags and store them in the list of links
for link in soup.find_all('a'):
    #print(link.get('href'))
    lst_links.append(link.get('href'))</pre></li>
				<li>Print the links by using the following command:<pre>lst_links[:30]</pre><p>The output is as follows:</p><pre>['/wiki/Main_Page',
 '/catalog/',
 '/ebooks/',
 '/browse/recent/last1',
 '/browse/scores/top',
 '/wiki/Gutenberg:Offline_Catalogs',
 '/catalog/world/mybookmarks',
 '/wiki/Main_Page',
'https://www.paypal.com/xclick/business=donate%40gutenberg.org&amp;item_name=Donation+to+Project+Gutenberg',
 '/wiki/Gutenberg:Project_Gutenberg_Needs_Your_Donation',
 'http://www.ibiblio.org',
 'http://www.pgdp.net/',
 'pretty-pictures',
 '#books-last1',
 '#authors-last1',
 '#books-last7',
 '#authors-last7',
 '#books-last30',
 '#authors-last30',
 '/ebooks/1342',
 '/ebooks/84',
 '/ebooks/1080',
 '/ebooks/46',
 '/ebooks/219',
 '/ebooks/2542',
 '/ebooks/98',
 '/ebooks/345',
 '/ebooks/2701',
 '/ebooks/844',
 '/ebooks/11']</pre></li>
				<li>Use a regular expression to find the numeric digits in these links. These are the file numbers for the top 100 books. Initialize the empty list to hold the file numbers:<pre>booknum=[]</pre></li>
				<li>Numbers 19 to 118 in the original list of links have the top 100 eBooks' numbers. Loop over the appropriate range and use a regex to find the numeric digits in the link (href) string. Use the <code>findall()</code> method:<pre>for i in range(19,119):
    link=lst_links[i]
    link=link.strip()
    # Regular expression to find the numeric digits in the link (href) string
    n=re.findall('[0-9]+',link)
    if len(n)==1:
        # Append the filenumber casted as integer
        booknum.append(int(n[0]))</pre></li>
				<li>Print the file numbers:<pre>print ("\nThe file numbers for the top 100 ebooks on Gutenberg are shown below\n"+"-"*70)
print(booknum)</pre><p>The output is as follows:</p><pre>The file numbers for the top 100 ebooks on Gutenberg are shown below
----------------------------------------------------------------------
[1342, 84, 1080, 46, 219, 2542, 98, 345, 2701, 844, 11, 5200, 43, 16328, 76, 74, 1952, 6130, 2591, 1661, 41, 174, 23, 1260, 1497, 408, 3207, 1400, 30254, 58271, 1232, 25344, 58269, 158, 44881, 1322, 205, 2554, 1184, 2600, 120, 16, 58276, 5740, 34901, 28054, 829, 33, 2814, 4300, 100, 55, 160, 1404, 786, 58267, 3600, 19942, 8800, 514, 244, 2500, 2852, 135, 768, 58263, 1251, 3825, 779, 58262, 203, 730, 20203, 35, 1250, 45, 161, 30360, 7370, 58274, 209, 27827, 58256, 33283, 4363, 375, 996, 58270, 521, 58268, 36, 815, 1934, 3296, 58279, 105, 2148, 932, 1064, 13415]</pre></li>
				<li>What does the soup object's text look like? Use the .<code>text</code> method and print only the first 2,000 characters (do not print the whole thing as it is too long).<p>You will notice a lot of empty spaces/blanks here and there. Ignore them. They are part of the HTML page's markup and its whimsical nature:</p><pre>print(soup.text[:2000])
if (top != self) {
        top.location.replace (<a href="http://www.gutenberg.org">http://www.gutenberg.org</a>);
        alert ('Project Gutenberg is a FREE service with NO membership required. If you paid somebody else to get here, make them give you your money back!');
      }
    </pre><p>The output is as follows:</p><pre>Top 100 - Project Gutenberg
Online Book Catalog
 Book  Search
-- Recent  Books
-- Top  100
-- Offline Catalogs
-- My Bookmarks
Main Page
…
Pretty Pictures
Top 100 EBooks yesterday —
  Top 100 Authors yesterday —
  Top 100 EBooks last 7 days —
  Top 100 Authors last 7 days —
  Top 100 EBooks last 30 days —
  Top 100 Authors last 30 days
Top 100 EBooks yesterday
Pride and Prejudice by Jane Austen (1826)
Frankenstein; Or, The Modern Prometheus by Mary Wollstonecraft Shelley (1367)
A Modest Proposal by Jonathan Swift (1020)
A Christmas Carol in Prose; Being a Ghost Story of Christmas by Charles Dickens (953)
Heart of Darkness by Joseph Conrad (887)
Et dukkehjem. English by Henrik Ibsen (761)
A Tale of Two Cities by Charles Dickens (741)
Dracula by Bram Stoker (732)
Moby Dick; Or, The Whale by Herman Melville (651)
The Importance of Being Earnest: A Trivial Comedy for Serious People by Oscar Wilde (646)
Alice's Adventures in Wonderland by Lewis Carrol</pre></li>
				<li>Search the extracted text (using regular expression) from the soup object to find the names of top 100 eBooks (yesterday's rank):<pre># Temp empty list of Ebook names
lst_titles_temp=[]</pre></li>
				<li>Create a starting index. It should point at the text <code>splitlines</code> method of <code>soup.text</code>. It splits the lines of the text of the soup object:<pre>start_idx=soup.text.splitlines().index('Top 100 EBooks yesterday')</pre></li>
				<li>Loop 1-100 to add the strings of the next 100 lines to this temporary list. Hint: use the <code>splitlines</code> method:<pre>for i in range(100):
    lst_titles_temp.append(soup.text.splitlines()[start_idx+2+i])</pre></li>
				<li>Use a regular expression to extract only text from the name strings and append them to an empty list. Use match and span to find the indices and use them:<pre>lst_titles=[]
for i in range(100):
    id1,id2=re.match('^[a-zA-Z ]*',lst_titles_temp[i]).span()
    lst_titles.append(lst_titles_temp[i][id1:id2])</pre></li>
				<li>Print the list of titles:<pre>for l in lst_titles:
    print(l)</pre><p>The output is as follows:</p><pre>Pride and Prejudice by Jane Austen 
Frankenstein
A Modest Proposal by Jonathan Swift 
A Christmas Carol in Prose
Heart of Darkness by Joseph Conrad 
Et dukkehjem
A Tale of Two Cities by Charles Dickens 
Dracula by Bram Stoker 
Moby Dick
The Importance of Being Earnest
Alice
Metamorphosis by Franz Kafka 
The Strange Case of Dr
Beowulf
…
The Russian Army and the Japanese War
Calculus Made Easy by Silvanus P
Beyond Good and Evil by Friedrich Wilhelm Nietzsche 
An Occurrence at Owl Creek Bridge by Ambrose Bierce 
Don Quixote by Miguel de Cervantes Saavedra 
Blue Jackets by Edward Greey 
The Life and Adventures of Robinson Crusoe by Daniel Defoe 
The Waterloo Campaign 
The War of the Worlds by H
Democracy in America 
Songs of Innocence
The Confessions of St
Modern French Masters by Marie Van Vorst 
Persuasion by Jane Austen 
The Works of Edgar Allan Poe 
The Fall of the House of Usher by Edgar Allan Poe 
The Masque of the Red Death by Edgar Allan Poe 
The Lady with the Dog and Other Stories by Anton Pavlovich Chekhov</pre></li>
			</ol>
			<h3 id="_idParaDest-291"><a id="_idTextAnchor346"/>Solution of Activity 10: Extracting the top 100 eBooks from Gutenberg.org</h3>
			<p>These are the steps to complete this activity:</p>
			<ol>
				<li value="1">Import <code>urllib.request</code>, <code>urllib.parse</code>, <code>urllib.error</code>, and <code>json</code>:<pre>import urllib.request, urllib.parse, urllib.error
import json</pre></li>
				<li>Load the secret API key (you have to get one from the OMDB website and use that; it has a 1,000 daily limit) from a JSON file, stored in the same folder into a variable, by using <code>json.loads()</code>: <h4>Note</h4><p class="callout">The following cell will not be executed in the solution notebook because the author cannot give out their private API key.</p></li>
				<li>The students/users/instructor will need to obtain a key and store it in a JSON file. We are calling this file <code>APIkeys.json</code>. </li>
				<li>Open the <code>APIkeys.json</code> file by using the following command:<pre>with open('APIkeys.json') as f:
    keys = json.load(f)
    omdbapi = keys['OMDBapi']</pre><p>The final URL to be passed should look like this: <a href="http://www.omdbapi.com/?t=movie_name&amp;apikey=secretapikey">http://www.omdbapi.com/?t=movie_name&amp;apikey=secretapikey</a>.</p></li>
				<li>Assign the OMDB portal (<a href="http://www.omdbapi.com/?">http://www.omdbapi.com/?</a>) as a string to a variable called <code>serviceurl</code> by using the following command:<pre>serviceurl = 'http://www.omdbapi.com/?'</pre></li>
				<li>Create a variable called <code>apikey</code> with the last portion of the URL (<code>&amp;apikey=secretapikey</code>), where <code>secretapikey</code> is your own API key. The movie name portion is <code>t=movie_name</code>, and will be addressed later:<pre>apikey = '&amp;apikey='+omdbapi</pre></li>
				<li>Write a utility function called <code>print_json</code> to print the movie data from a JSON file (which we will get from the portal). Here are the keys of a JSON file: 'Title', 'Year', 'Rated', 'Released', 'Runtime', 'Genre', 'Director', 'Writer', 'Actors', 'Plot', 'Language','Country', 'Awards', 'Ratings', 'Metascore', 'imdbRating', 'imdbVotes', and 'imdbID':<pre>def print_json(json_data):
    list_keys=['Title', 'Year', 'Rated', 'Released', 'Runtime', 'Genre', 'Director', 'Writer', 
               'Actors', 'Plot', 'Language', 'Country', 'Awards', 'Ratings', 
               'Metascore', 'imdbRating', 'imdbVotes', 'imdbID']
    print("-"*50)
    for k in list_keys:
        if k in list(json_data.keys()):
            print(f"{k}: {json_data[k]}")
    print("-"*50)</pre></li>
				<li>Write a utility function to download a poster of the movie based on the information from the JSON dataset and save it in your local folder. Use the <code>os</code> module. The poster data is stored in the JSON key <code>Poster</code>. You may want to split the name of the <code>Poster</code> file and extract the file extension only. Let's say that the extension is <code>jpg</code>. We would later join this extension to the movie name and create a filename such as <code>movie.jpg</code>. Use the open Python command open to open a file and write the poster data. Close the file after you're done. This function may not return anything. It just saves the poster data as an image file:<pre>def save_poster(json_data):
    import os
    title = json_data['Title']
    poster_url = json_data['Poster']
    # Splits the poster url by '.' and picks up the last string as file extension
    poster_file_extension=poster_url.split('.')[-1]
    # Reads the image file from web
    poster_data = urllib.request.urlopen(poster_url).read()
        
    savelocation=os.getcwd()+'\\'+'Posters'+'\\'
    # Creates new directory if the directory does not exist. Otherwise, just use the existing path.
    if not os.path.isdir(savelocation):
        os.mkdir(savelocation)
    
    filename=savelocation+str(title)+'.'+poster_file_extension
    f=open(filename,'wb')
    f.write(poster_data)
    f.close()</pre></li>
				<li>Write a utility function called <code>search_movie</code> to search a movie by its name, print the downloaded JSON data (use the <code>print_json</code> function for this), and save the movie poster in the local folder (use the <code>save_poster</code> function for this). Use a <code>try-except</code> loop for this, that is, try to connect to the web portal. If successful, proceed, but if not (that is, if an exception is raised), then just print an error message. Use the previously created variables <code>serviceurl</code> and <code>apikey</code>. You have to pass on a dictionary with a key, <code>t</code>, and the movie name as the corresponding value to the <code>urllib.parse.urlencode</code> function and then add the <code>serviceurl</code> and <code>apikey</code> to the output of the function to construct the full URL. This URL will be used for accessing the data. The JSON data has a key called <code>Response</code>. If it is <code>True</code>, that means that the read was successful. Check this before processing the data. If it was not successful, then print the JSON key <code>Error</code>, which will contain the appropriate error message that's returned by the movie database:<pre>def search_movie(title):
    try:
        url = serviceurl + urllib.parse.urlencode({'t': str(title)})+apikey
        print(f'Retrieving the data of "{title}" now... ')
        print(url)
        uh = urllib.request.urlopen(url)
        data = uh.read()
        json_data=json.loads(data)
        
        if json_data['Response']=='True':
            print_json(json_data)
            # Asks user whether to download the poster of the movie
            if json_data['Poster']!='N/A':
                save_poster(json_data)
        else:
            print("Error encountered: ",json_data['Error'])
    
    except urllib.error.URLError as e:
        print(f"ERROR: {e.reason}"</pre></li>
				<li>Test the <code>search_movie</code> function by entering <code>Titanic</code>:<pre>search_movie("Titanic")</pre><p>The following is  the retrieved data for <code>Titanic</code>:</p><pre>http://www.omdbapi.com/?t=Titanic&amp;apikey=17cdc959
--------------------------------------------------
Title: Titanic
Year: 1997
Rated: PG-13
Released: 19 Dec 1997
Runtime: 194 min
Genre: Drama, Romance
Director: James Cameron
Writer: James Cameron
Actors: Leonardo DiCaprio, Kate Winslet, Billy Zane, Kathy Bates
Plot: A seventeen-year-old aristocrat falls in love with a kind but poor artist aboard the luxurious, ill-fated R.M.S. Titanic.
Language: English, Swedish
Country: USA
Awards: Won 11 Oscars. Another 111 wins &amp; 77 nominations.
Ratings: [{'Source': 'Internet Movie Database', 'Value': '7.8/10'}, {'Source': 'Rotten Tomatoes', 'Value': '89%'}, {'Source': 'Metacritic', 'Value': '75/100'}]
Metascore: 75
imdbRating: 7.8
imdbVotes: 913,780
imdbID: tt0120338
--------------------------------------------------</pre></li>
				<li>Test the <code>search_movie</code> function by entering <code>"Random_error"</code> (obviously, this will not be found, and you should be able to check whether your error catching code is working properly):<pre>search_movie("Random_error")</pre><p>Retrieve the data of <code>"Random_error"</code>:</p><pre>http://www.omdbapi.com/?t=Random_error&amp;apikey=17cdc959
Error encountered:  Movie not found!</pre></li>
			</ol>
			<p>Look for a folder called <code>Posters</code> in the same directory you are working in. It should contain a file called <code>Titanic.jpg</code>. Check the file.</p>
			<h3 id="_idParaDest-292">Solution of Activity 11: Retrieving D<a id="_idTextAnchor347"/>ata Correctly from Databases</h3>
			<p>These are the steps to complete this activity:</p>
			<ol>
				<li value="1">Connect to the supplied <code>petsDB</code> database:<pre>import sqlite3
conn = sqlite3.connect("petsdb")</pre></li>
				<li>Write a function to check whether the connection has been successful:<pre># a tiny function to make sure the connection is successful
def is_opened(conn):
    try:
        conn.execute("SELECT * FROM persons LIMIT 1")
        return True
    except sqlite3.ProgrammingError as e:
        print("Connection closed {}".format(e))
        return False
print(is_opened(conn))</pre><p>The output is as follows:</p><pre>True</pre></li>
				<li>Close the connection:<pre>conn.close()</pre></li>
				<li>Check whether the connection is open or closed:<pre>print(is_opened(conn))</pre><p>The output is as follows:</p><pre>False</pre></li>
				<li>Find out the different age groups are in the <code>persons</code> database. Connect to the supplied <code>petsDB</code> database:<pre>conn = sqlite3.connect("petsdb")
c = conn.cursor()</pre></li>
				<li>Execute the following command:<pre>for ppl, age in c.execute("SELECT count(*), age FROM persons GROUP BY age"):
    print("We have {} people aged {}".format(ppl, age))</pre><p>The output is as follows:</p><div><img src="img/C11065_08_17.jpg" alt="Figure 8.17: Section of output grouped by age" width="738" height="415"/></div><h6>Figure 8.17: Section of output grouped by age</h6></li>
				<li>To find out which age group has the highest number of people, execute the following command:<pre>sfor ppl, age in c.execute(
    "SELECT count(*), age FROM persons GROUP BY age ORDER BY count(*) DESC"):
    print("Highest number of people is {} and came from {} age group".format(ppl, age))
    break</pre><p>The output is as follows:</p><pre>Highest number of people is 5 and came from 73 age group</pre></li>
				<li>To find out how many people do not have a full name (the last name is blank/null), execute the following command:<pre>res = c.execute("SELECT count(*) FROM persons WHERE last_name IS null")
for row in res:
    print(row)</pre><p>The output is as follows:</p><pre>(60,)</pre></li>
				<li>To find out how many people have more than one pet, execute the following command:<pre>res = c.execute("SELECT count(*) FROM (SELECT count(owner_id) FROM pets GROUP BY owner_id HAVING count(owner_id) &gt;1)")
for row in res:
    print("{} People has more than one pets".format(row[0]))</pre><p>The output is as follows:</p><pre>43 People has more than one pets</pre></li>
				<li>To find out how many pets have received treatment, execute the following command:<pre>res = c.execute("SELECT count(*) FROM pets WHERE treatment_done=1")
for row in res:
    print(row)</pre><p>The output is as follows:</p><pre>(36,)</pre></li>
				<li>To find out how many pets have received treatment and the type of pet is known, execute the following command:<pre>res = c.execute("SELECT count(*) FROM pets WHERE treatment_done=1 AND pet_type IS NOT null")
for row in res:
    print(row)</pre><p>The output is as follows:</p><pre>(16,)</pre></li>
				<li>To find out how many pets are from the city called "east port", execute the following command:<pre>res = c.execute("SELECT count(*) FROM pets JOIN persons ON pets.owner_id = persons.id WHERE persons.city='east port'")
for row in res:
    print(row)</pre><p>The output is as follows:</p><pre>(49,)</pre></li>
				<li>To find out how many pets are from the city called "east port" and who received treatment, execute the following command:<pre>res = c.execute("SELECT count(*) FROM pets JOIN persons ON pets.owner_id = persons.id WHERE persons.city='east port' AND pets.treatment_done=1")
for row in res:
    print(row)</pre><p>The output is as follows:</p><pre>(11,)</pre></li>
			</ol>
			<h3 id="_idParaDest-293"><a id="_idTextAnchor348"/>Solution of Activity 12: Data Wrangling Task – Fixing UN Data</h3>
			<p>These are the steps to complete this activity:</p>
			<ol>
				<li value="1">Import the required libraries:<pre>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')s</pre></li>
				<li>Save the URL of the dataset and use the pandas <code>read_csv</code> method to directly pass this link and create a DataFrame:<pre>education_data_link="http://data.un.org/_Docs/SYB/CSV/SYB61_T07_Education.csv"
df1 = pd.read_csv(education_data_link)</pre></li>
				<li>Print the data in the DataFrame:<pre>df1.head()</pre><p>The output is as follows:</p><div><img src="img/C11065_9_3.jpg" alt="Figure 9.4: DataFrame after removing the first row" width="1096" height="264"/></div><h6>Figure 9.3: DataFrame from the UN data</h6></li>
				<li>As the first row does not contain useful information, use the <code>skiprows</code> parameter to remove the first row:<pre>df1 = pd.read_csv(education_data_link,skiprows=1)</pre></li>
				<li>Print the data in the DataFrame:<pre>df1.head()</pre><p>The output is as follows:</p><div><img src="img/C11065_9_4.jpg" alt="Figure 9.4: DataFrame after removing the first row" width="1112" height="251"/></div><h6>Figure 9.4: DataFrame after removing the first row</h6></li>
				<li>Drop the column Region/Country/Area and Source as they will not be very helpful:<pre>df2 = df1.drop(['Region/Country/Area','Source'],axis=1)</pre></li>
				<li>Assign the following names as the columns of the DataFrame: <code>['Region/Country/Area','Year','Data','Value','Footnotes']</code><pre>df2.columns=['Region/Country/Area','Year','Data','Enrollments (Thousands)','Footnotes']</pre></li>
				<li>Print the data in the DataFrame:<pre>df1.head()</pre><p>The output is as follows:</p><div><img src="img/C11065_9_5.jpg" alt="Figure 9.5: DataFrame after dropping Region/Country/Area and Source columns" width="817" height="184"/></div><h6>Figure 9.5: DataFrame after dropping Region/Country/Area and Source columns</h6></li>
				<li>Check how many unique values the <code>Footnotes</code> column contains:<pre>df2['Footnotes'].unique()</pre><p>The output is as follows:</p><div><img src="img/C11065_9_6.jpg" alt="Figure 9.6: Unique values of the Footnotes column" width="1098" height="72"/></div><h6>Figure 9.6: Unique values of the Footnotes column</h6></li>
				<li>Convert the <code>Value</code> column data into a numeric one for further processing:<pre>type(df2['Enrollments (Thousands)'][0])</pre><p>The output is as follows:</p><pre>str</pre></li>
				<li>Create a utility function to convert the strings in the Value column into floating-point numbers:<pre>def to_numeric(val):
    """
    Converts a given string (with one or more commas) to a numeric value
    """
    if ',' not in str(val):
        result = float(val)
    else:
        val=str(val)
        val=''.join(str(val).split(','))
        result=float(val)
    return result</pre></li>
				<li>Use the <code>apply</code> method to apply this function to the <code>Value</code> column data:<pre>df2['Enrollments (Thousands)']=df2['Enrollments (Thousands)'].apply(to_numeric)</pre></li>
				<li>Print the unique types of data in the <code>Data</code> column:<pre>df2['Data'].unique()</pre><p>The output is as follows:</p><div><img src="img/C11065_9_7.jpg" alt="" width="801" height="188"/></div><h6>Figure 9.7:Unique values in a column</h6></li>
				<li>Create three DataFrames by filtering and selecting them from the original DataFrame:<ul><li><strong class="bold">df_primary</strong>: Only students enrolled in primary education (thousands)</li><li><strong class="bold">df_secondary</strong>: Only students enrolled in secondary education (thousands)</li><li><strong class="bold">df_tertiary</strong>: Only students enrolled in tertiary education (thousands):<pre>df_primary = df2[df2['Data']=='Students enrolled in primary education (thousands)']
df_secondary = df2[df2['Data']=='Students enrolled in secondary education (thousands)']
df_tertiary = df2[df2['Data']=='Students enrolled in tertiary education (thousands)']</pre></li></ul></li>
				<li>Compare them using bar charts of the primary students' enrollment of a low-income country and a high-income country:<pre>primary_enrollment_india = df_primary[df_primary['Region/Country/Area']=='India']
primary_enrollment_USA = df_primary[df_primary['Region/Country/Area']=='United States of America']</pre></li>
				<li>Print the <code>primary_enrollment_india</code> data:<pre>primary_enrollment_india</pre><p>The output is as follows:</p><div><img src="img/C11065_9_8.jpg" alt="Figure 9.8: Data for the enrollment in primary education in India" width="824" height="175"/></div><h6>Figure 9.8: Data for the enrollment in primary education in India</h6></li>
				<li>Print the <code>primary_enrollment_USA</code> data:<pre>primary_enrollment_USA</pre><p>The output is as follows:</p><div><img src="img/C11065_9_9.jpg" alt="Figure 9.9: Data for the enrollment in primary education in USA" width="831" height="143"/></div><h6>Figure 9.9: Data for the enrollment in primary education in USA</h6></li>
				<li>Plot the data for India:<pre>plt.figure(figsize=(8,4))
plt.bar(primary_enrollment_india['Year'],primary_enrollment_india['Enrollments (Thousands)'])
plt.title("Enrollment in primary education\nin India (in thousands)",fontsize=16)
plt.grid(True)
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)
plt.xlabel("Year", fontsize=15)
plt.show()</pre><p>The output is as follows:</p><div><img src="img/C11065_9_10.jpg" alt="Figure 9.10: Bar plot for the enrollment in primary education in India" width="652" height="307"/></div><h6>Figure 9.10: Bar plot for the enrollment in primary education in India</h6></li>
				<li>Plot the data for the USA:<pre>plt.figure(figsize=(8,4))
plt.bar(primary_enrollment_USA['Year'],primary_enrollment_USA['Enrollments (Thousands)'])
plt.title("Enrollment in primary education\nin the United States of America (in thousands)",fontsize=16)
plt.grid(True)
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)
plt.xlabel("Year", fontsize=15)
plt.show()</pre><p>The output is as follows:</p><div><img src="img/C11065_9_11.jpg" alt="Figure 9.11: Bar plot for the enrollment in primary education in the USA" width="660" height="307"/></div><h6>Figure 9.11: Bar plot for the enrollment in primary education in the USA</h6><p>Data imputation: Clearly, we are missing some data. Let's say we decide to impute these data points by simple linear interpolation between the available data points. We can take out a pen and paper or a calculator and compute those values and manually create a dataset somehow. But being a data wrangler, we will of course take advantage of Python programming, and use pandas imputation methods for this task. But to do that, we first need to create a DataFrame with missing values inserted – that is, we need to append another DataFrame with missing values to the current DataFrame.</p><p><strong class="bold">(For India) Append the rows corresponding to missing the years </strong>–<strong class="bold"> 2004 - 2009, 2011 – 2013.</strong></p></li>
				<li>Find the missing years:<pre>missing_years = [y for y in range(2004,2010)]+[y for y in range(2011,2014)]</pre></li>
				<li>Print the value in the  <code>missing_years variable:</code><pre>missing_years</pre><p>The output is as follows:</p><pre>[2004, 2005, 2006, 2007, 2008, 2009, 2011, 2012, 2013]</pre></li>
				<li>Create a dictionary of values with <code>np.nan</code>. Note that there are 9 missing data points, so we need to create a list with identical values repeated 9 times:<pre>dict_missing = {'Region/Country/Area':['India']*9,'Year':missing_years,
                'Data':'Students enrolled in primary education (thousands)'*9,
                'Enrollments (Thousands)':[np.nan]*9,'Footnotes':[np.nan]*9}</pre></li>
				<li>Create a DataFrame of missing values (from the preceding dictionary) that we can <code>append</code>:<pre>df_missing = pd.DataFrame(data=dict_missing)</pre></li>
				<li>Append the new DataFrames to previously existing ones:<pre>primary_enrollment_india=primary_enrollment_india.append(df_missing,ignore_index=True,sort=True)</pre></li>
				<li>Print the data in <code>primary_enrollment_india</code>:<pre>primary_enrollment_india</pre><p>The output is as follows:</p><div><img src="img/C11065_9_12.jpg" alt="Figure 9.12: Data for the enrollment in primary education in India after appending the data" width="811" height="422"/></div><h6>Figure 9.12: Data for the enrollment in primary education in India after appending the data</h6></li>
				<li>Sort by <code>year</code> and reset the indices using <code>reset_index</code>. Use <code>inplace=True</code> to execute the changes on the DataFrame itself:<pre>primary_enrollment_india.sort_values(by='Year',inplace=True)
primary_enrollment_india.reset_index(inplace=True,drop=True)</pre></li>
				<li>Print the data in <code>primary_enrollment_india</code>:<pre>primary_enrollment_india</pre><p>The output is as follows:</p><div><img src="img/C11065_9_13.jpg" alt="Figure 9.13: Data for the enrollment in primary education in India after sorting the data" width="863" height="415"/></div><h6>Figure 9.13: Data for the enrollment in primary education in India after sorting the data</h6></li>
				<li>Use the <code>interpolate</code> method for linear interpolation. It fills all the <code>NaN</code> by linearly interpolated values. Check out this link for more details about this method: <a href="http://pandas.pydata.org/pandas-docs/version/0.17/generated/pandas.DataFrame.interpolate.html">http://pandas.pydata.org/pandas-docs/version/0.17/generated/pandas.DataFrame.interpolate.html</a>:<pre>primary_enrollment_india.interpolate(inplace=True)</pre></li>
				<li>Print the data in <code>primary_enrollment_india</code>:<pre>primary_enrollment_india</pre><p>The output is as follows:</p><div><img src="img/C11065_9_14.jpg" alt="" width="796" height="419"/></div><h6>Figure 9.14: Data for the enrollment in primary education in India after interpolating the data</h6></li>
				<li>Plot the data:<pre>plt.figure(figsize=(8,4))
plt.bar(primary_enrollment_india['Year'],primary_enrollment_india['Enrollments (Thousands)'])
plt.title("Enrollment in primary education\nin India (in thousands)",fontsize=16)
plt.grid(True)
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)
plt.xlabel("Year", fontsize=15)
plt.show()</pre><p>The output is as follows:</p><div><img src="img/C11065_9_15.jpg" alt="Figure 9.15: Bar plot for the enrollment in primary education in India" width="662" height="307"/></div><h6>Figure 9.15: Bar plot for the enrollment in primary education in India</h6></li>
				<li>Repeat the same steps for the USA:<pre>missing_years = [2004]+[y for y in range(2006,2010)]+[y for y in range(2011,2014)]+[2016]</pre></li>
				<li>Print the value in <code>missing_years</code>.<pre>missing_years</pre><p>The output is as follows:</p><pre>[2004, 2006, 2007, 2008, 2009, 2011, 2012, 2013, 2016]</pre></li>
				<li>Create <code>dict_missing</code>, as follows:<pre>dict_missing = {'Region/Country/Area':['United States of America']*9,'Year':missing_years, 'Data':'Students enrolled in primary education (thousands)'*9, 'Value':[np.nan]*9,'Footnotes':[np.nan]*9}</pre></li>
				<li>Create the DataFrame fpr <code>df_missing</code>, as follows:<pre>df_missing = pd.DataFrame(data=dict_missing)</pre></li>
				<li>Append this to the <code>primary_enrollment_USA</code> variable, as follows:<pre>primary_enrollment_USA=primary_enrollment_USA.append(df_missing,ignore_index=True,sort=True)</pre></li>
				<li>Sort the values in the <code>primary_enrollment_USA</code> variable, as follows:<pre>primary_enrollment_USA.sort_values(by='Year',inplace=True)</pre></li>
				<li>Reset the index of the <code>primary_enrollment_USA</code> variable, as follows:<pre>primary_enrollment_USA.reset_index(inplace=True,drop=True)</pre></li>
				<li>Interpolate the <code>primary_enrollment_USA</code> variable, as follows:<pre>primary_enrollment_USA.interpolate(inplace=True)</pre></li>
				<li>Print the <code>primary_enrollment_USA</code> variable:<pre>primary_enrollment_USA</pre><p>The output is as follows:</p><div><img src="img/C11065_9_16.jpg" alt="Figure 9.16: Data for the enrollment in primary education in USA after all operations have been completed" width="873" height="392"/></div><h6>Figure 9.16: Data for the enrollment in primary education in USA after all operations have been completed</h6></li>
				<li>Still, the first value is unfilled. We can use the <code>limit</code> and <code>limit_direction</code> parameters with the interpolate method to fill that. How did we know this? By searching on Google and looking at this StackOverflow page. Always search for the solution to your problem and look for what has already been done and try to implement it:<pre>primary_enrollment_USA.interpolate(method='linear',limit_direction='backward',limit=1)</pre><p>The output is as follows:</p><div><img src="img/C11065_9_17.jpg" alt="Figure 9.17: Data for the enrollment in primary education in the USA after limiting the data" width="869" height="379"/></div><h6>Figure 9.17: Data for the enrollment in primary education in the USA after limiting the data</h6></li>
				<li>Print the data in primary_enrollment_USA:<pre>primary_enrollment_USA</pre><p>The output is as follows:</p><div><img src="img/C11065_9_18.jpg" alt="Figure 9.18: Data for the enrollment in primary education in USA" width="872" height="391"/></div><h6>Figure 9.18: Data for the enrollment in primary education in USA</h6></li>
				<li>Plot the data:<pre>plt.figure(figsize=(8,4))
plt.bar(primary_enrollment_USA['Year'],primary_enrollment_USA['Enrollments (Thousands)'])
plt.title("Enrollment in primary education\nin the United States of America (in thousands)",fontsize=16)
plt.grid(True)
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)
plt.xlabel("Year", fontsize=15)
plt.show()</pre><p>The output is as follows:</p></li>
			</ol>
			<div><div><img src="img/C11065_9_19.jpg" alt="Figure 9.19: Bar plot for the enrollment in primary education in the USA" width="767" height="307"/>
				</div>
			</div>
			<h6>Figure 9.19: Bar plot for the enrollment in primary education in the USA</h6>
			<h3 id="_idParaDest-294"><a id="_idTextAnchor349"/>Activity 13: Data Wrangling Task – Cleaning GDP Data</h3>
			<p>These are the steps to complete this activity:</p>
			<ol>
				<li value="1">GDP data for India: We will try to read the GDP data for India from a CSV file that was found in a World Bank portal. It is given to you and also hosted on the Packt GitHub repository. But the Pandas <code>read_csv</code> method will throw an error in we try to read it normally. Let's look at a step-by-step guide on how we can read useful information from it:<pre>df3=pd.read_csv("India_World_Bank_Info.csv")</pre><p>The output is as follows:</p><pre>---------------------------------------------------------------------------
ParserError                               Traceback (most recent call last)
&lt;ipython-input-45-9239cae67df7&gt; in &lt;module&gt;()
…..
ParserError: Error tokenizing data. C error: Expected 1 fields in line 6, saw 3</pre><p>We can try and use the <code>error_bad_lines=False</code> option in this kind of situation.</p></li>
				<li>Read the India World Bank Information <code>.csv</code> file:<pre>df3=pd.read_csv("India_World_Bank_Info.csv",error_bad_lines=False)
df3.head(10)</pre><p>The output is as follows:</p><div><img src="img/C11065_9_20.jpg" alt="Figure 9.20: DataFrame from the India World Bank Information" width="693" height="311"/></div><h6>Figure 9.20: DataFrame from the India World Bank Information</h6><h4>Note:</h4><p class="callout">At times, the output may not found because there are three rows instead of the expected one row.</p></li>
				<li>Clearly, the delimiter in this file is tab (<code>\t</code>):<pre>df3=pd.read_csv("India_World_Bank_Info.csv",error_bad_lines=False,delimiter='\t')
df3.head(10)</pre><p>The output is as follows:</p><div><img src="img/C11065_9_21.jpg" alt="Figure 9.21: DataFrame from the India World Bank Information after using a delimiter" width="1134" height="706"/></div><h6>Figure 9.21: DataFrame from the India World Bank Information after using a delimiter</h6></li>
				<li>Use the <code>skiprows</code> parameter to skip the first 4 rows:<pre>df3=pd.read_csv("India_World_Bank_Info.csv",error_bad_lines=False,delimiter='\t',skiprows=4)
df3.head(10)</pre><p>The output is as follows:</p><div><img src="img/C11065_9_22.jpg" alt="Figure 9.22: DataFrame from the India World Bank Information after using skiprows" width="930" height="702"/></div><h6>Figure 9.22: DataFrame from the India World Bank Information after using skiprows</h6></li>
				<li>Closely examine the dataset: In this file, the columns are the yearly data and rows are the various types of information. Upon examining the file with Excel, we find that the column <code>Indicator Name</code> is the one with the name of the particular data type. We filter the dataset with the information we are interested in and also transpose (the rows and columns are interchanged) it to make it a similar format as our previous education dataset:<pre>df4=df3[df3['Indicator Name']=='GDP per capita (current US$)'].T
df4.head(10)</pre><p>The output is as follows:</p><div><img src="img/C11065_9_23.jpg" alt="Figure 9.23: DataFrame focusing on GDP per capita" width="526" height="265"/></div><h6>Figure 9.23: DataFrame focusing on GDP per capita</h6></li>
				<li>There is no index, so let's use <code>reset_index</code> again:<pre>df4.reset_index(inplace=True)
df4.head(10)</pre><p>The output is as follows:</p><div><img src="img/C11065_9_24.jpg" alt="Figure 9.24: DataFrame from the India World Bank Information using reset_index" width="455" height="256"/></div><h6>Figure 9.24: DataFrame from the India World Bank Information using reset_index</h6></li>
				<li>The first 3 rows aren't useful. We can redefine the DataFrame without them. Then, we re-index again:<pre>df4.drop([0,1,2],inplace=True)
df4.reset_index(inplace=True,drop=True)
df4.head(10)</pre><p>The output is as follows:</p><div><img src="img/C11065_9_25.jpg" alt="Figure 9.25: DataFrame from the India World Bank Information after dropping and resetting the index" width="533" height="250"/></div><h6>Figure 9.25: DataFrame from the India World Bank Information after dropping and resetting the index</h6></li>
				<li>Let's rename the columns properly (this is necessary for merging, which we will look at shortly):<pre>df4.columns=['Year','GDP']
df4.head(10)</pre><p>The output is as follows:</p><div><img src="img/C11065_9_26.jpg" alt="Figure 9.26: DataFrame focusing on Year and GDP" width="524" height="255"/></div><h6>Figure 9.26: DataFrame focusing on Year and GDP</h6></li>
				<li>It looks like that we have GDP data from 1960 onward. But we are interested in 2003 - 2016. Let's examine the last 20 rows:<pre>df4.tail(20)</pre><p>The output is as follows:</p><div><img src="img/C11065_9_27.jpg" alt="Figure 9.27: DataFrame from the India World Bank Information" width="836" height="470"/></div><h6>Figure 9.27: DataFrame from the India World Bank Information</h6></li>
				<li>So, we should be good with rows 43-56. Let's create a DataFrame called <code>df_gdp:</code><pre>df_gdp=df4.iloc[[i for i in range(43,57)]]
df_gdp</pre><p>The output is as follows:</p><div><img src="img/C11065_9_28.jpg" alt="Figure 9.28: DataFrame from the India World Bank Information" width="649" height="333"/></div><h6>Figure 9.28: DataFrame from the India World Bank Information</h6></li>
				<li>We need to reset the index again (for merging):<pre>df_gdp.reset_index(inplace=True,drop=True)
df_gdp</pre><p>The output is as follows:</p><div><img src="img/C11065_9_29.jpg" alt="Figure 9.29: DataFrame from the India World Bank Information" width="612" height="344"/></div><h6>Figure 9.29: DataFrame from the India World Bank Information</h6></li>
				<li>The year in this DataFrame is not of the <code>int</code> type. So, it will have problems merging with the education DataFrame:<pre>df_gdp['Year']</pre><p>The output is as follows:</p><div><img src="img/C11065_9_30.jpg" alt="" width="526" height="248"/></div><h6>Figure 9.30: DataFrame focusing on year</h6></li>
				<li>Use the <code>apply</code> method with Python's built-in <code>int</code> function. Ignore any warnings that are thrown:<pre>df_gdp['Year']=df_gdp['Year'].apply(int)</pre></li>
			</ol>
			<h3 id="_idParaDest-295"><a id="_idTextAnchor350"/>Solution of Activity 14: Data Wrangling Task – Merging UN Data and GDP Data</h3>
			<p>These are the steps to complete this activity:</p>
			<ol>
				<li value="1">Now, merge the two DataFrames, that is, <code>primary_enrollment_india</code> and <code>df_gdp</code>, on the <code>Year</code> column:<pre>primary_enrollment_with_gdp=primary_enrollment_india.merge(df_gdp,on='Year')
primary_enrollment_with_gdp</pre><p>The output is as follows:</p><div><img src="img/C11065_9_31.jpg" alt="" width="701" height="359"/></div><h6>Figure 9.31: Merged data</h6></li>
				<li>Now, we can drop the <code>Data</code>, <code>Footnotes</code>, and <code>Region/Country/Area</code> columns:<pre>primary_enrollment_with_gdp.drop(['Data','Footnotes','Region/Country/Area'],axis=1,inplace=True)
primary_enrollment_with_gdp</pre><p>The output is as follows:</p><div><img src="img/C11065_9_32.jpg" alt="Figure 9.32: Merged data after dropping the Data, Footnotes, and Region/Country/Area columns" width="628" height="353"/></div><h6>Figure 9.32: Merged data after dropping the Data, Footnotes, and Region/Country/Area columns</h6></li>
				<li>Rearrange the columns for proper viewing and presentation to a data scientist:<pre>primary_enrollment_with_gdp = primary_enrollment_with_gdp[['Year','Enrollments (Thousands)','GDP']]
primary_enrollment_with_gdp</pre><p>The output is as follows:</p><div><img src="img/C11065_9_33.jpg" alt="Figure 9.33: Merged data after rearranging the columns" width="592" height="333"/></div><h6>Figure 9.33: Merged data after rearranging the columns</h6></li>
				<li>Plot the data:<pre>plt.figure(figsize=(8,5))
plt.title("India's GDP per capita vs primary education enrollment",fontsize=16)
plt.scatter(primary_enrollment_with_gdp['GDP'],
            primary_enrollment_with_gdp['Enrollments (Thousands)'],
           edgecolor='k',color='orange',s=200)
plt.xlabel("GDP per capita (US $)",fontsize=15)
plt.ylabel("Primary enrollment (thousands)",fontsize=15)
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)
plt.grid(True)
plt.show()</pre><p>The output is as follows:</p></li>
			</ol>
			<div><div><img src="img/C11065_9_34.jpg" alt="Figure 9.34: Scatter plot of merged data" width="717" height="344"/>
				</div>
			</div>
			<h6>Figure 9.34: Scatter plot of merged data</h6>
			<h3 id="_idParaDest-296"><a id="_idTextAnchor351"/>Activity 15: Data Wrangling Task – Connecting the New Data to a Database</h3>
			<p>These are the steps to complete this activity:</p>
			<ol>
				<li value="1">Connect to a database and writing values it. We start by importing the <code>sqlite3</code> module of Python and then use the connect function to connect to a database. Designate <code>Year</code> as the <code>PRIMARY</code> <code>KEY</code> of this table:<pre>import sqlite3
with sqlite3.connect("Education_GDP.db") as conn:
    cursor = conn.cursor()
    cursor.execute("CREATE TABLE IF NOT EXISTS \
                   education_gdp(Year INT, Enrollment FLOAT, GDP FLOAT, PRIMARY KEY (Year))")</pre></li>
				<li>Run a loop with the dataset rows one by one to insert them in the table:<pre>with sqlite3.connect("Education_GDP.db") as conn:
    cursor = conn.cursor()
    for i in range(14):
        year = int(primary_enrollment_with_gdp.iloc[i]['Year'])
        enrollment = primary_enrollment_with_gdp.iloc[i]['Enrollments (Thousands)']
        gdp = primary_enrollment_with_gdp.iloc[i]['GDP']
        #print(year,enrollment,gdp)
        cursor.execute("INSERT INTO education_gdp (Year,Enrollment,GDP) VALUES(?,?,?)",(year,enrollment,gdp))</pre><p>If we look at the current folder, we should see a file called <code>Education_GDP.db</code>, and if we can examine that using a database viewer program, we can see the data transferred there.</p></li>
			</ol>
			<p>In these activities, we have examined a complete data wrangling flow, including reading data from the web and a local drive, filtering, cleaning, quick visualization, imputation, indexing, merging, and writing back to a database table. We also wrote custom functions to transform some of the data and saw how to handle situations where we may get errors upon reading the file.</p>
		</div>
	</div>



  </body></html>