- en: Chapter 3. SciPy for Linear Algebra
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will continue exploring the different SciPy modules through
    meaningful examples. We will start with the treatment of matrices (whether normal
    or sparse) with the modules on Linear Algebra—`linalg` and `sparse`. Note that
    `linalg` expands on the NumPy module with the same name.
  prefs: []
  type: TYPE_NORMAL
- en: This discipline of mathematics studies vector spaces and linear mappings between
    them. Matrices represent objects in this field in such a way that any property
    of the underlying objects may be obtained by performing adequate operations on
    the representing matrices. In this chapter, we assume that you are familiar with
    at least the basics of linear algebra, in particular with the notion of matrix
    multiplication, finding the determinant and inverse of a matrix, as well as their
    immediate applications in **vector calculus**.
  prefs: []
  type: TYPE_NORMAL
- en: Accordingly, in this chapter, we will explore how vectors and matrices are handled
    in Numpy/SciPy, how to create them, how to program standard mathematical operations
    between them, and how to represent this on a functional form. Next, we will solve
    linear system of equations expressed in the matrix form involving dense or sparse
    matrices. The corresponding IPython Notebook will help you test the functionality
    of the modules involved and modify each illustrative example according to your
    specific needs.
  prefs: []
  type: TYPE_NORMAL
- en: Vector creation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As mentioned in [Chapter 2](ch02.html "Chapter 2. Working with the NumPy Array
    As a First Step to SciPy"), *Working with the NumPy Array As a First Step to SciPy*,
    SciPy depends on NumPy''s main object''s `ndarray` data structure. You can look
    at one-dimensional arrays as vectors and vice versa (oriented points in an n-dimensional
    space). Consequently, a vector can be created via Numpy as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also use already defined arrays to create a new candidate. Some examples
    were presented in the previous chapter. Here we can reverse the already created
    vector and assign it to a new one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that in this example, we have to make a copy of the reverse of the elements
    of `vectorA` and assign it to `vectorB`. This way, by changing elements of `vectorB`,
    the elements of `vectorA` remain unchanged, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at `vectorA`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s make a copy of `vectorA` by reversing its elements and assigning it
    to `vectorB`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In the last code statement, we repeated the previous assignment to `vectorB`,
    bringing it back to its initial values taking the reverse of `vectorA`, once again.
  prefs: []
  type: TYPE_NORMAL
- en: Vector operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In addition to being mathematical entities studied in linear algebra, Vectors
    are widely used in physics and engineering as a convenient way to represent physical
    quantities as **displacement**, **velocity**, **acceleration**, force, and so
    on. Accordingly, basic operations between vectors can be performed via Numpy/SciPy
    operations as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Addition/subtraction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Addition/subtraction of vectors does not require any explicit loop to perform
    them. Let''s take a look at addition of two vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Further, we perform subtraction on two vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Scalar/Dot product
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Numpy has the built-in function dot to compute the scalar (`dot`) product between
    two vectors. We show you its use computing the `dot` product of `vectorA` and
    `vectorB` from the previous code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, to compute this product we could perform the element-wise product
    between the components of the vectors and then add the respective results. This
    is implemented in the following lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Cross/Vector product – on three-dimensional space vectors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, two vectors in 3 dimensions are created before applying the built-in
    function from NumPy to compute the cross product between the vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Further, we perform a `cross` operation of `vectorB` over `vectorA`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the last expression shows the expected result that `vectorA` cross
    `vectorB` is the negative of `vectorB` cross `vectorA`.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a matrix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In SciPy, a matrix structure is given to any one- or two-dimensional `ndarray`,
    with either the `matrix` or `mat` command. The complete syntax is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Creating matrices, the data may be given as `ndarray`, a string or a Python
    list (as the second example below), which is very convenient. When using strings,
    the semicolon denotes change of row and the comma, change of column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown a follows s:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at another example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Another technique to create a matrix from a two-dimensional array is to enforce
    the matrix structure on a new object, copying the data of the former with the
    `asmatrix` routine.
  prefs: []
  type: TYPE_NORMAL
- en: A matrix is said to be sparse ([http://en.wikipedia.org/wiki/Sparse_matrix](http://en.wikipedia.org/wiki/Sparse_matrix))
    if most of its entries are zeros. It is a waste of memory to input such matrices
    in the usual way, especially if the dimensions are large. SciPy provides different
    procedures to store such matrices effectively in memory. Most of the usual methods
    to input sparse matrices are contemplated in SciPy as routines in the `scipy.sparse`
    module. Some of those methods are **block sparse row** (`bsr_matrix`), **coordinate
    format** (`coo_matrix`), compressed sparse column or row (`csc_matrix`, `csr_matrix`),
    sparse matrix with diagonal storage (`dia_matrix`), dictionary with **Keys-based
    sorting** (`dok_matrix`), and **Row-based linked list** (`lil_matrix`).
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we would like to present one of these: the coordinate format.
    In this format, and given a sparse matrix `A`, we identify the coordinates of
    the nonzero elements, say *n* of them, and we create two n-dimensional `ndarray`
    arrays containing the columns and the rows of those entries, and a third `ndarray`
    containing the values of the corresponding entries. For instance, notice the following
    sparse matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating a matrix](img/7702OS_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The standard form of creating such matrices is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: A more memory-efficient way to create these matrices would be to properly store
    the nonzero elements. In this case, one of the nonzero entries is at the 1^(st)
    row and 2^(nd) column (or location `(0, 1)` in Python) with value, `10`. Another
    nonzero entry is at `(1, 2)` with value, `20`. A 3^(rd) nonzero entry, with the
    value `30`, is located at `(2, 3)`. The last nonzero entry of `A` is located at
    `(3, 4)`, and has the value, `40`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then have `ndarray` of rows, `ndarray` of columns, and another `ndarray`
    of values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We create the matrix `A` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Notice how the `todense` method turns sparse matrices into full matrices. Also
    note that it obviates any row or column of full zeros following the last nonzero
    element.
  prefs: []
  type: TYPE_NORMAL
- en: 'Associated to each input method, we have functions that identify sparse matrices
    of each kind. For instance, if we suspect that `A` is a sparse matrix in the `coo_matrix`
    format, we may use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'All the array routines are cast to matrices, provided the input is a matrix.
    This is very convenient for matrix creation, especially thanks to stacking commands
    (`hstack`, `vstack`, `tile`). Besides these, matrices enjoy one more amazing stacking
    command, `bmat`. This routine allows the stacking of matrices by means of strings,
    making use of the convention: semicolon for change of row and comma for change
    of column. Also, it allows matrix names inside of the string to be evaluated.
    The following example is enlightening:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The main difference between arrays and matrices is in regards to the behavior
    of the product of two objects of the same type. For example, multiplication between
    two arrays means *element-wise multiplication of the entries of the two arrays*
    and requires two objects of the same shape. The following code snippet is an example
    of multiplication between two arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'On the other hand, matrix multiplication requires a first matrix with shape
    (*m*, *n*), and a second matrix with shape (*n*, *p*)—the number of columns in
    the first matrix must be the same as the number of rows in the second matrix.
    This operation offers a new matrix of shape (*m*, *p*), as shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Creating a matrix](img/7702OS_03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following is the code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, to obtain the matrix product between two conforming matrices
    as `ndarray` objects, we don''t really need to transform the `ndarray` object
    to a matrix object if not needed. The matrix product could be obtained directly
    via the `numpy.dot` function introduced earlier in the *Scalar/Dot product* section
    of this chapter. Let''s take a look at the following `numpy.dot` command example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'If we desire to perform an element-wise multiplication of the elements of two
    matrices, we can do so with the versatile `numpy.multiply` command, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The other difference between arrays and matrices worth noticing is in regard
    to their shapes. While we allow arrays to have one dimension; their corresponding
    matrices must have at least two. This is very important to have in mind when we
    transpose either object. Let''s take a look at the following code snippet implementing
    `shape()` and `transpose()` commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: As it has been shown, SciPy offers quite a number of basic tools to instantiate
    and manipulate matrices, with many related methods to follow. This also allows
    us to speed up computations in the cases where special matrices are used.
  prefs: []
  type: TYPE_NORMAL
- en: The `scipy.linalg` module provides commands to create special matrices such
    as block diagonal matrices from provided arrays (`block_diag`), **circulant matrices**
    (circulant), companion matrices (`companion`), **Hadamard matrices** (`hadamard`),
    **Hankel matrices** (`hankel`), **Hilbert** and **inverse Hilbert matrices** (`hilbert`,
    `invhilbert`), **Leslie matrices** (`leslie`), **square Pascal matrices** (`pascal`),
    **Toeplitz matrices** (`toeplitz`), **lower-triangular matrices** (`tril`), and
    **upper-triangular matrices** (`triu`).
  prefs: []
  type: TYPE_NORMAL
- en: Let's see an example on **optimal weighings**.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we are given *p* objects to be weighed in *n* weighings with a two-pan
    balance. We create an *n* x *p* matrix of plus and minus one, where a positive
    value in the *(i, j)* position indicates that the *j^(th)* object is placed in
    the left pan of the balance in the *i^(th)* weighing and a negative value that
    the *j^(th)* object corresponding is in the right pan.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is known that optimal weighings are designed by submatrices of Hadamard
    matrices. For the problem of designing an optimal weighing for eight objects with
    three weighings, we could then explore different choices of three rows of a Hadamard
    matrix of order eight. The only requirement is that the sum of the elements on
    the row of the matrix is zero (so that the same number of objects are placed on
    each pan). Through slicing, we can accomplish just that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: The `scipy.sparse` module has its own set of special matrices. The most common
    are matrices of those along diagonals (`eye`), identity matrices (`identity`),
    matrices from diagonals (`diags`, `spdiags`), block diagonal matrices from sparse
    matrices (`block_diag`), matrices from sparse sub-blocks (`bmat`), column-wise
    and row-wise stacks (`hstack`, `vstack`), and random matrices of a given shape
    and density with uniformly distributed values (`rand`).
  prefs: []
  type: TYPE_NORMAL
- en: Matrix methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Besides inheriting all the array methods, matrices enjoy four extra attributes:
    `T` for transpose, `H` for conjugate transpose, `I` for inverse, and `A` to cast
    as `ndarray`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Operations between matrices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have briefly covered the most basic operation between two matrices; the
    matrix product. For any other kind of product, we resort to the basic utilities
    in the NumPy libraries, as: dot product for arrays or vectors (`dot`, `vdot`),
    inner and outer products of two arrays (`inner`, `outer`), **tensor dot product**
    along specified axes (`tensordot`), or the **Kronecker product** of two arrays
    (`kron`).'
  prefs: []
  type: TYPE_NORMAL
- en: Let's see an example of creating an **orthonormal** basis.
  prefs: []
  type: TYPE_NORMAL
- en: Create an orthonormal basis in the nine-dimensional real space from an orthonormal
    basis of the three-dimensional real space.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s choose, for example, the orthonormal basis formed by the vectors as
    shown in following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Operations between matrices](img/7702OS_03_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We compute the desired basis by collecting these vectors in a matrix and using
    a Kronecker product, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'The columns of matrix `B` shown previously, give us an orthonormal basis directly.
    For instance, the vectors with odd indices would be the columns of the following
    submatrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Functions on matrices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `scipy.linalg` module offers a useful set of functions on matrices. The
    basic two commands on square matrices are `inv` (for the inverse of a matrix)
    and `det` (for the determinant). The power of a square matrix is given by the
    standard exponentiation; that is, if `A` is a square matrix, then `A**2` indicates
    the matrix product `A*A`, which is shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'It should be pointed out that as a type array, the product of `A*A` (or `A**2`)
    is calculated by squaring each element of the array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: More advanced commands compute matrix functions that rely on the power series
    representation of expressions involving matrix powers, such as the matrix **exponential**
    (for which there are three possibilities—`expm`, `expm2`, and `expm3`), the matrix
    **logarithm** (`logm`), matrix **trigonometric functions** (`cosm`, `sinm`, `tanm`),
    matrix **hyperbolic trigonometric functions** (`coshm`, `sinhm`, `tanhm`), the
    **matrix sign function** (`signm`), or the matrix **square root** (`sqrtm`).
  prefs: []
  type: TYPE_NORMAL
- en: Notice the difference between the application of the normal exponential function
    on a matrix, and the result of a matrix exponential function.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the former case, we obtain the application of `numpy.exp` to each entry
    of the matrix; in the latter, we actually compute the exponential of the matrix
    following the power series representation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Functions on matrices](img/7702OS_03_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding formula is illustrated in this code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s perform the `exp()` operation on `A`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s perform the `expm()` operation on `A`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: For sparse square matrices, we have an optimized inverse function, as well as
    a matrix exponential—`scipy.sparse.linalg.inv`, `scipy.sparse.linalg.expm`.
  prefs: []
  type: TYPE_NORMAL
- en: For general matrices, we have the basic norm function (norm), as well as two
    versions of the **Moore-Penrose pseudoinverse** (`pinv` and `pinv2`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once again, we need to emphasize how important it is to rely on these functions,
    rather than coding their equivalent expressions manually. For instance, note the
    `norm` computation of vectors or matrices, `scipy.linalg.norm`. Let''s show you,
    by example, the 2-norm of a two-dimensional vector `v=numpy.matrix([x,y])`, where
    at least one of the `x` and `y` values is extremely large—large enough so that
    `x*x` overflows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s perform the `sqrt()` operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is an error which is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Eigenvalue problems and matrix decompositions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another set of operations heavily used on matrices is to compute and handle
    eigenvalues and eigenvectors of square matrices. These two problems rank among
    the most complex operations that we can perform on square matrices, and extensive
    research has been put in place to obtain good algorithms with low complexity and
    optimal usage of memory resources. SciPy has state-of-the-art code to implement
    these ideas.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the computation of eigenvalues, the `scipy.linalg` module provides three
    routines: `eigvals` (for any ordinary or general eigenvalue problem), `eigvalsh`
    (if the matrix is symmetric of complex **Hermitian**), and `eigvals_banded` (if
    the matrix is banded). To compute the eigenvectors, we similarly have three corresponding
    choices: `eig`, `eigh`, and `eigh_banded`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The syntax used in all cases is very similar. For example, for the general
    case of eigenvalues, we use the following line of code where matrix `A` must be
    square:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: This should be the only parameter passed to the routine if we wish to solve
    an ordinary eigenvalue problem. If we wish to generalize this, we may provide
    an extra square matrix (of the same dimensions as matrix `A`). This is passed
    in the `B` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The module also offers an extensive collection of functions that compute different
    decompositions of matrices, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pivoted LU decomposition**: This function allows us to use the `lu` and `lufactor`
    commands.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Singular value decomposition**: This function allows us to use the `svd`
    command. To compute the singular values, we issue `svdvals`. If we wish to compose
    the sigma matrix in the singular value decomposition from its singular values,
    we do so with the `diagsvd` routine. If we wish to compute an orthogonal basis
    for the range of a matrix using SVD, we can accomplish this with the `orth` command.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cholesky decomposition**: This function allows us to use the `cholesky`,
    `cholesky_banded`, and `cho_factor` commands.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**QR and QZ decompositions**: This function allows us to use the `qr` and `qz`
    commands. If we wish to multiply a matrix with the matrix Q of a decomposition,
    we use the syntactic sugar `qr_multiply`, rather than performing this procedure
    in two steps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Schur and Hessenberg decompositions**: This function allows us to use the
    `schur` and `Hessenberg` commands. If we wish to convert a real Schur form to
    complex, we have the `rsf2csf` routine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At this point, we have an interesting application—image compression, which makes
    use of some of the routines explained so far.
  prefs: []
  type: TYPE_NORMAL
- en: Image compression via the singular value decomposition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is a very simple application where a square image `A` of size *n* x *n*,
    and stored as `ndarray` is regarded as a matrix, and where a singular value decomposition
    (SVD) is performed on it. This operation is visible in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image compression via the singular value decomposition](img/7702OS_03_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'From all the singular values of `s` we choose a fraction, together with their
    corresponding left and right singular vectors `u`, `v`. We compute a new matrix
    by collecting them according to the formula given in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image compression via the singular value decomposition](img/7702OS_03_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Note, for example, the similarity between the original (512 singular values)
    and an approximation using only 32 singular values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following images, of which the picture to the left is the
    original image and the picture to the right, the approximation using 32 singular
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image compression via the singular value decomposition](img/7702OS_03_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Using the `svd` approximation we managed to compress the original image of 262,144
    coefficients (512 * 512)to only 32,800 coefficients ((2 * 32 * 512) + 32), or
    to one-eighth of the original information.
  prefs: []
  type: TYPE_NORMAL
- en: Solvers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the fundamental applications of linear algebra is to solve large systems
    of linear equations. For the basic systems of the form *Ax=b*, for any square
    matrix `A` and general matrix `b` (with as many rows as columns in `A`), we have
    two generic methods to find *x* (`solve` for dense matrices and `spsolve` for
    sparse matrices), using the following syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'There are solvers that are even more sophisticated in SciPy, with enhanced
    performance for situations in which the structure of the matrix `A` is known.
    For dense matrices we have three commands in the `scipy.linalg` module: `solve_banded`
    (for banded matrices), `solveh_banded` (if besides banded, `A` is Hermitian),
    and `solve_triangular` (for triangular matrices).'
  prefs: []
  type: TYPE_NORMAL
- en: 'When a solution is not possible (for example, if `A` is a singular matrix),
    it is still possible to obtain a matrix *x* that minimizes the `norm` of *b-Ax*
    in a least-squares sense. We can compute such a matrix with the `lstsq` command,
    which has the following syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of this function is a tuple that contains the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The solution found (as `ndarray`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The sum of residues (as another `ndarray`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The effective rank of the matrix `A`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The singular values of the matrix `A` (as another `ndarray`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s illustrate this routine with a simple example, to solve the following
    system:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Solvers](img/7702OS_03_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following is the code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s move further into the code and perform the following operations on `b`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Further, let''s perform the `lstsq` operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: The `overwrite_` options are designed to enhance performance of the algorithms,
    and should be used carefully, since they destroy the original data.
  prefs: []
  type: TYPE_NORMAL
- en: The truly fastest solvers in SciPy are based upon decomposition of matrices.
    Reducing the system into something simpler easily solves huge and really complicated
    systems of linear equations. We may accomplish this using decomposition techniques
    presented in the *Eigenvalue problems and matrix decompositions* and *Image compression
    via the singular value decomposition* subsections under the *Matrix methods* section
    of this chapter, but of course, the SciPy philosophy is to help us deal with all
    nuisances of memory and resources internally. To this end, the module also has
    the `lu_solve` (for solutions based on LU decompositions), and `cho_solve`, `cho_solve_banded`
    (for solutions based on Cholesky decompositions).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you will also find solvers for very complex matrix equations—the **Sylvester**
    equation (`solve_sylvester`), both the continuous and discrete algebraic **Riccati**
    equations (`solve_continuous_are`, `solve_discrete_are`) and both the continuous
    and discrete **Lyapunov** equations (`solve_discrete_lyapunov`, `solve_lyapunov`).
  prefs: []
  type: TYPE_NORMAL
- en: Most of the matrix decompositions and solutions to eigenvalue problems are contemplated
    for sparse matrices in the `scipy.sparse.linalg` module with a similar naming
    convention, but with much more robust use of computer resources and error control.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter explored the treatment of vectors, matrices (whether normal or
    sparse) with the modules on linear algebra—`linalg` and `sparse.linalg`, which
    expand and improve the NumPy module with the same name.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 4](ch04.html "Chapter 4. SciPy for Numerical Analysis"), *SciPy
    for Numerical Analysis*, we will continue discussing details of the options available
    in SciPy to perform numerical computations efficiently, will cover how to evaluate
    special functions found in applied mathematics and mathematical physics problems.
    This will be discussed in details of doing regression, interpolation and optimization
    via SciPy.
  prefs: []
  type: TYPE_NORMAL
