<html><head></head><body>
  <div id="sbo-rt-content"><div class="chapter" title="Chapter 10. Structured Streaming"><div class="titlepage"><div><div><h1 class="title"><a id="ch10"/>Chapter 10. Structured Streaming</h1></div></div></div><p>This chapter will provide a jump-start on the concepts behind Spark Streaming and how this has evolved into Structured Streaming. An important aspect of Structured Streaming is that it utilizes Spark DataFrames. This shift in paradigm will make it easier for Python developers to start working with Spark Streaming.</p><p>In this chapter, your will learn:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">What is Spark Streaming?</li><li class="listitem" style="list-style-type: disc">Why do we need Spark Streaming?</li><li class="listitem" style="list-style-type: disc">What is the Spark Streaming application data flow?</li><li class="listitem" style="list-style-type: disc">Simple streaming application using DStream</li><li class="listitem" style="list-style-type: disc">A quick primer on Spark Streaming global aggregations</li><li class="listitem" style="list-style-type: disc">Introducing Structured Streaming</li></ul></div><p>Note, for the initial sections of this chapter, the example code used will be in Scala, as this was how most Spark Streaming code was written. When we start focusing on Structured Streaming, we will work with Python examples.</p><div class="section" title="What is Spark Streaming?"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec66"/>What is Spark Streaming?</h1></div></div></div><p>At its core, Spark<a id="id648" class="indexterm"/> Streaming is a scalable, fault-tolerant streaming system that takes the RDD batch paradigm (that is, processing data in batches) and speeds it up. While it is a slight over-simplification, basically Spark Streaming operates in mini-batches or batch intervals (from 500ms to larger interval windows). </p><p>As noted in the following diagram, Spark Streaming receives an input data stream and internally breaks that data stream into multiple smaller batches (the size of which is based on the <span class="emphasis"><em>batch interval</em></span>). The Spark engine processes those batches of input data to a result set of batches of processed data.</p><div class="mediaobject"><img src="images/B05793_10_01.jpg" alt="What is Spark Streaming?"/><div class="caption"><p>Source: Apache Spark Streaming Programming Guide at: <a class="ulink" href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">http://spark.apache.org/docs/latest/streaming-programming-guide.html</a>
</p></div></div><p>The key abstraction for<a id="id649" class="indexterm"/> Spark Streaming is Discretized Stream (DStream), which represents the previously mentioned small batches that make up the stream of data. DStreams are built on RDDs, allowing Spark developers to work within the same context of RDDs and batches, only now applying it to their streaming problems. Also, an important aspect is that, because you are using Apache Spark, Spark Streaming integrates with MLlib, SQL, DataFrames, and GraphX.</p><p>The following figure denotes the basic components of Spark Streaming:</p><div class="mediaobject"><img src="images/B05793_10_02.jpg" alt="What is Spark Streaming?"/><div class="caption"><p>Source: Apache Spark Streaming Programming Guide at: <a class="ulink" href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">http://spark.apache.org/docs/latest/streaming-programming-guide.html</a>
</p></div></div><p>Spark Streaming is a high-level API that provides fault-tolerant <span class="emphasis"><em>exactly-once</em></span> semantics for stateful operations. Spark Streaming has built in <span class="emphasis"><em>receivers</em></span> that can take on many sources, with the most common being Apache Kafka, Flume, HDFS/S3, Kinesis, and Twitter. For example, the most commonly used integration between Kafka and Spark Streaming is<a id="id650" class="indexterm"/> well documented in the Spark Streaming + Kafka Integration Guide found at: <a class="ulink" href="https://spark.apache.org/docs/latest/streaming-kafka-integration.html">https://spark.apache.org/docs/latest/streaming-kafka-integration.html</a>.</p><p>Also, you can create your own <span class="emphasis"><em>custom receiver</em></span>, such as the Meetup Receiver (<a class="ulink" href="https://github.com/actions/meetup-stream/blob/master/src/main/scala/receiver/MeetupReceiver.scala">https://github.com/actions/meetup-stream/blob/master/src/main/scala/receiver/MeetupReceiver.scala</a>), which allows you<a id="id651" class="indexterm"/> to read the Meetup Streaming API (<a class="ulink" href="https://www.meetup.com/meetup_api/docs/stream/2/rsvps/)">https://www.meetup.com/meetup_api/docs/stream/2/rsvps/)</a> using Spark Streaming.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note62"/>Note</h3><p>
<span class="strong"><strong>Watch the Meetup Receiver in Action</strong></span>
</p><p>If you are interested in seeking the Spark Streaming Meetup Receiver in action, you can refer to the Databricks notebooks at: <a class="ulink" href="https://github.com/dennyglee/databricks/tree/master/notebooks/Users/denny%40databricks.com/content/Streaming%20Meetup%20RSVPs">https://github.com/dennyglee/databricks/tree/master/notebooks/Users/denny%40databricks.com/content/Streaming%20Meetup%20RSVPs</a> which utilize the previously mentioned Meetup Receiver.</p><p>The following is a screenshot of the notebook in action left window, while viewing the Spark UI (Streaming Tab) on the right.</p><div class="mediaobject"><img src="images/B05793_10_03a.jpg" alt="What is Spark Streaming?"/></div><div class="mediaobject"><img src="images/B05793_10_03b.jpg" alt="What is Spark Streaming?"/></div><p>You will<a id="id652" class="indexterm"/> be able to use Spark Streaming to receive Meetup RSVPs from around the country (or world) and get a near real-time summary of Meetup RSVPs by state (or country). Note, these notebooks are currently written in <code class="literal">Scala</code>.</p></div></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Why do we need Spark Streaming?"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec67"/>Why do we need Spark Streaming?</h1></div></div></div><p>As noted by Tathagata Das – committer and member of the project management committee (PMC) to the Apache Spark project and lead developer of Spark Streaming – in the Datanami article <span class="emphasis"><em>Spark Streaming: What is It and Who's Using it</em></span> (<a class="ulink" href="https://www.datanami.com/2015/11/30/spark-streaming-what-is-it-and-whos-using-it/">https://www.datanami.com/2015/11/30/spark-streaming-what-is-it-and-whos-using-it/</a>), there is a <span class="emphasis"><em>business need</em></span> for streaming. With the prevalence of online transactions and social media, as well as sensors and devices, companies are generating and processing<a id="id653" class="indexterm"/> more data at a faster rate.</p><p>The ability to develop actionable<a id="id654" class="indexterm"/> insight at scale and in real time provides those businesses with a competitive advantage. Whether you are detecting fraudulent transactions, providing real-time detection of sensor anomalies, or reacting to the next viral tweet, streaming analytics is becoming increasingly important in data scientists' and data engineer's toolbox.</p><p>The reason Spark Streaming is itself being rapidly adopted is because Apache Spark unifies all of these disparate data processing paradigms (Machine Learning via ML and MLlib, Spark SQL, and Streaming) within the same framework. So, you can go from training machine learning models (ML or MLlib), to scoring data with these models (Streaming) and perform<a id="id655" class="indexterm"/> analysis using your favourite BI tool (SQL) – all within the same framework. Companies including Uber, Netflix, and Pinterest often showcase their Spark Streaming use cases:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>How Uber Uses Spark and Hadoop to Optimize Customer Experience</em></span>: <a class="ulink" href="https://www.datanami.com/2015/10/05/how-uber-uses-spark-and-hadoop-to-optimize-customer-experience/">https://www.datanami.com/2015/10/05/how-uber-uses-spark-and-hadoop-to-optimize-customer-experience/</a></li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>Spark and Spark Streaming at Netflix</em></span>: <a class="ulink" href="https://spark-summit.org/2015/events/spark-and-spark-streaming-at-netflix/">https://spark-summit.org/2015/events/spark-and-spark-streaming-at-netflix/</a></li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>Can Spark Streaming survive Chaos Monkey?</em></span> <a class="ulink" href="http://techblog.netflix.com/2015/03/can-spark-streaming-survive-chaos-monkey.html">http://techblog.netflix.com/2015/03/can-spark-streaming-survive-chaos-monkey.html</a></li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>Real-time analytics at Pinterest</em></span>: <a class="ulink" href="https://engineering.pinterest.com/blog/real-time-analytics-pinterest">https://engineering.pinterest.com/blog/real-time-analytics-pinterest</a></li></ul></div><p>Currently, there are<a id="id656" class="indexterm"/> four broad use cases surrounding Spark Streaming:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Streaming ETL</strong></span>: Data is continuously being cleansed and aggregated prior to being pushed<a id="id657" class="indexterm"/> downstream. This is commonly done to reduce the amount of data to be stored in the final data store.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Triggers</strong></span>: Real-time detection of behavioral or anomaly events trigger immediate and<a id="id658" class="indexterm"/> downstream actions. For example, a device that is within the proximity of a detector or beacon will trigger an alert.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Data enrichment</strong></span>: Real-time data joined to other datasets allowing for richer analysis. For example, including real-time weather information with flight information to<a id="id659" class="indexterm"/> build better travel alerts.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Complex sessions and continuous learning</strong></span>: Multiple sets of events associated with<a id="id660" class="indexterm"/> real-time streams<a id="id661" class="indexterm"/> are continuously analyzed and/or updating machine learning models. For example, the stream of user activity associated with an online game that allows us to better segment the user.</li></ul></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="What is the Spark Streaming application data flow?"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec68"/>What is the Spark Streaming application data flow?</h1></div></div></div><p>The following<a id="id662" class="indexterm"/> figure provides the data flow between the Spark driver, workers, streaming sources and targets:</p><div class="mediaobject"><img src="images/B05793_10_04.jpg" alt="What is the Spark Streaming application data flow?"/></div><p>It all starts with the Spark Streaming Context, represented by <code class="literal">ssc.start()</code> in the preceding figure:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">When the Spark Streaming Context starts, the driver will execute a long-running task on the executors (that is, the Spark workers).</li><li class="listitem">The <span class="strong"><strong>Receiver</strong></span> on the executors (<span class="strong"><strong>Executor 1</strong></span> in this diagram) receives a data stream from<a id="id663" class="indexterm"/> the Streaming Sources. With the incoming data stream, the receiver divides the stream into blocks and keeps these blocks in memory.</li><li class="listitem">These blocks are also replicated to another executor to avoid data loss.</li><li class="listitem">The block ID information is transmitted to the <span class="strong"><strong>Block Management Master</strong></span> on the driver.</li><li class="listitem">For every batch interval configured within Spark Streaming Context (commonly this is every 1 second), the driver will launch Spark tasks to process the blocks. Those blocks are then persisted to any number of target data stores, including cloud storage (for example, S3, WASB, and so on), relational data stores (for example, MySQL, PostgreSQL, and so on), and NoSQL stores.</li></ol></div><p>Suffice it to say, there are a lot of moving parts for a streaming application that need to be continually optimized<a id="id664" class="indexterm"/> and configured. Most of the documentation for Spark Streaming is more complete in Scala, so, as you are working with<a id="id665" class="indexterm"/> the Python APIs, you may sometimes need to reference the Scala version of the documentation instead. If this happens to you, please file a bug and/or fill out a PR if you have a proposed fix (<a class="ulink" href="https://issues.apache.org/jira/browse/spark/">https://issues.apache.org/jira/browse/spark/</a>).</p><p>For a deeper dive on this topic, please refer to:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><span class="emphasis"><em>Spark 1.6 Streaming Programming Guide</em></span>: <a class="ulink" href="https://spark.apache.org/docs/1.6.0/streaming-programming-guide.html">https://spark.apache.org/docs/1.6.0/streaming-programming-guide.html</a></li><li class="listitem"><span class="emphasis"><em>Tathagata Das' Deep Dive with Spark Streaming (Spark Meetup 2013-06-17)</em></span>: <a class="ulink" href="http://www.slideshare.net/spark-project/deep-divewithsparkstreaming-tathagatadassparkmeetup20130617">http://www.slideshare.net/spark-project/deep-divewithsparkstreaming-tathagatadassparkmeetup20130617</a></li></ol></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Simple streaming application using DStreams"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec69"/>Simple streaming application using DStreams</h1></div></div></div><p>Let's create a simple word count example using Spark Streaming in Python. For this example, we will<a id="id666" class="indexterm"/> be working with DStream – the Discretized Stream<a id="id667" class="indexterm"/> of small batches that make up the stream of data. The example used for this section of the book can be found in its entirety at: <a class="ulink" href="https://github.com/drabastomek/learningPySpark/blob/master/Chapter10/streaming_word_count.py">https://github.com/drabastomek/learningPySpark/blob/master/Chapter10/streaming_word_count.py</a>.</p><p>This word count example will use the Linux / Unix <code class="literal">nc</code> command – it is a simple utility that reads and writes data across network connections. We will use two different bash terminals, one using the <code class="literal">nc</code> command to send words to our computer's local port (<code class="literal">9999</code>) and one terminal that will run Spark Streaming to receive those words and count them. The initial set of commands for our script are noted here:</p><div class="informalexample"><pre class="programlisting">1. # Create a local SparkContext and Streaming Contexts
2. from pyspark import SparkContext
3. from pyspark.streaming import StreamingContext
4. 
5. # Create sc with two working threads 
6. sc = SparkContext("local[2]", "NetworkWordCount")
7. 
8. # Create local StreamingContextwith batch interval of 1 second
9. ssc = StreamingContext(sc, 1)
10. 
11. # Create DStream that connects to localhost:9999
12. lines = ssc.socketTextStream("localhost", 9999)</pre></div><p>Here are some important call outs for the preceding commands:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">The <code class="literal">StreamingContext</code> on line 9 is the entry point into Spark Streaming</li><li class="listitem">The <code class="literal">1</code> of <code class="literal">...(sc, 1)</code> on line 9 is the <span class="emphasis"><em>batch interval</em></span>; in this case, we are running micro-batches every second.</li><li class="listitem">The <code class="literal">lines</code> on line 12 is the <code class="literal">DStream</code> representing the data stream extracted via the <code class="literal">ssc.socketTextStream</code>.</li><li class="listitem">As noted in the description, the <code class="literal">ssc.socketTextStream</code> is the Spark Streaming method to review a text stream for a particular socket; in this case, your local computer on socket <code class="literal">9999</code>.</li></ol></div><p>The next few<a id="id668" class="indexterm"/> lines of code (as described in the comments), split the lines DStream into words and then, using RDDs, count each word in each batch of data<a id="id669" class="indexterm"/> and print this information out to the console (line number 9):</p><div class="informalexample"><pre class="programlisting">1. # Split lines into words
2. words = lines.flatMap(lambda line: line.split(" "))
3. 
4. # Count each word in each batch
5. pairs = words.map(lambda word: (word, 1))
6. wordCounts = pairs.reduceByKey(lambda x, y: x + y)
7. 
8. # Print the first ten elements of each RDD in this DStream 
9. wordCounts.pprint()</pre></div><p>The final set of lines of the code start Spark Streaming (<code class="literal">ssc.start()</code>) and then await a termination command to stop running (for example, <code class="literal">&lt;Ctrl&gt;&lt;C&gt;</code>). If no termination command is sent, then the Spark Streaming program will continue running.</p><div class="informalexample"><pre class="programlisting"># Start the computation
ssc.start()             

# Wait for the computation to terminate
ssc.awaitTermination()  </pre></div><p>Now that you have your script, as noted earlier, open two terminal windows – one for your <code class="literal">nc</code> command, and one for your Spark Streaming Program. To start the <code class="literal">nc </code>command, from one of your terminals, type:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>nc –lk 9999</strong></span>
</pre></div><p>Everything you type from this point onwards in that terminal will be transmitted to port <code class="literal">9999</code>, as noted in the following screenshot:</p><div class="mediaobject"><img src="images/B05793_10_05.jpg" alt="Simple streaming application using DStreams"/></div><p>In this example (as noted previously), I typed the words <span class="strong"><strong>green</strong></span> three times and <span class="strong"><strong>blue</strong></span> five times. From the<a id="id670" class="indexterm"/> other terminal screen, let's run the Python<a id="id671" class="indexterm"/> streaming script you just created. In this example, I named the script <code class="literal">streaming_word_count.py../bin/spark-submit streaming_word_count.py localhost 9999</code>.</p><p>The command will run the <code class="literal">streaming_word_count.py</code> script, reading your local computer (that is, <code class="literal">localhost</code>) port <code class="literal">9999</code> to receive any words sent to that socket. As you have already sent information to the port on the first screen, shortly after starting up the script, your Spark Streaming program will read the words sent to port <code class="literal">9999</code> and perform a word count as noted in the following screenshot:</p><div class="mediaobject"><img src="images/B05793_10_06.jpg" alt="Simple streaming application using DStreams"/></div><p>The <code class="literal">streaming_word_count.py</code> script will continue to read and print any new information to the console. Going back<a id="id672" class="indexterm"/> to our first terminal (with the <code class="literal">nc</code> command), we now can type our next set of words, as noted in the<a id="id673" class="indexterm"/> following screenshot:</p><div class="mediaobject"><img src="images/B05793_10_07.jpg" alt="Simple streaming application using DStreams"/></div><p>Reviewing the streaming script in the second terminal, you will notice that this script continues to run every second (that is, the configured <span class="emphasis"><em>batch interval</em></span>), and you will notice the calculated word count for <code class="literal">gohawks</code> a few seconds later:</p><div class="mediaobject"><img src="images/B05793_10_08.jpg" alt="Simple streaming application using DStreams"/></div><p>With this relatively<a id="id674" class="indexterm"/> simple script, now you can see Spark<a id="id675" class="indexterm"/> Streaming in action with Python. But if you continue typing words into the <code class="literal">nc</code> terminal, you will notice that this information is not aggregated. For example, if we continue to write green in the <code class="literal">nc</code> terminal (as noted here):</p><div class="mediaobject"><img src="images/B05793_10_09.jpg" alt="Simple streaming application using DStreams"/></div><p>The Spark Streaming<a id="id676" class="indexterm"/> terminal will report the current snapshot<a id="id677" class="indexterm"/> of data; that is, the two additional green values (as noted here):</p><div class="mediaobject"><img src="images/B05793_10_10.jpg" alt="Simple streaming application using DStreams"/></div><p>What did not happen<a id="id678" class="indexterm"/> was the concept of global aggregations<a id="id679" class="indexterm"/>, where we would keep <span class="emphasis"><em>state</em></span> for this information. What this means is that, instead of reporting 2 new greens, we could get Spark Streaming to give us the overall counts of green, for example, 7 greens, 5 blues, and 1 gohawks. We will talk about global aggregations in the form of <code class="literal">UpdateStateByKey /</code> <code class="literal">mapWithState</code> in the next section.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip40"/>Tip</h3><p>For other<a id="id680" class="indexterm"/> good PySpark<a id="id681" class="indexterm"/> Streaming examples, check out:</p><p>Network Wordcount (in Apache Spark GitHub repo): <a class="ulink" href="https://github.com/apache/spark/blob/master/examples/src/main/python/streaming/network_wordcount.py">https://github.com/apache/spark/blob/master/examples/src/main/python/streaming/network_wordcount.py</a>
</p><p>Python Streaming Examples: <a class="ulink" href="https://github.com/apache/spark/tree/master/examples/src/main/python/streaming">https://github.com/apache/spark/tree/master/examples/src/main/python/streaming</a>
</p><p>S3 FileStream<a id="id682" class="indexterm"/> Wordcount (Databricks notebook): <a class="ulink" href="https://docs.cloud.databricks.com/docs/latest/databricks_guide/index.html#07%20Spark%20Streaming/06%20FileStream%20Word%20Count%20-%20Python.html">https://docs.cloud.databricks.com/docs/latest/databricks_guide/index.html#07%20Spark%20Streaming/06%20FileStream%20Word%20Count%20-%20Python.html</a>
</p></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="A quick primer on global aggregations"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec70"/>A quick primer on global aggregations</h1></div></div></div><p>As noted in<a id="id683" class="indexterm"/> the previous section, so far, our script has performed a point in time streaming word count. The following diagram denotes the <span class="strong"><strong>lines DStream</strong></span> and its micro-batches as per how our script had<a id="id684" class="indexterm"/> executed in the previous section:</p><div class="mediaobject"><img src="images/B05793_10_11.jpg" alt="A quick primer on global aggregations"/></div><p>At the 1 second mark, our Python Spark Streaming script returned the value of <code class="literal">{(blue, 5), (green, 3)}</code>, at the 2 second mark it returned <code class="literal">{(gohawks, 1)}</code>, and at the 4 second mark, it returned <code class="literal">{(green, 2)}</code>. But what if you had wanted the aggregate word count over a specific time window?</p><p>The following<a id="id685" class="indexterm"/> figure represents us calculating a stateful aggregation:</p><div class="mediaobject"><img src="images/B05793_10_12.jpg" alt="A quick primer on global aggregations"/></div><p>In this case, we have a time window between 0-5 seconds. Note, that in our script we have not got the specified time window: each second, we calculate the cumulative sum of the words. Therefore, at the 2 second mark, the output is not just the <code class="literal">green</code> and <code class="literal">blue</code> from the 1 second mark, but it also includes the <code class="literal">gohawks</code> from the 2 second mark: <code class="literal">{(blue, 5), (green, 3), (gohawks, 1)}</code>. At the 4 second mark, the additional 2 <code class="literal">greens</code> provide us a total of <code class="literal">{(blue, 5), (green, 5), (gohawks, 1)}</code>.</p><p>For those of you who regularly work with relational databases, this seems to be just a <code class="literal">GROUP BY, SUM()</code> statement. Yet, in the case of streaming analytics, the duration to persist the data long enough to run a <code class="literal">GROUP BY, SUM()</code> statement is longer than the <span class="emphasis"><em>batch interval</em></span> (for example, 1 second). This means that we would constantly be running behind and trying<a id="id686" class="indexterm"/> to catch up with the data stream.</p><p>For example, if you were to run the <span class="emphasis"><em>1. Streaming and DataFrames.scala</em></span> Databricks notebook at <a class="ulink" href="https://github.com/dennyglee/databricks/blob/master/notebooks/Users/denny%40databricks.com/content/Streaming%20Meetup%20RSVPs/1.%20Streaming%20and%20DataFrames.scala">https://github.com/dennyglee/databricks/blob/master/notebooks/Users/denny%40databricks.com/content/Streaming%20Meetup%20RSVPs/1.%20Streaming%20and%20DataFrames.scala</a>, and you were to view the Streaming jobs in the Spark UI, you would get something like the following figure:</p><div class="mediaobject"><img src="images/B05793_10_13.jpg" alt="A quick primer on global aggregations"/></div><p>Notice in the graph that the <span class="strong"><strong>Scheduling Delay</strong></span> and <span class="strong"><strong>Total Delay</strong></span> numbers are rapidly increasing (for example, average Total Delay is <span class="strong"><strong>54 seconds 254 ms</strong></span> and the actual Total Delay is &gt; 2min) and way outside the <span class="emphasis"><em>batch interval</em></span> threshold of 1 second. The reason we see this delay is because, inside the streaming code for that notebook, we had also run the following code:</p><div class="informalexample"><pre class="programlisting">// Populate `meetup_stream` table
sqlContext.sql("insert into meetup_stream select * from meetup_stream_json")</pre></div><p>That is, inserting<a id="id687" class="indexterm"/> any new chunks of data (that is, 1 second RDD micro-batches), converting them into a DataFrame (<code class="literal">meetup_stream_json</code> table), and inserting the data into a persistent table (<code class="literal">meetup_stream</code> table). Persisting the data in this fashion led to slow streaming performance with the ever-increasing scheduling delays. To solve this problem via <span class="emphasis"><em>streaming analytics</em></span>, this is where creating global aggregations via <code class="literal">UpdateStateByKey</code> (Spark 1.5 and before) or <code class="literal">mapWithState</code> (Spark 1.6 onwards) come in.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip41"/>Tip</h3><p>For more information on Spark Streaming visualizations, please take the time to review <span class="emphasis"><em>New Visualizations for Understanding Apache Spark Streaming Applications:</em></span> <a class="ulink" href="https://databricks.com/blog/2015/07/08/new-visualizations-for-understanding-apache-spark-streaming-applications.html">https://databricks.com/blog/2015/07/08/new-visualizations-for-understanding-apache-spark-streaming-applications.html</a>.</p></div></div><p>Knowing this, let's re-write the original <code class="literal">streaming_word_count.py</code> so that we now have a <span class="emphasis"><em>stateful</em></span> version called <code class="literal">stateful_streaming_word_count.py</code>; you can get the full version of this script at <a class="ulink" href="https://github.com/drabastomek/learningPySpark/blob/master/Chapter10/stateful_streaming_word_count.py">https://github.com/drabastomek/learningPySpark/blob/master/Chapter10/stateful_streaming_word_count.py</a>.</p><p> The initial set of commands for our script are noted here:</p><div class="informalexample"><pre class="programlisting"> 1. # Create a local SparkContext and Streaming Contexts
 2. from pyspark import SparkContext
 3. from pyspark.streaming import StreamingContext
 4. 
 5. # Create sc with two working threads 
 6. sc = SparkContext("local[2]", "StatefulNetworkWordCount")
 7. 
 8. # Create local StreamingContext with batch interval of 1 sec
 9. ssc = StreamingContext(sc, 1)
10. 
11. # Create checkpoint for local StreamingContext
12. ssc.checkpoint("checkpoint")
13. 
14. # Define updateFunc: sum of the (key, value) pairs
15. def updateFunc(new_values, last_sum):
16.   return sum(new_values) + (last_sum or 0)
17. 
18. # Create DStream that connects to localhost:9999
19. lines = ssc.socketTextStream("localhost", 9999)</pre></div><p>If you recall <code class="literal">streaming_word_count.py</code>, the primary differences start at line 11:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <code class="literal">ssc.checkpoint("checkpoint")</code> on line 12 configures a Spark Streaming <span class="emphasis"><em>checkpoint</em></span>. To ensure that Spark Streaming is fault tolerant due to its continual operation, it needs<a id="id688" class="indexterm"/> to checkpoint enough information to fault-tolerant storage, so it can recover from failures. Note, we will not dive deep into this concept (though more information is available in the following <span class="emphasis"><em>Tip</em></span> section), as many of these configurations will be abstracted away with Structured Streaming.</li><li class="listitem" style="list-style-type: disc">The <code class="literal">updateFunc</code> on line 15 tells the program to update the application's <span class="emphasis"><em>state</em></span> (later in the code) via <code class="literal">UpdateStateByKey</code>. In this case, it is returning a sum of the previous value (<code class="literal">last_sum</code>) and the sum of the new values (<code class="literal">sum(new_values) + (last_sum or 0)</code>).</li><li class="listitem" style="list-style-type: disc">At line 19, we have the same <code class="literal">ssc.socketTextStream</code> as the previous script.<div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip42"/>Tip</h3><p>For more information on Spark Streaming <span class="emphasis"><em>checkpoint</em></span>, some good references are:</p><p>Spark Streaming<a id="id689" class="indexterm"/> Programming Guide &gt; Checkpoint: <a class="ulink" href="https://spark.apache.org/docs/1.6.0/streaming-programming-guide.html#checkpointing">https://spark.apache.org/docs/1.6.0/streaming-programming-guide.html#checkpointing</a>
</p><p>Exploring Stateful<a id="id690" class="indexterm"/> Streaming with Apache Spark: <a class="ulink" href="http://asyncified.io/2016/07/31/exploring-stateful-streaming-with-apache-spark/">http://asyncified.io/2016/07/31/exploring-stateful-streaming-with-apache-spark/</a>
</p></div></div></li></ul></div><p>The final section of the code is as follows:</p><div class="informalexample"><pre class="programlisting"> 1. # Calculate running counts
 2. running_counts = lines.flatMap(lambda line: line.split(" "))\
 3.           .map(lambda word: (word, 1))\
 4.           .updateStateByKey(updateFunc)
 5. 
 6. # Print the first ten elements of each RDD generated in this 
 7. # stateful DStream to the console
 8. running_counts.pprint()
 9. 
10. # Start the computation
11. ssc.start()             
12. 
13. # Wait for the computation to terminate
14. ssc.awaitTermination()  </pre></div><p>While lines 10-14 are identical to the previous script, the difference is that we now have a <code class="literal">running_counts</code> variable that splits to get the words and runs a map function to count each word in each batch (in the previous script this was the <code class="literal">words</code> and <code class="literal">pairs</code> variables).</p><p>The primary difference is the use of the <code class="literal">updateStateByKey</code> method, which will execute the previously noted <code class="literal">updateFunc</code> that performs the sum. <code class="literal">updateStateByKey</code> is Spark Streaming's method to<a id="id691" class="indexterm"/> perform calculations against your stream of data and update the state for each key in a performant manner. It is important to note that you would typically use <code class="literal">updateStateByKey</code> for Spark 1.5 and earlier; the performance of these <span class="emphasis"><em>stateful</em></span> global aggregations is proportional to the <span class="emphasis"><em>size of the state</em></span>. From Spark 1.6 onwards, you should use <code class="literal">mapWithState</code>, as the performance is proportional to the <span class="emphasis"><em>size of the batch</em></span>.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip44"/>Tip</h3><p>Note, there is more code typically involved with <code class="literal">mapWithState</code> (in comparison to <code class="literal">updateStateByKey</code>), hence the examples were written using <code class="literal">updateStateByKey</code>.</p><p>For more information about stateful Spark Streaming, including the use of <code class="literal">mapWithState</code>, please refer to:</p><p>Stateful Network<a id="id692" class="indexterm"/> Wordcount Python example: <a class="ulink" href="https://github.com/apache/spark/blob/master/examples/src/main/python/streaming/stateful_network_wordcount.py">https://github.com/apache/spark/blob/master/examples/src/main/python/streaming/stateful_network_wordcount.py</a>
</p><p>Global Aggregation<a id="id693" class="indexterm"/> using mapWithState (Scala): <a class="ulink" href="https://docs.cloud.databricks.com/docs/latest/databricks_guide/index.html#07%20Spark%20Streaming/12%20Global%20Aggregations%20-%20mapWithState.html">https://docs.cloud.databricks.com/docs/latest/databricks_guide/index.html#07%20Spark%20Streaming/12%20Global%20Aggregations%20-%20mapWithState.html</a>
</p><p>Word count<a id="id694" class="indexterm"/> using mapWithState (Scala): <a class="ulink" href="https://docs.cloud.databricks.com/docs/spark/1.6/examples/Streaming%20mapWithState.html">https://docs.cloud.databricks.com/docs/spark/1.6/examples/Streaming%20mapWithState.html</a>
</p><p>Faster Stateful<a id="id695" class="indexterm"/> Stream Processing in Apache Spark Streaming: <a class="ulink" href="https://databricks.com/blog/2016/02/01/faster-stateful-stream-processing-in-apache-spark-streaming.html">https://databricks.com/blog/2016/02/01/faster-stateful-stream-processing-in-apache-spark-streaming.html</a>
</p></div></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Introducing Structured Streaming"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec71"/>Introducing Structured Streaming</h1></div></div></div><p>With Spark 2.0, the Apache<a id="id696" class="indexterm"/> Spark community is working on simplifying streaming by introducing the concept of <span class="emphasis"><em>structured streaming</em></span> which bridges the concepts of streaming with Datasets/DataFrames (as noted in the following diagram):</p><div class="mediaobject"><img src="images/B05793_10_14.jpg" alt="Introducing Structured Streaming"/></div><p>As noted in earlier chapters on DataFrames, the execution of SQL and/or DataFrame queries within the Spark SQL Engine (and Catalyst Optimizer) revolves around building a logical plan, building numerous physical plans, the engine choosing the correct physical plan based on its<a id="id697" class="indexterm"/> cost optimizer, and then generating the code (i.e. <span class="emphasis"><em>code gen</em></span>) that will deliver the results in a performant manner. What <span class="emphasis"><em>Structured Streaming</em></span> introduces is the concept of an <span class="strong"><strong>Incremental Execution Plan</strong></span>. When working with blocks of data, structured streaming repeatedly applies the execution plan for every new set of blocks it receives. By running in this manner, the engine can take advantage of the optimizations included within Spark DataFrames/Datasets and apply them to an incoming data stream. It will also be easier to integrate other DataFrame<a id="id698" class="indexterm"/> optimized components of Spark, including ML Pipelines, GraphFrames, TensorFrames, and many others.</p><p>Using structured streaming will also simplify your code. For example, the following is a pseudo-code example <span class="emphasis"><em>batch aggregation</em></span> that reads a data stream from S3 and saves it to a MySQL database:</p><div class="informalexample"><pre class="programlisting">logs = spark.read.json('s3://logs')

logs.groupBy(logs.UserId).agg(sum(logs.Duration))
.write.jdbc('jdbc:mysql//...')</pre></div><p>The following is a pseudo-code example for a <span class="emphasis"><em>continous aggregation</em></span>:</p><div class="informalexample"><pre class="programlisting">logs = spark.readStream.json('s3://logs').load()

sq = logs.groupBy(logs.UserId).agg(sum(logs.Duration))
.writeStream.format('json').start()</pre></div><p>The reason for creating the <code class="literal">sq</code> variable is that it allows you to check the status of your structured streaming job and terminate it, as per the following:</p><div class="informalexample"><pre class="programlisting"># Will return true if the `sq` stream is active
sq.isActive

# Will terminate the `sq` stream
sq.stop()</pre></div><p>Let's take the stateful streaming word count script that had used <code class="literal">updateStateByKey</code> and make it a structured<a id="id699" class="indexterm"/> streaming word count script; you can get the complete <code class="literal">structured_streaming_word_count.py</code> script at: <a class="ulink" href="https://github.com/drabastomek/learningPySpark/blob/master/Chapter10/structured_streaming_word_count.py">https://github.com/drabastomek/learningPySpark/blob/master/Chapter10/structured_streaming_word_count.py</a>.</p><p>As opposed to the previous scripts, we are now working with the more familiar DataFrames code as noted here:</p><div class="informalexample"><pre class="programlisting"># Import the necessary classes and create a local SparkSession
from pyspark.sql import SparkSession
from pyspark.sql.functions import explode
from pyspark.sql.functions import split

spark = SparkSession \
   .builder \
   .appName("StructuredNetworkWordCount") \
   .getOrCreate()</pre></div><p>The first lines of the<a id="id700" class="indexterm"/> script import the necessary classes and establish the current <code class="literal">SparkSession</code>. But, as opposed to the previous streaming scripts, as in the next lines of the script noted here, you do not need to establish a Streaming Context as this is already included within the <code class="literal">SparkSession</code>:</p><div class="informalexample"><pre class="programlisting"> 1. # Create DataFrame representing the stream of input lines
 2. # from connection to localhost:9999
 3.  lines = spark\
 4.    .readStream\
 5.    .format('socket')\
 6.    .option('host', 'localhost')\
 7.   .option('port', 9999)\
 8.   .load()
 9.
10. # Split the lines into words
11. words = lines.select(
12.   explode(
13.          split(lines.value, ' ')
14.   ).alias('word')
15.   )
16.
17. # Generate running word count
18. wordCounts = words.groupBy('word').count()</pre></div><p>Instead, the streaming portion of the code is initiated by calling <code class="literal">readStream</code> in line 4.</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Lines 3-8 initiate the <span class="emphasis"><em>reading</em></span> of the data stream from port <code class="literal">9999</code>, just like the previous two scripts</li><li class="listitem" style="list-style-type: disc">Instead of running RDD <code class="literal">flatMap</code>, <code class="literal">map</code>, and <code class="literal">reduceByKey</code> functions to split the lines read into words and count each word in each batch, we can use the PySpark SQL functions <code class="literal">explode</code> and <code class="literal">split</code> as noted in lines 10-15</li><li class="listitem" style="list-style-type: disc">Instead of running <code class="literal">updateStateByKey</code> or creating an <code class="literal">updateFunc</code> as per the stateful streaming word count script, we can generate the running word count with a familiar DataFrame <code class="literal">groupBy</code> statement and <code class="literal">count()</code>, as noted in lines 17-18</li></ul></div><p>To output this data to the console, we will use <code class="literal">writeStream</code>, as noted here:</p><div class="informalexample"><pre class="programlisting"> 1. # Start running the query that prints the 
 2. # running counts to the console
 3. query = wordCounts\
 4.     .writeStream\
 5.     .outputMode('complete')\
 6.     .format('console')\
 7.     .start()
 8. 
 9. # Await Spark Streaming termination
10. query.awaitTermination()</pre></div><p>Instead of using <code class="literal">pprint()</code>, we're explicitly calling out <code class="literal">writeStream</code> to write the stream, and defining the format<a id="id701" class="indexterm"/> and output mode. While it is a little longer to write, these methods and properties are syntactically similar with other DataFrame calls and you would only need to change the <code class="literal">outputMode</code> and <code class="literal">format</code> properties to save it to a Database, file system, console, and so on. Finally, as noted in line 10, we will run <code class="literal">awaitTermination</code> to await to cancel this streaming job.</p><p>Let's go back and run our <code class="literal">nc</code> job in the first terminal:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ nc –lk 9999</strong></span>
<span class="strong"><strong>green green green blue blue blue blue blue</strong></span>
<span class="strong"><strong>gohawks</strong></span>
<span class="strong"><strong>green green</strong></span>
</pre></div><p>Check the following output. As you can see, you get the advantages of stateful streaming but using the more familiar DataFrame API:</p><div class="mediaobject"><img src="images/B05793_10_15.jpg" alt="Introducing Structured Streaming"/></div></div></div>


  <div id="sbo-rt-content"><div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec72"/>Summary</h1></div></div></div><p>It is important to note that Structured Streaming is currently (at the time of writing) not production-ready. It is, however, a paradigm shift in Spark that will hopefully make it easier for data scientists and data engineers to build <span class="strong"><strong>continuous applications</strong></span>. While not explicitly called out in the previous sections, when working with streaming applications, there are many potential problems that you will need to design for, such as late events, partial outputs, state recovery on failure, distributed reads and writes, and so on. With structured streaming, many of these issues will be abstracted away to make it easier for you to build <span class="emphasis"><em>continuous applications</em></span>.</p><p>We encourage you to try Spark Structured Streaming so you will be able to easily build streaming applications as structured streaming matures. As Reynold Xin noted in his Spark Summit 2016 East presentation <span class="emphasis"><em>The Future of Real-Time in Spark</em></span> (<a class="ulink" href="http://www.slideshare.net/rxin/the-future-of-realtime-in-spark">http://www.slideshare.net/rxin/the-future-of-realtime-in-spark</a>):</p><div class="blockquote"><blockquote class="blockquote"><p>"The simplest way to perform streaming analytics is not having to <span class="emphasis"><em>reason</em></span> about streaming."</p></blockquote></div><p>For more information, here are some additional Structured Streaming resources:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>PySpark 2.1 Documentation: pyspark.sql.module</em></span>: <a class="ulink" href="http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html">http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html</a></li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>Introducing Apache Spark 2.1</em></span>: <a class="ulink" href="https://databricks.com/blog/2016/12/29/introducing-apache-spark-2-1.html">https://databricks.com/blog/2016/12/29/introducing-apache-spark-2-1.html</a></li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>Structuring Apache Spark 2.0: SQL, DataFrames, Datasets and Streaming - by Michael Armbrust</em></span>: <a class="ulink" href="http://www.slideshare.net/databricks/structuring-spark-dataframes-datasets-and-streaming-62871797">http://www.slideshare.net/databricks/structuring-spark-dataframes-datasets-and-streaming-62871797</a></li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>Structured Streaming Programming Guide</em></span>: <a class="ulink" href="http://spark.apache.org/docs/latest/streaming-programming-guide.html">http://spark.apache.org/docs/latest/streaming-programming-guide.html</a></li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>Structured Streaming (aka Streaming DataFrames) [SPARK-8360]</em></span>: <a class="ulink" href="https://issues.apache.org/jira/browse/SPARK-8360">https://issues.apache.org/jira/browse/SPARK-8360</a></li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>Structured Streaming Programming Abstraction, Semantics, and APIs ­Apache JIRA</em></span>: <a class="ulink" href="https://issues.apache.org/jira/secure/attachment/12793410/StructuredStreamingProgrammingAbstractionSemanticsandAPIs-ApacheJIRA.pdf">https://issues.apache.org/jira/secure/attachment/12793410/StructuredStreamingProgrammingAbstractionSemanticsandAPIs-ApacheJIRA.pdf</a></li></ul></div><p>In the next chapter we will show you how to modularize and package up your PySpark application and submit it for execution programmatically.</p></div></div>
</body></html>