- en: Chapter 7. Unstructured Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, we looked at different ways of building and fitting
    models on structured data. Unfortunately, these otherwise extremely useful methods
    are of no use (yet) when dealing with, for example, a pile of PDF documents. Hence,
    the following pages will focus on methods to deal with non-tabular data, such
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: Extracting metrics from a collection of text documents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Filtering and parsing **natural language texts** (**NLP**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing unstructured data in a structured way
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text mining is the process of analyzing natural language text; in most cases
    from online content, such as emails and social media streams (Twitter or Facebook).
    In this chapter, we are going to cover the most used methods of the `tm` package—although,
    there is a variety of further types of unstructured data, such as text, image,
    audio, video, non-digital contents, and so on, which we cannot discuss for the
    time being.
  prefs: []
  type: TYPE_NORMAL
- en: Importing the corpus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A corpus is basically a collection of text documents that you want to include
    in the analytics. Use the `getSources` function to see the available options to
    import a corpus with the `tm` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'So, we can import text documents from a `data.frame`, a `vector`, or directly
    from a uniform resource identifier with the `URISource` function. The latter stands
    for a collection of hyperlinks or file paths, although this is somewhat easier
    to handle with `DirSource`, which imports all the textual documents found in the
    referenced directory on our hard drive. By calling the `getReaders` function in
    the R console, you can see the supported text file formats:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: So, there are some nifty functions to read and parse MS Word, PDFs, plain text,
    or XML files among a few other file formats. The previous `Reut` reader stands
    for the Reuters demo corpus that is bundled with the `tm` package.
  prefs: []
  type: TYPE_NORMAL
- en: 'But let''s not stick to some factory default demo files! You can see the package
    examples in the vignette or reference manual. As we have already fetched some
    textual data in [Chapter 2](ch02.html "Chapter 2. Getting Data from the Web"),
    *Getting Data from the Web*, let''s see how we can process and analyze that content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The preceding command requires a live Internet connection and could take 15-120
    seconds to download and parse the referenced HTML page. Please note that the content
    of the downloaded HTML file might be different from what is shown in this chapter,
    so please be prepared for slightly different outputs in your R session, as compared
    to what we published in this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, now we have a `data.frame` with more than 5,000 R package names and short
    descriptions. Let''s build a corpus from the vector source of package descriptions,
    so that we can parse those further and see the most important trends in package
    development:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We have just created a `VCorpus` (in-memory) object, which currently holds
    5,880 package descriptions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'As the default `print` method (see the preceding output) shows a concise overview
    on the corpus, we will need to use another function to inspect the actual content:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Here, we can see the first three documents in the corpus, along with some metadata.
    Until now, we have not done much more than when in the [Chapter 2](ch02.html "Chapter 2. Getting
    Data from the Web"), *Getting Data from the Web*, we visualized a wordcloud of
    the expression used in the package descriptions. But that's exactly where the
    journey begins with text mining!
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning the corpus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the nicest features of the `tm` package is the variety of bundled transformations
    to be applied on corpora (corpuses). The `tm_map` function provides a convenient
    way of running the transformations on the corpus to filter out all the data that
    is irrelevant in the actual research. To see the list of available transformation
    methods, simply call the `getTransformations` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We should usually start with removing the most frequently used, so called stopwords
    from the corpus. These are the most common, short function terms, which usually
    carry less important meanings than the other expressions in the corpus, especially
    the keywords. The package already includes such lists of words in different languages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Skimming through this list verifies that removing these rather unimportant
    words will not really modify the meaning of the R package descriptions. Although
    there are some rare cases in which removing the stopwords is not a good idea at
    all! Carefully examine the output of the following R command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This does not suggest that the memorable quote from Shakespeare is meaningless,
    or that we can ignore any of the stopwords in all cases. Sometimes, these words
    have a very important role in the context, where replacing the words with a space
    is not useful, but rather deteriorative. Although I would suggest, that in most
    cases, removing the stopwords is highly practical for keeping the number of words
    to process at a low level.
  prefs: []
  type: TYPE_NORMAL
- en: 'To iteratively apply the previous call on each document in our corpus, the
    `tm_map` function is extremely useful:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Simply pass the corpus and the transformation function, along with its parameters,
    to `tm_map`, which takes and returns a corpus of any number of documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that the most common function words and a few special characters
    are now gone from the package descriptions. But what happens if someone starts
    the description with uppercase stopwords? This is shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s clear that the uppercase version of the `to` common word was not removed
    from the sentence, and the trailing dot was also preserved. For this end, usually,
    we should simply transform the uppercase letters to lowercase, and replace the
    punctuations with a space to keep the clutter among the keywords at a minimal
    level:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: So, we first called the `tolower` function from the `base` package to transform
    all characters from upper to lower case. Please note that we had to wrap the `tolower`
    function in the `content_transformer` function, so that our transformation really
    complies with the `tm` package's object structure. This is usually required when
    using a transformation function outside of the `tm` package.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we removed all the punctuation marks from the text with the help of the
    `removePunctutation` function. The punctuations marks are the ones referred to
    as `[:punct:]` in regular expressions, including the following characters: `!`
    `"` `#` `$` `%` `&` `''` `( )` `*` `+` `,` `-` `.` `/` `:` `;` `<` `=` `>` `?`
    `@` `[` `\` `]` `^` `_` `` ` `` `{` `|` `}` `~''`. Usually, it''s safe to remove
    these separators, especially when we analyze the words on their own and not their
    relations.'
  prefs: []
  type: TYPE_NORMAL
- en: And we also removed the multiple whitespace characters from the document, so
    that we find only one space between the filtered words.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the most frequent words in the corpus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have cleared up our corpus a bit, we can generate a much more useful
    wordcloud, as compared to the proof-of-concept demo we generated in [Chapter 2](ch02.html
    "Chapter 2. Getting Data from the Web"), *Getting Data from the Web*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![Visualizing the most frequent words in the corpus](img/2028OS_07_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Further cleanup
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are still some small disturbing glitches in the wordlist. Maybe, we do
    not really want to keep numbers in the package descriptions at all (or we might
    want to replace all numbers with a placeholder text, such as `NUM`), and there
    are some frequent technical words that can be ignored as well, for example, `package`.
    Showing the plural version of nouns is also redundant. Let's improve our corpus
    with some further tweaks, step by step!
  prefs: []
  type: TYPE_NORMAL
- en: 'Removing the numbers from the package descriptions is fairly straightforward,
    as based on the previous examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'To remove some frequent domain-specific words with less important meanings,
    let''s see the most common words in the documents. For this end, first we have
    to compute the `TermDocumentMatrix` function that can be passed later to the `findFreqTerms`
    function to identify the most popular terms in the corpus, based on frequency:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This object is basically a matrix which includes the words in the rows and
    the documents in the columns, where the cells show the number of occurrences.
    For example, let''s take a look at the first 5 words'' occurrences in the first
    20 documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Extracting the overall number of occurrences for each word is fairly easy.
    In theory, we could compute the `rowSums` function of this sparse matrix. But
    let''s simply call the `findFreqTerms` function, which does exactly what we were
    up to. Let''s show all those terms that show up in the descriptions at least a
    100 times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Manually reviewing this list suggests ignoring the `based` and `using` words,
    besides the previously suggested `package` term:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Stemming words
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, let's get rid of the plural forms of the nouns, which also occur in the
    preceding top 20 lists of the most common words! This is not as easy as it sounds.
    We might apply some regular expressions to cut the trailing `s` from the words,
    but this method has many drawbacks, such as not taking into account some evident
    English grammar rules.
  prefs: []
  type: TYPE_NORMAL
- en: 'But we can, instead, use some stemming algorithms, especially Porter''s stemming
    algorithm, which is available in the `SnowballC` package. The `wordStem` function
    supports 16 languages (take a look at the `getStemLanguages` for details), and
    can identify the stem of a character vector as easily as calling the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The only penalty here is the fact that Porter''s algorithm does not provide
    real English words in all cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'So later, we will have to tweak the results further; to reconstruct the words
    with the help of a language lexicon database. The easiest way to construct such
    a database is copying the words of the already existing corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, let''s stem all the words in the documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Now, we called the `stemDocument` function, which is a wrapper around the `SnowballC`
    package's `wordStem` function. We specified only one parameter, which sets the
    language of the stemming algorithm. And now, let's call the `stemCompletion` function
    on our previously defined directory, and let's formulate each stem to the shortest
    relevant word found in the database.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, it''s not as straightforward as the previous examples, as the
    `stemCompletion` function takes a character vector of words instead of documents
    that we have in our corpus. So thus, we have to write our own transformation function
    with the previously used `content_transformer` helper. The basic idea is to split
    each documents into words by a space, apply the `stemCompletion` function, and
    then concatenate the words into sentences again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The preceding example is rather resource hungry, so please be prepared for high
    CPU usage for around 30 to 60 minutes on a standard PC. As you can (technically)
    run the forthcoming code samples without actually performing this step, you may
    feel free to skip to the next code chunk, if in a hurry.
  prefs: []
  type: TYPE_NORMAL
- en: 'It took some time, huh? Well, we had to iterate through all the words in each
    document found in the corpus , but it''s well worth the trouble! Let''s see the
    top used terms in the cleaned corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'While previously the very same command returned 23 terms, out of which we removed
    3, now we see more than 30 words occurring more than 100 times in the corpus.
    We got rid of the plural versions of the nouns and a few other similar variations
    of the same terms, so the density of the document term matrix also increased:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: We not only decreased the number of different words to be indexed in the next
    steps, but we also identified a few new terms that are to be ignored in our further
    analysis, for example, `set` does not seem to be an important word in the package
    descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: Lemmatisation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While stemming terms, we started to remove characters from the end of words
    in the hope of finding the stem, which is a heuristic process often resulting
    in not-existing words, as we have seen previously. We tried to overcome this issue
    by completing these stems to the shortest meaningful words by using a dictionary,
    which might result in derivation in the meaning of the term, for example, removing
    the `ness` suffix.
  prefs: []
  type: TYPE_NORMAL
- en: Another way to reduce the number of inflectional forms of different terms, instead
    of deconstructing and then trying to rebuild the words, is morphological analysis
    with the help of a dictionary. This process is called lemmatisation, which looks
    for lemma (the canonical form of a word) instead of stems.
  prefs: []
  type: TYPE_NORMAL
- en: The Stanford NLP Group created and maintains a Java-based NLP tool called Stanford
    CoreNLP, which supports lemmatization besides many other NLP algorithms such as
    tokenization, sentence splitting, POS tagging, and syntactic parsing.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can use CoreNLP from R via the `rJava` package, or you might install the
    `coreNLP` package, which includes some wrapper functions around the `CoreNLP`
    Java library, which are meant for providing easy access to, for example, lammatisation.
    Please note that after installing the R package, you have to use the `downloadCoreNLP`
    function to actually install and make accessible the features of the Java library.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the associations among terms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previously computed `TermDocumentMatrix`, can also be used to identify the
    association between the cleaned terms found in the corpus. This simply suggests
    the correlation coefficient computed on the joint occurrence of term-pairs in
    the same document, which can be queried easily with the `findAssocs` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see which words are associated with `data`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Only four terms seem to have a higher correlation coefficient than 0.1, and
    it's not surprising at all that `analyzing` is among the top associated words.
    Probably, we can ignore the `set` term, but it seems that `longitudinal` and `big`
    data are pretty frequent idioms in package descriptions. So, what other `big`
    terms do we have?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Checking the original corpus reveals that there are several R packages starting
    with **pbd**, which stands for **Programming with Big Data**. The `pbd` packages
    are usually tied to Open MPI, which pretty well explains the high association
    between these terms.
  prefs: []
  type: TYPE_NORMAL
- en: Some other metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'And, of course, we can use the standard data analysis tools as well after quantifying
    our package descriptions a bit. Let''s see, for example, the length of the documents
    in the corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'So, the average package description consists of around 40 characters, while
    there is a package with only two characters in the description. Well, two characters
    after removing numbers, punctuations, and the common words. To see which package
    has this very short description, we might simply call the `which.min` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'And this is what''s strange about it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'So, this is not a real package after all, but rather an empty row in the original
    table. Let''s visually inspect the overall number of characters in the package
    descriptions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '![Some other metrics](img/2028OS_07_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The histogram suggests that most packages have a rather short description with
    no more than one sentence, based on the fact that an average English sentence
    includes around 15-20 words with 75-100 characters.
  prefs: []
  type: TYPE_NORMAL
- en: The segmentation of documents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To identify the different groups of cleaned terms, based on the frequency and
    association of the terms in the documents of the corpus, one might directly use
    our `tdm` matrix to run, for example, the classic hierarchical cluster algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, if you would rather like to cluster the R packages based
    on their description, we should compute a new matrix with `DocumentTermMatrix`,
    instead of the previously used `TermDocumentMatrix`. Then, calling the clustering
    algorithm on this matrix would result in the segmentation of the packages.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more details on the available methods, algorithms, and guidance on choosing
    the appropriate functions for clustering, please see [Chapter 10](ch10.html "Chapter 10. Classification
    and Clustering"), *Classification and Clustering*. For now, we will fall back
    to the traditional `hclust` function, which provides a built-in way of running
    hierarchical clustering on distance matrices. For a quick demo, let''s demonstrate
    this on the so-called `Hadleyverse`, which describes a useful collection of R
    packages developed by Hadley Wickham:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s identify which elements of the `v` corpus hold the cleaned terms
    of the previously listed packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'And then, we can simply compute the (dis)similarity matrix of the used terms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '![The segmentation of documents](img/2028OS_07_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Besides the `reshape2` and `tidyr` packages that we covered in [Chapter 4](ch04.html
    "Chapter 4. Restructuring Data"), *Restructuring Data*, we can see two separate
    clusters in the previous plot (the highlighted terms in the following list are
    copied from the package descriptions):'
  prefs: []
  type: TYPE_NORMAL
- en: Packages that *make* things a bit *easier*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Others dealing with the language, *documentation* and *grammar*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To verify this, you might be interested in the cleansed terms for each package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: An alternative and probably more appropriate, long-term approach for clustering
    documents based on NLP algorithms, would be fitting topic models, for example,
    via the `topicmodels` package. This R package comes with a detailed and very useful
    vignette, which includes some theoretical background as well as some hands-on
    examples. But for a quick start, you might simply try to run the `LDA` or `CTM`
    functions on our previously created `DocumentTermMatrix`, and specify the number
    of topics for the models to be built. A good start, based on our previous clustering
    example, might be `k=3`.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The preceding examples and quick theoretical background introduced text mining
    algorithms to structure plain English texts into numbers for further analysis.
    In the next chapter, we will concentrate on some similarly important methods in
    the process of data analysis, such as how to polish this kind of data in the means
    of identifying outliers, extreme values, and how to handle missing data.
  prefs: []
  type: TYPE_NORMAL
