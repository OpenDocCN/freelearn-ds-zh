- en: Chapter 7. Unstructured Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章。非结构化数据
- en: 'In the previous chapter, we looked at different ways of building and fitting
    models on structured data. Unfortunately, these otherwise extremely useful methods
    are of no use (yet) when dealing with, for example, a pile of PDF documents. Hence,
    the following pages will focus on methods to deal with non-tabular data, such
    as:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们探讨了在结构化数据上构建和拟合模型的不同方法。不幸的是，这些在其他情况下非常有用的方法在处理例如一堆PDF文档时（目前）毫无用处。因此，接下来的几页将重点介绍处理非表格数据的方法，例如：
- en: Extracting metrics from a collection of text documents
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从文本文档集合中提取度量
- en: Filtering and parsing **natural language texts** (**NLP**)
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过滤和解析**自然语言文本**（**NLP**）
- en: Visualizing unstructured data in a structured way
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以结构化的方式可视化非结构化数据
- en: Text mining is the process of analyzing natural language text; in most cases
    from online content, such as emails and social media streams (Twitter or Facebook).
    In this chapter, we are going to cover the most used methods of the `tm` package—although,
    there is a variety of further types of unstructured data, such as text, image,
    audio, video, non-digital contents, and so on, which we cannot discuss for the
    time being.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 文本挖掘是分析自然语言文本的过程；在大多数情况下来自在线内容，如电子邮件和社交媒体流（Twitter或Facebook）。在本章中，我们将介绍`tm`包中最常用的方法——尽管还有许多其他类型的非结构化数据，如文本、图像、音频、视频、非数字内容等，我们目前无法讨论。
- en: Importing the corpus
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导入语料库
- en: 'A corpus is basically a collection of text documents that you want to include
    in the analytics. Use the `getSources` function to see the available options to
    import a corpus with the `tm` package:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 语料库基本上是你想要包含在分析中的文本文档集合。使用`getSources`函数查看使用`tm`包导入语料库的可用选项：
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'So, we can import text documents from a `data.frame`, a `vector`, or directly
    from a uniform resource identifier with the `URISource` function. The latter stands
    for a collection of hyperlinks or file paths, although this is somewhat easier
    to handle with `DirSource`, which imports all the textual documents found in the
    referenced directory on our hard drive. By calling the `getReaders` function in
    the R console, you can see the supported text file formats:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以使用`URISource`函数从`data.frame`、`vector`或直接从统一资源标识符导入文本文档。后者代表一组超链接或文件路径，尽管使用`DirSource`处理起来要容易一些，因为`DirSource`可以导入硬盘上引用目录中找到的所有文本文档。在R控制台中调用`getReaders`函数，你可以看到支持的文本文件格式：
- en: '[PRE1]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: So, there are some nifty functions to read and parse MS Word, PDFs, plain text,
    or XML files among a few other file formats. The previous `Reut` reader stands
    for the Reuters demo corpus that is bundled with the `tm` package.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，有一些巧妙的函数可以读取和解析MS Word、PDF、纯文本或XML文件等几种其他文件格式。之前的`Reut`读取器代表与`tm`包捆绑的Reuter演示语料库。
- en: 'But let''s not stick to some factory default demo files! You can see the package
    examples in the vignette or reference manual. As we have already fetched some
    textual data in [Chapter 2](ch02.html "Chapter 2. Getting Data from the Web"),
    *Getting Data from the Web*, let''s see how we can process and analyze that content:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们不要局限于一些工厂默认的演示文件！你可以在vignette或参考手册中查看包示例。因为我们已经在[第2章](ch02.html "第2章。从网络获取数据")中获取了一些文本数据，即“从网络获取数据”，让我们看看我们如何处理和分析这些内容：
- en: '[PRE2]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Tip
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: The preceding command requires a live Internet connection and could take 15-120
    seconds to download and parse the referenced HTML page. Please note that the content
    of the downloaded HTML file might be different from what is shown in this chapter,
    so please be prepared for slightly different outputs in your R session, as compared
    to what we published in this book.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的命令需要一个活跃的互联网连接，可能需要15-120秒来下载和解析引用的HTML页面。请注意，下载的HTML文件的内容可能与本章中显示的内容不同，因此请准备好在R会话中可能出现的略微不同的输出，与我们在这本书中发布的内容相比。
- en: 'So, now we have a `data.frame` with more than 5,000 R package names and short
    descriptions. Let''s build a corpus from the vector source of package descriptions,
    so that we can parse those further and see the most important trends in package
    development:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有一个包含超过5,000个R包名称和简短描述的`data.frame`。让我们从包描述的向量源构建语料库，这样我们就可以进一步解析它们并查看包开发中的最重要趋势：
- en: '[PRE3]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We have just created a `VCorpus` (in-memory) object, which currently holds
    5,880 package descriptions:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚创建了一个`VCorpus`（内存中）对象，它目前包含5,880个包描述：
- en: '[PRE4]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As the default `print` method (see the preceding output) shows a concise overview
    on the corpus, we will need to use another function to inspect the actual content:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如默认的`print`方法（参见前面的输出）所示，它对语料库提供了一个简洁的概述，因此我们需要使用另一个函数来检查实际内容：
- en: '[PRE5]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here, we can see the first three documents in the corpus, along with some metadata.
    Until now, we have not done much more than when in the [Chapter 2](ch02.html "Chapter 2. Getting
    Data from the Web"), *Getting Data from the Web*, we visualized a wordcloud of
    the expression used in the package descriptions. But that's exactly where the
    journey begins with text mining!
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到语料库中的前三个文档，以及一些元数据。到目前为止，我们做的没有比在[第2章](ch02.html "第2章。从网络获取数据")，*从网络获取数据*时更多，我们可视化了一个用于包描述的表达式的词云。但那正是文本挖掘之旅的开始！
- en: Cleaning the corpus
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 清理语料库
- en: 'One of the nicest features of the `tm` package is the variety of bundled transformations
    to be applied on corpora (corpuses). The `tm_map` function provides a convenient
    way of running the transformations on the corpus to filter out all the data that
    is irrelevant in the actual research. To see the list of available transformation
    methods, simply call the `getTransformations` function:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '`tm`包最令人愉悦的特性之一是它提供了多种捆绑的转换，可以应用于语料库（corpuses）。`tm_map`函数提供了一种方便的方式来对语料库执行转换，以过滤掉实际研究中所有不相关的数据。要查看可用的转换方法列表，只需调用`getTransformations`函数：'
- en: '[PRE6]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We should usually start with removing the most frequently used, so called stopwords
    from the corpus. These are the most common, short function terms, which usually
    carry less important meanings than the other expressions in the corpus, especially
    the keywords. The package already includes such lists of words in different languages:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常应该从移除语料库中最常用的所谓停用词开始。这些是最常见的、简短的函数术语，它们通常比语料库中的其他表达式（尤其是关键词）的意义不那么重要。该包已经包含了不同语言的此类单词列表：
- en: '[PRE7]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Skimming through this list verifies that removing these rather unimportant
    words will not really modify the meaning of the R package descriptions. Although
    there are some rare cases in which removing the stopwords is not a good idea at
    all! Carefully examine the output of the following R command:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 快速浏览这个列表可以验证，移除这些相对不重要的词并不会真正改变R包描述的意义。尽管有些罕见的情况，移除停用词根本不是个好主意！仔细检查以下R命令的输出：
- en: '[PRE8]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: This does not suggest that the memorable quote from Shakespeare is meaningless,
    or that we can ignore any of the stopwords in all cases. Sometimes, these words
    have a very important role in the context, where replacing the words with a space
    is not useful, but rather deteriorative. Although I would suggest, that in most
    cases, removing the stopwords is highly practical for keeping the number of words
    to process at a low level.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不暗示莎士比亚的著名引言没有意义，或者我们可以在所有情况下忽略任何停用词。有时，这些词在上下文中扮演着非常重要的角色，用空格替换这些词并不有用，反而会降低质量。尽管如此，我建议，在大多数情况下，移除停用词对于将需要处理的单词数量保持在较低水平是非常实用的。
- en: 'To iteratively apply the previous call on each document in our corpus, the
    `tm_map` function is extremely useful:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 要迭代地对语料库中的每个文档应用之前的调用，`tm_map`函数非常有用：
- en: '[PRE9]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Simply pass the corpus and the transformation function, along with its parameters,
    to `tm_map`, which takes and returns a corpus of any number of documents:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 简单地将语料库、转换函数及其参数传递给`tm_map`，它接受并返回任何数量的文档的语料库：
- en: '[PRE10]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We can see that the most common function words and a few special characters
    are now gone from the package descriptions. But what happens if someone starts
    the description with uppercase stopwords? This is shown in the following example:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，最常见的函数词和一些特殊字符现在已从包描述中消失。但如果有人以大写停用词开始描述呢？以下是一个示例：
- en: '[PRE11]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'It''s clear that the uppercase version of the `to` common word was not removed
    from the sentence, and the trailing dot was also preserved. For this end, usually,
    we should simply transform the uppercase letters to lowercase, and replace the
    punctuations with a space to keep the clutter among the keywords at a minimal
    level:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，句子中并没有移除`to`这个常用词的大写版本，并且句尾的点也被保留了。为此，通常我们只需将大写字母转换为小写，并用空格替换标点符号，以将关键词之间的杂乱程度降至最低：
- en: '[PRE12]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: So, we first called the `tolower` function from the `base` package to transform
    all characters from upper to lower case. Please note that we had to wrap the `tolower`
    function in the `content_transformer` function, so that our transformation really
    complies with the `tm` package's object structure. This is usually required when
    using a transformation function outside of the `tm` package.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们首先从`base`包中调用了`tolower`函数来将所有字符从大写转换为小写。请注意，我们必须将`tolower`函数包装在`content_transformer`函数中，以便我们的转换真正符合`tm`包的对象结构。通常，在使用`tm`包之外的转换函数时，这是必需的。
- en: 'Then, we removed all the punctuation marks from the text with the help of the
    `removePunctutation` function. The punctuations marks are the ones referred to
    as `[:punct:]` in regular expressions, including the following characters: `!`
    `"` `#` `$` `%` `&` `''` `( )` `*` `+` `,` `-` `.` `/` `:` `;` `<` `=` `>` `?`
    `@` `[` `\` `]` `^` `_` `` ` `` `{` `|` `}` `~''`. Usually, it''s safe to remove
    these separators, especially when we analyze the words on their own and not their
    relations.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用`removePunctutation`函数帮助移除了文本中的所有标点符号。这些标点符号是正则表达式中所指的`[:punct:]`，包括以下字符：`!`
    `"` `#` `$` `%` `&` `'` `( )` `*` `+` `,` `-` `.` `/` `:` `;` `<` `=` `>` `?`
    `@` `[` `\` `]` `^` `_` `` ` `` `{` `|` `}` `~'`。通常，移除这些分隔符是安全的，尤其是在我们单独分析单词而不是分析它们之间的关系时。
- en: And we also removed the multiple whitespace characters from the document, so
    that we find only one space between the filtered words.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还从文档中移除了多余的空白字符，这样我们只会在过滤后的单词之间找到单个空格。
- en: Visualizing the most frequent words in the corpus
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化语料库中最常见的单词
- en: 'Now that we have cleared up our corpus a bit, we can generate a much more useful
    wordcloud, as compared to the proof-of-concept demo we generated in [Chapter 2](ch02.html
    "Chapter 2. Getting Data from the Web"), *Getting Data from the Web*:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经清理了我们的语料库，我们可以生成一个比我们在[第2章](ch02.html "第2章。从网络获取数据")中生成的概念验证演示更有用的词云：
- en: '[PRE13]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![Visualizing the most frequent words in the corpus](img/2028OS_07_01.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![可视化语料库中最常见的单词](img/2028OS_07_01.jpg)'
- en: Further cleanup
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步清理
- en: There are still some small disturbing glitches in the wordlist. Maybe, we do
    not really want to keep numbers in the package descriptions at all (or we might
    want to replace all numbers with a placeholder text, such as `NUM`), and there
    are some frequent technical words that can be ignored as well, for example, `package`.
    Showing the plural version of nouns is also redundant. Let's improve our corpus
    with some further tweaks, step by step!
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 单词列表中仍然存在一些小的干扰错误。也许，我们根本不想在包描述中保留数字（或者我们可能想用占位文本，如`NUM`替换所有数字），还有一些可以忽略的常见技术词汇，例如`package`。显示名词的复数形式也是多余的。让我们逐步通过一些进一步的调整来改进我们的语料库！
- en: 'Removing the numbers from the package descriptions is fairly straightforward,
    as based on the previous examples:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 从包描述中移除数字相当直接，如前例所示：
- en: '[PRE14]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'To remove some frequent domain-specific words with less important meanings,
    let''s see the most common words in the documents. For this end, first we have
    to compute the `TermDocumentMatrix` function that can be passed later to the `findFreqTerms`
    function to identify the most popular terms in the corpus, based on frequency:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了移除一些意义不重要的常见领域特定词汇，让我们看看文档中最常见的单词。为此，我们首先必须计算`TermDocumentMatrix`函数，该函数可以稍后传递给`findFreqTerms`函数，以根据频率识别语料库中最流行的术语：
- en: '[PRE15]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This object is basically a matrix which includes the words in the rows and
    the documents in the columns, where the cells show the number of occurrences.
    For example, let''s take a look at the first 5 words'' occurrences in the first
    20 documents:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这个对象基本上是一个矩阵，包括行中的单词和列中的文档，其中单元格显示出现次数。例如，让我们看看前20个文档中前5个单词的出现次数：
- en: '[PRE16]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Extracting the overall number of occurrences for each word is fairly easy.
    In theory, we could compute the `rowSums` function of this sparse matrix. But
    let''s simply call the `findFreqTerms` function, which does exactly what we were
    up to. Let''s show all those terms that show up in the descriptions at least a
    100 times:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 提取每个单词的总出现次数相当简单。从理论上讲，我们可以计算这个稀疏矩阵的`rowSums`函数。但让我们简单地调用`findFreqTerms`函数，它正好是我们想要做的。让我们展示那些在描述中至少出现100次的术语：
- en: '[PRE17]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Manually reviewing this list suggests ignoring the `based` and `using` words,
    besides the previously suggested `package` term:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 手动审查这个列表建议忽略`based`和`using`这两个词，除了之前建议的`package`术语：
- en: '[PRE18]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Stemming words
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词干提取
- en: Now, let's get rid of the plural forms of the nouns, which also occur in the
    preceding top 20 lists of the most common words! This is not as easy as it sounds.
    We might apply some regular expressions to cut the trailing `s` from the words,
    but this method has many drawbacks, such as not taking into account some evident
    English grammar rules.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们去除名词的复数形式，这些形式也出现在前面最常用的20个单词列表中！这并不像听起来那么简单。我们可能应用一些正则表达式来从单词中剪切掉尾部的`s`，但这种方法有很多缺点，例如没有考虑到一些明显的英语语法规则。
- en: 'But we can, instead, use some stemming algorithms, especially Porter''s stemming
    algorithm, which is available in the `SnowballC` package. The `wordStem` function
    supports 16 languages (take a look at the `getStemLanguages` for details), and
    can identify the stem of a character vector as easily as calling the function:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们可以使用一些词干算法，特别是可用的`SnowballC`包中的Porter词干算法。`wordStem`函数支持16种语言（详细信息请参阅`getStemLanguages`），可以像调用函数一样轻松地识别字符向量的词干：
- en: '[PRE19]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The only penalty here is the fact that Porter''s algorithm does not provide
    real English words in all cases:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这里唯一的缺点是Porter算法并不总是在所有情况下提供真正的英语单词：
- en: '[PRE20]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'So later, we will have to tweak the results further; to reconstruct the words
    with the help of a language lexicon database. The easiest way to construct such
    a database is copying the words of the already existing corpus:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，稍后我们还需要进一步调整结果；通过帮助语言词典数据库来重建单词。构建此类数据库的最简单方法是从已存在的语料库中复制单词：
- en: '[PRE21]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Then, let''s stem all the words in the documents:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，让我们对文档中的所有单词进行词干提取：
- en: '[PRE22]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Now, we called the `stemDocument` function, which is a wrapper around the `SnowballC`
    package's `wordStem` function. We specified only one parameter, which sets the
    language of the stemming algorithm. And now, let's call the `stemCompletion` function
    on our previously defined directory, and let's formulate each stem to the shortest
    relevant word found in the database.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们调用了`stemDocument`函数，这是一个围绕`SnowballC`包的`wordStem`函数的包装器。我们只指定了一个参数，该参数设置了词干算法的语言。现在，让我们在我们的先前定义的目录上调用`stemCompletion`函数，并将每个词干与数据库中找到的最短相关单词相匹配。
- en: 'Unfortunately, it''s not as straightforward as the previous examples, as the
    `stemCompletion` function takes a character vector of words instead of documents
    that we have in our corpus. So thus, we have to write our own transformation function
    with the previously used `content_transformer` helper. The basic idea is to split
    each documents into words by a space, apply the `stemCompletion` function, and
    then concatenate the words into sentences again:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这并不像前面的例子那么简单，因为`stemCompletion`函数接受一个单词字符向量而不是我们语料库中的文档。因此，我们必须编写自己的转换函数，使用之前使用的`content_transformer`辅助函数。基本思想是将每个文档通过空格分割成单词，应用`stemCompletion`函数，然后将单词再次连接成句子：
- en: '[PRE23]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Tip
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: The preceding example is rather resource hungry, so please be prepared for high
    CPU usage for around 30 to 60 minutes on a standard PC. As you can (technically)
    run the forthcoming code samples without actually performing this step, you may
    feel free to skip to the next code chunk, if in a hurry.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的例子相当占用资源，所以请准备好在标准PC上大约30到60分钟的高CPU使用率。由于你可以（技术上）运行即将到来的代码示例而不实际执行此步骤，如果你赶时间，可以自由跳到下一个代码块。
- en: 'It took some time, huh? Well, we had to iterate through all the words in each
    document found in the corpus , but it''s well worth the trouble! Let''s see the
    top used terms in the cleaned corpus:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这花了些时间，对吧？好吧，我们必须遍历语料库中找到的每个文档中的所有单词，但这很值得麻烦！让我们看看清理后的语料库中最常用的术语：
- en: '[PRE24]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'While previously the very same command returned 23 terms, out of which we removed
    3, now we see more than 30 words occurring more than 100 times in the corpus.
    We got rid of the plural versions of the nouns and a few other similar variations
    of the same terms, so the density of the document term matrix also increased:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然之前相同的命令返回了23个术语，我们从中去掉了3个，但现在我们在语料库中看到了超过30个单词出现超过100次。我们去掉了名词的复数形式和一些其他类似的术语变体，因此文档术语矩阵的密度也增加了：
- en: '[PRE25]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We not only decreased the number of different words to be indexed in the next
    steps, but we also identified a few new terms that are to be ignored in our further
    analysis, for example, `set` does not seem to be an important word in the package
    descriptions.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不仅减少了在下一步中需要索引的不同单词数量，而且还识别出了一些在进一步分析中需要忽略的新术语，例如，“set”似乎在包描述中不是一个重要的单词。
- en: Lemmatisation
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词形还原
- en: While stemming terms, we started to remove characters from the end of words
    in the hope of finding the stem, which is a heuristic process often resulting
    in not-existing words, as we have seen previously. We tried to overcome this issue
    by completing these stems to the shortest meaningful words by using a dictionary,
    which might result in derivation in the meaning of the term, for example, removing
    the `ness` suffix.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行词干提取时，我们开始从单词的末尾移除字符，希望能找到词干，这是一个启发式过程，通常会导致出现之前未见过的单词，正如我们之前所看到的。我们试图通过使用词典将这些词干补充到最短的有意义单词，从而克服这个问题，这可能会导致术语意义的派生，例如，移除`ness`后缀。
- en: Another way to reduce the number of inflectional forms of different terms, instead
    of deconstructing and then trying to rebuild the words, is morphological analysis
    with the help of a dictionary. This process is called lemmatisation, which looks
    for lemma (the canonical form of a word) instead of stems.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种减少不同术语屈折形式数量的方法，而不是先分解然后尝试重建单词，是借助词典进行形态分析。这个过程被称为词元化，它寻找的是词元（单词的规范形式）而不是词干。
- en: The Stanford NLP Group created and maintains a Java-based NLP tool called Stanford
    CoreNLP, which supports lemmatization besides many other NLP algorithms such as
    tokenization, sentence splitting, POS tagging, and syntactic parsing.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 斯坦福NLP小组创建并维护了一个基于Java的NLP工具，称为Stanford CoreNLP，它支持词元化，除了许多其他NLP算法，如分词、句子分割、词性标注和句法分析。
- en: Tip
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: You can use CoreNLP from R via the `rJava` package, or you might install the
    `coreNLP` package, which includes some wrapper functions around the `CoreNLP`
    Java library, which are meant for providing easy access to, for example, lammatisation.
    Please note that after installing the R package, you have to use the `downloadCoreNLP`
    function to actually install and make accessible the features of the Java library.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过`rJava`包使用CoreNLP，或者您可能安装`coreNLP`包，该包包括围绕`CoreNLP` Java库的一些包装函数，旨在提供对例如词元化的简单访问。请注意，在安装R包之后，您必须使用`downloadCoreNLP`函数来实际安装并使Java库的功能可用。
- en: Analyzing the associations among terms
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析术语之间的关联
- en: The previously computed `TermDocumentMatrix`, can also be used to identify the
    association between the cleaned terms found in the corpus. This simply suggests
    the correlation coefficient computed on the joint occurrence of term-pairs in
    the same document, which can be queried easily with the `findAssocs` function.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 之前计算出的`TermDocumentMatrix`也可以用来识别语料库中发现的清洁术语之间的关联。这仅仅意味着在相同文档中词对联合出现时计算的关联系数，这可以通过`findAssocs`函数轻松查询。
- en: 'Let''s see which words are associated with `data`:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看哪些单词与`data`相关联：
- en: '[PRE26]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Only four terms seem to have a higher correlation coefficient than 0.1, and
    it's not surprising at all that `analyzing` is among the top associated words.
    Probably, we can ignore the `set` term, but it seems that `longitudinal` and `big`
    data are pretty frequent idioms in package descriptions. So, what other `big`
    terms do we have?
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 只有四个术语似乎具有高于0.1的相关系数，而“分析”是其中之一，位于关联词的前列，这并不令人惊讶。可能我们可以忽略“set”这个术语，但“longitudinal”和“big”数据似乎在包描述中相当常见。那么，我们还有哪些“big”术语呢？
- en: '[PRE27]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Checking the original corpus reveals that there are several R packages starting
    with **pbd**, which stands for **Programming with Big Data**. The `pbd` packages
    are usually tied to Open MPI, which pretty well explains the high association
    between these terms.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 检查原始语料库揭示，有几个以**pbd**开头的R包，这代表**Programming with Big Data**。`pbd`包通常与Open MPI相关联，这很好地解释了这些术语之间的高关联性。
- en: Some other metrics
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一些其他指标
- en: 'And, of course, we can use the standard data analysis tools as well after quantifying
    our package descriptions a bit. Let''s see, for example, the length of the documents
    in the corpus:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在量化我们的包描述之后，我们也可以使用标准的数据分析工具。让我们看看，例如，语料库中文档的长度：
- en: '[PRE28]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'So, the average package description consists of around 40 characters, while
    there is a package with only two characters in the description. Well, two characters
    after removing numbers, punctuations, and the common words. To see which package
    has this very short description, we might simply call the `which.min` function:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，平均包描述大约由40个字符组成，而有一个包的描述中只有两个字符。好吧，去掉数字、标点符号和常用词后，两个字符。为了查看哪个包有如此简短的描述，我们可能简单地调用`which.min`函数：
- en: '[PRE29]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'And this is what''s strange about it:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 而这正是它的奇怪之处：
- en: '[PRE30]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'So, this is not a real package after all, but rather an empty row in the original
    table. Let''s visually inspect the overall number of characters in the package
    descriptions:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这根本不是一个真正的包，而是一个原始表格中的空行。让我们直观地检查包描述中的总字符数：
- en: '[PRE31]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '![Some other metrics](img/2028OS_07_02.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![一些其他指标](img/2028OS_07_02.jpg)'
- en: The histogram suggests that most packages have a rather short description with
    no more than one sentence, based on the fact that an average English sentence
    includes around 15-20 words with 75-100 characters.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 直方图表明，大多数包的描述相当简短，不超过一句话，这是基于平均英语句子包含大约15-20个单词，75-100个字符的事实。
- en: The segmentation of documents
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文档的分割
- en: To identify the different groups of cleaned terms, based on the frequency and
    association of the terms in the documents of the corpus, one might directly use
    our `tdm` matrix to run, for example, the classic hierarchical cluster algorithm.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 为了根据语料库文档中术语的频率和关联识别不同的清洗术语组，可以直接使用我们的`tdm`矩阵运行，例如，经典的层次聚类算法。
- en: On the other hand, if you would rather like to cluster the R packages based
    on their description, we should compute a new matrix with `DocumentTermMatrix`,
    instead of the previously used `TermDocumentMatrix`. Then, calling the clustering
    algorithm on this matrix would result in the segmentation of the packages.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果你更愿意根据描述对R包进行聚类，我们应该使用`DocumentTermMatrix`计算一个新的矩阵，而不是之前使用的`TermDocumentMatrix`。然后，在这个矩阵上调用聚类算法将导致包的分割。
- en: 'For more details on the available methods, algorithms, and guidance on choosing
    the appropriate functions for clustering, please see [Chapter 10](ch10.html "Chapter 10. Classification
    and Clustering"), *Classification and Clustering*. For now, we will fall back
    to the traditional `hclust` function, which provides a built-in way of running
    hierarchical clustering on distance matrices. For a quick demo, let''s demonstrate
    this on the so-called `Hadleyverse`, which describes a useful collection of R
    packages developed by Hadley Wickham:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 关于可用的方法、算法以及选择聚类适当函数的指导，请参阅[第10章](ch10.html "第10章。分类和聚类")，*分类和聚类*。现在，我们将退回到传统的`hclust`函数，它提供了一种在距离矩阵上运行层次聚类的内置方式。为了快速演示，让我们在所谓的`Hadleyverse`上展示这一点，它描述了由Hadley
    Wickham开发的有用R包集合：
- en: '[PRE32]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now, let''s identify which elements of the `v` corpus hold the cleaned terms
    of the previously listed packages:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们确定`v`语料库中哪些元素包含了之前列出的包的清洗后的术语：
- en: '[PRE33]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'And then, we can simply compute the (dis)similarity matrix of the used terms:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以简单地计算使用术语的（不）相似度矩阵：
- en: '[PRE34]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '![The segmentation of documents](img/2028OS_07_03.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![文档的分割](img/2028OS_07_03.jpg)'
- en: 'Besides the `reshape2` and `tidyr` packages that we covered in [Chapter 4](ch04.html
    "Chapter 4. Restructuring Data"), *Restructuring Data*, we can see two separate
    clusters in the previous plot (the highlighted terms in the following list are
    copied from the package descriptions):'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们在[第4章](ch04.html "第4章。数据重构")中介绍的`reshape2`和`tidyr`包，*数据重构*，我们还可以在之前的图表中看到两个单独的聚类（以下列表中突出显示的术语是从包描述中复制的）：
- en: Packages that *make* things a bit *easier*
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使事情变得*更容易*的包
- en: Others dealing with the language, *documentation* and *grammar*
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他处理语言、*文档*和*语法*的
- en: 'To verify this, you might be interested in the cleansed terms for each package:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证这一点，你可能对每个包的清洗术语感兴趣：
- en: '[PRE35]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: An alternative and probably more appropriate, long-term approach for clustering
    documents based on NLP algorithms, would be fitting topic models, for example,
    via the `topicmodels` package. This R package comes with a detailed and very useful
    vignette, which includes some theoretical background as well as some hands-on
    examples. But for a quick start, you might simply try to run the `LDA` or `CTM`
    functions on our previously created `DocumentTermMatrix`, and specify the number
    of topics for the models to be built. A good start, based on our previous clustering
    example, might be `k=3`.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 基于NLP算法对文档进行聚类的另一种可能更合适、长期的方法是拟合主题模型，例如，通过`topicmodels`包。这个R包附带了一个详细且非常有用的vignette，其中包含一些理论背景和一些实际示例。但为了快速入门，你可能会尝试在我们的先前创建的`DocumentTermMatrix`上运行`LDA`或`CTM`函数，并指定要构建的模型的主题数量。根据我们之前的聚类示例，一个好的起点可能是`k=3`。
- en: Summary
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: The preceding examples and quick theoretical background introduced text mining
    algorithms to structure plain English texts into numbers for further analysis.
    In the next chapter, we will concentrate on some similarly important methods in
    the process of data analysis, such as how to polish this kind of data in the means
    of identifying outliers, extreme values, and how to handle missing data.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的示例和快速理论背景介绍了文本挖掘算法，将普通英文文本结构化为数字以便进一步分析。在下一章中，我们将集中讨论数据分析过程中一些同样重要的方法，例如如何通过识别异常值、极值来打磨这类数据，以及如何处理缺失数据。
