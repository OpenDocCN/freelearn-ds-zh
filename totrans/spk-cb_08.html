<html><head></head><body><div class="chapter" title="Chapter&#xA0;8.&#xA0;Supervised Learning with MLlib &#x2013; Classification"><div class="titlepage"><div><div><h1 class="title"><a id="ch08"/>Chapter 8. Supervised Learning with MLlib – Classification</h1></div></div></div><p>This chapter is divided into the following recipes:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Doing classification using logistic regression</li><li class="listitem" style="list-style-type: disc">Doing binary classification using SVM</li><li class="listitem" style="list-style-type: disc">Doing classification using decision trees</li><li class="listitem" style="list-style-type: disc">Doing classification using Random Forests</li><li class="listitem" style="list-style-type: disc">Doing classification using Gradient Boosted Trees</li><li class="listitem" style="list-style-type: disc">Doing classification with Naïve Bayes</li></ul></div><div class="section" title="Introduction"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec56"/>Introduction</h1></div></div></div><p>The classification problem is like the regression problem discussed in the previous chapter except that the outcome variable <span class="emphasis"><em>y</em></span> takes only a few discrete values. In binary classification, <span class="emphasis"><em>y</em></span> takes only two values: 0 or 1. You can also think of values that the response variable can take in classification as representing categories.</p></div></div>
<div class="section" title="Doing classification using logistic regression"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec57"/>Doing classification using logistic regression</h1></div></div></div><p>In <a id="id418" class="indexterm"/>classification, the response variable <span class="emphasis"><em>y</em></span> has <a id="id419" class="indexterm"/>discreet values as opposed to continuous values. Some examples are e-mail (spam/non-spam), transactions (safe/fraudulent), and so on.</p><p>The <span class="emphasis"><em>y</em></span> variable in the following equation can take on two values, 0 or 1:</p><div class="mediaobject"><img src="graphics/3056_08_01.jpg" alt="Doing classification using logistic regression"/></div><p>Here, 0 is referred to as a negative class and 1 means a positive class. Though we are calling them a positive or negative class, it is only for convenience's sake. Algorithms are neutral about this assignment.</p><p>Linear <a id="id420" class="indexterm"/>regression, though it works well<a id="id421" class="indexterm"/> for regression tasks, hits a few limitations for classification tasks. These include:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The fitting process is very susceptible to outliers</li><li class="listitem" style="list-style-type: disc">There is no guarantee that the hypothesis function <span class="emphasis"><em>h(x)</em></span> will fit in the range 0 (negative class) to 1 (positive class)</li></ul></div><p>Logistic regression guarantees that <span class="emphasis"><em>h(x)</em></span> will fit between 0 and 1. Though logistic regression has the word regression in it, it is more of a misnomer and it is very much a classification algorithm:</p><div class="mediaobject"><img src="graphics/3056_08_02.jpg" alt="Doing classification using logistic regression"/></div><p>In linear regression, the hypothesis function is as follows:</p><div class="mediaobject"><img src="graphics/3056_08_03.jpg" alt="Doing classification using logistic regression"/></div><p>In logistic regression, we slightly modify the hypothesis equation like this:</p><div class="mediaobject"><img src="graphics/3056_08_04.jpg" alt="Doing classification using logistic regression"/></div><p>The <span class="emphasis"><em>g</em></span> function is <a id="id422" class="indexterm"/>called the <span class="strong"><strong>sigmoid function</strong></span> or <span class="strong"><strong>logistic function</strong></span> and is <a id="id423" class="indexterm"/>defined as follows for a real number <span class="emphasis"><em>t</em></span>:</p><div class="mediaobject"><img src="graphics/3056_08_05.jpg" alt="Doing classification using logistic regression"/></div><p>This is what the sigmoid function looks like as a graph:</p><div class="mediaobject"><img src="graphics/3056_08_06.jpg" alt="Doing classification using logistic regression"/></div><p>As you can see, as <span class="emphasis"><em>t</em></span> approaches negative infinity, <span class="emphasis"><em>g(t)</em></span> approaches 0 and, as <span class="emphasis"><em>t</em></span> approaches infinity, <span class="emphasis"><em>g(t)</em></span> approaches 1. So, this guarantees that the hypothesis function output will never<a id="id424" class="indexterm"/> fall out of the 0 to 1 range.</p><p>Now<a id="id425" class="indexterm"/> the hypothesis function can be rewritten as:</p><div class="mediaobject"><img src="graphics/3056_08_07.jpg" alt="Doing classification using logistic regression"/></div><p><span class="emphasis"><em>h(x)</em></span> is the estimated probability that <span class="emphasis"><em>y = 1</em></span> for a given predictor <span class="emphasis"><em>x</em></span>, so <span class="emphasis"><em>h(x)</em></span> can also be rewritten as:</p><div class="mediaobject"><img src="graphics/3056_08_08.jpg" alt="Doing classification using logistic regression"/></div><p>In other words, the hypothesis function is showing the probability of <span class="emphasis"><em>y</em></span> being 1 given feature matrix <span class="emphasis"><em>x</em></span>, parameterized by <span class="inlinemediaobject"><img src="graphics/3056_08_09.jpg" alt="Doing classification using logistic regression"/></span>. This probability can be any real number between 0 and 1 but our goal of classification does not allow us to have continuous values; we can only have two values 0 or 1 indicating the negative or positive class.</p><p>Let's say that we predict <span class="emphasis"><em>y = 1</em></span> if </p><div class="mediaobject"><img src="graphics/3056_08_10.jpg" alt="Doing classification using logistic regression"/></div><p> and <span class="emphasis"><em>y = 0</em></span> otherwise. If we look at the sigmoid function graph again, we realize that, when the <span class="inlinemediaobject"><img src="graphics/3056_08_11.jpg" alt="Doing classification using logistic regression"/></span> sigmoid function is <span class="inlinemediaobject"><img src="graphics/3056_08_12.jpg" alt="Doing classification using logistic regression"/></span>, that is, for positive values of <span class="emphasis"><em>t</em></span>, it will predict the positive class:</p><p>Since <span class="inlinemediaobject"><img src="graphics/3056_08_13.jpg" alt="Doing classification using logistic regression"/></span>, this means for <span class="inlinemediaobject"><img src="graphics/3056_08_14.jpg" alt="Doing classification using logistic regression"/></span> the positive class will be predicted. To better illustrate this, let's expand it to a non-matrix form for a bivariate case:</p><div class="mediaobject"><img src="graphics/3056_08_15.jpg" alt="Doing classification using logistic regression"/></div><p>The<a id="id426" class="indexterm"/> plane represented by the equation <span class="inlinemediaobject"><img src="graphics/3056_08_16.jpg" alt="Doing classification using logistic regression"/></span> will decide whether a given vector belongs to the positive class <a id="id427" class="indexterm"/>or negative class. This line is called the decision boundary.</p><p>This boundary does not have to be linear depending on the training set. If training data does not separate across a linear boundary, higher-level polynomial features can be added to facilitate it. An example can be to add two new features by squaring x1 and x2 as follows:</p><div class="mediaobject"><img src="graphics/3056_08_17.jpg" alt="Doing classification using logistic regression"/></div><p>Please note that, to the learning algorithm, this enhancement is exactly the same as the following equation:</p><div class="mediaobject"><img src="graphics/3056_08_18.jpg" alt="Doing classification using logistic regression"/></div><p>The learning algorithm will treat the introduction of polynomials just as another feature. This gives you great power in the fitting process. It means any complex decision boundary can be created with the right choice of polynomials and parameters.</p><p>Let's spend some time trying to understand how we choose the right value for parameters like we did in the case of linear regression. The cost function <span class="emphasis"><em>J</em></span> in the case of linear regression was:</p><div class="mediaobject"><img src="graphics/3056_08_19.jpg" alt="Doing classification using logistic regression"/></div><p>As <a id="id428" class="indexterm"/>you know, we are averaging<a id="id429" class="indexterm"/> the cost in this cost function. Let's represent this in terms of cost term:</p><div class="mediaobject"><img src="graphics/3056_08_20.jpg" alt="Doing classification using logistic regression"/></div><p>In other words, the cost term is the cost the algorithm has to pay if it predicts <span class="emphasis"><em>h(x)</em></span> for the real response variable value <span class="emphasis"><em>y</em></span>:</p><div class="mediaobject"><img src="graphics/3056_08_21.jpg" alt="Doing classification using logistic regression"/></div><p>This cost works fine for linear regression but, for logistic regression, this cost function is non-convex (that is, it leads to multiple local minimums) and we need to find a better convex way to estimate the cost.</p><p>The cost functions that work well for logistic regression are the following:</p><div class="mediaobject"><img src="graphics/3056_08_22.jpg" alt="Doing classification using logistic regression"/></div><p>Let's put these two cost functions into one by combining the two:</p><div class="mediaobject"><img src="graphics/3056_08_23.jpg" alt="Doing classification using logistic regression"/></div><p>Let's<a id="id430" class="indexterm"/> put back this cost function<a id="id431" class="indexterm"/> to <span class="emphasis"><em>J</em></span>:</p><div class="mediaobject"><img src="graphics/3056_08_24.jpg" alt="Doing classification using logistic regression"/></div><p>The goal would be to minimize the cost, that is, minimize the value of <span class="inlinemediaobject"><img src="graphics/3056_08_25.jpg" alt="Doing classification using logistic regression"/></span>. This is done using the gradient descent algorithm. Spark has two classes that support logistic regression:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">LogisticRegressionWithSGD</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">LogisticRegressionWithLBFGS</code></li></ul></div><p>The <code class="literal">LogisticRegressionWithLBFGS</code> class is preferred as it eliminates the step of optimizing the step size.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec80"/>Getting ready</h2></div></div></div><p>In 2006, Suzuki, Tsurusaki, and Kodama did some research on the distribution of an endangered burrowing spider on different beaches in Japan (<a class="ulink" href="https://www.jstage.jst.go.jp/article/asjaa/55/2/55_2_79/_pdf">https://www.jstage.jst.go.jp/article/asjaa/55/2/55_2_79/_pdf</a>).</p><p>Let's see some data about grain size and the presence of spiders:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Grain size (mm)</p>
</th><th style="text-align: left" valign="bottom">
<p>Spider present</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>0.245</p>
</td><td style="text-align: left" valign="top">
<p>Absent</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>0.247</p>
</td><td style="text-align: left" valign="top">
<p>Absent</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>0.285</p>
</td><td style="text-align: left" valign="top">
<p>Present</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>0.299</p>
</td><td style="text-align: left" valign="top">
<p>Present</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>0.327</p>
</td><td style="text-align: left" valign="top">
<p>Present</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>0.347</p>
</td><td style="text-align: left" valign="top">
<p>Present</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>0.356</p>
</td><td style="text-align: left" valign="top">
<p>Absent</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>0.36</p>
</td><td style="text-align: left" valign="top">
<p>Present</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>0.363</p>
</td><td style="text-align: left" valign="top">
<p>Absent</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>0.364</p>
</td><td style="text-align: left" valign="top">
<p>Present</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>0.398</p>
</td><td style="text-align: left" valign="top">
<p>Absent</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>0.4</p>
</td><td style="text-align: left" valign="top">
<p>Present</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>0.409</p>
</td><td style="text-align: left" valign="top">
<p>Absent</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>0.421</p>
</td><td style="text-align: left" valign="top">
<p>Present</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>0.432</p>
</td><td style="text-align: left" valign="top">
<p>Absent</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>0.473</p>
</td><td style="text-align: left" valign="top">
<p>Present</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>0.509</p>
</td><td style="text-align: left" valign="top">
<p>Present</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>0.529</p>
</td><td style="text-align: left" valign="top">
<p>Present</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>0.561</p>
</td><td style="text-align: left" valign="top">
<p>Absent</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>0.569</p>
</td><td style="text-align: left" valign="top">
<p>Absent</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>0.594</p>
</td><td style="text-align: left" valign="top">
<p>Present</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>0.638</p>
</td><td style="text-align: left" valign="top">
<p>Present</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>0.656</p>
</td><td style="text-align: left" valign="top">
<p>Present</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>0.816</p>
</td><td style="text-align: left" valign="top">
<p>Present</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>0.853</p>
</td><td style="text-align: left" valign="top">
<p>Present</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>0.938</p>
</td><td style="text-align: left" valign="top">
<p>Present</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>1.036</p>
</td><td style="text-align: left" valign="top">
<p>Present</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>1.045</p>
</td><td style="text-align: left" valign="top">
<p>Present</p>
</td></tr></tbody></table></div><p>We <a id="id432" class="indexterm"/>will use this data to train the<a id="id433" class="indexterm"/> algorithm. Absent will be denoted as 0 and present will be denoted as 1.</p></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec81"/>How to do it…</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Start the Spark shell:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell</strong></span>
</pre></div></li><li class="listitem">Import statistics and related classes:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.linalg.Vectors</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.regression.LabeledPoint</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS</strong></span>
</pre></div></li><li class="listitem">Create <a id="id434" class="indexterm"/>a <code class="literal">LabeledPoint</code> array <a id="id435" class="indexterm"/>with the presence or absence of spiders being the label:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val points = Array(</strong></span>
<span class="strong"><strong>LabeledPoint(0.0,Vectors.dense(0.245)),</strong></span>
<span class="strong"><strong>LabeledPoint(0.0,Vectors.dense(0.247)),</strong></span>
<span class="strong"><strong>LabeledPoint(1.0,Vectors.dense(0.285)),</strong></span>
<span class="strong"><strong>LabeledPoint(1.0,Vectors.dense(0.299)),</strong></span>
<span class="strong"><strong>LabeledPoint(1.0,Vectors.dense(0.327)),</strong></span>
<span class="strong"><strong>LabeledPoint(1.0,Vectors.dense(0.347)),</strong></span>
<span class="strong"><strong>LabeledPoint(0.0,Vectors.dense(0.356)),</strong></span>
<span class="strong"><strong>LabeledPoint(1.0,Vectors.dense(0.36)),</strong></span>
<span class="strong"><strong>LabeledPoint(0.0,Vectors.dense(0.363)),</strong></span>
<span class="strong"><strong>LabeledPoint(1.0,Vectors.dense(0.364)),</strong></span>
<span class="strong"><strong>LabeledPoint(0.0,Vectors.dense(0.398)),</strong></span>
<span class="strong"><strong>LabeledPoint(1.0,Vectors.dense(0.4)),</strong></span>
<span class="strong"><strong>LabeledPoint(0.0,Vectors.dense(0.409)),</strong></span>
<span class="strong"><strong>LabeledPoint(1.0,Vectors.dense(0.421)),</strong></span>
<span class="strong"><strong>LabeledPoint(0.0,Vectors.dense(0.432)),</strong></span>
<span class="strong"><strong>LabeledPoint(1.0,Vectors.dense(0.473)),</strong></span>
<span class="strong"><strong>LabeledPoint(1.0,Vectors.dense(0.509)),</strong></span>
<span class="strong"><strong>LabeledPoint(1.0,Vectors.dense(0.529)),</strong></span>
<span class="strong"><strong>LabeledPoint(0.0,Vectors.dense(0.561)),</strong></span>
<span class="strong"><strong>LabeledPoint(0.0,Vectors.dense(0.569)),</strong></span>
<span class="strong"><strong>LabeledPoint(1.0,Vectors.dense(0.594)),</strong></span>
<span class="strong"><strong>LabeledPoint(1.0,Vectors.dense(0.638)),</strong></span>
<span class="strong"><strong>LabeledPoint(1.0,Vectors.dense(0.656)),</strong></span>
<span class="strong"><strong>LabeledPoint(1.0,Vectors.dense(0.816)),</strong></span>
<span class="strong"><strong>LabeledPoint(1.0,Vectors.dense(0.853)),</strong></span>
<span class="strong"><strong>LabeledPoint(1.0,Vectors.dense(0.938)),</strong></span>
<span class="strong"><strong>LabeledPoint(1.0,Vectors.dense(1.036)),</strong></span>
<span class="strong"><strong>LabeledPoint(1.0,Vectors.dense(1.045)))</strong></span>
</pre></div></li><li class="listitem">Create an RDD of the preceding data:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val spiderRDD = sc.parallelize(points)</strong></span>
</pre></div></li><li class="listitem">Train<a id="id436" class="indexterm"/> a model using<a id="id437" class="indexterm"/> this data (intercept is the value when all predictors are zero):<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val lr = new LogisticRegressionWithLBFGS().setIntercept(true)</strong></span>
<span class="strong"><strong>scala&gt; val model = lr.run(spiderRDD)</strong></span>
</pre></div></li><li class="listitem">Predict the presence of spiders for grain size <code class="literal">0.938</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val predict = model.predict(Vectors.dense(0.938))</strong></span>
</pre></div></li></ol></div></div></div>
<div class="section" title="Doing binary classification using SVM"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec58"/>Doing binary classification using SVM</h1></div></div></div><p>Classification is <a id="id438" class="indexterm"/>a technique to put data into different <a id="id439" class="indexterm"/>classes based on its utility. For example, an e-commerce company can apply two labels "will buy" or "will not buy" to potential visitors.</p><p>This classification is done by providing some already labeled data to machine learning algorithms <a id="id440" class="indexterm"/>called <span class="strong"><strong>training data</strong></span>. The challenge is how to mark the boundary between two classes. Let's take a simple example as shown in the following figure:</p><div class="mediaobject"><img src="graphics/3056_08_26.jpg" alt="Doing binary classification using SVM"/></div><p>In the preceding case, we designated gray and black to the "will not buy" and "will buy" labels. Here, drawing <a id="id441" class="indexterm"/>a line between the two classes<a id="id442" class="indexterm"/> is as easy as follows:</p><div class="mediaobject"><img src="graphics/3056_08_27.jpg" alt="Doing binary classification using SVM"/></div><p>Is this the best we can do? Not really, let's try to do a better job. The black classifier is not really equidistant from the "will buy" and "will not buy" carts. Let's make a better attempt like the following:</p><div class="mediaobject"><img src="graphics/3056_08_28.jpg" alt="Doing binary classification using SVM"/></div><p>Now this <a id="id443" class="indexterm"/>is looking good. This in fact is what the <a id="id444" class="indexterm"/>SVM algorithm does. You can see in the preceding diagram that in fact there are only three carts that decide the slope of the line: two black carts above the line, and one gray cart below the line. These carts are called <span class="strong"><strong>support vectors</strong></span><a id="id445" class="indexterm"/> and the rest of the carts, that is, the vectors, are irrelevant.</p><p>Sometimes it's not easy to draw a line and a curve may be needed to separate two classes like the following:</p><div class="mediaobject"><img src="graphics/3056_08_29.jpg" alt="Doing binary classification using SVM"/></div><p>Sometimes <a id="id446" class="indexterm"/>even that is not enough. In that case, we<a id="id447" class="indexterm"/> need more than two dimensions to resolve the problem. Rather than a classified line, what we need is a hyperplane. In fact, whenever data is too cluttered, adding extra dimensions help to find a hyperplane to separate classes. The following diagram illustrates this:</p><div class="mediaobject"><img src="graphics/3056_08_30.jpg" alt="Doing binary classification using SVM"/></div><p>This does not mean that adding extra dimensions is always a good idea. Most of the time, our goal is to reduce dimensions and keep only the relevant dimensions/features. A whole <a id="id448" class="indexterm"/>set of algorithms is dedicated to dimensionality<a id="id449" class="indexterm"/> reduction; we will cover these in later chapters.</p><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec82"/>How to do it…</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">The Spark library comes loaded with sample <code class="literal">libsvm</code> data. We will use this and load the data into HDFS:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hdfs dfs -put /opt/infoobjects/spark/data/mllib/sample_libsvm_data.txt /user/hduser/sample_libsvm_data.txt</strong></span>
</pre></div></li><li class="listitem">Start the Spark shell:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell</strong></span>
</pre></div></li><li class="listitem">Perform the required imports:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.classification.SVMWithSGD</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.regression.LabeledPoint</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.linalg.Vectors</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.util.MLUtils</strong></span>
</pre></div></li><li class="listitem">Load the data as the RDD:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val svmData = MLUtils.loadLibSVMFile(sc,"sample_libsvm_data.txt")</strong></span>
</pre></div></li><li class="listitem">Count the number of records:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; svmData.count</strong></span>
</pre></div></li><li class="listitem">Now let's divide the dataset into half training data and half testing data:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val trainingAndTest = svmData.randomSplit(Array(0.5,0.5))</strong></span>
</pre></div></li><li class="listitem">Assign the <code class="literal">training</code> and <code class="literal">test</code> data:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val trainingData = trainingAndTest(0)</strong></span>
<span class="strong"><strong>scala&gt; val testData = trainingAndTest(1)</strong></span>
</pre></div></li><li class="listitem">Train the algorithm and build the model for 100 iterations (you can try different iterations but you will see that, at a certain point, the results start to converge and that is a good number to choose):<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val model = SVMWithSGD.train(trainingData,100)</strong></span>
</pre></div></li><li class="listitem">Now we can use this model to predict a label for any dataset. Let's predict the label for the first point in the test data:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val label = model.predict(testData.first.features)</strong></span>
</pre></div></li><li class="listitem">Let's <a id="id450" class="indexterm"/>create a tuple that has the first <a id="id451" class="indexterm"/>value as a prediction for test data and a second value actual label, which will help us compute the accuracy of our algorithm:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val predictionsAndLabels = testData.map( r =&gt; (model.predict(r.features),r.label))</strong></span>
</pre></div></li><li class="listitem">You can count how many records have prediction and actual label mismatches:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; predictionsAndLabels.filter(p =&gt; p._1 != p._2).count</strong></span>
</pre></div></li></ol></div></div></div>
<div class="section" title="Doing classification using decision trees"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec59"/>Doing classification using decision trees</h1></div></div></div><p>Decision trees are the most intuitive among machine learning algorithms. We use decision trees in daily life all the time.</p><p>Decision tree algorithms have a lot of useful features:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Easy to understand and interpret</li><li class="listitem" style="list-style-type: disc">Work with both categorical and continuous features</li><li class="listitem" style="list-style-type: disc">Work with missing features</li><li class="listitem" style="list-style-type: disc">Do not require feature scaling</li></ul></div><p>Decision tree<a id="id452" class="indexterm"/> algorithms work in an upside-down <a id="id453" class="indexterm"/>order in which an expression containing a feature is evaluated at every level and that splits the dataset into two categories. We'll help you understand this with the simple example of a dumb charade, which most of us played in college. I guessed an animal and asked my coworker ask me questions to work out my choice. Here's how her questioning went:</p><p>Q1: Is it a big animal?</p><p>A: Yes</p><p>Q2: Does this animal live more than 40 years?</p><p>A: Yes</p><p>Q3: Is this animal an elephant?</p><p>A: Yes</p><p>This is an obviously oversimplified case in which she knew I had postulated an elephant (what else <a id="id454" class="indexterm"/>would you guess in a Big Data world?). Let's <a id="id455" class="indexterm"/>expand this example to include some more animals as in the following figure (grayed boxes are classes):</p><div class="mediaobject"><img src="graphics/3056_08_31.jpg" alt="Doing classification using decision trees"/></div><p>The preceding example is a case of multiclass classification. In this recipe, we are going to focus on binary classification.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec83"/>Getting ready</h2></div></div></div><p>Whenever our son has to take tennis lessons in the morning, the night before the instructor checks the weather reports and decides whether the next morning would be good to play tennis. This recipe will use this example to build a decision tree.</p><p>Let's decide on the features of weather that affect the decision whether to play tennis in the morning or not:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Rain</li><li class="listitem" style="list-style-type: disc">Wind speed</li><li class="listitem" style="list-style-type: disc">Temperature</li></ul></div><p>Let's build a table of the different combinations:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Rain</p>
</th><th style="text-align: left" valign="bottom">
<p>Windy</p>
</th><th style="text-align: left" valign="bottom">
<p>Temperature</p>
</th><th style="text-align: left" valign="bottom">
<p>Play tennis?</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>Hot</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>Normal</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>Cool</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>Hot</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td><td style="text-align: left" valign="top">
<p>Cool</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>Hot</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>Normal</p>
</td><td style="text-align: left" valign="top">
<p>Yes</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td><td style="text-align: left" valign="top">
<p>Cool</p>
</td><td style="text-align: left" valign="top">
<p>No</p>
</td></tr></tbody></table></div><p>Now how<a id="id456" class="indexterm"/> do we build a decision tree? We <a id="id457" class="indexterm"/>can start with one of three features: rain, windy, or temperature. The rule is to start with a feature so that the maximum information gain is possible.</p><p>On a rainy day, as you can see in the table, other features do not matter and there is no play. The same is true for high wind velocity.</p><p>Decision trees, like most other algorithms, take feature values only as double values. So, let's do the mapping:</p><div class="mediaobject"><img src="graphics/3056_08_32.jpg" alt="Getting ready"/></div><p>The positive class is 1.0 and the negative class is 0.0. Let's load the data using the CSV format using the first value as a label:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$vi tennis.csv</strong></span>
<span class="strong"><strong>0.0,1.0,1.0,2.0</strong></span>
<span class="strong"><strong>0.0,1.0,1.0,1.0</strong></span>
<span class="strong"><strong>0.0,1.0,1.0,0.0</strong></span>
<span class="strong"><strong>0.0,0.0,1.0,2.0</strong></span>
<span class="strong"><strong>0.0,0.0,1.0,0.0</strong></span>
<span class="strong"><strong>1.0,0.0,0.0,2.0</strong></span>
<span class="strong"><strong>1.0,0.0,0.0,1.0</strong></span>
<span class="strong"><strong>0.0,0.0,0.0,0.0</strong></span>
</pre></div></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec84"/>How to do it…</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Start the Spark shell:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell</strong></span>
</pre></div></li><li class="listitem">Perform <a id="id458" class="indexterm"/>the required imports:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.tree.DecisionTree</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.regression.LabeledPoint</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.linalg.Vectors</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.tree.configuration.Algo._</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.tree.impurity.Entropy</strong></span>
</pre></div></li><li class="listitem">Load the<a id="id459" class="indexterm"/> file:<div class="informalexample"><pre class="programlisting">scala&gt; val data = sc.textFile("tennis.csv")</pre></div></li><li class="listitem">Parse the data and load it into <code class="literal">LabeledPoint</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val parsedData = data.map {</strong></span>
<span class="strong"><strong>line =&gt;  val parts = line.split(',').map(_.toDouble)</strong></span>
<span class="strong"><strong> LabeledPoint(parts(0), Vectors.dense(parts.tail)) }</strong></span>
</pre></div></li><li class="listitem">Train the algorithm with this data:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val model = DecisionTree.train(parsedData, Classification, Entropy, 3)</strong></span>
</pre></div></li><li class="listitem">Create a vector for no rain, high wind, and a cool temperature:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val v=Vectors.dense(0.0,1.0,0.0)</strong></span>
</pre></div></li><li class="listitem">Predict whether tennis should be played:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; model.predict(v)</strong></span>
</pre></div></li></ol></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec85"/>How it works…</h2></div></div></div><p>Let's draw the decision tree for tennis that we created in this recipe:</p><div class="mediaobject"><img src="graphics/3056_08_33.jpg" alt="How it works…"/></div><p>This model has a depth of three levels. Which attribute to select depends upon how we can maximize<a id="id460" class="indexterm"/> information gain. The way it is <a id="id461" class="indexterm"/>measured is by measuring the purity of the split. Purity means that, whether or not certainty is increasing, then that given dataset will be considered as positive or negative. In this example, this equates to whether the chances of play are increasing or the chances of not playing are increasing.</p><p>Purity is measured using entropy. Entropy is a measure of disorder in a system. In this context, it is easier to understand it as a measure of uncertainty:</p><div class="mediaobject"><img src="graphics/3056_08_34.jpg" alt="How it works…"/></div><p>The highest level of purity is 0 and the lowest is 1. Let's try to determine the purity using the formula.</p><p>When rain is yes, the probability of playing tennis is <span class="emphasis"><em>p+</em></span> is 0/3 = 0. The probability of not playing tennis <span class="emphasis"><em>p_</em></span> is 3/3 = 1:</p><div class="mediaobject"><img src="graphics/3056_08_35.jpg" alt="How it works…"/></div><p>This is a pure set.</p><p>When rain is <a id="id462" class="indexterm"/>a no, the probability of playing<a id="id463" class="indexterm"/> tennis is <span class="emphasis"><em>p+</em></span> is 2/5 = 0.4. The probability of not playing tennis <span class="emphasis"><em>p_</em></span> is 3/5 = 0.6:</p><div class="mediaobject"><img src="graphics/3056_08_36.jpg" alt="How it works…"/></div><p>This is almost an impure set. The most impure would be the case where the probability is 0.5.</p><p>Spark uses three measures to determine impurity:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Gini impurity (classification)</li><li class="listitem" style="list-style-type: disc">Entropy (classification)</li><li class="listitem" style="list-style-type: disc">Variance (regression)</li></ul></div><p>Information gain is the difference between the parent node impurity and the weighted sum of two child node impurities. Let's look at the first split, which partitions data of size eight to two datasets of size three (left) and five (right). Let's call the first split <span class="emphasis"><em>s1</em></span>, the parent node <span class="emphasis"><em>rain</em></span>, the left child <span class="emphasis"><em>no rain</em></span>, and the right child <span class="emphasis"><em>wind</em></span>. So the information gain would be:</p><div class="mediaobject"><img src="graphics/3056_08_37.jpg" alt="How it works…"/></div><p>As we <a id="id464" class="indexterm"/>calculated impurity for <span class="emphasis"><em>no rain</em></span> and <span class="emphasis"><em>wind</em></span> already <a id="id465" class="indexterm"/>for the entropy, let's calculate the entropy for <span class="emphasis"><em>rain</em></span>:</p><div class="mediaobject"><img src="graphics/3056_08_38.jpg" alt="How it works…"/></div><p>Let's calculate the information gain now:</p><div class="mediaobject"><img src="graphics/3056_08_39.jpg" alt="How it works…"/></div><p>So the information gain is 0.2 in the first split. Is this the best we can achieve? Let's see what our algorithm comes up with. First, let's find out the depth of the tree:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; model.depth</strong></span>
<span class="strong"><strong>Int = 2</strong></span>
</pre></div><p>Here, the depth is <code class="literal">2</code> compared to the <code class="literal">3</code> we intuitively built, so this model seems to be better optimized. Let's look at the structure of the tree:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; model.toDebugString</strong></span>
<span class="strong"><strong>String =  "DecisionTreeModel classifier of depth 2 with 5 nodes</strong></span>
<span class="strong"><strong>If (feature 1 &lt;= 0.0)</strong></span>
<span class="strong"><strong>   If (feature 2 &lt;= 0.0)</strong></span>
<span class="strong"><strong>     Predict: 0.0</strong></span>
<span class="strong"><strong>   Else (feature 2 &gt; 0.0)</strong></span>
<span class="strong"><strong>     Predict: 1.0</strong></span>
<span class="strong"><strong>Else (feature 1 &gt; 0.0)</strong></span>
<span class="strong"><strong>    Predict: 0.0</strong></span>
</pre></div><p>Let's <a id="id466" class="indexterm"/>build it visually to get a better understanding:</p><div class="mediaobject"><img src="graphics/3056_08_40.jpg" alt="How it works…"/></div><p>We will<a id="id467" class="indexterm"/> not go into detail here as we already did this with the previous model. We will straightaway calculate the information gain: 0.44</p><p>As you can see in this case, the information gain is 0.44, which is more than double the first model.</p><p>If you look at the second level nodes, the impurity is zero. In this case, it is great as we got it at a depth of 2. Image a situation in which the depth is 50. In that case, the decision tree would work <a id="id468" class="indexterm"/>well for training data and would do badly for test data. This situation is called <span class="strong"><strong>overfitting</strong></span>.</p><p>One solution<a id="id469" class="indexterm"/> to avoid overfitting is pruning. You <a id="id470" class="indexterm"/>divide your training data into two sets: the training set and validation set. You train the model using the training set. Now you test with the model against the validation set by slowly removing the left nodes. If removing the leaf node (which is mostly a singleton—that is, it contains only one data point) improves the performance of the model, this leaf node is pruned from the model.</p></div></div>
<div class="section" title="Doing classification using Random Forests"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec60"/>Doing classification using Random Forests</h1></div></div></div><p>Sometimes one decision tree is not enough, so a set of decision trees is used to produce more powerful models. These<a id="id471" class="indexterm"/> are called <span class="strong"><strong>ensemble learning algorithms</strong></span>. Ensemble learning algorithms are not limited to using decision trees as base models.</p><p>The most<a id="id472" class="indexterm"/> popular among the ensemble<a id="id473" class="indexterm"/> learning algorithms is Random Forest. In Random Forest, rather than growing one single tree, <span class="emphasis"><em>K</em></span> trees are grown. Every tree is given a random subset <span class="emphasis"><em>S</em></span> of training data. To add a twist to it, every tree only uses a subset of features. When it comes to making predictions, a majority vote is done on the trees and that becomes the prediction.</p><p>Let's explain this with an example. The goal is to make a prediction for a given person about whether he/she has good credit or bad credit.</p><p>To do this, we will provide labeled training data—that is, in this case, a person with features and labels whether he/she has good credit or bad credit. Now we do not want to create feature bias so we will provide a randomly selected set of features. There is another reason to provide a randomly selected subset of features and that is because most real-world data has hundreds if not thousands of features. Text classification algorithms, for example, typically have 50k-100k features.</p><p>In this case, to add flavor to the story we are not going to provide features, but we will ask different people why they think a person has good or bad credit. Now by definition, different people are exposed to different features (sometimes overlapping) of a person, which gives us the same functionality as randomly selected features.</p><p>Our first example is Jack who carries a label "bad credit." We will start with Joey who works at Jack's favorite bar, the Elephant Bar. The only way a person can deduce why a given label was given is by asking yes/no questions. Let's see what Joey says:</p><p>Q1: Does Jack tip well? (Feature: generosity)</p><p>A: No</p><p>Q2: Does Jack spend at least $60 per visit? (Feature: spendthrift)</p><p>A: Yes</p><p>Q3: Does he tend to get into bar fights even at the smallest provocation? (Feature: volatile)</p><p>A: Yes</p><p>That explains why Jack has bad credit.</p><p>We now ask Jack's girlfriend, Stacey:</p><p>Q1: When <a id="id474" class="indexterm"/>we hangout, does Jack always <a id="id475" class="indexterm"/>cover the bill? (Feature: generosity)</p><p>A: No</p><p>Q2: Has Jack paid me back the $500 he owes me? (Feature: responsibility)</p><p>A: No</p><p>Q3: Does he overspend sometimes just to show off? (Feature: spendthrift)</p><p>A: Yes</p><p>That explains why Jack has bad credit.</p><p>We now ask Jack's best friend George:</p><p>Q1: When Jack and I hang out at my apartment, does he clean up after himself? (Feature: organized)</p><p>A: No</p><p>Q2: Did Jack arrive empty-handed during my Super Bowl potluck? (Feature: care)</p><p>A: Yes</p><p>Q3: Has he used the "I forgot my wallet at home" excuse for me to cover his tab at restaurants? (Feature: responsibility)</p><p>A: Yes</p><p>That explains why Jack has bad credit.</p><p>Now we talk about Jessica who has good credit. Let's ask Stacey who happens to be Jessica's sister:</p><p>Q1: Whenever I run short of money, does Jessica offer to help? (Feature: generosity)</p><p>A: Yes</p><p>Q2: Does Jessica pay her bills on time? (Feature: responsibility)</p><p>A: Yes</p><p>Q3: Does Jessica offer to babysit my child? (Feature: care)</p><p>A: Yes</p><p>That explains why Jessica has good credit.</p><p>Now we ask George who happens to be her husband:</p><p>Q1: Does <a id="id476" class="indexterm"/>Jessica keep the house tidy? (Feature: organized)</p><p>A: Yes</p><p>Q2: Does she <a id="id477" class="indexterm"/>expect expensive gifts? (Feature: spendthrift)</p><p>A: No</p><p>Q3: Does she get upset when you forget to mow the lawn? (Feature: volatile)</p><p>A: No</p><p>That explains why Jessica has good credit.</p><p>Now let's ask Joey, the bartender at the Elephant Bar:</p><p>Q1: Whenever she comes to the bar with friends, is she mostly the designated driver? (Feature: responsible)</p><p>A: Yes</p><p>Q2: Does she always take leftovers home? (Feature: spendthrift)</p><p>A: Yes</p><p>Q3: Does she tip well? (Feature: generosity)</p><p>A: Yes</p><p>The way Random Forest works is that it does random selection on two levels:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">A subset of the data</li><li class="listitem" style="list-style-type: disc">A subset of features to split that data</li></ul></div><p>Both these subsets can overlap.</p><p>In our example, we have six features and we are going to assign three features to each tree. This way, there is a good chance we will have an overlap.</p><p>Let's add eight more people to our training dataset:</p><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Names</p>
</th><th style="text-align: left" valign="bottom">
<p>Label</p>
</th><th style="text-align: left" valign="bottom">
<p>Generosity</p>
</th><th style="text-align: left" valign="bottom">
<p>Responsibility</p>
</th><th style="text-align: left" valign="bottom">
<p>Care</p>
</th><th style="text-align: left" valign="bottom">
<p>Organization</p>
</th><th style="text-align: left" valign="bottom">
<p>Spendthrift</p>
</th><th style="text-align: left" valign="bottom">
<p>Volatile</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>Jack</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Jessica</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Jenny</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Rick</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Pat</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Jeb</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Jay</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Nat</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Ron</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>Mat</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td></tr></tbody></table></div><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec86"/>Getting ready</h2></div></div></div><p>Let's <a id="id478" class="indexterm"/>put the data we created into the <code class="literal">libsvm</code> format<a id="id479" class="indexterm"/> in the following file:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>rf_libsvm_data.txt</strong></span>
<span class="strong"><strong>0 5:1 6:1</strong></span>
<span class="strong"><strong>1 1:1 2:1 3:1 4:1</strong></span>
<span class="strong"><strong>0 3:1 5:1 6:1</strong></span>
<span class="strong"><strong>1 1:1 2:1 4:1</strong></span>
<span class="strong"><strong>0 5:1 6:1</strong></span>
<span class="strong"><strong>1 1:1 2:1 3:1 4:1</strong></span>
<span class="strong"><strong>0 1:1 5:1 6:1</strong></span>
<span class="strong"><strong>1 2:1 3:1 4:1</strong></span>
<span class="strong"><strong>0 1:1 5:1 6:1</strong></span>
</pre></div><p>Now upload it to HDFS:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hdfs dfs -put rf_libsvm_data.txt</strong></span>
</pre></div></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec87"/>How to do it…</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Start the Spark shell:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell</strong></span>
</pre></div></li><li class="listitem">Perform the required imports:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.tree.RandomForest</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.tree.configuration.Strategy</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.util.MLUtils</strong></span>
</pre></div></li><li class="listitem">Load and parse the data:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val data =</strong></span>
<span class="strong"><strong>  MLUtils.loadLibSVMFile(sc, "rf_libsvm_data.txt")</strong></span>
</pre></div></li><li class="listitem">Split the data into the <code class="literal">training</code> and <code class="literal">test</code> datasets:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val splits = data.randomSplit(Array(0.7, 0.3))</strong></span>
<span class="strong"><strong>scala&gt; val (trainingData, testData) = (splits(0), splits(1))</strong></span>
</pre></div></li><li class="listitem">Create <a id="id480" class="indexterm"/>a classification as a tree <a id="id481" class="indexterm"/>strategy (Random Forest also supports regression):<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val treeStrategy = Strategy.defaultStrategy("Classification")</strong></span>
</pre></div></li><li class="listitem">Train the model:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val model = RandomForest.trainClassifier(trainingData,</strong></span>
<span class="strong"><strong>  treeStrategy, numTrees=3, featureSubsetStrategy="auto", seed = 12345)</strong></span>
</pre></div></li><li class="listitem">Evaluate the model on test instances and compute the test error:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val testErr = testData.map { point =&gt;</strong></span>
<span class="strong"><strong>  val prediction = model.predict(point.features)</strong></span>
<span class="strong"><strong>  if (point.label == prediction) 1.0 else 0.0</strong></span>
<span class="strong"><strong>}.mean()</strong></span>
<span class="strong"><strong>scala&gt; println("Test Error = " + testErr)</strong></span>
</pre></div></li><li class="listitem">Check the model:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; println("Learned Random Forest:n" + model.toDebugString)</strong></span>
<span class="strong"><strong>Learned Random Forest:nTreeEnsembleModel classifier with 3 trees</strong></span>
<span class="strong"><strong>    Tree 0:</strong></span>
<span class="strong"><strong>  If (feature 5 &lt;= 0.0)</strong></span>
<span class="strong"><strong>    Predict: 1.0</strong></span>
<span class="strong"><strong>  Else (feature 5 &gt; 0.0)</strong></span>
<span class="strong"><strong>    Predict: 0.0</strong></span>
<span class="strong"><strong>    Tree 1:</strong></span>
<span class="strong"><strong>      If (feature 3 &lt;= 0.0)</strong></span>
<span class="strong"><strong>       Predict: 0.0</strong></span>
<span class="strong"><strong>     Else (feature 3 &gt; 0.0)</strong></span>
<span class="strong"><strong>      Predict: 1.0</strong></span>
<span class="strong"><strong>   Tree 2:</strong></span>
<span class="strong"><strong>      If (feature 0 &lt;= 0.0)</strong></span>
<span class="strong"><strong>       Predict: 0.0</strong></span>
<span class="strong"><strong>     Else (feature 0 &gt; 0.0)</strong></span>
<span class="strong"><strong>      Predict: 1.0</strong></span>
</pre></div></li></ol></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec88"/>How it works…</h2></div></div></div><p>As you <a id="id482" class="indexterm"/>can see in such a small example, three<a id="id483" class="indexterm"/> trees are using different features. In real-world use cases with thousands of features and training data, this would not happen, but most of the trees would differ in how they look at features and the vote of the majority will win. Please remember that, in the case of regression, averaging is done over trees to get a final value.</p></div></div>
<div class="section" title="Doing classification using Gradient Boosted Trees"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec61"/>Doing classification using Gradient Boosted Trees</h1></div></div></div><p>Another <a id="id484" class="indexterm"/>ensemble learning algorithm is <span class="strong"><strong>Gradient Boosted Trees</strong></span> (<span class="strong"><strong>GBTs</strong></span>). GBTs train one tree at a time, where each new tree improves<a id="id485" class="indexterm"/> upon the shortcomings <a id="id486" class="indexterm"/>of previously trained trees.</p><p>As GBTs train one tree at a time, they can take longer than Random Forest.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec89"/>Getting ready</h2></div></div></div><p>We are going to use the same data we used in the previous recipe.</p></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec90"/>How to do it…</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Start the Spark shell:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell</strong></span>
</pre></div></li><li class="listitem">Perform the required imports:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.tree.GradientBoostedTrees</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.tree.configuration.BoostingStrategy</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.util.MLUtils</strong></span>
</pre></div></li><li class="listitem">Load and parse the data:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val data =</strong></span>
<span class="strong"><strong>  MLUtils.loadLibSVMFile(sc, "rf_libsvm_data.txt")</strong></span>
</pre></div></li><li class="listitem">Split the data into <code class="literal">training</code> and <code class="literal">test</code> datasets:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val splits = data.randomSplit(Array(0.7, 0.3))</strong></span>
<span class="strong"><strong>scala&gt; val (trainingData, testData) = (splits(0), splits(1))</strong></span>
</pre></div></li><li class="listitem">Create <a id="id487" class="indexterm"/>a classification<a id="id488" class="indexterm"/> as a boosting strategy and set the number of iterations to <code class="literal">3</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val boostingStrategy =</strong></span>
<span class="strong"><strong>  BoostingStrategy.defaultParams("Classification")</strong></span>
<span class="strong"><strong>scala&gt; boostingStrategy.numIterations = 3</strong></span>
</pre></div></li><li class="listitem">Train the model:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val model = GradientBoostedTrees.train(trainingData, boostingStrategy)</strong></span>
</pre></div></li><li class="listitem">Evaluate the model on the test instances and compute the test error:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val testErr = testData.map { point =&gt;</strong></span>
<span class="strong"><strong>  val prediction = model.predict(point.features)</strong></span>
<span class="strong"><strong>  if (point.label == prediction) 1.0 else 0.0</strong></span>
<span class="strong"><strong>}.mean()</strong></span>
<span class="strong"><strong>scala&gt; println("Test Error = " + testErr)</strong></span>
</pre></div></li><li class="listitem">Check the model:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; println("Learned Random Forest:n" + model.toDebugString)</strong></span>
</pre></div></li></ol></div><p>In this case, the accuracy of the model is 0.9, which is less than what we got in the case of Random Forest.</p></div></div>
<div class="section" title="Doing classification with Na&#xEF;ve Bayes"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec62"/>Doing classification with Naïve Bayes</h1></div></div></div><p>Let's consider building an e-mail spam filter using machine learning. Here we are interested in<a id="id489" class="indexterm"/> two classes: spam for unsolicited <a id="id490" class="indexterm"/>messages and non-spam for regular emails:</p><div class="mediaobject"><img src="graphics/3056_08_42.jpg" alt="Doing classification with Naïve Bayes"/></div><p>The first challenge is that, when given an e-mail, how do we represent it as feature vector <span class="emphasis"><em>x</em></span>. An e-mail is just bunch of text or a collection of words (therefore, this problem domain falls into a broader category called <span class="strong"><strong>text classification</strong></span>). Let's represent an e-mail with a feature vector<a id="id491" class="indexterm"/> with the length equal to the size of the dictionary. If a given word in a dictionary appears in an e-mail, the value will be 1; otherwise 0. Let's build a vector representing e-mail with the content <span class="emphasis"><em>online pharmacy sale</em></span>:</p><div class="mediaobject"><img src="graphics/3056_08_43.jpg" alt="Doing classification with Naïve Bayes"/></div><p>The <a id="id492" class="indexterm"/>dictionary of words in this feature vector is<a id="id493" class="indexterm"/> called <span class="emphasis"><em>vocabulary</em></span> and the dimensions of the vector are the same as the size of vocabulary. If the vocabulary size is 10,000, the possible values in this feature vector will be 210,000.</p><p>Our goal is to model the probability of <span class="emphasis"><em>x</em></span> given <span class="emphasis"><em>y</em></span>. To model <span class="emphasis"><em>P(x|y)</em></span>, we will make a strong assumption, and that assumption is that <span class="emphasis"><em>x</em></span>'s are conditionally independent. This assumption is <a id="id494" class="indexterm"/>called the <span class="strong"><strong>Naïve Bayes assumption</strong></span> and the <a id="id495" class="indexterm"/>algorithm based on this assumption is called the <span class="strong"><strong>Naïve Bayes classifier</strong></span>.</p><p>For example, for <span class="emphasis"><em>y =1</em></span>, which means spam, the probability of "online" appearing and "pharmacy appearing" are independent. This is a strong assumption that has nothing to do with reality but works out really well when it comes to getting good predictions.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec91"/>Getting ready</h2></div></div></div><p>Spark comes bundled with a sample dataset to use with Naïve Bayes. Let's load this dataset to HDFS:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ hdfs dfs -put /opt/infoobjects/spark/data/mllib/sample_naive_bayes_data.txt</strong></span>
<span class="strong"><strong> sample_naive_bayes_data.txt</strong></span>
</pre></div></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec92"/>How to do it…</h2></div></div></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Start the Spark shell:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ spark-shell</strong></span>
</pre></div></li><li class="listitem">Perform the required imports:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.classification.NaiveBayes</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.linalg.Vectors</strong></span>
<span class="strong"><strong>scala&gt; import org.apache.spark.mllib.regression.LabeledPoint</strong></span>
</pre></div></li><li class="listitem">Load the data into RDD:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val data = sc.textFile("sample_naive_bayes_data.txt")</strong></span>
</pre></div></li><li class="listitem">Parse the<a id="id496" class="indexterm"/> data into <code class="literal">LabeledPoint</code>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val parsedData = data.map { line =&gt;</strong></span>
<span class="strong"><strong>  val parts = line.split(',')</strong></span>
<span class="strong"><strong>  LabeledPoint(parts(0).toDouble, Vectors.dense(parts(1).split(' ').map(_.toDouble)))</strong></span>
<span class="strong"><strong>}</strong></span>
</pre></div></li><li class="listitem">Split the <a id="id497" class="indexterm"/>data half and half into the <code class="literal">training</code> and <code class="literal">test</code> datasets:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>scala&gt; val splits = parsedData.randomSplit(Array(0.5, 0.5), seed = 11L)</strong></span>
<span class="strong"><strong>scala&gt; val training = splits(0)</strong></span>
<span class="strong"><strong>scala&gt; val test = splits(1)</strong></span>
</pre></div></li><li class="listitem">Train the model with the <code class="literal">training</code> dataset:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>val model = NaiveBayes.train(training, lambda = 1.0)</strong></span>
</pre></div></li><li class="listitem">Predict the label of the <code class="literal">test</code> dataset:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>val predictionAndLabel = test.map(p =&gt; (model.predict(p.features), p.label))</strong></span>
</pre></div></li></ol></div></div></div></body></html>