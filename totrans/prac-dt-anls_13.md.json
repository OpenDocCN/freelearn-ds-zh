["```py\nIn[]: import nltk\n```", "```py\nIn[]: nltk.download('brown')\n```", "```py\nIn[]: from nltk.corpus import brown\n```", "```py\nIn[]: brown.words()\n```", "```py\nIn[]: count_of_words = len(brown.words())\n```", "```py\nIn[]: print('Count of all the words found the Brown Corpus =',format(count_of_words,',d'))\n```", "```py\nIn[]: nltk.download('punkt')\n```", "```py\nIn[]: input_sentence = \"Seth and Becca love to run down to the playground when the weather is nice.\"\n```", "```py\nIn[]: nltk.word_tokenize(input_sentence)\n```", "```py\nIn[]: from nltk.tokenize import sent_tokenize\n```", "```py\nIn[]: input_data = \"Seth and Becca love the playground.  When it sunny, they head down there to play.\"\n```", "```py\nIn[]: print(sent_tokenize(input_data))\n```", "```py\nIn[]: from nltk.probability import FreqDist\n```", "```py\nIn[]: input_data = FreqDist(brown.words())\nprint(input_data)\n```", "```py\nIn[]: input_data.most_common(10)\n```", "```py\nIn[]: from nltk.stem import PorterStemmer\n```", "```py\nIn[]: my_word_stemmer = PorterStemmer()\n```", "```py\nIn[]: my_word_stemmer.stem('fishing')\n```", "```py\nIn[]: nltk.download('wordnet')\n```", "```py\nIn[]: from nltk.stem import WordNetLemmatizer\nmy_word_lemmatizer = WordNetLemmatizer()\n```", "```py\nIn[]: my_word_lemmatizer.lemmatize('fishing')\n```", "```py\nIn[]: my_list_of_words = brown.words()[:10]\n```", "```py\nIn[]: for x in my_list_of_words :    \n    print('word =', x, ': stem =', my_word_stemmer.stem(x), ': lemma =', my_word_lemmatizer.lemmatize(x))\n```", "```py\nIn[]: nltk.download('stopwords')\n```", "```py\nIn[]: from nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n```", "```py\nIn[]: input_data = “Seth and Becca love the playground.  When it's sunny, they head down there to play.”\n```", "```py\nIn[]: stop_words = set(stopwords.words('english'))word_tokens = word_tokenize(input_data)\n```", "```py\nIn[]: input_data_cleaned = [x for x in word_tokens if not x in stop_words]\ninput_data_cleaned = []\n\nfor x in word_tokens:    \n    if x not in stop_words:        \n        input_data_cleaned.append(x)       \nprint(word_tokens)\nprint(input_data_cleaned)\n```"]