- en: Chapter 13. High-Performance Computing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, you learned about a number of built-in functions and
    various packages tailored for data manipulation. Although these packages rely
    on different techniques and may be built under a different philosophy, they all
    make data filtering and aggregating much easier.
  prefs: []
  type: TYPE_NORMAL
- en: However, data processing is more than simple filtering and aggregating. Sometimes,
    it involves simulation and other computationintensive tasks. Compared to high-performance
    programming languages such as C and C++, R is much slower due to its dynamic design
    and the current implementation that prioritizes stability, ease, and power in
    statistical analysis and visualization over performance and language features.
    However, well-written R code can still be fast enough for most purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, I''ll demonstrate the following techniques to help you write
    R code with high performance:'
  prefs: []
  type: TYPE_NORMAL
- en: Measuring code performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Profiling code to find bottleneck
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using built-in functions and vectorization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using multiple cores by parallel computing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing C++ with Rcpp and related packages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding code performance issues
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From the very beginning, R is designed for statistical computing and data visualization
    and is widely used by academia and industry. For most data analysis purposes,
    correctness is more important than performance. In other words, getting a correct
    result in 1 minute should be better than getting an incorrect one in 20 seconds.
    A result that is three times faster is not automatically three times more valid
    than a slow but correct result. Therefore, performance should not be a concern
    before you are sure about the correctness of your code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s assume that you are 100 percent sure that your code is correct but it
    runs a bit slowly. Now, is it necessary for you to optimize the code so that it
    can run faster. Well, it depends. Before making a decision, it is helpful to divide
    the time of problem solving into three parts: time of development, execution,
    and future maintenance.'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we have been working on a problem for an hour. Since we didn't take
    performance into account at the beginning, the code does not run very fast. It
    takes us 50 minutes to think about the problem and implement the solution. Then,
    it takes 1 minute to run and produce an answer. Since the code aligns well with
    the problem and looks straightforward, future improvements can be easily integrated
    into the solution, so it takes us less time to maintain.
  prefs: []
  type: TYPE_NORMAL
- en: Then, suppose another developer has been working on the same problem but attempts
    to write an extremely high-performance code at the beginning. It takes time to
    work out a solution to the problem, but takes much more time to optimize the structure
    of the code so that it can run faster. It may take two hours to think of and implement
    a high-performance solution. Then, it takes 0.1 second to run and produce an answer.
    Since the code is particularly optimized to squeeze the hardware, it is probably
    not flexible for future refinement, especially when the problem is updated, which
    would cost more time to maintain.
  prefs: []
  type: TYPE_NORMAL
- en: The second developer can happily claim that her code has 600 times the performance
    of our code, but it may not be worth doing so because it may cost much more human
    time. In many cases, human time is more expensive than computer time.
  prefs: []
  type: TYPE_NORMAL
- en: However, if the code is frequently used, say, if billions of iterations are
    required, a small improvement of performance of each iteration can help save a
    large amount of time. In this case, code performance really matters.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take an example of a simple algorithm that produces a numeric vector of
    cumulative sum, that is, each element of the output vector is the sum of all previous
    elements of the input vector. The code will be examined in different contexts
    in the discussion that follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although R provides a built-in function, `cumsum`, to do this, we will implement
    an R version at the moment to help understand performance issues. The algorithm
    is easy to implement as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The algorithm only uses a `for` loop to accumulate each element of the input
    vector `x` into `sum_x`. In each iteration, it appends `sum_x` to the output vector
    `y`. We can rewrite the algorithm as the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'An alternative implementation is to use index to access the input vector `x`
    and access/modify the output vector `y`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We know that R provides a built-in function `cumsum()` to do exactly the same
    thing. The two preceding implementations should yield exactly the same results
    as `cumsum()`. Here, we will generate some random numbers and check whether they
    are consistent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, `all.equal()` checks whether all corresponding elements
    of two vectors are equal. From the results, we are sure that `my_cumsum1()`, `my_cumsum2()`
    and `cumsum()` are consistent. In the next section, we'll measure the time required
    for each version of `cumsum`.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring code performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although the three functions will output the same results given the same input,
    their performance difference can be quite obvious. To reveal the difference in
    performance, we need tools to measure the execution time of code. The simplest
    one is `system.time()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To measure the execution time of any expression, we just wrap the code with
    the function. Here, we will measure how much time it takes by `my_cumsum1()` to
    compute over a numeric vector of 100 elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The timer results in three columns: `user`, `system`, and `elapsed`. It is
    the user time that we should pay more attention to. It measures the CPU time charged
    for executing the code. For more details, run `?proc.time` and see the difference
    between these measures.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The results suggest that the code simply runs too fast to measure. We can try
    timing `my_cumsum2()`, and the results are mostly the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The same thing happens with the built-in function `cumsum()` too:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The timing does not really work because the input is too small. Now, we will
    generate a vector of `1000` numbers and do it again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Now, we are sure that `my_cumsum1()` and `my_cumsum2()` indeed take some time
    to compute the results but show no remarkable contrast. However, `cumsum()` is
    still too fast to measure.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will again use a larger input for all three functions and see whether their
    performance difference can be revealed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is quite clear: `my_cumsum1()` looks more than 10 times slower than
    `my_cumsum2()`, and `cumsum()` is still way too fast than both our implementations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the performance difference may not be constant, especially when we
    provide even larger inputs as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding results make quite an astonishing contrast: `my_cumsum1()` can
    be 200 times slower than `my_cumsum2()` when the length of input vector is at
    100,000 level. The `cumsum()` function is consistently super fast in all previous
    results.'
  prefs: []
  type: TYPE_NORMAL
- en: The `system.time()` function can help measure the execution time of a code chunk,
    but it is not very accurate. On the one hand, each time, the measure can result
    in different values so that we should repeat the timing for enough times to make
    a valid comparison. On the other hand, the resolution of the timer may not be
    high enough to address the real difference in performance of the code of interest.
  prefs: []
  type: TYPE_NORMAL
- en: 'A package named `microbenchmark` serves as a more accurate solution to comparing
    the performance of different expressions. To install the package, run the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'When the package is ready, we will load the package and call `microbenchmark()`
    to directly compare the performance of the three functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Note that `microbenchmark()`, by default, runs each expression 100 times so
    that it can provide more quantiles of the execution time. Maybe to your surprise,
    `my_cumsum1()` is a bit faster than `my_cumsum2()` when the input vector has 100
    elements. Also, note that the unit of the time numbers is nanoseconds (1 second
    is 1,000,000,000 nanoseconds).
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we will try an input of `1000` numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Now, `my_cumsum2()` gets a bit faster than `my_cumsum1()`, but both are way
    slower than the built-in `cumsum()`. Note that the unit becomes microseconds now.
  prefs: []
  type: TYPE_NORMAL
- en: 'For input of `5000` numbers, the performance difference between `my_cumsum1()`
    and `my_cumsum2()` gets even greater:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The same thing happens with an input of `10000` elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In all previous benchmarks, the performance of `cumsum()` looks very stable
    and does not increase significantly as the length of input increases.
  prefs: []
  type: TYPE_NORMAL
- en: 'To better understand the performance dynamics of the three functions, we will
    create the following function to visualize how they perform, provided an input
    of different lengths:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The logic of the function is straightforward: `ns` is a vector of all lengths
    of input vectors we want to test with these functions. Note that `microbenchmark()`
    returns in a data frame of all tests results, and `summary(microbenchmark())`
    returns the summary table we saw previously. We tag each summary with `n`, stack
    all benchmark results, and use the `ggplot2` package to visualize the results.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will do the benchmarking from `100` to `3000` elements of step `100`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will create a plot to show contrast between the performance of the
    three functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following benchmarks of the three versions of `cumsum` we
    intend to compare:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Measuring code performance](img/image_13_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding chart, we put together the results of all three functions.
    The dots indicate the median, and the error bar shows the 75^(th) and 25^(th)
    quantile.
  prefs: []
  type: TYPE_NORMAL
- en: It is very clear that the performance of `my_cumsum1()` decreases faster for
    longer input, the performance of `my_cumsum2()` almost decreases linearly as the
    input gets longer, while `cumsum(x)` is extremely fast and its performance does
    not seem to decay significantly as the input gets longer.
  prefs: []
  type: TYPE_NORMAL
- en: 'For small input, `my_cumsum1()` can be faster than `my_cumsum2()`, as we demonstrated
    earlier. We can do a benchmarking that focuses more on small input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This time, we will limit the length of input vector from `2` to `500` of stop
    `10`. Since the functions will be executed almost twice the number of times than
    the previous benchmarking, to keep the total execution time down, we will reduce
    `times` from the default `100` to `50`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The following graphics illustrates the performance difference at smaller inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Measuring code performance](img/image_13_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: From the chart, we can see that for small input of less than around `400` numbers,
    `my_cumsum1()` is faster than `my_cumsum2()`. The performance of `my_cumsum1()`
    decays much faster than `my_cumsum2()` as the input gets more elements.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dynamics of performance ranking can be better illustrated by a benchmarking
    of input from `10` to `800` elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The plot generated is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Measuring code performance](img/image_13_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In conclusion, a small difference in implementation may result in big performance
    gaps. For a small input, the gap is usually not obvious, but when the input gets
    larger, the performance difference can be very significant and thus should not
    be ignored. To compare the performance of multiple expressions, we can use `microbenchmark`
    instead of `system.time()` to get more accurate and more useful results.
  prefs: []
  type: TYPE_NORMAL
- en: Profiling code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, you learned how to use `microbenchmark()` to benchmark
    expressions. This can be useful when we have several alternative solutions to
    a problem and want to see which has better performance and when we optimize an
    expression and want to see whether the performance actually gets better than the
    original code.
  prefs: []
  type: TYPE_NORMAL
- en: However, it is usually the case that, when we feel the code is slow, it is not
    easy to locate the expression that contributes most to slowing down the entire
    program. Such an expression is called a "performance bottleneck." To improve code
    performance, it is best to resolve the bottleneck first.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, R provides profiling tools to help us find the bottleneck, that
    is, the code that runs most slowly, which should be the top focus for improving
    code performance.
  prefs: []
  type: TYPE_NORMAL
- en: Profiling code with Rprof
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: R provides a built-in function, `Rprof()`, for code profiling. When profiling
    starts, a sampling procedure is running with all subsequent code until the profiling
    is ended. The sampling basically looks at which function R is executing every
    20 milliseconds by default. In this way, if a function is very slow, it is likely
    that most of the execution time is spent on that function call.
  prefs: []
  type: TYPE_NORMAL
- en: The sampling approach may not produce very accurate results, but it serves our
    purpose in most cases. In the following example, we will use `Rprof()` to profile
    the code in which we call `my_cumsum1()` and try to find which part slows down
    the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The way of using `Rprof()` is very simple: call `Rprof()` to start profiling,
    run the code you want to profile, call `Rprof(NULL)` to stop profiling, and finally
    call `summaryRprof()` to see the profiling summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Note that we used `tempfile()` to create a temporary file to store profiling
    data. If we don't supply such a file to `Rprof()`, it will automatically create
    `Rprof.out` in the current working directory. The default also applies to `summaryRprof()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The profiling results summarize the profiling data into a readable format:
    `$by.self` sorts the timing by `self.time`, while `$by.total` sorts by `total.time`.
    More specifically, the `self.time` of a function is the time spent executing code
    in the function only, and the `total.time` of a function is the total execution
    time of the function.'
  prefs: []
  type: TYPE_NORMAL
- en: To figure out which part slows down the function, we should pay more attention
    to `self.time` because it addresses the independent time of execution of each
    function.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding profiling results show that `c` takes up a major part of the execution
    time, that is, `y <- c(y, sum_x)` contributes most to slowing down the function.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can do the same thing to `my_cumsum2()`. The profiling results suggest that
    most time is spent on `my_cumsum2()`, but that is normal because that''s the only
    thing we do in the code. No particular function in `my_cumsum2()` takes up major
    part of time to execute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: In a practical situation, the code we want to profile is usually complicated
    enough. It may involve many different functions. Such a profiling summary can
    be less helpful if we only see the timing of each function it tracks. Fortunately,
    `Rprof()` supports line profiling, that is, it can tell us the timing of each
    line of code when we specify `line.profiling = TRUE` and use `source(..., keep.source
    = TRUE)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will create a script file at `code/my_cumsum1.R` with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will profile this script file with `Rprof()` and `source()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This time, it no longer shows function names but line numbers in the script
    file. We can easily locate the lines that cost most time by looking at the top
    rows in `$by.self`. The `my_cumsum1.R#6` file refers to `y <- c(y, sum_x)`, which
    is consistent with the previous profiling results.
  prefs: []
  type: TYPE_NORMAL
- en: Profiling code with profvis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `Rprof()` function provides useful information to help us find which part
    of the code is too slow so that we can improve the implementation. RStudio also
    released an enhanced profiling tool, `profvis` ([https://rstudio.github.io/profvis/](https://rstudio.github.io/profvis/)),
    which provides interactive visualization for profiling R code.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is an R package and has been integrated into RStudio. To install the package,
    run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'As soon as the package is installed, we can use `profvis` to profile an expression
    and visualize the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'When the profiling is finished, a new tab will appear with an interactive user
    interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Profiling code with profvis](img/image_13_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The upper pane shows the code, memory usage, and timing, whereas the lower pane
    shows the timeline of function calling as well as when garbage collection occurs.
    We can click and select a certain line of code and see the timeline of function
    execution. Compared with the results produced by `summaryRprof()`, this interactive
    visualization provides much richer information that enables us to know more about
    how the code is executed over a long time. In this way, we can easily identify
    the slow code and some patterns that may induce problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can do exactly the same thing with `my_cumsum2()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This time, the profiling results in the following statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Profiling code with profvis](img/image_13_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We can easily identify which part takes the most time and decide whether it
    is acceptable. In all code, there is always a part that takes most time, but this
    does not indicate that it is too slow. If the code serves our purpose and the
    performance is acceptable, then there may not be a need to optimize the performance
    at the risk of modifying the code into an incorrect version.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding why code can be slow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous sections, you learned about the tools for timing and profiling
    code. To solve the same problem, one function can be blazing fast, and the other
    can be ridiculously slow. It is helpful to understand what can make code slow.
  prefs: []
  type: TYPE_NORMAL
- en: First, R is a dynamic programming language. By design, it provides highly flexible
    data structures and code-execution mechanisms. Therefore, it is hard for the code
    interpreter to know in advance how to deal with the next function call until it
    is actually called. This is not the case for strong-typed static programming languages
    such as C and C++. Many things are determined at compile time rather than runtime,
    so the program knows a lot ahead of time, and optimization can be intensively
    performed. By contrast, R trades flexibility for performance, but well-written
    R code can exhibit acceptable, if not good, performance.
  prefs: []
  type: TYPE_NORMAL
- en: The top reason why R code can be slow is that our code may intensively create,
    allocate, or copy data structures. This is exactly why `my_cumsum1()` and `my_cumsum2()`
    show great difference in performance when the input gets longer. The `my_cumsum1()`
    function always grows the vector, which means that in each iteration the vector
    is copied to a new address and a new element is appended. As a result, the more
    iterations we have, the more elements it has to copy, and then the code gets slower.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be made explicit by the following benchmarking: `grow_by_index` means
    we initialize an empty list. The `preallocated` function means we initialize a
    list with pre-allocated positions, that is, a list of `n NULL` values with all
    positions allocated. In both cases, we modify the `i`^(th) element of the list,
    but the difference is that we''ll grow the first list in each iteration, and this
    does not happen with the second list because it is already fully allocated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are clear: intensively growing a list can significantly slow down
    the code, while modifying a pre-allocated list within range is fast. The same
    logic also applies to atomic vectors and matrices. Growing a data structure in
    R is generally slow because it triggers reallocation, that is, copying the original
    data structure to a new memory address. This is very expensive in R, especially
    when the data is large.'
  prefs: []
  type: TYPE_NORMAL
- en: However, accurate pre-allocation is not always feasible because it requires
    that we know the total number prior to the iteration. Sometimes, we can only ask
    for a result to store repeatedly without knowing the exact total number. In this
    case, maybe it is still a good idea to pre-allocate a list or vector with a reasonable
    length. When the iteration is over, if the number of iterations does not reach
    the pre-allocated length, we can take a subset of the list or vector. In this
    way, we can avoid intensive reallocation of data structures.
  prefs: []
  type: TYPE_NORMAL
- en: Boosting code performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we demonstrated how to use profiling tools to identify
    a performance bottleneck in the code. In this section, you will learn about a
    number of approaches to boosting code performance.
  prefs: []
  type: TYPE_NORMAL
- en: Using built-in functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Previously, we demonstrated the performance difference between `my_cumsum1()`,
    `my_cumsum2()` and the built-in function `cumsum()`. Although `my_cumsum2()` is
    faster than `my_cumsum1()`, when the input vector contains many numbers, `cumsum()`
    is much faster than them. Also, its performance does not decay significantly even
    as the input gets longer. If we evaluate `cumsum`, we can see that it is a primitive
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'A primitive function in R is implemented in C/C++/Fortran, compiled to native
    instructions, and thus, is extremely efficient. Another example is `diff()`. Here,
    we will implement computing vector difference sequence in R:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We can verify that the implementation is correct:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Therefore, both `diff_for()` and built-in `diff()` must return the same result
    for the same input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: However, there's a big gap in performance between the two functions.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Built-in functions are, in most cases, way faster than equivalent R implementations.
    This is true not only for vector functions, but also for matrices. For example,
    here is a simple 3 by 4 integer matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We can write a function to transpose the matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'In the function, we will first create a matrix of the same type as the input,
    but with the number and names of rows and columns exchanged, respectively. Then,
    we will iterate over columns and rows to transpose the matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The built-in function of matrix transpose is `t()`. We can easily verify that
    both functions return the same results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'However, they may exhibit great difference in performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The performance difference gets even more significant when the input matrix
    is larger. Here, we will create a new matrix with `1000` rows and `25` columns.
    While the results are the same, the performance can be very different:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that `t()` is a generic function that works with both matrix and data
    frame. S3 dispatching to find the right method for the input, also has some overhead.
    Therefore, directly calling `t.default()` on a matrix is slightly faster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: All previous examples show that, in most cases, it is much better to use built-in
    functions if provided than reinventing the wheel in R. These functions get rid
    of the overhead of R code and, thus, can be extremely efficient even if the input
    is huge.
  prefs: []
  type: TYPE_NORMAL
- en: Using vectorization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A special subset of built-in functions are arithmetic operators such as `+`,
    `-`, `*`, `/`, `^`, and `%%`. These operators are not only extremely efficient
    but also vectorized.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we implement `+` in R:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we would randomly generate `x` and `y`. The `add(x, y)`, and `x + y` arguments
    should return exactly the same results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The following benchmarking shows that the performance difference is huge:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, suppose we need to calculate the sum of the reciprocal of first `n` positive
    integers squared. We can easily implement the algorithm using a `for` loop as
    the following function `algo1_for`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: The function takes an input `n`, iterates `n` times to accumulate as supposed,
    and returns the result.
  prefs: []
  type: TYPE_NORMAL
- en: 'A better approach is to use vectorized calculation directly without any necessity
    of a `for` loop, just like how `algo1_vec()` is implemented:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The two functions yield the same results, given an ordinary input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'However, their performance is very different:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Vectorization is a highly recommended way of writing R code. It is not only
    of high performance but, also makes the code easier to understand.
  prefs: []
  type: TYPE_NORMAL
- en: Using byte-code compiler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we saw the power of vectorization. Sometimes, however,
    the problem dictates a for loop, and it is hard to vectorize the code. In this
    case, we may consider using R byte-code compiler to compile the function so that
    the function no longer needs parsing and may run faster.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will load the compiler package, which is distributed along with R.
    We will use `cmpfun()` to compile a given R function. For example, we will compile
    `diff_for()` and store the compiled function as `diff_cmp()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: When we look at `diff_cmp()`, it does not look very different from `diff_for()`,
    but it has an additional tag of the `bytecode` address.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we will run the benchmarking again with `diff_cmp()` this time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: It looks amazing that the compiled version, `diff_cmp()`, is much faster than
    `diff_for()` even though we didn't modify anything but compiled it into bytecode.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will do the same thing with `algo1_for()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will conduct the benchmarking with the compiled version included:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Again, the compiled version becomes more than six times faster than the original
    version, even if we didn't change a bit of code.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, compiling is no magic if it is used to compile a fully vectorized
    function. Here, we will compile `algo1_vec()` and compare its performance with
    the original version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Note that the compiled function shows no significant performance improvement.
    To know more about how the compiler works, type `?compile` and read the documentation.
  prefs: []
  type: TYPE_NORMAL
- en: Using Intel MKL-powered R distribution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The R distribution we normally use is single threaded, that is, only one CPU
    thread is used to execute all R code. The good thing is that the execution model
    is simple and safe, but it does not take advantage of multicore computing.
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft R Open (MRO, see [https://mran.microsoft.com/open/](https://mran.microsoft.com/open/))
    is an enhanced distribution of R. Powered by Intel Math Kernel Library (MKL, see [https://software.intel.com/en-us/intel-mkl](https://software.intel.com/en-us/intel-mkl)),
    MRO enhances the matrix algorithms by automatically taking advantage of multithreading
    computation. On a multicore computer, MRO can be 10-80 times faster than the official
    R implementation at matrix multiplication, Cholesky factorization, QR decomposition,
    singular value decomposition, principal component analysis, and linear discriminant
    analysis. For more details, visit [https://mran.microsoft.com/documents/rro/multithread/](https://mran.microsoft.com/documents/rro/multithread/)
    and see the benchmarking.
  prefs: []
  type: TYPE_NORMAL
- en: Using parallel computing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we mentioned in the previous section, R is single threaded in design but
    still allows multiprocessing parallel computing, that is, running multiple R sessions
    to compute. This technique is supported by a parallel library, which is also distributed
    along with R.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we need to do a simulation: we need to generate a random path that
    follows a certain random process and see whether at any point, the value goes
    beyond a fixed margin around the starting point.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code generates one realization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'The plot generated is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using parallel computing](img/image_13_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The preceding graph shows the path and 10 percent margin. It is clear that between
    index 300 and 500, the value goes beyond the upper margin multiple times.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is just one path. A valid simulation requires that the generator run as
    many times as necessary to produce statistically meaningful results. The following
    function parameterizes the random path generator and returns a list of summary
    indicators of interest. Note that `signal` indicates whether any point on the
    path goes beyond the margin:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can run the generator for one time and see its summarized result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'To perform the simulation, we need to run the function many times. In practice,
    we may need to run at least millions of realizations, which may take us a considerable
    amount of time. Here, we will measure how much time it costs to run ten thousand
    iterations of this simulation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'When the simulation is finished, we can convert all results into one data table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'We can calculate the realized probability of `signal == TRUE`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: What if the problem gets more practical and requires us to run millions of times?
    In this case, some researchers may turn to programming languages implemented with
    much higher performance such as C and C++, which are extremely efficient and flexible.
    They are great tools in implementing algorithms but require more effort to deal
    with the compiler, linker, and data input/output.
  prefs: []
  type: TYPE_NORMAL
- en: Note that each iteration in the preceding simulation is completely independent
    of each other, so it is better accomplished by parallel computing.
  prefs: []
  type: TYPE_NORMAL
- en: Since different operating systems have different implementations of process
    and threading model, some features that are available for Linux and MacOS are
    not available for Windows. Thus, performing parallel computing on Windows can
    be a bit more verbose.
  prefs: []
  type: TYPE_NORMAL
- en: Using parallel computing on Windows
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'On Windows, we need to create a local cluster of multiple R sessions to run
    parallel computing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: The `detectCores()` function returns the number of cores your computer is equipped
    with. Creating a cluster of more than that number of nodes is allowed but usually
    does no good because your computer cannot perform more tasks than that simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we can call `parLapply()`, the parallel version of `lapply()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the time consumed is reduced to more than half of the original time.
    Now, we no longer need the cluster. We can call `stopCluster()` to kill the R
    sessions just created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'When we call `parLapply()`, it automatically schedules the task for each cluster
    node. More specifically, all cluster nodes run `simulate()` with one of `1:10000`
    exclusively at the same time so that the computation is done in parallel. Finally,
    all results are collected so that we get a list just like the results from `lapply()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'The parallel code looks simple because `simulate()` is self-contained and does
    not rely on user-defined external variables or datasets. If we run a function
    in parallel that refers to a variable in the master session (the current session
    that creates the cluster), it will not find the variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: All nodes fail because each of them starts as a fresh R session with no user
    variables defined. To let the cluster nodes get the value of the variable they
    need, we have to export them to all nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example demonstrates how this works. Suppose we have a data frame
    of numbers. We want to take random samples from the data frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'If we perform the sampling in parallel, all nodes must share the data frame
    and the function. To do this, we can use `clusterEvalQ()` to evaluate an expression
    on each cluster node. First, we will make a cluster just as we did earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Sys.getpid()` function returns the process ID of the current R session.
    Since there are four nodes in the cluster, each is an R session with a unique
    process ID. We can call `clusterEvalQ()` with `Sys.getpid()` and see the process
    ID of each node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'To see the variables in the global environment of each node, we can call `ls()`,
    just like we call in our own working environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'As we mentioned, all cluster nodes are, by default, initialized with an empty
    global environment. To export `data` and `take_sample` to each node, we can call
    `clusterExport()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can see that all nodes have `data` and `take_sample`. Now, we can let
    each node call `take_sample()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, we can use `clusterCall()` and `<<-` to create global variables
    in each node, while `<-` only creates local variables in the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: Note that `clusterCall()` returns the returned value from each node. In the
    preceding code, we will use `invisible()` to suppress the values they return.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since each cluster node is started in a fresh state, they only load basic packages.
    To let each node load the given packages, we can also use `clusterEvalQ()`. The
    following code lets each node attach the `data.table` package so that `parLapply()`
    can run a function in which `data.table` functions are used on each node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'A list of data summary is returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'When we don''t need the cluster any more, we will run the following code to
    release it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: Using parallel computing on Linux and MacOS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Using parallel computing on Linux and MacOS can be much easier than on Windows.
    Without having to manually create a socket-based cluster, `mclapply()` directly
    forks the current R session into multiple R sessions, with everything preserved
    to continue running in parallel and schedule tasks for each child R session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Therefore, we don''t have to export the variables because they are immediately
    available in each fork process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, we can create jobs to be done in parallel with much flexibility. For
    example, we will create a job that generates `10` random numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'As long as the job is created, we can choose to collect the results from the
    job with `mccollect()`. Then, the function will not return until the job is finished:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also programmatically create a number of jobs to run in parallel. For
    example, we create `8` jobs, and each sleeps for a random time. Then, `mccollect()`
    won''t return until all jobs are finished sleeping. Since the jobs are run in
    parallel, the time `mccollect()` takes won''t be too long:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: This allows us to customize the task-scheduling mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: Using Rcpp
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we mentioned, parallel computing works when each iteration is independent
    so that the final results do not rely on the order of execution. However, not
    all tasks are so ideal like this. Therefore, the use of parallel computing may
    be undermined. What if we really want the algorithm to run fast and easily interact
    with R? The answer is by writing the algorithm in C++ via Rcpp ([http://www.rcpp.org/](http://www.rcpp.org/)).
  prefs: []
  type: TYPE_NORMAL
- en: C++ code usually runs very fast, because it is compiled to native instructions
    and is thus much closer to hardware level than a scripting language like R. Rcpp
    is a package that enables us to write C++ code with seamless R and C++ integration.
    With Rcpp, we can write C++ code in which we can call R functions and take advantage
    of R data structures. It allows us to write high-performance code and preserve
    the power of data manipulation in R at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: To use Rcpp, we first need to ensure that the system is prepared for computing
    native code with the right toolchain. Under Windows, Rtools is needed and can
    be found at [https://cran.r-project.org/bin/windows/Rtools/](https://cran.r-project.org/bin/windows/Rtools/).
    Under Linux and MacOS, a properly installed C/C++ toolchain is required.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the toolchain is properly installed, run the following code to install
    the package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will create a C++ source file at `code/rcpp-demo.cpp` with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code is written in C++. If you are not familiar with C++ syntax,
    you can quickly pick up the simplest part by going through [http://www.learncpp.com/](http://www.learncpp.com/).
    The language design and supported features are much richer and more complex than
    R. Don't expect to be an expert in a short period of time, but getting started
    with the basics usually allows you to write simple algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: If you read the preceding code, it looks very different from typical R code.
    Since C++ is a strong-typed language, we need to specify the types of function
    arguments and the return type of functions. A function that is commented with
    `[[Rcpp::export]]` will be captured by Rcpp, and when we source the code in RStudio
    or use `Rcpp::sourceCpp` directly, these C++ functions will be automatically compiled
    and ported to our working environment in R.
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding C++ function simply takes a numeric vector and returns a new
    numeric vector with all `x` elements doubled. Note that the `NumericVector` class
    is provided by `Rcpp.h` included at the beginning of the source file. In fact,
    `Rcpp.h` provides the C++ proxy of all commonly used R data structures. Now, we
    will call `Rcpp::sourceCpp()` to compile and load the source file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'The function compiles the source code, links it to necessary shared libraries,
    and exposes an R function to the environment. The beauty is that all of these
    are done automatically, which makes it much easier to write algorithms for non-professional
    C++ developers. Now, we have an R function to call it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that `timeTwo` in R does not look like an ordinary function, but
    performs a native call to the C++ function. The function works with single numeric
    input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'It also works with a multi-element numeric vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can use very simple C++ language constructs to reimplement the `algo1_for`
    algorithm in C++. Now, we will create a C++ source file at `code/rcpp-algo1.cpp`
    with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we don''t use any R but C++ data structures in `algo1_cpp`. When
    we source the code, Rcpp will handle all the porting for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'The function works with a single numeric input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'If we supply a numeric vector, an error will occur:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can do the benchmarking again. This time, we will add `algo1_cpp` to
    the list of alternative implementations. Here, we will compare the version using
    a `for` loop in R, the byte-code compiled version using for loop in R, the vectorized
    version, and the C++ version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: It is amazing that the C++ version is even faster than the vectorized version.
    Although the functions used by the vectorized version are primitive functions
    and are already very fast, they still have some overhead due to method dispatching
    and argument checking. Our C++ version is specialized to the task, so it can be
    slightly faster than the vectorized version.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another example is the C++ implementation of `diff_for()` as the following
    code shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding C++ code, `diff_cpp()` takes a numeric vector and returns
    a numeric vector. The function simply creates a new vector, and calculates and
    stores the differences between the consecutive two elements in `x` iteratively.
    Then, we will source the code file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'It is easy to verify whether the function works as supposed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will do the benchmarking again with five different calls: the version
    using a for loop in R (`diff_for`), the byte-code compiled version (`diff_cmp`),
    the vectorized version (`diff`), the vectorized version without method dispatch
    (`diff.default`), and our C++ version (`diff_cpp`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: It appears that the C++ version is the fastest.
  prefs: []
  type: TYPE_NORMAL
- en: In recent years, a rapidly growing number of R packages have used Rcpp to either
    boost performance or directly link to popular libraries that provide high-performance
    algorithms. For example, RcppArmadillo and RcppEigen provide high-performance
    linear algebra algorithms, RcppDE provides fast implementations for global optimization
    by differential evolution in C++, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: To know more about Rcpp and related packages, visit its official website ([http://www.rcpp.org/](http://www.rcpp.org/)).
    I also recommend the book*Seamless R and C++ Integration with Rcpp* by Rcpp's
    author Dirk Eddelbuettel at [http://www.rcpp.org/book/](http://www.rcpp.org/book/).
  prefs: []
  type: TYPE_NORMAL
- en: OpenMP
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we mentioned in the section on parallel computing, an R session runs in a
    single thread. However, in Rcpp code, we can use multithreading to boost the performance.
    One multithreading technique is OpenMP ([http://openmp.org](http://openmp.org)),
    which is supported by most modern C++ compilers (see [http://openmp.org/wp/openmp-compilers/](http://openmp.org/wp/openmp-compilers/)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Several articles discuss and demonstrate the use of OpenMP with Rcpp at [http://gallery.rcpp.org/tags/openmp/](http://gallery.rcpp.org/tags/openmp/).
    Here, we will provide a simple example. We will create a C++ source file with
    the following code at `code/rcpp-diff-openmp.cpp`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: Note that Rcpp will recognize the comment in the first line and add necessary
    options to the compiler so that OpenMP is enabled. To use OpenMP, we need to include
    `omp.h`. Then, we can set the number of threads by calling `omp_set_num_threads(n)`
    and use `#pragma omp parallel for` to indicate that the following for loop should
    be parallelized. If the number of threads is set to `1`, then the code also runs
    normally.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will source the C++ code file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: 'First, let''s see whether the function works properly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will start benchmarking with a 1000-number input vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'Unfortunately, even with multi-threading, `diff_cpp_omp()` is slower than its
    single-threaded C++ implementation. This is because using multithreading has some
    overhead. If the input is small, the time to initialize multiple threads may take
    a significant part of the whole computing time. However, if the input is large
    enough, the advantage of multi-threading will exceed its cost. Here, we will use
    `100000` numbers as the input vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: The cost of creating multiple threads is small relative to the performance boost
    of using them. As a result, the version powered by OpenMP is even faster than
    the simple C++ version.
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact, the feature set of OpenMP is much richer than we have demonstrated.
    For more details, read the official documentation. For more examples, I recommend
    *Guide into OpenMP: Easy multithreading programming for C++* by Joel Yliluoma
    at [http://bisqwit.iki.fi/story/howto/openmp/](http://bisqwit.iki.fi/story/howto/openmp/).'
  prefs: []
  type: TYPE_NORMAL
- en: RcppParallel
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another approach to taking advantage of multi-threading with Rcpp is RcppParallel
    ([http://rcppcore.github.io/RcppParallel/](http://rcppcore.github.io/RcppParallel/)).
    This package includes Intel TBB ([https://www.threadingbuildingblocks.org/](https://www.threadingbuildingblocks.org/))
    and TinyThread ([http://tinythreadpp.bitsnbites.eu/](http://tinythreadpp.bitsnbites.eu/)).
    It provides thread-safe vector and matrix wrapper data structures as well as high-level
    parallel functions.
  prefs: []
  type: TYPE_NORMAL
- en: To perform multi-threading parallel computing with RcppParallel, we need to
    implement a `Worker` to handle how a slice of input is transformed to the output.
    Then, RcppParallel will take care of the rest of the work such as multithreading
    task scheduling.
  prefs: []
  type: TYPE_NORMAL
- en: Here's a short demo. We will create a C++ source file with the following code
    at `code/rcpp-parallel.cpp`. Note that we need to declare to Rcpp that it depends
    on RcppParallel and uses C++ 11 for using lambda function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we will implement a `Worker` called `Transformer` that transforms each
    element `x` of a matrix `to 1 / (1 + x ^ 2)`. Then, in `par_transform`, we will
    create an instance of `Transformer` and call `parallelFor` with it so that it
    automatically takes advantage of multithreading:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: 'We can easily verify that the function works with a small matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: 'It produces exactly the same results as the vectorized R expression. Now, we
    can take a look at its performance when the input matrix is very large:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: It appears that the multi-threading version is almost 1x faster than the vectorized
    version.
  prefs: []
  type: TYPE_NORMAL
- en: RcppParallel is more powerful than we have demonstrated. For more detailed introduction
    and examples, visit [http://rcppcore.github.io/RcppParallel](http://rcppcore.github.io/RcppParallel).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you learned when performance may or may not matter, how to
    measure the performance of R code, how to use profiling tools to identify the
    slowest part of code, and why such code can be slow. Then, we introduced the most
    important ways to boost the code performance: using built-in functions if possible,
    taking advantage of vectorization, using the byte-code compiler, using parallel
    computing, writing code in C++ via Rcpp, and using multi-threading techniques
    in C++. High-performance computing is quite an advanced topic, and there''s still
    a lot more to learn if you want to apply it in practice. This chapter demonstrates
    that using R does not always mean slow code. Instead, we can achieve high performance
    if we want.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will introduce another useful topic: web scraping.
    To scrape data from webpages, we need to understand how web pages are structured
    and how to extract data from their source code. You will learn the basic idea
    and representation of HTML, XML, and CSS, and how to analyze a target webpage
    so that we can correctly extract the information we want from webpages.'
  prefs: []
  type: TYPE_NORMAL
