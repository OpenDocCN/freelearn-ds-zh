<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer031">
<h1 class="chapter-number" id="_idParaDest-22"><a id="_idTextAnchor022"/>1</h1>
<h1 id="_idParaDest-23"><a id="_idTextAnchor023"/>Introduction to Data Ingestion</h1>
<p>Welcome to the fantastic world of data! Are you ready to embark on a thrilling journey into data ingestion? If so, this is the perfect book to start! Ingesting data is the first step into the big <span class="No-Break">data world.</span></p>
<p><strong class="bold">Data ingestion</strong> is a process <a id="_idIndexMarker000"/>that involves gathering and importing data and also <a id="_idIndexMarker001"/>storing it properly so that the subsequent <strong class="bold">extract, transform, and load</strong> (<strong class="bold">ETL</strong>) pipeline can utilize the data. To make it happen, we must be cautious about the tools we will use and how to configure <span class="No-Break">them properly.</span></p>
<p>In our book journey, we will use <strong class="bold">Python</strong> and <strong class="bold">PySpark</strong> to retrieve data from different data sources and learn <a id="_idIndexMarker002"/>how to store them properly. To orchestrate all this, the basic <a id="_idIndexMarker003"/>concepts of <strong class="bold">Airflow</strong> will be implemented, along with efficient monitoring to guarantee that our pipelines <span class="No-Break">are covered.</span></p>
<p>This chapter will introduce some basic concepts about data ingestion and how to set up your environment to start <span class="No-Break">the tasks.</span></p>
<p>In this chapter, you will build and learn the <span class="No-Break">following recipes:</span></p>
<ul>
<li>Setting up Python and <span class="No-Break">the environment</span></li>
<li><span class="No-Break">Installing PySpark</span></li>
<li>Configuring Docker <span class="No-Break">for MongoDB</span></li>
<li>Configuring Docker <span class="No-Break">for Airflow</span></li>
<li><span class="No-Break">Logging libraries</span></li>
<li><span class="No-Break">Creating schemas</span></li>
<li>Applying data governance <span class="No-Break">in ingestion</span></li>
<li>Implementing <span class="No-Break">data replication</span></li>
</ul>
<h1 id="_idParaDest-24"><a id="_idTextAnchor024"/>Technical requirements</h1>
<p>The commands inside the recipes of this chapter use Linux syntax. If you don’t use a Linux-based system, you may need to adapt <span class="No-Break">the commands:</span></p>
<ul>
<li>Docker or <span class="No-Break">Docker Desktop</span></li>
<li>The SQL client of your choice (recommended); we recommend DBeaver, since it has a <span class="No-Break">community-free version</span></li>
</ul>
<p>You can find the code from this chapter in this GitHub <span class="No-Break">repository: </span><a href="https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook"><span class="No-Break">https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook</span></a><span class="No-Break">.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">Windows users might get an error message such as <strong class="bold">Docker Desktop requires a newer WSL kernel version.</strong> This can be fixed by following the steps <span class="No-Break">here: </span><a href="https://docs.docker.com/desktop/windows/wsl/"><span class="No-Break">https://docs.docker.com/desktop/windows/wsl/</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-25"><a id="_idTextAnchor025"/>Setting up Python and its environment</h1>
<p>In the data <a id="_idIndexMarker004"/>world, languages such as <strong class="bold">Java</strong>, <strong class="bold">Scala</strong>, or <strong class="bold">Python</strong> are commonly <a id="_idIndexMarker005"/>used. The first two languages are used due to their compatibility with the big data tools environment, such as <strong class="bold">Hadoop</strong> and <strong class="bold">Spark</strong>, the central core of which runs on a <strong class="bold">Java Virtual Machine</strong> (<strong class="bold">JVM</strong>). However, in <a id="_idIndexMarker006"/>the past few years, the use of Python for data engineering and data science has increased significantly due to the language’s versatility, ease of understanding, and many open source libraries built by <span class="No-Break">the community.</span></p>
<h2 id="_idParaDest-26"><a id="_idTextAnchor026"/>Getting ready</h2>
<p>Let’s create a folder for <span class="No-Break">our project:</span></p>
<ol>
<li>First, open your <a id="_idIndexMarker007"/>system command line. Since I use the <strong class="bold">Windows Subsystem for Linux</strong> (<strong class="bold">WSL</strong>), I will open the <span class="No-Break">WSL application.</span></li>
<li>Go to your home directory and create a folder <span class="No-Break">as follows:</span><pre class="console">
<strong class="bold">$ mkdir my-project</strong></pre></li>
<li>Go inside <span class="No-Break">this folder:</span><pre class="console">
<strong class="bold">$ cd my-project</strong></pre></li>
<li>Check your Python version on your operating system <span class="No-Break">as follows:</span><pre class="console">
<strong class="bold">$ python -–version</strong></pre></li>
</ol>
<p>Depending on your operational system, you might or might not have output here – for example, WSL 20.04 users might have the following output:</p>
<pre class="console">
<strong class="bold">Command 'python' not found, did you mean:</strong>
<strong class="bold"> command 'python3' from deb python3</strong>
<strong class="bold"> command 'python' from deb python-is-python3</strong></pre>
<p>If your Python <a id="_idIndexMarker008"/>path is configured to use the <strong class="source-inline">python</strong> command, you will <a id="_idIndexMarker009"/>see output similar to this:</p>
<pre class="console">
<strong class="bold">Python 3.9.0</strong></pre>
<p>Sometimes, your Python path might be configured to be invoked using <strong class="source-inline">python3</strong>. You can try it using the following command:</p>
<pre class="console">
<strong class="bold">$ python3 --version</strong></pre>
<p>The output will be similar to the <strong class="source-inline">python</strong> command, as follows:</p>
<pre class="console">
<strong class="bold">Python 3.9.0</strong></pre>
<ol>
<li value="5">Now, let’s check our <strong class="source-inline">pip</strong> version. This check is essential, since some operating systems have more than one Python <span class="No-Break">version installed:</span><pre class="console">
<strong class="bold">$ pip --version</strong></pre></li>
</ol>
<p>You should see similar output:</p>
<pre class="source-code">
pip 20.0.2 from /usr/lib/python3/dist-packages/pip (python 3.9)</pre>
<p>If your <strong class="bold">operating system</strong> (<strong class="bold">OS</strong>) uses a Python version below <strong class="source-inline">3.8x</strong> or doesn’t have the language <a id="_idIndexMarker010"/>installed, proceed to the <em class="italic">How to do it</em> steps; otherwise, you are ready to start the following <em class="italic">Installing </em><span class="No-Break"><em class="italic">PySpark</em></span><span class="No-Break"> recipe.</span></p>
<h2 id="_idParaDest-27"><a id="_idTextAnchor027"/>How to do it…</h2>
<p>We are going to <a id="_idIndexMarker011"/>use the official installer from Python.org. You can find the <a id="_idIndexMarker012"/>link for it <span class="No-Break">here: </span><a href="https://www.python.org/downloads/"><span class="No-Break">https://www.python.org/downloads/</span></a><span class="No-Break">:</span></p>
<p class="callout-heading">Note</p>
<p class="callout">For Windows users, it is important to check your OS version, since Python 3.10 may not be yet compatible with Windows 7, or your processor type (32-bits <span class="No-Break">or 64-bits).</span></p>
<ol>
<li>Download one of the <span class="No-Break">stable versions.</span></li>
</ol>
<p>At the time of writing, the stable recommended versions compatible with the tools and resources presented here are <strong class="source-inline">3.8</strong>, <strong class="source-inline">3.9</strong>, and <strong class="source-inline">3.10</strong>. I will use the <strong class="source-inline">3.9</strong> version and download it using the following link: <a href="https://www.python.org/downloads/release/python-390/">https://www.python.org/downloads/release/python-390/</a>. Scrolling down the page, you will find a list of links to Python installers according to OS, as shown in the following screenshot.</p>
<div>
<div class="IMG---Figure" id="_idContainer010">
<img alt="Figure 1.1 – Python.org download files for version 3.9" height="822" src="image/Figure_1.1_B19453.jpg" width="1447"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.1 – Python.org download files for version 3.9</p>
<ol>
<li value="2">After downloading the installation file, double-click it and follow the instructions in the wizard <a id="_idIndexMarker013"/>window. To avoid complexity, choose the <a id="_idIndexMarker014"/>recommended <span class="No-Break">settings displayed.</span></li>
</ol>
<p>The following screenshot shows how it looks on Windows:</p>
<div>
<div class="IMG---Figure" id="_idContainer011">
<img alt="Figure 1.2 – The Python Installer for Windows" height="398" src="image/Figure_1.2_B19453.jpg" width="660"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.2 – The Python Installer for Windows</p>
<ol>
<li value="3">If you are a Linux user, you can install it from the source using the <span class="No-Break">following commands:</span><pre class="console">
<strong class="bold">$ wget https://www.python.org/ftp/python/3.9.1/Python-3.9.1.tgz</strong>
<strong class="bold">$ tar -xf Python-3.9.1.tgz</strong>
<strong class="bold">$ ./configure –enable-optimizations</strong>
<strong class="bold">$ make -j 9</strong></pre></li>
</ol>
<p>After installing Python, you should be able to execute the <strong class="source-inline">pip</strong> command. If not, refer to the <strong class="source-inline">pip</strong> official documentation page <span class="No-Break">here: </span><a href="https://pip.pypa.io/en/stable/installation/"><span class="No-Break">https://pip.pypa.io/en/stable/installation/</span></a><span class="No-Break">.</span></p>
<h2 id="_idParaDest-28"><a id="_idTextAnchor028"/>How it works…</h2>
<p>Python is an <strong class="bold">interpreted language</strong>, and its interpreter extends several functions made with <strong class="bold">C</strong> or <strong class="bold">C++</strong>. The language package also comes with several built-in libraries and, of course, <span class="No-Break">the interpreter.</span></p>
<p>The interpreter <a id="_idIndexMarker015"/>works like a Unix shell and can be found in <a id="_idIndexMarker016"/>the <strong class="source-inline">usr/local/bin</strong> <span class="No-Break">directory: </span><a href="https://docs.python.org/3/tutorial/interpreter.xhtml"><span class="No-Break">https://docs.python.org/3/tutorial/interpreter.xhtml</span></a><span class="No-Break">.</span></p>
<p>Lastly, note that many Python third-party packages in this book require the <strong class="source-inline">pip</strong> command to be installed. This is because <strong class="source-inline">pip</strong> (an acronym for <strong class="bold">Pip Installs Packages</strong>) is the default <a id="_idIndexMarker017"/>package manager for Python; therefore, it is used <a id="_idIndexMarker018"/>to install, upgrade, and manage the Python packages and dependencies from the <strong class="bold">Python Package </strong><span class="No-Break"><strong class="bold">Index</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">PyPI</strong></span><span class="No-Break">).</span></p>
<h2 id="_idParaDest-29"><a id="_idTextAnchor029"/>There’s more…</h2>
<p>Even if you don’t have any Python versions on your machine, you can still install them using the command line or <strong class="bold">HomeBrew</strong> (for <strong class="bold">macOS</strong> users). Windows users can also download them from the MS <span class="No-Break">Windows Store.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">If you choose to download Python from the Windows Store, ensure you use an application made by the Python <span class="No-Break">Software Foundation.</span></p>
<h2 id="_idParaDest-30"><a id="_idTextAnchor030"/>See also</h2>
<p>You can use <strong class="source-inline">pip</strong> to install convenient third-party applications, such as Jupyter. This is an open source, web-based, interactive (and user-friendly) computing platform, often used by data scientists and data engineers. You can install it from the official website <span class="No-Break">here: </span><a href="https://jupyter.org/install"><span class="No-Break">https://jupyter.org/install</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-31"><a id="_idTextAnchor031"/>Installing PySpark</h1>
<p>To process, clean, and transform vast amounts of data, we need a tool that provides resilience and <a id="_idIndexMarker019"/>distributed processing, and that’s why <strong class="bold">PySpark</strong> is a good fit. It gets an API over the Spark library that lets you use <span class="No-Break">its applications.</span></p>
<h2 id="_idParaDest-32"><a id="_idTextAnchor032"/>Getting ready</h2>
<p>Before starting the PySpark installation, we need to check our Java version in our <span class="No-Break">operational system:</span></p>
<ol>
<li>Here, we check the <span class="No-Break">Java version:</span><pre class="console">
<strong class="bold">$ java -version</strong></pre></li>
</ol>
<p>You should see output similar to this:</p>
<pre class="console">
<strong class="bold">openjdk version "1.8.0_292"</strong>
<strong class="bold">OpenJDK Runtime Environment (build 1.8.0_292-8u292-b10-0ubuntu1~20.04-b10)</strong>
<strong class="bold">OpenJDK 64-Bit Server VM (build 25.292-b10, mixed mode)</strong></pre>
<p>If everything is correct, you should see the preceding message as the output of the command and the <strong class="bold">OpenJDK 18</strong> version or higher. However, some systems don’t have any Java version installed by default, and to cover this, we need to proceed to <em class="italic">step 2</em>.</p>
<ol>
<li value="2">Now, we <a id="_idIndexMarker020"/>download the <strong class="bold">Java Development </strong><span class="No-Break"><strong class="bold">Kit</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">JDK</strong></span><span class="No-Break">).</span></li>
</ol>
<p>Go to <a href="https://www.oracle.com/java/technologies/downloads/">https://www.oracle.com/java/technologies/downloads/</a>, select your <strong class="bold">OS</strong>, and download <a id="_idIndexMarker021"/>the most recent version of JDK. At the time of writing, it is JDK 19.</p>
<p>The download page of the JDK will look as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer012">
<img alt="Figure 1.3 – The JDK 19 downloads official web page" height="784" src="image/Figure_1.3_B19453.jpg" width="979"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.3 – The JDK 19 downloads official web page</p>
<p>Execute the downloaded application. Click on the application to start the installation process. The following window will appear:</p>
<p class="callout-heading">Note</p>
<p class="callout">Depending on your OS, the installation window may appear <span class="No-Break">slightly different.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer013">
<img alt="Figure 1.4 – The Java installation wizard window" height="429" src="image/Figure_1.4_B19453.jpg" width="763"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.4 – The Java installation wizard window</p>
<p>Click <strong class="bold">Next</strong> for the following two questions, and the application will start the installation. You <a id="_idIndexMarker022"/>don’t need to worry about where the JDK will be installed. By default, the application is configured, as standard, to be compatible with other tools’ installations.</p>
<ol>
<li value="3">Next, we again check our Java version. When executing the command again, you should see the <span class="No-Break">following version:</span><pre class="console">
<strong class="bold">$ java -version</strong>
<strong class="bold">openjdk version "1.8.0_292"</strong>
<strong class="bold">OpenJDK Runtime Environment (build 1.8.0_292-8u292-b10-0ubuntu1~20.04-b10)</strong>
<strong class="bold">OpenJDK 64-Bit Server VM (build 25.292-b10, mixed mode)</strong></pre></li>
</ol>
<h2 id="_idParaDest-33"><a id="_idTextAnchor033"/>How to do it…</h2>
<p>Here are the steps to perform <span class="No-Break">this recipe:</span></p>
<ol>
<li>Install PySpark <span class="No-Break">from PyPi:</span><pre class="console">
<strong class="bold">$ pip install pyspark</strong></pre></li>
</ol>
<p>If the command runs successfully, the installation output’s last line will look like this:</p>
<pre class="console">
Successfully built pyspark
Installing collected packages: py4j, pyspark
Successfully installed py4j-0.10.9.5 pyspark-3.3.2</pre>
<ol>
<li value="2">Execute the <strong class="source-inline">pyspark</strong> command to open the interactive shell. When executing the <strong class="source-inline">pyspark</strong> command <a id="_idIndexMarker023"/>in your command line, you should see <span class="No-Break">this message:</span><pre class="console">
<strong class="bold">$ pyspark</strong>
<strong class="bold">Python 3.8.10 (default, Jun 22 2022, 20:18:18)</strong>
<strong class="bold">[GCC 9.4.0] on linux</strong>
<strong class="bold">Type "help", "copyright", "credits" or "license" for more information.</strong>
<strong class="bold">22/10/08 15:06:11 WARN Utils: Your hostname, DESKTOP-DVUDB98 resolves to a loopback address: 127.0.1.1; using 172.29.214.162 instead (on interface eth0)</strong>
<strong class="bold">22/10/08 15:06:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address</strong>
<strong class="bold">22/10/08 15:06:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</strong>
<strong class="bold">Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties</strong>
<strong class="bold">Setting default log level to "WARN".</strong>
<strong class="bold">To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).</strong>
<strong class="bold">Welcome to</strong>
<strong class="bold">      ____              __</strong>
<strong class="bold">     / __/__  ___ _____/ /__</strong>
<strong class="bold">    _\ \/ _ \/ _ `/ __/</strong><strong class="bold">  '_/</strong>
<strong class="bold">   /__ / .__/\_,_/_/ /_/\_\   version 3.1.2</strong>
<strong class="bold">      /_/</strong>
<strong class="bold">Using Python version 3.8.10 (default, Jun 22 2022 20:18:18)</strong>
<strong class="bold">Spark context Web UI available at http://172.29.214.162:4040</strong>
<strong class="bold">Spark context available as 'sc' (master = local[*], app id = local-1665237974112).</strong>
<strong class="bold">SparkSession available as 'spark'.</strong>
<strong class="bold">&gt;&gt;&gt;</strong></pre></li>
</ol>
<p>You can observe some interesting messages here, such as the Spark version and the Python used from PySpark.</p>
<ol>
<li value="3">Finally, we exit the interactive shell <span class="No-Break">as follows:</span><pre class="console">
<strong class="bold">&gt;&gt;&gt; exit()</strong>
<strong class="bold">$</strong></pre></li>
</ol>
<h2 id="_idParaDest-34"><a id="_idTextAnchor034"/>How it works…</h2>
<p>As seen at the beginning of this recipe, Spark is a robust framework that runs on top of the JVM. It is also an open source tool for creating resilient and distributed processing output from vast <a id="_idIndexMarker024"/>data. With the growth in popularity of the Python language in the past few years, it became necessary to have a solution that adapts Spark to run <span class="No-Break">alongside Python.</span></p>
<p>PySpark is an interface that interacts with <strong class="bold">Spark APIs via Py4J</strong>, dynamically allowing Python code to interact with the JVM. We first need to have Java installed on our OS to use Spark. When we install PySpark, it already comes with Spark and Py4J components installed, making it easy to start the application and build <span class="No-Break">the code.</span></p>
<h2 id="_idParaDest-35"><a id="_idTextAnchor035"/>There’s more…</h2>
<p>Anaconda is a convenient <a id="_idIndexMarker025"/>way to install PySpark and other data science tools. This tool encapsulates all manual processes and has a friendly interface for interacting with and installing Python components, such as <strong class="bold">NumPy</strong>, <strong class="bold">pandas</strong>, <span class="No-Break">or </span><span class="No-Break"><strong class="bold">Jupyter</strong></span><span class="No-Break">:</span></p>
<ol>
<li>To install Anaconda, go to the official website and select <strong class="bold">Products</strong> | <strong class="bold">Anaconda </strong><span class="No-Break"><strong class="bold">Distribution</strong></span><span class="No-Break">: </span><a href="https://www.anaconda.com/products/distribution"><span class="No-Break">https://www.anaconda.com/products/distribution</span></a><span class="No-Break">.</span></li>
<li>Download the distribution according to <span class="No-Break">your OS.</span></li>
</ol>
<p>For more <a id="_idIndexMarker026"/>detailed information about how to install Anaconda and other powerful commands, refer <span class="No-Break">to </span><a href="https://docs.anaconda.com/"><span class="No-Break">https://docs.anaconda.com/</span></a><span class="No-Break">.</span></p>
<h3>Using virtualenv with PySpark</h3>
<p>It is possible <a id="_idIndexMarker027"/>to configure and use <strong class="source-inline">virtualenv</strong> with PySpark, and Anaconda <a id="_idIndexMarker028"/>does it automatically if you choose this type of installation. However, for the other installation methods, we need to make some additional steps to make our Spark cluster (locally or on the server) run it, which includes indicating the <strong class="source-inline">virtualenv /bin/</strong> folder and where your PySpark <span class="No-Break">path is.</span></p>
<h2 id="_idParaDest-36"><a id="_idTextAnchor036"/>See also</h2>
<p>There is a nice article about this topic, <em class="italic">Using VirtualEnv with PySpark</em>, by jzhang, <span class="No-Break">here: </span><a href="https://community.cloudera.com/t5/Community-Articles/Using-VirtualEnv-with-PySpark/ta-p/245932"><span class="No-Break">https://community.cloudera.com/t5/Community-Articles/Using-VirtualEnv-with-PySpark/ta-p/245932</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-37"><a id="_idTextAnchor037"/>Configuring Docker for MongoDB</h1>
<p><strong class="bold">MongoDB</strong> is a <strong class="bold">Not Only SQL</strong> (<strong class="bold">NoSQL</strong>) document-oriented database, widely used to store <strong class="bold">Internet of Things</strong> (<strong class="bold">IoT</strong>) data, application <a id="_idIndexMarker029"/>logs, and so on. A NoSQL <a id="_idIndexMarker030"/>database is a non-relational <a id="_idIndexMarker031"/>database that stores unstructured data differently <a id="_idIndexMarker032"/>from relational databases such as MySQL or PostgreSQL. Don’t worry too much about this now; we will cover it in more detail in <a href="B19453_05.xhtml#_idTextAnchor161"><span class="No-Break"><em class="italic">Chapter 5</em></span></a><span class="No-Break">.</span></p>
<p>Your cluster production environment can handle huge amounts of data and create resilient <span class="No-Break">data storage.</span></p>
<h2 id="_idParaDest-38"><a id="_idTextAnchor038"/>Getting ready</h2>
<p>Following the good practice of code organization, let’s start creating a folder inside our project to store the <span class="No-Break">Docker image:</span></p>
<p>Create a folder inside our project directory to store the MongoDB Docker image and data <span class="No-Break">as follows:</span></p>
<pre class="console">
my-project$ mkdir mongo-local
my-project$ cd mongo-local</pre>
<h2 id="_idParaDest-39"><a id="_idTextAnchor039"/>How to do it…</h2>
<p>Here are the steps to try out <span class="No-Break">this recipe:</span></p>
<ol>
<li>First, we <a id="_idIndexMarker033"/>pull the Docker image from Docker Hub <a id="_idIndexMarker034"/><span class="No-Break">as follows:</span><pre class="console">
<strong class="bold">my-project/mongo-local$ docker pull mongo</strong></pre></li>
</ol>
<p>You should see the following message in your command line:</p>
<pre class="console">
<strong class="bold">Using default tag: latest</strong>
<strong class="bold">latest: Pulling from library/mongo</strong>
<strong class="bold">(...)</strong>
<strong class="bold">bc8341d9c8d5: Pull complete</strong>
<strong class="bold">(...)</strong>
<strong class="bold">Status: Downloaded newer image for mongo:latest</strong>
<strong class="bold">docker.io/library/mongo:latest</strong></pre>
<p class="callout-heading">Note</p>
<p class="callout">If you are a WSL user, an error might occur if you use the WSL 1 version instead of version 2. You can easily fix this by following the steps <span class="No-Break">here: </span><a href="https://learn.microsoft.com/en-us/windows/wsl/install"><span class="No-Break">https://learn.microsoft.com/en-us/windows/wsl/install</span></a><span class="No-Break">.</span></p>
<ol>
<li value="2">Then, we run the MongoDB server <span class="No-Break">as follows:</span><pre class="console">
<strong class="bold">my-project/mongo-local$ docker run \</strong>
<strong class="bold">--name mongodb-local \</strong>
<strong class="bold">-p 27017:27017 \</strong>
<strong class="bold">-e MONGO_INITDB_ROOT_USERNAME="your_username" \</strong>
<strong class="bold">-e MONGO_INITDB_ROOT_PASSWORD="your_password"\</strong>
<strong class="bold">-d mongo:latest</strong></pre></li>
</ol>
<p>We then check <a id="_idIndexMarker035"/>our server. To do this, we can use the <a id="_idIndexMarker036"/>command line to see which Docker images are running:</p>
<pre class="console">
<strong class="bold">my-project/mongo-local$ docker ps</strong></pre>
<p>We then see this on the screen:</p>
<div>
<div class="IMG---Figure" id="_idContainer014">
<img alt="Figure 1.5 – MongoDB and Docker running container" height="49" src="image/Figure_1.5_B19453.jpg" width="1152"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.5 – MongoDB and Docker running container</p>
<p>We can even check on the Docker Desktop application to see whether our container is running:</p>
<div>
<div class="IMG---Figure" id="_idContainer015">
<img alt="Figure 1.6 – The Docker Desktop vision of the MongoDB container running" height="64" src="image/Figure_1.6_B19453.jpg" width="270"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.6 – The Docker Desktop vision of the MongoDB container running</p>
<ol>
<li value="3">Finally, we need to stop our container. We need to use <strong class="source-inline">Container ID</strong> to stop the container, which we previously saw when checking the Docker running images. We will rerun it in <a href="B19453_05.xhtml#_idTextAnchor161"><span class="No-Break"><em class="italic">Chapter 5</em></span></a><span class="No-Break">:</span><pre class="console">
<strong class="bold">my-project/mongo-local$ docker stop 427cc2e5d40e</strong></pre></li>
</ol>
<h2 id="_idParaDest-40"><a id="_idTextAnchor040"/>How it works…</h2>
<p>MongoDB’s architecture uses the concept of <strong class="bold">distributed processing</strong>, where the <strong class="source-inline">main</strong> node interacts <a id="_idIndexMarker037"/>with clients’ requests, such as queries and <a id="_idIndexMarker038"/>document manipulation. It distributes the requests automatically among its shards, which are a subset of a larger data <span class="No-Break">collection here.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer016">
<img alt="Figure 1.7 – MongoDB architecture" height="1141" src="image/Figure_1.7_B19453.jpg" width="1201"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.7 – MongoDB architecture</p>
<p>Since we may also have other running projects or software applications inside our machine, isolating any database or application server used in development is a good practice. In this way, we ensure nothing interferes with our local servers, and the debug process can be <span class="No-Break">more manageable.</span></p>
<p>This Docker image setting <a id="_idIndexMarker039"/>creates a MongoDB server locally and even <a id="_idIndexMarker040"/>allows us to make additional changes if we want to simulate any other scenario for testing <span class="No-Break">or development.</span></p>
<p>The commands we used are <span class="No-Break">as follows:</span></p>
<ul>
<li>The <strong class="source-inline">--name</strong> command defines the name we give to <span class="No-Break">our container.</span></li>
<li>The <strong class="source-inline">-p</strong> command specifies the port our container will open so that we can access it <span class="No-Break">via </span><span class="No-Break"><strong class="source-inline">localhost:27017</strong></span><span class="No-Break">.</span></li>
<li><strong class="source-inline">-e</strong> command defines the environment variables. In this case, we set the <strong class="source-inline">root</strong> username and password for our <span class="No-Break">MongoDB container.</span></li>
<li><strong class="source-inline">-d</strong> is detached mode – that is, the Docker process will run in the background, and we will not see input or output. However, we can still use <strong class="source-inline">docker ps</strong> to check the <span class="No-Break">container status.</span></li>
<li><strong class="source-inline">mongo:latest</strong> indicates Docker pulling this image’s <span class="No-Break">latest version.</span></li>
</ul>
<h2 id="_idParaDest-41"><a id="_idTextAnchor041"/>There’s more…</h2>
<p>For frequent users, manually configuring other parameters for the MongoDB container, such as the version, image port, database name, and database credentials, is <span class="No-Break">also possible.</span></p>
<p>A version of this image with example values is also available as a <strong class="source-inline">docker-compose</strong> file in the official <a id="_idIndexMarker041"/>documentation <span class="No-Break">here: </span><a href="https://hub.docker.com/_/mongo"><span class="No-Break">https://hub.docker.com/_/mongo</span></a><span class="No-Break">.</span></p>
<p>The <strong class="source-inline">docker-compose</strong> file for MongoDB looks similar <span class="No-Break">to this:</span></p>
<pre class="console">
# Use your own values for username and password
version: '3.1'
services:
  mongo:
    image: mongo
    restart: always
    environment:
      MONGO_INITDB_ROOT_USERNAME: root
      MONGO_INITDB_ROOT_PASSWORD: example
  mongo-express:
    image: mongo-express
    restart: always
    ports:
      - 8081:8081
    environment:
      ME_CONFIG_MONGODB_ADMINUSERNAME: root
      ME_CONFIG_MONGODB_ADMINPASSWORD: example
      ME_CONFIG_MONGODB_URL: mongodb://root:example@mongo:27017/</pre>
<h2 id="_idParaDest-42"><a id="_idTextAnchor042"/>See also</h2>
<p>You can check out MongoDB at the complete Docker Hub documentation <span class="No-Break">here: </span><a href="https://hub.docker.com/_/mongo"><span class="No-Break">https://hub.docker.com/_/mongo</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-43"><a id="_idTextAnchor043"/>Configuring Docker for Airflow</h1>
<p>In this book, we <a id="_idIndexMarker042"/>will use <strong class="bold">Airflow</strong> to orchestrate data ingests and <a id="_idIndexMarker043"/>provide logs to monitor <span class="No-Break">our pipelines.</span></p>
<p>Airflow can be installed directly on your local machine and any server using PyPi<strong class="bold"> </strong>(<a href="https://pypi.org/project/apache-airflow/">https://pypi.org/project/apache-airflow/</a>) or a Docker container (<a href="https://hub.docker.com/r/apache/airflow">https://hub.docker.com/r/apache/airflow</a>). An official and supported version of Airflow can be found on Docker Hub, and the <strong class="bold">Apache Foundation</strong> community <span class="No-Break">maintains it.</span></p>
<p>However, there are some additional steps to configure our Airflow. Thankfully, the Apache Foundation also has a <strong class="source-inline">docker-compose</strong> file that contains all other requirements to make Airflow work. We just need to complete a few <span class="No-Break">more steps.</span></p>
<h2 id="_idParaDest-44"><a id="_idTextAnchor044"/>Getting ready</h2>
<p>Let’s start by initializing our Docker application on our machine. You can use the desktop version or the <span class="No-Break">CLI command.</span></p>
<p>Make sure you are inside your project folder for this. Create a folder to store Airflow internal components and the <span class="No-Break"><strong class="source-inline">docker-compose.yaml</strong></span><span class="No-Break"> file:</span></p>
<pre class="console">
my-project$ mkdir airflow-local
my-project$ cd airflow-local</pre>
<h2 id="_idParaDest-45"><a id="_idTextAnchor045"/>How to do it…</h2>
<ol>
<li>First, we fetch the <strong class="source-inline">docker-compose.yaml</strong> file directly from the Airflow <span class="No-Break">official docs:</span><pre class="console">
<strong class="bold">my-project/airflow-local$ curl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.3.0/docker-compose.yaml'</strong></pre></li>
</ol>
<p>You should see output like this:</p>
<div>
<div class="IMG---Figure" id="_idContainer017">
<img alt="Figure 1.8 – Airflow container image download progress" height="48" src="image/Figure_1.8_B19453.jpg" width="635"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.8 – Airflow container image download progress</p>
<p class="callout-heading">Note</p>
<p class="callout">Check the most stable version of this <strong class="source-inline">docker-compose</strong> file when you download it, since new, more appropriate versions may be available after this book <span class="No-Break">is published.</span></p>
<ol>
<li value="2">Next, we create the <strong class="source-inline">dags</strong>, <strong class="source-inline">logs</strong>, and <strong class="source-inline">plugins</strong> folders <span class="No-Break">as follows:</span><pre class="console">
<strong class="bold">my-project/airflow-local$ mkdir ./dags ./logs ./plugins</strong></pre></li>
<li>Then, we create <a id="_idIndexMarker044"/>and set the Airflow user <span class="No-Break">as </span><span class="No-Break"><a id="_idIndexMarker045"/></span><span class="No-Break">follows:</span><pre class="console">
<strong class="bold">my-project/airflow-local$ echo -e "AIRFLOW_UID=$(id -u)\nAIRFLOW_GID=0" &gt; .env</strong></pre></li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">If you have any error messages related to the <strong class="source-inline">AIRFLOW_UID</strong> variable, you can create a <strong class="source-inline">.env</strong> file in the same folder where your <strong class="source-inline">docker-compose.yaml</strong> file is and define the variable <span class="No-Break">as </span><span class="No-Break"><strong class="source-inline">AIRFLOW_UID=50000</strong></span><span class="No-Break">.</span></p>
<ol>
<li value="4">Then, we initialize <span class="No-Break">the database:</span><pre class="console">
<strong class="bold">my-project/airflow-local$ docker-compose up airflow-init</strong></pre></li>
</ol>
<p>After executing the command, you should see output similar to this:</p>
<pre class="console">
<strong class="bold">Creating network "airflow-local_default" with the default driver</strong>
<strong class="bold">Creating volume "airflow-local_postgres-db-volume" with default driver</strong>
<strong class="bold">Pulling postgres (postgres:13)...</strong>
<strong class="bold">13: Pulling from library/postgres</strong>
<strong class="bold">(...)</strong>
<strong class="bold">Status: Downloaded newer image for postgres:13</strong>
<strong class="bold">Pulling redis (redis:latest)...</strong>
<strong class="bold">latest: Pulling from library/redis</strong>
<strong class="bold">bd159e379b3b: Already exists</strong>
<strong class="bold">(...)</strong>
<strong class="bold">Status: Downloaded newer image for redis:latest</strong>
<strong class="bold">Pulling airflow-init (apache/airflow:2.3.0)...</strong>
<strong class="bold">2.3.0: Pulling from apache/airflow</strong>
<strong class="bold">42c077c10790: Pull complete</strong>
<strong class="bold">(...)</strong>
<strong class="bold">Status: Downloaded newer image for apache/airflow:2.3.0</strong>
<strong class="bold">Creating airflow-local_postgres_1 ... done</strong>
<strong class="bold">Creating airflow-local_redis_1    ... done</strong>
<strong class="bold">Creating airflow-local_airflow-init_1 ... done</strong>
<strong class="bold">Attaching to airflow-local_airflow-init_1</strong>
<strong class="bold">(...)</strong>
<strong class="bold">airflow-init_1       | [2022-10-09 09:49:26,250] {manager.py:213} INFO - Added user airflow</strong>
<strong class="bold">airflow-init_1       | User "airflow" created with role "Admin"</strong>
<strong class="bold">(...)</strong>
<strong class="bold">airflow-local_airflow-init_1 exited with code 0</strong></pre>
<ol>
<li value="5">Then, we start the <span class="No-Break">Airflow service:</span><pre class="console">
<strong class="bold">my-project/airflow-local$ docker-compose up</strong></pre></li>
<li>Then, we need to check the Docker processes. Using the following CLI command, you will see the Docker <span class="No-Break">images running:</span><pre class="console">
<strong class="bold">my-project/airflow-local$ docker ps</strong></pre></li>
</ol>
<p>These are the images we see:</p>
<div>
<div class="IMG---Figure" id="_idContainer018">
<img alt="Figure 1.9 – The docker ps command output" height="169" src="image/Figure_1.9_B19453.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.9 – The docker ps command output</p>
<p>In the <a id="_idIndexMarker046"/>Docker Desktop application, you can also <a id="_idIndexMarker047"/>see the same containers running but with a more friendly interface:</p>
<div>
<div class="IMG---Figure" id="_idContainer019">
<img alt="Figure 1.10 – A Docker desktop view of the Airflow containers running" height="840" src="image/Figure_1.10_B19453.jpg" width="661"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.10 – A Docker desktop view of the Airflow containers running</p>
<ol>
<li value="7">Then, we access Airflow in a <span class="No-Break">web browser:</span></li>
</ol>
<p>In your preferred browser, type <strong class="source-inline">http://localhost:8080/home</strong>. The following screen will appear:</p>
<div>
<div class="IMG---Figure" id="_idContainer020">
<img alt="Figure 1.11 – The Airflow UI login page" height="467" src="image/Figure_1.11_B19453.jpg" width="724"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.11 – The Airflow UI login page</p>
<ol>
<li value="8">Then, we log in to the <a id="_idIndexMarker048"/>Airflow platform. Since it’s a local application used for testing and <a id="_idIndexMarker049"/>learning, the default credentials (username and password) for administrative access in Airflow <span class="No-Break">are </span><span class="No-Break"><strong class="source-inline">airflow</strong></span><span class="No-Break">.</span></li>
</ol>
<p>When logged in, the following screen will appear:</p>
<div>
<div class="IMG---Figure" id="_idContainer021">
<img alt="Figure 1.12 – The Airflow UI main page" height="559" src="image/Figure_1.12_B19453.jpg" width="1014"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.12 – The Airflow UI main page</p>
<ol>
<li value="9">Then, we stop our containers. We can stop our containers until we reach <a href="B19453_09.xhtml#_idTextAnchor319"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>, when we will explore data ingest <span class="No-Break">in Airflow:</span><pre class="console">
<strong class="bold">my-project/airflow-local$ docker-compose stop</strong></pre></li>
</ol>
<h2 id="_idParaDest-46"><a id="_idTextAnchor046"/>How it works…</h2>
<p>Airflow is an open source platform that allows batch data pipeline development, monitoring, and scheduling. However, it requires other components, such as an internal database, to store metadata to work correctly. In this example, we use PostgreSQL to store the metadata and <strong class="bold">Redis</strong> to <span class="No-Break">cache information.</span></p>
<p>All this can be <a id="_idIndexMarker050"/>installed directly in our machine environment one <a id="_idIndexMarker051"/>by one. Even though it seems quite simple, it may not be due to compatibility issues with OS, other software versions, and <span class="No-Break">so on.</span></p>
<p>Docker can create an isolated environment and provide all the requirements to make it work. With <strong class="source-inline">docker-compose</strong>, it becomes even simpler, since we can create dependencies between the components that can only be created if the others <span class="No-Break">are healthy.</span></p>
<p>You can also open the <strong class="source-inline">docker-compose.yaml</strong> file we downloaded for this recipe and take a look to explore it better. We will also cover it in detail in <a href="B19453_09.xhtml#_idTextAnchor319"><span class="No-Break"><em class="italic">Chapter 9</em></span></a><span class="No-Break">.</span></p>
<h2 id="_idParaDest-47"><a id="_idTextAnchor047"/>See also</h2>
<p>If you want to learn more about how this <strong class="source-inline">docker-compose</strong> file works, you can look at the Apache Airflow official Docker documentation on the Apache Airflow documentation <a id="_idIndexMarker052"/><span class="No-Break">page: </span><a href="https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.xhtml"><span class="No-Break">https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.xhtml</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-48"><a id="_idTextAnchor048"/>Creating schemas</h1>
<p><strong class="bold">Schemas</strong> are considered <a id="_idIndexMarker053"/>blueprints of a database or table. While some databases strictly require schema definition, others can work without it. However, in some cases, it is advantageous to work with data schemas to ensure that the application data architecture is maintained and can receive the desired <span class="No-Break">data input.</span></p>
<h2 id="_idParaDest-49"><a id="_idTextAnchor049"/>Getting ready</h2>
<p>Let’s imagine we need to create a database for a school to store information about the students, the courses, and the instructors. With this information, we know we have at least three tables <span class="No-Break">so far.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer022">
<img alt="Figure 1.13 – A table diagram for three entities" height="241" src="image/Figure_1.13_B19453.jpg" width="1128"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.13 – A table diagram for three entities</p>
<p>In this recipe, we will <a id="_idIndexMarker054"/>cover how schemas work using the <strong class="bold">Entity Relationship Diagram</strong> (<strong class="bold">ERD</strong>), a visual representation of relationships between <a id="_idIndexMarker055"/>entities in a database, to exemplify how schemas <span class="No-Break">are connected.</span></p>
<h2 id="_idParaDest-50"><a id="_idTextAnchor050"/>How to do it…</h2>
<p>Here are the steps to <span class="No-Break">try this:</span></p>
<ol>
<li>We define the type of schema. The following figure helps us understand how to go <span class="No-Break">about this:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer023">
<img alt="Figure 1.14 – A diagram to help you decide which schema to use" height="603" src="image/Figure_1.14_B19453.jpg" width="1260"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.14 – A diagram to help you decide which schema to use</p>
<ol>
<li value="2">Then, we <a id="_idIndexMarker056"/>define the fields and the data type for each <span class="No-Break">table column:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer024">
<img alt="Figure 1.15 – A definition of the columns of each table" height="254" src="image/Figure_1.15_B19453.jpg" width="476"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.15 – A definition of the columns of each table</p>
<ol>
<li value="3">Next, we define which fields can be empty <span class="No-Break">or </span><span class="No-Break"><strong class="source-inline">NULL</strong></span><span class="No-Break">:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer025">
<img alt="Figure 1.16 – A definition of which columns can be NULL" height="530" src="image/Figure_1.16_B19453.jpg" width="1457"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.16 – A definition of which columns can be NULL</p>
<ol>
<li value="4">Then, we create the relationship between <span class="No-Break">the tables:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer026">
<img alt="Figure 1.17 – A relationship diagram of the tables" height="746" src="image/Figure_1.17_B19453.jpg" width="1498"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.17 – A relationship diagram of the tables</p>
<h2 id="_idParaDest-51"><a id="_idTextAnchor051"/>How it works…</h2>
<p>When designing data schemas, the first thing we need to do is define their type. As we can see in the diagram in <em class="italic">step 1</em>, applying the schema architecture depends on the <span class="No-Break">data’s purpose.</span></p>
<p>After that, the tables <a id="_idIndexMarker057"/>are designed. Deciding how to define data types can vary, depending project or purpose, but deciding what values a column can receive is important. For instance, the <strong class="source-inline">officeRoom</strong> on <strong class="source-inline">Teacher</strong> table can be an <strong class="source-inline">Integer</strong> type if we know the room’s identification is always numeric, or a <strong class="source-inline">String</strong> type if it is unsure how identifications are made (for example, <span class="No-Break"><strong class="source-inline">Room 3-D</strong></span><span class="No-Break">).</span></p>
<p>Another important topic covered in <em class="italic">step 3</em> is how to define which of the columns can accept <strong class="source-inline">NULL</strong> fields. Can <a id="_idIndexMarker058"/>a field for a student’s name be empty? If not, we need to create a constraint to forbid this type <span class="No-Break">of insert.</span></p>
<p>Finally, based on the type of schema, a definition of the relationship between the tables <span class="No-Break">is made.</span></p>
<h2 id="_idParaDest-52"><a id="_idTextAnchor052"/>See also</h2>
<p>If you want to know more about database schema designs and their application, read this article by Mark <span class="No-Break">Smallcombe: </span><a href="https://www.integrate.io/blog/database-schema-examples/"><span class="No-Break">https://www.integrate.io/blog/database-schema-examples/</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-53"><a id="_idTextAnchor053"/>Applying data governance in ingestion</h1>
<p><strong class="bold">Data governance</strong> is a <a id="_idIndexMarker059"/>set of methodologies that ensure that data is secure, available, well-stored, documented, private, <span class="No-Break">and accurate.</span></p>
<h2 id="_idParaDest-54"><a id="_idTextAnchor054"/>Getting ready</h2>
<p><strong class="bold">Data ingestion</strong> is the <a id="_idIndexMarker060"/>beginning of the data pipeline process, but it doesn’t mean data governance is not heavily applied. The governance status in the final data pipeline output depends on how it was implemented during <span class="No-Break">the ingestion.</span></p>
<p>The following diagram shows how data ingestion is <span class="No-Break">commonly conducted:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer027">
<img alt="Figure 1.18 – The data ingestion process" height="615" src="image/Figure_1.18_B19453.jpg" width="1080"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.18 – The data ingestion process</p>
<p>Let’s analyze the steps in <span class="No-Break">the diagram:</span></p>
<ol>
<li><strong class="bold">Getting data from the source</strong>: The first step is to define the type of data, its periodicity, where we will gather it, and why we <span class="No-Break">need it.</span></li>
<li><strong class="bold">Writing the scripts to ingest data</strong>: Based on the answers to the previous step, we can begin planning how our code will behave and some <span class="No-Break">basic steps.</span></li>
<li><strong class="bold">Storing data in a temporary database or other types of storage</strong>: Between the ingest and the transformation phase, data is typically stored in a temporary database <span class="No-Break">or repository.</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer028">
<img alt="Figure 1.19 – Data governance pillars" height="782" src="image/Figure_1.19_B19453.jpg" width="830"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.19 – Data governance pillars</p>
<h2 id="_idParaDest-55"><a id="_idTextAnchor055"/>How to do it…</h2>
<p>Step by step, let’s <a id="_idIndexMarker061"/>attribute the pillars in <span class="No-Break"><em class="italic">Figure 1</em></span><em class="italic">.19</em> to the <span class="No-Break">ingestion phase:</span></p>
<ol>
<li>A concern for accessibility <a id="_idIndexMarker062"/>needs to be applied at the data source level, defining the individuals that are allowed to see or <span class="No-Break">retrieve data.</span></li>
<li>Next, it is necessary to catalog our data to understand it better. Since data ingestion is only covered here, it is more relevant to cover the <span class="No-Break">data sources.</span></li>
<li>The quality pillar will be applied to the ingestion and staging area, where we control the data and keep its quality aligned with <span class="No-Break">the source.</span></li>
<li>Then, let’s define ownership. We know the data source <em class="italic">belongs</em> to a business area or a company. However, when we ingested the data and put it in temporary or staging storage, it becomes our responsibility to <span class="No-Break">maintain it.</span></li>
<li>The last pillar involves <a id="_idIndexMarker063"/>keeping data secure for the whole pipeline. Security is vital in all steps, since we may be handling private or <span class="No-Break">sensitive information.</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer029">
<img alt="Figure 1.20 – Adding to data ingestion" height="1056" src="image/Figure_1.20_B19453.jpg" width="1305"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.20 – Adding to data ingestion</p>
<h2 id="_idParaDest-56"><a id="_idTextAnchor056"/>How it works…</h2>
<p>While some <a id="_idIndexMarker064"/>articles define “pillars” to create governance good practices, the best way to understand how to apply them is to understand how they are composed. As you saw in the previous <em class="italic">How to do it…</em> section, we attributed some items to our pipeline, and now we <a id="_idIndexMarker065"/>can understand how they are connected to the <span class="No-Break">following topics:</span></p>
<ul>
<li><strong class="bold">Data accessibility</strong>: Data accessibility is how people from a group, organization, or <a id="_idIndexMarker066"/>project can see and use data. The information needs to be readily available for use. At the same time, it needs to be available for the people involved in the process. For example, sensitive data accessibility should be restricted to some people or programs. In the diagram we built, we applied it to our data sources, since we need to understand and retrieve data. For the same reason, it can be applied for temporary storage needs <span class="No-Break">as well.</span></li>
<li><strong class="bold">Data catalog</strong>: Cataloging and documenting data are essential for business and engineering teams. When <a id="_idIndexMarker067"/>we know what types of information rely on our databases or data lakes and have quick access to these documents, the action time to solve a problem <span class="No-Break">becomes short.</span></li>
</ul>
<p>Again, documenting our data sources can make the ingest process quicker, since we need to make a discovery every time we need to ingest data.</p>
<ul>
<li><strong class="bold">Data quality</strong>: Quality is constantly preoccupied with ingesting, processing, and loading data. Tracking <a id="_idIndexMarker068"/>and monitoring data’s expected income and outcome by its periodicity is essential. For example, if we expect to ingest 300 GB of data per day and suddenly it drops to 1 GB, something is very wrong and will affect the quality of our final output. Other quality parameters can be the number of columns, partitioning, and so on, which we will explore later in <span class="No-Break">this book.</span></li>
<li><strong class="bold">Ownership</strong>: Who is responsible <a id="_idIndexMarker069"/>for the data? This definition is crucial to make contact with the owner if there are problems or attribute responsibility to keep and <span class="No-Break">maintain data.</span></li>
<li><strong class="bold">Security</strong>: A <a id="_idIndexMarker070"/>concerning topic nowadays is data security. With so many regulations about data privacy, it became an obligation of data engineers and scientists to know, at least, the basics of encryption, sensitive data, and how to avoid data leaks. Even languages and libraries that are used for work need to be evaluated. That’s why this item is attributed to the three steps in <span class="No-Break"><em class="italic">Figure 1</em></span><span class="No-Break"><em class="italic">.19</em></span><span class="No-Break">.</span></li>
</ul>
<p>In addition to the topics we <a id="_idIndexMarker071"/>explored, a global data governance project has a vital role called a <strong class="bold">data steward</strong>, which is responsible for managing an organization’s data assets and ensuring <a id="_idIndexMarker072"/>that data is accurate, consistent, and secure. In summary, data stewardship is managing and overseeing an organization’s <span class="No-Break">data assets.</span></p>
<h2 id="_idParaDest-57"><a id="_idTextAnchor057"/>See also</h2>
<p>You can read more about a recent vulnerability found in one of the most used tools for data engineering <span class="No-Break">here: </span><a href="https://www.ncsc.gov.uk/information/log4j-vulnerability-what-everyone-needs-to-know"><span class="No-Break">https://www.ncsc.gov.uk/information/log4j-vulnerability-what-everyone-needs-to-know</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-58"><a id="_idTextAnchor058"/>Implementing data replication</h1>
<p><strong class="bold">Data replication</strong> is a <a id="_idIndexMarker073"/>process applied in data environments to create multiple copies of data and store them on different locations, servers, or sites. This technique is commonly implemented to create better availability and avoid data loss if there is downtime, or even a natural disaster that affects a <span class="No-Break">data center.</span></p>
<h2 id="_idParaDest-59"><a id="_idTextAnchor059"/>Getting ready</h2>
<p>You will find across papers and articles different types (or even names) on the best way for <strong class="bold">data replication</strong> decision. In this recipe, you will learn how to decide which kind of replication better suits your application <span class="No-Break">or software.</span></p>
<h2 id="_idParaDest-60"><a id="_idTextAnchor060"/>How to do it…</h2>
<p>Let’s begin to build our fundamental pillars to implement <span class="No-Break">data replication:</span></p>
<ol>
<li>First, we need to decide the size of our replication, and it can be done using a portion or all the <span class="No-Break">stored data.</span></li>
<li>The next step is to consider when replication will take place. It can be done synchronously when new data arrives in storage or within a <span class="No-Break">specific timeframe.</span></li>
<li>The last fundamental pillar is whether the data is incremented or in a <span class="No-Break">bulk form.</span></li>
</ol>
<p>In the end, we will have a diagram that looks like the following:</p>
<div>
<div class="IMG---Figure" id="_idContainer030">
<img alt="Figure 1.21 – A data replication model decision diagram" height="513" src="image/Figure_1.21_B19453.jpg" width="1347"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.21 – A data replication model decision diagram</p>
<h2 id="_idParaDest-61"><a id="_idTextAnchor061"/>How it works…</h2>
<p>Analyzing the preceding figure, we have three main questions to answer, regarding the extension, the frequency, and whether our replication will be incremental <span class="No-Break">or bulk.</span></p>
<p>For the first <a id="_idIndexMarker074"/>question, we decide whether the replication will be complete or partial. In other words, either the data will consistently be replicated no matter what type of transaction or change was made, or just a portion of the data will be replicated. A real example of this would be keeping track of all store sales or just the most <span class="No-Break">expensive ones.</span></p>
<p>The second question, related to the frequency, is to decide when a replication needs to be done. This question also needs to take into consideration related costs. Real-time replication is often more expensive, but the synchronicity guarantees almost no <span class="No-Break">data inconsistency.</span></p>
<p>Lastly, it is relevant to consider how data will be transported to the replication site. In most cases, a scheduler with a script can replicate small data batches and reduce transportation costs. However, a bulk replication can be used in the data ingestion process, such as copying all the current batch’s raw data from a source to <span class="No-Break">cold storage.</span></p>
<h2 id="_idParaDest-62"><a id="_idTextAnchor062"/>There’s more…</h2>
<p>One method of data replication that has seen an increase in use in the past few years is <strong class="bold">cold storage</strong>, which is used to retain data used infrequently or is even inactive. The costs related to this type of replication are meager and guarantee data longevity. You can find cold storage solutions in <a id="_idIndexMarker075"/>all cloud providers, such as <strong class="bold">Amazon Glacier</strong>, <strong class="bold">Azure Cool Blob</strong>, and <strong class="bold">Google Cloud </strong><span class="No-Break"><strong class="bold">Storage Nearline</strong></span><span class="No-Break">.</span></p>
<p>Besides replication, regulatory compliance such as <strong class="bold">General Data Protection Regulation</strong> (<strong class="bold">GDPR</strong>) laws benefit <a id="_idIndexMarker076"/>from this type of storage, since, for some case scenarios, users’ data need to be kept for <span class="No-Break">some years.</span></p>
<p>In this chapter, we explored the basic concepts and laid the foundation for the following chapters and recipes in this book. We started with a Python installation, prepared our Docker containers, and saw data governance and replication concepts. You will observe over the upcoming chapters that almost all topics interconnect, and you will understand the relevance of understanding them at the beginning of the <span class="No-Break">ETL process.</span></p>
<h1 id="_idParaDest-63"><a id="_idTextAnchor063"/>Further reading</h1>
<ul>
<li><a href="https://www.manageengine.com/device-control/data-replication.xhtml"><span class="No-Break">https://www.manageengine.com/device-control/data-replication.xhtml</span></a></li>
</ul>
</div>
</div></body></html>