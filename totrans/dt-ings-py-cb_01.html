<html><head></head><body>
<div><div><h1 class="chapter-number" id="_idParaDest-22"><a id="_idTextAnchor022"/>1</h1>
<h1 id="_idParaDest-23"><a id="_idTextAnchor023"/>Introduction to Data Ingestion</h1>
<p>Welcome to the fantastic world of data! Are you ready to embark on a thrilling journey into data ingestion? If so, this is the perfect book to start! Ingesting data is the first step into the big data world.</p>
<p><strong class="bold">Data ingestion</strong> is a process <a id="_idIndexMarker000"/>that involves gathering and importing data and also <a id="_idIndexMarker001"/>storing it properly so that the subsequent <strong class="bold">extract, transform, and load</strong> (<strong class="bold">ETL</strong>) pipeline can utilize the data. To make it happen, we must be cautious about the tools we will use and how to configure them properly.</p>
<p>In our book journey, we will use <strong class="bold">Python</strong> and <strong class="bold">PySpark</strong> to retrieve data from different data sources and learn <a id="_idIndexMarker002"/>how to store them properly. To orchestrate all this, the basic <a id="_idIndexMarker003"/>concepts of <strong class="bold">Airflow</strong> will be implemented, along with efficient monitoring to guarantee that our pipelines are covered.</p>
<p>This chapter will introduce some basic concepts about data ingestion and how to set up your environment to start the tasks.</p>
<p>In this chapter, you will build and learn the following recipes:</p>
<ul>
<li>Setting up Python and the environment</li>
<li>Installing PySpark</li>
<li>Configuring Docker for MongoDB</li>
<li>Configuring Docker for Airflow</li>
<li>Logging libraries</li>
<li>Creating schemas</li>
<li>Applying data governance in ingestion</li>
<li>Implementing data replication</li>
</ul>
<h1 id="_idParaDest-24"><a id="_idTextAnchor024"/>Technical requirements</h1>
<p>The commands inside the recipes of this chapter use Linux syntax. If you don’t use a Linux-based system, you may need to adapt the commands:</p>
<ul>
<li>Docker or Docker Desktop</li>
<li>The SQL client of your choice (recommended); we recommend DBeaver, since it has a community-free version</li>
</ul>
<p>You can find the code from this chapter in this GitHub repository: <a href="https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook">https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook</a>.</p>
<p class="callout-heading">Note</p>
<p class="callout">Windows users might get an error message such as <strong class="bold">Docker Desktop requires a newer WSL kernel version.</strong> This can be fixed by following the steps here: <a href="https://docs.docker.com/desktop/windows/wsl/">https://docs.docker.com/desktop/windows/wsl/</a>.</p>
<h1 id="_idParaDest-25"><a id="_idTextAnchor025"/>Setting up Python and its environment</h1>
<p>In the data <a id="_idIndexMarker004"/>world, languages such as <strong class="bold">Java</strong>, <strong class="bold">Scala</strong>, or <strong class="bold">Python</strong> are commonly <a id="_idIndexMarker005"/>used. The first two languages are used due to their compatibility with the big data tools environment, such as <strong class="bold">Hadoop</strong> and <strong class="bold">Spark</strong>, the central core of which runs on a <strong class="bold">Java Virtual Machine</strong> (<strong class="bold">JVM</strong>). However, in <a id="_idIndexMarker006"/>the past few years, the use of Python for data engineering and data science has increased significantly due to the language’s versatility, ease of understanding, and many open source libraries built by the community.</p>
<h2 id="_idParaDest-26"><a id="_idTextAnchor026"/>Getting ready</h2>
<p>Let’s create a folder for our project:</p>
<ol>
<li>First, open your <a id="_idIndexMarker007"/>system command line. Since I use the <strong class="bold">Windows Subsystem for Linux</strong> (<strong class="bold">WSL</strong>), I will open the WSL application.</li>
<li>Go to your home directory and create a folder as follows:<pre class="console">
<strong class="bold">$ mkdir my-project</strong></pre></li>
<li>Go inside this folder:<pre class="console">
<strong class="bold">$ cd my-project</strong></pre></li>
<li>Check your Python version on your operating system as follows:<pre class="console">
<strong class="bold">$ python -–version</strong></pre></li>
</ol>
<p>Depending on your operational system, you might or might not have output here – for example, WSL 20.04 users might have the following output:</p>
<pre class="console">
<strong class="bold">Command 'python' not found, did you mean:</strong>
<strong class="bold"> command 'python3' from deb python3</strong>
<strong class="bold"> command 'python' from deb python-is-python3</strong></pre>
<p>If your Python <a id="_idIndexMarker008"/>path is configured to use the <code>python</code> command, you will <a id="_idIndexMarker009"/>see output similar to this:</p>
<pre class="console">
<strong class="bold">Python 3.9.0</strong></pre>
<p>Sometimes, your Python path might be configured to be invoked using <code>python3</code>. You can try it using the following command:</p>
<pre class="console">
<strong class="bold">$ python3 --version</strong></pre>
<p>The output will be similar to the <code>python</code> command, as follows:</p>
<pre class="console">
<strong class="bold">Python 3.9.0</strong></pre>
<ol>
<li value="5">Now, let’s check our <code>pip</code> version. This check is essential, since some operating systems have more than one Python version installed:<pre class="console">
<strong class="bold">$ pip --version</strong></pre></li>
</ol>
<p>You should see similar output:</p>
<pre class="source-code">
pip 20.0.2 from /usr/lib/python3/dist-packages/pip (python 3.9)</pre>
<p>If your <code>3.8x</code> or doesn’t have the language <a id="_idIndexMarker010"/>installed, proceed to the <em class="italic">How to do it</em> steps; otherwise, you are ready to start the following <em class="italic">Installing </em><em class="italic">PySpark</em> recipe.</p>
<h2 id="_idParaDest-27"><a id="_idTextAnchor027"/>How to do it…</h2>
<p>We are going to <a id="_idIndexMarker011"/>use the official installer from Python.org. You can find the <a id="_idIndexMarker012"/>link for it here: <a href="https://www.python.org/downloads/">https://www.python.org/downloads/</a>:</p>
<p class="callout-heading">Note</p>
<p class="callout">For Windows users, it is important to check your OS version, since Python 3.10 may not be yet compatible with Windows 7, or your processor type (32-bits or 64-bits).</p>
<ol>
<li>Download one of the stable versions.</li>
</ol>
<p>At the time of writing, the stable recommended versions compatible with the tools and resources presented here are <code>3.8</code>, <code>3.9</code>, and <code>3.10</code>. I will use the <code>3.9</code> version and download it using the following link: <a href="https://www.python.org/downloads/release/python-390/">https://www.python.org/downloads/release/python-390/</a>. Scrolling down the page, you will find a list of links to Python installers according to OS, as shown in the following screenshot.</p>
<div><div><img alt="Figure 1.1 – Python.org download files for version 3.9" height="822" src="img/Figure_1.1_B19453.jpg" width="1447"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.1 – Python.org download files for version 3.9</p>
<ol>
<li value="2">After downloading the installation file, double-click it and follow the instructions in the wizard <a id="_idIndexMarker013"/>window. To avoid complexity, choose the <a id="_idIndexMarker014"/>recommended settings displayed.</li>
</ol>
<p>The following screenshot shows how it looks on Windows:</p>
<div><div><img alt="Figure 1.2 – The Python Installer for Windows" height="398" src="img/Figure_1.2_B19453.jpg" width="660"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.2 – The Python Installer for Windows</p>
<ol>
<li value="3">If you are a Linux user, you can install it from the source using the following commands:<pre class="console">
<strong class="bold">$ wget https://www.python.org/ftp/python/3.9.1/Python-3.9.1.tgz</strong>
<strong class="bold">$ tar -xf Python-3.9.1.tgz</strong>
<strong class="bold">$ ./configure –enable-optimizations</strong>
<strong class="bold">$ make -j 9</strong></pre></li>
</ol>
<p>After installing Python, you should be able to execute the <code>pip</code> command. If not, refer to the <code>pip</code> official documentation page here: <a href="https://pip.pypa.io/en/stable/installation/">https://pip.pypa.io/en/stable/installation/</a>.</p>
<h2 id="_idParaDest-28"><a id="_idTextAnchor028"/>How it works…</h2>
<p>Python is an <strong class="bold">interpreted language</strong>, and its interpreter extends several functions made with <strong class="bold">C</strong> or <strong class="bold">C++</strong>. The language package also comes with several built-in libraries and, of course, the interpreter.</p>
<p>The interpreter <a id="_idIndexMarker015"/>works like a Unix shell and can be found in <a id="_idIndexMarker016"/>the <code>usr/local/bin</code> directory: <a href="https://docs.python.org/3/tutorial/interpreter.xhtml">https://docs.python.org/3/tutorial/interpreter.xhtml</a>.</p>
<p>Lastly, note that many Python third-party packages in this book require the <code>pip</code> command to be installed. This is because <code>pip</code> (an acronym for <strong class="bold">Pip Installs Packages</strong>) is the default <a id="_idIndexMarker017"/>package manager for Python; therefore, it is used <a id="_idIndexMarker018"/>to install, upgrade, and manage the Python packages and dependencies from the <strong class="bold">Python Package </strong><strong class="bold">Index</strong> (<strong class="bold">PyPI</strong>).</p>
<h2 id="_idParaDest-29"><a id="_idTextAnchor029"/>There’s more…</h2>
<p>Even if you don’t have any Python versions on your machine, you can still install them using the command line or <strong class="bold">HomeBrew</strong> (for <strong class="bold">macOS</strong> users). Windows users can also download them from the MS Windows Store.</p>
<p class="callout-heading">Note</p>
<p class="callout">If you choose to download Python from the Windows Store, ensure you use an application made by the Python Software Foundation.</p>
<h2 id="_idParaDest-30"><a id="_idTextAnchor030"/>See also</h2>
<p>You can use <code>pip</code> to install convenient third-party applications, such as Jupyter. This is an open source, web-based, interactive (and user-friendly) computing platform, often used by data scientists and data engineers. You can install it from the official website here: <a href="https://jupyter.org/install">https://jupyter.org/install</a>.</p>
<h1 id="_idParaDest-31"><a id="_idTextAnchor031"/>Installing PySpark</h1>
<p>To process, clean, and transform vast amounts of data, we need a tool that provides resilience and <a id="_idIndexMarker019"/>distributed processing, and that’s why <strong class="bold">PySpark</strong> is a good fit. It gets an API over the Spark library that lets you use its applications.</p>
<h2 id="_idParaDest-32"><a id="_idTextAnchor032"/>Getting ready</h2>
<p>Before starting the PySpark installation, we need to check our Java version in our operational system:</p>
<ol>
<li>Here, we check the Java version:<pre class="console">
<strong class="bold">$ java -version</strong></pre></li>
</ol>
<p>You should see output similar to this:</p>
<pre class="console">
<strong class="bold">openjdk version "1.8.0_292"</strong>
<strong class="bold">OpenJDK Runtime Environment (build 1.8.0_292-8u292-b10-0ubuntu1~20.04-b10)</strong>
<strong class="bold">OpenJDK 64-Bit Server VM (build 25.292-b10, mixed mode)</strong></pre>
<p>If everything is correct, you should see the preceding message as the output of the command and the <strong class="bold">OpenJDK 18</strong> version or higher. However, some systems don’t have any Java version installed by default, and to cover this, we need to proceed to <em class="italic">step 2</em>.</p>
<ol>
<li value="2">Now, we <a id="_idIndexMarker020"/>download the <strong class="bold">Java Development </strong><strong class="bold">Kit</strong> (<strong class="bold">JDK</strong>).</li>
</ol>
<p>Go to <a href="https://www.oracle.com/java/technologies/downloads/">https://www.oracle.com/java/technologies/downloads/</a>, select your <strong class="bold">OS</strong>, and download <a id="_idIndexMarker021"/>the most recent version of JDK. At the time of writing, it is JDK 19.</p>
<p>The download page of the JDK will look as follows:</p>
<div><div><img alt="Figure 1.3 – The JDK 19 downloads official web page" height="784" src="img/Figure_1.3_B19453.jpg" width="979"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.3 – The JDK 19 downloads official web page</p>
<p>Execute the downloaded application. Click on the application to start the installation process. The following window will appear:</p>
<p class="callout-heading">Note</p>
<p class="callout">Depending on your OS, the installation window may appear slightly different.</p>
<div><div><img alt="Figure 1.4 – The Java installation wizard window" height="429" src="img/Figure_1.4_B19453.jpg" width="763"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.4 – The Java installation wizard window</p>
<p>Click <strong class="bold">Next</strong> for the following two questions, and the application will start the installation. You <a id="_idIndexMarker022"/>don’t need to worry about where the JDK will be installed. By default, the application is configured, as standard, to be compatible with other tools’ installations.</p>
<ol>
<li value="3">Next, we again check our Java version. When executing the command again, you should see the following version:<pre class="console">
<strong class="bold">$ java -version</strong>
<strong class="bold">openjdk version "1.8.0_292"</strong>
<strong class="bold">OpenJDK Runtime Environment (build 1.8.0_292-8u292-b10-0ubuntu1~20.04-b10)</strong>
<strong class="bold">OpenJDK 64-Bit Server VM (build 25.292-b10, mixed mode)</strong></pre></li>
</ol>
<h2 id="_idParaDest-33"><a id="_idTextAnchor033"/>How to do it…</h2>
<p>Here are the steps to perform this recipe:</p>
<ol>
<li>Install PySpark from PyPi:<pre class="console">
<strong class="bold">$ pip install pyspark</strong></pre></li>
</ol>
<p>If the command runs successfully, the installation output’s last line will look like this:</p>
<pre class="console">
Successfully built pyspark
Installing collected packages: py4j, pyspark
Successfully installed py4j-0.10.9.5 pyspark-3.3.2</pre>
<ol>
<li value="2">Execute the <code>pyspark</code> command to open the interactive shell. When executing the <code>pyspark</code> command <a id="_idIndexMarker023"/>in your command line, you should see this message:<pre class="console">
<strong class="bold">$ pyspark</strong>
<strong class="bold">Python 3.8.10 (default, Jun 22 2022, 20:18:18)</strong>
<strong class="bold">[GCC 9.4.0] on linux</strong>
<strong class="bold">Type "help", "copyright", "credits" or "license" for more information.</strong>
<strong class="bold">22/10/08 15:06:11 WARN Utils: Your hostname, DESKTOP-DVUDB98 resolves to a loopback address: 127.0.1.1; using 172.29.214.162 instead (on interface eth0)</strong>
<strong class="bold">22/10/08 15:06:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address</strong>
<strong class="bold">22/10/08 15:06:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</strong>
<strong class="bold">Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties</strong>
<strong class="bold">Setting default log level to "WARN".</strong>
<strong class="bold">To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).</strong>
<strong class="bold">Welcome to</strong>
<strong class="bold">      ____              __</strong>
<strong class="bold">     / __/__  ___ _____/ /__</strong>
<strong class="bold">    _\ \/ _ \/ _ `/ __/</strong><strong class="bold">  '_/</strong>
<strong class="bold">   /__ / .__/\_,_/_/ /_/\_\   version 3.1.2</strong>
<strong class="bold">      /_/</strong>
<strong class="bold">Using Python version 3.8.10 (default, Jun 22 2022 20:18:18)</strong>
<strong class="bold">Spark context Web UI available at http://172.29.214.162:4040</strong>
<strong class="bold">Spark context available as 'sc' (master = local[*], app id = local-1665237974112).</strong>
<strong class="bold">SparkSession available as 'spark'.</strong>
<strong class="bold">&gt;&gt;&gt;</strong></pre></li>
</ol>
<p>You can observe some interesting messages here, such as the Spark version and the Python used from PySpark.</p>
<ol>
<li value="3">Finally, we exit the interactive shell as follows:<pre class="console">
<strong class="bold">&gt;&gt;&gt; exit()</strong>
<strong class="bold">$</strong></pre></li>
</ol>
<h2 id="_idParaDest-34"><a id="_idTextAnchor034"/>How it works…</h2>
<p>As seen at the beginning of this recipe, Spark is a robust framework that runs on top of the JVM. It is also an open source tool for creating resilient and distributed processing output from vast <a id="_idIndexMarker024"/>data. With the growth in popularity of the Python language in the past few years, it became necessary to have a solution that adapts Spark to run alongside Python.</p>
<p>PySpark is an interface that interacts with <strong class="bold">Spark APIs via Py4J</strong>, dynamically allowing Python code to interact with the JVM. We first need to have Java installed on our OS to use Spark. When we install PySpark, it already comes with Spark and Py4J components installed, making it easy to start the application and build the code.</p>
<h2 id="_idParaDest-35"><a id="_idTextAnchor035"/>There’s more…</h2>
<p>Anaconda is a convenient <a id="_idIndexMarker025"/>way to install PySpark and other data science tools. This tool encapsulates all manual processes and has a friendly interface for interacting with and installing Python components, such as <strong class="bold">NumPy</strong>, <strong class="bold">pandas</strong>, or <strong class="bold">Jupyter</strong>:</p>
<ol>
<li>To install Anaconda, go to the official website and select <strong class="bold">Products</strong> | <strong class="bold">Anaconda </strong><strong class="bold">Distribution</strong>: <a href="https://www.anaconda.com/products/distribution">https://www.anaconda.com/products/distribution</a>.</li>
<li>Download the distribution according to your OS.</li>
</ol>
<p>For more <a id="_idIndexMarker026"/>detailed information about how to install Anaconda and other powerful commands, refer to <a href="https://docs.anaconda.com/">https://docs.anaconda.com/</a>.</p>
<h3>Using virtualenv with PySpark</h3>
<p>It is possible <a id="_idIndexMarker027"/>to configure and use <code>virtualenv</code> with PySpark, and Anaconda <a id="_idIndexMarker028"/>does it automatically if you choose this type of installation. However, for the other installation methods, we need to make some additional steps to make our Spark cluster (locally or on the server) run it, which includes indicating the <code>virtualenv /bin/</code> folder and where your PySpark path is.</p>
<h2 id="_idParaDest-36"><a id="_idTextAnchor036"/>See also</h2>
<p>There is a nice article about this topic, <em class="italic">Using VirtualEnv with PySpark</em>, by jzhang, here: <a href="https://community.cloudera.com/t5/Community-Articles/Using-VirtualEnv-with-PySpark/ta-p/245932">https://community.cloudera.com/t5/Community-Articles/Using-VirtualEnv-with-PySpark/ta-p/245932</a>.</p>
<h1 id="_idParaDest-37"><a id="_idTextAnchor037"/>Configuring Docker for MongoDB</h1>
<p><strong class="bold">MongoDB</strong> is a <strong class="bold">Not Only SQL</strong> (<strong class="bold">NoSQL</strong>) document-oriented database, widely used to store <strong class="bold">Internet of Things</strong> (<strong class="bold">IoT</strong>) data, application <a id="_idIndexMarker029"/>logs, and so on. A NoSQL <a id="_idIndexMarker030"/>database is a non-relational <a id="_idIndexMarker031"/>database that stores unstructured data differently <a id="_idIndexMarker032"/>from relational databases such as MySQL or PostgreSQL. Don’t worry too much about this now; we will cover it in more detail in <a href="B19453_05.xhtml#_idTextAnchor161"><em class="italic">Chapter 5</em></a>.</p>
<p>Your cluster production environment can handle huge amounts of data and create resilient data storage.</p>
<h2 id="_idParaDest-38"><a id="_idTextAnchor038"/>Getting ready</h2>
<p>Following the good practice of code organization, let’s start creating a folder inside our project to store the Docker image:</p>
<p>Create a folder inside our project directory to store the MongoDB Docker image and data as follows:</p>
<pre class="console">
my-project$ mkdir mongo-local
my-project$ cd mongo-local</pre>
<h2 id="_idParaDest-39"><a id="_idTextAnchor039"/>How to do it…</h2>
<p>Here are the steps to try out this recipe:</p>
<ol>
<li>First, we <a id="_idIndexMarker033"/>pull the Docker image from Docker Hub <a id="_idIndexMarker034"/>as follows:<pre class="console">
<strong class="bold">my-project/mongo-local$ docker pull mongo</strong></pre></li>
</ol>
<p>You should see the following message in your command line:</p>
<pre class="console">
<strong class="bold">Using default tag: latest</strong>
<strong class="bold">latest: Pulling from library/mongo</strong>
<strong class="bold">(...)</strong>
<strong class="bold">bc8341d9c8d5: Pull complete</strong>
<strong class="bold">(...)</strong>
<strong class="bold">Status: Downloaded newer image for mongo:latest</strong>
<strong class="bold">docker.io/library/mongo:latest</strong></pre>
<p class="callout-heading">Note</p>
<p class="callout">If you are a WSL user, an error might occur if you use the WSL 1 version instead of version 2. You can easily fix this by following the steps here: <a href="https://learn.microsoft.com/en-us/windows/wsl/install">https://learn.microsoft.com/en-us/windows/wsl/install</a>.</p>
<ol>
<li value="2">Then, we run the MongoDB server as follows:<pre class="console">
<strong class="bold">my-project/mongo-local$ docker run \</strong>
<strong class="bold">--name mongodb-local \</strong>
<strong class="bold">-p 27017:27017 \</strong>
<strong class="bold">-e MONGO_INITDB_ROOT_USERNAME="your_username" \</strong>
<strong class="bold">-e MONGO_INITDB_ROOT_PASSWORD="your_password"\</strong>
<strong class="bold">-d mongo:latest</strong></pre></li>
</ol>
<p>We then check <a id="_idIndexMarker035"/>our server. To do this, we can use the <a id="_idIndexMarker036"/>command line to see which Docker images are running:</p>
<pre class="console">
<strong class="bold">my-project/mongo-local$ docker ps</strong></pre>
<p>We then see this on the screen:</p>
<div><div><img alt="Figure 1.5 – MongoDB and Docker running container" height="49" src="img/Figure_1.5_B19453.jpg" width="1152"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.5 – MongoDB and Docker running container</p>
<p>We can even check on the Docker Desktop application to see whether our container is running:</p>
<div><div><img alt="Figure 1.6 – The Docker Desktop vision of the MongoDB container running" height="64" src="img/Figure_1.6_B19453.jpg" width="270"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.6 – The Docker Desktop vision of the MongoDB container running</p>
<ol>
<li value="3">Finally, we need to stop our container. We need to use <code>Container ID</code> to stop the container, which we previously saw when checking the Docker running images. We will rerun it in <a href="B19453_05.xhtml#_idTextAnchor161"><em class="italic">Chapter 5</em></a>:<pre class="console">
<strong class="bold">my-project/mongo-local$ docker stop 427cc2e5d40e</strong></pre></li>
</ol>
<h2 id="_idParaDest-40"><a id="_idTextAnchor040"/>How it works…</h2>
<p>MongoDB’s architecture uses the concept of <code>main</code> node interacts <a id="_idIndexMarker037"/>with clients’ requests, such as queries and <a id="_idIndexMarker038"/>document manipulation. It distributes the requests automatically among its shards, which are a subset of a larger data collection here.</p>
<div><div><img alt="Figure 1.7 – MongoDB architecture" height="1141" src="img/Figure_1.7_B19453.jpg" width="1201"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.7 – MongoDB architecture</p>
<p>Since we may also have other running projects or software applications inside our machine, isolating any database or application server used in development is a good practice. In this way, we ensure nothing interferes with our local servers, and the debug process can be more manageable.</p>
<p>This Docker image setting <a id="_idIndexMarker039"/>creates a MongoDB server locally and even <a id="_idIndexMarker040"/>allows us to make additional changes if we want to simulate any other scenario for testing or development.</p>
<p>The commands we used are as follows:</p>
<ul>
<li>The <code>--name</code> command defines the name we give to our container.</li>
<li>The <code>-p</code> command specifies the port our container will open so that we can access it via <code>localhost:27017</code>.</li>
<li><code>-e</code> command defines the environment variables. In this case, we set the <code>root</code> username and password for our MongoDB container.</li>
<li><code>-d</code> is detached mode – that is, the Docker process will run in the background, and we will not see input or output. However, we can still use <code>docker ps</code> to check the container status.</li>
<li><code>mongo:latest</code> indicates Docker pulling this image’s latest version.</li>
</ul>
<h2 id="_idParaDest-41"><a id="_idTextAnchor041"/>There’s more…</h2>
<p>For frequent users, manually configuring other parameters for the MongoDB container, such as the version, image port, database name, and database credentials, is also possible.</p>
<p>A version of this image with example values is also available as a <code>docker-compose</code> file in the official <a id="_idIndexMarker041"/>documentation here: <a href="https://hub.docker.com/_/mongo">https://hub.docker.com/_/mongo</a>.</p>
<p>The <code>docker-compose</code> file for MongoDB looks similar to this:</p>
<pre class="console">
# Use your own values for username and password
version: '3.1'
services:
  mongo:
    image: mongo
    restart: always
    environment:
      MONGO_INITDB_ROOT_USERNAME: root
      MONGO_INITDB_ROOT_PASSWORD: example
  mongo-express:
    image: mongo-express
    restart: always
    ports:
      - 8081:8081
    environment:
      ME_CONFIG_MONGODB_ADMINUSERNAME: root
      ME_CONFIG_MONGODB_ADMINPASSWORD: example
      ME_CONFIG_MONGODB_URL: mongodb://root:example@mongo:27017/</pre>
<h2 id="_idParaDest-42"><a id="_idTextAnchor042"/>See also</h2>
<p>You can check out MongoDB at the complete Docker Hub documentation here: <a href="https://hub.docker.com/_/mongo">https://hub.docker.com/_/mongo</a>.</p>
<h1 id="_idParaDest-43"><a id="_idTextAnchor043"/>Configuring Docker for Airflow</h1>
<p>In this book, we <a id="_idIndexMarker042"/>will use <strong class="bold">Airflow</strong> to orchestrate data ingests and <a id="_idIndexMarker043"/>provide logs to monitor our pipelines.</p>
<p>Airflow can be installed directly on your local machine and any server using PyPi<strong class="bold"> </strong>(<a href="https://pypi.org/project/apache-airflow/">https://pypi.org/project/apache-airflow/</a>) or a Docker container (<a href="https://hub.docker.com/r/apache/airflow">https://hub.docker.com/r/apache/airflow</a>). An official and supported version of Airflow can be found on Docker Hub, and the <strong class="bold">Apache Foundation</strong> community maintains it.</p>
<p>However, there are some additional steps to configure our Airflow. Thankfully, the Apache Foundation also has a <code>docker-compose</code> file that contains all other requirements to make Airflow work. We just need to complete a few more steps.</p>
<h2 id="_idParaDest-44"><a id="_idTextAnchor044"/>Getting ready</h2>
<p>Let’s start by initializing our Docker application on our machine. You can use the desktop version or the CLI command.</p>
<p>Make sure you are inside your project folder for this. Create a folder to store Airflow internal components and the <code>docker-compose.yaml</code> file:</p>
<pre class="console">
my-project$ mkdir airflow-local
my-project$ cd airflow-local</pre>
<h2 id="_idParaDest-45"><a id="_idTextAnchor045"/>How to do it…</h2>
<ol>
<li>First, we fetch the <code>docker-compose.yaml</code> file directly from the Airflow official docs:<pre class="console">
<strong class="bold">my-project/airflow-local$ curl -LfO 'https://airflow.apache.org/docs/apache-airflow/2.3.0/docker-compose.yaml'</strong></pre></li>
</ol>
<p>You should see output like this:</p>
<div><div><img alt="Figure 1.8 – Airflow container image download progress" height="48" src="img/Figure_1.8_B19453.jpg" width="635"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.8 – Airflow container image download progress</p>
<p class="callout-heading">Note</p>
<p class="callout">Check the most stable version of this <code>docker-compose</code> file when you download it, since new, more appropriate versions may be available after this book is published.</p>
<ol>
<li value="2">Next, we create the <code>dags</code>, <code>logs</code>, and <code>plugins</code> folders as follows:<pre class="console">
<strong class="bold">my-project/airflow-local$ mkdir ./dags ./logs ./plugins</strong></pre></li>
<li>Then, we create <a id="_idIndexMarker044"/>and set the Airflow user as <a id="_idIndexMarker045"/>follows:<pre class="console">
<strong class="bold">my-project/airflow-local$ echo -e "AIRFLOW_UID=$(id -u)\nAIRFLOW_GID=0" &gt; .env</strong></pre></li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">If you have any error messages related to the <code>AIRFLOW_UID</code> variable, you can create a <code>.env</code> file in the same folder where your <code>docker-compose.yaml</code> file is and define the variable as <code>AIRFLOW_UID=50000</code>.</p>
<ol>
<li value="4">Then, we initialize the database:<pre class="console">
<strong class="bold">my-project/airflow-local$ docker-compose up airflow-init</strong></pre></li>
</ol>
<p>After executing the command, you should see output similar to this:</p>
<pre class="console">
<strong class="bold">Creating network "airflow-local_default" with the default driver</strong>
<strong class="bold">Creating volume "airflow-local_postgres-db-volume" with default driver</strong>
<strong class="bold">Pulling postgres (postgres:13)...</strong>
<strong class="bold">13: Pulling from library/postgres</strong>
<strong class="bold">(...)</strong>
<strong class="bold">Status: Downloaded newer image for postgres:13</strong>
<strong class="bold">Pulling redis (redis:latest)...</strong>
<strong class="bold">latest: Pulling from library/redis</strong>
<strong class="bold">bd159e379b3b: Already exists</strong>
<strong class="bold">(...)</strong>
<strong class="bold">Status: Downloaded newer image for redis:latest</strong>
<strong class="bold">Pulling airflow-init (apache/airflow:2.3.0)...</strong>
<strong class="bold">2.3.0: Pulling from apache/airflow</strong>
<strong class="bold">42c077c10790: Pull complete</strong>
<strong class="bold">(...)</strong>
<strong class="bold">Status: Downloaded newer image for apache/airflow:2.3.0</strong>
<strong class="bold">Creating airflow-local_postgres_1 ... done</strong>
<strong class="bold">Creating airflow-local_redis_1    ... done</strong>
<strong class="bold">Creating airflow-local_airflow-init_1 ... done</strong>
<strong class="bold">Attaching to airflow-local_airflow-init_1</strong>
<strong class="bold">(...)</strong>
<strong class="bold">airflow-init_1       | [2022-10-09 09:49:26,250] {manager.py:213} INFO - Added user airflow</strong>
<strong class="bold">airflow-init_1       | User "airflow" created with role "Admin"</strong>
<strong class="bold">(...)</strong>
<strong class="bold">airflow-local_airflow-init_1 exited with code 0</strong></pre>
<ol>
<li value="5">Then, we start the Airflow service:<pre class="console">
<strong class="bold">my-project/airflow-local$ docker-compose up</strong></pre></li>
<li>Then, we need to check the Docker processes. Using the following CLI command, you will see the Docker images running:<pre class="console">
<strong class="bold">my-project/airflow-local$ docker ps</strong></pre></li>
</ol>
<p>These are the images we see:</p>
<div><div><img alt="Figure 1.9 – The docker ps command output" height="169" src="img/Figure_1.9_B19453.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.9 – The docker ps command output</p>
<p>In the <a id="_idIndexMarker046"/>Docker Desktop application, you can also <a id="_idIndexMarker047"/>see the same containers running but with a more friendly interface:</p>
<div><div><img alt="Figure 1.10 – A Docker desktop view of the Airflow containers running" height="840" src="img/Figure_1.10_B19453.jpg" width="661"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.10 – A Docker desktop view of the Airflow containers running</p>
<ol>
<li value="7">Then, we access Airflow in a web browser:</li>
</ol>
<p>In your preferred browser, type <code>http://localhost:8080/home</code>. The following screen will appear:</p>
<div><div><img alt="Figure 1.11 – The Airflow UI login page" height="467" src="img/Figure_1.11_B19453.jpg" width="724"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.11 – The Airflow UI login page</p>
<ol>
<li value="8">Then, we log in to the <a id="_idIndexMarker048"/>Airflow platform. Since it’s a local application used for testing and <a id="_idIndexMarker049"/>learning, the default credentials (username and password) for administrative access in Airflow are <code>airflow</code>.</li>
</ol>
<p>When logged in, the following screen will appear:</p>
<div><div><img alt="Figure 1.12 – The Airflow UI main page" height="559" src="img/Figure_1.12_B19453.jpg" width="1014"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.12 – The Airflow UI main page</p>
<ol>
<li value="9">Then, we stop our containers. We can stop our containers until we reach <a href="B19453_09.xhtml#_idTextAnchor319"><em class="italic">Chapter 9</em></a>, when we will explore data ingest in Airflow:<pre class="console">
<strong class="bold">my-project/airflow-local$ docker-compose stop</strong></pre></li>
</ol>
<h2 id="_idParaDest-46"><a id="_idTextAnchor046"/>How it works…</h2>
<p>Airflow is an open source platform that allows batch data pipeline development, monitoring, and scheduling. However, it requires other components, such as an internal database, to store metadata to work correctly. In this example, we use PostgreSQL to store the metadata and <strong class="bold">Redis</strong> to cache information.</p>
<p>All this can be <a id="_idIndexMarker050"/>installed directly in our machine environment one <a id="_idIndexMarker051"/>by one. Even though it seems quite simple, it may not be due to compatibility issues with OS, other software versions, and so on.</p>
<p>Docker can create an isolated environment and provide all the requirements to make it work. With <code>docker-compose</code>, it becomes even simpler, since we can create dependencies between the components that can only be created if the others are healthy.</p>
<p>You can also open the <code>docker-compose.yaml</code> file we downloaded for this recipe and take a look to explore it better. We will also cover it in detail in <a href="B19453_09.xhtml#_idTextAnchor319"><em class="italic">Chapter 9</em></a>.</p>
<h2 id="_idParaDest-47"><a id="_idTextAnchor047"/>See also</h2>
<p>If you want to learn more about how this <code>docker-compose</code> file works, you can look at the Apache Airflow official Docker documentation on the Apache Airflow documentation <a id="_idIndexMarker052"/>page: <a href="https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.xhtml">https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.xhtml</a>.</p>
<h1 id="_idParaDest-48"><a id="_idTextAnchor048"/>Creating schemas</h1>
<p><strong class="bold">Schemas</strong> are considered <a id="_idIndexMarker053"/>blueprints of a database or table. While some databases strictly require schema definition, others can work without it. However, in some cases, it is advantageous to work with data schemas to ensure that the application data architecture is maintained and can receive the desired data input.</p>
<h2 id="_idParaDest-49"><a id="_idTextAnchor049"/>Getting ready</h2>
<p>Let’s imagine we need to create a database for a school to store information about the students, the courses, and the instructors. With this information, we know we have at least three tables so far.</p>
<div><div><img alt="Figure 1.13 – A table diagram for three entities" height="241" src="img/Figure_1.13_B19453.jpg" width="1128"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.13 – A table diagram for three entities</p>
<p>In this recipe, we will <a id="_idIndexMarker054"/>cover how schemas work using the <strong class="bold">Entity Relationship Diagram</strong> (<strong class="bold">ERD</strong>), a visual representation of relationships between <a id="_idIndexMarker055"/>entities in a database, to exemplify how schemas are connected.</p>
<h2 id="_idParaDest-50"><a id="_idTextAnchor050"/>How to do it…</h2>
<p>Here are the steps to try this:</p>
<ol>
<li>We define the type of schema. The following figure helps us understand how to go about this:</li>
</ol>
<div><div><img alt="Figure 1.14 – A diagram to help you decide which schema to use" height="603" src="img/Figure_1.14_B19453.jpg" width="1260"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.14 – A diagram to help you decide which schema to use</p>
<ol>
<li value="2">Then, we <a id="_idIndexMarker056"/>define the fields and the data type for each table column:</li>
</ol>
<div><div><img alt="Figure 1.15 – A definition of the columns of each table" height="254" src="img/Figure_1.15_B19453.jpg" width="476"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.15 – A definition of the columns of each table</p>
<ol>
<li value="3">Next, we define which fields can be empty or <code>NULL</code>:</li>
</ol>
<div><div><img alt="Figure 1.16 – A definition of which columns can be NULL" height="530" src="img/Figure_1.16_B19453.jpg" width="1457"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.16 – A definition of which columns can be NULL</p>
<ol>
<li value="4">Then, we create the relationship between the tables:</li>
</ol>
<div><div><img alt="Figure 1.17 – A relationship diagram of the tables" height="746" src="img/Figure_1.17_B19453.jpg" width="1498"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.17 – A relationship diagram of the tables</p>
<h2 id="_idParaDest-51"><a id="_idTextAnchor051"/>How it works…</h2>
<p>When designing data schemas, the first thing we need to do is define their type. As we can see in the diagram in <em class="italic">step 1</em>, applying the schema architecture depends on the data’s purpose.</p>
<p>After that, the tables <a id="_idIndexMarker057"/>are designed. Deciding how to define data types can vary, depending project or purpose, but deciding what values a column can receive is important. For instance, the <code>officeRoom</code> on <code>Teacher</code> table can be an <code>Integer</code> type if we know the room’s identification is always numeric, or a <code>String</code> type if it is unsure how identifications are made (for example, <code>Room 3-D</code>).</p>
<p>Another important topic covered in <em class="italic">step 3</em> is how to define which of the columns can accept <code>NULL</code> fields. Can <a id="_idIndexMarker058"/>a field for a student’s name be empty? If not, we need to create a constraint to forbid this type of insert.</p>
<p>Finally, based on the type of schema, a definition of the relationship between the tables is made.</p>
<h2 id="_idParaDest-52"><a id="_idTextAnchor052"/>See also</h2>
<p>If you want to know more about database schema designs and their application, read this article by Mark Smallcombe: <a href="https://www.integrate.io/blog/database-schema-examples/">https://www.integrate.io/blog/database-schema-examples/</a>.</p>
<h1 id="_idParaDest-53"><a id="_idTextAnchor053"/>Applying data governance in ingestion</h1>
<p><strong class="bold">Data governance</strong> is a <a id="_idIndexMarker059"/>set of methodologies that ensure that data is secure, available, well-stored, documented, private, and accurate.</p>
<h2 id="_idParaDest-54"><a id="_idTextAnchor054"/>Getting ready</h2>
<p><strong class="bold">Data ingestion</strong> is the <a id="_idIndexMarker060"/>beginning of the data pipeline process, but it doesn’t mean data governance is not heavily applied. The governance status in the final data pipeline output depends on how it was implemented during the ingestion.</p>
<p>The following diagram shows how data ingestion is commonly conducted:</p>
<div><div><img alt="Figure 1.18 – The data ingestion process" height="615" src="img/Figure_1.18_B19453.jpg" width="1080"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.18 – The data ingestion process</p>
<p>Let’s analyze the steps in the diagram:</p>
<ol>
<li><strong class="bold">Getting data from the source</strong>: The first step is to define the type of data, its periodicity, where we will gather it, and why we need it.</li>
<li><strong class="bold">Writing the scripts to ingest data</strong>: Based on the answers to the previous step, we can begin planning how our code will behave and some basic steps.</li>
<li><strong class="bold">Storing data in a temporary database or other types of storage</strong>: Between the ingest and the transformation phase, data is typically stored in a temporary database or repository.</li>
</ol>
<div><div><img alt="Figure 1.19 – Data governance pillars" height="782" src="img/Figure_1.19_B19453.jpg" width="830"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.19 – Data governance pillars</p>
<h2 id="_idParaDest-55"><a id="_idTextAnchor055"/>How to do it…</h2>
<p>Step by step, let’s <a id="_idIndexMarker061"/>attribute the pillars in <em class="italic">Figure 1</em><em class="italic">.19</em> to the ingestion phase:</p>
<ol>
<li>A concern for accessibility <a id="_idIndexMarker062"/>needs to be applied at the data source level, defining the individuals that are allowed to see or retrieve data.</li>
<li>Next, it is necessary to catalog our data to understand it better. Since data ingestion is only covered here, it is more relevant to cover the data sources.</li>
<li>The quality pillar will be applied to the ingestion and staging area, where we control the data and keep its quality aligned with the source.</li>
<li>Then, let’s define ownership. We know the data source <em class="italic">belongs</em> to a business area or a company. However, when we ingested the data and put it in temporary or staging storage, it becomes our responsibility to maintain it.</li>
<li>The last pillar involves <a id="_idIndexMarker063"/>keeping data secure for the whole pipeline. Security is vital in all steps, since we may be handling private or sensitive information.</li>
</ol>
<div><div><img alt="Figure 1.20 – Adding to data ingestion" height="1056" src="img/Figure_1.20_B19453.jpg" width="1305"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.20 – Adding to data ingestion</p>
<h2 id="_idParaDest-56"><a id="_idTextAnchor056"/>How it works…</h2>
<p>While some <a id="_idIndexMarker064"/>articles define “pillars” to create governance good practices, the best way to understand how to apply them is to understand how they are composed. As you saw in the previous <em class="italic">How to do it…</em> section, we attributed some items to our pipeline, and now we <a id="_idIndexMarker065"/>can understand how they are connected to the following topics:</p>
<ul>
<li><strong class="bold">Data accessibility</strong>: Data accessibility is how people from a group, organization, or <a id="_idIndexMarker066"/>project can see and use data. The information needs to be readily available for use. At the same time, it needs to be available for the people involved in the process. For example, sensitive data accessibility should be restricted to some people or programs. In the diagram we built, we applied it to our data sources, since we need to understand and retrieve data. For the same reason, it can be applied for temporary storage needs as well.</li>
<li><strong class="bold">Data catalog</strong>: Cataloging and documenting data are essential for business and engineering teams. When <a id="_idIndexMarker067"/>we know what types of information rely on our databases or data lakes and have quick access to these documents, the action time to solve a problem becomes short.</li>
</ul>
<p>Again, documenting our data sources can make the ingest process quicker, since we need to make a discovery every time we need to ingest data.</p>
<ul>
<li><strong class="bold">Data quality</strong>: Quality is constantly preoccupied with ingesting, processing, and loading data. Tracking <a id="_idIndexMarker068"/>and monitoring data’s expected income and outcome by its periodicity is essential. For example, if we expect to ingest 300 GB of data per day and suddenly it drops to 1 GB, something is very wrong and will affect the quality of our final output. Other quality parameters can be the number of columns, partitioning, and so on, which we will explore later in this book.</li>
<li><strong class="bold">Ownership</strong>: Who is responsible <a id="_idIndexMarker069"/>for the data? This definition is crucial to make contact with the owner if there are problems or attribute responsibility to keep and maintain data.</li>
<li><strong class="bold">Security</strong>: A <a id="_idIndexMarker070"/>concerning topic nowadays is data security. With so many regulations about data privacy, it became an obligation of data engineers and scientists to know, at least, the basics of encryption, sensitive data, and how to avoid data leaks. Even languages and libraries that are used for work need to be evaluated. That’s why this item is attributed to the three steps in <em class="italic">Figure 1</em><em class="italic">.19</em>.</li>
</ul>
<p>In addition to the topics we <a id="_idIndexMarker071"/>explored, a global data governance project has a vital role called a <strong class="bold">data steward</strong>, which is responsible for managing an organization’s data assets and ensuring <a id="_idIndexMarker072"/>that data is accurate, consistent, and secure. In summary, data stewardship is managing and overseeing an organization’s data assets.</p>
<h2 id="_idParaDest-57"><a id="_idTextAnchor057"/>See also</h2>
<p>You can read more about a recent vulnerability found in one of the most used tools for data engineering here: <a href="https://www.ncsc.gov.uk/information/log4j-vulnerability-what-everyone-needs-to-know">https://www.ncsc.gov.uk/information/log4j-vulnerability-what-everyone-needs-to-know</a>.</p>
<h1 id="_idParaDest-58"><a id="_idTextAnchor058"/>Implementing data replication</h1>
<p><strong class="bold">Data replication</strong> is a <a id="_idIndexMarker073"/>process applied in data environments to create multiple copies of data and store them on different locations, servers, or sites. This technique is commonly implemented to create better availability and avoid data loss if there is downtime, or even a natural disaster that affects a data center.</p>
<h2 id="_idParaDest-59"><a id="_idTextAnchor059"/>Getting ready</h2>
<p>You will find across papers and articles different types (or even names) on the best way for <strong class="bold">data replication</strong> decision. In this recipe, you will learn how to decide which kind of replication better suits your application or software.</p>
<h2 id="_idParaDest-60"><a id="_idTextAnchor060"/>How to do it…</h2>
<p>Let’s begin to build our fundamental pillars to implement data replication:</p>
<ol>
<li>First, we need to decide the size of our replication, and it can be done using a portion or all the stored data.</li>
<li>The next step is to consider when replication will take place. It can be done synchronously when new data arrives in storage or within a specific timeframe.</li>
<li>The last fundamental pillar is whether the data is incremented or in a bulk form.</li>
</ol>
<p>In the end, we will have a diagram that looks like the following:</p>
<div><div><img alt="Figure 1.21 – A data replication model decision diagram" height="513" src="img/Figure_1.21_B19453.jpg" width="1347"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.21 – A data replication model decision diagram</p>
<h2 id="_idParaDest-61"><a id="_idTextAnchor061"/>How it works…</h2>
<p>Analyzing the preceding figure, we have three main questions to answer, regarding the extension, the frequency, and whether our replication will be incremental or bulk.</p>
<p>For the first <a id="_idIndexMarker074"/>question, we decide whether the replication will be complete or partial. In other words, either the data will consistently be replicated no matter what type of transaction or change was made, or just a portion of the data will be replicated. A real example of this would be keeping track of all store sales or just the most expensive ones.</p>
<p>The second question, related to the frequency, is to decide when a replication needs to be done. This question also needs to take into consideration related costs. Real-time replication is often more expensive, but the synchronicity guarantees almost no data inconsistency.</p>
<p>Lastly, it is relevant to consider how data will be transported to the replication site. In most cases, a scheduler with a script can replicate small data batches and reduce transportation costs. However, a bulk replication can be used in the data ingestion process, such as copying all the current batch’s raw data from a source to cold storage.</p>
<h2 id="_idParaDest-62"><a id="_idTextAnchor062"/>There’s more…</h2>
<p>One method of data replication that has seen an increase in use in the past few years is <strong class="bold">cold storage</strong>, which is used to retain data used infrequently or is even inactive. The costs related to this type of replication are meager and guarantee data longevity. You can find cold storage solutions in <a id="_idIndexMarker075"/>all cloud providers, such as <strong class="bold">Amazon Glacier</strong>, <strong class="bold">Azure Cool Blob</strong>, and <strong class="bold">Google Cloud </strong><strong class="bold">Storage Nearline</strong>.</p>
<p>Besides replication, regulatory compliance such as <strong class="bold">General Data Protection Regulation</strong> (<strong class="bold">GDPR</strong>) laws benefit <a id="_idIndexMarker076"/>from this type of storage, since, for some case scenarios, users’ data need to be kept for some years.</p>
<p>In this chapter, we explored the basic concepts and laid the foundation for the following chapters and recipes in this book. We started with a Python installation, prepared our Docker containers, and saw data governance and replication concepts. You will observe over the upcoming chapters that almost all topics interconnect, and you will understand the relevance of understanding them at the beginning of the ETL process.</p>
<h1 id="_idParaDest-63"><a id="_idTextAnchor063"/>Further reading</h1>
<ul>
<li><a href="https://www.manageengine.com/device-control/data-replication.xhtml">https://www.manageengine.com/device-control/data-replication.xhtml</a></li>
</ul>
</div>
</div></body></html>