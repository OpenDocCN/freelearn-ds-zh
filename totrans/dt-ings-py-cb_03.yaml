- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data Discovery – Understanding Our Data before Ingesting It
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you may already have noticed, **data ingestion** is not just retrieving data
    from a source and inserting it in another place. It involves understanding some
    business concepts, secure access to the data, and how to store it, and now it
    is essential to discover our data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data discovery** is the process of understanding our data’s patterns and
    behaviors, ensuring the whole data pipeline will be successful. In this process,
    we will understand how our data is modeled and used, so we can set up and plan
    our ingestion using the best fit.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn about the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Documenting the data discovery process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring OpenMetadata
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connecting OpenMetadata to our database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can also find the code from this chapter in its GitHub repository here:
    [https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook](https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook).'
  prefs: []
  type: TYPE_NORMAL
- en: Documenting the data discovery process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In recent years, manual data discovery has been rapidly deprecated, giving rise
    to **machine learning** and other automated solutions, bringing fast insights
    into data in storage or online spreadsheets, such as Google Sheets.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, many small companies are just starting out their businesses or
    data areas, so implementing a paid or cost-related solution might not be a good
    idea right away. As data professionals, we also need to be malleable when applying
    the first solution to a problem – there will always be space to improve it later.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe will cover the steps to start the data discovery process effectively.
    Even though, here, the process is more related to the manual discovery steps,
    you will see it also applies to the automated ones.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start by downloading the datasets.
  prefs: []
  type: TYPE_NORMAL
- en: For this recipe, we are going to use the *The evolution of genes in viruses
    and bacteria* dataset ([https://www.kaggle.com/datasets/thedevastator/the-evolution-of-genes-in-viruses-and-bacteria](https://www.kaggle.com/datasets/thedevastator/the-evolution-of-genes-in-viruses-and-bacteria)),
    and another one containing *hospital administration* information ([https://www.kaggle.com/datasets/girishvutukuri/hospital-administration](https://www.kaggle.com/datasets/girishvutukuri/hospital-administration)).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This recipe does not require the use of the exact datasets mentioned – it covers
    generically how to apply the methodology to datasets or any data sources. Feel
    free to use any data you want.
  prefs: []
  type: TYPE_NORMAL
- en: The next stage is creating the documentation. You can use any software or online
    application that suits you – the important thing is to have a place to detail
    and catalog the information.
  prefs: []
  type: TYPE_NORMAL
- en: I will use **Notion** ([https://www.notion.so/](https://www.notion.so/)). Its
    home page is shown in *Figure 3**.1*. It offers a free plan and allows you to
    create separate places for different types of documentation. However, some companies
    use **Confluence by Atlassian** to document their data. It will always depend
    on the scenario you are in.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – Notion home page](img/Figure_3.01_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 – Notion home page
  prefs: []
  type: TYPE_NORMAL
- en: This is an optional stage where we are creating a Notion account. On the main
    page, click on **Get** **Notion free**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another page will appear and you can use your Google or Apple email to create
    an account, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – Notion Sign up page](img/Figure_3.02_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 – Notion Sign up page
  prefs: []
  type: TYPE_NORMAL
- en: After that, you should see a blank page with a welcome message from Notion.
    If any other action is required, just follow the page instructions.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s imagine a scenario where we work at a hospital and need to apply the
    data discovery process. Here is how we go about it:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Identifying our data sources**: Two main departments need their data to be
    ingested—the administration and research departments. We know they usually keep
    their CSV files in a local data center so we can access them via the intranet.
    Don’t mind the filenames; generally, in a real application, they are not supported.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following are the research department’s files:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3 – Research files on the evolution of genes in E. coli](img/Figure_3.03_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 – Research files on the evolution of genes in E. coli
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the administration department’s files:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4 – Hospital administration files](img/Figure_3.04_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 – Hospital administration files
  prefs: []
  type: TYPE_NORMAL
- en: '**Categorizing data per department or project**: Here, we create folders and
    subfolders related to the department and the type of data (on patients or specific
    diseases).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.5 – Research Department page](img/Figure_3.05_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 – Research Department page
  prefs: []
  type: TYPE_NORMAL
- en: '**Identifying the datasets or databases**: When looking at the files, we can
    find four patterns. There are the exclusive datasets: **E.Coli Genomes**, **Protein
    Annotations**, **Escherichia Virus** in general, and **Patients**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.6 – Subsections created by research type and hospital administration
    topic](img/Figure_3.06_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.6 – Subsections created by research type and hospital administration
    topic
  prefs: []
  type: TYPE_NORMAL
- en: '**Describing our data**: Now, at the dataset level, we need to have helpful
    information about it, such as the overall description of that dataset table, when
    it is updated, where other teams can find it, a description of each column of
    the table, and, last but not least, all metadata.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.7 – Patient data documentation using Notion](img/Figure_3.07_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.7 – Patient data documentation using Notion
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The description of where the file is stored may not be applied in all cases.
    You can find the reference of the database name instead, such as `'admin_database.patients'`.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When starting data discovery, the first objective is identifying patterns and
    categorizing them to create a logical flow. Usually, the first categorizations
    are by department or project, followed by database and dataset identification,
    and finally, describing the data inside.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some ways to document data discovery manually. People more used to
    the old-fashioned style of **BI** (short for **Business Intelligence**) tend to
    create more beautiful visualization models to apply discovery. However, this recipe’s
    objective is to create a catalog using a simple tool such as Notion:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Categorizing data as per department or project**: The first thing we did
    was to identify the department responsible for each piece of data. Who is the
    contact in the case of an ingestion problem or if the dataset is broken? In formal
    terms, they are also known as data stewards. In some companies, categorization
    by project can also be applied since some companies can have their particular
    necessities and data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Identifying the datasets or databases**: Here, we have only used datasets.
    Under the projects and/or departments, we insert the name of each table and other
    helpful information. If the tables are periodically updated, it is a good practice
    to also document that.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Describing our data**: Finally, we document the expected columns with their
    data types in detail. It helps data engineers plan their scripts when ingesting
    raw data; if something goes wrong after the automation, they can easily detect
    the issue.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You might notice that some data behaves strangely. For instance, the **medical_speciality**
    column in *Figure 3**.7* has values described and a number to reference something
    else. In a real-world project, it would be necessary to create auxiliary data
    inside our ingestion to make a pattern and later facilitate the report or dashboards.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring OpenMetadata
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**OpenMetadata** is an open source tool used for metadata management, allowing
    the process of **data discovery** and **governance**. You can find more about
    it here: [https://open-metadata.org/](https://open-metadata.org/).'
  prefs: []
  type: TYPE_NORMAL
- en: By performing a few steps, it is possible to create a local or production instance
    using **Docker** or **Kubernetes**. OpenMetadata can connect to multiple resources,
    such as **MySQL**, **Redis**, **Redshift**, **BigQuery**, and others, to bring
    the information needed to build a data catalog.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before starting our configuration, we must install **OpenMetadata** and ensure
    the Docker containers are running correctly. Let us see how it is done:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'At the time this book was written, the application was in the 0.12 version
    and with some documentation and installation improvements. This means the best
    approach to installing it may change over time. Please refer to the official documentation
    for it here: [https://docs.open-metadata.org/quick-start/local-deployment](https://docs.open-metadata.org/quick-start/local-deployment).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create a folder and `virtualenv` (optional):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Since we are using a Docker environment to deploy the application locally,
    you can create it with `virtualenv` or not:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we install OpenMetadata as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then we check the installation, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After downloading the **Python** package and **Docker**, we will proceed with
    the configurations as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Running containers**: It may take some time to finish when you execute it
    for the first time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'It is common for this type of error to appear:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Error response from daemon: driver failed programming external connectivity
    on endpoint openmetadata_ingestion (3670b9566add98a3e79cd9a252d2d0d377dac627b4be94b669482f6ccce350e0):
    Bind for 0.0.0.0:8080 failed: port is** **already allocated**'
  prefs: []
  type: TYPE_NORMAL
- en: It means other containers or applications are already using port `8080`. To
    solve this, specify another port (such as `8081`) or stop the other applications.
  prefs: []
  type: TYPE_NORMAL
- en: The first time you run this command, the results might take a while due to other
    containers associated with it.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the end, you should see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.8 – Command line showing success running OpenMetadata containers](img/Figure_3.08_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.8 – Command line showing success running OpenMetadata containers
  prefs: []
  type: TYPE_NORMAL
- en: '`http://localhost:8585` address:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.9 – OpenMetadata sign-in page in the browser](img/Figure_3.09_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.9 – OpenMetadata sign-in page in the browser
  prefs: []
  type: TYPE_NORMAL
- en: '**Creating a user account and logging in**: To access the UI panel, we need
    to create a user account as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.10 – Creating a user account in the OpenMetadata Create Account
    section](img/Figure_3.10_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.10 – Creating a user account in the OpenMetadata Create Account section
  prefs: []
  type: TYPE_NORMAL
- en: 'After that, we will be redirected to the main page and be able to access the
    panel, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.11 – Main page of OpenMetadata](img/Figure_3.11_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.11 – Main page of OpenMetadata
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Is also possible to log in using the default admin user by inserting the [admin@openmetadata.org](mailto:admin@openmetadata.org)
    username and `admin` as the password.
  prefs: []
  type: TYPE_NORMAL
- en: 'For production matters, please refer to the Enable Security Guide here: [https://docs.open-metadata.org/deployment/docker/security](https://docs.open-metadata.org/deployment/docker/security).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Creating teams**: In the **Settings** section, you should see several possible
    configurations, from creating users to access the console to integrations with
    messengers such as **Slack** or **MS Teams**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Some ingestion and integration requires the user to be allocated to a team.
    To create a team, we first need to log in as `admin`. Then, go to **Settings**
    | **Teams** | **Create** **new team**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.12 – Creating a team in the OpenMetadata settings](img/Figure_3.12_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.12 – Creating a team in the OpenMetadata settings
  prefs: []
  type: TYPE_NORMAL
- en: '**Adding users to our teams**: Select the team you just created and go to the
    **Users** tab. Then select the user you want to add.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.13 – Adding users to a team](img/Figure_3.13_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.13 – Adding users to a team
  prefs: []
  type: TYPE_NORMAL
- en: Creating teams is very convenient to keep track of users’ activity and define
    a group of roles and policies. In the following case, all users added to this
    team will be able to navigate through and create their data discovery pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.14 – Team page and the default associated Data Consumer role](img/Figure_3.14_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.14 – Team page and the default associated Data Consumer role
  prefs: []
  type: TYPE_NORMAL
- en: We must have a Data Steward or Administrator role for the activities in this
    chapter and the following recipe. The Data Steward role has almost the same permissions
    as the Administrator role since it is a position that is responsible for defining
    and implementing data policies, standards, and procedures to govern data usage
    and ensure consistency.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can read more about the **Roles and Policies** of OpenMetadata here: [https://github.com/open-metadata/OpenMetadata/issues/4199](https://github.com/open-metadata/OpenMetadata/issues/4199).'
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, let’s understand a bit more about how OpenMetadata works.
  prefs: []
  type: TYPE_NORMAL
- en: OpenMetadata is an open source metadata management tool designed to help organizations
    to manage their data and metadata across different systems or platforms. Since
    it centralizes data information in one place, it makes it easier to discover and
    understand data.
  prefs: []
  type: TYPE_NORMAL
- en: It is also a flexible and extensible tool, allowing integration with tools such
    as Apache Kafka, Apache Hive, and others since it uses programming languages such
    as **Python** (main core code) and Java behind the scenes.
  prefs: []
  type: TYPE_NORMAL
- en: To orchestrate and ingest the metadata from sources, OpenMetadata counts the
    sources using Airflow code. If you look at its core, all Airflow code can be found
    in `openmetadata-ingestion`. For more heavy users who want to debug any problems
    related to the ingestion process in this framework, Airflow can be easily accessed
    at `http://localhost:8080/`, when the metadata Docker container is up and running.
  prefs: []
  type: TYPE_NORMAL
- en: 'It also uses **MySQL DB** to store user information and relationships and an
    **Elasticsearch** container to create efficient indexes. Refer to the following
    figure ([https://docs.open-metadata.org/developers/architecture](https://docs.open-metadata.org/developers/architecture)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.15 – OpenMetadata architecture diagram Font source: OpenMetadata
    documentation](img/Figure_3.15_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.15 – OpenMetadata architecture diagram Font source: OpenMetadata documentation'
  prefs: []
  type: TYPE_NORMAL
- en: 'For more detailed information about the design decisions, you can access the
    **Main Concepts** page and explore in detail the ideas behind them: [https://docs.open-metadata.org/main-concepts/high-level-design](https://docs.open-metadata.org/main-concepts/high-level-design).'
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We saw how **OpenMetadata** can be easily configured and installed locally on
    our machines and a brief overview of its architecture. However, other good options
    on the market can be used to document data, or even a **SaaS** solution of **OpenMetadata**
    using **Google Cloud**.
  prefs: []
  type: TYPE_NORMAL
- en: OpenMetadata SaaS sandbox
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recently, OpenMetadata implemented a **Software as a Service** (**SaaS**) sandbox
    ([https://sandbox.open-metadata.org/signin](https://sandbox.open-metadata.org/signin))
    using Google, making it easier to deploy and start the discovery and catalog process.
    However, it may have costs applied, so keep that in mind.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can read more about OpenMetadata in their blog: [https://blog.open-metadata.org/why-openmetadata-is-the-right-choice-for-you-59e329163cac](https://blog.open-metadata.org/why-openmetadata-is-the-right-choice-for-you-59e329163cac)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Explore OpenMetadata on GitHub: [https://github.com/open-metadata/OpenMetadata](https://github.com/open-metadata/OpenMetadata)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connecting OpenMetadata to our database
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have configured our **Data Discovery** tool, let’s create a sample
    connection to our local database instance. Let’s try to use PostgreSQL to do an
    easy integration and practice another database usage.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, ensure our application runs appropriately by accessing the `http://localhost:8585/my-data
    address`.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Inside OpenMetadata, the user must have the `admin` user using the previous
    credentials we saw.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can check the Docker status here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.16 – Active containers are shown in the Docker desktop application](img/Figure_3.16_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.16 – Active containers are shown in the Docker desktop application
  prefs: []
  type: TYPE_NORMAL
- en: Use PostgreSQL for testing. Since we already have a Google project ready, let
    us create a SQL instance using the PostgreSQL engine.
  prefs: []
  type: TYPE_NORMAL
- en: As we kept the queries to create the database and tables in [*Chapter 2*](B19453_02.xhtml#_idTextAnchor064),
    we can build it again in Postgres. The queries can also be found in the GitHub
    repository of this chapter. However, feel free to create your own data.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.17 – Google Cloud console header for SQL instances](img/Figure_3.17_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.17 – Google Cloud console header for SQL instances
  prefs: []
  type: TYPE_NORMAL
- en: Remember to let this instance allow public access; otherwise, our local OpenMetadata
    instance won’t be able to access it.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Go to the OpenMetadata home page by typing `http://localhost:8585/my-data`
    in the browser header:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Adding a new database to OpenMetadata**: Go to **Settings** | **Services**
    | **Databases** and click on **Add new Database Service**. Some options will appear.
    Click on **Postgres**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.18 – OpenMetadata page to add a database as a source](img/Figure_3.18_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.18 – OpenMetadata page to add a database as a source
  prefs: []
  type: TYPE_NORMAL
- en: Click on `CookBookData`.
  prefs: []
  type: TYPE_NORMAL
- en: '**Adding our connection settings**: After clicking on **Next** again, a page
    with some fields to input the MySQL connection settings will appear:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.19 – Adding new database connection information](img/Figure_3.19_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.19 – Adding new database connection information
  prefs: []
  type: TYPE_NORMAL
- en: '**Testing our connection**: With all the credentials in place, we need to test
    the connection to the database.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.20 – Connection test successful message for database connection](img/Figure_3.20_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.20 – Connection test successful message for database connection
  prefs: []
  type: TYPE_NORMAL
- en: '**Creating an ingestion pipeline**: You can leave all the fields as they are
    without worrying about the **database tool** (**DBT**). For **Schedule Interval**,
    you can set what suits you best. I will leave it as **Daily**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.21 – Adding database metadata ingestion](img/Figure_3.21_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.21 – Adding database metadata ingestion
  prefs: []
  type: TYPE_NORMAL
- en: '**Ingesting the metadata**: Heading to **Ingestions**, our database metadata
    is successfully ingested.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.22 – Postgres metadata successfully ingested](img/Figure_3.22_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.22 – Postgres metadata successfully ingested
  prefs: []
  type: TYPE_NORMAL
- en: '**Exploring our metadata**: To explore the metadata, go to **Explore** | **Tables**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 3.23 – Explore page showing the tables metadata ingested](img/Figure_3.23_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.23 – Explore page showing the tables metadata ingested
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see that the `people` table is there with other internal tables:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.24 – The people table metadata](img/Figure_3.24_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.24 – The people table metadata
  prefs: []
  type: TYPE_NORMAL
- en: Here, you can explore some functionalities of the application, such as defining
    the level of importance to the organization and the owners, querying the table,
    and others.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we saw previously, OpenMetadata uses Python to build and connect to different
    sources.
  prefs: []
  type: TYPE_NORMAL
- en: In `Connection Scheme` uses `psycopg2`, a widely used library in Python. All
    other arguments are passed to the behind-the-scenes Python code to create a connection
    string.
  prefs: []
  type: TYPE_NORMAL
- en: For each metadata ingestion, OpenMetadata will create a new Airflow **Directed
    Acyclic Graph** (**DAG**) to process it based on a generic one. Having a separate
    DAG for each metadata ingestion makes debugging more manageable in case of errors.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.25 – Airflow DAGs created by OpenMetadata](img/Figure_3.25_B19453.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.25 – Airflow DAGs created by OpenMetadata
  prefs: []
  type: TYPE_NORMAL
- en: If you open the Airflow instance used by OpenMetadata, you can see it clearly
    and have other information about the metadata ingestion. It’s a nice place to
    debug in case an error occurs. Understanding how our solution works and where
    to look in case of a problem helps identify and solve issues more efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[https://nira.com/data-discovery/](https://nira.com/data-discovery/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://coresignal.com/blog/data-discovery/](https://coresignal.com/blog/data-discovery/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.polymerhq.io/blog/diligence/what-is-data-discovery-guide/](https://www.polymerhq.io/blog/diligence/what-is-data-discovery-guide/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://bi-survey.com/data-discovery](https://bi-survey.com/data-discovery)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.heavy.ai/technical-glossary/data-discovery](https://www.heavy.ai/technical-glossary/data-discovery)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.datapine.com/blog/what-are-data-discovery-tools/](https://www.datapine.com/blog/what-are-data-discovery-tools/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.knowsolution.com.br/data-discovery-como-relaciona-bi-descubra/](https://www.knowsolution.com.br/data-discovery-como-relaciona-bi-descubra/)
    (in Portuguese)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you are interested in learning more about other data discovery tools available
    on the market, here are some:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Tableau**: Tableau ([https://www.tableau.com/](https://www.tableau.com/))
    is more extensively used for data visualizations and dashboards but comes with
    some features to discover and catalog data. You can read more about how to use
    Tableau for data discovery on their resources page here: [https://www.tableau.com/learn/whitepapers/data-driven-organization-7-keys-data-discovery](https://www.tableau.com/learn/whitepapers/data-driven-organization-7-keys-data-discovery).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OpenDataDiscovery** (free and open source): OpenDataDiscovery has recently
    arrived on the market and can provide a very nice starting point. Check it out
    here: [https://opendatadiscovery.org/](https://opendatadiscovery.org/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Atlan**: Atlan ([https://atlan.com/](https://atlan.com/)) is a complete solution
    and also brings a data governance structure; however, the costs can be high and
    it requires a call with their sales team to start an **MVP** (short for **Minimum**
    **Viable Product**).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Alation**: Alation is an enterprise tool that provides several data solutions
    that include all pillars of data governance. Find out more here: [https://www.alation.com/](https://www.alation.com/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
