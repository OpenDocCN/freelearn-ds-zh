<html><head></head><body><div class="chapter" title="Chapter&#xA0;10.&#xA0;R and Big Data"><div class="titlepage"><div><div><h1 class="title"><a id="ch10"/>Chapter 10. R and Big Data</h1></div></div></div><p>We have come to the final chapter of this book where we will go to the very limits of large-scale data processing. The<a id="id377" class="indexterm"/> term <span class="emphasis"><em>Big Data</em></span> has been used to describe the ever growing volume, velocity, and variety of data being generated on the Internet in connected devices and many other places. Many organizations now have massive datasets that measure in petabytes (one petabyte is 1,048,576 gigabytes), more than ever before. Processing and analyzing Big Data is extremely challenging for traditional data processing tools and database architectures.</p><p>In 2005, Doug Cutting and Mike Cafarella at Yahoo! developed Hadoop, based on earlier work by Google, to address these challenges. They set out to develop a new data platform to process, index, and query billions of web pages efficiently. With Hadoop, the work which would have previously required very expensive supercomputers can now be done on large clusters of inexpensive standard servers. As the volume of data grows, more servers can simply be added to a Hadoop cluster to increase the storage capacity and computing power. Since then, Hadoop and its ecosystem of tools has become one of the most popular suites of tools to collect, store, process and analyze large datasets. In this chapter, we will learn how to tap into the power of Hadoop from R.</p><p>This chapter covers the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Understanding Hadoop</li><li class="listitem" style="list-style-type: disc">Setting up Hadoop on Amazon Web Services</li><li class="listitem" style="list-style-type: disc">Processing large datasets in batches using RHadoop</li></ul></div><div class="section" title="Understanding Hadoop"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec56"/>Understanding Hadoop</h1></div></div></div><p>Before we learn how <a id="id378" class="indexterm"/>to use <a id="id379" class="indexterm"/>Hadoop (for more information refer to <a class="ulink" href="http://hadoop.apache.org/">http://hadoop.apache.org/</a>) and related tools in R, we need to understand the basics of Hadoop. For our purposes, it suffices to know that Hadoop comprises two key components: the <span class="strong"><strong>Hadoop Distributed File System (HDFS)</strong></span> and <a id="id380" class="indexterm"/>the <span class="strong"><strong>MapReduce</strong></span> framework to execute data <a id="id381" class="indexterm"/>processing tasks. Hadoop includes many other components for task scheduling, job management, and others, but we shall not concern ourselves with those in this book.</p><p>HDFS, as the name suggests, is a virtual filesystem that is distributed across a cluster of servers. HDFS stores files in blocks, with a default block size of 128 MB. For example, a 1 GB file is split into eight blocks of 128 MB, which are distributed to different servers in the cluster. Furthermore, to prevent data loss due to server failure, the blocks are replicated. By default, they are replicated three times—there are three copies of each block of data in the cluster, and each copy is stored on a different server. That way, even if a few servers in the cluster fail, the data is not lost and can be re-replicated to ensure high availability.</p><p>MapReduce is the framework to process the data stored in HDFS in a data parallel way. Notice how the distributed nature of data storage makes Hadoop a good fit for data parallel algorithms that we learned about in <a class="link" href="ch08.html" title="Chapter 8. Multiplying Performance with Parallel Computing">Chapter 8</a>, <span class="emphasis"><em>Multiplying Performance with Parallel Computing</em></span>—the chunks of data stored on each worker node are processed simultaneously in parallel, and then the results from each node are combined to produce the final results. MapReduce works very similarly to the data parallel algorithms in <a class="link" href="ch08.html" title="Chapter 8. Multiplying Performance with Parallel Computing">Chapter 8</a>, <span class="emphasis"><em>Multiplying Performance with Parallel Computing</em></span>, except that the data already resides in the worker nodes; it does not have to be distributed every time a task is run as was the case with a cluster of servers that run R. <span class="strong"><strong>Map</strong></span><a id="id382" class="indexterm"/> refers to the step of performing computations on the data in each worker node, or mapping data to their corresponding output. <span class="strong"><strong>Reduce</strong></span><a id="id383" class="indexterm"/> refers to the process of combining, or reducing the results of the worker nodes into the final results.</p><p>Data in MapReduce is represented as key-value pairs. Every MapReduce operation is essentially a transformation from one set of key-value pairs to another set of key-value pairs. A <a id="id384" class="indexterm"/>
<span class="strong"><strong>mapper</strong></span> might, for example, read a single customer record from a database and produce a key-value pair such as <code class="literal">("Alice</code> <code class="literal">", 32)</code> that contains the name of a customer (<code class="literal">"Alice"</code>) as the key and the reward points she or he collected in a given week (<code class="literal">32</code>) as the corresponding value. After the map step, all the key-value pairs are sorted by the key, and the pairs with the same key are given <a id="id385" class="indexterm"/>to individual <span class="strong"><strong>reducers</strong></span>. So, for example, there would be one reducer for all pairs with the key <code class="literal">"Alice"</code>, another reducer for the key <code class="literal">"Bob"</code>, and another for <code class="literal">"Charlie"</code>. A reducer takes all the key-value pairs it is given, performs computations on them, and returns the results as another key-value pair.</p><p>The reducers in our simple example could compute the mean of weekly reward points collected by all <a id="id386" class="indexterm"/>customers. The MapReduce system then collects the results of all the reducers as the final output, which could be something like <code class="literal">[("Alice", 26.5), ("Bob", 42.3), ("Charlie", 35.6), ...]</code>.</p><p>While HDFS and MapReduce are the foundation of Hadoop, they are not suited for all data processing tasks. One key reason is that data stored in HDFS resides on the hard drives of the servers. Each time a MapReduce task is performed, the data has to be read from the disk, and the results of the computations are written back to the disk. Thus, HDFS and MapReduce perform reasonably well for sizeable batch processing tasks where the time to complete the computational tasks exceeds the overheads of reading/writing data and other overheads of running a Hadoop cluster.</p></div></div>
<div class="section" title="Setting up Hadoop on Amazon Web Services"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec57"/>Setting up Hadoop on Amazon Web Services</h1></div></div></div><p>There are many <a id="id387" class="indexterm"/>ways to <a id="id388" class="indexterm"/>set up a Hadoop cluster. We can install Hadoop on a single server in pseudo-distributed mode to simulate a cluster, or on an actual cluster of servers, or virtual machines in fully distributed mode. There are also several distributions of Hadoop available from the vanilla open source version provided by the Apache Foundation to commercial distributions such as Cloudera, Hortonworks, and MapR. Covering all the different ways of setting up Hadoop is beyond the scope of this book. We instead provide instructions for one way to set up Hadoop and other relevant tools for the purpose of the examples in this chapter. If you are using an existing Hadoop cluster or setting up one in a different way, you might have to modify some of the steps.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note20"/>Note</h3><p>Because Hadoop and its associated tools are mostly developed for Linux/Unix based operating systems, the code in this chapter will probably not work on Windows. If you are a Windows user, follow the instructions in this chapter to set up Hadoop, R, and the required packages on Amazon Web Services.</p></div></div><p><span class="strong"><strong>Amazon Web Services</strong></span> (<span class="strong"><strong>AWS</strong></span>) has a service called <span class="strong"><strong>Elastic MapReduce</strong></span> (<span class="strong"><strong>EMR</strong></span>) that allows us to rent and <a id="id389" class="indexterm"/>run a Hadoop cluster on an hourly basis. Creating a Hadoop cluster is as simple as specifying the number of servers in the cluster, the size of each server, and the instructions to set up the required tools on each server. To set up an account with AWS, follow the instructions in <span class="emphasis"><em>Preface</em></span>. Running the examples in this chapter on AWS will cost some money for as long as the EMR cluster is running. Check this link <a id="id390" class="indexterm"/>out <a class="ulink" href="http://aws.amazon.com/elasticmapreduce/pricing/">http://aws.amazon.com/elasticmapreduce/pricing/</a> for the latest EMR prices.</p><p>We also need a script that sets up the required tools on each server. Save the following script as <code class="literal">emr-bootstrap.sh</code>. This <a id="id391" class="indexterm"/>scripts installs<a id="id392" class="indexterm"/> the R packages needed for this chapter, including <code class="literal">rhdfs</code>, <code class="literal">rmr2</code>, and <code class="literal">R.utils</code> on every server in the Hadoop cluster.</p><div class="informalexample"><pre class="programlisting">#!/bin/bash
# Set unix environment variables
cat &lt;&lt; EOF &gt;&gt; $HADOOP_HOME/.bashrc
export HADOOP_CMD=$HADOOP_HOME/bin/hadoop
export HADOOP_STREAMING=$HADOOP_HOME/contrib/streaming/hadoop-streaming.jar
EOF
. $HADOOP_HOME/.bashrc
# Fix hadoop tmp permission
sudo chmod 777 -R /mnt/var/lib/hadoop/tmp
# Install dependencies
sudo yum install -y libcurl-devel 
# Install R packages
sudo –E R CMD javareconf
sudo –E R --no-save &lt;&lt; EOF
install.packages("R.utils", repos="http://cran.rstudio.com")
EOF
# Install HadoopR dependencies
sudo –E R --no-save &lt;&lt; EOF
install.packages(
    c("bitops", "caTools", "digest", "functional", "plyr", "Rcpp",
      "reshape2", "rJava", "RJSONIO", "stringr"),
    repos="http://cran.rstudio.com")
EOF
# Install rhdfs package
wget https://raw.githubusercontent.com/RevolutionAnalytics/rhdfs/master/build/rhdfs_1.0.8.tar.gz
sudo -E R CMD INSTALL rhdfs_1.0.8.tar.gz
# Install rmr2 package
wget https://raw.githubusercontent.com/RevolutionAnalytics/rmr2/master/build/rmr2_3.2.0.tar.gz
sudo –E R CMD INSTALL rmr2_3.2.0.tar.gz</pre></div><p>Upload <code class="literal">emr-bootstrap.sh</code> into the AWS Simple Storage Service (S3) so that the EMR servers can pick it up during the first run. To do this:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Go to the AWS console, and click on <span class="strong"><strong>S3</strong></span>.</li><li class="listitem">Create a new bucket to store the script in by clicking on <span class="strong"><strong>Create Bucket</strong></span>.</li><li class="listitem">Click on the bucket that was just created and click on <span class="strong"><strong>Upload</strong></span> to upload the script.</li></ol></div><p>Next, follow these steps to create the Hadoop cluster:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Go to the AWS Console and click on <span class="strong"><strong>EMR</strong></span>.</li><li class="listitem">Click on <span class="strong"><strong>Create cluster</strong></span>.</li><li class="listitem">Under <span class="strong"><strong>Software Configuration</strong></span>, select the Amazon Hadoop distribution (the examples in this chapter were tested with Amazon Machine Image (AMI) version 3.2.1).</li><li class="listitem">Remove Hive and Pig from the applications list, as they are not needed.</li><li class="listitem">Under <span class="strong"><strong>Hardware Configuration</strong></span>, select the instance types for the Hadoop servers. The<a id="id393" class="indexterm"/> instance types for both the master and core nodes should have at least 15 GB of RAM, such <a id="id394" class="indexterm"/>as the <code class="literal">m1.xlarge</code> or <code class="literal">m3.xlarge</code> instance types. Enter the number of nodes you would like in the cluster. Given the default HDFS replication factor of three, there should be at least three core nodes. Task nodes are optional.</li><li class="listitem">Under <span class="strong"><strong>Security and Access</strong></span>, select the EC2 key pair to log in to the cluster with.</li><li class="listitem">Under <span class="strong"><strong>Bootstrap Actions</strong></span>, select <span class="strong"><strong>Custom action,</strong></span> then click on <span class="strong"><strong>Configure and add</strong></span>. In the dialog box that appears under <span class="strong"><strong>S3 location</strong></span>, enter or browse for the S3 location where <code class="literal">emr-bootstrap.sh</code> was uploaded.</li><li class="listitem">(Optional) Enable logging under <span class="strong"><strong>Cluster Configuration</strong></span> to have all Hadoop logs automatically stored in the S3 bucket. To use this option, first create an S3 bucket to store the logs in, and enter the name of the bucket in the <span class="strong"><strong>Log folder S3 location</strong></span> field. While optional, storing Hadoop logs is useful for tracing errors and debugging, which can be challenging without the logs, as an executed program gets spawned across multiple processes and computer nodes in Hadoop.</li><li class="listitem">Click on <span class="strong"><strong>Create cluster</strong></span> and wait a few minutes while the cluster is being set up.</li></ol></div><p>Once the EMR cluster is ready, get the Master Public DNS from the cluster details page, and log in to the master server from the command line using your AWS EC2 security key (replace <code class="literal">hadoop.pem</code> with the name of your key):</p><div class="informalexample"><pre class="programlisting">$ ssh –i hadoop.pem root@master-public-dns</pre></div><p>Once you are logged in, run R, which comes preinstalled with the EMR cluster:</p><div class="informalexample"><pre class="programlisting">$ R</pre></div></div>
<div class="section" title="Processing large datasets in batches using Hadoop"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec58"/>Processing large datasets in batches using Hadoop</h1></div></div></div><p>Batch processing is the <a id="id395" class="indexterm"/>most basic type of task that HDFS and MapReduce can perform. Similar to the data parallel algorithms in <a class="link" href="ch08.html" title="Chapter 8. Multiplying Performance with Parallel Computing">Chapter 8</a>, <span class="emphasis"><em>Multiplying Performance with Parallel Computing</em></span>, the master node sends a set of instructions to the worker nodes, which execute the instructions on the blocks of data stored on them. The results are then <a id="id396" class="indexterm"/>written to the disk in HDFS.</p><p>When an aggregate result is required, both the map and reduce steps are performed on the data. For example, in order to compute the mean of a distributed dataset, the mappers on the worker nodes first compute the sum and number of elements in each local chunk of data. The reducers then add up all these results to compute the global mean.</p><p>At other times, only the map step is performed when aggregation is not required. This is common in data transformation or cleaning operations where the data is simply being transformed form one format to another. One example of this is extracting email addresses from a set of documents. In this case, the results of the mappers on the worker nodes are stored as new datasets in HDFS, and reducers are not needed.</p><p>The R community has developed several packages to perform MapReduce tasks from R. One of these is the <a id="id397" class="indexterm"/>RHadoop family <a id="id398" class="indexterm"/>of packages developed by Revolution Analytics (for more information refer to <a class="ulink" href="https://github.com/RevolutionAnalytics/RHadoop">https://github.com/RevolutionAnalytics/RHadoop</a>). RHadoop <a id="id399" class="indexterm"/>includes the <a id="id400" class="indexterm"/>packages <code class="literal">rhdfs</code>, which provides functions to manipulate files and directories in HDFS, and <code class="literal">rmr2</code>, which <a id="id401" class="indexterm"/>exposes the functionality of MapReduce as R functions. These functions <a id="id402" class="indexterm"/>make it easy to use MapReduce without having to program with the Hadoop Java APIs. Instead, <code class="literal">rmr2</code> runs a copy of R on every worker node, and the mappers and reducers are written as R functions to be applied on each chunk of data.</p><p>If you did not use the Hadoop setup instructions in the preceding section, follow the installation<a id="id403" class="indexterm"/> instructions<a id="id404" class="indexterm"/> for <code class="literal">rhdfs</code> and <code class="literal">rmr2</code> at <a class="ulink" href="https://github.com/RevolutionAnalytics/RHadoop/wiki">https://github.com/RevolutionAnalytics/RHadoop/wiki</a>.</p><div class="section" title="Uploading data to HDFS"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec29"/>Uploading data to HDFS</h2></div></div></div><p>The first thing to <a id="id405" class="indexterm"/>do for this is to get data into HDFS. For this chapter, we will use the <a id="id406" class="indexterm"/>Google Books Ngrams data (for more information refer to <a class="ulink" href="http://storage.googleapis.com/books/ngrams/books/datasetsv2.html">http://storage.googleapis.com/books/ngrams/books/datasetsv2.html</a>). Here, n-grams are consecutive words that <a id="id407" class="indexterm"/>appear in the <a id="id408" class="indexterm"/>text where <span class="emphasis"><em>n</em></span> represents the number of words in a phrase—a 1-gram is simply a word (for example, "Batman"), a 2-gram is two consecutive words (for example, "Darth Vader"), and a 6-gram is six consecutive words (for example, "Humpty Dumpty sat on a wall"). We will use the data of 1-grams for our examples.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note21"/>Note</h3><p>The dataset for this chapter is large enough to test the performance of Hadoop on a small cluster, but it is still relatively small compared to many other real-world datasets. Tools such <a id="id409" class="indexterm"/>as <code class="literal">ffdf</code> (<a class="link" href="ch07.html" title="Chapter 7. Processing Large Datasets with Limited RAM">Chapter 7</a>, <span class="emphasis"><em>Processing Large Datasets with Limited RAM</em></span>) can probably be used to process this dataset on a single machine. But when the data size gets much larger, Hadoop or other Big Data tools might be the only way to process the data.</p></div></div><p>The following code downloads the 1-grams data and uploads them into HDFS. Google provides the data in separate files, with one file for each letter of the alphabet containing the words that start with that letter. In this code, <code class="literal">hdfs.init()</code> first initializes the connection to HDFS. Then, <code class="literal">hdfs.mkdir()</code> creates the directory <code class="literal">/ngrams/data</code> in HDFS where the data will be stored. The code in the <code class="literal">for</code> loop downloads each file, decompresses it, and uploads it to HDFS using <code class="literal">hdfs.put()</code>:</p><div class="informalexample"><pre class="programlisting">library(rhdfs)
library(R.utils)
hdfs.init()
hdfs.mkdir("/ngrams/data")
files &lt;- paste0("googlebooks-eng-all-1gram-20120701-", letters)
for (f in files) {
    gzfile &lt;- paste0(f, ".gz")
    url &lt;- paste0("http://storage.googleapis.com/",
                  "books/ngrams/books/",
                  gzfile)
    download.file(url, destfile = gzfile)
    gunzip(gzfile)
    hdfs.put(f, paste0("/ngrams/data/", f))
    file.remove(f)
}</pre></div><p>We can <a id="id410" class="indexterm"/>check that <a id="id411" class="indexterm"/>all the files <a id="id412" class="indexterm"/>have been uploaded successfully into HDFS:</p><div class="informalexample"><pre class="programlisting">hdfs.ls("/ngrams/data")
## permission  owner      group       size          modtime
## 1  -rw-r--r-- hadoop supergroup 1801526075 2014-10-05 09:59
## 2  -rw-r--r-- hadoop supergroup 1268392934 2014-10-05 10:00
## 3  -rw-r--r-- hadoop supergroup 2090710388 2014-10-05 10:01
## 4  -rw-r--r-- hadoop supergroup 1252213884 2014-10-05 10:01
## 5  -rw-r--r-- hadoop supergroup 1085415448 2014-10-05 10:02
## file
## 1  /ngrams/data/googlebooks-eng-all-1gram-20120701-a
## 2  /ngrams/data/googlebooks-eng-all-1gram-20120701-b
## 3  /ngrams/data/googlebooks-eng-all-1gram-20120701-c
## 4  /ngrams/data/googlebooks-eng-all-1gram-20120701-d
## 5  /ngrams/data/googlebooks-eng-all-1gram-20120701-e
## $ hdfs dfs -du -h /ngrams
# Output truncated</pre></div></div><div class="section" title="Analyzing HDFS data with RHadoop"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec30"/>Analyzing HDFS data with RHadoop</h2></div></div></div><p>Now that the <a id="id413" class="indexterm"/>data is <a id="id414" class="indexterm"/>loaded into HDFS, we can use MapReduce to analyze the data. Say we want to compare the popularity of Batman versus Superman since the 1950s. The Google Ngrams data might provide some insight into that.</p><p>Each line of the Ngrams data is a tab-seperated list of values, starting with the Ngram, followed by the year, number of occurrences of that Ngram, and number of books that the Ngram appeared in. For example, the following command line indicates that in 1978, the word "mountain" appeared 1,435,642 times in 1,453 books in the Google Books library.</p><div class="informalexample"><pre class="programlisting">mountain    1978    1435642    1453</pre></div><p>To compare the popularity of Batman and Superman, we need to find the lines of code that represent these two words from 1950 onwards and collate the occurrence values.</p><p>Since the data consists of tab-separated text files, we need to specify the input format so that the <code class="literal">rmr2</code> functions know how to read the files. This can be done using the <code class="literal">make.input.format()</code> function:</p><div class="informalexample"><pre class="programlisting">library(rmr2)
input.format &lt;- make.input.format(
    format = "csv", sep = "\t",
    col.names = c("ngram", "year", "occurrences", "books"),
    colClasses = c("character", "integer", "integer", "integer"))</pre></div><p>For delimited text files such as comma-separated values or tab-separated values, <code class="literal">make.input.format()</code> accepts most of the same arguments as <code class="literal">read.table()</code>. In fact, <code class="literal">rmr2</code> uses <code class="literal">read.table()</code> to read each chunk of data into a data frame.</p><p>Besides delimited text files, <code class="literal">rmr2</code> can also read/write data as raw text (<code class="literal">format = "text"</code>), JSON (<code class="literal">"json"</code>), R's internal data serialization format (<code class="literal">"native"</code>), Hadoop SequenceFiles (<code class="literal">"sequence.typedbytes"</code>), HBase tables (<code class="literal">"hbase"</code>), and Hive or Pig (<code class="literal">"pig.hive"</code>). See the package documentation for the arguments associated with these data types.</p><p>The map step <a id="id415" class="indexterm"/>of our analysis involves filtering each line of data to find the relevant records. We will define<a id="id416" class="indexterm"/> a <code class="literal">mapper</code> function, as shown in the following code, that accepts a set of keys and a set of values as arguments. Since the Ngrams data does not contain keys, the <code class="literal">keys</code> argument is <code class="literal">NULL</code>. The argument <code class="literal">values</code> is a data frame that contains a chunk of data. The mapper function looks for rows of the data frame that contain the words we are interested in, for the year 1950 or later. If any relevant rows are found, the <code class="literal">keyval()</code> function is called to return key-value pairs that will be passed to the reduce function. In this case, the keys are the words and the values are the corresponding years and occurrence counts:</p><div class="informalexample"><pre class="programlisting">mapper &lt;- function(keys, values) {
    values$ngram &lt;- tolower(values$ngram)
    superheroes &lt;- values$ngram %in% c("batman", "superman") &amp;
        values$year &gt;= 1950L
    if (any(superheroes)) {
        keyval(values$ngram[superheroes],
               values[superheroes, c("year", "occurrences")])
    }
}</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note22"/>Note</h3><p>If you are familiar with MapReduce, you might have noticed that <code class="literal">rmr2</code> allows the mapper to accept and emit key-value pairs as lists and data frames that represent a whole chunk of data instead of one record at a time, as is the case with classical MapReduce. This can help with R's performance; vectorized R operations can be used to process the whole chunk of data.</p></div></div><p>The next step occurs behind the scenes, where MapReduce collects all the data emitted by the mappers and groups them by key. In this example, it will find two groups that correspond to the <code class="literal">"batman"</code> and <code class="literal">"superman"</code> keys. MapReduce then calls the reducer function to process one group of data at a time.</p><p>The job of the reducer, given the data for a particular superhero, is to sum the number of occurrences of this superhero's name by year, using <code class="literal">tapply()</code>. This is required because the words in the Ngrams dataset are case sensitive. So, for example, we need to <a id="id417" class="indexterm"/>add up the <a id="id418" class="indexterm"/>number of times that "Batman", "batman", and "BATMAN" appear in each year. The reducer then returns the superhero's name as the key, and a data frame that contains the total number of occurrences by year as the value. The code for the reducer is shown here:</p><div class="informalexample"><pre class="programlisting">reducer &lt;- function(key, values) {
    val &lt;- tapply(values$occurrences, values$year, sum)
    val &lt;- data.frame(year = as.integer(names(val)),
                      occurrences = val)
    keyval(key, val)
}</pre></div><p>Now that we have defined our mapper and reducer functions, we can execute the MapReduce job using <code class="literal">mapreduce()</code>. We will call this function, specify the input directory and data format, the output directory where the results are to be written, and the mapper and reducer functions.</p><div class="informalexample"><pre class="programlisting">job &lt;- mapreduce(input = "/ngrams/data",
                 input.format = input.format,
                 output = "/ngrams/batmanVsuperman",
                 map = mapper, reduce = reducer)</pre></div><p>When this MapReduce job executes, the resultant key-value pairs are written to HDFS in the <code class="literal">/ngrams/batmanVsuperman</code> folder. We can use <code class="literal">from.dfs()</code> to retrieve the results from HDFS into R objects. This function returns a list with two components: <code class="literal">key</code> and <code class="literal">value</code>. In this case, <code class="literal">key</code> is a character vector that specifies the superhero's name for each row of data, and <code class="literal">val</code> is a data frame that contains the corresponding years and occurrence counts.</p><div class="informalexample"><pre class="programlisting">results &lt;- from.dfs(job)
batman &lt;- results$val[results$key == "batman", ]
head(batman)
##      year occurrences
## 1950 1950         153
## 1951 1951         105
## 1952 1952         173
## 1953 1953         133
## 1954 1954         359
## 1955 1955         150
superman &lt;- results$val[results$key == "superman", ]
head(superman)
##      year occurrences
## 1950 1950        1270
## 1951 1951        1130
## 1952 1952        1122
## 1953 1953         917
## 1954 1954        1222
## 1955 1955        1087</pre></div><p>Let's plot the<a id="id419" class="indexterm"/> results in <a id="id420" class="indexterm"/>order to compare how popular these two superheroes have been over the years:</p><div class="mediaobject"><img src="graphics/9263OS_10_01.jpg" alt="Analyzing HDFS data with RHadoop"/><div class="caption"><p>Popularity of Batman versus Superman since the 1950s, according to Google Books</p></div></div><p>While both the superheroes popularity steadily increased over the years, there is an interesting spike in the number of times Superman was mentioned in books in the 1970s. This could be due to the release of the multi Academy Award-winning film, <span class="emphasis"><em>Superman</em></span> starring Christopher Reeve in 1978. However, this surge in popularity was short-lived.</p><p>The time it takes to complete the MapReduce algorithm depends on the size of the data, the complexity of the task, and the number of nodes in the cluster. We tested this example using the <code class="literal">m1.xlarge</code> AWS servers, which have 4 CPUs and 15 GB of RAM each, with <a id="id421" class="indexterm"/>cluster sizes ranging from 4 to 32 core nodes (in EMR terminology, these are nodes that store <a id="id422" class="indexterm"/>data and process them). The following figure shows how the execution time decreases as more nodes are added to the cluster:</p><div class="mediaobject"><img src="graphics/9263OS_10_02.jpg" alt="Analyzing HDFS data with RHadoop"/><div class="caption"><p>Execution time as cluster size increases</p></div></div><p>Because <code class="literal">rmr2</code> starts an instance of R on each Hadoop node to process the data, the efficiency of the MapReduce task depends on that of the R code for the mapper and reducer functions. Many of the techniques in this book to improve the performance of serial R programs can be applied when you design the mapper and reducer functions too. Furthermore, every MapReduce job incurs overheads of starting a new job, reading data from the disk, and coordinating the execution of the job across the cluster. Where possible, combining individual tasks into larger MapReduce tasks that can be executed at one go will help to improve the overall performance by reducing these overheads.</p><p>Once you are done using the Hadoop cluster, remember to terminate the cluster from the AWS EMR console to prevent unexpected charges.</p></div><div class="section" title="Other Hadoop packages for R"><div class="titlepage"><div><div><h2 class="title"><a id="ch10lvl2sec31"/>Other Hadoop packages for R</h2></div></div></div><p>While the scope <a id="id423" class="indexterm"/>of this book allows us to cover only a few R packages that interface with Hadoop, the community has developed many more packages to bring the power of Hadoop to R. Here are a few more packages that can be useful:</p><p>Besides <code class="literal">rhdfs</code> and <code class="literal">rmr2</code>, RHadoop also provides other packages:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">plyrmr</code>: It <a id="id424" class="indexterm"/>provides<a id="id425" class="indexterm"/> functionality similar to <code class="literal">plyr</code> on MapReduce</li><li class="listitem" style="list-style-type: disc"><code class="literal">rhbase</code>: It <a id="id426" class="indexterm"/>provides <a id="id427" class="indexterm"/>functions to work with HBase data</li><li class="listitem" style="list-style-type: disc"><code class="literal">ravro</code>: It <a id="id428" class="indexterm"/>provides<a id="id429" class="indexterm"/> reading/writing of data in the Avro format</li></ul></div><p>Another family of packages<a id="id430" class="indexterm"/> called <code class="literal">RHIPE</code> (for more information refer to <a class="ulink" href="http://www.datadr.org/">http://www.datadr.org/</a>) provides similar MapReduce capabilities with a slightly different syntax:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">RHIPE</code>: This <a id="id431" class="indexterm"/>package <a id="id432" class="indexterm"/>provides the core HDFS and MapReduce functionality</li><li class="listitem" style="list-style-type: disc"><code class="literal">datadr</code>: It <a id="id433" class="indexterm"/>provides <a id="id434" class="indexterm"/>data manipulation capabilities similar to <code class="literal">plyr</code>/<code class="literal">dplyr</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">Trelliscope</code>: It <a id="id435" class="indexterm"/>provides <a id="id436" class="indexterm"/>visualization of large datasets in HDFS</li></ul></div><p>At the time of writing, <code class="literal">RHIPE</code> does not support YARN or MapReduce 2.0. An older version of Hadoop is required to use the <code class="literal">RHIPE</code> packages until this is fixed.</p><p>Another<a id="id437" class="indexterm"/> package, <code class="literal">Segue</code> (for more information refer to <a class="ulink" href="https://code.google.com/p/segue/">https://code.google.com/p/segue/</a>) takes a <a id="id438" class="indexterm"/>different <a id="id439" class="indexterm"/>approach. It does not provide full MapReduce capabilities. Rather, it treats Amazon's EMR as an additional computational resource for computationally heavy R tasks. This is similar to cluster computing in <a class="link" href="ch08.html" title="Chapter 8. Multiplying Performance with Parallel Computing">Chapter 8</a>, <span class="emphasis"><em>Multiplying Performance with Parallel Computing</em></span>, but using EMR as the computational cluster. The <code class="literal">Segue</code> package provides the <code class="literal">emrlapply()</code> function that performs a parallel <code class="literal">lapply</code> operation on an EMR cluster; this is analogous to <code class="literal">mclapply()</code> from the <code class="literal">parallel</code> package.</p></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch10lvl1sec59"/>Summary</h1></div></div></div><p>In this chapter, we learned how to set up a Hadoop cluster on Amazon Elastic MapReduce, and how to use the RHadoop family of packages in order to analyze data in HDFS using MapReduce. We saw how the performance of the MapReduce task improves dramatically as more servers are added to the Hadoop cluster, but the performance eventually reaches a limit due to Amdahl's law (<a class="link" href="ch08.html" title="Chapter 8. Multiplying Performance with Parallel Computing">Chapter 8</a>, <span class="emphasis"><em>Multiplying Performance with Parallel Computing</em></span>).</p><p>Hadoop and its ecosystem of tools is rapidly evolving. Other tools are being actively developed to make Hadoop perform even better. For example, Apache Spark (<a class="ulink" href="http://spark.apache.org/">http://spark.apache.org/</a>) provides Resilient Distributed Datasets (RDDs) that store data in memory across a Hadoop cluster. This allows data to be read from HDFS once and to be used many times in order to dramatically improve the performance of interactive tasks like data exploration and iterative algorithms like gradient descent or k-means clustering. Another example is Apache Storm (<a class="ulink" href="http://storm.incubator.apache.org/">http://storm.incubator.apache.org/</a>) that allows you to process real-time data streams. Because these tools and their associated R interfaces are being actively developed, they will likely change by the time you read this book, so we have decided not to include them here. But they are worth looking into if you have specific needs like in-memory analytics or real-time data processing.</p><p>We have come to the end of the book. It has been an exhilarating journey looking at a whole spectrum of techniques to improve the performance of R programs, from optimizing memory utilization and computational speed to multiplying computational power with parallel programming and cluster computing. What we have covered here is only the basics; there is much more to learn about writing more efficient R code. There are other resources that dive into specific topics in far greater detail than we can here. Package documentation is always useful to read, though sometimes cryptic; sometimes, the only way to find out what works is to try. Of course, there is the great community of R users in online forums, mailing lists and other places, who are always eager to help with answers and tips.</p><p>We hope that you have enjoyed this book and learned from it as much as we have writing it. Thank you for joining us in this journey, and we wish you the very best in exploring the world of R high-performance computing.</p></div></body></html>