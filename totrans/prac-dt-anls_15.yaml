- en: Bringing It All Together
  prefs: []
  type: TYPE_NORMAL
- en: Welcome to the capstone chapter, where we'll walk through some examples to show
    you how the skills learned you've throughout this book can be applied. In this
    chapter, we will learn about open source real-world datasets, some tips on how
    to report results, and a capstone project that blends, transforms, and visualizes
    data from multiple sources. Data analysis is a craft and a journey rewarded by
    the fact that you never stop learning new ways to work with data, provide insights,
    and answer questions. The **data literacy** skills of reading, working with, analyzing,
    and arguing with data is agnostic to any technology, but in my experience, nothing
    replaces the experience of using a specific technology head down and hands-on.
    Our tool of choice in this book was Jupyter Notebook, along with the extendable
    libraries available when using the Python ecosystem, such as pandas, NumPy, and
    NTLK. As you continue to practice and apply these skills, you will become a fungible
    asset who can solve problems using data personally and professionally.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Discovering real-world datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reporting results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Capstone project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The GitHub repository for this book can be found at [https://github.com/PacktPublishing/Practical-Data-Analysis-using-Jupyter-Notebook/tree/master/Chapter12](https://github.com/PacktPublishing/Practical-Data-Analysis-using-Jupyter-Notebook/tree/master/Chapter12).
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, you can download and install the required software for this chapter
    from: [https://www.anaconda.com/products/individual](https://www.anaconda.com/products/individual).
  prefs: []
  type: TYPE_NORMAL
- en: Discovering real-world datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this book, I have emphasized that the power of analytics comes from
    blending data together from multiple sources. An individual data source alone
    rarely includes all the fields required to answer key questions. For example,
    if you have a timestamp field but not a geographic field about a user, you can't
    answer any questions about the data related to where an event took place.
  prefs: []
  type: TYPE_NORMAL
- en: As a good data analyst, always offer up creative solutions that have filled
    data gaps or offer a different perspective by including an external data source.
    Finding new data sources is much easier today than ever before. Let's go over
    a few examples.
  prefs: []
  type: TYPE_NORMAL
- en: Data.gov
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Data.gov** is managed by the United States General Services Administration,
    which offers hundreds of thousands of datasets regarding various topics at the
    State and Federal levels. Most are curated from specific agencies and posted for
    public use. They are open source with limited restrictions. What I like about
    using data.gov is its catalog, which allows you to search across all the topics,
    tags, formats, and organizations. The site was created using open source technologies,
    including CKAN.org, which stands for Comprehensive Knowledge Archive Network.
    This is a platform explicitly built as an open data portal for organizations to
    host datasets and make them transparent. This process democratizes datasets and
    creates standards for publishers to follow, such as exposing data formats (CSV,
    API, and XML, for example) of the source and providing details about how often
    the data is refreshed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a screenshot from the data.gov website where you can search
    for open source datasets from the United States government:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fef37123-cbaa-4aad-aa70-628a21f5ed24.png)'
  prefs: []
  type: TYPE_IMG
- en: The DATA.GOV ([data.gov](http://data.gov)) site is a good starting point but
    it can be overwhelming to find specific data elements. I find the next example
    easier and faster to OVind and use datasets.
  prefs: []
  type: TYPE_NORMAL
- en: The Humanitarian Data Exchange
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**The Humanitarian Data Exchange** (**HDX**) has become topical due to the
    COVID-19 pandemic but has been sponsoring open source datasets for years. These
    datasets contain health-specific statistics from around the world with a focus
    on helping humanity. This is a true example of what is commonly known as **Data
    for Good** because the site provides transparency on the impact it has on people
    for free. What I like about this site is how it integrates the Creative Commons
    License into the data catalog so that you can understand any limitations around
    reusing or distributing the data from the source. Part of their terms of service
    is to restrict the use of any **Personally Identifiable Information** (**PII**)
    so that the data already adheres to regulations that support protecting individuals
    from being directly identified.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a screenshot from The Humanitarian Data Exchange website ([data.humdata.org](http://data.humdata.org)),
    where you can search for humanitarian datasets about locations all around the
    world:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c2e615e5-4ea4-4874-8ed7-2986ce574145.png)'
  prefs: []
  type: TYPE_IMG
- en: If you are searching for financial data elements categorized by global statistical
    measures, You can begin your search at **The World Bank** data portal.
  prefs: []
  type: TYPE_NORMAL
- en: The World Bank
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**The World Bank** has an open data repository that includes thousands of datasets
    categorized and conformed by country with metrics classified as indicators. The
    site allows you to compare your data to global baseline metrics such as **Gross
    Domestic Product** (**GDP**), which creates thresholds and performance metrics
    for your analysis. I find that the website is easy to navigate and is quick to
    identify datasets that can easily be joined to other datasets because it includes
    defined data type values such as ISO country codes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a screenshot from the World Bank Open Data website ([data.worldbank.org](http://data.worldbank.org)),
    where you can search for financially focused datasets as they impact countries
    around the world:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7edc9087-995c-4cc9-adf1-b4eb4978ce27.png)'
  prefs: []
  type: TYPE_IMG
- en: The World Bank data portal has many rich examples that include data visualizations
    for quick analysis and preview before you start using them. The next site we will
    look at, **Our World in Data**, has similar usability features.
  prefs: []
  type: TYPE_NORMAL
- en: Our World in Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Our World in Data** site started with research data from Oxford University
    but has evolved into an online publication based on scientific studies focused
    on helping the world solve problems using data. I enjoy the site because you can
    uncover historical trends that provide context regarding how humanity is improving
    in many cases, but not at the same pace when you look at different countries.
    I also find their data visualizations easy to use and navigate; for example, you
    can filter and compare results between different countries or regions. Their data
    and site have become invaluable during the COVID-19 pandemic as you can track
    cases and compare progress between different countries and within the United States.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a screenshot from the Our World in Data website ([ourworldindata.org](https://ourworldindata.org/)),
    where you can explore thousands of charts and open source data focused on helping
    solve global problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b09a8c73-26d4-426d-b9ae-7e5eae3579fa.png)'
  prefs: []
  type: TYPE_IMG
- en: With only the few examples I showcased here, you can see that you have access
    to thousands of datasets available to use for research, blending, or learning.
    Be mindful of the open source licenses that may restrict distribution or limit
    how often you can extract the data. This becomes important when building any automated
    data pipelines or using APIs to pull data on demand. You will probably have to
    find alternative paid data companies in those situations. In either case, even
    when the source data is conformed and structured, it may not answer all the questions
    required for your analysis. Another point to note is that the aggregation level
    of the data might not be very high. For example, if data is aggregated by country,
    you can't join the data by city. In those situations, being transparent in terms
    of how you are using the data from external sources is important, along with quoting
    the source and providing a disclaimer stating that external data is being used.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will cover some best practices that can be used to report the results
    from your analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Reporting results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How to present your analysis results will vary by the audience, the time available,
    and the level of detail required to tell a story about the data. Your data may
    have an inherent bias, be incomplete, or require more attributes in order to create
    a complete picture, so don't be afraid to include this information in your analysis.
    For example, if you have done some research on climate change, which is a very
    broad topic, presenting the consumers of your analysis with a narrow scope of
    assumptions specific to your dataset is important. How and where you include this
    information is not as important as ensuring it is available for peer review.
  prefs: []
  type: TYPE_NORMAL
- en: Storytelling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Storytelling with data requires some practice and you need time to sell your
    message to the audience. Like any good story, presenting the data results in a
    cadence with a beginning, middle, and end will help with the flow of the analysis
    being consumed. I also find using analogies to compare your findings will offer
    some connectivity between the data and the intended consumers. Knowing who you
    are presenting the results of your analysis to is just as important as understanding
    the data itself.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in poker, knowing the probability of your hand and your bankroll
    are not the only factors for success. Who you are playing against is a contributing
    factor to how much you will win or lose. So, understanding the reactions of the
    players at the table will help you make decisions to fold, call, or raise during
    the game.
  prefs: []
  type: TYPE_NORMAL
- en: So, when presenting results, think like a poker player and be mindful of who
    you are presenting your data to in order to convince the audience about your conclusions.
    For example, if you are presenting to senior management, time is limited, so being
    direct, brief, and skipping to the summary results is suggested. If the audience
    is your peers, then walking through the journey of how you came to your conclusions
    will resonate because it builds credibility and trust.
  prefs: []
  type: TYPE_NORMAL
- en: Regardless of the audience, if you present a chart with trends becoming higher
    over time, be prepared to offer proof of the underlining source, along with how
    the metric was calculated. Without that information, doubt about the ability to
    recreate your findings will lead to your analysis being dismissed.
  prefs: []
  type: TYPE_NORMAL
- en: A good data analyst will be able to **read the room** and understand how much
    detail is required when presenting results. There have been plenty of times when
    I have presented findings where I had to cut my message short because when I looked
    up to see how engaged the people were, I realized they were lost. Rather than
    continuing, I stopped and offered up questions so I could provide more clarity.
    Just changing the format to offer time for questions in the middle of the presentation
    helped both the audience and myself refocus our attention on the conclusions.
  prefs: []
  type: TYPE_NORMAL
- en: So, be authentic and provide transparency in your analysis. If you make a mistake
    or misinterpret the data, the audience will be forgiving as long as you continue
    to improve and avoid repeating a missed step. I find having peers, mentors, and
    managers who can provide honest and constructive feedback before you present yourself,
    helps improve your messaging and presentation of the data artifacts.
  prefs: []
  type: TYPE_NORMAL
- en: From a **data literacy** perspective, focus less on the technology used and
    more on the insights gained in your analysis. Realize that the people interpreting
    your results will come from a diverse set of perspectives. A CEO can understand
    a chart of the balance sheet of company financial data but probably does not care
    which NumPy library was used for your analysis. In our final exercise in this
    book, we will create a capstone project with a focus on answering a real-world
    question using data from multiple sources.
  prefs: []
  type: TYPE_NORMAL
- en: The Capstone project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For our real-world dataset example, we are going to use two different sources
    and blend them together using the techniques we've learned throughout this book.
    Since **Know Your Data** (**KYD**) still applies, let's walk through the sources.
  prefs: []
  type: TYPE_NORMAL
- en: KYD sources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first source is from the World Bank and is a list of green bonds, which
    are used to fund the reduction of carbon emissions and climate-related projects.
    It was downloaded from the website, so it's a snapshot based on a point in time
    stored as a CSV file with 115 rows and 10 columns, including a header.
  prefs: []
  type: TYPE_NORMAL
- en: 'A visual preview of the data in Microsoft Excel can be seen in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/44f8155a-c7d3-4463-90d7-7705eeba8666.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The source data has some insights that we can mine through *as is*, such as
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: How many **bonds** are issued by **Currency?**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the total distribution of the bonds by **Currency**?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which **bonds** are maturing in the next 3, 5, 7, or 10 years?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, we have a data gap for the questions related to the local currency
    for the country of issuance. Since currency exchange rates fluctuate daily, there
    are more questions we could answer if we had that information available to join
    by the Currency field. So, our second source of data that we want to work with
    is from the **Humanitarian Data Exchange** (**HDX**) site. This includes the **Foreign
    Exchange** (**FX**) rate by country designated by the currency as it relates to
    the **United States Dollar** (**USD**) by date from `1/4/1999` to `5/7/2020` in
    the date format of M/D/YYYY. This is another CSV file that can be downloaded with
    5,465 rows and 34 columns on a specific date. There is a header row, and the first
    record of data includes metadata tags prefixed with a hash sign, #, which is used
    by the HDX site for metadata management and cataloging.
  prefs: []
  type: TYPE_NORMAL
- en: 'A visual preview of the data in Microsoft Excel can be seen in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5fa20245-f3b0-49a3-b7d2-ed686d00b10e.png)'
  prefs: []
  type: TYPE_IMG
- en: Previewing data in Microsoft Excel or any spreadsheet tool is a best practice
    if you wish to visually understand the structure of the data before working with
    it in Jupyter Notebook. If the data volumes are large, break off a sample so it
    can be loaded on your workstation.
  prefs: []
  type: TYPE_NORMAL
- en: So, our bonds data with currencies lists the values by rows and our FX rate
    data is listing the values for each currency by column with a value assigned for
    each specific date. There are a few ways to solve this but for our example, we
    are interested in blending the latest FX rate by currency to our bond data so
    that we can convert the **USD Equivalent** value into the **Local CCY Equivalent**.
    That way, we can perform analysis of the data in either USD or the respective
    country's currency and report the findings.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s open a new Jupyter Notebook session and get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to import the libraries required to work with and analyze the
    results by including the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We will read in the first `.csv` file using the `pandas` library and assign
    the result to a variable named `df_greenbonds`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Be sure to upload the source CSV file in the correct file location so that you
    can reference it in your Jupyter Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'To validate all the records have loaded successfully, we need to run a `shape()`
    function against the DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look as follows, where the values `115` and `10` will be displayed.
    These match the number of rows and columns in the source CSV file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bada3bff-2665-4c0b-907d-548e9c7b298f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To preview the DataFrame, we can run the following `head()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look as follows, where the DataFrame results will be displayed
    in the Notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7f52c4c7-6f01-4706-b86f-ef1e63ddb8ed.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will read in the second CSV file using the `pandas` library and assign the
    result to a variable named `df_fx_rates`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Be sure to upload the source CSV file to the correct file location so that you
    can reference it in your Jupyter Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'To validate all the records have loaded successfully, run a `shape()` function
    against the DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look as follows, where the values `5464` and `34` will be displayed
    in parentheses. These match the number of rows and columns in the source CSV file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eb736af9-b7b4-4988-98bc-b649b6a4d695.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To preview the DataFrame, we can run the following `head()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look as follows, where the DataFrame results will be displayed
    in the Notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0157ac3b-a660-434d-a8ee-3a5f90472b42.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since we know from the prior chapters that data is inherently messy and requires
    some cleanup, we will delete the first row because it contains the HDX hashtag
    metadata values, which are not required for our analysis:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: It is recommended that you clean up as you go through the analysis step by step
    in case you need to troubleshoot and recreate any prior steps in the data analysis
    workflow.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output will look as follows, where the DataFrame results will be displayed
    in the Notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/207cab68-a260-4634-9d3b-17becd4eb3e3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For our analysis, we want to focus on the latest FX rate available in the file.
    You could take the first row available in the DataFrame, but a more robust method
    would be to use the `max()` function so that how the data is sorted becomes irrelevant.
    To verify that the correct value will work before we filter the DataFrame, use
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look as follows, where the results from the command will be
    displayed in the Notebook. In this dataset, the max date at the time of download
    is `2020-05-07`, with the date format as `YYYY-MM-DD`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ad385d86-1983-4768-ba44-59ed153e846d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the prior step, we are confident that our filter will use the correct
    `Date` value, so we will create a new DataFrame with only one specific date value
    so that we can join the results in later steps. The new DataFrame is named `df_fx_rates_max_date`
    and is a result of filtering the original DataFrame, named `df_fx_rates`, by the
    `Date` field, where only the calculated max `Date` value will be returned. We
    will add the following `head()` function to validate the results in the Notebook:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look as follows, where the results from the command will be
    displayed in the Notebook. The new DataFrame, named `df_fx_rates_max_date`, will
    only have one record with a header containing 34 columns. Each column will represent
    the latest available currency value using the three-letter country''s designation,
    such as `EUR`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2b1883d9-90d4-407a-b20d-7fb90fcc302c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We still have more work to do in order to join this data to our original bond
    DataFrame. We need to transform it using the `transpose()` function, which will
    change all the columns into rows. In other technologies, this concept is called
    a pivot, crosstab, or crosstable; this was covered in more detail in [Chapter
    4](c6eceec8-e006-404b-9be1-bc96de29991a.xhtml),*Creating Your First pandas DataFrame*.
    The results are stored in a new DataFrame called `df_rates_transposed`. We rename
    the columns to make it easier to work with them. We also need to run the following `head()`
    command to preview the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look as follows, where the new DataFrame, named `df_rates_transposed`,
    is displayed. Here, all the columns have been converted into rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4767865b-6765-4e5d-9f19-c50afe379b74.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The goal is for our reference table to have all the FX rates by currency values
    in the same format. However, notice that, on the first row in the following diagram, the
    `Date` value is mixed with `Currency_values`, which need to have the same data
    type. The need to have conformed and consistent data values represented in structured
    data has been reinforced throughout this book, so we will clean up the DataFrame
    by dropping the `Date` record. We will also use the `reindex()` function to make
    it easier to join in the next step and then run the following `head()` command
    to verify the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look as follows, where the new DataFrame, named `df_rates_transposed`,
    is displayed as before, except now, the `Date` record has been deleted. This means
    the first row will be `EUR` with a `Currency_Value` of `1.0783`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/909df028-ecd0-4dae-8ed1-a2fa8a8e42dc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We are now ready to join the transformed and cleaned FX rates to our original
    bonds source using the common `Currency` join key field. Because we want all the
    records from the `df_greenbonds` source and only the matching values from `df_rates_transposed`,
    we will use a left join. To display and verify the results, we use the following `head()`
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look as follows, where the results of the left join are stored
    in the `df_greenbonds_revised` DataFrame. The following screenshot shows a table
    with 5 rows and 11 columns. It includes a header row and index values that are
    not labeled:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d3fb925a-3455-4a43-a0a6-f3b4a28f4e38.png)'
  prefs: []
  type: TYPE_IMG
- en: Like in the preceding diagram, be sure to scroll to the right to see that a
    new column called `Currency_Value` is included.
  prefs: []
  type: TYPE_NORMAL
- en: 'An advantage of constantly running the `head()` command to validate results
    in each step is that you can make observations about the data as you prepare and
    clean it for further analysis. In the preceding screenshot , we can see a `null()` value
    in `Currency_Value`, which is displayed as `NaN`. We covered working with NaN
    values in [Chapter 5](bea8a62c-4469-47e3-a668-10fbb91815ea.xhtml),*Gathering and
    Loading Data in Python*. This is a result of the left join and is expected because
    there was no value of `USD` in the FX rates source data. This makes sense because
    you don''t need to convert the currency of USD. However, this will have an impact
    when we attempt to create calculations from the values in this column. The solution,
    in this case, is to just convert all the `NaN` values to `1` because the FX rate
    conversion for USD is 1\. There will be no output after running this command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Since our CSV file sources do not include a data dictionary, the defined data
    type for each field is unknown. We can solve any inconsistencies within the data
    values by applying the `astype()` function. We will focus on the two columns we
    used for calculating the local currency rate by converting them into a `dtype`
    of the `float` type. There will be no output after running this command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to create a new calculated column in our existing DataFrame
    that will divide the `USD Equivalent` column by `Currency_Value`. The result will
    be stored in a new column named `Local CCY` in the same DataFrame. There will
    be no output after running this command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can convert the data types of the specific columns back into integers
    and focus our attention on the key columns by explicitly identifying them. We
    can do this by using the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look as follows, where the results from the preceding commands
    will be displayed in the Notebook. The original column, `USD Equivalent`, is displayed
    as an integer and we now have the `Local CCY` column available to the right of
    `Currency_Value`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/61e98320-646e-48fa-9bff-8a19b9877049.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To analyze the results, let''s group the data by `Currency` and only sum the
    values for both the `USD Equivalent` and `Local CCY` fields using the following
    commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look as follows, where the data is now aggregated by `Currency`,
    with the total sum of `USD Equivalent` and `Local CCY` also being displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/547ec7ea-af5e-466d-af9e-725a732d4f33.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Another type of analysis would be to see how the data is distributed by `Currency`
    visually by creating a horizontal bar chart using the `matplotlib` library''s `plot()`
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6b65612c-0f3b-435a-9b33-2c82427cfd0e.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, an overwhelming number of green bonds have been issued in USD
    because it has the largest bar by a large margin compared to the other currencies.
    The reason why this is the case is not evident by looking exclusively at this
    data, so additional analysis will need to be done. When you present your findings
    to others within the organization, this is often the reality, where blending data
    together offers more insights but then leads to more unanswered questions. This,
    in turn, leads to the need for more data to be added to your existing sources.
    Finding a balance between when to stop blending more and more data together is
    challenging, so adding incremental milestones to present your findings is encouraged.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With that, we have walked through many of the different concepts we covered
    throughout this book in one comprehensive exercise. In this chapter, we learned
    more about real-world data sources that can be used for analysis. We also created
    a repeatable workflow that can be summarized as a workflow that collects external
    data sources, joins them together, and then analyzes the results. Since we know
    the reality of working with data is never that straightforward, we walked through
    some inherent challenges of working with it. We have broken down the steps of
    collecting multiple sources, transforming them, and cleansing, joining, grouping,
    and visualizing the results. The more you work hands-on with data, the easier
    it is to apply these concepts to any dataset with the foundation remaining constant.
    As you increase your data literacy skills when it comes to working with data,
    you will notice the syntax and tools will change but that the challenges and opportunities
    to solve problems remain the same. I encourage you to continue investing in yourself
    by continuously learning more about data. I hope you find it as fulfilling a journey
    as I do!
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Humanitarian Data Exchange site: [https://data.humdata.org/](https://data.humdata.org/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data.gov – the US government's open data: [https://www.data.gov/](https://www.data.gov/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Creative Commons site: [https://creativecommons.org/](https://creativecommons.org/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Open Data Commons site: [https://opendatacommons.org/](https://opendatacommons.org/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Our World in Data site: [https://ourworldindata.org/](https://ourworldindata.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The World Bank Open Data site: [https://data.worldbank.org/](https://data.worldbank.org/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
