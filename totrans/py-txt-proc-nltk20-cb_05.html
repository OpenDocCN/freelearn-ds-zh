<html><head></head><body><div class="chapter" title="Chapter&#xA0;5.&#xA0;Extracting Chunks"><div class="titlepage"><div><div><h1 class="title"><a id="ch05"/>Chapter 5. Extracting Chunks</h1></div></div></div><p>In this chapter, we will cover:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Chunking and chinking with regular expressions</li><li class="listitem" style="list-style-type: disc">Merging and splitting chunks with regular expressions</li><li class="listitem" style="list-style-type: disc">Expanding and removing chunks with regular expressions</li><li class="listitem" style="list-style-type: disc">Partial parsing with regular expressions</li><li class="listitem" style="list-style-type: disc">Training a tagger-based chunker</li><li class="listitem" style="list-style-type: disc">Classification-based chunking</li><li class="listitem" style="list-style-type: disc">Extracting named entities</li><li class="listitem" style="list-style-type: disc">Extracting proper noun chunks</li><li class="listitem" style="list-style-type: disc">Extracting location chunks</li><li class="listitem" style="list-style-type: disc">Training a named entity chunker</li></ul></div><div class="section" title="Introduction"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec49"/>Introduction</h1></div></div></div><a id="id318" class="indexterm"/><p>
<span class="strong"><strong>Chunk extraction</strong></span> or <a id="id319" class="indexterm"/>
<span class="strong"><strong>partial parsing</strong></span> is the process of extracting short phrases from a part-of-speech tagged sentence. This is different than <a id="id320" class="indexterm"/>full parsing, in that we are interested in standalone <a id="id321" class="indexterm"/>
<span class="strong"><strong>chunks</strong></span> or <span class="strong"><strong>phrases</strong></span>
<a id="id322" class="indexterm"/> instead of full parse trees. The idea is that meaningful phrases can be extracted from a sentence by simply looking for particular patterns of part-of-speech tags.</p><p>As in <a class="link" href="ch04.html" title="Chapter 4. Part-of-Speech Tagging">Chapter 4</a>, <span class="emphasis"><em>Part-of-Speech Tagging</em></span>, we will be using the <a id="id323" class="indexterm"/>
<span class="strong"><strong>Penn Treebank corpus</strong></span> for basic training and testing chunk extraction. We will also be using the CoNLL 2000 corpus as it has a simpler and more flexible format that supports multiple chunk types (refer to the <span class="emphasis"><em>Creating a chunked phrase corpus</em></span> recipe in <a class="link" href="ch03.html" title="Chapter 3. Creating Custom Corpora">Chapter 3</a>, <span class="emphasis"><em>Creating Custom Corpora</em></span> for more details on the <code class="literal">conll2000</code> corpus and IOB tags).</p></div></div>
<div class="section" title="Chunking and chinking with regular expressions"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec50"/>Chunking and chinking with regular expressions</h1></div></div></div><a id="id324" class="indexterm"/><a id="id325" class="indexterm"/><a id="id326" class="indexterm"/><a id="id327" class="indexterm"/><a id="id328" class="indexterm"/><p>Using modified regular expressions, we can define <span class="strong"><strong>chunk patterns</strong></span>. These are patterns of part-of-speech tags that define what kinds of words make up a chunk. We can also define patterns for what kinds of words should not be in a chunk. These unchunked words are known as <a id="id329" class="indexterm"/>
<span class="strong"><strong>chinks</strong></span>.</p><a id="id330" class="indexterm"/><p>A <code class="literal">ChunkRule</code> specifies what to include in a chunk, while <a id="id331" class="indexterm"/>a <code class="literal">ChinkRule</code> specifies what to exclude from a chunk. In other words, <a id="id332" class="indexterm"/>
<span class="strong"><strong>chunking</strong></span> creates chunks, while <span class="strong"><strong>chinking</strong></span>
<a id="id333" class="indexterm"/> breaks up those chunks.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec173"/>Getting ready</h2></div></div></div><p>We first need to know how to define chunk patterns. These are modified regular expressions designed to match sequences of part-of-speech tags. An individual tag is specified by surrounding angle brackets, such as <code class="literal">&lt;NN&gt;</code> to match a noun tag. Multiple tags can then be combined, as in <code class="literal">&lt;DT&gt;&lt;NN&gt;</code> to match a determiner followed by a noun. Regular expression syntax can be used within the angle brackets to match individual tag patterns, so you can do <code class="literal">&lt;NN.*&gt;</code> to match all nouns including <code class="literal">NN</code> and <code class="literal">NNS</code>. You can also use regular expression syntax outside of the angle brackets to match patterns of tags. <code class="literal">&lt;DT&gt;?&lt;NN.*&gt;+</code> will match an optional determiner followed by one or more nouns. The chunk patterns are converted internally to regular expressions using the <a id="id334" class="indexterm"/>
<code class="literal">tag_pattern2re_pattern()</code> function:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from nltk.chunk import tag_pattern2re_pattern
&gt;&gt;&gt; tag_pattern2re_pattern('&lt;DT&gt;?&lt;NN.*&gt;+')
'(&lt;(DT)&gt;)?(&lt;(NN[^\\{\\}&lt;&gt;]*)&gt;)+'</pre></div><p>You don't have to use this function to do chunking, but it might be useful or interesting to see how your chunk patterns convert to regular expressions.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec174"/>How to do it...</h2></div></div></div><p>The pattern for specifying a chunk is to use surrounding curly braces, such as <code class="literal">{&lt;DT&gt;&lt;NN&gt;}</code>. To specify a chink, you flip the braces, as in <code class="literal">}&lt;VB&gt;{</code>. These rules can be combined into a <a id="id335" class="indexterm"/>
<span class="strong"><strong>grammar</strong></span> for a particular phrase type. Here's a grammar for noun-phrases that combines both a chunk and a chink pattern, along with the result of parsing the sentence "The book has many chapters":</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from nltk.chunk import RegexpParser
&gt;&gt;&gt; chunker = RegexpParser(r'''
... NP:
...    {&lt;DT&gt;&lt;NN.*&gt;&lt;.*&gt;*&lt;NN.*&gt;}
...    }&lt;VB.*&gt;{
... ''')
&gt;&gt;&gt; chunker.parse([('the', 'DT'), ('book', 'NN'), ('has', 'VBZ'), ('many', 'JJ'), ('chapters', 'NNS')])
Tree('S', [Tree('NP', [('the', 'DT'), ('book', 'NN')]), ('has', 'VBZ'), Tree('NP', [('many', 'JJ'), ('chapters', 'NNS')])])</pre></div><p>The grammar tells the <code class="literal">RegexpParser</code> that there are two rules for parsing <code class="literal">NP</code> chunks. The first chunk pattern says that a chunk starts with a <span class="emphasis"><em>determiner</em></span> followed by any kind of <span class="emphasis"><em>noun</em></span>. Then any number of other words is allowed, until a final noun is found. The second pattern says that verbs should be <span class="emphasis"><em>chinked</em></span>, thus separating any large chunks that contain a verb. The result is a tree with two noun-phrase chunks: "the book" and "many chapters".</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note18"/>Note</h3><p>Tagged sentences are always parsed into a <code class="literal">Tree</code> (found in the <code class="literal">nltk.tree</code> module). The top node of the <code class="literal">Tree</code> is '<code class="literal">S</code>', which stands for <span class="emphasis"><em>sentence</em></span>. Any chunks found will be subtrees whose nodes will refer to the chunk type. In this case, the chunk type is '<code class="literal">NP</code>' for <span class="emphasis"><em>noun-phrase</em></span>. Trees can be drawn calling the <code class="literal">draw()</code> <a id="id336" class="indexterm"/>method, as in <code class="literal">t.draw()</code>.</p></div></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec175"/>How it works...</h2></div></div></div><a id="id337" class="indexterm"/><a id="id338" class="indexterm"/><a id="id339" class="indexterm"/><a id="id340" class="indexterm"/><a id="id341" class="indexterm"/><p>Here's what happens, step-by-step:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">The sentence is converted into a flat <code class="literal">Tree</code>, as shown in the following figure:<div class="mediaobject"><img src="graphics/3609_05_01.jpg" alt="How it works..."/></div></li><li class="listitem">The <code class="literal">Tree</code> is used to create a <code class="literal">ChunkString</code>.</li><li class="listitem"><code class="literal">RegexpParser</code> parses the grammar to create a <code class="literal">NP RegexpChunkParser</code> with the given rules.</li><li class="listitem">A <code class="literal">ChunkRule</code> is created and applied to the <code class="literal">ChunkString</code>, which matches the entire sentence into a chunk, as shown in the following figure:<div class="mediaobject"><img src="graphics/3609_05_02.jpg" alt="How it works..."/></div></li><li class="listitem">A <code class="literal">ChinkRule</code> is created and applied to the same <code class="literal">ChunkString</code>, which splits the big chunk into two smaller chunks with a verb between them, as shown in the following figure:<div class="mediaobject"><img src="graphics/3609_05_03.jpg" alt="How it works..."/></div></li><li class="listitem">The <code class="literal">ChunkString</code> is converted back to a <code class="literal">Tree</code>, now with two <span class="strong"><strong>NP</strong></span> chunk subtrees, as shown in the following figure:<div class="mediaobject"><img src="graphics/3609_05_04.jpg" alt="How it works..."/></div></li></ol></div><p>You <a id="id342" class="indexterm"/>
<a id="id343" class="indexterm"/>
<a id="id344" class="indexterm"/>
<a id="id345" class="indexterm"/>
<a id="id346" class="indexterm"/>can do this yourself using the classes in <code class="literal">nltk.chunk.regexp</code>. <code class="literal">ChunkRule</code> and <code class="literal">ChinkRule</code> are both subclasses of <code class="literal">RegexpChunkRule</code> and require two arguments: the pattern, and a description of the rule. <code class="literal">ChunkString</code> is an object that starts with a flat tree, which is then modified by each rule when it is passed in to the rule's <code class="literal">apply()</code> method. A <code class="literal">ChunkString</code> is converted back to a <code class="literal">Tree</code> with the <code class="literal">to_chunkstruct()</code> method. Here's the code to demonstrate it:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from nltk.chunk.regexp import ChunkString, ChunkRule, ChinkRule
&gt;&gt;&gt; from nltk.tree import Tree
&gt;&gt;&gt; t = Tree('S', [('the', 'DT'), ('book', 'NN'), ('has', 'VBZ'), ('many', 'JJ'), ('chapters', 'NNS')])
&gt;&gt;&gt; cs = ChunkString(t)
&gt;&gt;&gt; cs
&lt;ChunkString: '&lt;DT&gt;&lt;NN&gt;&lt;VBZ&gt;&lt;JJ&gt;&lt;NNS&gt;'&gt;
&gt;&gt;&gt; ur = ChunkRule('&lt;DT&gt;&lt;NN.*&gt;&lt;.*&gt;*&lt;NN.*&gt;', 'chunk determiners and nouns')
&gt;&gt;&gt; ur.apply(cs)
&gt;&gt;&gt; cs
&lt;ChunkString: '{&lt;DT&gt;&lt;NN&gt;&lt;VBZ&gt;&lt;JJ&gt;&lt;NNS&gt;}'&gt;
&gt;&gt;&gt; ir = ChinkRule('&lt;VB.*&gt;', 'chink verbs')
&gt;&gt;&gt; ir.apply(cs)
&gt;&gt;&gt; cs
&lt;ChunkString: '{&lt;DT&gt;&lt;NN&gt;}&lt;VBZ&gt;{&lt;JJ&gt;&lt;NNS&gt;}'&gt;
&gt;&gt;&gt; cs.to_chunkstruct()
Tree('S', [Tree('CHUNK', [('the', 'DT'), ('book', 'NN')]), ('has', 'VBZ'), Tree('CHUNK', [('many', 'JJ'), ('chapters', 'NNS')])])</pre></div><p>The <a id="id347" class="indexterm"/>
<a id="id348" class="indexterm"/>
<a id="id349" class="indexterm"/>
<a id="id350" class="indexterm"/>
<a id="id351" class="indexterm"/>preceding tree diagrams can be drawn at each step by calling <code class="literal">cs.to_chunkstruct().draw()</code>.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec176"/>There's more...</h2></div></div></div><p>You will notice that the subtrees from the <code class="literal">ChunkString</code> are tagged as <code class="literal">'CHUNK'</code> and not <code class="literal">'NP'</code>. That's because the previous rules are phrase agnostic; they create chunks without needing to know what kind of chunks they are.</p><p>Internally, the <code class="literal">RegexpParser</code> creates a <code class="literal">RegexpChunkParser</code> for each chunk phrase type. So if you are only chunking <code class="literal">NP</code> phrases, there will only be one <code class="literal">RegexpChunkParser</code>. The <code class="literal">RegexpChunkParser</code> gets all the rules for the specific chunk type, and handles applying the rules in order and converting the <code class="literal">'CHUNK'</code> trees to the specific chunk type, such as <code class="literal">'NP'</code>.</p><p>Here's some code to illustrate the usage of <code class="literal">RegexpChunkParser</code>. We pass the previous two rules into the <code class="literal">RegexpChunkParser</code>, and then parse the same sentence tree we created before. The resulting tree is just like what we got from applying both rules in order, except <code class="literal">'CHUNK'</code> has been replaced with <code class="literal">'NP'</code> in the two subtrees. This is because <code class="literal">RegexpChunkParser</code> defaults to <code class="literal">chunk_node='NP'</code>.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from nltk.chunk import RegexpChunkParser
&gt;&gt;&gt; chunker = RegexpChunkParser([ur, ir])
&gt;&gt;&gt; chunker.parse(t)
Tree('S', [Tree('NP', [('the', 'DT'), ('book', 'NN')]), ('has', 'VBZ'), Tree('NP', [('many', 'JJ'), ('chapters', 'NNS')])])</pre></div><div class="section" title="Different chunk types"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec56"/>Different chunk types</h3></div></div></div><p>If y<a id="id352" class="indexterm"/>ou wanted to parse a different chunk type, then you could pass that in as <code class="literal">chunk_node</code> to <code class="literal">RegexpChunkParser</code>. Here's the same code we have just seen, but instead of <code class="literal">'NP'</code> subtrees, we will call them <code class="literal">'CP'</code> for <span class="emphasis"><em>custom phrase</em></span>.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from nltk.chunk import RegexpChunkParser
&gt;&gt;&gt; chunker = RegexpChunkParser([ur, ir], chunk_node='CP')
&gt;&gt;&gt; chunker.parse(t)
Tree('S', [Tree('CP', [('the', 'DT'), ('book', 'NN')]), ('has', 'VBZ'), Tree('CP', [('many', 'JJ'), ('chapters', 'NNS')])])</pre></div><p>
<code class="literal">RegexpParser</code> does this internally when you specify multiple phrase types. This will be covered in <span class="emphasis"><em>Partial parsing with regular expressions</em></span>.</p></div><div class="section" title="Alternative patterns"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec57"/>Alternative patterns</h3></div></div></div><p>The same parsing results can be obtained by using two chunk patterns in the grammar, and discarding the chink pattern:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; chunker = RegexpParser(r'''
... NP:
...    {&lt;DT&gt;&lt;NN.*&gt;}
...    {&lt;JJ&gt;&lt;NN.*&gt;}
... ''')
&gt;&gt;&gt; chunker.parse(t)
Tree('S', [Tree('NP', [('the', 'DT'), ('book', 'NN')]), ('has', 'VBZ'), Tree('NP', [('many', 'JJ'), ('chapters', 'NNS')])])</pre></div><p>In fact, you could reduce the two chunk patterns into a single pattern.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; chunker = RegexpParser(r'''
... NP:
...    {(&lt;DT&gt;|&lt;JJ&gt;)&lt;NN.*&gt;}
... ''')
&gt;&gt;&gt; chunker.parse(t)
Tree('S', [Tree('NP', [('the', 'DT'), ('book', 'NN')]), ('has', 'VBZ'), Tree('NP', [('many', 'JJ'), ('chapters', 'NNS')])])</pre></div><p>How you create and combine patterns is really up to you. Pattern creation is a process of trial and error, and entirely depends on what your data looks like and which patterns are easiest to express.</p></div><div class="section" title="Chunk rule with context"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec58"/>Chunk rule with context</h3></div></div></div><p>You <a id="id353" class="indexterm"/>can also create chunk rules with a surrounding tag context. For example, if your pattern is <code class="literal">&lt;DT&gt;{&lt;NN&gt;}</code>, which will be parsed into a <code class="literal">ChunkRuleWithContext</code>. Any time there's a tag on either side of the curly braces, you will get a <code class="literal">ChunkRuleWithContext</code> instead of a <code class="literal">ChunkRule</code>. This can allow you to be more specific about when to parse particular kinds of chunks.</p><p>Here<a id="id354" class="indexterm"/>'s an example of using <code class="literal">ChunkWithContext</code> directly. It takes four arguments: the left context, the pattern to chunk, the right context, and a description:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from nltk.chunk.regexp import ChunkRuleWithContext
&gt;&gt;&gt; ctx = ChunkRuleWithContext('&lt;DT&gt;', '&lt;NN.*&gt;', '&lt;.*&gt;', 'chunk nouns only after determiners')
&gt;&gt;&gt; cs = ChunkString(t)
&gt;&gt;&gt; cs
&lt;ChunkString: '&lt;DT&gt;&lt;NN&gt;&lt;VBZ&gt;&lt;JJ&gt;&lt;NNS&gt;'&gt;
&gt;&gt;&gt; ctx.apply(cs)
&gt;&gt;&gt; cs
&lt;ChunkString: '&lt;DT&gt;{&lt;NN&gt;}&lt;VBZ&gt;&lt;JJ&gt;&lt;NNS&gt;'&gt;
&gt;&gt;&gt; cs.to_chunkstruct()
Tree('S', [('the', 'DT'), Tree('CHUNK', [('book', 'NN')]), ('has', 'VBZ'), ('many', 'JJ'), ('chapters', 'NNS')])</pre></div><p>This<a id="id355" class="indexterm"/> example only chunks nouns that follow a determiner, therefore ignoring the noun that follows an adjective. Here's how it would look using the <code class="literal">RegexpParser</code>:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; chunker = RegexpParser(r'''
... NP:
...    &lt;DT&gt;{&lt;NN.*&gt;}
... ''')
&gt;&gt;&gt; chunker.parse(t)
Tree('S', [('the', 'DT'), Tree('NP', [('book', 'NN')]), ('has', 'VBZ'), ('many', 'JJ'), ('chapters', 'NNS')])</pre></div></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec177"/>See also</h2></div></div></div><p>In the next recipe, we will cover merging and splitting chunks.</p></div></div>
<div class="section" title="Merging and splitting chunks with regular expressions"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec51"/>Merging and splitting chunks with regular expressions</h1></div></div></div><p>In t<a id="id356" class="indexterm"/>
<a id="id357" class="indexterm"/>his recipe, we will cover two more rules for chunking. A <code class="literal">MergeRule</code> can merge two chunks together based on the end of the first chunk and the beginning of the second chunk. A <code class="literal">SplitRule</code> will split a chunk into two based on the specified split pattern.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec178"/>How to do it...</h2></div></div></div><p>A <code class="literal">Sp</code>
<a id="id358" class="indexterm"/>
<code class="literal">litRule</code> is specified with two opposing curly braces surrounded by a pattern on either side. To split a chunk after a noun, you would do <code class="literal">&lt;NN.*&gt;}{&lt;.*&gt;</code>. A <code class="literal">Merg</code>
<a id="id359" class="indexterm"/>
<code class="literal">eRule</code> is specified by flipping the curly braces, and will join chunks where the end of the first chunk matches the left pattern, and the beginning of the next chunk matches the right pattern. To merge two chunks where the first ends with a noun and the second begins with a noun, you would use <code class="literal">&lt;NN.*&gt;{}&lt;NN.*&gt;</code>.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note19"/>Note</h3><p>The order of rules is very important and re-ordering can affect the results. The <code class="literal">RegexpParser</code> applies the rules one at a time from top to bottom, so each rule will be applied to the <code class="literal">ChunkString</code> resulting from the previous rule.</p></div></div><p>Here<a id="id360" class="indexterm"/>
<a id="id361" class="indexterm"/>'s an example of splitting and merging, starting with the sentence tree as shown next:</p><div class="mediaobject"><img src="graphics/3609_05_05.jpg" alt="How to do it..."/></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">The whole sentence is chunked, as shown in the following diagram:<div class="mediaobject"><img src="graphics/3609_05_06.jpg" alt="How to do it..."/></div></li><li class="listitem">The chunk is split into multiple chunks after every noun, as shown in the following tree:<div class="mediaobject"><img src="graphics/3609_05_07.jpg" alt="How to do it..."/></div></li><li class="listitem">Each chunk with a determiner is split into separate chunks, creating four chunks where there were three:<div class="mediaobject"><img src="graphics/3609_05_08.jpg" alt="How to do it..."/></div></li><li class="listitem">Chunks ending with a noun are merged with the next chunk if it begins with a noun, reducing the four chunks back down to three, as shown in the following diagram:<div class="mediaobject"><img src="graphics/3609_05_09.jpg" alt="How to do it..."/></div></li></ol></div><p>Using the<a id="id362" class="indexterm"/>
<a id="id363" class="indexterm"/> <code class="literal">RegexpParser</code>, the code looks like this:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; chunker = RegexpParser(r'''
... NP:
...     {&lt;DT&gt;&lt;.*&gt;*&lt;NN.*&gt;}
...     &lt;NN.*&gt;}{&lt;.*&gt;
...     &lt;.*&gt;}{&lt;DT&gt;
...     &lt;NN.*&gt;{}&lt;NN.*&gt;
... ''')
&gt;&gt;&gt; sent = [('the', 'DT'), ('sushi', 'NN'), ('roll', 'NN'), ('was', 'VBD'), ('filled', 'VBN'), ('with', 'IN'), ('the', 'DT'), ('fish', 'NN')]
&gt;&gt;&gt; chunker.parse(sent)
Tree('S', [Tree('NP', [('the', 'DT'), ('sushi', 'NN'), ('roll', 'NN')]), Tree('NP', [('was', 'VBD'), ('filled', 'VBN'), ('with', 'IN')]), Tree('NP', [('the', 'DT'), ('fish', 'NN')])])</pre></div><p>And the final tree of <code class="literal">NP</code> chunks is shown in the following diagram:</p><div class="mediaobject"><img src="graphics/3609_05_10.jpg" alt="How to do it..."/></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec179"/>How it works...</h2></div></div></div><p>The <code class="literal">MergeRule</code><a id="id364" class="indexterm"/>
<a id="id365" class="indexterm"/>
<a id="id366" class="indexterm"/>
<a id="id367" class="indexterm"/> and <code class="literal">SplitRule</code> classes take three arguments: the left pattern, right pattern, and a description. The <code class="literal">RegexpParser</code> takes care of splitting the original patterns on the curly braces to get the left and right sides, but you can also create these manually. Here's a step-by-step walkthrough of how the original sentence is modified by applying each rule:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from nltk.chunk.regexp import MergeRule, SplitRule
&gt;&gt;&gt; cs = ChunkString(Tree('S', sent))
&gt;&gt;&gt; cs
&lt;ChunkString: '&lt;DT&gt;&lt;NN&gt;&lt;NN&gt;&lt;VBD&gt;&lt;VBN&gt;&lt;IN&gt;&lt;DT&gt;&lt;NN&gt;'&gt;
&gt;&gt;&gt; ur = ChunkRule('&lt;DT&gt;&lt;.*&gt;*&lt;NN.*&gt;', 'chunk determiner to noun')
&gt;&gt;&gt; ur.apply(cs)
&gt;&gt;&gt; cs
&lt;ChunkString: '{&lt;DT&gt;&lt;NN&gt;&lt;NN&gt;&lt;VBD&gt;&lt;VBN&gt;&lt;IN&gt;&lt;DT&gt;&lt;NN&gt;}'&gt;
&gt;&gt;&gt; sr1 = SplitRule('&lt;NN.*&gt;', '&lt;.*&gt;', 'split after noun')
&gt;&gt;&gt; sr1.apply(cs)
&gt;&gt;&gt; cs
&lt;ChunkString: '{&lt;DT&gt;&lt;NN&gt;}{&lt;NN&gt;}{&lt;VBD&gt;&lt;VBN&gt;&lt;IN&gt;&lt;DT&gt;&lt;NN&gt;}'&gt;
&gt;&gt;&gt; sr2 = SplitRule('&lt;.*&gt;', '&lt;DT&gt;', 'split before determiner')
&gt;&gt;&gt; sr2.apply(cs)
&gt;&gt;&gt; cs
&lt;ChunkString: '{&lt;DT&gt;&lt;NN&gt;}{&lt;NN&gt;}{&lt;VBD&gt;&lt;VBN&gt;&lt;IN&gt;}{&lt;DT&gt;&lt;NN&gt;}'&gt;
&gt;&gt;&gt; mr = MergeRule('&lt;NN.*&gt;', '&lt;NN.*&gt;', 'merge nouns')
&gt;&gt;&gt; mr.apply(cs)
&gt;&gt;&gt; cs
&lt;ChunkString: '{&lt;DT&gt;&lt;NN&gt;&lt;NN&gt;}{&lt;VBD&gt;&lt;VBN&gt;&lt;IN&gt;}{&lt;DT&gt;&lt;NN&gt;}'&gt;
&gt;&gt;&gt; cs.to_chunkstruct()
Tree('S', [Tree('CHUNK', [('the', 'DT'), ('sushi', 'NN'), ('roll', 'NN')]), Tree('CHUNK', [('was', 'VBD'), ('filled', 'VBN'), ('with', 'IN')]), Tree('CHUNK', [('the', 'DT'), ('fish', 'NN')])])</pre></div></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec180"/>There's more...</h2></div></div></div><p>The parsing of the rules and splitting of left and right patterns is done in the static <code class="literal">parse()</code> me<a id="id368" class="indexterm"/>thod of the <code class="literal">RegexpChunkRule</code> superclass. This is called by the <code class="literal">RegexpParser</code> to get the list of rules to pass in to the <code class="literal">RegexpChunkParser</code>. Here are some examples of parsing the patterns used before:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from nltk.chunk.regexp import RegexpChunkRule
&gt;&gt;&gt; RegexpChunkRule.parse('{&lt;DT&gt;&lt;.*&gt;*&lt;NN.*&gt;}')
&lt;ChunkRule: '&lt;DT&gt;&lt;.*&gt;*&lt;NN.*&gt;'&gt;
&gt;&gt;&gt; RegexpChunkRule.parse('&lt;.*&gt;}{&lt;DT&gt;')
&lt;SplitRule: '&lt;.*&gt;', '&lt;DT&gt;'&gt;
&gt;&gt;&gt; RegexpChunkRule.parse('&lt;NN.*&gt;{}&lt;NN.*&gt;')
&lt;MergeRule: '&lt;NN.*&gt;', '&lt;NN.*&gt;'&gt;</pre></div><div class="section" title="Rule descriptions"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec59"/>Rule descriptions</h3></div></div></div><p>Descriptions for each rule can be specified with a comment string after the rule (a comment string must start with <code class="literal">#</code>). If no comment string is found, the rule's description will be empty. Here's an example:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; RegexpChunkRule.parse('{&lt;DT&gt;&lt;.*&gt;*&lt;NN.*&gt;} # chunk everything').descr()
'chunk everything'
&gt;&gt;&gt; RegexpChunkRule.parse('{&lt;DT&gt;&lt;.*&gt;*&lt;NN.*&gt;}').descr()
''</pre></div><p>Comment string descriptions can also be used within grammar strings that are passed to <code class="literal">RegexpParser</code>.</p></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec181"/>See also</h2></div></div></div><p>The previous recipe goes over how to use <code class="literal">ChunkRule</code> and how rules are passed in to <code class="literal">RegexpChunkParser</code>.</p></div></div>
<div class="section" title="Expanding and removing chunks with regular expressions"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec52"/>Expanding and removing chunks with regular expressions</h1></div></div></div><p>There are <a id="id369" class="indexterm"/>
<a id="id370" class="indexterm"/>three <code class="literal">RegexpChunkRule</code> subclasses that are not supported by <code class="literal">RegexpChunkRule.parse()</code> and therefore must be created manually if you want to use them. These rules are:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem"><code class="literal">ExpandLeftRule</code>: Adds unchu<a id="id371" class="indexterm"/>nked (chink) words to the left of a chunk to the chunk.</li><li class="listitem"><code class="literal">ExpandRightRule</code>: Adds unchun<a id="id372" class="indexterm"/>ked (chink) words to the right of a chunk to the chunk.</li><li class="listitem"><code class="literal">UnChunkRule</code>: Unchunk a<a id="id373" class="indexterm"/>ny matching chunk.</li></ol></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec182"/>How to do it...</h2></div></div></div><p>
<code class="literal">ExpandLeft</code>
<a id="id374" class="indexterm"/>
<a id="id375" class="indexterm"/>
<code class="literal">Rule</code> and <code class="literal">ExpandRightRule</code> both take two patterns along with a description as arguments. For <code class="literal">ExpandLeftRule</code>, the first pattern is the chink we want to add to the beginning of the chunk, while the right pattern will match the beginning of the chunk we want to expand. With <code class="literal">ExpandRightRule</code>, the left pattern should match the end of the chunk we want to expand, and the right pattern matches the chink we want to add to the end of the chunk. The idea is similar to the <code class="literal">MergeRule</code>, but in this case we are merging chink words instead of other chunks.</p><p>
<code class="literal">UnChunkRule</code> is the opposite of <code class="literal">ChunkRule</code>. Any chunk that exactly matches the <code class="literal">UnChunkRule</code> pattern will be unchunked, and become a chink. Here's some code demonstrating usage with the <code class="literal">RegexpChunkParser</code>:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from nltk.chunk.regexp import ChunkRule, ExpandLeftRule, ExpandRightRule, UnChunkRule
&gt;&gt;&gt; from nltk.chunk import RegexpChunkParser
&gt;&gt;&gt; ur = ChunkRule('&lt;NN&gt;', 'single noun')
&gt;&gt;&gt; el = ExpandLeftRule('&lt;DT&gt;', '&lt;NN&gt;', 'get left determiner')
&gt;&gt;&gt; er = ExpandRightRule('&lt;NN&gt;', '&lt;NNS&gt;', 'get right plural noun')
&gt;&gt;&gt; un = UnChunkRule('&lt;DT&gt;&lt;NN.*&gt;*', 'unchunk everything')
&gt;&gt;&gt; chunker = RegexpChunkParser([ur, el, er, un])
&gt;&gt;&gt; sent = [('the', 'DT'), ('sushi', 'NN'), ('rolls', 'NNS')]
&gt;&gt;&gt; chunker.parse(sent)
Tree('S', [('the', 'DT'), ('sushi', 'NN'), ('rolls', 'NNS')])</pre></div><p>You will notice the end result is a flat sentence, which is exactly what we started with. That's because the final <code class="literal">UnChunkRule</code> undid the chunk created by the previous rules. Read on to see the step-by-step procedure of what happened.</p></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec183"/>How it works...</h2></div></div></div><p>The preceding rules were applied in the following order, starting with the sentence tree shown below:</p><div class="mediaobject"><img src="graphics/3609_05_11.jpg" alt="How it works..."/></div><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Make single nouns into a chunk, as shown in the following diagram:<div class="mediaobject"><img src="graphics/3609_05_12.jpg" alt="How it works..."/></div></li><li class="listitem">Expand left determiners into chunks that begin with a noun, as shown in the following diagram:<div class="mediaobject"><img src="graphics/3609_05_13.jpg" alt="How it works..."/></div></li><li class="listitem">Expand right <a id="id376" class="indexterm"/><a id="id377" class="indexterm"/>plural nouns into chunks that end with a noun, chunking the whole sentence as shown in the following diagram:<div class="mediaobject"><img src="graphics/3609_05_14.jpg" alt="How it works..."/></div></li><li class="listitem">Unchunk every chunk that is a determiner + noun + plural noun, resulting in the original sentence tree, as shown in the following diagram:<div class="mediaobject"><img src="graphics/3609_05_15.jpg" alt="How it works..."/></div></li></ol></div><p>Here's the code showing each step:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from nltk.chunk.regexp import ChunkString
&gt;&gt;&gt; from nltk.tree import Tree
&gt;&gt;&gt; cs = ChunkString(Tree('S', sent))
&gt;&gt;&gt; cs
&lt;ChunkString: '&lt;DT&gt;&lt;NN&gt;&lt;NNS&gt;'&gt;
&gt;&gt;&gt; ur.apply(cs)
&gt;&gt;&gt; cs
&lt;ChunkString: '&lt;DT&gt;{&lt;NN&gt;}&lt;NNS&gt;'&gt;
&gt;&gt;&gt; el.apply(cs)
&gt;&gt;&gt; cs
&lt;ChunkString: '{&lt;DT&gt;&lt;NN&gt;}&lt;NNS&gt;'&gt;
&gt;&gt;&gt; er.apply(cs)
&gt;&gt;&gt; cs
&lt;ChunkString: '{&lt;DT&gt;&lt;NN&gt;&lt;NNS&gt;}'&gt;
&gt;&gt;&gt; un.apply(cs)
&gt;&gt;&gt; cs
&lt;ChunkString: '&lt;DT&gt;&lt;NN&gt;&lt;NNS&gt;'&gt;</pre></div></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec184"/>There's more...</h2></div></div></div><p>I<a id="id378" class="indexterm"/>
<a id="id379" class="indexterm"/>n practice, you can probably get away with only using the previous four rules: <code class="literal">ChunkRule</code>, <code class="literal">ChinkRule</code>, <code class="literal">MergeRule</code>, and <code class="literal">SplitRule</code>. But if you do need very fine-grained control over chunk parsing and removing, now you know how to do it with the expansion and unchunk rules.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec185"/>See also</h2></div></div></div><p>The previous two recipes covered the more common chunk rules that are supported by <code class="literal">RegexpChunkRule.parse()</code> and <code class="literal">RegexpParser</code>.</p></div></div>
<div class="section" title="Partial parsing with regular expressions"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec53"/>Partial parsing with regular expressions</h1></div></div></div><p>S<a id="id380" class="indexterm"/>
<a id="id381" class="indexterm"/>o far, we have only been parsing noun-phrases. But <code class="literal">RegexpParser</code> supports grammar with multiple phrase types, such as <span class="emphasis"><em>verb-phrases</em></span> and <span class="emphasis"><em>prepositional-phrases</em></span>. We can put the rules we have learned to use and define a grammar that can be evaluated against the <code class="literal">conll2000</code> corpus, which has <code class="literal">NP</code>, <code class="literal">VP</code>, and <code class="literal">PP</code> phrases.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec186"/>How to do it...</h2></div></div></div><p>We will define a grammar to parse three phrase types. For noun-phrases, we have a <code class="literal">ChunkRule</code> that looks for an optional determiner followed by one or more nouns. We then have a <code class="literal">MergeRule</code> for adding an adjective to the front of a noun chunk. For prepositional-phrases, we simply chunk any <code class="literal">IN</code> word, such as "in" or "on". For verb-phrases, we chunk an optional modal word (such as "should") followed by a verb.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note20"/>Note</h3><p>Each grammar rule is followed by a <code class="literal">#</code> comment. This comment is passed in to each rule as the description. Comments are optional, but they can be helpful notes for understanding what the rule does, and will be included in trace output.</p></div></div><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; chunker = RegexpParser(r'''
... NP:
... {&lt;DT&gt;?&lt;NN.*&gt;+}  # chunk optional determiner with nouns
... &lt;JJ&gt;{}&lt;NN.*&gt;  # merge adjective with noun chunk
... PP:
... {&lt;IN&gt;}  # chunk preposition
... VP:
... {&lt;MD&gt;?&lt;VB.*&gt;}  # chunk optional modal with verb
... ''')
&gt;&gt;&gt; from nltk.corpus import conll2000
&gt;&gt;&gt; score = chunker.evaluate(conll2000.chunked_sents())
&gt;&gt;&gt; score.accuracy()
0.61485735457576884</pre></div><p>When we call <code class="literal">evaluate()</code> on the <code class="literal">chunker</code>, we give it a list of chunked sentences and get back a <code class="literal">ChunkScore</code> object, which can give us the accuracy of the <code class="literal">chunker</code>, along with a number of other metrics.</p></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec187"/>How it works...</h2></div></div></div><p>T<a id="id382" class="indexterm"/>
<a id="id383" class="indexterm"/>he <code class="literal">RegexpParser</code> parses the grammar string into sets of rules, one set of rules for each phrase type. These rules are used to create a <code class="literal">RegexpChunkParser</code>. The rules are parsed using <code class="literal">RegexpChunkRule.parse()</code>, which returns one of the five subclasses: <code class="literal">ChunkRule</code>, <code class="literal">ChinkRule</code>, <code class="literal">MergeRule</code>, <code class="literal">SplitRule</code>, or <code class="literal">ChunkRuleWithContext</code>.</p><p>Now that the grammar has been translated into sets of rules, these rules are used to parse a tagged sentence into a <code class="literal">Tree</code> structure. <code class="literal">RegexpParser</code> inherits from <code class="literal">ChunkParserI</code>, which provides a <code class="literal">parse()</code> method to parse the tagged words. Whenever a part of the tagged tokens match a chunk rule, a subtree is constructed so that the tagged tokens become the leaves of a <code class="literal">Tree</code> whose node string is the chunk tag. <code class="literal">ChunkParserI</code> also provides the <code class="literal">evaluate()</code> method, which compares the given chunked sentences to the output of the <code class="literal">parse()</code> method to construct and return a <code class="literal">ChunkScore</code> object.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec188"/>There's more...</h2></div></div></div><p>Y<a id="id384" class="indexterm"/>ou can also evaluate this <code class="literal">chunker</code> on the <code class="literal">treebank_chunk</code> corpus.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from nltk.corpus import treebank_chunk
&gt;&gt;&gt; treebank_score = chunker.evaluate(treebank_chunk.chunked_sents())
&gt;&gt;&gt; treebank_score.accuracy()
0.49033970276008493</pre></div><p>The <code class="literal">treebank_chunk</code> corpus is a special version of the <code class="literal">treebank</code> corpus that provides a <code class="literal">chunked_sents()</code> method. The regular <code class="literal">treebank</code> corpus cannot provide that method due to its file format.</p><div class="section" title="ChunkScore metrics"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec60"/>ChunkScore metrics</h3></div></div></div><p>
<code class="literal">ChunkScore</code><a id="id385" class="indexterm"/> provides a few other metrics besides accuracy. Of the chunks the <code class="literal">chunker</code> was able to guess, precision tells you how many were correct. Recall tells you how well the <code class="literal">chunker</code> did at finding correct chunks, compared to how many total chunks there were.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; score.precision()
0.60201948127375005
&gt;&gt;&gt; score.recall()
0.60607250250584699</pre></div><p>You can also get lists of chunks that were missed by the <code class="literal">chunker</code>, chunks that were incorrectly found, correct chunks, and guessed chunks. These can be useful to figure out how to improve your chunk grammar.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; len(score.missed())
47161
&gt;&gt;&gt; len(score.incorrect())
47967
&gt;&gt;&gt; len(score.correct())
119720
&gt;&gt;&gt; len(score.guessed())
120526</pre></div><p>As you can see by the number of incorrect chunks, and by comparing <code class="literal">guessed()</code> and <code class="literal">correct()</code>, our chunker guessed that there were more chunks that actually existed. And it also missed a good number of correct chunks.</p></div><div class="section" title="Looping and tracing"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec61"/>Looping and tracing</h3></div></div></div><p>I<a id="id386" class="indexterm"/>f you want to apply the chunk rules in your grammar more than once, you pass <code class="literal">loop=2</code> into <code class="literal">RegexpParser</code> at initialization. The default is <code class="literal">loop=1</code>.</p><p>T<a id="id387" class="indexterm"/>o watch an internal trace of the chunking process, pass <code class="literal">trace=1</code> into <code class="literal">RegexpParser</code>. To get even more output, pass in <code class="literal">trace=2</code>. This will give you a printout of what the chunker is doing as it is doing it. Rule comments/descriptions will be included in the trace output, giving you a good idea of which rule is applied when.</p></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec189"/>See also</h2></div></div></div><p>If coming up with regular expression chunk patterns seems like too much work, then read the next recipes where we will cover how to train a chunker based on a corpus of chunked sentences.</p></div></div>
<div class="section" title="Training a tagger-based chunker"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec54"/>Training a tagger-based chunker</h1></div></div></div><p>T<a id="id388" class="indexterm"/>raining a chunker can be a great alternative to manually specifying regular expression chunk patterns. Instead of a painstaking process of trial and error to get the exact right patterns, we can use existing corpus data to train chunkers much like we did in <a class="link" href="ch04.html" title="Chapter 4. Part-of-Speech Tagging">Chapter 4</a>, <span class="emphasis"><em>Part-of-Speech Tagging</em></span>.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec190"/>How to do it...</h2></div></div></div><p>As with the part-of-speech tagging, we will use the treebank corpus data for training. But this time we will use the <code class="literal">treebank_chunk</code> corpus, which is specifically formatted to produce chunked sentences in the form of trees. These <code class="literal">chunked_sents()</code> will be used by a <code class="literal">TagChunker</code> class to train a tagger-based chunker. The <code class="literal">TagChunker</code> uses a helper function <code class="literal">conll_tag_chunks()</code> to extract a list of <code class="literal">(pos, iob)</code> tuples from a list of <code class="literal">Tree</code>. These <code class="literal">(pos, iob)</code> tuples are then used to train a tagger in the same way <code class="literal">(word, pos)</code> tuples were used in <a class="link" href="ch04.html" title="Chapter 4. Part-of-Speech Tagging">Chapter 4</a>, <span class="emphasis"><em>Part-of-Speech Tagging</em></span> to train part-of-speech taggers. But instead of learning part-of-speech tags for words, we are learning IOB tags for part-of-speech tags. Here's the code from <code class="literal">chunkers.py</code>:</p><div class="informalexample"><pre class="programlisting">import nltk.chunk, itertools
from nltk.tag import UnigramTagger, BigramTagger
from tag_util import backoff_tagger

def conll_tag_chunks(chunk_sents):
  tagged_sents = [nltk.chunk.tree2conlltags(tree) for tree in chunk_sents]
  return [[(t, c) for (w, t, c) in sent] for sent in tagged_sents]

class TagChunker(nltk.chunk.ChunkParserI):
  def __init__(self, train_chunks, tagger_classes=[UnigramTagger, BigramTagger]):
    train_sents = conll_tag_chunks(train_chunks)
    self.tagger = backoff_tagger(train_sents, tagger_classes)

  def parse(self, tagged_sent):
    if not tagged_sent: return None
    (words, tags) = zip(*tagged_sent)
    chunks = self.tagger.tag(tags)
    wtc = itertools.izip(words, chunks)
    return nltk.chunk.conlltags2tree([(w,t,c) for (w,(t,c)) in wtc])</pre></div><p>Once we have our trained <code class="literal">TagChunker</code>, we can then evaluate the <code class="literal">ChunkScore</code> the same way we did for the <code class="literal">RegexpParser</code> in the previous recipes.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from chunkers import TagChunker
&gt;&gt;&gt; from nltk.corpus import treebank_chunk
&gt;&gt;&gt; train_chunks = treebank_chunk.chunked_sents()[:3000]
&gt;&gt;&gt; test_chunks = treebank_chunk.chunked_sents()[3000:]
&gt;&gt;&gt; chunker = TagChunker(train_chunks)
&gt;&gt;&gt; score = chunker.evaluate(test_chunks)
&gt;&gt;&gt; score.accuracy()
0.97320393352514278
&gt;&gt;&gt; score.precision()
0.91665343705350055
&gt;&gt;&gt; score.recall()
0.9465573770491803</pre></div><p>Pretty darn accurate! Training a chunker is clearly a great alternative to manually specified grammars and regular expressions.</p></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec191"/>How it works...</h2></div></div></div><p>R<a id="id389" class="indexterm"/>ecall from the <span class="emphasis"><em>Creating a chunked phrase corpus</em></span> recipe in <a class="link" href="ch03.html" title="Chapter 3. Creating Custom Corpora">Chapter 3</a>, <span class="emphasis"><em>Creating Custom Corpora</em></span> that the <code class="literal">conll2000</code> corpus defines chunks using IOB tags, which specify the type of chunk and where it begins and ends. We can train a part-of-speech tagger on these IOB tag patterns, and then use that to power a <code class="literal">ChunkerI</code> subclass. But first we need to transform a <code class="literal">Tree</code> that you would get from the <code class="literal">chunked_sents()</code> method of a corpus into a format usable by a part-of-speech tagger. This is what <code class="literal">conll_tag_chunks()</code> does. It uses <code class="literal">nltk.chunk.tree2conlltags()</code> to convert a sentence <code class="literal">Tree</code> into a list of 3-tuples of the form <code class="literal">(word, pos, iob)</code> where <code class="literal">pos</code> is the part-of-speech tag and <code class="literal">iob</code> is an IOB tag, such as <code class="literal">B-NP</code> to mark the beginning of a noun-phrase, or <code class="literal">I-NP</code> to mark that the word is inside the noun-phrase. The reverse of this method is <code class="literal">nltk.chunk.conlltags2tree()</code>. Here's some code to demonstrate these <code class="literal">nltk.chunk</code> functions:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; import nltk.chunk
&gt;&gt;&gt; from nltk.tree import Tree
&gt;&gt;&gt; t = Tree('S', [Tree('NP', [('the', 'DT'), ('book', 'NN')])])
&gt;&gt;&gt; nltk.chunk.tree2conlltags(t)
[('the', 'DT', 'B-NP'), ('book', 'NN', 'I-NP')]
&gt;&gt;&gt; nltk.chunk.conlltags2tree([('the', 'DT', 'B-NP'), ('book', 'NN', 'I-NP')])
Tree('S', [Tree('NP', [('the', 'DT'), ('book', 'NN')])])</pre></div><p>The next step is to convert these 3-tuples into 2-tuples that the tagger can recognize. Because the <code class="literal">RegexpParser</code> uses part-of-speech tags for chunk patterns, we will do that here too and use part-of-speech tags as if they were words to tag. By simply dropping the <code class="literal">word</code> from 3-tuple <code class="literal">(word, pos, iob)</code>, the <code class="literal">conll_tag_chunks()</code> function returns a list of 2-tuples of the form <code class="literal">(pos, iob)</code>. When given the preceding example <code class="literal">Tree</code> in a list, the results are in a format we can feed to a tagger.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; conll_tag_chunks([t])
[[('DT', 'B-NP'), ('NN', 'I-NP')]]</pre></div><p>The final step is a subclass of <code class="literal">ChunkParserI</code> called <code class="literal">TagChunker</code>. It trains on a list of chunk trees using an internal tagger. This internal tagger is composed of a <code class="literal">UnigramTagger</code> and a <code class="literal">BigramTagger</code> in a backoff chain, using the <code class="literal">backoff_tagger()</code> method created in the <span class="emphasis"><em>Training and combining Ngram taggers</em></span> recipe in <a class="link" href="ch04.html" title="Chapter 4. Part-of-Speech Tagging">Chapter 4</a>, <span class="emphasis"><em>Part-of-Speech Tagging</em></span>.</p><p>Finally, <code class="literal">ChunkerI</code> subclasses must implement a <code class="literal">parse()</code> method that expects a part-of-speech tagged sentence. We unzip that sentence into a list of words and part-of-speech tags. The tags are then tagged by the tagger to get IOB tags, which are then re-combined with the words and part-of-speech tags to create 3-tuples we can pass to <code class="literal">nltk.chunk.conlltags2tree()</code> to return a final <code class="literal">Tree</code>.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec192"/>There's more...</h2></div></div></div><p>S<a id="id390" class="indexterm"/>ince we have been talking about the <code class="literal">conll</code> IOB tags, let us see how the <code class="literal">TagChunker</code> does on the <code class="literal">conll2000</code> corpus:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from nltk.corpus import conll2000
&gt;&gt;&gt; conll_train = conll2000.chunked_sents('train.txt')
&gt;&gt;&gt; conll_test = conll2000.chunked_sents('test.txt')
&gt;&gt;&gt; chunker = TagChunker(conll_train)
&gt;&gt;&gt; score = chunker.evaluate(conll_test)
&gt;&gt;&gt; score.accuracy()
0.89505456234037617
&gt;&gt;&gt; score.precision()
0.81148419743556754
&gt;&gt;&gt; score.recall()
0.86441916769448635</pre></div><p>Not quite as good as on <code class="literal">treebank_chunk</code>, but <code class="literal">conll2000</code> is a much larger corpus, so it's not too surprising.</p><div class="section" title="Using different taggers"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec62"/>Using different taggers</h3></div></div></div><p>If you want to use different tagger classes with the <code class="literal">TagChunker</code>, you can pass them in as <code class="literal">tagger_classes</code>. For example, here's the <code class="literal">TagChunker</code> using just a <code class="literal">UnigramTagger</code>:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from nltk.tag import UnigramTagger
&gt;&gt;&gt; uni_chunker = TagChunker(train_chunks, tagger_classes=[UnigramTagger])
&gt;&gt;&gt; score = uni_chunker.evaluate(test_chunks)
&gt;&gt;&gt; score.accuracy()
0.96749259243354657</pre></div><p>The <code class="literal">tagger_classes</code> will be passed directly into the <code class="literal">backoff_tagger()</code> function, which means they must be subclasses of <code class="literal">SequentialBackoffTagger</code>. In testing, the default of <code class="literal">tagger_classes=[UnigramTagger, BigramTagger]</code> produces the best results.</p></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec193"/>See also</h2></div></div></div><p>The <span class="emphasis"><em>Training and combining Ngram taggers</em></span> recipe in <a class="link" href="ch04.html" title="Chapter 4. Part-of-Speech Tagging">Chapter 4</a>, <span class="emphasis"><em>Part-of-Speech Tagging</em></span> covers backoff tagging with a <code class="literal">UnigramTagger</code> and <code class="literal">BigramTagger</code>. <code class="literal">ChunkScore</code> metrics returned by the <code class="literal">evaluate()</code> method of a chunker were explained in the previous recipe.</p></div></div>
<div class="section" title="Classification-based chunking"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec55"/>Classification-based chunking</h1></div></div></div><p>U<a id="id391" class="indexterm"/>
<a id="id392" class="indexterm"/>
<a id="id393" class="indexterm"/>nlike most part-of-speech taggers, the <code class="literal">ClassifierBasedTagger</code> learns from features. That means we can create a <code class="literal">ClassifierChunker</code> that can learn from both the words and part-of-speech tags, instead of only the part-of-speech tags as the <code class="literal">TagChunker</code> does.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec194"/>How to do it...</h2></div></div></div><p>For the <code class="literal">ClassifierChunker</code>, we don't want to discard the words from the training sentences, as we did in the previous recipe. Instead, to remain compatible with the 2-tuple <code class="literal">(word, pos)</code> format required for training a <code class="literal">ClassiferBasedTagger</code>, we convert the <code class="literal">(word, pos, iob)</code> 3-tuples from <code class="literal">nltk.chunk.tree2conlltags()</code> into <code class="literal">((word, pos), iob)</code> 2-tuples using the <code class="literal">chunk_trees2train_chunks()</code> function. This code can be found in <code class="literal">chunkers.py</code>:</p><div class="informalexample"><pre class="programlisting">import nltk.chunk
from nltk.tag import ClassifierBasedTagger

def chunk_trees2train_chunks(chunk_sents):
  tag_sents = [nltk.chunk.tree2conlltags(sent) for sent in chunk_sents]
  return [[((w,t),c) for (w,t,c) in sent] for sent in tag_sents]</pre></div><p>Next, we need a feature detector function to pass into <code class="literal">ClassifierBasedTagger</code>. Our default feature detector function, <code class="literal">prev_next_pos_iob()</code>, knows that the list of <code class="literal">tokens</code> is really a list of <code class="literal">(word, pos)</code> tuples, and can use that to return a feature set suitable for a classifier. To give the classifier as much information as we can, this feature set contains the current, previous and next word, and part-of-speech tag, along with the previous IOB tag.</p><div class="informalexample"><pre class="programlisting">def prev_next_pos_iob(tokens, index, history):
  word, pos = tokens[index]

  if index == 0:
    prevword, prevpos, previob = ('&lt;START&gt;',)*3
  else:
    prevword, prevpos = tokens[index-1]
    previob = history[index-1]

  if index == len(tokens) - 1:
    nextword, nextpos = ('&lt;END&gt;',)*2
  else:
    nextword, nextpos = tokens[index+1]

  feats = {
    'word': word,
    'pos': pos,
    'nextword': nextword,
    'nextpos': nextpos,
    'prevword': prevword,
    'prevpos': prevpos,
    'previob': previob
  }
  return feats</pre></div><p>N<a id="id394" class="indexterm"/>
<a id="id395" class="indexterm"/>
<a id="id396" class="indexterm"/>ow we can define the <code class="literal">ClassifierChunker</code>, which uses an internal <code class="literal">ClassifierBasedTagger</code> with features extracted using <code class="literal">prev_next_pos_iob()</code>, and training sentences from <code class="literal">chunk_trees2train_chunks()</code>. As a subclass of <code class="literal">ChunkerParserI</code>, it implements the <code class="literal">parse()</code> method, which converts the <code class="literal">((w, t), c)</code> tuples produced by the internal tagger into a <code class="literal">Tree</code> using <code class="literal">nltk.chunk.conlltags2tree()</code>.</p><div class="informalexample"><pre class="programlisting">class ClassifierChunker(nltk.chunk.ChunkParserI):
  def __init__(self, train_sents, feature_detector=prev_next_pos_iob, **kwargs):
    if not feature_detector:
      feature_detector = self.feature_detector

    train_chunks = chunk_trees2train_chunks(train_sents)
    self.tagger = ClassifierBasedTagger(train=train_chunks,
      feature_detector=feature_detector, **kwargs)

  def parse(self, tagged_sent):
    if not tagged_sent: return None
    chunks = self.tagger.tag(tagged_sent)
    return nltk.chunk.conlltags2tree([(w,t,c) for ((w,t),c) in chunks])</pre></div><p>Using the same <code class="literal">train_chunks</code> and <code class="literal">test_chunks</code> from the <code class="literal">treebank_chunk</code> corpus in the previous recipe, we can evaluate this code from <code class="literal">chunkers.py</code>:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from chunkers import ClassifierChunker
&gt;&gt;&gt; chunker = ClassifierChunker(train_chunks)
&gt;&gt;&gt; score = chunker.evaluate(test_chunks)
&gt;&gt;&gt; score.accuracy()
0.97217331558380216
&gt;&gt;&gt; score.precision()
0.92588387933830685
&gt;&gt;&gt; score.recall()
0.93590163934426229</pre></div><p>C<a id="id397" class="indexterm"/>
<a id="id398" class="indexterm"/>
<a id="id399" class="indexterm"/>ompared to the <code class="literal">TagChunker</code>, all the scores have gone up a bit. Let us see how it does on <code class="literal">conll2000</code>:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; chunker = ClassifierChunker(conll_train)
&gt;&gt;&gt; score = chunker.evaluate(conll_test)
&gt;&gt;&gt; score.accuracy()
0.92646220740021534
&gt;&gt;&gt; score.precision()
0.87379243109102189
&gt;&gt;&gt; score.recall()
0.90073546206203459</pre></div><p>This is much improved over the <code class="literal">TagChunker</code>.</p></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec195"/>How it works...</h2></div></div></div><p>Like the <code class="literal">TagChunker</code> in the previous recipe, we are training a part-of-speech tagger for IOB tagging. But in this case, we want to include the word as a feature to power a classifier. By creating nested 2-tuples of the form <code class="literal">((word, pos), iob)</code>, we can pass the word through the tagger into our feature detector function. <code class="literal">chunk_trees2train_chunks()</code> produces these nested 2-tuples, and <code class="literal">prev_next_pos_iob()</code> is aware of them and uses each element as a feature. The following features are extracted:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The current word and part-of-speech tag</li><li class="listitem" style="list-style-type: disc">The previous word, part-of-speech tag, and IOB tag</li><li class="listitem" style="list-style-type: disc">The next word and part-of-speech tag</li></ul></div><p>The arguments to <code class="literal">prev_next_pos_iob()</code> look the same as the <code class="literal">feature_detector()</code> method of the <code class="literal">ClassifierBasedTagger</code>: <code class="literal">tokens</code>, <code class="literal">index</code>, and <code class="literal">history</code>. But this time, <code class="literal">tokens</code> will be a list of <code class="literal">(word, pos)</code> 2-tuples, and <code class="literal">history</code> will be a list of IOB tags. The special feature values <code class="literal">'&lt;START&gt;'</code> and <code class="literal">'&lt;END&gt;'</code> are used if there are no previous or next tokens.</p><p>T<a id="id400" class="indexterm"/>
<a id="id401" class="indexterm"/>
<a id="id402" class="indexterm"/>he <code class="literal">ClassifierChunker</code> uses an internal <code class="literal">ClassifierBasedTagger</code> and <code class="literal">prev_next_pos_iob()</code> as its default <code class="literal">feature_detector</code>. The results from the tagger, which are in the same nested 2-tuple form, are then reformatted into 3-tuples to return a final <code class="literal">Tree</code> using <code class="literal">nltk.chunk.conlltags2tree()</code>.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec196"/>There's more...</h2></div></div></div><p>You can use your own feature detector function by passing it in to the <code class="literal">ClassifierChunker</code> as <code class="literal">feature_detector</code>. The <code class="literal">tokens</code> will contain a list of <code class="literal">(word, tag)</code> tuples, and <code class="literal">history</code> will be a list of the previous IOB tags found.</p><div class="section" title="Using a different classifier builder"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec63"/>Using a different classifier builder</h3></div></div></div><p>The <code class="literal">ClassifierBasedTagger</code> defaults to using <code class="literal">NaiveBayesClassifier.train</code> as its <code class="literal">classifier_builder</code>. But you can use any classifier you want by overriding the <code class="literal">classifier_builder</code> keyword argument. Here's an example using <code class="literal">MaxentClassifier.train</code>:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from nltk.classify import MaxentClassifier
&gt;&gt;&gt; builder = lambda toks: MaxentClassifier.train(toks, trace=0, max_iter=10, min_lldelta=0.01)
&gt;&gt;&gt; me_chunker = ClassifierChunker(train_chunks, classifier_builder=builder)
&gt;&gt;&gt; score = me_chunker.evaluate(test_chunks)
&gt;&gt;&gt; score.accuracy()
0.9748357452655988
&gt;&gt;&gt; score.precision()
0.93794355504208615
&gt;&gt;&gt; score.recall()
0.93163934426229511</pre></div><p>Instead of using <code class="literal">MaxentClassifier.train</code> directly, it has been wrapped in a <code class="literal">lambda</code> so that its output is quiet (<code class="literal">trace=0</code>) and it finishes in a reasonable amount of time. As you can see, the scores are slightly different compared to using the <code class="literal">NaiveBayesClassifier</code>.</p></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec197"/>See also</h2></div></div></div><p>The previous recipe, <span class="emphasis"><em>Training a tagger-based chunker</em></span>, introduced the idea of using a part-of-speech tagger for training a chunker. The <span class="emphasis"><em>Classifier-based tagging</em></span> recipe in <a class="link" href="ch04.html" title="Chapter 4. Part-of-Speech Tagging">Chapter 4</a>, <span class="emphasis"><em>Part-of-Speech Tagging</em></span> describes <code class="literal">ClassifierBasedPOSTagger</code>, which is a subclass of <code class="literal">ClassifierBasedTagger</code>. In <a class="link" href="ch07.html" title="Chapter 7. Text Classification">Chapter 7</a>, <span class="emphasis"><em>Text Classification</em></span>, we will cover classification in detail.</p></div></div>
<div class="section" title="Extracting named entities"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec56"/>Extracting named entities</h1></div></div></div><p>
<span class="strong"><strong>Named entity recognition</strong></span><a id="id403" class="indexterm"/>
<a id="id404" class="indexterm"/> is a specific kind of chunk extraction that uses <span class="strong"><strong>entity tags</strong></span><a id="id405" class="indexterm"/> instead of, or in addition to, chunk tags. Common entity tags include <code class="literal">PERSON</code>, <code class="literal">ORGANIZATION</code>, and <code class="literal">LOCATION</code>. Part-of-speech tagged sentences are parsed into chunk trees as with normal chunking, but the nodes of the trees can be entity tags instead of chunk phrase tags.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec198"/>How to do it...</h2></div></div></div><p>N<a id="id406" class="indexterm"/>LTK comes with a pre-trained named entity chunker. This chunker has been trained on data from the ACE program, a <span class="strong"><strong>NIST</strong></span> (<span class="strong"><strong>National Institute of Standards and Technology</strong></span>)<a id="id407" class="indexterm"/> sponsored program for <span class="strong"><strong>Automatic Content Extraction</strong></span>, which you can read more about here: <a class="ulink" href="http://www.itl.nist.gov/iad/894.01/tests/ace/">http://www.itl.nist.gov/iad/894.01/tests/ace/</a>. Unfortunately, this data is not included in the NLTK corpora, but the trained chunker is. This chunker can be used through the <code class="literal">ne_chunk()</code> method in the <code class="literal">nltk.chunk</code> module. <code class="literal">ne_chunk()</code> will chunk a single sentence into a <code class="literal">Tree</code>. The following is an example using <code class="literal">ne_chunk()</code> on the first tagged sentence of the <code class="literal">treebank_chunk</code> corpus:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from nltk.chunk import ne_chunk
&gt;&gt;&gt; ne_chunk(treebank_chunk.tagged_sents()[0])
Tree('S', [Tree('PERSON', [('Pierre', 'NNP')]), Tree('ORGANIZATION', [('Vinken', 'NNP')]), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')])</pre></div><p>You can see two entity tags are found: <code class="literal">PERSON</code> and <code class="literal">ORGANIZATION</code>. Each of these subtrees contain a list of the words that are recognized as a <code class="literal">PERSON</code> or <code class="literal">ORGANIZATION</code>. To extract these named entities, we can write a simple helper method that will get the leaves of all the subtrees we are interested in.</p><div class="informalexample"><pre class="programlisting">def sub_leaves(tree, node):
  return [t.leaves() for t in tree.subtrees(lambda s: s.node == node)]</pre></div><p>Then we can call this method to get all the <code class="literal">PERSON</code> or <code class="literal">ORGANIZATION</code> leaves from a tree.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; tree = ne_chunk(treebank_chunk.tagged_sents()[0])
&gt;&gt;&gt; from chunkers import sub_leaves
&gt;&gt;&gt; sub_leaves(tree, 'PERSON')
[[('Pierre', 'NNP')]]
&gt;&gt;&gt; sub_leaves(tree, 'ORGANIZATION')
[[('Vinken', 'NNP')]]</pre></div><p>You may notice that the chunker has mistakenly separated "Vinken" into its own <code class="literal">ORGANIZATION Tree</code> instead of including it with the <code class="literal">PERSON Tree</code> containing "Pierre". Such is the case with statistical natural language processing—you can't always expect perfection.</p></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec199"/>How it works...</h2></div></div></div><a id="id408" class="indexterm"/><a id="id409" class="indexterm"/><p>The pre-trained named entity chunker is much like any other chunker, and in fact uses a <code class="literal">MaxentClassifier</code> powered <code class="literal">ClassifierBasedTagger</code> to determine IOB tags. But instead of <code class="literal">B-NP</code> and <code class="literal">I-NP</code> IOB tags, it uses <code class="literal">B-PERSON</code>, <code class="literal">I-PERSON</code>, <code class="literal">B-ORGANIZATION</code>, <code class="literal">I-ORGANIZATION</code>, and more. It also uses the <code class="literal">O</code> tag to mark words that are not part of a named entity (and thus outside the named entity subtrees).</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec200"/>There's more...</h2></div></div></div><p>To process multiple sentences at a time, you can use <code class="literal">batch_ne_chunk()</code>. Here's an example where we process the first 10 sentences from <code class="literal">treebank_chunk.tagged_sents()</code> and get the <code class="literal">ORGANIZATION sub_leaves()</code>:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from nltk.chunk import batch_ne_chunk
&gt;&gt;&gt; trees = batch_ne_chunk(treebank_chunk.tagged_sents()[:10])
&gt;&gt;&gt; [sub_leaves(t, 'ORGANIZATION') for t in trees]
[[[('Vinken', 'NNP')]], [[('Elsevier', 'NNP')]], [[('Consolidated', 'NNP'), ('Gold', 'NNP'), ('Fields', 'NNP')]], [], [], [[('Inc.', 'NNP')], [('Micronite', 'NN')]], [[('New', 'NNP'), ('England', 'NNP'), ('Journal', 'NNP')]], [[('Lorillard', 'NNP')]], [], []]</pre></div><p>You can see there are a couple of multi-word <code class="literal">ORGANIZATION</code> chunks, such as "New England Journal". There are also a few sentences that have no <code class="literal">ORGANIZATION</code> chunks, as indicated by the empty lists <code class="literal">[]</code>.</p><div class="section" title="Binary named entity extraction"><div class="titlepage"><div><div><h3 class="title"><a id="ch05lvl3sec64"/>Binary named entity extraction</h3></div></div></div><a id="id410" class="indexterm"/><p>If you don't care about the particular kind of named entity to extract, you can pass <code class="literal">binary=True</code> into <code class="literal">ne_chunk()</code> or <code class="literal">batch_ne_chunk()</code>. Now, all named entities will be tagged with <code class="literal">NE</code>:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; ne_chunk(treebank_chunk.tagged_sents()[0], binary=True)
Tree('S', [Tree('NE', [('Pierre', 'NNP'), ('Vinken', 'NNP')]), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')])</pre></div><a id="id411" class="indexterm"/><p>If we get the <code class="literal">sub_leaves()</code>, we can see that "Pierre Vinken" is correctly combined into a single named entity.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; sub_leaves(ne_chunk(treebank_chunk.tagged_sents()[0], binary=True), 'NE')
[[('Pierre', 'NNP'), ('Vinken', 'NNP')]]</pre></div></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec201"/>See also</h2></div></div></div><p>In the next recipe, we will create our own simple named entity chunker.</p></div></div>
<div class="section" title="Extracting proper noun chunks"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec57"/>Extracting proper noun chunks</h1></div></div></div><a id="id412" class="indexterm"/><p>A simple way to do named entity extraction is to chunk all proper nouns (tagged with <code class="literal">NNP</code>). We can tag these chunks as <code class="literal">NAME</code>, since the definition of a proper noun is the name of a person, place, or thing.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec202"/>How to do it...</h2></div></div></div><p>Using the <code class="literal">RegexpParser</code>, we can create a very simple grammar that combines all proper nouns into a <code class="literal">NAME</code> chunk. Then we can test this on the first tagged sentence of <code class="literal">treebank_chunk</code> to compare the results to the previous recipe.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; chunker = RegexpParser(r'''
... NAME:
...   {&lt;NNP&gt;+}
... ''')
&gt;&gt;&gt; sub_leaves(chunker.parse(treebank_chunk.tagged_sents()[0]), 'NAME')
[[('Pierre', 'NNP'), ('Vinken', 'NNP')], [('Nov.', 'NNP')]]</pre></div><p>Although we get "Nov." as a <code class="literal">NAME</code> chunk, this isn't a wrong result, as "Nov." is the name of a month.</p></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec203"/>How it works...</h2></div></div></div><a id="id413" class="indexterm"/><p>The <code class="literal">NAME</code> chunker is a simple usage of the <code class="literal">RegexpParser</code>, covered in <span class="emphasis"><em>Chunking and chinking with regular expressions</em></span>, <span class="emphasis"><em>Merging and splitting chunks with regular expressions</em></span>, and <span class="emphasis"><em>Partial parsing with regular expressions</em></span> recipes of this chapter. All sequences of <code class="literal">NNP</code> tagged words are combined into <code class="literal">NAME</code> chunks.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec204"/>There's more...</h2></div></div></div><p>If we wanted to be sure to only chunk the names of people, then we can build a <code class="literal">PersonChunker</code> that uses the <code class="literal">names</code> corpus for chunking. This class can be found in <code class="literal">chunkers.py</code>:</p><div class="informalexample"><pre class="programlisting">import nltk.chunk
from nltk.corpus import names

class PersonChunker(nltk.chunk.ChunkParserI):
  def __init__(self):
    self.name_set = set(names.words())

  def parse(self, tagged_sent):
    iobs = []
    in_person = False

    for word, tag in tagged_sent:
      if word in self.name_set and in_person:
        iobs.append((word, tag, 'I-PERSON'))
      elif word in self.name_set:
        iobs.append((word, tag, 'B-PERSON'))
        in_person = True
      else:
        iobs.append((word, tag, 'O'))
        in_person = False

    return nltk.chunk.conlltags2tree(iobs)</pre></div><p>The <code class="literal">PersonChunker</code> iterates over the tagged sentence, checking if each word is in its <code class="literal">names_set</code> (constructed from the <code class="literal">names</code> corpus). If the current word is in the <code class="literal">names_set</code>, then it uses either the <code class="literal">B-PERSON</code> or <code class="literal">I-PERSON</code> IOB tags, depending on whether the previous word was also in the <code class="literal">names_set</code>. Any word that's not in the <code class="literal">names_set</code> gets the <code class="literal">O</code> IOB tag. When complete, the list of IOB tags is converted to a <code class="literal">Tree</code> using <code class="literal">nltk.chunk.conlltags2tree()</code>. Using it on the same tagged sentence as before, we get the following result:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from chunkers import PersonChunker
&gt;&gt;&gt; chunker = PersonChunker()
&gt;&gt;&gt; sub_leaves(chunker.parse(treebank_chunk.tagged_sents()[0]), 'PERSON')
[[('Pierre', 'NNP')]]</pre></div><p>We no longer get "Nov.", but we have also lost "Vinken", as it is not found in the <code class="literal">names</code> corpus. This recipe highlights some of the difficulties of chunk extraction and natural language processing in general:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">If you use general patterns, you will get general results</li><li class="listitem" style="list-style-type: disc">If you are looking for specific results, you must use specific data</li><li class="listitem" style="list-style-type: disc">If your specific data is incomplete, your results will be incomplete too</li></ul></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec205"/>See also</h2></div></div></div><p>The previous recipe defines the <code class="literal">sub_leaves()</code> method used to show the found chunks. In the next recipe, we will cover how to find <code class="literal">LOCATION</code> chunks based on the <code class="literal">gazetteers</code> corpus.</p></div></div>
<div class="section" title="Extracting location chunks"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec58"/>Extracting location chunks</h1></div></div></div><a id="id414" class="indexterm"/><p>To identify location chunks, we can make a different kind of <code class="literal">ChunkParserI</code> subclass that uses the <code class="literal">gazetteers</code> corpus to identify location words. <code class="literal">gazetteers</code> is a <code class="literal">WordListCorpusReader</code> that contains the following location words:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Country names</li><li class="listitem" style="list-style-type: disc">U.S. states and abbreviations</li><li class="listitem" style="list-style-type: disc">Major U.S. cities</li><li class="listitem" style="list-style-type: disc">Canadian provinces</li><li class="listitem" style="list-style-type: disc">Mexican states</li></ul></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec206"/>How to do it...</h2></div></div></div><p>The <code class="literal">LocationChunker</code>, found in <code class="literal">chunkers.py</code>, iterates over a tagged sentence looking for words that are found in the <code class="literal">gazetteers</code> corpus. When it finds one or more location words, it creates a <code class="literal">LOCATION</code> chunk using IOB tags. The helper method <code class="literal">iob_locations()</code> is where the IOB <code class="literal">LOCATION</code> tags are produced, and the <code class="literal">parse()</code> method converts these IOB tags to a <code class="literal">Tree</code>.</p><div class="informalexample"><pre class="programlisting">import nltk.chunk
from nltk.corpus import gazetteers

class LocationChunker(nltk.chunk.ChunkParserI):
  def __init__(self):
    self.locations = set(gazetteers.words())
    self.lookahead = 0

    for loc in self.locations:
      nwords = loc.count(' ')

      if nwords &gt; self.lookahead:
        self.lookahead = nwords

  def iob_locations(self, tagged_sent):
    i = 0
    l = len(tagged_sent)
    inside = False

    while i &lt; l:
      word, tag = tagged_sent[i]
      j = i + 1
      k = j + self.lookahead
      nextwords, nexttags = [], []
      loc = False

      while j &lt; k:
        if ' '.join([word] + nextwords) in self.locations:
          if inside:
            yield word, tag, 'I-LOCATION'
          else:
            yield word, tag, 'B-LOCATION'

          for nword, ntag in zip(nextwords, nexttags):
            yield nword, ntag, 'I-LOCATION'

          loc, inside = True, True
          i = j
          break

        if j &lt; l:
          nextword, nexttag = tagged_sent[j]
          nextwords.append(nextword)
          nexttags.append(nexttag)
          j += 1
        else:
          break

      if not loc:
        inside = False
        i += 1
        yield word, tag, 'O'

  def parse(self, tagged_sent):
    iobs = self.iob_locations(tagged_sent)
    return nltk.chunk.conlltags2tree(iobs)</pre></div><a id="id415" class="indexterm"/><p>We can use the <code class="literal">LocationChunker</code> to parse the following sentence into two locations, "San Francisco, CA is cold compared to San Jose, CA":</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from chunkers import LocationChunker
&gt;&gt;&gt; t = loc.parse([('San', 'NNP'), ('Francisco', 'NNP'), ('CA', 'NNP'), ('is', 'BE'), ('cold', 'JJ'), ('compared', 'VBD'), ('to', 'TO'), ('San', 'NNP'), ('Jose', 'NNP'), ('CA', 'NNP')])
&gt;&gt;&gt; sub_leaves(t, 'LOCATION')
[[('San', 'NNP'), ('Francisco', 'NNP'), ('CA', 'NNP')], [('San', 'NNP'), ('Jose', 'NNP'), ('CA', 'NNP')]]</pre></div><p>And the result is that we get two <code class="literal">LOCATION</code> chunks, just as expected.</p></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec207"/>How it works...</h2></div></div></div><a id="id416" class="indexterm"/><a id="id417" class="indexterm"/><p>The <code class="literal">LocationChunker</code> starts by constructing a <code class="literal">set</code> of all locations in the <code class="literal">gazetteers</code> corpus. Then it finds the maximum number of words in a single location string, so it knows how many words it must look ahead when parsing a tagged sentence.</p><a id="id418" class="indexterm"/><p>The <code class="literal">parse()</code> method calls a helper method <code class="literal">iob_locations()</code>, which generates 3-tuples of the form <code class="literal">(word, pos, iob)</code> where <code class="literal">iob</code> is either <code class="literal">O</code> if the word is not a location, or <code class="literal">B-LOCATION</code> or <code class="literal">I-LOCATION</code> for <code class="literal">LOCATION</code> chunks. <code class="literal">iob_locations()</code> finds location chunks by looking at the current word and the next words to check if the combined word is in the locations <code class="literal">set</code>. Multiple location words that are next to each other are then put into the same <code class="literal">LOCATION</code> chunk, such as in the preceding example with "San Francisco" and "CA".</p><p>Like in the previous recipe, it's simpler and more convenient to construct a list of <code class="literal">(word, pos, iob)</code> tuples to pass in to <code class="literal">nltk.chunk.conlltags2tree()</code> to return a <code class="literal">Tree</code>. The alternative is to construct a <code class="literal">Tree</code> manually, but that requires keeping track of children, subtrees, and where you currently are in the <code class="literal">Tree</code>.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec208"/>There's more...</h2></div></div></div><p>One of the nice aspects of this <code class="literal">LocationChunker</code> is that it doesn't care about the part-of-speech tags. As long as the location words are found in the locations set, any part-of-speech tag will do.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec209"/>See also</h2></div></div></div><p>In the next recipe, we will cover how to train a named entity chunker using the <code class="literal">ieer</code> corpus.</p></div></div>
<div class="section" title="Training a named entity chunker"><div class="titlepage"><div><div><h1 class="title"><a id="ch05lvl1sec59"/>Training a named entity chunker</h1></div></div></div><a id="id419" class="indexterm"/><p>You can train your own named entity chunker using the <code class="literal">ieer</code> corpus, which stands for <a id="id420" class="indexterm"/>
<span class="strong"><strong>Information Extraction—Entity Recognition</strong></span> (<span class="strong"><strong>ieer</strong></span>). It takes a bit of extra work though, because the <code class="literal">ieer</code> corpus has chunk trees, but no part-of-speech tags for words.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec210"/>How to do it...</h2></div></div></div><a id="id421" class="indexterm"/><a id="id422" class="indexterm"/><a id="id423" class="indexterm"/><p>Using the <code class="literal">ieertree2conlltags()</code> and <code class="literal">ieer_chunked_sents()</code> functions in <code class="literal">chunkers.py</code>, we can create named entity chunk trees from the <code class="literal">ieer</code> corpus to train the <code class="literal">ClassifierChunker</code> created in <span class="emphasis"><em>Classification-based chunking</em></span> recipe of this chapter.</p><div class="informalexample"><pre class="programlisting">import nltk.tag, nltk.chunk, itertools
from nltk.corpus import ieer

def ieertree2conlltags(tree, tag=nltk.tag.pos_tag):
  words, ents = zip(*tree.pos())
  iobs = []
  prev = None

  for ent in ents:
    if ent == tree.node:
      iobs.append('O')
      prev = None
    elif prev == ent:
      iobs.append('I-%s' % ent)
    else:
      iobs.append('B-%s' % ent)
      prev = ent

  words, tags = zip(*tag(words))
  return itertools.izip(words, tags, iobs)

def ieer_chunked_sents(tag=nltk.tag.pos_tag):
  for doc in ieer.parsed_docs():
    tagged = ieertree2conlltags(doc.text, tag)
    yield nltk.chunk.conlltags2tree(tagged)</pre></div><p>We will use 80 out of 94 sentences for training, and the rest for testing. Then we can see how it does on the first sentence of the <code class="literal">treebank_chunk</code> corpus.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from chunkers import ieer_chunked_sents, ClassifierChunker
&gt;&gt;&gt; from nltk.corpus import treebank_chunk
&gt;&gt;&gt; ieer_chunks = list(ieer_chunked_sents())
&gt;&gt;&gt; len(ieer_chunks)
94
&gt;&gt;&gt; chunker = ClassifierChunker(ieer_chunks[:80])
&gt;&gt;&gt; chunker.parse(treebank_chunk.tagged_sents()[0])
Tree('S', [Tree('LOCATION', [('Pierre', 'NNP'), ('Vinken', 'NNP')]), (',', ','), Tree('DURATION', [('61', 'CD'), ('years', 'NNS')]), Tree('MEASURE', [('old', 'JJ')]), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), Tree('DATE', [('Nov.', 'NNP'), ('29', 'CD')]), ('.', '.')])</pre></div><a id="id424" class="indexterm"/><p>So it found a correct <code class="literal">DURATION</code> and <code class="literal">DATE</code>, but tagged "Pierre Vinken" as a <code class="literal">LOCATION</code>. Let us see how it scores against the rest of <code class="literal">ieer</code> chunk trees:</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; score = chunker.evaluate(ieer_chunks[80:])
&gt;&gt;&gt; score.accuracy()
0.88290183880706252
&gt;&gt;&gt; score.precision()
0.40887174541947929
&gt;&gt;&gt; score.recall()
0.50536352800953521</pre></div><p>Accuracy is pretty good, but precision and recall are very low. That means lots of false negatives and false positives.</p></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec211"/>How it works...</h2></div></div></div><p>The truth is, we are not working with ideal training data. The <code class="literal">ieer</code> trees generated by <a id="id425" class="indexterm"/>
<code class="literal">ieer_chunked_sents()</code> are not entirely accurate. First, there are no explicit sentence breaks, so each document is a single tree. Second, the words are not explicitly tagged, so we have to guess using <code class="literal">nltk.tag.pos_tag()</code>.</p><p>The <code class="literal">ieer</code> corpus provides a <code class="literal">parsed_docs()</code> method that returns a list of documents with a <code class="literal">text</code> attribute. This <code class="literal">text</code> attribute is a document <code class="literal">Tree</code> that is converted to a list of 3-tuples of the form <code class="literal">(word, pos, iob)</code>. To get these final 3-tuples, we must first flatten the <code class="literal">Tree</code> using <code class="literal">tree.pos()</code>, which returns a list of 2-tuples of the form <code class="literal">(word, entity)</code>, where entity is either the entity tag or the top tag of the tree. Any words whose entity is the top tag are outside the named entity chunks and get the IOB tag <code class="literal">O</code>. All words that have unique entity tags are either the beginning of or inside a named entity chunk. Once we have all the IOB tags, then we can get the part-of-speech tags of all the words and join the words, part-of-speech tags, and IOB tags into 3-tuples using <code class="literal">itertools.izip()</code>.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec212"/>There's more...</h2></div></div></div><p>Despite the non-ideal training data, the <code class="literal">ieer</code> corpus provides a good place to start for training a named entity chunker. The data comes from the <span class="emphasis"><em>New York Times</em></span> and <span class="emphasis"><em>AP Newswire</em></span> reports. Each doc from <code class="literal">ieer.parsed_docs()</code> also contains a headline attribute that is a <code class="literal">Tree</code>.</p><div class="informalexample"><pre class="programlisting">&gt;&gt;&gt; from nltk.corpus import ieer
&gt;&gt;&gt; ieer.parsed_docs()[0].headline
Tree('DOCUMENT', ['Kenyans', 'protest', 'tax', 'hikes'])</pre></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch05lvl2sec213"/>See also</h2></div></div></div><p>The <span class="emphasis"><em>Extracting named entities</em></span> recipe in this chapter, covers the pre-trained named entity chunker that comes included with NLTK.</p></div></div></body></html>