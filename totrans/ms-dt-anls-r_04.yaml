- en: Chapter 4. Restructuring Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We already covered the most basic methods for restructuring data in the [Chapter
    3](ch03.html "Chapter 3. Filtering and Summarizing Data"), *Filtering and Summarizing
    Data*, but of course, there are several other, more complex tasks that we will
    master in the forthcoming pages.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just to give a quick example on how diversified tools are needed for getting
    the data in a form that can be used for real data analysis: Hadley Wickham, one
    of the best known R developers and users, spent one third of his PhD thesis on
    reshaping data. As he says, "it is unavoidable before doing any exploratory data
    analysis or visualization."'
  prefs: []
  type: TYPE_NORMAL
- en: 'So now, besides the previous examples of restructuring data, such as the counting
    of elements in each group, we will focus on some more advanced features, as listed
    next:'
  prefs: []
  type: TYPE_NORMAL
- en: Transposing matrices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Splitting, applying, and joining data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computing margins of tables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Merging data frames
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Casting and melting data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transposing matrices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the most used, but often not mentioned, methods for restructuring data
    is transposing matrices. This simply means switching the columns with rows and
    vice versa, via the `t` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Of course, this `S3` method also works with `data.frame`, and actually, with
    any tabular object. For more advanced features, such as transposing a multi-dimensional
    table, take a look at the `aperm` function from the `base` package.
  prefs: []
  type: TYPE_NORMAL
- en: Filtering data by string matching
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although some filtering algorithms were already discussed in the previous chapters,
    the `dplyr` package contains some magic features that have not yet been covered
    and are worth mentioning here. As we all know by this time, the `subset` function
    in `base`, or the `filter` function from `dplyr` is used for filtering rows, and
    the `select` function can be used to choose a subset of columns.
  prefs: []
  type: TYPE_NORMAL
- en: The function filtering rows usually takes an R expression, which returns the
    IDs of the rows to drop, similar to the `which` function. On the other hand, providing
    such R expressions to describe column names is often more problematic for the
    `select` function; it's harder if not impossible to evaluate R expressions on
    column names.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `dplyr` package provides some useful functions to select some columns of
    the data, based on column name patterns. For example, we can keep only the variables
    ending with the string, `delay`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Of course, there is a similar helper function to check the first characters
    of the column names with `starts_with`, and both functions can ignore (by default)
    or take into account the upper or lower case of the characters with the `ignore.case`
    parameter. And we have the more general, `contains` function, looking for substrings
    in the column names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The other option is that we might need a more complex approach with regular
    expressions, which is another extremely important skill for data scientists. Now,
    we will provide a regular expression to the `matches` function, which is to be
    fitted against all the columns names. Let''s select all the columns with a name
    comprising of 5 or 6 characters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We can keep all column names that do not match a regular expression by using
    a negative sign before the expression. For example, let''s identify the most frequent
    number of characters in the columns'' names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'And then, let''s remove all the columns with 7 or 8 characters from the dataset.
    Now, we will show the column names from the filtered dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Rearranging data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes, we do not want to filter any part of the data (neither the rows,
    nor the columns), but the data is simply not in the most useful order due to convenience
    or performance issues, as we have seen, for instance, in [Chapter 3](ch03.html
    "Chapter 3. Filtering and Summarizing Data"), *Filtering and Summarizing Data*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides the base `sort` and `order` functions, or providing the order of variables
    passed to the `[` operator, we can also use some SQL-like solutions with the `sqldf`
    package, or query the data in the right format directly from the database. And
    the previously mentioned `dplyr` package also provides an effective method for
    ordering data. Let''s sort the `hflights` data, based on the actual elapsed time
    for each of the quarter million flights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Well, it''s pretty straightforward that flights departing to Austin are among
    the first few records shown. For improved readability, the above three R expressions
    can be called in a much nicer way with the pipe operator from the automatically
    imported `magrittr` package, which provides a simple way to pass an R object as
    the first argument of the subsequent R expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'So, instead of nesting R functions, we can now start our R command with the
    core object and pass the results of each evaluated R expression to the next one
    in the chain. In most cases, this makes the code more convenient to read. Although
    most hardcore R programmers have already gotten used to reading the nested function
    calls from inside-out, believe me, it''s pretty easy to get used to this nifty
    feature! Do not let me confuse you with the inspiring painting of René Magritte,
    which became the slogan, "This is not a pipe," and a symbol of the `magrittr`
    package:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Rearranging data](img/2028OS_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'There is no limit to the number of chainable R expressions and objects one
    can have. For example, let''s also filter a few cases and variables to see how
    easy it is to follow the data restructuring steps with `dplyr`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'So, now we have filtered the original dataset a few times to see the closest
    airport after Austin, and the code is indeed easy to read and understand. This
    is a nice and efficient way to filter data, although some prefer to use nifty
    one-liners with the `data.table` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Almost perfect! The only problem is that we got different results due to the
    missing values, which were ordered at the beginning of the dataset while we defined
    the `data.table` object to be indexed by `ActualElapsedTime`. To overcome this
    issue, let''s drop the `NA` values, and instead of specifying the column names
    as strings along with forcing the `with` parameter to be `FALSE`, let''s pass
    a list of column names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This is exactly the same results as we have seen before. Please note that in
    this example, we have omitted the `NA` values after transforming `data.frame`
    to `data.table`, indexed by the `ActualElapsedTime` variable, which is a lot faster
    compared to calling `na.omit` on `hflights` first and then evaluating all the
    other R expressions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: dplyr versus data.table
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You might now be wondering, "which package should we use?"
  prefs: []
  type: TYPE_NORMAL
- en: The `dplyr` and `data.table` packages provide a spectacularly different syntax
    and a slightly less determinative difference in performance. Although `data.table`
    seems to be slightly more effective on larger datasets, there is no clear winner
    in this spectrum—except for doing aggregations on a high number of groups. And
    to be honest, the syntax of `dplyr`, provided by the `magrittr` package, can be
    also used by the `data.table` objects if needed.
  prefs: []
  type: TYPE_NORMAL
- en: Also, there is another R package that provides pipes in R, called the `pipeR`
    package, which claims to be a lot more effective on larger datasets than `magrittr`.
    This performance gain is due to the fact that the `pipeR` operators do not try
    to be smart like the F# language's `|>`-compatible operator in `magrittr`. Sometimes,
    this performance overhead is estimated to be 5-15 times more than the ones where
    no pipes are used at all.
  prefs: []
  type: TYPE_NORMAL
- en: One should take into account the community and support behind an R package before
    spending a reasonable amount of time learning about its usage. In a nutshell,
    the `data.table` package is now mature enough, without doubt, for production usage,
    as the development was started around 6 years ago by Matt Dowle, who was working
    for a large hedge fund at that time. The development has been continuous since
    then. Matt and Arun (co-developer of the package) release new features and performance
    tweaks from time to time, and they both seem to be keen on providing support on
    the public R forums and channels, such as mailing lists and StackOverflow.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, `dplyr` is shipped by Hadley Wickham and RStudio, one of
    the most well-known persons and trending companies in the R community, which translates
    to an even larger user-base, community, and kind-of-instant support on StackOverflow
    and GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: In short, I suggest using the packages that fit your needs best, after dedicating
    some time to discover the power and features they make available. If you are coming
    from an SQL background, you'll probably find `data.table` a lot more convenient,
    while others rather opt for the Hadleyverse (take a look at the R package with
    this name; it installs a bunch of useful R packages developed by Hadley). You
    should not mix the two approaches in a single project, as both for readability
    and performance issues, it's better to stick to only one syntax at a time.
  prefs: []
  type: TYPE_NORMAL
- en: To get a deeper understanding of the pros and cons of the different approaches,
    I will continue to provide multiple implementations of the same problem in the
    following few pages as well.
  prefs: []
  type: TYPE_NORMAL
- en: Computing new variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most trivial actions we usually perform while restructuring a dataset
    is to create a new variable. For a traditional `data.frame`, it's as simple as
    assigning a `vector` to a new variable of the R object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, this method also works with `data.table`, but the usage is deprecated
    due to the fact that there is a much more efficient way of creating one, or even
    multiple columns in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: We have just computed the distances, in kilometers, between the origin and destination
    airports with a simple division; although all the hardcore users can head for
    the `udunits2` package, which includes a bunch of conversion tools based on Unidata's
    `udunits` library.
  prefs: []
  type: TYPE_NORMAL
- en: And as can be seen previously, data.table uses that special := assignment operator
    inside of the square brackets, which might seem strange at first glance, but you
    will love it!
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `:=` operator can be more than 500 times faster than the traditional `<-`
    assignment, which is based on the official `data.table` documentation. This speedup
    is due to not copying the whole dataset into the memory like R used to do before
    the 3.1 version. Since then, R has used shallow copies, which greatly improved
    the performance of column updates, but is still beaten by `data.table` powerful
    in-place updates.
  prefs: []
  type: TYPE_NORMAL
- en: 'Compare the speed of how the preceding computation was run with the traditional
    `<-` operator and `data.table`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This is impressive, right? But it's worth double checking what we've just done.
    The first traditional call, of course, create/updates the `DistanceKMs` variable,
    but what happens in the second call? The `data.table` syntax did not return anything
    (visibly), but in the background, the `hflights_dt` R object was updated in-place
    due to the `:=` operator.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Please note that the `:=` operator can produce unexpected results when used
    inside of `knitr`, such as returning the `data.table` visible after the creation
    of a new variable, or strange rendering of the command when the return is `echo
    = TRUE`. As a workaround, Matt Dowle suggests increasing the `depthtrigger` option
    of `data.table`, or one can simply reassign the `data.table` object with the same
    name. Another solution might be to use my `pander` package over `knitr`. :)
  prefs: []
  type: TYPE_NORMAL
- en: But once again, how was it so fast?
  prefs: []
  type: TYPE_NORMAL
- en: Memory profiling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The magic of the `data.table` package—besides having more than 50 percent of
    C code in the sources—is copying objects in memory only if it's truly necessary.
    This means that R often copies objects in memory while updating those, and `data.table`
    tries to keep these resource-hungry actions at a minimal level. Let's verify this
    by analyzing the previous example with the help of the `pryr` package, which provides
    convenient access to some helper functions for memory profiling and understanding
    R-internals.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s recreate the `data.table` object and let''s take a note of the
    pointer value (location address of the object in the memory), so that we will
    be able to verify later if the new variable simply updated the same R object,
    or if it was copied in the memory while the operation took place:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Okay, so `0x62c88c0` refers to the location where `hflights_dt` is stored at
    the moment. Now, let''s check if it changes due to the traditional assignment
    operator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This is definitely a different location, which means that adding a new column
    to the R object also requires R to copy the whole object in the memory. Just imagine,
    we now moved 21 columns in memory due to adding another one.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, to bring about the usage of `:=` in `data.table`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The location of the R object in the memory did not change! And copying objects
    in the memory can cost you a lot of resources, thus a lot of time. Take a look
    at the following example, which is a slightly updated version of the above traditional
    variable assignment call, but with an added convenience layer of `within`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Here, using the `within` function probably copies the R object once more in
    the memory, and hence brings about the relatively serious performance overhead.
    Although the absolute time difference between the preceding examples might not
    seem very significant (not in the statistical context), but just imagine how the
    needless memory updates can affect the processing time of your data analysis with
    some larger datasets!
  prefs: []
  type: TYPE_NORMAL
- en: Creating multiple variables at a time
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One nice feature of `data.table` is the creation of multiple columns with a
    single command, which can be extremely useful in some cases. For example, we might
    be interested in the distance of airports in feet as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'So, it''s as simple as providing a character vector of the desired variable
    names on the left-hand side and the `list` of appropriate values on the right-hand
    side of the `:=` operator. This feature can easily be used for some more complex
    tasks. For example, let''s create the dummy variables of the airline carriers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Although it''s not a one-liner, and it also introduces a helper variable, it''s
    not that complex to see what we did:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we saved the `unique` carrier names in a character vector.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we defined the new variables' name with the help of that.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We iterated our anonymous function over this character vector as well, to return
    `TRUE` or `FALSE` if the carrier name matched the given column.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The given column was converted to `0` or `1` through `as.numeric`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: And then, we simply checked the structure of all columns whose names start with
    `carrier`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is not perfect, as we usually leave out one label from the dummy variables
    to reduce redundancy. In the current situation, the last new column is simply
    the linear combination of the other newly created columns, thus information is
    duplicated. For this end, it's usually a good practice to leave out, for example,
    the last category by passing `-1` to the `n` argument in the `head` function.
  prefs: []
  type: TYPE_NORMAL
- en: Computing new variables with dplyr
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The usage of `mutate` from the `dplyr` package is identical to that of the
    base `within` function, although `mutate` is a lot quicker than `within`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'If the analogy of `mutate` and `within` has not been made straightforward by
    the previous example, it''s probably also useful to show the same example without
    using pipes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Merging datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Besides the previously described elementary actions on a single dataset, joining
    multiple data sources is one of the most used methods in everyday action. The
    most often used solution for such a task is to simply call the `merge` S3 method,
    which can act as a traditional SQL inner and left/right/full outer joiner of operations—represented
    in a brief summary by C.L. Moffatt (2008) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Merging datasets](img/2028OS_06_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The `dplyr` package provides some easy ways for doing the previously presented
    join operations right from R, in an easy way:'
  prefs: []
  type: TYPE_NORMAL
- en: '`inner_join`: This joins the variables of all the rows, which are found in
    both datasets'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`left_join`: This includes all the rows from the first dataset and join variables
    from the other table'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`semi_join`: This includes only those rows from the first dataset that are
    found in the other one as well'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`anti_join`: This is similar to `semi_join`, but includes only those rows from
    the first dataset that are not found in the other one'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: For more examples, take a look at the *Two-table verbs* `dplyr` vignette, and
    the Data Wrangling cheat sheet listed in the *References* chapter at the end of
    the book.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: These features are also supported by the `mult` argument of `[` operator of
    `data.table` call, but for the time being, let's stick to the simpler use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we will merge a tiny dataset with the `hflights`
    data. Let''s create the `data.frame` demo by assigning names to the possible values
    of the `DayOfWeek` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see how we can left-join the previously defined `data.frame` with another
    `data.frame` and other tabular objects, as `merge` also supports fast operations
    on, for example, `data.table`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The prior example automatically merged the two tables via the `DayOfWeek` variable,
    which was part of both datasets and resulted in an extra variable in the original
    `hflights` dataset. However, we had to pass the variable name in the second example,
    as the `by` argument of `merge.data.table` defaults to the key variable of the
    object, which was missing then. One thing to note is that merging with `data.table`
    was a lot faster than the traditional tabular object type.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Any ideas on how to improve the previous didactical example? Instead of merging,
    the new variable could be computed as well. See for example, the weekdays function
    from base R: `weekdays(as.Date(with(hflights, paste(Year, Month, DayofMonth, sep
    = ''-''))))`.'
  prefs: []
  type: TYPE_NORMAL
- en: A much simpler way of merging datasets is when you simply want to add new rows
    or columns to the dataset with the same structure. For this end, `rbind` and `cbind`,
    or `rBind` and `cBind` for sparse matrices, do a wonderful job.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most often used functions along with these base commands is `do.call`,
    which can execute the `rbind` or `cbind` call on all elements of a `list`, thus
    enabling us, for example, to join a list of data frames. Such lists are usually
    created by `lapply` or the related functions from the `plyr` package. Similarly,
    `rbindlist` can be called to merge a `list` of `data.table` objects in a much
    faster way.
  prefs: []
  type: TYPE_NORMAL
- en: Reshaping data in a flexible way
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Hadley Wickham has written several R packages to tweak data structures, for
    example, a major part of his thesis concentrated on how to reshape data frames
    with his `reshape` package. Since then, this general aggregation and restructuring
    package has been renewed to be more efficient with the most commonly used tasks,
    and it was released with a new version number attached to the name: `reshape2`
    package.'
  prefs: []
  type: TYPE_NORMAL
- en: This was a total rewrite of the `reshape` package, which improves speed at the
    cost of functionality. Currently, the most important feature of `reshape2` is
    the possibility to convert between the so-called long (narrow) and wide tabular
    data format. This basically pertains to the columns being stacked below each other,
    or arranged beside each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'These features were presented in Hadley''s works with the following image on
    data restructuring, with the related `reshape` functions and simple use cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Reshaping data in a flexible way](img/2028OS_06_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As the `reshape` package is not under active development anymore, and its parts
    were outsourced to `reshape2`, `plyr`, and most recently to `dplyr`, we will only
    focus on the commonly used features of `reshape2` in the following pages. This
    will basically consist of the `melt` and `cast` functions, which provides a smart
    way of melting the data into a standardized form of measured and identifier variables
    (long table format), which can later be casted to a new shape for further analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Converting wide tables to the long table format
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Melting a data frame means that we transform the tabular data to key-value
    pairs, based on the given identifier variables. The original column names become
    the categories of the newly created `variable` column, while all numeric values
    of those (measured variables) are included in the new `value` column. Here''s
    a quick example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: So, we have just restructured the original `data.frame`, which had 21 variables
    and a quarter of a million records, into only 7 columns and more than 3.5 million
    records. Six out of the seven columns are factor type identifier variables, and
    the last column stores all the values. But why is it useful? Why should we transform
    the traditional wide tabular format to the much longer type of data?
  prefs: []
  type: TYPE_NORMAL
- en: For example, we might be interested in comparing the distribution of flight
    time with the actual elapsed time of the flight, which might not be straightforward
    to plot with the original data format. Although plotting a scatter plot of the
    above variables with the `ggplot2` package is extremely easy, how would you create
    two separate boxplots comparing the distributions?
  prefs: []
  type: TYPE_NORMAL
- en: 'The problem here is that we have two separate variables for the time measurements,
    while `ggplot` requires one `numeric` and one `factor` variable, from which the
    latter will be used to provide the labels on the *x*-axis. For this end, let''s
    restructure our dataset with `melt` by specifying the two numeric variables to
    treat as measurement variables and dropping all other columns— or in other words,
    not having any identifier variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In general, it's not a good idea to melt a dataset without identifier variables,
    as casting it later becomes cumbersome, if not impossible.
  prefs: []
  type: TYPE_NORMAL
- en: 'Please note that now we have exactly twice as many rows than we had before,
    and the `variable` column is a factor with only two levels, which represent the
    two measurement variables. And this resulting `data.frame` is now easy to plot
    with the two newly created columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '![Converting wide tables to the long table format](img/2028OS_06_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Well, the previous example might not seem mission critical, and to be honest,
    I first used the `reshape` package when I needed some similar transformation to
    be able to produce some nifty `ggplot2` charts—as the previous problem simply
    does not exist if someone is using `base` graphics. For example, you can simply
    pass the two separate variables of the original dataset to the `boxplot` function.
  prefs: []
  type: TYPE_NORMAL
- en: So, this is kind of entering the world of Hadley Wickham's R packages, and the
    journey indeed offers some great data analysis practices. Thus, I warmly suggest
    reading further, for example, on how using `ggplot2` is not easy, if not impossible,
    without knowing how to reshape datasets efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Converting long tables to the wide table format
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Casting a dataset is the opposite of melting, like turning key-value pairs
    into a tabular data format. But bear in mind that the key-value pairs can always
    be combined together in a variety of ways, so this process can result in extremely
    diversified outputs. Thus, you need a table and a formula to cast, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This example shows how to aggregate the measured flight times for each month
    in 2011 with the help of melting and casting the `hflights` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we melted the `data.frame` with the IDs being the `Month`, where we only
    kept two numeric variables for the flight times.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we casted the resulting `data.frame` with a simple formula to show the
    mean of each month for all measurement variables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'I am pretty sure that now you can quickly restructure this data to be able
    to plot two separate lines for this basic time-series:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '![Converting long tables to the wide table format](img/2028OS_06_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'But of course, melting and casting can be used for a variety of things besides
    aggregating. For example, we can restructure our original database to have a special
    `Month`, which includes all the records of the data. This, of course, doubles
    the number of rows in the dataset, but also lets us easily generate a table on
    the data with margins. Here''s a quick example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: This is very similar to what we have seen previously, but as an intermediate
    step, we have converted the `Month` variable to be factor with a special level,
    which resulted in the last line of this table. This row represents the overall
    arithmetic average of the related measure variables.
  prefs: []
  type: TYPE_NORMAL
- en: Tweaking performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some further good news on `reshape2` is that `data.table` has decent support
    for melting and casting, with highly improved performance. Matt Dowle has published
    some benchmarks with a 5-10 percent improvement in the processing times of using
    `cast` and `melt` on `data.table` objects instead of the traditional data frames,
    which is highly impressive.
  prefs: []
  type: TYPE_NORMAL
- en: To verify these results on your own dataset, simply transform the `data.frame`
    objects to `data.table` before calling the `reshape2` functions, as the `data.table`
    package already ships the appropriate `S3` methods to extend `reshape2`.
  prefs: []
  type: TYPE_NORMAL
- en: The evolution of the reshape packages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned before, `reshape2` was a complete rewrite of the `reshape` package,
    based on around 5 years of experience in using and developing the latter. This
    update also included some trade-offs, as the original reshape tasks were split
    among multiple packages. Thus, `reshape2` now offers a lot less compared to the
    kind of magic features that were supported by `reshape`. Just check, for example
    `reshape::cast`; especially the `margins` and `add.missing` argument!
  prefs: []
  type: TYPE_NORMAL
- en: 'But as it turns out, even `reshape2` offers a lot more than simply melting
    and casting data frames. The birth of the `tidyr` package was inspired by this
    fact: to have a package in the Hadleyverse that supports easy data cleaning and
    transformation between the long and wide table formats. In `tidyr` parlance, these
    operations are called `gather` and `spread`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Just to give a quick example of this new syntax, let''s re-implement the previous
    examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we focused on how to transform raw data into an appropriately
    structured format, before we could run statistical tests. This process is a really
    important part of our everyday actions, and it takes most of a data scientist's
    time. But after reading this chapter, you should be confident in how to restructure
    your data in most cases— so, this is the right time to focus on building some
    models, which we will do in the next chapter.
  prefs: []
  type: TYPE_NORMAL
