<html><head></head><body><div class="chapter" title="Chapter&#xA0;1.&#xA0;Setting Up a Spark Virtual Environment"><div class="titlepage"><div><div><h1 class="title"><a id="ch01"/>Chapter 1. Setting Up a Spark Virtual Environment</h1></div></div></div><p>In this chapter, we will build an isolated virtual environment for development purposes. The environment will be powered by Spark and the PyData libraries provided by the Python Anaconda distribution. These libraries include Pandas, Scikit-Learn, Blaze, Matplotlib, Seaborn, and Bokeh. We will perform the following activities:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Setting up the development environment using the Anaconda Python distribution. This will include enabling the IPython Notebook environment powered by PySpark for our data exploration tasks.</li><li class="listitem" style="list-style-type: disc">Installing and enabling Spark, and the PyData libraries such as Pandas, Scikit- Learn, Blaze, Matplotlib, and Bokeh.</li><li class="listitem" style="list-style-type: disc">Building a <code class="literal">word count</code> example app to ensure that everything is working fine.</li></ul></div><p>The last decade has seen the rise and dominance of data-driven behemoths such as Amazon, Google, Twitter, LinkedIn, and Facebook. These corporations, by seeding, sharing, or disclosing their infrastructure concepts, software practices, and data processing frameworks, have fostered a vibrant open source software community. This has transformed the enterprise technology, systems, and software architecture.</p><p>This includes new infrastructure and DevOps (short for development and operations), concepts leveraging virtualization, cloud technology, and software-defined networks.</p><p>To process petabytes of data, Hadoop was developed and open sourced, taking its inspiration <a id="id0" class="indexterm"/>from the <span class="strong"><strong>Google File System</strong></span> (<span class="strong"><strong>GFS</strong></span>) and the adjoining distributed computing framework, MapReduce. Overcoming the complexities of scaling while keeping costs under control has also led to a proliferation of new data stores. Examples of recent database technology include Cassandra, a columnar database; MongoDB, a document database; and Neo4J, a graph database.</p><p>Hadoop, thanks to its ability to process huge datasets, has fostered a vast ecosystem to query data more iteratively and interactively with Pig, Hive, Impala, and Tez. Hadoop is cumbersome as it operates only in batch mode using MapReduce. Spark is creating a revolution in the analytics and data processing realm by targeting the shortcomings of disk input-output and bandwidth-intensive MapReduce jobs.</p><p>Spark is written in Scala, and therefore integrates natively with the <span class="strong"><strong>Java Virtual Machine</strong></span> (<span class="strong"><strong>JVM</strong></span>) powered ecosystem. Spark had early on provided Python API and bindings <a id="id1" class="indexterm"/>by enabling PySpark. The Spark architecture and ecosystem is inherently polyglot, with an obvious strong presence of Java-led systems.</p><p>This book will focus on PySpark and the PyData ecosystem. Python is one of the preferred languages in the academic and scientific community for data-intensive processing. Python has developed a rich ecosystem of libraries and tools in data manipulation with Pandas and Blaze, in Machine Learning with Scikit-Learn, and in data visualization with Matplotlib, Seaborn, and Bokeh. Hence, the aim of this book is to build an end-to-end architecture for data-intensive applications powered by Spark and Python. In order to put these concepts in to practice, we will analyze social networks such as Twitter, GitHub, and Meetup. We will focus on the activities and social interactions of Spark and the Open Source Software community by tapping into GitHub, Twitter, and Meetup.</p><p>Building data-intensive applications requires highly scalable infrastructure, polyglot storage, seamless data integration, multiparadigm analytics processing, and efficient visualization. The following paragraph describes the data-intensive app architecture blueprint that we will adopt throughout the book. It is the backbone of the book. We will discover Spark in the context of the broader PyData ecosystem.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip02"/>Tip</h3><p>
<span class="strong"><strong>Downloading the example code</strong></span>
</p><p>You can download the example code files for all Packt books you have purchased from your account at <a class="ulink" href="http://www.packtpub.com">http://www.packtpub.com</a>. If you purchased this book elsewhere, you can visit <a class="ulink" href="http://www.packtpub.com/support">http://www.packtpub.com/support</a> and register to have the files e-mailed directly to you.</p></div></div><div class="section" title="Understanding the architecture of data-intensive applications"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec08"/>Understanding the architecture of data-intensive applications</h1></div></div></div><p>In <a id="id2" class="indexterm"/>order to understand the architecture of data-intensive applications, the following conceptual framework is used. The is architecture is designed on the following five layers:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Infrastructure layer</li><li class="listitem" style="list-style-type: disc">Persistence layer</li><li class="listitem" style="list-style-type: disc">Integration layer</li><li class="listitem" style="list-style-type: disc">Analytics layer</li><li class="listitem" style="list-style-type: disc">Engagement layer</li></ul></div><p>The following screenshot depicts the five layers of the <span class="strong"><strong>Data Intensive App Framework</strong></span>:</p><div class="mediaobject"><img src="graphics/B03968_01_01.jpg" alt="Understanding the architecture of data-intensive applications"/></div><p>From<a id="id3" class="indexterm"/> the bottom up, let's go through the layers and their main purpose.</p><div class="section" title="Infrastructure layer"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec07"/>Infrastructure layer</h2></div></div></div><p>The <a id="id4" class="indexterm"/>infrastructure layer is primarily <a id="id5" class="indexterm"/>concerned with virtualization, scalability, and continuous integration. In practical terms, and in terms of virtualization, we will go through building our own development environment in a VirtualBox and virtual machine powered by Spark and the Anaconda distribution of Python. If we wish to scale from there, we can create a similar environment in the cloud. The practice of creating a segregated development environment and moving into test and production deployment can be<a id="id6" class="indexterm"/> automated <a id="id7" class="indexterm"/>and can be part of a continuous integration<a id="id8" class="indexterm"/> cycle <a id="id9" class="indexterm"/>powered by DevOps tools such as <span class="strong"><strong>Vagrant</strong></span>, <span class="strong"><strong>Chef</strong></span>, <span class="strong"><strong>Puppet</strong></span>, and <span class="strong"><strong>Docker</strong></span>. Docker is a very popular open source project that eases the installation and deployment of new environments. The book will be limited to building the virtual machine using VirtualBox. From a data-intensive app architecture point of view, we are describing the essential steps of the infrastructure layer by mentioning scalability and continuous integration beyond just virtualization.</p></div><div class="section" title="Persistence layer"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec08"/>Persistence layer</h2></div></div></div><p>The persistence layer manages the various repositories in accordance with data needs and shapes. It ensures the set up and management of the polyglot data stores. It includes relational <a id="id10" class="indexterm"/>database management <a id="id11" class="indexterm"/>systems such as <span class="strong"><strong>MySQL</strong></span> and <span class="strong"><strong>PostgreSQL</strong></span>; key-value data<a id="id12" class="indexterm"/> stores such as <span class="strong"><strong>Hadoop</strong></span>, <span class="strong"><strong>Riak</strong></span>, and <span class="strong"><strong>Redis</strong></span>; columnar databases<a id="id13" class="indexterm"/> such as <span class="strong"><strong>HBase</strong></span> and <span class="strong"><strong>Cassandra</strong></span>; document databases <a id="id14" class="indexterm"/>such as <span class="strong"><strong>MongoDB</strong></span> and <span class="strong"><strong>Couchbase</strong></span>; and graph <a id="id15" class="indexterm"/>databases<a id="id16" class="indexterm"/> such as <span class="strong"><strong>Neo4j</strong></span>. The persistence<a id="id17" class="indexterm"/> layer manages various filesystems such as Hadoop's HDFS. It interacts with various storage systems from native hard drives to Amazon S3. It manages various file storage formats such as <code class="literal">csv</code>, <code class="literal">json</code>, and <code class="literal">parquet</code>, which is a column-oriented format.</p></div><div class="section" title="Integration layer"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec09"/>Integration layer</h2></div></div></div><p>The<a id="id18" class="indexterm"/> integration layer focuses on<a id="id19" class="indexterm"/> data acquisition, transformation, quality, persistence, consumption, and governance. It is essentially driven by the following five Cs: <span class="emphasis"><em>connect</em></span>, <span class="emphasis"><em>collect</em></span>, <span class="emphasis"><em>correct</em></span>, <span class="emphasis"><em>compose</em></span>, and <span class="emphasis"><em>consume</em></span>.</p><p>The five steps describe the lifecycle of data. They are focused on how to acquire the dataset of interest, explore it, iteratively refine and enrich the collected information, and get it ready for consumption. So, the steps perform the following operations:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Connect</strong></span>: Targets the best way to acquire data from the various data sources, APIs<a id="id20" class="indexterm"/> offered by these sources, the input format, input schemas if they exist, the rate of data collection, and limitations from providers</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Correct</strong></span>: Focuses<a id="id21" class="indexterm"/> on transforming data for further processing and also ensures that the quality and consistency of the data received are maintained</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Collect</strong></span>: Looks <a id="id22" class="indexterm"/>at which data to store where and in what format, to ease data composition and consumption at later stages</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Compose</strong></span>: Concentrates<a id="id23" class="indexterm"/> its attention on how to mash up the various data sets collected, and enrich the information in order to build a compelling data-driven product</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Consume</strong></span>: Takes <a id="id24" class="indexterm"/>care of data provisioning and rendering and how the right data reaches the right individual at the right time</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Control</strong></span>: This sixth <span class="emphasis"><em>additional</em></span> step will sooner or later be required as the data, the<a id="id25" class="indexterm"/> organization, and the participants grow and it is about ensuring data governance</li></ul></div><p>The following diagram depicts the iterative process of data acquisition and refinement for consumption:</p><div class="mediaobject"><img src="graphics/B03968_01_02.jpg" alt="Integration layer"/></div></div><div class="section" title="Analytics layer"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec10"/>Analytics layer</h2></div></div></div><p>The analytics<a id="id26" class="indexterm"/> layer is where <a id="id27" class="indexterm"/>Spark processes data with the various models, algorithms, and machine learning pipelines in order to derive insights. For our purpose, in this book, the analytics layer is powered by Spark. We will delve deeper in subsequent chapters into the merits of Spark. In a nutshell, what makes it so powerful is that it allows multiple paradigms of analytics processing in a single unified platform. It allows batch, streaming, and interactive analytics. Batch processing on large datasets with longer latency periods allows us to extract patterns and insights that can feed into real-time events in streaming mode. Interactive and iterative analytics are more suited for data exploration. Spark offers bindings and APIs in Python and R. With its <span class="strong"><strong>SparkSQL</strong></span> module and <a id="id28" class="indexterm"/>the Spark Dataframe, it offers a very familiar analytics interface.</p></div><div class="section" title="Engagement layer"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec11"/>Engagement layer</h2></div></div></div><p>The <a id="id29" class="indexterm"/>engagement layer interacts with <a id="id30" class="indexterm"/>the end user and provides dashboards, interactive visualizations, and alerts. We will focus here on the tools provided by the PyData ecosystem such as Matplotlib, Seaborn, and Bokeh.</p></div></div></div>
<div class="section" title="Understanding Spark"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec09"/>Understanding Spark</h1></div></div></div><p>Hadoop <a id="id31" class="indexterm"/>scales horizontally as the data grows. Hadoop runs on commodity hardware, so it is cost-effective. Intensive data applications are enabled by scalable, distributed processing frameworks that allow organizations to analyze petabytes of data on large commodity clusters. Hadoop is the first open source implementation of map-reduce. Hadoop<a id="id32" class="indexterm"/> relies on a distributed framework for storage called <span class="strong"><strong>HDFS</strong></span> (<span class="strong"><strong>Hadoop Distributed File System</strong></span>). Hadoop runs map-reduce tasks in batch jobs. Hadoop requires persisting the data to disk at each map, shuffle, and reduce process step. The overhead and the latency of such batch jobs adversely impact the performance.</p><p>Spark is a fast, distributed general analytics computing engine for large-scale data processing. The major breakthrough from Hadoop is that Spark allows data sharing between processing steps through in-memory processing of data pipelines.</p><p>Spark is unique in that it allows four different styles of data analysis and processing. Spark can be used in:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Batch</strong></span>: This<a id="id33" class="indexterm"/> mode is used for manipulating large datasets, typically performing large map-reduce jobs</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Streaming</strong></span>: This<a id="id34" class="indexterm"/> mode is used to process incoming information in near real time</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Iterative</strong></span>: This <a id="id35" class="indexterm"/>mode is for machine learning algorithms such as a gradient descent where the data is accessed repetitively in order to reach convergence</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Interactive</strong></span>: This<a id="id36" class="indexterm"/> mode is used for data exploration as large chunks of data are in memory and due to the very quick response time of Spark</li></ul></div><p>The following figure highlights the preceding four processing styles:</p><div class="mediaobject"><img src="graphics/B03968_01_03.jpg" alt="Understanding Spark"/></div><p>Spark operates in three modes: one single mode, standalone on a single machine and two distributed modes on a cluster of machines—on Yarn, the Hadoop distributed resource manager, or on Mesos, the open source cluster manager developed at Berkeley concurrently with Spark:</p><div class="mediaobject"><img src="graphics/B03968_01_04.jpg" alt="Understanding Spark"/></div><p>Spark offers a polyglot interface in Scala, Java, Python, and R.</p><div class="section" title="Spark libraries"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec12"/>Spark libraries</h2></div></div></div><p>Spark<a id="id37" class="indexterm"/> comes with batteries included, with some powerful libraries:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>SparkSQL</strong></span>: This <a id="id38" class="indexterm"/>provides the SQL-like ability to interrogate structured data and interactively explore large datasets</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>SparkMLLIB</strong></span>: This<a id="id39" class="indexterm"/> provides major algorithms and a pipeline framework for machine learning</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Spark Streaming</strong></span>: This<a id="id40" class="indexterm"/> is for near real-time analysis of data using micro batches and sliding widows on incoming streams of data</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Spark GraphX</strong></span>: This<a id="id41" class="indexterm"/> is for graph processing and computation on complex connected entities and relationships</li></ul></div><div class="section" title="PySpark in action"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl3sec01"/>PySpark in action</h3></div></div></div><p>Spark is<a id="id42" class="indexterm"/> written in Scala. The whole Spark ecosystem naturally leverages the JVM environment and capitalizes on HDFS natively. Hadoop HDFS is one of the many data stores supported by Spark. Spark is agnostic and from the beginning interacted with multiple data sources, types, and formats.</p><p>PySpark is not a transcribed version of Spark on a Java-enabled dialect of Python such as Jython. PySpark provides integrated API bindings around Spark and enables full usage of the Python ecosystem within all the nodes of the cluster with the pickle Python serialization and, more importantly, supplies access to the rich ecosystem of Python's machine learning libraries such as Scikit-Learn or data processing such as Pandas.</p><p>When we initialize a Spark program, the first thing a Spark program must do is to create a <code class="literal">SparkContext</code> object. It tells Spark how to access the cluster. The Python program creates a <code class="literal">PySparkContext</code>. Py4J is the gateway that binds the Python program to the Spark JVM <code class="literal">SparkContext</code>. The JVM <code class="literal">SparkContextserializes</code> the application codes and the closures and sends them to the cluster for execution. The cluster manager allocates resources and schedules, and ships the closures to the Spark workers in the cluster who activate Python virtual machines as required. In each machine, the Spark Worker is managed by an executor that controls computation, storage, and cache.</p><p>Here's an<a id="id43" class="indexterm"/> example of how the Spark driver manages both the PySpark context and the Spark context with its local filesystems and its interactions with the Spark worker through the cluster manager:</p><div class="mediaobject"><img src="graphics/B03968_01_05.jpg" alt="PySpark in action"/></div></div><div class="section" title="The Resilient Distributed Dataset"><div class="titlepage"><div><div><h3 class="title"><a id="ch01lvl3sec02"/>The Resilient Distributed Dataset</h3></div></div></div><p>Spark <a id="id44" class="indexterm"/>applications consist of a driver <a id="id45" class="indexterm"/>program that runs the user's main function, creates distributed datasets on the cluster, and executes various parallel operations (transformations and actions) on those datasets.</p><p>Spark applications are run as an independent set of processes, coordinated by a <code class="literal">SparkContext</code> in a driver program.</p><p>The <code class="literal">SparkContext</code> will <a id="id46" class="indexterm"/>be allocated system resources (machines, memory, CPU) from the <span class="strong"><strong>Cluster manager</strong></span>.</p><p>The <code class="literal">SparkContext</code> manages executors who manage workers in the cluster. The driver program has Spark jobs that need to run. The jobs are split into tasks submitted to the executor for completion. The executor takes care of computation, storage, and caching in each machine.</p><p>The key building block in Spark is the <span class="strong"><strong>RDD</strong></span> (<span class="strong"><strong>Resilient Distributed Dataset</strong></span>). A dataset is a collection of elements. Distributed means the dataset can be on any node in the cluster. Resilient means that the dataset could get lost or partially lost without major harm to the computation in progress as Spark will re-compute from the data lineage in memory, also known as the <span class="strong"><strong>DAG</strong></span> (short for <span class="strong"><strong>Directed Acyclic Graph</strong></span>) of operations. Basically, Spark will snapshot in memory a state of the RDD in the cache. If one of the computing machines crashes during operation, Spark rebuilds the RDDs from the cached RDD <a id="id47" class="indexterm"/>and<a id="id48" class="indexterm"/> the DAG of operations. RDDs<a id="id49" class="indexterm"/> recover from node failure.</p><p>There are two types of operation on RDDs:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Transformations</strong></span>: A transformation takes an existing RDD and leads to a pointer <a id="id50" class="indexterm"/>of a new transformed RDD. An RDD is immutable. Once created, it cannot be changed. Each transformation creates a new RDD. Transformations are lazily evaluated. Transformations are executed only when an action occurs. In the case of failure, the data lineage of transformations rebuilds the RDD.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Actions</strong></span>: An action on an RDD triggers a Spark job and yields a value. An action <a id="id51" class="indexterm"/>operation causes Spark to execute the (lazy) transformation operations that are required to compute the RDD returned by the action. The action results in a DAG of operations. The DAG is compiled into stages where each stage is executed as a series of tasks. A task is a fundamental unit of work.</li></ul></div><p>Here's some useful information on RDDs:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">RDDs are created from a data source such as an HDFS file or a DB query. There are three ways to create an RDD:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Reading from a datastore</li><li class="listitem" style="list-style-type: disc">Transforming an existing RDD</li><li class="listitem" style="list-style-type: disc">Using an in-memory collection</li></ul></div></li><li class="listitem" style="list-style-type: disc">RDDs are transformed with functions such as <code class="literal">map</code> or <code class="literal">filter</code>, which yield new RDDs.</li><li class="listitem" style="list-style-type: disc">An action such as first, take, collect, or count on an RDD will deliver the results into the Spark driver. The Spark driver is the client through which the user interacts with the Spark cluster.</li></ul></div><p>The following diagram illustrates the RDD transformation and action:</p><p> </p><div class="mediaobject"><img src="graphics/B03968_01_06.jpg" alt="The Resilient Distributed Dataset"/></div><p>
</p></div></div></div>
<div class="section" title="Understanding Anaconda"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec10"/>Understanding Anaconda</h1></div></div></div><p>Anaconda is <a id="id52" class="indexterm"/>a widely used free Python distribution maintained<a id="id53" class="indexterm"/> by <span class="strong"><strong>Continuum </strong></span>(<a class="ulink" href="https://www.continuum.io/">https://www.continuum.io/</a>). We will use the prevailing software stack provided by Anaconda to generate our apps. In this book, we will use PySpark and the PyData ecosystem. The PyData ecosystem is promoted, supported, and maintained by <span class="strong"><strong>Continuum</strong></span> and powered by the <span class="strong"><strong>Anaconda</strong></span> Python distribution. The Anaconda Python distribution essentially saves time and aggravation in the installation of the Python environment; we will use it in conjunction with Spark. Anaconda has its own package management that supplements the traditional <code class="literal">pip</code> <code class="literal">install</code> and <code class="literal">easy-install</code>. Anaconda comes with batteries included, namely some of the most important packages such as Pandas, Scikit-Learn, Blaze, Matplotlib, and Bokeh. An upgrade to any of the installed library is a simple command at the console:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ conda update</strong></span>
</pre></div><p>A list of installed libraries in our environment can be obtained with command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ conda list</strong></span>
</pre></div><p>The key components of the stack are as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Anaconda</strong></span>: This<a id="id54" class="indexterm"/> is a free Python distribution with almost 200 Python packages for science, math, engineering, and data analysis.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Conda</strong></span>: This is a package manager that takes care of all the dependencies of installing a<a id="id55" class="indexterm"/> complex software stack. This is not restricted to <a id="id56" class="indexterm"/>Python and manages the install process for R and other languages.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Numba</strong></span>: This <a id="id57" class="indexterm"/>provides the power to speed up code in Python with high-performance functions and just-in-time compilation.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Blaze</strong></span>: This<a id="id58" class="indexterm"/> enables large scale data analytics by offering a uniform and adaptable interface to access a variety of data providers, which include streaming Python, Pandas, SQLAlchemy, and Spark.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Bokeh</strong></span>: This <a id="id59" class="indexterm"/>provides interactive data visualizations for large and streaming datasets.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Wakari</strong></span>: This<a id="id60" class="indexterm"/> allows us to share and deploy IPython Notebooks and other apps on a hosted environment.</li></ul></div><p>The following figure shows the components of the Anaconda stack:</p><div class="mediaobject"><img src="graphics/B03968_01_07.jpg" alt="Understanding Anaconda"/></div></div>
<div class="section" title="Setting up the Spark powered environment"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec11"/>Setting up the Spark powered environment</h1></div></div></div><p>In this<a id="id61" class="indexterm"/> section, we will learn to set up Spark:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Create a segregated development environment in a virtual machine running on Ubuntu 14.04, so it does not interfere with any existing system.</li><li class="listitem" style="list-style-type: disc">Install Spark 1.3.0 with its dependencies, namely.</li><li class="listitem" style="list-style-type: disc">Install the Anaconda Python 2.7 environment with all the required libraries such as Pandas, Scikit-Learn, Blaze, and Bokeh, and enable PySpark, so it can be accessed through IPython Notebooks.</li><li class="listitem" style="list-style-type: disc">Set up the backend or data stores of our environment. We will use MySQL as the relational database, MongoDB as the document store, and Cassandra as the columnar database.</li></ul></div><p>Each storage backend serves a specific purpose depending on the nature of the data to be handled. The MySQL RDBMs is used for standard tabular processed information that can be easily queried using SQL. As we will be processing a lot of JSON-type data from various APIs, the easiest way to store them is in a document. For real-time and time-series-related information, Cassandra is best suited as a columnar database.</p><p>The following diagram gives a view of the environment we will build and use throughout the book:</p><div class="mediaobject"><img src="graphics/B03968_01_08.jpg" alt="Setting up the Spark powered environment"/></div><div class="section" title="Setting up an Oracle VirtualBox with Ubuntu"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec13"/>Setting up an Oracle VirtualBox with Ubuntu</h2></div></div></div><p>Setting up a clean new VirtualBox <a id="id62" class="indexterm"/>environment on Ubuntu 14.04 is the safest way to create a development environment that does not conflict with existing libraries and can be later replicated in the cloud using a similar list of commands.</p><p>In order to set up an environment with Anaconda and Spark, we will create a VirtualBox virtual machine running Ubuntu 14.04.</p><p>Let's go through the steps of using VirtualBox with Ubuntu:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Oracle<a id="id63" class="indexterm"/> VirtualBox VM is free and can be downloaded from <a class="ulink" href="https://www.virtualbox.org/wiki/Downloads">https://www.virtualbox.org/wiki/Downloads</a>. The installation is pretty straightforward.</li><li class="listitem">After installing VirtualBox, let's open the Oracle VM VirtualBox Manager and click the <span class="strong"><strong>New</strong></span> button.</li><li class="listitem">We'll give the new VM a name, and select Type <span class="strong"><strong>Linux</strong></span> and Version <span class="strong"><strong>Ubuntu (64 bit)</strong></span>.</li><li class="listitem">You need to download the ISO from the Ubuntu website and allocate sufficient RAM (4 GB recommended) and disk space (20 GB recommended). We will <a id="id64" class="indexterm"/>use the Ubuntu 14.04.1 LTS release, which is found here: <a class="ulink" href="http://www.ubuntu.com/download/desktop">http://www.ubuntu.com/download/desktop</a>.</li><li class="listitem">Once the installation <a id="id65" class="indexterm"/>completed, it is advisable to install the VirtualBox Guest Additions by going to (from the VirtualBox menu, with the new VM running) <span class="strong"><strong>Devices</strong></span> | <span class="strong"><strong>Insert Guest Additions CD image</strong></span>. Failing to provide the guest additions in a Windows host gives a very limited user interface with reduced window sizes.</li><li class="listitem">Once the additional installation completes, reboot the VM, and it will be ready to use. It is helpful to enable the shared clipboard by selecting the VM and clicking <span class="strong"><strong>Settings</strong></span>, then go to <span class="strong"><strong>General</strong></span> | <span class="strong"><strong>Advanced</strong></span> | <span class="strong"><strong>Shared Clipboard</strong></span> and click on <span class="strong"><strong>Bidirectional</strong></span>.</li></ol></div></div><div class="section" title="Installing Anaconda with Python 2.7"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec14"/>Installing Anaconda with Python 2.7</h2></div></div></div><p>PySpark currently runs only <a id="id66" class="indexterm"/>on Python 2.7. (There are requests from the community to upgrade to Python 3.3.) To install Anaconda, follow these steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Download the Anaconda Installer for Linux 64-bit Python 2.7 from <a class="ulink" href="http://continuum.io/downloads#all">http://continuum.io/downloads#all</a>.</li><li class="listitem">After <a id="id67" class="indexterm"/>downloading the Anaconda installer, open a terminal and navigate to the directory or folder where the installer has been saved. From here, run the following command, replacing the <code class="literal">2.x.x</code> in the command with the version number of the downloaded installer file:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># install anaconda 2.x.x</strong></span>
<span class="strong"><strong>bash Anaconda-2.x.x-Linux-x86[_64].sh</strong></span>
</pre></div></li><li class="listitem">After accepting the license terms, you will be asked to specify the install location (which <code class="literal">defaults to ~/anaconda</code>).</li><li class="listitem">After the self-extraction is finished, you should add the anaconda binary directory to your PATH environment variable:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># add anaconda to PATH</strong></span>
<span class="strong"><strong>bash Anaconda-2.x.x-Linux-x86[_64].sh</strong></span>
</pre></div></li></ol></div></div><div class="section" title="Installing Java 8"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec15"/>Installing Java 8</h2></div></div></div><p>Spark<a id="id68" class="indexterm"/> runs on the JVM and requires<a id="id69" class="indexterm"/> the Java <span class="strong"><strong>SDK</strong></span> (short for <span class="strong"><strong>Software Development Kit</strong></span>) and not <a id="id70" class="indexterm"/>the <span class="strong"><strong>JRE</strong></span> (short for <span class="strong"><strong>Java Runtime Environment</strong></span>), as we will build apps with Spark. The recommended version is Java Version <a id="id71" class="indexterm"/>7 or higher. Java 8 is the most suitable, as it includes many of the functional programming techniques available with Scala and Python.</p><p>To install Java 8, follow these steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Install Oracle Java 8 using the following commands:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># install oracle java 8</strong></span>
<span class="strong"><strong>$ sudo apt-get install software-properties-common</strong></span>
<span class="strong"><strong>$ sudo add-apt-repository ppa:webupd8team/java</strong></span>
<span class="strong"><strong>$ sudo apt-get update</strong></span>
<span class="strong"><strong>$ sudo apt-get install oracle-java8-installer</strong></span>
</pre></div></li><li class="listitem">Set the <code class="literal">JAVA_HOME</code> environment variable and ensure that the Java program is on your PATH.</li><li class="listitem">Check that <code class="literal">JAVA_HOME</code> is properly installed:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># </strong></span>
<span class="strong"><strong>$ echo JAVA_HOME</strong></span>
</pre></div></li></ol></div></div><div class="section" title="Installing Spark"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec16"/>Installing Spark</h2></div></div></div><p>Head <a id="id72" class="indexterm"/>over to the Spark download page at <a class="ulink" href="http://spark.apache.org/downloads.html">http://spark.apache.org/downloads.html</a>.</p><p>The <a id="id73" class="indexterm"/>Spark download page offers the possibility to download earlier versions of Spark and different package and download types. We will select the latest release, pre-built for Hadoop 2.6 and later. The easiest way to install Spark is to use a Spark package prebuilt for Hadoop 2.6 and later, rather than build it from source. Move the file to the directory <code class="literal">~/spark</code> under the root directory.</p><p>Download the latest release of Spark—Spark 1.5.2, released on November 9, 2015:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Select Spark release <span class="strong"><strong>1.5.2 (Nov 09 2015),</strong></span></li><li class="listitem">Chose the package type <span class="strong"><strong>Prebuilt for Hadoop 2.6 and later</strong></span>,</li><li class="listitem">Chose the download type <span class="strong"><strong>Direct Download</strong></span>,</li><li class="listitem">Download Spark: <span class="strong"><strong>spark-1.5.2-bin-hadoop2.6.tgz</strong></span>,</li><li class="listitem">Verify this release using the 1.3.0 signatures and checksums,</li></ol></div><p>This can also be accomplished by running:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># download spark</strong></span>
<span class="strong"><strong>$ wget http://d3kbcqa49mib13.cloudfront.net/spark-1.5.2-bin-hadoop2.6.tgz</strong></span>
</pre></div><p>Next, we'll extract the files and clean up:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># extract, clean up, move the unzipped files under the spark directory</strong></span>
<span class="strong"><strong>$ tar -xf spark-1.5.2-bin-hadoop2.6.tgz</strong></span>
<span class="strong"><strong>$ rm spark-1.5.2-bin-hadoop2.6.tgz</strong></span>
<span class="strong"><strong>$ sudo mv spark-* spark</strong></span>
</pre></div><p>Now, we<a id="id74" class="indexterm"/> can run the Spark Python interpreter with:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># run spark</strong></span>
<span class="strong"><strong>$ cd ~/spark</strong></span>
<span class="strong"><strong>./bin/pyspark</strong></span>
</pre></div><p>You should see something like this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>Welcome to</strong></span>
<span class="strong"><strong>      ____              __</strong></span>
<span class="strong"><strong>     / __/__  ___ _____/ /__</strong></span>
<span class="strong"><strong>    _\ \/ _ \/ _ `/ __/  '_/</strong></span>
<span class="strong"><strong>   /__ / .__/\_,_/_/ /_/\_\   version 1.5.2</strong></span>
<span class="strong"><strong>      /_/</strong></span>
<span class="strong"><strong>Using Python version 2.7.6 (default, Mar 22 2014 22:59:56)</strong></span>
<span class="strong"><strong>SparkContext available as sc.</strong></span>
<span class="strong"><strong>&gt;&gt;&gt; </strong></span>
</pre></div><p>The interpreter will have already provided us with a Spark context object, <code class="literal">sc</code>, which we can see by running:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; print(sc)</strong></span>
<span class="strong"><strong>&lt;pyspark.context.SparkContext object at 0x7f34b61c4e50&gt;</strong></span>
</pre></div></div><div class="section" title="Enabling IPython Notebook"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec17"/>Enabling IPython Notebook</h2></div></div></div><p>We will <a id="id75" class="indexterm"/>work with IPython Notebook for a friendlier user experience than the console.</p><p>You can launch IPython Notebook by using the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ IPYTHON_OPTS="notebook --pylab inline"  ./bin/pyspark</strong></span>
</pre></div><p>Launch PySpark with <code class="literal">IPYNB</code> in the directory <code class="literal">examples/AN_Spark</code> where Jupyter or IPython Notebooks are stored:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># cd to  /home/an/spark/spark-1.5.0-bin-hadoop2.6/examples/AN_Spark</strong></span>
<span class="strong"><strong># launch command using python 2.7 and the spark-csv package:</strong></span>
<span class="strong"><strong>$ IPYTHON_OPTS='notebook' /home/an/spark/spark-1.5.0-bin-hadoop2.6/bin/pyspark --packages com.databricks:spark-csv_2.11:1.2.0</strong></span>

<span class="strong"><strong># launch command using python 3.4 and the spark-csv package:</strong></span>
<span class="strong"><strong>$ IPYTHON_OPTS='notebook' PYSPARK_PYTHON=python3</strong></span>
<span class="strong"><strong> /home/an/spark/spark-1.5.0-bin-hadoop2.6/bin/pyspark --packages com.databricks:spark-csv_2.11:1.2.0</strong></span>
</pre></div></div></div>
<div class="section" title="Building our first app with PySpark"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec12"/>Building our first app with PySpark</h1></div></div></div><p>We are <a id="id76" class="indexterm"/>ready to check now that everything is working<a id="id77" class="indexterm"/> fine. The obligatory word count will be put to the test in processing a word count on the first chapter of this book.</p><p>The code we will be running is listed here:</p><div class="informalexample"><pre class="programlisting"># Word count on 1st Chapter of the Book using PySpark

# import regex module
import re
# import add from operator module
from operator import add


# read input file
file_in = sc.textFile('/home/an/Documents/A00_Documents/Spark4Py 20150315')

# count lines
print('number of lines in file: %s' % file_in.count())

# add up lengths of each line
chars = file_in.map(lambda s: len(s)).reduce(add)
print('number of characters in file: %s' % chars)

# Get words from the input file
words =file_in.flatMap(lambda line: re.split('\W+', line.lower().strip()))
# words of more than 3 characters
words = words.filter(lambda x: len(x) &gt; 3)
# set count 1 per word
words = words.map(lambda w: (w,1))
# reduce phase - sum count all the words
words = words.reduceByKey(add)</pre></div><p>In this program, we are first reading the file from the directory <code class="literal">/home/an/Documents/A00_Documents/Spark4Py 20150315</code> into <code class="literal">file_in</code>.</p><p>We are then introspecting the file by counting the number of lines and the number of characters per line.</p><p>We are splitting the input file in to words and getting them in lower case. For our word count purpose, we are choosing words longer than three characters in order to avoid shorter and much more frequent words such as <span class="emphasis"><em>the</em></span>, <span class="emphasis"><em>and</em></span>, <span class="emphasis"><em>for</em></span> to skew the count in their favor. Generally, they <a id="id78" class="indexterm"/>are considered stop words <a id="id79" class="indexterm"/>and should be filtered out in any language processing task.</p><p>At this stage, we are getting ready for the MapReduce steps. To each word, we map a value of <code class="literal">1</code> and reduce it by summing all the unique words.</p><p>Here are illustrations of the code in the IPython Notebook. The first 10 cells are preprocessing the word count on the dataset, which is retrieved from the local file directory.</p><div class="mediaobject"><img src="graphics/B03968_01_09.jpg" alt="Building our first app with PySpark"/></div><p>Swap<a id="id80" class="indexterm"/> the word count tuples in the format <code class="literal">(count, word)</code> in<a id="id81" class="indexterm"/> order to sort by <code class="literal">count</code>, which is now the primary key of the tuple:</p><div class="informalexample"><pre class="programlisting"># create tuple (count, word) and sort in descending
words = words.map(lambda x: (x[1], x[0])).sortByKey(False)

# take top 20 words by frequency
words.take(20)</pre></div><p>In order<a id="id82" class="indexterm"/> to display our result, we are creating the tuple <code class="literal">(count, word)</code> and <a id="id83" class="indexterm"/>displaying the top 20 most frequently used words in descending order:</p><div class="mediaobject"><img src="graphics/B03968_01_10.jpg" alt="Building our first app with PySpark"/></div><p>Let's<a id="id84" class="indexterm"/> create <a id="id85" class="indexterm"/>a histogram function:</p><div class="informalexample"><pre class="programlisting"># create function for histogram of most frequent words

% matplotlib inline
import matplotlib.pyplot as plt
#

def histogram(words):
    count = map(lambda x: x[1], words)
    word = map(lambda x: x[0], words)
    plt.barh(range(len(count)), count,color = 'grey')
    plt.yticks(range(len(count)), word)

# Change order of tuple (word, count) from (count, word) 
words = words.map(lambda x:(x[1], x[0]))
words.take(25)

# display histogram
histogram(words.take(25))</pre></div><p>Here, we visualize the most frequent words by plotting them in a bar chart. We have to first swap the tuple from the original <code class="literal">(count, word)</code> to <code class="literal">(word, count)</code>:</p><div class="mediaobject"><img src="graphics/B03968_01_11.jpg" alt="Building our first app with PySpark"/></div><p>So here <a id="id86" class="indexterm"/>you have it: the most frequent words used<a id="id87" class="indexterm"/> in the first chapter are <span class="strong"><strong>Spark</strong></span>, followed by <span class="strong"><strong>Data</strong></span> and <span class="strong"><strong>Anaconda</strong></span>.</p></div>
<div class="section" title="Virtualizing the environment with Vagrant"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec13"/>Virtualizing the environment with Vagrant</h1></div></div></div><p>In order <a id="id88" class="indexterm"/>to create a portable Python and Spark <a id="id89" class="indexterm"/>environment that can be easily shared and <a id="id90" class="indexterm"/>cloned, the development environment can be built with a <code class="literal">vagrantfile</code>.</p><p>We will point to the <span class="strong"><strong>Massive Open Online Courses</strong></span> (<span class="strong"><strong>MOOCs</strong></span>) delivered by <span class="emphasis"><em>Berkeley University and Databricks</em></span>:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>Introduction to Big Data with Apache Spark, Professor Anthony D. Joseph</em></span> can be <a id="id91" class="indexterm"/>found at <a class="ulink" href="https://www.edx.org/course/introduction-big-data-apache-spark-uc-berkeleyx-cs100-1x">https://www.edx.org/course/introduction-big-data-apache-spark-uc-berkeleyx-cs100-1x</a></li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>Scalable Machine Learning, Professor</em></span> <span class="emphasis"><em>Ameet Talwalkar</em></span> can be found at <a class="ulink" href="https://www.edx.org/course/scalable-machine-learning-uc-berkeleyx-cs190-1x">https://www.edx.org/course/scalable-machine-learning-uc-berkeleyx-cs190-1x</a></li></ul></div><p>The course labs were executed on IPython Notebooks powered by PySpark. They can be found in the following GitHub repository: <a class="ulink" href="https://github.com/spark-mooc/mooc-setup/">https://github.com/spark-mooc/mooc-setup/</a>.</p><p>Once you<a id="id92" class="indexterm"/> have set up Vagrant on your machine, follow these instructions to get started: <a class="ulink" href="https://docs.vagrantup.com/v2/getting-started/index.html">https://docs.vagrantup.com/v2/getting-started/index.html</a>.</p><p>Clone the <code class="literal">spark-mooc/mooc-setup/ github</code> repository in your work directory and launch the command <code class="literal">$ vagrant up</code>, within the cloned directory:</p><p>Be aware that the version of Spark may be outdated as the <code class="literal">vagrantfile</code> may not be up-to-date.</p><p>You will see an output similar to this:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>C:\Programs\spark\edx1001\mooc-setup-master&gt;vagrant up</strong></span>
<span class="strong"><strong>Bringing machine 'sparkvm' up with 'virtualbox' provider...</strong></span>
<span class="strong"><strong>==&gt; sparkvm: Checking if box 'sparkmooc/base' is up to date...</strong></span>
<span class="strong"><strong>==&gt; sparkvm: Clearing any previously set forwarded ports...</strong></span>
<span class="strong"><strong>==&gt; sparkvm: Clearing any previously set network interfaces...</strong></span>
<span class="strong"><strong>==&gt; sparkvm: Preparing network interfaces based on configuration...</strong></span>
<span class="strong"><strong>    sparkvm: Adapter 1: nat</strong></span>
<span class="strong"><strong>==&gt; sparkvm: Forwarding ports...</strong></span>
<span class="strong"><strong>    sparkvm: 8001 =&gt; 8001 (adapter 1)</strong></span>
<span class="strong"><strong>    sparkvm: 4040 =&gt; 4040 (adapter 1)</strong></span>
<span class="strong"><strong>    sparkvm: 22 =&gt; 2222 (adapter 1)</strong></span>
<span class="strong"><strong>==&gt; sparkvm: Booting VM...</strong></span>
<span class="strong"><strong>==&gt; sparkvm: Waiting for machine to boot. This may take a few minutes...</strong></span>
<span class="strong"><strong>    sparkvm: SSH address: 127.0.0.1:2222</strong></span>
<span class="strong"><strong>    sparkvm: SSH username: vagrant</strong></span>
<span class="strong"><strong>    sparkvm: SSH auth method: private key</strong></span>
<span class="strong"><strong>    sparkvm: Warning: Connection timeout. Retrying...</strong></span>
<span class="strong"><strong>    sparkvm: Warning: Remote connection disconnect. Retrying...</strong></span>
<span class="strong"><strong>==&gt; sparkvm: Machine booted and ready!</strong></span>
<span class="strong"><strong>==&gt; sparkvm: Checking for guest additions in VM...</strong></span>
<span class="strong"><strong>==&gt; sparkvm: Setting hostname...</strong></span>
<span class="strong"><strong>==&gt; sparkvm: Mounting shared folders...</strong></span>
<span class="strong"><strong>    sparkvm: /vagrant =&gt; C:/Programs/spark/edx1001/mooc-setup-master</strong></span>
<span class="strong"><strong>==&gt; sparkvm: Machine already provisioned. Run `vagrant provision` or use the `--provision`</strong></span>
<span class="strong"><strong>==&gt; sparkvm: to force provisioning. Provisioners marked to run always will still run.</strong></span>

<span class="strong"><strong>C:\Programs\spark\edx1001\mooc-setup-master&gt;</strong></span>
</pre></div><p>This <a id="id93" class="indexterm"/>will <a id="id94" class="indexterm"/>launch the IPython Notebooks powered by PySpark on <code class="literal">localhost:8001</code>:</p><div class="mediaobject"><img src="graphics/B03968_01_12.jpg" alt="Virtualizing the environment with Vagrant"/></div></div>
<div class="section" title="Moving to the cloud"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec14"/>Moving to the cloud</h1></div></div></div><p>As we are dealing with distributed systems, an environment on a virtual machine running on a single laptop is limited for exploration and learning. We can move to the cloud in order to experience the power and scalability of the Spark distributed framework.</p><div class="section" title="Deploying apps in Amazon Web Services"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec18"/>Deploying apps in Amazon Web Services</h2></div></div></div><p>Once<a id="id95" class="indexterm"/> we <a id="id96" class="indexterm"/>are ready to scale our apps, we can migrate our development environment to <span class="strong"><strong>Amazon</strong></span>
<a id="id97" class="indexterm"/>
<span class="strong"><strong> Web Services</strong></span> (<span class="strong"><strong>AWS</strong></span>).</p><p>How to run<a id="id98" class="indexterm"/> Spark on EC2 is clearly described in the following page: <a class="ulink" href="https://spark.apache.org/docs/latest/ec2-scripts.html">https://spark.apache.org/docs/latest/ec2-scripts.html</a>.</p><p>We<a id="id99" class="indexterm"/> emphasize five key steps<a id="id100" class="indexterm"/> in setting up the AWS Spark environment:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Create<a id="id101" class="indexterm"/> an AWS EC2 key pair via the AWS console <a class="ulink" href="http://aws.amazon.com/console/">http://aws.amazon.com/console/</a>.</li><li class="listitem">Export your key pair to your environment:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>export AWS_ACCESS_KEY_ID=accesskeyid</strong></span>
<span class="strong"><strong>export AWS_SECRET_ACCESS_KEY=secretaccesskey</strong></span>
</pre></div></li><li class="listitem">Launch your cluster:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>~$ cd $SPARK_HOME/ec2</strong></span>
<span class="strong"><strong>ec2$ ./spark-ec2 -k &lt;keypair&gt; -i &lt;key-file&gt; -s &lt;num-slaves&gt; launch &lt;cluster-name&gt;</strong></span>
</pre></div></li><li class="listitem">SSH into a cluster to run Spark jobs:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>ec2$ ./spark-ec2 -k &lt;keypair&gt; -i &lt;key-file&gt; login &lt;cluster-name&gt;</strong></span>
</pre></div></li><li class="listitem">Destroy your cluster after usage:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>ec2$ ./spark-ec2 destroy &lt;cluster-name&gt;</strong></span>
</pre></div></li></ol></div></div><div class="section" title="Virtualizing the environment with Docker"><div class="titlepage"><div><div><h2 class="title"><a id="ch01lvl2sec19"/>Virtualizing the environment with Docker</h2></div></div></div><p>In order<a id="id102" class="indexterm"/> to create a portable Python and Spark<a id="id103" class="indexterm"/> environment that can be easily shared and cloned, the development environment can be built in Docker containers.</p><p>We wish capitalize on Docker's two main functions:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Creating isolated containers that can be easily deployed on different operating systems or in the cloud.</li><li class="listitem" style="list-style-type: disc">Allowing easy sharing of the development environment image with all its dependencies using The DockerHub. The DockerHub is similar to GitHub. It allows easy cloning and version control. The snapshot image of the configured environment can be the baseline for further enhancements.</li></ul></div><p>The following diagram illustrates a Docker-enabled environment with Spark, Anaconda, and the database server and their respective data volumes.</p><div class="mediaobject"><img src="graphics/B03968_01_13.jpg" alt="Virtualizing the environment with Docker"/></div><p>Docker <a id="id104" class="indexterm"/>offers the ability to clone and deploy<a id="id105" class="indexterm"/> an environment from the Dockerfile.</p><p>You can find<a id="id106" class="indexterm"/> an example Dockerfile with a PySpark and Anaconda setup at the following address: <a class="ulink" href="https://hub.docker.com/r/thisgokeboysef/pyspark-docker/~/dockerfile/">https://hub.docker.com/r/thisgokeboysef/pyspark-docker/~/dockerfile/</a>.</p><p>Install Docker as per the instructions provided at the following links:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><a class="ulink" href="http://docs.docker.com/mac/started/">http://docs.docker.com/mac/started/</a> if you are on Mac OS X</li><li class="listitem" style="list-style-type: disc"><a class="ulink" href="http://docs.docker.com/linux/started/">http://docs.docker.com/linux/started/</a> if you are on Linux</li><li class="listitem" style="list-style-type: disc"><a class="ulink" href="http://docs.docker.com/windows/started/">http://docs.docker.com/windows/started/</a> if you are on Windows</li></ul></div><p>Install the docker container with the Dockerfile provided earlier with the following command:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ docker pull thisgokeboysef/pyspark-docker</strong></span>
</pre></div><p>Other great sources of information on how to <span class="emphasis"><em>dockerize</em></span> your environment can be seen at Lab41. The GitHub repository contains the necessary code:</p><p>
<a class="ulink" href="https://github.com/Lab41/ipython-spark-docker">https://github.com/Lab41/ipython-spark-docker</a>
</p><p>The supporting blog post is rich in information on thought <a id="id107" class="indexterm"/>processes <a id="id108" class="indexterm"/>involved in building the docker environment: <a class="ulink" href="http://lab41.github.io/blog/2015/04/13/ipython-on-spark-on-docker/">http://lab41.github.io/blog/2015/04/13/ipython-on-spark-on-docker/</a>.</p></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch01lvl1sec15"/>Summary</h1></div></div></div><p>We set the context of building data-intensive apps by describing the overall architecture structured around the infrastructure, persistence, integration, analytics, and engagement layers. We also discussed Spark and Anaconda with their respective building blocks. We set up an environment in a VirtualBox with Anaconda and Spark and demonstrated a word count app using the text content of the first chapter as input.</p><p>In the next chapter, we will delve more deeply into the architecture blueprint for data-intensive apps and tap into the Twitter, GitHub, and Meetup APIs to get a feel of the data we will be mining with Spark.</p></div></body></html>