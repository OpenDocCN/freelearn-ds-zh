<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer220">
<h1 class="chapter-number" id="_idParaDest-273"><a id="_idTextAnchor280"/>8</h1>
<h1 id="_idParaDest-274"><a id="_idTextAnchor281"/>Designing Monitored Data Workﬂows</h1>
<p>Logging code is a good practice that allows developers to debug faster and provide maintenance more effectively for applications or systems. There is no strict rule when inserting logs, but knowing when not to spam your monitoring or alerting tool while using it is excellent. Creating several logging messages unnecessarily will obfuscate the instance when something significant happens. That’s why it is crucial to understand the best practices when inserting logs <span class="No-Break">into code.</span></p>
<p>This chapter will show how to create efficient and well-formatted logs using Python and PySpark for data pipelines with practical examples that can be applied in <span class="No-Break">real-world projects.</span></p>
<p>In this chapter, we have the <span class="No-Break">following recipes:</span></p>
<ul>
<li><span class="No-Break">Inserting logs</span></li>
<li>Using <span class="No-Break">log-level types</span></li>
<li>Creating <span class="No-Break">standardized logs</span></li>
<li>Monitoring our data ingest <span class="No-Break">ﬁle size</span></li>
<li>Logging based <span class="No-Break">on data</span></li>
<li>Retrieving <span class="No-Break">SparkSession metrics</span></li>
</ul>
<h1 id="_idParaDest-275"><a id="_idTextAnchor282"/>Technical requirements</h1>
<p>You can find the code from this chapter in the GitHub repository <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook"><span class="No-Break">https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-276"><a id="_idTextAnchor283"/>Inserting logs</h1>
<p>As mentioned in the introduction of this chapter, adding logging functionality to your applications is <a id="_idIndexMarker545"/>essential for debugging or making improvements later on. However, creating several log messages without necessity may generate confusion or even cause us to miss crucial alerts. In any case, knowing which kind of message to show <span class="No-Break">is indispensable.</span></p>
<p>This recipe will cover how to create helpful log messages using Python and when to <span class="No-Break">insert them.</span></p>
<h2 id="_idParaDest-277"><a id="_idTextAnchor284"/>Getting ready</h2>
<p>We will use only Python code. Make sure you have Python version 3.7 or above. You can use the following <a id="_idIndexMarker546"/>command to check it on your <strong class="bold">command-line </strong><span class="No-Break"><strong class="bold">interface</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">CLI</strong></span><span class="No-Break">):</span></p>
<pre class="source-code">
$ python3 –-version
Python 3.8.10</pre>
<p>The following code execution can be done on a Python shell or a <span class="No-Break">Jupyter notebook.</span></p>
<h2 id="_idParaDest-278"><a id="_idTextAnchor285"/>How to do it…</h2>
<p>To perform this exercise, we will make a function that reads and returns the first line of a CSV file using the best logging practices. Here is how we <span class="No-Break">do it:</span></p>
<ol>
<li>First, let’s import the libraries we will use and set the primary configuration for our <span class="No-Break"><strong class="source-inline">logging</strong></span><span class="No-Break"> library:</span><pre class="source-code">
import csv
import logging
logging.basicConfig(filename='our_application.log', level=logging.INFO)</pre></li>
</ol>
<p>Notice that we passed a filename parameter to the <strong class="source-inline">basicConfig</strong> method. Our logs will be <span class="No-Break">stored there.</span></p>
<ol>
<li value="2">Next, we will create a simple function to read and return a CSV file’s first line. Observe that <strong class="source-inline">logging.info()</strong> calls are inserted inside the functions with a message, <span class="No-Break">as follows:</span><pre class="source-code">
def get_csv_first_line (csv_file):
    logging.info(f"Starting function to read first line")
    try:
        with open(csv_file, 'r') as file:
            logging.info(f" Opening and reading the CSV file")
            reader = csv.reader(file)
            first_row = next(reader)
        return first_row
    except Exception as e:
        logging.error(f"Error when reading the CSV file: {e}")
        raise</pre></li>
<li>Then, let’s call <a id="_idIndexMarker547"/>our function, passing a CSV file as an example. Here, I will use the <strong class="source-inline">listings.csv</strong> file, which you can find in the GitHub repository <span class="No-Break">as follows:</span><pre class="source-code">
get_csv_first_line("listings.csv")</pre></li>
</ol>
<p>You should see the <span class="No-Break">following output:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer192">
<img alt="Figure 8.1 – gets_csv_first_line function output" height="466" src="image/Figure_8.01_B19453.jpg" width="402"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1 – gets_csv_first_line function output</p>
<ol>
<li value="4">Let’s check the directory where we executed our Python script or Jupyter notebook. You should see a file named <strong class="source-inline">our_application.log</strong>. If you click on it, the result should be <span class="No-Break">as follows:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer193">
<img alt="Figure 8.2 – our_application.log content" height="228" src="image/Figure_8.02_B19453.jpg" width="597"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.2 – our_application.log content</p>
<p>As you can see, we had <a id="_idIndexMarker548"/>two different outputs: one with the function results (<em class="italic">step 3</em>) and another that creates a file containing the log messages (<span class="No-Break"><em class="italic">step 4</em></span><span class="No-Break">).</span></p>
<h2 id="_idParaDest-279"><a id="_idTextAnchor286"/>How it works…</h2>
<p>Let’s start understanding how the code works by looking at the <span class="No-Break">first lines:</span></p>
<pre class="source-code">
import logging
logging.basicConfig(filename='our_application.log', level=logging.INFO)</pre>
<p>After importing the built-in logging library, we called a method named <strong class="source-inline">basicConfig()</strong>, which sets the primary configuration for the subsequent calls in our function. The <strong class="source-inline">filename</strong> parameter indicates we want to save the logs into a file, and the <strong class="source-inline">level</strong> parameter sets the log level at which we want to start seeing messages. This will be covered in more detail in the <em class="italic">Using log-level types</em> recipe later in <span class="No-Break">this chapter.</span></p>
<p>Then, we proceeded by creating our function and inserting the logging calls. Looking closely, we inserted <span class="No-Break">the following:</span></p>
<pre class="source-code">
logging.info(f"Starting function to read first line")
logging.info(f"Opening and reading the CSV file")</pre>
<p>These two logs are informative and track an action or inform us as we pass through a part of the code or <a id="_idIndexMarker549"/>module. The best practice is to keep it as clean and objective as possible, so the next person (or even yourself) can identify where to start to look to solve <span class="No-Break">a problem.</span></p>
<p>The next log informs us of an error, as you can <span class="No-Break">see here:</span></p>
<pre class="source-code">
logging.error(f"Error when reading the CSV file: {e}")</pre>
<p>The way to make the call for this error method is similar to the <strong class="source-inline">.info()</strong> ones. In this case, the best practice is to use only exception clauses and pass the error as a string function, as we did by passing the <strong class="source-inline">e</strong> variable in curly brackets. This way, even if we cannot see the Python traceback, we will store it in a file or <span class="No-Break">monitoring application.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">It is a common practice to encapsulate the exception output in a variable, as in <strong class="source-inline">except Exception as e</strong>. It allows us to control how we show or get a part of the <span class="No-Break">error message.</span></p>
<p>Since our function was executed successfully, we don’t expect to see any error message in our <strong class="source-inline">our_application.log</strong> file, as you can <span class="No-Break">see here:</span></p>
<pre class="source-code">
INFO:root:Starting function to read first line
INFO:root:Reading file</pre>
<p>If we look closely at the structure of the saved log, we will notice a pattern. The first word on each line, <strong class="source-inline">INFO</strong>, indicates the log level; after this, we see the <strong class="source-inline">root</strong> word, which indicates <a id="_idIndexMarker550"/>the logging hierarchy; and finally, we get the message we inserted into <span class="No-Break">the code.</span></p>
<p>We can optimize and format our logs in many ways, but we won’t worry about this for now. We will cover the logging hierarchy in more detail in the <em class="italic">Formatting </em><span class="No-Break"><em class="italic">logs</em></span><span class="No-Break"> recipe.</span></p>
<h2 id="_idParaDest-280"><a id="_idTextAnchor287"/>See also</h2>
<p>See more about initiating the logs in Python in the official documentation <span class="No-Break">here: </span><a href="https://docs.python.org/3/howto/logging.xhtml#logging-to-a-file"><span class="No-Break">https://docs.python.org/3/howto/logging.xhtml#logging-to-a-file</span></a></p>
<h1 id="_idParaDest-281"><a id="_idTextAnchor288"/>Using log-level types</h1>
<p>Now that we have <a id="_idIndexMarker551"/>been introduced to how and where to insert logs, let’s understand log types or levels. Each log level has its own degree of relevance inside any system. For instance, the console output does not show debug messages <span class="No-Break">by default.</span></p>
<p>We already covered how to log levels using PySpark in the <em class="italic">Inserting formatted SparkSession logs to facilitate your work recipe</em> in <a href="B19453_06.xhtml#_idTextAnchor195"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>. Now we will do the same using only Python. This recipe aims to show how to set logging levels at the beginning of your script and insert the different levels inside your code to create a hierarchy of priority for your logs. With this, you can create a structured script that allows you or your team to monitor and <span class="No-Break">identify errors.</span></p>
<h2 id="_idParaDest-282"><a id="_idTextAnchor289"/>Getting ready</h2>
<p>We will use only Python code. Make sure you have Python version 3.7 or above. You can use the following command on your CLI to check <span class="No-Break">your version:</span></p>
<pre class="source-code">
$ python3 –-version
Python 3.8.10</pre>
<p>The following code execution can be done on a Python shell or a <span class="No-Break">Jupyter notebook.</span></p>
<h2 id="_idParaDest-283"><a id="_idTextAnchor290"/>How to do it…</h2>
<p>Let’s use the same example we had in the previous, <em class="italic">Inserting logs</em> recipe, and make <span class="No-Break">some enhancements:</span></p>
<ol>
<li>Let’s start by importing the libraries and defining <strong class="source-inline">basicConfig</strong>. This time, we will set the log level <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">DEBUG</strong></span><span class="No-Break">:</span><pre class="source-code">
import csv
import logging
logging.basicConfig(filename='our_application.log', level=logging.DEBUG)</pre></li>
<li>Then, before declaring the function, we will insert a <strong class="source-inline">DEBUG</strong> log informing that we are about to test <span class="No-Break">this script:</span><pre class="source-code">
logging.debug(f"Start testing function")</pre></li>
<li>Next, as we saw in the <em class="italic">Inserting logs</em> recipe, we will build a function that reads a CSV file and <a id="_idIndexMarker552"/>returns the first line but with slight changes. Let’s insert a <strong class="source-inline">DEBUG</strong> message after the first line of the CSV is executed successfully, and a <strong class="source-inline">CRITICAL</strong> message if we enter <span class="No-Break">the exception:</span><pre class="source-code">
def gets_csv_first_line (csv_file):
    logging.info(f"Starting function to read first line")
    try:
        with open(csv_file, 'r') as file:
            logging.info(f"Reading file")
            reader = csv.reader(file)
            first_row = next(reader)
            logging.debug(f"Finished without problems")
        return first_row
    except Exception as e:
        logging.debug(f"Entered into a exception")
        logging.error(f"Error when reading the CSV file: {e}")
        logging.critical(f"This is a critical error, and the application needs to stop!")
        raise</pre></li>
<li>Finally, before <a id="_idIndexMarker553"/>we make the call to the function, let’s insert a warning message informing that we are about to <span class="No-Break">start it:</span><pre class="source-code">
logging.warning(f"Starting the function to get the first line of a CSV")
gets_csv_first_line("listings.csv")</pre></li>
<li>After calling the function, you should see the following output in the <span class="No-Break"><strong class="source-inline">our_application.log</strong></span><span class="No-Break"> file:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer194">
<img alt="Figure 8.3 – our_application.log updated with different log levels" height="272" src="image/Figure_8.03_B19453.jpg" width="820"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.3 – our_application.log updated with different log levels</p>
<p>It informs us the function was correctly executed and no <span class="No-Break">error occurred.</span></p>
<ol>
<li value="6">Let’s now simulate an error. You should now see the following message inside the <span class="No-Break"><strong class="source-inline">our_application.log</strong></span><span class="No-Break"> file:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer195">
<img alt="Figure 8.4 – our_application.log showing an ERROR message" height="169" src="image/Figure_8.04_B19453.jpg" width="924"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.4 – our_application.log showing an ERROR message</p>
<p>As you can see, we entered the exception, and we can see the <strong class="source-inline">ERROR</strong> and <span class="No-Break"><strong class="source-inline">CRITICAL</strong></span><span class="No-Break"> messages.</span></p>
<h2 id="_idParaDest-284"><a id="_idTextAnchor291"/>How it works…</h2>
<p>Although it may seem irrelevant, we have made beneficial improvements to our function. Each log level <a id="_idIndexMarker554"/>corresponds to a different degree of criticality relating to what is happening. Let’s take a look at the following figure , which shows the weight of <span class="No-Break">each level:</span></p>
<p class="IMG---Figure"> </p>
<div>
<div class="IMG---Figure" id="_idContainer196">
<img alt="Figure 8.5 – Diagram of log level weight according to criticality" height="192" src="image/Figure_8.05_B19453.jpg" width="924"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.5 – Diagram of log level weight according to criticality</p>
<p>Depending on where the log level is inserted, it can prevent the script from continuing and creating a chain of errors since we can add different error handlers according to <span class="No-Break">the level.</span></p>
<p>By default, the Python logging library is configured to show messages only from the <strong class="bold">WARNING</strong> level upward. With this, <strong class="bold">DEBUG</strong> and <strong class="bold">INFO</strong> messages will not be displayed or saved. That’s why, at the beginning of our script, we redefined this initial level to <strong class="source-inline">DEBUG</strong>, as you can <span class="No-Break">see here:</span></p>
<pre class="source-code">
logging.basicConfig(filename='our_application.log', level=logging.DEBUG)</pre>
<p>The purpose of showing only <strong class="bold">WARNING</strong> messages and above is to avoid spamming the console output or a log file with unnecessary system information. In the following figure, you can see how Python internally organizes its <span class="No-Break">log levels:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer197">
<img alt="Figure 8.6 – Log level detailed description and when it’s best to use them (source: https://docs.python.org/3/howto/logging.xhtml)" height="357" src="image/Figure_8.06_B19453.jpg" width="1311"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.6 – Log level detailed description and when it’s best to use them (source: <a href="https://docs.python.org/3/howto/logging.xhtml">https://docs.python.org/3/howto/logging.xhtml</a>)</p>
<p>You can use this <a id="_idIndexMarker555"/>table as a reference when setting your log messages inside your code. It can also be found in the official Python documentation <span class="No-Break">at </span><a href="https://docs.python.org/3/howto/logging.xhtml#when-to-use-logging"><span class="No-Break">https://docs.python.org/3/howto/logging.xhtml#when-to-use-logging</span></a><span class="No-Break">.</span></p>
<p>In this recipe, we tried to cover all the log severity levels to demonstrate the recommended places to insert them. Even though it may seem like simple stuff, knowing when each level should be used makes all the difference and brings maturity to <span class="No-Break">your application.</span></p>
<h2 id="_idParaDest-285"><a id="_idTextAnchor292"/>There’s more…</h2>
<p>Usually, each language has its structured form of logging levels. However, there is an <em class="italic">agreement</em> in the software engineering world about how the levels should be used. The following figure shows <a id="_idIndexMarker556"/>a fantastic decision diagram created by <em class="italic">Taco Jan Osinga</em> about the behavior of logging levels at the <strong class="bold">Operating System </strong>(<span class="No-Break"><strong class="bold">OS</strong></span><span class="No-Break">) level.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer198">
<img alt="Figure 8.7 – Decision diagram of log levels by Taco Jan Osinga (source: https://stackoverflow.com/users/3476764/taco-jan-osinga?tab=profile)" height="680" src="image/Figure_8.07_B19453.jpg" width="972"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.7 – Decision diagram of log levels by Taco Jan Osinga (source: https://stackoverflow.com/users/3476764/taco-jan-osinga?tab=profile)</p>
<h2 id="_idParaDest-286"><a id="_idTextAnchor293"/>See also</h2>
<p>For more detailed information about the foundations of Python logs, refer to the official <span class="No-Break">documentation:</span><a href="https://docs.python.org/3/howto/logging.xhtml"><span class="No-Break"> https://docs.python.org/3/howto/logging.xhtml</span></a></p>
<h1 id="_idParaDest-287"><a id="_idTextAnchor294"/>Creating standardized logs</h1>
<p>Now that we know <a id="_idIndexMarker557"/>the best practices for inserting logs and using log levels, we can add more relevant information to our logs to help us monitor our code. Information such as date and time or the module or function executed helps us determine where an issue occurred or where improvements <span class="No-Break">are required.</span></p>
<p>Creating standardized formatting for application logs or (in our case) data pipeline logs makes the debugging process more manageable, and there are a variety of ways to do this. One way of doing it is to create <strong class="source-inline">.ini</strong> or <strong class="source-inline">.conf</strong> files that hold the configuration on how the logs will be formatted and applied to our wider Python code, <span class="No-Break">for instance.</span></p>
<p>In this recipe, we will learn how to create a configuration file that will dictate how the logs will be formatted across the code and shown in the <span class="No-Break">execution output.</span></p>
<h2 id="_idParaDest-288"><a id="_idTextAnchor295"/>Getting ready</h2>
<p>Let’s use the same code as the previous <em class="italic">Using log-level types</em> recipe, but with <span class="No-Break">more improvements!</span></p>
<p>You can use the following code to follow the steps of this recipe in a new file or notebook, or <a id="_idIndexMarker558"/>reuse the function from the <em class="italic">Using log-level types</em> recipe. I prefer to make a copy so the first piece of code is <span class="No-Break">left intact:</span></p>
<pre class="source-code">
Def gets_csv_first_line(csv_file):
    logger.debug(f"Start testing function")
    logger.info(f"Starting function to read first line")
    try:
        with open(csv_file, 'r') as file:
            logger.info(f"Reading file")
            reader = csv.reader(file)
            first_row = next(reader)
            logger.debug(f"Finished without problems")
        return first_row
    except Exception as e:
        logger.debug(f"Entered into a exception")
        logger.error(f"Error when reading the CSV file: {e}")
        logger.critical(f"This is a critical error, and the application needs to stop!")
        raise</pre>
<h2 id="_idParaDest-289"><a id="_idTextAnchor296"/>How to do it…</h2>
<p>Here are the steps to perform <span class="No-Break">this recipe:</span></p>
<ol>
<li>To start our exercise, let’s create a file called <strong class="source-inline">logging.conf</strong>. My recommendation is to store it in the same location as your Python scripts. However, feel free to keep it somewhere else, but do remember we will need the file’s <span class="No-Break">path later.</span></li>
<li>Next, paste the <a id="_idIndexMarker559"/>following code inside the <strong class="source-inline">logging.conf</strong> file and <span class="No-Break">save it:</span><pre class="source-code">
[loggers]
keys=root,data_ingest
[handlers]
keys=fileHandler, consoleHandler
[formatters]
keys=logFormatter
[logger_root]
level=DEBUG
handlers=fileHandler
[logger_data_ingest]
level=DEBUG
handlers=fileHandler, consoleHandler
qualname=data_ingest
propagate=0
[handler_consoleHandler]
class=StreamHandler
level=DEBUG
formatter=logFormatter
args=(sys.stdout,)
[handler_fileHandler]
class=FileHandler
level=DEBUG
formatter=logFormatter
args=('data_ingest.log', 'a')
[formatter_logFormatter]
format=%(asctime)s - %(name)s - %(levelname)s - %(message)s</pre></li>
<li>Then, insert the following <strong class="source-inline">import</strong> statements, the <strong class="source-inline">config.fileConfig()</strong> method, and <a id="_idIndexMarker560"/>the <strong class="source-inline">logger</strong> variable before the <strong class="source-inline">gets_csv_first_line()</strong> function, as you can <span class="No-Break">see here:</span><pre class="source-code">
import csv
import logging
from logging import config
# Loading configuration file
config.fileConfig("logging.conf")
# Creates a log configuration
logger = logging.getLogger("data_ingest")
def gets_csv_first_line(csv_file):
    …</pre></li>
</ol>
<p>Observe that we are passing <strong class="source-inline">logging.conf</strong> as a parameter for the <strong class="source-inline">config.fileConfig()</strong> method. Pass the whole path if you stored it in a different directory level of your <span class="No-Break">Python script.</span></p>
<ol>
<li value="4">Now, let’s call our function by passing a CSV file. As usual, I will use the <strong class="source-inline">listings.csv</strong> file for <span class="No-Break">this exercise:</span><pre class="source-code">
gets_csv_first_line("listings."sv")</pre></li>
</ol>
<p>You should see the following output in your notebook cell or <span class="No-Break">Python shell:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer199">
<img alt="Figure 8.8 – Console output with formatted logs" height="110" src="image/Figure_8.08_B19453.jpg" width="980"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.8 – Console output with formatted logs</p>
<ol>
<li value="5">Then, check <a id="_idIndexMarker561"/>your directory. You should see a file named <strong class="source-inline">data_ingest.log</strong>. Open it, and you should see something like the <span class="No-Break">following screenshot:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer200">
<img alt="Figure 8.9 – The data_ingest.log file with formatted logs" height="280" src="image/Figure_8.09_B19453.jpg" width="1025"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.9 – The data_ingest.log file with formatted logs</p>
<p>As you can observe, we created a standardized log format for both console and file output. Let’s now understand how we did it in the <span class="No-Break">next section.</span></p>
<h2 id="_idParaDest-290"><a id="_idTextAnchor297"/>How it works…</h2>
<p>Before jumping into the code, let’s first understand what a configuration file is. <strong class="bold">Configuration files</strong>, commonly <a id="_idIndexMarker562"/>with a <strong class="source-inline">.conf</strong> or <strong class="source-inline">.ini</strong> file extension, offer a useful way to create customized applications to interact with other applications. You can find some of them inside your OS in the <strong class="source-inline">/etc</strong> or <strong class="source-inline">/</strong><span class="No-Break"><strong class="source-inline">var</strong></span><span class="No-Break"> directories.</span></p>
<p>Our case is no different. At the beginning of our recipe, we created a configuration file called <strong class="source-inline">logging.conf</strong> that holds the pattern for the Python logs we will apply across <span class="No-Break">our application.</span></p>
<p>Now, let’s take a <a id="_idIndexMarker563"/>look inside the <strong class="source-inline">logging.conf</strong> file. Looking closely, it is possible to see some values inside square brackets. Let’s start with the first three, as you can <span class="No-Break">see here:</span></p>
<pre class="source-code">
[loggers]
[handlers]
[formatters]</pre>
<p>These parameters are modular components of the Python logging library, allowing easy customization due to their detachment from each other. In short, they represent <span class="No-Break">the following:</span></p>
<ul>
<li>Loggers are used by the code and expose the interface for itself. By default, there is a <strong class="source-inline">root</strong> logger used by Python. For new loggers, we use the <span class="No-Break"><strong class="source-inline">key</strong></span><span class="No-Break"> argument.</span></li>
<li>Handlers send the logs to the configured destination. In our case, we created two: <strong class="source-inline">fileHandler</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">consoleHandler</strong></span><span class="No-Break">.</span></li>
<li>Formatters create a layout for the <span class="No-Break">log records.</span></li>
</ul>
<p>After declaring the basic parameters, we inserted two customized loggers and handlers, as you can observe in the following piece <span class="No-Break">of code:</span></p>
<pre class="source-code">
[logger_root]
[logger_data_ingest]
[handler_consoleHandler]
[handler_fileHandler]</pre>
<p>Creating a customization for the <strong class="source-inline">root</strong> <strong class="source-inline">Logger</strong> is not mandatory, but here we wanted to change the default log level to <strong class="source-inline">DEBUG</strong> and always send it to <strong class="source-inline">fileHandler</strong>. For <strong class="source-inline">logger_data_ingest</strong>, we also <span class="No-Break">passed </span><span class="No-Break"><strong class="source-inline">consoleHandler</strong></span><span class="No-Break">.</span></p>
<p>Speaking of handlers, they have a fundamental role here. Although they share the same log level and <strong class="source-inline">Formatter</strong>, they inherit different classes. The <strong class="source-inline">StreamHandler</strong> class catches the log records, and with <strong class="source-inline">args=(sys.stdout,)</strong> it gets all the system outputs for display in the console output. <strong class="source-inline">FileHandler</strong> works similarly, saving all the results at the <strong class="source-inline">DEBUG</strong> level <span class="No-Break">and above.</span></p>
<p>Finally, <strong class="source-inline">Formatter</strong> dictates<a id="_idIndexMarker564"/> how the log will be displayed. There are many ways to set the format, even passing the line of the code where the log was executed. You can see all the possible attributes <span class="No-Break">at </span><span class="No-Break">https://docs.python.org/3/library/logging.xhtml#logrecord-attributes</span><span class="No-Break">.</span></p>
<p>The official Python documentation has an excellent diagram, shown in the following figure, that outlines the relationship between these modifiers and another one we didn’t cover here, <span class="No-Break">called </span><span class="No-Break"><strong class="source-inline">Filter</strong></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer201">
<img alt="Figure 8.10 – Logging flow diagram (source: https://docs.python.org/3/howto/logging.xhtml#logging-flow)" height="758" src="image/Figure_8.10_B19453.jpg" width="955"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.10 – Logging flow diagram (source: https://docs.python.org/3/howto/logging.xhtml#logging-flow)</p>
<p>In our exercise, we created a simple logger handler called <strong class="source-inline">data_ingest</strong> along with the <strong class="source-inline">gets_csv_first_line()</strong> function. Now, imagine how it could be expanded through a whole application <a id="_idIndexMarker565"/>or system. Using a single configuration file, we can create several patterns with different applications for different scripts or ETL phases. Let’s take a look at the first lines of <span class="No-Break">our code:</span></p>
<pre class="source-code">
(...)
config.fileConfig("logging.conf")
logger = logging.getLogger("data_ingest")
(...)</pre>
<p><strong class="source-inline">config.fileConfig()</strong> loads the configuration file and <strong class="source-inline">logging.getLogger()</strong> loads the <strong class="source-inline">Logger</strong> instance to use. It will <a id="_idIndexMarker566"/>use the <strong class="source-inline">root</strong> as default if it doesn’t find the <span class="No-Break">proper </span><span class="No-Break"><strong class="source-inline">Logger</strong></span><span class="No-Break">.</span></p>
<p>Software engineers commonly use this best practice in a real-world application to avoid code redundancy and create a <span class="No-Break">centralized solution.</span></p>
<h2 id="_idParaDest-291"><a id="_idTextAnchor298"/>There’s more…</h2>
<p>There are <a id="_idIndexMarker567"/>some other acceptable file formats with which to create log configurations. For example, we can use a <strong class="bold">YAML Ain’t Markup Language</strong> (<strong class="bold">YAML</strong>) file or a <span class="No-Break">Python dictionary.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer202">
<img alt="Figure 8.11 – Configuration file formatting with YAML (source: https://docs.python.org/3/howto/logging.xhtml#configuring-logging)" height="516" src="image/Figure_8.11_B19453.jpg" width="861"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.11 – Configuration file formatting with YAML (source: https://docs.python.org/3/howto/logging.xhtml#configuring-logging)</p>
<p>If you want to know more about using the <strong class="source-inline">logging.config</strong> package to create improved <a id="_idIndexMarker568"/>YAML or dictionary configurations, refer to the official documentation <span class="No-Break">here: </span><a href="https://docs.python.org/3/library/logging.config.xhtml#logging-config-api"><span class="No-Break">https://docs.python.org/3/library/logging.config.xhtml#logging-config-api</span></a></p>
<h2 id="_idParaDest-292"><a id="_idTextAnchor299"/>See also</h2>
<p>To read and <a id="_idIndexMarker569"/>understand more about how handlers work, refer to the official documentation <span class="No-Break">here: </span><a href="https://docs.python.org/3/library/logging.handlers.xhtml"><span class="No-Break">https://docs.python.org/3/library/logging.handlers.xhtml</span></a></p>
<h1 id="_idParaDest-293"><a id="_idTextAnchor300"/>Monitoring our data ingest ﬁle size</h1>
<p>When ingesting data, we can track a few items to ensure the incoming information is what we expect. One <a id="_idIndexMarker570"/>of the most important of these items is the data size we are ingesting, which can mean file size or the size of chunks of <span class="No-Break">streaming data.</span></p>
<p>Logging the size of incoming data allows the creation of intelligent and efficient monitoring. If at some point the size of incoming data diverges from what is expected, we can take action to investigate and resolve <span class="No-Break">the issue.</span></p>
<p>In this recipe, we will create simple Python code that logs the size of ingested files, which is very valuable in <span class="No-Break">data monitoring.</span></p>
<h2 id="_idParaDest-294"><a id="_idTextAnchor301"/>Getting ready</h2>
<p>We will use only Python code. Make sure you have Python version 3.7 or above. You can use the following command on your CLI to check <span class="No-Break">your version:</span></p>
<pre class="source-code">
$ python3 –-version
Python 3.8.10</pre>
<p>The following code execution can be done using a Python shell or a <span class="No-Break">Jupyter notebook.</span></p>
<h2 id="_idParaDest-295"><a id="_idTextAnchor302"/>How to do it…</h2>
<p>This exercise will create a simple Python function to read a file path and return its size in bytes by default. If we want to return the value in megabytes, we only need to pass the input parameter <span class="No-Break">as </span><span class="No-Break"><strong class="source-inline">True</strong></span><span class="No-Break">:</span></p>
<ol>
<li>Let’s start by importing the <span class="No-Break"><strong class="source-inline">os</strong></span><span class="No-Break"> library:</span><pre class="source-code">
import os</pre></li>
<li>Then, we declare our function that requires a file path, along with an optional parameter to convert the size <span class="No-Break">to megabytes:</span><pre class="source-code">
def get_file_size(file_name, s_megabytes=False):</pre></li>
<li>Let’s use <strong class="source-inline">os.stat()</strong> to retrieve information from <span class="No-Break">the file:</span><pre class="source-code">
    file_stats = os.stat(file_name)</pre></li>
<li>Since it is optional, we can create an <strong class="source-inline">if</strong> condition to convert the <strong class="source-inline">bytes</strong> value to <strong class="source-inline">megabytes</strong>. If not flagged as <strong class="source-inline">True</strong>, we return the value in <strong class="source-inline">bytes</strong>, as you can see in the <span class="No-Break">following code:</span><pre class="source-code">
    if s_megabytes:
        return f"The file size in megabytes is: {file_stats.st_size / (1024 * 1024)}"
    return f"The file size in bytes is: {file_stats.st_size}"</pre></li>
<li>Finally, let’s <a id="_idIndexMarker571"/>call our function, passing a dataset we <span class="No-Break">already used:</span><pre class="source-code">
file_name = "listings.csv"
get_file_size(file_name)</pre></li>
</ol>
<p>For the <strong class="source-inline">listings.csv</strong> file, you should see the <span class="No-Break">following output:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer203">
<img alt="Figure 8.12 – File size in bytes" height="34" src="image/Figure_8.12_B19453.jpg" width="417"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.12 – File size in bytes</p>
<p>If we execute it by passing <strong class="source-inline">s_megabytes</strong> as <strong class="source-inline">True</strong>, we will see the <span class="No-Break">following output:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer204">
<img alt="Figure 8.13 – File size in megabytes" height="35" src="image/Figure_8.13_B19453.jpg" width="598"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.13 – File size in megabytes</p>
<p>Feel free to test it using any file path on your machine and check whether the size is the same as that indicated in the <span class="No-Break">console output.</span></p>
<h2 id="_idParaDest-296"><a id="_idTextAnchor303"/>How it works…</h2>
<p>A file’s size estimation is convenient when working with data. Let’s understand the pieces of code we used to achieve <span class="No-Break">this estimation.</span></p>
<p>The first operation we used was the <strong class="source-inline">os.stat()</strong> method to retrieve information about the file, as you can <span class="No-Break">see here:</span></p>
<pre class="source-code">
file_stats = os.stat(file_name)</pre>
<p>This method <a id="_idIndexMarker572"/>interacts directly with your OS. If we execute it in isolation, we will have the following output for the <span class="No-Break"><strong class="source-inline">listings.csv</strong></span><span class="No-Break"> file:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer205">
<img alt="Figure 8.14  – Attributes of the listings.csv file when using os.stat_result" height="84" src="image/Figure_8.14_B19453.jpg" width="804"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.14 – Attributes of the listings.csv file when using os.stat_result</p>
<p>In our case, we only need <strong class="source-inline">st_size</strong> to bring the <strong class="source-inline">bytes</strong> estimation, so we called it later in the <strong class="source-inline">return</strong> clause <span class="No-Break">as follows:</span></p>
<pre class="source-code">
file_stats.st_size</pre>
<p>If you want to know more about the other results shown, you can refer to the official documentation page <span class="No-Break">here: </span><a href="https://docs.python.org/3/library/stat.xhtml#stat.filemode"><span class="No-Break">https://docs.python.org/3/library/stat.xhtml#stat.filemode</span></a></p>
<p>Lastly, to provide the result in megabytes, we only need to do a simple conversion using the <strong class="source-inline">st_size</strong> value, where 1 KB is 1,024 bytes, and 1 MB is equal to 1,024 KB. You can see the conversion <span class="No-Break">formula here:</span></p>
<pre class="source-code">
file_stats.st_size / (1024 * 1024)</pre>
<h2 id="_idParaDest-297"><a id="_idTextAnchor304"/>There’s more…</h2>
<p>This recipe showed how easy it is to create a Python function that retrieves the file size. Unfortunately, at the time of writing, there was no straightforward solution to perform the same thing <span class="No-Break">using PySpark.</span></p>
<p>Glenn Franxman, a software engineer, proposed on his GitHub a workaround solution using Spark internals to estimate the size of a DataFrame. You can see his code on his GitHub at the following link – make sure to give the proper credits if you do use <span class="No-Break">it: </span><a href="https://gist.github.com/gfranxman/4fd0719ff2618039182dd7ea1a702f8e"><span class="No-Break">https://gist.github.com/gfranxman/4fd0719ff2618039182dd7ea1a702f8e</span></a></p>
<p>Let’s use Glenn’s code in <a id="_idIndexMarker573"/>an example to estimate the DataFrame size and see how <span class="No-Break">it works:</span></p>
<pre class="source-code">
<strong class="bold">from pyspark.serializers import PickleSerializer, AutoBatchedSerializer</strong>
<strong class="bold">def _to_java_object_rdd(rdd):</strong>
<strong class="bold">    """ Return a JavaRDD of Object by unpickling</strong>
<strong class="bold">    It will convert each Python object into Java object by Pyrolite, whenever the</strong>
<strong class="bold">    RDD is serialized in batch or not.</strong>
<strong class="bold">    """</strong>
<strong class="bold">    rdd = rdd._reserialize(AutoBatchedSerializer(PickleSerializer()))</strong>
<strong class="bold">    return rdd.ctx._jvm.org.apache.spark.mllib.api.python.SerDe.pythonToJava(rdd._jrdd, True)</strong>
<strong class="bold">def estimate_df_size(df):</strong>
<strong class="bold">    JavaObj = _to_java_object_rdd(df.rdd)</strong>
<strong class="bold">    nbytes = spark._jvm.org.apache.spark.util.SizeEstimator.estimate(JavaObj)</strong>
<strong class="bold">    </strong><strong class="bold">return nbytes</strong></pre>
<p>To execute the preceding code, you must have a SparkSession initiated. Once you have this and a DataFrame, execute the code and call the <strong class="source-inline">estimate_df_size()</strong> function <span class="No-Break">as follows:</span></p>
<pre class="source-code">
<strong class="bold">estimate_df_size(df)</strong></pre>
<p>You should see the following output in bytes, depending on which DataFrame you <span class="No-Break">are using:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer206">
<img alt="Figure 8.15 – DataFrame size in bytes" height="28" src="image/Figure_8.15_B19453.jpg" width="593"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.15 – DataFrame size in bytes</p>
<p>Remember that <a id="_idIndexMarker574"/>this solution will only work if you pass a DataFrame as a parameter. Our Python code works well for other file estimations and doesn’t have performance issues when estimating <span class="No-Break">big files.</span></p>
<h2 id="_idParaDest-298"><a id="_idTextAnchor305"/>See also</h2>
<p>Unlike PySpark, Scala <a id="_idIndexMarker575"/>has a <strong class="source-inline">SizeEstimator</strong> function to return the size of a DataFrame. You can find more <span class="No-Break">here: </span><a href="https://spark.apache.org/docs/latest/api/java/index.xhtml?org/apache/spark/util/SizeEstimator.xhtml"><span class="No-Break">https://spark.apache.org/docs/latest/api/java/index.xhtml?org/apache/spark/util/SizeEstimator.xhtml</span></a></p>
<h1 id="_idParaDest-299"><a id="_idTextAnchor306"/>Logging based on data</h1>
<p>As mentioned in the <em class="italic">Monitoring our data ingest ﬁle size</em> recipe, logging our ingest is a good practice in the <a id="_idIndexMarker576"/>data field. There are several ways to explore our ingestion logs to increase the process’s reliability and our confidence in it. In this recipe, we will start to get into the data operations field (or <strong class="bold">DataOps</strong>), where the goal is to track the behavior of data from the source until it reaches its <span class="No-Break">final destination.</span></p>
<p>This recipe will explore other metrics we can track to create a reliable <span class="No-Break">data pipeline.</span></p>
<h2 id="_idParaDest-300"><a id="_idTextAnchor307"/>Getting ready</h2>
<p>For this exercise, let’s imagine we have two simple data ingests, one from a database and another from an API. Since this is a straightforward pipeline, let’s visualize it with the <span class="No-Break">following diagram:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer207">
<img alt="Figure 8.16 – Data ingestion phases" height="399" src="image/Figure_8.16_B19453.jpg" width="666"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.16 – Data ingestion phases</p>
<p>With this in mind, let’s <a id="_idIndexMarker577"/>explore the instances we can log to make <span class="No-Break">monitoring efficient.</span></p>
<h2 id="_idParaDest-301"><a id="_idTextAnchor308"/>How to do it…</h2>
<p>Let’s define the essential metrics based on each layer (or step) we saw in the <span class="No-Break">preceding diagram:</span></p>
<ol>
<li><strong class="bold">Data sources</strong>: Let’s <a id="_idIndexMarker578"/>start with the first layer of the ingestion—the sources. Knowing we are handling two different data sources, we must create additional metrics for them. See the following figure <span class="No-Break">for reference:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer208">
<img alt="Figure 8.17 – Database metrics to monitor" height="269" src="image/Figure_8.17_B19453.jpg" width="665"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.17 – Database metrics to monitor</p>
<ol>
<li value="2"><strong class="bold">Ingestion</strong>: Now that the source is logged and monitored, let’s move on to the ingestion layer. As we <a id="_idIndexMarker579"/>saw previously in this chapter, we can log information such as errors, informative parts of the code execution, file size, and so on. Let’s insert more content here, such as the schema and the time taken to retrieve or process data. We will end up with a diagram similar to <span class="No-Break">the following:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer209">
<img alt="Figure 8.18 – Data ingestion metrics to monitor" height="302" src="image/Figure_8.18_B19453.jpg" width="809"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.18 – Data ingestion metrics to monitor</p>
<ol>
<li value="3"><strong class="bold">Staging layer</strong>: Lastly, let’s cover the final layer after ingestion. The goal here is to ensure we <a id="_idIndexMarker580"/>maintain the integrity of the data, so verifying whether the schema still matches the data is crucial. We can also add logs to monitor the number of Parquet files and their sizes. See the following figure <span class="No-Break">for reference:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer210">
<img alt="Figure 8.19 – Staging layer metrics to monitor" height="246" src="image/Figure_8.19_B19453.jpg" width="887"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.19 – Staging layer metrics to monitor</p>
<p>Now that we have <a id="_idIndexMarker581"/>covered the essential topics to be monitored, let’s understand why they <span class="No-Break">were chosen.</span></p>
<h2 id="_idParaDest-302"><a id="_idTextAnchor309"/>How it works…</h2>
<p>Since the first recipe of this chapter, we have perpetually reinforced how logs are relevant to getting your system to work correctly. Here, we put it all together to see, albeit from a high level, how storing specific information can help us <span class="No-Break">with monitoring.</span></p>
<p>Starting with the data source layer, the metrics chosen were based on the response of the data and the availability to retrieve data. Understanding whether we can begin the ingestion process is fundamental, and even more important is knowing whether the data size is what <span class="No-Break">we expect.</span></p>
<p>Imagine the following scenario: every day, we ingest 50 MB of data from an API. One day, however, we received 10 KB. With proper logging and monitoring functionalities, we can quickly review the issue in terms of historic events. We can expand the data size check to the subsequent layers we covered in <span class="No-Break">the recipe.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">We purposely intercalate the words “step” and “layer” when referring to the phases of the ingestion process since it can vary in different works of literature and in different companies’ <span class="No-Break">internal processes.</span></p>
<p>Another way to log and <a id="_idIndexMarker582"/>monitor our data is by using schema validation. <strong class="bold">Schema validation</strong> (when applicable) guarantees that nothing has changed at the <a id="_idIndexMarker583"/>source. Therefore, the results for transformation or aggregation tend to be linear. We can also implement an auxiliary function or job to check <a id="_idIndexMarker584"/>that fields containing <strong class="bold">sensitive</strong> or <strong class="bold">Personally Identifiable Information</strong> (<strong class="bold">PII</strong>) are <span class="No-Break">adequately anonymized.</span></p>
<p>Monitoring the <em class="italic">parquet file size or count</em> is crucial to verify that quality is being maintained. As seen in <a href="B19453_07.xhtml#_idTextAnchor227"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, the number of parquet files can interfere with other applications’ reading quality or even the ETL’s <span class="No-Break">subsequent phases.</span></p>
<p>Finally, it is essential to point out that we covered logs here to ensure the quality and reliability of our data ingestion. Remember that the best practice is to align the records we got from the code with the examples we <span class="No-Break">saw here.</span></p>
<h2 id="_idParaDest-303"><a id="_idTextAnchor310"/>There’s more…</h2>
<p>The content of this recipe is part of a more extensive subject called <strong class="bold">data observability</strong>. Data observability is a union of data operations, quality, and governance. The objective is to <a id="_idIndexMarker585"/>centralize everything to make the management and monitoring of data processes efficient and reliable, bringing health <span class="No-Break">to data.</span></p>
<p>We will discuss this further in <a href="B19453_12.xhtml#_idTextAnchor433"><span class="No-Break"><em class="italic">Chapter 12</em></span></a>. However, if you are curious about the topic, Databand (an IBM company) has a good introduction <span class="No-Break">at </span><a href="https://databand.ai/data-observability/"><span class="No-Break">https://databand.ai/data-observability/</span></a><span class="No-Break">.</span></p>
<h2 id="_idParaDest-304"><a id="_idTextAnchor311"/>See also</h2>
<p>Find out more <a id="_idIndexMarker586"/>about monitoring ETL pipelines at the DataGaps blog page, <span class="No-Break">here: </span><a href="https://www.datagaps.com/blog/monitoring-your-etl-test-data-pipelines-in-production-dataops-suite/"><span class="No-Break">https://www.datagaps.com/blog/monitoring-your-etl-test-data-pipelines-in-production-dataops-suite/</span></a></p>
<h1 id="_idParaDest-305"><a id="_idTextAnchor312"/>Retrieving SparkSession metrics</h1>
<p>Until now, we created <a id="_idIndexMarker587"/>our logs to provide more information and be more useful for monitoring. Logging allows us to build customized metrics based on the necessity of our pipeline and code. However, we can also take advantage of built-in metrics from frameworks and <span class="No-Break">programming languages.</span></p>
<p>When we create a <strong class="source-inline">SparkSession</strong>, it provides a web UI with useful metrics that can be used to monitor our pipelines. Using this, the following recipe shows you how to access and retrieve metric information from SparkSession, and use it as a tool when ingesting or processing <span class="No-Break">a DataFrame.</span></p>
<h2 id="_idParaDest-306"><a id="_idTextAnchor313"/>Getting ready</h2>
<p>You can execute this recipe using the PySpark command line or the <span class="No-Break">Jupyter Notebook.</span></p>
<p>Before exploring the Spark UI metrics, let’s create a simple <strong class="source-inline">SparkSession</strong> using the <span class="No-Break">following code:</span></p>
<pre class="source-code">
from pyspark.sql import SparkSession
spark = SparkSession.builder \
      .master("local[1]") \
      .appName("chapter8_monitoring") \
      .config("spark.executor.memory", '3g') \
      .config("spark.executor.cores", '3') \
      .config("spark.cores.max", '3') \
      .getOrCreate()</pre>
<p>Then, let’s read a JSON file and call the <strong class="source-inline">.show()</strong> method <span class="No-Break">as follows:</span></p>
<pre class="source-code">
df_json = spark.read.option("multiline","true") \
                    .json('github_events.json')
df_json.show()</pre>
<p>I am using a dataset <a id="_idIndexMarker588"/>called <strong class="source-inline">github_events.json</strong>, which we worked with previously in <a href="B19453_04.xhtml#_idTextAnchor127"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>. However, feel free to use whatever you prefer, since the objective here is not to observe the schema of the dataset, but to see what we can find out from the <span class="No-Break">Spark UI.</span></p>
<h2 id="_idParaDest-307"><a id="_idTextAnchor314"/>How to do it…</h2>
<ol>
<li>With the <strong class="source-inline">SparkSession</strong> initiated as outlined in the <em class="italic">Getting ready</em> section, we can use the <strong class="source-inline">spark</strong> command to retrieve a link to the Spark UI, <span class="No-Break">as follows:</span><pre class="source-code">
<strong class="bold">spark</strong></pre></li>
</ol>
<p>You should see the <span class="No-Break">following output:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer211">
<img alt="Figure 8.20 – spark command output" height="299" src="image/Figure_8.20_B19453.jpg" width="276"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.20 – spark command output</p>
<ol>
<li value="2">Click on <strong class="bold">Spark UI</strong>, and your browser will open a new tab. You should see a page <span class="No-Break">like this:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer212">
<img alt="Figure 8.21 – Spark UI: Jobs page view" height="813" src="image/Figure_8.21_B19453.jpg" width="1563"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.21 – Spark UI: Jobs page view</p>
<p>If you are not <a id="_idIndexMarker589"/>using the Jupyter Notebook, you can access this interface by pointing your browser <span class="No-Break">to </span><a href="http://localhost:4040/"><span class="No-Break">http://localhost:4040/</span></a><span class="No-Break">.</span></p>
<p>My page looks more crowded because I expanded <strong class="bold">Event Timeline</strong> and <strong class="bold">Completed Jobs</strong> – you can do the same by clicking <span class="No-Break">on them.</span></p>
<ol>
<li value="3">Next, let’s explore the first completed job further. Click on <strong class="bold">showString at NativeMethodAccessorImpl.java:0</strong> and you should see the <span class="No-Break">following page:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer213">
<img alt="Figure 8.22 – Spark UI: Stage page view for a specific job" height="922" src="image/Figure_8.22_B19453.jpg" width="1561"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.22 – Spark UI: Stage page view for a specific job</p>
<p>Here, we can see <a id="_idIndexMarker590"/>the task status of this job in more detail, covering things such as how much memory it used, the time taken to execute it, and <span class="No-Break">so on.</span></p>
<p>Note also that it switched to the <strong class="bold">Stages</strong> tab at the <span class="No-Break">top menu.</span></p>
<ol>
<li value="4">Now, click on the <strong class="bold">Executors</strong> button at the top of the page. You should see a page similar <span class="No-Break">to this:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer214">
<img alt="Figure 8.23 – Spark UI: Executors page view" height="561" src="image/Figure_8.23_B19453.jpg" width="1550"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.23 – Spark UI: Executors page view</p>
<p>All the metrics <a id="_idIndexMarker591"/>here are related to the Spark drivers <span class="No-Break">and nodes.</span></p>
<ol>
<li value="5">Then, click on the <strong class="bold">SQL</strong> button in the top menu. You should see the <span class="No-Break">following page:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer215">
<img alt="Figure 8.24 – Spark UI: SQL page view" height="1048" src="image/Figure_8.24_B19453.jpg" width="1533"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.24 – Spark UI: SQL page view</p>
<p>On this page, it is possible to see the queries executed by Spark internally. If we used an explicit query <a id="_idIndexMarker592"/>in our code, we would see here how it was <span class="No-Break">performed internally.</span></p>
<p>You don’t need to worry about the <strong class="bold">Details</strong> menu being expanded. The objective is to show that it is possible to track Spark’s actions when executing the <strong class="source-inline">.</strong><span class="No-Break"><strong class="source-inline">show()</strong></span><span class="No-Break"> method.</span></p>
<h2 id="_idParaDest-308"><a id="_idTextAnchor315"/>How it works…</h2>
<p>Now that we have explored Spark UI, let’s understand how each tab is organized and some of the steps we did <span class="No-Break">with them.</span></p>
<p>In <em class="italic">step 2</em>, we had a first glance at the interface. This interface makes it possible to see an event timeline with information about when the driver was created and executed. Also, we can observe the jobs marked on the timeline, <span class="No-Break">as follows:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer216">
<img alt="Figure 8.25 – Detailed view of the Event Timeline expanded menu" height="323" src="image/Figure_8.25_B19453.jpg" width="1555"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.25 – Detailed view of the Event Timeline expanded menu</p>
<p>We can observe how they interact when working with bigger jobs and more complex parallel tasks. Unfortunately, we would need a dedicated project and several datasets to simulate this, but <a id="_idIndexMarker593"/>you now know where to look for <span class="No-Break">future reference.</span></p>
<p>Then, we selected <strong class="bold">showString at NativeMethodAccessorImpl.java:0</strong>, which redirected us to the <strong class="bold">Stages</strong> page. This page offers more detailed information about Spark’s tasks, whether the task was successful <span class="No-Break">or not.</span></p>
<p>An excellent metric and visualization tool is <strong class="bold">DAG Visualization</strong> (referring to directed acyclic graphs), which can be expanded and will show something <span class="No-Break">like this:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer217">
<img alt="Figure 8.26 – DAG Visualization of a job" height="790" src="image/Figure_8.26_B19453.jpg" width="455"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.26 – DAG Visualization of a job</p>
<p>This offers an excellent overview of each step performed at each stage. We can also consult this to understand which part was problematic in the event of an error based on the <span class="No-Break">traceback message.</span></p>
<p>Since we selected <a id="_idIndexMarker594"/>a specific task (or job), it showed its stages and details. However, we can display all the steps executed if we go directly to <strong class="bold">Stages</strong>. Doing so, you should see something <span class="No-Break">like this:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer218">
<img alt="Figure 8.27 – Spark UI: Stages overview with all jobs executed" height="618" src="image/Figure_8.27_B19453.jpg" width="1551"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.27 – Spark UI: Stages overview with all jobs executed</p>
<p>Although the description messages are not straightforward, we can get the gist of what each of them is doing. <strong class="source-inline">Stage id 0</strong> refers to the reading JSON function, and <strong class="source-inline">Stage id 1</strong> with the <strong class="source-inline">showString</strong> message refers to the <strong class="source-inline">.show()</strong> <span class="No-Break">method call.</span></p>
<p>The <strong class="bold">Executors</strong> page shows <a id="_idIndexMarker595"/>the metrics related to the core of Spark and how it is performing. You can use this information to understand your cluster’s behavior and whether any tuning is needed. For more detailed information about each field, refer to the Spark official documentation <span class="No-Break">at </span><span class="No-Break">https://spark.apache.org/docs/latest/monitoring.xhtml#executor-metrics</span><span class="No-Break">.</span></p>
<p>Last but not least, we saw the <strong class="bold">SQL</strong> page, where it was possible to see how Spark internally shuffles and aggregates the data behind the scenes, like <strong class="bold">Stages</strong>, taking advantage of a more visual form of execution, as you can see in the <span class="No-Break">following screenshot:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer219">
<img alt="Figure 8.28 – Flow diagram of the SQL query internally executed" height="662" src="image/Figure_8.28_B19453.jpg" width="614"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.28 – Flow diagram of the SQL query internally executed</p>
<p>Here, we can see <a id="_idIndexMarker596"/>that the query is related to the <strong class="source-inline">.show()</strong> method. There is helpful information inside it, including the number of output rows, the files read, and <span class="No-Break">their sizes.</span></p>
<h2 id="_idParaDest-309"><a id="_idTextAnchor316"/>There’s more…</h2>
<p>Even though Spark metrics are handy, you might wonder how to use them when hosting your PySpark jobs on cloud providers such as AWS or <span class="No-Break">Google Cloud.</span></p>
<p><strong class="bold">AWS</strong> provides a <a id="_idIndexMarker597"/>simple solution to enable Spark UI when using <strong class="bold">AWS Glue</strong>. You can find out more about it <span class="No-Break">at </span><a href="https://docs.aws.amazon.com/glue/latest/dg/monitor-spark-ui-jobs.xhtml"><span class="No-Break">https://docs.aws.amazon.com/glue/latest/dg/monitor-spark-ui-jobs.xhtml</span></a><span class="No-Break">.</span></p>
<p><strong class="bold">Google Data Proc</strong> provides a web <a id="_idIndexMarker598"/>interface for its cluster, where you can also see metrics for <strong class="bold">Hadoop</strong> and <strong class="bold">YARN</strong>. Since Spark runs on top of YARN, you won’t find a link directly for Spark UI, but you can use the YARN interface to access it. You can find out more <span class="No-Break">here: </span><a href="https://cloud.google.com/dataproc/docs/concepts/accessing/cluster-web-interfaces"><span class="No-Break">https://cloud.google.com/dataproc/docs/concepts/accessing/cluster-web-interfaces</span></a><span class="No-Break">.</span></p>
<h2 id="_idParaDest-310"><a id="_idTextAnchor317"/>See also</h2>
<p><em class="italic">Towards Data Science</em> has <a id="_idIndexMarker599"/>a fantastic article about Spark <span class="No-Break">metrics: </span><span class="No-Break">https://towardsdatascience.com/monitoring-of-spark-applications-3ca0c271c4e0</span></p>
<h1 id="_idParaDest-311"><a id="_idTextAnchor318"/>Further reading</h1>
<ul>
<li><span class="No-Break">https://spark.apache.org/docs/latest/monitoring.xhtml#executor-task-metrics</span></li>
<li><a href="https://developer.here.com/documentation/metrics-and-logs/user_guide/topics/spark-ui.xhtml"><span class="No-Break">https://developer.here.com/documentation/metrics-and-logs/user_guide/topics/spark-ui.xhtml</span></a></li>
<li><a href="https://github.com/LucaCanali/Miscellaneous/blob/master/Spark_Notes/Spark_TaskMetrics.md"><span class="No-Break">https://github.com/LucaCanali/Miscellaneous/blob/master/Spark_Notes/Spark_TaskMetrics.md</span></a></li>
<li><a href="https://docs.python.org/3/howto/logging.xhtml"><span class="No-Break">https://docs.python.org/3/howto/logging.xhtml</span></a></li>
<li><a href="http://datadoghq.com/blog/python-logging-best-practices/"><span class="No-Break">datadoghq.com/blog/python-logging-best-practices/</span></a></li>
<li><a href="https://coralogix.com/blog/python-logging-best-practices-tips/"><span class="No-Break">https://coralogix.com/blog/python-logging-best-practices-tips/</span></a></li>
</ul>
</div>
</div></body></html>