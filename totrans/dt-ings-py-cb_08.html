<html><head></head><body>
<div><div><h1 class="chapter-number" id="_idParaDest-273"><a id="_idTextAnchor280"/>8</h1>
<h1 id="_idParaDest-274"><a id="_idTextAnchor281"/>Designing Monitored Data Workﬂows</h1>
<p>Logging code is a good practice that allows developers to debug faster and provide maintenance more effectively for applications or systems. There is no strict rule when inserting logs, but knowing when not to spam your monitoring or alerting tool while using it is excellent. Creating several logging messages unnecessarily will obfuscate the instance when something significant happens. That’s why it is crucial to understand the best practices when inserting logs into code.</p>
<p>This chapter will show how to create efficient and well-formatted logs using Python and PySpark for data pipelines with practical examples that can be applied in real-world projects.</p>
<p>In this chapter, we have the following recipes:</p>
<ul>
<li>Inserting logs</li>
<li>Using log-level types</li>
<li>Creating standardized logs</li>
<li>Monitoring our data ingest ﬁle size</li>
<li>Logging based on data</li>
<li>Retrieving SparkSession metrics</li>
</ul>
<h1 id="_idParaDest-275"><a id="_idTextAnchor282"/>Technical requirements</h1>
<p>You can find the code from this chapter in the GitHub repository at <a href="https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook">https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook</a>.</p>
<h1 id="_idParaDest-276"><a id="_idTextAnchor283"/>Inserting logs</h1>
<p>As mentioned in the introduction of this chapter, adding logging functionality to your applications is <a id="_idIndexMarker545"/>essential for debugging or making improvements later on. However, creating several log messages without necessity may generate confusion or even cause us to miss crucial alerts. In any case, knowing which kind of message to show is indispensable.</p>
<p>This recipe will cover how to create helpful log messages using Python and when to insert them.</p>
<h2 id="_idParaDest-277"><a id="_idTextAnchor284"/>Getting ready</h2>
<p>We will use only Python code. Make sure you have Python version 3.7 or above. You can use the following <a id="_idIndexMarker546"/>command to check it on your <strong class="bold">command-line </strong><strong class="bold">interface</strong> (<strong class="bold">CLI</strong>):</p>
<pre class="source-code">
$ python3 –-version
Python 3.8.10</pre>
<p>The following code execution can be done on a Python shell or a Jupyter notebook.</p>
<h2 id="_idParaDest-278"><a id="_idTextAnchor285"/>How to do it…</h2>
<p>To perform this exercise, we will make a function that reads and returns the first line of a CSV file using the best logging practices. Here is how we do it:</p>
<ol>
<li>First, let’s import the libraries we will use and set the primary configuration for our <code>logging</code> library:<pre class="source-code">
import csv
import logging
logging.basicConfig(filename='our_application.log', level=logging.INFO)</pre></li>
</ol>
<p>Notice that we passed a filename parameter to the <code>basicConfig</code> method. Our logs will be stored there.</p>
<ol>
<li value="2">Next, we will create a simple function to read and return a CSV file’s first line. Observe that <code>logging.info()</code> calls are inserted inside the functions with a message, as follows:<pre class="source-code">
def get_csv_first_line (csv_file):
    logging.info(f"Starting function to read first line")
    try:
        with open(csv_file, 'r') as file:
            logging.info(f" Opening and reading the CSV file")
            reader = csv.reader(file)
            first_row = next(reader)
        return first_row
    except Exception as e:
        logging.error(f"Error when reading the CSV file: {e}")
        raise</pre></li>
<li>Then, let’s call <a id="_idIndexMarker547"/>our function, passing a CSV file as an example. Here, I will use the <code>listings.csv</code> file, which you can find in the GitHub repository as follows:<pre class="source-code">
get_csv_first_line("listings.csv")</pre></li>
</ol>
<p>You should see the following output:</p>
<div><div><img alt="Figure 8.1 – gets_csv_first_line function output" height="466" src="img/Figure_8.01_B19453.jpg" width="402"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1 – gets_csv_first_line function output</p>
<ol>
<li value="4">Let’s check the directory where we executed our Python script or Jupyter notebook. You should see a file named <code>our_application.log</code>. If you click on it, the result should be as follows:</li>
</ol>
<div><div><img alt="Figure 8.2 – our_application.log content" height="228" src="img/Figure_8.02_B19453.jpg" width="597"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.2 – our_application.log content</p>
<p>As you can see, we had <a id="_idIndexMarker548"/>two different outputs: one with the function results (<em class="italic">step 3</em>) and another that creates a file containing the log messages (<em class="italic">step 4</em>).</p>
<h2 id="_idParaDest-279"><a id="_idTextAnchor286"/>How it works…</h2>
<p>Let’s start understanding how the code works by looking at the first lines:</p>
<pre class="source-code">
import logging
logging.basicConfig(filename='our_application.log', level=logging.INFO)</pre>
<p>After importing the built-in logging library, we called a method named <code>basicConfig()</code>, which sets the primary configuration for the subsequent calls in our function. The <code>filename</code> parameter indicates we want to save the logs into a file, and the <code>level</code> parameter sets the log level at which we want to start seeing messages. This will be covered in more detail in the <em class="italic">Using log-level types</em> recipe later in this chapter.</p>
<p>Then, we proceeded by creating our function and inserting the logging calls. Looking closely, we inserted the following:</p>
<pre class="source-code">
logging.info(f"Starting function to read first line")
logging.info(f"Opening and reading the CSV file")</pre>
<p>These two logs are informative and track an action or inform us as we pass through a part of the code or <a id="_idIndexMarker549"/>module. The best practice is to keep it as clean and objective as possible, so the next person (or even yourself) can identify where to start to look to solve a problem.</p>
<p>The next log informs us of an error, as you can see here:</p>
<pre class="source-code">
logging.error(f"Error when reading the CSV file: {e}")</pre>
<p>The way to make the call for this error method is similar to the <code>.info()</code> ones. In this case, the best practice is to use only exception clauses and pass the error as a string function, as we did by passing the <code>e</code> variable in curly brackets. This way, even if we cannot see the Python traceback, we will store it in a file or monitoring application.</p>
<p class="callout-heading">Note</p>
<p class="callout">It is a common practice to encapsulate the exception output in a variable, as in <code>except Exception as e</code>. It allows us to control how we show or get a part of the error message.</p>
<p>Since our function was executed successfully, we don’t expect to see any error message in our <code>our_application.log</code> file, as you can see here:</p>
<pre class="source-code">
INFO:root:Starting function to read first line
INFO:root:Reading file</pre>
<p>If we look closely at the structure of the saved log, we will notice a pattern. The first word on each line, <code>INFO</code>, indicates the log level; after this, we see the <code>root</code> word, which indicates <a id="_idIndexMarker550"/>the logging hierarchy; and finally, we get the message we inserted into the code.</p>
<p>We can optimize and format our logs in many ways, but we won’t worry about this for now. We will cover the logging hierarchy in more detail in the <em class="italic">Formatting </em><em class="italic">logs</em> recipe.</p>
<h2 id="_idParaDest-280"><a id="_idTextAnchor287"/>See also</h2>
<p>See more about initiating the logs in Python in the official documentation here: <a href="https://docs.python.org/3/howto/logging.xhtml#logging-to-a-file">https://docs.python.org/3/howto/logging.xhtml#logging-to-a-file</a></p>
<h1 id="_idParaDest-281"><a id="_idTextAnchor288"/>Using log-level types</h1>
<p>Now that we have <a id="_idIndexMarker551"/>been introduced to how and where to insert logs, let’s understand log types or levels. Each log level has its own degree of relevance inside any system. For instance, the console output does not show debug messages by default.</p>
<p>We already covered how to log levels using PySpark in the <em class="italic">Inserting formatted SparkSession logs to facilitate your work recipe</em> in <a href="B19453_06.xhtml#_idTextAnchor195"><em class="italic">Chapter 6</em></a>. Now we will do the same using only Python. This recipe aims to show how to set logging levels at the beginning of your script and insert the different levels inside your code to create a hierarchy of priority for your logs. With this, you can create a structured script that allows you or your team to monitor and identify errors.</p>
<h2 id="_idParaDest-282"><a id="_idTextAnchor289"/>Getting ready</h2>
<p>We will use only Python code. Make sure you have Python version 3.7 or above. You can use the following command on your CLI to check your version:</p>
<pre class="source-code">
$ python3 –-version
Python 3.8.10</pre>
<p>The following code execution can be done on a Python shell or a Jupyter notebook.</p>
<h2 id="_idParaDest-283"><a id="_idTextAnchor290"/>How to do it…</h2>
<p>Let’s use the same example we had in the previous, <em class="italic">Inserting logs</em> recipe, and make some enhancements:</p>
<ol>
<li>Let’s start by importing the libraries and defining <code>basicConfig</code>. This time, we will set the log level to <code>DEBUG</code>:<pre class="source-code">
import csv
import logging
logging.basicConfig(filename='our_application.log', level=logging.DEBUG)</pre></li>
<li>Then, before declaring the function, we will insert a <code>DEBUG</code> log informing that we are about to test this script:<pre class="source-code">
logging.debug(f"Start testing function")</pre></li>
<li>Next, as we saw in the <em class="italic">Inserting logs</em> recipe, we will build a function that reads a CSV file and <a id="_idIndexMarker552"/>returns the first line but with slight changes. Let’s insert a <code>DEBUG</code> message after the first line of the CSV is executed successfully, and a <code>CRITICAL</code> message if we enter the exception:<pre class="source-code">
def gets_csv_first_line (csv_file):
    logging.info(f"Starting function to read first line")
    try:
        with open(csv_file, 'r') as file:
            logging.info(f"Reading file")
            reader = csv.reader(file)
            first_row = next(reader)
            logging.debug(f"Finished without problems")
        return first_row
    except Exception as e:
        logging.debug(f"Entered into a exception")
        logging.error(f"Error when reading the CSV file: {e}")
        logging.critical(f"This is a critical error, and the application needs to stop!")
        raise</pre></li>
<li>Finally, before <a id="_idIndexMarker553"/>we make the call to the function, let’s insert a warning message informing that we are about to start it:<pre class="source-code">
logging.warning(f"Starting the function to get the first line of a CSV")
gets_csv_first_line("listings.csv")</pre></li>
<li>After calling the function, you should see the following output in the <code>our_application.log</code> file:</li>
</ol>
<div><div><img alt="Figure 8.3 – our_application.log updated with different log levels" height="272" src="img/Figure_8.03_B19453.jpg" width="820"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.3 – our_application.log updated with different log levels</p>
<p>It informs us the function was correctly executed and no error occurred.</p>
<ol>
<li value="6">Let’s now simulate an error. You should now see the following message inside the <code>our_application.log</code> file:</li>
</ol>
<div><div><img alt="Figure 8.4 – our_application.log showing an ERROR message" height="169" src="img/Figure_8.04_B19453.jpg" width="924"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.4 – our_application.log showing an ERROR message</p>
<p>As you can see, we entered the exception, and we can see the <code>ERROR</code> and <code>CRITICAL</code> messages.</p>
<h2 id="_idParaDest-284"><a id="_idTextAnchor291"/>How it works…</h2>
<p>Although it may seem irrelevant, we have made beneficial improvements to our function. Each log level <a id="_idIndexMarker554"/>corresponds to a different degree of criticality relating to what is happening. Let’s take a look at the following figure , which shows the weight of each level:</p>
<p class="IMG---Figure"> </p>
<div><div><img alt="Figure 8.5 – Diagram of log level weight according to criticality" height="192" src="img/Figure_8.05_B19453.jpg" width="924"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.5 – Diagram of log level weight according to criticality</p>
<p>Depending on where the log level is inserted, it can prevent the script from continuing and creating a chain of errors since we can add different error handlers according to the level.</p>
<p>By default, the Python logging library is configured to show messages only from the <code>DEBUG</code>, as you can see here:</p>
<pre class="source-code">
logging.basicConfig(filename='our_application.log', level=logging.DEBUG)</pre>
<p>The purpose of showing only <strong class="bold">WARNING</strong> messages and above is to avoid spamming the console output or a log file with unnecessary system information. In the following figure, you can see how Python internally organizes its log levels:</p>
<div><div><img alt="Figure 8.6 – Log level detailed description and when it’s best to use them (source: https://docs.python.org/3/howto/logging.xhtml)" height="357" src="img/Figure_8.06_B19453.jpg" width="1311"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.6 – Log level detailed description and when it’s best to use them (source: <a href="https://docs.python.org/3/howto/logging.xhtml">https://docs.python.org/3/howto/logging.xhtml</a>)</p>
<p>You can use this <a id="_idIndexMarker555"/>table as a reference when setting your log messages inside your code. It can also be found in the official Python documentation at <a href="https://docs.python.org/3/howto/logging.xhtml#when-to-use-logging">https://docs.python.org/3/howto/logging.xhtml#when-to-use-logging</a>.</p>
<p>In this recipe, we tried to cover all the log severity levels to demonstrate the recommended places to insert them. Even though it may seem like simple stuff, knowing when each level should be used makes all the difference and brings maturity to your application.</p>
<h2 id="_idParaDest-285"><a id="_idTextAnchor292"/>There’s more…</h2>
<p>Usually, each language has its structured form of logging levels. However, there is an <em class="italic">agreement</em> in the software engineering world about how the levels should be used. The following figure shows <a id="_idIndexMarker556"/>a fantastic decision diagram created by <em class="italic">Taco Jan Osinga</em> about the behavior of logging levels at the <strong class="bold">Operating System </strong>(<strong class="bold">OS</strong>) level.</p>
<div><div><img alt="Figure 8.7 – Decision diagram of log levels by Taco Jan Osinga (source: https://stackoverflow.com/users/3476764/taco-jan-osinga?tab=profile)" height="680" src="img/Figure_8.07_B19453.jpg" width="972"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.7 – Decision diagram of log levels by Taco Jan Osinga (source: https://stackoverflow.com/users/3476764/taco-jan-osinga?tab=profile)</p>
<h2 id="_idParaDest-286"><a id="_idTextAnchor293"/>See also</h2>
<p>For more detailed information about the foundations of Python logs, refer to the official documentation:<a href="https://docs.python.org/3/howto/logging.xhtml"> https://docs.python.org/3/howto/logging.xhtml</a></p>
<h1 id="_idParaDest-287"><a id="_idTextAnchor294"/>Creating standardized logs</h1>
<p>Now that we know <a id="_idIndexMarker557"/>the best practices for inserting logs and using log levels, we can add more relevant information to our logs to help us monitor our code. Information such as date and time or the module or function executed helps us determine where an issue occurred or where improvements are required.</p>
<p>Creating standardized formatting for application logs or (in our case) data pipeline logs makes the debugging process more manageable, and there are a variety of ways to do this. One way of doing it is to create <code>.ini</code> or <code>.conf</code> files that hold the configuration on how the logs will be formatted and applied to our wider Python code, for instance.</p>
<p>In this recipe, we will learn how to create a configuration file that will dictate how the logs will be formatted across the code and shown in the execution output.</p>
<h2 id="_idParaDest-288"><a id="_idTextAnchor295"/>Getting ready</h2>
<p>Let’s use the same code as the previous <em class="italic">Using log-level types</em> recipe, but with more improvements!</p>
<p>You can use the following code to follow the steps of this recipe in a new file or notebook, or <a id="_idIndexMarker558"/>reuse the function from the <em class="italic">Using log-level types</em> recipe. I prefer to make a copy so the first piece of code is left intact:</p>
<pre class="source-code">
Def gets_csv_first_line(csv_file):
    logger.debug(f"Start testing function")
    logger.info(f"Starting function to read first line")
    try:
        with open(csv_file, 'r') as file:
            logger.info(f"Reading file")
            reader = csv.reader(file)
            first_row = next(reader)
            logger.debug(f"Finished without problems")
        return first_row
    except Exception as e:
        logger.debug(f"Entered into a exception")
        logger.error(f"Error when reading the CSV file: {e}")
        logger.critical(f"This is a critical error, and the application needs to stop!")
        raise</pre>
<h2 id="_idParaDest-289"><a id="_idTextAnchor296"/>How to do it…</h2>
<p>Here are the steps to perform this recipe:</p>
<ol>
<li>To start our exercise, let’s create a file called <code>logging.conf</code>. My recommendation is to store it in the same location as your Python scripts. However, feel free to keep it somewhere else, but do remember we will need the file’s path later.</li>
<li>Next, paste the <a id="_idIndexMarker559"/>following code inside the <code>logging.conf</code> file and save it:<pre class="source-code">
[loggers]
keys=root,data_ingest
[handlers]
keys=fileHandler, consoleHandler
[formatters]
keys=logFormatter
[logger_root]
level=DEBUG
handlers=fileHandler
[logger_data_ingest]
level=DEBUG
handlers=fileHandler, consoleHandler
qualname=data_ingest
propagate=0
[handler_consoleHandler]
class=StreamHandler
level=DEBUG
formatter=logFormatter
args=(sys.stdout,)
[handler_fileHandler]
class=FileHandler
level=DEBUG
formatter=logFormatter
args=('data_ingest.log', 'a')
[formatter_logFormatter]
format=%(asctime)s - %(name)s - %(levelname)s - %(message)s</pre></li>
<li>Then, insert the following <code>import</code> statements, the <code>config.fileConfig()</code> method, and <a id="_idIndexMarker560"/>the <code>logger</code> variable before the <code>gets_csv_first_line()</code> function, as you can see here:<pre class="source-code">
import csv
import logging
from logging import config
# Loading configuration file
config.fileConfig("logging.conf")
# Creates a log configuration
logger = logging.getLogger("data_ingest")
def gets_csv_first_line(csv_file):
    …</pre></li>
</ol>
<p>Observe that we are passing <code>logging.conf</code> as a parameter for the <code>config.fileConfig()</code> method. Pass the whole path if you stored it in a different directory level of your Python script.</p>
<ol>
<li value="4">Now, let’s call our function by passing a CSV file. As usual, I will use the <code>listings.csv</code> file for this exercise:<pre class="source-code">
gets_csv_first_line("listings."sv")</pre></li>
</ol>
<p>You should see the following output in your notebook cell or Python shell:</p>
<div><div><img alt="Figure 8.8 – Console output with formatted logs" height="110" src="img/Figure_8.08_B19453.jpg" width="980"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.8 – Console output with formatted logs</p>
<ol>
<li value="5">Then, check <a id="_idIndexMarker561"/>your directory. You should see a file named <code>data_ingest.log</code>. Open it, and you should see something like the following screenshot:</li>
</ol>
<div><div><img alt="Figure 8.9 – The data_ingest.log file with formatted logs" height="280" src="img/Figure_8.09_B19453.jpg" width="1025"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.9 – The data_ingest.log file with formatted logs</p>
<p>As you can observe, we created a standardized log format for both console and file output. Let’s now understand how we did it in the next section.</p>
<h2 id="_idParaDest-290"><a id="_idTextAnchor297"/>How it works…</h2>
<p>Before jumping into the code, let’s first understand what a configuration file is. <code>.conf</code> or <code>.ini</code> file extension, offer a useful way to create customized applications to interact with other applications. You can find some of them inside your OS in the <code>/etc</code> or <code>/</code><code>var</code> directories.</p>
<p>Our case is no different. At the beginning of our recipe, we created a configuration file called <code>logging.conf</code> that holds the pattern for the Python logs we will apply across our application.</p>
<p>Now, let’s take a <a id="_idIndexMarker563"/>look inside the <code>logging.conf</code> file. Looking closely, it is possible to see some values inside square brackets. Let’s start with the first three, as you can see here:</p>
<pre class="source-code">
[loggers]
[handlers]
[formatters]</pre>
<p>These parameters are modular components of the Python logging library, allowing easy customization due to their detachment from each other. In short, they represent the following:</p>
<ul>
<li>Loggers are used by the code and expose the interface for itself. By default, there is a <code>root</code> logger used by Python. For new loggers, we use the <code>key</code> argument.</li>
<li>Handlers send the logs to the configured destination. In our case, we created two: <code>fileHandler</code> and <code>consoleHandler</code>.</li>
<li>Formatters create a layout for the log records.</li>
</ul>
<p>After declaring the basic parameters, we inserted two customized loggers and handlers, as you can observe in the following piece of code:</p>
<pre class="source-code">
[logger_root]
[logger_data_ingest]
[handler_consoleHandler]
[handler_fileHandler]</pre>
<p>Creating a customization for the <code>root</code> <code>Logger</code> is not mandatory, but here we wanted to change the default log level to <code>DEBUG</code> and always send it to <code>fileHandler</code>. For <code>logger_data_ingest</code>, we also passed <code>consoleHandler</code>.</p>
<p>Speaking of handlers, they have a fundamental role here. Although they share the same log level and <code>Formatter</code>, they inherit different classes. The <code>StreamHandler</code> class catches the log records, and with <code>args=(sys.stdout,)</code> it gets all the system outputs for display in the console output. <code>FileHandler</code> works similarly, saving all the results at the <code>DEBUG</code> level and above.</p>
<p>Finally, <code>Formatter</code> dictates<a id="_idIndexMarker564"/> how the log will be displayed. There are many ways to set the format, even passing the line of the code where the log was executed. You can see all the possible attributes at https://docs.python.org/3/library/logging.xhtml#logrecord-attributes.</p>
<p>The official Python documentation has an excellent diagram, shown in the following figure, that outlines the relationship between these modifiers and another one we didn’t cover here, called <code>Filter</code>.</p>
<div><div><img alt="Figure 8.10 – Logging flow diagram (source: https://docs.python.org/3/howto/logging.xhtml#logging-flow)" height="758" src="img/Figure_8.10_B19453.jpg" width="955"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.10 – Logging flow diagram (source: https://docs.python.org/3/howto/logging.xhtml#logging-flow)</p>
<p>In our exercise, we created a simple logger handler called <code>data_ingest</code> along with the <code>gets_csv_first_line()</code> function. Now, imagine how it could be expanded through a whole application <a id="_idIndexMarker565"/>or system. Using a single configuration file, we can create several patterns with different applications for different scripts or ETL phases. Let’s take a look at the first lines of our code:</p>
<pre class="source-code">
(...)
config.fileConfig("logging.conf")
logger = logging.getLogger("data_ingest")
(...)</pre>
<p><code>config.fileConfig()</code> loads the configuration file and <code>logging.getLogger()</code> loads the <code>Logger</code> instance to use. It will <a id="_idIndexMarker566"/>use the <code>root</code> as default if it doesn’t find the proper <code>Logger</code>.</p>
<p>Software engineers commonly use this best practice in a real-world application to avoid code redundancy and create a centralized solution.</p>
<h2 id="_idParaDest-291"><a id="_idTextAnchor298"/>There’s more…</h2>
<p>There are <a id="_idIndexMarker567"/>some other acceptable file formats with which to create log configurations. For example, we can use a <strong class="bold">YAML Ain’t Markup Language</strong> (<strong class="bold">YAML</strong>) file or a Python dictionary.</p>
<div><div><img alt="Figure 8.11 – Configuration file formatting with YAML (source: https://docs.python.org/3/howto/logging.xhtml#configuring-logging)" height="516" src="img/Figure_8.11_B19453.jpg" width="861"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.11 – Configuration file formatting with YAML (source: https://docs.python.org/3/howto/logging.xhtml#configuring-logging)</p>
<p>If you want to know more about using the <code>logging.config</code> package to create improved <a id="_idIndexMarker568"/>YAML or dictionary configurations, refer to the official documentation here: <a href="https://docs.python.org/3/library/logging.config.xhtml#logging-config-api">https://docs.python.org/3/library/logging.config.xhtml#logging-config-api</a></p>
<h2 id="_idParaDest-292"><a id="_idTextAnchor299"/>See also</h2>
<p>To read and <a id="_idIndexMarker569"/>understand more about how handlers work, refer to the official documentation here: <a href="https://docs.python.org/3/library/logging.handlers.xhtml">https://docs.python.org/3/library/logging.handlers.xhtml</a></p>
<h1 id="_idParaDest-293"><a id="_idTextAnchor300"/>Monitoring our data ingest ﬁle size</h1>
<p>When ingesting data, we can track a few items to ensure the incoming information is what we expect. One <a id="_idIndexMarker570"/>of the most important of these items is the data size we are ingesting, which can mean file size or the size of chunks of streaming data.</p>
<p>Logging the size of incoming data allows the creation of intelligent and efficient monitoring. If at some point the size of incoming data diverges from what is expected, we can take action to investigate and resolve the issue.</p>
<p>In this recipe, we will create simple Python code that logs the size of ingested files, which is very valuable in data monitoring.</p>
<h2 id="_idParaDest-294"><a id="_idTextAnchor301"/>Getting ready</h2>
<p>We will use only Python code. Make sure you have Python version 3.7 or above. You can use the following command on your CLI to check your version:</p>
<pre class="source-code">
$ python3 –-version
Python 3.8.10</pre>
<p>The following code execution can be done using a Python shell or a Jupyter notebook.</p>
<h2 id="_idParaDest-295"><a id="_idTextAnchor302"/>How to do it…</h2>
<p>This exercise will create a simple Python function to read a file path and return its size in bytes by default. If we want to return the value in megabytes, we only need to pass the input parameter as <code>True</code>:</p>
<ol>
<li>Let’s start by importing the <code>os</code> library:<pre class="source-code">
import os</pre></li>
<li>Then, we declare our function that requires a file path, along with an optional parameter to convert the size to megabytes:<pre class="source-code">
def get_file_size(file_name, s_megabytes=False):</pre></li>
<li>Let’s use <code>os.stat()</code> to retrieve information from the file:<pre class="source-code">
    file_stats = os.stat(file_name)</pre></li>
<li>Since it is optional, we can create an <code>if</code> condition to convert the <code>bytes</code> value to <code>megabytes</code>. If not flagged as <code>True</code>, we return the value in <code>bytes</code>, as you can see in the following code:<pre class="source-code">
    if s_megabytes:
        return f"The file size in megabytes is: {file_stats.st_size / (1024 * 1024)}"
    return f"The file size in bytes is: {file_stats.st_size}"</pre></li>
<li>Finally, let’s <a id="_idIndexMarker571"/>call our function, passing a dataset we already used:<pre class="source-code">
file_name = "listings.csv"
get_file_size(file_name)</pre></li>
</ol>
<p>For the <code>listings.csv</code> file, you should see the following output:</p>
<div><div><img alt="Figure 8.12 – File size in bytes" height="34" src="img/Figure_8.12_B19453.jpg" width="417"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.12 – File size in bytes</p>
<p>If we execute it by passing <code>s_megabytes</code> as <code>True</code>, we will see the following output:</p>
<div><div><img alt="Figure 8.13 – File size in megabytes" height="35" src="img/Figure_8.13_B19453.jpg" width="598"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.13 – File size in megabytes</p>
<p>Feel free to test it using any file path on your machine and check whether the size is the same as that indicated in the console output.</p>
<h2 id="_idParaDest-296"><a id="_idTextAnchor303"/>How it works…</h2>
<p>A file’s size estimation is convenient when working with data. Let’s understand the pieces of code we used to achieve this estimation.</p>
<p>The first operation we used was the <code>os.stat()</code> method to retrieve information about the file, as you can see here:</p>
<pre class="source-code">
file_stats = os.stat(file_name)</pre>
<p>This method <a id="_idIndexMarker572"/>interacts directly with your OS. If we execute it in isolation, we will have the following output for the <code>listings.csv</code> file:</p>
<div><div><img alt="Figure 8.14  – Attributes of the listings.csv file when using os.stat_result" height="84" src="img/Figure_8.14_B19453.jpg" width="804"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.14 – Attributes of the listings.csv file when using os.stat_result</p>
<p>In our case, we only need <code>st_size</code> to bring the <code>bytes</code> estimation, so we called it later in the <code>return</code> clause as follows:</p>
<pre class="source-code">
file_stats.st_size</pre>
<p>If you want to know more about the other results shown, you can refer to the official documentation page here: <a href="https://docs.python.org/3/library/stat.xhtml#stat.filemode">https://docs.python.org/3/library/stat.xhtml#stat.filemode</a></p>
<p>Lastly, to provide the result in megabytes, we only need to do a simple conversion using the <code>st_size</code> value, where 1 KB is 1,024 bytes, and 1 MB is equal to 1,024 KB. You can see the conversion formula here:</p>
<pre class="source-code">
file_stats.st_size / (1024 * 1024)</pre>
<h2 id="_idParaDest-297"><a id="_idTextAnchor304"/>There’s more…</h2>
<p>This recipe showed how easy it is to create a Python function that retrieves the file size. Unfortunately, at the time of writing, there was no straightforward solution to perform the same thing using PySpark.</p>
<p>Glenn Franxman, a software engineer, proposed on his GitHub a workaround solution using Spark internals to estimate the size of a DataFrame. You can see his code on his GitHub at the following link – make sure to give the proper credits if you do use it: <a href="https://gist.github.com/gfranxman/4fd0719ff2618039182dd7ea1a702f8e">https://gist.github.com/gfranxman/4fd0719ff2618039182dd7ea1a702f8e</a></p>
<p>Let’s use Glenn’s code in <a id="_idIndexMarker573"/>an example to estimate the DataFrame size and see how it works:</p>
<pre class="source-code">
<strong class="bold">from pyspark.serializers import PickleSerializer, AutoBatchedSerializer</strong>
<strong class="bold">def _to_java_object_rdd(rdd):</strong>
<strong class="bold">    """ Return a JavaRDD of Object by unpickling</strong>
<strong class="bold">    It will convert each Python object into Java object by Pyrolite, whenever the</strong>
<strong class="bold">    RDD is serialized in batch or not.</strong>
<strong class="bold">    """</strong>
<strong class="bold">    rdd = rdd._reserialize(AutoBatchedSerializer(PickleSerializer()))</strong>
<strong class="bold">    return rdd.ctx._jvm.org.apache.spark.mllib.api.python.SerDe.pythonToJava(rdd._jrdd, True)</strong>
<strong class="bold">def estimate_df_size(df):</strong>
<strong class="bold">    JavaObj = _to_java_object_rdd(df.rdd)</strong>
<strong class="bold">    nbytes = spark._jvm.org.apache.spark.util.SizeEstimator.estimate(JavaObj)</strong>
<strong class="bold">    </strong><strong class="bold">return nbytes</strong></pre>
<p>To execute the preceding code, you must have a SparkSession initiated. Once you have this and a DataFrame, execute the code and call the <code>estimate_df_size()</code> function as follows:</p>
<pre class="source-code">
<strong class="bold">estimate_df_size(df)</strong></pre>
<p>You should see the following output in bytes, depending on which DataFrame you are using:</p>
<div><div><img alt="Figure 8.15 – DataFrame size in bytes" height="28" src="img/Figure_8.15_B19453.jpg" width="593"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.15 – DataFrame size in bytes</p>
<p>Remember that <a id="_idIndexMarker574"/>this solution will only work if you pass a DataFrame as a parameter. Our Python code works well for other file estimations and doesn’t have performance issues when estimating big files.</p>
<h2 id="_idParaDest-298"><a id="_idTextAnchor305"/>See also</h2>
<p>Unlike PySpark, Scala <a id="_idIndexMarker575"/>has a <code>SizeEstimator</code> function to return the size of a DataFrame. You can find more here: <a href="https://spark.apache.org/docs/latest/api/java/index.xhtml?org/apache/spark/util/SizeEstimator.xhtml">https://spark.apache.org/docs/latest/api/java/index.xhtml?org/apache/spark/util/SizeEstimator.xhtml</a></p>
<h1 id="_idParaDest-299"><a id="_idTextAnchor306"/>Logging based on data</h1>
<p>As mentioned in the <em class="italic">Monitoring our data ingest ﬁle size</em> recipe, logging our ingest is a good practice in the <a id="_idIndexMarker576"/>data field. There are several ways to explore our ingestion logs to increase the process’s reliability and our confidence in it. In this recipe, we will start to get into the data operations field (or <strong class="bold">DataOps</strong>), where the goal is to track the behavior of data from the source until it reaches its final destination.</p>
<p>This recipe will explore other metrics we can track to create a reliable data pipeline.</p>
<h2 id="_idParaDest-300"><a id="_idTextAnchor307"/>Getting ready</h2>
<p>For this exercise, let’s imagine we have two simple data ingests, one from a database and another from an API. Since this is a straightforward pipeline, let’s visualize it with the following diagram:</p>
<div><div><img alt="Figure 8.16 – Data ingestion phases" height="399" src="img/Figure_8.16_B19453.jpg" width="666"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.16 – Data ingestion phases</p>
<p>With this in mind, let’s <a id="_idIndexMarker577"/>explore the instances we can log to make monitoring efficient.</p>
<h2 id="_idParaDest-301"><a id="_idTextAnchor308"/>How to do it…</h2>
<p>Let’s define the essential metrics based on each layer (or step) we saw in the preceding diagram:</p>
<ol>
<li><strong class="bold">Data sources</strong>: Let’s <a id="_idIndexMarker578"/>start with the first layer of the ingestion—the sources. Knowing we are handling two different data sources, we must create additional metrics for them. See the following figure for reference:</li>
</ol>
<div><div><img alt="Figure 8.17 – Database metrics to monitor" height="269" src="img/Figure_8.17_B19453.jpg" width="665"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.17 – Database metrics to monitor</p>
<ol>
<li value="2"><strong class="bold">Ingestion</strong>: Now that the source is logged and monitored, let’s move on to the ingestion layer. As we <a id="_idIndexMarker579"/>saw previously in this chapter, we can log information such as errors, informative parts of the code execution, file size, and so on. Let’s insert more content here, such as the schema and the time taken to retrieve or process data. We will end up with a diagram similar to the following:</li>
</ol>
<div><div><img alt="Figure 8.18 – Data ingestion metrics to monitor" height="302" src="img/Figure_8.18_B19453.jpg" width="809"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.18 – Data ingestion metrics to monitor</p>
<ol>
<li value="3"><strong class="bold">Staging layer</strong>: Lastly, let’s cover the final layer after ingestion. The goal here is to ensure we <a id="_idIndexMarker580"/>maintain the integrity of the data, so verifying whether the schema still matches the data is crucial. We can also add logs to monitor the number of Parquet files and their sizes. See the following figure for reference:</li>
</ol>
<div><div><img alt="Figure 8.19 – Staging layer metrics to monitor" height="246" src="img/Figure_8.19_B19453.jpg" width="887"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.19 – Staging layer metrics to monitor</p>
<p>Now that we have <a id="_idIndexMarker581"/>covered the essential topics to be monitored, let’s understand why they were chosen.</p>
<h2 id="_idParaDest-302"><a id="_idTextAnchor309"/>How it works…</h2>
<p>Since the first recipe of this chapter, we have perpetually reinforced how logs are relevant to getting your system to work correctly. Here, we put it all together to see, albeit from a high level, how storing specific information can help us with monitoring.</p>
<p>Starting with the data source layer, the metrics chosen were based on the response of the data and the availability to retrieve data. Understanding whether we can begin the ingestion process is fundamental, and even more important is knowing whether the data size is what we expect.</p>
<p>Imagine the following scenario: every day, we ingest 50 MB of data from an API. One day, however, we received 10 KB. With proper logging and monitoring functionalities, we can quickly review the issue in terms of historic events. We can expand the data size check to the subsequent layers we covered in the recipe.</p>
<p class="callout-heading">Note</p>
<p class="callout">We purposely intercalate the words “step” and “layer” when referring to the phases of the ingestion process since it can vary in different works of literature and in different companies’ internal processes.</p>
<p>Another way to log and <a id="_idIndexMarker582"/>monitor our data is by using schema validation. <strong class="bold">Schema validation</strong> (when applicable) guarantees that nothing has changed at the <a id="_idIndexMarker583"/>source. Therefore, the results for transformation or aggregation tend to be linear. We can also implement an auxiliary function or job to check <a id="_idIndexMarker584"/>that fields containing <strong class="bold">sensitive</strong> or <strong class="bold">Personally Identifiable Information</strong> (<strong class="bold">PII</strong>) are adequately anonymized.</p>
<p>Monitoring the <em class="italic">parquet file size or count</em> is crucial to verify that quality is being maintained. As seen in <a href="B19453_07.xhtml#_idTextAnchor227"><em class="italic">Chapter 7</em></a>, the number of parquet files can interfere with other applications’ reading quality or even the ETL’s subsequent phases.</p>
<p>Finally, it is essential to point out that we covered logs here to ensure the quality and reliability of our data ingestion. Remember that the best practice is to align the records we got from the code with the examples we saw here.</p>
<h2 id="_idParaDest-303"><a id="_idTextAnchor310"/>There’s more…</h2>
<p>The content of this recipe is part of a more extensive subject called <strong class="bold">data observability</strong>. Data observability is a union of data operations, quality, and governance. The objective is to <a id="_idIndexMarker585"/>centralize everything to make the management and monitoring of data processes efficient and reliable, bringing health to data.</p>
<p>We will discuss this further in <a href="B19453_12.xhtml#_idTextAnchor433"><em class="italic">Chapter 12</em></a>. However, if you are curious about the topic, Databand (an IBM company) has a good introduction at <a href="https://databand.ai/data-observability/">https://databand.ai/data-observability/</a>.</p>
<h2 id="_idParaDest-304"><a id="_idTextAnchor311"/>See also</h2>
<p>Find out more <a id="_idIndexMarker586"/>about monitoring ETL pipelines at the DataGaps blog page, here: <a href="https://www.datagaps.com/blog/monitoring-your-etl-test-data-pipelines-in-production-dataops-suite/">https://www.datagaps.com/blog/monitoring-your-etl-test-data-pipelines-in-production-dataops-suite/</a></p>
<h1 id="_idParaDest-305"><a id="_idTextAnchor312"/>Retrieving SparkSession metrics</h1>
<p>Until now, we created <a id="_idIndexMarker587"/>our logs to provide more information and be more useful for monitoring. Logging allows us to build customized metrics based on the necessity of our pipeline and code. However, we can also take advantage of built-in metrics from frameworks and programming languages.</p>
<p>When we create a <code>SparkSession</code>, it provides a web UI with useful metrics that can be used to monitor our pipelines. Using this, the following recipe shows you how to access and retrieve metric information from SparkSession, and use it as a tool when ingesting or processing a DataFrame.</p>
<h2 id="_idParaDest-306"><a id="_idTextAnchor313"/>Getting ready</h2>
<p>You can execute this recipe using the PySpark command line or the Jupyter Notebook.</p>
<p>Before exploring the Spark UI metrics, let’s create a simple <code>SparkSession</code> using the following code:</p>
<pre class="source-code">
from pyspark.sql import SparkSession
spark = SparkSession.builder \
      .master("local[1]") \
      .appName("chapter8_monitoring") \
      .config("spark.executor.memory", '3g') \
      .config("spark.executor.cores", '3') \
      .config("spark.cores.max", '3') \
      .getOrCreate()</pre>
<p>Then, let’s read a JSON file and call the <code>.show()</code> method as follows:</p>
<pre class="source-code">
df_json = spark.read.option("multiline","true") \
                    .json('github_events.json')
df_json.show()</pre>
<p>I am using a dataset <a id="_idIndexMarker588"/>called <code>github_events.json</code>, which we worked with previously in <a href="B19453_04.xhtml#_idTextAnchor127"><em class="italic">Chapter 4</em></a>. However, feel free to use whatever you prefer, since the objective here is not to observe the schema of the dataset, but to see what we can find out from the Spark UI.</p>
<h2 id="_idParaDest-307"><a id="_idTextAnchor314"/>How to do it…</h2>
<ol>
<li>With the <code>SparkSession</code> initiated as outlined in the <em class="italic">Getting ready</em> section, we can use the <code>spark</code> command to retrieve a link to the Spark UI, as follows:<pre class="source-code">
<strong class="bold">spark</strong></pre></li>
</ol>
<p>You should see the following output:</p>
<div><div><img alt="Figure 8.20 – spark command output" height="299" src="img/Figure_8.20_B19453.jpg" width="276"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.20 – spark command output</p>
<ol>
<li value="2">Click on <strong class="bold">Spark UI</strong>, and your browser will open a new tab. You should see a page like this:</li>
</ol>
<div><div><img alt="Figure 8.21 – Spark UI: Jobs page view" height="813" src="img/Figure_8.21_B19453.jpg" width="1563"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.21 – Spark UI: Jobs page view</p>
<p>If you are not <a id="_idIndexMarker589"/>using the Jupyter Notebook, you can access this interface by pointing your browser to <a href="http://localhost:4040/">http://localhost:4040/</a>.</p>
<p>My page looks more crowded because I expanded <strong class="bold">Event Timeline</strong> and <strong class="bold">Completed Jobs</strong> – you can do the same by clicking on them.</p>
<ol>
<li value="3">Next, let’s explore the first completed job further. Click on <strong class="bold">showString at NativeMethodAccessorImpl.java:0</strong> and you should see the following page:</li>
</ol>
<div><div><img alt="Figure 8.22 – Spark UI: Stage page view for a specific job" height="922" src="img/Figure_8.22_B19453.jpg" width="1561"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.22 – Spark UI: Stage page view for a specific job</p>
<p>Here, we can see <a id="_idIndexMarker590"/>the task status of this job in more detail, covering things such as how much memory it used, the time taken to execute it, and so on.</p>
<p>Note also that it switched to the <strong class="bold">Stages</strong> tab at the top menu.</p>
<ol>
<li value="4">Now, click on the <strong class="bold">Executors</strong> button at the top of the page. You should see a page similar to this:</li>
</ol>
<div><div><img alt="Figure 8.23 – Spark UI: Executors page view" height="561" src="img/Figure_8.23_B19453.jpg" width="1550"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.23 – Spark UI: Executors page view</p>
<p>All the metrics <a id="_idIndexMarker591"/>here are related to the Spark drivers and nodes.</p>
<ol>
<li value="5">Then, click on the <strong class="bold">SQL</strong> button in the top menu. You should see the following page:</li>
</ol>
<div><div><img alt="Figure 8.24 – Spark UI: SQL page view" height="1048" src="img/Figure_8.24_B19453.jpg" width="1533"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.24 – Spark UI: SQL page view</p>
<p>On this page, it is possible to see the queries executed by Spark internally. If we used an explicit query <a id="_idIndexMarker592"/>in our code, we would see here how it was performed internally.</p>
<p>You don’t need to worry about the <code>.</code><code>show()</code> method.</p>
<h2 id="_idParaDest-308"><a id="_idTextAnchor315"/>How it works…</h2>
<p>Now that we have explored Spark UI, let’s understand how each tab is organized and some of the steps we did with them.</p>
<p>In <em class="italic">step 2</em>, we had a first glance at the interface. This interface makes it possible to see an event timeline with information about when the driver was created and executed. Also, we can observe the jobs marked on the timeline, as follows:</p>
<div><div><img alt="Figure 8.25 – Detailed view of the Event Timeline expanded menu" height="323" src="img/Figure_8.25_B19453.jpg" width="1555"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.25 – Detailed view of the Event Timeline expanded menu</p>
<p>We can observe how they interact when working with bigger jobs and more complex parallel tasks. Unfortunately, we would need a dedicated project and several datasets to simulate this, but <a id="_idIndexMarker593"/>you now know where to look for future reference.</p>
<p>Then, we selected <strong class="bold">showString at NativeMethodAccessorImpl.java:0</strong>, which redirected us to the <strong class="bold">Stages</strong> page. This page offers more detailed information about Spark’s tasks, whether the task was successful or not.</p>
<p>An excellent metric and visualization tool is <strong class="bold">DAG Visualization</strong> (referring to directed acyclic graphs), which can be expanded and will show something like this:</p>
<div><div><img alt="Figure 8.26 – DAG Visualization of a job" height="790" src="img/Figure_8.26_B19453.jpg" width="455"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.26 – DAG Visualization of a job</p>
<p>This offers an excellent overview of each step performed at each stage. We can also consult this to understand which part was problematic in the event of an error based on the traceback message.</p>
<p>Since we selected <a id="_idIndexMarker594"/>a specific task (or job), it showed its stages and details. However, we can display all the steps executed if we go directly to <strong class="bold">Stages</strong>. Doing so, you should see something like this:</p>
<div><div><img alt="Figure 8.27 – Spark UI: Stages overview with all jobs executed" height="618" src="img/Figure_8.27_B19453.jpg" width="1551"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.27 – Spark UI: Stages overview with all jobs executed</p>
<p>Although the description messages are not straightforward, we can get the gist of what each of them is doing. <code>Stage id 0</code> refers to the reading JSON function, and <code>Stage id 1</code> with the <code>showString</code> message refers to the <code>.show()</code> method call.</p>
<p>The <strong class="bold">Executors</strong> page shows <a id="_idIndexMarker595"/>the metrics related to the core of Spark and how it is performing. You can use this information to understand your cluster’s behavior and whether any tuning is needed. For more detailed information about each field, refer to the Spark official documentation at https://spark.apache.org/docs/latest/monitoring.xhtml#executor-metrics.</p>
<p>Last but not least, we saw the <strong class="bold">SQL</strong> page, where it was possible to see how Spark internally shuffles and aggregates the data behind the scenes, like <strong class="bold">Stages</strong>, taking advantage of a more visual form of execution, as you can see in the following screenshot:</p>
<div><div><img alt="Figure 8.28 – Flow diagram of the SQL query internally executed" height="662" src="img/Figure_8.28_B19453.jpg" width="614"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.28 – Flow diagram of the SQL query internally executed</p>
<p>Here, we can see <a id="_idIndexMarker596"/>that the query is related to the <code>.show()</code> method. There is helpful information inside it, including the number of output rows, the files read, and their sizes.</p>
<h2 id="_idParaDest-309"><a id="_idTextAnchor316"/>There’s more…</h2>
<p>Even though Spark metrics are handy, you might wonder how to use them when hosting your PySpark jobs on cloud providers such as AWS or Google Cloud.</p>
<p><strong class="bold">AWS</strong> provides a <a id="_idIndexMarker597"/>simple solution to enable Spark UI when using <strong class="bold">AWS Glue</strong>. You can find out more about it at <a href="https://docs.aws.amazon.com/glue/latest/dg/monitor-spark-ui-jobs.xhtml">https://docs.aws.amazon.com/glue/latest/dg/monitor-spark-ui-jobs.xhtml</a>.</p>
<p><strong class="bold">Google Data Proc</strong> provides a web <a id="_idIndexMarker598"/>interface for its cluster, where you can also see metrics for <strong class="bold">Hadoop</strong> and <strong class="bold">YARN</strong>. Since Spark runs on top of YARN, you won’t find a link directly for Spark UI, but you can use the YARN interface to access it. You can find out more here: <a href="https://cloud.google.com/dataproc/docs/concepts/accessing/cluster-web-interfaces">https://cloud.google.com/dataproc/docs/concepts/accessing/cluster-web-interfaces</a>.</p>
<h2 id="_idParaDest-310"><a id="_idTextAnchor317"/>See also</h2>
<p><em class="italic">Towards Data Science</em> has <a id="_idIndexMarker599"/>a fantastic article about Spark metrics: https://towardsdatascience.com/monitoring-of-spark-applications-3ca0c271c4e0</p>
<h1 id="_idParaDest-311"><a id="_idTextAnchor318"/>Further reading</h1>
<ul>
<li>https://spark.apache.org/docs/latest/monitoring.xhtml#executor-task-metrics</li>
<li><a href="https://developer.here.com/documentation/metrics-and-logs/user_guide/topics/spark-ui.xhtml">https://developer.here.com/documentation/metrics-and-logs/user_guide/topics/spark-ui.xhtml</a></li>
<li><a href="https://github.com/LucaCanali/Miscellaneous/blob/master/Spark_Notes/Spark_TaskMetrics.md">https://github.com/LucaCanali/Miscellaneous/blob/master/Spark_Notes/Spark_TaskMetrics.md</a></li>
<li><a href="https://docs.python.org/3/howto/logging.xhtml">https://docs.python.org/3/howto/logging.xhtml</a></li>
<li><a href="http://datadoghq.com/blog/python-logging-best-practices/">datadoghq.com/blog/python-logging-best-practices/</a></li>
<li><a href="https://coralogix.com/blog/python-logging-best-practices-tips/">https://coralogix.com/blog/python-logging-best-practices-tips/</a></li>
</ul>
</div>
</div></body></html>