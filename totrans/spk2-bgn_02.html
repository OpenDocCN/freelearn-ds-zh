<html><head></head><body><div class="chapter" title="Chapter&#xA0;2.&#xA0;Spark Programming Model"><div class="titlepage"><div><div><h1 class="title"><a id="ch02"/>Chapter 2. Spark Programming Model</h1></div></div></div><p>
<span class="strong"><strong>Extract</strong></span>, <span class="strong"><strong>Transform</strong></span>, and <span class="strong"><strong>Load</strong></span> (<span class="strong"><strong>ETL</strong></span>) tools proliferated along with the growth of the data in organizations. The need to move data from one source to one or more destinations, processing it on the fly before it reaches its destination, were all the requirements of the time. Most of the time, these ETL tools were supporting only a few types of data, only a few types of data sources and destinations, and were closed to extension to allow them to support new data types and new sources and destinations. Because of these stringent limitations on the tools, sometimes even a one-step transformation process had to be done in multiple steps. These convoluted approaches mandated the need to have unnecessary wastage in terms of manpower, as well as other computing resources. The main argument from the commercial ETL vendors all the time remained the same, one size doesn't fit all. So use <span class="emphasis"><em>our</em></span> suite of tools instead of the point products available on the market. Many organizations got into vendor lock-in because of the profuse need to process data. Almost all the tools introduced before the year 2005 did not make use of the real power of the multi-core architecture of the computers if they supported running their tools on the commodity hardware. So, simple but voluminous data processing jobs took hours and sometimes even days to complete with these tools.</p><p>Spark became an instant hit in the market because of its ability to process a huge amount of data types and a growing number of data sources and data destinations. The most important and basic data abstraction Spark provides is the <span class="strong"><strong>resilient distributed dataset</strong></span> (<span class="strong"><strong>RDD</strong></span>). As discussed in the previous chapter, Spark supports distributed processing on a cluster of nodes. The moment there is a cluster of nodes, there is a good chance that when the data processing is going on, some of the nodes can die. When such failures happen, the framework should be capable of coming out of such failures. Spark is designed to do that and that is what the <span class="emphasis"><em>resilient</em></span> part in the RDD signifies. If there is a huge amount of data to be processed and there are nodes available in the cluster, the framework should have the capability to split the big dataset into smaller chunks and distribute them to be processed on more than one node in a cluster, in parallel. Spark is capable of doing that and that is what the <span class="emphasis"><em>distributed</em></span> part in the RDD signifies. In other words, Spark is designed from the ground up to have its basic dataset abstraction capable of getting split into smaller pieces deterministically and distributed to more than one node in the cluster for parallel processing, while elegantly handling the failures in the nodes.</p><p>We will cover the following topics in this chapter:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Functional programming with Spark</li><li class="listitem" style="list-style-type: disc">Spark RDD</li><li class="listitem" style="list-style-type: disc">Data transformations and actions</li><li class="listitem" style="list-style-type: disc">Spark monitoring</li><li class="listitem" style="list-style-type: disc">Spark programming basics</li><li class="listitem" style="list-style-type: disc">Creating RDDs from files</li><li class="listitem" style="list-style-type: disc">Spark libraries</li></ul></div><div class="section" title="Functional programming with Spark"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec13"/>Functional programming with Spark</h1></div></div></div><p>The mutation of objects at run time, and the inability to get consistent results from a program or function because of the side effect that the program logic creates makes many applications very complex. If the functions in programming languages start behaving exactly like mathematical functions in such a way that the output of the function depends only on the inputs, that gives lots of predictability to applications. The computer programming paradigm giving lots of emphasis to the process of building such functions and other elements based on that, and using those functions just in the way that any other data types are being used, is popularly known as the functional programming paradigm. Out of the JVM-based programming languages, Scala is one of the most important ones that has very strong functional programming capabilities without losing any object orientation. Spark is written predominantly in Scala. Because of that itself, Spark has taken lots of very good concepts from Scala.</p></div></div>
<div class="section" title="Understanding Spark RDD"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec14"/>Understanding Spark RDD</h1></div></div></div><p>The most important feature that Spark took from Scala is the ability to use functions as parameters to the Spark transformations and Spark actions. Quite often, the RDD in Spark behaves just like a collection object in Scala. Because of that, some of the data transformation method names of Scala collections are used in Spark RDD to do the same thing. This is a very neat approach and those who have expertise in Scala will find it very easy to program with RDDs. We will see a few important features in the following sections.</p><div class="section" title="Spark RDD is immutable"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec10"/>Spark RDD is immutable</h2></div></div></div><p>There are some strong rules based on which an RDD is created. Once an RDD is created, intentionally or unintentionally, it cannot be changed. This gives another insight into the construction of an RDD. Because of that, when the nodes processing some part of an RDD die, the driver program can recreate those parts and assign the task of processing it to another node, ultimately, completing the data processing job successfully.</p><p>Since the RDD is immutable, splitting a big one to smaller ones, distributing them to various worker nodes for processing, and finally compiling the results to produce the final result can be done safely without worrying about the underlying data getting changed.</p></div><div class="section" title="Spark RDD is distributable"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec11"/>Spark RDD is distributable</h2></div></div></div><p>If Spark is run in a cluster mode where there are multiple worker nodes available to take the tasks, all these nodes will have different execution contexts. The individual tasks are distributed and run on different JVMs. All these activities of a big RDD getting divided into smaller chunks, getting distributed for processing to the worker nodes, and finally, assembling the results back, are completely hidden from the users.</p><p>Spark has its own mechanism for recovering from the system faults and other forms of errors which occur during the data processing and hence this data abstraction is highly resilient.</p></div><div class="section" title="Spark RDD lives in memory"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec12"/>Spark RDD lives in memory</h2></div></div></div><p>Spark does keep all the RDDs in the memory as much as it can. Only in rare situations, where Spark is running out of memory or if the data size is growing beyond the capacity, is it written to disk. Most of the processing on RDD happens in the memory, and that is the reason why Spark is able to process the data at a lightning fast speed.</p></div><div class="section" title="Spark RDD is strongly typed"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec13"/>Spark RDD is strongly typed</h2></div></div></div><p>Spark RDD can be created using any supported data types. These data types can be Scala/Java supported intrinsic data types or custom created data types such as your own classes. The biggest advantage coming out of this design decision is the freedom from runtime errors. If it is going to break because of a data type issue, it will break during compile time.</p><p>The following table captures the structure of an RDD containing tuples of a retail bank account data. It is of the type RDD[(string, string, string, double)]:</p><div class="informaltable"><table border="1"><colgroup><col/><col/><col/><col/></colgroup><thead><tr><th>
<p>
<span class="strong"><strong>AccountNo</strong></span>
</p>
</th><th>
<p>
<span class="strong"><strong>FirstName</strong></span>
</p>
</th><th>
<p>
<span class="strong"><strong>LastName</strong></span>
</p>
</th><th>
<p>
<span class="strong"><strong>AccountBalance</strong></span>
</p>
</th></tr></thead><tbody><tr><td>
<p>SB001</p>
</td><td>
<p>John</p>
</td><td>
<p>Mathew</p>
</td><td>
<p>250.00</p>
</td></tr><tr><td>
<p>SB002</p>
</td><td>
<p>Tracy</p>
</td><td>
<p>Mason</p>
</td><td>
<p>450.00</p>
</td></tr><tr><td>
<p>SB003</p>
</td><td>
<p>Paul</p>
</td><td>
<p>Thomson</p>
</td><td>
<p>560.00</p>
</td></tr><tr><td>
<p>SB004</p>
</td><td>
<p>Samantha</p>
</td><td>
<p>Grisham</p>
</td><td>
<p>650.00</p>
</td></tr><tr><td>
<p>SB005</p>
</td><td>
<p>John</p>
</td><td>
<p>Grove</p>
</td><td>
<p>1000.00</p>
</td></tr></tbody></table></div><p>Suppose this RDD is going through a process to calculate the total amount of all these accounts in a cluster of three nodes, N1, N2, and N3; it can be split and distributed for something such as parallelizing the data processing. The following table contains the elements of the RDD[(string, string, string, double)] distributed to node N1 for processing:</p><div class="informaltable"><table border="1"><colgroup><col/><col/><col/><col/></colgroup><thead><tr><th>
<p>
<span class="strong"><strong>AccountNo</strong></span>
</p>
</th><th>
<p>
<span class="strong"><strong>FirstName</strong></span>
</p>
</th><th>
<p>
<span class="strong"><strong>LastName</strong></span>
</p>
</th><th>
<p>
<span class="strong"><strong>AccountBalance</strong></span>
</p>
</th></tr></thead><tbody><tr><td>
<p>SB001</p>
</td><td>
<p>John</p>
</td><td>
<p>Mathew</p>
</td><td>
<p>250.00</p>
</td></tr><tr><td>
<p>SB002</p>
</td><td>
<p>Tracy</p>
</td><td>
<p>Mason</p>
</td><td>
<p>450.00</p>
</td></tr></tbody></table></div><p>The following table contains the elements of the RDD[(string, string, string, double)] distributed to node N2 for processing:</p><div class="informaltable"><table border="1"><colgroup><col/><col/><col/><col/></colgroup><thead><tr><th>
<p>
<span class="strong"><strong>AccountNo</strong></span>
</p>
</th><th>
<p>
<span class="strong"><strong>FirstName</strong></span>
</p>
</th><th>
<p>
<span class="strong"><strong>LastName</strong></span>
</p>
</th><th>
<p>
<span class="strong"><strong>AccountBalance</strong></span>
</p>
</th></tr></thead><tbody><tr><td>
<p>SB003</p>
</td><td>
<p>Paul</p>
</td><td>
<p>Thomson</p>
</td><td>
<p>560.00</p>
</td></tr><tr><td>
<p>SB004</p>
</td><td>
<p>Samantha</p>
</td><td>
<p>Grisham</p>
</td><td>
<p>650.00</p>
</td></tr><tr><td>
<p>SB005</p>
</td><td>
<p>John</p>
</td><td>
<p>Grove</p>
</td><td>
<p>1000.00</p>
</td></tr></tbody></table></div><p>On node N1, the summation process happens and the result is returned to the Spark driver program. Similarly, on node N2, the summation process happens, the result is returned to the Spark driver program, and the final result is computed.</p><p>Spark has very deterministic rules on splitting a big RDD into smaller chunks for distribution to various nodes and because of that, even if something happens to, say, node N1, Spark knows how to recreate exactly the chunk that was lost in the node N1 and continue with the data processing operation by sending the same payload to node N3.</p><p>Figure 1 captures the essence of the process:</p><p>
</p><div class="mediaobject"><img alt="Spark RDD is strongly typed" src="graphics/image_02_002.jpg"/><div class="caption"><p>Figure 1</p></div></div><p>
</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="tip10"/>Tip</h3><p>Spark does a lot of processing in its driver memory and in the executor memory on the cluster nodes. Spark has various parameters that can be configured and fine-tuned so that the required resources are made available before the processing starts.</p></div></div></div></div>
<div class="section" title="Data transformations and actions with RDDs"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec15"/>Data transformations and actions with RDDs</h1></div></div></div><p>Spark does the data processing using the RDDs. From the relevant data source such as text files and NoSQL data stores, data is read to form the RDDs. On such an RDD, various data transformations are performed and finally, the result is collected. To be precise, Spark comes with Spark transformations and Spark actions that act upon RDDs. Let us take the following RDD capturing a list of retail banking transactions, which is of the type RDD[(string, string, double)]:</p><div class="informaltable"><table border="1"><colgroup><col/><col/><col/></colgroup><thead><tr><th>
<p>
<span class="strong"><strong>AccountNo</strong></span>
</p>
</th><th>
<p>
<span class="strong"><strong>TranNo</strong></span>
</p>
</th><th>
<p>
<span class="strong"><strong>TranAmount</strong></span>
</p>
</th></tr></thead><tbody><tr><td>
<p>SB001</p>
</td><td>
<p>TR001</p>
</td><td>
<p>250.00</p>
</td></tr><tr><td>
<p>SB002</p>
</td><td>
<p>TR004</p>
</td><td>
<p>450.00</p>
</td></tr><tr><td>
<p>SB003</p>
</td><td>
<p>TR010</p>
</td><td>
<p>120.00</p>
</td></tr><tr><td>
<p>SB001</p>
</td><td>
<p>TR012</p>
</td><td>
<p>-120.00</p>
</td></tr><tr><td>
<p>SB001</p>
</td><td>
<p>TR015</p>
</td><td>
<p>-10.00</p>
</td></tr><tr><td>
<p>SB003</p>
</td><td>
<p>TR020</p>
</td><td>
<p>100.00</p>
</td></tr></tbody></table></div><p>To calculate the account level summary of the transactions from the RDD of the form <code class="literal">(AccountNo,TranNo,TranAmount)</code>:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">First it has to be transformed to the form of key-value pairs <code class="literal">(AccountNo,TranAmount)</code>, where <code class="literal">AccountNo </code>is the key but there will be multiple elements with the same key.</li><li class="listitem">On this key, do a summation operation on <code class="literal">TranAmount</code>, resulting in another RDD of the form (AccountNo,TotalAmount),where every AccountNo will have only one element and TotalAmount is the sum of all the TranAmount for the given AccountNo.</li><li class="listitem">Now sort the key-value pairs on the <code class="literal">AccountNo</code> and store the output.</li></ol></div><p>In the whole process described, all are Spark transformations except the storing of the output. Storing of the output is a <span class="strong"><strong>Spark action</strong></span>. Spark does all these operations on a need-to-do basis. Spark does not act when a Spark transformation is applied. The real act happens when the first Spark action in the chain is called. Then it diligently applies all the preceding Spark transformations in order, and then does the first encountered Spark action. This is based on the concept called <span class="strong"><strong>Lazy Evaluation</strong></span>.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note11"/>Note</h3><p>In the programming language context of declaring and using variables, <span class="emphasis"><em>Lazy Evaluation</em></span> means that a variable is evaluated only when it is first used in the program. </p></div></div><p>Apart from this action of storing the output to disk, there are many other possible Spark actions including, but not limited to, some of the ones given in the following list:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Collecting all the contents in the resultant RDD to an array residing in the driver program</li><li class="listitem" style="list-style-type: disc">Counting the number of elements in the RDD</li><li class="listitem" style="list-style-type: disc">Counting the number of elements for each key in the RDD element</li><li class="listitem" style="list-style-type: disc">Taking the first element in the RDD</li><li class="listitem" style="list-style-type: disc">Taking a given number of elements from the RDD commonly used for top N reports</li><li class="listitem" style="list-style-type: disc">Taking a sample of elements from the RDD</li><li class="listitem" style="list-style-type: disc">Iterating through all the elements in the RDD</li></ul></div><p>In this example, many transformations are done on various RDDs that get created on the fly till the process is completed. In other words, whenever a transformation is done on an RDD, a new RDD gets created. This is because RDDs are inherently immutable. These RDDs that are getting created at the end of each transformation can be saved for future reference, or they will go out of scope eventually.</p><p>To summarize, the process of creating one or more RDDs and applying transformations and actions on them is a very common usage pattern seen ubiquitously in Spark applications.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note12"/>Note</h3><p>The table referred in the preceding data transformation example contains the values in an RDD of type the RDD[(string, string, double)]. In this RDD, there are multiple elements, and each one is a tuple of the type (string, string, double). It is very common among programmers and the user community, for easy reference and conveying ideas, that the term <code class="literal">record</code> is being used to refer one to element in the RDD. In Spark RDD there is no concept of records, rows and columns. In other words the term <code class="literal">record </code>is mistakenly used synonymously to an element in the RDD, which may be a complex data type such as a tuple or a non-scalar data type. In this book, this practice is highly refrained to use the correct terms.</p></div></div><p>In Spark there are a good amount of Spark transformations available. These are really powerful because most of these take functions as input parameters to do the transformation. In other words, these transformations act on the RDD based on the functions that are defined and supplied by the user. This becomes even more powerful with Spark's uniform programming model. Whether the programming language of choice is Scala, Java, Python, or R, the way Spark transformations and Spark actions are used is similar. This lets the organizations choose their programming language of choice.</p><p>In Spark, even though the number of Spark actions are limited in number, they are really powerful, and users can write their own Spark actions if there is a need. There are many Spark connector programs that are available in the market, mainly to read and write data from various data stores. These connector programs are designed and developed either by the user community or by the data store vendors themselves to have connectivity to Spark. In addition to the available Spark actions, they may define their own actions to supplement existing sets of Spark actions. For example, the Spark Cassandra Connector is used to connect to Cassandra from Spark. This has an action <code class="literal">saveToCassandra</code>.</p></div>
<div class="section" title="Monitoring with Spark"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec16"/>Monitoring with Spark</h1></div></div></div><p>The previous chapter covered the details of the installation and development tool setup that is required for developing and running data processing applications using Spark. In most of the real-world applications, the Spark applications can become very complex with a really huge<span class="strong"><strong> directed acyclic graph</strong></span>  (<span class="strong"><strong>DAG</strong></span>) of Spark transformations and Spark actions. Spark comes with really powerful monitoring tools for monitoring the jobs that are running in a given Spark ecosystem. The monitoring doesn't start automatically.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="tip13"/>Tip</h3><p>Note that this is a completely optional step for running Spark applications. If enabled, it will give a very good insight into the way the Spark applications are run. Discretion has to be used to enable this in production environments, as it can affect the response time of the applications.</p></div></div><p>First of all, there are some configuration changes to be made. The event logging mechanism should be turned on. For this, take the following steps:</p><pre class="programlisting">
<span class="strong"><strong>$ cd $SPARK_HOME &#13;
$ cd conf &#13;
$ cp spark-defaults.conf.template spark-defaults.conf</strong></span>
</pre><p>Once the preceding steps are completed, edit the newly created <code class="literal">spark-defaults.conf</code> file to have the following properties:</p><pre class="programlisting">
<span class="strong"><strong>spark.eventLog.enabled           true &#13;
spark.eventLog.dir               &lt;give a log directory location&gt; </strong></span>
</pre><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="tip14"/>Tip</h3><p>Once the preceding steps are completed, make sure that the previously used log directory exists in the filesystem. </p></div></div><p>Apart from the preceding configuration file changes, there are many properties in that configuration file that can be changed to fine tune the Spark runtime. The most important among them that is used frequently is the Spark driver memory. If the applications are dealing with a huge amount of data, it is a good idea to customize this property <code class="literal">spark.driver.memory</code>to have a higher value. Then run the following commands to start the Spark master:</p><pre class="programlisting">
<span class="strong"><strong>$ cd $SPARK_HOME &#13;
$ ./sbin/start-master.sh</strong></span>
</pre><p>Once the preceding steps are completed, make sure that the Spark web <span class="strong"><strong>user interface</strong></span> (<span class="strong"><strong>UI</strong></span>) is starting up by going to <code class="literal">http://localhost:8080/</code>. The assumption here is that there is no other application running in the <code class="literal">8080</code> port. If for some reason, there is a need to run this application on a different port, the command line option <code class="literal">--webui-port &lt;PORT&gt; </code>can be used in the script while starting the web user interface. </p><p>The web UI should look something similar to that shown in Figure 2:</p><p>
</p><div class="mediaobject"><img alt="Monitoring with Spark" src="graphics/image_02_003.jpg"/><div class="caption"><p>Figure 2</p></div></div><p>
</p><p>The most important information to be noted in the preceding figure is the fully-qualified Spark master URL (not the REST URL). It is going to be used again and again for many of the hands-on exercises that are going to be discussed in this book. The URL can change from system to system and the DNS settings. Also note that throughout this book, for all the hands-on exercises, Spark standalone deployment is used, which is the easiest among the deployments to get started with a single computer. </p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="tip15"/>Tip</h3><p>These Spark application monitoring steps are given now to make the readers familiar with the toolset that Spark provides. Those who are familiar with these tools or those who are very confident of the application behavior need not need the help of these tools. But to understand the concepts, debugging, and some visualizations of the processes, these tools definitely provide immense help.</p></div></div><p>From the Spark web UI that is given in Figure 2, it can be noted that there are no worker nodes available to do any task, and there are no running applications. The following steps capture the instructions to start the worker nodes. Note how the Spark master URL is being used while starting the worker node: </p><pre class="programlisting">
<span class="strong"><strong>$ cd $SPARK_HOME &#13;
$ ./sbin/start-slave.sh spark://Rajanarayanans-MacBook-Pro.local:7077</strong></span>
</pre><p>Once the worker node is started, in the Spark web UI, the newly started worker node is displayed. The  <code class="literal">$SPARK_HOME/conf/slaves.template</code>template captures the default worker nodes that will be started with the invocation of the preceding command. </p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="note16"/>Note</h3><p>If additional worker nodes are required, copy the <code class="literal">slaves.template</code> file to name it to slaves and have the entries captured in there. When a spark-shell, pyspark, or sparkR is started, instructions can be given to it to use a given Spark master. This is useful when there is a need to run Spark applications or statements on a remote Spark cluster or against a given Spark master. If nothing is given, the Spark applications will run in the local mode.</p></div></div><pre class="programlisting">
<span class="strong"><strong>$ cd $SPARK_HOME &#13;
$ ./bin/spark-shell --master spark://Rajanarayanans-MacBook-Pro.local:7077 &#13;
</strong></span>
</pre><p>The Spark web UI will look similar to that shown in Figure 3 once a worker node is started successfully. After this, if an application is run with the preceding Spark master URL, even that application's details will be displayed in the Spark web UI. A detailed coverage of the applications is to follow in this chapter. Use the following scripts to stop the workers and master processes:</p><pre class="programlisting">
<span class="strong"><strong>$ cd $SPARK_HOME &#13;
$ ./sbin/stop-all.sh</strong></span>
</pre><p>
</p><div class="mediaobject"><img alt="Monitoring with Spark" src="graphics/image_02_004.jpg"/><div class="caption"><p>Figure 3</p></div></div><p>
</p></div>
<div class="section" title="The basics of programming with Spark"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec17"/>The basics of programming with Spark</h1></div></div></div><p>Spark programming revolves around RDDs. In any Spark application, the input data to be processed is taken to create an appropriate RDD. To begin with, start with the most basic way of creating an RDD, which is from a list. The input data used for this <code class="literal">hello world</code> kind of application is a small collection of retail banking transactions. To explain the core concepts, only some very elementary data items have been picked up. The transaction records contain account numbers and transaction amounts.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="tip17"/>Tip</h3><p>In these use cases and all the upcoming use cases in the book, if the term record is used, that will be in the business or use case context.</p></div></div><p>The use cases selected for elucidating the Spark transformations and Spark actions here are given as follows:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">The transaction records are coming as comma-separated values.</li><li class="listitem">Filter out only the good transaction records from the list. The account number should start with <code class="literal">SB</code> and the transaction amount should be greater than zero.</li><li class="listitem">Find all the high value transaction records with a transaction amount greater than 1000.</li><li class="listitem">Find all the transaction records where the account number is bad.</li><li class="listitem">Find all the transaction records where the transaction amount is less than or equal to zero.</li><li class="listitem">Find a combined list of all the bad transaction records.</li><li class="listitem">Find the total of all the transaction amounts.</li><li class="listitem">Find the maximum of all the transaction amounts.</li><li class="listitem">Find the minimum of all the transaction amounts.</li><li class="listitem">Find all the good account numbers.</li></ol></div><p>The approach that is going to be followed throughout the book for any application that is going to be developed begins with the Spark REPL for the appropriate language. Start the Scala REPL for Spark and make sure that it starts without any errors and the prompt is seen. For this application, we will enable monitoring to learn how to do that and use it along with the development process. Other than explicitly starting the Spark master and the slaves separately, Spark comes with a script that will start both of these together using a single script. Then, fire up the Scala REPL with the Spark master URL:</p><pre class="programlisting">
<span class="strong"><strong>$ cd $SPARK_HOME &#13;
$ ./sbin/start-all.sh &#13;
$ ./bin/spark-shell --master spark://Rajanarayanans-MacBook-Pro.local:7077 &#13;
</strong></span>
</pre><p>At the Scala REPL prompt, try the following statements. The output of the statements is given in bold. Note that <code class="literal">scala&gt; </code>is the Scala REPL prompt:</p><pre class="programlisting">
<span class="strong"><strong>scala&gt; val acTransList = Array("SB10001,1000", "SB10002,1200", "SB10003,8000", "SB10004,400", "SB10005,300", "SB10006,10000", "SB10007,500", "SB10008,56", "SB10009,30","SB10010,7000", "CR10001,7000", "SB10002,-10") &#13;
acTransList: Array[String] = Array(SB10001,1000, SB10002,1200, SB10003,8000, SB10004,400, SB10005,300, SB10006,10000, SB10007,500, SB10008,56, SB10009,30, SB10010,7000, CR10001,7000, SB10002,-10) &#13;
scala&gt; val acTransRDD = sc.parallelize(acTransList) &#13;
acTransRDD: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:23 &#13;
scala&gt; val goodTransRecords = acTransRDD.filter(_.split(",")(1).toDouble &gt; 0).filter(_.split(",")(0).startsWith("SB")) &#13;
goodTransRecords: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at filter at &lt;console&gt;:25 &#13;
scala&gt; val highValueTransRecords = goodTransRecords.filter(_.split(",")(1).toDouble &gt; 1000) &#13;
highValueTransRecords: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[3] at filter at &lt;console&gt;:27 &#13;
scala&gt; val badAmountLambda = (trans: String) =&gt; trans.split(",")(1).toDouble &lt;= 0 &#13;
badAmountLambda: String =&gt; Boolean = &lt;function1&gt; &#13;
scala&gt; val badAcNoLambda = (trans: String) =&gt; trans.split(",")(0).startsWith("SB") == false &#13;
badAcNoLambda: String =&gt; Boolean = &lt;function1&gt; &#13;
scala&gt; val badAmountRecords = acTransRDD.filter(badAmountLambda) &#13;
badAmountRecords: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[4] at filter at &lt;console&gt;:27 &#13;
scala&gt; val badAccountRecords = acTransRDD.filter(badAcNoLambda) &#13;
badAccountRecords: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[5] at filter at &lt;console&gt;:27 &#13;
scala&gt; val badTransRecords  = badAmountRecords.union(badAccountRecords) &#13;
badTransRecords: org.apache.spark.rdd.RDD[String] = UnionRDD[6] at union at &lt;console&gt;:33</strong></span>
</pre><p>All the preceding statements fall into one category except the first RDD creation and two function value definitions. They are all Spark transformations. Here is the step-by-step detail capturing what has been done so far:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The value <code class="literal">acTransList</code> is the array containing the comma separated transaction records.</li><li class="listitem" style="list-style-type: disc">The value <code class="literal">acTransRDD</code> is the RDD created out of the array where <code class="literal">sc</code> is the Spark context or the Spark driver and the RDD is created in a parallelized way so that the RDD elements can form a distributed dataset. In other words, an instruction is given to the Spark driver to form a parallel collection or RDD from the given collection of values. </li><li class="listitem" style="list-style-type: disc">The value <code class="literal">goodTransRecords</code> is the RDD created from <code class="literal">acTransRDD</code>after filtering the conditions transaction amount is &gt; 0 and the account number starts with <code class="literal">SB</code>.</li><li class="listitem" style="list-style-type: disc">The value <code class="literal">highValueTransRecords</code> is the RDD created from <code class="literal">goodTransRecords</code>after filtering the conditions transaction amount is &gt; 1000.</li><li class="listitem" style="list-style-type: disc">The next two statements are storing the function definitions in a Scala value for easy reference later.</li><li class="listitem" style="list-style-type: disc">The values <code class="literal">badAmountRecords</code> and <code class="literal">badAccountRecords</code> are RDDs created from <code class="literal">acTransRDD</code> to filter the bad records containing the wrong transaction amount and invalid account number, respectively.</li><li class="listitem" style="list-style-type: disc">The value <code class="literal">badTransRecords</code> contains the union of the elements of both of the <code class="literal">badAmountRecords</code> and <code class="literal">badAccountRecords</code> RDDs.</li></ul></div><p>The Spark web UI for this application so far will not show anything at this point because only Spark transformations have been executed. The real activity will start only after the first Spark action is executed.</p><p>The following statements are the continuation of the already executed statements: </p><pre class="programlisting">
<span class="strong"><strong>scala&gt; acTransRDD.collect() &#13;
res0: Array[String] = Array(SB10001,1000, SB10002,1200, SB10003,8000, SB10004,400, SB10005,300, SB10006,10000, SB10007,500, SB10008,56, SB10009,30, SB10010,7000, CR10001,7000, SB10002,-10) &#13;
scala&gt; goodTransRecords.collect() &#13;
res1: Array[String] = Array(SB10001,1000, SB10002,1200, SB10003,8000, SB10004,400, SB10005,300, SB10006,10000, SB10007,500, SB10008,56, SB10009,30, SB10010,7000) &#13;
scala&gt; highValueTransRecords.collect() &#13;
res2: Array[String] = Array(SB10002,1200, SB10003,8000, SB10006,10000, SB10010,7000) &#13;
scala&gt; badAccountRecords.collect() &#13;
res3: Array[String] = Array(CR10001,7000) &#13;
scala&gt; badAmountRecords.collect() &#13;
res4: Array[String] = Array(SB10002,-10) &#13;
scala&gt; badTransRecords.collect() &#13;
res5: Array[String] = Array(SB10002,-10, CR10001,7000) &#13;
</strong></span>
</pre><p>All the preceding statements did one thing, which is execute a Spark action on the RDDs <span class="emphasis"><em>defined</em></span> earlier. All the evaluations of the RDDs happened only when a Spark action was called on those RDDs. The following statements are doing some of the calculations on the RDDs:</p><pre class="programlisting">
<span class="strong"><strong>scala&gt; val sumAmount = goodTransRecords.map(trans =&gt; trans.split(",")(1).toDouble).reduce(_ + _) &#13;
sumAmount: Double = 28486.0 &#13;
scala&gt; val maxAmount = goodTransRecords.map(trans =&gt; trans.split(",")(1).toDouble).reduce((a, b) =&gt; if (a &gt; b) a else b) &#13;
maxAmount: Double = 10000.0 &#13;
scala&gt; val minAmount = goodTransRecords.map(trans =&gt; trans.split(",")(1).toDouble).reduce((a, b) =&gt; if (a &lt; b) a else b) &#13;
minAmount: Double = 30.0</strong></span>
</pre><p>The preceding numbers calculated the sum, maximum and minimum, of all transaction amounts from the good records. In all the preceding transformations, the transaction records are processed one at a time. From those records, the account number and transaction amount are extracted and processed. It was done like that because the use case requirement was like that. Now the comma-separated values in each transaction record are split without looking at whether it is an account number or a transaction amount. The resulting RDD will contain a collection with all these mixed up. Out of that, if the elements starting with <code class="literal">SB</code> are picked up, that will result in good account numbers. The following statements are going to do that:</p><pre class="programlisting">
<span class="strong"><strong>scala&gt; val combineAllElements = acTransRDD.flatMap(trans =&gt; trans.split(",")) &#13;
combineAllElements: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[10] at flatMap at &lt;console&gt;:25 &#13;
scala&gt; val allGoodAccountNos = combineAllElements.filter(_.startsWith("SB")) &#13;
allGoodAccountNos: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[11] at filter at &lt;console&gt;:27 &#13;
scala&gt; combineAllElements.collect() &#13;
res10: Array[String] = Array(SB10001, 1000, SB10002, 1200, SB10003, 8000, SB10004, 400, SB10005, 300, SB10006, 10000, SB10007, 500, SB10008, 56, SB10009, 30, SB10010, 7000, CR10001, 7000, SB10002, -10) &#13;
scala&gt; allGoodAccountNos.distinct().collect() &#13;
res14: Array[String] = Array(SB10006, SB10010, SB10007, SB10008, SB10009, SB10001, SB10002, SB10003, SB10004, SB10005)</strong></span>
</pre><p>Now at this point, if the Spark web UI is opened, unlike what is seen in Figure 3, one difference can be noticed. Since some Spark actions have been done, an application entry will show up. Since the Scala REPL of Spark is still running, it is shown in the list of applications that are still running. The following Figure 4 captures that:</p><p>
</p><div class="mediaobject"><img alt="The basics of programming with Spark" src="graphics/image_02_005.jpg"/><div class="caption"><p>Figure 4</p></div></div><p>
</p><p>Navigate by clicking on the application ID to see all the metrics related to the running applications including the DAG visualizations and many more.</p><p>These statements covered all the use cases discussed, and it is worth going through the Spark transformations covered so far. These are some of the basic but very important transformations that will be used in most of the applications again and again:</p><div class="informaltable"><table border="1"><colgroup><col/><col/></colgroup><thead><tr><th>
<p>
<span class="strong"><strong>Spark transformation</strong></span>
</p>
</th><th>
<p>
<span class="strong"><strong>What it does</strong></span>
</p>
</th></tr></thead><tbody><tr><td>
<p>
<code class="literal">filter(fn)</code>
</p>
</td><td>
<p><span class="strong"><strong>Iterates through all the elements of the RDD, applies the function that is passed, and picks up the elements that return true as evaluated by the function on the element.</strong></span></p>
</td></tr><tr><td>
<p>
<code class="literal">map(fn)</code>
</p>
</td><td>
<p>Iterates through all the elements of the RDD, applies the function that is passed, and picks up the output returned by the function.</p>
</td></tr><tr><td>
<p>
<code class="literal">flatMap(fn)</code>
</p>
</td><td>
<p>Iterates through all the elements of the RDD, applies the function that is passed, and picks up the output returned by the function. The big difference here as compared to the Spark transformation <code class="literal">map(fn)</code> is that the function acts on a single element and returns a flat collection of elements. For example, it takes one banking transaction record and splits it into multiple fields, resulting in a collection from a single element.</p>
</td></tr><tr><td>
<p>
<code class="literal">union(other)</code>
</p>
</td><td>
<p>Takes the union of all the elements of this RDD and the other RDD.</p>
</td></tr></tbody></table></div><p>It is also worth going through the Spark actions covered so far. These are some of the basic ones, but more actions will be covered in due course.</p><div class="informaltable"><table border="1"><colgroup><col/><col/></colgroup><thead><tr><th>
<p>
<span class="strong"><strong>Spark action</strong></span>
</p>
</th><th>
<p>
<span class="strong"><strong>What it does</strong></span>
</p>
</th></tr></thead><tbody><tr><td>
<p>
<code class="literal">collect()</code>
</p>
</td><td>
<p><span class="strong"><strong>Collects all the elements in the RDD to an array in the Spark driver.</strong></span></p>
</td></tr><tr><td>
<p>
<code class="literal">reduce(fn)</code>
</p>
</td><td>
<p>Applies the function fn on all the elements of the RDD and the final result is calculated as defined by the function. It should be a function that takes two parameters and returns one, which is also commutative and associative.</p>
</td></tr><tr><td>
<p>
<code class="literal">foreach(fn)</code>
</p>
</td><td>
<p>Applies the function fn on all the elements of the RDD. This is mainly used for side effects. The Spark transformation <code class="literal">map(fn)</code> applies the function to all the elements of the RDD and returns another RDD. But the <code class="literal">foreach(fn)</code> Spark transformation does not return an RDD. For example, <code class="literal">foreach(println)</code> will take each element from the RDD and print it onto the console. Even though it is not used in the use cases covered here, it is worth mentioning.</p>
</td></tr></tbody></table></div><p>The next step in the Spark learning process is to try the statements in the Python REPL, covering exactly the same use case. The variable definitions have been maintained as similar as possible in both the languages to have easy assimilation of ideas. There may be minor variations in the way they are used here as compared to the Scala way; conceptually, it is independent of the language of choice.</p><p>Start the Python REPL for Spark and make sure that it starts without any errors and the prompt is seen. While playing around with Scala code, the monitoring was already enabled. Now fire up the Python REPL with the Spark master URL:</p><pre class="programlisting">
<span class="strong"><strong>$ cd $SPARK_HOME &#13;
$ ./bin/pyspark --master spark://Rajanarayanans-MacBook-Pro.local:7077 &#13;
</strong></span>
</pre><p>At the Python REPL prompt, try the following statements. The output of the statements is given in bold. Note that <code class="literal">&gt;&gt;&gt; </code>is the Python REPL prompt:</p><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; from decimal import Decimal &#13;
&gt;&gt;&gt; acTransList = ["SB10001,1000", "SB10002,1200", "SB10003,8000", "SB10004,400", "SB10005,300", "SB10006,10000", "SB10007,500", "SB10008,56", "SB10009,30","SB10010,7000", "CR10001,7000", "SB10002,-10"] &#13;
&gt;&gt;&gt; acTransRDD = sc.parallelize(acTransList) &#13;
&gt;&gt;&gt; goodTransRecords = acTransRDD.filter(lambda trans: Decimal(trans.split(",")[1]) &gt; 0).filter(lambda trans: (trans.split(",")[0]).startswith('SB') == True) &#13;
&gt;&gt;&gt; highValueTransRecords = goodTransRecords.filter(lambda trans: Decimal(trans.split(",")[1]) &gt; 1000) &#13;
&gt;&gt;&gt; badAmountLambda = lambda trans: Decimal(trans.split(",")[1]) &lt;= 0 &#13;
&gt;&gt;&gt; badAcNoLambda = lambda trans: (trans.split(",")[0]).startswith('SB') == False &#13;
&gt;&gt;&gt; badAmountRecords = acTransRDD.filter(badAmountLambda) &#13;
&gt;&gt;&gt; badAccountRecords = acTransRDD.filter(badAcNoLambda) &#13;
&gt;&gt;&gt; badTransRecords  = badAmountRecords.union(badAccountRecords) &#13;
&gt;&gt;&gt; acTransRDD.collect() &#13;
['SB10001,1000', 'SB10002,1200', 'SB10003,8000', 'SB10004,400', 'SB10005,300', 'SB10006,10000', 'SB10007,500', 'SB10008,56', 'SB10009,30', 'SB10010,7000', 'CR10001,7000', 'SB10002,-10'] &#13;
&gt;&gt;&gt; goodTransRecords.collect() &#13;
['SB10001,1000', 'SB10002,1200', 'SB10003,8000', 'SB10004,400', 'SB10005,300', 'SB10006,10000', 'SB10007,500', 'SB10008,56', 'SB10009,30', 'SB10010,7000'] &#13;
&gt;&gt;&gt; highValueTransRecords.collect() &#13;
['SB10002,1200', 'SB10003,8000', 'SB10006,10000', 'SB10010,7000'] &#13;
&gt;&gt;&gt; badAccountRecords.collect() &#13;
['CR10001,7000'] &#13;
&gt;&gt;&gt; badAmountRecords.collect() &#13;
['SB10002,-10'] &#13;
&gt;&gt;&gt; badTransRecords.collect() &#13;
['SB10002,-10', 'CR10001,7000'] &#13;
&gt;&gt;&gt; sumAmounts = goodTransRecords.map(lambda trans: Decimal(trans.split(",")[1])).reduce(lambda a,b : a+b) &#13;
&gt;&gt;&gt; sumAmounts &#13;
Decimal('28486') &#13;
&gt;&gt;&gt; maxAmount = goodTransRecords.map(lambda trans: Decimal(trans.split(",")[1])).reduce(lambda a,b : a if a &gt; b else b) &#13;
&gt;&gt;&gt; maxAmount &#13;
Decimal('10000') &#13;
&gt;&gt;&gt; minAmount = goodTransRecords.map(lambda trans: Decimal(trans.split(",")[1])).reduce(lambda a,b : a if a &lt; b else b) &#13;
&gt;&gt;&gt; minAmount &#13;
Decimal('30') &#13;
&gt;&gt;&gt; combineAllElements = acTransRDD.flatMap(lambda trans: trans.split(",")) &#13;
&gt;&gt;&gt; combineAllElements.collect() &#13;
['SB10001', '1000', 'SB10002', '1200', 'SB10003', '8000', 'SB10004', '400', 'SB10005', '300', 'SB10006', '10000', 'SB10007', '500', 'SB10008', '56', 'SB10009', '30', 'SB10010', '7000', 'CR10001', '7000', 'SB10002', '-10'] &#13;
&gt;&gt;&gt; allGoodAccountNos = combineAllElements.filter(lambda trans: trans.startswith('SB') == True) &#13;
&gt;&gt;&gt; allGoodAccountNos.distinct().collect() &#13;
['SB10005', 'SB10006', 'SB10008', 'SB10002', 'SB10003', 'SB10009', 'SB10010', 'SB10004', 'SB10001', 'SB10007']</strong></span>
</pre><p>The real power of the uniform programming model of Spark is very clearly visible if both the Scala and Python code sets are compared. The Spark transformations and Spark actions are the same in both the language implementations. The way functions are passed into these are different because of the programming language syntax differences. </p><p>Before running the Python REPL for Spark, the Scala REPL was closed and this was done on purpose. Then the Spark web UI should look something similar to that shown in Figure 5. Since the Scala REPL was closed, that is getting listed under the completed applications list. Since the Python REPL is still open, that is getting listed under the running applications list. Note the application names of both the Scala REPL and Python REPL of Spark in the Spark web UI. These are standard names. When custom applications are run from files, there are ways to assign custom names while defining the Spark context object for the applications to facilitate monitoring of the applications and for logging purposes. These details will be covered later in this chapter. </p><p>It is a good idea to spend time with the Spark web UI, getting familiar with all the metrics that are being captured, and how the DAG visualization is given in the UI. It will help a lot while debugging complex Spark applications. </p><p>
</p><div class="mediaobject"><img alt="The basics of programming with Spark" src="graphics/image_02_006.jpg"/><div class="caption"><p>Figure 5</p></div></div><p>
</p><div class="section" title="MapReduce"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec14"/>MapReduce</h2></div></div></div><p>Since day one, Spark has been placed as the replacement for Hadoop MapReduce programs. In general, data processing jobs are done in MapReduce style if that job can be divided into multiple tasks and they can be executed in parallel, and the final results can be computed after collecting the results from all these distributed pieces. Unlike Hadoop MapReduce, Spark can do this even if the DAG of activities is more than the two stages, such as Map and Reduce. Spark is designed to do that and that is one of the biggest value propositions that Spark highlights. </p><p>This section is going to continue with the same retail banking application and pick up some of the use cases that are ideal candidates for the MapReduce kind of data processing.</p><p>The use cases selected for elucidating the MapReduce kind of data processing here are given as follows:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">The retail banking transaction records come with account numbers and the transaction amounts in comma-separated strings.</li><li class="listitem">Pair the transactions to have key/value pairs such as (<code class="literal">AccNo</code>, <code class="literal">TranAmount</code>).</li><li class="listitem">Find an account level summary of all the transactions to get the account balance.</li></ol></div><p>At the Scala REPL prompt, try the following statements:</p><pre class="programlisting">
<span class="strong"><strong>scala&gt; val acTransList = Array("SB10001,1000", "SB10002,1200", "SB10001,8000", "SB10002,400", "SB10003,300", "SB10001,10000", "SB10004,500", "SB10005,56", "SB10003,30","SB10002,7000", "SB10001,-100", "SB10002,-10") &#13;
acTransList: Array[String] = Array(SB10001,1000, SB10002,1200, SB10001,8000, SB10002,400, SB10003,300, SB10001,10000, SB10004,500, SB10005,56, SB10003,30, SB10002,7000, SB10001,-100, SB10002,-10) &#13;
scala&gt; val acTransRDD = sc.parallelize(acTransList) &#13;
acTransRDD: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:23 &#13;
scala&gt; val acKeyVal = acTransRDD.map(trans =&gt; (trans.split(",")(0), trans.split(",")(1).toDouble)) &#13;
acKeyVal: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[1] at map at &lt;console&gt;:25 &#13;
scala&gt; val accSummary = acKeyVal.reduceByKey(_ + _).sortByKey() &#13;
accSummary: org.apache.spark.rdd.RDD[(String, Double)] = ShuffledRDD[5] at sortByKey at &lt;console&gt;:27 &#13;
scala&gt; accSummary.collect() &#13;
res0: Array[(String, Double)] = Array((SB10001,18900.0), (SB10002,8590.0), (SB10003,330.0), (SB10004,500.0), (SB10005,56.0)) &#13;
</strong></span>
</pre><p>Here is the step-by-step detail capturing what has been done so far:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">The value <code class="literal">acTransList</code> is the array containing the comma-separated transaction records.</li><li class="listitem">The value <code class="literal">acTransRDD</code> is the RDD created out of the array, where sc is the Spark context or the Spark driver and the RDD is created in a parallelized way so that the RDD elements can form a distributed dataset.</li><li class="listitem">Transform the <code class="literal">acTransRDD</code> to <code class="literal">acKeyVal</code> to have key-value pairs of the form (K,V), where the account number is chosen as the key. In this set of elements in the RDD, there will be multiple elements with the same key. </li><li class="listitem">In the next step, the key-value pairs are grouped by the key and a reduction function has been passed, which will add the transaction amount to form key-value pairs containing one element for a specific key in the RDD and the total of all the amounts for the same key. Then sort the elements on the key before producing the final result.</li><li class="listitem">Collect the elements to an array at the driver level.</li></ol></div><p>Assuming that the RDD <code class="literal">acKeyVal</code> is partitioned into two parts and distributed to a cluster for processing, Figure 6 captures the essence of the processing: </p><p>
</p><div class="mediaobject"><img alt="MapReduce" src="graphics/image_02_008.jpg"/><div class="caption"><p>Figure 6</p></div></div><p>
</p><p>The following table captures the Spark actions that are introduced in this use case:</p><div class="informaltable"><table border="1"><colgroup><col/><col/></colgroup><thead><tr><th>
<p>
<span class="strong"><strong>Spark action</strong></span>
</p>
</th><th>
<p>
<span class="strong"><strong>What it does?</strong></span>
</p>
</th></tr></thead><tbody><tr><td>
<p>
<code class="literal">reduceByKey(fn,[noOfTasks])</code>
</p>
</td><td>
<p><span class="strong"><strong>Applies the function fn on the RDD of the form (K,V) and is reduced to remove duplicate keys and apply the function passed as the parameter to be acted on the values at the key level.</strong></span></p>
</td></tr><tr><td>
<p>
<code class="literal">sortByKey([ascending], [numTasks])</code>
</p>
</td><td>
<p>Sorts the RDD elements if the RDD is of the form (K,V) by its key K</p>
</td></tr></tbody></table></div><p>The <code class="literal">reduceByKey</code>action deserves a special mention. In Figure 6, the grouping of the elements by the key is a well-known operation. But in the next step, for the same key, the function passed to as a parameter takes two parameters and returns one. It is not very intuitive to get this right and you may be wondering from where the two inputs are coming while iterating through the values of the (K,V) pair for each key. This behavior takes the concept from the Scala collection method <code class="literal">reduceLeft</code>. The following Figure 7, with the values of the key <span class="strong"><strong>SB10001</strong></span> doing the <code class="literal">reduceByKey(_ + _)</code>operation, is an attempt to explain the concept. This is just for the elucidation purposes of this example and the actual Spark implementation to do the same may be different:</p><p>
</p><div class="mediaobject"><img alt="MapReduce" src="graphics/image_02_010.jpg"/><div class="caption"><p>Figure 7</p></div></div><p>
</p><p>On the right-hand side of Figure 7, the <code class="literal">reduceLeft </code>operation of the Scala collection method is illustrated. That is an attempt to give some insight into from where the two parameters are coming for the <code class="literal">reduceLeft</code>function. As a matter of fact, many of the transformations that are being used on Spark RDD are adapted from Scala collection methods. </p><p>At the Python REPL prompt, try the following statements:</p><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; from decimal import Decimal &#13;
&gt;&gt;&gt; acTransList = ["SB10001,1000", "SB10002,1200", "SB10001,8000", "SB10002,400", "SB10003,300", "SB10001,10000", "SB10004,500", "SB10005,56", "SB10003,30","SB10002,7000", "SB10001,-100", "SB10002,-10"] &#13;
&gt;&gt;&gt; acTransRDD = sc.parallelize(acTransList) &#13;
&gt;&gt;&gt; acKeyVal = acTransRDD.map(lambda trans: (trans.split(",")[0],Decimal(trans.split(",")[1]))) &#13;
&gt;&gt;&gt; accSummary = acKeyVal.reduceByKey(lambda a,b : a+b).sortByKey() &#13;
&gt;&gt;&gt; accSummary.collect() &#13;
[('SB10001', Decimal('18900')), ('SB10002', Decimal('8590')), ('SB10003', Decimal('330')), ('SB10004', Decimal('500')), ('SB10005', Decimal('56'))] &#13;
</strong></span>
</pre><p>The <code class="literal">reduceByKey </code>took an input parameter, which is a function. Similar to this, there is another transformation that does the key-based operation in a slightly different way. It is <code class="literal">groupByKey()</code>. This gathers all the values of a given key and forms the list of values from all the individual elements. </p><p>If there is a need to do multiple levels of processing with the same value elements as a collection for each key, this is the suitable transformation. In other words, if there are many (K,V) pairs, this transformation returns (K, Iterable&lt;V&gt;) for each key. </p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="tip18"/>Tip</h3><p>The only thing the developer needs to be cognizant about is to make sure that the number of such (K,V) pairs is not really huge so that the operation doesn't create performance problems. There is no hard and fast rule to find this out and it rather depends on the use case. </p></div></div><p>In all the preceding code snippets, for extracting account numbers or any other field from the comma-separated transaction record, split(<code class="literal">,</code>) is used multiple times within the <code class="literal">map()</code> transformation. This is to demonstrate the use of array elements within <code class="literal">map()</code>, or any other transformation or method. A better way of extracting the fields of the transaction record is to transform them as a tuple containing the required fields and then use the fields from the tuple to employ them in some of the following code snippets. In this way, there is no need to call split (<code class="literal">,</code>) repeatedly for each field extraction.</p></div><div class="section" title="Joins"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec15"/>Joins</h2></div></div></div><p>In the <span class="strong"><strong>Relational Database Management Systems</strong></span> (<span class="strong"><strong>RDBMS</strong></span>) world, joining multiple tables rows based on a key is a very common practice. When it comes to the NoSQL data stores, joining multiple tables became a real problem because many of the NoSQL data stores don't have support for the table joins. In the NoSQL world, redundancy is allowed. Whether a technology supports table joins or not, business use cases mandate joins of datasets based on keys all the time. Because of this, it is imperative to have the joins done in a batch mode in many of the use cases. </p><p>Spark provides transformations to join multiple RDDs based on a key. This supports many use cases. These days there are many NoSQL data stores having connectors to talk to Spark. When working with such data stores, it is very simple to construct RDDs of data from multiple tables, do the join from Spark, and store the results back into the data stores in batch mode or even in near-to-real-time mode. Spark transformations are available for left outer join and right outer join, as well as full outer join. </p><p>The use cases selected for elucidating the join of multiple datasets using a key are given as follows.</p><p>The first dataset contains a retail banking master records summary with an account number, first name, and last name. The second dataset contains the retail banking account balance with an account number, and balance amount. The key on both of the datasets is the account number. Join the two datasets and create one dataset containing the account number, full name, and balance amount.</p><p>At the Scala REPL prompt, try the following statements:</p><pre class="programlisting">
<span class="strong"><strong>scala&gt; val acMasterList = Array("SB10001,Roger,Federer", "SB10002,Pete,Sampras", "SB10003,Rafael,Nadal", "SB10004,Boris,Becker", "SB10005,Ivan,Lendl") &#13;
acMasterList: Array[String] = Array(SB10001,Roger,Federer, SB10002,Pete,Sampras, SB10003,Rafel,Nadal, SB10004,Boris,Becker, SB10005,Ivan,Lendl) &#13;
scala&gt; val acBalList = Array("SB10001,50000", "SB10002,12000", "SB10003,3000", "SB10004,8500", "SB10005,5000") &#13;
acBalList: Array[String] = Array(SB10001,50000, SB10002,12000, SB10003,3000, SB10004,8500, SB10005,5000) &#13;
scala&gt; val acMasterRDD = sc.parallelize(acMasterList) &#13;
acMasterRDD: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[0] at parallelize at &lt;console&gt;:23 &#13;
scala&gt; val acBalRDD = sc.parallelize(acBalList) &#13;
acBalRDD: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[1] at parallelize at &lt;console&gt;:23 &#13;
scala&gt; val acMasterTuples = acMasterRDD.map(master =&gt; master.split(",")).map(masterList =&gt; (masterList(0), masterList(1) + " " + masterList(2))) &#13;
acMasterTuples: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[3] at map at &lt;console&gt;:25 &#13;
scala&gt; val acBalTuples = acBalRDD.map(trans =&gt; trans.split(",")).map(transList =&gt; (transList(0), transList(1))) &#13;
acBalTuples: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[5] at map at &lt;console&gt;:25 &#13;
scala&gt; val acJoinTuples = acMasterTuples.join(acBalTuples).sortByKey().map{case (accno, (name, amount)) =&gt; (accno, name,amount)} &#13;
acJoinTuples: org.apache.spark.rdd.RDD[(String, String, String)] = MapPartitionsRDD[12] at map at &lt;console&gt;:33 &#13;
scala&gt; acJoinTuples.collect() &#13;
res0: Array[(String, String, String)] = Array((SB10001,Roger Federer,50000), (SB10002,Pete Sampras,12000), (SB10003,Rafael Nadal,3000), (SB10004,Boris Becker,8500), (SB10005,Ivan Lendl,5000)) &#13;
</strong></span>
</pre><p>All the statements given previously must be familiar by now, except the Spark transformation join. Similar to this transformation, the <code class="literal">leftOuterJoin</code>, <code class="literal">rightOuterJoin</code>, and <code class="literal">fullOuterJoin</code>are also available with the same usage pattern:</p><div class="informaltable"><table border="1"><colgroup><col/><col/></colgroup><thead><tr><th>
<p>
<span class="strong"><strong>Spark transformation</strong></span>
</p>
</th><th>
<p>
<span class="strong"><strong>What it does</strong></span>
</p>
</th></tr></thead><tbody><tr><td>
<p>
<code class="literal">join(other, [numTasks])</code>
</p>
</td><td>
<p><span class="strong"><strong>Joins this RDD with the other RDD, and the elements are joined together based on the key. Suppose the original RDD is of the form (K,V1) and the second RDD is of the form (K,V2), then the join operation will produce tuples of the form (K, (V1,V2)) with all the pairs of each key.</strong></span></p>
</td></tr></tbody></table></div><p>At the Python REPL prompt, try the following statements:</p><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; acMasterList = ["SB10001,Roger,Federer", "SB10002,Pete,Sampras", "SB10003,Rafael,Nadal", "SB10004,Boris,Becker", "SB10005,Ivan,Lendl"] &#13;
&gt;&gt;&gt; acBalList = ["SB10001,50000", "SB10002,12000", "SB10003,3000", "SB10004,8500", "SB10005,5000"] &#13;
&gt;&gt;&gt; acMasterRDD = sc.parallelize(acMasterList) &#13;
&gt;&gt;&gt; acBalRDD = sc.parallelize(acBalList) &#13;
&gt;&gt;&gt; acMasterTuples = acMasterRDD.map(lambda master: master.split(",")).map(lambda masterList: (masterList[0], masterList[1] + " " + masterList[2])) &#13;
&gt;&gt;&gt; acBalTuples = acBalRDD.map(lambda trans: trans.split(",")).map(lambda transList: (transList[0], transList[1])) &#13;
&gt;&gt;&gt; acJoinTuples = acMasterTuples.join(acBalTuples).sortByKey().map(lambda tran: (tran[0], tran[1][0],tran[1][1])) &#13;
&gt;&gt;&gt; acJoinTuples.collect() &#13;
[('SB10001', 'Roger Federer', '50000'), ('SB10002', 'Pete Sampras', '12000'), ('SB10003', 'Rafael Nadal', '3000'), ('SB10004', 'Boris Becker', '8500'), ('SB10005', 'Ivan Lendl', '5000')] &#13;
</strong></span>
</pre></div><div class="section" title="More actions"><div class="titlepage"><div><div><h2 class="title"><a id="ch02lvl2sec16"/>More actions</h2></div></div></div><p>So far, the focus was mainly on Spark transformations. Spark actions are also important. To get insight into some more important Spark actions, take the following use cases, continuing from where it was stopped in the preceding section's use cases: </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">From the list containing account numbers, names, and account balances, get the one that has the highest account balance</li><li class="listitem" style="list-style-type: disc">From the list containing account numbers, names, and account balances, get the top three having the highest account balance</li><li class="listitem" style="list-style-type: disc">Count the number of balance transaction records at an account level</li><li class="listitem" style="list-style-type: disc">Count the total number of balance transaction records</li><li class="listitem" style="list-style-type: disc">Print the name and account balance of all the accounts</li><li class="listitem" style="list-style-type: disc">Calculate the total of the account balance</li></ul></div><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="tip19"/>Tip</h3><p>It is a very common requirement to iterate through the elements in a collection, do some mathematical calculation on each of the elements, and at the end of it, use the result. The RDD is partitioned and distributed across worker nodes. If any normal variable is used for storing the cumulative result while iterating through the RDD elements, it may not yield the correct result. In such situations, instead of using regular variables, use Spark provided accumulators.</p></div></div><p>At the Scala REPL prompt, try the following statements:</p><pre class="programlisting">
<span class="strong"><strong>scala&gt; val acNameAndBalance = acJoinTuples.map{case (accno, name,amount) =&gt; (name,amount)} &#13;
acNameAndBalance: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[46] at map at &lt;console&gt;:35 &#13;
scala&gt; val acTuplesByAmount = acBalTuples.map{case (accno, amount) =&gt; (amount.toDouble, accno)}.sortByKey(false) &#13;
acTuplesByAmount: org.apache.spark.rdd.RDD[(Double, String)] = ShuffledRDD[50] at sortByKey at &lt;console&gt;:27 &#13;
scala&gt; acTuplesByAmount.first() &#13;
res19: (Double, String) = (50000.0,SB10001) &#13;
scala&gt; acTuplesByAmount.take(3) &#13;
res20: Array[(Double, String)] = Array((50000.0,SB10001), (12000.0,SB10002), (8500.0,SB10004)) &#13;
scala&gt; acBalTuples.countByKey() &#13;
res21: scala.collection.Map[String,Long] = Map(SB10001 -&gt; 1, SB10005 -&gt; 1, SB10004 -&gt; 1, SB10002 -&gt; 1, SB10003 -&gt; 1) &#13;
scala&gt; acBalTuples.count() &#13;
res22: Long = 5 &#13;
scala&gt; acNameAndBalance.foreach(println) &#13;
(Boris Becker,8500) &#13;
(Rafel Nadal,3000) &#13;
(Roger Federer,50000) &#13;
(Pete Sampras,12000) &#13;
(Ivan Lendl,5000) &#13;
scala&gt; val balanceTotal = sc.accumulator(0.0, "Account Balance Total") &#13;
balanceTotal: org.apache.spark.Accumulator[Double] = 0.0 &#13;
scala&gt; acBalTuples.map{case (accno, amount) =&gt; amount.toDouble}.foreach(bal =&gt; balanceTotal += bal) &#13;
scala&gt; balanceTotal.value &#13;
res8: Double = 78500.0) &#13;
</strong></span>
</pre><p>The following table captures the Spark actions that are introduced in this use case:</p><div class="informaltable"><table border="1"><colgroup><col/><col/></colgroup><thead><tr><th>
<p>
<span class="strong"><strong>Spark action</strong></span>
</p>
</th><th>
<p>
<span class="strong"><strong>What it does</strong></span>
</p>
</th></tr></thead><tbody><tr><td>
<p>
<code class="literal">first()</code>
</p>
</td><td>
<p><span class="strong"><strong>Returns the first element in the RDD.</strong></span></p>
</td></tr><tr><td>
<p>
<code class="literal">take(n)</code>
</p>
</td><td>
<p>Returns an array of the first <code class="literal">n</code> elements from the RDD.</p>
</td></tr><tr><td>
<p>
<code class="literal">countByKey()</code>
</p>
</td><td>
<p>Returns the count of elements by the key. If the RDD contains (K,V) pairs, this will return a dictionary of <code class="literal">(K, numOfValues)</code>.</p>
</td></tr><tr><td>
<p>
<code class="literal">count()</code>
</p>
</td><td>
<p>Returns the number of elements in the RDD.</p>
</td></tr><tr><td>
<p>
<code class="literal">foreach(fn)</code>
</p>
</td><td>
<p>Applies the function fn to each element in the RDD. In the preceding use case, Spark Accumulator is being used with <code class="literal">foreach(fn)</code>.</p>
</td></tr></tbody></table></div><p>At the Python REPL prompt, try the following statements:</p><pre class="programlisting">
<span class="strong"><strong>&gt;&gt;&gt; acNameAndBalance = acJoinTuples.map(lambda tran: (tran[1],tran[2])) &#13;
&gt;&gt;&gt; acTuplesByAmount = acBalTuples.map(lambda tran: (Decimal(tran[1]), tran[0])).sortByKey(False) &#13;
&gt;&gt;&gt; acTuplesByAmount.first() &#13;
(Decimal('50000'), 'SB10001') &#13;
&gt;&gt;&gt; acTuplesByAmount.take(3) &#13;
[(Decimal('50000'), 'SB10001'), (Decimal('12000'), 'SB10002'), (Decimal('8500'), 'SB10004')] &#13;
&gt;&gt;&gt; acBalTuples.countByKey() &#13;
defaultdict(&lt;class 'int'&gt;, {'SB10005': 1, 'SB10002': 1, 'SB10003': 1, 'SB10004': 1, 'SB10001': 1}) &#13;
&gt;&gt;&gt; acBalTuples.count() &#13;
5 &#13;
&gt;&gt;&gt; acNameAndBalance.foreach(print) &#13;
('Pete Sampras', '12000') &#13;
('Roger Federer', '50000') &#13;
('Rafael Nadal', '3000') &#13;
('Boris Becker', '8500') &#13;
('Ivan Lendl', '5000') &#13;
&gt;&gt;&gt; balanceTotal = sc.accumulator(0.0) &#13;
&gt;&gt;&gt; balanceTotal.value0.0&gt;&gt;&gt; acBalTuples.foreach(lambda bals: balanceTotal.add(float(bals[1]))) &#13;
&gt;&gt;&gt; balanceTotal.value &#13;
78500.0</strong></span>
</pre></div></div>
<div class="section" title="Creating RDDs from files"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec18"/>Creating RDDs from files</h1></div></div></div><p>So far, the focus of the discussion was on the RDD functionality and programming with RDDs. In all the preceding use cases, the RDD creation was done from the collection objects. But in the real-world use cases, the data will come from files stored in the local filesystems, and HDFS. Quite often, the data will come from NoSQL data stores such as Cassandra. It is possible to create RDDs by reading the contents from these data sources. Once RDD is created, then all the operations are uniform, as given in the preceding use cases. The data files coming out of the filesystems may be fixed width, comma-separated, or any other format. But the common pattern used for reading such data files is to read the data line by line and split the line to have the necessary separation of data items. In the case of data coming from other sources, the appropriate Spark connector program is to be used and the appropriate API for reading data is to be used. </p><p>Many third-party libraries are available to read the contents from various types of text files. For example, the Spark CSV library available from GitHub is a very useful one for creating RDDs from CSV files.</p><p>The following table captures the way text files are read from various sources, such as local filesystems, HDFS, and so on. As discussed earlier, the processing of the text file is up to the use case requirements: </p><div class="informaltable"><table border="1"><colgroup><col/><col/><col/></colgroup><thead><tr><th>
<p>
<span class="strong"><strong>File location</strong></span>
</p>
</th><th>
<p>
<span class="strong"><strong>RDD creation</strong></span>
</p>
</th><th>
<p>
<span class="strong"><strong>What it does</strong></span>
</p>
</th></tr></thead><tbody><tr><td>
<p>Local filesystem</p>
</td><td>
<p>
<code class="literal">val textFile = sc.textFile("README.md")</code>
</p>
</td><td>
<p><span class="strong"><strong>Creates an RDD by reading the contents of the file named</strong></span> <code class="literal">README.md</code> <span class="strong"><strong>from the directory from where the Spark shell is invoked. Here, the RDD is of the type RDD[string] and the elements will be lines from the file.</strong></span></p>
</td></tr><tr><td>
<p>HDFS</p>
</td><td>
<p>
<code class="literal">val textFile = sc.textFile ("hdfs://&lt;location in HDFS&gt;")</code>
</p>
</td><td>
<p>Creates an RDD by reading the contents of the file specified in the HDFS URL</p>
</td></tr></tbody></table></div><p>The most important aspect while reading the files from the local filesystem is that the file should be available in all the nodes of the Spark worker nodes. Apart from these two file locations given in the preceding table, any supported filesystem URI may be used. </p><p>Just like reading the contents from files in various filesystems, it is also possible to write the RDD onto files using the <code class="literal">saveAsTextFile</code>(path) Spark action.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="tip20"/>Tip</h3><p>All the Spark application use cases discussed here are run on the appropriate language's REPL of Spark. When writing applications, they will be written in proper source code files. In the case of Scala and Java, the application code files have to be compiled, packaged, and run with proper library dependencies, and are typically built using maven or sbt. This will be covered in detail when designing data processing applications using Spark, in the last chapter of this book.</p></div></div></div>
<div class="section" title="Understanding the Spark library stack"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec19"/>Understanding the Spark library stack</h1></div></div></div><p>Spark comes with a core data processing engine and a stack of libraries working on top of the core engine. It is very important to understand the concept of stacking libraries on top of the core framework.</p><p>All these libraries that are making use of the services provided by the core framework support the data abstractions offered by the core framework and much more. Before Spark came onto market, there were lots of independent open source products doing what the library stack in discussion here is now doing. The biggest disadvantage with these point products was their interoperability. They don't stack together well. They were implemented in different programming languages. The programming language of choice supported by these products, and the lack of uniformity in the APIs exposed by these products, were really challenging to get one application done with two or more such products. That is the relevance of the stack of libraries that work on top of Spark. They all work together with the same programming model. This helps organizations to standardize on the data processing toolset without vendorlock-in.</p><p>Spark comes with the following stack of domain-specific libraries, and Figure 8 gives a comprehensive picture of the whole ecosystem as seen by a developer: </p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Spark SQL</strong></span></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Spark Streaming</strong></span></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Spark MLlib</strong></span></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Spark GraphX</strong></span></li></ul></div><p>
</p><div class="mediaobject"><img alt="Understanding the Spark library stack" src="graphics/image_02_012.jpg"/><div class="caption"><p>Figure 8</p></div></div><p>
</p><p>In any organization, structured data is still very widely used. The most ubiquitous data access mechanism with structured data is SQL. Spark SQL provides the capability to write SQL-like queries on top of the structured data abstraction called the DataFrame API. DataFrame and SQL go very well and support data coming from various sources, such as Hive, Avro, Parquet, JSON, and many more. Once the data is loaded into the Spark context, they can be operated as if they are all coming from the same source. In other words, if required, SQL-like queries can be used to join data coming from different sources, such as Hive and JSON. Another big advantage that Spark SQL and the DataFrame API bring onto the developers table is the ease of use and no need-to-know functional programming methods, which is a requirement to do programming with RDDs.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="tip21"/>Tip</h3><p>Using Spark SQL and the DataFrame API, data can be read from various data sources and processed as if it is all coming from a unified source. Spark transformations and Spark actions support uniform programming interfaces. So the data source unification, API unification, and ability to use multiple programming languages to write data processing applications help the organizations to standardize on one data processing framework.</p></div></div><p>The data ingestion into the organizational data sinks is increasing every day. At the same time, the velocity at which data is getting ingested is also increasing. Spark Streaming provides the library to process the data that is ingested from various sources at a very high velocity.</p><p>In the past, data scientists had the challenge of building their own implementations of the machine learning algorithms and utilities in their programming language of choice. Quite often, such programming languages don't interoperate with the data processing toolset of the organization. Spark MLlib provides the unification process, where it comes with a lot of machine learning algorithms and utilities working on top of the Spark data processing engine. </p><p>The IoT applications, especially the social media applications, mandated the need to have data processing capabilities where the data fits into a graph-like structure. For example, the connections in LinkedIn, relationship between friends in Facebook, workflow applications, and many such use cases, make use of the graph abstraction extensively. Using the graph to do various computations requires very high data processing capabilities and employment of sophisticated algorithms. Spark GraphX library comes with an API for graphs and makes use of Spark's parallel computing paradigm.</p><div class="note" style="" title="Note"><div class="inner"><h3 class="title"><a id="tip22"/>Tip</h3><p>There are many Spark libraries available that are developed by the community for various purposes. Many such third-party library packages are featured in the site <a class="ulink" href="http://spark-packages.org/">http://spark-packages.org/</a>. The number of packages is growing day by day as the Spark user community is growing. When developing data processing applications in Spark, if there is a need to have a domain-specific library, it would be a good idea to check this site first and see whether anybody has already developed it. </p></div></div></div>
<div class="section" title="Reference"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec20"/>Reference</h1></div></div></div><p>For more information plese visit:<a class="ulink" href="https://github.com/databricks/spark-csv">https://github.com/databricks/spark-csv</a>
</p></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch02lvl1sec21"/>Summary</h1></div></div></div><p>This chapter discussed the basic programming model of Spark with its primary dataset abstraction RDDs. The creation of RDDs from various data sources, and processing of the data in RDDs using Spark transformations and Spark actions, were covered using Scala and Python APIs. All the important features of the Spark programming model were covered with the help of real-world use cases. This chapter also discussed the library stack that comes with Spark and what each one is doing. In summary, Spark comes with a very user-friendly programming model and in turn provides a very powerful data processing toolset.</p><p>The next chapter will discuss the Dataset API and the DataFrame API. The Dataset API is going to be the new way of programming with Spark, while the DataFrame API deals with more structured data. Spark SQL is also introduced to manipulate structured data and show how that can be intermixed with any Spark data processing application.</p></div></body></html>