<html><head></head><body><div><div><h1 id="_idParaDest-101"><a id="_idTextAnchor103"/>Chapter 6: Putting Everything Together: Semantic Parsing with spaCy</h1>
			<p>This is a purely hands-on section. In this chapter, we will apply what we have learned hitherto to <strong class="bold">Airline Travel Information System</strong> (<strong class="bold">ATIS</strong>), a well-known airplane ticket reservation system dataset. First of all, we will get to know our dataset and make the basic statistics. As the first <strong class="bold">natural language understanding </strong>(<strong class="bold">NLU</strong>) task, we will extract the named entities with two different methods, with spaCy Matcher, and by walking on the dependency tree.</p>
			<p>The next task is to determine the intent of the user utterance. We will explore intent recognition in different ways, too: by extracting the verbs and their direct objects, by using wordlists, and by walking on the dependency tree to recognize multiple intents. Then you will match your keywords to synonyms from a synonyms list to detect semantic similarity.</p>
			<p>Also, you'll do keyword matching with word vector-based semantic similarity methods. Finally, we will combine all this information to generate a semantic representation for the dataset utterances. </p>
			<p>By the end of this chapter, you'll learn how to semantically process a real-world dataset completely. You'll learn how to extract entities, recognize intents, and perform semantic similarity calculations. The tools of this chapter are really what you'll build for a real-world <strong class="bold">natural language processing </strong>(<strong class="bold">NLP</strong>) pipeline, including an NLU chatbot and an NLU customer support application.</p>
			<p>In this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li>Extracting named entities</li>
				<li>Using dependency relations for intent recognition</li>
				<li>Semantic similarity methods for semantic parsing</li>
				<li>Putting it all together</li>
			</ul>
			<h1 id="_idParaDest-102"><a id="_idTextAnchor104"/>Technical requirements</h1>
			<p>In this chapter, we'll process a dataset. The dataset and the chapter code can be found at <a href="https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter06">https://github.com/PacktPublishing/Mastering-spaCy/tree/main/Chapter06</a>.</p>
			<p>We used the pandas library of Python to manipulate our dataset, besides using spaCy. We also used the awk command-line tool. pandas can be installed via pip and awk is preinstalled in many Linux distributions.</p>
			<h1 id="_idParaDest-103"><a id="_idTextAnchor105"/>Extracting named entities</h1>
			<p>In many NLP applications, including <a id="_idIndexMarker355"/>semantic parsing, we start looking for meaning in a text by examining the entity types and placing an entity extraction component into our NLP pipelines. <strong class="bold">Named entities</strong> play a key role in understanding the meaning of user text.</p>
			<p>We'll also start a semantic parsing pipeline by extracting the named entities from our corpus. To understand what sort of entities we want to extract, first, we'll get to know the ATIS dataset.</p>
			<h2 id="_idParaDest-104"><a id="_idTextAnchor106"/>Getting to know the ATIS dataset</h2>
			<p>Throughout this chapter, we'll<a id="_idIndexMarker356"/> work with the ATIS corpus. ATIS is a well-known dataset; it's one of the standard benchmark datasets for intent classification. The dataset consists of customer utterances who want to book a flight, get information about the flights, including flight costs, flight destinations, and timetables.</p>
			<p>No matter what the NLP task is, you should always go over your corpus with a naked eye. We want to get to know our corpus so that we integrate our observations of corpus into our code. While viewing our text data, we usually keep an eye on the following:</p>
			<ul>
				<li>What kind of utterances are there? Is it a short text corpus or does the corpus consist of long documents or medium-length paragraphs?</li>
				<li>What sort of entities does the corpus include? People's names, city names, country names, organization names, and so on. Which ones do we want to extract?</li>
				<li>How is punctuation used? Is the text correctly punctuated, or is no punctuation used at all?</li>
				<li>How are the grammatical rules followed? Is the capitalization correct? Did users follow the grammatical rules? Are there misspelled words?</li>
			</ul>
			<p>Before starting any processing, we'll examine our corpus. Let's go ahead and download the dataset:</p>
			<pre>$ wget 
<a href="https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter06/data/atis_intents.csv">https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter06/data/atis_intents.csv</a></pre>
			<p>The dataset is a two-column <a id="_idIndexMarker357"/>CSV file. First, we'll get some insights into the dataset <a id="_idIndexMarker358"/>statistics with <strong class="bold">pandas</strong>. pandas is a popular data manipulation library that is frequently used by data scientists. You can read more at <a href="https://pandas.pydata.org/pandas-docs/version/0.15/tutorials.html">https://pandas.pydata.org/pandas-docs/version/0.15/tutorials.html</a>:</p>
			<ol>
				<li>Let's begin by reading the CSV file into Python. We'll use the <code>read_csv</code> method of pandas:<pre>import pandas as pd
dataset = pd.read_csv("data/atis_intents.csv", header=None)</pre><p>The dataset variable holds the CSV as an object for us.</p></li>
				<li>Next, we'll call <code>head()</code> on the dataset object. <code>head()</code> outputs the first 10 columns of the dataset:<pre>dataset.<strong class="bold">head()</strong></pre><p>The result looks as follows:</p><div><img src="img/B16570_6_1.jpg" alt="Figure 6.1 – Overview of the dataset&#13;&#10;" width="601" height="666"/></div><p class="figure-caption">Figure 6.1 – Overview of the dataset</p><p>As you see, the dataset <a id="_idIndexMarker359"/>object contains rows and columns. It is indeed a CSV object. The first column contains the intent, and the second column contains the user utterance.</p></li>
				<li>Now we can print some example utterances:<pre>for text in dataset[1].head():
    print(text)
i want to fly from boston at 838 am and arrive in denver at 1110 in the morning
what flights are available from pittsburgh to baltimore on thursday morning
what is the arrival time in san francisco for the 755 am flight leaving washington
cheapest airfare from tacoma to orlando
round trip fares from pittsburgh to philadelphia under 1000 dollars</pre><p>As we can see, the first user wants to book a flight; they included the destination, the source <a id="_idIndexMarker360"/>cities, and the flight time. The third user is asking about the arrival time of a specific flight and the fifth user made a query with a price limit. The utterances are not capitalized or punctuated. This is because these utterances are an output of a speech-to-text engine.</p></li>
				<li>Last, we can see the distribution of the number of utterances by intent:<pre>grouped = dataset.groupby(0).size()
print(grouped)
atis_abbreviation                            147
atis_aircraft                                 81
atis_aircraft#atis_flight#atis_flight_no      1
atis_airfare                                 423
atis_airfare#atis_flight_time                  1
atis_airline                                 157
atis_airline#atis_flight_no                    2
atis_airport                                  20
atis_capacity                                 16
atis_cheapest                                  1
atis_city                                     19
atis_distance                                 20
atis_flight                                 3666</pre><p>You can find the dataset exploration code at the book's GitHub repository under <a href="https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter06/ATIS_dataset_exploration.ipynb">https://github.com/PacktPublishing/Mastering-spaCy/blob/main/Chapter06/ATIS_dataset_exploration.ipynb</a>.</p></li>
				<li>After this point, we'll process just the utterance text. Hence, we can drop the first column. To do so, we'll play a small trick with the Unix tool awk:<pre>awk -F '<strong class="bold">,</strong>' '{print $2}' atis_intents.csv  &gt; <strong class="bold">atis_utterances.txt</strong></pre></li>
			</ol>
			<p>Here, we printed the second <a id="_idIndexMarker361"/>column of the input CSV file (where the filed separator is a <code>,</code>) and directed the output into a text file called <code>atis_utterances.txt</code>. Now that our utterances are ready to process, we can go ahead and extract the entities.</p>
			<h2 id="_idParaDest-105"><a id="_idTextAnchor107"/>Extracting named entities with Matcher</h2>
			<p>As we have already seen, this is a <a id="_idIndexMarker362"/>flights dataset. Hence, we expect to<a id="_idIndexMarker363"/> see city/country names, airport names, and airline names:</p>
			<ol>
				<li value="1">Here are some examples:<pre>does american airlines fly from boston to san francisco
what flights go from dallas to tampa
show me the flights from montreal to chicago
what flights do you have from ontario
The users also provide the dates, times, days of the weeks they wish to fly on. These entities include numbers, month names, day of the week names as well as time adverbs such as next week, today, tomorrow, next month. Let's see some example entities:
list flights from atlanta to boston leaving between 6 pm and 10 pm on august eighth
i need a flight after 6 pm on wednesday from oakland to salt lake city
show me flights from minneapolis to seattle on july second
what flights leave after 7 pm from philadelphia to boston</pre></li>
				<li>Also, the <code>atis_abbreviation</code> intent contains utterances that are inquiries about some abbreviations. Flight <a id="_idIndexMarker364"/>abbreviations can be <a id="_idIndexMarker365"/>fare codes (for example, M = Economy), airline name codes (for example, United Airlines = UA), and airport codes (for example, Berlin Airport = BER), and so on. Examples include the following: <pre>what does the abbreviation ua mean
what does restriction ap 57 mean
explain restriction ap please
what's fare code yn</pre></li>
				<li>Let's visualize some utterances from the dataset. The following screenshot shows the entities with their types: <div><img src="img/B16570_6_2.jpg" alt="Figure 6.2 – Example corpus sentences with entities and entity types highlighted; generated by the displaCy online demo" width="1012" height="605"/></div><p class="figure-caption">Figure 6.2 – Example corpus sentences with entities and entity types highlighted; generated by the displaCy online demo</p></li>
				<li>We can see all the entity types and their frequencies more systematically. The following code segment makes the following actions:<p>a) It reads the text file of <a id="_idIndexMarker366"/>utterances that we <a id="_idIndexMarker367"/>created in the preceding dataset exploration subsection.</p><p>b) It iterates over each utterance and creates a Doc object.</p><p>c) It extracts entities from the current doc object.</p><p>d) It updates the global entity label list with the labels of the entities.</p><p>e) It finally calculates the frequency of each label with a counter object.</p><p>Here is the code:</p><pre>from collections import Counter
import spacy
nlp = spacy.load("en_core_web_md")
corpus = open("atis_utterances.txt", "r").read().split("\n")
all_ent_labels = []
for sentence in corpus:
     doc = nlp(sentence.strip())
      ents = doc.ents
      all_ent_labels += [ent.label_ for ent in ents]
c = Counter(all_ent_labels)
print(c)
Counter({'<code>GPE</code> (location names), <code>DATE</code>, <code>TIME</code>, and <code>ORGANIZATION</code>. Obviously, the location entities refer to <a id="_idIndexMarker368"/>destination and source cities/countries, hence<a id="_idIndexMarker369"/> they play a very important role in the overall semantic success of our application.</p></li>
				<li>We'll first extract the location entities by spaCy Matcher by searching for a pattern of the <code>preposition location_name</code> form. The following code extracts location entities preceded with a preposition:<pre>import spacy
from spacy.matcher import Matcher  
nlp = spacy.load("en_core_web_md")
matcher = Matcher(nlp.vocab)
pattern = [{"POS": "ADP"}, {"ENT_TYPE": "GPE"}]
matcher.add("prepositionLocation", [pattern])
doc = nlp("show me flights from denver to boston on tuesday")
matches = matcher(doc)
for mid, start, end in matches:
    print(doc[start:end])
... 
from denver
to boston</pre><p>We already saw how <a id="_idIndexMarker370"/>to initialize a Matcher object and <a id="_idIndexMarker371"/>add patterns to it. Still, we'll recall how to use a Matcher object to extract the matches now. Here's what we did in this code segment: </p><p>a) We started by importing <code>spacy</code> and the <code>spacy.matcher</code> class in <em class="italic">lines 1-2</em>.</p><p>b) We created a language pipeline object, <code>nlp</code>, at <em class="italic">line 3</em>.</p><p>c) In <em class="italic">line 4</em>, we initialized the Matcher object with the language vocabulary.</p><p>d) In line 5, we created a<a id="_idIndexMarker372"/> pattern matching two tokens, a preposition (<code>POS</code> tag <code>ADP</code> means an <em class="italic">adposition = preposition + postposition</em>) and a location entity (label <code>GPE</code> means a <strong class="bold">location entity</strong>).</p><p>e) We added this pattern to the Matcher object.</p><p>f) Finally, we asked for the matches in an example corpus sentence and printed the matches.</p></li>
				<li>Although the <code>from</code> and <code>to</code> prepositions dominate in this dataset, verbs about leaving and arriving can be used with a variety of prepositions. Here are some more example sentences from the dataset:<pre>doc = nlp("i'm looking for a flight that goes from ontario to westchester and stops in chicago")
matches = matcher(doc)
for mid, start, end in matches:
   print(doc[start:end])
... 
from ontario
to westchester
in chicago</pre><p>The second example<a id="_idIndexMarker373"/> sentence is a question <a id="_idIndexMarker374"/>sentence:</p><pre>doc = nlp("what flights arrive in chicago on sunday on continental")
matches = matcher(doc)
for mid, start, end in matches:
   print(doc[start:end])
... 
in chicago</pre><p>Another example sentence from the dataset contains an abbreviation in a destination entity:</p><pre>doc = nlp("yes i'd like a flight from long beach to st. louis by way of dallas")
matches = matcher(doc)
for mid, start, end in matches:
   print(doc[start:end])
... 
from long
to st
of dallas</pre><p>Our last example <a id="_idIndexMarker375"/>sentence is again a question <a id="_idIndexMarker376"/>sentence:</p><pre>doc = nlp("what are the evening flights flying out of dallas")
matches = matcher(doc)
for mid, start, end in matches:
   print(doc[start:end])
... 
of dallas</pre><p>Here, we see some phrasal verbs such as <code>arrive in</code>, as well as preposition and verb combinations such as <code>stop in</code> and <code>fly out of</code>. <code>By the way of Dallas</code> does not include a verb at all. The user indicated that they want to make a stop at Dallas. <code>to</code>, <code>from</code>, <code>in</code>, <code>out</code>, and <code>of</code> are common prepositions that are used in a traveling context. </p></li>
				<li>After extracting the locations, we can now extract the airline information. The <code>ORG</code> entity label means an organization and it corresponds to airline company names in our dataset. The following code segment extracts the organization names, possibly multi-worded names:<pre>pattern = [{"ENT_TYPE": "ORG", "<code>ORG</code>. We wanted to capture one or more occurrences to capture the multi-word entities as well, which is why we used the <code>OP: "+"</code> operator. </p></li>
				<li>Extracting dates and times are not very different; you can replicate the preceding with code with <code>ENT_TYPE: DATE</code> and <code>ENT_TYPE: TIME</code>. We encourage you to try<a id="_idIndexMarker377"/> it yourself. The following <a id="_idIndexMarker378"/>screenshot exhibits how the date and time entities look in detail:<div><img src="img/B16570_6_3.jpg" alt="Figure 6.3 – Example dataset sentences with date and time entities highlighted. The image is generated by the displaCy online demo" width="1650" height="277"/></div><p class="figure-caption">Figure 6.3 – Example dataset sentences with date and time entities highlighted. The image is generated by the displaCy online demo</p></li>
				<li>Next, we'll extract abbreviation type entities. Extracting the abbreviation entities is a bit trickier. First, we will have a look at how the abbreviations appear:<pre>what does restriction ap 57 mean?
what does the abbreviation co mean?
what does fare code qo mean
what is the abbreviation d10
what does code y mean
what does the fare code f and fn mean
what is booking class c</pre><p>Only one of these<a id="_idIndexMarker379"/> sentences includes an entity. The first <a id="_idIndexMarker380"/>example sentence includes an <code>AMOUNT</code> entity, which is <code>57</code>. Other than that, abbreviations are not marked with any entity type at all. In this case, we have to provide some custom rules to the Matcher. Let's make some observations first, and then form a Matcher pattern:</p><p>a) An abbreviation can be broken into two parts – letters, and digits.</p><p>b) The letter part can be 1-2 characters long.</p><p>c) The digit part is also 1-2 characters long.</p><p>d) The presence of digits indicates an abbreviation entity.</p><p>e) The presence of the following words indicates an abbreviation entity: class, code, abbreviation.</p><p>f) The <code>POS</code> tag of an abbreviation is a noun. If the candidate word is a 1-letter or 2-letter word, then we can look at the <code>POS</code> tag and see whether it's a noun. This approach eliminates the false positives, such as <em class="italic">us</em> (pronoun), <em class="italic">me</em> (pronoun), <em class="italic">a</em> (determiner), and <em class="italic">an</em> (determiner).</p></li>
				<li>Let's now put these observations into Matcher patterns:<pre>pattern1 = [{"TEXT": {"REGEX": "\w{1,2}\d{1,2}"}}]
pattern2 = [{"SHAPE": { "IN": ["x", "xx"]}}, {"SHAPE": { "IN": ["d", "dd"]}}]
pattern3 = [{"TEXT": {"IN": ["class", "code", "abbrev", "abbreviation"]}}, {"SHAPE": { "IN": ["x", "xx"]}}]
pattern4 =   [{"POS": "NOUN", "SHAPE": { "IN": ["x", "xx"]}}]</pre><p>Then we create <a id="_idIndexMarker381"/>a Matcher object with the patterns we defined:</p><pre>matcher = Matcher(nlp.vocab)
matcher.add("abbrevEntities", [pattern1, pattern2, pattern3, pattern4])</pre><p>We're now ready to <a id="_idIndexMarker382"/>feed our sentences into the matcher:</p><pre>sentences = [
'what does restriction ap 57 mean',
'what does the abbreviation co mean', 
'what does fare code qo mean',
 'what is the abbreviation d10', 
'what does code y mean', 
'what does the fare code f and fn mean',
 'what is booking class c'
]
        18. We're ready to feed our sentences to the matcher:for sent in sentences:
   doc = nlp(sent)
   matches = matcher(doc)
   for mid, start, end in matches:
     print(doc[start:end])
... 
ap 57
57
abbreviation co
co
code qo
d10
code y
code f
class c
c</pre><p>In the preceding code, we defined four patterns:</p><p>a) The first pattern matches to a single token, which consists of 1-2 letters and 1-2 digits. For example, <code>d1</code>, <code>d10</code>, <code>ad1</code>, and <code>ad21</code> will match this pattern.</p><p>b) The second<a id="_idIndexMarker383"/> pattern matches to 2-token abbreviations <a id="_idIndexMarker384"/>where the first token is 1-2 letters and the second token 1-2 digits. The abbreviations <code>ap 5</code>, <code>ap 57</code>, <code>a 5</code>, and <code>a 57</code> will match this pattern.</p><p>c) The third pattern matches to two tokens too. The first token is a context clue word, such as <code>class</code> or <code>code</code>, and the second token should be a 1-2 letter token. Some example matches are <code>code f</code>, <code>code y</code>, and <code>class c</code>.</p><p>d) The fourth pattern extracts 1-2 letter short words whose <code>POS</code> tag is <code>NOUN</code>. Some example matches from the preceding sentences are <code>c</code> and <code>co</code>.</p></li>
			</ol>
			<p>spaCy Matcher makes life easy for us by allowing us to make use of token shape, context clues, and a token <code>POS</code> tag. We made a very successful entity extraction in this subsection by extracting locations, airline names, dates, times, and abbreviations. In the next subsection, we'll go deeper into the sentence syntax and extract entities from the sentences where context does not offer many clues.</p>
			<h2 id="_idParaDest-106"><a id="_idTextAnchor108"/>Using dependency trees for extracting entities</h2>
			<p>In the <a id="_idIndexMarker385"/>previous subsection, we extracted <a id="_idIndexMarker386"/>entities where the context provides obvious clues. Extracting the destination city from the following sentence is easy. We can look for the <code>to + GPE</code> pattern:</p>
			<pre>I want to fly to Munich tomorrow. </pre>
			<p>But suppose the user provides one of the following sentences instead:</p>
			<pre>I'm going to a conference in Munich. I need an air ticket.
My sister's wedding will be held in Munich. I'd like to book a flight.</pre>
			<p>Here, the preposition <code>to</code> refers to <code>conference</code>, not <code>Munich</code>, in the first sentence. In this sentence, we need a pattern such as <code>to + .... + GPE</code>. Then, we have to be careful what words can come in between "to" and the city name, as well as what words should not come. For instance, this sentence carries a completely different meaning and shouldn't match:</p>
			<pre>I want to book a flight to my conference without stopping at Berlin.</pre>
			<p>In the second sentence, there's no <code>to</code> at all. Here, as we see from these examples, we need to examine the syntactic relations between words. In <a href="B16570_03_Final_JM_ePub.xhtml#_idTextAnchor055"><em class="italic">Chapter 3</em></a><em class="italic">, Linguistic Features</em>, we already saw how to interpret dependency trees to understand the relations between words. In this subsection, we'll walk the dependency trees.</p>
			<p>Walking a dependency tree means visiting the tokens in a custom order, not necessarily from left to right. Usually, we stop iterating over the dependency tree once we find what we're looking for. Again, a dependency tree shows the syntactic relations between its words. Recall from <a href="B16570_03_Final_JM_ePub.xhtml#_idTextAnchor055"><em class="italic">Chapter 3</em></a><em class="italic">, Linguistic Features</em>, that the relations are represented with directed arrows, connecting the head and the child of a relation. Every word in a sentence has to involve at least one relation. This fact guarantees that we'll visit each word while walking through the sentence.</p>
			<p class="callout-heading">Recall</p>
			<p class="callout">Before proceeding to code, first, let's remember some concepts about dependency trees. ROOT is a special dependency label and is always assigned to the main verb of the sentence. spaCy shows syntactic relations with arcs. One of the tokens is the syntactic parent (called the HEAD) and the other is dependent (called the CHILD). By way of an example, in <em class="italic">Figure 6.3</em>, <strong class="bold">going</strong> has 3 syntactic children – <strong class="bold">I</strong>, <strong class="bold">m</strong>, and <strong class="bold">to</strong>. Equivalently, the syntactic head of <strong class="bold">to</strong> is <strong class="bold">going</strong> (the same applies to <strong class="bold">I</strong> and <strong class="bold">m</strong>).</p>
			<p>Coming back to our examples, we'll iterate<a id="_idIndexMarker387"/> the utterance dependency trees to find out whether the preposition <strong class="bold">to</strong> is syntactically related<a id="_idIndexMarker388"/> to the location entity, <strong class="bold">Munich</strong>. First of all, let's see the dependency parse of our example sentence <em class="italic">I'm going to a conference in Munich</em> and also remember what a dependency tree looks like:</p>
			<div><div><img src="img/B16570_6_4.jpg" alt="Figure 6.4 – Dependency parse of the example sentence&#13;&#10;" width="1650" height="324"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.4 – Dependency parse of the example sentence</p>
			<p>There are no incoming arcs into the verb <strong class="bold">going</strong>, so <strong class="bold">going</strong> is the root of the dependency tree (when we examine the code, we'll see that the dependency label is <strong class="bold">ROOT</strong>). This is supposed to happen because <strong class="bold">going</strong> is the main verb of the sentence. If we follow the arc to its immediate right, we encounter <strong class="bold">to</strong>; jumping over the arcs to the right we reach <strong class="bold">Munich</strong>. This shows that there's a syntactic relation between <strong class="bold">to</strong> and <strong class="bold">Munich</strong>.</p>
			<p>Let's now iterate over the dependency tree with code. There are two possible ways to connect <strong class="bold">to</strong> and <strong class="bold">Munich</strong>:</p>
			<ul>
				<li>Left to right. We start from <strong class="bold">to</strong> and try to reach <strong class="bold">Munich</strong> by visiting "to"'s syntactic children. This approach may not be a very good idea, because if "to" has more than one child, then we need to check each child and keep track of all the possible paths.</li>
				<li>Right to left. We start from <strong class="bold">Munich</strong>, jump onto its head, and follow the head's head, and so on. Since each word has exactly one head, it's guaranteed that there will be only one path. Then we determine whether <strong class="bold">to</strong> is on this path or not.</li>
			</ul>
			<p>The following code <a id="_idIndexMarker389"/>segments implement the second approach, start the <a id="_idIndexMarker390"/>dependency tree walk from <code>Munich</code>, and look for <code>to</code>:</p>
			<pre>import spacy
nlp = spacy.load("en_core_web_md")
def reach_parent(source_token, dest_token):
  source_token = source_token.head
   while source_token != dest_token:
     if source_token.head == source_token:
       return None
    source_token = source_token.head
   return source_token
doc = nlp("I'm going to a conference in Munich.")
doc[-2]
Munich
doc[3]
to
doc[-1]
.
reach_parent(doc[-2], doc[3])
to
reach_parent(doc[-1], doc[3])
None</pre>
			<p>In the <code>reach_parent</code> function, the following applies:</p>
			<ul>
				<li>We start from a source token and try to reach the destination token.</li>
				<li>In the <code>while</code> loop, we iterate over the head of each token, starting from the source token.</li>
				<li>The loop stops when we reach either the source token or the root of the sentence.</li>
				<li>We test for reaching the root via the <code>source_token == source_token.head</code> line. Because the root token always refers to itself, its head is itself (remember that in the dependency tree, there are no incoming arcs into the root).</li>
				<li>Finally, we tested our function on two different test cases. In the first one, the source and destination are related, whereas in the second test, there's no relation, hence the function returns <code>None</code>.</li>
			</ul>
			<p>This approach is very different from the previous subsection's rather straightforward approach. Natural language is complicated and challenging to process, and it's important to know the approaches available and have the necessary tools in your toolbox when you need to use them.</p>
			<p>Next, we'll dive into a <a id="_idIndexMarker391"/>popular subject – intent<a id="_idIndexMarker392"/> recognition with a syntactic approach. Let's see some key points of designing a good intent recognition component, including recognizing multiple intents as well.</p>
			<h1 id="_idParaDest-107"><a id="_idTextAnchor109"/>Using dependency relations for intent recognition</h1>
			<p>After extracting the entities, we<a id="_idIndexMarker393"/> want to find out what sort of intent the user carries – to book a flight, to purchase a meal on their already booked flight, cancel their flight, and so on. If you look at the intents list again, you will see that every intent includes a verb (to book) and an object that the verb acts on (flight, hotel, meal).</p>
			<p>In this section, we'll extract transitive verbs and their direct objects from utterances. We'll begin our intent recognition section by extracting the transitive verb and the direct object of the verb. Then, we'll explore how to understand a user's intent by recognizing <a id="_idIndexMarker394"/>synonyms of verbs and nouns. Finally, we'll see how to determine a user's intent with semantic similarity methods. Before we move on to extracting transitive verbs and their direct objects, let's first quickly go over the concepts of transitive verbs and direct/indirect objects.</p>
			<h2 id="_idParaDest-108"><a id="_idTextAnchor110"/>Linguistic primer</h2>
			<p>In this section, we'll explore some linguistic concepts related<a id="_idIndexMarker395"/> to sentence structure, including verbs and verb-object relations. A verb is a very important<a id="_idIndexMarker396"/> component of the sentence as it indicates the action in the sentence. The object of the sentence is the thing/person that is affected by the action of the verb. Hence, there's a natural connection between the sentence verb and objects. The concept of transitivity captures verb-object relations. A transitive verb is a verb that needs <a id="_idIndexMarker397"/>an object to act upon. Let's see some examples:</p>
			<pre>I bought flowers.
He loved his cat.
He borrowed my book.</pre>
			<p>In these example sentences, <code>bought</code>, <code>loved</code>, and <code>borrowed</code> are transitive verbs. In the first sentence, <code>bought</code> is the transitive verb and <code>flowers</code> is its object, the thing that has been bought by the sentence subject, <code>I</code>. <code>Loved</code> – <code>his cat</code> and <code>borrowed</code> – <code>my book</code> are transitive verb-object examples. We'll focus on the first sentence again - what happens if we erase the <code>flowers</code> object?</p>
			<pre>I bought</pre>
			<p>Bought <strong class="bold">what</strong>? Without an object, this sentence doesn't carry any meaning at all. In the preceding sentences, each of the objects completes the meaning of the verb. This is a way of understanding whether a verb is transitive or not – erase the object and check whether the sentence remains semantically intact.</p>
			<p>Some verbs are transitive and some <a id="_idIndexMarker398"/>verbs are intransitive. An <strong class="bold">intransitive verb</strong> is the opposite of a transitive verb; it doesn't need an object to act upon. Let's see some examples:</p>
			<pre>Yesterday I <strong class="bold">slept</strong> for 8 hours.
The cat <strong class="bold">ran</strong> towards me.
When I went out, the sun was <strong class="bold">shining</strong>.
Her cat <strong class="bold">died</strong> 3 days ago.</pre>
			<p>In all the <a id="_idIndexMarker399"/>preceding sentences, the verbs make sense without an object. If we erase all the words other than the subject and object, these sentences are still meaningful:</p>
			<pre>I slept.
The cat ran.
The sun was shining.
Her cat died.</pre>
			<p>Pairing an intransitive verb with an object doesn't make sense. You can't run someone/something, you can't shine something/someone, and you certainly cannot die something/someone. </p>
			<h3>Sentence object</h3>
			<p>As we remarked before, the object is the thing/person that is affected by the verb's action. The action stated by the verb is committed by the sentence subject and the sentence object gets affected. </p>
			<p>A sentence can be direct or indirect. A <strong class="bold">direct object</strong> answers the questions <strong class="bold">whom?</strong> / <strong class="bold">what?</strong> You can find the direct <a id="_idIndexMarker400"/>object by asking <strong class="bold">The subject {verb} what/who?</strong>. Here are some examples:</p>
			<pre>I bought flowers.  I bought what? - flowers
He loved his cat.  He loved who?  - his cat
He borrowed my book. He borrowed what? - my book</pre>
			<p>An <strong class="bold">indirect object</strong> answers<a id="_idIndexMarker401"/> the questions <strong class="bold">for what?</strong>/<strong class="bold">for whom?</strong>/<strong class="bold">to whom?</strong>. Let's see some examples:</p>
			<pre>He gave <strong class="bold">me</strong> his book.  He gave his book to whom?  - me
He gave his book to <strong class="bold">me</strong>. He gave his book to whom? -me</pre>
			<p>Indirect objects are often preceded by the prepositions to, for, from, and so on. As you can see from these examples, an indirect object is also an object and is affected by the verb's action, but its role in the sentence is a bit different. An indirect object is sometimes viewed as the recipient of the direct object.</p>
			<p>This is all you need to know about transitive/intransitive verbs and direct/indirect objects to digest this chapter's material. If you want to learn more about sentence syntax, you can read the great book <em class="italic">Linguistic Fundamentals for Natural Language Processing</em> by <strong class="bold">Emily Bender</strong>: (<a href="https://dl.acm.org/doi/book/10.5555/2534456">https://dl.acm.org/doi/book/10.5555/2534456</a>). We have covered the basics of sentence syntax, but this is still a great resource to learn about syntax in depth.</p>
			<h2 id="_idParaDest-109"><a id="_idTextAnchor111"/>Extracting transitive verbs and their direct objects</h2>
			<p>While recognizing the intent, usually <a id="_idIndexMarker402"/>we apply these steps to a user<a id="_idIndexMarker403"/> utterance:</p>
			<ol>
				<li value="1">Splitting the sentence into tokens.</li>
				<li>Dependency parsing is performed by spaCy. We walk the dependency tree to extract the tokens and relations that we're interested in, which are the verb and the direct object, as shown in the following diagram:</li>
			</ol>
			<div><div><img src="img/B16570_6_5.jpg" alt="Figure 6.5 – Dependency parse of an example sentence from the corpus&#13;&#10;" width="1606" height="377"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.5 – Dependency parse of an example sentence from the corpus</p>
			<p>In this example sentence, the<a id="_idIndexMarker404"/> transitive verb is <strong class="bold">find</strong> and the direct object is <strong class="bold">a flight</strong>. The relation <strong class="bold">dobj</strong> connects a transitive verb to its direct object. If we follow the arc, semantically, we see that the user wants to commit the action of finding and the object they want to find is a flight. We can merge <strong class="bold">find</strong> and <strong class="bold">a flight</strong> into a single word, <strong class="bold">findAflight</strong> or <strong class="bold">findFlight</strong>, which can be this intent's name. Other intents can be <strong class="bold">bookFlight</strong>, <strong class="bold">cancelFlight</strong>, <strong class="bold">bookMeal</strong>, and so on.</p>
			<p>Let's extract the verb and the direct object in a<a id="_idIndexMarker405"/> more systematic way. We'll first spot the direct object by looking for the <code>dobj</code> label in the sentence. To locate the transitive verb, we look at the direct object's syntactic head. A sentence can include more than one verb, hence we're careful while processing the verbs. Here is the code:</p>
			<pre>import spacy
nlp = spacy.load("en_core_web_md")
doc = nlp("find a flight from washington to sf")
for token in doc:
  if token.dep_ == "dobj":
    print(token.head.text + token.text.capitalize())
findFlight</pre>
			<p>In this code segment, the following applies:</p>
			<ul>
				<li> We applied the pipeline to our sample sentence.</li>
				<li>Next, we spotted the direct object by looking for a token whose dependency label is <code>dobj</code>.</li>
				<li>When we located a direct object, we spotted the corresponding transitive verb by obtaining the direct object's syntactic head.</li>
				<li>Finally, we printed the verb and the object to generate this intent's name. </li>
			</ul>
			<p>Great! Intent recognition was<a id="_idIndexMarker406"/> successful! Here, we recognized a single intent. Some<a id="_idIndexMarker407"/> utterances may carry multiple intents. In the next section, we'll learn how to recognize multiple intents based on the techniques of this section.</p>
			<h2 id="_idParaDest-110"><a id="_idTextAnchor112"/>Extracting multiple intents with conjunction relation </h2>
			<p>Some utterances carry<a id="_idIndexMarker408"/> multiple intents. For example, consider the following utterance from the corpus:</p>
			<pre>show all flights and fares from denver to san francisco</pre>
			<p>Here, the user wants to list all the flights and, at the same time, wants to see the fare info. One way of processing is considering these intents as a single and complex intent. In that case, we can express this complex intent as <code>action: show, objects: flights, fares</code>. Another and more common way of processing this sort of utterance is to label the utterance with multiple intents. In the dataset, this example utterance is marked with two intents as <code>atis_flight#atis_airfare</code>:</p>
			<div><div><img src="img/B16570_6_6.jpg" alt="Figure 6.6 – Dependency tree of the example dataset utterance. The conj relation is between &quot;flight&quot; and &quot;fares&quot; " width="1639" height="483"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.6 – Dependency tree of the example dataset utterance. The conj relation is between "flight" and "fares" </p>
			<p>In the preceding diagram, we see that the <strong class="bold">dobj</strong> arc connects <strong class="bold">show</strong> and <strong class="bold">flights</strong>. The <strong class="bold">conj</strong> arc connects <strong class="bold">flights</strong> and <strong class="bold">fares</strong> to indicate the conjunction relation between them. The conjunction relation is<a id="_idIndexMarker409"/> built by a conjunction such as <strong class="bold">and</strong> or <strong class="bold">or</strong> and indicates that a noun is joined to another noun by this conjunction. In this situation, we extract the direct object and its conjuncts. Let's now see how we can turn this process into code:</p>
			<pre>import spacy
nlp = spacy.load("en_core_web_md")
doc = nlp("show all flights and fares from denver to san francisco") 
for token in doc:
   if token.dep_ == "dobj":
     dobj = token.text
     conj = [t.text for t in token.conjuncts]
     verb = donj.head
print(verb, dobj, conj)
show flights ['fares']</pre>
			<p>Here, we looped over all tokens to locate the direct object of the sentence. When we found the direct object, we <a id="_idIndexMarker410"/>obtained its conjuncts. After that, finding the transitive verb is the same as we did in the previous code segment, we extracted the direct object's head. After extracting the verb and two objects, if we want, we can combine the two to create two intent names – <code>showFlights</code> and <code>showFares</code>.</p>
			<h2 id="_idParaDest-111"><a id="_idTextAnchor113"/>Recognizing the intent using wordlists</h2>
			<p>In some cases, tokens other than<a id="_idIndexMarker411"/> the transitive verb and the direct <a id="_idIndexMarker412"/>object contain the semantics of the user intent. In that case, you need to go further down in the syntactic relations and explore the sentence structure deeper.</p>
			<p>As an example, consider the following utterance from our dataset:</p>
			<pre>i want to make a reservation for a flight</pre>
			<p>In this sentence, the <strong class="bold">verb-object</strong> pair that best describes the user intent is <strong class="bold">want-flight</strong>. However, if we look at the parse tree in <em class="italic">Figure 6.7</em>, we see that <strong class="bold">want</strong> and <strong class="bold">flight</strong> are not directly related in the parse tree. <strong class="bold">want</strong> is related to the transitive verb <strong class="bold">make</strong>, and <strong class="bold">flight</strong> is related to the direct object <strong class="bold">reservation</strong>, respectively:</p>
			<div><div><img src="img/B16570_6_7.jpg" alt="Figure 6.7 – Parse tree of the example sentence from the dataset&#13;&#10;" width="1659" height="328"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.7 – Parse tree of the example sentence from the dataset</p>
			<p>What will we do then? We can play a<a id="_idIndexMarker413"/> trick and keep a list of helper verbs such as <strong class="bold">would like</strong>, <strong class="bold">want</strong>, <strong class="bold">make</strong>, and <strong class="bold">need</strong>. Here's the<a id="_idIndexMarker414"/> code:</p>
			<pre>doc = nlp("i want to make a reservation for a flight")
dObj =None
tVerb = None
# Extract the direct object and its transitive verb
for token in doc:
  If token.dep_ == "dobj":
    dObj = token
    tVerb = token.head
# Extract the helper verb
intentVerb = None
<strong class="bold">verbList</strong> = ["want", "like", "need", "order"]
if tVerb.text in verbList:
  intentVerb = tVerb
else:
  if tVerb.head.dep_ == "<strong class="bold">ROOT</strong>":
    helperVerb = tVerb.head
# Extract the object of the intent
intentObj = None
<strong class="bold">objList</strong> = ["flight", "meal", "booking"]
if dObj.text in objList:
  intentObj = dObj
else:
  for child in dObj.children:
    if child.dep_ == "prep":
      intentObj = list(child.children)[0]
      break
    elif child.dep_ == "compound":
      intentObj = child
      break 
print(intentVerb.text + intentObj.text.capitalize())
wantFlight</pre>
			<p>Here's what we did step by step:</p>
			<ol>
				<li value="1">We started by locating the direct object and its transitive verb.</li>
				<li>Once we found them, we compared them against our predefined lists of words. For this example, we used two shortened lists, <code>verbList</code> contains a list of helper verbs, and <code>objList</code> contains a list of the possible object words we want to extract.</li>
				<li>We checked the<a id="_idIndexMarker415"/> transitive verb. If it's not in the list of helper verbs, then <a id="_idIndexMarker416"/>we checked the main verb of the sentence (marked by <code>ROOT</code>), which is the head of the transitive verb. If the transitive verb is the main verb of the sentence, then the syntactic head of this verb is itself (<code>tVerb.head</code> is <code>tVerb)</code>. Hence, the line <code>if tVerb.head.dep_ == "ROOT"</code> evaluates to <code>True</code> and this implementation works.</li>
				<li>Next, we checked the direct object. If it's not in the list of possible objects, then we check its syntactic children. For each child, we check whether the child is a preposition of the direct object. If so, we pick up the child's child (it can have only one child).</li>
				<li>Finally, we printed the string that represents the intent name, which is <code>wantFlight</code>.</li>
			</ol>
			<p>At this point, take a deep breath. It takes time to digest and process information, especially when it's about <a id="_idIndexMarker417"/>sentence syntax. You can try different sentences<a id="_idIndexMarker418"/> from the corpus and see what the script does by checkpointing/putting prints into the code.</p>
			<p>In the next section, we'll explore a very handy tool, using synonym lists. Let's move ahead to the next section and learn how to make the best of semantic similarity.</p>
			<h1 id="_idParaDest-112"><a id="_idTextAnchor114"/>Semantic similarity methods for semantic parsing</h1>
			<p>Natural language allows us to<a id="_idIndexMarker419"/> express the same concept in different ways and with different words. Every language has synonyms and semantically related words.</p>
			<p>As an NLP developer, while developing a semantic parser for a chatbot application, text classification, or any other semantic application, you should keep in my mind that users use a fairly wide set of phrases and expressions for each intent. In fact, if you're building a chatbot by using a platform <a id="_idIndexMarker420"/>such as RASA (<a href="https://rasa.com/">https://rasa.com/</a>) or on a platform such as<a id="_idIndexMarker421"/> Dialogflow (<a href="https://dialogflow.cloud.google.com/">https://dialogflow.cloud.google.com/</a>), you're asked to provide as many utterance examples as you can provide for each intent. Then, these utterances are used to train the intent classifier behind the scenes.</p>
			<p>There are usually two ways to recognize semantic similarity, either with a synonyms dictionary or with word vector-based semantic similarity methods. In this section, we will discuss both approaches. Let's start with how to use a synonyms dictionary to detect semantic similarity.</p>
			<h2 id="_idParaDest-113"><a id="_idTextAnchor115"/>Using synonyms lists for semantic similarity</h2>
			<p>We already went<a id="_idIndexMarker422"/> through our dataset and saw that different verbs are used to express the same actions. For instance, <strong class="bold">landing</strong>, <strong class="bold">arriving</strong>, and <strong class="bold">flying to</strong> verbs carry the same meaning, whereas <strong class="bold">leaving</strong>, <strong class="bold">departing</strong>, and <strong class="bold">flying from</strong> verbs form another <a id="_idIndexMarker423"/>semantic group.</p>
			<p>We already saw that in most cases, the transitive verbs and direct objects express the intent. An easy way to determine whether two utterances represent the same intent is to check whether the verbs and the direct objects are synonyms.</p>
			<p>Let's take an example and compare two example utterances from the dataset. First, we prepare a small synonyms dictionary. We include only the base forms of the verbs and nouns. While doing the comparison, we also use the base form of the words:</p>
			<pre>verbSynsets = [
("show", "list"),
("book", "make a reservation", "buy", "reserve")
] 
objSynsets = [
("meal", "food"),
("aircraft", "airplane", "plane")
]</pre>
			<p>Each <strong class="bold">synonym set</strong> (<strong class="bold">synset</strong>) includes the set of synonyms for our domain. We usually include the language-general synonyms (airplane-plane) and the domain-specific synonyms (book-buy).</p>
			<p>The synsets are ready to use, and we're ready to move onto the spaCy code. Let's go step by step:</p>
			<ol>
				<li value="1">First, we construct two doc objects corresponding to the two utterances we want to compare, <code>doc</code> and <code>doc2</code>:<pre><strong class="bold">doc</strong> = nlp("show me all aircrafts that cp uses")
<strong class="bold">doc2</strong> = nlp("list all meals on my flight")</pre></li>
				<li>Then, we extract the transitive verb and direct object of the first utterance:<pre>for token in doc:
   if token.dep_ == "dobj":
     obj = token.lemma_
     verb = token.head.lemma_
     break</pre></li>
				<li>Then we do the same for the second utterance:<pre>for token in doc2:
   if token.dep_ == "dobj":
     obj2 = token.lemma_
     verb2 = token.head.lemma_
     break
verb, obj
('show' , 'aircraft')
verb2, obj2
('list', 'meal')</pre></li>
				<li>We obtained a synset of the<a id="_idIndexMarker424"/> first verb shown. Then, we checked whether the <a id="_idIndexMarker425"/>second verb list is in this synset, which returns <code>True</code>:<pre>vsyn = [syn for syn in verbSynsets if verb in item]
vsyn[0]
("<strong class="bold">show</strong>", "<strong class="bold">list</strong>")
v2 in vsyn[0]
<strong class="bold">True</strong></pre></li>
				<li>Similarly, we obtain the synset of the first direct object – aircraft. Then we check whether the second direct object meal is in this synset, which is obviously not true:<pre>osyn = [syn for syn in objSynsets if obj in item]
osyn[0]
("aircraft", "airplane", "plane")
obj2 in vsyn[0]
False</pre></li>
				<li> We deduce that the preceding two utterances do not refer to the same intent.</li>
			</ol>
			<p>Synonym lists are great for semantic similarity calculations, and many real-world NLP applications benefit from such precompiled lists. Using synonyms is not always applicable though. Making a<a id="_idIndexMarker426"/> dictionary look up each word in a sentence<a id="_idIndexMarker427"/> can become inefficient for big synsets. In the next section, we'll introduce a more efficient way of calculating semantic similarity with word vectors.</p>
			<h2 id="_idParaDest-114"><a id="_idTextAnchor116"/>Using word vectors to recognize semantic similarity</h2>
			<p>In <a href="B16570_05_Final_JM_ePub.xhtml#_idTextAnchor087"><em class="italic">Chapter 5</em></a>, <em class="italic">Working with Word Vectors and Semantic Similarity</em>, we already saw that the word vector carries<a id="_idIndexMarker428"/> semantics, including synonymity information. Synonym lists are handy if you work in a very specific domain and the <a id="_idIndexMarker429"/>number of synonyms is rather low. Working with big synsets can become inefficient at some point because we have to make a dictionary look up the verbs and direct objects each time. However, word vectors offer us a very convenient and vector-based way to calculate semantic similarity.</p>
			<p>Let's go over the code from the previous subsection again. This time, we'll calculate the semantic distance between words with spaCy word vectors. Let's go step by step:</p>
			<ol>
				<li value="1">First, we construct two <code>doc</code> objects that we want to compare:<pre>doc = nlp("show me all aircrafts that cp uses")
doc2 = nlp("list all meals on my flight")</pre></li>
				<li>Then we extract the verb and object of the first sentence:<pre>for token in doc:
   if token.dep_ == "dobj":
     obj = token
     verb = token.head
     break</pre></li>
				<li>We repeat <a id="_idIndexMarker430"/>the same <a id="_idIndexMarker431"/>procedure on the second sentence:<pre>for token in doc2:
   if token.dep_ == "dobj":
     obj2 = token
     verb2 = token.head
     break
verb, obj
('show' , 'aircraft')
verb2, obj2
('list', 'meal')</pre></li>
				<li>Now, we calculate the semantic similarity between two direct objects using the word vector-based similarity method of spaCy:<pre>obj.similarity(obj2)
0.15025872                    # A very low score, we can deduce these 2 utterances are not related at this point.</pre></li>
				<li>Finally, we calculate the similarity between the verbs:<pre>verb.similarity(verb2)
0.33161193</pre><p>The preceding code is different<a id="_idIndexMarker432"/> from the previous code. This <a id="_idIndexMarker433"/>time, we used the token objects directly; no lemma extraction is required. Then we called the <code>token.similarity(token2)</code> method of spaCy to calculate the semantic distance between the direct objects. The resulting score is very low. At this point, we deduce that these two utterances do not represent the same intent.</p></li>
			</ol>
			<p>This is an easy and efficient way of calculating semantic similarity. We remarked in the very first chapter that spaCy provides easy-to-use and efficient tools for NLP developers, and now we can see why.</p>
			<h1 id="_idParaDest-115"><a id="_idTextAnchor117"/>Putting it all together</h1>
			<p>We already extracted the entities and recognized the intent in several ways. We're now ready to put it all together to <a id="_idIndexMarker434"/>calculate a semantic representation for a user utterance!</p>
			<ol>
				<li value="1">We'll process the example dataset utterance:<pre>show me flights from denver to philadelphia on tuesday</pre><p>We'll hold a dictionary object to hold the result. The result will include the entities and the intent.</p></li>
				<li>Let's extract the entities:<pre>import spacy
from spacy.matcher import Matcher  
nlp = spacy.load("en_core_web_md")
matcher = Matcher(nlp.vocab)
pattern = [{"POS": "ADP"}, {"ENT_TYPE": "GPE"}]
matcher.add("prepositionLocation", [pattern])
# Location entities
doc = nlp("show me flights from denver to philadelphia on tuesday")
matches = matcher(doc)
for mid, start, end in matches:
    print(doc[start:end])
... 
from denver
to philadelphia
# All entities:
ents = doc.ents
(denver, philedelphia, tuesday)</pre></li>
				<li>With this information, we<a id="_idIndexMarker435"/> can generate the following semantic representation:<pre>{
'utterance': 'show me flights from denver to philadelphia on tuesday',
'entities': {
              'date': 'tuesday',
              'locations': {
                                 'from': 'denver',
                                 'to': 'philadelphia'
                                 }
                  }
}</pre></li>
				<li>Next, we'll perform intent recognition to generate a complete semantic parsing:<pre>import spacy
nlp = spacy.load("en_core_web_md")
doc = nlp("show me flights from denver to philadelphia on tuesday")
for token in doc:
  if token.dep_ == "dobj":
    print(token.head.lemma_ + token.lemma_.capitalize())
showFlight</pre></li>
				<li>After <a id="_idIndexMarker436"/>determining the intent, our semantic parse for this utterance now looks like this:<pre>{
'utterance': 'show me flights from denver to philadelphia on tuesday',
'intent ': ' showFlight',
'entities': {
              'date': 'tuesday',
              'locations': {
                                 'from': 'denver',
                                 'to': 'philadelphia'
                                 }
                  }
}</pre></li>
			</ol>
			<p>The final result is that the complete semantic representation of this utterance, intent, and entities is extracted. This is a machine-readable and usable output. We pass this result to the system component that made the call to the NLP application to generate a response action.</p>
			<h1 id="_idParaDest-116"><a id="_idTextAnchor118"/>Summary</h1>
			<p>Congratulations! You have made it to the end of a very intense chapter!</p>
			<p>In this chapter, you learned how to generate a complete semantic parse of utterances. First, you made a discovery on your dataset to get insights about the dataset analytics. Then, you learned to extract entities with two different techniques – with spaCy Matcher and by walking on the dependency tree. Next, you learned different ways of performing intent recognition by analyzing the sentence structure. Finally, you put all the information together to generate a semantic parse.</p>
			<p>In the next chapters, we will shift toward more machine learning methods. The next section concerns how to train spaCy NLP pipeline components on your own data. Let's move ahead and customize spaCy for ourselves!</p>
		</div>
	</div></body></html>