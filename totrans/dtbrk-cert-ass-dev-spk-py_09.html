<html><head></head><body>
		<div id="_idContainer037">
			<h1 id="_idParaDest-241" class="chapter-number"><a id="_idTextAnchor242"/>9</h1>
			<h1 id="_idParaDest-242"><a id="_idTextAnchor243"/>Mock Test 1</h1>
			<h1 id="_idParaDest-243"><a id="_idTextAnchor244"/>Questions</h1>
			<p>Try your hand at these practice questions to test your knowledge of <span class="No-Break">Apache Spark:</span></p>
			<p><span class="No-Break"><strong class="bold">Question 1:</strong></span></p>
			<p>Which statement does not accurately describe a feature of the <span class="No-Break">Spark driver?</span></p>
			<ol class="margin-left">
				<li class="Alphabets">The Spark driver serves as the node where the main method of a Spark application runs to co-ordinate <span class="No-Break">the application</span></li>
				<li class="Alphabets">The Spark driver can be horizontally scaled to enhance overall <span class="No-Break">processing throughput</span></li>
				<li class="Alphabets">The Spark driver houses the <span class="No-Break">SparkContext object</span></li>
				<li class="Alphabets">The Spark driver is tasked with scheduling the execution of data by using different worker nodes in <span class="No-Break">cluster mode</span></li>
				<li class="Alphabets">Optimal performance dictates that the Spark driver should be positioned as close as possible to <span class="No-Break">worker nodes</span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 2</strong></span><span class="No-Break">:</span></p>
			<p>Which of these statements accurately <span class="No-Break">describes stages?</span></p>
			<ol class="margin-left">
				<li class="Alphabets">Tasks within a stage can be simultaneously executed by <span class="No-Break">multiple machines</span></li>
				<li class="Alphabets">Various stages within a job can <span class="No-Break">run concurrently</span></li>
				<li class="Alphabets">Stages comprise one or <span class="No-Break">more jobs</span></li>
				<li class="Alphabets">Stages temporarily store transactions before committing them <span class="No-Break">through actions</span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 3:</strong></span></p>
			<p>Which of these statements accurately describes Spark’s cluster <span class="No-Break">execution mode?</span></p>
			<ol class="margin-left">
				<li class="Alphabets">Cluster mode runs executor processes on <span class="No-Break">gateway nodes</span></li>
				<li class="Alphabets">Cluster mode involves the driver being hosted on a <span class="No-Break">gateway machine</span></li>
				<li class="Alphabets">In cluster mode, the Spark driver and the cluster manager are <span class="No-Break">not co-located</span></li>
				<li class="Alphabets">The driver in cluster mode is located on a <span class="No-Break">worker node</span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 4:</strong></span></p>
			<p>Which of these statements accurately describes Spark’s client <span class="No-Break">execution mode?</span></p>
			<ol class="margin-left">
				<li class="Alphabets">Client mode runs executor processes on <span class="No-Break">gateway nodes</span></li>
				<li class="Alphabets">In client mode, the driver is co-located with <span class="No-Break">the executor</span></li>
				<li class="Alphabets">In client mode, the Spark driver and the cluster manager <span class="No-Break">are co-located</span></li>
				<li class="Alphabets">In client mode, the driver is found on an <span class="No-Break">edge node</span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 5:</strong></span></p>
			<p>Which statement accurately describes Spark’s standalone <span class="No-Break">deployment mode?</span></p>
			<ol class="margin-left">
				<li class="Alphabets">Standalone mode utilizes only one executor per worker for <span class="No-Break">each application</span></li>
				<li class="Alphabets">In standalone mode, the driver is located on a <span class="No-Break">worker node</span></li>
				<li class="Alphabets">In standalone mode, the cluster does not need <span class="No-Break">the driver</span></li>
				<li class="Alphabets">In standalone mode, the driver is found on an <span class="No-Break">edge node</span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 6</strong></span><span class="No-Break">:</span></p>
			<p>What is a task <span class="No-Break">in Spark?</span></p>
			<ol class="margin-left">
				<li class="Alphabets">The unit of work performed for each data partition within a task <span class="No-Break">is slots</span></li>
				<li class="Alphabets">Tasks are the second-smallest entity that can be executed <span class="No-Break">within Spark</span></li>
				<li class="Alphabets">Tasks featuring wide dependencies can be combined into a <span class="No-Break">single task</span></li>
				<li class="Alphabets">A task is a single unit of work done by a partition <span class="No-Break">within Spark</span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 7</strong></span><span class="No-Break">:</span></p>
			<p>Which of the following is the highest level in Spark’s <span class="No-Break">execution hierarchy?</span></p>
			<ol class="margin-left">
				<li class="Alphabets"><span class="No-Break">Job</span></li>
				<li class="Alphabets"><span class="No-Break">Task</span></li>
				<li class="Alphabets"><span class="No-Break">Executor</span></li>
				<li class="Alphabets"><span class="No-Break">Stage</span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 8</strong></span><span class="No-Break">:</span></p>
			<p>How can the concept of slots be accurately described in <span class="No-Break">Spark’s context?</span></p>
			<ol class="margin-left">
				<li class="Alphabets">The creation and termination of slots align with the workload of <span class="No-Break">an executor</span></li>
				<li class="Alphabets">Spark strategically stores data on disk across various slots to enhance <span class="No-Break">I/O performance</span></li>
				<li class="Alphabets">Each slot is consistently confined to a <span class="No-Break">solitary core</span></li>
				<li class="Alphabets">Slots enable the tasks to run <span class="No-Break">in parallel</span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 9</strong></span><span class="No-Break">:</span></p>
			<p>What is the role of an executor <span class="No-Break">in Spark?</span></p>
			<ol class="margin-left">
				<li class="Alphabets">The executor’s role is to request the transformation of operations <span class="No-Break">into DAG</span></li>
				<li class="Alphabets">There can only be one executor within a <span class="No-Break">Spark environment</span></li>
				<li class="Alphabets">The executor processes partitions in an optimized and <span class="No-Break">distributed manner</span></li>
				<li class="Alphabets">The executor schedules queries <span class="No-Break">for execution</span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 10</strong></span><span class="No-Break">:</span></p>
			<p>What is the role of shuffle <span class="No-Break">in Spark?</span></p>
			<ol class="margin-left">
				<li class="Alphabets">Shuffle broadcasts variables to <span class="No-Break">different partitions</span></li>
				<li class="Alphabets">With shuffle, data is written to <span class="No-Break">the disk</span></li>
				<li class="Alphabets">The shuffle command transforms data <span class="No-Break">in Spark</span></li>
				<li class="Alphabets">Shuffles are a <span class="No-Break">narrow transformation</span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 11</strong></span><span class="No-Break">:</span></p>
			<p>What is the role of actions <span class="No-Break">in Spark?</span></p>
			<ol class="margin-left">
				<li class="Alphabets">Actions only read data from <span class="No-Break">a disk</span></li>
				<li class="Alphabets">Actions are used to modify <span class="No-Break">existing RDDs</span></li>
				<li class="Alphabets">Actions trigger the execution <span class="No-Break">of tasks</span></li>
				<li class="Alphabets">Actions are used to establish <span class="No-Break">stage boundaries</span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 12</strong></span><span class="No-Break">:</span></p>
			<p>Which of the following is one of the tasks of the cluster manager <span class="No-Break">in Spark?</span></p>
			<ol class="margin-left">
				<li class="Alphabets">In the event of an executor failure, the cluster manager will collaborate with the driver to initiate a <span class="No-Break">new executor</span></li>
				<li class="Alphabets">The cluster manager can coalesce partitions to increase the speed of complex <span class="No-Break">data processing</span></li>
				<li class="Alphabets">The cluster manager collects runtime statistics <span class="No-Break">of queries</span></li>
				<li class="Alphabets">The cluster manager creates <span class="No-Break">query plans</span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 13</strong></span><span class="No-Break">:</span></p>
			<p>Which of the following is one of the tasks of adaptive query execution <span class="No-Break">in Spark?</span></p>
			<ol class="margin-left">
				<li class="Alphabets">Adaptive query execution can coalesce partitions to increase the speed of complex <span class="No-Break">data processing</span></li>
				<li class="Alphabets">In the event of an executor failure, the adaptive query execution feature will collaborate with the driver to initiate a <span class="No-Break">new executor</span></li>
				<li class="Alphabets">Adaptive query execution creates <span class="No-Break">query plans</span></li>
				<li class="Alphabets">Adaptive query execution is responsible for spawning multiple executors to carry our tasks <span class="No-Break">in Spark</span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 14</strong></span><span class="No-Break">:</span></p>
			<p>Which of the following operations is considered <span class="No-Break">a transformation?</span></p>
			<ol class="margin-left">
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">df.select()</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">df.show()</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">df.head()</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">df.count()</strong></span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 15</strong></span><span class="No-Break">:</span></p>
			<p>What is a feature of lazy evaluation <span class="No-Break">in Spark?</span></p>
			<ol class="margin-left">
				<li class="Alphabets">Spark will fail a job only during execution but not <span class="No-Break">during definition</span></li>
				<li class="Alphabets">Spark will fail a job only <span class="No-Break">during definition</span></li>
				<li class="Alphabets">Spark will execute upon receiving a <span class="No-Break">transformation operation</span></li>
				<li class="Alphabets">Spark will fail upon receiving <span class="No-Break">an action</span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 16</strong></span><span class="No-Break">:</span></p>
			<p>Which of the following statements about Spark’s execution hierarchy <span class="No-Break">is correct?</span></p>
			<ol class="margin-left">
				<li class="Alphabets">In Spark’s execution hierarchy, tasks are above the level <span class="No-Break">of jobs</span></li>
				<li class="Alphabets">In Spark’s execution hierarchy, multiple jobs are contained in <span class="No-Break">a stage</span></li>
				<li class="Alphabets">In Spark’s execution hierarchy, a job can potentially span multiple <span class="No-Break">stage boundaries</span></li>
				<li class="Alphabets">In Spark’s execution hierarchy, slots are the <span class="No-Break">smallest unit</span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 17</strong></span><span class="No-Break">:</span></p>
			<p>Which of the following is the characteristic of the <span class="No-Break">Spark driver?</span></p>
			<ol class="margin-left">
				<li class="Alphabets">The worker nodes are responsible for transforming Spark operations into DAGs when the driver sends <span class="No-Break">a command</span></li>
				<li class="Alphabets">The Spark driver is responsible for executing tasks and returning results <span class="No-Break">to executors</span></li>
				<li class="Alphabets">Spark driver can be scaled by adding more machines so that the performance of Spark tasks can <span class="No-Break">be improved</span></li>
				<li class="Alphabets">The Spark driver processes partitions in an optimized and <span class="No-Break">distributed fashion</span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 18</strong></span><span class="No-Break">:</span></p>
			<p>Which of the following statements about broadcast variables <span class="No-Break">is accurate?</span></p>
			<ol class="margin-left">
				<li class="Alphabets">Broadcast variables are only present on <span class="No-Break">driver nodes</span></li>
				<li class="Alphabets">Broadcast variables can only be used for tables that fit <span class="No-Break">into memory</span></li>
				<li class="Alphabets">Broadcast variables are not immutable, meaning they can be shared <span class="No-Break">across clusters</span></li>
				<li class="Alphabets">Broadcast variables are not shared across the <span class="No-Break">worker nodes</span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 19</strong></span><span class="No-Break">:</span></p>
			<p>Which of the following code blocks returns unique values in columns <strong class="source-inline">employee_state</strong> and <strong class="source-inline">employee_salary</strong> in DataFrame <strong class="source-inline">df</strong> for <span class="No-Break">all columns?</span></p>
			<ol class="margin-left">
				<li class="Alphabets"><strong class="source-inline">Df.select('employee_state').join(df.select('employee_salary'), </strong><span class="No-Break"><strong class="source-inline">col('employee_state')==col('employee_salary'), 'left').show()</strong></span></li>
				<li class="Alphabets"><strong class="source-inline">df.select(col('employee_state'), </strong><span class="No-Break"><strong class="source-inline">col('employee_salary')).agg({'*': 'count'}).show()</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">df.select('employee_state', 'employee_salary').distinct().show()</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">df.select('employee_state').union(df.select('employee_salary')).distinct().show()</strong></span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 20</strong></span><span class="No-Break">:</span></p>
			<p>Which of the following code blocks reads a Parquet file from the <strong class="source-inline">my_fle_path</strong> location, where the file name is <strong class="source-inline">my_file.parquet</strong>, into a <span class="No-Break">DataFrame </span><span class="No-Break"><strong class="source-inline">df</strong></span><span class="No-Break">?</span></p>
			<ol class="margin-left">
				<li class="Alphabets"><strong class="source-inline">df = </strong><span class="No-Break"><strong class="source-inline">spark.mode("parquet").read("my_fle_path/my_file.parquet")</strong></span></li>
				<li class="Alphabets"><strong class="source-inline">df = </strong><span class="No-Break"><strong class="source-inline">spark.read.path("my_fle_path/my_file.parquet")</strong></span></li>
				<li class="Alphabets"><strong class="source-inline">df = </strong><span class="No-Break"><strong class="source-inline">spark.read().parquet("my_fle_path/my_file.parquet")</strong></span></li>
				<li class="Alphabets"><strong class="source-inline">df = </strong><span class="No-Break"><strong class="source-inline">spark.read.parquet("/my_fle_path/my_file.parquet")</strong></span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 21</strong></span><span class="No-Break">:</span></p>
			<p>Which of the following code blocks performs an inner join of the <strong class="source-inline">salarydf</strong> and <strong class="source-inline">employeedf</strong> DataFrames for columns <strong class="source-inline">employeeSalaryID</strong> and <span class="No-Break"><strong class="source-inline">employeeID</strong></span><span class="No-Break">, respectively?</span></p>
			<ol class="margin-left">
				<li class="Alphabets"><strong class="source-inline">salarydf.join(employeedf, salarydf.employeeID == </strong><span class="No-Break"><strong class="source-inline">employeedf.employeeSalaryID)</strong></span></li>
				<li class="Alphabets"><ol><li class="lower-roman"><span class="No-Break"><strong class="source-inline">Salarydf.createOrReplaceTempView(salarydf)</strong></span></li><li class="lower-roman"><span class="No-Break"><strong class="source-inline">employeedf.createOrReplaceTempView('employeedf')</strong></span></li><li class="lower-roman"><strong class="source-inline">spark.sql("SELECT * FROM salarydf CROSS JOIN employeedf ON </strong><span class="No-Break"><strong class="source-inline">employeeSalaryID ==employeeID")</strong></span></li></ol></li>
				<li class="Alphabets"><ol><li class="lower-roman"><span class="No-Break"><strong class="source-inline">salarydf</strong></span></li><li class="lower-roman"><strong class="source-inline">.</strong><span class="No-Break"><strong class="source-inline">join(employeedf, col(employeeID)==col(employeeSalaryID))</strong></span></li></ol></li>
				<li class="Alphabets"><ol><li class="lower-roman"><span class="No-Break"><strong class="source-inline">Salarydf.createOrReplaceTempView(salarydf)</strong></span></li><li class="lower-roman"><span class="No-Break"><strong class="source-inline">employeedf.createOrReplaceTempView('employeedf')</strong></span></li><li class="lower-roman"><strong class="source-inline">SELECT * </strong><span class="No-Break"><strong class="source-inline">FROM salarydf</strong></span></li><li class="lower-roman"><strong class="source-inline">INNER </strong><span class="No-Break"><strong class="source-inline">JOIN employeedf</strong></span></li><li class="lower-roman"><strong class="source-inline">ON salarydf.employeeSalaryID == </strong><span class="No-Break"><strong class="source-inline">employeedf. employeeID</strong></span></li></ol></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 22</strong></span><span class="No-Break">:</span></p>
			<p>Which of the following code blocks returns the <strong class="source-inline">df</strong> DataFrame sorted in descending order by column salary, showing missing values in <span class="No-Break">the end?</span></p>
			<ol class="margin-left">
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">df.sort(nulls_last("salary"))</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">df.orderBy("salary").nulls_last()</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">df.sort("salary", ascending=False)</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">df.nulls_last("salary")</strong></span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 23</strong></span><span class="No-Break">:</span></p>
			<p>The following code block contains an error. The code block should return a copy of the <strong class="source-inline">df</strong> DataFrame, where the name of the column state is changed to <strong class="source-inline">stateID</strong>. Find <span class="No-Break">the error.</span></p>
			<p><span class="No-Break">Code block:</span></p>
			<pre class="source-code">
df.withColumn("stateID", "state")</pre>			<ol class="margin-left">
				<li class="Alphabets">The arguments to the method <strong class="source-inline">"stateID"</strong> and <strong class="source-inline">"state"</strong> should <span class="No-Break">be swapped</span></li>
				<li class="Alphabets">The <strong class="source-inline">withColumn</strong> method should be replaced by the <span class="No-Break"><strong class="source-inline">withColumnRenamed</strong></span><span class="No-Break"> method</span></li>
				<li class="Alphabets">The <strong class="source-inline">withColumn</strong> method should be replaced by <strong class="source-inline">withColumnRenamed</strong> method, and the arguments to the method need to <span class="No-Break">be reordered</span></li>
				<li class="Alphabets">There is no such method whereby the column name can <span class="No-Break">be changed</span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 24</strong></span><span class="No-Break">:</span></p>
			<p>Which of the following code blocks performs an inner join between the <strong class="source-inline">salarydf</strong> and <strong class="source-inline">employeedf</strong> DataFrames, using the <strong class="source-inline">employeeID</strong> and <strong class="source-inline">salaryEmployeeID</strong> columns as join <span class="No-Break">keys, respectively?</span></p>
			<ol class="margin-left">
				<li class="Alphabets"><strong class="source-inline">salarydf.join(employeedf, "inner", salarydf.employeedf == </strong><span class="No-Break"><strong class="source-inline">employeeID.salaryEmployeeID)</strong></span></li>
				<li class="Alphabets"><strong class="source-inline">salarydf.join(employeedf, employeeID == </strong><span class="No-Break"><strong class="source-inline">salaryEmployeeID)</strong></span></li>
				<li class="Alphabets"><strong class="source-inline">salarydf.join(employeedf, salarydf.salaryEmployeeID == </strong><span class="No-Break"><strong class="source-inline">employeedf.employeeID, "inner")</strong></span></li>
				<li class="Alphabets"><strong class="source-inline">salarydf.join(employeedf, salarydf.employeeID == </strong><span class="No-Break"><strong class="source-inline">employeedf.salaryEmployeeID, "inner")</strong></span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 25</strong></span><span class="No-Break">:</span></p>
			<p>The following code block should return a <strong class="source-inline">df</strong> DataFrame, where the <strong class="source-inline">employeeID</strong> column is converted into an integer. Choose the answer that correctly fills the blanks in the code block to <span class="No-Break">accomplish this:</span></p>
			<pre class="source-code">
df.__1__(__2__.__3__(__4__))</pre>			<ol class="margin-left">
				<li class="Alphabets"><ol><li class="lower-roman"><span class="No-Break"><strong class="source-inline">select</strong></span></li><li class="lower-roman"><span class="No-Break"><strong class="source-inline">col("employeeID")</strong></span></li><li class="lower-roman"><span class="No-Break"><strong class="source-inline">as</strong></span></li><li class="lower-roman"><span class="No-Break"><strong class="source-inline">IntegerType</strong></span></li></ol></li>
				<li class="Alphabets"><ol><li class="lower-roman"><span class="No-Break"><strong class="source-inline">select</strong></span></li><li class="lower-roman"><span class="No-Break"><strong class="source-inline">col("employeeID")</strong></span></li><li class="lower-roman"><span class="No-Break"><strong class="source-inline">as</strong></span></li><li class="lower-roman"><span class="No-Break"><strong class="source-inline">Integer</strong></span></li></ol></li>
				<li class="Alphabets"><ol><li class="lower-roman"><span class="No-Break"><strong class="source-inline">cast</strong></span></li><li class="lower-roman"><strong class="source-inline">"</strong><span class="No-Break"><strong class="source-inline">employeeID"</strong></span></li><li class="lower-roman"><span class="No-Break"><strong class="source-inline">as</strong></span></li><li class="lower-roman"><span class="No-Break"><strong class="source-inline">IntegerType()</strong></span></li></ol></li>
				<li class="Alphabets"><ol><li class="lower-roman"><span class="No-Break"><strong class="source-inline">select</strong></span></li><li class="lower-roman"><span class="No-Break"><strong class="source-inline">col("employeeID")</strong></span></li><li class="lower-roman"><span class="No-Break"><strong class="source-inline">cast</strong></span></li><li class="lower-roman"><span class="No-Break"><strong class="source-inline">IntegerType()</strong></span></li></ol></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 26</strong></span><span class="No-Break">:</span></p>
			<p>Find the number of records that are not empty in the column department of the resulting DataFrame when we join the <strong class="source-inline">employeedf</strong> and <strong class="source-inline">salarydf</strong> DataFrames for the <strong class="source-inline">employeeID</strong> and <strong class="source-inline">employeeSalaryID</strong> columns, respectively. Which code blocks (in order) should be executed to <span class="No-Break">achieve this?</span></p>
			<p><span class="No-Break">1. </span><span class="No-Break"><strong class="source-inline">.filter(col("department").isNotNull())</strong></span></p>
			<p><span class="No-Break">2. </span><span class="No-Break"><strong class="source-inline">.count()</strong></span></p>
			<p>3. <strong class="source-inline">employeedf.join(salarydf, employeedf.employeeID == </strong><span class="No-Break"><strong class="source-inline">salarydf.employeeSalaryID)</strong></span></p>
			<p>4. <strong class="source-inline">employeedf.join(salarydf, employeedf.employeeID ==salarydf. </strong><span class="No-Break"><strong class="source-inline">employeeSalaryID, how='inner')</strong></span></p>
			<p><span class="No-Break">5. </span><span class="No-Break"><strong class="source-inline">.filter(col(department).isnotnull())</strong></span></p>
			<p><span class="No-Break">6. </span><span class="No-Break"><strong class="source-inline">.sum(col(department))</strong></span></p>
			<ol class="margin-left">
				<li class="Alphabets">3, <span class="No-Break">1, 6</span></li>
				<li class="Alphabets">3, <span class="No-Break">1, 2</span></li>
				<li class="Alphabets">4, <span class="No-Break">1, 2</span></li>
				<li class="Alphabets">3, <span class="No-Break">5, 2</span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 27</strong></span><span class="No-Break">:</span></p>
			<p>Which of the following code blocks returns only those rows from the <strong class="source-inline">df</strong> DataFrame in which the values in the column state <span class="No-Break">are unique?</span></p>
			<ol class="margin-left">
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">df.dropDuplicates(subset=["state"]).show()</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">df.distinct(subset=["state"]).show()</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">df.drop_duplicates(subset=["state"]).show()</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">df.unique("state").show()</strong></span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 28</strong></span><span class="No-Break">:</span></p>
			<p>The following code block contains an error. The code block should return a copy of the <strong class="source-inline">df</strong> DataFrame with an additional column named <strong class="source-inline">squared_number</strong>, which has the square of the column number. Find <span class="No-Break">the error.</span></p>
			<p><span class="No-Break">Code block:</span></p>
			<pre class="source-code">
df.withColumnRenamed(col("number"), pow(col("number"), 0.2).alias("squared_number"))</pre>			<ol class="margin-left">
				<li class="Alphabets">The arguments to the <strong class="source-inline">withColumnRenamed</strong> method need to <span class="No-Break">be reordered</span></li>
				<li class="Alphabets">The <strong class="source-inline">withColumnRenamed</strong> method should be replaced by the <span class="No-Break"><strong class="source-inline">withColumn</strong></span><span class="No-Break"> method</span></li>
				<li class="Alphabets">The <strong class="source-inline">withColumnRenamed</strong> method should be replaced by the <strong class="source-inline">select</strong> method, and <strong class="source-inline">0.2</strong> should be replaced <span class="No-Break">with </span><span class="No-Break"><strong class="source-inline">2</strong></span></li>
				<li class="Alphabets">The argument <strong class="source-inline">0.2</strong> should be replaced <span class="No-Break">by </span><span class="No-Break"><strong class="source-inline">2</strong></span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 29</strong></span><span class="No-Break">:</span></p>
			<p>Which of the following code blocks returns a new DataFrame in which column salary is renamed to <strong class="source-inline">new_salary</strong> and employee is renamed to <strong class="source-inline">new_employee</strong> in the <span class="No-Break"><strong class="source-inline">df</strong></span><span class="No-Break"> DataFrame?</span></p>
			<ol class="margin-left">
				<li class="Alphabets"><strong class="source-inline">df.withColumnRenamed(salary, </strong><span class="No-Break"><strong class="source-inline">new_salary).withColumnRenamed(employee, new_employee)</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">df.withColumnRenamed("salary", "new_salary")</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">df.withColumnRenamed("employee", "new_employee")</strong></span></li>
				<li class="Alphabets"><strong class="source-inline">df.withColumn("salary", "</strong><span class="No-Break"><strong class="source-inline">new_salary").withColumn("employee", "new_employee")</strong></span></li>
				<li class="Alphabets"><strong class="source-inline">df.withColumnRenamed("salary", "</strong><span class="No-Break"><strong class="source-inline">new_salary").withColumnRenamed("employee", "new_employee")</strong></span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 30</strong></span><span class="No-Break">:</span></p>
			<p>Which of the following code blocks returns a copy of the <strong class="source-inline">df</strong> DataFrame, where the column salary has been renamed <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">employeeSalary</strong></span><span class="No-Break">?</span></p>
			<ol class="margin-left">
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">df.withColumn(["salary", "employeeSalary"])</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">df.withColumnRenamed("salary").alias("employeeSalary ")</strong></span></li>
				<li class="Alphabets"><strong class="source-inline">df.withColumnRenamed("salary", "</strong><span class="No-Break"><strong class="source-inline">employeeSalary ")</strong></span></li>
				<li class="Alphabets"><strong class="source-inline">df.withColumn("salary", "</strong><span class="No-Break"><strong class="source-inline">employeeSalary ")</strong></span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 31</strong></span><span class="No-Break">:</span></p>
			<p>The following code block contains an error. The code block should save the <strong class="source-inline">df</strong> DataFrame to the <strong class="source-inline">my_file_path</strong> path as a Parquet file, appending to any existing parquet file. Find <span class="No-Break">the error.</span></p>
			<pre class="source-code">
df.format("parquet").option("mode", "append").save(my_file_path)</pre>			<ol class="margin-left">
				<li class="Alphabets">The code is not saved to the <span class="No-Break">correct path</span></li>
				<li class="Alphabets">The <strong class="source-inline">save()</strong> and <strong class="source-inline">format</strong> functions should <span class="No-Break">be swapped</span></li>
				<li class="Alphabets">The code block is missing a reference to <span class="No-Break">the </span><span class="No-Break"><strong class="source-inline">DataFrameWriter</strong></span></li>
				<li class="Alphabets">The <strong class="source-inline">option</strong> mode should be overwritten to correctly write <span class="No-Break">the file</span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 32</strong></span><span class="No-Break">:</span></p>
			<p>How can we reduce the <strong class="source-inline">df</strong> DataFrame from 12 to <span class="No-Break">6 partitions?</span></p>
			<ol class="margin-left">
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">df.repartition(12)</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">df.coalesce(6).shuffle()</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">df.coalesce(6, shuffle=True)</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">df.repartition(6)</strong></span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 33</strong></span><span class="No-Break">:</span></p>
			<p>Which of the following code blocks returns a DataFrame where the timestamp column is converted into unix epoch timestamps in a new column named <strong class="source-inline">record_timestamp</strong> with a format of day, month, <span class="No-Break">and year?</span></p>
			<ol class="margin-left">
				<li class="Alphabets"><strong class="source-inline">df.withColumn("record_timestamp", </strong><span class="No-Break"><strong class="source-inline">from_unixtime(unix_timestamp(col("timestamp")), "dd-MM-yyyy"))</strong></span></li>
				<li class="Alphabets"><strong class="source-inline">df.withColumnRenamed("record_timestamp", </strong><span class="No-Break"><strong class="source-inline">from_unixtime(unix_timestamp(col("timestamp")), "dd-MM-yyyy"))</strong></span></li>
				<li class="Alphabets"><strong class="source-inline">df.select ("record_timestamp", </strong><span class="No-Break"><strong class="source-inline">from_unixtime(unix_timestamp(col("timestamp")), "dd-MM-yyyy"))</strong></span></li>
				<li class="Alphabets"><strong class="source-inline">df.withColumn("record_timestamp", </strong><span class="No-Break"><strong class="source-inline">from_unixtime(unix_timestamp(col("timestamp")), "MM-dd-yyyy"))</strong></span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 34</strong></span><span class="No-Break">:</span></p>
			<p>Which of the following code blocks creates a new DataFrame by appending the rows of the DataFrame <strong class="source-inline">salaryDf</strong> to the rows of the DataFrame <strong class="source-inline">employeeDf</strong>, regardless of the fact that both DataFrames have different <span class="No-Break">column names?</span></p>
			<ol class="margin-left">
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">salaryDf.join(employeeDf)</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">salaryDf.union(employeeDf)</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">salaryDf.concat(employeeDf)</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">salaryDf.unionAll(employeeDf)</strong></span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 35</strong></span><span class="No-Break">:</span></p>
			<p>The following code block contains an error. The code block should calculate the total of all salaries in the <strong class="source-inline">employee_salary</strong> column across each department. Find <span class="No-Break">the error.</span></p>
			<pre class="source-code">
df.agg("department").sum("employee_salary")</pre>			<ol class="margin-left">
				<li class="Alphabets">Instead of <strong class="source-inline">avg("value")</strong>, <strong class="source-inline">avg(col("value"))</strong> should <span class="No-Break">be used</span></li>
				<li class="Alphabets">All column names should be wrapped in <span class="No-Break"><strong class="source-inline">col()</strong></span><span class="No-Break"> operators</span></li>
				<li class="Alphabets"><strong class="source-inline">"storeId"</strong> and “<strong class="source-inline">value"</strong> should <span class="No-Break">be swapped</span></li>
				<li class="Alphabets"><strong class="source-inline">Agg</strong> should be replaced <span class="No-Break">by </span><span class="No-Break"><strong class="source-inline">groupBy</strong></span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 36</strong></span><span class="No-Break">:</span></p>
			<p>The following code block contains an error. The code block is intended to perform a cross-join of the <strong class="source-inline">salarydf</strong> and <strong class="source-inline">employeedf</strong> DataFrames for the <strong class="source-inline">employeeSalaryID</strong> and <strong class="source-inline">employeeID</strong> columns, respectively. Find <span class="No-Break">the error.</span></p>
			<pre class="source-code">
employeedf.join(salarydf, [salarydf.employeeSalaryID, employeedf.employeeID], "cross")</pre>			<ol class="margin-left">
				<li class="Alphabets">The join type <strong class="source-inline">"cross"</strong> in the argument needs to be replaced <span class="No-Break">with </span><span class="No-Break"><strong class="source-inline">crossJoin</strong></span></li>
				<li class="Alphabets">[<strong class="source-inline">salarydf.employeeSalaryID, employeedf.employeeID</strong>] should be replaced by <strong class="source-inline">salarydf.employeeSalaryID == </strong><span class="No-Break"><strong class="source-inline">employeedf.employeeID</strong></span></li>
				<li class="Alphabets">The <strong class="source-inline">"cross"</strong> argument should be eliminated since <strong class="source-inline">"cross"</strong> is the default <span class="No-Break">join type</span></li>
				<li class="Alphabets">The <strong class="source-inline">"cross"</strong> argument should be eliminated from the call and <strong class="source-inline">join</strong> should be replaced <span class="No-Break">by </span><span class="No-Break"><strong class="source-inline">crossJoin</strong></span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 37</strong></span><span class="No-Break">:</span></p>
			<p>The following code block contains an error. The code block should display the schema of the <strong class="source-inline">df </strong>DataFrame. Find <span class="No-Break">the error.</span></p>
			<pre class="source-code">
df.rdd.printSchema()</pre>			<ol class="margin-left">
				<li class="Alphabets">In Spark, we cannot print the schema of <span class="No-Break">a DataFrame</span></li>
				<li class="Alphabets"><strong class="source-inline">printSchema</strong> is not callable through <strong class="source-inline">df.rdd</strong> and should be called directly <span class="No-Break">from </span><span class="No-Break"><strong class="source-inline">df</strong></span></li>
				<li class="Alphabets">There is no method in Spark <span class="No-Break">named </span><span class="No-Break"><strong class="source-inline">printSchema()</strong></span></li>
				<li class="Alphabets">The <strong class="source-inline">print_schema()</strong> method should be used instead <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">printSchema()</strong></span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 38</strong></span><span class="No-Break">:</span></p>
			<p>The following code block should write the <strong class="source-inline">df</strong> DataFrame as a Parquet file to the <strong class="source-inline">filePath</strong> path, replacing any existing file. Choose the answer that correctly fills the blanks in the code block to <span class="No-Break">accomplish this:</span></p>
			<pre class="source-code">
df.__1__.format("parquet").__2__(__3__).__4__(filePath)</pre>			<ol class="margin-left">
				<li class="Alphabets"><ol><li class="lower-roman"><span class="No-Break"><strong class="source-inline">save</strong></span></li><li class="lower-roman"><span class="No-Break"><strong class="source-inline">mode</strong></span></li><li class="lower-roman"><strong class="source-inline">"</strong><span class="No-Break"><strong class="source-inline">ignore"</strong></span></li><li class="lower-roman"><span class="No-Break"><strong class="source-inline">path</strong></span></li></ol></li>
				<li class="Alphabets"><ol><li class="lower-roman"><span class="No-Break"><strong class="source-inline">store</strong></span></li><li class="lower-roman"><span class="No-Break"><strong class="source-inline">with</strong></span></li><li class="lower-roman"><strong class="source-inline">"</strong><span class="No-Break"><strong class="source-inline">replace"</strong></span></li><li class="lower-roman"><span class="No-Break"><strong class="source-inline">path</strong></span></li></ol></li>
				<li class="Alphabets"><ol><li class="lower-roman"><span class="No-Break"><strong class="source-inline">write</strong></span></li><li class="lower-roman"><span class="No-Break"><strong class="source-inline">mode</strong></span></li><li class="lower-roman"><strong class="source-inline">"</strong><span class="No-Break"><strong class="source-inline">overwrite"</strong></span></li><li class="lower-roman"><span class="No-Break"><strong class="source-inline">save</strong></span></li></ol></li>
				<li class="Alphabets"><ol><li class="lower-roman"><span class="No-Break"><strong class="source-inline">save</strong></span></li><li class="lower-roman"><span class="No-Break"><strong class="source-inline">mode</strong></span></li><li class="lower-roman"><strong class="source-inline">"</strong><span class="No-Break"><strong class="source-inline">overwrite"</strong></span></li><li class="lower-roman"><span class="No-Break"><strong class="source-inline">path</strong></span></li></ol></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 39</strong></span><span class="No-Break">:</span></p>
			<p>The following code block contains an error. The code block is supposed to sort the <strong class="source-inline">df</strong> DataFrame according to salary in descending order. Then, it should sort based on the bonus column, putting nulls to last. Find <span class="No-Break">the error.</span></p>
			<pre class="source-code">
df.orderBy ('salary', asc_nulls_first(col('bonus')))
transactionsDf.orderBy('value', asc_nulls_first(col('predError')))</pre>			<ol class="margin-left">
				<li class="Alphabets">The <strong class="source-inline">salary</strong> column should be sorted in a descending way. Moreover, it should be wrapped in a <span class="No-Break"><strong class="source-inline">col()</strong></span><span class="No-Break"> operator</span></li>
				<li class="Alphabets">The <strong class="source-inline">salary</strong> column should be wrapped by the <span class="No-Break"><strong class="source-inline">col()</strong></span><span class="No-Break"> operator</span></li>
				<li class="Alphabets">The <strong class="source-inline">bonus</strong> column should be sorted in a descending way, putting <span class="No-Break"><strong class="source-inline">nulls</strong></span><span class="No-Break"> last</span></li>
				<li class="Alphabets">The <strong class="source-inline">bonus</strong> column should be sorted by <span class="No-Break"><strong class="source-inline">desc_nulls_first()</strong></span><span class="No-Break"> instead</span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 40</strong></span><span class="No-Break">:</span></p>
			<p>The following code block contains an error. The code block should use the <strong class="source-inline">square_root_method</strong> Python method to find the square root of the <strong class="source-inline">salary</strong> column in the <strong class="source-inline">df</strong> DataFrame and return it in a new column called <strong class="source-inline">sqrt_salary</strong>. Find <span class="No-Break">the error.</span></p>
			<pre class="source-code">
square_root_method_udf = udf(square_root_method)
df.withColumn("sqrt_salary", square_root_method("salary"))</pre>			<ol class="margin-left">
				<li class="Alphabets">There is no return type specified <span class="No-Break">for </span><span class="No-Break"><strong class="source-inline">square_root_method</strong></span></li>
				<li class="Alphabets">In the second line of the code, Spark needs to call <strong class="source-inline">squre_root_method_udf</strong> instead <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">square_root_method</strong></span></li>
				<li class="Alphabets"><strong class="source-inline">udf</strong> is not registered <span class="No-Break">with Spark</span></li>
				<li class="Alphabets">A new column needs to <span class="No-Break">be added</span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 41</strong></span><span class="No-Break">:</span></p>
			<p>The following code block contains an error. The code block should return the <strong class="source-inline">df</strong> DataFrame with <strong class="source-inline">employeeID</strong> renamed to <strong class="source-inline">employeeIdColumn</strong>. Find <span class="No-Break">the error.</span></p>
			<pre class="source-code">
df.withColumn("employeeIdColumn", "employeeID")</pre>			<ol class="margin-left">
				<li class="Alphabets">Instead of <strong class="source-inline">withColumn</strong>, the <strong class="source-inline">withColumnRenamed</strong> method should <span class="No-Break">be used</span></li>
				<li class="Alphabets">Instead of <strong class="source-inline">withColumn</strong>, the <strong class="source-inline">withColumnRenamed</strong> method should be used and argument <strong class="source-inline">"employeeIdColumn"</strong> should be swapped with <span class="No-Break">argument </span><span class="No-Break"><strong class="source-inline">"employeeID"</strong></span></li>
				<li class="Alphabets">Arguments <strong class="source-inline">"employeeIdColumn"</strong> and <strong class="source-inline">"employeeID"</strong> should <span class="No-Break">be swapped</span></li>
				<li class="Alphabets">The <strong class="source-inline">withColumn</strong> operator should be replaced with the <span class="No-Break"><strong class="source-inline">withColumnRenamed</strong></span><span class="No-Break"> operator</span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 42</strong></span><span class="No-Break">:</span></p>
			<p>Which of the following code blocks will return a new DataFrame with the same columns as DataFrame <strong class="source-inline">df</strong>, except for the <span class="No-Break"><strong class="source-inline">salary</strong></span><span class="No-Break"> column?</span></p>
			<ol class="margin-left">
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">df.drop("salary")</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">df.drop(col(salary))</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">df.drop(salary)</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">df.delete("salary")</strong></span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 43</strong></span><span class="No-Break">:</span></p>
			<p>Which of the following code blocks returns a DataFrame showing the mean of the salary column from the <strong class="source-inline">df</strong> DataFrame, grouped by <span class="No-Break">column department?</span></p>
			<ol class="margin-left">
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">df.groupBy("department").agg(avg("salary"))</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">df.groupBy(col(department).avg())</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">df.groupBy("department").avg(col("salary"))</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">df.groupBy("department").agg(average("salary"))</strong></span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 44</strong></span><span class="No-Break">:</span></p>
			<p>Which of the following code blocks creates a DataFrame that shows the mean of the salary column of the <strong class="source-inline">salaryDf</strong> DataFrame, based on the department and state columns, where age is greater <span class="No-Break">than 35?</span></p>
			<ol>
				<li><strong class="source-inline">salaryDf.filter(col("age") &gt; </strong><span class="No-Break"><strong class="source-inline">35)</strong></span></li>
				<li><strong class="source-inline">.</strong><span class="No-Break"><strong class="source-inline">filter(col("employeeID")</strong></span></li>
				<li><strong class="source-inline">.</strong><span class="No-Break"><strong class="source-inline">filter(col("employeeID").isNotNull())</strong></span></li>
				<li><strong class="source-inline">.</strong><span class="No-Break"><strong class="source-inline">groupBy("department")</strong></span></li>
				<li><strong class="source-inline">.</strong><span class="No-Break"><strong class="source-inline">groupBy("department", "state")</strong></span></li>
				<li><strong class="source-inline">.</strong><span class="No-Break"><strong class="source-inline">agg(avg("salary").alias("mean_salary"))</strong></span></li>
				<li><strong class="source-inline">.</strong><span class="No-Break"><strong class="source-inline">agg(average("salary").alias("mean_salary"))</strong></span><ol><li class="Alphabets"><span class="No-Break">1,2,5,6</span></li><li class="Alphabets"><span class="No-Break">1,3,5,6</span></li><li class="Alphabets"><span class="No-Break">1,3,6,7</span></li><li class="Alphabets"><span class="No-Break">1,2,4,6</span></li></ol></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 45</strong></span><span class="No-Break">:</span></p>
			<p>The following code block contains an error. The code block needs to cache the <strong class="source-inline">df</strong> DataFrame so that this DataFrame is fault-tolerant. Find <span class="No-Break">the error.</span></p>
			<pre class="source-code">
df.persist(StorageLevel.MEMORY_AND_DISK_3)</pre>			<ol class="margin-left">
				<li class="Alphabets"><strong class="source-inline">persist()</strong> is not a function of the <span class="No-Break">API DataFrame</span></li>
				<li class="Alphabets"><strong class="source-inline">df.write()</strong> should be used in conjunction with <strong class="source-inline">df.persist</strong> to correctly write <span class="No-Break">the DataFrame</span></li>
				<li class="Alphabets">The storage level is incorrect and should <span class="No-Break">be </span><span class="No-Break"><strong class="source-inline">MEMORY_AND_DISK_2</strong></span></li>
				<li class="Alphabets"><strong class="source-inline">df.cache()</strong> should be used instead <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">df.persist()</strong></span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 46</strong></span><span class="No-Break">:</span></p>
			<p>Which of the following code blocks concatenates the rows of the <strong class="source-inline">salaryDf</strong> and <strong class="source-inline">employeeDf </strong>DataFrames without any duplicates (assuming the columns of both DataFrames <span class="No-Break">are similar)?</span></p>
			<ol class="margin-left">
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">salaryDf.concat(employeeDf).unique()</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">spark.union(salaryDf, employeeDf).distinct()</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">salaryDf.union(employeeDf).unique()</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">salaryDf.union(employeeDf).distinct()</strong></span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 47</strong></span><span class="No-Break">:</span></p>
			<p>Which of the following code blocks reads a complete folder of CSV files from <strong class="source-inline">filePath</strong> with <span class="No-Break">column headers?</span></p>
			<ol class="margin-left">
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">spark.option("header",True).csv(filePath)</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">spark.read.load(filePath)</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">spark.read().option("header",True).load(filePath)</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">spark.read.format("csv").option("header",True).load(filePath)</strong></span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 48</strong></span><span class="No-Break">:</span></p>
			<p>The following code block contains an error. The <strong class="source-inline">df</strong> DataFrame contains columns [<strong class="source-inline">employeeID</strong>,<strong class="source-inline"> salary</strong>, and <strong class="source-inline">department</strong>]. The code block should return a DataFrame that contains only the <strong class="source-inline">employeeID</strong> and <strong class="source-inline">salary</strong> columns from DataFrame <strong class="source-inline">df</strong>. Find <span class="No-Break">the error.</span></p>
			<pre class="source-code">
df.select(col(department))</pre>			<ol class="margin-left">
				<li class="Alphabets">All column names from the <strong class="source-inline">df</strong> DataFrame should be specified in the <span class="No-Break"><strong class="source-inline">select</strong></span><span class="No-Break"> arguments</span></li>
				<li class="Alphabets">The <strong class="source-inline">select</strong> operator should be replaced by a <strong class="source-inline">drop</strong> operator, and all the column names from the <strong class="source-inline">df</strong> DataFrame should be listed as <span class="No-Break">a list</span></li>
				<li class="Alphabets">The <strong class="source-inline">select</strong> operator should be replaced by a <span class="No-Break"><strong class="source-inline">drop</strong></span><span class="No-Break"> operator</span></li>
				<li class="Alphabets">The column name <strong class="source-inline">department</strong> should be listed <span class="No-Break">like </span><span class="No-Break"><strong class="source-inline">col("department")</strong></span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 49</strong></span><span class="No-Break">:</span></p>
			<p>The following code block contains an error. The code block should write DataFrame <strong class="source-inline">df</strong> as a Parquet file to the <strong class="source-inline">filePath</strong> location, after partitioning it for the <strong class="source-inline">department</strong> column. Find <span class="No-Break">the error.</span></p>
			<pre class="source-code">
df.write.partition("department").parquet()</pre>			<ol class="margin-left">
				<li class="Alphabets"><strong class="source-inline">partitionBy()</strong> method should be used instead <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">partition()</strong></span><span class="No-Break">.</span></li>
				<li class="Alphabets"><strong class="source-inline">partitionBy()</strong> method should be used instead of <strong class="source-inline">partition()</strong> and <strong class="source-inline">filePath</strong> should be added to the <span class="No-Break"><strong class="source-inline">parquet</strong></span><span class="No-Break"> method</span></li>
				<li class="Alphabets">The <strong class="source-inline">partition()</strong> method should be called before the write method and <strong class="source-inline">filePath</strong> should be added to <span class="No-Break"><strong class="source-inline">parquet</strong></span><span class="No-Break"> method</span></li>
				<li class="Alphabets">The <strong class="source-inline">"department"</strong> column should be wrapped in a <span class="No-Break"><strong class="source-inline">col()</strong></span><span class="No-Break"> operator</span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 50</strong></span><span class="No-Break">:</span></p>
			<p>Which of the following code blocks removes the cached <strong class="source-inline">df</strong> DataFrame from memory <span class="No-Break">and disk?</span></p>
			<ol class="margin-left">
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">df.unpersist()</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">drop df</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">df.clearCache()</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">df.persist()</strong></span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 51</strong></span><span class="No-Break">:</span></p>
			<p>The following code block should return a copy of the <strong class="source-inline">df</strong> DataFrame with an additional column: <strong class="source-inline">test_column</strong>, which has a value of <strong class="source-inline">19</strong>. Choose the answer that correctly fills the blanks in the code block to <span class="No-Break">accomplish this:</span></p>
			<pre class="source-code">
df.__1__(__2__, __3__)</pre>			<ol class="margin-left">
				<li class="Alphabets"><ol><li class="lower-roman"><span class="No-Break"><strong class="source-inline">withColumn</strong></span></li><li class="lower-roman"><strong class="source-inline">'</strong><span class="No-Break"><strong class="source-inline">test_column'</strong></span></li><li class="lower-roman"><span class="No-Break"><strong class="source-inline">19</strong></span></li></ol></li>
				<li class="Alphabets"><ol><li class="lower-roman"><span class="No-Break"><strong class="source-inline">withColumnRenamed</strong></span></li><li class="lower-roman"><span class="No-Break"><strong class="source-inline">test_column</strong></span></li><li class="lower-roman"><span class="No-Break"><strong class="source-inline">lit(19)</strong></span></li></ol></li>
				<li class="Alphabets"><ol><li class="lower-roman"><span class="No-Break"><strong class="source-inline">withColumn</strong></span></li><li class="lower-roman"><strong class="source-inline">'</strong><span class="No-Break"><strong class="source-inline">test_column'</strong></span></li><li class="lower-roman"><span class="No-Break"><strong class="source-inline">lit(19)</strong></span></li></ol></li>
				<li class="Alphabets"><ol><li class="lower-roman"><span class="No-Break"><strong class="source-inline">withColumnRenamed</strong></span></li><li class="lower-roman"><span class="No-Break"><strong class="source-inline">test_column</strong></span></li><li class="lower-roman"><span class="No-Break"><strong class="source-inline">19</strong></span></li></ol></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 52</strong></span><span class="No-Break">:</span></p>
			<p>The following code block should return a DataFrame with the columns <strong class="source-inline">employeeId</strong>, <strong class="source-inline">salary</strong>, <strong class="source-inline">bonus</strong>, and <strong class="source-inline">department</strong> from <strong class="source-inline">transactionsDf </strong>DataFrame. Choose the answer that correctly fills the blanks to <span class="No-Break">accomplish this:</span></p>
			<pre class="source-code">
df.__1__(__2__)</pre>			<ol class="margin-left">
				<li class="Alphabets"><ol><li class="lower-roman"><span class="No-Break"><strong class="source-inline">drop</strong></span></li><li class="lower-roman"><strong class="source-inline">"employeeId", "salary", "</strong><span class="No-Break"><strong class="source-inline">bonus", "department"</strong></span></li></ol></li>
				<li class="Alphabets"><ol><li class="lower-roman"><span class="No-Break"><strong class="source-inline">filter</strong></span></li><li class="lower-roman"><strong class="source-inline">"employeeId, salary, </strong><span class="No-Break"><strong class="source-inline">bonus, department"</strong></span></li></ol></li>
				<li class="Alphabets"><ol><li class="lower-roman"><span class="No-Break"><strong class="source-inline">select</strong></span></li><li class="lower-roman"><strong class="source-inline">["employeeId", "salary", "</strong><span class="No-Break"><strong class="source-inline">bonus", "department"]</strong></span></li></ol></li>
				<li class="Alphabets"><ol><li class="lower-roman"><span class="No-Break"><strong class="source-inline">select</strong></span></li><li class="lower-roman"><strong class="source-inline">col(["employeeId", "</strong><span class="No-Break"><strong class="source-inline">salary", "bonus","department"])</strong></span></li></ol></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 53</strong></span><span class="No-Break">:</span></p>
			<p>Which of the following code blocks returns a DataFrame with the <strong class="source-inline">salary</strong> column converted into a string in the <span class="No-Break"><strong class="source-inline">df</strong></span><span class="No-Break"> DataFrame?</span></p>
			<ol class="margin-left">
				<li class="Alphabets"><strong class="source-inline">df.withColumn("salary", </strong><span class="No-Break"><strong class="source-inline">castString("salary", "string"))</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">df.withColumn("salary", col("salary").cast("string"))</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">df.select(cast("salary", "string"))</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">df.withColumn("salary", col("salary").castString("string"))</strong></span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 54</strong></span><span class="No-Break">:</span></p>
			<p>The following code block contains an error. The code block should combine data from DataFrames <strong class="source-inline">salaryDf</strong> and <strong class="source-inline">employeeDf</strong>, showing all rows of DataFrame <strong class="source-inline">salaryDf</strong> that have a matching value in column <strong class="source-inline">employeeSalaryID</strong> with a value in column <strong class="source-inline">employeeID</strong> of DataFrame <strong class="source-inline">employeeDf</strong>. Find <span class="No-Break">the error.</span></p>
			<pre class="source-code">
employeeDf.join(salaryDf, employeeDf.employeeID==employeeSalaryID)</pre>			<ol class="margin-left">
				<li class="Alphabets">The <strong class="source-inline">join</strong> statement is missing the right-hand DataFrame, where the column name <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">employeeSalaryID</strong></span></li>
				<li class="Alphabets">The <strong class="source-inline">union</strong> method should be used instead <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">join</strong></span></li>
				<li class="Alphabets">Instead of <strong class="source-inline">join</strong>, <strong class="source-inline">innerJoin</strong> should have <span class="No-Break">been used</span></li>
				<li class="Alphabets"><strong class="source-inline">salaryDf</strong> should come in place <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">employeeDf</strong></span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 55</strong></span><span class="No-Break">:</span></p>
			<p>Which of the following code blocks reads a JSON file stored at <strong class="source-inline">my_file_path</strong> as <span class="No-Break">a DataFrame?</span></p>
			<ol class="margin-left">
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">spark.read.json(my_file_path)</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">spark.read(my_file_path, source="json")</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">spark.read.path(my_file_path)</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">spark.read().json(my_file_path)</strong></span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 56</strong></span><span class="No-Break">:</span></p>
			<p>The following code block contains an error. The code block should return a new DataFrame filtered by the rows where <strong class="source-inline">salary</strong> column is greater than 2000 in DataFrame <strong class="source-inline">df</strong>. Find <span class="No-Break">the error.</span></p>
			<pre class="source-code">
df.where("col(salary) &gt;= 2000")</pre>			<ol class="margin-left">
				<li class="Alphabets">Instead of <strong class="source-inline">where()</strong>, <strong class="source-inline">filter()</strong> should <span class="No-Break">be used</span></li>
				<li class="Alphabets">The argument to the <strong class="source-inline">where</strong> method should be <strong class="source-inline">"col(salary) &gt; </strong><span class="No-Break"><strong class="source-inline">2000"</strong></span></li>
				<li class="Alphabets">Instead of <strong class="source-inline">&gt;=</strong>, the operator <strong class="source-inline">&gt;</strong> should <span class="No-Break">be used</span></li>
				<li class="Alphabets">The argument to the <strong class="source-inline">where</strong> method should be <strong class="source-inline">"salary &gt; </strong><span class="No-Break"><strong class="source-inline">2000"</strong></span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 57</strong></span><span class="No-Break">:</span></p>
			<p>Which of the following code blocks returns a DataFrame in which the <strong class="source-inline">salary</strong> and <strong class="source-inline">state</strong> columns are dropped from the <span class="No-Break"><strong class="source-inline">df</strong></span><span class="No-Break"> DataFrame?</span></p>
			<ol class="margin-left">
				<li class="Alphabets"><strong class="source-inline">df.withColumn ("</strong><span class="No-Break"><strong class="source-inline">salary", "state")</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">df.drop(["salary", "state"])</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">df.drop("salary", "state")</strong></span></li>
				<li class="Alphabets"><strong class="source-inline">df.withColumnRenamed ("</strong><span class="No-Break"><strong class="source-inline">salary", "state")</strong></span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 58</strong></span><span class="No-Break">:</span></p>
			<p>Which of the following code blocks returns a two-column DataFrame that contains counts of each department in the <span class="No-Break"><strong class="source-inline">df</strong></span><span class="No-Break"> DataFrame?</span></p>
			<ol class="margin-left">
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">df.count("department").distinct()</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">df.count("department")</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">df.groupBy("department").count()</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">df.groupBy("department").agg(count("department"))</strong></span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 59</strong></span><span class="No-Break">:</span></p>
			<p>Which of the following code blocks prints the schema of a DataFrame and contains both column names <span class="No-Break">and types?</span></p>
			<ol class="margin-left">
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">print(df.columns)</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">df.printSchema()</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">df.rdd.printSchema()</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">df.print_schema()</strong></span></li>
			</ol>
			<p><span class="No-Break"><strong class="bold">Question 60</strong></span><span class="No-Break">:</span></p>
			<p>Which of the following code blocks creates a new DataFrame with three columns: <strong class="source-inline">department</strong>, <strong class="source-inline">age</strong>, and <strong class="source-inline">max_salary</strong> and has the maximum salary for each employee from each department and each age group from the <span class="No-Break"><strong class="source-inline">df</strong></span><span class="No-Break"> DataFrame?</span></p>
			<ol class="margin-left">
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">df.max(salary)</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">df.groupBy(["department", "age"]).agg(max("salary").alias("max_salary"))</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">df.agg(max(salary).alias(max_salary')</strong></span></li>
				<li class="Alphabets"><span class="No-Break"><strong class="source-inline">df.groupby(department).agg(max(salary).alias(max_salary)</strong></span></li>
			</ol>
			<h2 id="_idParaDest-244"><a id="_idTextAnchor245"/>Answers</h2>
			<ol>
				<li>B</li>
				<li>A</li>
				<li>D</li>
				<li>D</li>
				<li>A</li>
				<li>D</li>
				<li>A</li>
				<li>D</li>
				<li>C</li>
				<li>B</li>
				<li>C</li>
				<li>A</li>
				<li>A</li>
				<li>A</li>
				<li>A</li>
				<li>C</li>
				<li>B</li>
				<li>B</li>
				<li>D</li>
				<li>D</li>
				<li>D</li>
				<li>C</li>
				<li>C</li>
				<li>D</li>
				<li>D</li>
				<li>C</li>
				<li>A</li>
				<li>C</li>
				<li>E</li>
				<li>C</li>
				<li>C</li>
				<li>D</li>
				<li>A</li>
				<li>B</li>
				<li>D</li>
				<li>B</li>
				<li>B</li>
				<li>C</li>
				<li>A</li>
				<li>B</li>
				<li>B</li>
				<li>A</li>
				<li>A</li>
				<li>A</li>
				<li>C</li>
				<li>D</li>
				<li>D</li>
				<li>C</li>
				<li>B</li>
				<li>A</li>
				<li>C</li>
				<li>C</li>
				<li>B</li>
				<li>A</li>
				<li>A</li>
				<li>D</li>
				<li>C</li>
				<li>C</li>
				<li>B</li>
				<li>B</li>
			</ol>
		</div>
	</body></html>