<html><head></head><body>
		<div><h1 id="_idParaDest-241" class="chapter-number"><a id="_idTextAnchor242"/>9</h1>
			<h1 id="_idParaDest-242"><a id="_idTextAnchor243"/>Mock Test 1</h1>
			<h1 id="_idParaDest-243"><a id="_idTextAnchor244"/>Questions</h1>
			<p>Try your hand at these practice questions to test your knowledge of Apache Spark:</p>
			<p><strong class="bold">Question 1:</strong></p>
			<p>Which statement does not accurately describe a feature of the Spark driver?</p>
			<ol class="margin-left">
				<li class="Alphabets">The Spark driver serves as the node where the main method of a Spark application runs to co-ordinate the application</li>
				<li class="Alphabets">The Spark driver can be horizontally scaled to enhance overall processing throughput</li>
				<li class="Alphabets">The Spark driver houses the SparkContext object</li>
				<li class="Alphabets">The Spark driver is tasked with scheduling the execution of data by using different worker nodes in cluster mode</li>
				<li class="Alphabets">Optimal performance dictates that the Spark driver should be positioned as close as possible to worker nodes</li>
			</ol>
			<p><strong class="bold">Question 2</strong>:</p>
			<p>Which of these statements accurately describes stages?</p>
			<ol class="margin-left">
				<li class="Alphabets">Tasks within a stage can be simultaneously executed by multiple machines</li>
				<li class="Alphabets">Various stages within a job can run concurrently</li>
				<li class="Alphabets">Stages comprise one or more jobs</li>
				<li class="Alphabets">Stages temporarily store transactions before committing them through actions</li>
			</ol>
			<p><strong class="bold">Question 3:</strong></p>
			<p>Which of these statements accurately describes Spark’s cluster execution mode?</p>
			<ol class="margin-left">
				<li class="Alphabets">Cluster mode runs executor processes on gateway nodes</li>
				<li class="Alphabets">Cluster mode involves the driver being hosted on a gateway machine</li>
				<li class="Alphabets">In cluster mode, the Spark driver and the cluster manager are not co-located</li>
				<li class="Alphabets">The driver in cluster mode is located on a worker node</li>
			</ol>
			<p><strong class="bold">Question 4:</strong></p>
			<p>Which of these statements accurately describes Spark’s client execution mode?</p>
			<ol class="margin-left">
				<li class="Alphabets">Client mode runs executor processes on gateway nodes</li>
				<li class="Alphabets">In client mode, the driver is co-located with the executor</li>
				<li class="Alphabets">In client mode, the Spark driver and the cluster manager are co-located</li>
				<li class="Alphabets">In client mode, the driver is found on an edge node</li>
			</ol>
			<p><strong class="bold">Question 5:</strong></p>
			<p>Which statement accurately describes Spark’s standalone deployment mode?</p>
			<ol class="margin-left">
				<li class="Alphabets">Standalone mode utilizes only one executor per worker for each application</li>
				<li class="Alphabets">In standalone mode, the driver is located on a worker node</li>
				<li class="Alphabets">In standalone mode, the cluster does not need the driver</li>
				<li class="Alphabets">In standalone mode, the driver is found on an edge node</li>
			</ol>
			<p><strong class="bold">Question 6</strong>:</p>
			<p>What is a task in Spark?</p>
			<ol class="margin-left">
				<li class="Alphabets">The unit of work performed for each data partition within a task is slots</li>
				<li class="Alphabets">Tasks are the second-smallest entity that can be executed within Spark</li>
				<li class="Alphabets">Tasks featuring wide dependencies can be combined into a single task</li>
				<li class="Alphabets">A task is a single unit of work done by a partition within Spark</li>
			</ol>
			<p><strong class="bold">Question 7</strong>:</p>
			<p>Which of the following is the highest level in Spark’s execution hierarchy?</p>
			<ol class="margin-left">
				<li class="Alphabets">Job</li>
				<li class="Alphabets">Task</li>
				<li class="Alphabets">Executor</li>
				<li class="Alphabets">Stage</li>
			</ol>
			<p><strong class="bold">Question 8</strong>:</p>
			<p>How can the concept of slots be accurately described in Spark’s context?</p>
			<ol class="margin-left">
				<li class="Alphabets">The creation and termination of slots align with the workload of an executor</li>
				<li class="Alphabets">Spark strategically stores data on disk across various slots to enhance I/O performance</li>
				<li class="Alphabets">Each slot is consistently confined to a solitary core</li>
				<li class="Alphabets">Slots enable the tasks to run in parallel</li>
			</ol>
			<p><strong class="bold">Question 9</strong>:</p>
			<p>What is the role of an executor in Spark?</p>
			<ol class="margin-left">
				<li class="Alphabets">The executor’s role is to request the transformation of operations into DAG</li>
				<li class="Alphabets">There can only be one executor within a Spark environment</li>
				<li class="Alphabets">The executor processes partitions in an optimized and distributed manner</li>
				<li class="Alphabets">The executor schedules queries for execution</li>
			</ol>
			<p><strong class="bold">Question 10</strong>:</p>
			<p>What is the role of shuffle in Spark?</p>
			<ol class="margin-left">
				<li class="Alphabets">Shuffle broadcasts variables to different partitions</li>
				<li class="Alphabets">With shuffle, data is written to the disk</li>
				<li class="Alphabets">The shuffle command transforms data in Spark</li>
				<li class="Alphabets">Shuffles are a narrow transformation</li>
			</ol>
			<p><strong class="bold">Question 11</strong>:</p>
			<p>What is the role of actions in Spark?</p>
			<ol class="margin-left">
				<li class="Alphabets">Actions only read data from a disk</li>
				<li class="Alphabets">Actions are used to modify existing RDDs</li>
				<li class="Alphabets">Actions trigger the execution of tasks</li>
				<li class="Alphabets">Actions are used to establish stage boundaries</li>
			</ol>
			<p><strong class="bold">Question 12</strong>:</p>
			<p>Which of the following is one of the tasks of the cluster manager in Spark?</p>
			<ol class="margin-left">
				<li class="Alphabets">In the event of an executor failure, the cluster manager will collaborate with the driver to initiate a new executor</li>
				<li class="Alphabets">The cluster manager can coalesce partitions to increase the speed of complex data processing</li>
				<li class="Alphabets">The cluster manager collects runtime statistics of queries</li>
				<li class="Alphabets">The cluster manager creates query plans</li>
			</ol>
			<p><strong class="bold">Question 13</strong>:</p>
			<p>Which of the following is one of the tasks of adaptive query execution in Spark?</p>
			<ol class="margin-left">
				<li class="Alphabets">Adaptive query execution can coalesce partitions to increase the speed of complex data processing</li>
				<li class="Alphabets">In the event of an executor failure, the adaptive query execution feature will collaborate with the driver to initiate a new executor</li>
				<li class="Alphabets">Adaptive query execution creates query plans</li>
				<li class="Alphabets">Adaptive query execution is responsible for spawning multiple executors to carry our tasks in Spark</li>
			</ol>
			<p><strong class="bold">Question 14</strong>:</p>
			<p>Which of the following operations is considered a transformation?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>df.select()</code></li>
				<li class="Alphabets"><code>df.show()</code></li>
				<li class="Alphabets"><code>df.head()</code></li>
				<li class="Alphabets"><code>df.count()</code></li>
			</ol>
			<p><strong class="bold">Question 15</strong>:</p>
			<p>What is a feature of lazy evaluation in Spark?</p>
			<ol class="margin-left">
				<li class="Alphabets">Spark will fail a job only during execution but not during definition</li>
				<li class="Alphabets">Spark will fail a job only during definition</li>
				<li class="Alphabets">Spark will execute upon receiving a transformation operation</li>
				<li class="Alphabets">Spark will fail upon receiving an action</li>
			</ol>
			<p><strong class="bold">Question 16</strong>:</p>
			<p>Which of the following statements about Spark’s execution hierarchy is correct?</p>
			<ol class="margin-left">
				<li class="Alphabets">In Spark’s execution hierarchy, tasks are above the level of jobs</li>
				<li class="Alphabets">In Spark’s execution hierarchy, multiple jobs are contained in a stage</li>
				<li class="Alphabets">In Spark’s execution hierarchy, a job can potentially span multiple stage boundaries</li>
				<li class="Alphabets">In Spark’s execution hierarchy, slots are the smallest unit</li>
			</ol>
			<p><strong class="bold">Question 17</strong>:</p>
			<p>Which of the following is the characteristic of the Spark driver?</p>
			<ol class="margin-left">
				<li class="Alphabets">The worker nodes are responsible for transforming Spark operations into DAGs when the driver sends a command</li>
				<li class="Alphabets">The Spark driver is responsible for executing tasks and returning results to executors</li>
				<li class="Alphabets">Spark driver can be scaled by adding more machines so that the performance of Spark tasks can be improved</li>
				<li class="Alphabets">The Spark driver processes partitions in an optimized and distributed fashion</li>
			</ol>
			<p><strong class="bold">Question 18</strong>:</p>
			<p>Which of the following statements about broadcast variables is accurate?</p>
			<ol class="margin-left">
				<li class="Alphabets">Broadcast variables are only present on driver nodes</li>
				<li class="Alphabets">Broadcast variables can only be used for tables that fit into memory</li>
				<li class="Alphabets">Broadcast variables are not immutable, meaning they can be shared across clusters</li>
				<li class="Alphabets">Broadcast variables are not shared across the worker nodes</li>
			</ol>
			<p><strong class="bold">Question 19</strong>:</p>
			<p>Which of the following code blocks returns unique values in columns <code>employee_state</code> and <code>employee_salary</code> in DataFrame <code>df</code> for all columns?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>Df.select('employee_state').join(df.select('employee_salary'), </code><code>col('employee_state')==col('employee_salary'), 'left').show()</code></li>
				<li class="Alphabets"><code>df.select(col('employee_state'), </code><code>col('employee_salary')).agg({'*': 'count'}).show()</code></li>
				<li class="Alphabets"><code>df.select('employee_state', 'employee_salary').distinct().show()</code></li>
				<li class="Alphabets"><code>df.select('employee_state').union(df.select('employee_salary')).distinct().show()</code></li>
			</ol>
			<p><strong class="bold">Question 20</strong>:</p>
			<p>Which of the following code blocks reads a Parquet file from the <code>my_fle_path</code> location, where the file name is <code>my_file.parquet</code>, into a DataFrame <code>df</code>?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>df = </code><code>spark.mode("parquet").read("my_fle_path/my_file.parquet")</code></li>
				<li class="Alphabets"><code>df = </code><code>spark.read.path("my_fle_path/my_file.parquet")</code></li>
				<li class="Alphabets"><code>df = </code><code>spark.read().parquet("my_fle_path/my_file.parquet")</code></li>
				<li class="Alphabets"><code>df = </code><code>spark.read.parquet("/my_fle_path/my_file.parquet")</code></li>
			</ol>
			<p><strong class="bold">Question 21</strong>:</p>
			<p>Which of the following code blocks performs an inner join of the <code>salarydf</code> and <code>employeedf</code> DataFrames for columns <code>employeeSalaryID</code> and <code>employeeID</code>, respectively?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>salarydf.join(employeedf, salarydf.employeeID == </code><code>employeedf.employeeSalaryID)</code></li>
				<li class="Alphabets"><ol><li class="lower-roman"><code>Salarydf.createOrReplaceTempView(salarydf)</code></li><li class="lower-roman"><code>employeedf.createOrReplaceTempView('employeedf')</code></li><li class="lower-roman"><code>spark.sql("SELECT * FROM salarydf CROSS JOIN employeedf ON </code><code>employeeSalaryID ==employeeID")</code></li></ol></li>
				<li class="Alphabets"><ol><li class="lower-roman"><code>salarydf</code></li><li class="lower-roman"><code>.</code><code>join(employeedf, col(employeeID)==col(employeeSalaryID))</code></li></ol></li>
				<li class="Alphabets"><ol><li class="lower-roman"><code>Salarydf.createOrReplaceTempView(salarydf)</code></li><li class="lower-roman"><code>employeedf.createOrReplaceTempView('employeedf')</code></li><li class="lower-roman"><code>SELECT * </code><code>FROM salarydf</code></li><li class="lower-roman"><code>INNER </code><code>JOIN employeedf</code></li><li class="lower-roman"><code>ON salarydf.employeeSalaryID == </code><code>employeedf. employeeID</code></li></ol></li>
			</ol>
			<p><strong class="bold">Question 22</strong>:</p>
			<p>Which of the following code blocks returns the <code>df</code> DataFrame sorted in descending order by column salary, showing missing values in the end?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>df.sort(nulls_last("salary"))</code></li>
				<li class="Alphabets"><code>df.orderBy("salary").nulls_last()</code></li>
				<li class="Alphabets"><code>df.sort("salary", ascending=False)</code></li>
				<li class="Alphabets"><code>df.nulls_last("salary")</code></li>
			</ol>
			<p><strong class="bold">Question 23</strong>:</p>
			<p>The following code block contains an error. The code block should return a copy of the <code>df</code> DataFrame, where the name of the column state is changed to <code>stateID</code>. Find the error.</p>
			<p>Code block:</p>
			<pre class="source-code">
df.withColumn("stateID", "state")</pre>			<ol class="margin-left">
				<li class="Alphabets">The arguments to the method <code>"stateID"</code> and <code>"state"</code> should be swapped</li>
				<li class="Alphabets">The <code>withColumn</code> method should be replaced by the <code>withColumnRenamed</code> method</li>
				<li class="Alphabets">The <code>withColumn</code> method should be replaced by <code>withColumnRenamed</code> method, and the arguments to the method need to be reordered</li>
				<li class="Alphabets">There is no such method whereby the column name can be changed</li>
			</ol>
			<p><strong class="bold">Question 24</strong>:</p>
			<p>Which of the following code blocks performs an inner join between the <code>salarydf</code> and <code>employeedf</code> DataFrames, using the <code>employeeID</code> and <code>salaryEmployeeID</code> columns as join keys, respectively?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>salarydf.join(employeedf, "inner", salarydf.employeedf == </code><code>employeeID.salaryEmployeeID)</code></li>
				<li class="Alphabets"><code>salarydf.join(employeedf, employeeID == </code><code>salaryEmployeeID)</code></li>
				<li class="Alphabets"><code>salarydf.join(employeedf, salarydf.salaryEmployeeID == </code><code>employeedf.employeeID, "inner")</code></li>
				<li class="Alphabets"><code>salarydf.join(employeedf, salarydf.employeeID == </code><code>employeedf.salaryEmployeeID, "inner")</code></li>
			</ol>
			<p><strong class="bold">Question 25</strong>:</p>
			<p>The following code block should return a <code>df</code> DataFrame, where the <code>employeeID</code> column is converted into an integer. Choose the answer that correctly fills the blanks in the code block to accomplish this:</p>
			<pre class="source-code">
df.__1__(__2__.__3__(__4__))</pre>			<ol class="margin-left">
				<li class="Alphabets"><ol><li class="lower-roman"><code>select</code></li><li class="lower-roman"><code>col("employeeID")</code></li><li class="lower-roman"><code>as</code></li><li class="lower-roman"><code>IntegerType</code></li></ol></li>
				<li class="Alphabets"><ol><li class="lower-roman"><code>select</code></li><li class="lower-roman"><code>col("employeeID")</code></li><li class="lower-roman"><code>as</code></li><li class="lower-roman"><code>Integer</code></li></ol></li>
				<li class="Alphabets"><ol><li class="lower-roman"><code>cast</code></li><li class="lower-roman"><code>"</code><code>employeeID"</code></li><li class="lower-roman"><code>as</code></li><li class="lower-roman"><code>IntegerType()</code></li></ol></li>
				<li class="Alphabets"><ol><li class="lower-roman"><code>select</code></li><li class="lower-roman"><code>col("employeeID")</code></li><li class="lower-roman"><code>cast</code></li><li class="lower-roman"><code>IntegerType()</code></li></ol></li>
			</ol>
			<p><strong class="bold">Question 26</strong>:</p>
			<p>Find the number of records that are not empty in the column department of the resulting DataFrame when we join the <code>employeedf</code> and <code>salarydf</code> DataFrames for the <code>employeeID</code> and <code>employeeSalaryID</code> columns, respectively. Which code blocks (in order) should be executed to achieve this?</p>
			<p>1. <code>.filter(col("department").isNotNull())</code></p>
			<p>2. <code>.count()</code></p>
			<p>3. <code>employeedf.join(salarydf, employeedf.employeeID == </code><code>salarydf.employeeSalaryID)</code></p>
			<p>4. <code>employeedf.join(salarydf, employeedf.employeeID ==salarydf. </code><code>employeeSalaryID, how='inner')</code></p>
			<p>5. <code>.filter(col(department).isnotnull())</code></p>
			<p>6. <code>.sum(col(department))</code></p>
			<ol class="margin-left">
				<li class="Alphabets">3, 1, 6</li>
				<li class="Alphabets">3, 1, 2</li>
				<li class="Alphabets">4, 1, 2</li>
				<li class="Alphabets">3, 5, 2</li>
			</ol>
			<p><strong class="bold">Question 27</strong>:</p>
			<p>Which of the following code blocks returns only those rows from the <code>df</code> DataFrame in which the values in the column state are unique?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>df.dropDuplicates(subset=["state"]).show()</code></li>
				<li class="Alphabets"><code>df.distinct(subset=["state"]).show()</code></li>
				<li class="Alphabets"><code>df.drop_duplicates(subset=["state"]).show()</code></li>
				<li class="Alphabets"><code>df.unique("state").show()</code></li>
			</ol>
			<p><strong class="bold">Question 28</strong>:</p>
			<p>The following code block contains an error. The code block should return a copy of the <code>df</code> DataFrame with an additional column named <code>squared_number</code>, which has the square of the column number. Find the error.</p>
			<p>Code block:</p>
			<pre class="source-code">
df.withColumnRenamed(col("number"), pow(col("number"), 0.2).alias("squared_number"))</pre>			<ol class="margin-left">
				<li class="Alphabets">The arguments to the <code>withColumnRenamed</code> method need to be reordered</li>
				<li class="Alphabets">The <code>withColumnRenamed</code> method should be replaced by the <code>withColumn</code> method</li>
				<li class="Alphabets">The <code>withColumnRenamed</code> method should be replaced by the <code>select</code> method, and <code>0.2</code> should be replaced with <code>2</code></li>
				<li class="Alphabets">The argument <code>0.2</code> should be replaced by <code>2</code></li>
			</ol>
			<p><strong class="bold">Question 29</strong>:</p>
			<p>Which of the following code blocks returns a new DataFrame in which column salary is renamed to <code>new_salary</code> and employee is renamed to <code>new_employee</code> in the <code>df</code> DataFrame?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>df.withColumnRenamed(salary, </code><code>new_salary).withColumnRenamed(employee, new_employee)</code></li>
				<li class="Alphabets"><code>df.withColumnRenamed("salary", "new_salary")</code></li>
				<li class="Alphabets"><code>df.withColumnRenamed("employee", "new_employee")</code></li>
				<li class="Alphabets"><code>df.withColumn("salary", "</code><code>new_salary").withColumn("employee", "new_employee")</code></li>
				<li class="Alphabets"><code>df.withColumnRenamed("salary", "</code><code>new_salary").withColumnRenamed("employee", "new_employee")</code></li>
			</ol>
			<p><strong class="bold">Question 30</strong>:</p>
			<p>Which of the following code blocks returns a copy of the <code>df</code> DataFrame, where the column salary has been renamed to <code>employeeSalary</code>?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>df.withColumn(["salary", "employeeSalary"])</code></li>
				<li class="Alphabets"><code>df.withColumnRenamed("salary").alias("employeeSalary ")</code></li>
				<li class="Alphabets"><code>df.withColumnRenamed("salary", "</code><code>employeeSalary ")</code></li>
				<li class="Alphabets"><code>df.withColumn("salary", "</code><code>employeeSalary ")</code></li>
			</ol>
			<p><strong class="bold">Question 31</strong>:</p>
			<p>The following code block contains an error. The code block should save the <code>df</code> DataFrame to the <code>my_file_path</code> path as a Parquet file, appending to any existing parquet file. Find the error.</p>
			<pre class="source-code">
df.format("parquet").option("mode", "append").save(my_file_path)</pre>			<ol class="margin-left">
				<li class="Alphabets">The code is not saved to the correct path</li>
				<li class="Alphabets">The <code>save()</code> and <code>format</code> functions should be swapped</li>
				<li class="Alphabets">The code block is missing a reference to the <code>DataFrameWriter</code></li>
				<li class="Alphabets">The <code>option</code> mode should be overwritten to correctly write the file</li>
			</ol>
			<p><strong class="bold">Question 32</strong>:</p>
			<p>How can we reduce the <code>df</code> DataFrame from 12 to 6 partitions?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>df.repartition(12)</code></li>
				<li class="Alphabets"><code>df.coalesce(6).shuffle()</code></li>
				<li class="Alphabets"><code>df.coalesce(6, shuffle=True)</code></li>
				<li class="Alphabets"><code>df.repartition(6)</code></li>
			</ol>
			<p><strong class="bold">Question 33</strong>:</p>
			<p>Which of the following code blocks returns a DataFrame where the timestamp column is converted into unix epoch timestamps in a new column named <code>record_timestamp</code> with a format of day, month, and year?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>df.withColumn("record_timestamp", </code><code>from_unixtime(unix_timestamp(col("timestamp")), "dd-MM-yyyy"))</code></li>
				<li class="Alphabets"><code>df.withColumnRenamed("record_timestamp", </code><code>from_unixtime(unix_timestamp(col("timestamp")), "dd-MM-yyyy"))</code></li>
				<li class="Alphabets"><code>df.select ("record_timestamp", </code><code>from_unixtime(unix_timestamp(col("timestamp")), "dd-MM-yyyy"))</code></li>
				<li class="Alphabets"><code>df.withColumn("record_timestamp", </code><code>from_unixtime(unix_timestamp(col("timestamp")), "MM-dd-yyyy"))</code></li>
			</ol>
			<p><strong class="bold">Question 34</strong>:</p>
			<p>Which of the following code blocks creates a new DataFrame by appending the rows of the DataFrame <code>salaryDf</code> to the rows of the DataFrame <code>employeeDf</code>, regardless of the fact that both DataFrames have different column names?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>salaryDf.join(employeeDf)</code></li>
				<li class="Alphabets"><code>salaryDf.union(employeeDf)</code></li>
				<li class="Alphabets"><code>salaryDf.concat(employeeDf)</code></li>
				<li class="Alphabets"><code>salaryDf.unionAll(employeeDf)</code></li>
			</ol>
			<p><strong class="bold">Question 35</strong>:</p>
			<p>The following code block contains an error. The code block should calculate the total of all salaries in the <code>employee_salary</code> column across each department. Find the error.</p>
			<pre class="source-code">
df.agg("department").sum("employee_salary")</pre>			<ol class="margin-left">
				<li class="Alphabets">Instead of <code>avg("value")</code>, <code>avg(col("value"))</code> should be used</li>
				<li class="Alphabets">All column names should be wrapped in <code>col()</code> operators</li>
				<li class="Alphabets"><code>"storeId"</code> and “<code>value"</code> should be swapped</li>
				<li class="Alphabets"><code>Agg</code> should be replaced by <code>groupBy</code></li>
			</ol>
			<p><strong class="bold">Question 36</strong>:</p>
			<p>The following code block contains an error. The code block is intended to perform a cross-join of the <code>salarydf</code> and <code>employeedf</code> DataFrames for the <code>employeeSalaryID</code> and <code>employeeID</code> columns, respectively. Find the error.</p>
			<pre class="source-code">
employeedf.join(salarydf, [salarydf.employeeSalaryID, employeedf.employeeID], "cross")</pre>			<ol class="margin-left">
				<li class="Alphabets">The join type <code>"cross"</code> in the argument needs to be replaced with <code>crossJoin</code></li>
				<li class="Alphabets">[<code>salarydf.employeeSalaryID, employeedf.employeeID</code>] should be replaced by <code>salarydf.employeeSalaryID == </code><code>employeedf.employeeID</code></li>
				<li class="Alphabets">The <code>"cross"</code> argument should be eliminated since <code>"cross"</code> is the default join type</li>
				<li class="Alphabets">The <code>"cross"</code> argument should be eliminated from the call and <code>join</code> should be replaced by <code>crossJoin</code></li>
			</ol>
			<p><strong class="bold">Question 37</strong>:</p>
			<p>The following code block contains an error. The code block should display the schema of the <code>df </code>DataFrame. Find the error.</p>
			<pre class="source-code">
df.rdd.printSchema()</pre>			<ol class="margin-left">
				<li class="Alphabets">In Spark, we cannot print the schema of a DataFrame</li>
				<li class="Alphabets"><code>printSchema</code> is not callable through <code>df.rdd</code> and should be called directly from <code>df</code></li>
				<li class="Alphabets">There is no method in Spark named <code>printSchema()</code></li>
				<li class="Alphabets">The <code>print_schema()</code> method should be used instead of <code>printSchema()</code></li>
			</ol>
			<p><strong class="bold">Question 38</strong>:</p>
			<p>The following code block should write the <code>df</code> DataFrame as a Parquet file to the <code>filePath</code> path, replacing any existing file. Choose the answer that correctly fills the blanks in the code block to accomplish this:</p>
			<pre class="source-code">
df.__1__.format("parquet").__2__(__3__).__4__(filePath)</pre>			<ol class="margin-left">
				<li class="Alphabets"><ol><li class="lower-roman"><code>save</code></li><li class="lower-roman"><code>mode</code></li><li class="lower-roman"><code>"</code><code>ignore"</code></li><li class="lower-roman"><code>path</code></li></ol></li>
				<li class="Alphabets"><ol><li class="lower-roman"><code>store</code></li><li class="lower-roman"><code>with</code></li><li class="lower-roman"><code>"</code><code>replace"</code></li><li class="lower-roman"><code>path</code></li></ol></li>
				<li class="Alphabets"><ol><li class="lower-roman"><code>write</code></li><li class="lower-roman"><code>mode</code></li><li class="lower-roman"><code>"</code><code>overwrite"</code></li><li class="lower-roman"><code>save</code></li></ol></li>
				<li class="Alphabets"><ol><li class="lower-roman"><code>save</code></li><li class="lower-roman"><code>mode</code></li><li class="lower-roman"><code>"</code><code>overwrite"</code></li><li class="lower-roman"><code>path</code></li></ol></li>
			</ol>
			<p><strong class="bold">Question 39</strong>:</p>
			<p>The following code block contains an error. The code block is supposed to sort the <code>df</code> DataFrame according to salary in descending order. Then, it should sort based on the bonus column, putting nulls to last. Find the error.</p>
			<pre class="source-code">
df.orderBy ('salary', asc_nulls_first(col('bonus')))
transactionsDf.orderBy('value', asc_nulls_first(col('predError')))</pre>			<ol class="margin-left">
				<li class="Alphabets">The <code>salary</code> column should be sorted in a descending way. Moreover, it should be wrapped in a <code>col()</code> operator</li>
				<li class="Alphabets">The <code>salary</code> column should be wrapped by the <code>col()</code> operator</li>
				<li class="Alphabets">The <code>bonus</code> column should be sorted in a descending way, putting <code>nulls</code> last</li>
				<li class="Alphabets">The <code>bonus</code> column should be sorted by <code>desc_nulls_first()</code> instead</li>
			</ol>
			<p><strong class="bold">Question 40</strong>:</p>
			<p>The following code block contains an error. The code block should use the <code>square_root_method</code> Python method to find the square root of the <code>salary</code> column in the <code>df</code> DataFrame and return it in a new column called <code>sqrt_salary</code>. Find the error.</p>
			<pre class="source-code">
square_root_method_udf = udf(square_root_method)
df.withColumn("sqrt_salary", square_root_method("salary"))</pre>			<ol class="margin-left">
				<li class="Alphabets">There is no return type specified for <code>square_root_method</code></li>
				<li class="Alphabets">In the second line of the code, Spark needs to call <code>squre_root_method_udf</code> instead of <code>square_root_method</code></li>
				<li class="Alphabets"><code>udf</code> is not registered with Spark</li>
				<li class="Alphabets">A new column needs to be added</li>
			</ol>
			<p><strong class="bold">Question 41</strong>:</p>
			<p>The following code block contains an error. The code block should return the <code>df</code> DataFrame with <code>employeeID</code> renamed to <code>employeeIdColumn</code>. Find the error.</p>
			<pre class="source-code">
df.withColumn("employeeIdColumn", "employeeID")</pre>			<ol class="margin-left">
				<li class="Alphabets">Instead of <code>withColumn</code>, the <code>withColumnRenamed</code> method should be used</li>
				<li class="Alphabets">Instead of <code>withColumn</code>, the <code>withColumnRenamed</code> method should be used and argument <code>"employeeIdColumn"</code> should be swapped with argument <code>"employeeID"</code></li>
				<li class="Alphabets">Arguments <code>"employeeIdColumn"</code> and <code>"employeeID"</code> should be swapped</li>
				<li class="Alphabets">The <code>withColumn</code> operator should be replaced with the <code>withColumnRenamed</code> operator</li>
			</ol>
			<p><strong class="bold">Question 42</strong>:</p>
			<p>Which of the following code blocks will return a new DataFrame with the same columns as DataFrame <code>df</code>, except for the <code>salary</code> column?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>df.drop("salary")</code></li>
				<li class="Alphabets"><code>df.drop(col(salary))</code></li>
				<li class="Alphabets"><code>df.drop(salary)</code></li>
				<li class="Alphabets"><code>df.delete("salary")</code></li>
			</ol>
			<p><strong class="bold">Question 43</strong>:</p>
			<p>Which of the following code blocks returns a DataFrame showing the mean of the salary column from the <code>df</code> DataFrame, grouped by column department?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>df.groupBy("department").agg(avg("salary"))</code></li>
				<li class="Alphabets"><code>df.groupBy(col(department).avg())</code></li>
				<li class="Alphabets"><code>df.groupBy("department").avg(col("salary"))</code></li>
				<li class="Alphabets"><code>df.groupBy("department").agg(average("salary"))</code></li>
			</ol>
			<p><strong class="bold">Question 44</strong>:</p>
			<p>Which of the following code blocks creates a DataFrame that shows the mean of the salary column of the <code>salaryDf</code> DataFrame, based on the department and state columns, where age is greater than 35?</p>
			<ol>
				<li><code>salaryDf.filter(col("age") &gt; </code><code>35)</code></li>
				<li><code>.</code><code>filter(col("employeeID")</code></li>
				<li><code>.</code><code>filter(col("employeeID").isNotNull())</code></li>
				<li><code>.</code><code>groupBy("department")</code></li>
				<li><code>.</code><code>groupBy("department", "state")</code></li>
				<li><code>.</code><code>agg(avg("salary").alias("mean_salary"))</code></li>
				<li><code>.</code><code>agg(average("salary").alias("mean_salary"))</code><ol><li class="Alphabets">1,2,5,6</li><li class="Alphabets">1,3,5,6</li><li class="Alphabets">1,3,6,7</li><li class="Alphabets">1,2,4,6</li></ol></li>
			</ol>
			<p><strong class="bold">Question 45</strong>:</p>
			<p>The following code block contains an error. The code block needs to cache the <code>df</code> DataFrame so that this DataFrame is fault-tolerant. Find the error.</p>
			<pre class="source-code">
df.persist(StorageLevel.MEMORY_AND_DISK_3)</pre>			<ol class="margin-left">
				<li class="Alphabets"><code>persist()</code> is not a function of the API DataFrame</li>
				<li class="Alphabets"><code>df.write()</code> should be used in conjunction with <code>df.persist</code> to correctly write the DataFrame</li>
				<li class="Alphabets">The storage level is incorrect and should be <code>MEMORY_AND_DISK_2</code></li>
				<li class="Alphabets"><code>df.cache()</code> should be used instead of <code>df.persist()</code></li>
			</ol>
			<p><strong class="bold">Question 46</strong>:</p>
			<p>Which of the following code blocks concatenates the rows of the <code>salaryDf</code> and <code>employeeDf </code>DataFrames without any duplicates (assuming the columns of both DataFrames are similar)?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>salaryDf.concat(employeeDf).unique()</code></li>
				<li class="Alphabets"><code>spark.union(salaryDf, employeeDf).distinct()</code></li>
				<li class="Alphabets"><code>salaryDf.union(employeeDf).unique()</code></li>
				<li class="Alphabets"><code>salaryDf.union(employeeDf).distinct()</code></li>
			</ol>
			<p><strong class="bold">Question 47</strong>:</p>
			<p>Which of the following code blocks reads a complete folder of CSV files from <code>filePath</code> with column headers?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>spark.option("header",True).csv(filePath)</code></li>
				<li class="Alphabets"><code>spark.read.load(filePath)</code></li>
				<li class="Alphabets"><code>spark.read().option("header",True).load(filePath)</code></li>
				<li class="Alphabets"><code>spark.read.format("csv").option("header",True).load(filePath)</code></li>
			</ol>
			<p><strong class="bold">Question 48</strong>:</p>
			<p>The following code block contains an error. The <code>df</code> DataFrame contains columns [<code>employeeID</code>,<code> salary</code>, and <code>department</code>]. The code block should return a DataFrame that contains only the <code>employeeID</code> and <code>salary</code> columns from DataFrame <code>df</code>. Find the error.</p>
			<pre class="source-code">
df.select(col(department))</pre>			<ol class="margin-left">
				<li class="Alphabets">All column names from the <code>df</code> DataFrame should be specified in the <code>select</code> arguments</li>
				<li class="Alphabets">The <code>select</code> operator should be replaced by a <code>drop</code> operator, and all the column names from the <code>df</code> DataFrame should be listed as a list</li>
				<li class="Alphabets">The <code>select</code> operator should be replaced by a <code>drop</code> operator</li>
				<li class="Alphabets">The column name <code>department</code> should be listed like <code>col("department")</code></li>
			</ol>
			<p><strong class="bold">Question 49</strong>:</p>
			<p>The following code block contains an error. The code block should write DataFrame <code>df</code> as a Parquet file to the <code>filePath</code> location, after partitioning it for the <code>department</code> column. Find the error.</p>
			<pre class="source-code">
df.write.partition("department").parquet()</pre>			<ol class="margin-left">
				<li class="Alphabets"><code>partitionBy()</code> method should be used instead of <code>partition()</code>.</li>
				<li class="Alphabets"><code>partitionBy()</code> method should be used instead of <code>partition()</code> and <code>filePath</code> should be added to the <code>parquet</code> method</li>
				<li class="Alphabets">The <code>partition()</code> method should be called before the write method and <code>filePath</code> should be added to <code>parquet</code> method</li>
				<li class="Alphabets">The <code>"department"</code> column should be wrapped in a <code>col()</code> operator</li>
			</ol>
			<p><strong class="bold">Question 50</strong>:</p>
			<p>Which of the following code blocks removes the cached <code>df</code> DataFrame from memory and disk?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>df.unpersist()</code></li>
				<li class="Alphabets"><code>drop df</code></li>
				<li class="Alphabets"><code>df.clearCache()</code></li>
				<li class="Alphabets"><code>df.persist()</code></li>
			</ol>
			<p><strong class="bold">Question 51</strong>:</p>
			<p>The following code block should return a copy of the <code>df</code> DataFrame with an additional column: <code>test_column</code>, which has a value of <code>19</code>. Choose the answer that correctly fills the blanks in the code block to accomplish this:</p>
			<pre class="source-code">
df.__1__(__2__, __3__)</pre>			<ol class="margin-left">
				<li class="Alphabets"><ol><li class="lower-roman"><code>withColumn</code></li><li class="lower-roman"><code>'</code><code>test_column'</code></li><li class="lower-roman"><code>19</code></li></ol></li>
				<li class="Alphabets"><ol><li class="lower-roman"><code>withColumnRenamed</code></li><li class="lower-roman"><code>test_column</code></li><li class="lower-roman"><code>lit(19)</code></li></ol></li>
				<li class="Alphabets"><ol><li class="lower-roman"><code>withColumn</code></li><li class="lower-roman"><code>'</code><code>test_column'</code></li><li class="lower-roman"><code>lit(19)</code></li></ol></li>
				<li class="Alphabets"><ol><li class="lower-roman"><code>withColumnRenamed</code></li><li class="lower-roman"><code>test_column</code></li><li class="lower-roman"><code>19</code></li></ol></li>
			</ol>
			<p><strong class="bold">Question 52</strong>:</p>
			<p>The following code block should return a DataFrame with the columns <code>employeeId</code>, <code>salary</code>, <code>bonus</code>, and <code>department</code> from <code>transactionsDf </code>DataFrame. Choose the answer that correctly fills the blanks to accomplish this:</p>
			<pre class="source-code">
df.__1__(__2__)</pre>			<ol class="margin-left">
				<li class="Alphabets"><ol><li class="lower-roman"><code>drop</code></li><li class="lower-roman"><code>"employeeId", "salary", "</code><code>bonus", "department"</code></li></ol></li>
				<li class="Alphabets"><ol><li class="lower-roman"><code>filter</code></li><li class="lower-roman"><code>"employeeId, salary, </code><code>bonus, department"</code></li></ol></li>
				<li class="Alphabets"><ol><li class="lower-roman"><code>select</code></li><li class="lower-roman"><code>["employeeId", "salary", "</code><code>bonus", "department"]</code></li></ol></li>
				<li class="Alphabets"><ol><li class="lower-roman"><code>select</code></li><li class="lower-roman"><code>col(["employeeId", "</code><code>salary", "bonus","department"])</code></li></ol></li>
			</ol>
			<p><strong class="bold">Question 53</strong>:</p>
			<p>Which of the following code blocks returns a DataFrame with the <code>salary</code> column converted into a string in the <code>df</code> DataFrame?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>df.withColumn("salary", </code><code>castString("salary", "string"))</code></li>
				<li class="Alphabets"><code>df.withColumn("salary", col("salary").cast("string"))</code></li>
				<li class="Alphabets"><code>df.select(cast("salary", "string"))</code></li>
				<li class="Alphabets"><code>df.withColumn("salary", col("salary").castString("string"))</code></li>
			</ol>
			<p><strong class="bold">Question 54</strong>:</p>
			<p>The following code block contains an error. The code block should combine data from DataFrames <code>salaryDf</code> and <code>employeeDf</code>, showing all rows of DataFrame <code>salaryDf</code> that have a matching value in column <code>employeeSalaryID</code> with a value in column <code>employeeID</code> of DataFrame <code>employeeDf</code>. Find the error.</p>
			<pre class="source-code">
employeeDf.join(salaryDf, employeeDf.employeeID==employeeSalaryID)</pre>			<ol class="margin-left">
				<li class="Alphabets">The <code>join</code> statement is missing the right-hand DataFrame, where the column name is <code>employeeSalaryID</code></li>
				<li class="Alphabets">The <code>union</code> method should be used instead of <code>join</code></li>
				<li class="Alphabets">Instead of <code>join</code>, <code>innerJoin</code> should have been used</li>
				<li class="Alphabets"><code>salaryDf</code> should come in place of <code>employeeDf</code></li>
			</ol>
			<p><strong class="bold">Question 55</strong>:</p>
			<p>Which of the following code blocks reads a JSON file stored at <code>my_file_path</code> as a DataFrame?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>spark.read.json(my_file_path)</code></li>
				<li class="Alphabets"><code>spark.read(my_file_path, source="json")</code></li>
				<li class="Alphabets"><code>spark.read.path(my_file_path)</code></li>
				<li class="Alphabets"><code>spark.read().json(my_file_path)</code></li>
			</ol>
			<p><strong class="bold">Question 56</strong>:</p>
			<p>The following code block contains an error. The code block should return a new DataFrame filtered by the rows where <code>salary</code> column is greater than 2000 in DataFrame <code>df</code>. Find the error.</p>
			<pre class="source-code">
df.where("col(salary) &gt;= 2000")</pre>			<ol class="margin-left">
				<li class="Alphabets">Instead of <code>where()</code>, <code>filter()</code> should be used</li>
				<li class="Alphabets">The argument to the <code>where</code> method should be <code>"col(salary) &gt; </code><code>2000"</code></li>
				<li class="Alphabets">Instead of <code>&gt;=</code>, the operator <code>&gt;</code> should be used</li>
				<li class="Alphabets">The argument to the <code>where</code> method should be <code>"salary &gt; </code><code>2000"</code></li>
			</ol>
			<p><strong class="bold">Question 57</strong>:</p>
			<p>Which of the following code blocks returns a DataFrame in which the <code>salary</code> and <code>state</code> columns are dropped from the <code>df</code> DataFrame?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>df.withColumn ("</code><code>salary", "state")</code></li>
				<li class="Alphabets"><code>df.drop(["salary", "state"])</code></li>
				<li class="Alphabets"><code>df.drop("salary", "state")</code></li>
				<li class="Alphabets"><code>df.withColumnRenamed ("</code><code>salary", "state")</code></li>
			</ol>
			<p><strong class="bold">Question 58</strong>:</p>
			<p>Which of the following code blocks returns a two-column DataFrame that contains counts of each department in the <code>df</code> DataFrame?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>df.count("department").distinct()</code></li>
				<li class="Alphabets"><code>df.count("department")</code></li>
				<li class="Alphabets"><code>df.groupBy("department").count()</code></li>
				<li class="Alphabets"><code>df.groupBy("department").agg(count("department"))</code></li>
			</ol>
			<p><strong class="bold">Question 59</strong>:</p>
			<p>Which of the following code blocks prints the schema of a DataFrame and contains both column names and types?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>print(df.columns)</code></li>
				<li class="Alphabets"><code>df.printSchema()</code></li>
				<li class="Alphabets"><code>df.rdd.printSchema()</code></li>
				<li class="Alphabets"><code>df.print_schema()</code></li>
			</ol>
			<p><strong class="bold">Question 60</strong>:</p>
			<p>Which of the following code blocks creates a new DataFrame with three columns: <code>department</code>, <code>age</code>, and <code>max_salary</code> and has the maximum salary for each employee from each department and each age group from the <code>df</code> DataFrame?</p>
			<ol class="margin-left">
				<li class="Alphabets"><code>df.max(salary)</code></li>
				<li class="Alphabets"><code>df.groupBy(["department", "age"]).agg(max("salary").alias("max_salary"))</code></li>
				<li class="Alphabets"><code>df.agg(max(salary).alias(max_salary')</code></li>
				<li class="Alphabets"><code>df.groupby(department).agg(max(salary).alias(max_salary)</code></li>
			</ol>
			<h2 id="_idParaDest-244"><a id="_idTextAnchor245"/>Answers</h2>
			<ol>
				<li>B</li>
				<li>A</li>
				<li>D</li>
				<li>D</li>
				<li>A</li>
				<li>D</li>
				<li>A</li>
				<li>D</li>
				<li>C</li>
				<li>B</li>
				<li>C</li>
				<li>A</li>
				<li>A</li>
				<li>A</li>
				<li>A</li>
				<li>C</li>
				<li>B</li>
				<li>B</li>
				<li>D</li>
				<li>D</li>
				<li>D</li>
				<li>C</li>
				<li>C</li>
				<li>D</li>
				<li>D</li>
				<li>C</li>
				<li>A</li>
				<li>C</li>
				<li>E</li>
				<li>C</li>
				<li>C</li>
				<li>D</li>
				<li>A</li>
				<li>B</li>
				<li>D</li>
				<li>B</li>
				<li>B</li>
				<li>C</li>
				<li>A</li>
				<li>B</li>
				<li>B</li>
				<li>A</li>
				<li>A</li>
				<li>A</li>
				<li>C</li>
				<li>D</li>
				<li>D</li>
				<li>C</li>
				<li>B</li>
				<li>A</li>
				<li>C</li>
				<li>C</li>
				<li>B</li>
				<li>A</li>
				<li>A</li>
				<li>D</li>
				<li>C</li>
				<li>C</li>
				<li>B</li>
				<li>B</li>
			</ol>
		</div>
	</body></html>