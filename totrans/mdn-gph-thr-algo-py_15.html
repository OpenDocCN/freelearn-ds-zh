<html><head></head><body>
<div id="_idContainer132" class="calibre2">
<h1 class="chapter-number" id="_idParaDest-128"><a id="_idTextAnchor128" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.1.1">10</span></h1>
<h1 id="_idParaDest-129" class="calibre5"><a id="_idTextAnchor129" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.2.1">Pathway Mining</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.3.1">In this chapter, we’ll explore pathway mining, where</span><a id="_idIndexMarker465" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.4.1"> we use network science and reasoning algorithms to uncover paths that exist within sequential data. </span><span class="kobospan" id="kobo.4.2">Pathways to outcomes are common in both the medical field, where disease progression often follows a pathway from one disease state to another, and the educational field, where course material often builds on prior course material within a degree program such as law or medicine. </span><span class="kobospan" id="kobo.4.3">We’ll consider a simulated example of medical courses leading to student success or failure in a hypothetical medical school to understand which courses may require extra support for struggling students to ensure ultimate </span><span><span class="kobospan" id="kobo.5.1">program success.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.6.1">By the end of this chapter, you’ll understand how to spot problems involving pathways to an outcome of interest, apply advanced reasoning algorithms to find likely pathways within a dataset and interpret the results to intervene at key points in the pathway to a given outcome of interest. </span><span class="kobospan" id="kobo.6.2">We’ll consider pathway mining within the context of education, but many problems in the real world involve pathways. </span><span class="kobospan" id="kobo.6.3">Let’s explore some of these scenarios in </span><span><span class="kobospan" id="kobo.7.1">more depth.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.8.1">Specifically, we will cover the following topics in </span><span><span class="kobospan" id="kobo.9.1">this chapter:</span></span></p>
<ul class="calibre10">
<li class="calibre11"><span class="kobospan" id="kobo.10.1">Introduction to Bayesian networks and </span><span><span class="kobospan" id="kobo.11.1">causal pathways</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.12.1">Educational </span><span><span class="kobospan" id="kobo.13.1">pathway example</span></span></li>
<li class="calibre11"><span class="kobospan" id="kobo.14.1">Analyzing course sequencing to find optimal student pathways </span><span><span class="kobospan" id="kobo.15.1">to graduation</span></span></li>
</ul>
<h1 id="_idParaDest-130" class="calibre5"><a id="_idTextAnchor130" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.16.1">Technical requirements</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.17.1">You will require Jupyter Notebook to run the practical examples in this chapter. </span></p>
<p class="calibre3"><span class="kobospan" id="kobo.18.1">The code for this chapter is available </span><span><span class="kobospan" id="kobo.19.1">here: </span></span><a href="https://github.com/PacktPublishing/Modern-Graph-Theory-Algorithms-with-Python" class="pcalibre calibre6 pcalibre1"><span><span class="kobospan" id="kobo.20.1">https://github.com/PacktPublishing/Modern-Graph-Theory-Algorithms-with-Python</span></span></a></p>
<h1 id="_idParaDest-131" class="calibre5"><a id="_idTextAnchor131" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.21.1">Introduction to Bayesian networks and causal pathways</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.22.1">Many outcomes of interest across industries involve a sequence of events or choices to reach the outcome of interest. </span><span class="kobospan" id="kobo.22.2">The arrival of packages depends on the safe arrival of packages at each relay point between shipping</span><a id="_idIndexMarker466" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.23.1"> and arrival. </span><span class="kobospan" id="kobo.23.2">Machinery failure can involve </span><strong class="bold"><span class="kobospan" id="kobo.24.1">single points of failure</span></strong><span class="kobospan" id="kobo.25.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.26.1">SPOFs</span></strong><span class="kobospan" id="kobo.27.1">) or cascades of failure, in which multiple parts of the machine fail before the machinery itself fails. </span></p>
<p class="calibre3"><span class="kobospan" id="kobo.28.1">Consider the pathway</span><a id="_idIndexMarker467" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.29.1"> to drug addiction. </span><span class="kobospan" id="kobo.29.2">First, a person must be in a situation where drug use occurs—through friends, through family, or through a new social group. </span><span class="kobospan" id="kobo.29.3">Then, a person must try an addictive substance. </span><span class="kobospan" id="kobo.29.4">Then, they must like the substance enough to continue using the substance frequently enough to reach a point of physical or psychological dependence on the drug. </span><span><em class="italic"><span class="kobospan" id="kobo.30.1">Figure 10</span></em></span><em class="italic"><span class="kobospan" id="kobo.31.1">.1</span></em><span class="kobospan" id="kobo.32.1"> shows this sequence of steps in a </span><span><span class="kobospan" id="kobo.33.1">diagram format:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer122">
<span class="kobospan" id="kobo.34.1"><img alt="Figure 10.1 – A sequential progression of events leading to drug addiction" src="image/B21087_10_01.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.35.1">Figure 10.1 – A sequential progression of events leading to drug addiction</span></p>
<p class="calibre3"><span><em class="italic"><span class="kobospan" id="kobo.36.1">Figure 10</span></em></span><em class="italic"><span class="kobospan" id="kobo.37.1">.1</span></em><span class="kobospan" id="kobo.38.1"> looks a lot like a directed network, where each situation is a vertex and each step in the pathway is a directed edge. </span><span class="kobospan" id="kobo.38.2">Each edge representing a progression step might be weighted by a probability of progression from one vertex to the next. </span><span class="kobospan" id="kobo.38.3">Let’s consider a population of adolescents at high risk of trying a new type of drug and some cross-sectional data on populations at each stage of use that a researcher has collected on the population to determine risks at each step. </span><span class="kobospan" id="kobo.38.4">Let’s say that all adolescents have been in a situation where they could try the drug, but only 30% of those who could try it actually do try it. </span><span class="kobospan" id="kobo.38.5">Of those who try the drug, only 20% like it enough to continue using the drug. </span><span class="kobospan" id="kobo.38.6">However, of those who do continue using it, 40% will become dependent on the drug. </span><span><em class="italic"><span class="kobospan" id="kobo.39.1">Figure 10</span></em></span><em class="italic"><span class="kobospan" id="kobo.40.1">.2</span></em><span class="kobospan" id="kobo.41.1"> summarizes </span><span><span class="kobospan" id="kobo.42.1">this information:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer123">
<span class="kobospan" id="kobo.43.1"><img alt="Figure 10.2 – A pathway to drug addiction with probability of transition at each step in the pathway" src="image/B21087_10_02.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.44.1">Figure 10.2 – A pathway to drug addiction with probability of transition at each step in the pathway</span></p>
<p class="calibre3"><span><em class="italic"><span class="kobospan" id="kobo.45.1">Figure 10</span></em></span><em class="italic"><span class="kobospan" id="kobo.46.1">.2</span></em><span class="kobospan" id="kobo.47.1"> shows a chain of probabilities as drug use progresses through different stages of use. </span><span class="kobospan" id="kobo.47.2">We can find the probability of reaching each stage by multiplying the transition probability of each step before a given stage. </span><span class="kobospan" id="kobo.47.3">For instance, regular use in this population involves trying the drug (30% chance of doing so) and starting regular use (20% chance of doing so); this gives a 6% chance (0.3 multiplied by 0.2) of an adolescent in this population using this new drug regularly. </span><span class="kobospan" id="kobo.47.4">Given that 40% of these adolescents using the new drug regularly end up dependent on it, we’d expect any given adolescent in this population to have a 2.4% chance of progressing to dependency on the new drug given their environment that is conducive to trying </span><span><span class="kobospan" id="kobo.48.1">the drug.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.49.1">Mathematics provides us with a formal way to study these pathways or construct them from a set of data. </span><span class="kobospan" id="kobo.49.2">Let’s turn to the mathematical tools we need to formalize this intuition of probabilities across a sequence </span><span><span class="kobospan" id="kobo.50.1">of events.</span></span></p>
<h2 id="_idParaDest-132" class="calibre7"><a id="_idTextAnchor132" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.51.1">Bayes’ Theorem</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.52.1">The probability of an event that</span><a id="_idIndexMarker468" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.53.1"> depends on other</span><a id="_idIndexMarker469" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.54.1"> events is called </span><strong class="bold"><span class="kobospan" id="kobo.55.1">conditional probability</span></strong><span class="kobospan" id="kobo.56.1">. </span><span class="kobospan" id="kobo.56.2">In probability theory, conditional probability is a measure of the probability of an event occurring, given that another event has already occurred. </span><span class="kobospan" id="kobo.56.3">In </span><span><em class="italic"><span class="kobospan" id="kobo.57.1">Figure 10</span></em></span><em class="italic"><span class="kobospan" id="kobo.58.1">.2</span></em><span class="kobospan" id="kobo.59.1">, the progression to drug dependence relies on regularly using the new drug, which relies upon trying the drug for the first time, which relies upon being in a situation where people use the drug. </span><span class="kobospan" id="kobo.59.2">While conditional probability doesn’t need to involve this many conditional steps, it does involve a prior event that influences the probability of an event of </span><span><span class="kobospan" id="kobo.60.1">interest occurring.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.61.1">Going back to our example in </span><span><em class="italic"><span class="kobospan" id="kobo.62.1">Figure 10</span></em></span><em class="italic"><span class="kobospan" id="kobo.63.1">.2</span></em><span class="kobospan" id="kobo.64.1">, we have a universe where an adolescent is exposed to drug use, represented in </span><span><em class="italic"><span class="kobospan" id="kobo.65.1">Figure 10</span></em></span><em class="italic"><span class="kobospan" id="kobo.66.1">.3</span></em><span class="kobospan" id="kobo.67.1"> as </span><span><span class="kobospan" id="kobo.68.1">a rectangle:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer124">
<span class="kobospan" id="kobo.69.1"><img alt="Figure 10.3 – A universe in which drug use is possible" src="image/B21087_10_03.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.70.1">Figure 10.3 – A universe in which drug use is possible</span></p>
<p class="calibre3"><span><em class="italic"><span class="kobospan" id="kobo.71.1">Figure 10</span></em></span><em class="italic"><span class="kobospan" id="kobo.72.1">.3</span></em><span class="kobospan" id="kobo.73.1"> shows an event that is 100% for our group of adolescents; all of them are exposed to this new drug at home, at school, or while they are with friends. </span><span class="kobospan" id="kobo.73.2">However, the event of trying the new drug only occurs 30% of the time, given a new universe that is much smaller (shown in </span><span><em class="italic"><span class="kobospan" id="kobo.74.1">Figure 10</span></em></span><span><em class="italic"><span class="kobospan" id="kobo.75.1">.4</span></em></span><span><span class="kobospan" id="kobo.76.1">):</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer125">
<span class="kobospan" id="kobo.77.1"><img alt="Figure 10.4 – A new universe of trying the drug within our initial universe of being exposed to drug use" src="image/B21087_10_04.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.78.1">Figure 10.4 – A new universe of trying the drug within our initial universe of being exposed to drug use</span></p>
<p class="calibre3"><span><em class="italic"><span class="kobospan" id="kobo.79.1">Figure 10</span></em></span><em class="italic"><span class="kobospan" id="kobo.80.1">.4</span></em><span class="kobospan" id="kobo.81.1"> shows a new universe, where a subset</span><a id="_idIndexMarker470" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.82.1"> of adolescents try the new drug. </span><span class="kobospan" id="kobo.82.2">Of these adolescents, 20% will regularly use the drug. </span><span class="kobospan" id="kobo.82.3">This creates a new universe, as shown in </span><span><em class="italic"><span class="kobospan" id="kobo.83.1">Figure 10</span></em></span><span><em class="italic"><span class="kobospan" id="kobo.84.1">.5</span></em></span><span><span class="kobospan" id="kobo.85.1">:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer126">
<span class="kobospan" id="kobo.86.1"><img alt="Figure 10.5 – A smaller universe containing adolescents who regularly use the new drug" src="image/B21087_10_05.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.87.1">Figure 10.5 – A smaller universe containing adolescents who regularly use the new drug</span></p>
<p class="calibre3"><span><em class="italic"><span class="kobospan" id="kobo.88.1">Figure 10</span></em></span><em class="italic"><span class="kobospan" id="kobo.89.1">.5</span></em><span class="kobospan" id="kobo.90.1"> shows a very small universe of adolescents compared to the initial universe of all adolescents at risk of trying this new drug (as we would expect, given that only 6% of adolescents in this population end up using the new drug regularly). </span><span class="kobospan" id="kobo.90.2">Among those who regularly use the new drug, we can split the universe shown in </span><span><em class="italic"><span class="kobospan" id="kobo.91.1">Figure 10</span></em></span><em class="italic"><span class="kobospan" id="kobo.92.1">.5</span></em><span class="kobospan" id="kobo.93.1"> again to obtain the set of adolescents who become dependent on the drug. </span><span class="kobospan" id="kobo.93.2">We won’t visualize this universe, as it is too small to visualize well within the small rectangle of regular </span><span><span class="kobospan" id="kobo.94.1">drug use.</span></span></p>
<p class="calibre3"><strong class="bold"><span class="kobospan" id="kobo.95.1">Bayes’ Theorem</span></strong><span class="kobospan" id="kobo.96.1"> provides a formula for calculating conditional probability by relating prior events’ probabilities to the event of interest. </span><span class="kobospan" id="kobo.96.2">We can compute the probability of an event, </span><em class="italic"><span class="kobospan" id="kobo.97.1">A</span></em><span class="kobospan" id="kobo.98.1">, given event </span><em class="italic"><span class="kobospan" id="kobo.99.1">B</span></em><span class="kobospan" id="kobo.100.1"> through the </span><span><span class="kobospan" id="kobo.101.1">following formula:</span></span></p>
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.102.1">P(A|B)=(P(A)*P(B|A))/P(B)</span></strong></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.103.1">Here, </span><em class="italic"><span class="kobospan" id="kobo.104.1">P(A|B)</span></em><span class="kobospan" id="kobo.105.1"> is the probability of event </span><em class="italic"><span class="kobospan" id="kobo.106.1">A</span></em><span class="kobospan" id="kobo.107.1"> occurring given event </span><em class="italic"><span class="kobospan" id="kobo.108.1">B</span></em><span class="kobospan" id="kobo.109.1"> occurring, </span><em class="italic"><span class="kobospan" id="kobo.110.1">P(A)</span></em><span class="kobospan" id="kobo.111.1"> is the probability of event </span><em class="italic"><span class="kobospan" id="kobo.112.1">A</span></em><span class="kobospan" id="kobo.113.1"> happening, </span><em class="italic"><span class="kobospan" id="kobo.114.1">P(B|A)</span></em><span class="kobospan" id="kobo.115.1"> is the probability of event </span><em class="italic"><span class="kobospan" id="kobo.116.1">B</span></em><span class="kobospan" id="kobo.117.1"> occurring given event </span><em class="italic"><span class="kobospan" id="kobo.118.1">A</span></em><span class="kobospan" id="kobo.119.1"> occurring, and </span><em class="italic"><span class="kobospan" id="kobo.120.1">P(B)</span></em><span class="kobospan" id="kobo.121.1"> is the probability of event </span><span><em class="italic"><span class="kobospan" id="kobo.122.1">B</span></em></span><span><span class="kobospan" id="kobo.123.1"> occurring.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.124.1">To make this more concrete, let’s go back to our drug use example. </span><span class="kobospan" id="kobo.124.2">We’ll call event </span><em class="italic"><span class="kobospan" id="kobo.125.1">A</span></em><span class="kobospan" id="kobo.126.1"> trying the new drug and event </span><em class="italic"><span class="kobospan" id="kobo.127.1">B</span></em><span class="kobospan" id="kobo.128.1"> being present where the drug is used. </span><span class="kobospan" id="kobo.128.2">The probability of event </span><em class="italic"><span class="kobospan" id="kobo.129.1">B</span></em><span class="kobospan" id="kobo.130.1"> is 100%, as is the probability of event </span><em class="italic"><span class="kobospan" id="kobo.131.1">B</span></em><span class="kobospan" id="kobo.132.1"> given event </span><em class="italic"><span class="kobospan" id="kobo.133.1">A</span></em><span class="kobospan" id="kobo.134.1">. </span><span class="kobospan" id="kobo.134.2">The probability of event </span><em class="italic"><span class="kobospan" id="kobo.135.1">A</span></em><span class="kobospan" id="kobo.136.1"> is 30%. </span><span class="kobospan" id="kobo.136.2">Let’s plug the values into </span><span><span class="kobospan" id="kobo.137.1">our formula:</span></span></p>
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.138.1">P(A|B)=(0.3*1)/1</span></strong></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.139.1">This gives us our expected 30%. </span><span class="kobospan" id="kobo.139.2">However, within probability</span><a id="_idIndexMarker471" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.140.1"> theory, not all events have a probability of 100%, and many calculations will be more complicated as conditional events are added. </span><span class="kobospan" id="kobo.140.2">Within real-world data, we may need to estimate these probabilities with an algorithm given the data we’ve collected. </span><span class="kobospan" id="kobo.140.3">Let’s move on to event chains such as our drug </span><span><span class="kobospan" id="kobo.141.1">use example.</span></span></p>
<h2 id="_idParaDest-133" class="calibre7"><a id="_idTextAnchor133" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.142.1">Causal pathways</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.143.1">Conditional probability can involve</span><a id="_idIndexMarker472" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.144.1"> more than two events of interest with conditional relationships. </span><strong class="bold"><span class="kobospan" id="kobo.145.1">Causal pathways</span></strong><span class="kobospan" id="kobo.146.1"> involve chains of conditional probability that can be relatively short, such as our drug dependence example, or very, very long and complex, such as protein activation pathways contributing to </span><span><span class="kobospan" id="kobo.147.1">disease risk.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.148.1">Thanks to Bayes’ Theorem, we can chain conditional probabilities together in a piecewise fashion until all conditional probabilities are linked into a final pathway. </span><span class="kobospan" id="kobo.148.2">This works quite well for estimating effect sizes and progression rates for well-known causal pathways. </span><span class="kobospan" id="kobo.148.3">However, many times, we don’t know the exact sequence of events leading to an outcome of interest and simply collect a lot of data we think is related to the outcome. </span><span class="kobospan" id="kobo.148.4">To analyze this data, we’ll need an algorithm. </span><span class="kobospan" id="kobo.148.5">Fortunately, one exists. </span><span class="kobospan" id="kobo.148.6">Let’s dive into Bayesian networks and their application to causal </span><span><span class="kobospan" id="kobo.149.1">pathway mining.</span></span></p>
<h2 id="_idParaDest-134" class="calibre7"><a id="_idTextAnchor134" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.150.1">Bayesian networks</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.151.1">A </span><strong class="bold"><span class="kobospan" id="kobo.152.1">Bayesian network</span></strong><span class="kobospan" id="kobo.153.1"> depicts a set of variables and their</span><a id="_idIndexMarker473" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.154.1"> conditional probabilities as a </span><strong class="bold"><span class="kobospan" id="kobo.155.1">directed acyclic graph</span></strong><span class="kobospan" id="kobo.156.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.157.1">DAG</span></strong><span class="kobospan" id="kobo.158.1">). </span><span class="kobospan" id="kobo.158.2">Vertices that are not</span><a id="_idIndexMarker474" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.159.1"> connected by an edge are conditionally independent (not dependent on each other). </span><span class="kobospan" id="kobo.159.2">Vertices connected by an edge are </span><span><span class="kobospan" id="kobo.160.1">conditionally dependent.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.161.1">The </span><strong class="bold"><span class="kobospan" id="kobo.162.1">chain rule of probability</span></strong><span class="kobospan" id="kobo.163.1"> allows us to construct a Bayesian</span><a id="_idIndexMarker475" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.164.1"> network as a product of conditional probabilities. </span><span class="kobospan" id="kobo.164.2">Consider events </span><em class="italic"><span class="kobospan" id="kobo.165.1">A</span></em><span class="kobospan" id="kobo.166.1">, </span><em class="italic"><span class="kobospan" id="kobo.167.1">B</span></em><span class="kobospan" id="kobo.168.1">, and </span><em class="italic"><span class="kobospan" id="kobo.169.1">C</span></em><span class="kobospan" id="kobo.170.1">. </span><span class="kobospan" id="kobo.170.2">Returning to our drug use example, event </span><em class="italic"><span class="kobospan" id="kobo.171.1">A</span></em><span class="kobospan" id="kobo.172.1"> might be regular use, event </span><em class="italic"><span class="kobospan" id="kobo.173.1">B</span></em><span class="kobospan" id="kobo.174.1"> trying the drug, and event </span><em class="italic"><span class="kobospan" id="kobo.175.1">C</span></em><span class="kobospan" id="kobo.176.1"> might be around the drug. </span><span class="kobospan" id="kobo.176.2">Our causal chain is thus </span><span><span class="kobospan" id="kobo.177.1">the following:</span></span></p>
<p class="calibre3"><span><strong class="source-inline"><span class="kobospan" id="kobo.178.1">P(A,B,C)=P(A|B,C)*P(B|C)*P(C)</span></strong></span></p>
<p class="calibre3"><em class="italic"><span class="kobospan" id="kobo.179.1">P(A, B, C)</span></em><span class="kobospan" id="kobo.180.1"> refers to the probability of all events occurring. </span><em class="italic"><span class="kobospan" id="kobo.181.1">P(A|B, C)</span></em><span class="kobospan" id="kobo.182.1"> refers to the probability of A occurring given that </span><em class="italic"><span class="kobospan" id="kobo.183.1">B</span></em><span class="kobospan" id="kobo.184.1"> and </span><em class="italic"><span class="kobospan" id="kobo.185.1">C</span></em><span class="kobospan" id="kobo.186.1"> have occurred. </span><em class="italic"><span class="kobospan" id="kobo.187.1">P(B|C)</span></em><span class="kobospan" id="kobo.188.1"> is the probability of </span><em class="italic"><span class="kobospan" id="kobo.189.1">B</span></em><span class="kobospan" id="kobo.190.1"> occurring given that </span><em class="italic"><span class="kobospan" id="kobo.191.1">C</span></em><span class="kobospan" id="kobo.192.1"> has occurred. </span><span class="kobospan" id="kobo.192.2">Because these events are usually inferred from data, we need to use an algorithm to estimate the joint probability distribution </span><em class="italic"><span class="kobospan" id="kobo.193.1">P(A,B,C).</span></em><span class="kobospan" id="kobo.194.1"> Typically, this is done with an expectation-maximization algorithm through the computation of expected values conditional on the data. </span><span class="kobospan" id="kobo.194.2">Then, the algorithm maximizes the complete likelihood, assuming that the expected values computed are the correct ones. </span><span class="kobospan" id="kobo.194.3">Values are then adjusted again given the likelihood computed in the last step. </span><span class="kobospan" id="kobo.194.4">Once the expected values and likelihood converge, the </span><span><span class="kobospan" id="kobo.195.1">algorithm stops.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.196.1">Now that we know the basics of Bayesian networks, let’s turn to an educational data example of causal pathways and datasets upon which Bayesian networks </span><span><span class="kobospan" id="kobo.197.1">can learn.</span></span></p>
<h1 id="_idParaDest-135" class="calibre5"><a id="_idTextAnchor135" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.198.1">Educational pathway example</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.199.1">One of the common uses of Bayesian</span><a id="_idIndexMarker476" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.200.1"> probability and networks is in educational research. </span><span class="kobospan" id="kobo.200.2">Course sequencing involves building upon prior knowledge, and students taking courses conditional on prior knowledge must first obtain that prior knowledge before they can succeed in the course at hand. </span><span class="kobospan" id="kobo.200.3">Prerequisite courses allow professors to require students to take certain courses before taking their courses. </span><span class="kobospan" id="kobo.200.4">Entry to university and then to graduate programs is conditional on successful completion of exams at the previous level of education. </span><span class="kobospan" id="kobo.200.5">Thus, education is a field in which Bayesian networks arise naturally. </span><span class="kobospan" id="kobo.200.6">Let’s dive into </span><span><span class="kobospan" id="kobo.201.1">an example.</span></span></p>
<h2 id="_idParaDest-136" class="calibre7"><a id="_idTextAnchor136" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.202.1">Outcomes in education</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.203.1">Many outcomes in education </span><a id="_idIndexMarker477" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.204.1">are the culmination of the long sequence of courses students take. </span><span class="kobospan" id="kobo.204.2">For instance, in South Africa and the United States, a student hoping to practice law must take many courses and take a final examination before being able to practice law independently. </span><span class="kobospan" id="kobo.204.3">Passing the examination relies on prior success in coursework and experience gained in internships and other hands-on legal activities. </span><span class="kobospan" id="kobo.204.4">Most medical degrees follow a similar educational approach, with a combination of coursework, practical experience, and final examination culminating in professional status in </span><span><span class="kobospan" id="kobo.205.1">the field.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.206.1">Understanding key milestones and turning points within these pathways ensures as many students as possible pass the final examinations to obtain their licenses. </span><span class="kobospan" id="kobo.206.2">However, many courses exist, and not all students take the same courses throughout their education. </span><span class="kobospan" id="kobo.206.3">Medical students may focus on courses most related to their medical subspecialty of interest. </span><span class="kobospan" id="kobo.206.4">Law students may intern within different branches of legal practice to see what type of law they might like to practice. </span><span class="kobospan" id="kobo.206.5">Thus, the data is often incomplete; coupled with small program sizes, this creates a difficult data </span><span><span class="kobospan" id="kobo.207.1">mining scenario.</span></span></p>
<h2 id="_idParaDest-137" class="calibre7"><a id="_idTextAnchor137" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.208.1">Course sequences</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.209.1">One of the major caveats</span><a id="_idIndexMarker478" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.210.1"> in professional education is course dependencies. </span><span class="kobospan" id="kobo.210.2">For instance, prior to taking a pathology course, medical students typically finish human anatomy. </span><span class="kobospan" id="kobo.210.3">You would expect that success in pathology is at least partially dependent on success in human anatomy. </span><span class="kobospan" id="kobo.210.4">However, it may not be the case that success in pathology depends on success in a human genetics course. </span><span class="kobospan" id="kobo.210.5">It may also be the case that success in these three courses does not influence final examination results (unlikely, </span><span><span class="kobospan" id="kobo.211.1">but possible).</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.212.1">To make matters even more complicated, certain modules within a course may be more related to student outcomes than other modules. </span><span class="kobospan" id="kobo.212.2">Thus, even a course-level analysis might not be sufficient to pinpoint exactly where students’ success or failure usually stems. </span><span class="kobospan" id="kobo.212.3">When looking at these types of real-world problems, it is important to consider the level of analysis under which the pathway </span><span><span class="kobospan" id="kobo.213.1">is scrutinized.</span></span></p>
<h2 id="_idParaDest-138" class="calibre7"><a id="_idTextAnchor138" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.214.1">Antecedents to success</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.215.1">Two approaches are common</span><a id="_idIndexMarker479" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.216.1"> when collecting data related to student success. </span><span class="kobospan" id="kobo.216.2">One approach, the agnostic one, does not make assumptions about which courses or modules relate most to the outcome. </span><span class="kobospan" id="kobo.216.3">Advantages of this approach include ensuring that all possible data is collected so that any existing relationships may be found. </span><span class="kobospan" id="kobo.216.4">However, the amount of course/module data collected may be large relative to sample size, leading to worse performance </span><span><span class="kobospan" id="kobo.217.1">of algorithms.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.218.1">Another approach is to have knowledge about pathways of interest prior to collecting the data. </span><span class="kobospan" id="kobo.218.2">This approach limits the amount of data collected, allowing algorithms to run on a sufficient sample size for good performance; however, if the guess is wrong, the results do not reflect the true pathway that exists in </span><span><span class="kobospan" id="kobo.219.1">the system.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.220.1">As we simulate data, we’ll take the prior knowledge approach to generate a small dataset to demonstrate how Bayesian networks find pathways in datasets. </span><span class="kobospan" id="kobo.220.2">Let’s dive into the dataset simulation and see Bayesian networks </span><span><span class="kobospan" id="kobo.221.1">in action.</span></span></p>
<h1 id="_idParaDest-139" class="calibre5"><a id="_idTextAnchor139" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.222.1">Analyzing course sequencing to find optimal student pathways to graduation</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.223.1">In this example, we’ll work</span><a id="_idIndexMarker480" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.224.1"> with a dataset representing</span><a id="_idIndexMarker481" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.225.1"> a medical program to understand pathways to the successful completion of a medical degree. </span><span class="kobospan" id="kobo.225.2">In a real medical program, we’d likely include all courses and potentially other factors, such as clinical experiences and research projects required for graduation. </span><span class="kobospan" id="kobo.225.3">However, to run a simple example, we’ll assume this data mining has already been done to identify courses related to </span><span><span class="kobospan" id="kobo.226.1">graduation outcomes.</span></span></p>
<h2 id="_idParaDest-140" class="calibre7"><a id="_idTextAnchor140" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.227.1">Introduction to a dataset</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.228.1">Let’s imagine a medical program </span><a id="_idIndexMarker482" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.229.1">with many courses leading to a final licensing exam. </span><span class="kobospan" id="kobo.229.2">Some courses aren’t emphasized by the final licensing exam very much (but are still important to study before entering the field). </span><span class="kobospan" id="kobo.229.3">A handful of courses, though, do show up regularly on the licensing exam, and some build on prior important licensing courses. </span><span class="kobospan" id="kobo.229.4">Let’s suppose human anatomy, cellular biology, pathology, microbiology, and neuroscience are five courses that are typically associated with success on the licensing exam. </span><span class="kobospan" id="kobo.229.5">Some material may overlap across the courses—particularly human anatomy, pathology, </span><span><span class="kobospan" id="kobo.230.1">and microbiology.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.231.1">We can use Python to simulate</span><a id="_idIndexMarker483" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.232.1"> student performance in five courses, with three having an overlap of material over the course sequence and two being relatively unrelated to the other three courses, and on a final exam to explore how we would mine for course pathways related to success on a final outcome—our final exam. </span><span class="kobospan" id="kobo.232.2">The </span><strong class="source-inline"><span class="kobospan" id="kobo.233.1">numpy</span></strong><span class="kobospan" id="kobo.234.1"> package has several useful functions to first generate binomial distributions with different success probabilities (</span><strong class="source-inline"><span class="kobospan" id="kobo.235.1">random.binomial()</span></strong><span class="kobospan" id="kobo.236.1">) and then to select outcomes from different distributions conditional on other generated probability distributions (using the </span><span><strong class="source-inline"><span class="kobospan" id="kobo.237.1">where()</span></strong></span><span><span class="kobospan" id="kobo.238.1"> clause):</span></span></p>
<ol class="calibre15">
<li class="calibre11"><span class="kobospan" id="kobo.239.1">Let’s see this in action with </span><strong class="source-inline1"><span class="kobospan" id="kobo.240.1">Script 10.1</span></strong><span class="kobospan" id="kobo.241.1">, where we first import our packages and then generate our conditional course distributions for </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.242.1">500</span></strong></span><span><span class="kobospan" id="kobo.243.1"> students:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.244.1">
#import needed packages
import pandas as pd
import numpy as np
#create conditional courses that relate to final exam passage #rates
#course 1, with low passage rates in general
course1=np.random.binomial(1,0.75,500)
#course 2, with low passage rates on the first attempt if course #1 was failed
course2a=np.random.binomial(1,0.95,500)
course2b=np.random.binomial(1,0.5,500)
course2=np.where(course1&gt;0,course2a,course2b)
#course 3, with passage rates relative dependent on prior #performance
course3a=np.random.binomial(1,0.95,500)
course3b=np.random.binomial(1,0.65,500)
course3=np.where(course2+course1&gt;1,course3a,course3b)</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.245.1">We’ll then add to </span><strong class="source-inline1"><span class="kobospan" id="kobo.246.1">Script 10.1</span></strong><span class="kobospan" id="kobo.247.1"> our final two courses</span><a id="_idIndexMarker484" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.248.1"> and the dependent performance on the </span><span><span class="kobospan" id="kobo.249.1">final exam:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.250.1">
#create two other courses that are not related to performance on #final exam
course4=np.random.binomial(1,0.8,200)
course5=np.random.binomial(1,0.85,200)
#create final exam passage rates
passa=np.random.binomial(1,0.95,200)
passb=np.random.binomial(1,0.75,200)
pass_final=np.where(course1+course2+course3&gt;2,
    course3a,course3b)</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.251.1">Now that we have our course data on pass/fail performance, we can create a data frame containing this data to pass into our pathway mining with a Bayesian network. </span><span class="kobospan" id="kobo.251.2">Let’s add this piece to </span><strong class="source-inline1"><span class="kobospan" id="kobo.252.1">Script 10.1</span></strong><span class="kobospan" id="kobo.253.1"> to prepare for our </span><span><span class="kobospan" id="kobo.254.1">pathway mining:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.255.1">
Course_Data=pd.DataFrame([course1, course2, course3,
    course4, course5, pass_final],
    index=['Course_1', 'Course_2', 'Course_3',
    'Course_4','Course_5',
    'Pass_Final_Exam']).transpose()</span></pre></li> </ol>
<p class="calibre3"><span class="kobospan" id="kobo.256.1">Now that we have</span><a id="_idIndexMarker485" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.257.1"> our dataset, we can turn our attention to the Bayesian network </span><span><span class="kobospan" id="kobo.258.1">we’ll create.</span></span></p>
<h2 id="_idParaDest-141" class="calibre7"><a id="_idTextAnchor141" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.259.1">bnlearn analysis</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.260.1">Python has an easy-to-use package</span><a id="_idIndexMarker486" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.261.1"> to fit Bayesian networks to datasets such as the one we generated in </span><strong class="source-inline"><span class="kobospan" id="kobo.262.1">Script 10.1</span></strong><span class="kobospan" id="kobo.263.1">: the </span><strong class="source-inline"><span class="kobospan" id="kobo.264.1">bnlearn</span></strong><span class="kobospan" id="kobo.265.1"> package. </span><span class="kobospan" id="kobo.265.2">If you do not have the current version of </span><strong class="source-inline"><span class="kobospan" id="kobo.266.1">numpy</span></strong><span class="kobospan" id="kobo.267.1"> installed, you’ll need to update your </span><strong class="source-inline"><span class="kobospan" id="kobo.268.1">numpy</span></strong><span class="kobospan" id="kobo.269.1"> version before installing the </span><strong class="source-inline"><span class="kobospan" id="kobo.270.1">bnlearn</span></strong><span class="kobospan" id="kobo.271.1"> package to avoid installation errors. </span><span class="kobospan" id="kobo.271.2">We assume that you have completed this step. </span><span class="kobospan" id="kobo.271.3">Follow the </span><span><span class="kobospan" id="kobo.272.1">next steps:</span></span></p>
<p class="callout-heading"><span class="kobospan" id="kobo.273.1">Note</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.274.1">There is a </span><strong class="source-inline"><span class="kobospan" id="kobo.275.1">pandas</span></strong><span class="kobospan" id="kobo.276.1"> dependency as well, so we will provide an example of installing a specific version of a package in the following code to deal with the </span><strong class="source-inline"><span class="kobospan" id="kobo.277.1">pandas</span></strong><span class="kobospan" id="kobo.278.1"> versioning dependency. </span><span class="kobospan" id="kobo.278.2">You’ll need to restart your Jupyter kernel after </span><span><span class="kobospan" id="kobo.279.1">this installation.</span></span></p>
<ol class="calibre15">
<li class="calibre11"><span class="kobospan" id="kobo.280.1">First, we’ll install the </span><strong class="source-inline1"><span class="kobospan" id="kobo.281.1">bnlearn</span></strong><span class="kobospan" id="kobo.282.1"> package and load it with </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.283.1">Script 10.2</span></strong></span><span><span class="kobospan" id="kobo.284.1">:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.285.1">
#install bnlearn package if not already in directory and import
!pip install pandas==1.5.3
!pip install bnlearn
import bnlearn as bn</span></pre></li> <li class="calibre11"><span class="kobospan" id="kobo.286.1">Now that we have our package installed, we can fit a Bayesian network to our </span><strong class="source-inline1"><span class="kobospan" id="kobo.287.1">Course_Data</span></strong><span class="kobospan" id="kobo.288.1"> dataset using the function’s default parameters by adding to </span><span><strong class="source-inline1"><span class="kobospan" id="kobo.289.1">Script 10.2</span></strong></span><span><span class="kobospan" id="kobo.290.1">:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.291.1">
#fit Bayesian network
model = bn.structure_learning.fit(Course_Data)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.292.1">The default parameters include a hill-climbing algorithm, which searches the local model space in a greedy fashion (where each step adjusts a single edge), and the </span><strong class="bold"><span class="kobospan" id="kobo.293.1">Bayesian inference criterion</span></strong><span class="kobospan" id="kobo.294.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.295.1">BIC</span></strong><span class="kobospan" id="kobo.296.1">) used as a performance measurement (which is a model deviance-based</span><a id="_idIndexMarker487" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.297.1"> measurement penalized by the number of </span><span><span class="kobospan" id="kobo.298.1">model parameters).</span></span></p></li> <li class="calibre11"><span class="kobospan" id="kobo.299.1">The </span><strong class="source-inline1"><span class="kobospan" id="kobo.300.1">bnlearn</span></strong><span class="kobospan" id="kobo.301.1"> package has a nice table</span><a id="_idIndexMarker488" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.302.1"> summary of dependencies found in the Bayesian network to show which variables are related. </span><span class="kobospan" id="kobo.302.2">Let’s add this piece to </span><strong class="source-inline1"><span class="kobospan" id="kobo.303.1">Script 10.2</span></strong><span class="kobospan" id="kobo.304.1"> and examine the printed table’s results in </span><span><em class="italic"><span class="kobospan" id="kobo.305.1">Figure 10</span></em></span><span><em class="italic"><span class="kobospan" id="kobo.306.1">.6</span></em></span><span><span class="kobospan" id="kobo.307.1">:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.308.1">
#print dependencies
print(model['adjmat'])</span></pre></li> </ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer127">
<span class="kobospan" id="kobo.309.1"><img alt="Figure 10.6 – A summary of our Bayesian network’s results" src="image/B21087_10_06.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.310.1">Figure 10.6 – A summary of our Bayesian network’s results</span></p>
<p class="calibre3"><span><em class="italic"><span class="kobospan" id="kobo.311.1">Figure 10</span></em></span><em class="italic"><span class="kobospan" id="kobo.312.1">.6</span></em><span class="kobospan" id="kobo.313.1"> shows which relationships exist between different variables. </span><span class="kobospan" id="kobo.313.2">The target is the dependent event, while the source is the prior event upon which the target depends. </span><span class="kobospan" id="kobo.313.3">If the dependency was found to exist, the cell relating the target and source will read as </span><strong class="source-inline"><span class="kobospan" id="kobo.314.1">True</span></strong><span class="kobospan" id="kobo.315.1">, while if the dependency was not found to exist, the cell will read </span><strong class="source-inline"><span class="kobospan" id="kobo.316.1">False</span></strong><span class="kobospan" id="kobo.317.1">. </span><span class="kobospan" id="kobo.317.2">Because this is a naïve analysis with respect to the timing of courses, we’ll ignore the directionality of the results (such as the final exam not being dependent on any of the courses prior to it according to the directionality of source and target, but courses being found dependent on final exam performance). </span><span class="kobospan" id="kobo.317.3">Typically, Bayesian network analysis is used to find relationships rather than explicit directionality, which can be tested with other statistical methods (discussed later in </span><span><span class="kobospan" id="kobo.318.1">this chapter).</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.319.1">We do find several of the dependencies</span><a id="_idIndexMarker489" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.320.1"> we simulated. </span><strong class="source-inline"><span class="kobospan" id="kobo.321.1">Course_2</span></strong><span class="kobospan" id="kobo.322.1"> was found to be dependent on </span><strong class="source-inline"><span class="kobospan" id="kobo.323.1">Course_1</span></strong><span class="kobospan" id="kobo.324.1">, and the final exam performance is dependent on </span><strong class="source-inline"><span class="kobospan" id="kobo.325.1">Course_1</span></strong><span class="kobospan" id="kobo.326.1">, </span><strong class="source-inline"><span class="kobospan" id="kobo.327.1">Course_2</span></strong><span class="kobospan" id="kobo.328.1">, and </span><strong class="source-inline"><span class="kobospan" id="kobo.329.1">Course_3</span></strong><span class="kobospan" id="kobo.330.1">. </span><span class="kobospan" id="kobo.330.2">While this is not 100% accurate, we did find all four of our dependencies as being within a </span><span><span class="kobospan" id="kobo.331.1">causal pathway.</span></span></p>
<ol class="calibre15">
<li value="4" class="calibre11"><span class="kobospan" id="kobo.332.1">By adding to </span><strong class="source-inline1"><span class="kobospan" id="kobo.333.1">Script 10.2</span></strong><span class="kobospan" id="kobo.334.1">, we can visualize the DAG we found in </span><span><span class="kobospan" id="kobo.335.1">our analysis:</span></span><pre class="source-code"><span class="kobospan1" id="kobo.336.1">
#plot Bayesian network derived from dataset
bn.plot(model)</span></pre><p class="calibre3"><span class="kobospan" id="kobo.337.1">This plot should show something like </span><span><em class="italic"><span class="kobospan" id="kobo.338.1">Figure 10</span></em></span><em class="italic"><span class="kobospan" id="kobo.339.1">.7</span></em><span class="kobospan" id="kobo.340.1">, showing the four related variables we simulated as existing within a course </span><span><span class="kobospan" id="kobo.341.1">success pathway:</span></span></p></li> </ol>
<div class="calibre2">
<div class="img---figure" id="_idContainer128">
<span class="kobospan" id="kobo.342.1"><img alt="Figure 10.7 – A plot of the Bayesian network’s results " src="image/B21087_10_07.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.343.1">Figure 10.7 – A plot of the Bayesian network’s results</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.344.1">Our results are pretty good</span><a id="_idIndexMarker490" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.345.1"> for a small dataset that we simulated with conditional and random distribution draws. </span><span class="kobospan" id="kobo.345.2">However, Bayesian networks can be very sensitive to algorithms used in fitting. </span><span class="kobospan" id="kobo.345.3">Let’s rerun our analysis using an exhaustive search, which scores all possible Bayesian network structures to choose the best model, rather than a hill-climbing algorithm. </span><span class="kobospan" id="kobo.345.4">Note that due to the increased compute time and power needed to fit the model, it is not advised as a fitting algorithm for large datasets or datasets with many variables </span><span><span class="kobospan" id="kobo.346.1">to explore.</span></span></p>
<p class="callout-heading"><span class="kobospan" id="kobo.347.1">Note</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.348.1">Depending on your system, you may or may not have the following script run to completion on your system. </span><span class="kobospan" id="kobo.348.2">On our machine, the following script took over an hour </span><span><span class="kobospan" id="kobo.349.1">to run.</span></span></p>
<p class="calibre3"><strong class="source-inline"><span class="kobospan" id="kobo.350.1">Script 10.3</span></strong><span class="kobospan" id="kobo.351.1"> runs this new fit of a Bayesian network to our three courses designed to depend on prior </span><span><span class="kobospan" id="kobo.352.1">course performance:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.353.1">
#fit Bayesian network
model = bn.structure_learning.fit(Course_Data,methodtype='ex')</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.354.1">We can now examine the table of relationships found by our Bayesian network, shown in </span><span><em class="italic"><span class="kobospan" id="kobo.355.1">Figure 10</span></em></span><em class="italic"><span class="kobospan" id="kobo.356.1">.8</span></em><span class="kobospan" id="kobo.357.1">, by adding to </span><span><strong class="source-inline"><span class="kobospan" id="kobo.358.1">Script 10.3</span></strong></span><span><span class="kobospan" id="kobo.359.1">:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.360.1">
#print dependencies
print(model['adjmat'])</span></pre> <div class="calibre2">
<div class="img---figure" id="_idContainer129">
<span class="kobospan" id="kobo.361.1"><img alt="Figure 10.8 – Bayesian network table of dependencies among three courses we simulated" src="image/B21087_10_08.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.362.1">Figure 10.8 – Bayesian network table of dependencies among three courses we simulated</span></p>
<p class="calibre3"><span><em class="italic"><span class="kobospan" id="kobo.363.1">Figure 10</span></em></span><em class="italic"><span class="kobospan" id="kobo.364.1">.8</span></em><span class="kobospan" id="kobo.365.1"> shows that this exhaustive search algorithm does find all three courses to be dependent, as well as the correct directionality of those dependencies—with passing </span><strong class="source-inline"><span class="kobospan" id="kobo.366.1">Course_2</span></strong><span class="kobospan" id="kobo.367.1"> being dependent on passing </span><strong class="source-inline"><span class="kobospan" id="kobo.368.1">Course_1</span></strong><span class="kobospan" id="kobo.369.1"> and with passing </span><strong class="source-inline"><span class="kobospan" id="kobo.370.1">Course_3</span></strong><span class="kobospan" id="kobo.371.1"> dependent on passing </span><strong class="source-inline"><span class="kobospan" id="kobo.372.1">Course_1</span></strong> <span><span class="kobospan" id="kobo.373.1">and </span></span><span><strong class="source-inline"><span class="kobospan" id="kobo.374.1">Course_2</span></strong></span><span><span class="kobospan" id="kobo.375.1">.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.376.1">Let’s now visualize these results</span><a id="_idIndexMarker491" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.377.1"> with a graph representation of our Bayesian network, adding the following to </span><span><strong class="source-inline"><span class="kobospan" id="kobo.378.1">Script 10.3</span></strong></span><span><span class="kobospan" id="kobo.379.1">:</span></span></p>
<pre class="source-code"><span class="kobospan1" id="kobo.380.1">
#plot Bayesian network derived from dataset
bn.plot(model)</span></pre> <p class="calibre3"><span class="kobospan" id="kobo.381.1">This yields the output shown in </span><span><em class="italic"><span class="kobospan" id="kobo.382.1">Figure 10</span></em></span><em class="italic"><span class="kobospan" id="kobo.383.1">.9</span></em><span class="kobospan" id="kobo.384.1">, which shows the dependencies found among the three courses. </span><span class="kobospan" id="kobo.384.2">Note the directionality in </span><span><em class="italic"><span class="kobospan" id="kobo.385.1">Figure 10</span></em></span><em class="italic"><span class="kobospan" id="kobo.386.1">.6</span></em><span class="kobospan" id="kobo.387.1"> matches the dependencies we simulated in </span><span><span class="kobospan" id="kobo.388.1">our dataset:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer130">
<span class="kobospan" id="kobo.389.1"><img alt="" role="presentation" src="image/B21087_10_09.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.390.1">Figure 10.9 – A plot of course pass dependencies among the three courses simulated to have interdependencies</span></p>
<p class="calibre3"><span class="kobospan" id="kobo.391.1">Note that while the exhaustive search finds the correct causal pathway, it is impractical for most real-world problems, as its run time limits are used on datasets with more variables. </span><span class="kobospan" id="kobo.391.2">Even for our full dataset, the runtime to compute a Bayesian network with exhaustive search is impractical. </span><span class="kobospan" id="kobo.391.3">However, the greedy hill-climbing algorithm was good enough to parse through our data and find likely relationships that exist in the dataset. </span><span class="kobospan" id="kobo.391.4">In the real world, mining for pathways is usually just a first step in understanding a system. </span><span class="kobospan" id="kobo.391.5">Identifying the main components</span><a id="_idIndexMarker492" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.392.1"> usually suffices for the next steps, which we’ll </span><span><span class="kobospan" id="kobo.393.1">discuss next.</span></span></p>
<h2 id="_idParaDest-142" class="calibre7"><a id="_idTextAnchor142" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.394.1">Structural equation models</span></h2>
<p class="calibre3"><span class="kobospan" id="kobo.395.1">Once we have an idea</span><a id="_idIndexMarker493" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.396.1"> of which parts of a pathway may lead to an outcome of interest, we can form a hypothesis regarding the logical steps between these parts. </span><span class="kobospan" id="kobo.396.2">For instance, in our course example, we may know that most students take </span><strong class="source-inline"><span class="kobospan" id="kobo.397.1">Course_1</span></strong><span class="kobospan" id="kobo.398.1"> before </span><strong class="source-inline"><span class="kobospan" id="kobo.399.1">Course_2</span></strong><span class="kobospan" id="kobo.400.1"> and </span><strong class="source-inline"><span class="kobospan" id="kobo.401.1">Course_2</span></strong><span class="kobospan" id="kobo.402.1"> before </span><strong class="source-inline"><span class="kobospan" id="kobo.403.1">Course_3</span></strong><span class="kobospan" id="kobo.404.1">. </span><span class="kobospan" id="kobo.404.2">This leads to a hypothesis that </span><strong class="source-inline"><span class="kobospan" id="kobo.405.1">Course_1</span></strong><span class="kobospan" id="kobo.406.1"> impacts performance in </span><strong class="source-inline"><span class="kobospan" id="kobo.407.1">Course_2</span></strong><span class="kobospan" id="kobo.408.1"> and </span><strong class="source-inline"><span class="kobospan" id="kobo.409.1">Course_3</span></strong><span class="kobospan" id="kobo.410.1"> and that </span><strong class="source-inline"><span class="kobospan" id="kobo.411.1">Course_2</span></strong><span class="kobospan" id="kobo.412.1"> impacts performance in </span><strong class="source-inline"><span class="kobospan" id="kobo.413.1">Course_3</span></strong><span class="kobospan" id="kobo.414.1">. </span><span class="kobospan" id="kobo.414.2">All three courses are assumed to influence performance in the licensing exam. </span><span class="kobospan" id="kobo.414.3">This gives us the hypothesized pathway shown in </span><span><em class="italic"><span class="kobospan" id="kobo.415.1">Figure 10</span></em></span><span><em class="italic"><span class="kobospan" id="kobo.416.1">.10</span></em></span><span><span class="kobospan" id="kobo.417.1">:</span></span></p>
<div class="calibre2">
<div class="img---figure" id="_idContainer131">
<span class="kobospan" id="kobo.418.1"><img alt="Figure 10.10 – Hypothesized pathway leading to performance in the final licensing exam" src="image/B21087_10_10.jpg" class="calibre4"/></span>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US"><span class="kobospan" id="kobo.419.1">Figure 10.10 – Hypothesized pathway leading to performance in the final licensing exam</span></p>
<p class="calibre3"><span><em class="italic"><span class="kobospan" id="kobo.420.1">Figure 10</span></em></span><em class="italic"><span class="kobospan" id="kobo.421.1">.10</span></em><span class="kobospan" id="kobo.422.1"> shows the sequence</span><a id="_idIndexMarker494" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.423.1"> of events leading to the licensing exam. </span><span class="kobospan" id="kobo.423.2">Performance in prior courses influences performance in future courses and, ultimately, the licensing step. </span><span class="kobospan" id="kobo.423.3">Failure along the way increases the likelihood of future failure, and success along the way increases the likelihood of </span><span><span class="kobospan" id="kobo.424.1">future success.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.425.1">Now that we have a hypothesized pathway, we can collect data from another year of students going through this pathway to test our hypothesis. </span><span class="kobospan" id="kobo.425.2">Because we are testing several relationships, we’d want a relatively large sample size. </span><span class="kobospan" id="kobo.425.3">Perhaps 500 students per year take the licensing exam. </span><span class="kobospan" id="kobo.425.4">We may want 2 years’ worth of new or historical data on students’ pathways to that licensing exam to test our hypothesis with enough statistical power to find effects </span><span><span class="kobospan" id="kobo.426.1">that exist.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.427.1">Given that random error influences regression model fitting and statistical results, running six logistic regression models is not ideal; we’d likely find false positives within our set of regression models. </span><span class="kobospan" id="kobo.427.2">However, a handy framework exists to model multiple regression models’ fit to causal pathways of interest: </span><strong class="bold"><span class="kobospan" id="kobo.428.1">structural equation models</span></strong><span class="kobospan" id="kobo.429.1"> (</span><strong class="bold"><span class="kobospan" id="kobo.430.1">SEMs</span></strong><span class="kobospan" id="kobo.431.1">). </span><span class="kobospan" id="kobo.431.2">SEMs provide a framework</span><a id="_idIndexMarker495" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.432.1"> for fitting causal pathway data within the regression framework, including those with fully measured variables and </span><strong class="bold"><span class="kobospan" id="kobo.433.1">latent variables</span></strong><span class="kobospan" id="kobo.434.1">—those inferred from relationships</span><a id="_idIndexMarker496" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.435.1"> that exist among measured variables but are not measured directly in </span><span><span class="kobospan" id="kobo.436.1">the data.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.437.1">While SEMs are beyond the scope of this book, many frameworks for fitting models exist, and some of the same estimation algorithms and goodness-of-fit statistics that we saw in Bayesian network model fitting are used to fit SEMs. </span><span class="kobospan" id="kobo.437.2">R and Mplus are more commonly used to fit SEMs than Python, but Python includes the </span><strong class="source-inline"><span class="kobospan" id="kobo.438.1">semopy</span></strong><span class="kobospan" id="kobo.439.1"> package to fit SEMs. </span><span class="kobospan" id="kobo.439.2">Note that the types of SEMs are limited compared to the other two software systems and that not all estimation algorithms or goodness-of-fit statistics exist in Python as of 2023. </span><span class="kobospan" id="kobo.439.3">However, if you are interested, I encourage you to explore SEMs as the next step in pathway mining, and references are provided at the end of this chapter if you would like to go further</span><a id="_idIndexMarker497" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.440.1"> in the field of </span><span><span class="kobospan" id="kobo.441.1">pathway mining.</span></span></p>
<h1 id="_idParaDest-143" class="calibre5"><a id="_idTextAnchor143" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.442.1">Summary</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.443.1">In this chapter, we introduced causal pathways and conditional probability theory through a social science example, building a network-based data mining tool called Bayesian networks. </span><span class="kobospan" id="kobo.443.2">We then simulated data from an educational pathway to implement Bayesian networks in Python. </span><span class="kobospan" id="kobo.443.3">These tools provided a starting point for collecting additional data that could be analyzed to confirm hypotheses constructed from Bayesian networks through a class of models called SEMs. </span><span class="kobospan" id="kobo.443.4">In the next chapter, we’ll pivot from causal pathways to look at another niche subfield in analytics: computational linguistics, where we will study languages and their relationships over long periods </span><span><span class="kobospan" id="kobo.444.1">of time.</span></span></p>
<h1 id="_idParaDest-144" class="calibre5"><a id="_idTextAnchor144" class="pcalibre calibre6 pcalibre1"/><span class="kobospan" id="kobo.445.1">References</span></h1>
<p class="calibre3"><span class="kobospan" id="kobo.446.1">Gladwin, T. </span><span class="kobospan" id="kobo.446.2">E., Figner, B., Crone, E. </span><span class="kobospan" id="kobo.446.3">A., &amp; Wiers, R. </span><span class="kobospan" id="kobo.446.4">W. </span><span class="kobospan" id="kobo.446.5">(2011). </span><span class="kobospan" id="kobo.446.6">Addiction, adolescence, and the integration of control and motivation. </span><em class="italic"><span class="kobospan" id="kobo.447.1">Developmental cognitive neuroscience, </span></em><span><em class="italic"><span class="kobospan" id="kobo.448.1">1(4), 364-376.</span></em></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.449.1">Heckerman, D. </span><span class="kobospan" id="kobo.449.2">(2008). </span><span class="kobospan" id="kobo.449.3">A tutorial on learning with Bayesian networks. </span><em class="italic"><span class="kobospan" id="kobo.450.1">Innovations in Bayesian networks: Theory and </span></em><span><em class="italic"><span class="kobospan" id="kobo.451.1">applications, 33-82.</span></em></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.452.1">Hoffman, K. </span><span class="kobospan" id="kobo.452.2">I. </span><span class="kobospan" id="kobo.452.3">(1993). </span><span class="kobospan" id="kobo.452.4">The USMLE, the NBME subject examinations, and assessment of individual academic achievement. </span><em class="italic"><span class="kobospan" id="kobo.453.1">Academic Medicine, </span></em><span><em class="italic"><span class="kobospan" id="kobo.454.1">68(10), 740-7.</span></em></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.455.1">Hoyle, R. </span><span class="kobospan" id="kobo.455.2">H. </span><span class="kobospan" id="kobo.455.3">(Ed.). </span><span class="kobospan" id="kobo.455.4">(1995). </span><em class="italic"><span class="kobospan" id="kobo.456.1">Structural equation modeling: Concepts, issues, and </span></em><span><em class="italic"><span class="kobospan" id="kobo.457.1">applications. </span><span class="kobospan" id="kobo.457.2">Sage.</span></em></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.458.1">Igolkina, A. </span><span class="kobospan" id="kobo.458.2">A., &amp; Meshcheryakov, G. </span><span class="kobospan" id="kobo.458.3">(2020). </span><span class="kobospan" id="kobo.458.4">semopy: A Python package for structural equation modeling. </span><em class="italic"><span class="kobospan" id="kobo.459.1">Structural Equation Modeling: A Multidisciplinary Journal, </span></em><span><em class="italic"><span class="kobospan" id="kobo.460.1">27(6), 952-963.</span></em></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.461.1">Kaufman, K. </span><span class="kobospan" id="kobo.461.2">A., LaSalle-Ricci, V. </span><span class="kobospan" id="kobo.461.3">H., Glass, C. </span><span class="kobospan" id="kobo.461.4">R., &amp; Arnkoff, D. </span><span class="kobospan" id="kobo.461.5">B. </span><span class="kobospan" id="kobo.461.6">(2007). </span><em class="italic"><span class="kobospan" id="kobo.462.1">Passing the bar exam: Psychological, educational, and demographic predictors of success. </span><span class="kobospan" id="kobo.462.2">J. </span><span class="kobospan" id="kobo.462.3">Legal Educ., </span></em><span><em class="italic"><span class="kobospan" id="kobo.463.1">57, 205.</span></em></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.464.1">Mak, K. </span><span class="kobospan" id="kobo.464.2">K., Jeong, J., Lee, H. </span><span class="kobospan" id="kobo.464.3">K., &amp; Lee, K. </span><span class="kobospan" id="kobo.464.4">(2018). </span><span class="kobospan" id="kobo.464.5">Mediating effect of internet addiction on the association between resilience and depression among Korean University students: a structural equation modeling approach. </span><em class="italic"><span class="kobospan" id="kobo.465.1">Psychiatry Investigation, </span></em><span><em class="italic"><span class="kobospan" id="kobo.466.1">15(10), 962.</span></em></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.467.1">Meca, A., Sabet, R. </span><span class="kobospan" id="kobo.467.2">F., Farrelly, C. </span><span class="kobospan" id="kobo.467.3">M., Benitez, C. </span><span class="kobospan" id="kobo.467.4">G., Schwartz, S. </span><span class="kobospan" id="kobo.467.5">J., Gonzales-Backen, M., ... </span><span class="kobospan" id="kobo.467.6">&amp; Lizzi, K. </span><span class="kobospan" id="kobo.467.7">M. </span><span class="kobospan" id="kobo.467.8">(2017). </span><span class="kobospan" id="kobo.467.9">Personal and cultural identity development in recently immigrated Hispanic adolescents: Links with psychosocial functioning. </span><em class="italic"><span class="kobospan" id="kobo.468.1">Cultural diversity and ethnic minority psychology, </span></em><span><em class="italic"><span class="kobospan" id="kobo.469.1">23(3), 348.</span></em></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.470.1">Ross, S. </span><span class="kobospan" id="kobo.470.2">M. </span><span class="kobospan" id="kobo.470.3">(2014). </span><em class="italic"><span class="kobospan" id="kobo.471.1">Introduction to probability models. </span></em><span><em class="italic"><span class="kobospan" id="kobo.472.1">Academic Press.</span></em></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.473.1">Scutari, M. </span><span class="kobospan" id="kobo.473.2">(2009). </span><em class="italic"><span class="kobospan" id="kobo.474.1">Learning Bayesian networks with the bnlearn R package. </span><span class="kobospan" id="kobo.474.2">arXiv </span></em><span><em class="italic"><span class="kobospan" id="kobo.475.1">preprint</span></em></span><span><span class="kobospan" id="kobo.476.1"> arXiv:0908.3817.</span></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.477.1">Turner, M. </span><span class="kobospan" id="kobo.477.2">E., &amp; Stevens, C. </span><span class="kobospan" id="kobo.477.3">D. </span><span class="kobospan" id="kobo.477.4">(1959). </span><em class="italic"><span class="kobospan" id="kobo.478.1">The regression analysis of causal paths. </span><span class="kobospan" id="kobo.478.2">Biometrics, </span></em><span><em class="italic"><span class="kobospan" id="kobo.479.1">15(2), 236-258.</span></em></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.480.1">Violato, C., &amp; Hecker, K. </span><span class="kobospan" id="kobo.480.2">G. </span><span class="kobospan" id="kobo.480.3">(2007). </span><em class="italic"><span class="kobospan" id="kobo.481.1">How to use structural equation modeling in medical education research: A brief guide. </span><span class="kobospan" id="kobo.481.2">Teaching and learning in medicine, </span></em><span><em class="italic"><span class="kobospan" id="kobo.482.1">19(4), 362-371.</span></em></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.483.1">Wise, R. </span><span class="kobospan" id="kobo.483.2">A., &amp; Koob, G. </span><span class="kobospan" id="kobo.483.3">F. </span><span class="kobospan" id="kobo.483.4">(2014). </span><span class="kobospan" id="kobo.483.5">The development and maintenance of drug addiction. </span><em class="italic"><span class="kobospan" id="kobo.484.1">Neuropsychopharmacology, </span></em><span><em class="italic"><span class="kobospan" id="kobo.485.1">39(2), 254-262.</span></em></span></p>
<p class="calibre3"><span class="kobospan" id="kobo.486.1">Wu, W., Garcia, K., Chandrahas, S., Siddiqui, A., Baronia, R., &amp; Ibrahim, Y. </span><span class="kobospan" id="kobo.486.2">(2021). </span><span class="kobospan" id="kobo.486.3">Predictors of performance on USMLE step 1. </span><em class="italic"><span class="kobospan" id="kobo.487.1">The Southwest Respiratory and Critical Care Chronicles, </span></em><span><em class="italic"><span class="kobospan" id="kobo.488.1">9(39), 63-72.</span></em></span></p>
</div>
</body></html>