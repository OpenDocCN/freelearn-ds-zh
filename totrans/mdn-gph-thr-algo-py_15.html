<html><head></head><body>
<div><h1 class="chapter-number" id="_idParaDest-128"><a id="_idTextAnchor128" class="pcalibre calibre6 pcalibre1"/>10</h1>
<h1 id="_idParaDest-129" class="calibre5"><a id="_idTextAnchor129" class="pcalibre calibre6 pcalibre1"/>Pathway Mining</h1>
<p class="calibre3">In this chapter, we’ll explore pathway mining, where<a id="_idIndexMarker465" class="pcalibre calibre6 pcalibre1"/> we use network science and reasoning algorithms to uncover paths that exist within sequential data. Pathways to outcomes are common in both the medical field, where disease progression often follows a pathway from one disease state to another, and the educational field, where course material often builds on prior course material within a degree program such as law or medicine. We’ll consider a simulated example of medical courses leading to student success or failure in a hypothetical medical school to understand which courses may require extra support for struggling students to ensure ultimate program success.</p>
<p class="calibre3">By the end of this chapter, you’ll understand how to spot problems involving pathways to an outcome of interest, apply advanced reasoning algorithms to find likely pathways within a dataset and interpret the results to intervene at key points in the pathway to a given outcome of interest. We’ll consider pathway mining within the context of education, but many problems in the real world involve pathways. Let’s explore some of these scenarios in more depth.</p>
<p class="calibre3">Specifically, we will cover the following topics in this chapter:</p>
<ul class="calibre10">
<li class="calibre11">Introduction to Bayesian networks and causal pathways</li>
<li class="calibre11">Educational pathway example</li>
<li class="calibre11">Analyzing course sequencing to find optimal student pathways to graduation</li>
</ul>
<h1 id="_idParaDest-130" class="calibre5"><a id="_idTextAnchor130" class="pcalibre calibre6 pcalibre1"/>Technical requirements</h1>
<p class="calibre3">You will require Jupyter Notebook to run the practical examples in this chapter. </p>
<p class="calibre3">The code for this chapter is available here: <a href="https://github.com/PacktPublishing/Modern-Graph-Theory-Algorithms-with-Python" class="pcalibre calibre6 pcalibre1">https://github.com/PacktPublishing/Modern-Graph-Theory-Algorithms-with-Python</a></p>
<h1 id="_idParaDest-131" class="calibre5"><a id="_idTextAnchor131" class="pcalibre calibre6 pcalibre1"/>Introduction to Bayesian networks and causal pathways</h1>
<p class="calibre3">Many outcomes of interest across industries involve a sequence of events or choices to reach the outcome of interest. The arrival of packages depends on the safe arrival of packages at each relay point between shipping<a id="_idIndexMarker466" class="pcalibre calibre6 pcalibre1"/> and arrival. Machinery failure can involve <strong class="bold">single points of failure</strong> (<strong class="bold">SPOFs</strong>) or cascades of failure, in which multiple parts of the machine fail before the machinery itself fails. </p>
<p class="calibre3">Consider the pathway<a id="_idIndexMarker467" class="pcalibre calibre6 pcalibre1"/> to drug addiction. First, a person must be in a situation where drug use occurs—through friends, through family, or through a new social group. Then, a person must try an addictive substance. Then, they must like the substance enough to continue using the substance frequently enough to reach a point of physical or psychological dependence on the drug. <em class="italic">Figure 10</em><em class="italic">.1</em> shows this sequence of steps in a diagram format:</p>
<div><div><img alt="Figure 10.1 – A sequential progression of events leading to drug addiction" src="img/B21087_10_01.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 10.1 – A sequential progression of events leading to drug addiction</p>
<p class="calibre3"><em class="italic">Figure 10</em><em class="italic">.1</em> looks a lot like a directed network, where each situation is a vertex and each step in the pathway is a directed edge. Each edge representing a progression step might be weighted by a probability of progression from one vertex to the next. Let’s consider a population of adolescents at high risk of trying a new type of drug and some cross-sectional data on populations at each stage of use that a researcher has collected on the population to determine risks at each step. Let’s say that all adolescents have been in a situation where they could try the drug, but only 30% of those who could try it actually do try it. Of those who try the drug, only 20% like it enough to continue using the drug. However, of those who do continue using it, 40% will become dependent on the drug. <em class="italic">Figure 10</em><em class="italic">.2</em> summarizes this information:</p>
<div><div><img alt="Figure 10.2 – A pathway to drug addiction with probability of transition at each step in the pathway" src="img/B21087_10_02.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 10.2 – A pathway to drug addiction with probability of transition at each step in the pathway</p>
<p class="calibre3"><em class="italic">Figure 10</em><em class="italic">.2</em> shows a chain of probabilities as drug use progresses through different stages of use. We can find the probability of reaching each stage by multiplying the transition probability of each step before a given stage. For instance, regular use in this population involves trying the drug (30% chance of doing so) and starting regular use (20% chance of doing so); this gives a 6% chance (0.3 multiplied by 0.2) of an adolescent in this population using this new drug regularly. Given that 40% of these adolescents using the new drug regularly end up dependent on it, we’d expect any given adolescent in this population to have a 2.4% chance of progressing to dependency on the new drug given their environment that is conducive to trying the drug.</p>
<p class="calibre3">Mathematics provides us with a formal way to study these pathways or construct them from a set of data. Let’s turn to the mathematical tools we need to formalize this intuition of probabilities across a sequence of events.</p>
<h2 id="_idParaDest-132" class="calibre7"><a id="_idTextAnchor132" class="pcalibre calibre6 pcalibre1"/>Bayes’ Theorem</h2>
<p class="calibre3">The probability of an event that<a id="_idIndexMarker468" class="pcalibre calibre6 pcalibre1"/> depends on other<a id="_idIndexMarker469" class="pcalibre calibre6 pcalibre1"/> events is called <strong class="bold">conditional probability</strong>. In probability theory, conditional probability is a measure of the probability of an event occurring, given that another event has already occurred. In <em class="italic">Figure 10</em><em class="italic">.2</em>, the progression to drug dependence relies on regularly using the new drug, which relies upon trying the drug for the first time, which relies upon being in a situation where people use the drug. While conditional probability doesn’t need to involve this many conditional steps, it does involve a prior event that influences the probability of an event of interest occurring.</p>
<p class="calibre3">Going back to our example in <em class="italic">Figure 10</em><em class="italic">.2</em>, we have a universe where an adolescent is exposed to drug use, represented in <em class="italic">Figure 10</em><em class="italic">.3</em> as a rectangle:</p>
<div><div><img alt="Figure 10.3 – A universe in which drug use is possible" src="img/B21087_10_03.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 10.3 – A universe in which drug use is possible</p>
<p class="calibre3"><em class="italic">Figure 10</em><em class="italic">.3</em> shows an event that is 100% for our group of adolescents; all of them are exposed to this new drug at home, at school, or while they are with friends. However, the event of trying the new drug only occurs 30% of the time, given a new universe that is much smaller (shown in <em class="italic">Figure 10</em><em class="italic">.4</em>):</p>
<div><div><img alt="Figure 10.4 – A new universe of trying the drug within our initial universe of being exposed to drug use" src="img/B21087_10_04.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 10.4 – A new universe of trying the drug within our initial universe of being exposed to drug use</p>
<p class="calibre3"><em class="italic">Figure 10</em><em class="italic">.4</em> shows a new universe, where a subset<a id="_idIndexMarker470" class="pcalibre calibre6 pcalibre1"/> of adolescents try the new drug. Of these adolescents, 20% will regularly use the drug. This creates a new universe, as shown in <em class="italic">Figure 10</em><em class="italic">.5</em>:</p>
<div><div><img alt="Figure 10.5 – A smaller universe containing adolescents who regularly use the new drug" src="img/B21087_10_05.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 10.5 – A smaller universe containing adolescents who regularly use the new drug</p>
<p class="calibre3"><em class="italic">Figure 10</em><em class="italic">.5</em> shows a very small universe of adolescents compared to the initial universe of all adolescents at risk of trying this new drug (as we would expect, given that only 6% of adolescents in this population end up using the new drug regularly). Among those who regularly use the new drug, we can split the universe shown in <em class="italic">Figure 10</em><em class="italic">.5</em> again to obtain the set of adolescents who become dependent on the drug. We won’t visualize this universe, as it is too small to visualize well within the small rectangle of regular drug use.</p>
<p class="calibre3"><strong class="bold">Bayes’ Theorem</strong> provides a formula for calculating conditional probability by relating prior events’ probabilities to the event of interest. We can compute the probability of an event, <em class="italic">A</em>, given event <em class="italic">B</em> through the following formula:</p>
<p class="calibre3"><code>P(A|B)=(P(A)*P(B|A))/P(B)</code></p>
<p class="calibre3">Here, <em class="italic">P(A|B)</em> is the probability of event <em class="italic">A</em> occurring given event <em class="italic">B</em> occurring, <em class="italic">P(A)</em> is the probability of event <em class="italic">A</em> happening, <em class="italic">P(B|A)</em> is the probability of event <em class="italic">B</em> occurring given event <em class="italic">A</em> occurring, and <em class="italic">P(B)</em> is the probability of event <em class="italic">B</em> occurring.</p>
<p class="calibre3">To make this more concrete, let’s go back to our drug use example. We’ll call event <em class="italic">A</em> trying the new drug and event <em class="italic">B</em> being present where the drug is used. The probability of event <em class="italic">B</em> is 100%, as is the probability of event <em class="italic">B</em> given event <em class="italic">A</em>. The probability of event <em class="italic">A</em> is 30%. Let’s plug the values into our formula:</p>
<p class="calibre3"><code>P(A|B)=(0.3*1)/1</code></p>
<p class="calibre3">This gives us our expected 30%. However, within probability<a id="_idIndexMarker471" class="pcalibre calibre6 pcalibre1"/> theory, not all events have a probability of 100%, and many calculations will be more complicated as conditional events are added. Within real-world data, we may need to estimate these probabilities with an algorithm given the data we’ve collected. Let’s move on to event chains such as our drug use example.</p>
<h2 id="_idParaDest-133" class="calibre7"><a id="_idTextAnchor133" class="pcalibre calibre6 pcalibre1"/>Causal pathways</h2>
<p class="calibre3">Conditional probability can involve<a id="_idIndexMarker472" class="pcalibre calibre6 pcalibre1"/> more than two events of interest with conditional relationships. <strong class="bold">Causal pathways</strong> involve chains of conditional probability that can be relatively short, such as our drug dependence example, or very, very long and complex, such as protein activation pathways contributing to disease risk.</p>
<p class="calibre3">Thanks to Bayes’ Theorem, we can chain conditional probabilities together in a piecewise fashion until all conditional probabilities are linked into a final pathway. This works quite well for estimating effect sizes and progression rates for well-known causal pathways. However, many times, we don’t know the exact sequence of events leading to an outcome of interest and simply collect a lot of data we think is related to the outcome. To analyze this data, we’ll need an algorithm. Fortunately, one exists. Let’s dive into Bayesian networks and their application to causal pathway mining.</p>
<h2 id="_idParaDest-134" class="calibre7"><a id="_idTextAnchor134" class="pcalibre calibre6 pcalibre1"/>Bayesian networks</h2>
<p class="calibre3">A <strong class="bold">Bayesian network</strong> depicts a set of variables and their<a id="_idIndexMarker473" class="pcalibre calibre6 pcalibre1"/> conditional probabilities as a <strong class="bold">directed acyclic graph</strong> (<strong class="bold">DAG</strong>). Vertices that are not<a id="_idIndexMarker474" class="pcalibre calibre6 pcalibre1"/> connected by an edge are conditionally independent (not dependent on each other). Vertices connected by an edge are conditionally dependent.</p>
<p class="calibre3">The <strong class="bold">chain rule of probability</strong> allows us to construct a Bayesian<a id="_idIndexMarker475" class="pcalibre calibre6 pcalibre1"/> network as a product of conditional probabilities. Consider events <em class="italic">A</em>, <em class="italic">B</em>, and <em class="italic">C</em>. Returning to our drug use example, event <em class="italic">A</em> might be regular use, event <em class="italic">B</em> trying the drug, and event <em class="italic">C</em> might be around the drug. Our causal chain is thus the following:</p>
<p class="calibre3"><code>P(A,B,C)=P(A|B,C)*P(B|C)*P(C)</code></p>
<p class="calibre3"><em class="italic">P(A, B, C)</em> refers to the probability of all events occurring. <em class="italic">P(A|B, C)</em> refers to the probability of A occurring given that <em class="italic">B</em> and <em class="italic">C</em> have occurred. <em class="italic">P(B|C)</em> is the probability of <em class="italic">B</em> occurring given that <em class="italic">C</em> has occurred. Because these events are usually inferred from data, we need to use an algorithm to estimate the joint probability distribution <em class="italic">P(A,B,C).</em> Typically, this is done with an expectation-maximization algorithm through the computation of expected values conditional on the data. Then, the algorithm maximizes the complete likelihood, assuming that the expected values computed are the correct ones. Values are then adjusted again given the likelihood computed in the last step. Once the expected values and likelihood converge, the algorithm stops.</p>
<p class="calibre3">Now that we know the basics of Bayesian networks, let’s turn to an educational data example of causal pathways and datasets upon which Bayesian networks can learn.</p>
<h1 id="_idParaDest-135" class="calibre5"><a id="_idTextAnchor135" class="pcalibre calibre6 pcalibre1"/>Educational pathway example</h1>
<p class="calibre3">One of the common uses of Bayesian<a id="_idIndexMarker476" class="pcalibre calibre6 pcalibre1"/> probability and networks is in educational research. Course sequencing involves building upon prior knowledge, and students taking courses conditional on prior knowledge must first obtain that prior knowledge before they can succeed in the course at hand. Prerequisite courses allow professors to require students to take certain courses before taking their courses. Entry to university and then to graduate programs is conditional on successful completion of exams at the previous level of education. Thus, education is a field in which Bayesian networks arise naturally. Let’s dive into an example.</p>
<h2 id="_idParaDest-136" class="calibre7"><a id="_idTextAnchor136" class="pcalibre calibre6 pcalibre1"/>Outcomes in education</h2>
<p class="calibre3">Many outcomes in education <a id="_idIndexMarker477" class="pcalibre calibre6 pcalibre1"/>are the culmination of the long sequence of courses students take. For instance, in South Africa and the United States, a student hoping to practice law must take many courses and take a final examination before being able to practice law independently. Passing the examination relies on prior success in coursework and experience gained in internships and other hands-on legal activities. Most medical degrees follow a similar educational approach, with a combination of coursework, practical experience, and final examination culminating in professional status in the field.</p>
<p class="calibre3">Understanding key milestones and turning points within these pathways ensures as many students as possible pass the final examinations to obtain their licenses. However, many courses exist, and not all students take the same courses throughout their education. Medical students may focus on courses most related to their medical subspecialty of interest. Law students may intern within different branches of legal practice to see what type of law they might like to practice. Thus, the data is often incomplete; coupled with small program sizes, this creates a difficult data mining scenario.</p>
<h2 id="_idParaDest-137" class="calibre7"><a id="_idTextAnchor137" class="pcalibre calibre6 pcalibre1"/>Course sequences</h2>
<p class="calibre3">One of the major caveats<a id="_idIndexMarker478" class="pcalibre calibre6 pcalibre1"/> in professional education is course dependencies. For instance, prior to taking a pathology course, medical students typically finish human anatomy. You would expect that success in pathology is at least partially dependent on success in human anatomy. However, it may not be the case that success in pathology depends on success in a human genetics course. It may also be the case that success in these three courses does not influence final examination results (unlikely, but possible).</p>
<p class="calibre3">To make matters even more complicated, certain modules within a course may be more related to student outcomes than other modules. Thus, even a course-level analysis might not be sufficient to pinpoint exactly where students’ success or failure usually stems. When looking at these types of real-world problems, it is important to consider the level of analysis under which the pathway is scrutinized.</p>
<h2 id="_idParaDest-138" class="calibre7"><a id="_idTextAnchor138" class="pcalibre calibre6 pcalibre1"/>Antecedents to success</h2>
<p class="calibre3">Two approaches are common<a id="_idIndexMarker479" class="pcalibre calibre6 pcalibre1"/> when collecting data related to student success. One approach, the agnostic one, does not make assumptions about which courses or modules relate most to the outcome. Advantages of this approach include ensuring that all possible data is collected so that any existing relationships may be found. However, the amount of course/module data collected may be large relative to sample size, leading to worse performance of algorithms.</p>
<p class="calibre3">Another approach is to have knowledge about pathways of interest prior to collecting the data. This approach limits the amount of data collected, allowing algorithms to run on a sufficient sample size for good performance; however, if the guess is wrong, the results do not reflect the true pathway that exists in the system.</p>
<p class="calibre3">As we simulate data, we’ll take the prior knowledge approach to generate a small dataset to demonstrate how Bayesian networks find pathways in datasets. Let’s dive into the dataset simulation and see Bayesian networks in action.</p>
<h1 id="_idParaDest-139" class="calibre5"><a id="_idTextAnchor139" class="pcalibre calibre6 pcalibre1"/>Analyzing course sequencing to find optimal student pathways to graduation</h1>
<p class="calibre3">In this example, we’ll work<a id="_idIndexMarker480" class="pcalibre calibre6 pcalibre1"/> with a dataset representing<a id="_idIndexMarker481" class="pcalibre calibre6 pcalibre1"/> a medical program to understand pathways to the successful completion of a medical degree. In a real medical program, we’d likely include all courses and potentially other factors, such as clinical experiences and research projects required for graduation. However, to run a simple example, we’ll assume this data mining has already been done to identify courses related to graduation outcomes.</p>
<h2 id="_idParaDest-140" class="calibre7"><a id="_idTextAnchor140" class="pcalibre calibre6 pcalibre1"/>Introduction to a dataset</h2>
<p class="calibre3">Let’s imagine a medical program <a id="_idIndexMarker482" class="pcalibre calibre6 pcalibre1"/>with many courses leading to a final licensing exam. Some courses aren’t emphasized by the final licensing exam very much (but are still important to study before entering the field). A handful of courses, though, do show up regularly on the licensing exam, and some build on prior important licensing courses. Let’s suppose human anatomy, cellular biology, pathology, microbiology, and neuroscience are five courses that are typically associated with success on the licensing exam. Some material may overlap across the courses—particularly human anatomy, pathology, and microbiology.</p>
<p class="calibre3">We can use Python to simulate<a id="_idIndexMarker483" class="pcalibre calibre6 pcalibre1"/> student performance in five courses, with three having an overlap of material over the course sequence and two being relatively unrelated to the other three courses, and on a final exam to explore how we would mine for course pathways related to success on a final outcome—our final exam. The <code>numpy</code> package has several useful functions to first generate binomial distributions with different success probabilities (<code>random.binomial()</code>) and then to select outcomes from different distributions conditional on other generated probability distributions (using the <code>where()</code> clause):</p>
<ol class="calibre15">
<li class="calibre11">Let’s see this in action with <strong class="source-inline1">Script 10.1</strong>, where we first import our packages and then generate our conditional course distributions for <strong class="source-inline1">500</strong> students:<pre class="source-code">
#import needed packages
import pandas as pd
import numpy as np
#create conditional courses that relate to final exam passage #rates
#course 1, with low passage rates in general
course1=np.random.binomial(1,0.75,500)
#course 2, with low passage rates on the first attempt if course #1 was failed
course2a=np.random.binomial(1,0.95,500)
course2b=np.random.binomial(1,0.5,500)
course2=np.where(course1&gt;0,course2a,course2b)
#course 3, with passage rates relative dependent on prior #performance
course3a=np.random.binomial(1,0.95,500)
course3b=np.random.binomial(1,0.65,500)
course3=np.where(course2+course1&gt;1,course3a,course3b)</pre></li> <li class="calibre11">We’ll then add to <strong class="source-inline1">Script 10.1</strong> our final two courses<a id="_idIndexMarker484" class="pcalibre calibre6 pcalibre1"/> and the dependent performance on the final exam:<pre class="source-code">
#create two other courses that are not related to performance on #final exam
course4=np.random.binomial(1,0.8,200)
course5=np.random.binomial(1,0.85,200)
#create final exam passage rates
passa=np.random.binomial(1,0.95,200)
passb=np.random.binomial(1,0.75,200)
pass_final=np.where(course1+course2+course3&gt;2,
    course3a,course3b)</pre></li> <li class="calibre11">Now that we have our course data on pass/fail performance, we can create a data frame containing this data to pass into our pathway mining with a Bayesian network. Let’s add this piece to <strong class="source-inline1">Script 10.1</strong> to prepare for our pathway mining:<pre class="source-code">
Course_Data=pd.DataFrame([course1, course2, course3,
    course4, course5, pass_final],
    index=['Course_1', 'Course_2', 'Course_3',
    'Course_4','Course_5',
    'Pass_Final_Exam']).transpose()</pre></li> </ol>
<p class="calibre3">Now that we have<a id="_idIndexMarker485" class="pcalibre calibre6 pcalibre1"/> our dataset, we can turn our attention to the Bayesian network we’ll create.</p>
<h2 id="_idParaDest-141" class="calibre7"><a id="_idTextAnchor141" class="pcalibre calibre6 pcalibre1"/>bnlearn analysis</h2>
<p class="calibre3">Python has an easy-to-use package<a id="_idIndexMarker486" class="pcalibre calibre6 pcalibre1"/> to fit Bayesian networks to datasets such as the one we generated in <code>Script 10.1</code>: the <code>bnlearn</code> package. If you do not have the current version of <code>numpy</code> installed, you’ll need to update your <code>numpy</code> version before installing the <code>bnlearn</code> package to avoid installation errors. We assume that you have completed this step. Follow the next steps:</p>
<p class="callout-heading">Note</p>
<p class="calibre3">There is a <code>pandas</code> dependency as well, so we will provide an example of installing a specific version of a package in the following code to deal with the <code>pandas</code> versioning dependency. You’ll need to restart your Jupyter kernel after this installation.</p>
<ol class="calibre15">
<li class="calibre11">First, we’ll install the <strong class="source-inline1">bnlearn</strong> package and load it with <strong class="source-inline1">Script 10.2</strong>:<pre class="source-code">
#install bnlearn package if not already in directory and import
!pip install pandas==1.5.3
!pip install bnlearn
import bnlearn as bn</pre></li> <li class="calibre11">Now that we have our package installed, we can fit a Bayesian network to our <strong class="source-inline1">Course_Data</strong> dataset using the function’s default parameters by adding to <strong class="source-inline1">Script 10.2</strong>:<pre class="source-code">
#fit Bayesian network
model = bn.structure_learning.fit(Course_Data)</pre><p class="calibre3">The default parameters include a hill-climbing algorithm, which searches the local model space in a greedy fashion (where each step adjusts a single edge), and the <strong class="bold">Bayesian inference criterion</strong> (<strong class="bold">BIC</strong>) used as a performance measurement (which is a model deviance-based<a id="_idIndexMarker487" class="pcalibre calibre6 pcalibre1"/> measurement penalized by the number of model parameters).</p></li> <li class="calibre11">The <strong class="source-inline1">bnlearn</strong> package has a nice table<a id="_idIndexMarker488" class="pcalibre calibre6 pcalibre1"/> summary of dependencies found in the Bayesian network to show which variables are related. Let’s add this piece to <strong class="source-inline1">Script 10.2</strong> and examine the printed table’s results in <em class="italic">Figure 10</em><em class="italic">.6</em>:<pre class="source-code">
#print dependencies
print(model['adjmat'])</pre></li> </ol>
<div><div><img alt="Figure 10.6 – A summary of our Bayesian network’s results" src="img/B21087_10_06.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 10.6 – A summary of our Bayesian network’s results</p>
<p class="calibre3"><em class="italic">Figure 10</em><em class="italic">.6</em> shows which relationships exist between different variables. The target is the dependent event, while the source is the prior event upon which the target depends. If the dependency was found to exist, the cell relating the target and source will read as <code>True</code>, while if the dependency was not found to exist, the cell will read <code>False</code>. Because this is a naïve analysis with respect to the timing of courses, we’ll ignore the directionality of the results (such as the final exam not being dependent on any of the courses prior to it according to the directionality of source and target, but courses being found dependent on final exam performance). Typically, Bayesian network analysis is used to find relationships rather than explicit directionality, which can be tested with other statistical methods (discussed later in this chapter).</p>
<p class="calibre3">We do find several of the dependencies<a id="_idIndexMarker489" class="pcalibre calibre6 pcalibre1"/> we simulated. <code>Course_2</code> was found to be dependent on <code>Course_1</code>, and the final exam performance is dependent on <code>Course_1</code>, <code>Course_2</code>, and <code>Course_3</code>. While this is not 100% accurate, we did find all four of our dependencies as being within a causal pathway.</p>
<ol class="calibre15">
<li value="4" class="calibre11">By adding to <strong class="source-inline1">Script 10.2</strong>, we can visualize the DAG we found in our analysis:<pre class="source-code">
#plot Bayesian network derived from dataset
bn.plot(model)</pre><p class="calibre3">This plot should show something like <em class="italic">Figure 10</em><em class="italic">.7</em>, showing the four related variables we simulated as existing within a course success pathway:</p></li> </ol>
<div><div><img alt="Figure 10.7 – A plot of the Bayesian network’s results " src="img/B21087_10_07.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 10.7 – A plot of the Bayesian network’s results</p>
<p class="calibre3">Our results are pretty good<a id="_idIndexMarker490" class="pcalibre calibre6 pcalibre1"/> for a small dataset that we simulated with conditional and random distribution draws. However, Bayesian networks can be very sensitive to algorithms used in fitting. Let’s rerun our analysis using an exhaustive search, which scores all possible Bayesian network structures to choose the best model, rather than a hill-climbing algorithm. Note that due to the increased compute time and power needed to fit the model, it is not advised as a fitting algorithm for large datasets or datasets with many variables to explore.</p>
<p class="callout-heading">Note</p>
<p class="calibre3">Depending on your system, you may or may not have the following script run to completion on your system. On our machine, the following script took over an hour to run.</p>
<p class="calibre3"><code>Script 10.3</code> runs this new fit of a Bayesian network to our three courses designed to depend on prior course performance:</p>
<pre class="source-code">
#fit Bayesian network
model = bn.structure_learning.fit(Course_Data,methodtype='ex')</pre> <p class="calibre3">We can now examine the table of relationships found by our Bayesian network, shown in <em class="italic">Figure 10</em><em class="italic">.8</em>, by adding to <code>Script 10.3</code>:</p>
<pre class="source-code">
#print dependencies
print(model['adjmat'])</pre> <div><div><img alt="Figure 10.8 – Bayesian network table of dependencies among three courses we simulated" src="img/B21087_10_08.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 10.8 – Bayesian network table of dependencies among three courses we simulated</p>
<p class="calibre3"><em class="italic">Figure 10</em><em class="italic">.8</em> shows that this exhaustive search algorithm does find all three courses to be dependent, as well as the correct directionality of those dependencies—with passing <code>Course_2</code> being dependent on passing <code>Course_1</code> and with passing <code>Course_3</code> dependent on passing <code>Course_1</code> and <code>Course_2</code>.</p>
<p class="calibre3">Let’s now visualize these results<a id="_idIndexMarker491" class="pcalibre calibre6 pcalibre1"/> with a graph representation of our Bayesian network, adding the following to <code>Script 10.3</code>:</p>
<pre class="source-code">
#plot Bayesian network derived from dataset
bn.plot(model)</pre> <p class="calibre3">This yields the output shown in <em class="italic">Figure 10</em><em class="italic">.9</em>, which shows the dependencies found among the three courses. Note the directionality in <em class="italic">Figure 10</em><em class="italic">.6</em> matches the dependencies we simulated in our dataset:</p>
<div><div><img alt="" role="presentation" src="img/B21087_10_09.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 10.9 – A plot of course pass dependencies among the three courses simulated to have interdependencies</p>
<p class="calibre3">Note that while the exhaustive search finds the correct causal pathway, it is impractical for most real-world problems, as its run time limits are used on datasets with more variables. Even for our full dataset, the runtime to compute a Bayesian network with exhaustive search is impractical. However, the greedy hill-climbing algorithm was good enough to parse through our data and find likely relationships that exist in the dataset. In the real world, mining for pathways is usually just a first step in understanding a system. Identifying the main components<a id="_idIndexMarker492" class="pcalibre calibre6 pcalibre1"/> usually suffices for the next steps, which we’ll discuss next.</p>
<h2 id="_idParaDest-142" class="calibre7"><a id="_idTextAnchor142" class="pcalibre calibre6 pcalibre1"/>Structural equation models</h2>
<p class="calibre3">Once we have an idea<a id="_idIndexMarker493" class="pcalibre calibre6 pcalibre1"/> of which parts of a pathway may lead to an outcome of interest, we can form a hypothesis regarding the logical steps between these parts. For instance, in our course example, we may know that most students take <code>Course_1</code> before <code>Course_2</code> and <code>Course_2</code> before <code>Course_3</code>. This leads to a hypothesis that <code>Course_1</code> impacts performance in <code>Course_2</code> and <code>Course_3</code> and that <code>Course_2</code> impacts performance in <code>Course_3</code>. All three courses are assumed to influence performance in the licensing exam. This gives us the hypothesized pathway shown in <em class="italic">Figure 10</em><em class="italic">.10</em>:</p>
<div><div><img alt="Figure 10.10 – Hypothesized pathway leading to performance in the final licensing exam" src="img/B21087_10_10.jpg" class="calibre4"/>
</div>
</div>
<p class="img---caption" lang="en-US" xml:lang="en-US">Figure 10.10 – Hypothesized pathway leading to performance in the final licensing exam</p>
<p class="calibre3"><em class="italic">Figure 10</em><em class="italic">.10</em> shows the sequence<a id="_idIndexMarker494" class="pcalibre calibre6 pcalibre1"/> of events leading to the licensing exam. Performance in prior courses influences performance in future courses and, ultimately, the licensing step. Failure along the way increases the likelihood of future failure, and success along the way increases the likelihood of future success.</p>
<p class="calibre3">Now that we have a hypothesized pathway, we can collect data from another year of students going through this pathway to test our hypothesis. Because we are testing several relationships, we’d want a relatively large sample size. Perhaps 500 students per year take the licensing exam. We may want 2 years’ worth of new or historical data on students’ pathways to that licensing exam to test our hypothesis with enough statistical power to find effects that exist.</p>
<p class="calibre3">Given that random error influences regression model fitting and statistical results, running six logistic regression models is not ideal; we’d likely find false positives within our set of regression models. However, a handy framework exists to model multiple regression models’ fit to causal pathways of interest: <strong class="bold">structural equation models</strong> (<strong class="bold">SEMs</strong>). SEMs provide a framework<a id="_idIndexMarker495" class="pcalibre calibre6 pcalibre1"/> for fitting causal pathway data within the regression framework, including those with fully measured variables and <strong class="bold">latent variables</strong>—those inferred from relationships<a id="_idIndexMarker496" class="pcalibre calibre6 pcalibre1"/> that exist among measured variables but are not measured directly in the data.</p>
<p class="calibre3">While SEMs are beyond the scope of this book, many frameworks for fitting models exist, and some of the same estimation algorithms and goodness-of-fit statistics that we saw in Bayesian network model fitting are used to fit SEMs. R and Mplus are more commonly used to fit SEMs than Python, but Python includes the <code>semopy</code> package to fit SEMs. Note that the types of SEMs are limited compared to the other two software systems and that not all estimation algorithms or goodness-of-fit statistics exist in Python as of 2023. However, if you are interested, I encourage you to explore SEMs as the next step in pathway mining, and references are provided at the end of this chapter if you would like to go further<a id="_idIndexMarker497" class="pcalibre calibre6 pcalibre1"/> in the field of pathway mining.</p>
<h1 id="_idParaDest-143" class="calibre5"><a id="_idTextAnchor143" class="pcalibre calibre6 pcalibre1"/>Summary</h1>
<p class="calibre3">In this chapter, we introduced causal pathways and conditional probability theory through a social science example, building a network-based data mining tool called Bayesian networks. We then simulated data from an educational pathway to implement Bayesian networks in Python. These tools provided a starting point for collecting additional data that could be analyzed to confirm hypotheses constructed from Bayesian networks through a class of models called SEMs. In the next chapter, we’ll pivot from causal pathways to look at another niche subfield in analytics: computational linguistics, where we will study languages and their relationships over long periods of time.</p>
<h1 id="_idParaDest-144" class="calibre5"><a id="_idTextAnchor144" class="pcalibre calibre6 pcalibre1"/>References</h1>
<p class="calibre3">Gladwin, T. E., Figner, B., Crone, E. A., &amp; Wiers, R. W. (2011). Addiction, adolescence, and the integration of control and motivation. <em class="italic">Developmental cognitive neuroscience, </em><em class="italic">1(4), 364-376.</em></p>
<p class="calibre3">Heckerman, D. (2008). A tutorial on learning with Bayesian networks. <em class="italic">Innovations in Bayesian networks: Theory and </em><em class="italic">applications, 33-82.</em></p>
<p class="calibre3">Hoffman, K. I. (1993). The USMLE, the NBME subject examinations, and assessment of individual academic achievement. <em class="italic">Academic Medicine, </em><em class="italic">68(10), 740-7.</em></p>
<p class="calibre3">Hoyle, R. H. (Ed.). (1995). <em class="italic">Structural equation modeling: Concepts, issues, and </em><em class="italic">applications. Sage.</em></p>
<p class="calibre3">Igolkina, A. A., &amp; Meshcheryakov, G. (2020). semopy: A Python package for structural equation modeling. <em class="italic">Structural Equation Modeling: A Multidisciplinary Journal, </em><em class="italic">27(6), 952-963.</em></p>
<p class="calibre3">Kaufman, K. A., LaSalle-Ricci, V. H., Glass, C. R., &amp; Arnkoff, D. B. (2007). <em class="italic">Passing the bar exam: Psychological, educational, and demographic predictors of success. J. Legal Educ., </em><em class="italic">57, 205.</em></p>
<p class="calibre3">Mak, K. K., Jeong, J., Lee, H. K., &amp; Lee, K. (2018). Mediating effect of internet addiction on the association between resilience and depression among Korean University students: a structural equation modeling approach. <em class="italic">Psychiatry Investigation, </em><em class="italic">15(10), 962.</em></p>
<p class="calibre3">Meca, A., Sabet, R. F., Farrelly, C. M., Benitez, C. G., Schwartz, S. J., Gonzales-Backen, M., ... &amp; Lizzi, K. M. (2017). Personal and cultural identity development in recently immigrated Hispanic adolescents: Links with psychosocial functioning. <em class="italic">Cultural diversity and ethnic minority psychology, </em><em class="italic">23(3), 348.</em></p>
<p class="calibre3">Ross, S. M. (2014). <em class="italic">Introduction to probability models. </em><em class="italic">Academic Press.</em></p>
<p class="calibre3">Scutari, M. (2009). <em class="italic">Learning Bayesian networks with the bnlearn R package. arXiv </em><em class="italic">preprint</em> arXiv:0908.3817.</p>
<p class="calibre3">Turner, M. E., &amp; Stevens, C. D. (1959). <em class="italic">The regression analysis of causal paths. Biometrics, </em><em class="italic">15(2), 236-258.</em></p>
<p class="calibre3">Violato, C., &amp; Hecker, K. G. (2007). <em class="italic">How to use structural equation modeling in medical education research: A brief guide. Teaching and learning in medicine, </em><em class="italic">19(4), 362-371.</em></p>
<p class="calibre3">Wise, R. A., &amp; Koob, G. F. (2014). The development and maintenance of drug addiction. <em class="italic">Neuropsychopharmacology, </em><em class="italic">39(2), 254-262.</em></p>
<p class="calibre3">Wu, W., Garcia, K., Chandrahas, S., Siddiqui, A., Baronia, R., &amp; Ibrahim, Y. (2021). Predictors of performance on USMLE step 1. <em class="italic">The Southwest Respiratory and Critical Care Chronicles, </em><em class="italic">9(39), 63-72.</em></p>
</div>
</body></html>