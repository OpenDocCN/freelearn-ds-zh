["```py\n$ cd $SPARK_HOME \n$ cd conf \n$ cp spark-defaults.conf.template spark-defaults.conf\n\n```", "```py\nspark.eventLog.enabled           true \nspark.eventLog.dir               <give a log directory location> \n\n```", "```py\n$ cd $SPARK_HOME \n$ ./sbin/start-master.sh\n\n```", "```py\n$ cd $SPARK_HOME \n$ ./sbin/start-slave.sh spark://Rajanarayanans-MacBook-Pro.local:7077\n\n```", "```py\n$ cd $SPARK_HOME \n$ ./bin/spark-shell --master spark://Rajanarayanans-MacBook-Pro.local:7077 \n\n```", "```py\n$ cd $SPARK_HOME \n$ ./sbin/stop-all.sh\n\n```", "```py\n$ cd $SPARK_HOME \n$ ./sbin/start-all.sh \n$ ./bin/spark-shell --master spark://Rajanarayanans-MacBook-Pro.local:7077 \n\n```", "```py\nscala> val acTransList = Array(\"SB10001,1000\", \"SB10002,1200\", \"SB10003,8000\", \"SB10004,400\", \"SB10005,300\", \"SB10006,10000\", \"SB10007,500\", \"SB10008,56\", \"SB10009,30\",\"SB10010,7000\", \"CR10001,7000\", \"SB10002,-10\") \nacTransList: Array[String] = Array(SB10001,1000, SB10002,1200, SB10003,8000, SB10004,400, SB10005,300, SB10006,10000, SB10007,500, SB10008,56, SB10009,30, SB10010,7000, CR10001,7000, SB10002,-10) \nscala> val acTransRDD = sc.parallelize(acTransList) \nacTransRDD: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[0] at parallelize at <console>:23 \nscala> val goodTransRecords = acTransRDD.filter(_.split(\",\")(1).toDouble > 0).filter(_.split(\",\")(0).startsWith(\"SB\")) \ngoodTransRecords: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at filter at <console>:25 \nscala> val highValueTransRecords = goodTransRecords.filter(_.split(\",\")(1).toDouble > 1000) \nhighValueTransRecords: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[3] at filter at <console>:27 \nscala> val badAmountLambda = (trans: String) => trans.split(\",\")(1).toDouble <= 0 \nbadAmountLambda: String => Boolean = <function1> \nscala> val badAcNoLambda = (trans: String) => trans.split(\",\")(0).startsWith(\"SB\") == false \nbadAcNoLambda: String => Boolean = <function1> \nscala> val badAmountRecords = acTransRDD.filter(badAmountLambda) \nbadAmountRecords: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[4] at filter at <console>:27 \nscala> val badAccountRecords = acTransRDD.filter(badAcNoLambda) \nbadAccountRecords: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[5] at filter at <console>:27 \nscala> val badTransRecords  = badAmountRecords.union(badAccountRecords) \nbadTransRecords: org.apache.spark.rdd.RDD[String] = UnionRDD[6] at union at <console>:33\n\n```", "```py\nscala> acTransRDD.collect() \nres0: Array[String] = Array(SB10001,1000, SB10002,1200, SB10003,8000, SB10004,400, SB10005,300, SB10006,10000, SB10007,500, SB10008,56, SB10009,30, SB10010,7000, CR10001,7000, SB10002,-10) \nscala> goodTransRecords.collect() \nres1: Array[String] = Array(SB10001,1000, SB10002,1200, SB10003,8000, SB10004,400, SB10005,300, SB10006,10000, SB10007,500, SB10008,56, SB10009,30, SB10010,7000) \nscala> highValueTransRecords.collect() \nres2: Array[String] = Array(SB10002,1200, SB10003,8000, SB10006,10000, SB10010,7000) \nscala> badAccountRecords.collect() \nres3: Array[String] = Array(CR10001,7000) \nscala> badAmountRecords.collect() \nres4: Array[String] = Array(SB10002,-10) \nscala> badTransRecords.collect() \nres5: Array[String] = Array(SB10002,-10, CR10001,7000) \n\n```", "```py\nscala> val sumAmount = goodTransRecords.map(trans => trans.split(\",\")(1).toDouble).reduce(_ + _) \nsumAmount: Double = 28486.0 \nscala> val maxAmount = goodTransRecords.map(trans => trans.split(\",\")(1).toDouble).reduce((a, b) => if (a > b) a else b) \nmaxAmount: Double = 10000.0 \nscala> val minAmount = goodTransRecords.map(trans => trans.split(\",\")(1).toDouble).reduce((a, b) => if (a < b) a else b) \nminAmount: Double = 30.0\n\n```", "```py\nscala> val combineAllElements = acTransRDD.flatMap(trans => trans.split(\",\")) \ncombineAllElements: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[10] at flatMap at <console>:25 \nscala> val allGoodAccountNos = combineAllElements.filter(_.startsWith(\"SB\")) \nallGoodAccountNos: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[11] at filter at <console>:27 \nscala> combineAllElements.collect() \nres10: Array[String] = Array(SB10001, 1000, SB10002, 1200, SB10003, 8000, SB10004, 400, SB10005, 300, SB10006, 10000, SB10007, 500, SB10008, 56, SB10009, 30, SB10010, 7000, CR10001, 7000, SB10002, -10) \nscala> allGoodAccountNos.distinct().collect() \nres14: Array[String] = Array(SB10006, SB10010, SB10007, SB10008, SB10009, SB10001, SB10002, SB10003, SB10004, SB10005)\n\n```", "```py\n$ cd $SPARK_HOME \n$ ./bin/pyspark --master spark://Rajanarayanans-MacBook-Pro.local:7077 \n\n```", "```py\n>>> from decimal import Decimal \n>>> acTransList = [\"SB10001,1000\", \"SB10002,1200\", \"SB10003,8000\", \"SB10004,400\", \"SB10005,300\", \"SB10006,10000\", \"SB10007,500\", \"SB10008,56\", \"SB10009,30\",\"SB10010,7000\", \"CR10001,7000\", \"SB10002,-10\"] \n>>> acTransRDD = sc.parallelize(acTransList) \n>>> goodTransRecords = acTransRDD.filter(lambda trans: Decimal(trans.split(\",\")[1]) > 0).filter(lambda trans: (trans.split(\",\")[0]).startswith('SB') == True) \n>>> highValueTransRecords = goodTransRecords.filter(lambda trans: Decimal(trans.split(\",\")[1]) > 1000) \n>>> badAmountLambda = lambda trans: Decimal(trans.split(\",\")[1]) <= 0 \n>>> badAcNoLambda = lambda trans: (trans.split(\",\")[0]).startswith('SB') == False \n>>> badAmountRecords = acTransRDD.filter(badAmountLambda) \n>>> badAccountRecords = acTransRDD.filter(badAcNoLambda) \n>>> badTransRecords  = badAmountRecords.union(badAccountRecords) \n>>> acTransRDD.collect() \n['SB10001,1000', 'SB10002,1200', 'SB10003,8000', 'SB10004,400', 'SB10005,300', 'SB10006,10000', 'SB10007,500', 'SB10008,56', 'SB10009,30', 'SB10010,7000', 'CR10001,7000', 'SB10002,-10'] \n>>> goodTransRecords.collect() \n['SB10001,1000', 'SB10002,1200', 'SB10003,8000', 'SB10004,400', 'SB10005,300', 'SB10006,10000', 'SB10007,500', 'SB10008,56', 'SB10009,30', 'SB10010,7000'] \n>>> highValueTransRecords.collect() \n['SB10002,1200', 'SB10003,8000', 'SB10006,10000', 'SB10010,7000'] \n>>> badAccountRecords.collect() \n['CR10001,7000'] \n>>> badAmountRecords.collect() \n['SB10002,-10'] \n>>> badTransRecords.collect() \n['SB10002,-10', 'CR10001,7000'] \n>>> sumAmounts = goodTransRecords.map(lambda trans: Decimal(trans.split(\",\")[1])).reduce(lambda a,b : a+b) \n>>> sumAmounts \nDecimal('28486') \n>>> maxAmount = goodTransRecords.map(lambda trans: Decimal(trans.split(\",\")[1])).reduce(lambda a,b : a if a > b else b) \n>>> maxAmount \nDecimal('10000') \n>>> minAmount = goodTransRecords.map(lambda trans: Decimal(trans.split(\",\")[1])).reduce(lambda a,b : a if a < b else b) \n>>> minAmount \nDecimal('30') \n>>> combineAllElements = acTransRDD.flatMap(lambda trans: trans.split(\",\")) \n>>> combineAllElements.collect() \n['SB10001', '1000', 'SB10002', '1200', 'SB10003', '8000', 'SB10004', '400', 'SB10005', '300', 'SB10006', '10000', 'SB10007', '500', 'SB10008', '56', 'SB10009', '30', 'SB10010', '7000', 'CR10001', '7000', 'SB10002', '-10'] \n>>> allGoodAccountNos = combineAllElements.filter(lambda trans: trans.startswith('SB') == True) \n>>> allGoodAccountNos.distinct().collect() \n['SB10005', 'SB10006', 'SB10008', 'SB10002', 'SB10003', 'SB10009', 'SB10010', 'SB10004', 'SB10001', 'SB10007']\n\n```", "```py\nscala> val acTransList = Array(\"SB10001,1000\", \"SB10002,1200\", \"SB10001,8000\", \"SB10002,400\", \"SB10003,300\", \"SB10001,10000\", \"SB10004,500\", \"SB10005,56\", \"SB10003,30\",\"SB10002,7000\", \"SB10001,-100\", \"SB10002,-10\") \nacTransList: Array[String] = Array(SB10001,1000, SB10002,1200, SB10001,8000, SB10002,400, SB10003,300, SB10001,10000, SB10004,500, SB10005,56, SB10003,30, SB10002,7000, SB10001,-100, SB10002,-10) \nscala> val acTransRDD = sc.parallelize(acTransList) \nacTransRDD: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[0] at parallelize at <console>:23 \nscala> val acKeyVal = acTransRDD.map(trans => (trans.split(\",\")(0), trans.split(\",\")(1).toDouble)) \nacKeyVal: org.apache.spark.rdd.RDD[(String, Double)] = MapPartitionsRDD[1] at map at <console>:25 \nscala> val accSummary = acKeyVal.reduceByKey(_ + _).sortByKey() \naccSummary: org.apache.spark.rdd.RDD[(String, Double)] = ShuffledRDD[5] at sortByKey at <console>:27 \nscala> accSummary.collect() \nres0: Array[(String, Double)] = Array((SB10001,18900.0), (SB10002,8590.0), (SB10003,330.0), (SB10004,500.0), (SB10005,56.0)) \n\n```", "```py\n>>> from decimal import Decimal \n>>> acTransList = [\"SB10001,1000\", \"SB10002,1200\", \"SB10001,8000\", \"SB10002,400\", \"SB10003,300\", \"SB10001,10000\", \"SB10004,500\", \"SB10005,56\", \"SB10003,30\",\"SB10002,7000\", \"SB10001,-100\", \"SB10002,-10\"] \n>>> acTransRDD = sc.parallelize(acTransList) \n>>> acKeyVal = acTransRDD.map(lambda trans: (trans.split(\",\")[0],Decimal(trans.split(\",\")[1]))) \n>>> accSummary = acKeyVal.reduceByKey(lambda a,b : a+b).sortByKey() \n>>> accSummary.collect() \n[('SB10001', Decimal('18900')), ('SB10002', Decimal('8590')), ('SB10003', Decimal('330')), ('SB10004', Decimal('500')), ('SB10005', Decimal('56'))] \n\n```", "```py\nscala> val acMasterList = Array(\"SB10001,Roger,Federer\", \"SB10002,Pete,Sampras\", \"SB10003,Rafael,Nadal\", \"SB10004,Boris,Becker\", \"SB10005,Ivan,Lendl\") \nacMasterList: Array[String] = Array(SB10001,Roger,Federer, SB10002,Pete,Sampras, SB10003,Rafel,Nadal, SB10004,Boris,Becker, SB10005,Ivan,Lendl) \nscala> val acBalList = Array(\"SB10001,50000\", \"SB10002,12000\", \"SB10003,3000\", \"SB10004,8500\", \"SB10005,5000\") \nacBalList: Array[String] = Array(SB10001,50000, SB10002,12000, SB10003,3000, SB10004,8500, SB10005,5000) \nscala> val acMasterRDD = sc.parallelize(acMasterList) \nacMasterRDD: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[0] at parallelize at <console>:23 \nscala> val acBalRDD = sc.parallelize(acBalList) \nacBalRDD: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[1] at parallelize at <console>:23 \nscala> val acMasterTuples = acMasterRDD.map(master => master.split(\",\")).map(masterList => (masterList(0), masterList(1) + \" \" + masterList(2))) \nacMasterTuples: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[3] at map at <console>:25 \nscala> val acBalTuples = acBalRDD.map(trans => trans.split(\",\")).map(transList => (transList(0), transList(1))) \nacBalTuples: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[5] at map at <console>:25 \nscala> val acJoinTuples = acMasterTuples.join(acBalTuples).sortByKey().map{case (accno, (name, amount)) => (accno, name,amount)} \nacJoinTuples: org.apache.spark.rdd.RDD[(String, String, String)] = MapPartitionsRDD[12] at map at <console>:33 \nscala> acJoinTuples.collect() \nres0: Array[(String, String, String)] = Array((SB10001,Roger Federer,50000), (SB10002,Pete Sampras,12000), (SB10003,Rafael Nadal,3000), (SB10004,Boris Becker,8500), (SB10005,Ivan Lendl,5000)) \n\n```", "```py\n>>> acMasterList = [\"SB10001,Roger,Federer\", \"SB10002,Pete,Sampras\", \"SB10003,Rafael,Nadal\", \"SB10004,Boris,Becker\", \"SB10005,Ivan,Lendl\"] \n>>> acBalList = [\"SB10001,50000\", \"SB10002,12000\", \"SB10003,3000\", \"SB10004,8500\", \"SB10005,5000\"] \n>>> acMasterRDD = sc.parallelize(acMasterList) \n>>> acBalRDD = sc.parallelize(acBalList) \n>>> acMasterTuples = acMasterRDD.map(lambda master: master.split(\",\")).map(lambda masterList: (masterList[0], masterList[1] + \" \" + masterList[2])) \n>>> acBalTuples = acBalRDD.map(lambda trans: trans.split(\",\")).map(lambda transList: (transList[0], transList[1])) \n>>> acJoinTuples = acMasterTuples.join(acBalTuples).sortByKey().map(lambda tran: (tran[0], tran[1][0],tran[1][1])) \n>>> acJoinTuples.collect() \n[('SB10001', 'Roger Federer', '50000'), ('SB10002', 'Pete Sampras', '12000'), ('SB10003', 'Rafael Nadal', '3000'), ('SB10004', 'Boris Becker', '8500'), ('SB10005', 'Ivan Lendl', '5000')] \n\n```", "```py\nscala> val acNameAndBalance = acJoinTuples.map{case (accno, name,amount) => (name,amount)} \nacNameAndBalance: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[46] at map at <console>:35 \nscala> val acTuplesByAmount = acBalTuples.map{case (accno, amount) => (amount.toDouble, accno)}.sortByKey(false) \nacTuplesByAmount: org.apache.spark.rdd.RDD[(Double, String)] = ShuffledRDD[50] at sortByKey at <console>:27 \nscala> acTuplesByAmount.first() \nres19: (Double, String) = (50000.0,SB10001) \nscala> acTuplesByAmount.take(3) \nres20: Array[(Double, String)] = Array((50000.0,SB10001), (12000.0,SB10002), (8500.0,SB10004)) \nscala> acBalTuples.countByKey() \nres21: scala.collection.Map[String,Long] = Map(SB10001 -> 1, SB10005 -> 1, SB10004 -> 1, SB10002 -> 1, SB10003 -> 1) \nscala> acBalTuples.count() \nres22: Long = 5 \nscala> acNameAndBalance.foreach(println) \n(Boris Becker,8500) \n(Rafel Nadal,3000) \n(Roger Federer,50000) \n(Pete Sampras,12000) \n(Ivan Lendl,5000) \nscala> val balanceTotal = sc.accumulator(0.0, \"Account Balance Total\") \nbalanceTotal: org.apache.spark.Accumulator[Double] = 0.0 \nscala> acBalTuples.map{case (accno, amount) => amount.toDouble}.foreach(bal => balanceTotal += bal) \nscala> balanceTotal.value \nres8: Double = 78500.0) \n\n```", "```py\n>>> acNameAndBalance = acJoinTuples.map(lambda tran: (tran[1],tran[2])) \n>>> acTuplesByAmount = acBalTuples.map(lambda tran: (Decimal(tran[1]), tran[0])).sortByKey(False) \n>>> acTuplesByAmount.first() \n(Decimal('50000'), 'SB10001') \n>>> acTuplesByAmount.take(3) \n[(Decimal('50000'), 'SB10001'), (Decimal('12000'), 'SB10002'), (Decimal('8500'), 'SB10004')] \n>>> acBalTuples.countByKey() \ndefaultdict(<class 'int'>, {'SB10005': 1, 'SB10002': 1, 'SB10003': 1, 'SB10004': 1, 'SB10001': 1}) \n>>> acBalTuples.count() \n5 \n>>> acNameAndBalance.foreach(print) \n('Pete Sampras', '12000') \n('Roger Federer', '50000') \n('Rafael Nadal', '3000') \n('Boris Becker', '8500') \n('Ivan Lendl', '5000') \n>>> balanceTotal = sc.accumulator(0.0) \n>>> balanceTotal.value0.0>>> acBalTuples.foreach(lambda bals: balanceTotal.add(float(bals[1]))) \n>>> balanceTotal.value \n78500.0\n\n```"]