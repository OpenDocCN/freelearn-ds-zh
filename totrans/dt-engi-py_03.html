<html><head></head><body>
		<div><h1 id="_idParaDest-27"><em class="italic"><a id="_idTextAnchor027"/>Chapter <a id="_idTextAnchor028"/>2</em>: Building Our Data Engineering Infrastructure</h1>
			<p>In the previous chapter, you learned what data engineers do and their roles and responsibilities. You were also introduced to some of the tools that they use, primarily the different types of databases, programming languages, and data pipeline creation and scheduling tools.</p>
			<p>In this chapter, you will install and configure several tools that will help you throughout the rest of this book. You will learn how to install and configure two different databases – PostgreSQL and Elasticsearch – two tools to assist in building workflows – Airflow and Apache NiFi, and two administrative tools – pgAdmin for PostgreSQL and Kibana for Elasticsearch. </p>
			<p>With these tools, you will be able to write data engineering pipelines to move data from one source to another and also be able to visualize the results. As you learn how to build pipelines, being able to see the data and how it has transformed will be useful to you in debugging any errors. As you progress, you may no longer need these tools, but other roles and users you will support may require them, so having a basic understanding of the tools will be useful.</p>
			<p>The following topics will be covered in this chapter:</p>
			<ul>
				<li>Installing and configuring Apache NiFi</li>
				<li>Installing and configuring Apache Airflow</li>
				<li>Installing and configuring Elasticsearch</li>
				<li>Installing and configuring Kibana</li>
				<li>Installing and configuring PostgreSQL</li>
				<li>Installing pgAdmin 4</li>
			</ul>
			<h1 id="_idParaDest-28"><a id="_idTextAnchor029"/>Installing and configuring Apache NiFi</h1>
			<p>Apache NiFi is<a id="_idIndexMarker057"/> the primary tool used in this book for building data engineering pipelines. NiFi allows you to build data pipelines using prebuilt processors that you can configure for your needs. You do not need to write any code to get NiFi pipelines working. It also provides a scheduler to set how frequently you would like your pipelines to run. In addition, it will handle backpressure – if one task works faster than another, you can slow down the task.</p>
			<p>To<a id="_idIndexMarker058"/> install Apache NiFi, you will need to<a id="_idIndexMarker059"/> download it from <a href="https://nifi.apache.org/download.html">https://nifi.apache.org/download.html</a>:</p>
			<ol>
				<li>By using <code>curl</code>, you can <a id="_idIndexMarker060"/>download NiFi using the following command line: <pre><strong class="bold">curl </strong><a href="https://mirrors.estointernet.in/apache/nifi/1.12.1/nifi-1.12.1-bin.tar.gz">https://mirrors.estointernet.in/apache/nifi/1.12.1/nifi-1.12.1-bin.tar.gz</a></pre></li>
				<li>Extract the NiFi files from the <code>.tar.gz</code> file using the following command:<pre><strong class="bold">tar xvzf nifi.tar.gz</strong></pre></li>
				<li>You will now have a folder named <code>nifi-1.12.1</code>. You can run NiFi by executing the following from inside the folder:<pre><strong class="bold"> bin/nifi.sh start</strong></pre></li>
				<li>If you already have Java installed and configured, when you run the status tool as shown in the following snippet, you will see a path set for <code>JAVA_HOME</code>:<pre><strong class="bold">sudo bin/nifi.sh status</strong></pre></li>
				<li>If you do not see <code>JAVA_HOME</code> set, you may need to install Java using the following command:<pre><strong class="bold">sudo apt install openjdk-11-jre-headless</strong></pre></li>
				<li>Then, you should edit <code>.bash_profile</code> to include the following line so that NiFi can find the <code>JAVA_HOME</code> variable:<pre><strong class="bold">export JAVA_HOME=/usr/lib/jvm/java11-openjdk-amd64</strong></pre></li>
				<li>Lastly, reload <code>.bash_profile</code>:<pre><strong class="bold">source .bash_profile</strong></pre></li>
				<li>When <a id="_idIndexMarker061"/>you run for the<a id="_idIndexMarker062"/> status on NiFi, you should now see a path for <code>JAVA_HOME</code>:<div><img src="img/B15739_02_01.jpg" alt="Figure 2.1 – NiFi is running&#13;&#10;"/></div><p class="figure-caption">Figure 2.1 – NiFi is running</p></li>
				<li>When NiFi is ready, which may take a minute, open your web browser and go to <code>http://localhost:8080/nifi/</code>. You should be seeing the following screen:</li>
			</ol>
			<div><div><img src="img/B15739_02_02.jpg" alt="Figure 2.2 – The NiFi GUI&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.2 – The NiFi GUI</p>
			<p>In later chapters, you will <a id="_idIndexMarker063"/>learn about many of the available configurations for NiFi, but <a id="_idIndexMarker064"/>for now, you will only change the port NiFi runs on. In <code>conf/nifi.properties</code>, change <code>nifi.web.http.port=8080</code> under the <code>web properties</code> heading to <code>9300</code>, as shown:</p>
			<pre># web properties #
nifi.web.http.port=9300</pre>
			<p>If your firewall is on, you may need to open the port:</p>
			<pre>sudo ufw allow 9300/tcp</pre>
			<p>Now, you can relaunch NiFi and view the GUI at <code>http://localhost:9300/nifi/</code>.</p>
			<h2 id="_idParaDest-29"><a id="_idTextAnchor030"/>A quick tour of NiFi</h2>
			<p>The NiFi GUI will be blank because you have not added any processors or processor groups. At the top of the screen are the component toolbar and the status bar. The component toolbar has the tools needed for building a data flow. The status bar, as the title suggests, gives an overview of<a id="_idIndexMarker065"/> the current status of your <a id="_idIndexMarker066"/>NiFi instance:</p>
			<div><div><img src="img/B15739_02_03.jpg" alt="Figure 2.3 – NiFi component toolbar and status bar&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.3 – NiFi component toolbar and status bar</p>
			<p>The tool you will use the most is the <strong class="bold">Processor</strong> tool. The other tools, from left to right, are as follows: </p>
			<ul>
				<li><strong class="bold">Input Port </strong></li>
				<li><strong class="bold">Output Port</strong></li>
				<li><strong class="bold">Processor Group</strong></li>
				<li><strong class="bold">Remote Processor Group</strong></li>
				<li><strong class="bold">Funnel</strong></li>
				<li><strong class="bold">Template</strong></li>
				<li><strong class="bold">Label</strong></li>
			</ul>
			<p>With these limited tools, you are able to build complex data flows.</p>
			<p>A NiFi data flow is made <a id="_idIndexMarker067"/>up of processors, connections, and relationships. NiFi has over 100 processors all ready for you to use. By clicking the <strong class="bold">Processor</strong> tool and dragging it on to the canvas, you will be prompted to select the processor you would like to use, as shown in the following screenshot:</p>
			<div><div><img src="img/B15739_02_04.jpg" alt="Figure 2.4 – Processors you can add to the canvas&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.4 – Processors you can add to the canvas</p>
			<p>Using the search bar, you <a id="_idIndexMarker068"/>can search for <code>GenerateFlowFile</code>. Select the processor and it will be added to the canvas. This processor will allow you to create FlowFiles with text. Drag the <code>PutFile</code>, then select the processor. This processor will save the FlowFile to disk as a file. You should now have a canvas as in the following screenshot:</p>
			<div><div><img src="img/B15739_02_05.jpg" alt="Figure 2.5 – Processors added to the canvas – with errors&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.5 – Processors added to the canvas – with errors</p>
			<p>When you add the<a id="_idIndexMarker069"/> processors, there will be a caution symbol in the left corner of the box. They have not been configured, so you will get warnings and errors. The preceding screenshot shows that the <code>PutFile</code> processor is missing the <code>Directory</code> parameter, there is no upstream connection, and the relationships for success and failure have not been handled. </p>
			<p>To configure the processor, you can either double-click on the processor or right-click and select <strong class="bold">Properties</strong>. The following screenshot shows the properties for the processor:</p>
			<div><div><img src="img/B15739_02_06.jpg" alt="Figure 2.6 – Configuring the GenerateFlowFile processor&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.6 – Configuring the GenerateFlowFile processor</p>
			<p>The following <a id="_idIndexMarker070"/>steps should be followed to configure a processor:</p>
			<ol>
				<li value="1">You must have a value set for any parameters that are bold. Each parameter has a question mark icon to help you. </li>
				<li>You can also right-click on the processes and select the option to use. </li>
				<li>For <code>GenerateFlowfile</code>, all the required parameters are already filled out.</li>
				<li>In the preceding screenshot, I have added a value to the parameter of <strong class="bold">Custom Text</strong>. To add custom properties, you can click the plus sign at the upper-right of the window. You will be prompted for a name and value. I have added my property filename and set the value to <strong class="bold">This is a file from nifi</strong>.</li>
				<li>Once configured, the yellow warning icon in the box will turn into a square (stop button).</li>
			</ol>
			<p>Now that you have configured the first processor, you need to create a connection and specify a relationship – a relationship is usually on success or failure, but the relationship types change based on<a id="_idIndexMarker071"/> the processor.</p>
			<p>To create a connection, hover over the processor box and a circle and arrow will appear: </p>
			<ol>
				<li value="1">Drag the circle to the processor underneath it (<code>PutFile</code>).<p>It will snap into place, then prompt you to specify which relationship you want to make this connection for. The only choice will be <strong class="bold">Success</strong> and it will already be checked. </p></li>
				<li>Select <code>GenerateFlowFile</code> processor and select <strong class="bold">Run</strong>. </li>
			</ol>
			<p>The red square icon will change to a green play button. You should now have a data flow as in the following screenshot:</p>
			<div><div><img src="img/B15739_02_07.jpg" alt="Figure 2.7 – Data flow half running&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.7 – Data flow half running</p>
			<p>Between the two processor boxes, you can see the queue. It will show the number of FlowFiles and the size. If you right-click on the queue, you will see a list of the FlowFiles and you can get details about each one, see their contents, and download them. The following screenshot shows the list view of<a id="_idIndexMarker072"/> FlowFiles in a queue:</p>
			<div><div><img src="img/B15739_02_08.jpg" alt="Figure 2.8 – List of FlowFiles in the queue&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.8 – List of FlowFiles in the queue</p>
			<p>You can view the details of the flow and the contents. The details view has two tables – details and attributes. From the <strong class="bold">DETAILS</strong> tab, you will see some of the NiFi metadata and have the ability to view or download the FlowFile. The <strong class="bold">ATTRIBUTES</strong> tab contains attributes assigned by NiFi and any attributes you may have created in the data pipeline. The <strong class="bold">DETAILS</strong> tab is shown in the following screenshot:</p>
			<div><div><img src="img/B15739_02_09.jpg" alt="Figure 2.9 – Details of a FlowFile&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.9 – Details of a FlowFile</p>
			<p>From the <strong class="bold">DETAILS</strong> tab, if you<a id="_idIndexMarker073"/> select to view the FlowFile, you will see the contents in the window. This works best for text-based data, but there is also an option to view the FlowFile in hex format. There is also the option to display raw or formatted text. The following screenshot shows the raw FlowFile data, which is just a simple text string:</p>
			<div><div><img src="img/B15739_02_10.jpg" alt="Figure 2.10 – Contents of a FlowFile&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.10 – Contents of a FlowFile</p>
			<p>The <code>PutFile</code> process saved the <a id="_idIndexMarker074"/>FlowFile as a file on your machine at <code>opt/nifioutput</code>. The location can be specified in the configuration of the processor. If you do not have root privileges, you can change the location to your home directory. You now have a complete data flow. It is not a very good data flow, but it will generate a file every 10 seconds and write it to disk, hence overwriting the old file. The screenshot that follows shows the directory that was configured in the processor, with the text file that was configured for the output. It also shows the contents of the file, which will match the contents of the FlowFiles generated by the <code>GenerateFlowFile</code> processor:</p>
			<div><div><img src="img/B15739_02_11.jpg" alt="Figure 2.11 – Output of the data flow&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.11 – Output of the data flow</p>
			<p>NiFi will be the primary focus of<a id="_idIndexMarker075"/> this book and you will learn much more about building data flows starting with the next chapter. The other tool you will learn about is Apache Airflow, which we will install next.</p>
			<h2 id="_idParaDest-30"><a id="_idTextAnchor031"/>PostgreSQL driver</h2>
			<p>Later in this chapter, you will install PostgreSQL. In<a id="_idIndexMarker076"/> order to connect to a PostgreSQL database using a NiFi <code>ExecuteSQL</code> processor, you need a connection pool, and that<a id="_idIndexMarker077"/> requires a <strong class="bold">Java Database Connectivity</strong> (<strong class="bold">JDBC</strong>) driver for the database you will be connecting to. This section shows you how to download that driver for use later. To download it, go to <a href="https://jdbc.postgresql.org/download.html">https://jdbc.postgresql.org/download.html</a> and download the <strong class="bold">PostgreSQL JDBC 4.2 driver, 42.2.10</strong>.</p>
			<p>Make a new folder in your NiFi installation directory named <code>drivers</code>. Move the <code>postgresql-42.2.10.jar</code> file into the folder. You will later reference this <code>jar</code> file in your NiFi processor.</p>
			<h1 id="_idParaDest-31"><a id="_idTextAnchor032"/>Installing and configuring Apache Airflow</h1>
			<p>Apache <a id="_idIndexMarker078"/>Airflow performs the same role as Apache NiFi; however, it allows <a id="_idIndexMarker079"/>you to create your data flows using pure Python. If you are a strong Python developer, this is probably an ideal tool for you. It is currently one of the most popular open source data pipeline tools. What it lacks in a polished GUI – compared to NiFi – it more than makes up for in the power and freedom to create tasks.</p>
			<p>Installing Apache Airflow can be accomplished using <code>pip</code>. But, before installing Apache Airflow, you can change the location of the Airflow install by exporting <code>AIRFLOW_HOME</code>. If you want Airflow to install to <code>opt/airflow</code>, export the <code>AIRLFOW_HOME</code> variable, as shown:</p>
			<pre>export AIRFLOW_HOME=/opt/airflow</pre>
			<p>The default location for Airflow is <code>~/airflow</code>, and for this book, this is the location I will use. The next consideration before installing Airflow is to determine which sub-packages you want to install. If you do not specify any, Airflow installs only what it needs to run. If you know that you will work with PostgreSQL, then you should install the sub-package by running the following:</p>
			<pre>apache-airflow[postgres]</pre>
			<p>There is an option to install everything using <code>all</code>, or all the databases using <code>all_dbs</code>. This book will install <code>postgreSQL</code>, <code>slack</code>, and <code>celery</code>. The following table lists all the options:</p>
			<div><div><img src="img/B15739_02_35.jpg" alt="Figure 2.12 – Table of all package command options&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.12 – Table of all package command options</p>
			<p>To install <a id="_idIndexMarker080"/>Apache Airflow, with<a id="_idIndexMarker081"/> the options for <code>postgreSQL</code>, <code>slack</code>, and <code>celery</code>, use the following command:</p>
			<pre>pip install 'apache-airflow[postgres,slack,celery]' </pre>
			<p>To run Airflow, you need to initialize the database using the following:</p>
			<pre>airflow initdb</pre>
			<p>The default database for Airflow is SQLite. This is acceptable for testing and running on a single machine, but to run in production and in clusters, you will need to change the database to something else, such as PostgreSQL.</p>
			<p class="callout-heading">No Command Airflow</p>
			<p class="callout">If the <code>airflow</code> command cannot be found, you may need to add it to your path:</p>
			<pre>export PATH=$PATH:/home/&lt;username&gt;/.local/bin</pre>
			<p>The Airflow web server runs on port <code>8080</code>, the same port as Apache NiFi. You already changed the NiFi port to <code>9300</code> in the <code>nifi.properties</code> file, so you can start the Airflow web server using the following command:</p>
			<pre>airflow webserver</pre>
			<p>If you did not change the NiFi port, or have any other processes running on port <code>8080</code>, you can specify the port for Airflow using the <code>-p</code> flag, as shown:</p>
			<pre>airflow webserver -p 8081</pre>
			<p>Next, start the<a id="_idIndexMarker082"/> Airflow scheduler so that you can run your data flows at<a id="_idIndexMarker083"/> set intervals. Run this command in a different terminal so that you do not kill the web server:</p>
			<pre>airflow scheduler</pre>
			<p>Airflow will run without the scheduler, but you will receive a warning when you launch the web server if the scheduler is not running. The warning is shown in the following screenshot:</p>
			<div><div><img src="img/B15739_02_36.jpg" alt="Figure 2.13 – Error message. The scheduler is not running&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.13 – Error message. The scheduler is not running</p>
			<p>When the scheduler runs, you will see the warning about parallelism being set to 1 because of the use of SQLite. You can ignore this warning for now, but later, you will want to be able to <a id="_idIndexMarker084"/>run more than one task at a time. The warning is shown in the<a id="_idIndexMarker085"/> following screenshot:</p>
			<div><div><img src="img/B15739_02_12.jpg" alt="Figure 2.14 – Scheduler running but warning about SQLite&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.14 – Scheduler running but warning about SQLite</p>
			<p>With the database initialized, the web server running, and the scheduler running, you can now browse to <code>http://localhost:8080</code> and see the Airflow GUI. Airflow installs several example <a id="_idIndexMarker086"/>data flows (<strong class="bold">Directed Acyclic Graphs</strong> (<strong class="bold">DAGs</strong>)) during install. You should see them on the main screen, as shown:</p>
			<div><div><img src="img/B15739_02_13.jpg" alt="Figure 2.15 – Airflow installing several examples&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.15 – Airflow installing several examples</p>
			<p>Airflow DAGs<a id="_idIndexMarker087"/> are created using code, so this section will not dive deeply <a id="_idIndexMarker088"/>into the GUI, but you will explore it more as it is relevant in later chapters. Select the first DAG – <code>example_bash_operator</code> – and you will be taken to the tree view. Click the <strong class="bold">Graph View</strong> tab and you should see the DAG shown in the following screenshot:</p>
			<div><div><img src="img/B15739_02_14.jpg" alt="Figure 2.16 – Graph view of the execute_bash_operator DAG&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.16 – Graph view of the execute_bash_operator DAG</p>
			<p>The graph view clearly <a id="_idIndexMarker089"/>shows the dependencies in the DAG and the order in which <a id="_idIndexMarker090"/>tasks will run. To watch the DAG run, switch back to <strong class="bold">Tree View</strong>. To the left of the DAG name, switch the DAG to <strong class="bold">On</strong>. Select <strong class="bold">Trigger DAG</strong> and you will be prompted whether you want to run it now. Select <strong class="bold">Yes</strong> and the page will refresh. I have run the DAG several times, and you can see the status of those runs in the following screenshot:</p>
			<div><div><img src="img/B15739_02_15.jpg" alt="Figure 2.17 – Multiple runs of the execute_bash_operator DAG&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.17 – Multiple runs of the execute_bash_operator DAG</p>
			<p>Notice that there are two <a id="_idIndexMarker091"/>completed, successful runs of the DAG and three runs that <a id="_idIndexMarker092"/>are still running, with four queued tasks in those runs waiting. The examples are great for learning how to use the Airflow GUI, but they will be cluttered later. While this does not necessarily create a problem, it will be easier to find the tasks you created without all the extras.</p>
			<p>You can remove the examples by editing the <code>airflow.cfg</code> file. Using <code>vi</code> or an editor of your choice, find the following line and change <code>True</code> to <code>False</code>:</p>
			<pre>load_examples = True</pre>
			<p>The <code>airflow.cfg</code> file is shown in the following screenshot, with the cursor at the line you need to edit:</p>
			<div><div><img src="img/B15739_02_16.jpg" alt="Figure 2.18 – Setting load_examples = False&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.18 – Setting load_examples = False</p>
			<p>Once you have edited the <code>airflow.cfg</code> file, you must shut down the web server. Once the web server has <a id="_idIndexMarker093"/>stopped, the changes to the configuration need to be loaded<a id="_idIndexMarker094"/> into the database. Remember that you set up the database earlier as the first step after <code>pip</code>, installing Airflow using the following command:</p>
			<pre>airflow initdb</pre>
			<p>To make changes to the database, which is what you want to do after changing the <code>airflow.cfg</code> file, you need to reset it. You can do that using the following snippet:</p>
			<pre>airflow resetdb</pre>
			<p>This will load in the changes from <code>airflow.cfg</code> to the metadata database. Now, you can restart the web server. When you open the GUI at <code>http://localhost:8080</code>, it should be empty, as shown in the following screenshot:</p>
			<div><div><img src="img/B15739_02_17.jpg" alt="Figure 2.19 – Clean Airflow. Not a single DAG in sight&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.19 – Clean Airflow. Not a single DAG in sight</p>
			<p>Airflow is clean and ready to load in the DAGs that you will create in the next chapter.</p>
			<h1 id="_idParaDest-32"><a id="_idTextAnchor033"/>Installing and configuring Elasticsearch</h1>
			<p>Elasticsearch is a<a id="_idIndexMarker095"/> search engine. In this book, you will use it as a NoSQL database. You will move<a id="_idIndexMarker096"/> data both to and from Elasticsearch to other locations. To download Elasticsearch, take the following steps:</p>
			<ol>
				<li value="1">Use <code>curl</code> to download the files, as shown:<pre><strong class="bold">curl https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.6.0-darwin-x86_64.tar.gz --output elasticsearch.tar.gz</strong></pre></li>
				<li>Extract the files using the following command:<pre><strong class="bold">tar xvzf elasticsearch.tar.gz</strong></pre></li>
				<li>You can edit the <code>config/elasticsearch.yml</code> file to name your node and cluster. Later in this book, you will set up an Elasticsearch cluster with multiple nodes. For now, I have changed the following properties:<pre><strong class="bold">cluster.name: DataEngineeringWithPython </strong>
<strong class="bold">node.name: OnlyNode</strong></pre></li>
				<li>Now, you can start Elasticsearch. To start Elasticsearch, run the following: <pre><strong class="bold">bin/elasticsearch</strong></pre></li>
				<li>Once Elasticsearch<a id="_idIndexMarker097"/> has started, you can see the results<a id="_idIndexMarker098"/> at <code>http://localhost:9200</code>. You should see the following output:</li>
			</ol>
			<div><div><img src="img/B15739_02_18.jpg" alt="Figure 2.20 – Elasticsearch running&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.20 – Elasticsearch running</p>
			<p>Now that you have a NoSQL database running, you will need a relational database as well.</p>
			<h1 id="_idParaDest-33"><a id="_idTextAnchor034"/>Installing and configuring Kibana</h1>
			<p>Elasticsearch does not <a id="_idIndexMarker099"/>ship with a GUI, but rather an API. To add a GUI to Elasticsearch, you can<a id="_idIndexMarker100"/> use Kibana. By using Kibana, you can better manage and interact with Elasticsearch. Kibana will allow you to access the Elasticsearch API in a GUI, but more importantly, you can use it to build visualizations and dashboards of your data held in Elasticsearch. To install Kibana, take the following steps:</p>
			<ol>
				<li value="1">Using <code>wget</code>, add the key:<pre><strong class="bold">wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -</strong></pre></li>
				<li>Then, add the repository along with it:<pre><strong class="bold">echo "deb https://artifacts.elastic.co/packages/7.x/apt stable main" | sudo tee -a /etc/apt/sources.list.d/elastic-7.x.list</strong></pre></li>
				<li>Lastly, update <code>apt</code> and install Kibana:<pre><strong class="bold">sudo apt-get update</strong>
<strong class="bold">sudo apt-get install kibana</strong></pre></li>
				<li>The configuration files for Kibana are located in <code>etc/kibana</code> and the application is in <code>/usr/share/kibana/bin</code>. To launch Kibana, run the following:<pre><strong class="bold">bin/kibana</strong></pre></li>
				<li>When Kibana is ready, browse to <code>http://localhost:5601</code>. Kibana will look for any instance of Elasticsearch running on <code>localhost</code> at port <code>9200</code>. This is where you installed Elasticsearch earlier, and also why you did not change the port in the configuration. When Kibana opens, you will be asked to choose between <strong class="bold">Try our sample data</strong> and <strong class="bold">Explore on my own</strong>, as shown:</li>
			</ol>
			<div><div><img src="img/B15739_02_19.jpg" alt="Figure 2.21 – First launch of Kibana&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.21 – First launch of Kibana</p>
			<p><strong class="bold">Explore on my own</strong> will take<a id="_idIndexMarker101"/> you to the main Kibana screen, but since you have not <a id="_idIndexMarker102"/>created an Elasticsearch index and have not loaded any data, the application will be blank.</p>
			<p>To see the different tools available in Kibana, select <strong class="bold">Try our sample data</strong>, and choose the e-commerce data. The following screenshot shows the options for <strong class="bold">Load our Sample Data</strong>:</p>
			<div><div><img src="img/B15739_02_20.jpg" alt="Figure 2.22 – Load sample data and visualizations&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.22 – Load sample data and visualizations</p>
			<p>Once you have loaded the<a id="_idIndexMarker103"/> sample data, select the <strong class="bold">Discover</strong> icon. From the <strong class="bold">Discover</strong> section, you <a id="_idIndexMarker104"/>are able to look at records in the data. If there are dates, you will see a bar chart of counts on given time ranges. You can select a bar or change the date ranges from this tab. Selecting a record will show the data as a table or the JSON representation of the document. You can also run queries on the data from this tab and save them as objects to be used later in visualizations. The following screenshot shows the main <strong class="bold">Discover</strong> screen:</p>
			<div><div><img src="img/B15739_02_21.jpg" alt="Figure 2.23 – The Discover tab&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.23 – The Discover tab</p>
			<p>From the data available in the <strong class="bold">Discover</strong> tab or from a saved query, you can create visualizations. The<a id="_idIndexMarker105"/> visualizations include bar charts – horizontal and vertical, pie/donut<a id="_idIndexMarker106"/> charts, counts, markdown, heatmaps, and even a map widget to handle geospatial data. The e-commerce data contains geospatial data at the country level, but maps can also handle coordinates. The following screenshot shows a region map of the e-commerce data:</p>
			<div><div><img src="img/B15739_02_22.jpg" alt="Figure 2.24 – A map visualization&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.24 – A map visualization</p>
			<p>When you have created <a id="_idIndexMarker107"/>several visualizations, from a single index or from multiple <a id="_idIndexMarker108"/>Elasticsearch indices, you can add them to a dashboard. Kibana allows you to load widgets using data from multiple indices. When you query or filter within the dashboard, as long as the field name exists in each of the indices, all of the widgets will update. The following screenshot shows a dashboard, made up of multiple visualizations of the e-commerce data:</p>
			<div><div><img src="img/B15739_02_23.jpg" alt="Figure 2.25 – A dashboard using multiple widgets from the e-commerce data&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.25 – A dashboard using multiple widgets from the e-commerce data</p>
			<p>The <strong class="bold">Developer Tools</strong> tab <a id="_idIndexMarker109"/>comes in handy to quickly test Elasticsearch queries before you<a id="_idIndexMarker110"/> implement them in a data engineering pipeline. From this tab, you can create indices and data, execute queries to filter, search, or aggregate data. The results are displayed in the main window. The following screenshot shows a record being added to an index, then a search happening for a specific ID:</p>
			<div><div><img src="img/B15739_02_24.jpg" alt="Figure 2.26 – A query on a single test record&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.26 – A query on a single test record</p>
			<p>Now that you have installed<a id="_idIndexMarker111"/> Elasticsearch and Kibana, the next two sections will walk you<a id="_idIndexMarker112"/> through installing PostgreSQL and pgAdmin 4. After that, you will have both a SQL and a NoSQL database to explore. </p>
			<h1 id="_idParaDest-34"><a id="_idTextAnchor035"/>Installing and configuring PostgreSQL</h1>
			<p>PostgreSQL is an<a id="_idIndexMarker113"/> open source relational database. It compares to Oracle or<a id="_idIndexMarker114"/> Microsoft SQL Server. PostgreSQL also has a plugin – postGIS – which allows spatial capabilities in PostgreSQL. In this book, it will be the relational database of choice. PostgreSQL can be installed on Linux as a package:</p>
			<ol>
				<li value="1">For a Debian-based system, use <code>apt-get</code>, as shown:<pre><strong class="bold">sudo apt-get install postgresql-11</strong></pre></li>
				<li>Once the packages have finished installing, you can start the database with the following:<pre><strong class="bold">sudo pg_ctlcluster 11 main start</strong></pre></li>
				<li>The default user, <code>postgres</code>, does not have a password. To add one, connect to the default database:<pre><strong class="bold">sudo -u postgres psql</strong></pre></li>
				<li>Once connected, you can alter the user and assign a password:<pre><strong class="bold">ALTER USER postgres PASSWORD ‚postgres';</strong></pre></li>
				<li>To create a <a id="_idIndexMarker115"/>database, you can<a id="_idIndexMarker116"/> enter the following command:<pre><strong class="bold">sudo -u postgres createdb dataengineering</strong></pre></li>
			</ol>
			<p>Using the command line is fast, but sometimes, a GUI makes life easier. PostgreSQL has an administration tool – pgAdmin 4. </p>
			<h1 id="_idParaDest-35"><a id="_idTextAnchor036"/>Installing pgAdmin 4</h1>
			<p>pgAdmin 4 will make managing <a id="_idIndexMarker117"/>PostgreSQL much easier if you are new to relational databases. The web-based GUI will allow you to view your data and allow you to visually create tables. To install pgAdmin 4, take the following steps:</p>
			<ol>
				<li value="1">You need to add the repository to Ubuntu. The following commands should be added to the repository:<pre><strong class="bold">wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -</strong>
<strong class="bold">sudo sh -c 'echo "deb http://apt.postgresql.org/pub/repos/apt/ `lsb_release -cs`-pgdg main" &gt;&gt; /etc/apt/sources.list.d/pgdg.list'</strong>
<strong class="bold">sudo apt update</strong>
<strong class="bold">sudo apt install pgadmin4 pgadmin4-apache2 -y</strong></pre></li>
				<li>You will be prompted to enter an email address for a username and then for a password. You should see the following screen:<div><img src="img/B15739_02_25.jpg" alt="Figure 2.27 – Creating a user for pgAdmin 4&#13;&#10;"/></div><p class="figure-caption">Figure 2.27 – Creating a user for pgAdmin 4</p></li>
				<li>When the install has <a id="_idIndexMarker118"/>completed, you can browse to <code>http://localhost/pgadmin4</code> and you will be presented with the login screen, as shown in the following screenshot. Enter the credentials for the user you just created during the install:</li>
			</ol>
			<div><div><img src="img/B15739_02_26.jpg" alt="Figure 2.28 – Logging in to pgAdmin 4&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.28 – Logging in to pgAdmin 4</p>
			<p>Once you have logged in, you<a id="_idIndexMarker119"/> can manage your databases from the GUI. The next section will give you a brief tour of pgAdmin 4.</p>
			<h2 id="_idParaDest-36"><a id="_idTextAnchor037"/>A tour of pgAdmin 4</h2>
			<p>After you log in to pgAdmin 4, you<a id="_idIndexMarker120"/> will see a dashboard with a server icon on the left side. There are currently no servers configured, so you will want to add the server you installed earlier in this chapter.</p>
			<p>Click on the <strong class="bold">Add new server</strong> icon on the dashboard. You will see a pop-up window. Add the information for your PostgreSQL instance, as shown in the following screenshot:</p>
			<div><div><img src="img/B15739_02_27.jpg" alt="Figure 2.29 – Adding a new server&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.29 – Adding a new server</p>
			<p>Once you add the server, you <a id="_idIndexMarker121"/>can expand the server icon and you should see the database you created earlier – <code>dataengineering</code>. Expand the <code>dataengineering</code> database, then <code>schemas</code>, then <code>public</code>. You will be able to right-click on <strong class="bold">Tables</strong> to add a table to the database, as shown in the following screenshot:</p>
			<div><div><img src="img/B15739_02_28.jpg" alt="Figure 2.30 – Creating a table&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.30 – Creating a table</p>
			<p>To populate the table with <a id="_idIndexMarker122"/>data, name the table, then select the <strong class="bold">Columns</strong> tab. Create a table with some information about people. The table is shown in the following screenshot:</p>
			<div><div><img src="img/B15739_02_29.jpg" alt="Figure 2.31 – Table data&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.31 – Table data</p>
			<p>In the next chapter, you will use Python to populate this table with data using the <code>faker</code> library.</p>
			<h1 id="_idParaDest-37"><a id="_idTextAnchor038"/>Summary</h1>
			<p>In this chapter, you learned how to install and configure many of the tools used by data engineers. Having done so, you now have a working environment in which you can build data pipelines. In production, you would not run all these tools on a single machine, but for the next few chapters, this will help you learn and get started quickly. You now have two working databases – Elasticsearch and PostgreSQL – as well as two tools for building data pipelines – Apache NiFi and Apache Airflow.</p>
			<p>In the next chapter, you will start to use Apache NiFi and Apache Airflow (Python) to connect to files, as well as Elasticsearch and PostgreSQL. You will build your first pipeline in NiFi and Airflow to move a CSV to a database.</p>
		</div>
	</body></html>