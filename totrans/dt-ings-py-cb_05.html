<html><head></head><body>
<div><div><h1 class="chapter-number" id="_idParaDest-161"><a id="_idTextAnchor161"/>5</h1>
<h1 id="_idParaDest-162"><a id="_idTextAnchor162"/>Ingesting Data from Structured and Unstructured Databases</h1>
<p>Nowadays, we can store and retrieve data from multiple sources, and the optimal storage method depends on the type of information being processed. For example, most APIs make data available in an unstructured format as this allows the sharing of data of multiple formats (for example, audio, video, and image) and has low storage costs via the use of data lakes. However, if we want to make quantitative data available for use with several tools to support analysis, then the most reliable option might be structured data.</p>
<p>Ultimately, whether you are a data analyst, scientist, or engineer, it is essential to understand how to manage both structured and unstructured data.</p>
<p>In this chapter, we will cover the following recipes:</p>
<ul>
<li>Configuring a JDBC connection</li>
<li>Ingesting data from a JDBC database using SQL</li>
<li>Connecting to a NoSQL database (MongoDB)</li>
<li>Creating our NoSQL table in MongoDB</li>
<li>Ingesting data from MongoDB using PySpark</li>
</ul>
<h1 id="_idParaDest-163"><a id="_idTextAnchor163"/>Technical requirements</h1>
<p>You can find the code from this chapter in the GitHub repository at <a href="https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook">https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook</a>.</p>
<p>Using the <strong class="bold">Jupyter Notebook</strong> is not mandatory but allows us to explore the code interactively. Since we will execute both Python and PySpark code, Jupyter can help us to understand the scripts better. Once you have Jupyter installed, you can execute it using the following line:</p>
<pre class="source-code">
$ jupyter notebook</pre>
<p>It is recommended to create a separate folder to store the Python files or notebooks we will cover in this chapter; however, feel free to organize it in the most appropriate way for you.</p>
<h1 id="_idParaDest-164"><a id="_idTextAnchor164"/>Configuring a JDBC connection</h1>
<p>Working with different <a id="_idIndexMarker353"/>systems brings the challenge of finding an efficient way to connect the systems. An adaptor, or a driver, is the solution to this communication problem, creating a bridge to translate information from one system to another.</p>
<p><strong class="bold">JDBC</strong>, or <strong class="bold">Java Database Connectivity</strong>, is used to facilitate communication between Java-based <a id="_idIndexMarker354"/>systems and databases. This recipe covers configuring JDBC in SparkSession to connect to a PostgreSQL database, using best practices as always.</p>
<h2 id="_idParaDest-165"><a id="_idTextAnchor165"/>Getting ready</h2>
<p>Before configuring SparkSession, we need to download the <code>.jars</code> file (Java Archive). You can do this at <a href="https://jdbc.postgresql.org/">https://jdbc.postgresql.org/</a> on the PostgreSQL official site.</p>
<p>Select <strong class="bold">Download</strong>, and you will be redirected to another page:</p>
<div><div><img alt="Figure 5.1 – PostgreSQL JDBC home page" height="796" src="img/Figure_5.1_B19453.jpg" width="1029"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.1 – PostgreSQL JDBC home page</p>
<p>Then, select the <strong class="bold">Java 8 </strong><strong class="bold">Download</strong> button.</p>
<p>Keep this <code>.jar</code> file somewhere safe, as you will need it later. I suggest keeping it inside the folder where your<a id="_idIndexMarker355"/> code is.</p>
<p>For the PostgreSQL database, you can use a Docker image or the instance we created on Google Cloud in <em class="italic">Chapter 4</em>. If you opt for the Docker image, ensure it is up and running.</p>
<p>The final preparatory step for this recipe is to import a dataset to be used. We will use the <code>word_population.csv</code> file (which you can find in the GitHub repository of this book, at <a href="https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_5/datasets">https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_5/datasets</a>). Import it using DBeaver or any other SQL IDE of your choice. We will use this dataset with SQL in the <em class="italic">Ingesting data from a JDBC database using SQL </em>recipe later in this chapter.</p>
<p>To import data into DBeaver, create a table with the name of your choice under the Postgres database. I chose to give my table the exact name of the CSV file. You don’t need to insert any columns for now.</p>
<p>Then, right-click on the table and select <strong class="bold">Import Data</strong>, as shown in the following screenshot:</p>
<div><div><img alt="Figure 5.2 – Importing data on a table using DBeaver" height="671" src="img/Figure_5.2_B19453.jpg" width="495"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.2 – Importing data on a table using DBeaver</p>
<p>A new window<a id="_idIndexMarker356"/> will open, showing the options to use a CSV file or a database table. Select <strong class="bold">CSV</strong> and then <strong class="bold">Next</strong>, as follows:</p>
<div><div><img alt="Figure 5.3 – Importing CSV files into a table using DBeaver" height="628" src="img/Figure_5.3_B19453.jpg" width="818"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.3 – Importing CSV files into a table using DBeaver</p>
<p>A new window will open where you can select the file. Choose the <code>world_population.csv</code> file and select the <strong class="bold">Next</strong> button, leaving the default settings shown as follows:</p>
<div><div><img alt="Figure 5.4 – CSV file successfully imported into the world_population table" height="786" src="img/Figure_5.4_B19453.jpg" width="780"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.4 – CSV file successfully imported into the world_population table</p>
<p>If all succeeds, you<a id="_idIndexMarker357"/> should be able to see the <code>world_population</code> table populated with the columns and data:</p>
<div><div><img alt="Figure 5.5 – The world_population table populated with data from the CSV" height="780" src="img/Figure_5.5_B19453.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.5 – The world_population table populated with data from the CSV</p>
<h2 id="_idParaDest-166"><a id="_idTextAnchor166"/>How to do it…</h2>
<p>I will use a Jupyter <a id="_idIndexMarker358"/>notebook to insert and execute the code to make this exercise more dynamic. Here is how we do it:</p>
<ol>
<li><code>SparkSession</code>, we will need an additional class called <code>SparkConf</code> to set our new configuration:<pre class="source-code">
from pyspark.conf import SparkConf
from pyspark.sql import SparkSession</pre></li>
<li><code>SparkConf(</code>), which we instantiated, we can set the path to the <code>.jar</code> with <code>spark.jars</code>:<pre class="source-code">
conf = SparkConf()
conf.set('spark.jars', /path/to/your/postgresql-42.5.1.jar')</pre></li>
</ol>
<p>You will see a <code>SparkConf</code> object created, as shown in the following output:</p>
<div><div><img alt="Figure 5.6 – SparkConf object" height="25" src="img/Figure_5.6_B19453.jpg" width="434"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.6 – SparkConf object</p>
<ol>
<li value="3"><code>SparkSession</code> and create it:<pre class="source-code">
spark = SparkSession.builder \
        .config(conf=conf) \
        .master("local") \
        .appName("Postgres Connection Test") \
        .getOrCreate()</pre></li>
</ol>
<p>If a warning message appears as in the following screenshot, you can ignore it:</p>
<div><div><img alt="Figure 5.7 – SparkSession initialization warning messages" height="159" src="img/Figure_5.7_B19453.jpg" width="1090"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.7 – SparkSession initialization warning messages</p>
<ol>
<li value="4"><strong class="bold">Connecting to our database</strong>: Finally, we can connect to the PostgreSQL database by passing the required credentials including host, database name, username, and password as follows:<pre class="source-code">
df= spark.read.format("jdbc") \
    .options(url="jdbc:postgresql://localhost:5432/postgres",
             dbtable="world_population",
             user="root",
             password="root",
             driver="org.postgresql.Driver") \
    .load()</pre></li>
</ol>
<p>If the credentials are correct, we should expect no output here.</p>
<ol>
<li value="5"><code>.printSchema()</code>, it is possible now to see the table columns:<pre class="source-code">
df.printSchema()</pre></li>
</ol>
<p>Executing the code will show the following output:</p>
<div><div><img alt="Figure 5.8 – DataFrame of the world_population schema" height="313" src="img/Figure_5.8_B19453.jpg" width="491"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.8 – DataFrame of the world_population schema</p>
<h2 id="_idParaDest-167"><a id="_idTextAnchor167"/>How it works…</h2>
<p>We can observe that PySpark (and Spark) require<a id="_idIndexMarker359"/> additional configuration to create a connection with a database. In this recipe, using the PostgreSQL <code>.jars</code> file is essential to make it work.</p>
<p>Let’s understand what kind of configuration Spark requires by looking at our code:</p>
<pre class="source-code">
conf = SparkConf()
conf.set('spark.jars', '/path/to/your/postgresql-42.5.1.jar')</pre>
<p>We started by instantiating the <code>SparkConf()</code> method, responsible for defining configurations used in SparkSession. After instantiating the class, we used the <code>set()</code> method to pass a key-value pair parameter: <code>spark.jars</code>. If more than one <code>.jars</code> file was used, the paths could be passed on the value parameter separated by commas. It is also possible to define more than one <code>conf.set()</code>method; they just need to be included one after the other.</p>
<p>It is on the second line of SparkSession where the set of configurations is passed, as you can see in the following code:</p>
<pre class="source-code">
spark = SparkSession.builder \
        .config(conf=conf) \
        .master("local") \
    (...)</pre>
<p>Then, with our SparkSession<a id="_idIndexMarker360"/> instantiated, we can use it to read our database, as you can see in the following code:</p>
<pre class="source-code">
df= spark.read.format("jdbc") \
    .options(url="jdbc:postgresql://localhost:5432/postgres",
             dbtable="world_population",
             user="root",
             password="root",
             driver="org.postgresql.Driver") \
    .load()</pre>
<p>Since we are handling a third-party application, we must set the format for reading the output using the <code>.format()</code> method. The <code>.options()</code> method will carry the authentication values and the driver.</p>
<p class="callout-heading">Note</p>
<p class="callout">With time you will observe that there are a few diverse ways to declare the <code>.options()</code> key-value pairs. For example, another frequently used format is .<code>options("driver", "org.postgresql.Driver)</code>. Both ways are correct depending on the <em class="italic">taste</em> of the developer.</p>
<h2 id="_idParaDest-168"><a id="_idTextAnchor168"/>There’s more…</h2>
<p>This recipe covered how to use a JDBC driver, and the<a id="_idIndexMarker361"/> same logic applies to <strong class="bold">Open Database Connectivity</strong> (<strong class="bold">ODBC</strong>). However, determining the criteria for using JDBC or ODBC requires understanding which data source we are ingesting data from.</p>
<p>The ODBC connection in Spark is usually associated with Spark Thrift Server, a Spark SQL extension from Apache HiveServer2 that allows users to execute SQL queries in <strong class="bold">Business Intelligence </strong>(<strong class="bold">BI</strong>) tools <a id="_idIndexMarker362"/>such as MS PowerBI or Tableau. See the following diagram for an outline of this relationship:</p>
<div><div><img alt="Figure 5.9 – Spark Thrift architecture, provided by Cloudera documentation (https://docs.cloudera.com/HDPDocuments/HDP3/HDP-3.1.5/developing-spark-applications/content/using_spark_sql.xhtml)" height="491" src="img/Figure_5.9_B19453.jpg" width="799"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.9 – Spark Thrift architecture, provided by Cloudera documentation (<a href="https://docs.cloudera.com/HDPDocuments/HDP3/HDP-3.1.5/developing-spark-applications/content/using_spark_sql.xhtml">https://docs.cloudera.com/HDPDocuments/HDP3/HDP-3.1.5/developing-spark-applications/content/using_spark_sql.xhtml</a>)</p>
<p>By contrast to JDBC, ODBC is used in real-life projects that are smaller and more specific to certain system integrations. It also requires the use of another Python library called <code>pyodbc</code>. You can read more about it at <a href="https://kontext.tech/article/290/connect-to-sql-server-in-spark-pyspark">https://kontext.tech/article/290/connect-to-sql-server-in-spark-pyspark</a>.</p>
<h3>Debugging connection errors</h3>
<p>PySpark errors can be <a id="_idIndexMarker363"/>very confusing and lead to misinterpretations. It happens because the errors are often related to a problem on the JVM, and Py4J (a Python interpreter that communicates dynamically with the JVM) consolidates the message with other Python errors that may have occurred.</p>
<p>Some error messages are prevalent and can easily be identified when managing database connections. Let’s take a look at an error that occurred when using the following code:</p>
<pre class="source-code">
df= spark.read.format("jdbc") \
    .options(url="jdbc:postgresql://localhost:5432/postgres",
             dbtable="world_population",
             user="root",
             password="root") \
    .load()</pre>
<p>Here is the error <a id="_idIndexMarker364"/>message that resulted:</p>
<div><div><img alt="Figure 5.10 – Py4JJavaError message" height="439" src="img/Figure_5.10_B19453.jpg" width="996"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.10 – Py4JJavaError message</p>
<p>In the first line, we see <code>Py4JJavaError</code> informing us of an error when calling the load function. Continuing to the second line, we can see the message: <code>java.sql.SQLException: No suitable driver</code>. It informs us that even though the <code>.jars</code> file is configured and set, PySpark doesn’t know which drive to use to load data from PostgreSQL. This can be easily fixed by adding the <code>driver</code> parameter under <code>.options()</code>. Refer to the following code:</p>
<pre class="source-code">
df= spark.read.format("jdbc") \
    .options(url="jdbc:postgresql://localhost:5432/postgres",
             dbtable="world_population",
             user="root",
             password="root",
             driver="org.postgresql.Driver") \
    .load()</pre>
<h2 id="_idParaDest-169"><a id="_idTextAnchor169"/>See also</h2>
<p>Find more about<a id="_idIndexMarker365"/> Spark Thrift <a id="_idIndexMarker366"/>Server at <a href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/content/spark-sql-thrift-server.xhtml">https://jaceklaskowski.gitbooks.io/mastering-spark-sql/content/spark-sql-thrift-server.xhtml</a>.</p>
<h1 id="_idParaDest-170"><a id="_idTextAnchor170"/>Ingesting data from a JDBC database using SQL</h1>
<p>With the connection tested<a id="_idIndexMarker367"/> and SparkSession configured, the next step is to ingest the data from PostgreSQL, filter it, and save it in an analytical <a id="_idIndexMarker368"/>format called a Parquet file. Don’t worry about how Parquet files work for now; we will cover it in the following chapters.</p>
<p>This recipe aims to use the connection we created with our JDBC database and ingest the data from the <code>world_population</code> table.</p>
<h2 id="_idParaDest-171"><a id="_idTextAnchor171"/>Getting ready</h2>
<p>This recipe will use the same dataset and code as the <em class="italic">Configuring a JDBC connection</em> recipe to connect to the PostgreSQL database. Ensure your Docker container is running or your PostgreSQL server is up.</p>
<p>This recipe continues from the content presented in <em class="italic">Configuring a JDBC connection</em>. We will now learn how to ingest the data inside the Postgres database.</p>
<h2 id="_idParaDest-172"><a id="_idTextAnchor172"/>How to do it…</h2>
<p>Following on from our previous code, let’s read the data in our database as follows:</p>
<ol>
<li><code>world_population</code> table:<pre class="source-code">
df= spark.read.format("jdbc") \
    .options(url="jdbc:postgresql://localhost:5432/postgres",
             dbtable="world_population",
             user="root",
             password="root",
             driver="org.postgresql.Driver") \
    .load()</pre></li>
<li><strong class="bold">Creating a TempView</strong>: Using the<a id="_idIndexMarker369"/> exact name of our table (for organization purposes), we create a temporary view in the Spark<a id="_idIndexMarker370"/> default database from the DataFrame:<pre class="source-code">
df.createOrReplaceTempView("world_population")</pre></li>
</ol>
<p>There is no output expected here.</p>
<ol>
<li value="3"><code>spark</code> variable:<pre class="source-code">
spark.sql("select * from world_population").show(3)</pre></li>
</ol>
<p>Depending on the size of your monitor, the output may look confusing, as follows:</p>
<div><div><img alt="Figure 5.11 – world_population view using Spark SQL" height="388" src="img/Figure_5.11_B19453.jpg" width="1001"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.11 – world_population view using Spark SQL</p>
<ol>
<li value="4"><strong class="bold">Filtering data</strong>: Using a SQL <a id="_idIndexMarker371"/>statement, let’s filter only the South American countries in our DataFrame:<pre class="source-code">
south_america = spark.sql("select * from world_population where continent = 'South America' ")</pre></li>
</ol>
<p>Since we attribute the results to a<a id="_idIndexMarker372"/> variable, there is no output.</p>
<ol>
<li value="5"><code>.toPandas()</code> function to bring a more user-friendly view:<pre class="source-code">
south_america.toPandas()</pre></li>
</ol>
<p>This is how the result appears:</p>
<div><div><img alt="Figure 5.12 – south_america countries with toPandas() visualization" height="648" src="img/Figure_5.12_B19453.jpg" width="1000"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.12 – south_america countries with toPandas() visualization</p>
<ol>
<li value="6"><strong class="bold">Saving our work</strong>: Now, we <a id="_idIndexMarker373"/>can save our filtered data as follows:<pre class="source-code">
south_america.write.parquet('south_america_population')</pre></li>
</ol>
<p>Looking at your script’s folder, you <a id="_idIndexMarker374"/>should see a folder named <code>south_america_population</code>. Inside, you should see the following output:</p>
<div><div><img alt="Figure 5.13 – south_america data in the Parquet file" height="299" src="img/Figure_5.13_B19453.jpg" width="813"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.13 – south_america data in the Parquet file</p>
<p>This is our filtered and ingested DataFrame in an analytical format.</p>
<h2 id="_idParaDest-173"><a id="_idTextAnchor173"/>How it works…</h2>
<p>A significant advantage of working with Spark is the possibility of using SQL statements to filter and query data from a DataFrame. It allows data analytics and BI teams to help the data engineers by<a id="_idIndexMarker375"/> handling queries. This helps to build analytical data and insert it into data warehouses.</p>
<p>Nevertheless, there are <a id="_idIndexMarker376"/>some considerations we need to take to execute a SQL statement properly. One of them is using <code>.createOrReplaceTempView()</code>, as seen in this line of code:</p>
<pre class="source-code">
df.createOrReplaceTempView("world_population")</pre>
<p>Behind the scenes, this temporary view will work as a SQL table and organize the data from the DataFrame without needing physical files.</p>
<p>Then we used the instantiated <code>SparkSession</code> variable to execute the SQL statements. Note that the name of the table is the same as the temporary view:</p>
<pre class="source-code">
spark.sql("select * from world_population").show(3)</pre>
<p>After doing the SQL queries we required, we proceeded to save our files using the .<code>write()</code> method, as follows:</p>
<pre class="source-code">
south_america.write.parquet('south_america_population')</pre>
<p>The parameter inside the <code>parquet()</code> method defines the file’s path and name. Several other configurations are available when writing Parquet files, which we will cover later, in <a href="B19453_07.xhtml#_idTextAnchor227"><em class="italic">Chapter 7</em></a>.</p>
<h2 id="_idParaDest-174"><a id="_idTextAnchor174"/>There’s more…</h2>
<p>Although we used a temporary view to make our SQL statements, it is also possible to use the filtering and aggregation functions from the DataFrame. Let’s use the example from this recipe by filtering only the South American countries:</p>
<pre class="source-code">
df.filter(df['continent'] == 'South America').show(10)</pre>
<p>You should see the following output:</p>
<div><div><img alt="Figure 5.14 – South American countries filtered using DataFrame operations" height="737" src="img/Figure_5.14_B19453.jpg" width="1004"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.14 – South American countries filtered using DataFrame operations</p>
<p>It is essential to<a id="_idIndexMarker377"/> understand that not all SQL functions can be used <a id="_idIndexMarker378"/>as DataFrame operations. You can see more practical examples of filtering and aggregation functions using DataFrame operations at <a href="https://spark.apache.org/docs/2.2.0/sql-programming-guide.xhtml">https://spark.apache.org/docs/2.2.0/sql-programming-guide.xhtml</a>.</p>
<h2 id="_idParaDest-175"><a id="_idTextAnchor175"/>See also</h2>
<p><em class="italic">TowardsDataScience</em> has a fantastic blog post about SQL functions using PySpark, at <a href="https://towardsdatascience.com/pyspark-and-sparksql-basics-6cb4bf967e53">https://towardsdatascience.com/pyspark-and-sparksql-basics-6cb4bf967e53</a>.</p>
<h1 id="_idParaDest-176"><a id="_idTextAnchor176"/>Connecting to a NoSQL database (MongoDB)</h1>
<p>MongoDB is <a id="_idIndexMarker379"/>an open source, unstructured, document-oriented database made in C++. It is well known in the data world for its scalability, flexibility, and speed.</p>
<p>As someone who will work with data (or maybe already does), it is essential to know how to explore a MongoDB (or any other unstructured) database. MongoDB has some peculiarities, which we will explore practically here.</p>
<p>In this recipe, you will learn how to <a id="_idIndexMarker380"/>create a connection to access MongoDB documents via Studio 3T Free, a MongoDB GUI.</p>
<h2 id="_idParaDest-177"><a id="_idTextAnchor177"/>Getting ready</h2>
<p>To start our work with this robust database, first, we need to install and create a MongoDB server on our local machine. We already configured a MongoDB Docker container in <a href="B19453_01.xhtml#_idTextAnchor022"><em class="italic">Chapter 1</em></a>, so let’s get it up and running. You can do this using Docker Desktop or via the command line using the following command:</p>
<pre class="source-code">
my-project/mongo-local$ docker run \
--name mongodb-local \
-p 27017:27017 \
-e MONGO_INITDB_ROOT_USERNAME=&lt;your_username&gt; \
-e MONGO_INITDB_ROOT_PASSWORD=&lt;your_password&gt;\
-d mongo:latest</pre>
<p>Don’t forget to change the variables using the username and password of your choice.</p>
<p>On Docker Desktop, you should see the following:</p>
<div><div><img alt="Figure 5.15 – MongoDB Docker container running" height="44" src="img/Figure_5.15_B19453.jpg" width="223"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.15 – MongoDB Docker container running</p>
<p>The next step is to download and configure Studio 3T Free, free software the development community uses to connect to MongoDB servers. You can download this software from <a href="https://studio3t.com/download-studio3t-free">https://studio3t.com/download-studio3t-free</a> and follow the installer’s steps for your given OS.</p>
<p>During the installation, a message may appear like that shown in the following figure. If so, you can leave the fields blank. We don’t need password encryption for local or testing purposes.</p>
<div><div><img alt="Figure 5.16 – Studio 3T Free password encryption message" height="607" src="img/Figure_5.16_B19453.jpg" width="845"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.16 – Studio 3T Free password encryption message</p>
<p>When the<a id="_idIndexMarker381"/> installation process is finished, you will see the following window:</p>
<p class="IMG---Figure"> </p>
<div><div><img alt="Figure 5.17 – Studio 3T Free connection window" height="623" src="img/Figure_5.17_B19453.jpg" width="916"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.17 – Studio 3T Free connection window</p>
<p>We are now ready to connect our MongoDB instance to the IDE.</p>
<h2 id="_idParaDest-178"><a id="_idTextAnchor178"/>How to do it…</h2>
<p>Now that we have Studio 3T installed, let’s connect to <a id="_idIndexMarker382"/>our local MongoDB instance:</p>
<ol>
<li><strong class="bold">Creating the connection</strong>: Right after you open Studio 3T, a window will appear and ask you to insert the connection string or manually configure it. Select the second option and click on <strong class="bold">Next</strong>.</li>
</ol>
<p>You will have something like this:</p>
<div><div><img alt="Figure 5.18 – Studio 3T New Connection initial options" height="553" src="img/Figure_5.18_B19453.jpg" width="747"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.18 – Studio 3T New Connection initial options</p>
<ol>
<li value="2"><code>localhost</code> and <code>27017</code>.</li>
</ol>
<p>Your screen should look as follows for now:</p>
<p class="IMG---Figure"> </p>
<div><div><img alt="Figure 5.19 – New Connection server information" height="645" src="img/Figure_5.19_B19453.jpg" width="591"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.19 – New Connection server information</p>
<p>Now select the <strong class="bold">Authentication</strong> tab under the <strong class="bold">Connection group</strong> field, and from the <strong class="bold">Authentication Mode</strong> drop-down menu, choose <strong class="bold">Basic</strong>.</p>
<p>Three fields will<a id="_idIndexMarker383"/> appear—<code>admin</code> in the <strong class="bold">Authentication </strong><strong class="bold">DB</strong> field.</p>
<p class="IMG---Figure"> </p>
<div><div><img alt="Figure 5.20 – New Connection Authentication information" height="654" src="img/Figure_5.20_B19453.jpg" width="571"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.20 – New Connection Authentication information</p>
<ol>
<li value="3"><strong class="bold">Testing our connection</strong>: With this<a id="_idIndexMarker384"/> configuration, we should be able to test our database connection. In the lower-left corner, select the <strong class="bold">Test </strong><strong class="bold">Connection</strong> button.</li>
</ol>
<p>If the credentials you provided are correct, you will see the following output:</p>
<div><div><img alt="Figure 5.21 – Test connection successful" height="637" src="img/Figure_5.21_B19453.jpg" width="641"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.21 – Test connection successful</p>
<p>Click on the <strong class="bold">Save</strong> button, and<a id="_idIndexMarker385"/> the window will close.</p>
<ol>
<li value="4"><strong class="bold">Connecting to our database</strong>: After we save our configuration, a window with the available connections will appear, including our newly created one:</li>
</ol>
<p class="IMG---Figure"> </p>
<div><div><img alt="Figure 5.22 – Connection manager with the connection created" height="544" src="img/Figure_5.22_B19453.jpg" width="906"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.22 – Connection manager with the connection created</p>
<p>Select the <strong class="bold">Connect</strong> button, and<a id="_idIndexMarker386"/> three default databases will appear: <strong class="bold">admin</strong>, <strong class="bold">config</strong>, and <strong class="bold">local</strong>, as shown in the following screenshot:</p>
<div><div><img alt="Figure 5.23 – The main page of the local MongoDB with the default databases on the server" height="886" src="img/Figure_5.23_B19453.jpg" width="1378"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.23 – The main page of the local MongoDB with the default databases on the server</p>
<p>We have now finished our MongoDB configuration and are ready for the following recipes in this chapter and others, including <em class="italic">Chapters 6</em>, <em class="italic">11</em>, and <em class="italic">12</em>.</p>
<h2 id="_idParaDest-179"><a id="_idTextAnchor179"/>How it works…</h2>
<p>Like<a id="_idIndexMarker387"/> available databases, creating and running MongoDB through a Docker container is straightforward. Check the following commands:</p>
<pre class="source-code">
my-project/mongo-local$ docker run \
--name mongodb-local \
-p 27017:27017 \
-e MONGO_INITDB_ROOT_USERNAME=&lt;your_username&gt; \
-e MONGO_INITDB_ROOT_PASSWORD=&lt;your_password&gt;\
-d mongo:latest</pre>
<p>As we saw in the <em class="italic">Getting ready</em> section, the most crucial information to be passed is the username and password (using the <code>-e</code> parameter), the ports over which to connect (using the <code>-p</code> parameter), and the container image version, which is the latest available.</p>
<p>The architecture of the MongoDB container connected to Studio 3T Free is even more straightforward. Once the connection port is available, we can easily access the database. You can see the architectural representation as follows:</p>
<div><div><img alt="Figure 5.24 – MongoDB with Docker image connected to Studio 3T Free" height="427" src="img/Figure_5.24_B19453.jpg" width="752"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.24 – MongoDB with Docker image connected to Studio 3T Free</p>
<p>As described at the beginning of this recipe, MongoDB is a document-oriented database. Its structure is similar to a JSON file, except each line is interpreted as a document and has its own <code>ObjectId</code>, as follows:</p>
<div><div><img alt="Figure 5.25 – MongoDB document format" height="238" src="img/Figure_5.25_B19453.jpg" width="314"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.25 – MongoDB document format</p>
<p>The group of documents<a id="_idIndexMarker388"/> is referred to as a <em class="italic">collection</em>, which is <a id="_idIndexMarker389"/>better understood as a table representation in a structured database. You can see how it is hierarchically organized in the schema shown here:</p>
<div><div><img alt="Figure 5.26 – MongoDB data structure" height="538" src="img/Figure_5.26_B19453.jpg" width="1047"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.26 – MongoDB data structure</p>
<p>As we observed when logging in to MongoDB using Studio 3T Free, there are three default databases: <code>admin</code>, <code>config</code>, and <code>local</code>. For now, let’s disregard the last two since they pertain to operational working of the data. The <code>admin</code> database is the main one created by the <code>root</code> user. That’s why we provided this database for the <strong class="bold">Authentication DB</strong> option in <em class="italic">step 3</em>.</p>
<p>Creating a user to ingest <a id="_idIndexMarker390"/>data and access specific databases or collections is generally recommended. However, we will keep using root access here and in the following recipes in this book for demonstration purposes.</p>
<h2 id="_idParaDest-180"><a id="_idTextAnchor180"/>There’s more…</h2>
<p>The connection string will vary depending on how your MongoDB server is configured. For instance, when <em class="italic">replicas</em> or <em class="italic">sharded clusters</em> are in place, we need to specify which instances we want to connect to.</p>
<p class="callout-heading">Note</p>
<p class="callout">Sharded clusters are a complex and interesting topic. You can read more and go deeper on the topic in MongoDB’s official documentation at <a href="https://www.mongodb.com/docs/manual/core/sharded-cluster-components/">https://www.mongodb.com/docs/manual/core/sharded-cluster-components/</a>.</p>
<p>Let’s see an example of a standalone server string connection using basic authentication mode:</p>
<pre class="source-code">
mongodb://mongo-server-user:some_password@mongo-host01.example.com:27017/?authSource=admin</pre>
<p>As you can see, it is similar to other database connections. If we wanted to connect to a local server, we would change the host to <code>localhost</code>.</p>
<p>Now, for a replica or sharded cluster, the string connection looks like this:</p>
<pre class="source-code">
mongodb://mongo-server-user:some_password@mongo-host01.example.com:27017, mongo-host02.example.com:27017, mongo-hosta03.example.com:27017/?authSource=admin</pre>
<p>The <code>authSource=admin</code> parameter in this URI is essential to inform MongoDB that we want to authenticate using the administration user of the database. Without it, an error or authentication will be raised, like the following output:</p>
<pre class="source-code">
MongoError: Authentication failed</pre>
<p>Another way to avoid<a id="_idIndexMarker391"/> this error is to create a specific user to access the database and collection.</p>
<h3>SRV URI connection</h3>
<p>MongoDB introduced the <strong class="bold">Domain Name System </strong>(<strong class="bold">DNS</strong>) seed list <a id="_idIndexMarker392"/>connection, constructed <a id="_idIndexMarker393"/>by a <strong class="bold">DNS Service Record </strong>(<strong class="bold">SRV</strong>) specification <a id="_idIndexMarker394"/>of data in the DNS, to try to solve this verbose string. We saw the possibility of using an SRV URI to configure the MongoDB connection in the first step of this recipe.</p>
<p>Here’s an example of how it looks:</p>
<pre class="source-code">
mongodb+srv://my-server.example.com/</pre>
<p>It is similar to the standard connection string format we saw earlier. However, we need to indicate the use of SRV at the beginning and then provide the DNS entry.</p>
<p>This type of connection is advantageous when handling replicas or nodes since the SRV creates a single identity for the cluster. You can find a more detailed explanation of this, along with an outline of how to configure it, in the MongoDB official documentation at <a href="https://www.mongodb.com/docs/manual/reference/connection-string/#dns-seed-list-connection-format">https://www.mongodb.com/docs/manual/reference/connection-string/#dns-seed-list-connection-format</a>.</p>
<h2 id="_idParaDest-181"><a id="_idTextAnchor181"/>See also</h2>
<p>If you are interested, other <a id="_idIndexMarker395"/>MongoDB GUI tools are available on the market: <a href="https://www.guru99.com/top-20-mongodb-tools.xhtml">https://www.guru99.com/top-20-mongodb-tools.xhtml</a>.</p>
<h1 id="_idParaDest-182"><a id="_idTextAnchor182"/>Creating our NoSQL table in MongoDB</h1>
<p>After successfully<a id="_idIndexMarker396"/> connecting and understanding how Studio <a id="_idIndexMarker397"/>3T works, we will now import some MongoDB collections. We have seen in the <em class="italic">Connecting to a NoSQL database (MongoDB) </em>recipe how to get started with MongoDB, and in this recipe, we will import a MongoDB database and come to understand its structure. Although MongoDB has a specific format to <a id="_idIndexMarker398"/>organize data internally, understanding how a NoSQL database behaves is crucial <a id="_idIndexMarker399"/>when working with data ingestion.</p>
<p>We will practice by ingesting the imported collections in the following recipes in this chapter.</p>
<h2 id="_idParaDest-183"><a id="_idTextAnchor183"/>Getting ready</h2>
<p>For this recipe, we will use a sample dataset of Airbnb reviews called <code>listingsAndReviews.json</code>. You can find this dataset in the GitHub repository of this book at <a href="https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_5/datasets/sample_airbnb">https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_5/datasets/sample_airbnb</a>. After downloading it, put the file into our <code>mongo-local</code> directory, created in <a href="B19453_01.xhtml#_idTextAnchor022"><em class="italic">Chapter 1</em></a>.</p>
<p>I kept mine inside the <code>sample_airbnb</code> folder just for organization purposes, as you can see in the following screenshot:</p>
<div><div><img alt="Figure 5.27 – Command line with listingsAndReviews.json" height="36" src="img/Figure_5.27_B19453.jpg" width="649"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.27 – Command line with listingsAndReviews.json</p>
<p>After downloading the dataset, we need to install <code>pymongo</code>, a Python library to connect to and manage MongoDB operations. To install it, use the following command:</p>
<pre class="source-code">
$ pip3 install pymongo</pre>
<p>Feel free to create <code>virtualenv</code> for this installation.</p>
<p>We are now ready to start inserting data into MongoDB. Don’t forget to check that your Docker image is up and running before we begin.</p>
<h2 id="_idParaDest-184"><a id="_idTextAnchor184"/>How to do it…</h2>
<p>Here are the steps to perform this recipe:</p>
<ol>
<li><code>pymongo</code>, we can easily establish a connection with the MongoDB database. Refer to the following code:<pre class="source-code">
import json
import os
from pymongo import MongoClient, InsertOne
mongo_client = pymongo.MongoClient("mongodb://root:root@localhost:27017/")</pre></li>
<li><strong class="bold">Defining our database and collection</strong>: We will create a database and collection instance using the<a id="_idIndexMarker400"/> client connection we instantiated.</li>
</ol>
<p>For the <code>json_collection</code> variable, insert the <a id="_idIndexMarker401"/>path where you put the Airbnb sample dataset:</p>
<pre class="source-code">
db_cookbook = mongo_client.db_airbnb
collection = db_cookbook.reviews
json_collection = "sample_airbnb/listingsAndReviews.json"</pre>
<ol>
<li value="3"><code>bulk_write</code> function, we will insert all the documents inside the JSON file into the sales collection we created and close the connection:<pre class="source-code">
requesting_collection = []
with open(json_collection) as f:
    for object in f:
        my_dict = json.loads(object)
        requesting.append(InsertOne(my_dict))
result = collection.bulk_write(requesting_collection)
mongo_client.close()</pre></li>
</ol>
<p>No output is expected from this operation, but we can check the database to see if it is successful.</p>
<ol>
<li value="4"><strong class="bold">Checking the MongoDB database results</strong>: Let’s check our database to see if the data was inserted <a id="_idIndexMarker402"/>correctly.</li>
</ol>
<p>Open Studio 3T Free and <a id="_idIndexMarker403"/>refresh the connection (right-click on the connection name and select <strong class="bold">Refresh All</strong>). You should see a new database named <strong class="bold">db_airbnb</strong> has been created, containing a <strong class="bold">reviews</strong> collection, as shown in the following screenshot:</p>
<div><div><img alt="Figure 5.28 – Database and collection successfully created on MongoDB" height="514" src="img/Figure_5.28_B19453.jpg" width="1176"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.28 – Database and collection successfully created on MongoDB</p>
<p>With the collection now created and containing some data, let’s go deeper into how the code works.</p>
<h2 id="_idParaDest-185"><a id="_idTextAnchor185"/>How it works…</h2>
<p>As you can see, the code we implemented is straightforward, using just a few lines to create and insert data in our database. However, there are important points to pay attention to due to the particularities of MongoDB.</p>
<p>Let’s examine the code line by line now:</p>
<pre class="source-code">
mongo_client = pymongo.MongoClient("mongodb://root:root@localhost:27017/")</pre>
<p>This line defines the<a id="_idIndexMarker404"/> connection to our MongoDB database, and from <a id="_idIndexMarker405"/>this instance, we can create a new database and its collections.</p>
<p class="callout-heading">Note</p>
<p class="callout">Observe that the URI connection contains hardcoded values for the username and password. This must be avoided in real applications, and even development servers. It is recommended to store those values as environment variables or use a secret manager vault.</p>
<p>Next, we define the database and collection names; you may have noticed we didn’t create them previously in our database. At the time of execution of the code, MongoDB checks whether the database exists; if not, MongoDB will create it. The same rule applies to the <strong class="bold">reviews</strong> collection.</p>
<p>Notice the collection derives from the <code>db_cookbook</code> instance, which makes it clear that it is linked to the <code>db_airbnb</code> database:</p>
<pre class="source-code">
db_cookbook = mongo_client.db_airbnb
collection = db_cookbook.reviews</pre>
<p>Following the code, the next step is to open the JSON file and parse every line. Here we start to see some tricky peculiarities of MongoDB:</p>
<pre class="source-code">
requesting_collection = []
with open(json_collection) as f:
    for object in f:
        my_dict = json.loads(object)
        requesting_collection.append(InsertOne(my_dict))</pre>
<p>It is common to wonder why we actually need to parse the lines of JSON, since MongoDB accepts this format. Let’s check our <code>listingsAndReviews.json</code> file, as shown in the following screenshot:</p>
<div><div><img alt="Figure 5.29 – JSON file with MongoDB document lines" height="296" src="img/Figure_5.29_B19453.jpg" width="1379"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.29 – JSON file with MongoDB document lines</p>
<p>If we use any tool to verify<a id="_idIndexMarker406"/> this as valid JSON, it will certainly say it’s not a valid format. This happens because<a id="_idIndexMarker407"/> each line of this file represents one document of the MongoDB collection. Trying to open that file using only the conventional <code>open()</code> and <code>json.loads()</code> methods will produce an error like the following:</p>
<pre class="source-code">
json.decoder.JSONDecodeError: Extra data: line 2 column 1 (char 190)</pre>
<p>To make it acceptable to the Python interpreter, we need to open and read each line individually and append it to the <code>requesting_collection</code> list. Also, the <code>InsertOne()</code> method will ensure that each line is inserted separately. A problem that occurs while inserting a specific row will be much easier to identify.</p>
<p>Finally, the <code>bulk_write()</code> will take the list of documents and insert them into the MongoDB database:</p>
<pre class="source-code">
result = collection.bulk_write(requesting_collection)</pre>
<p>This operation will finish without returning any output or error messages if everything is OK.</p>
<h2 id="_idParaDest-186"><a id="_idTextAnchor186"/>There’s more…</h2>
<p>We have seen how simple it is to create a Python script to insert data into our MongoDB server. Nevertheless, MongoDB has database tools to provide the same result and can be executed via the command line. The <code>mongoimport</code> command is used to insert data into our database, as you can see in the following code:</p>
<pre class="source-code">
mongoimport --host localhost --port 27017-d db_name -c collection_name --file path/to/file.json</pre>
<p>If you are interested to learn more about the other database tools and commands available, check the official MongoDB documentation at <a href="https://www.mongodb.com/docs/database-tools/installation/installation/">https://www.mongodb.com/docs/database-tools/installation/installation/</a>.</p>
<h3>Restrictions on field names</h3>
<p>When loading data into<a id="_idIndexMarker408"/> MongoDB, one big problem is the restrictions on characters used in the field names. Due to MongoDB server versions or programming language specificities, sometimes the key names of fields come with a <code>$</code> prefix, and, by default, MongoDB is not compatible with it, creating an error like the following output:</p>
<pre class="source-code">
localhost:27017: $oid is not valid for storage.</pre>
<p>In this case, a JSON file dump was exported from a MongoDB server, and the reference of <code>ObjectID</code> came with the <code>$</code> prefix. Even though the more recent versions of MongoDB have started to accept these characters (see the thread here: <a href="https://jira.mongodb.org/browse/SERVER-41628?fbclid=IwAR1t5Ld58LwCi69SrMCcDbhPGf2EfBWe_AEurxGkEWHpZTHaEIde0_AZ-uM%5D">https://jira.mongodb.org/browse/SERVER-41628?fbclid=IwAR1t5Ld58LwCi69SrMCcDbhPGf2EfBWe_AEurxGkEWHpZTHaEIde0_AZ-uM%5D</a>), it is a good practice to avoid using them where possible. In this case, we have two main options: remove<a id="_idIndexMarker409"/> all the restricted characters using a script, or encode the JSON file into a <strong class="bold">Binary JavaScript Object Notation </strong>(<strong class="bold">BSON</strong>) file. You can find out more about encoding the file into BSON format at <a href="https://kb.objectrocket.com/mongo-db/how-to-use-python-to-encode-a-json-file-into-mongodb-bson-documents-545">https://kb.objectrocket.com/mongo-db/how-to-use-python-to-encode-a-json-file-into-mongodb-bson-documents-545</a>.</p>
<h2 id="_idParaDest-187"><a id="_idTextAnchor187"/>See also</h2>
<p>You can read <a id="_idIndexMarker410"/>more about the MongoDB restrictions on field names at <a href="https://www.mongodb.com/docs/manual/reference/limits/#mongodb-limit-Restrictions-on-Field-Names">https://www.mongodb.com/docs/manual/reference/limits/#mongodb-limit-Restrictions-on-Field-Names</a>.</p>
<h1 id="_idParaDest-188"><a id="_idTextAnchor188"/>Ingesting data from MongoDB using PySpark</h1>
<p>Even though it seems<a id="_idIndexMarker411"/> impractical to create and ingest the <a id="_idIndexMarker412"/>data ourselves, this exercise can be applied to real-life projects. People who work with data are often involved in the architectural process of defining the type of database, helping other engineers to insert data from applications into a database server, and later ingesting only the relevant information for dashboards or other analytical tools.</p>
<p>So far, we have created and evaluated our server and then created collections inside our MongoDB instance. With all this <a id="_idIndexMarker413"/>preparation, we can now ingest our data using PySpark.</p>
<h2 id="_idParaDest-189"><a id="_idTextAnchor189"/>Getting ready</h2>
<p>This recipe requires the <a id="_idIndexMarker414"/>execution of the <em class="italic">Creating our NoSQL table in MongoDB</em> recipe due to data insertion. However, you can create and insert other documents into the MongoDB database and use them here. If you do this, ensure you set the suitable configurations to make it run properly.</p>
<p>Also, as in the <em class="italic">Creating our NoSQL table in MongoDB</em> recipe, check that the Docker container is up and running since this is our MongoDB instance’s primary data source. Let’s proceed to the ingesting!</p>
<div><div><img alt="Figure 5.30 – Docker container for MongoDB is running" height="44" src="img/Figure_5.30._B19453.jpg" width="233"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.30 – Docker container for MongoDB is running</p>
<h2 id="_idParaDest-190"><a id="_idTextAnchor190"/>How to do it…</h2>
<p>You need to perform the following steps to try this recipe:</p>
<ol>
<li><code>SparkSession</code>, but this time passing specific configurations for reading our MongoDB database, <code>db_airbnb</code>, such as the URI and the <code>.jars</code>:<pre class="source-code">
from pyspark.sql import SparkSession
spark = SparkSession.builder \
      .master("local[1]") \
      .appName("MongoDB Ingest") \
      .config("spark.executor.memory", '3g') \
      .config("spark.executor.cores", '1') \
      .config("spark.cores.max", '1') \
      .config("spark.mongodb.input.uri", "mongodb://root:root@127.0.0.1/db_airbnb?authSource=admin&amp;readPreference=primaryPreferred") \
      .config("spark.mongodb.input.collection", "reviews") \
      .config("spark.jars.packages","org.mongodb.spark:mongo-spark-connector_2.12:3.0.1") \
      .getOrCreate()</pre></li>
</ol>
<p>We should expect a <a id="_idIndexMarker415"/>significant output<a id="_idIndexMarker416"/> here since Spark downloads the package and sets the rest of the configuration we passed:</p>
<div><div><img alt="Figure 5.31 – SparkSession being initialized with MongoDB configurations" height="605" src="img/Figure_5.31._B19453.jpg" width="1087"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.31 – SparkSession being initialized with MongoDB configurations</p>
<ol>
<li><code>SparkSession</code> we instantiated. Here, no output is expected <a id="_idIndexMarker418"/>because the <code>SparkSession</code> is set only to send logs at the <code>WARN</code> level:<pre class="source-code">
df = spark.read.format("mongo").load()</pre></li>
<li><strong class="bold">Getting our DataFrame schema</strong>: We can see the collection’s schema using the print operation on the DataFrame:<pre class="source-code">
df.printSchema()</pre></li>
</ol>
<p>You should observe the following output:</p>
<div><div><img alt="Figure 5.32 – Reviews DataFrame collection schema printed" height="658" src="img/Figure_5.32._B19453.jpg" width="555"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.32 – Reviews DataFrame collection schema printed</p>
<p>As you can observe, the structure is similar to a JSON file with nested objects. Unstructured data is usually presented in this form and can hold a large amount of information to create a Python script to insert data into our data. Now, let’s go deeper and understand our code.</p>
<h2 id="_idParaDest-191"><a id="_idTextAnchor191"/>How it works…</h2>
<p>MongoDB required a few<a id="_idIndexMarker419"/> additional configurations in <code>SparkSession</code> to execute the <code>.read</code> function. It is essential to understand why we used the configurations instead of just using code from the documentation. Let’s<a id="_idIndexMarker420"/> explore the code for it:</p>
<pre class="source-code">
config("spark.mongodb.input.uri", "mongodb://root:root@127.0.0.1/db_airbnb?authSource=admin) \</pre>
<p>Note the use of <code>spark.mongodb.input.uri</code>, which tells our <code>SparkSession</code> that a <em class="italic">read</em> operation needs to be performed using a MongoDB URI. If, for instance, we wanted to do a <em class="italic">write</em> operation (or both read and write), we would just need to add the <code>spark.mongodb.output.uri</code> configuration.</p>
<p>Next, we pass the URI containing the user and password information, the name of the database, and the authentication source. Since we use the root user to retrieve the data, this last parameter is set to <code>admin</code>.</p>
<p>Next, we define the name of our collection to be used in the read operation:</p>
<pre class="source-code">
.config("spark.mongodb.input.collection", "reviews")\</pre>
<p class="callout-heading">Note</p>
<p class="callout">Even though it might seem odd to define these parameters in the SparkSession, and it is possible to set the database and collection, this is a good practice that has been adopted by the community when manipulating MongoDB connections.</p>
<pre class="source-code">
.config("spark.jars.packages","org.mongodb.spark:mongo-spark-connector_2.12:3.0.1")</pre>
<p>Another new configuration here is <code>spark.jars.packages</code>. When using this key with the <code>.config()</code> method, Spark will search its available online packages, download them, and place them in the <code>.jar</code> folders to be used. Although this is an advantageous way to set the <code>.jar</code> connectors, this is not available for all databases.</p>
<p>Once the connection is established, the reading process is remarkably similar to the JDBC: we pass the <code>.format()</code> of the database (here, <code>mongo</code>), and since the database and collection name are already set, we <a id="_idIndexMarker421"/>don’t need to configure <code>.option()</code>:</p>
<pre class="source-code">
df = spark.read.format("mongo").load()</pre>
<p>When executing <code>.load()</code>, Spark will verify whether the connection is valid and throw an error if not. In the following<a id="_idIndexMarker422"/> screenshot, you can see an example of the error message when the credentials are not correct:</p>
<div><div><img alt="Figure 5.33 – Py4JJavaError: Authentication error to MongoDB connection" height="217" src="img/Figure_5.33._B19453.jpg" width="1083"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.33 – Py4JJavaError: Authentication error to MongoDB connection</p>
<p>Even though we are handling an unstructured data format, as soon as PySpark transforms our collection into a DataFrame, all the filtering, cleaning, and manipulating of data is pretty much the same as PySpark data.</p>
<h2 id="_idParaDest-192"><a id="_idTextAnchor192"/>There’s more…</h2>
<p>As we saw previously, PySpark error messages can be confusing and cause discomfort at first glance. Let’s explore other common errors when ingesting data from a MongoDB database without the proper configuration.</p>
<p>In this example, let’s not set <code>spark.jars.packages</code> in the <code>SparkSession</code> configuration:</p>
<pre class="source-code">
spark = SparkSession.builder \
      (...)
      .config("spark.mongodb.input.uri", "mongodb://root:root@127.0.0.1/db_aibnb?authSource=admin") \
      .config("spark.mongodb.input.collection", "reviews")
     .getOrCreate()
df = spark.read.format("mongo").load()</pre>
<p>If you try to execute the<a id="_idIndexMarker423"/> preceding code (passing <a id="_idIndexMarker424"/>the rest of the memory settings), you will get the following output:</p>
<div><div><img alt="Figure 5.34 – java.lang.ClassNotFoundException error when the MongoDB package is not set in the configuration" height="659" src="img/Figure_5.34._B19453.jpg" width="1099"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.34 – java.lang.ClassNotFoundException error when the MongoDB package is not set in the configuration</p>
<p>Looking carefully at the second line, which begins with <code>java.lang.ClassNotFoundException</code>, the JVM highlights a missing package or class that needs to be searched for in a third-party repository. The package contains the connector code to our JVM and establishes communication with the database server.</p>
<p>Another widespread error message is <code>IllegalArgumentException</code>. This type of error indicates to the developer that an argument was wrongly passed to a method or class. Usually, when related to database connections, it refers to an invalid string connection, as in the following screenshot:</p>
<div><div><img alt="Figure 5.35 – IllegalArgumentException error when the URI is invalid" height="184" src="img/Figure_5.35._B19453.jpg" width="1097"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.35 – IllegalArgumentException error when the URI is invalid</p>
<p>Although it seems <a id="_idIndexMarker425"/>unclear, there is a typo in the URI, where <code>db_aibnb/?</code> contains an extra forward slash. Removing it and running <code>SparkSession</code> <a id="_idIndexMarker426"/>again will make this error disappear.</p>
<p class="callout-heading">Note</p>
<p class="callout">It is recommended to shut down and restart the kernel processes when re-defining the SparkSession configurations because SparkSession tends to append to the processes rather than replacing them.</p>
<h2 id="_idParaDest-193"><a id="_idTextAnchor193"/>See also</h2>
<ul>
<li>MongoDB <a id="_idIndexMarker427"/>Spark connector documentation: <a href="https://www.mongodb.com/docs/spark-connector/current/configuration/">https://www.mongodb.com/docs/spark-connector/current/configuration/</a></li>
<li>You can check the MongoDB documentation for a full explanation of how the MongoDB connector behaves with PySpark: <a href="https://www.mongodb.com/docs/spark-connector/current/read-from-mongodb/">https://www.mongodb.com/docs/spark-connector/current/read-from-mongodb/</a></li>
<li>There are also<a id="_idIndexMarker428"/> some interesting use cases of MongoDB here: <a href="https://www.mongodb.com/use-cases">https://www.mongodb.com/use-cases</a></li>
</ul>
<h1 id="_idParaDest-194"><a id="_idTextAnchor194"/>Further reading</h1>
<ul>
<li><a href="https://www.talend.com/resources/structured-vs-unstructured-data/">https://www.talend.com/resources/structured-vs-unstructured-data/</a></li>
<li><a href="https://careerfoundry.com/en/blog/data-analytics/structured-vs-unstructured-data/">https://careerfoundry.com/en/blog/data-analytics/structured-vs-unstructured-data/</a></li>
<li><a href="https://www.dba-ninja.com/2022/04/is-mongodbsrv-necessary-for-a-mongodb-connection.xhtml">https://www.dba-ninja.com/2022/04/is-mongodbsrv-necessary-for-a-mongodb-connection.xhtml</a></li>
<li><a href="https://www.mongodb.com/docs/manual/reference/connection-string/#connection-string-options">https://www.mongodb.com/docs/manual/reference/connection-string/#connection-string-options</a></li>
<li><a href="https://sparkbyexamples.com/spark/spark-createorreplacetempview-explained/">https://sparkbyexamples.com/spark/spark-createorreplacetempview-explained/</a></li>
</ul>
</div>
</div></body></html>