<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer138">
<h1 class="chapter-number" id="_idParaDest-161"><a id="_idTextAnchor161"/>5</h1>
<h1 id="_idParaDest-162"><a id="_idTextAnchor162"/>Ingesting Data from Structured and Unstructured Databases</h1>
<p>Nowadays, we can store and retrieve data from multiple sources, and the optimal storage method depends on the type of information being processed. For example, most APIs make data available in an unstructured format as this allows the sharing of data of multiple formats (for example, audio, video, and image) and has low storage costs via the use of data lakes. However, if we want to make quantitative data available for use with several tools to support analysis, then the most reliable option might be <span class="No-Break">structured data.</span></p>
<p>Ultimately, whether you are a data analyst, scientist, or engineer, it is essential to understand how to manage both structured and <span class="No-Break">unstructured data.</span></p>
<p>In this chapter, we will cover the <span class="No-Break">following recipes:</span></p>
<ul>
<li>Configuring a <span class="No-Break">JDBC connection</span></li>
<li>Ingesting data from a JDBC database <span class="No-Break">using SQL</span></li>
<li>Connecting to a NoSQL <span class="No-Break">database (MongoDB)</span></li>
<li>Creating our NoSQL table <span class="No-Break">in MongoDB</span></li>
<li>Ingesting data from MongoDB <span class="No-Break">using PySpark</span></li>
</ul>
<h1 id="_idParaDest-163"><a id="_idTextAnchor163"/>Technical requirements</h1>
<p>You can find the code from this chapter in the GitHub repository <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook"><span class="No-Break">https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook</span></a><span class="No-Break">.</span></p>
<p>Using the <strong class="bold">Jupyter Notebook</strong> is not mandatory but allows us to explore the code interactively. Since we will execute both Python and PySpark code, Jupyter can help us to understand the scripts better. Once you have Jupyter installed, you can execute it using the <span class="No-Break">following line:</span></p>
<pre class="source-code">
$ jupyter notebook</pre>
<p>It is recommended to create a separate folder to store the Python files or notebooks we will cover in this chapter; however, feel free to organize it in the most appropriate way <span class="No-Break">for you.</span></p>
<h1 id="_idParaDest-164"><a id="_idTextAnchor164"/>Configuring a JDBC connection</h1>
<p>Working with different <a id="_idIndexMarker353"/>systems brings the challenge of finding an efficient way to connect the systems. An adaptor, or a driver, is the solution to this communication problem, creating a bridge to translate information from one system <span class="No-Break">to another.</span></p>
<p><strong class="bold">JDBC</strong>, or <strong class="bold">Java Database Connectivity</strong>, is used to facilitate communication between Java-based <a id="_idIndexMarker354"/>systems and databases. This recipe covers configuring JDBC in SparkSession to connect to a PostgreSQL database, using best practices <span class="No-Break">as always.</span></p>
<h2 id="_idParaDest-165"><a id="_idTextAnchor165"/>Getting ready</h2>
<p>Before configuring SparkSession, we need to download the <strong class="source-inline">.jars</strong> file (Java Archive). You can do this at <a href="https://jdbc.postgresql.org/">https://jdbc.postgresql.org/</a> on the PostgreSQL <span class="No-Break">official site.</span></p>
<p>Select <strong class="bold">Download</strong>, and you will be redirected to <span class="No-Break">another page:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer103">
<img alt="Figure 5.1 – PostgreSQL JDBC home page" height="796" src="image/Figure_5.1_B19453.jpg" width="1029"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.1 – PostgreSQL JDBC home page</p>
<p>Then, select the <strong class="bold">Java 8 </strong><span class="No-Break"><strong class="bold">Download</strong></span><span class="No-Break"> button.</span></p>
<p>Keep this <strong class="source-inline">.jar</strong> file somewhere safe, as you will need it later. I suggest keeping it inside the folder where your<a id="_idIndexMarker355"/> <span class="No-Break">code is.</span></p>
<p>For the PostgreSQL database, you can use a Docker image or the instance we created on Google Cloud in <span class="No-Break"><em class="italic">Chapter 4</em></span>. If you opt for the Docker image, ensure it is up <span class="No-Break">and running.</span></p>
<p>The final preparatory step for this recipe is to import a dataset to be used. We will use the <strong class="source-inline">word_population.csv</strong> file (which you can find in the GitHub repository of this book, at <a href="https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_5/datasets">https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_5/datasets</a>). Import it using DBeaver or any other SQL IDE of your choice. We will use this dataset with SQL in the <em class="italic">Ingesting data from a JDBC database using SQL </em>recipe later in <span class="No-Break">this chapter.</span></p>
<p>To import data into DBeaver, create a table with the name of your choice under the Postgres database. I chose to give my table the exact name of the CSV file. You don’t need to insert any columns <span class="No-Break">for now.</span></p>
<p>Then, right-click on the table and select <strong class="bold">Import Data</strong>, as shown in the <span class="No-Break">following screenshot:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer104">
<img alt="Figure 5.2 – Importing data on a table using DBeaver" height="671" src="image/Figure_5.2_B19453.jpg" width="495"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.2 – Importing data on a table using DBeaver</p>
<p>A new window<a id="_idIndexMarker356"/> will open, showing the options to use a CSV file or a database table. Select <strong class="bold">CSV</strong> and then <strong class="bold">Next</strong>, <span class="No-Break">as follows:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer105">
<img alt="Figure 5.3 – Importing CSV files into a table using DBeaver" height="628" src="image/Figure_5.3_B19453.jpg" width="818"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.3 – Importing CSV files into a table using DBeaver</p>
<p>A new window will open where you can select the file. Choose the <strong class="source-inline">world_population.csv</strong> file and select the <strong class="bold">Next</strong> button, leaving the default settings shown <span class="No-Break">as follows:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer106">
<img alt="Figure 5.4 – CSV file successfully imported into the world_population table" height="786" src="image/Figure_5.4_B19453.jpg" width="780"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.4 – CSV file successfully imported into the world_population table</p>
<p>If all succeeds, you<a id="_idIndexMarker357"/> should be able to see the <strong class="source-inline">world_population</strong> table populated with the columns <span class="No-Break">and data:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer107">
<img alt="Figure 5.5 – The world_population table populated with data from the CSV" height="780" src="image/Figure_5.5_B19453.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.5 – The world_population table populated with data from the CSV</p>
<h2 id="_idParaDest-166"><a id="_idTextAnchor166"/>How to do it…</h2>
<p>I will use a Jupyter <a id="_idIndexMarker358"/>notebook to insert and execute the code to make this exercise more dynamic. Here is how we <span class="No-Break">do it:</span></p>
<ol>
<li><strong class="bold">Importing the PySpark libraries</strong>: Besides <strong class="source-inline">SparkSession</strong>, we will need an additional class called <strong class="source-inline">SparkConf</strong> to set our <span class="No-Break">new configuration:</span><pre class="source-code">
from pyspark.conf import SparkConf
from pyspark.sql import SparkSession</pre></li>
<li><strong class="bold">Using SparkConf to set the .jar path</strong>: Using <strong class="source-inline">SparkConf(</strong>), which we instantiated, we can set the path to the <strong class="source-inline">.jar</strong> <span class="No-Break">with </span><span class="No-Break"><strong class="source-inline">spark.jars</strong></span><span class="No-Break">:</span><pre class="source-code">
conf = SparkConf()
conf.set('spark.jars', /path/to/your/postgresql-42.5.1.jar')</pre></li>
</ol>
<p>You will see a <strong class="source-inline">SparkConf</strong> object created, as shown in the <span class="No-Break">following output:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer108">
<img alt="Figure 5.6 – SparkConf object" height="25" src="image/Figure_5.6_B19453.jpg" width="434"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.6 – SparkConf object</p>
<ol>
<li value="3"><strong class="bold">Creating the SparkSession instance</strong>: The next step is to pass the new configuration to <strong class="source-inline">SparkSession</strong> and <span class="No-Break">create it:</span><pre class="source-code">
spark = SparkSession.builder \
        .config(conf=conf) \
        .master("local") \
        .appName("Postgres Connection Test") \
        .getOrCreate()</pre></li>
</ol>
<p>If a warning message appears as in the following screenshot, you can <span class="No-Break">ignore it:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer109">
<img alt="Figure 5.7 – SparkSession initialization warning messages" height="159" src="image/Figure_5.7_B19453.jpg" width="1090"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.7 – SparkSession initialization warning messages</p>
<ol>
<li value="4"><strong class="bold">Connecting to our database</strong>: Finally, we can connect to the PostgreSQL database by passing the required credentials including host, database name, username, and password <span class="No-Break">as follows:</span><pre class="source-code">
df= spark.read.format("jdbc") \
    .options(url="jdbc:postgresql://localhost:5432/postgres",
             dbtable="world_population",
             user="root",
             password="root",
             driver="org.postgresql.Driver") \
    .load()</pre></li>
</ol>
<p>If the credentials are correct, we should expect no <span class="No-Break">output here.</span></p>
<ol>
<li value="5"><strong class="bold">Getting the schema of the DataFrame</strong>: Using PySpark <strong class="source-inline">.printSchema()</strong>, it is possible now to see the <span class="No-Break">table columns:</span><pre class="source-code">
df.printSchema()</pre></li>
</ol>
<p>Executing the code will show the <span class="No-Break">following output:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer110">
<img alt="Figure 5.8 – DataFrame of the world_population schema" height="313" src="image/Figure_5.8_B19453.jpg" width="491"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.8 – DataFrame of the world_population schema</p>
<h2 id="_idParaDest-167"><a id="_idTextAnchor167"/>How it works…</h2>
<p>We can observe that PySpark (and Spark) require<a id="_idIndexMarker359"/> additional configuration to create a connection with a database. In this recipe, using the PostgreSQL <strong class="source-inline">.jars</strong> file is essential to make <span class="No-Break">it work.</span></p>
<p>Let’s understand what kind of configuration Spark requires by looking at <span class="No-Break">our code:</span></p>
<pre class="source-code">
conf = SparkConf()
conf.set('spark.jars', '/path/to/your/postgresql-42.5.1.jar')</pre>
<p>We started by instantiating the <strong class="source-inline">SparkConf()</strong> method, responsible for defining configurations used in SparkSession. After instantiating the class, we used the <strong class="source-inline">set()</strong> method to pass a key-value pair parameter: <strong class="source-inline">spark.jars</strong>. If more than one <strong class="source-inline">.jars</strong> file was used, the paths could be passed on the value parameter separated by commas. It is also possible to define more than one <strong class="source-inline">conf.set()</strong>method; they just need to be included one after <span class="No-Break">the other.</span></p>
<p>It is on the second line of SparkSession where the set of configurations is passed, as you can see in the <span class="No-Break">following code:</span></p>
<pre class="source-code">
spark = SparkSession.builder \
        .config(conf=conf) \
        .master("local") \
    (...)</pre>
<p>Then, with our SparkSession<a id="_idIndexMarker360"/> instantiated, we can use it to read our database, as you can see in the <span class="No-Break">following code:</span></p>
<pre class="source-code">
df= spark.read.format("jdbc") \
    .options(url="jdbc:postgresql://localhost:5432/postgres",
             dbtable="world_population",
             user="root",
             password="root",
             driver="org.postgresql.Driver") \
    .load()</pre>
<p>Since we are handling a third-party application, we must set the format for reading the output using the <strong class="source-inline">.format()</strong> method. The <strong class="source-inline">.options()</strong> method will carry the authentication values and <span class="No-Break">the driver.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">With time you will observe that there are a few diverse ways to declare the <strong class="source-inline">.options()</strong> key-value pairs. For example, another frequently used format is .<strong class="source-inline">options("driver", "org.postgresql.Driver)</strong>. Both ways are correct depending on the <em class="italic">taste</em> of <span class="No-Break">the developer.</span></p>
<h2 id="_idParaDest-168"><a id="_idTextAnchor168"/>There’s more…</h2>
<p>This recipe covered how to use a JDBC driver, and the<a id="_idIndexMarker361"/> same logic applies to <strong class="bold">Open Database Connectivity</strong> (<strong class="bold">ODBC</strong>). However, determining the criteria for using JDBC or ODBC requires understanding which data source we are ingesting <span class="No-Break">data from.</span></p>
<p>The ODBC connection in Spark is usually associated with Spark Thrift Server, a Spark SQL extension from Apache HiveServer2 that allows users to execute SQL queries in <strong class="bold">Business Intelligence </strong>(<strong class="bold">BI</strong>) tools <a id="_idIndexMarker362"/>such as MS PowerBI or Tableau. See the following diagram for an outline of <span class="No-Break">this relationship:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer111">
<img alt="Figure 5.9 – Spark Thrift architecture, provided by Cloudera documentation (https://docs.cloudera.com/HDPDocuments/HDP3/HDP-3.1.5/developing-spark-applications/content/using_spark_sql.xhtml)" height="491" src="image/Figure_5.9_B19453.jpg" width="799"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.9 – Spark Thrift architecture, provided by Cloudera documentation (<a href="https://docs.cloudera.com/HDPDocuments/HDP3/HDP-3.1.5/developing-spark-applications/content/using_spark_sql.xhtml">https://docs.cloudera.com/HDPDocuments/HDP3/HDP-3.1.5/developing-spark-applications/content/using_spark_sql.xhtml</a>)</p>
<p>By contrast to JDBC, ODBC is used in real-life projects that are smaller and more specific to certain system integrations. It also requires the use of another Python library called <strong class="source-inline">pyodbc</strong>. You can read more about it <span class="No-Break">at </span><a href="https://kontext.tech/article/290/connect-to-sql-server-in-spark-pyspark"><span class="No-Break">https://kontext.tech/article/290/connect-to-sql-server-in-spark-pyspark</span></a><span class="No-Break">.</span></p>
<h3>Debugging connection errors</h3>
<p>PySpark errors can be <a id="_idIndexMarker363"/>very confusing and lead to misinterpretations. It happens because the errors are often related to a problem on the JVM, and Py4J (a Python interpreter that communicates dynamically with the JVM) consolidates the message with other Python errors that may <span class="No-Break">have occurred.</span></p>
<p>Some error messages are prevalent and can easily be identified when managing database connections. Let’s take a look at an error that occurred when using the <span class="No-Break">following code:</span></p>
<pre class="source-code">
df= spark.read.format("jdbc") \
    .options(url="jdbc:postgresql://localhost:5432/postgres",
             dbtable="world_population",
             user="root",
             password="root") \
    .load()</pre>
<p>Here is the error <a id="_idIndexMarker364"/>message <span class="No-Break">that resulted:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer112">
<img alt="Figure 5.10 – Py4JJavaError message" height="439" src="image/Figure_5.10_B19453.jpg" width="996"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.10 – Py4JJavaError message</p>
<p>In the first line, we see <strong class="source-inline">Py4JJavaError</strong> informing us of an error when calling the load function. Continuing to the second line, we can see the message: <strong class="source-inline">java.sql.SQLException: No suitable driver</strong>. It informs us that even though the <strong class="source-inline">.jars</strong> file is configured and set, PySpark doesn’t know which drive to use to load data from PostgreSQL. This can be easily fixed by adding the <strong class="source-inline">driver</strong> parameter under <strong class="source-inline">.options()</strong>. Refer to the <span class="No-Break">following code:</span></p>
<pre class="source-code">
df= spark.read.format("jdbc") \
    .options(url="jdbc:postgresql://localhost:5432/postgres",
             dbtable="world_population",
             user="root",
             password="root",
             driver="org.postgresql.Driver") \
    .load()</pre>
<h2 id="_idParaDest-169"><a id="_idTextAnchor169"/>See also</h2>
<p>Find more about<a id="_idIndexMarker365"/> Spark Thrift <a id="_idIndexMarker366"/>Server <span class="No-Break">at </span><a href="https://jaceklaskowski.gitbooks.io/mastering-spark-sql/content/spark-sql-thrift-server.xhtml"><span class="No-Break">https://jaceklaskowski.gitbooks.io/mastering-spark-sql/content/spark-sql-thrift-server.xhtml</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-170"><a id="_idTextAnchor170"/>Ingesting data from a JDBC database using SQL</h1>
<p>With the connection tested<a id="_idIndexMarker367"/> and SparkSession configured, the next step is to ingest the data from PostgreSQL, filter it, and save it in an analytical <a id="_idIndexMarker368"/>format called a Parquet file. Don’t worry about how Parquet files work for now; we will cover it in the <span class="No-Break">following chapters.</span></p>
<p>This recipe aims to use the connection we created with our JDBC database and ingest the data from the <span class="No-Break"><strong class="source-inline">world_population</strong></span><span class="No-Break"> table.</span></p>
<h2 id="_idParaDest-171"><a id="_idTextAnchor171"/>Getting ready</h2>
<p>This recipe will use the same dataset and code as the <em class="italic">Configuring a JDBC connection</em> recipe to connect to the PostgreSQL database. Ensure your Docker container is running or your PostgreSQL server <span class="No-Break">is up.</span></p>
<p>This recipe continues from the content presented in <em class="italic">Configuring a JDBC connection</em>. We will now learn how to ingest the data inside the <span class="No-Break">Postgres database.</span></p>
<h2 id="_idParaDest-172"><a id="_idTextAnchor172"/>How to do it…</h2>
<p>Following on from our previous code, let’s read the data in our database <span class="No-Break">as follows:</span></p>
<ol>
<li><strong class="bold">Creating our DataFrame</strong>: Using the previous connection settings, let’s read the data from PostgreSQL using the <span class="No-Break"><strong class="source-inline">world_population</strong></span><span class="No-Break"> table:</span><pre class="source-code">
df= spark.read.format("jdbc") \
    .options(url="jdbc:postgresql://localhost:5432/postgres",
             dbtable="world_population",
             user="root",
             password="root",
             driver="org.postgresql.Driver") \
    .load()</pre></li>
<li><strong class="bold">Creating a TempView</strong>: Using the<a id="_idIndexMarker369"/> exact name of our table (for organization purposes), we create a temporary view in the Spark<a id="_idIndexMarker370"/> default database from <span class="No-Break">the DataFrame:</span><pre class="source-code">
df.createOrReplaceTempView("world_population")</pre></li>
</ol>
<p>There is no output <span class="No-Break">expected here.</span></p>
<ol>
<li value="3"><strong class="bold">Using SQL to filter data</strong>: With the temporary view, we can use the SQL function from SparkSession, instantiated by the <span class="No-Break"><strong class="source-inline">spark</strong></span><span class="No-Break"> variable:</span><pre class="source-code">
spark.sql("select * from world_population").show(3)</pre></li>
</ol>
<p>Depending on the size of your monitor, the output may look confusing, <span class="No-Break">as follows:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer113">
<img alt="Figure 5.11 – world_population view using Spark SQL" height="388" src="image/Figure_5.11_B19453.jpg" width="1001"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.11 – world_population view using Spark SQL</p>
<ol>
<li value="4"><strong class="bold">Filtering data</strong>: Using a SQL <a id="_idIndexMarker371"/>statement, let’s filter only the South American countries in <span class="No-Break">our DataFrame:</span><pre class="source-code">
south_america = spark.sql("select * from world_population where continent = 'South America' ")</pre></li>
</ol>
<p>Since we attribute the results to a<a id="_idIndexMarker372"/> variable, there is <span class="No-Break">no output.</span></p>
<ol>
<li value="5"><strong class="bold">Using toPandas()</strong>: To ensure this is the correct information that we want to ingest, let’s use the <strong class="source-inline">.toPandas()</strong> function to bring a more <span class="No-Break">user-friendly view:</span><pre class="source-code">
south_america.toPandas()</pre></li>
</ol>
<p>This is how the <span class="No-Break">result appears:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer114">
<img alt="Figure 5.12 – south_america countries with toPandas() visualization" height="648" src="image/Figure_5.12_B19453.jpg" width="1000"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.12 – south_america countries with toPandas() visualization</p>
<ol>
<li value="6"><strong class="bold">Saving our work</strong>: Now, we <a id="_idIndexMarker373"/>can save our filtered data <span class="No-Break">as follows:</span><pre class="source-code">
south_america.write.parquet('south_america_population')</pre></li>
</ol>
<p>Looking at your script’s folder, you <a id="_idIndexMarker374"/>should see a folder named <strong class="source-inline">south_america_population</strong>. Inside, you should see the <span class="No-Break">following output:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer115">
<img alt="Figure 5.13 – south_america data in the Parquet file" height="299" src="image/Figure_5.13_B19453.jpg" width="813"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.13 – south_america data in the Parquet file</p>
<p>This is our filtered and ingested DataFrame in an <span class="No-Break">analytical format.</span></p>
<h2 id="_idParaDest-173"><a id="_idTextAnchor173"/>How it works…</h2>
<p>A significant advantage of working with Spark is the possibility of using SQL statements to filter and query data from a DataFrame. It allows data analytics and BI teams to help the data engineers by<a id="_idIndexMarker375"/> handling queries. This helps to build analytical data and insert it into <span class="No-Break">data warehouses.</span></p>
<p>Nevertheless, there are <a id="_idIndexMarker376"/>some considerations we need to take to execute a SQL statement properly. One of them is using <strong class="source-inline">.createOrReplaceTempView()</strong>, as seen in this line <span class="No-Break">of code:</span></p>
<pre class="source-code">
df.createOrReplaceTempView("world_population")</pre>
<p>Behind the scenes, this temporary view will work as a SQL table and organize the data from the DataFrame without needing <span class="No-Break">physical files.</span></p>
<p>Then we used the instantiated <strong class="source-inline">SparkSession</strong> variable to execute the SQL statements. Note that the name of the table is the same as the <span class="No-Break">temporary view:</span></p>
<pre class="source-code">
spark.sql("select * from world_population").show(3)</pre>
<p>After doing the SQL queries we required, we proceeded to save our files using the .<strong class="source-inline">write()</strong> method, <span class="No-Break">as follows:</span></p>
<pre class="source-code">
south_america.write.parquet('south_america_population')</pre>
<p>The parameter inside the <strong class="source-inline">parquet()</strong> method defines the file’s path and name. Several other configurations are available when writing Parquet files, which we will cover later, in <a href="B19453_07.xhtml#_idTextAnchor227"><span class="No-Break"><em class="italic">Chapter 7</em></span></a><span class="No-Break">.</span></p>
<h2 id="_idParaDest-174"><a id="_idTextAnchor174"/>There’s more…</h2>
<p>Although we used a temporary view to make our SQL statements, it is also possible to use the filtering and aggregation functions from the DataFrame. Let’s use the example from this recipe by filtering only the South <span class="No-Break">American countries:</span></p>
<pre class="source-code">
df.filter(df['continent'] == 'South America').show(10)</pre>
<p>You should see the <span class="No-Break">following output:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer116">
<img alt="Figure 5.14 – South American countries filtered using DataFrame operations" height="737" src="image/Figure_5.14_B19453.jpg" width="1004"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.14 – South American countries filtered using DataFrame operations</p>
<p>It is essential to<a id="_idIndexMarker377"/> understand that not all SQL functions can be used <a id="_idIndexMarker378"/>as DataFrame operations. You can see more practical examples of filtering and aggregation functions using DataFrame operations <span class="No-Break">at </span><a href="https://spark.apache.org/docs/2.2.0/sql-programming-guide.xhtml"><span class="No-Break">https://spark.apache.org/docs/2.2.0/sql-programming-guide.xhtml</span></a><span class="No-Break">.</span></p>
<h2 id="_idParaDest-175"><a id="_idTextAnchor175"/>See also</h2>
<p><em class="italic">TowardsDataScience</em> has a fantastic blog post about SQL functions using PySpark, <span class="No-Break">at </span><a href="https://towardsdatascience.com/pyspark-and-sparksql-basics-6cb4bf967e53"><span class="No-Break">https://towardsdatascience.com/pyspark-and-sparksql-basics-6cb4bf967e53</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-176"><a id="_idTextAnchor176"/>Connecting to a NoSQL database (MongoDB)</h1>
<p>MongoDB is <a id="_idIndexMarker379"/>an open source, unstructured, document-oriented database made in C++. It is well known in the data world for its scalability, flexibility, <span class="No-Break">and speed.</span></p>
<p>As someone who will work with data (or maybe already does), it is essential to know how to explore a MongoDB (or any other unstructured) database. MongoDB has some peculiarities, which we will explore <span class="No-Break">practically here.</span></p>
<p>In this recipe, you will learn how to <a id="_idIndexMarker380"/>create a connection to access MongoDB documents via Studio 3T Free, a <span class="No-Break">MongoDB GUI.</span></p>
<h2 id="_idParaDest-177"><a id="_idTextAnchor177"/>Getting ready</h2>
<p>To start our work with this robust database, first, we need to install and create a MongoDB server on our local machine. We already configured a MongoDB Docker container in <a href="B19453_01.xhtml#_idTextAnchor022"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>, so let’s get it up and running. You can do this using Docker Desktop or via the command line using the <span class="No-Break">following command:</span></p>
<pre class="source-code">
my-project/mongo-local$ docker run \
--name mongodb-local \
-p 27017:27017 \
-e MONGO_INITDB_ROOT_USERNAME=&lt;your_username&gt; \
-e MONGO_INITDB_ROOT_PASSWORD=&lt;your_password&gt;\
-d mongo:latest</pre>
<p>Don’t forget to change the variables using the username and password of <span class="No-Break">your choice.</span></p>
<p>On Docker Desktop, you should see <span class="No-Break">the following:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer117">
<img alt="Figure 5.15 – MongoDB Docker container running" height="44" src="image/Figure_5.15_B19453.jpg" width="223"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.15 – MongoDB Docker container running</p>
<p>The next step is to download and configure Studio 3T Free, free software the development community uses to connect to MongoDB servers. You can download this software from <a href="https://studio3t.com/download-studio3t-free">https://studio3t.com/download-studio3t-free</a> and follow the installer’s steps for your <span class="No-Break">given OS.</span></p>
<p>During the installation, a message may appear like that shown in the following figure. If so, you can leave the fields blank. We don’t need password encryption for local or <span class="No-Break">testing purposes.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer118">
<img alt="Figure 5.16 – Studio 3T Free password encryption message" height="607" src="image/Figure_5.16_B19453.jpg" width="845"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.16 – Studio 3T Free password encryption message</p>
<p>When the<a id="_idIndexMarker381"/> installation process is finished, you will see the <span class="No-Break">following window:</span></p>
<p class="IMG---Figure"> </p>
<div>
<div class="IMG---Figure" id="_idContainer119">
<img alt="Figure 5.17 – Studio 3T Free connection window" height="623" src="image/Figure_5.17_B19453.jpg" width="916"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.17 – Studio 3T Free connection window</p>
<p>We are now ready to connect our MongoDB instance to <span class="No-Break">the IDE.</span></p>
<h2 id="_idParaDest-178"><a id="_idTextAnchor178"/>How to do it…</h2>
<p>Now that we have Studio 3T installed, let’s connect to <a id="_idIndexMarker382"/>our local <span class="No-Break">MongoDB instance:</span></p>
<ol>
<li><strong class="bold">Creating the connection</strong>: Right after you open Studio 3T, a window will appear and ask you to insert the connection string or manually configure it. Select the second option and click <span class="No-Break">on </span><span class="No-Break"><strong class="bold">Next</strong></span><span class="No-Break">.</span></li>
</ol>
<p>You will have something <span class="No-Break">like this:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer120">
<img alt="Figure 5.18 – Studio 3T New Connection initial options" height="553" src="image/Figure_5.18_B19453.jpg" width="747"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.18 – Studio 3T New Connection initial options</p>
<ol>
<li value="2"><strong class="bold">Inserting the connection information</strong>: In the new window that appears, give your database connection an appropriate name in the <strong class="bold">Connection name </strong>field. Leave the information filled in by default by Studio 3T, where <strong class="bold">Connection Type</strong> is set to <strong class="source-inline">localhost</strong> and <strong class="bold">Port</strong> <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">27017</strong></span><span class="No-Break">.</span></li>
</ol>
<p>Your screen should look as follows <span class="No-Break">for now:</span></p>
<p class="IMG---Figure"> </p>
<div>
<div class="IMG---Figure" id="_idContainer121">
<img alt="Figure 5.19 – New Connection server information" height="645" src="image/Figure_5.19_B19453.jpg" width="591"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.19 – New Connection server information</p>
<p>Now select the <strong class="bold">Authentication</strong> tab under the <strong class="bold">Connection group</strong> field, and from the <strong class="bold">Authentication Mode</strong> drop-down menu, <span class="No-Break">choose </span><span class="No-Break"><strong class="bold">Basic</strong></span><span class="No-Break">.</span></p>
<p>Three fields will<a id="_idIndexMarker383"/> appear—<strong class="bold">User name</strong>, <strong class="bold">Password</strong>, and <strong class="bold">Authentication DB</strong>. Fill them in with the credentials you used when creating the Docker image for MongoDB, and enter <strong class="source-inline">admin</strong> in the <strong class="bold">Authentication </strong><span class="No-Break"><strong class="bold">DB</strong></span><span class="No-Break"> field.</span></p>
<p class="IMG---Figure"> </p>
<div>
<div class="IMG---Figure" id="_idContainer122">
<img alt="Figure 5.20 – New Connection Authentication information" height="654" src="image/Figure_5.20_B19453.jpg" width="571"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.20 – New Connection Authentication information</p>
<ol>
<li value="3"><strong class="bold">Testing our connection</strong>: With this<a id="_idIndexMarker384"/> configuration, we should be able to test our database connection. In the lower-left corner, select the <strong class="bold">Test </strong><span class="No-Break"><strong class="bold">Connection</strong></span><span class="No-Break"> button.</span></li>
</ol>
<p>If the credentials you provided are correct, you will see the <span class="No-Break">following output:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer123">
<img alt="Figure 5.21 – Test connection successful" height="637" src="image/Figure_5.21_B19453.jpg" width="641"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.21 – Test connection successful</p>
<p>Click on the <strong class="bold">Save</strong> button, and<a id="_idIndexMarker385"/> the window <span class="No-Break">will close.</span></p>
<ol>
<li value="4"><strong class="bold">Connecting to our database</strong>: After we save our configuration, a window with the available connections will appear, including our newly <span class="No-Break">created one:</span></li>
</ol>
<p class="IMG---Figure"> </p>
<div>
<div class="IMG---Figure" id="_idContainer124">
<img alt="Figure 5.22 – Connection manager with the connection created" height="544" src="image/Figure_5.22_B19453.jpg" width="906"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.22 – Connection manager with the connection created</p>
<p>Select the <strong class="bold">Connect</strong> button, and<a id="_idIndexMarker386"/> three default databases will appear: <strong class="bold">admin</strong>, <strong class="bold">config</strong>, and <strong class="bold">local</strong>, as shown in the <span class="No-Break">following screenshot:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer125">
<img alt="Figure 5.23 – The main page of the local MongoDB with the default databases on the server" height="886" src="image/Figure_5.23_B19453.jpg" width="1378"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.23 – The main page of the local MongoDB with the default databases on the server</p>
<p>We have now finished our MongoDB configuration and are ready for the following recipes in this chapter and others, including <em class="italic">Chapters 6</em>, <em class="italic">11</em>, <span class="No-Break">and </span><span class="No-Break"><em class="italic">12</em></span><span class="No-Break">.</span></p>
<h2 id="_idParaDest-179"><a id="_idTextAnchor179"/>How it works…</h2>
<p>Like<a id="_idIndexMarker387"/> available databases, creating and running MongoDB through a Docker container is straightforward. Check the <span class="No-Break">following commands:</span></p>
<pre class="source-code">
my-project/mongo-local$ docker run \
--name mongodb-local \
-p 27017:27017 \
-e MONGO_INITDB_ROOT_USERNAME=&lt;your_username&gt; \
-e MONGO_INITDB_ROOT_PASSWORD=&lt;your_password&gt;\
-d mongo:latest</pre>
<p>As we saw in the <em class="italic">Getting ready</em> section, the most crucial information to be passed is the username and password (using the <strong class="source-inline">-e</strong> parameter), the ports over which to connect (using the <strong class="source-inline">-p</strong> parameter), and the container image version, which is the <span class="No-Break">latest available.</span></p>
<p>The architecture of the MongoDB container connected to Studio 3T Free is even more straightforward. Once the connection port is available, we can easily access the database. You can see the architectural representation <span class="No-Break">as follows:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer126">
<img alt="Figure 5.24 – MongoDB with Docker image connected to Studio 3T Free" height="427" src="image/Figure_5.24_B19453.jpg" width="752"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.24 – MongoDB with Docker image connected to Studio 3T Free</p>
<p>As described at the beginning of this recipe, MongoDB is a document-oriented database. Its structure is similar to a JSON file, except each line is interpreted as a document and has its own <strong class="source-inline">ObjectId</strong>, <span class="No-Break">as follows:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer127">
<img alt="Figure 5.25 – MongoDB document format" height="238" src="image/Figure_5.25_B19453.jpg" width="314"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.25 – MongoDB document format</p>
<p>The group of documents<a id="_idIndexMarker388"/> is referred to as a <em class="italic">collection</em>, which is <a id="_idIndexMarker389"/>better understood as a table representation in a structured database. You can see how it is hierarchically organized in the schema <span class="No-Break">shown here:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer128">
<img alt="Figure 5.26 – MongoDB data structure" height="538" src="image/Figure_5.26_B19453.jpg" width="1047"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.26 – MongoDB data structure</p>
<p>As we observed when logging in to MongoDB using Studio 3T Free, there are three default databases: <strong class="source-inline">admin</strong>, <strong class="source-inline">config</strong>, and <strong class="source-inline">local</strong>. For now, let’s disregard the last two since they pertain to operational working of the data. The <strong class="source-inline">admin</strong> database is the main one created by the <strong class="source-inline">root</strong> user. That’s why we provided this database for the <strong class="bold">Authentication DB</strong> option in <span class="No-Break"><em class="italic">step 3</em></span><span class="No-Break">.</span></p>
<p>Creating a user to ingest <a id="_idIndexMarker390"/>data and access specific databases or collections is generally recommended. However, we will keep using root access here and in the following recipes in this book for <span class="No-Break">demonstration purposes.</span></p>
<h2 id="_idParaDest-180"><a id="_idTextAnchor180"/>There’s more…</h2>
<p>The connection string will vary depending on how your MongoDB server is configured. For instance, when <em class="italic">replicas</em> or <em class="italic">sharded clusters</em> are in place, we need to specify which instances we want to <span class="No-Break">connect to.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">Sharded clusters are a complex and interesting topic. You can read more and go deeper on the topic in MongoDB’s official documentation <span class="No-Break">at </span><a href="https://www.mongodb.com/docs/manual/core/sharded-cluster-components/"><span class="No-Break">https://www.mongodb.com/docs/manual/core/sharded-cluster-components/</span></a><span class="No-Break">.</span></p>
<p>Let’s see an example of a standalone server string connection using basic <span class="No-Break">authentication mode:</span></p>
<pre class="source-code">
mongodb://mongo-server-user:some_password@mongo-host01.example.com:27017/?authSource=admin</pre>
<p>As you can see, it is similar to other database connections. If we wanted to connect to a local server, we would change the host <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">localhost</strong></span><span class="No-Break">.</span></p>
<p>Now, for a replica or sharded cluster, the string connection looks <span class="No-Break">like this:</span></p>
<pre class="source-code">
mongodb://mongo-server-user:some_password@mongo-host01.example.com:27017, mongo-host02.example.com:27017, mongo-hosta03.example.com:27017/?authSource=admin</pre>
<p>The <strong class="source-inline">authSource=admin</strong> parameter in this URI is essential to inform MongoDB that we want to authenticate using the administration user of the database. Without it, an error or authentication will be raised, like the <span class="No-Break">following output:</span></p>
<pre class="source-code">
MongoError: Authentication failed</pre>
<p>Another way to avoid<a id="_idIndexMarker391"/> this error is to create a specific user to access the database <span class="No-Break">and collection.</span></p>
<h3>SRV URI connection</h3>
<p>MongoDB introduced the <strong class="bold">Domain Name System </strong>(<strong class="bold">DNS</strong>) seed list <a id="_idIndexMarker392"/>connection, constructed <a id="_idIndexMarker393"/>by a <strong class="bold">DNS Service Record </strong>(<strong class="bold">SRV</strong>) specification <a id="_idIndexMarker394"/>of data in the DNS, to try to solve this verbose string. We saw the possibility of using an SRV URI to configure the MongoDB connection in the first step of <span class="No-Break">this recipe.</span></p>
<p>Here’s an example of how <span class="No-Break">it looks:</span></p>
<pre class="source-code">
mongodb+srv://my-server.example.com/</pre>
<p>It is similar to the standard connection string format we saw earlier. However, we need to indicate the use of SRV at the beginning and then provide the <span class="No-Break">DNS entry.</span></p>
<p>This type of connection is advantageous when handling replicas or nodes since the SRV creates a single identity for the cluster. You can find a more detailed explanation of this, along with an outline of how to configure it, in the MongoDB official documentation <span class="No-Break">at </span><a href="https://www.mongodb.com/docs/manual/reference/connection-string/#dns-seed-list-connection-format"><span class="No-Break">https://www.mongodb.com/docs/manual/reference/connection-string/#dns-seed-list-connection-format</span></a><span class="No-Break">.</span></p>
<h2 id="_idParaDest-181"><a id="_idTextAnchor181"/>See also</h2>
<p>If you are interested, other <a id="_idIndexMarker395"/>MongoDB GUI tools are available on the <span class="No-Break">market: </span><a href="https://www.guru99.com/top-20-mongodb-tools.xhtml"><span class="No-Break">https://www.guru99.com/top-20-mongodb-tools.xhtml</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-182"><a id="_idTextAnchor182"/>Creating our NoSQL table in MongoDB</h1>
<p>After successfully<a id="_idIndexMarker396"/> connecting and understanding how Studio <a id="_idIndexMarker397"/>3T works, we will now import some MongoDB collections. We have seen in the <em class="italic">Connecting to a NoSQL database (MongoDB) </em>recipe how to get started with MongoDB, and in this recipe, we will import a MongoDB database and come to understand its structure. Although MongoDB has a specific format to <a id="_idIndexMarker398"/>organize data internally, understanding how a NoSQL database behaves is crucial <a id="_idIndexMarker399"/>when working with <span class="No-Break">data ingestion.</span></p>
<p>We will practice by ingesting the imported collections in the following recipes in <span class="No-Break">this chapter.</span></p>
<h2 id="_idParaDest-183"><a id="_idTextAnchor183"/>Getting ready</h2>
<p>For this recipe, we will use a sample dataset of Airbnb reviews called <strong class="source-inline">listingsAndReviews.json</strong>. You can find this dataset in the GitHub repository of this book at <a href="https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_5/datasets/sample_airbnb">https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_5/datasets/sample_airbnb</a>. After downloading it, put the file into our <strong class="source-inline">mongo-local</strong> directory, created in <a href="B19453_01.xhtml#_idTextAnchor022"><span class="No-Break"><em class="italic">Chapter 1</em></span></a><span class="No-Break">.</span></p>
<p>I kept mine inside the <strong class="source-inline">sample_airbnb</strong> folder just for organization purposes, as you can see in the <span class="No-Break">following screenshot:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer129">
<img alt="Figure 5.27 – Command line with listingsAndReviews.json" height="36" src="image/Figure_5.27_B19453.jpg" width="649"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.27 – Command line with listingsAndReviews.json</p>
<p>After downloading the dataset, we need to install <strong class="source-inline">pymongo</strong>, a Python library to connect to and manage MongoDB operations. To install it, use the <span class="No-Break">following command:</span></p>
<pre class="source-code">
$ pip3 install pymongo</pre>
<p>Feel free to create <strong class="source-inline">virtualenv</strong> for <span class="No-Break">this installation.</span></p>
<p>We are now ready to start inserting data into MongoDB. Don’t forget to check that your Docker image is up and running before <span class="No-Break">we begin.</span></p>
<h2 id="_idParaDest-184"><a id="_idTextAnchor184"/>How to do it…</h2>
<p>Here are the steps to perform <span class="No-Break">this recipe:</span></p>
<ol>
<li><strong class="bold">Creating our connection</strong>: By importing and using <strong class="source-inline">pymongo</strong>, we can easily establish a connection with the MongoDB database. Refer to the <span class="No-Break">following code:</span><pre class="source-code">
import json
import os
from pymongo import MongoClient, InsertOne
mongo_client = pymongo.MongoClient("mongodb://root:root@localhost:27017/")</pre></li>
<li><strong class="bold">Defining our database and collection</strong>: We will create a database and collection instance using the<a id="_idIndexMarker400"/> client connection <span class="No-Break">we instantiated.</span></li>
</ol>
<p>For the <strong class="source-inline">json_collection</strong> variable, insert the <a id="_idIndexMarker401"/>path where you put the Airbnb <span class="No-Break">sample dataset:</span></p>
<pre class="source-code">
db_cookbook = mongo_client.db_airbnb
collection = db_cookbook.reviews
json_collection = "sample_airbnb/listingsAndReviews.json"</pre>
<ol>
<li value="3"><strong class="bold">Reading and bulk-inserting data</strong>: Using the <strong class="source-inline">bulk_write</strong> function, we will insert all the documents inside the JSON file into the sales collection we created and close <span class="No-Break">the connection:</span><pre class="source-code">
requesting_collection = []
with open(json_collection) as f:
    for object in f:
        my_dict = json.loads(object)
        requesting.append(InsertOne(my_dict))
result = collection.bulk_write(requesting_collection)
mongo_client.close()</pre></li>
</ol>
<p>No output is expected from this operation, but we can check the database to see if it <span class="No-Break">is successful.</span></p>
<ol>
<li value="4"><strong class="bold">Checking the MongoDB database results</strong>: Let’s check our database to see if the data was <span class="No-Break">inserted </span><span class="No-Break"><a id="_idIndexMarker402"/></span><span class="No-Break">correctly.</span></li>
</ol>
<p>Open Studio 3T Free and <a id="_idIndexMarker403"/>refresh the connection (right-click on the connection name and select <strong class="bold">Refresh All</strong>). You should see a new database named <strong class="bold">db_airbnb</strong> has been created, containing a <strong class="bold">reviews</strong> collection, as shown in the <span class="No-Break">following screenshot:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer130">
<img alt="Figure 5.28 – Database and collection successfully created on MongoDB" height="514" src="image/Figure_5.28_B19453.jpg" width="1176"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.28 – Database and collection successfully created on MongoDB</p>
<p>With the collection now created and containing some data, let’s go deeper into how the <span class="No-Break">code works.</span></p>
<h2 id="_idParaDest-185"><a id="_idTextAnchor185"/>How it works…</h2>
<p>As you can see, the code we implemented is straightforward, using just a few lines to create and insert data in our database. However, there are important points to pay attention to due to the particularities <span class="No-Break">of MongoDB.</span></p>
<p>Let’s examine the code line by <span class="No-Break">line now:</span></p>
<pre class="source-code">
mongo_client = pymongo.MongoClient("mongodb://root:root@localhost:27017/")</pre>
<p>This line defines the<a id="_idIndexMarker404"/> connection to our MongoDB database, and from <a id="_idIndexMarker405"/>this instance, we can create a new database and <span class="No-Break">its collections.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">Observe that the URI connection contains hardcoded values for the username and password. This must be avoided in real applications, and even development servers. It is recommended to store those values as environment variables or use a secret <span class="No-Break">manager vault.</span></p>
<p>Next, we define the database and collection names; you may have noticed we didn’t create them previously in our database. At the time of execution of the code, MongoDB checks whether the database exists; if not, MongoDB will create it. The same rule applies to the <span class="No-Break"><strong class="bold">reviews</strong></span><span class="No-Break"> collection.</span></p>
<p>Notice the collection derives from the <strong class="source-inline">db_cookbook</strong> instance, which makes it clear that it is linked to the <span class="No-Break"><strong class="source-inline">db_airbnb</strong></span><span class="No-Break"> database:</span></p>
<pre class="source-code">
db_cookbook = mongo_client.db_airbnb
collection = db_cookbook.reviews</pre>
<p>Following the code, the next step is to open the JSON file and parse every line. Here we start to see some tricky peculiarities <span class="No-Break">of MongoDB:</span></p>
<pre class="source-code">
requesting_collection = []
with open(json_collection) as f:
    for object in f:
        my_dict = json.loads(object)
        requesting_collection.append(InsertOne(my_dict))</pre>
<p>It is common to wonder why we actually need to parse the lines of JSON, since MongoDB accepts this format. Let’s check our <strong class="source-inline">listingsAndReviews.json</strong> file, as shown in the <span class="No-Break">following screenshot:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer131">
<img alt="Figure 5.29 – JSON file with MongoDB document lines" height="296" src="image/Figure_5.29_B19453.jpg" width="1379"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.29 – JSON file with MongoDB document lines</p>
<p>If we use any tool to verify<a id="_idIndexMarker406"/> this as valid JSON, it will certainly say it’s not a valid format. This happens because<a id="_idIndexMarker407"/> each line of this file represents one document of the MongoDB collection. Trying to open that file using only the conventional <strong class="source-inline">open()</strong> and <strong class="source-inline">json.loads()</strong> methods will produce an error like <span class="No-Break">the following:</span></p>
<pre class="source-code">
json.decoder.JSONDecodeError: Extra data: line 2 column 1 (char 190)</pre>
<p>To make it acceptable to the Python interpreter, we need to open and read each line individually and append it to the <strong class="source-inline">requesting_collection</strong> list. Also, the <strong class="source-inline">InsertOne()</strong> method will ensure that each line is inserted separately. A problem that occurs while inserting a specific row will be much easier <span class="No-Break">to identify.</span></p>
<p>Finally, the <strong class="source-inline">bulk_write()</strong> will take the list of documents and insert them into the <span class="No-Break">MongoDB database:</span></p>
<pre class="source-code">
result = collection.bulk_write(requesting_collection)</pre>
<p>This operation will finish without returning any output or error messages if everything <span class="No-Break">is OK.</span></p>
<h2 id="_idParaDest-186"><a id="_idTextAnchor186"/>There’s more…</h2>
<p>We have seen how simple it is to create a Python script to insert data into our MongoDB server. Nevertheless, MongoDB has database tools to provide the same result and can be executed via the command line. The <strong class="source-inline">mongoimport</strong> command is used to insert data into our database, as you can see in the <span class="No-Break">following code:</span></p>
<pre class="source-code">
mongoimport --host localhost --port 27017-d db_name -c collection_name --file path/to/file.json</pre>
<p>If you are interested to learn more about the other database tools and commands available, check the official MongoDB documentation <span class="No-Break">at </span><a href="https://www.mongodb.com/docs/database-tools/installation/installation/"><span class="No-Break">https://www.mongodb.com/docs/database-tools/installation/installation/</span></a><span class="No-Break">.</span></p>
<h3>Restrictions on field names</h3>
<p>When loading data into<a id="_idIndexMarker408"/> MongoDB, one big problem is the restrictions on characters used in the field names. Due to MongoDB server versions or programming language specificities, sometimes the key names of fields come with a <strong class="source-inline">$</strong> prefix, and, by default, MongoDB is not compatible with it, creating an error like the <span class="No-Break">following output:</span></p>
<pre class="source-code">
localhost:27017: $oid is not valid for storage.</pre>
<p>In this case, a JSON file dump was exported from a MongoDB server, and the reference of <strong class="source-inline">ObjectID</strong> came with the <strong class="source-inline">$</strong> prefix. Even though the more recent versions of MongoDB have started to accept these characters (see the thread here: <a href="https://jira.mongodb.org/browse/SERVER-41628?fbclid=IwAR1t5Ld58LwCi69SrMCcDbhPGf2EfBWe_AEurxGkEWHpZTHaEIde0_AZ-uM%5D">https://jira.mongodb.org/browse/SERVER-41628?fbclid=IwAR1t5Ld58LwCi69SrMCcDbhPGf2EfBWe_AEurxGkEWHpZTHaEIde0_AZ-uM%5D</a>), it is a good practice to avoid using them where possible. In this case, we have two main options: remove<a id="_idIndexMarker409"/> all the restricted characters using a script, or encode the JSON file into a <strong class="bold">Binary JavaScript Object Notation </strong>(<strong class="bold">BSON</strong>) file. You can find out more about encoding the file into BSON format <span class="No-Break">at </span><a href="https://kb.objectrocket.com/mongo-db/how-to-use-python-to-encode-a-json-file-into-mongodb-bson-documents-545"><span class="No-Break">https://kb.objectrocket.com/mongo-db/how-to-use-python-to-encode-a-json-file-into-mongodb-bson-documents-545</span></a><span class="No-Break">.</span></p>
<h2 id="_idParaDest-187"><a id="_idTextAnchor187"/>See also</h2>
<p>You can read <a id="_idIndexMarker410"/>more about the MongoDB restrictions on field names <span class="No-Break">at </span><a href="https://www.mongodb.com/docs/manual/reference/limits/#mongodb-limit-Restrictions-on-Field-Names"><span class="No-Break">https://www.mongodb.com/docs/manual/reference/limits/#mongodb-limit-Restrictions-on-Field-Names</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-188"><a id="_idTextAnchor188"/>Ingesting data from MongoDB using PySpark</h1>
<p>Even though it seems<a id="_idIndexMarker411"/> impractical to create and ingest the <a id="_idIndexMarker412"/>data ourselves, this exercise can be applied to real-life projects. People who work with data are often involved in the architectural process of defining the type of database, helping other engineers to insert data from applications into a database server, and later ingesting only the relevant information for dashboards or other <span class="No-Break">analytical tools.</span></p>
<p>So far, we have created and evaluated our server and then created collections inside our MongoDB instance. With all this <a id="_idIndexMarker413"/>preparation, we can now ingest our data <span class="No-Break">using PySpark.</span></p>
<h2 id="_idParaDest-189"><a id="_idTextAnchor189"/>Getting ready</h2>
<p>This recipe requires the <a id="_idIndexMarker414"/>execution of the <em class="italic">Creating our NoSQL table in MongoDB</em> recipe due to data insertion. However, you can create and insert other documents into the MongoDB database and use them here. If you do this, ensure you set the suitable configurations to make it <span class="No-Break">run properly.</span></p>
<p>Also, as in the <em class="italic">Creating our NoSQL table in MongoDB</em> recipe, check that the Docker container is up and running since this is our MongoDB instance’s primary data source. Let’s proceed to <span class="No-Break">the ingesting!</span></p>
<div>
<div class="IMG---Figure" id="_idContainer132">
<img alt="Figure 5.30 – Docker container for MongoDB is running" height="44" src="image/Figure_5.30._B19453.jpg" width="233"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.30 – Docker container for MongoDB is running</p>
<h2 id="_idParaDest-190"><a id="_idTextAnchor190"/>How to do it…</h2>
<p>You need to perform the following steps to try <span class="No-Break">this recipe:</span></p>
<ol>
<li><strong class="bold">Creating the SparkSession</strong>: As usual, the first thing to do is create <strong class="source-inline">SparkSession</strong>, but this time passing specific configurations for reading our MongoDB database, <strong class="source-inline">db_airbnb</strong>, such as the URI and <span class="No-Break">the </span><span class="No-Break"><strong class="source-inline">.jars</strong></span><span class="No-Break">:</span><pre class="source-code">
from pyspark.sql import SparkSession
spark = SparkSession.builder \
      .master("local[1]") \
      .appName("MongoDB Ingest") \
      .config("spark.executor.memory", '3g') \
      .config("spark.executor.cores", '1') \
      .config("spark.cores.max", '1') \
      .config("spark.mongodb.input.uri", "mongodb://root:root@127.0.0.1/db_airbnb?authSource=admin&amp;readPreference=primaryPreferred") \
      .config("spark.mongodb.input.collection", "reviews") \
      .config("spark.jars.packages","org.mongodb.spark:mongo-spark-connector_2.12:3.0.1") \
      .getOrCreate()</pre></li>
</ol>
<p>We should expect a <a id="_idIndexMarker415"/>significant output<a id="_idIndexMarker416"/> here since Spark downloads the package and sets the rest of the configuration <span class="No-Break">we passed:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer133">
<img alt="Figure 5.31 – SparkSession being initialized with MongoDB configurations" height="605" src="image/Figure_5.31._B19453.jpg" width="1087"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.31 – SparkSession being initialized with MongoDB configurations</p>
<ol>
<li><strong class="bold">Reading the reviews collection</strong>: With the connection established, we can now read the collection<a id="_idIndexMarker417"/> using <strong class="source-inline">SparkSession</strong> we instantiated. Here, no output is expected <a id="_idIndexMarker418"/>because the <strong class="source-inline">SparkSession</strong> is set only to send logs at the <span class="No-Break"><strong class="source-inline">WARN</strong></span><span class="No-Break"> level:</span><pre class="source-code">
df = spark.read.format("mongo").load()</pre></li>
<li><strong class="bold">Getting our DataFrame schema</strong>: We can see the collection’s schema using the print operation on <span class="No-Break">the DataFrame:</span><pre class="source-code">
df.printSchema()</pre></li>
</ol>
<p>You should observe the <span class="No-Break">following output:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer134">
<img alt="Figure 5.32 – Reviews DataFrame collection schema printed" height="658" src="image/Figure_5.32._B19453.jpg" width="555"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.32 – Reviews DataFrame collection schema printed</p>
<p>As you can observe, the structure is similar to a JSON file with nested objects. Unstructured data is usually presented in this form and can hold a large amount of information to create a Python script to insert data into our data. Now, let’s go deeper and understand <span class="No-Break">our code.</span></p>
<h2 id="_idParaDest-191"><a id="_idTextAnchor191"/>How it works…</h2>
<p>MongoDB required a few<a id="_idIndexMarker419"/> additional configurations in <strong class="source-inline">SparkSession</strong> to execute the <strong class="source-inline">.read</strong> function. It is essential to understand why we used the configurations instead of just using code from the documentation. Let’s<a id="_idIndexMarker420"/> explore the code <span class="No-Break">for it:</span></p>
<pre class="source-code">
config("spark.mongodb.input.uri", "mongodb://root:root@127.0.0.1/db_airbnb?authSource=admin) \</pre>
<p>Note the use of <strong class="source-inline">spark.mongodb.input.uri</strong>, which tells our <strong class="source-inline">SparkSession</strong> that a <em class="italic">read</em> operation needs to be performed using a MongoDB URI. If, for instance, we wanted to do a <em class="italic">write</em> operation (or both read and write), we would just need to add the <span class="No-Break"><strong class="source-inline">spark.mongodb.output.uri</strong></span><span class="No-Break"> configuration.</span></p>
<p>Next, we pass the URI containing the user and password information, the name of the database, and the authentication source. Since we use the root user to retrieve the data, this last parameter is set <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">admin</strong></span><span class="No-Break">.</span></p>
<p>Next, we define the name of our collection to be used in the <span class="No-Break">read operation:</span></p>
<pre class="source-code">
.config("spark.mongodb.input.collection", "reviews")\</pre>
<p class="callout-heading">Note</p>
<p class="callout">Even though it might seem odd to define these parameters in the SparkSession, and it is possible to set the database and collection, this is a good practice that has been adopted by the community when manipulating <span class="No-Break">MongoDB connections.</span></p>
<pre class="source-code">
.config("spark.jars.packages","org.mongodb.spark:mongo-spark-connector_2.12:3.0.1")</pre>
<p>Another new configuration here is <strong class="source-inline">spark.jars.packages</strong>. When using this key with the <strong class="source-inline">.config()</strong> method, Spark will search its available online packages, download them, and place them in the <strong class="source-inline">.jar</strong> folders to be used. Although this is an advantageous way to set the <strong class="source-inline">.jar</strong> connectors, this is not available for <span class="No-Break">all databases.</span></p>
<p>Once the connection is established, the reading process is remarkably similar to the JDBC: we pass the <strong class="source-inline">.format()</strong> of the database (here, <strong class="source-inline">mongo</strong>), and since the database and collection name are already set, we <a id="_idIndexMarker421"/>don’t need to <span class="No-Break">configure </span><span class="No-Break"><strong class="source-inline">.option()</strong></span><span class="No-Break">:</span></p>
<pre class="source-code">
df = spark.read.format("mongo").load()</pre>
<p>When executing <strong class="source-inline">.load()</strong>, Spark will verify whether the connection is valid and throw an error if not. In the following<a id="_idIndexMarker422"/> screenshot, you can see an example of the error message when the credentials are <span class="No-Break">not correct:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer135">
<img alt="Figure 5.33 – Py4JJavaError: Authentication error to MongoDB connection" height="217" src="image/Figure_5.33._B19453.jpg" width="1083"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.33 – Py4JJavaError: Authentication error to MongoDB connection</p>
<p>Even though we are handling an unstructured data format, as soon as PySpark transforms our collection into a DataFrame, all the filtering, cleaning, and manipulating of data is pretty much the same as <span class="No-Break">PySpark data.</span></p>
<h2 id="_idParaDest-192"><a id="_idTextAnchor192"/>There’s more…</h2>
<p>As we saw previously, PySpark error messages can be confusing and cause discomfort at first glance. Let’s explore other common errors when ingesting data from a MongoDB database without the <span class="No-Break">proper configuration.</span></p>
<p>In this example, let’s not set <strong class="source-inline">spark.jars.packages</strong> in the <span class="No-Break"><strong class="source-inline">SparkSession</strong></span><span class="No-Break"> configuration:</span></p>
<pre class="source-code">
spark = SparkSession.builder \
      (...)
      .config("spark.mongodb.input.uri", "mongodb://root:root@127.0.0.1/db_aibnb?authSource=admin") \
      .config("spark.mongodb.input.collection", "reviews")
     .getOrCreate()
df = spark.read.format("mongo").load()</pre>
<p>If you try to execute the<a id="_idIndexMarker423"/> preceding code (passing <a id="_idIndexMarker424"/>the rest of the memory settings), you will get the <span class="No-Break">following output:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer136">
<img alt="Figure 5.34 – java.lang.ClassNotFoundException error when the MongoDB package is not set in the configuration" height="659" src="image/Figure_5.34._B19453.jpg" width="1099"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.34 – java.lang.ClassNotFoundException error when the MongoDB package is not set in the configuration</p>
<p>Looking carefully at the second line, which begins with <strong class="source-inline">java.lang.ClassNotFoundException</strong>, the JVM highlights a missing package or class that needs to be searched for in a third-party repository. The package contains the connector code to our JVM and establishes communication with the <span class="No-Break">database server.</span></p>
<p>Another widespread error message is <strong class="source-inline">IllegalArgumentException</strong>. This type of error indicates to the developer that an argument was wrongly passed to a method or class. Usually, when related to database connections, it refers to an invalid string connection, as in the <span class="No-Break">following screenshot:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer137">
<img alt="Figure 5.35 – IllegalArgumentException error when the URI is invalid" height="184" src="image/Figure_5.35._B19453.jpg" width="1097"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.35 – IllegalArgumentException error when the URI is invalid</p>
<p>Although it seems <a id="_idIndexMarker425"/>unclear, there is a typo in the URI, where <strong class="source-inline">db_aibnb/?</strong> contains an extra forward slash. Removing it and running <strong class="source-inline">SparkSession</strong> <a id="_idIndexMarker426"/>again will make this <span class="No-Break">error disappear.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">It is recommended to shut down and restart the kernel processes when re-defining the SparkSession configurations because SparkSession tends to append to the processes rather than <span class="No-Break">replacing them.</span></p>
<h2 id="_idParaDest-193"><a id="_idTextAnchor193"/>See also</h2>
<ul>
<li>MongoDB <a id="_idIndexMarker427"/>Spark connector <span class="No-Break">documentation: </span><a href="https://www.mongodb.com/docs/spark-connector/current/configuration/"><span class="No-Break">https://www.mongodb.com/docs/spark-connector/current/configuration/</span></a></li>
<li>You can check the MongoDB documentation for a full explanation of how the MongoDB connector behaves with <span class="No-Break">PySpark: </span><a href="https://www.mongodb.com/docs/spark-connector/current/read-from-mongodb/"><span class="No-Break">https://www.mongodb.com/docs/spark-connector/current/read-from-mongodb/</span></a></li>
<li>There are also<a id="_idIndexMarker428"/> some interesting use cases of MongoDB <span class="No-Break">here: </span><a href="https://www.mongodb.com/use-cases"><span class="No-Break">https://www.mongodb.com/use-cases</span></a></li>
</ul>
<h1 id="_idParaDest-194"><a id="_idTextAnchor194"/>Further reading</h1>
<ul>
<li><a href="https://www.talend.com/resources/structured-vs-unstructured-data/"><span class="No-Break">https://www.talend.com/resources/structured-vs-unstructured-data/</span></a></li>
<li><a href="https://careerfoundry.com/en/blog/data-analytics/structured-vs-unstructured-data/"><span class="No-Break">https://careerfoundry.com/en/blog/data-analytics/structured-vs-unstructured-data/</span></a></li>
<li><a href="https://www.dba-ninja.com/2022/04/is-mongodbsrv-necessary-for-a-mongodb-connection.xhtml"><span class="No-Break">https://www.dba-ninja.com/2022/04/is-mongodbsrv-necessary-for-a-mongodb-connection.xhtml</span></a></li>
<li><a href="https://www.mongodb.com/docs/manual/reference/connection-string/#connection-string-options"><span class="No-Break">https://www.mongodb.com/docs/manual/reference/connection-string/#connection-string-options</span></a></li>
<li><a href="https://sparkbyexamples.com/spark/spark-createorreplacetempview-explained/"><span class="No-Break">https://sparkbyexamples.com/spark/spark-createorreplacetempview-explained/</span></a></li>
</ul>
</div>
</div></body></html>