<html><head></head><body><div class="chapter" title="Chapter&#xA0;3.&#xA0;Learning Logistic Regression / SGD Using Mahout"><div class="titlepage"><div><div><h1 class="title"><a id="ch03"/>Chapter 3. Learning Logistic Regression / SGD Using Mahout</h1></div></div></div><p>Instead of jumping directly into logistic regression, let's try to understand a few of its concepts. In this chapter, we will explore the following topics:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Introducing regression</li><li class="listitem" style="list-style-type: disc">Understanding linear regression</li><li class="listitem" style="list-style-type: disc">Cost function</li><li class="listitem" style="list-style-type: disc">Gradient descent</li><li class="listitem" style="list-style-type: disc">Logistic regression</li><li class="listitem" style="list-style-type: disc">Understanding SGD</li><li class="listitem" style="list-style-type: disc">Using Mahout for logistic regression</li></ul></div><div class="section" title="Introducing regression"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec17"/>Introducing regression</h1></div></div></div><p>Regression <a id="id101" class="indexterm"/>analysis is used for prediction and forecasting. It is used to find out the relationship between explanatory variables and target variables. Essentially, it is a statistical model that is used to find out the relationship among variables present in the datasets. An example that you can refer to for a better understanding of this term is this: determine the earnings of workers in a particular industry. Here, we will try to find out the factors that affect a worker's salary. These factors can be age, education, years of experience, particular skill set, location, and so on. We will try to make a model that will take all these variables into consideration and try to predict the salary. In regression analysis, we characterize the variation of the target variable around the regression function, which can be described by a probability distribution that is also of interest. There are a number of regression analysis techniques that are available. For example, linear regression, ordinary least squares regression, logistic regression, and so on.</p><div class="section" title="Understanding linear regression"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec17"/>Understanding linear regression</h2></div></div></div><p>In linear <a id="id102" class="indexterm"/>regression, we create a model to predict the value of a target variable with the help of an explanatory variable. To understand this better, let's look at an example.</p><p>A company X <a id="id103" class="indexterm"/>that deals in selling coffee has noticed that in the month of monsoon, their sales increased to quite an extent. So they have come up with a formula to find the relation between rain and their per cup coffee sale, which is shown as follows:</p><p>
<span class="emphasis"><em>C = 1.5R+800</em></span>
</p><p>So, for 2 mm of rain, there is a demand of 803 cups of coffee. Now if you go into minute details, you will realize that we have the data for rainfall and per cup coffee sale, and we are trying to build a model that can predict the demand for coffee based on the rainfall. We have data in the form of <span class="emphasis"><em>(R1, C1), (R2, C2)…. (Ri, Ci)</em></span>. Here, we will build the model in a manner that keeps the error in the actual and predicted values at a minimum.</p><div class="section" title="Cost function"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec03"/>Cost function</h3></div></div></div><p>In the <a id="id104" class="indexterm"/>equation <span class="emphasis"><em>C = 1.5R+800</em></span>, the two values 1.5 and 800 are parameters and these values affect the end result. We can write this equation as <span class="emphasis"><em>C= p0+p1R</em></span>. As we discussed earlier, our goal is to reduce the difference between the actual value and the predicted value, and this is dependent on the values of <span class="emphasis"><em>p0</em></span> and <span class="emphasis"><em>p1</em></span>. Let's assume that the predicted value is <span class="emphasis"><em>Cp</em></span> and the actual value is <span class="emphasis"><em>C</em></span> so that the difference will be <span class="emphasis"><em>(Cp-C)</em></span>. This can be written as <span class="emphasis"><em>(p0+p1R-C)</em></span>.To minimize this error, we define the error function, which is also called the <span class="strong"><strong>cost function</strong></span>.</p><p>The cost <a id="id105" class="indexterm"/>function can be defined with the following formula:</p><div class="mediaobject"><img src="graphics/4959OS_03_01.jpg" alt="Cost function"/></div><p>Here, <span class="strong"><strong>i</strong></span> is the ith sample and <span class="strong"><strong>N</strong></span> is the number of training examples. We calculate costs for different sets of <span class="strong"><strong>p0</strong></span> and <span class="strong"><strong>p1</strong></span> and finally select the <span class="strong"><strong>p0</strong></span> and <span class="strong"><strong>p1</strong></span> that gives the least cost (<span class="emphasis"><em>C</em></span>). This is the model that will be used to make predictions for new input.</p></div><div class="section" title="Gradient descent"><div class="titlepage"><div><div><h3 class="title"><a id="ch03lvl3sec04"/>Gradient descent</h3></div></div></div><p>Gradient <a id="id106" class="indexterm"/>descent starts with an initial set of parameter values, <span class="emphasis"><em>p0</em></span> and <span class="emphasis"><em>p1</em></span>, and iteratively moves towards a set of parameter values that minimizes the cost function. We can visualize this error function graphically, where width and length can be considered as the parameters <span class="emphasis"><em>p0</em></span> and <span class="emphasis"><em>p1</em></span> and height as the cost function. Our goal is to find the values for <span class="emphasis"><em>p0</em></span> and <span class="emphasis"><em>p1</em></span> in a way that our cost function will be minimal. We <a id="id107" class="indexterm"/>start the algorithm with some values of <span class="emphasis"><em>p0</em></span> and <span class="emphasis"><em>p1</em></span> and iteratively work towards the minimum value. A good way to ensure that the gradient descent is working correctly is to make sure that the cost function decreases for each iteration. In this case, the cost function surface is convex and we will try to find out the minimum value. This can be seen in the following figure:</p><div class="mediaobject"><img src="graphics/4959OS_03_02.jpg" alt="Gradient descent"/></div></div></div><div class="section" title="Logistic regression"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec18"/>Logistic regression</h2></div></div></div><p>Logistic regression <a id="id108" class="indexterm"/>is used to ascertain the probability of an event. Generally, logistic regression refers to problems where the outcome is binary, for example, in building a model that is based on a customer's income, travel uses, gender, and other features to predict whether he or she will buy a particular car or not. So, the answer will be a simple yes or no. When the outcome is composed of more than one category, this is called <span class="strong"><strong>multinomial logistic regression</strong></span>.</p><p>Logistic regression is based on the <span class="strong"><strong>sigmoid function</strong></span>. Predictor variables are combined with linear weight and then passed to this function, which generates the output in the range of 0–1. An output close to 1 indicates that an item belongs to a certain class. Let's first <a id="id109" class="indexterm"/>understand the sigmoid or logistic function. It <a id="id110" class="indexterm"/>can be defined by the following formula:</p><p>
<span class="emphasis"><em>F (z) = 1/1+e (-z)</em></span>
</p><p>With a single <a id="id111" class="indexterm"/>explanatory variable, <span class="emphasis"><em>z</em></span> will be defined as <span class="emphasis"><em>z = β0 + β1*x</em></span>. This <a id="id112" class="indexterm"/>equation is explained as follows:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>z</strong></span>: This is called <a id="id113" class="indexterm"/>the dependent variable. This is the variable that we would like to predict. During the creation of the model, we have this variable with us in the training set, and we build the model to predict this variable. The known values of z are called observed values.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>x</strong></span>: This is the <a id="id114" class="indexterm"/>explanatory or independent variable. These <a id="id115" class="indexterm"/>variables are used to predict the dependent variable z. For example, to predict the sales of a newly launched product at a particular location, we might include explanatory variables such as the price of the product, the average income of the people of that location, and so on.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>β0</strong></span>: This is called the <a id="id116" class="indexterm"/>regression intercept. If all explanatory variables are zero, then this parameter is equal to the dependent variable z.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>β1</strong></span>: These are values for each explanatory variable.</li></ul></div><p>The graph of the logistic function is as follows:</p><div class="mediaobject"><img src="graphics/4959OS_03_03.jpg" alt="Logistic regression"/></div><p>With a little bit of mathematics, we can change this equation as follows:</p><p>
<span class="emphasis"><em>ln(F(x)/(1-F(x)) = β0 + β1*x</em></span>
</p><p>In the case of linear regression, the cost function graph was convex, but here, it is not going to be convex. Finding the minimum values for parameters in a way that our predicted output is close to the actual one will be difficult. In a cost function, while calculating for logistic regression, we will replace our <span class="emphasis"><em>Cp</em></span> value of linear regression with the function <span class="emphasis"><em>F(z)</em></span>. To <a id="id117" class="indexterm"/>make convex logistic regression cost functions, we will replace <span class="emphasis"><em>(p0+p1Ri-Ci)2</em></span> with one of the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>log (1/1+e (-(β0 + β1*x)))</em></span> if the actual occurrence of an event is 1, this function will represent the cost.</li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>log (1-(1/1+e (-(β0 + β1*x))))</em></span> if the actual occurrence of an event is 0, this function will represent the cost.</li></ul></div><p>We will have to remember that in logistic regression, we calculate the class probability. So, if the probability of an event occurring (customer buying a car, being defrauded, and so on ) is <span class="emphasis"><em>p</em></span>, the probability of non-occurrence is <span class="emphasis"><em>1-p</em></span>.</p></div><div class="section" title="Stochastic Gradient Descent"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec19"/>Stochastic Gradient Descent</h2></div></div></div><p>Gradient descent <a id="id118" class="indexterm"/>minimizes the cost function. For very large datasets, gradient descent is a very expensive procedure. Stochastic Gradient Descent (SGD) is a modification of the gradient descent algorithm to handle large datasets. Gradient descent computes the gradient using the whole dataset, while SGD computes the gradient using a single sample. So, gradient descent loads the full dataset and tries to find out the local minimum on the graph and then repeat the full process again, while SGD adjusts the cost function for every sample, one by one. A major advantage that SGD has over gradient descent is that its speed of computation is a whole lot faster. Large datasets in RAM generally <a id="id119" class="indexterm"/>cannot be held as the storage is limited. In SGD, the burden on the RAM is reduced, wherein each sample or batch of samples are loaded and worked with, the results for which are stored, and so on.</p></div><div class="section" title="Using Mahout for logistic regression"><div class="titlepage"><div><div><h2 class="title"><a id="ch03lvl2sec20"/>Using Mahout for logistic regression</h2></div></div></div><p>Mahout <a id="id120" class="indexterm"/>has implementations for logistic regression using <a id="id121" class="indexterm"/>SGD. It is very easy to understand and use. So let's get started.</p><p>
<span class="strong"><strong>Dataset</strong></span>
</p><p>We will <a id="id122" class="indexterm"/>use the <span class="strong"><strong>Wisconsin Diagnostic Breast Cancer</strong></span> (<span class="strong"><strong>WDBC</strong></span>) dataset. This is a dataset for breast cancer tumors and data is available from 1995 onwards. It has 569 instances of breast tumor cases and has 30 features to <a id="id123" class="indexterm"/>predict the diagnosis, which is categorized as either benign or malignant.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note04"/>Note</h3><p>More details on the preceding dataset is available at <a class="ulink" href="http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.names">http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.names</a>.</p></div></div><p>
<span class="strong"><strong>Preparing the training and test data</strong></span>
</p><p>You can download the <code class="literal">wdbc.data</code> dataset from <a class="ulink" href="http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data">http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data</a>.</p><p>Now, save <a id="id124" class="indexterm"/>it as a CSV file and include the following header line:</p><p>
<code class="literal">ID_Number,Diagnosis,Radius,Texture,Perimeter,Area,Smoothness,Compactness,Concavity,ConcavePoints,Symmetry,Fractal_Dimension,RadiusStdError,TextureStdError,PerimeterStdError,AreaStdError,SmoothnessStdError,CompactnessStdError,ConcavityStdError,ConcavePointStdError,Symmetrystderror,FractalDimensionStderror,WorstRadius,worsttexture,worstperimeter,worstarea,worstsmoothness,worstcompactness,worstconcavity,worstconcavepoints,worstsymmentry,worstfractaldimensions</code>
</p><p>Now, we will have to perform the following steps to prepare this data to be used by the Mahout logistic regression algorithm:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">We will make the target class numeric. In this case, the second field diagnosis is the target variable. We will change malignant to 0 and benign to 1. Use the following code snippet to introduce the changes. We can use this strategy for small datasets, but for huge datasets, we have different strategies, which we <a id="id125" class="indexterm"/>will cover in <a class="link" href="ch04.html" title="Chapter 4. Learning the Naïve Bayes Classification Using Mahout">Chapter 4</a>, <span class="emphasis"><em>Learning the Naïve Bayes Classification Using Mahout</em></span>:<div class="informalexample"><pre class="programlisting">public void convertTargetToInteger() throws IOException{
  //Read the data
  BufferedReader br = new BufferedReader(new FileReader("wdbc.csv"));
  String line =null;
  //Create the file to save the resulted data
  File wdbcData = new File("&lt;Your Destination location for file.&gt;");
  FileWriter fw = new FileWriter(wdbcData);
  //We are adding header to the new file
  fw.write("ID_Number"+","+"Diagnosis"+","+"Radius"+","+"Texture"+","+"Perimeter"+","+"Area"+","+"Smoothness"+","+"Compactness"+","+"Concavity"+","+"ConcavePoints"+","+"Symmetry"+","+"Fractal_Dimension"+","+"RadiusStdError"+","+"TextureStdError"+","+"PerimeterStdError"+","+"AreaStdError"+","+"SmoothnessStdError"+","+"CompactnessStdError"+","+"ConcavityStdError"+","+"ConcavePointStdError"+","+"Symmetrystderror"+","+"FractalDimensionStderror"+","+"WorstRadius"+","+"worsttexture"+","+"worstperimeter"+","+"worstarea"+","+"worstsmoothness"+","+"worstcompactness"+","+"worstconcavity"+","+"worstconcavepoints"+","+"worstsymmentry"+","+"worstfractaldimensions"+"\n");

  /*In the while loop we are reading line by line and checking the last field- parts[1] and changing it to numeric value accordingly*/
  while((line=br.readLine())!=null){
    String []parts = line.split(",");
    if(parts[1].equals("M")){
    fw.write(parts[0]+","+"0"+","+parts[2]+","+parts[3]+","+parts[4]+","+parts[5]+","+parts[6]+","+parts[7]+","+parts[8]+","+parts[9]+","+parts[10]+","+parts[11]+","+parts[12]+","+parts[13]+","+parts[14]+","+parts[15]+","+parts[16]+","+parts[17]+","+parts[18]+","+parts[19]+","+parts[20]+","+parts[21]+","+parts[22]+","+parts[23]+","+parts[24]+","+parts[25]+","+parts[26]+","+parts[27]+","+parts[28]+","+parts[29]+","+parts[30]+","+parts[31]+"\n");
    }

    if(parts[1].equals("B")){
      fw.write(parts[0]+","+"1"+","+parts[2]+","+parts[3]+","+parts[4]+","+parts[5]+","+parts[6]+","+parts[7]+","+parts[8]+","+parts[9]+","+parts[10]+","+parts[11]+","+parts[12]+","+parts[13]+","+parts[14]+","+parts[15]+","+parts[16]+","+parts[17]+","+parts[18]+","+parts[19]+","+parts[20]+","+parts[21]+","+parts[22]+","+parts[23]+","+parts[24]+","+parts[25]+","+parts[26]+","+parts[27]+","+parts[28]+","+parts[29]+","+parts[30]+","+parts[31]+"\n");
    }
  }
  fw.close();
  br.close();
}</pre></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="tip02"/>Tip</h3><p>
<span class="strong"><strong>Downloading the example code</strong></span>
</p><p>You can download the example code files from your account at <a class="ulink" href="http://www.packtpub.com">http://www.packtpub.com</a> for all the Packt Publishing books you have purchased. If you purchased this book elsewhere, you can visit <a class="ulink" href="http://www.packtpub.com/support">http://www.packtpub.com/support</a> and register to have the files e-mailed directly to you.</p></div></div></li><li class="listitem">We will <a id="id126" class="indexterm"/>have to split the dataset into <a id="id127" class="indexterm"/>training and test datasets and then shuffle the datasets so that we can mix them up, which can be done using the following code snippet:<div class="informalexample"><pre class="programlisting">public void dataPrepration() throws Exception {
  // Reading the dataset created by earlier method convertTargetToInteger and here we are using google guava api's.
  List&lt;String&gt; result = Resources.readLines(Resources.getResource("wdbc.csv"), Charsets.UTF_8);
  //This is to remove header before the randomization process. Otherwise it can appear in the middle of dataset.
  List&lt;String&gt; raw = result.subList(1, 570);
  Random random = new Random();
  //Shuffling the dataset.
  Collections.shuffle(raw, random);
  //Splitting dataset into training and test examples.
  List&lt;String&gt; train = raw.subList(0, 470);
  List&lt;String&gt; test = raw.subList(470, 569);
  File trainingData = new File("&lt;your Location&gt;/ wdbcTrain.csv");
  File testData = new File("&lt;your Location&gt;/ wdbcTest.csv");
  writeCSV(train, trainingData);
  writeCSV(test, testData);
}
//This method is writing the list to desired file location.
public void writeCSV(List&lt;String&gt; list, File file) throws IOException{
  FileWriter fw = new FileWriter(file);
  fw.write("ID_Number"+","+"Diagnosis"+","+"Radius"+","+"Texture"+","+"Perimeter"+","+"Area"+","+"Smoothness"+","+"Compactness"+","+"Concavity"+","+"ConcavePoints"+","+"Symmetry"+","+"Fractal_Dimension"+","+"RadiusStdError"+","+"TextureStdError"+","+"PerimeterStdError"+","+"AreaStdError"+","+"SmoothnessStdError"+","+"CompactnessStdError"+","+"ConcavityStdError"+","+"ConcavePointStdError"+","+"Symmetrystderror"+","+"FractalDimensionStderror"+","+"WorstRadius"+","+"worsttexture"+","+"worstperimeter"+","+"worstarea"+","+"worstsmoothness"+","+"worstcompactness"+","+"worstconcavity"+","+"worstconcavepoints"+","+"worstsymmentry"+","+"worstfractaldimensions"+"\n");
  for(int i=0;i&lt; list.size();i++){
    fw.write(list.get(i)+"\n");
  }
  fw.close();
}</pre></div></li></ol></div><p>
<span class="strong"><strong>Training the model</strong></span>
</p><p>We will use the <a id="id128" class="indexterm"/>training dataset and trainlogistic <a id="id129" class="indexterm"/>algorithm to prepare the model. Use the following command to create the model:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>mahout trainlogistic --input /tmp/wdbcTrain.csv --output /tmp//model --target Diagnosis --categories 2 --predictors Radius Texture Perimeter Area Smoothness Compactness Concavity ConcavePoints Symmetry Fractal_Dimension RadiusStdError TextureStdError PerimeterStdError AreaStdError SmoothnessStdError CompactnessStdError ConcavityStdError ConcavePointStdError Symmetrystderror FractalDimensionStderror WorstRadius worsttexture worstperimeter worstarea worstsmoothness worstcompactness worstconcavity worstconcavepoints worstsymmentry worstfractaldimensions  --types numeric --features 30 --passes 90 --rate 300</strong></span>
</pre></div><p>This command will give you the following output:</p><div class="mediaobject"><img src="graphics/4959OS_03_04.jpg" alt="Using Mahout for logistic regression"/></div><p>Let's understand the parameters used in this command:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">trainlogistic</code>: This <a id="id130" class="indexterm"/>is the algorithm that Mahout provides to build the model using your input parameters.</li><li class="listitem" style="list-style-type: disc"><code class="literal">input</code>: This is <a id="id131" class="indexterm"/>the location of the input file.</li><li class="listitem" style="list-style-type: disc"><code class="literal">output</code>: This is the <a id="id132" class="indexterm"/>location of the model file.</li><li class="listitem" style="list-style-type: disc"><code class="literal">target</code>: This is <a id="id133" class="indexterm"/>the name of the target variable <a id="id134" class="indexterm"/>that we want to predict from the dataset.</li><li class="listitem" style="list-style-type: disc"><code class="literal">categories</code>: This <a id="id135" class="indexterm"/>refers to the number of predicted classes.</li><li class="listitem" style="list-style-type: disc"><code class="literal">predictors</code>: This <a id="id136" class="indexterm"/>features in the dataset used to predict the target variable.</li><li class="listitem" style="list-style-type: disc"><code class="literal">types</code>: This is a list of the <a id="id137" class="indexterm"/>types of predictor variables. (Here all are numeric but it could be word or text as well.)</li><li class="listitem" style="list-style-type: disc"><code class="literal">features</code>: This <a id="id138" class="indexterm"/>is the size of the feature vector used to build the model.</li><li class="listitem" style="list-style-type: disc"><code class="literal">passes</code>: This <a id="id139" class="indexterm"/>specifies the number of times the input data should be re-examined during training. Small input files may need to be examined dozens of times. Very large input files probably don't even need to be completely examined.</li><li class="listitem" style="list-style-type: disc"><code class="literal">rate</code>: This sets the <a id="id140" class="indexterm"/>initial learning rate. This can be large if you have lots of data or use lots of passes because it decreases progressively as data is examined.</li></ul></div><p>Now our model is <a id="id141" class="indexterm"/>ready to move on to the next step of evaluation. To evaluate the model further, we can use the same dataset and check the confusion and AUC matrix. The command for this will be as follows:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>mahout runlogistic --input /tmp/wdbcTrain.csv --model /tmp//model  --auc --confusion</strong></span>
</pre></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">runlogistic</code>: This <a id="id142" class="indexterm"/>is the algorithm to run the logistic regression model over an input dataset</li><li class="listitem" style="list-style-type: disc"><code class="literal">model</code>: This is the <a id="id143" class="indexterm"/>location of the model file</li><li class="listitem" style="list-style-type: disc"><code class="literal">auc</code>: This prints the <a id="id144" class="indexterm"/>AUC score for the model versus the input data after the data is read</li><li class="listitem" style="list-style-type: disc"><code class="literal">confusion</code>: This <a id="id145" class="indexterm"/>prints the confusion matrix for a particular threshold</li></ul></div><p>The output of the previous command is shown in  the following screenshot:</p><div class="mediaobject"><img src="graphics/4959OS_03_05.jpg" alt="Using Mahout for logistic regression"/></div><p>Now, these matrices show that the model is not bad. Having 0.88 as the value for AUC is good, but we will check this on test data as well. The confusion matrix informs us that out of 172 malignant tumors, it has correctly classified 151 instances and that 34 benign tumors are also classified as malignant. In the case of benign tumors, out of 298, it has correctly classified 264.</p><p>If the model does not provide good results, we have a number of options.</p><p>Change the parameters in the feature vector, increasing them if we are selecting few features. This should be done one at a time, and we should test the result again with each generated model. We should get a model where AUC is close to 1.</p><p>Let's run the same algorithm on test data as well:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>mahout runlogistic --input /tmp/wdbcTest.csv --model /tmp//model  --auc –confusion</strong></span>
</pre></div><div class="mediaobject"><img src="graphics/4959OS_03_06.jpg" alt="Using Mahout for logistic regression"/></div><p>So this model <a id="id146" class="indexterm"/>works almost the same on test data as well. It has classified 34 out of the 40 malignant tumors correctly.</p></div></div></div>
<div class="section" title="Summary"><div class="titlepage"><div><div><h1 class="title"><a id="ch03lvl1sec18"/>Summary</h1></div></div></div><p>In this chapter, we discussed logistic regression and how we can use this algorithm available in Apache Mahout. We used the Wisconsin Diagnostic Breast Cancer dataset and randomly broke it into two datasets: one for training and the other for testing. We created the logistic regression model using Mahout and also ran test data over this model. Now, we will move on to the next chapter where you will learn about the Naïve Bayes classification and also the most frequently used classification technique: text classification.</p></div></body></html>