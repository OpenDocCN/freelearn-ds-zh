["```py\nsalary_data.groupby('Department')\n```", "```py\n<pyspark.sql.group.GroupedData at 0x7fc8495a3c10>\n```", "```py\nsalary_data.groupby(‘Department’).avg().show()\n```", "```py\n+----------+------------------+  \n|Department| avg(Salary)      |  \n+----------+------------------+  \n| null     |            3750.0|  \n| Sales    |            2600.0|  \n| Field-eng| 4166.666666666667|  \n| Finance  |3333.3333333333335|  \n+----------+------------------+ \n```", "```py\nfrom pyspark.sql.functions import col, round\nsalary_data.groupBy('Department')\\\n  .sum('Salary')\\\n  .withColumn('sum(Salary)',round(col('sum(Salary)'), 2))\\\n  .withColumnRenamed('sum(Salary)', 'Salary')\\\n  .orderBy('Department')\\\n  .show()\n```", "```py\n+----------+------------------+\n|Department|    sum(Salary)   |\n+----------+------------------+\n| null     |              7500|\n| Field-eng|             12500|\n| Finance  |             10000|\n| Sales    |              5200|\n+----------+------------------+\n```", "```py\nDataframe1.join(Dataframe2, on, how)\n```", "```py\nsalary_data_with_id = [(1, \"John\", \"Field-eng\", 3500), \\\n    (2, \"Robert\", \"Sales\", 4000), \\\n    (3, \"Maria\", \"Finance\", 3500), \\\n    (4, \"Michael\", \"Sales\", 3000), \\\n    (5, \"Kelly\", \"Finance\", 3500), \\\n    (6, \"Kate\", \"Finance\", 3000), \\\n    (7, \"Martin\", \"Finance\", 3500), \\\n    (8, \"Kiran\", \"Sales\", 2200), \\\n  ]\ncolumns= [\"ID\", \"Employee\", \"Department\", \"Salary\"]\nsalary_data_with_id = spark.createDataFrame(data = salary_data_with_id, schema = columns)\nsalary_data_with_id.show()\n```", "```py\n+---+--------+----------+------+\n|ID |Employee|Department|Salary|\n+---+--------+----------+------+\n| 1 | John   | Field-eng|  3500|\n| 2 | Robert | Sales    |  4000|\n| 3 | Maria  | Finance  |  3500|\n| 4 | Michael| Sales    |  3000|\n| 5 | Kelly  | Finance  |  3500|\n| 6 | Kate   | Finance  |  3000|\n| 7 | Martin | Finance  |  3500|\n| 8 | Kiran  | Sales    |  2200|\n+---+--------+----------+------+\n```", "```py\nemployee_data = [(1, \"NY\", \"M\"), \\\n    (2, \"NC\", \"M\"), \\\n    (3, \"NY\", \"F\"), \\\n    (4, \"TX\", \"M\"), \\\n    (5, \"NY\", \"F\"), \\\n    (6, \"AZ\", \"F\") \\\n  ]\ncolumns= [\"ID\", \"State\", \"Gender\"]\nemployee_data = spark.createDataFrame(data = employee_data, schema = columns)\nemployee_data.show()\n```", "```py\n+---+-----+------+\n| ID|State|Gender|\n+---+-----+------+\n|  1|   NY|     M|\n|  2|   NC|     M|\n|  3|   NY|     F|\n|  4|   TX|     M|\n|  5|   NY|     F|\n|  6|   AZ|     F|\n+---+-----+------+\n```", "```py\nsalary_data_with_id.join(employee_data,salary_data_with_id.ID ==  employee_data.ID,\"inner\").show()\n```", "```py\n+---+--------+----------+------+---+-----+------+\n| ID|Employee|Department|Salary| ID|State|Gender|\n+---+--------+----------+------+---+-----+------+\n|  1|    John| Field-eng|  3500|  1|   NY|     M|\n|  2|  Robert|     Sales|  4000|  2|   NC|     M|\n|  3|   Maria|   Finance|  3500|  3|   NY|     F|\n|  4| Michael|     Sales|  3000|  4|   TX|     M|\n|  5|   Kelly|   Finance|  3500|  5|   NY|     F|\n|  6|    Kate|   Finance|  3000|  6|   AZ|     F|\n+---+--------+----------+------+---+-----+------+\n```", "```py\nsalary_data_with_id.join(employee_data,salary_data_with_id.ID ==  employee_data.ID,\"outer\").show()\n```", "```py\n+---+--------+----------+------+----+-----+------+\n| ID|Employee|Department|Salary|  ID|State|Gender|\n+---+--------+----------+------+----+-----+------+\n|  1|    John| Field-eng|  3500|   1|   NY|     M|\n|  2|  Robert|     Sales|  4000|   2|   NC|     M|\n|  3|   Maria|   Finance|  3500|   3|   NY|     F|\n|  4| Michael|     Sales|  3000|   4|   TX|     M|\n|  5|   Kelly|   Finance|  3500|   5|   NY|     F|\n|  6|    Kate|   Finance|  3000|   6|   AZ|     F|\n|  7|  Martin|   Finance|  3500|null| null|  null|\n|  8|   Kiran|     Sales|  2200|null| null|  null|\n+---+--------+----------+------+----+-----+------+\n```", "```py\nsalary_data_with_id.join(employee_data,salary_data_with_id.ID ==  employee_data.ID,\"left\").show()\n```", "```py\n+---+--------+----------+------+----+-----+------+\n| ID|Employee|Department|Salary|  ID|State|Gender|\n+---+--------+----------+------+----+-----+------+\n|  1|    John| Field-eng|  3500|   1|   NY|     M|\n|  2|  Robert|     Sales|  4000|   2|   NC|     M|\n|  3|   Maria|   Finance|  3500|   3|   NY|     F|\n|  4| Michael|     Sales|  3000|   4|   TX|     M|\n|  5|   Kelly|   Finance|  3500|   5|   NY|     F|\n|  6|    Kate|   Finance|  3000|   6|   AZ|     F|\n|  7|  Martin|   Finance|  3500|null| null|  null|\n|  8|   Kiran|     Sales|  2200|null| null|  null|\n+---+--------+----------+------+----+-----+------+\n```", "```py\nsalary_data_with_id.join(employee_data,salary_data_with_id.ID ==  employee_data.ID,\"right\").show()\n```", "```py\n+---+--------+----------+------+---+-----+------+\n| ID|Employee|Department|Salary| ID|State|Gender|\n+---+--------+----------+------+---+-----+------+\n|  1|    John| Field-eng|  3500|  1|   NY|     M|\n|  2|  Robert|     Sales|  4000|  2|   NC|     M|\n|  3|   Maria|   Finance|  3500|  3|   NY|     F|\n|  4| Michael|     Sales|  3000|  4|   TX|     M|\n|  5|   Kelly|   Finance|  3500|  5|   NY|     F|\n|  6|    Kate|   Finance|  3000|  6|   AZ|     F|\n+---+--------+----------+------+---+-----+------+\n```", "```py\nsalary_data_with_id_2 = [(1, \"John\", \"Field-eng\", 3500), \\\n    (2, \"Robert\", \"Sales\", 4000), \\\n    (3, \"Aliya\", \"Finance\", 3500), \\\n    (4, \"Nate\", \"Sales\", 3000), \\\n  ]\ncolumns2= [\"ID\", \"Employee\", \"Department\", \"Salary\"]\nsalary_data_with_id_2 = spark.createDataFrame(data = salary_data_with_id_2, schema = columns2)\nsalary_data_with_id_2.printSchema()\nsalary_data_with_id_2.show(truncate=False)\n```", "```py\nroot\n |-- ID: long (nullable = true)\n |-- Employee: string (nullable = true)\n |-- Department: string (nullable = true)\n |-- Salary: long (nullable = true)\n+---+--------+----------+------+\n|ID |Employee|Department|Salary|\n+---+--------+----------+------+\n|1  |John    |Field-eng |3500  |\n|2  |Robert  |Sales     |4000  |\n|3  |Aliya   |Finance   |3500  |\n|4  |Nate    |Sales     |3000  |\n+---+--------+----------+------+\n```", "```py\nunionDF = salary_data_with_id.union(salary_data_with_id_2)\nunionDF.show(truncate=False)\n```", "```py\n+---+--------+----------+------+\n|ID |Employee|Department|Salary|\n+---+--------+----------+------+\n|1  |John    |Field-eng |3500  |\n|2  |Robert  |Sales     |4000  |\n|3  |Maria   |Finance   |3500  |\n|4  |Michael |Sales     |3000  |\n|5  |Kelly   |Finance   |3500  |\n|6  |Kate    |Finance   |3000  |\n|7  |Martin  |Finance   |3500  |\n|8  |Kiran   |Sales     |2200  |\n|1  |John    |Field-eng |3500  |\n|2  |Robert  |Sales     |4000  |\n|3  |Aliya   |Finance   |3500  |\n|4  |Nate    |Sales     |3000  |\n+---+--------+----------+------+\n```", "```py\nsalary_data_with_id.write.csv('salary_data.csv', mode='overwrite', header=True)\nspark.read.csv('/salary_data.csv', header=True).show()\n```", "```py\n+---+--------+----------+------+\n|ID |Employee|Department|Salary|\n+---+--------+----------+------+\n| 1 | John   | Field-eng|  3500|\n| 2 | Robert | Sales    |  4000|\n| 3 | Maria  | Finance  |  3500|\n| 4 | Michael| Sales    |  3000|\n| 5 | Kelly  | Finance  |  3500|\n| 6 | Kate   | Finance  |  3000|\n| 7 | Martin | Finance  |  3500|\n| 8 | Kiran  | Sales    |  2200|\n+---+--------+----------+------+\n```", "```py\nfrom pyspark.sql.types import *\nfilePath = '/salary_data.csv'\ncolumns= [\"ID\", \"State\", \"Gender\"] \nschema = StructType([\n      StructField(\"ID\", IntegerType(),True),\n  StructField(\"State\",  StringType(),True),\n  StructField(\"Gender\",  StringType(),True)\n])\nread_data = spark.read.format(\"csv\").option(\"header\",\"true\").schema(schema).load(filePath)\nread_data.show()\n```", "```py\n+---+--------+----------+------+\n|ID |Employee|Department|Salary|\n+---+--------+----------+------+\n| 1 | John   | Field-eng|  3500|\n| 2 | Robert | Sales    |  4000|\n| 3 | Maria  | Finance  |  3500|\n| 4 | Michael| Sales    |  3000|\n| 5 | Kelly  | Finance  |  3500|\n| 6 | Kate   | Finance  |  3000|\n| 7 | Martin | Finance  |  3500|\n| 8 | Kiran  | Sales    |  2200|\n+---+--------+----------+------+\n```", "```py\nsalary_data_with_id.write.parquet('salary_data.parquet', mode='overwrite')\nspark.read.parquet(' /salary_data.parquet').show()\n```", "```py\n+---+--------+----------+------+\n|ID |Employee|Department|Salary|\n+---+--------+----------+------+\n| 1 | John   | Field-eng|  3500|\n| 2 | Robert | Sales    |  4000|\n| 3 | Maria  | Finance  |  3500|\n| 4 | Michael| Sales    |  3000|\n| 5 | Kelly  | Finance  |  3500|\n| 6 | Kate   | Finance  |  3000|\n| 7 | Martin | Finance  |  3500|\n| 8 | Kiran  | Sales    |  2200|\n+---+--------+----------+------+\n```", "```py\nsalary_data_with_id.write.orc('salary_data.orc', mode='overwrite')\nspark.read.orc(' /salary_data.orc').show()\n```", "```py\n+---+--------+----------+------+\n|ID |Employee|Department|Salary|\n+---+--------+----------+------+\n| 1 | John   | Field-eng|  3500|\n| 2 | Robert | Sales    |  4000|\n| 3 | Maria  | Finance  |  3500|\n| 4 | Michael| Sales    |  3000|\n| 5 | Kelly  | Finance  |  3500|\n| 6 | Kate   | Finance  |  3000|\n| 7 | Martin | Finance  |  3500|\n| 8 | Kiran  | Sales    |  2200|\n+---+--------+----------+------+\n```", "```py\nsalary_data_with_id.write.format(\"delta\").save(\"/FileStore/tables/salary_data_with_id\", mode='overwrite')\ndf = spark.read.load(\"/FileStore/tables/salary_data_with_id\")\ndf.show()\n```", "```py\n+---+--------+----------+------+\n| ID|Employee|Department|Salary|\n+---+--------+----------+------+\n|  7|  Martin|   Finance|  3500|\n|  4| Michael|     Sales|  3000|\n|  6|    Kate|   Finance|  3000|\n|  2|  Robert|     Sales|  4000|\n|  1|    John| Field-eng|  3500|\n|  5|   Kelly|   Finance|  3500|\n|  3|   Maria|   Finance|  3500|\n|  8|   Kiran|     Sales|  2200|\n+---+--------+----------+------+\n```", "```py\nsalary_data_with_id.createOrReplaceTempView(\"SalaryTable\")\nspark.sql(\"SELECT count(*) from SalaryTable\").show()\n```", "```py\n+--------+\n|count(1)|\n+--------+\n|       8|\n+--------+\n```", "```py\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.types import IntegerType\n# Define a UDF in Python\ndef my_udf_function(input_param):\n# Your custom logic here\nreturn processed_value\n# Register the UDF with Spark\nmy_udf = udf(my_udf_function, IntegerType())\n# Using the UDF in a DataFrame operation\ndf = df.withColumn(\"new_column\", my_udf(df[\"input_column\"]))\n```", "```py\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.types._\n// Define a UDF in Scala\nval myUDF: UserDefinedFunction = udf((inputParam: InputType) => {\n// Your custom logic here\nprocessedValue }, OutputType)\n// Using the UDF in a DataFrame operation\nval df = df.withColumn(\"newColumn\", myUDF(col(\"inputColumn\")))\n```", "```py\n# SparkSession setup\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.appName(\"CatalystOptimizerExample\").getOrCreate()\n# Load data\ndf = spark.read.csv(\"/salary_data.csv\", header=True, inferSchema=True) \n# Query with Catalyst Optimizer \nresult_df = df.select(\"employee\", \"department\").filter(df[\"salary\"] > 3500) \n# Explain the optimized query plan \nresult_df.explain() \n```", "```py\n# Cache a DataFrame\ndf.cache()\n# Unpersist the cached DataFrame\ndf.unpersist()\n```", "```py\n# Repartition a DataFrame into 8 partitions\ndf.repartition(8)\n```", "```py\n# Coalesce a DataFrame to 4 partitions\ndf.coalesce(4)\n```"]