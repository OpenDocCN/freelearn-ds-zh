["```py\ndf = spark.createDataFrame([\n        (1, 144.5, 5.9, 33, 'M'),\n        (2, 167.2, 5.4, 45, 'M'),\n        (3, 124.1, 5.2, 23, 'F'),\n        (4, 144.5, 5.9, 33, 'M'),\n        (5, 133.2, 5.7, 54, 'F'),\n        (3, 124.1, 5.2, 23, 'F'),\n        (5, 129.2, 5.3, 42, 'M'),\n    ], ['id', 'weight', 'height', 'age', 'gender'])\n```", "```py\nprint('Count of rows: {0}'.format(df.count()))\nprint('Count of distinct rows: {0}'.format(df.distinct().count()))\n```", "```py\ndf = df.dropDuplicates()\n```", "```py\nprint('Count of ids: {0}'.format(df.count()))\nprint('Count of distinct ids: {0}'.format(\n    df.select([\n        c for c in df.columns if c != 'id'\n    ]).distinct().count())\n)\n```", "```py\ndf = df.dropDuplicates(subset=[\n    c for c in df.columns if c != 'id'\n])\n```", "```py\nimport pyspark.sql.functions as fn\n\ndf.agg(\n    fn.count('id').alias('count'),\n    fn.countDistinct('id').alias('distinct')\n).show()\n```", "```py\ndf.withColumn('new_id', fn.monotonically_increasing_id()).show()\n```", "```py\ndf_miss = spark.createDataFrame([â€©        (1, 143.5, 5.6, 28,   'M',  100000),\n        (2, 167.2, 5.4, 45,   'M',  None),\n        (3, None , 5.2, None, None, None),\n        (4, 144.5, 5.9, 33,   'M',  None),\n        (5, 133.2, 5.7, 54,   'F',  None),\n        (6, 124.1, 5.2, None, 'F',  None),\n        (7, 129.2, 5.3, 42,   'M',  76000),\n    ], ['id', 'weight', 'height', 'age', 'gender', 'income'])\n```", "```py\ndf_miss.rdd.map(\n    lambda row: (row['id'], sum([c == None for c in row]))\n).collect()\n```", "```py\ndf_miss.where('id == 3').show()\n```", "```py\ndf_miss.agg(*[\n    (1 - (fn.count(c) / fn.count('*'))).alias(c + '_missing')\n    for c in df_miss.columns\n]).show()\n```", "```py\ndf_miss_no_income = df_miss.select([\n    c for c in df_miss.columns if c != 'income'\n])\n```", "```py\ndf_miss_no_income.dropna(thresh=3).show()\n```", "```py\nmeans = df_miss_no_income.agg(\n    *[fn.mean(c).alias(c) \n        for c in df_miss_no_income.columns if c != 'gender']\n).toPandas().to_dict('records')[0]\n\nmeans['gender'] = 'missing'\n\ndf_miss_no_income.fillna(means).show()\n```", "```py\ndf_outliers = spark.createDataFrame([\n        (1, 143.5, 5.3, 28),\n        (2, 154.2, 5.5, 45),\n        (3, 342.3, 5.1, 99),\n        (4, 144.5, 5.5, 33),\n        (5, 133.2, 5.4, 54),\n        (6, 124.1, 5.1, 21),\n        (7, 129.2, 5.3, 42),\n    ], ['id', 'weight', 'height', 'age'])\n```", "```py\ncols = ['weight', 'height', 'age']\nbounds = {}\n\nfor col in cols:\n    quantiles = df_outliers.approxQuantile(\n        col, [0.25, 0.75], 0.05\n    )\n\n    IQR = quantiles[1] - quantiles[0]\n\n    bounds[col] = [\n        quantiles[0] - 1.5 * IQR, \n        quantiles[1] + 1.5 * IQR\n]\n```", "```py\noutliers = df_outliers.select(*['id'] + [\n    (\n        (df_outliers[c] < bounds[c][0]) | \n        (df_outliers[c] > bounds[c][1])\n    ).alias(c + '_o') for c in cols\n])\noutliers.show()\n```", "```py\ndf_outliers = df_outliers.join(outliers, on='id')\ndf_outliers.filter('weight_o').select('id', 'weight').show()\ndf_outliers.filter('age_o').select('id', 'age').show()\n```", "```py\nimport pyspark.sql.types as typ\n```", "```py\nfraud = sc.textFile('ccFraud.csv.gz')\nheader = fraud.first()\n\nfraud = fraud \\\n    .filter(lambda row: row != header) \\\n    .map(lambda row: [int(elem) for elem in row.split(',')])\n```", "```py\nfields = [\n    *[\n        typ.StructField(h[1:-1], typ.IntegerType(), True)\n        for h in header.split(',')\n    ]\n]\nschema = typ.StructType(fields)\n```", "```py\nfraud_df = spark.createDataFrame(fraud, schema)\n```", "```py\nfraud_df.printSchema()\n```", "```py\nfraud_df.groupby('gender').count().show()\n```", "```py\nnumerical = ['balance', 'numTrans', 'numIntlTrans']\ndesc = fraud_df.describe(numerical)\ndesc.show()\n```", "```py\nfraud_df.agg({'balance': 'skewness'}).show()\n```", "```py\nfraud_df.corr('balance', 'numTrans')\n```", "```py\nn_numerical = len(numerical)\n\ncorr = []\n\nfor i in range(0, n_numerical):\n    temp = [None] * i\n\n    for j in range(i, n_numerical):\n        temp.append(fraud_df.corr(numerical[i], numerical[j]))\n    corr.append(temp)\n```", "```py\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\nimport bokeh.charts as chrt\nfrom bokeh.io import output_notebook\n\noutput_notebook()\n```", "```py\nhists = fraud_df.select('balance').rdd.flatMap(\n    lambda row: row\n).histogram(20)\n```", "```py\ndata = {\n    'bins': hists[0][:-1],\n    'freq': hists[1]\n}\nplt.bar(data['bins'], data['freq'], width=2000)\nplt.title('Histogram of \\'balance\\'')\n```", "```py\nb_hist = chrt.Bar(\n    data, \n    values='freq', label='bins', \n    title='Histogram of \\'balance\\'')\nchrt.show(b_hist)\n```", "```py\ndata_driver = {\n    'obs': fraud_df.select('balance').rdd.flatMap(\n        lambda row: row\n    ).collect()\n}\nplt.hist(data_driver['obs'], bins=20)\nplt.title('Histogram of \\'balance\\' using .hist()')\nb_hist_driver = chrt.Histogram(\n    data_driver, values='obs', \n    title='Histogram of \\'balance\\' using .Histogram()', \n    bins=20\n)\nchrt.show(b_hist_driver)\n```", "```py\ndata_sample = fraud_df.sampleBy(\n    'gender', {1: 0.0002, 2: 0.0002}\n).select(numerical)\n```", "```py\ndata_multi = dict([\n    (elem, data_sample.select(elem).rdd \\\n        .flatMap(lambda row: row).collect()) \n    for elem in numerical\n])\nsctr = chrt.Scatter(data_multi, x='balance', y='numTrans')\nchrt.show(sctr)\n```"]