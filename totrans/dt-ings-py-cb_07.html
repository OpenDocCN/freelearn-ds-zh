<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer186">
<h1 class="chapter-number" id="_idParaDest-227"><a id="_idTextAnchor227"/>7</h1>
<h1 id="_idParaDest-228"><a id="_idTextAnchor228"/>Ingesting Analytical Data</h1>
<p>Analytical data is a bundle of data that serves various areas (such as finances, marketing, and sales) in a company, university, or any other institution, to facilitate decision-making, especially for strategic matters. When transposing analytical data to a data pipeline or a usual <strong class="bold">Extract, Transform, and Load</strong> (<strong class="bold">ETL</strong>) process, it corresponds to the final step, where data is already ingested, cleaned, aggregated, and has other transformations accordingly to <span class="No-Break">business rules.</span></p>
<p>There are plenty of scenarios where data engineers must retrieve data from a data warehouse or any other storage containing analytical data. The objective of this chapter is to learn how to read analytical data and its standard formats and cover practical use cases related to the reverse <span class="No-Break">ETL concept.</span></p>
<p>In this chapter, we will learn about the <span class="No-Break">following topics:</span></p>
<ul>
<li><a id="_idTextAnchor229"/>Ingesting <span class="No-Break">Parquet ﬁles</span></li>
<li><a id="_idTextAnchor230"/>Ingesting <span class="No-Break">Avro files</span></li>
<li><a id="_idTextAnchor231"/>Applying schemas to <span class="No-Break">analytical data</span></li>
<li>Filtering data and handling <span class="No-Break">common issues</span></li>
<li><a id="_idTextAnchor232"/>Ingesting <span class="No-Break">partitioned data</span></li>
<li><a id="_idTextAnchor233"/>Applying <span class="No-Break">reverse ETL</span></li>
<li><a id="_idTextAnchor234"/>Selecting analytical data for <span class="No-Break">reverse ETL</span></li>
</ul>
<h1 id="_idParaDest-229"><a id="_idTextAnchor235"/>Technical requirements</h1>
<p>Like <a href="B19453_06.xhtml#_idTextAnchor195"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, in this chapter too, some recipes will need <strong class="source-inline">SparkSession</strong> initialized, and you can use the same session for all of them. You can use the following code to create <span class="No-Break">your session:</span></p>
<pre class="source-code">
from pyspark.sql import SparkSession
spark = SparkSession.builder \
      .master("local[1]") \
      .appName("chapter7_analytical_data") \
      .config("spark.executor.memory", '3g') \
      .config("spark.executor.cores", '2') \
      .config("spark.cores.max", '2') \
      .getOrCreate()</pre>
<p class="callout-heading">Note</p>
<p class="callout">A <strong class="source-inline">WARN</strong> message as output is expected in some cases, especially if you are using WSL on Windows, so you don’t need to worry if you <span class="No-Break">receive one.</span></p>
<p>You can also find the code from this chapter in its GitHub repository <span class="No-Break">here: </span><a href="https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook"><span class="No-Break">https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-230"><a id="_idTextAnchor236"/>Ingesting Parquet ﬁles</h1>
<p><strong class="bold">Apache Parquet</strong> is a columnar <a id="_idIndexMarker475"/>storage format that is open source and designed to <a id="_idIndexMarker476"/>support fast processing. It is available to any project in a <strong class="bold">Hadoop ecosystem</strong> and can be read in different <span class="No-Break">programming languages.</span></p>
<p>Due to its compression and fastness, this is one of the most used formats when needing to analyze data in <a id="_idIndexMarker477"/>great volume. The objective of this recipe is to understand how to read a collection of Parquet files using <strong class="bold">PySpark</strong> in a <span class="No-Break">real-world scenario.</span></p>
<h2 id="_idParaDest-231"><a id="_idTextAnchor237"/>Getting ready</h2>
<p>For this recipe, we will need <strong class="source-inline">SparkSession</strong> to be initialized. You can use the code provided at the beginning of this chapter to <span class="No-Break">do so.</span></p>
<p>The dataset for this recipe will be <em class="italic">Yellow Taxi Trip Records from New York</em>. You can download it by accessing the <strong class="bold">NYC Government website</strong> and selecting <strong class="bold">2022</strong> | <strong class="bold">January</strong> | <strong class="bold">Yellow Taxi Trip Records</strong> or using <span class="No-Break">this link:</span></p>
<p><a href="https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-01.parquet"><span class="No-Break">https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2022-01.parquet</span></a></p>
<p>Feel free to execute the code with a Jupyter notebook or a PySpark <span class="No-Break">shell session.</span></p>
<h2 id="_idParaDest-232"><a id="_idTextAnchor238"/>How to do it…</h2>
<p>Here are the <a id="_idIndexMarker478"/>steps to perform <span class="No-Break">this recipe:</span></p>
<ol>
<li><strong class="bold">Setting the Parquet file path</strong>: To create a DataFrame based on the Parquet file, we have two options: pass the filename or the path of the Parquet file. In the following code block, you can see an example of passing only the name of <span class="No-Break">the file:</span><pre class="source-code">
df = spark.read.parquet('chapter7_parquet_files/yellow_tripdata_2022-01.parquet')</pre></li>
</ol>
<p>For the second option, we remove the Parquet filename, and Spark handles the rest. You can see how the code looks in the following <span class="No-Break">code block:</span></p>
<pre class="source-code">
df = spark.read.parquet('chapter7_parquet_files/')</pre>
<ol>
<li value="2"><strong class="bold">Getting our DataFrame schema</strong>: Now, let’s use the <strong class="source-inline">.printSchema()</strong> function to see whether the DataFrame was <span class="No-Break">created successfully:</span><pre class="source-code">
df.printSchema()</pre></li>
</ol>
<p>You should see the <span class="No-Break">following output:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer158">
<img alt="Figure 7.1 – Yellow taxi trip DataFrame schema" height="434" src="image/Figure_7.1_B19453.jpg" width="540"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.1 – Yellow taxi trip DataFrame schema</p>
<ol>
<li value="3"><strong class="bold">Visualizing with pandas</strong>: This is an optional step since it requires your local machine to have enough processing capacity and can freeze your kernel trying to <span class="No-Break">process it.</span></li>
</ol>
<p>To take a <a id="_idIndexMarker479"/>better look at the DataFrame, let’s use <strong class="source-inline">.toPandas()</strong>, <span class="No-Break">as shown:</span></p>
<pre class="source-code">
df.toPandas().head(10)</pre>
<p>You should see the <span class="No-Break">following output:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer159">
<img alt="Figure 7.2 – Yellow taxi trip DataFrame with pandas visualization" height="368" src="image/Figure_7.2_B19453.jpg" width="989"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.2 – Yellow taxi trip DataFrame with pandas visualization</p>
<h2 id="_idParaDest-233"><a id="_idTextAnchor239"/>How it works…</h2>
<p>As we can observe in the preceding code, reading Parquet files is straightforward. Like many Hadoop tools, PySpark natively supports reading and <span class="No-Break">writing Parquet.</span></p>
<p>Similar to JSON and CSV files, we used a function that derives from the <strong class="source-inline">.read</strong> method to inform PySpark that a Parquet file will be read, <span class="No-Break">as follows:</span></p>
<pre class="source-code">
spark.read.parquet()</pre>
<p>We also saw two ways of reading, passing only the folder where the Parquet file is or passing the path with the Parquet filename. The best practice is to use the first case since there is usually more than one Parquet file, and reading just one may cause several errors. This is because each Parquet file inside the respective Parquet folder corresponds to a piece of <span class="No-Break">the data.</span></p>
<p>After reading and transforming the dataset into a DataFrame, we printed its schema using the <strong class="source-inline">.printSchema()</strong> method. As the name suggests, it will print and show the schema of the DataFrame. Since we didn’t specify the schema we want for the DataFrame, Spark will infer it based on the data pattern inside the columns. Don’t worry about this now; we will cover this further in the <em class="italic">Applying schemas to analytical </em><span class="No-Break"><em class="italic">data</em></span><span class="No-Break"> recipe.</span></p>
<p>Using the <strong class="source-inline">.printSchema()</strong> method before doing any operations in the DataFrame is an excellent practice for understanding the best ways to handle the <span class="No-Break">data inside.</span></p>
<p>Finally, as the last <a id="_idIndexMarker480"/>step of the recipe, we used the <strong class="source-inline">.toPandas()</strong> method to visualize our data better since the <strong class="source-inline">.show()</strong> Spark method is not intended to bring friendly visualizations like pandas. However, we must be cautious when using the <strong class="source-inline">.toPandas()</strong> method since it needs computational and memory power to translate the Spark DataFrame into a <span class="No-Break">pandas DataFrame.</span></p>
<h2 id="_idParaDest-234"><a id="_idTextAnchor240"/>There’s more…</h2>
<p>Now, let’s understand why <strong class="source-inline">parquet</strong> is an optimized file format for big data. Parquet files are column-oriented, stored <a id="_idIndexMarker481"/>in data blocks and in small chunks (a data fragment), allowing optimized reading and writing. In the following diagram, you can visually observe how <strong class="source-inline">parquet</strong> organizes data at a <span class="No-Break">high level:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer160">
<img alt="Figure 7.3 – Parquet file structure diagram by Darius Kharazi (https://dkharazi.github.io/blog/parquet)" height="260" src="image/Figure_7.3_B19453.jpg" width="709"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.3 – Parquet file structure diagram by Darius Kharazi (https://dkharazi.github.io/blog/parquet)</p>
<p>Parquet files can frequently <a id="_idIndexMarker482"/>be found in a compressed form. This adds another layer of efficiency to improve data storage and transfer. A compressed Parquet file will look <span class="No-Break">like this:</span></p>
<pre class="source-code">
yellow_tripdata_2022-01.snappy.parquet</pre>
<p>The <strong class="source-inline">snappy</strong> name informs us of the compression type and is crucial when creating a table in <strong class="bold">AWS Athena</strong>, <strong class="bold">Impala</strong>, or <strong class="bold">Hive</strong>. It is similar to the <strong class="source-inline">gzip</strong> format but more optimized for a massive volume <span class="No-Break">of data.</span></p>
<h2 id="_idParaDest-235"><a id="_idTextAnchor241"/>See also</h2>
<ul>
<li>You can read <a id="_idIndexMarker483"/>more about Apache Parquet in the official <span class="No-Break">documentation: </span><a href="https://parquet.apache.org/docs/overview/motivation/"><span class="No-Break">https://parquet.apache.org/docs/overview/motivation/</span></a></li>
<li>If you want to test <a id="_idIndexMarker484"/>other Parquet files and explore more data from <em class="italic">Open Targets Platform</em>, access this <span class="No-Break">link: </span><a href="http://ftp.ebi.ac.uk/pub/databases/opentargets/platform/22.11/output/etl/parquet/hpo/"><span class="No-Break">http://ftp.ebi.ac.uk/pub/databases/opentargets/platform/22.11/output/etl/parquet/hpo/</span></a></li>
</ul>
<h1 id="_idParaDest-236"><a id="_idTextAnchor242"/>Ingesting Avro files</h1>
<p>Like Parquet, <strong class="bold">Apache Avro</strong> is a widely <a id="_idIndexMarker485"/>used format to store analytical data. Apache Avro is a leading method of serialization to record data and relies on schemas. It <a id="_idIndexMarker486"/>also provides <strong class="bold">Remote Procedure Calls</strong> (<strong class="bold">RPCs</strong>), making <a id="_idIndexMarker487"/>transmitting data easier and resolving problems such as missing fields, extra fields, and <span class="No-Break">naming fields.</span></p>
<p>In this recipe, we will understand how to read an Avro file properly and later comprehend how <span class="No-Break">it works.</span></p>
<h2 id="_idParaDest-237"><a id="_idTextAnchor243"/>Getting ready</h2>
<p>This recipe will require <strong class="source-inline">SparkSession</strong> with some different configurations from the previous <em class="italic">Ingesting Parquet ﬁles</em> recipe. If you are already running <strong class="source-inline">SparkSession</strong>, stop it using the <span class="No-Break">following command:</span></p>
<pre class="source-code">
spark.stop()</pre>
<p>We will create another session in the <em class="italic">How to do </em><span class="No-Break"><em class="italic">it…</em></span><span class="No-Break"> section.</span></p>
<p>The dataset used here can be found at this <span class="No-Break">link: </span><a href="https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_7/ingesting_avro_files"><span class="No-Break">https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_7/ingesting_avro_files</span></a><span class="No-Break">.</span></p>
<p>Feel free to execute the code in a Jupyter notebook or your PySpark <span class="No-Break">shell session.</span></p>
<h2 id="_idParaDest-238"><a id="_idTextAnchor244"/>How to do it…</h2>
<p>Here are the steps to perform <span class="No-Break">this recipe:</span></p>
<ol>
<li><strong class="bold">Creating a custom SparkSession</strong>: To read an <strong class="source-inline">avro</strong> file, we need to import a <strong class="source-inline">.jars</strong> file in our <strong class="source-inline">SparkSession</strong> configuration, <span class="No-Break">as follows:</span><pre class="source-code">
from pyspark.sql import SparkSession
spark = SparkSession.builder \
      .master("local[1]") \
      .appName("chapter7_analytical_data_avro") \
      .config("spark.executor.memory", '3g') \
      .config("spark.executor.cores", '2') \
      .config("spark.cores.max", '2') \
      .config("spark.jars.packages", 'org.apache.spark:spark-avro_2.12:2.4.4') \
      .getOrCreate()</pre></li>
</ol>
<p>When executed, this code will provide an output similar <span class="No-Break">to this:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer161">
<img alt="Figure 7.4 – SparkSession logs when downloading an Avro file package" height="570" src="image/Figure_7.4_B19453.jpg" width="1225"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.4 – SparkSession logs when downloading an Avro file package</p>
<p>It <a id="_idIndexMarker488"/>means the <strong class="source-inline">avro</strong> package was successfully downloaded and ready to use. We will later cover how <span class="No-Break">it works.</span></p>
<ol>
<li value="2"><strong class="bold">Creating the DataFrame</strong>: With the <strong class="source-inline">.jars</strong> file configured, we will pass the file format to <strong class="source-inline">.read</strong> and add the <span class="No-Break">file’s path:</span><pre class="source-code">
df = spark.read.format('avro').load('chapter7_avro_files/file.avro')</pre></li>
<li><strong class="bold">Printing the schema</strong>: Using <strong class="source-inline">.printSchema()</strong>, let’s retrieve the schema of <span class="No-Break">this DataFrame:</span><pre class="source-code">
df.printSchema()</pre></li>
</ol>
<p>You will observe the <span class="No-Break">following output:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer162">
<img alt="Figure 7.5 – DataFrame schema from the Avro file" height="426" src="image/Figure_7.5_B19453.jpg" width="559"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.5 – DataFrame schema from the Avro file</p>
<p>As we can observe, this <a id="_idIndexMarker489"/>DataFrame contains the same data as the Parquet file covered in the last recipe, <em class="italic">Ingesting </em><span class="No-Break"><em class="italic">Parquet ﬁles</em></span><span class="No-Break">.</span></p>
<h2 id="_idParaDest-239"><a id="_idTextAnchor245"/>How it works…</h2>
<p>As you can observe, we started this recipe slightly differently by creating <strong class="source-inline">SparkSession</strong> with a custom configuration. This is because, since version 2.4, Spark does not natively provide an internal API to read or write <span class="No-Break">Avro files.</span></p>
<p>If you try to read the file used here without downloading the <strong class="source-inline">.jars</strong> file, you will get the following <span class="No-Break">error message:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer163">
<img alt="Figure 7.6 – Error message when Spark cannot find the Avro file package" height="634" src="image/Figure_7.6_B19453.jpg" width="987"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.6 – Error message when Spark cannot find the Avro file package</p>
<p>Reading the error message, we can notice it is recommended to search for a third-party (or external) <a id="_idIndexMarker490"/>source called <strong class="bold">Apache Avro Data Source</strong>, so Spark will be able to install <a id="_idIndexMarker491"/>the required packages and read the <strong class="source-inline">avro</strong> file. Check out the Spark third-parties documentation, which can be found <span class="No-Break">here: </span><a href="https://spark.apache.org/docs/latest/sql-data-sources-avro.xhtml#data-source-option"><span class="No-Break">https://spark.apache.org/docs/latest/sql-data-sources-avro.xhtml#data-source-option</span></a><span class="No-Break">.</span></p>
<p>Even though the documentation has some helpful information about how to set it for different languages, such as <strong class="bold">Scala</strong> or Python, you might find the <strong class="source-inline">org.apache.spark:spark-avro_2.12:3.3.1.jars</strong> file incompatible with some PySpark versions, and so the recommendation is to <span class="No-Break">use </span><span class="No-Break"><strong class="source-inline">org.apache.spark:spark-avro_2.12:2.4.4</strong></span><span class="No-Break">.</span></p>
<p><strong class="bold">Databricks</strong>, a company founded by the Spark creators, also has a public <strong class="source-inline">.jars</strong> file to be downloaded, but it is also incompatible with some versions of <span class="No-Break">PySpark: </span><span class="No-Break"><strong class="source-inline">com.databricks:spark-avro_2.11:4.0.0</strong></span><span class="No-Break">.</span></p>
<p>Due to the non-existence of an internal API to handle this file, we need to inform Spark of the format of the file, as <span class="No-Break">shown here:</span></p>
<pre class="source-code">
spark.read.format('avro')</pre>
<p>We did the same thing when reading <strong class="bold">MongoDB</strong> collections, as seen in <a href="B19453_05.xhtml#_idTextAnchor161"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, in the <em class="italic">Ingesting data from MongoDB using </em><span class="No-Break"><em class="italic">PySpark</em></span><span class="No-Break"> recipe.</span></p>
<p>Once our file is converted <a id="_idIndexMarker492"/>into a DataFrame, all other functionalities and operations are identical without prejudice. As we saw, Spark will infer the schema and transform it into a <span class="No-Break">columnar format.</span></p>
<h2 id="_idParaDest-240"><a id="_idTextAnchor246"/>There’s more…</h2>
<p>Now that we know about both Apache Parquet and Apache Avro, you might wonder when to use each. Even though both are used to store analytical data, some key <span class="No-Break">differences exist.</span></p>
<p>The main difference is how they store data. Parquet stores are in a columnar format, while Avro stores <a id="_idIndexMarker493"/>data in rows, which can be very efficient if you want to retrieve the entire row or dataset. However, columnar formats are much more optimized for aggregations or larger datasets, and <strong class="source-inline">parquet</strong> also supports more efficient queries using large-scale data <span class="No-Break">and compression.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer164">
<img alt="Figure 7.7 – Columnar versus row format" height="737" src="image/Figure_7.7_B19453.jpg" width="1463"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.7 – Columnar versus row format</p>
<p>On the other hand, Avro files <a id="_idIndexMarker494"/>are commonly used for data streaming. A good example is when using <strong class="bold">Kafka</strong> with <strong class="bold">Schema Registry</strong>, it will allow Kafka to verify the file’s expected schema in real time. You can see some example code in the Databricks documentation <span class="No-Break">here: </span><a href="https://docs.databricks.com/structured-streaming/avro-dataframe.xhtml"><span class="No-Break">https://docs.databricks.com/structured-streaming/avro-dataframe.xhtml</span></a><span class="No-Break">.</span></p>
<h2 id="_idParaDest-241"><a id="_idTextAnchor247"/>See also</h2>
<p>Read more <a id="_idIndexMarker495"/>about how Apache Avro works and its functionalities on the official documentation page <span class="No-Break">here: </span><a href="https://avro.apache.org/docs/"><span class="No-Break">https://avro.apache.org/docs/</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-242"><a id="_idTextAnchor248"/>Applying schemas to analytical data</h1>
<p>In the previous <a id="_idIndexMarker496"/>chapter, we saw how to apply schemas <a id="_idIndexMarker497"/>to structured and unstructured data, but the application of a schema is not limited to <span class="No-Break">raw files.</span></p>
<p>Even when working with already processed data, there will be cases when we need to cast the values of a column or change column names to be used by another department. In this recipe, we will learn how to apply a schema to Parquet files and how <span class="No-Break">it works<a id="_idTextAnchor249"/>.</span></p>
<h2 id="_idParaDest-243"><a id="_idTextAnchor250"/>Getting ready</h2>
<p>We will need <strong class="source-inline">SparkSession</strong> for this recipe. Ensure you have a session that is up and running. We will use the same dataset as in the <em class="italic">Ingesting Parquet </em><span class="No-Break"><em class="italic">ﬁles</em></span><span class="No-Break"> recipe.</span></p>
<p>Feel free to execute the code using a Jupyter notebook or your PySpark <span class="No-Break">shell session.</span></p>
<h2 id="_idParaDest-244"><a id="_idTextAnchor251"/>How to do it…</h2>
<p>Here are the steps to perform <span class="No-Break">this recipe:</span></p>
<ol>
<li><strong class="bold">Looking at our columns</strong>: As seen in the <em class="italic">Ingesting Parquet ﬁles</em> recipe, we can list the columns and their inferred data types. You can see the list <span class="No-Break">as follows:</span><pre class="source-code">
 VendorID: long
 tpep_pickup_datetime: timestamp
 tpep_dropoff_datetime: timestamp
 passenger_count: double
 trip_distance: double
 RatecodeID: double
 store_and_fwd_flag: string
 PULocationID: long
 DOLocationID: long
 payment_type: long
 fare_amount: double
 extra: double
 mta_tax: double
 tip_amount: double
 tolls_amount: double
 improvement_surcharge: double
 total_amount: double
 congestion_surcharge: double</pre></li>
<li><strong class="bold">Structuring our schema</strong>: With the columns noted, it is easier to structure a schema <a id="_idIndexMarker498"/>using PySpark code. Let’s use this opportunity <a id="_idIndexMarker499"/>to change the name of some columns, such as <strong class="source-inline">VendorID</strong>, to a more readable form. Refer to the <span class="No-Break">following code:</span><pre class="source-code">
from pyspark.sql.types import StructType, StructField, StringType, LongType, DoubleType, TimestampType
schema = StructType([ \
    StructField("vendorId", LongType() ,True), \
    StructField("tpep_pickup_datetime", TimestampType() ,True), \
    StructField("tpep_dropoff_datetime", TimestampType() ,True), \
    StructField("passenger_count", DoubleType() ,True), \
    StructField("trip_distance", DoubleType() ,True), \
    StructField("ratecodeId", DoubleType() ,True), \
    StructField("store_and_fwd_flag", StringType() ,True), \
    StructField("puLocationId", LongType() ,True), \
    StructField("doLocationId", LongType() ,True), \
    StructField("payment_type", LongType() ,True), \
    StructField("fare_amount", DoubleType() ,True), \
    StructField("extra", DoubleType() ,True), \
    StructField("mta_tax", DoubleType() ,True), \
    StructField("tip_amount", DoubleType() ,True), \
    StructField("tolls_amount", DoubleType() ,True), \
    StructField("improvement_surcharge", DoubleType() ,True), \
    StructField("total_amount", DoubleType() ,True), \
    StructField("congestion_surcharge", DoubleType() ,True), \
    StructField("airport_fee", DoubleType() ,True), \
])</pre></li>
<li><strong class="bold">Applying the schema</strong>: With the schema stored in a variable, we can attribute it to a new DataFrame and reread the <span class="No-Break"><strong class="source-inline">parquet</strong></span><span class="No-Break"> file:</span><pre class="source-code">
df_new_schema = spark.read.schema(schema).parquet('chapter7_parquet_files/yellow_tripdata_2022-01.parquet')</pre></li>
<li><strong class="bold">Printing the new DataFrame schema</strong>: When printing the schema, we can see the name <a id="_idIndexMarker500"/>of some columns changed as we <a id="_idIndexMarker501"/>set them on the schema object in <span class="No-Break"><em class="italic">step 1</em></span><span class="No-Break">:</span><pre class="source-code">
df_new_schema.printSchema()</pre></li>
</ol>
<p>Executing the preceding code will provide the <span class="No-Break">following output:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer165">
<img alt="Figure 7.8 – DataFrame with new schema applied" height="426" src="image/Figure_7.8_B19453.jpg" width="559"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.8 – DataFrame with new schema applied</p>
<ol>
<li value="5"><strong class="bold">Visualizing with pandas</strong>: This step is optional since it may require some processing capacity from your local machine. We can double-check whether the final output is correct using the <strong class="source-inline">.toPandas()</strong> function, <span class="No-Break">as follows:</span><pre class="source-code">
df_new_schema.toPandas().head(10)</pre></li>
</ol>
<p>The output looks <span class="No-Break">like this:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer166">
<img alt="Figure 7.9 – Yellow taxi trip DataFrame visualization with pandas" height="371" src="image/Figure_7.9_B19453.jpg" width="998"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.9 – Yellow taxi trip DataFrame visualization with pandas</p>
<p>As you can see, no numerical data has changed, and therefore the data <span class="No-Break">integrity remains.</span></p>
<h2 id="_idParaDest-245"><a id="_idTextAnchor252"/>How it works…</h2>
<p>As we can observe in this exercise, defining and setting the schema for a DataFrame is not complex. However, it can be a bit laborious when we think about knowing the data types or declaring each column of <span class="No-Break">the DataFrame.</span></p>
<p>The first step to start the schema definition is understanding the dataset we need to handle. This can <a id="_idIndexMarker502"/>be done by consulting a data catalog or even <a id="_idIndexMarker503"/>someone in more contact with the data. As a last option, you can create the DataFrame, let Spark infer the schema, and make adjustments when re-creating <span class="No-Break">the DataFrame.</span></p>
<p>When creating the schema structure in Spark, there are a few items we need to pay attention to, as you can <span class="No-Break">see here:</span></p>
<ul>
<li><strong class="bold">StructType</strong>: The first thing to do is to declare the <strong class="source-inline">StructType</strong> object, which represents the schema of a list <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">StructField</strong></span><span class="No-Break">.</span></li>
<li><strong class="bold">StructField</strong>: <strong class="source-inline">StructField </strong>will define the name, data type, and whether the column allows null or <span class="No-Break">empty fields.</span></li>
<li><strong class="bold">Data types</strong>: The last thing to bear in mind is where we will define the column’s data type; as you can imagine, a few data types are available. You can always consult the <a id="_idIndexMarker504"/>documentation to see the supported data types <span class="No-Break">here: </span><a href="https://spark.apache.org/docs/latest/sql-ref-datatypes.xhtml"><span class="No-Break">https://spark.apache.org/docs/latest/sql-ref-datatypes.xhtml</span></a><span class="No-Break">.</span></li>
</ul>
<p>Once we have <a id="_idIndexMarker505"/>defined the schema object, we can easily <a id="_idIndexMarker506"/>attribute it to the function that creates the DataFrame using the <strong class="source-inline">.schema()</strong> method, as we saw in <span class="No-Break"><em class="italic">step 3</em></span><span class="No-Break">.</span></p>
<p>With the DataFrame created, all the following commands remain <span class="No-Break">the same.</span></p>
<h2 id="_idParaDest-246"><a id="_idTextAnchor253"/>There’s more…</h2>
<p>Let’s do an experiment <a id="_idIndexMarker507"/>where instead of using <strong class="source-inline">TimestampType()</strong>, we will use <strong class="source-inline">DateType()</strong>. See the following portion of the <span class="No-Break">changed code:</span></p>
<pre class="source-code">
    StructField("tpep_pickup_datetime", DateType() ,True), \
    StructField("tpep_dropoff_datetime", DateType() ,True), \</pre>
<p>If we repeat the steps using the preceding code change, an error message will appear when we try to visualize <span class="No-Break">the data:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer167">
<img alt="Figure 7.10 – Error reading the DataFrame when attributing an incompatible data type" height="489" src="image/Figure_7.10_B19453.jpg" width="1183"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.10 – Error reading the DataFrame when attributing an incompatible data type</p>
<p>The reason behind this is the incompatibility of these two data types when formatting the data inside the column. <strong class="source-inline">DateType()</strong> uses the <strong class="source-inline">yyyy-MM-dd</strong> format, while <strong class="source-inline">TimestampType()</strong> uses <span class="No-Break"><strong class="source-inline">yyy-MM-dd HH:mm:ss.SSSS</strong></span><span class="No-Break">.</span></p>
<p>Looking closely at <a id="_idIndexMarker508"/>both columns, we see hour, minute, and second information. If we try to force it into another format, it could corrupt <span class="No-Break">the data.</span></p>
<h2 id="_idParaDest-247"><a id="_idTextAnchor254"/>See also</h2>
<p>Learn more about the Spark data types <span class="No-Break">here: </span><a href="https://spark.apache.org/docs/3.0.0-preview/sql-ref-datatypes.xhtml"><span class="No-Break">https://spark.apache.org/docs/3.0.0-preview/sql-ref-datatypes.xhtml</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-248"><a id="_idTextAnchor255"/>Filtering data and handling common issues</h1>
<p>Filtering data is a <a id="_idIndexMarker509"/>process of excluding or selecting only the necessary information <a id="_idIndexMarker510"/>to be used or stored. Even analytical data must be re-filtered to meet a specific need. An excellent example is <strong class="bold">data marts</strong> (we will cover them later in <span class="No-Break">this recipe).</span></p>
<p>This recipe aims to understand how to create and apply filters to our data using a <span class="No-Break">real-world example.</span></p>
<h2 id="_idParaDest-249"><a id="_idTextAnchor256"/>Getting ready</h2>
<p>This recipe requires <strong class="source-inline">SparkSession</strong>, so ensure yours is up and running. You can use the code provided at the beginning of the chapter or create <span class="No-Break">your own.</span></p>
<p>The dataset used here will be the same as in the <em class="italic">Ingesting Parquet </em><span class="No-Break"><em class="italic">ﬁles</em></span><span class="No-Break"> recipe.</span></p>
<p>To make this exercise more practical, let’s imagine we want to analyze two scenarios: how many trips each vendor made and what hour of the day there are more pickups. We will create some aggregations and filter our dataset to carry out <span class="No-Break">those analyses.</span></p>
<h2 id="_idParaDest-250"><a id="_idTextAnchor257"/>How to do it…</h2>
<p>Here are the steps to perform <span class="No-Break">this recipe:</span></p>
<ol>
<li><strong class="bold">Reading the Parquet file</strong>: Let’s start by reading our Parquet file, as the following <span class="No-Break">code shows:</span><pre class="source-code">
df = spark.read.parquet('chapter7_parquet_files/')</pre></li>
<li><strong class="bold">Getting the vendor in ascending order</strong>: Let’s imagine a scenario where we want to filter all the <strong class="source-inline">vendorId</strong> instances and how many trips each vendor carried out in January (the timeframe of our dataset). We can use a <strong class="source-inline">.groupBy()</strong> function with <strong class="source-inline">.count()</strong>, <span class="No-Break">as follows:</span><pre class="source-code">
df.groupBy("vendorId").count().orderBy("vendorId").show()</pre></li>
</ol>
<p>This is what the vendor trip count <span class="No-Break">looks like:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer168">
<img alt="Figure 7.11 – vendorId trips count" height="168" src="image/Figure_7.11_B19453.jpg" width="192"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.11 – vendorId trips count</p>
<ol>
<li value="3"><strong class="bold">Retrieving the busiest pickup hours</strong>: Now, let’s group by the hours of the pickups using <a id="_idIndexMarker511"/>the <strong class="source-inline">tpep_pickup_datetime</strong> column, as shown in the <a id="_idIndexMarker512"/>following code. Then, we make a count and order it in an <span class="No-Break">ascending flow:</span><pre class="source-code">
from pyspark.sql.functions import year, month, dayofmonth
df.groupBy(hour("tpep_pickup_datetime")).count().orderBy("count").show()</pre></li>
</ol>
<p>This is what the output <span class="No-Break">looks like:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer169">
<img alt="Figure 7.12 – Count of trips per hour" height="541" src="image/Figure_7.12_B19453.jpg" width="363"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.12 – Count of trips per hour</p>
<p>As you can observe, at <strong class="source-inline">14</strong> hours and <strong class="source-inline">19</strong> hours, there is an increase in the number of pickups. We can think of some possible reasons for this, such as lunchtime and <span class="No-Break">rush hour.</span></p>
<h2 id="_idParaDest-251"><a id="_idTextAnchor258"/>How it works…</h2>
<p>Since we are already very familiar with the reading operation to create a DataFrame, let’s go straight to <span class="No-Break"><em class="italic">step 2</em></span><span class="No-Break">:</span></p>
<pre class="source-code">
df.groupBy("vendorId").count().orderBy("vendorId").show()</pre>
<p>As you can observe, the <a id="_idIndexMarker513"/>chain of functions here closely resembles SQL operations. This is <a id="_idIndexMarker514"/>because, behind the scenes, we are using the native SQL methods a <span class="No-Break">DataFrame supports.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">Like the SQL operations, the order of the methods in the chain will influence the result, and even whether it will result in a success <span class="No-Break">or not.</span></p>
<p>In <em class="italic">step 3</em>, we added a little bit more complexity by extracting the hour value from the <strong class="source-inline">tpep_pickup_datetime</strong> column. That was only possible because this column is of the timestamp data type. Also, we ordered by the count column this time, which was created once we <a id="_idIndexMarker515"/>invoked the <strong class="source-inline">.count()</strong> function, similar to the SQL. You can see <a id="_idIndexMarker516"/>this in the <span class="No-Break">following code:</span></p>
<pre class="source-code">
df.groupBy(hour("tpep_pickup_datetime")).count().orderBy("count").show()</pre>
<p>There are <a id="_idIndexMarker517"/>plenty of other functions, such as <strong class="source-inline">.filter()</strong> and <strong class="source-inline">.select()</strong>. You can find more PySpark functions <span class="No-Break">here: </span><a href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.xhtml"><span class="No-Break">https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.xhtml</span></a><span class="No-Break">.</span></p>
<h2 id="_idParaDest-252"><a id="_idTextAnchor259"/>There’s more…</h2>
<p>The analysis in this recipe was carried out using SQL functions natively supported by PySpark. However, these functions are not a good fit for more <span class="No-Break">complex queries.</span></p>
<p>In those cases, the <a id="_idIndexMarker518"/>best practice is to use the <strong class="bold">SQL API</strong> of Spark. Let’s see how to do it in the code <span class="No-Break">that follows:</span></p>
<ol>
<li><strong class="bold">Creating a temporary view</strong>: First, we will create a temporary view based on our DataFrame. For this, we use the <strong class="source-inline">.createOrReplaceTempView()</strong> method and pass a name to our temporary view, <span class="No-Break">as follows:</span><pre class="source-code">
df.createOrReplaceTempView("ny_yellow_taxi_data")</pre></li>
<li><strong class="bold">Inserting our SQL query</strong>: Using the <strong class="source-inline">SparkSession</strong> variable (<strong class="source-inline">spark</strong>), we will invoke <strong class="source-inline">.sql()</strong> and pass a multi-lined string containing the desired SQL code. To make it easier to visualize the results, let’s also attribute it to a variable <span class="No-Break">called </span><span class="No-Break"><strong class="source-inline">vendor_groupby</strong></span><span class="No-Break">.</span></li>
</ol>
<p>Observe we use the temporary view name to indicate where the query will <span class="No-Break">be made:</span></p>
<pre class="source-code">
vendor_groupby = spark.sql(
"""
SELECT vendorId, COUNT(*) FROM ny_yellow_taxi_data
GROUP BY vendorId
ORDER BY COUNT(*)
"""
)</pre>
<p>Executing this code will not generate <span class="No-Break">an output.</span></p>
<ol>
<li value="3"><strong class="bold">Showing our results</strong>: Once the query is executed and instantiated on a variable, it will be a <a id="_idIndexMarker519"/>Spark DataFrame object, and the <strong class="source-inline">.show()</strong> method will work to bring the results, as shown in the <span class="No-Break">following code:</span><pre class="source-code">
vendor_groupby.show()</pre></li>
</ol>
<p>This is <span class="No-Break">the output:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer170">
<img alt="Figure 7.13 – vendorId counts of trips using SQL code" height="171" src="image/Figure_7.13_B19453.jpg" width="201"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.13 – vendorId counts of trips using SQL code</p>
<p>The downside of using the SQL API is that the error logs might sometimes be unclear. See the <span class="No-Break">following screenshot:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer171">
<img alt="Figure 7.14 – Spark error when SQL does not have the right syntax" height="265" src="image/Figure_7.14_B19453.jpg" width="996"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.14 – Spark error when SQL does not have the right syntax</p>
<p>This screenshot shows the result of a query where the syntax was incorrect when grouping by the <strong class="source-inline">tpep_pickup_datetime</strong> column. In scenarios like this, the best approach is to <a id="_idIndexMarker520"/>debug using baby steps, executing the query operations and conditionals one by one. If your DataFrame comes from a table in a database, try to reproduce the query directly on the database and see whether there is a more intuitive <span class="No-Break">error message.</span></p>
<h3>Data marts</h3>
<p>As mentioned <a id="_idIndexMarker521"/>at the beginning of this recipe, one common use case for ingesting and re-filtering analytical data is to use it in a <span class="No-Break">data mart.</span></p>
<p>Data marts are a smaller version of a data warehouse, with data concentrated on one subject, such as from a financial or sales department. The following diagram shows how they tend to <span class="No-Break">be organized:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer172">
<img alt="Figure 7.15 – Data marts diagram" height="453" src="image/Figure_7.15_B19453.jpg" width="1309"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.15 – Data marts diagram</p>
<p>Implementing the data mart concept has many benefits, such as reaching specific information or guaranteeing temporary access to a strict piece of data for a project without managing the security access of several users to the <span class="No-Break">data warehouse.</span></p>
<h2 id="_idParaDest-253"><a id="_idTextAnchor260"/>See also</h2>
<p>Find out <a id="_idIndexMarker522"/>more about data marts and data warehouse concepts on the Panoply.io <span class="No-Break">blog: </span><a href="https://panoply.io/data-warehouse-guide/data-mart-vs-data-warehouse/"><span class="No-Break">https://panoply.io/data-warehouse-guide/data-mart-vs-data-warehouse/</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-254"><a id="_idTextAnchor261"/>Ingesting partitioned data</h1>
<p>The practice of <a id="_idIndexMarker523"/>partitioning data is not recent. It was implemented in databases to distribute data across multiple disks or tables. Actually, data warehouses can partition data according to the purpose and use of the data inside. You can read more <span class="No-Break">here: </span><a href="https://www.tutorialspoint.com/dwh/dwh_partitioning_strategy.htm"><span class="No-Break">https://www.tutorialspoint.com/dwh/dwh_partitioning_strategy.htm</span></a><span class="No-Break">.</span></p>
<p>In our case, partitioning data is related to how our data will be split into small chunks <span class="No-Break">and processed.</span></p>
<p>In this recipe, we will learn how to ingest data that is already partitioned and how it can affect the performance of <span class="No-Break">our code.</span></p>
<h2 id="_idParaDest-255"><a id="_idTextAnchor262"/>Getting ready</h2>
<p>This recipe requires an initialized <strong class="source-inline">SparkSession</strong>. You can create your own or use the code provided at the beginning of <span class="No-Break">this chapter.</span></p>
<p>The data required to complete the steps can be found <span class="No-Break">here: </span><a href="https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_7/ingesting_partitioned_data"><span class="No-Break">https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook/tree/main/Chapter_7/ingesting_partitioned_data</span></a><span class="No-Break">.</span></p>
<p>You can use a Jupyter notebook or a PySpark shell session to execute <span class="No-Break">the code.</span></p>
<h2 id="_idParaDest-256"><a id="_idTextAnchor263"/>How to do it…</h2>
<p>Use the following steps to perform <span class="No-Break">this recipe:</span></p>
<ol>
<li><strong class="bold">Creating the DataFrame for the February data</strong>: Let’s use the usual way for creating a DataFrame from a Parquet file, but this time passing only the month we want <span class="No-Break">to read:</span><pre class="source-code">
df_partitioned = spark.read.parquet("chapter7_partitioned_files/month=2/")</pre></li>
</ol>
<p>You should see no output from <span class="No-Break">this execution.</span></p>
<ol>
<li value="2"><strong class="bold">Using pandas to show the results</strong>: Once the DataFrame is created, we can better visualize the results <span class="No-Break">using pandas:</span><pre class="source-code">
df_partitioned.toPandas()</pre></li>
</ol>
<p>You should observe <span class="No-Break">this output:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer173">
<img alt="Figure 7.16 – Yellow taxi trip DataFrame visualization using partitioned data" height="276" src="image/Figure_7.16_B19453.jpg" width="996"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.16 – Yellow taxi trip DataFrame visualization using partitioned data</p>
<p>Observe that in the <strong class="source-inline">tpep_pickup_datetime</strong> column, there is only data from February, and <a id="_idIndexMarker524"/>now we don’t need to be very preoccupied with the processing capacity of our <span class="No-Break">local machine.</span></p>
<h2 id="_idParaDest-257"><a id="_idTextAnchor264"/>How it works…</h2>
<p>This was a very simple recipe, but there are some important concepts that we need to understand <span class="No-Break">a bit.</span></p>
<p>As you can observe, all the magic happens during the creation of the DataFrame, where we pass not only the path where our Parquet files are stored but also the name of another subfolder containing the month reference. Let’s take a look at how this folder is organized in the <span class="No-Break">following screenshot:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer174">
<img alt="Figure 7.17 – Folder showing data partitioned by month" height="500" src="image/Figure_7.17_B19453.jpg" width="362"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.17 – Folder showing data partitioned by month</p>
<p class="callout-heading">Note</p>
<p class="callout">The <strong class="source-inline">_SUCCESS</strong> file indicates that the partitioning write process was <span class="No-Break">successfully made.</span></p>
<p>Inside the <strong class="source-inline">chapter7_partitioned_files</strong> folder, there are other subfolders with a number of <a id="_idIndexMarker525"/>references. Each of these subfolders represents a partition, in this case, categorized <span class="No-Break">by month.</span></p>
<p>If we look inside a subfolder, we can observe one or more Parquet files. Refer to the <span class="No-Break">following screenshot:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer175">
<img alt="Figure 7.18 – Parquet file for February" height="295" src="image/Figure_7.18_B19453.jpg" width="648"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.18 – Parquet file for February</p>
<p>Partitions are an optimized form of reading or writing a specific amount of data from a dataset. That’s why using pandas to visualize the DataFrame was faster <span class="No-Break">this time.</span></p>
<p>Partitioning also makes the execution of transformations faster since data will be processed using <a id="_idIndexMarker526"/>parallelism across the Spark internal worker nodes. You can visualize it better in the <span class="No-Break">following figure:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer176">
<img alt="Figure 7.19 – Partitioning parallelism diagram" height="543" src="image/Figure_7.19_B19453.jpg" width="1504"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.19 – Partitioning parallelism diagram</p>
<h2 id="_idParaDest-258"><a id="_idTextAnchor265"/>There’s more…</h2>
<p>As we saw, working with partitions to save data on a large scale brings several benefits. However, knowing how to partition your data is the key to reading and writing data in a performative way. Let’s list the three most important best practices when <span class="No-Break">writing partitions:</span></p>
<ul>
<li><strong class="bold">Use the right columns to partition</strong>: In this exercise, we saw an example of partitioning <a id="_idIndexMarker527"/>by the column called <strong class="source-inline">month</strong>, but it is possible to partition over any column and even to use a column with year, month, or day to bring more granularity. Normally, partitioning reflects what the best way to retrieve the data <span class="No-Break">will be.</span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer177">
<img alt="Figure 7.20 – Partition folders by month" height="500" src="image/Figure_7.20_B19453.jpg" width="385"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.20 – Partition folders by month</p>
<ul>
<li><strong class="bold">Coalesce small files</strong>: Depending on how our <strong class="source-inline">SparkSession</strong> is configured or where it will write the final files, Spark can create small <strong class="source-inline">parquet</strong>/<strong class="source-inline">avro</strong> files. From a large data scale perspective, reading these small files can <span class="No-Break">prejudice performance.</span></li>
</ul>
<p>A good practice is to use <strong class="source-inline">coalesce()</strong> while invoking the <strong class="source-inline">write()</strong> function to aggregate the files into a small amount. Here is <span class="No-Break">an example:</span></p>
<pre class="source-code">
df.coalesce(1).write.parquet('myfile/path')</pre>
<p>You can find a good article about it <span class="No-Break">here: </span><a href="https://www.educba.com/pyspark-coalesce/"><span class="No-Break">https://www.educba.com/pyspark-coalesce/</span></a><span class="No-Break">.</span></p>
<ul>
<li><strong class="bold">Avoid over-partitioning</strong>: This <a id="_idIndexMarker528"/>follows the same logic as the previous one. Over-partitioning will create small files since we split them using a granularity rule, and then Spark's parallelism will <span class="No-Break">be slowed.</span></li>
</ul>
<h2 id="_idParaDest-259"><a id="_idTextAnchor266"/>See also</h2>
<p>You can find more good practices <span class="No-Break">here: </span><a href="https://climbtheladder.com/10-spark-partitioning-best-practices/"><span class="No-Break">https://climbtheladder.com/10-spark-partitioning-best-practices/</span></a><span class="No-Break">.</span></p>
<p>Related to the topic of partitioning, we also have the <em class="italic">database sharding</em> concept. It is a very <a id="_idIndexMarker529"/>interesting topic, and the MongoDB official documentation has a very nice post about it <span class="No-Break">here: </span><a href="https://www.mongodb.com/features/database-sharding-explained"><span class="No-Break">https://www.mongodb.com/features/database-sharding-explained</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-260"><a id="_idTextAnchor267"/>Applying reverse ETL</h1>
<p>As the name suggests, <strong class="bold">reverse ETL</strong> takes data from a data warehouse and inserts it into a business <a id="_idIndexMarker530"/>application such as <strong class="bold">HubSpot</strong> or <strong class="bold">Salesforce</strong>. The reason behind this is to make data more operational and use business tools to bring more insights to data that is already in a format ready for analysis or <span class="No-Break">analytical format.</span></p>
<p>This recipe will teach us how to architect a reverse ETL pipeline and about the commonly <span class="No-Break">used tools.</span></p>
<h2 id="_idParaDest-261"><a id="_idTextAnchor268"/>Getting ready</h2>
<p>There are no technical requirements for this recipe. However, it is encouraged to use a whiteboard or a notepad to <span class="No-Break">take notes.</span></p>
<p>Here, we will work with a scenario where we are ingesting data from an <strong class="bold">e-learning platform</strong>. Imagine we received a request from the marketing department to better understand user actions on the platform using the <span class="No-Break">Salesforce system.</span></p>
<p>The objective here will be to create a diagram showing the data flow process from a source of data to the <span class="No-Break">Salesforce platform.</span></p>
<h2 id="_idParaDest-262"><a id="_idTextAnchor269"/>How to do it…</h2>
<p>To make this exercise more straightforward, we will assume we already have data stored in the database for the <span class="No-Break">e-learning platform:</span></p>
<ol>
<li><strong class="bold">Ingesting user action data from the website</strong>: Let’s imagine we have a frontend API that sends useful information about our user’s actions and behavior on the e-learning platform to our backend databases. Refer to the following diagram to see what it <span class="No-Break">looks like:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer178">
<img alt=" Figure 7.21 – Data flow from the frontend to an API in the backend" height="585" src="image/Figure_7.21_B19453.jpg" width="593"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> Figure 7.21 – Data flow from the frontend to an API in the backend</p>
<ol>
<li value="2"><strong class="bold">Processing it using ETL</strong>: With the available data, we can pick the necessary information that <a id="_idIndexMarker531"/>the marketing department needs and put it into the ETL process. We will ingest it from the backend database, apply any cleansing or transformations needed, and then load it into our <span class="No-Break">data warehouse.</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer179">
<img alt=" Figure 7.22 – Diagram showing backend storage to the data warehouse" height="169" src="image/Figure_7.22_B19453.jpg" width="341"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> Figure 7.22 – Diagram showing backend storage to the data warehouse</p>
<ol>
<li value="3"><strong class="bold">Storing data in a data warehouse</strong>: After the data is ready and transformed into an analytical format, it will be stored in the data warehouse. We don’t need to worry here about how data is modeled. Let’s assume a new analytical table will be created just for this <span class="No-Break">processing purpose.</span></li>
<li><strong class="bold">ETL to Salesforce</strong>: Once data is populated in the data warehouse, we need to insert it into the Salesforce system. Let’s do this using PySpark, as you can see in the <span class="No-Break">following figure:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer180">
<img alt="Figure 7.23 – Data warehouse data flow to Salesforce" height="213" src="image/Figure_7.23_B19453.jpg" width="681"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.23 – Data warehouse data flow to Salesforce</p>
<p>With data inside Salesforce, we can advise the marketing team and automate this process to be triggered on a <span class="No-Break">necessary schedule.</span></p>
<h2 id="_idParaDest-263"><a id="_idTextAnchor270"/>How it works…</h2>
<p>Although it seems <a id="_idIndexMarker532"/>complicated, the reverse ETL process is similar to an ingest job. In some cases, adding a few more transformations to fit the final application model might be necessary, but isn’t complex. Now, let’s take a closer look at what we did in <span class="No-Break">the recipe.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer181">
<img alt="Figure 7.24 – Reverse ETL diagram overview" height="345" src="image/Figure_7.24_B19453.jpg" width="1037"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.24 – Reverse ETL diagram overview</p>
<p>First, we need to understand whether we already have the requested raw data stored in our internal database to meet the marketing department’s needs. If we don’t, the data team is responsible for reaching out to the responsible developers to verify how to <span class="No-Break">accomplish that.</span></p>
<p>Once this is checked, we proceed with the usual ETL pipeline. Normally, there will be SQL transformations to filter or group information based on the needs of the analysis. Then, we store it in a <em class="italic">source of truth</em>, such as a main <span class="No-Break">data warehouse.</span></p>
<p>It is in <em class="italic">step 4</em> that the reverse ETL occurs. The origin of this name is because, normally, the ETL process involves retrieving data from an application such as Salesforce and storing it in a data <a id="_idIndexMarker533"/>warehouse. However, in recent years, these tools have become another form of better understanding how users are behaving or interacting with <span class="No-Break">our applications.</span></p>
<p>With user-centric feedback solutions with analytical data, we can get better insights into and access to specific results. Another example besides Salesforce can be a <strong class="bold">machine learning</strong> solution to predict whether some change in the e-learning platform would result in improved <span class="No-Break">user retention.</span></p>
<h2 id="_idParaDest-264"><a id="_idTextAnchor271"/>There’s more…</h2>
<p>To carry out reverse ETL, we can create our own solution or use a commercial one. Plenty of solutions <a id="_idIndexMarker534"/>on the market retrieve data from data warehouses and connect dynamically with business solutions. Some can also generate reports to provide feedback to the data warehouse again, improving the quality of information sent and even creating other analyses. The cons of these tools are that most are paid solutions, and free tiers tend to include one or <span class="No-Break">few connections.</span></p>
<p>One of the most <a id="_idIndexMarker535"/>used reverse ETL tools is <strong class="bold">Hightouch</strong>; you can find out more <span class="No-Break">here: </span><a href="https://hightouch.com/"><span class="No-Break">https://hightouch.com/</span></a><span class="No-Break">.</span></p>
<h2 id="_idParaDest-265"><a id="_idTextAnchor272"/>See also</h2>
<p>You can read more about reverse ETL at <em class="italic">Astasia Myers’</em> Medium <span class="No-Break">blog: </span><a href="https://medium.com/memory-leak/reverse-etl-a-primer-4e6694dcc7fb"><span class="No-Break">https://medium.com/memory-leak/reverse-etl-a-primer-4e6694dcc7fb</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-266"><a id="_idTextAnchor273"/>Selecting analytical data for reverse ETL</h1>
<p>Now that we <a id="_idIndexMarker536"/>know what reverse ETL is, the next step is to <a id="_idIndexMarker537"/>understand which types of analytical data are a good use case to load into a Salesforce application, <span class="No-Break">for example.</span></p>
<p>This recipe continues from the previous one, <em class="italic">Applying reverse ETL</em>, intending to illustrate a real scenario of deciding what data will be transferred into a <span class="No-Break">Salesforce application.</span></p>
<h2 id="_idParaDest-267"><a id="_idTextAnchor274"/>Getting ready</h2>
<p>This recipe has no technical requirements, but you can use a whiteboard or a notepad <span class="No-Break">for annotations.</span></p>
<p>Still using the example of a scenario where the marketing department requested data to be loaded into their Salesforce account, we will now go a little deeper to see what information is relevant for <span class="No-Break">their analysis.</span></p>
<p>We received a request from the marketing team to understand the user journey in the e-learning platform. They want to understand which courses are watched most and whether some need improvement. Currently, they don’t know what information we have in our <span class="No-Break">backend databases.</span></p>
<h2 id="_idParaDest-268"><a id="_idTextAnchor275"/>How to do it…</h2>
<p>Let’s work on this scenario in small steps. The objective here will be to understand what data we need to accomplish <span class="No-Break">the task:</span></p>
<ol>
<li><strong class="bold">Consulting the data catalog</strong>: To simplify our work, let’s assume our data engineers worked on creating a data catalog with the user information collected and stored in the backend databases. In the following diagram, we can better see how the information <span class="No-Break">is stored:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer182">
<img alt="Figure 7.25 – Tables of interest for reverse ETL highlighted" height="301" src="image/Figure_7.25_B19453.jpg" width="784"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.25 – Tables of interest for reverse ETL highlighted</p>
<p>As we <a id="_idIndexMarker538"/>can see, there are three tables with <a id="_idIndexMarker539"/>potentially relevant information to be retrieved: <strong class="source-inline">user_data</strong>, <strong class="source-inline">course_information</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">videos</strong></span><span class="No-Break">.</span></p>
<ol>
<li value="2"><strong class="bold">Selecting the raw data</strong>: We can see highlighted in the following diagram the columns that can provide the information needed for <span class="No-Break">the analysis:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer183">
<img alt="Figure 7.26 – Tables and respective columns highlighted as relevant for reverse ETL" height="439" src="image/Figure_7.26_B19453.jpg" width="1099"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.26 – Tables and respective columns highlighted as relevant for reverse ETL</p>
<ol>
<li value="3"><strong class="bold">Transforming and filtering data</strong>: Since we need a single table to load data into Salesforce, we can make a SQL filter and join <span class="No-Break">the information.</span></li>
</ol>
<h2 id="_idParaDest-269"><a id="_idTextAnchor276"/>How it works…</h2>
<p>As mentioned at the beginning of the recipe, the marketing team wants to understand the user journey in the e-learning application. First, let’s understand what a user <span class="No-Break">journey is.</span></p>
<p>A user journey is all the actions and interactions a user carries out on a system or application, from <a id="_idIndexMarker540"/>when they opt to use or buy a service <a id="_idIndexMarker541"/>until they log out or leave it. Information such as what type of content they have watched and for how long is very important in <span class="No-Break">this case.</span></p>
<p>Let’s see the fields we collected and why they are important. The first six columns will give us an idea of the user and where they live. We can use these pieces of information later to see whether there are any patterns for the predilection <span class="No-Break">of content.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer184">
<img alt="Figure 7.27 – user_data relevant columns for reverse ETL" height="414" src="image/Figure_7.27_B19453.jpg" width="678"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.27 – user_data relevant columns for reverse ETL</p>
<p>Then, the last columns provide information about the content this user is watching and whether there is any relationship between the types of content. For example, if they bought a Python course and a SQL course, we can use a tag (for example, <strong class="source-inline">programming course</strong>) from the content metadata to make a filer <span class="No-Break">of correlation.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer185">
<img alt="Figure 7.28 – course_information and videos with columns highlighted" height="555" src="image/Figure_7.28_B19453.jpg" width="782"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.28 – course_information and videos with columns highlighted</p>
<p>Feeding back all <a id="_idIndexMarker542"/>this information into Salesforce can help <a id="_idIndexMarker543"/>to answer the following questions about the <span class="No-Break">user’s journey:</span></p>
<ul>
<li>Is there a tendency to finish one course before <span class="No-Break">starting another?</span></li>
<li>Do users tend to watch multiple courses at the <span class="No-Break">same time?</span></li>
<li>Does the educational team need to reformulate a course because of a <span class="No-Break">high turnover?</span></li>
</ul>
<h2 id="_idParaDest-270"><a id="_idTextAnchor277"/>See also</h2>
<p>Find <a id="_idIndexMarker544"/>more use cases <span class="No-Break">here: </span><a href="https://www.datachannel.co/blogs/reverse-etl-use-cases-common-usage-patterns"><span class="No-Break">https://www.datachannel.co/blogs/reverse-etl-use-cases-common-usage-patterns</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-271"><a id="_idTextAnchor278"/>Further reading</h1>
<p>Here is a list of websites you can refer to, to enhance your <span class="No-Break">knowledge further:</span></p>
<ul>
<li><a href="https://segment.com/blog/reverse-etl/"><span class="No-Break">https://segment.com/blog/reverse-etl/</span></a></li>
<li><a href="https://hightouch.com/blog/reverse-etl"><span class="No-Break">https://hightouch.com/blog/reverse-etl</span></a></li>
<li><a href="https://www.oracle.com/br/autonomous-database/what-is-data-mart/"><span class="No-Break">https://www.oracle.com/br/autonomous-database/what-is-data-mart/</span></a></li>
<li><a href="https://www.netsuite.com/portal/resource/articles/ecommerce/customer-lifetime-value-clv.shtml"><span class="No-Break">https://www.netsuite.com/portal/resource/articles/ecommerce/customer-lifetime-value-clv.shtml</span></a></li>
<li><a href="https://www.datachannel.co/blogs/reverse-etl-use-cases-common-usage-patterns"><span class="No-Break">https://www.datachannel.co/blogs/reverse-etl-use-cases-common-usage-patterns</span></a></li>
</ul>
</div>
</div>

<div id="sbo-rt-content"><div class="Content" id="_idContainer187">
<h1 id="_idParaDest-272"><a id="_idTextAnchor279"/>Part 2: Structuring the Ingestion Pipeline</h1>
</div>
<div id="_idContainer188">
<p>In the book’s second part, you will be introduced to the monitoring practices and see how to create your very first data ingestion pipeline using the recommended tools on the market, all the while applying the best data <span class="No-Break">engineering practices.</span></p>
<p>This part has the <span class="No-Break">following chapters:</span></p>
<ul>
<li><a href="B19453_08.xhtml#_idTextAnchor280"><em class="italic">Chapter 8</em></a>, <em class="italic">Designing Monitored Data Workﬂows</em></li>
<li><a href="B19453_09.xhtml#_idTextAnchor319"><em class="italic">Chapter 9</em></a>, <em class="italic">Putting Everything Together with Airﬂow</em></li>
<li><a href="B19453_10.xhtml#_idTextAnchor364"><em class="italic">Chapter 10</em></a>, <em class="italic">Logging and Monitoring Your Data Ingest in Airﬂow</em></li>
<li><a href="B19453_11.xhtml#_idTextAnchor402"><em class="italic">Chapter 11</em></a><em class="italic">,</em> <em class="italic">Automating Your Data Ingestion Pipelines</em></li>
<li><a href="B19453_12.xhtml#_idTextAnchor433"><em class="italic">Chapter 12</em></a>, <em class="italic">Using Data Observability for Debugging, Error Handling, and Preventing Downtime</em></li>
</ul>
</div>
<div>
<div id="_idContainer189">
</div>
</div>
<div>
<div class="Basic-Graphics-Frame" id="_idContainer190">
</div>
</div>
<div>
<div id="_idContainer191">
</div>
</div>
</div></body></html>