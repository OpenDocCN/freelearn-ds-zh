<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer102">
<h1 class="chapter-number" id="_idParaDest-127"><a id="_idTextAnchor127"/>4</h1>
<h1 id="_idParaDest-128"><a id="_idTextAnchor128"/>Reading CSV and JSON Files and Solving Problems</h1>
<p>When working with data, we come across several different types of data, such as structured, semi-structured, and non-structured, and some specifics from other systems’ outputs. Yet two widespread file types are ingested, <strong class="bold">comma-separated values</strong> (<strong class="bold">CSV</strong>) and <strong class="bold">JavaScript Object Notation</strong> (<strong class="bold">JSON</strong>). There are many applications for these two files, which are widely used for data ingestion due to <span class="No-Break">their versatility.</span></p>
<p>In this chapter, you will learn more about these file formats and how to ingest them using Python and PySpark, apply the best practices, and solve ingestion and <span class="No-Break">transformation-related problems.</span></p>
<p>In this chapter, we will cover the <span class="No-Break">following recipes:</span></p>
<ul>
<li>Reading a <span class="No-Break">CSV ﬁle</span></li>
<li>Reading a <span class="No-Break">JSON ﬁle</span></li>
<li>Creating a SparkSession <span class="No-Break">for PySpark</span></li>
<li>Using PySpark to read <span class="No-Break">CSV ﬁles</span></li>
<li>Using PySpark to read <span class="No-Break">JSON ﬁles</span></li>
</ul>
<h1 id="_idParaDest-129"><a id="_idTextAnchor129"/>Technical requirements</h1>
<p>You can find the code for this chapter in this <strong class="bold">GitHub</strong> <span class="No-Break">repository: </span><a href="https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook"><span class="No-Break">https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook</span></a><span class="No-Break">.</span></p>
<p>Using <strong class="bold">Jupyter Notebook</strong> is not mandatory but can help you see how the code works interactively. Since we will execute Python and PySpark code, it can help us understand the scripts better. Once you have installed it, you can execute Jupyter using the <span class="No-Break">following command:</span></p>
<pre class="source-code">
$ jupyter notebook</pre>
<p>It is recommended to create a separate folder to store the Python files or notebooks we will create in this chapter; however, feel free to organize them however suits <span class="No-Break">you best.</span></p>
<h1 id="_idParaDest-130"><a id="_idTextAnchor130"/>Reading a CSV ﬁle</h1>
<p>A <strong class="bold">CSV</strong> file is a plain text file where commas<a id="_idIndexMarker292"/> separate each data point, and each line represents<a id="_idIndexMarker293"/> a new record. It is widely used in many<a id="_idIndexMarker294"/> areas, such<a id="_idIndexMarker295"/> as finance, marketing, and sales, to store data. Software such as <strong class="bold">Microsoft Excel</strong> and <strong class="bold">LibreOffice</strong>, and even online solutions such as <strong class="bold">Google Spreadsheets</strong>, provide reading and writing operations<a id="_idIndexMarker296"/> for this file. Visually it resembles a structured table, which greatly enhances the <span class="No-Break">file’s usability.</span></p>
<h2 id="_idParaDest-131"><a id="_idTextAnchor131"/>Getting ready</h2>
<p>You can download<a id="_idIndexMarker297"/> the CSV dataset for this from <strong class="bold">Kaggle</strong>. Use this link<a id="_idIndexMarker298"/> to download the file: <a href="https://www.kaggle.com/datasets/jfreyberg/spotify-chart-data">https://www.kaggle.com/datasets/jfreyberg/spotify-chart-data</a>. We are going to use the same Spotify dataset as in <a href="B19453_02.xhtml#_idTextAnchor064"><span class="No-Break"><em class="italic">Chapter 2</em></span></a><span class="No-Break">.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">Since Kaggle is a dynamic platform, the filename might change occasionally. After downloading it, I named the <span class="No-Break">file </span><span class="No-Break"><strong class="source-inline">spotify_data.csv</strong></span><span class="No-Break">.</span></p>
<p>For this recipe, we will use only Python and Jupyter Notebook to execute the code and create a more <span class="No-Break">friendly visualization.</span></p>
<h2 id="_idParaDest-132"><a id="_idTextAnchor132"/>How to do it…</h2>
<p>Follow these steps to try <span class="No-Break">this recipe:</span></p>
<ol>
<li>We begin by reading the <span class="No-Break">CSV file:</span><pre class="source-code">
import csv
filename = "     path/to/spotify_data.csv"
columns = []
rows = []
with open (filename, 'r', encoding="utf8") as f:
    csvreader = csv.reader(f)
    fields = next(csvreader)
    for row in csvreader:
        rows.append(row)</pre></li>
<li>Then, we print<a id="_idIndexMarker299"/> the column names <span class="No-Break">as follows:</span><pre class="source-code">
print(column for column in columns)</pre></li>
<li>Then, we print the first <span class="No-Break">ten columns:</span><pre class="source-code">
print('First 10 rows:')
for row in rows[:5]:
    for col in row:
        print(col)
    print('\n')</pre></li>
</ol>
<p>This is what the output <span class="No-Break">looks like:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer087">
<img alt="Figure 4.1 – First five rows of the spotify_data.csv file" height="619" src="image/Figure_4.1_B19453.jpg" width="444"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.1 – First five rows of the spotify_data.csv file</p>
<h2 id="_idParaDest-133"><a id="_idTextAnchor133"/>How it works…</h2>
<p>Have a look at the <span class="No-Break">following</span><span class="No-Break"><a id="_idIndexMarker300"/></span><span class="No-Break"> code:</span></p>
<pre class="source-code">
import csv
filename = "spotify_data.csv"
columns = []
rows = []</pre>
<p>In the first step of the <em class="italic">How to do it…</em> section, we imported the built-in library and specified the name of our file. Since it was at the same directory level as our Python script, there was no need to include the full path. Then we declared two lists: one to store our column names (or the first line of our CSV) and the other to store <span class="No-Break">our rows.</span></p>
<p>Then we proceeded with the <strong class="source-inline">with open</strong> statement. Behind the scenes, the <strong class="source-inline">with</strong> statement creates a context manager that simplifies opening and closing <span class="No-Break">file handlers.</span></p>
<p><strong class="source-inline">(filename, 'r')</strong> indicates we want to use <strong class="source-inline">filename</strong> and only read it (<strong class="source-inline">'r'</strong>). After that, we read the file and store the first line in our <strong class="source-inline">columns</strong> list using the <strong class="source-inline">next()</strong> method, which returns the following item from the iterator. For the rest of the records (or rows), we used the <strong class="source-inline">for</strong> iteration to store them in <span class="No-Break">a list.</span></p>
<p>Since both the declared <a id="_idIndexMarker301"/>variables are lists, we can easily read them using the <span class="No-Break"><strong class="source-inline">for</strong></span><span class="No-Break"> iterator.</span></p>
<h2 id="_idParaDest-134"><a id="_idTextAnchor134"/>There’s more…</h2>
<p>For this recipe, we used the built-in CSV library of Python and created a simple structure for handling the columns and rows of our CSV file; however, there is a more straightforward way to do it <span class="No-Break">using pandas.</span></p>
<p>pandas is a Python library that was built to analyze<a id="_idIndexMarker302"/> and manipulate data by converting it into structures called <strong class="bold">DataFrames</strong>. Don’t worry if this is a new concept for you; we will cover it in the following recipes <span class="No-Break">and chapters.</span></p>
<p>Let’s see an example of using pandas<a id="_idIndexMarker303"/> to read a <span class="No-Break">CSV file:</span></p>
<ol>
<li>We need to install pandas. To do this, use the <span class="No-Break">following command:</span><pre class="source-code">
<strong class="bold">$ pip install pandas</strong></pre></li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">Remember to use the <strong class="source-inline">pip</strong> command that is associated with your Python version. For some readers, it might be best to use <strong class="source-inline">pip3</strong>. You can verify the version of <strong class="source-inline">pip</strong> and the associated Python version using the <strong class="source-inline">pip –version</strong> command in <span class="No-Break">the CLI.</span></p>
<ol>
<li value="2">Then, we read the <span class="No-Break">CSV file:</span><pre class="source-code">
<strong class="bold">import pandas as pd</strong>
spotify_df = pd.read_csv('spotify_data.csv')
<strong class="bold">spotify_df.head()</strong></pre></li>
</ol>
<p>You should have the <span class="No-Break">following</span><span class="No-Break"><a id="_idIndexMarker304"/></span><span class="No-Break"> output:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer088">
<img alt="Figure 4.2 – spotify_df DataFrame first five lines" height="238" src="image/Figure_4.2_B19453.jpg" width="776"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.2 – spotify_df DataFrame first five lines</p>
<p class="callout-heading">Note</p>
<p class="callout">Due to its specific rendering capabilities, this <em class="italic">friendly</em> visualization can only be seen when using Jupyter Notebook. The output is entirely different if you execute the <strong class="source-inline">.head()</strong> method on the command line or in <span class="No-Break">your code.</span></p>
<h2 id="_idParaDest-135"><a id="_idTextAnchor135"/>See also</h2>
<p>There are other ways to install<a id="_idIndexMarker305"/> pandas, and you can explore one <span class="No-Break">here: </span><a href="https://pandas.pydata.org/docs/getting_started/install.xhtml"><span class="No-Break">https://pandas.pydata.org/docs/getting_started/install.xhtml</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-136"><a id="_idTextAnchor136"/>Reading a JSON ﬁle</h1>
<p><strong class="bold">JavaScript Object Notation</strong> (<strong class="bold">JSON</strong>) is a semi-structured data format. Some articles<a id="_idIndexMarker306"/> also define JSON as an unstructured<a id="_idIndexMarker307"/> data format, but the truth is this format can be used for <span class="No-Break">multiple purposes.</span></p>
<p>JSON structure uses nested objects and arrays and, due to its flexibility, many applications and APIs use it to export or share data. That is why describing this file format in this chapter <span class="No-Break">is essential.</span></p>
<p>This recipe will explore how to read a JSON file using a built-in Python library and explain how the <span class="No-Break">process works.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">JSON is an alternative to XML files, which are very verbose and require more coding to manipulate <span class="No-Break">their data.</span></p>
<h2 id="_idParaDest-137"><a id="_idTextAnchor137"/>Getting ready</h2>
<p>This recipe is going<a id="_idIndexMarker308"/> to use the GitHub Events JSON data, which can be found in the GitHub repository of this book at <a href="https://github.com/jdorfman/awesome-json-datasets">https://github.com/jdorfman/awesome-json-datasets</a> with other free <span class="No-Break">JSON data.</span></p>
<p>To retrieve the data, click on <strong class="bold">GitHub API</strong> | <strong class="bold">Events</strong>, copy the content from the page, and save it as a <strong class="source-inline">.</strong><span class="No-Break"><strong class="source-inline">json</strong></span><span class="No-Break"> file.</span></p>
<h2 id="_idParaDest-138"><a id="_idTextAnchor138"/>How to do it…</h2>
<p>Follow these steps to complete <span class="No-Break">this recipe:</span></p>
<ol>
<li>Let’s start by reading <span class="No-Break">the file:</span><pre class="source-code">
import json
filename_json = 'github_events.json'
with open (filename_json, 'r') as f:
    github_events = json.loads(f.read())</pre></li>
<li>Now, let’s get the data by making the lines interact with our <span class="No-Break">JSON file:</span><pre class="source-code">
<strong class="bold">id_list = [item['id'] for item in github_events]</strong>
print(id_list)</pre></li>
</ol>
<p>The output looks <span class="No-Break">as follows:</span></p>
<pre class="source-code">
<strong class="bold">['25208138097',</strong>
<strong class="bold"> '25208138110',</strong>
<strong class="bold"> (...)</strong>
<strong class="bold"> '25208138008',</strong>
<strong class="bold"> '25208137998']</strong></pre>
<h2 id="_idParaDest-139"><a id="_idTextAnchor139"/>How it works…</h2>
<p>Have a look at the <span class="No-Break">following code:</span></p>
<pre class="source-code">
import json
filename_json = 'github_events.json'</pre>
<p>As for the CSV file, Python<a id="_idIndexMarker309"/> also has a built-in library for JSON files, and we start our script by importing it. Then, we define a variable to refer to <span class="No-Break">our filename.</span></p>
<p>Have a look at the <span class="No-Break">following code:</span></p>
<pre class="source-code">
with open (filename_json, 'r') as f:
    github_events = json.loads(f.read())</pre>
<p>Using the <strong class="source-inline">open()</strong> function, we open the JSON file. The <strong class="source-inline">json.loads()</strong> statement can’t open JSON files. To do that, we used <strong class="source-inline">f.read()</strong>, which will return the file’s content as it is, which is then passed as an argument to the <span class="No-Break">first statement.</span></p>
<p>However, there is a trick here. Unlike in the CSV file, we don’t have one single line with the names of the columns. Instead, each data record has its own key representing the data. Since a JSON file is very similar to a Python dictionary, we need to iterate over each record to get all the <strong class="source-inline">id</strong> values in <span class="No-Break">the file.</span></p>
<p>To simplify the process a bit, we have created the following one-line <strong class="source-inline">for</strong> loop inside <span class="No-Break">a list:</span></p>
<pre class="source-code">
id_list = [item['id'] for item in github_events]</pre>
<h2 id="_idParaDest-140"><a id="_idTextAnchor140"/>There’s more…</h2>
<p>Even though using Python’s built-in JSON library seemed very simple, manipulating the data better or filtering by one line, in this case, can create<a id="_idIndexMarker310"/> unnecessary complexity. We can use <strong class="bold">pandas</strong> once more to simplify <span class="No-Break">the process.</span></p>
<p>Let’s read JSON with pandas. Check out the <span class="No-Break">following code:</span></p>
<pre class="source-code">
import pandas as pd
github_events = pd.read_json('github_events.json')
github_events.head(3)</pre>
<p>The output looks <span class="No-Break">as follows:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer089">
<img alt="Figure 4.3 – github_events DataFrame first five lines" height="332" src="image/Figure_4.3_B19453.jpg" width="707"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.3 – github_events DataFrame first five lines</p>
<p>Then, let’s get the <span class="No-Break"><strong class="source-inline">id</strong></span><span class="No-Break"> list:</span></p>
<pre class="source-code">
github_events['id']</pre>
<p>The output looks <span class="No-Break">as follows:</span></p>
<pre class="source-code">
0     25208138097
1     25208138110
2     25208138076
(...)
27    25208138012
28    25208138008
29    25208137998
Name: id, dtype: int64</pre>
<p>Like what we did before to read the CSV file, reading a JSON file with pandas was very simple and saved a lot of time and code to get the same information we needed. While in the first example, we needed to iterate over the JSON object list, pandas natively understands it as a DataFrame. Since every column behaves as a one-dimensional array, we can quickly get the values by passing the name of the column (or key) to the <span class="No-Break">DataFrame name.</span></p>
<p>As with any other library, pandas has limitations, such as reading multiple files simultaneously, parallelism, or reading large datasets. Having this in mind can prevent problems and help us use this <span class="No-Break">library</span><span class="No-Break"><a id="_idIndexMarker311"/></span><span class="No-Break"> optimally.</span></p>
<h3>Why DataFrames?</h3>
<p>DataFrames are bi-dimensional and size-mutable tabular structures<a id="_idIndexMarker312"/> and visually resemble a structured table. Due to their versatility, they are widely used in libraries such as pandas (as we <span class="No-Break">saw previously).</span></p>
<p><strong class="bold">PySpark</strong> is no different. <strong class="bold">Spark</strong> uses DataFrames as distributed data collections<a id="_idIndexMarker313"/> and can <em class="italic">parallelize</em> its tasks<a id="_idIndexMarker314"/> to process them through other cores or nodes. We will cover this in more depth in <a href="B19453_07.xhtml#_idTextAnchor227"><span class="No-Break"><em class="italic">Chapter 7</em></span></a><span class="No-Break">.</span></p>
<h2 id="_idParaDest-141"><a id="_idTextAnchor141"/>See also</h2>
<p>To find out more about<a id="_idIndexMarker315"/> the files supported by the pandas library, follow this <span class="No-Break">link: </span><a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/io.xhtml"><span class="No-Break">https://pandas.pydata.org/pandas-docs/stable/user_guide/io.xhtml</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-142"><a id="_idTextAnchor142"/>Creating a SparkSession for PySpark</h1>
<p>Previously introduced in <a href="B19453_01.xhtml#_idTextAnchor022"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>, <strong class="bold">PySpark</strong> is a Spark library that was designed<a id="_idIndexMarker316"/> to work <a id="_idIndexMarker317"/>with Python. PySpark<a id="_idIndexMarker318"/> uses a Python API to write <strong class="bold">Spark</strong> functionalities such as data<a id="_idIndexMarker319"/> manipulation, processing (batch or real-time), and <span class="No-Break">machine learning.</span></p>
<p>However, before ingesting or processing data using PySpark, we must initialize a SparkSession. This recipe will teach us how to create a SparkSession using PySpark and explain <span class="No-Break">its importance.</span></p>
<h2 id="_idParaDest-143"><a id="_idTextAnchor143"/>Getting ready</h2>
<p>We first need to ensure we have the correct PySpark version. We installed PySpark in <a href="B19453_01.xhtml#_idTextAnchor022"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>; however, checking if we are using the correct version is always good. Run the <span class="No-Break">following command:</span></p>
<pre class="source-code">
$ pyspark –version</pre>
<p>You should see the <span class="No-Break">following output:</span></p>
<pre class="source-code">
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.1.2
      /_/
Using Scala version 2.12.10, OpenJDK 64-Bit Server VM, 1.8.0_342
Branch HEAD
Compiled by user centos on 2021-05-24T04:27:48Z
Revision de351e30a90dd988b133b3d00fa6218bfcaba8b8
Url https://github.com/apache/spark
Type --help for more information.</pre>
<p>Next, we choose a code editor that can be any code editor that you want. I will use Jupyter due to the <span class="No-Break">interactive interface.</span></p>
<h2 id="_idParaDest-144"><a id="_idTextAnchor144"/>How to do it…</h2>
<p>Let’s see how to create <span class="No-Break">a SparkSession:</span></p>
<ol>
<li>We first create <strong class="source-inline">SparkSession</strong> <span class="No-Break">as follows:</span><pre class="source-code">
from pyspark.sql import SparkSession
spark = SparkSession.builder \
      .master("local[1]") \
      .appName("DataIngestion") \
      .config("spark.executor.memory", '1g') \
      .config("spark.executor.cores", '3') \
      .config("spark.cores.max", '3') \
      .enableHiveSupport() \
      .getOrCreate()</pre></li>
</ol>
<p>And these<a id="_idIndexMarker320"/> are the <span class="No-Break">warnings</span><span class="No-Break"><a id="_idIndexMarker321"/></span><span class="No-Break"> received:</span></p>
<pre class="source-code">
22/11/14 11:09:55 WARN Utils: Your hostname, DESKTOP-DVUDB98 resolves to a loopback address: 127.0.1.1; using 172.27.100.10 instead (on interface eth0)
22/11/14 11:09:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
22/11/14 11:09:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).</pre>
<p>We are not going to have an output when executing the code. Also, don’t worry about the <strong class="source-inline">WARN</strong> messages; they will not affect <span class="No-Break">our work.</span></p>
<ol>
<li value="2">Then we get the Spark UI. To do this, in your Jupyter cell, type and execute the name of your instantiated <span class="No-Break">as follows:</span><pre class="source-code">
spark</pre></li>
</ol>
<p>You should<a id="_idIndexMarker322"/> see the <span class="No-Break">following</span><span class="No-Break"><a id="_idIndexMarker323"/></span><span class="No-Break"> output:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer090">
<img alt="Figure 4.4 – Output of execution of instance" height="249" src="image/Figure_4.4_B19453.jpg" width="177"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.4 – Output of execution of instance</p>
<ol>
<li value="3">Next, we access SparkUI in the browser. To do this, we click on the <strong class="bold">Spark UI</strong> hyperlink. It will open a new tab in your browser, showing a graph with <strong class="bold">Executors</strong> and <strong class="bold">Jobs</strong>, and other <span class="No-Break">helpful information:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer091">
<img alt="Figure 4.5 – Spark UI home page with an Event Timeline graph" height="488" src="image/Figure_4.5_B19453.jpg" width="1010"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.5 – Spark UI home page with an Event Timeline graph</p>
<p>Since we still haven’t executed any processes, the graph is empty. Don’t worry; we will see it in action in the <span class="No-Break">following recipes.</span></p>
<h2 id="_idParaDest-145"><a id="_idTextAnchor145"/>How it works…</h2>
<p>As we saw<a id="_idIndexMarker324"/> at the beginning<a id="_idIndexMarker325"/> of this chapter, a is a fundamental part<a id="_idIndexMarker326"/> of starting our Spark jobs. It sets all the required configurations for <strong class="bold">Spark’s YARN</strong> (short for <strong class="bold">Yet Another Resource Manager</strong>) to allocate the memory, cores, and paths to write<a id="_idIndexMarker327"/> temporary and final outputs, among <span class="No-Break">other things.</span></p>
<p>With this in mind, let’s visit each step of <span class="No-Break">our code:</span></p>
<pre class="source-code">
 spark = .builder \</pre>
<p>To execute operations such as creating DataFrames, we need to use an instance of . The <strong class="source-inline">spark</strong> variable will be used to access DataFrames and other procedures. Now, take a look <span class="No-Break">at this:</span></p>
<pre class="source-code">
       .master("local[1]") \
      .appName("DataIngestion") \</pre>
<p>The <strong class="source-inline">.master()</strong> method indicates which type of distributed processing we have. Since this is our local machine used only for educational purposes, we defined it as <strong class="source-inline">"local[1]"</strong>, where the integer value needs to be greater than <strong class="source-inline">0</strong>, since it represents the number of partitions. The <strong class="source-inline">.appName()</strong> method defines the name of our <span class="No-Break">application session.</span></p>
<p>After declaring the name of our application, we can set the <strong class="source-inline">.</strong><span class="No-Break"><strong class="source-inline">config()</strong></span><span class="No-Break"> methods:</span></p>
<pre class="source-code">
      .config("spark.executor.memory", '1g') \
      .config("spark.executor.cores", '3') \
      .config("spark.cores.max", '3') \
      .enableHiveSupport() \</pre>
<p>Here, we defined two types of configuration: memory allocation and core allocation. <strong class="source-inline">spark.executor.memory</strong> tells YARN how much memory each executor can allocate to process data; <strong class="source-inline">g</strong> represents the size unit <span class="No-Break">of it.</span></p>
<p><strong class="source-inline">spark.executor.cores</strong> defines the number of executors used by YARN. By default, <strong class="source-inline">1</strong> is used. Next, <strong class="source-inline">spark.cores.max</strong> sets how many cores YARN <span class="No-Break">can scale.</span></p>
<p>Last, <strong class="source-inline">.enableHiveSupport()</strong> enables the support of <span class="No-Break">Hive queries.</span></p>
<p>Finally, let’s look <span class="No-Break">at this:</span></p>
<pre class="source-code">
      .getOrCreate()</pre>
<p><strong class="source-inline">.getOrCreate()</strong> is simple as its name. If there is a session<a id="_idIndexMarker328"/> with this name, it will retrieve<a id="_idIndexMarker329"/> its configurations of it. Otherwise, it will create a <span class="No-Break">new one.</span></p>
<h2 id="_idParaDest-146"><a id="_idTextAnchor146"/>There’s more…</h2>
<p>Looking at the Spark documentation page, you will see that most configurations are not required to start our jobs or ingestions. However, it is essential to remember that we are handling a scalable framework that was created to allocate resources in a single machine or in clusters to process vast amounts of data. With no limit set to a SparkSession, YARN will allocate every resource it needs to process or ingest data, and this can result in server downtime or even freeze your entire <span class="No-Break">local machine.</span></p>
<p>In a real-world scenario, a Kubernetes cluster is typically used to ingest or process shared data with other applications or users who do the same thing as you or your team. Physical memory and computational resources tend to be limited, so it is always an excellent practice to set the configurations, even in small projects that use serverless <span class="No-Break">cloud solutions.</span></p>
<h3>Getting all configurations</h3>
<p>It is also possible to retrieve the current configurations<a id="_idIndexMarker330"/> used for this application session using the <span class="No-Break">following code:</span></p>
<pre class="source-code">
spark.sparkContext.getConf().getAll()</pre>
<p>This is the result <span class="No-Break">we get:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer092">
<img alt="Figure 4.6 – configurations set for the recipe" height="323" src="image/Figure_4.6_B19453.jpg" width="608"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.6 – configurations set for the recipe</p>
<h2 id="_idParaDest-147"><a id="_idTextAnchor147"/>See also</h2>
<ul>
<li>See<a id="_idIndexMarker331"/> all <span class="No-Break">configurations: </span><a href="https://spark.apache.org/docs/latest/configuration.xhtml"><span class="No-Break">https://spark.apache.org/docs/latest/configuration.xhtml</span></a></li>
<li>In early versions of Spark, SparkContext was the starting point for working with data and was replaced by in the recent versions. You can read more about it <span class="No-Break">here: </span><a href="https://sparkbyexamples.com/spark/-vs-sparkcontext/"><span class="No-Break">https://sparkbyexamples.com/spark/-vs-sparkcontext/</span></a><span class="No-Break">.</span></li>
</ul>
<h1 id="_idParaDest-148"><a id="_idTextAnchor148"/>Using PySpark to read CSV ﬁles</h1>
<p>As expected, PySpark provides native support<a id="_idIndexMarker332"/> for reading and writing CSV files. It also allows<a id="_idIndexMarker333"/> data engineers to pass diverse kinds of setups in case the CSV has a different type of delimiter, special encoding, and <span class="No-Break">so on.</span></p>
<p>In this recipe, we are going to cover how to read CSV files using PySpark using the most common configurations, and we will explain why they <span class="No-Break">are needed.</span></p>
<h2 id="_idParaDest-149"><a id="_idTextAnchor149"/>Getting ready</h2>
<p>You can download the CSV datase<a id="_idIndexMarker334"/>t for this recipe from Kaggle: <a href="https://www.kaggle.com/datasets/jfreyberg/spotify-chart-data">https://www.kaggle.com/datasets/jfreyberg/spotify-chart-data</a>. We are going to use the same Spotify dataset as in <a href="B19453_02.xhtml#_idTextAnchor064"><span class="No-Break"><em class="italic">Chapter 2</em></span></a><span class="No-Break">.</span></p>
<p>As in the <em class="italic">Creating a SparkSession for PySpark</em> recipe, make sure PySpark is installed and running with the latest stable version. Also, using Jupyter Notebook <span class="No-Break">is optional.</span></p>
<h2 id="_idParaDest-150"><a id="_idTextAnchor150"/>How to do it…</h2>
<p>Let’s <span class="No-Break">get started:</span></p>
<ol>
<li>We first import and create a <span class="No-Break">SparkSession :</span><pre class="source-code">
from pyspark.sql import
spark = .builder \
      .master("local[1]") \
      .appName("DataIngestion_CSV") \
      .config("spark.executor.memory", '3g') \
      .config("spark.executor.cores", '1') \
      .config("spark.cores.max", '1') \
      .getOrCreate()</pre></li>
<li>Then, we read the <span class="No-Break">CSV file:</span><pre class="source-code">
df = spark.read.option('header',True).csv('spotify_data.csv')</pre></li>
<li>Then, we show <span class="No-Break">the data</span><pre class="source-code">
df.show()</pre></li>
</ol>
<p>This is the result <span class="No-Break">we get:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer093">
<img alt="Figure 4.7 – spotify_data.csv DataFrame vision using Spark" height="465" src="image/Figure_4.7_B19453.jpg" width="615"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.7 – spotify_data.csv DataFrame vision using Spark</p>
<h2 id="_idParaDest-151"><a id="_idTextAnchor151"/>How it works…</h2>
<p>Simple as it is, we need<a id="_idIndexMarker335"/> to understand<a id="_idIndexMarker336"/> how reading a file with Spark works to make sure it always <span class="No-Break">executes properly:</span></p>
<pre class="source-code">
df = spark.read.option('header',True).csv('spotify_data.csv')</pre>
<p>We attributed a variable to our code statement. This is a good practice when initializing any file reading because it allows us to control the version of the file and, if a change needs to be made, we attribute it to another variable. The name of the variable is also intentional since we are creating our <span class="No-Break">first DataFrame.</span></p>
<p>Using the <strong class="source-inline">.option()</strong> method allows us to tell PySpark which type of configuration we want to pass. In this case, we set <strong class="source-inline">header</strong> to <strong class="source-inline">True</strong>, which makes PySpark set the first row of the CSV file as the column names. If we didn’t pass the required<a id="_idIndexMarker337"/> configuration, the DataFrame<a id="_idIndexMarker338"/> would look <span class="No-Break">like this:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer094">
<img alt="Figure 4.8 – spotify_data.csv DataFrame without column names" height="463" src="image/Figure_4.8_B19453.jpg" width="617"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.8 – spotify_data.csv DataFrame without column names</p>
<h2 id="_idParaDest-152"><a id="_idTextAnchor152"/>There’s more…</h2>
<p>The content of the files can differ, but some setups are welcome when handling CSV files in PySpark. Here, please note that we are going to change the method <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">.options()</strong></span><span class="No-Break">:</span></p>
<pre class="source-code">
df = spark.read.options(header= 'True',
                       sep=',',
                       inferSchema='True') \
                .csv('spotify_data.csv')</pre>
<p><strong class="source-inline">header</strong>, <strong class="source-inline">sep</strong>, and <strong class="source-inline">inferSchema</strong> are the most commonly<a id="_idIndexMarker339"/> used setups when reading a CSV file. Although CSV stands for <strong class="bold">comma-separated values</strong>, it is not hard to find a system or application that exports this file with another type of separator (such as a pipe or a semicolon), and therefore it is useful to have <strong class="source-inline">sep</strong> (which stands for <span class="No-Break">separator) declared.</span></p>
<p>Let’s see an example of an error when reading a CSV that uses a pipe to separate the strings and passes the <span class="No-Break">wrong separator:</span></p>
<pre class="source-code">
df = spark.read.options(header= 'True',
                       sep=',',
                       inferSchema='True') \
                .csv('spotify_data_pipe.csv')</pre>
<p>This is how the <span class="No-Break">output looks:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer095">
<img alt="Figure 4.9 – CSV DataFrame reading without a proper sep definition" height="464" src="image/Figure_4.9_B19453.jpg" width="378"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.9 – CSV DataFrame reading without a proper sep definition</p>
<p>As you can see, it creates only one column with all the information. But if we pass <strong class="source-inline">sep='|'</strong>, it will <span class="No-Break">return correctly:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer096">
<img alt="Figure 4.10 – CSV DataFrame with the sep definition set to pipe" height="471" src="image/Figure_4.10_B19453.jpg" width="616"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.10 – CSV DataFrame with the sep definition set to pipe</p>
<h3>Other common .options() configurations</h3>
<p>There are other complex <a id="_idIndexMarker340"/>situations that, if not corrected during ingestion, can result in some problems with the other ETL steps. Here, I am using the <strong class="source-inline">listing.csv</strong> dataset, which can be found here <span class="No-Break">at </span><a href="http://data.insideairbnb.com/the-netherlands/north-holland/amsterdam/2022-03-08/visualisations/listings.csv"><span class="No-Break">http://data.insideairbnb.com/the-netherlands/north-holland/amsterdam/2022-03-08/visualisations/listings.csv</span></a><span class="No-Break">:</span></p>
<ol>
<li>We first read our <span class="No-Break">common configurations:</span><pre class="source-code">
df_broken = spark.read.options(header= 'True', sep=',',
                       inferSchema='True') \
                .csv('listings.csv')
df_broken.show()</pre></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer097">
<img alt="Figure 4.11 – listing.csv DataFrame" height="487" src="image/Figure_4.11_B19453.jpg" width="1080"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.11 – listing.csv DataFrame</p>
<p>At first glance, all is normal. However, what happens<a id="_idIndexMarker341"/> if I try to execute a simple group by <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">room_type</strong></span><span class="No-Break">?</span></p>
<pre class="source-code">
group = df_1.groupBy("room_type").count()
group.show()</pre>
<p>This is the output <span class="No-Break">we get:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer098">
<img alt="Figure 4.12 – Grouping df_broken by room_type" height="295" src="image/Figure_4.12_B19453.jpg" width="201"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.12 – Grouping df_broken by room_type</p>
<p>Ignoring <strong class="source-inline">group by</strong> for now, this happens because the file has plenty of escaped quotes and <span class="No-Break">line breaks.</span></p>
<ol>
<li value="2">Now, let’s set the <span class="No-Break">correct </span><span class="No-Break"><strong class="source-inline">.options()</strong></span><span class="No-Break">:</span><pre class="source-code">
df_1 = spark.read.options(header=True, sep=',',
                          multiLine=True, escape='"') \
                .csv('listings.csv')
group = df_1.groupBy("room_type").count()
group.show()</pre></li>
</ol>
<p>This is <span class="No-Break">the</span><span class="No-Break"><a id="_idIndexMarker342"/></span><span class="No-Break"> result:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer099">
<img alt="Figure 4.13 – Grouping by room_type using the right options() settings" height="141" src="image/Figure_4.13_B19453.jpg" width="193"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.13 – Grouping by room_type using the right options() settings</p>
<h2 id="_idParaDest-153"><a id="_idTextAnchor153"/>See also</h2>
<p>You can<a id="_idIndexMarker343"/> see more PySpark <strong class="source-inline">.options()</strong> in the official <span class="No-Break">documentation: </span><a href="https://spark.apache.org/docs/latest/sql-data-sources-csv.xhtml"><span class="No-Break">https://spark.apache.org/docs/latest/sql-data-sources-csv.xhtml</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-154"><a id="_idTextAnchor154"/>Using PySpark to read JSON ﬁles</h1>
<p>In the <em class="italic">Reading a JSON file</em> recipe, we saw that JSON files<a id="_idIndexMarker344"/> are widely used to transport<a id="_idIndexMarker345"/> and share data between applications, and we saw how to read a JSON file using simple <span class="No-Break">Python code.</span></p>
<p>However, with the increase in data size and sharing, using only Python to process a high volume of data can lead to performance or resilience issues. That’s why, for this type of scenario, it is highly recommended to use PySpark to read and process JSON files. As you might expect, PySpark comes with a straightforward <span class="No-Break">reading solution.</span></p>
<p>In this recipe, we will cover how to read a JSON file with PySpark, the common associated issues, and how to <span class="No-Break">solve them.</span></p>
<h2 id="_idParaDest-155"><a id="_idTextAnchor155"/>Getting ready</h2>
<p>As in the previous recipe, <em class="italic">Reading a JSON file</em>, we are going<a id="_idIndexMarker346"/> to use the <strong class="source-inline">GitHub Events</strong> JSON file. Also, the use of Jupyter<a id="_idIndexMarker347"/> Notebook <span class="No-Break">is optional.</span></p>
<h2 id="_idParaDest-156"><a id="_idTextAnchor156"/>How to do it…</h2>
<p>Here are the steps for <span class="No-Break">this recipe:</span></p>
<ol>
<li>We first create <span class="No-Break">the SparkSession:</span><pre class="source-code">
spark = .builder \
      .master("local[1]") \
      .appName("DataIngestion_JSON") \
      .config("spark.executor.memory", '3g') \
      .config("spark.executor.cores", '1') \
      .config("spark.cores.max", '1') \
      .getOrCreate()</pre></li>
<li>Then, we read the <span class="No-Break">JSON file:</span><pre class="source-code">
df_json = spark.read.option("multiline", "true") \
                    .json('github_events.json')</pre></li>
<li>Then, we show <span class="No-Break">the data:</span><pre class="source-code">
df_json.show()</pre></li>
</ol>
<p>This is how the <span class="No-Break">output appears:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer100">
<img alt="Figure 4.14 – df_json DataFrame" height="565" src="image/Figure_4.14_B19453.jpg" width="1112"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.14 – df_json DataFrame</p>
<h2 id="_idParaDest-157"><a id="_idTextAnchor157"/>How it works…</h2>
<p>Similar to CSV files, reading a JSON file<a id="_idIndexMarker348"/> using PySpark is very simple, requiring<a id="_idIndexMarker349"/> only one line of code. Like pandas, it ignores the brackets in the file and creates a table-structured DataFrame, even though we are handling a semi-structured data file. However, the great magic is <span class="No-Break">in</span><span class="No-Break"><strong class="source-inline">.option("multiline", "true")</strong></span><span class="No-Break">.</span></p>
<p>If you remember our JSON structure for this file, it is something <span class="No-Break">like this:</span></p>
<pre class="source-code">
[
  {
    "id": "25208138097",
    "type": "WatchEvent",
    "actor": {...},
    "repo": {...},
    "payload": {
      "action": "started"
    },
    "public": true,
    "created_at": "2022-11-13T22:52:04Z",
    "org": {...}
  },
  {
    "id": "25208138110",
    "type": "IssueCommentEvent",
    "actor": {...},
    "repo": {...},
  },...</pre>
<p>It is a multi-lined JSON since<a id="_idIndexMarker350"/> it has objects inside objects. The <strong class="source-inline">.option()</strong> setup passed<a id="_idIndexMarker351"/> when reading the file guarantees PySpark will read it as it should, and if we don’t pass this argument, an error like this <span class="No-Break">will appear:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer101">
<img alt="Figure 4.15 – This error is shown when .options() is not well defined for a multi-lined JSON file" height="193" src="image/Figure_4.15_B19453.jpg" width="684"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.15 – This error is shown when .options() is not well defined for a multi-lined JSON file</p>
<p>PySpark understands it as a <span class="No-Break">corrupted file.</span></p>
<h2 id="_idParaDest-158"><a id="_idTextAnchor158"/>There’s more…</h2>
<p>A very widely used configuration for reading JSON files is <strong class="source-inline">dropFieldIfAllNull</strong>. When set to <strong class="source-inline">true</strong>, if there is an empty array, it will drop it from <span class="No-Break">the schema.</span></p>
<p>Unstructured and semi-structured data are valuable due to their elasticity. Sometimes, applications can change their output and some fields become deprecated. To avoid changing the ingest script (especially if these changes can be frequent), <strong class="source-inline">dropFieldIfAllNull</strong> removes them from <span class="No-Break">the DataFrame.</span></p>
<h2 id="_idParaDest-159"><a id="_idTextAnchor159"/>See also</h2>
<ul>
<li>To find<a id="_idIndexMarker352"/> out more about PySpark <strong class="source-inline">.options()</strong>, refer to the official <span class="No-Break">documentation: </span><a href="https://spark.apache.org/docs/latest/sql-data-sources-json.xhtml"><span class="No-Break">https://spark.apache.org/docs/latest/sql-data-sources-json.xhtml</span></a></li>
</ul>
<h1 id="_idParaDest-160"><a id="_idTextAnchor160"/>Further reading</h1>
<ul>
<li><a href="https://docs.fileformat.com/spreadsheet/csv/"><span class="No-Break">https://docs.fileformat.com/spreadsheet/csv/</span></a></li>
<li><a href="https://codefather.tech/blog/python-with-open/"><span class="No-Break">https://codefather.tech/blog/python-with-open/</span></a></li>
<li><a href="https://www.programiz.com/python-programming/methods/built-in/next"><span class="No-Break">https://www.programiz.com/python-programming/methods/built-in/next</span></a></li>
<li><a href="https://spark.apache.org/docs/latest/sql-data-sources-csv.xhtml"><span class="No-Break">https://spark.apache.org/docs/latest/sql-data-sources-csv.xhtml</span></a></li>
<li><a href="https://sparkbyexamples.com/pyspark/pyspark-read-json-file-into-dataframe/"><span class="No-Break">https://sparkbyexamples.com/pyspark/pyspark-read-json-file-into-dataframe/</span></a></li>
</ul>
</div>
</div></body></html>