<html><head></head><body>
<div><div><h1 class="chapter-number" id="_idParaDest-127"><a id="_idTextAnchor127"/>4</h1>
<h1 id="_idParaDest-128"><a id="_idTextAnchor128"/>Reading CSV and JSON Files and Solving Problems</h1>
<p>When working with data, we come across several different types of data, such as structured, semi-structured, and non-structured, and some specifics from other systems’ outputs. Yet two widespread file types are ingested, <strong class="bold">comma-separated values</strong> (<strong class="bold">CSV</strong>) and <strong class="bold">JavaScript Object Notation</strong> (<strong class="bold">JSON</strong>). There are many applications for these two files, which are widely used for data ingestion due to their versatility.</p>
<p>In this chapter, you will learn more about these file formats and how to ingest them using Python and PySpark, apply the best practices, and solve ingestion and transformation-related problems.</p>
<p>In this chapter, we will cover the following recipes:</p>
<ul>
<li>Reading a CSV ﬁle</li>
<li>Reading a JSON ﬁle</li>
<li>Creating a SparkSession for PySpark</li>
<li>Using PySpark to read CSV ﬁles</li>
<li>Using PySpark to read JSON ﬁles</li>
</ul>
<h1 id="_idParaDest-129"><a id="_idTextAnchor129"/>Technical requirements</h1>
<p>You can find the code for this chapter in this <strong class="bold">GitHub</strong> repository: <a href="https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook">https://github.com/PacktPublishing/Data-Ingestion-with-Python-Cookbook</a>.</p>
<p>Using <strong class="bold">Jupyter Notebook</strong> is not mandatory but can help you see how the code works interactively. Since we will execute Python and PySpark code, it can help us understand the scripts better. Once you have installed it, you can execute Jupyter using the following command:</p>
<pre class="source-code">
$ jupyter notebook</pre>
<p>It is recommended to create a separate folder to store the Python files or notebooks we will create in this chapter; however, feel free to organize them however suits you best.</p>
<h1 id="_idParaDest-130"><a id="_idTextAnchor130"/>Reading a CSV ﬁle</h1>
<p>A <strong class="bold">CSV</strong> file is a plain text file where commas<a id="_idIndexMarker292"/> separate each data point, and each line represents<a id="_idIndexMarker293"/> a new record. It is widely used in many<a id="_idIndexMarker294"/> areas, such<a id="_idIndexMarker295"/> as finance, marketing, and sales, to store data. Software such as <strong class="bold">Microsoft Excel</strong> and <strong class="bold">LibreOffice</strong>, and even online solutions such as <strong class="bold">Google Spreadsheets</strong>, provide reading and writing operations<a id="_idIndexMarker296"/> for this file. Visually it resembles a structured table, which greatly enhances the file’s usability.</p>
<h2 id="_idParaDest-131"><a id="_idTextAnchor131"/>Getting ready</h2>
<p>You can download<a id="_idIndexMarker297"/> the CSV dataset for this from <strong class="bold">Kaggle</strong>. Use this link<a id="_idIndexMarker298"/> to download the file: <a href="https://www.kaggle.com/datasets/jfreyberg/spotify-chart-data">https://www.kaggle.com/datasets/jfreyberg/spotify-chart-data</a>. We are going to use the same Spotify dataset as in <a href="B19453_02.xhtml#_idTextAnchor064"><em class="italic">Chapter 2</em></a>.</p>
<p class="callout-heading">Note</p>
<p class="callout">Since Kaggle is a dynamic platform, the filename might change occasionally. After downloading it, I named the file <code>spotify_data.csv</code>.</p>
<p>For this recipe, we will use only Python and Jupyter Notebook to execute the code and create a more friendly visualization.</p>
<h2 id="_idParaDest-132"><a id="_idTextAnchor132"/>How to do it…</h2>
<p>Follow these steps to try this recipe:</p>
<ol>
<li>We begin by reading the CSV file:<pre class="source-code">
import csv
filename = "     path/to/spotify_data.csv"
columns = []
rows = []
with open (filename, 'r', encoding="utf8") as f:
    csvreader = csv.reader(f)
    fields = next(csvreader)
    for row in csvreader:
        rows.append(row)</pre></li>
<li>Then, we print<a id="_idIndexMarker299"/> the column names as follows:<pre class="source-code">
print(column for column in columns)</pre></li>
<li>Then, we print the first ten columns:<pre class="source-code">
print('First 10 rows:')
for row in rows[:5]:
    for col in row:
        print(col)
    print('\n')</pre></li>
</ol>
<p>This is what the output looks like:</p>
<div><div><img alt="Figure 4.1 – First five rows of the spotify_data.csv file" height="619" src="img/Figure_4.1_B19453.jpg" width="444"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.1 – First five rows of the spotify_data.csv file</p>
<h2 id="_idParaDest-133"><a id="_idTextAnchor133"/>How it works…</h2>
<p>Have a look at the following<a id="_idIndexMarker300"/> code:</p>
<pre class="source-code">
import csv
filename = "spotify_data.csv"
columns = []
rows = []</pre>
<p>In the first step of the <em class="italic">How to do it…</em> section, we imported the built-in library and specified the name of our file. Since it was at the same directory level as our Python script, there was no need to include the full path. Then we declared two lists: one to store our column names (or the first line of our CSV) and the other to store our rows.</p>
<p>Then we proceeded with the <code>with open</code> statement. Behind the scenes, the <code>with</code> statement creates a context manager that simplifies opening and closing file handlers.</p>
<p><code>(filename, 'r')</code> indicates we want to use <code>filename</code> and only read it (<code>'r'</code>). After that, we read the file and store the first line in our <code>columns</code> list using the <code>next()</code> method, which returns the following item from the iterator. For the rest of the records (or rows), we used the <code>for</code> iteration to store them in a list.</p>
<p>Since both the declared <a id="_idIndexMarker301"/>variables are lists, we can easily read them using the <code>for</code> iterator.</p>
<h2 id="_idParaDest-134"><a id="_idTextAnchor134"/>There’s more…</h2>
<p>For this recipe, we used the built-in CSV library of Python and created a simple structure for handling the columns and rows of our CSV file; however, there is a more straightforward way to do it using pandas.</p>
<p>pandas is a Python library that was built to analyze<a id="_idIndexMarker302"/> and manipulate data by converting it into structures called <strong class="bold">DataFrames</strong>. Don’t worry if this is a new concept for you; we will cover it in the following recipes and chapters.</p>
<p>Let’s see an example of using pandas<a id="_idIndexMarker303"/> to read a CSV file:</p>
<ol>
<li>We need to install pandas. To do this, use the following command:<pre class="source-code">
<strong class="bold">$ pip install pandas</strong></pre></li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">Remember to use the <code>pip</code> command that is associated with your Python version. For some readers, it might be best to use <code>pip3</code>. You can verify the version of <code>pip</code> and the associated Python version using the <code>pip –version</code> command in the CLI.</p>
<ol>
<li value="2">Then, we read the CSV file:<pre class="source-code">
<strong class="bold">import pandas as pd</strong>
spotify_df = pd.read_csv('spotify_data.csv')
<strong class="bold">spotify_df.head()</strong></pre></li>
</ol>
<p>You should have the following<a id="_idIndexMarker304"/> output:</p>
<div><div><img alt="Figure 4.2 – spotify_df DataFrame first five lines" height="238" src="img/Figure_4.2_B19453.jpg" width="776"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.2 – spotify_df DataFrame first five lines</p>
<p class="callout-heading">Note</p>
<p class="callout">Due to its specific rendering capabilities, this <em class="italic">friendly</em> visualization can only be seen when using Jupyter Notebook. The output is entirely different if you execute the <code>.head()</code> method on the command line or in your code.</p>
<h2 id="_idParaDest-135"><a id="_idTextAnchor135"/>See also</h2>
<p>There are other ways to install<a id="_idIndexMarker305"/> pandas, and you can explore one here: <a href="https://pandas.pydata.org/docs/getting_started/install.xhtml">https://pandas.pydata.org/docs/getting_started/install.xhtml</a>.</p>
<h1 id="_idParaDest-136"><a id="_idTextAnchor136"/>Reading a JSON ﬁle</h1>
<p><strong class="bold">JavaScript Object Notation</strong> (<strong class="bold">JSON</strong>) is a semi-structured data format. Some articles<a id="_idIndexMarker306"/> also define JSON as an unstructured<a id="_idIndexMarker307"/> data format, but the truth is this format can be used for multiple purposes.</p>
<p>JSON structure uses nested objects and arrays and, due to its flexibility, many applications and APIs use it to export or share data. That is why describing this file format in this chapter is essential.</p>
<p>This recipe will explore how to read a JSON file using a built-in Python library and explain how the process works.</p>
<p class="callout-heading">Note</p>
<p class="callout">JSON is an alternative to XML files, which are very verbose and require more coding to manipulate their data.</p>
<h2 id="_idParaDest-137"><a id="_idTextAnchor137"/>Getting ready</h2>
<p>This recipe is going<a id="_idIndexMarker308"/> to use the GitHub Events JSON data, which can be found in the GitHub repository of this book at <a href="https://github.com/jdorfman/awesome-json-datasets">https://github.com/jdorfman/awesome-json-datasets</a> with other free JSON data.</p>
<p>To retrieve the data, click on <code>.</code><code>json</code> file.</p>
<h2 id="_idParaDest-138"><a id="_idTextAnchor138"/>How to do it…</h2>
<p>Follow these steps to complete this recipe:</p>
<ol>
<li>Let’s start by reading the file:<pre class="source-code">
import json
filename_json = 'github_events.json'
with open (filename_json, 'r') as f:
    github_events = json.loads(f.read())</pre></li>
<li>Now, let’s get the data by making the lines interact with our JSON file:<pre class="source-code">
<strong class="bold">id_list = [item['id'] for item in github_events]</strong>
print(id_list)</pre></li>
</ol>
<p>The output looks as follows:</p>
<pre class="source-code">
<strong class="bold">['25208138097',</strong>
<strong class="bold"> '25208138110',</strong>
<strong class="bold"> (...)</strong>
<strong class="bold"> '25208138008',</strong>
<strong class="bold"> '25208137998']</strong></pre>
<h2 id="_idParaDest-139"><a id="_idTextAnchor139"/>How it works…</h2>
<p>Have a look at the following code:</p>
<pre class="source-code">
import json
filename_json = 'github_events.json'</pre>
<p>As for the CSV file, Python<a id="_idIndexMarker309"/> also has a built-in library for JSON files, and we start our script by importing it. Then, we define a variable to refer to our filename.</p>
<p>Have a look at the following code:</p>
<pre class="source-code">
with open (filename_json, 'r') as f:
    github_events = json.loads(f.read())</pre>
<p>Using the <code>open()</code> function, we open the JSON file. The <code>json.loads()</code> statement can’t open JSON files. To do that, we used <code>f.read()</code>, which will return the file’s content as it is, which is then passed as an argument to the first statement.</p>
<p>However, there is a trick here. Unlike in the CSV file, we don’t have one single line with the names of the columns. Instead, each data record has its own key representing the data. Since a JSON file is very similar to a Python dictionary, we need to iterate over each record to get all the <code>id</code> values in the file.</p>
<p>To simplify the process a bit, we have created the following one-line <code>for</code> loop inside a list:</p>
<pre class="source-code">
id_list = [item['id'] for item in github_events]</pre>
<h2 id="_idParaDest-140"><a id="_idTextAnchor140"/>There’s more…</h2>
<p>Even though using Python’s built-in JSON library seemed very simple, manipulating the data better or filtering by one line, in this case, can create<a id="_idIndexMarker310"/> unnecessary complexity. We can use <strong class="bold">pandas</strong> once more to simplify the process.</p>
<p>Let’s read JSON with pandas. Check out the following code:</p>
<pre class="source-code">
import pandas as pd
github_events = pd.read_json('github_events.json')
github_events.head(3)</pre>
<p>The output looks as follows:</p>
<div><div><img alt="Figure 4.3 – github_events DataFrame first five lines" height="332" src="img/Figure_4.3_B19453.jpg" width="707"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.3 – github_events DataFrame first five lines</p>
<p>Then, let’s get the <code>id</code> list:</p>
<pre class="source-code">
github_events['id']</pre>
<p>The output looks as follows:</p>
<pre class="source-code">
0     25208138097
1     25208138110
2     25208138076
(...)
27    25208138012
28    25208138008
29    25208137998
Name: id, dtype: int64</pre>
<p>Like what we did before to read the CSV file, reading a JSON file with pandas was very simple and saved a lot of time and code to get the same information we needed. While in the first example, we needed to iterate over the JSON object list, pandas natively understands it as a DataFrame. Since every column behaves as a one-dimensional array, we can quickly get the values by passing the name of the column (or key) to the DataFrame name.</p>
<p>As with any other library, pandas has limitations, such as reading multiple files simultaneously, parallelism, or reading large datasets. Having this in mind can prevent problems and help us use this library<a id="_idIndexMarker311"/> optimally.</p>
<h3>Why DataFrames?</h3>
<p>DataFrames are bi-dimensional and size-mutable tabular structures<a id="_idIndexMarker312"/> and visually resemble a structured table. Due to their versatility, they are widely used in libraries such as pandas (as we saw previously).</p>
<p><strong class="bold">PySpark</strong> is no different. <strong class="bold">Spark</strong> uses DataFrames as distributed data collections<a id="_idIndexMarker313"/> and can <em class="italic">parallelize</em> its tasks<a id="_idIndexMarker314"/> to process them through other cores or nodes. We will cover this in more depth in <a href="B19453_07.xhtml#_idTextAnchor227"><em class="italic">Chapter 7</em></a>.</p>
<h2 id="_idParaDest-141"><a id="_idTextAnchor141"/>See also</h2>
<p>To find out more about<a id="_idIndexMarker315"/> the files supported by the pandas library, follow this link: <a href="https://pandas.pydata.org/pandas-docs/stable/user_guide/io.xhtml">https://pandas.pydata.org/pandas-docs/stable/user_guide/io.xhtml</a>.</p>
<h1 id="_idParaDest-142"><a id="_idTextAnchor142"/>Creating a SparkSession for PySpark</h1>
<p>Previously introduced in <a href="B19453_01.xhtml#_idTextAnchor022"><em class="italic">Chapter 1</em></a>, <strong class="bold">PySpark</strong> is a Spark library that was designed<a id="_idIndexMarker316"/> to work <a id="_idIndexMarker317"/>with Python. PySpark<a id="_idIndexMarker318"/> uses a Python API to write <strong class="bold">Spark</strong> functionalities such as data<a id="_idIndexMarker319"/> manipulation, processing (batch or real-time), and machine learning.</p>
<p>However, before ingesting or processing data using PySpark, we must initialize a SparkSession. This recipe will teach us how to create a SparkSession using PySpark and explain its importance.</p>
<h2 id="_idParaDest-143"><a id="_idTextAnchor143"/>Getting ready</h2>
<p>We first need to ensure we have the correct PySpark version. We installed PySpark in <a href="B19453_01.xhtml#_idTextAnchor022"><em class="italic">Chapter 1</em></a>; however, checking if we are using the correct version is always good. Run the following command:</p>
<pre class="source-code">
$ pyspark –version</pre>
<p>You should see the following output:</p>
<pre class="source-code">
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.1.2
      /_/
Using Scala version 2.12.10, OpenJDK 64-Bit Server VM, 1.8.0_342
Branch HEAD
Compiled by user centos on 2021-05-24T04:27:48Z
Revision de351e30a90dd988b133b3d00fa6218bfcaba8b8
Url https://github.com/apache/spark
Type --help for more information.</pre>
<p>Next, we choose a code editor that can be any code editor that you want. I will use Jupyter due to the interactive interface.</p>
<h2 id="_idParaDest-144"><a id="_idTextAnchor144"/>How to do it…</h2>
<p>Let’s see how to create a SparkSession:</p>
<ol>
<li>We first create <code>SparkSession</code> as follows:<pre class="source-code">
from pyspark.sql import SparkSession
spark = SparkSession.builder \
      .master("local[1]") \
      .appName("DataIngestion") \
      .config("spark.executor.memory", '1g') \
      .config("spark.executor.cores", '3') \
      .config("spark.cores.max", '3') \
      .enableHiveSupport() \
      .getOrCreate()</pre></li>
</ol>
<p>And these<a id="_idIndexMarker320"/> are the warnings<a id="_idIndexMarker321"/> received:</p>
<pre class="source-code">
22/11/14 11:09:55 WARN Utils: Your hostname, DESKTOP-DVUDB98 resolves to a loopback address: 127.0.1.1; using 172.27.100.10 instead (on interface eth0)
22/11/14 11:09:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
22/11/14 11:09:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).</pre>
<p>We are not going to have an output when executing the code. Also, don’t worry about the <code>WARN</code> messages; they will not affect our work.</p>
<ol>
<li value="2">Then we get the Spark UI. To do this, in your Jupyter cell, type and execute the name of your instantiated as follows:<pre class="source-code">
spark</pre></li>
</ol>
<p>You should<a id="_idIndexMarker322"/> see the following<a id="_idIndexMarker323"/> output:</p>
<div><div><img alt="Figure 4.4 – Output of execution of instance" height="249" src="img/Figure_4.4_B19453.jpg" width="177"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.4 – Output of execution of instance</p>
<ol>
<li value="3">Next, we access SparkUI in the browser. To do this, we click on the <strong class="bold">Spark UI</strong> hyperlink. It will open a new tab in your browser, showing a graph with <strong class="bold">Executors</strong> and <strong class="bold">Jobs</strong>, and other helpful information:</li>
</ol>
<div><div><img alt="Figure 4.5 – Spark UI home page with an Event Timeline graph" height="488" src="img/Figure_4.5_B19453.jpg" width="1010"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.5 – Spark UI home page with an Event Timeline graph</p>
<p>Since we still haven’t executed any processes, the graph is empty. Don’t worry; we will see it in action in the following recipes.</p>
<h2 id="_idParaDest-145"><a id="_idTextAnchor145"/>How it works…</h2>
<p>As we saw<a id="_idIndexMarker324"/> at the beginning<a id="_idIndexMarker325"/> of this chapter, a is a fundamental part<a id="_idIndexMarker326"/> of starting our Spark jobs. It sets all the required configurations for <strong class="bold">Spark’s YARN</strong> (short for <strong class="bold">Yet Another Resource Manager</strong>) to allocate the memory, cores, and paths to write<a id="_idIndexMarker327"/> temporary and final outputs, among other things.</p>
<p>With this in mind, let’s visit each step of our code:</p>
<pre class="source-code">
 spark = .builder \</pre>
<p>To execute operations such as creating DataFrames, we need to use an instance of . The <code>spark</code> variable will be used to access DataFrames and other procedures. Now, take a look at this:</p>
<pre class="source-code">
       .master("local[1]") \
      .appName("DataIngestion") \</pre>
<p>The <code>.master()</code> method indicates which type of distributed processing we have. Since this is our local machine used only for educational purposes, we defined it as <code>"local[1]"</code>, where the integer value needs to be greater than <code>0</code>, since it represents the number of partitions. The <code>.appName()</code> method defines the name of our application session.</p>
<p>After declaring the name of our application, we can set the <code>.</code><code>config()</code> methods:</p>
<pre class="source-code">
      .config("spark.executor.memory", '1g') \
      .config("spark.executor.cores", '3') \
      .config("spark.cores.max", '3') \
      .enableHiveSupport() \</pre>
<p>Here, we defined two types of configuration: memory allocation and core allocation. <code>spark.executor.memory</code> tells YARN how much memory each executor can allocate to process data; <code>g</code> represents the size unit of it.</p>
<p><code>spark.executor.cores</code> defines the number of executors used by YARN. By default, <code>1</code> is used. Next, <code>spark.cores.max</code> sets how many cores YARN can scale.</p>
<p>Last, <code>.enableHiveSupport()</code> enables the support of Hive queries.</p>
<p>Finally, let’s look at this:</p>
<pre class="source-code">
      .getOrCreate()</pre>
<p><code>.getOrCreate()</code> is simple as its name. If there is a session<a id="_idIndexMarker328"/> with this name, it will retrieve<a id="_idIndexMarker329"/> its configurations of it. Otherwise, it will create a new one.</p>
<h2 id="_idParaDest-146"><a id="_idTextAnchor146"/>There’s more…</h2>
<p>Looking at the Spark documentation page, you will see that most configurations are not required to start our jobs or ingestions. However, it is essential to remember that we are handling a scalable framework that was created to allocate resources in a single machine or in clusters to process vast amounts of data. With no limit set to a SparkSession, YARN will allocate every resource it needs to process or ingest data, and this can result in server downtime or even freeze your entire local machine.</p>
<p>In a real-world scenario, a Kubernetes cluster is typically used to ingest or process shared data with other applications or users who do the same thing as you or your team. Physical memory and computational resources tend to be limited, so it is always an excellent practice to set the configurations, even in small projects that use serverless cloud solutions.</p>
<h3>Getting all configurations</h3>
<p>It is also possible to retrieve the current configurations<a id="_idIndexMarker330"/> used for this application session using the following code:</p>
<pre class="source-code">
spark.sparkContext.getConf().getAll()</pre>
<p>This is the result we get:</p>
<div><div><img alt="Figure 4.6 – configurations set for the recipe" height="323" src="img/Figure_4.6_B19453.jpg" width="608"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.6 – configurations set for the recipe</p>
<h2 id="_idParaDest-147"><a id="_idTextAnchor147"/>See also</h2>
<ul>
<li>See<a id="_idIndexMarker331"/> all configurations: <a href="https://spark.apache.org/docs/latest/configuration.xhtml">https://spark.apache.org/docs/latest/configuration.xhtml</a></li>
<li>In early versions of Spark, SparkContext was the starting point for working with data and was replaced by in the recent versions. You can read more about it here: <a href="https://sparkbyexamples.com/spark/-vs-sparkcontext/">https://sparkbyexamples.com/spark/-vs-sparkcontext/</a>.</li>
</ul>
<h1 id="_idParaDest-148"><a id="_idTextAnchor148"/>Using PySpark to read CSV ﬁles</h1>
<p>As expected, PySpark provides native support<a id="_idIndexMarker332"/> for reading and writing CSV files. It also allows<a id="_idIndexMarker333"/> data engineers to pass diverse kinds of setups in case the CSV has a different type of delimiter, special encoding, and so on.</p>
<p>In this recipe, we are going to cover how to read CSV files using PySpark using the most common configurations, and we will explain why they are needed.</p>
<h2 id="_idParaDest-149"><a id="_idTextAnchor149"/>Getting ready</h2>
<p>You can download the CSV datase<a id="_idIndexMarker334"/>t for this recipe from Kaggle: <a href="https://www.kaggle.com/datasets/jfreyberg/spotify-chart-data">https://www.kaggle.com/datasets/jfreyberg/spotify-chart-data</a>. We are going to use the same Spotify dataset as in <a href="B19453_02.xhtml#_idTextAnchor064"><em class="italic">Chapter 2</em></a>.</p>
<p>As in the <em class="italic">Creating a SparkSession for PySpark</em> recipe, make sure PySpark is installed and running with the latest stable version. Also, using Jupyter Notebook is optional.</p>
<h2 id="_idParaDest-150"><a id="_idTextAnchor150"/>How to do it…</h2>
<p>Let’s get started:</p>
<ol>
<li>We first import and create a SparkSession :<pre class="source-code">
from pyspark.sql import
spark = .builder \
      .master("local[1]") \
      .appName("DataIngestion_CSV") \
      .config("spark.executor.memory", '3g') \
      .config("spark.executor.cores", '1') \
      .config("spark.cores.max", '1') \
      .getOrCreate()</pre></li>
<li>Then, we read the CSV file:<pre class="source-code">
df = spark.read.option('header',True).csv('spotify_data.csv')</pre></li>
<li>Then, we show the data<pre class="source-code">
df.show()</pre></li>
</ol>
<p>This is the result we get:</p>
<div><div><img alt="Figure 4.7 – spotify_data.csv DataFrame vision using Spark" height="465" src="img/Figure_4.7_B19453.jpg" width="615"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.7 – spotify_data.csv DataFrame vision using Spark</p>
<h2 id="_idParaDest-151"><a id="_idTextAnchor151"/>How it works…</h2>
<p>Simple as it is, we need<a id="_idIndexMarker335"/> to understand<a id="_idIndexMarker336"/> how reading a file with Spark works to make sure it always executes properly:</p>
<pre class="source-code">
df = spark.read.option('header',True).csv('spotify_data.csv')</pre>
<p>We attributed a variable to our code statement. This is a good practice when initializing any file reading because it allows us to control the version of the file and, if a change needs to be made, we attribute it to another variable. The name of the variable is also intentional since we are creating our first DataFrame.</p>
<p>Using the <code>.option()</code> method allows us to tell PySpark which type of configuration we want to pass. In this case, we set <code>header</code> to <code>True</code>, which makes PySpark set the first row of the CSV file as the column names. If we didn’t pass the required<a id="_idIndexMarker337"/> configuration, the DataFrame<a id="_idIndexMarker338"/> would look like this:</p>
<div><div><img alt="Figure 4.8 – spotify_data.csv DataFrame without column names" height="463" src="img/Figure_4.8_B19453.jpg" width="617"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.8 – spotify_data.csv DataFrame without column names</p>
<h2 id="_idParaDest-152"><a id="_idTextAnchor152"/>There’s more…</h2>
<p>The content of the files can differ, but some setups are welcome when handling CSV files in PySpark. Here, please note that we are going to change the method to <code>.options()</code>:</p>
<pre class="source-code">
df = spark.read.options(header= 'True',
                       sep=',',
                       inferSchema='True') \
                .csv('spotify_data.csv')</pre>
<p><code>header</code>, <code>sep</code>, and <code>inferSchema</code> are the most commonly<a id="_idIndexMarker339"/> used setups when reading a CSV file. Although CSV stands for <code>sep</code> (which stands for separator) declared.</p>
<p>Let’s see an example of an error when reading a CSV that uses a pipe to separate the strings and passes the wrong separator:</p>
<pre class="source-code">
df = spark.read.options(header= 'True',
                       sep=',',
                       inferSchema='True') \
                .csv('spotify_data_pipe.csv')</pre>
<p>This is how the output looks:</p>
<div><div><img alt="Figure 4.9 – CSV DataFrame reading without a proper sep definition" height="464" src="img/Figure_4.9_B19453.jpg" width="378"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.9 – CSV DataFrame reading without a proper sep definition</p>
<p>As you can see, it creates only one column with all the information. But if we pass <code>sep='|'</code>, it will return correctly:</p>
<div><div><img alt="Figure 4.10 – CSV DataFrame with the sep definition set to pipe" height="471" src="img/Figure_4.10_B19453.jpg" width="616"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.10 – CSV DataFrame with the sep definition set to pipe</p>
<h3>Other common .options() configurations</h3>
<p>There are other complex <a id="_idIndexMarker340"/>situations that, if not corrected during ingestion, can result in some problems with the other ETL steps. Here, I am using the <code>listing.csv</code> dataset, which can be found here at <a href="http://data.insideairbnb.com/the-netherlands/north-holland/amsterdam/2022-03-08/visualisations/listings.csv">http://data.insideairbnb.com/the-netherlands/north-holland/amsterdam/2022-03-08/visualisations/listings.csv</a>:</p>
<ol>
<li>We first read our common configurations:<pre class="source-code">
df_broken = spark.read.options(header= 'True', sep=',',
                       inferSchema='True') \
                .csv('listings.csv')
df_broken.show()</pre></li>
</ol>
<div><div><img alt="Figure 4.11 – listing.csv DataFrame" height="487" src="img/Figure_4.11_B19453.jpg" width="1080"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.11 – listing.csv DataFrame</p>
<p>At first glance, all is normal. However, what happens<a id="_idIndexMarker341"/> if I try to execute a simple group by using <code>room_type</code>?</p>
<pre class="source-code">
group = df_1.groupBy("room_type").count()
group.show()</pre>
<p>This is the output we get:</p>
<div><div><img alt="Figure 4.12 – Grouping df_broken by room_type" height="295" src="img/Figure_4.12_B19453.jpg" width="201"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.12 – Grouping df_broken by room_type</p>
<p>Ignoring <code>group by</code> for now, this happens because the file has plenty of escaped quotes and line breaks.</p>
<ol>
<li value="2">Now, let’s set the correct <code>.options()</code>:<pre class="source-code">
df_1 = spark.read.options(header=True, sep=',',
                          multiLine=True, escape='"') \
                .csv('listings.csv')
group = df_1.groupBy("room_type").count()
group.show()</pre></li>
</ol>
<p>This is the<a id="_idIndexMarker342"/> result:</p>
<div><div><img alt="Figure 4.13 – Grouping by room_type using the right options() settings" height="141" src="img/Figure_4.13_B19453.jpg" width="193"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.13 – Grouping by room_type using the right options() settings</p>
<h2 id="_idParaDest-153"><a id="_idTextAnchor153"/>See also</h2>
<p>You can<a id="_idIndexMarker343"/> see more PySpark <code>.options()</code> in the official documentation: <a href="https://spark.apache.org/docs/latest/sql-data-sources-csv.xhtml">https://spark.apache.org/docs/latest/sql-data-sources-csv.xhtml</a>.</p>
<h1 id="_idParaDest-154"><a id="_idTextAnchor154"/>Using PySpark to read JSON ﬁles</h1>
<p>In the <em class="italic">Reading a JSON file</em> recipe, we saw that JSON files<a id="_idIndexMarker344"/> are widely used to transport<a id="_idIndexMarker345"/> and share data between applications, and we saw how to read a JSON file using simple Python code.</p>
<p>However, with the increase in data size and sharing, using only Python to process a high volume of data can lead to performance or resilience issues. That’s why, for this type of scenario, it is highly recommended to use PySpark to read and process JSON files. As you might expect, PySpark comes with a straightforward reading solution.</p>
<p>In this recipe, we will cover how to read a JSON file with PySpark, the common associated issues, and how to solve them.</p>
<h2 id="_idParaDest-155"><a id="_idTextAnchor155"/>Getting ready</h2>
<p>As in the previous recipe, <em class="italic">Reading a JSON file</em>, we are going<a id="_idIndexMarker346"/> to use the <code>GitHub Events</code> JSON file. Also, the use of Jupyter<a id="_idIndexMarker347"/> Notebook is optional.</p>
<h2 id="_idParaDest-156"><a id="_idTextAnchor156"/>How to do it…</h2>
<p>Here are the steps for this recipe:</p>
<ol>
<li>We first create the SparkSession:<pre class="source-code">
spark = .builder \
      .master("local[1]") \
      .appName("DataIngestion_JSON") \
      .config("spark.executor.memory", '3g') \
      .config("spark.executor.cores", '1') \
      .config("spark.cores.max", '1') \
      .getOrCreate()</pre></li>
<li>Then, we read the JSON file:<pre class="source-code">
df_json = spark.read.option("multiline", "true") \
                    .json('github_events.json')</pre></li>
<li>Then, we show the data:<pre class="source-code">
df_json.show()</pre></li>
</ol>
<p>This is how the output appears:</p>
<div><div><img alt="Figure 4.14 – df_json DataFrame" height="565" src="img/Figure_4.14_B19453.jpg" width="1112"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.14 – df_json DataFrame</p>
<h2 id="_idParaDest-157"><a id="_idTextAnchor157"/>How it works…</h2>
<p>Similar to CSV files, reading a JSON file<a id="_idIndexMarker348"/> using PySpark is very simple, requiring<a id="_idIndexMarker349"/> only one line of code. Like pandas, it ignores the brackets in the file and creates a table-structured DataFrame, even though we are handling a semi-structured data file. However, the great magic is in<code>.option("multiline", "true")</code>.</p>
<p>If you remember our JSON structure for this file, it is something like this:</p>
<pre class="source-code">
[
  {
    "id": "25208138097",
    "type": "WatchEvent",
    "actor": {...},
    "repo": {...},
    "payload": {
      "action": "started"
    },
    "public": true,
    "created_at": "2022-11-13T22:52:04Z",
    "org": {...}
  },
  {
    "id": "25208138110",
    "type": "IssueCommentEvent",
    "actor": {...},
    "repo": {...},
  },...</pre>
<p>It is a multi-lined JSON since<a id="_idIndexMarker350"/> it has objects inside objects. The <code>.option()</code> setup passed<a id="_idIndexMarker351"/> when reading the file guarantees PySpark will read it as it should, and if we don’t pass this argument, an error like this will appear:</p>
<div><div><img alt="Figure 4.15 – This error is shown when .options() is not well defined for a multi-lined JSON file" height="193" src="img/Figure_4.15_B19453.jpg" width="684"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.15 – This error is shown when .options() is not well defined for a multi-lined JSON file</p>
<p>PySpark understands it as a corrupted file.</p>
<h2 id="_idParaDest-158"><a id="_idTextAnchor158"/>There’s more…</h2>
<p>A very widely used configuration for reading JSON files is <code>dropFieldIfAllNull</code>. When set to <code>true</code>, if there is an empty array, it will drop it from the schema.</p>
<p>Unstructured and semi-structured data are valuable due to their elasticity. Sometimes, applications can change their output and some fields become deprecated. To avoid changing the ingest script (especially if these changes can be frequent), <code>dropFieldIfAllNull</code> removes them from the DataFrame.</p>
<h2 id="_idParaDest-159"><a id="_idTextAnchor159"/>See also</h2>
<ul>
<li>To find<a id="_idIndexMarker352"/> out more about PySpark <code>.options()</code>, refer to the official documentation: <a href="https://spark.apache.org/docs/latest/sql-data-sources-json.xhtml">https://spark.apache.org/docs/latest/sql-data-sources-json.xhtml</a></li>
</ul>
<h1 id="_idParaDest-160"><a id="_idTextAnchor160"/>Further reading</h1>
<ul>
<li><a href="https://docs.fileformat.com/spreadsheet/csv/">https://docs.fileformat.com/spreadsheet/csv/</a></li>
<li><a href="https://codefather.tech/blog/python-with-open/">https://codefather.tech/blog/python-with-open/</a></li>
<li><a href="https://www.programiz.com/python-programming/methods/built-in/next">https://www.programiz.com/python-programming/methods/built-in/next</a></li>
<li><a href="https://spark.apache.org/docs/latest/sql-data-sources-csv.xhtml">https://spark.apache.org/docs/latest/sql-data-sources-csv.xhtml</a></li>
<li><a href="https://sparkbyexamples.com/pyspark/pyspark-read-json-file-into-dataframe/">https://sparkbyexamples.com/pyspark/pyspark-read-json-file-into-dataframe/</a></li>
</ul>
</div>
</div></body></html>