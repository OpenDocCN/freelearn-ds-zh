<html><head></head><body><div><div><h1 id="_idParaDest-116"><em class="italics"><a id="_idTextAnchor136"/>Chapter 4</em></h1>
		</div>
		<div><h1 id="_idParaDest-117"><a id="_idTextAnchor137"/>A Deep Dive into Data Wrangling with Python</h1>
		</div>
		<div><h2>Learning Objectives</h2>
			<p>By the end of this chapter, you will be able to:</p>
			<ul>
				<li class="bullets">Perform subsetting, filtering, and grouping on pandas DataFrames</li>
				<li class="bullets">Apply Boolean filtering and indexing from a DataFrame to choose specific elements</li>
				<li class="bullets">Perform JOIN operations in pandas that are analogous to the SQL command</li>
				<li class="bullets">Identify missing or corrupted data and choose to drop or apply imputation techniques on missing or corrupted data</li>
			</ul>
			<p>In this chapter, we will learn about pandas DataFrames in detail.</p>
		</div>
		<div><h2 id="_idParaDest-118"><a id="_idTextAnchor138"/>Introduction</h2>
			<p>In this chapter, we will learn about several advanced operations involving pandas DataFrames and NumPy arrays. On completing the detailed activity for this chapter, you will have handled real-life datasets and understood the process of data wrangling.</p>
			<h2 id="_idParaDest-119"><a id="_idTextAnchor139"/>Subsetting, Filtering, and Grouping</h2>
			<p>One of the most important aspects of data wrangling is to curate the data carefully from the deluge of streaming data that pours into an organization or business entity from various sources. Lots of data is not always a good thing; rather, data needs to be useful and of high-quality to be effectively used in downstream activities of a data science pipeline such as machine learning and predictive model building. Moreover, one data source can be used for multiple purposes and this often requires different subsets of data to be processed by a data wrangling module. This is then passed on to separate analytics modules.</p>
			<p>For example, let's say you are doing data wrangling on US State level economic output. It is a fairly common scenario that one machine learning model may require data for large and populous states (such as California, Texas, and so on), while another model demands processed data for small and sparsely populated states (such as Montana or North Dakota). As the frontline of the data science process, it is the responsibility of the data wrangling module to satisfy the requirements of both these machine learning models. Therefore, as a data wrangling engineer, you have to filter and group data accordingly (based on the population of the state) before processing them and producing separate datasets as the final output for separate machine learning models.</p>
			<p>Also, in some cases, data sources may be biased, or the measurement may corrupt the incoming data occasionally. It is a good idea to try to filter only the error-free, good data for downstream modeling. From these examples and discussions, it is clear that filtering and grouping/bucketing data is an essential skill to have for any engineer that's engaged in the task of data wrangling. Let's proceed to learn about a few of these skills with pandas.</p>
			<h3 id="_idParaDest-120"><a id="_idTextAnchor140"/>Exercise 48: Loading and Examining a Superstore's Sales Data from an Excel File</h3>
			<p>In this exercise, we will load and examine an Excel file.</p>
			<ol>
				<li>To read an Excel file into pandas, you will need a small package called <code>xlrd</code> to be installed on your system. If you are working from inside this book's Docker container, then this package may not be available next time you start your container, and you have to follow the same step. Use the following code to install the xlrd package:<pre>!pip install xlrd</pre></li>
				<li>Load the Excel file from GitHub by using the simple pandas method <code>read_excel</code>:<pre>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
df = pd.read_excel("Sample - Superstore.xls")
df.head()</pre><p>Examine all the columns and check if they are useful for analysis:</p><div><img src="img/C11065_04_01.jpg" alt="Figure 4.1 Output of the Excel file in a DataFrame" width="1334" height="505"/></div><h6>Figure 4.1 Output of the Excel file in a DataFrame</h6><p>On examining the file, we can see that the first column, called <strong class="bold">Row ID</strong>, is not very useful.</p></li>
				<li>Drop this column altogether from the DataFrame by using the <code>drop</code> method:<pre>df.drop('Row ID',axis=1,inplace=True)</pre></li>
				<li>Check the number of rows and columns in the newly created dataset. We will use the <code>shape</code> function here:<pre>df.shape</pre><p>The output is as follows:</p><pre>(9994, 20)</pre><p>We can see that the dataset has 9,994 rows and 20 columns.</p></li>
			</ol>
			<h3 id="_idParaDest-121">S<a id="_idTextAnchor141"/>ubsetting the DataFrame</h3>
			<p><strong class="keyword">Subsetting</strong> involves the extraction of partial data based on specific columns and rows, as per business needs. Suppose we are interested only in the following information from this dataset: Customer ID, Customer Name, City, Postal Code, and Sales. For demonstration purposes, let's assume that we are only interested in 5 records – rows 5-9. We can subset the DataFrame to extract only this much information using a single line of Python code.</p>
			<p>Use the <code>loc</code> method to index the dataset by name of the columns and index of the rows:</p>
			<pre>df_subset = df.loc[
    [i for i in range(5,10)],
    ['Customer ID','Customer Name','City','Postal Code',
     'Sales']]
df_subset</pre>
			<p>The output is as follows:</p>
			<div><div><img src="img/C11065_04_02.jpg" alt="Figure 4.2: DataFrame indexed by name of the columns" width="965" height="255"/>
				</div>
			</div>
			<h6>Figure 4.2: DataFrame indexed by name of the columns</h6>
			<p>We need to pass on two arguments to the <code>loc</code> method – one for indicating the rows, and another for indicating the columns. These should be Python lists.</p>
			<p>For the rows, we have to pass a list [5,6,7,8,9], but instead of writing that explicitly, we use a list comprehension, that is, <code>[i for i in range(5,10)]</code>.</p>
			<p>Because the columns we are interested in are not contiguous, we cannot just put a continuous range and need to pass on a list containing the specific names. So, the second argument is just a simple list with specific column names.</p>
			<p>The dataset shows the fundamental concepts of the process of subsetting a DataFrame based on business requirements.</p>
			<h3 id="_idParaDest-122"><a id="_idTextAnchor142"/>An Example Use Case: Determining Statistics on Sales and Profit</h3>
			<p>This quick section shows a typical use case of subsetting. Suppose we want to calculate descriptive statistics (mean, median, standard deviation, and so on) of records 100-199 for sales and profit. This is how subsetting helps us to achieve that:</p>
			<pre>df_subset = df.loc[[i for i in range(100,200)],['Sales','Profit']]
df_subset.describe()</pre>
			<p>The output is as follows:</p>
			<div><div><img src="img/C11065_04_03.jpg" alt="Figure 4.3 Output of descriptive statistics of data" width="738" height="373"/>
				</div>
			</div>
			<h6>Figure 4.3 Output of descriptive statistics of data</h6>
			<p>Furthermore, we can create boxplots of sales and profit figures from this final data.</p>
			<p>We simply extract records 100-199 and run the <code>describe</code> function on it because we don't want to process all the data! For this particular business question, we are only interested in sales and profit numbers and therefore we should not take the easy route and run a describe function on all the data. For a real-life dataset, the number of rows and columns could often be in the millions, and we don't want to compute anything that is not asked for in the data wrangling task. We always aim to subset the exact data that is needed to be processed and run statistical or plotting functions on that partial data:</p>
			<div><div><img src="img/C11065_04_04.jpg" alt="Figure 4.4: Boxplot of sales and profit" width="794" height="417"/>
				</div>
			</div>
			<h6>Figure 4.4: Boxplot of sales and profit</h6>
			<h3 id="_idParaDest-123">Exer<a id="_idTextAnchor143"/>cise 49: The unique Function</h3>
			<p>Before continuing further with filtering methods, let's take a quick detour and explore a super useful function called <code>unique</code>. As the name suggests, this function is used to scan through the data quickly and extract only the unique values in a column or row.</p>
			<p>After loading the superstore sales data, you will notice that there are columns like "Country", "State", and "City". A natural question will be to ask how many countries/states/cities are present in the dataset:</p>
			<ol>
				<li value="1">Extract the countries/states/cities for which the information is in the database, with one simple line of code, as follows:<pre>df['State'].unique()</pre><p>The output is as follows:</p><div><img src="img/C11065_04_05.jpg" alt="Figure 4.5: Different states present in the dataset" width="1020" height="307"/></div><h6>Figure 4.5: Different states present in the dataset</h6><p>You w<a id="_idTextAnchor144"/>ill see a list of all the states whose data is present in the dataset.</p></li>
				<li>Use the <code>nunique</code> method to count the number of unique values, like so:<pre>df['State'].nunique()</pre><p>The output is as follows:</p><pre>49</pre><p>This returns 49 for this dataset. So, one out of 50 states in the US does not appear in this dataset.</p></li>
			</ol>
			<p>Similarly, if we run this function on the Country column, we get an array with only one element, <code>United States</code>. Immediately, we can see that we don't need to keep the country column at all, because there is no useful information in that column except that all the entries are the same. This is how a simple function helped us to decide about dropping a column altogether – that is, removing 9,994 pieces of unnecessary data!</p>
			<h3 id="_idParaDest-124">Condi<a id="_idTextAnchor145"/>tional Selection and Boolean Filtering</h3>
			<p>Often, we don't want to process the whole dataset and would like to select only a partial dataset whose contents satisfy a particular condition. This is probably the most common use case of any data wrangling task.</p>
			<p>In the context of our superstore sales dataset, think of these common questions that may arise from the daily activity of the business analytics team:</p>
			<ul>
				<li>What are the average sales and profit figures in California?</li>
				<li>Which states have the highest and lowest total sales?</li>
				<li>What consumer segment has the most variance in sales/profit?</li>
				<li>Among the top 5 states in sales, which shipping mode and product category are the most popular choices?</li>
			</ul>
			<p>Countless examples can be given where the business analytics team or the executive management want to glean insight from a particular subset of data that meet certain criteria.</p>
			<p>If you have any prior experience with SQL, you will know that these kinds of questions require fairly complex SQL query writing. Remember the WHERE clause?</p>
			<p>We will show you how to use conditional subsetting and Boolean filtering to answer such questions.</p>
			<p>First, we need to understand the critical concept of boolean indexing. This process essentially accepts a conditional expression as an argument and returns a dataset of booleans in which the <code>TRUE</code> value appears in places where the condition was satisfied. A simple example is shown in the following code. For demonstration purposes, we subset a small dataset of 10 records and 3 columns:</p>
			<pre>df_subset = df.loc[[i for i in range (10)],['Ship Mode','State','Sales']]
df_subset</pre>
			<p>The output is as follows:</p>
			<div><div><img src="img/C11065_04_06.jpg" alt="Figure 4.6: Sample dataset" width="828" height="443"/>
				</div>
			</div>
			<h6>Figure 4.6: Sample dataset</h6>
			<p>Now, if we just want to know the records with sales higher than $100, then we can write the following:</p>
			<pre>df_subset&gt;100</pre>
			<p>This produces the following boolean DataFrame:</p>
			<div><div><img src="img/C11065_04_07.jpg" alt="Figure 4.7: Records with sales higher than $100" width="775" height="436"/>
				</div>
			</div>
			<h6>Figure 4.7: Records with sales higher than $100</h6>
			<p>Note the True and False entries in the <strong class="bold">Sales</strong> column. Values in the <strong class="bold">Ship Mode</strong> and <strong class="bold">State</strong> columns were not impacted by this code because the comparison was with a numerical quantity, and the only numeric column in the original DataFrame was <strong class="bold">Sales</strong>.</p>
			<p>Now, let's see what happens if we pass this boolean DataFrame as an index to the original DataFrame:</p>
			<pre>df_subset[df_subset&gt;100]</pre>
			<p>The output is as follows:</p>
			<div><div><img src="img/C11065_04_08.jpg" alt="Figure 4.8: Results after passing the boolean DataFrame as an index to original DataFrame" width="559" height="297"/>
				</div>
			</div>
			<h6>Figure 4.8: Results after passing the boolean DataFrame as an index to the original DataFrame</h6>
			<p>The NaN values came from the fact that the preceding code tried to create a DataFrame with TRUE indices (in the Boolean DataFrame) only.</p>
			<p>The values which were TRUE in the boolen DataFrame were retained in the final output DataFrame.</p>
			<p>The program inserted <strong class="bold">NaN</strong> values for the rows where data was not available (because they were discarded due to the Sales value being &lt; $100).</p>
			<p>Now, we probably don't want to work with this resulting DataFrame with <code>Sales &gt; $100</code>. We can achieve that by simply passing only the <code>Sales</code> column:</p>
			<pre>df_subset[df_subset['Sales']&gt;100]</pre>
			<p>This produces the expected result:</p>
			<div><div><img src="img/C11065_04_09.jpg" alt="Figure 4.9: Results after removing the NaN values" width="657" height="126"/>
				</div>
			</div>
			<h6>Figure 4.9: Results after removing the NaN values</h6>
			<p>We are not limited to conditional expressions involving numeric quantities only. Let's try to extract high sales values (&gt; $100) for entries that do not involve Colorado.</p>
			<p>We can write the following code to accomplish that:</p>
			<pre>df_subset[(df_subset['State']!='Colorado') &amp; (df_subset['Sales']&gt;100)]</pre>
			<p>Note the use of a conditional involving string. In this expression, we are joining two conditionals by an &amp; operator. Both conditions must be wrapped inside parentheses.</p>
			<p>The first conditional expression simply matches the entries in the <code>State</code> column to the string <code>Colorado</code> and assigns TRUE/FALSE accordingly. The second conditional is the same as before. Together, joined by the &amp; operator, they extract only those rows for which <code>State</code> is not <code>Colorado</code> and <code>Sales</code> is <code>&gt; $100</code>. We get the following result:</p>
			<div><div><img src="img/C11065_04_10.jpg" alt="" width="448" height="65"/>
				</div>
			</div>
			<h6>Figure 4.10: Results where State is not California and Sales is higher than $100</h6>
			<h4><strong class="bold">Note</strong></h4>
			<p class="callout">Although, in theory, there is no limit on how complex a conditional you can build using individual expressions and &amp; (LOGICAL AND) and | (LOGICAL OR) operators, it is advisable to create intermediate boolean DataFrames with limited conditional expressions and build your final DataFrame step by step. This keeps the code legible and scalable.</p>
			<h3 id="_idParaDest-125">Exercise 5<a id="_idTextAnchor146"/>0: Setting and Resetting the Index</h3>
			<p>Sometimes, we may need to reset or eliminate the default index of a DataFrame and assign a new column as an index:</p>
			<ol>
				<li value="1">Create the <code>matrix_data</code>, <code>row_labels</code>, and <code>column_headings</code> functions using the following command:<pre>matrix_data = np.matrix(
    '22,66,140;42,70,148;30,62,125;35,68,160;25,62,152')
row_labels = ['A','B','C','D','E']
column_headings = ['Age', 'Height', 'Weight']</pre></li>
				<li>Create a DataFrame using the <code>matrix_data</code>, <code>row_labels</code>, and <code>column_headings</code> functions:<pre>df1 = pd.DataFrame(data=matrix_data, 
                   index=row_labels,
                   columns=column_headings)
print("\nThe DataFrame\n",'-'*25, sep='')
print(df1)</pre><p>The output is as follows:</p><div><img src="img/C11065_04_11.jpg" alt="" width="609" height="184"/></div><h6>Figure 4.11: The original DataFrame</h6></li>
				<li>Reset the index, as follows:<pre>print("\nAfter resetting index\n",'-'*35, sep='')
print(df1.reset_index())</pre><div><img src="img/C11065_04_12.jpg" alt="" width="607" height="176"/></div><h6>Figure 4.12: DataFrame after resetting the index</h6></li>
				<li>Reset the index with <code>drop</code> set to <code>True</code>, as follows:<pre>print("\nAfter resetting index with 'drop' option TRUE\n",'-'*45, sep='')
print(df1.reset_index(drop=True))</pre><div><img src="img/C11065_04_13.jpg" alt="Figure 4.13: DataFrame after resetting the index with the drop option set to true" width="691" height="181"/></div><h6>Figure 4.13: DataFrame after resetting the index with the drop option set to true</h6></li>
				<li>Add a new column using the following command:<pre>print("\nAdding a new column 'Profession'\n",'-'*45, sep='')
df1['Profession'] = "Student Teacher Engineer Doctor Nurse".split()
print(df1)</pre><p>The output is as follows:</p><div><img src="img/C11065_04_14.jpg" alt="" width="786" height="180"/></div><h6>Figure 4.14: DataFrame after adding a new column called Profession</h6></li>
				<li>Now, set the <code>Profession</code> column as an <code>index</code> using the following code:<pre>print("\nSetting 'Profession' column as index\n",'-'*45, sep='')
print (df1.set_index('Profession'))</pre><p>The output is as follows:</p></li>
			</ol>
			<div><div><img src="img/C11065_04_15.jpg" alt="" width="644" height="200"/>
				</div>
			</div>
			<h6>Figure 4.15: DataFrame after setting the Profession as an index</h6>
			<h3 id="_idParaDest-126">Exercise 51: Th<a id="_idTextAnchor147"/>e GroupBy Method</h3>
			<p>Group by refers to a process involving one or more of the following steps:</p>
			<ul>
				<li>Splitting the data into groups based on some criteria</li>
				<li>Applying a function to each group independently</li>
				<li>Combining the results into a data structure</li>
			</ul>
			<p>In many situations, we can split the dataset into groups and do something with those groups. In the apply step, we might wish to do one of the following:</p>
			<ul>
				<li><strong class="bold">Aggregation</strong>: Compute a summary statistic (or statistics) for each group – sum, mean, and so on</li>
				<li><strong class="bold">Transformation</strong>: Perform a group-specific computation and return a like-indexed object – z-transformation or filling missing data with a value</li>
				<li><strong class="bold">Filtration</strong>: Discard few groups, according to a group-wise computation that evaluates TRUE or FALSE</li>
			</ul>
			<p>There is, of course, a describe method to this <code>GroupBy</code> object, which produces the summary statistics in the form of a DataFrame.</p>
			<p><code>GroupBy</code> is not limited to a single variable. If you pass on multiple variables (as a list), then you will get back a structure essentially similar to a Pivot Table (from Excel). The following is an example where we group together all the states and cities from the whole dataset (the snapshot is a partial view only).</p>
			<h4>Note</h4>
			<p class="callout">The name <code>GroupBy</code> should be quite familiar to those who have used a SQL-based tool before.</p>
			<ol>
				<li value="1">Create a 10-record subset using the following command:<pre>df_subset = df.loc[[i for i in range (10)],['Ship Mode','State','Sales']]</pre></li>
				<li>Create a pandas DataFrame using the <code>groupby</code> object, as follows:<pre>byState = df_subset.groupby('State')</pre></li>
				<li>Calculate the mean sales figure by state by using the following command:<pre>print("\nGrouping by 'State' column and listing mean sales\n",'-'*50, sep='')
print(byState.mean())</pre><p>The output is as follows:</p><div><img src="img/C11065_04_16.jpg" alt="Figure 4.16: Output after grouping the state with the listing mean sales" width="679" height="197"/></div><h6>Figure 4.16: Output after grouping the state with the listing mean sales</h6></li>
				<li>Calculate the total sales figure by state by using the following command:<pre>print("\nGrouping by 'State' column and listing total sum of sales\n",'-'*50, sep='')
print(byState.sum())</pre><p>The output is as follows:</p><div><img src="img/C11065_04_17.jpg" alt="Figure 4.17: The output after grouping the state with the listing sum of sales" width="677" height="201"/></div><h6>Figure 4.17: The output after grouping the state with the listing sum of sales</h6></li>
				<li>Subset that DataFrame for a particular state and show the statistics:<pre>pd.DataFrame(byState.describe().loc['California'])</pre><p>The output is as follows:</p><div><img src="img/C11065_04_18.jpg" alt="Figure 4.18: Checking the statistics of a particular state" width="862" height="363"/></div><h6>Figure 4.18: Checking the statistics of a particular state</h6></li>
				<li>Perform a similar summarization by using the <code>Ship Mode</code> attribute:<pre>df_subset.groupby('Ship Mode').describe().loc[['Second Class','Standard Class']]</pre><p>The output will be as follows:</p><div><img src="img/C11065_04_19.jpg" alt="Figure 4.19: Checking the sales by summarizing the Ship Mode attribute" width="556" height="139"/></div><h6>Figure 4.19: Checking the sales by summarizing the Ship Mode attribute</h6><p>Note how pandas has grouped the data by <code>State</code> first and then by cities under each state.</p></li>
				<li>Display the complete summary statistics of sales by every city in each state – all by two lines of code by using the following command:<pre>byStateCity=df.groupby(['State','City'])
byStateCity.describe()['Sales']</pre><p>The output is as follows:</p></li>
			</ol>
			<div><div><img src="img/C11065_04_20.jpg" alt="Figure 4.20: Checking the summary statistics of sales" width="962" height="541"/>
				</div>
			</div>
			<h6>Figure 4.20: Checking the summary statistics of sales</h6>
			<h2 id="_idParaDest-127"><a id="_idTextAnchor148"/>Detecting Outliers and Handling Missing Values</h2>
			<p>Outlier detection and handling missing values fall under the subtle art of data quality checking. A modeling or data mining process is fundamentally a complex series of computations whose output quality largely depends on the quality and consistency of the input data being fed. The responsibility of maintaining and gate keeping that quality often falls on the shoulders of a data wrangling team.</p>
			<p>Apart from the obvious issue of poor quality data, missing data can sometimes wreak havoc with the machine learning (ML) model downstream. A few ML models, like Bayesian learning, are inherently robust to outliers and missing data, but commonly techniques like Decision Trees and Random Forest have an issue with missing data because the fundamental splitting strategy employed by these techniques depends on an individual piece of data and not a cluster. Therefore, it is almost always imperative to impute missing data before handing it over to such a ML model.</p>
			<p>Outlier detection is a subtle art. Often, there is no universally agreed definition of an outlier. In a statistical sense, a data point that falls outside a certain range may often be classified as an outlier, but to apply that definition, you need to have a fairly high degree of certainty about the assumption of the nature and parameters of the inherent statistical distribution about the data. It takes a lot of data to build that statistical certainty and even after that, an outlier may not be just an unimportant noise but a clue to something deeper. Let's take an example with some fictitious sales data from an American fast food chain restaurant. If we want to model the daily sales data as a time series, we observe an unusual spike in the data somewhere around mid-April:</p>
			<div><div><img src="img/C11065_04_21.jpg" alt="" width="1355" height="556"/>
				</div>
			</div>
			<h6>Figure 4.21: Fictitious sales data of an American fast food chain restaurant</h6>
			<p>A good data scientist or data wrangler should develop curiosity about this data point rather than just rejecting it just because it falls outside the statistical range. In the actual anecdote, the sales figure really spiked that day because of an unusual reason. So, the data was real. But just because it was real does not mean it is useful. In the final goal of building a smoothly varying time series model, this one point should not matter and should be rejected. But the chapter here is that we cannot reject outliers without paying some attention to them.</p>
			<p>Therefore, the key to outliers is their systematic and timely detection in an incoming stream of millions of data or while reading data from a cloud-based storage. In this topic, we will quickly go over some basic statistical tests for detecting outliers and some basic imputation techniques for filling up missing data.</p>
			<h3 id="_idParaDest-128">Missing Values in Pan<a id="_idTextAnchor149"/>das</h3>
			<p>One of the most useful functions to detect missing values is <code>isnull</code>. Here, we have a snapshot of a <code>DataFrame</code> called <code>df_missing</code> (sampled partially from the superstore DataFrame we are working with) with some missing values:</p>
			<div><div><img src="img/C11065_04_22.jpg" alt="Figure 4.22: DataFrame with missing values" width="1280" height="830"/>
				</div>
			</div>
			<h6>Figure 4.22: DataFrame with missing values</h6>
			<p>Now, if we simply run the following code, we will get a DataFrame that's the same size as the original with boolean values as TRUE for the places where a <strong class="bold">NaN</strong> was encountered. Therefore, it is simple to test for the presence of any <strong class="bold">NaN</strong>/missing value for any row or column of the DataFrame. You just have to add the particular row and column of this boolean DataFrame. If the result is greater than zero, then you know there are some TRUE values (because FALSE here is denoted by 0 and TRUE here is denoted by 1) and correspondingly some missing values. Try the following snippet:</p>
			<pre>df_missing=pd.read_excel("Sample - Superstore.xls",sheet_name="Missing")
df_missing</pre>
			<p>The output is as follows:</p>
			<div><div><img src="img/C11065_04_23.jpg" alt="Figure 4.23: DataFrame with the Excel values" width="543" height="282"/>
				</div>
			</div>
			<h6>Figure 4.23: DataFrame with the Excel values</h6>
			<p>Use the <code>isnull</code> function on the DataFrame and observe the results:</p>
			<pre>df_missing.isnull()</pre>
			<div><div><img src="img/C11065_04_24.jpg" alt="Figure 4.24 Output highlighting the missing values" width="506" height="266"/>
				</div>
			</div>
			<h6>Figure 4.24 Output highlighting the missing values</h6>
			<p>Here is an example of some very simple code to detect, count, and print out missing values in every column of a DataFrame:</p>
			<pre>for c in df_missing.columns:
    miss = df_missing[c].isnull().sum()
    if miss&gt;0:
        print("{} has {} missing value(s)".format(c,miss))
    else:
        print("{} has NO missing value!".format(c))</pre>
			<p>This code scans every column of the DataFrame, calls the <code>isnull</code> function, and sums up the returned object (a pandas Series object, in this case) to count the number of missing values. If the missing value is greater than zero, it prints out the message accordingly. The output looks as follows:</p>
			<div><div><img src="img/C11065_04_25.jpg" alt="Figure 4.25: Output of counting the missing values" width="890" height="170"/>
				</div>
			</div>
			<h6>Figure 4.25: Output of counting the missing values</h6>
			<h3 id="_idParaDest-129">Exercise 52: Filling in t<a id="_idTextAnchor150"/>he Missing Values with fillna</h3>
			<p>To handle missing values, you should first look for ways not to drop them altogether but to fill them somehow. The <code>fillna</code> method is a useful function for performing this task on pandas DataFrames. The <code>fillna</code> method may work for string data, but not for numerical columns like sales or profits. So, we should restrict ourselves in regards to this fixed string replacement to non-numeric text-based columns only. The <code>Pad</code> or <code>ffill</code> function is used to fill forward the data, that is, copy it from the preceding data of the series. </p>
			<p>The <code>mean</code> function can be used to fill using the average of the two values:</p>
			<ol>
				<li value="1">Fill all missing values with the string <code>FILL</code> by using the following command:<pre>df_missing.fillna('FILL')</pre><p>The output is as follows:</p><div><img src="img/C11065_04_26.jpg" alt="Figure 4.26: Missing values replaced with FILL" width="898" height="470"/></div><h6>Figure 4.26: Missing values replaced with FILL</h6></li>
				<li>Fill in the specified columns with the string <code>FILL</code> by using the following command:<pre>df_missing[['Customer','Product']].fillna('FILL')</pre><p>The output is as follows:</p><div><img src="img/C11065_04_27.jpg" alt="" width="567" height="324"/></div><h6>Figure 4.27: Specified columns replaced with FILL</h6><h4>Note</h4><p class="callout">In all of these cases, the function works on a copy of the original DataFrame. So, if you want to make the changes permanent, you have to assign the DataFrames that are returned by these functions to the original DataFrame object.</p></li>
				<li>Fill in the values using pad or backfill by using the following command:<pre>df_missing['Sales'].fillna(method='ffill')</pre></li>
				<li>Use <code>backfill</code> or <code>bfill</code> to fill backward, that is, copy from the next data in the series:<pre>df_missing['Sales'].fillna(method='bfill')</pre><div><img src="img/C11065_04_28.jpg" alt="Figure 4.28: Using forward fill and backward fill to fill in missing data" width="1420" height="407"/></div><h6>Figure 4.28: Using forward fill and backward fill to fill in missing data</h6></li>
				<li>You can also fill by using a function average of DataFrames. For example, we may want to fill the missing values in Sales by the average sales amount. Here is how we can do that:<pre>df_missing['Sales'].fillna(df_missing.mean()['Sales'])</pre></li>
			</ol>
			<div><div><img src="img/C11065_04_29.jpg" alt="" width="1266" height="405"/>
				</div>
			</div>
			<h6>Figure 4.29: Using average to fill in missing data</h6>
			<h3 id="_idParaDest-130">Exercise 53: Dropping Missing<a id="_idTextAnchor151"/> Values with dropna</h3>
			<p>This function is used to simply drop the rows or columns that contain NaN/missing values. However, there is some choice involved.</p>
			<p>If the axis parameter is set to zero, then rows containing missing values are dropped; if the axis parameter is set to one, then columns containing missing values are dropped. These are useful if we don't want to drop a particular row/column if the NaN values do not exceed a certain percentage.</p>
			<p>Two arguments that are useful for the <code>dropna</code>() method are as follows:</p>
			<ul>
				<li>The <code>how</code> argument determines if a row or column is removed from a DataFrame, when we have at least one NaN or all NaNs</li>
				<li>The <code>thresh</code> argument requires that many non-NaN values to keep the row/column</li>
			</ul>
			<ol>
				<li value="1">To set the axis parameter to zero and drop all missing rows, use the following command:<pre>df_missing.dropna(axis=0)</pre></li>
				<li>To set the axis parameter to one and drop all missing rows, use the following command:<pre>df_missing.dropna(axis=1)</pre><div><img src="img/C11065_04_30.jpg" alt="Figure 4.30: Dropping rows or columns to handle missing data" width="1280" height="720"/></div><h6>Figure 4.30: Dropping rows or columns to handle missing data</h6></li>
				<li>Drop the values with the axis set to one and thresh set to 10:<pre>df_missing.dropna(axis=1,thresh=10)</pre><p>The output is as follows:</p></li>
			</ol>
			<div><div><img src="img/C11065_04_31.jpg" alt="Figure 4.31: DataFrame with values dropped with axis=1 and thresh=10" width="674" height="338"/>
				</div>
			</div>
			<h6>Figure 4.31: DataFrame with values dropped with axis=1 and thresh=10</h6>
			<p>All of these methods work on a temporary copy. To make a permanent change, you have to set <code>inplace=True</code> or assign the result to the original DataFrame, that is, overwrite it.</p>
			<h3 id="_idParaDest-131">Outlier Detection Using a Simpl<a id="_idTextAnchor152"/>e Statistical Test</h3>
			<p>As we've already discussed, outliers in a dataset can occur due to many factors and in many ways:</p>
			<ul>
				<li>Data entry errors</li>
				<li>Experimental errors (data extraction related)</li>
				<li>Measurement errors due to noise or instrumental failure</li>
				<li>Data processing errors (data manipulation or mutations due to coding error)</li>
				<li>Sampling errors (extracting or mixing data from wrong or various sources)</li>
			</ul>
			<p>It is impossible to pin-point one universal method for outlier detection. Here, we will show you some simple tricks for numeric data using standard statistical tests.</p>
			<p>Boxplots may show unusual values. Corrupt two sales values by assigning negative, as follows:</p>
			<pre>df_sample = df[['Customer Name','State','Sales','Profit']].sample(n=50).copy()
df_sample['Sales'].iloc[5]=-1000.0
df_sample['Sales'].iloc[15]=-500.0</pre>
			<p>To plot the boxplot, use the following code:</p>
			<pre>df_sample.plot.box()
plt.title("Boxplot of sales and profit", fontsize=15)
plt.xticks(fontsize=15)
plt.yticks(fontsize=15)
plt.grid(True)</pre>
			<p>The output is as follows:</p>
			<div><div><img src="img/C11065_04_32.jpg" alt="" width="1057" height="418"/>
				</div>
			</div>
			<h6>Figure 4.32: Boxplot of sales and profit</h6>
			<p>We can create simple boxplots to check for any unusual/nonsensical values. For example, in the preceding example, we intentionally corrupted two sales values to be negative and they were readily caught in a boxplot.</p>
			<p>Note that profit may be negative, so those negative points are generally not suspicious. But sales cannot be negative in general, so they are detected as outliers.</p>
			<p>We can create a distribution of a numerical quantity and check for values that lie at the extreme end to see if they are truly part of the data or outlier. For example, if a distribution is almost normal, then any value more than 4 or 5 standard deviations away may be a suspect:</p>
			<div><div><img src="img/C11065_04_33.jpg" alt="Figure 4.33: Value away from the main outliers" width="888" height="355"/>
				</div>
			</div>
			<h6>Figure 4.33: Value away from the main outliers</h6>
			<h2 id="_idParaDest-132"><a id="_idTextAnchor153"/>Concatenating, Merging, and Joining</h2>
			<p>Merging and joining tables or datasets are highly common operations in the day-to-day job of a data wrangling professional. These operations are akin to the JOIN query in SQL for relational database tables. Often, the key data is present in multiple tables, and those records need to be brought into one combined table that's matching on that common key. This is an extremely common operation in any type of sales or transactional data, and therefore must be mastered by a data wrangler. The pandas library offers nice and intuitive built-in methods to perform various types of JOIN queries involving multiple DataFrame objects.</p>
			<h3 id="_idParaDest-133"><a id="_idTextAnchor154"/>Exercise 54: Concatenation</h3>
			<p>We will start by learning the concatenation of DataFrames along various axes (rows or columns). This is a very useful operation as it allows you to grow a DataFrame as the new data comes in or new feature columns need to be inserted in the table:</p>
			<ol>
				<li value="1">Sample 4 records each to create three DataFrames at random from the original sales dataset we are working with:<pre>df_1 = df[['Customer Name','State','Sales','Profit']].sample(n=4)
df_2 = df[['Customer Name','State','Sales','Profit']].sample(n=4)
df_3 = df[['Customer Name','State','Sales','Profit']].sample(n=4)</pre></li>
				<li>Create a combined DataFrame with all the rows concatenated by using the following code:<pre>df_cat1 = pd.concat([df_1,df_2,df_3], axis=0)
df_cat1</pre><div><img src="img/C11065_04_34.jpg" alt="Figure 4.34: Concatenating DataFrames together&#13;&#10;" width="1280" height="720"/></div><h6>Figure 4.34: Concatenating DataFrames together</h6></li>
				<li>You can also try concatenating along the columns, although that does not make any practical sense for this particular example. However, pandas fills in the unavailable values with <strong class="bold">NaN</strong> for that operation:<pre>df_cat2 = pd.concat([df_1,df_2,df_3], axis=1)
df_cat2</pre></li>
			</ol>
			<div><div><img src="img/C11065_04_35.jpg" alt="Figure 4.35: Output after concatenating the DataFrames" width="1007" height="375"/>
				</div>
			</div>
			<h6>Figure 4.35: Output after concatenating the DataFrames</h6>
			<h3 id="_idParaDest-134"><a id="_idTextAnchor155"/>Exercise 55: Merging by a Common Key</h3>
			<p>Merging by a common key is an extremely common operation for data tables as it allows you to rationalize multiple sources of data in one master database – that is, if they have some common features/keys.</p>
			<p>This is often the first step in building a large database for machine learning tasks where daily incoming data may be put into separate tables. However, at the end of the day, the most recent table needs to be merged with the master data table to be fed into the backend machine learning server, which will then update the model and its prediction capacity.</p>
			<p>Here, we will show a simple example of an inner join with Customer Name as the key:</p>
			<ol>
				<li value="1">One DataFrame, <code>df_1</code>, had shipping information associated with the customer name, and another table, <code>df_2</code>, had the product information tabulated. Our goal is to merge these tables into one DataFrame on the common customer name:<pre>df_1=df[['Ship Date','Ship Mode','Customer Name']][0:4]
df_1</pre><p>The output is as follows:</p><div><img src="img/C11065_04_36.jpg" alt="Figure 4.36: Entries in table df_1" width="457" height="135"/></div><h6>Figure 4.36: Entries in table df_1</h6><p>The second DataFrame is as follows:</p><pre>df_2=df[['Customer Name','Product Name','Quantity']][0:4]
df_2</pre><p>The output is as follows:</p><div><img src="img/C11065_04_37.jpg" alt="Figure 4.37: Entries in table df_2" width="610" height="142"/></div><h6>Figure 4.37: Entries in table df_2</h6></li>
				<li>Join these two tables by inner join by using the following command:<pre>pd.merge(df_1,df_2,on='Customer Name',how='inner')</pre><p>The output is as follows:</p><div><img src="img/C11065_04_38.jpg" alt="Figure 4.38: Inner join on table df_1 and table df_2" width="665" height="153"/></div><h6>Figure 4.38: Inner join on table df_1 and table df_2</h6></li>
				<li>Drop the duplicates by using the following command.<pre>pd.merge(df_1,df_2,on='Customer Name',how='inner').drop_duplicates()</pre><p>The output is as follows:</p><div><img src="img/C11065_04_39.jpg" alt="Figure 4.39: Inner join on table df_1 and table df_2 after dropping the duplicates" width="644" height="148"/></div><h6>Figure 4.39: Inner join on table df_1 and table df_2 after dropping the duplicates</h6></li>
				<li>Extract another small table called <code>df_3</code> to show the concept of an outer join:<pre>df_3=df[['Customer Name','Product Name','Quantity']][2:6]
df_3</pre><p>The output is as follows:</p><div><img src="img/C11065_04_40.jpg" alt="" width="673" height="152"/></div><h6>Figure 4.40: Creating table df_3</h6></li>
				<li>Perform an inner join on <code>df_1</code> and <code>df_3</code> by using the following command:<pre>pd.merge(df_1,df_3,on='Customer Name',how='inner').drop_duplicates()</pre><p>The output is as follows:</p><div><img src="img/C11065_04_41.jpg" alt="Figure 4.41: Merging table df_1 and table df_3 and dropping duplicates" width="686" height="92"/></div><h6>Figure 4.41: Merging table df_1 and table df_3 and dropping duplicates</h6></li>
				<li>Perform an outer join on <code>df_1</code> and <code>df_3</code> by using the following command:<pre>pd.merge(df_1,df_3,on='Customer Name',how='outer').drop_duplicates()</pre><p>The output is as follows:</p></li>
			</ol>
			<div><div><img src="img/C11065_04_42.jpg" alt="Figure 4.42: Outer join on table df_1 and table df_2 and dropping the duplicates" width="751" height="197"/>
				</div>
			</div>
			<h6>Figure 4.42: Outer join on table df_1 and table df_2 and dropping the duplicates</h6>
			<p>Notice how some <code>NaN</code> and <code>NaT</code> values are inserted automatically because no corresponding entries could be found for those records, as those are the entries with unique customer names from their respective tables. <code>NaT</code> represents a Not a Time object, as the objects in the Ship Date column are of the nature of Timestamp objects.</p>
			<h3 id="_idParaDest-135"><a id="_idTextAnchor156"/>Exercise 56: The join Method</h3>
			<p>Joining is performed based on <strong class="keyword">index</strong> <strong class="keyword">keys</strong> and is done by combining the columns of two potentially differently indexed DataFrames into a single one. It offers a faster way to accomplish merging by row indices. This is useful if the records in different tables are indexed differently but represent the same inherent data and you want to merge them into a single table:</p>
			<ol>
				<li value="1">Create the following tables with customer name as the index by using the following command:<pre>df_1=df[['Customer Name','Ship Date','Ship Mode']][0:4]
df_1.set_index(['Customer Name'],inplace=True)
df_1
df_2=df[['Customer Name','Product Name','Quantity']][2:6]
df_2.set_index(['Customer Name'],inplace=True)
df_2</pre><p>The outputs is as follows:</p><div><img src="img/C11065_04_43.jpg" alt="" width="869" height="205"/></div><h6>Figure 4.43: DataFrames df_1 and df_2</h6></li>
				<li>Perform a left join on <code>df_1</code> and <code>df_2</code> by using the following command:<pre>df_1.join(df_2,how='left').drop_duplicates()</pre><p>The output is as follows:</p><div><img src="img/C11065_04_44.jpg" alt="" width="745" height="175"/></div><h6>Figure 4.44: Left join on table df_1 and table df_2 after dropping the duplicates</h6></li>
				<li>Perform a right join on <code>df_1</code> and <code>df_2</code> by using the following command:<pre>df_1.join(df_2,how='right').drop_duplicates()</pre><p>The output is as follows:</p><div><img src="img/C11065_04_45.jpg" alt="" width="663" height="180"/></div><h6>Figure 4.45: Right join on table df_1 and table df_2 after dropping the duplicates</h6></li>
				<li>Perform an inner join on <code>df_1</code> and <code>df_2</code> by using the following command:<pre>df_1.join(df_2,how='inner').drop_duplicates()</pre><p>The output is as follows:</p><div><img src="img/C11065_04_46.jpg" alt="" width="630" height="116"/></div><h6>Figure 4.46: Inner join on table df_1 and table df_2 after dropping the duplicates</h6></li>
				<li>Perform an outer join on <code>df_1</code> and <code>df_2</code> by using the following command:<pre>df_1.join(df_2,how='outer').drop_duplicates()</pre><p>The output is as follows:</p></li>
			</ol>
			<div><div><img src="img/C11065_04_47.jpg" alt="" width="733" height="227"/>
				</div>
			</div>
			<h6>Figure 4.47: Outer join on table df_1 and table df_2 after dropping the duplicates</h6>
			<h2 id="_idParaDest-136"><a id="_idTextAnchor157"/>Useful Methods of Pandas</h2>
			<p>In this topic, we will discuss some small utility functions that are offered by pandas so that we can work efficiently with DataFrames. They don't fall under any particular group of function, so they are mentioned here under the Miscellaneous category.</p>
			<h3 id="_idParaDest-137"><a id="_idTextAnchor158"/>Exercise 57: Randomized Sampling</h3>
			<p>Sampling a random fraction of a big DataFrame is often very useful so that we can  practice other methods on them and test our ideas. If you have a database table of 1 million records, then it is not computationally effective to run your test scripts on the full table.</p>
			<p>However, you may also not want to extract only the first 100 elements as the data may have been sorted by a particular key and you may get an uninteresting table back, which may not represent the full statistical diversity of the parent database.</p>
			<p>In these situations, the <code>sample</code> method comes in super handy so that we can randomly choose a controlled fraction of the DataFrame:</p>
			<ol>
				<li value="1">Specify the number of samples that you require from the DataFrame by using the following command:<pre>df.sample(n=5)</pre><p>The output is as follows:</p><div><img src="img/C11065_04_48.jpg" alt="" width="815" height="398"/></div><h6>Figure 4.48: DataFrame with 5 samples</h6></li>
				<li>Specify a definite fraction (percentage) of data to be sampled by using the following command:<pre>df.sample(frac=0.1)</pre><p>The output is as follows:</p><div><img src="img/C11065_04_49.jpg" alt="" width="828" height="339"/></div><h6>Figure 4.49: DataFrame with 0.1% data sampled</h6><p>You can also choose if sampling is done with replacement, that is, whether the same record can be chosen more than once. The default replace choice is FALSE, that is, no repetition, and sampling will try to choose new elements only.</p></li>
				<li>Choose the sampling by using the following command:<pre>df.sample(frac=0.1, replace=True)</pre><p>The output is as follows:</p></li>
			</ol>
			<div><div><img src="img/C11065_04_50.jpg" alt="" width="864" height="754"/>
				</div>
			</div>
			<h6>Figure 4.50: DataFrame with 0.1% data sampled and repetition enabled</h6>
			<h3 id="_idParaDest-138"><a id="_idTextAnchor159"/>The value_counts Method</h3>
			<p>We discussed the <code>unique</code> method before, which finds and counts the unique records from a DataFrame. Another useful function in a similar vein is <code>value_counts</code>. This function returns an object containing counts of unique values. In the object that is returned, the first element is the most frequently used object. The elements are arranged in descending order.</p>
			<p>Let's consider a practical application of this method to illustrate the utility. Suppose your manager asks you to list the top 10 customers from the big sales database that you have. So, the business question is: which 10 customers' names occur the most frequently in the sales table? You can achieve the same with an SQL query if the data is in a RDBMS, but in pandas, this can be done by using one simple function:</p>
			<pre>df['Customer Name'].value_counts()[:10]</pre>
			<p>The output is as follows:</p>
			<div><div><img src="img/C11065_04_51.jpg" alt="" width="613" height="188"/>
				</div>
			</div>
			<h6>Figure 4.51: List of top 10 customers</h6>
			<p>The <code>value_counts</code> method returns a series of the counts of all unique customer names sorted by the frequency of the count. By asking for only the first 10 elements of that list, this code returns a series of the most frequently occurring top 10 customer names.</p>
			<h3 id="_idParaDest-139"><a id="_idTextAnchor160"/>Pivot Table Functionality</h3>
			<p>Similar to group by, pandas also offer pivot table functionality, which works the same as a pivot table in spreadsheet programs like MS Excel. For example, in this sales database, you want to know the average sales, profit, and quantity sold, by Region and State (two levels of index).</p>
			<p>We can extract this information by using one simple piece of code (we sample 100 records first for keeping the computation fast and then apply the code):</p>
			<pre>df_sample = df.sample(n=100)
df_sample.pivot_table(values=['Sales','Quantity','Profit'],index=['Region','State'],aggfunc='mean')</pre>
			<p>The output is as follows (note that your specific output may be different due to random sampling):</p>
			<div><div><img src="img/C11065_04_52.jpg" alt="" width="852" height="479"/>
				</div>
			</div>
			<h6>Figure 4.52: Sample of 100 records</h6>
			<h3 id="_idParaDest-140">Exercise 58: Sorting by Column Values – the sort_valu<a id="_idTextAnchor161"/>es Method</h3>
			<p>Sorting a table by a particular column is one of the most frequently used operations in the daily work of an analyst. Not surprisingly, pandas provide a simple and intuitive method for sorting called the <code>sort_values</code> method:</p>
			<ol>
				<li value="1">Take a random sample of 15 records and then show how we can sort by the Sales column and then by both the Sales and State columns together:<pre>df_sample=df[['Customer Name','State','Sales','Quantity']].sample(n=15)
df_sample</pre><p>The output is as follows:</p><div><img src="img/C11065_04_53.jpg" alt="Figure 4.53: Sample of 15 records" width="697" height="436"/></div><h6>Figure 4.53: Sample of 15 records</h6></li>
				<li>Sort the values with respect to <code>Sales</code> by using the following command:<pre>df_sample.sort_values(by='Sales')</pre><p>The output is as follows:</p><div><img src="img/C11065_04_54.jpg" alt="Figure 4.54: DataFrame with the Sales value sorted" width="718" height="438"/></div><h6>Figure 4.54: DataFrame with the Sales value sorted</h6></li>
				<li>Sort the values with respect to Sales and State:<pre>df_sample.sort_values(by=['State','Sales'])</pre><p>The output is as follows:</p></li>
			</ol>
			<div><div><img src="img/C11065_04_55.jpg" alt="" width="777" height="437"/>
				</div>
			</div>
			<h6>Figure 4.55: DataFrame sorted with respect to Sales and State</h6>
			<h3 id="_idParaDest-141">Exercise 59: Flexibility for User-Defined Functions with<a id="_idTextAnchor162"/> the apply Method</h3>
			<p>The pandas library provides great flexibility to work with user-defined functions of arbitrary complexity through the <code>apply</code> method. Much like the native Python <code>apply</code> function, this method accepts a user-defined function and additional arguments and returns a new column after applying the function on a particular column element-wise.</p>
			<p>As an example, suppose we want to create a column of categorical features like high/medium/low based on the sales price column. Note that it is a conversion from a numeric value to a categorical factor (string) based on certain conditions (threshold values of sales):</p>
			<ol>
				<li value="1">Create a user-defined function, as follows:<pre>def categorize_sales(price):
    if price &lt; 50:
        return "Low"
    elif price &lt; 200:
        return "Medium"
    else:
        return "High"</pre></li>
				<li>Sample 100 records randomly from the database:<pre>df_sample=df[['Customer Name','State','Sales']].sample(n=100)
df_sample.head(10)</pre><p>The output is as follows:</p><div><img src="img/C11065_04_56.jpg" alt="" width="601" height="312"/></div><h6>Figure 4.56: 100 sample records from the database</h6></li>
				<li>Use the <code>apply</code> method to apply the categorization function onto the <code>Sales</code> column:<h4>Note</h4><pre>df_sample['Sales Price Category']=df_sample['Sales'].apply(categorize_sales)
df_sample.head(10)</pre><p>The output is as follows:</p><div><img src="img/C11065_04_57.jpg" alt="" width="539" height="303"/></div><h6>Figure 4.57: DataFrame with 10 rows after using the apply function on the Sales column</h6></li>
				<li>The <code>apply</code> method also works with the built-in native Python functions. For practice, let's create another column for storing the length of the name of the customer. We can do that using the familiar <code>len</code> function:<pre>df_sample['Customer Name Length']=df_sample['Customer Name'].apply(len)
df_sample.head(10)</pre><p>The output is as follows:</p><div><img src="img/C11065_04_58.jpg" alt="" width="610" height="317"/></div><h6>Figure 4.58: DataFrame with a new column</h6></li>
				<li>Instead of writing out a separate function, we can even insert lambda expressions directly into the apply method for short functions. For example, let's say we are promoting our product and want to show the discounted sales price if the original price is <em class="italics">&gt; $200</em>. We can do this using a <code>lambda</code> function and the <code>apply</code> method:<pre>df_sample['Discounted Price']=df_sample['Sales'].apply(lambda x:0.85*x if x&gt;200 else x)
df_sample.head(10)</pre><p>The output is as follows:</p></li>
			</ol>
			<div><div><img src="img/C11065_04_59.jpg" alt="" width="694" height="304"/>
				</div>
			</div>
			<h6>Figure 4.59: Lambda function</h6>
			<h4>Note</h4>
			<p class="callout">The lambda function contains a conditional, and a discount is applied to those records where the original sales price is &gt; <a id="_idTextAnchor163"/>$200.</p>
			<h3 id="_idParaDest-142"><a id="_idTextAnchor164"/>Activity 6: Working with the Adult Income Dataset (UCI)</h3>
			<p>In this activity, you will work with the Adult Income Dataset from the UCI machine learning portal. The Adult Income dataset has been used in many machine learning papers that address classification problems. You will read the data from a CSV file into a pandas DataFrame and do some practice on the advanced data wrangling you learned about in this chapter.</p>
			<p>The aim of this activity is to practice various advanced pandas DataFrame operations, for example, for subsetting, applying user-defined functions, summary statistics, visualizations, boolean indexing, group by, and outlier detection on a real-life dataset. We have the data downloaded as a CSV file on the disk for your ease. However, it is recommended to practice data downloading on your own so that you are familiar with the process.</p>
			<p>Here is the URL for the dataset: <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/adult/">https://archive.ics.uci.edu/ml/machine-learning-databases/adult/</a>.</p>
			<p>Here is the URL for the description of the dataset and the variables: <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names">https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.names</a>.</p>
			<p>These are the steps that will help you solve this activity:</p>
			<ol>
				<li value="1">Load the necessary libraries.</li>
				<li>Read the adult income dataset from the following URL: <a href="https://github.com/TrainingByPackt/Data-Wrangling-with-Python/blob/master/Lesson04/Activity06/">https://github.com/TrainingByPackt/Data-Wrangling-with-Python/blob/master/Chapter04/Activity06/</a>.</li>
				<li>Create a script that will read a text file line by line.</li>
				<li>Add a name of <code>Income</code> for the response variable to the dataset.</li>
				<li>Find the missing values.</li>
				<li>Create a DataFrame with only age, education, and occupation by using subsetting.</li>
				<li>Plot a histogram of age with a bin size of 20.</li>
				<li>Create a function to strip the whitespace characters.</li>
				<li>Use the <code>apply</code> method to apply this function to all the columns with string values, create a new column, copy the values from this new column to the old column, and drop the new column.</li>
				<li>Find the number of people who are aged between 30 and 50.</li>
				<li>Group the records based on age and education to find how the mean age is distributed.</li>
				<li>Group by occupation and show the summary statistics of age. Find which profession has the oldest workers on average and which profession has its largest share of the workforce above the 75th percentile.</li>
				<li>Use subset and groupby to find outliers.</li>
				<li>Plot the values on a bar chart.</li>
				<li>Merge the data using common keys.<h4>Note</h4><p class="callout">The solution for this activity can be found on page 297.</p></li>
			</ol>
			<h2 id="_idParaDest-143"><a id="_idTextAnchor165"/>Summary</h2>
			<p>In this chapter, we dived deep into the pandas library to learn advanced data wrangling techniques. We started with some advanced subsetting and filtering on DataFrames and round this up by learning about boolean indexing and conditional selection of a subset of data. We also covered how to set and reset the index of a DataFrame, especially while initializing.</p>
			<p>Next, we learned about a particular topic that has a deep connection with traditional relational database systems – the group by method. Then, we dived deep into an important skill for data wrangling - checking for and handling missing data. We showed you how pandas help in handling missing data using various imputation techniques. We also discussed methods for dropping missing values. Furthermore, methods and usage examples of concatenation and merging of DataFrame objects were shown. We saw the join method and how it compares to a similar operation in SQL.</p>
			<p>Lastly, miscellaneous useful methods on DataFrames, such as randomized sampling, <code>unique</code>, <code>value_count</code>, <code>sort_values</code>, and pivot table functionality were covered. We also showed an example of running an arbitrary user-defined function on a DataFrame using the <code>apply</code> method.</p>
			<p>After learning about the basic and advanced data wrangling techniques with NumPy and pandas libraries, the natural question of data acquiring rises. In the next chapter, we will show you how to work with a wide variety of data sources, that is, you will learn how to read data in tabular format in pandas from different sources.</p>
		</div>
	</div>



  </body></html>