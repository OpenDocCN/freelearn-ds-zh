<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch04"/>Chapter 4. Part-of-Speech Tagging</h1></div></div></div><p>In this chapter, we will cover:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Default tagging</li><li class="listitem" style="list-style-type: disc">Training a unigram part-of-speech tagger</li><li class="listitem" style="list-style-type: disc">Combining taggers with backoff tagging</li><li class="listitem" style="list-style-type: disc">Training and combining Ngram taggers</li><li class="listitem" style="list-style-type: disc">Creating a model of likely word tags</li><li class="listitem" style="list-style-type: disc">Tagging with regular expressions</li><li class="listitem" style="list-style-type: disc">Affix tagging</li><li class="listitem" style="list-style-type: disc">Training a Brill tagger</li><li class="listitem" style="list-style-type: disc">Training the TnT tagger</li><li class="listitem" style="list-style-type: disc">Using WordNet for tagging</li><li class="listitem" style="list-style-type: disc">Tagging proper names</li><li class="listitem" style="list-style-type: disc">Classifier-based tagging</li></ul></div><div><div><div><div><h1 class="title"><a id="ch04lvl1sec36"/>Introduction</h1></div></div></div><a id="id207" class="indexterm"/><p>
<strong>Part-of-speech tagging</strong> is the process of converting a sentence, in the form of a list of words, into a list of tuples, where each tuple is of the form <code class="literal">(word, tag)</code>. The <a id="id208" class="indexterm"/>
<strong>tag</strong> is a part-of-speech tag<a id="id209" class="indexterm"/> and signifies whether the word is a noun, adjective, verb, and so on.</p><a id="id210" class="indexterm"/><p>Most of the taggers we will cover are <em>trainable</em>. They use a list of tagged sentences as their training data, such as what you get from the <a id="id211" class="indexterm"/>
<code class="literal">tagged_sents()</code> function of a <code class="literal">TaggedCorpusReader</code> (see the <em>Creating a part-of-speech tagged word corpus</em> recipe in <a class="link" href="ch03.html" title="Chapter 3. Creating Custom Corpora">Chapter 3</a>, <em>Creating Custom Corpora</em> for more details). With these training sentences, the tagger generates an internal model that will tell them how to tag a word. Other taggers use external data sources or match word patterns to choose a tag for a word.</p><p>All taggers in NLTK are in the <a id="id212" class="indexterm"/>
<code class="literal">nltk.tag</code> package and inherit from the <code class="literal">TaggerI</code> base class. <code class="literal">TaggerI</code> requires all subclasses to implement a <code class="literal">tag()</code> <a id="id213" class="indexterm"/>method, which takes a list of words as input, and returns a list of tagged words as output. <code class="literal">TaggerI</code> also provides an <code class="literal">evaluate()</code> method<a id="id214" class="indexterm"/> for evaluating the accuracy of the tagger (covered at the end of the <em>Default tagging</em> recipe). Many taggers can also be combined into a backoff chain, so that if one tagger cannot tag a word, the next tagger is used, and so on.</p><a id="id215" class="indexterm"/><p>Part-of-speech tagging is a necessary step before <em>chunking</em>, which is covered in <a class="link" href="ch05.html" title="Chapter 5. Extracting Chunks">Chapter 5</a>, <em>Extracting Chunks</em>. Without the part-of-speech tags, a chunker cannot know how to extract phrases from a sentence. But with part-of-speech tags, you can tell a chunker how to identify phrases based on tag patterns.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch04lvl1sec37"/>Default tagging</h1></div></div></div><a id="id216" class="indexterm"/><a id="id217" class="indexterm"/><p>Default tagging provides a baseline for part-of-speech tagging. It simply assigns the same part-of-speech tag to every token. We do this using the <code class="literal">DefaultTagger</code>.</p><div><div><div><div><h2 class="title"><a id="ch04lvl2sec123"/>Getting ready</h2></div></div></div><p>We are going to use the <code class="literal">treebank</code> corpus for most of this chapter because it's a common standard and is quick to load and test. But everything we do should apply equally well to <code class="literal">brown</code>, <code class="literal">conll2000</code>, and any other part-of-speech tagged corpus.</p></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec124"/>How to do it...</h2></div></div></div><p>The <code class="literal">DefaultTagger</code> takes a single argument—the tag you want to apply. We will give it <code class="literal">'NN'</code>, which is the tag for a singular noun.</p><div><pre class="programlisting">&gt;&gt;&gt; from nltk.tag import DefaultTagger
&gt;&gt;&gt; tagger = DefaultTagger('NN')
&gt;&gt;&gt; tagger.tag(['Hello', 'World'])
[('Hello', 'NN'), ('World', 'NN')]</pre></div><p>Every tagger has a <code class="literal">tag()</code> method that takes a list of tokens, where each token is a single word. This list of tokens is usually a list of words produced by a word tokenizer (see <a class="link" href="ch01.html" title="Chapter 1. Tokenizing Text and WordNet Basics">Chapter 1</a>, <em>Tokenizing Text and WordNet Basics</em> for more on tokenization). As you can see, <code class="literal">tag()</code> returns a list of tagged tokens, where a <a id="id218" class="indexterm"/>
<strong>tagged token</strong> is a tuple of <code class="literal">(word, tag)</code>.</p></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec125"/>How it works...</h2></div></div></div><a id="id219" class="indexterm"/><a id="id220" class="indexterm"/><p>
<code class="literal">DefaultTagger</code> is a subclass of <code class="literal">SequentialBackoffTagger</code>. Every subclass of <code class="literal">SequentialBackoffTagger</code> must implement the <a id="id221" class="indexterm"/>
<a id="id222" class="indexterm"/>
<code class="literal">choose_tag()</code> method, which takes three arguments:</p><div><ol class="orderedlist arabic"><li class="listitem">The list of <code class="literal">tokens</code>.</li><li class="listitem">The <code class="literal">index</code> of the current token whose tag we want to choose.</li><li class="listitem">The <code class="literal">history</code>, which is a list of the previous tags.</li></ol></div><a id="id223" class="indexterm"/><p>
<code class="literal">SequentialBackoffTagger</code> implements the <code class="literal">tag()</code> method, which calls the <code class="literal">choose_tag()</code> of the subclass for each index in the tokens list, while accumulating a history of the previously tagged tokens. This history is the reason for the <em>Sequential</em> in <code class="literal">SequentialBackoffTagger</code>. We will get to the <em>Backoff</em> portion of the name in the <em>Combining taggers with backoff tagging</em> recipe. The following is a diagram showing the inheritance tree:</p><div><img src="img/3609OS_04_01.jpg" alt="How it works..."/></div><p>The <code class="literal">choose_tag()</code> method of <code class="literal">DefaultTagger</code> is very simple—it returns the tag we gave it at initialization time. It does not care about the current token or the history.</p></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec126"/>There's more...</h2></div></div></div><p>There are a lot of different tags you could give to the <code class="literal">DefaultTagger</code>. You can find a complete list of possible tags for the <code class="literal">treebank</code> corpus at <a class="ulink" href="http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html">http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html</a>. These tags are also documented in <a class="link" href="apa.html" title="Appendix A. Penn Treebank Part-of-Speech Tags">Appendix</a>, <em>Penn Treebank Part-of-Speech Tags</em>.</p><div><div><div><div><h3 class="title"><a id="ch04lvl3sec42"/>Evaluating accuracy</h3></div></div></div><a id="id224" class="indexterm"/><p>To know how accurate a tagger is, you can use the <code class="literal">evaluate()</code> method, which takes a list of tagged tokens as a gold standard to evaluate the tagger. Using our default tagger created earlier, we can evaluate it against a subset of the <code class="literal">treebank</code> corpus tagged sentences.</p><div><pre class="programlisting">&gt;&gt;&gt; from nltk.corpus import treebank
&gt;&gt;&gt; test_sents = treebank.tagged_sents()[3000:]
&gt;&gt;&gt; tagger.evaluate(test_sents)
0.14331966328512843</pre></div><p>So by just choosing <code class="literal">'NN'</code> for every tag, we can achieve 14% accuracy testing on ¼th of the <code class="literal">treebank</code> corpus. We will be reusing these same <code class="literal">test_sents</code> for evaluating more taggers in upcoming recipes.</p></div><div><div><div><div><h3 class="title"><a id="ch04lvl3sec43"/>Batch tagging sentences</h3></div></div></div><a id="id225" class="indexterm"/><p>
<code class="literal">TaggerI</code> also implements a <code class="literal">batch_tag()</code> method that can be used to tag a list of sentences, instead of a single sentence. Here's an example of <a id="id226" class="indexterm"/>tagging two simple sentences:</p><div><pre class="programlisting">&gt;&gt;&gt; tagger.batch_tag([['Hello', 'world', '.'], ['How', 'are', 'you', '?']])
[[('Hello', 'NN'), ('world', 'NN'), ('.', 'NN')], [('How', 'NN'), ('are', 'NN'), ('you', 'NN'), ('?', 'NN')]]</pre></div><p>The result is a list of two tagged sentences, and of course every tag is <code class="literal">NN</code> because we are using the <code class="literal">DefaultTagger</code>. The <code class="literal">batch_tag()</code> method can be quite useful if you have many sentences you wish to tag all at once.</p></div><div><div><div><div><h3 class="title"><a id="ch04lvl3sec44"/>Untagging a tagged sentence</h3></div></div></div><a id="id227" class="indexterm"/><a id="id228" class="indexterm"/><p>Tagged sentences can be untagged using <a id="id229" class="indexterm"/>
<code class="literal">nltk.tag.untag()</code>. Calling this function with a tagged sentence will return a list of words without the tags.</p><div><pre class="programlisting">&gt;&gt;&gt; from nltk.tag import untag
&gt;&gt;&gt; untag([('Hello', 'NN'), ('World', 'NN')])
['Hello', 'World']</pre></div></div></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec127"/>See also</h2></div></div></div><p>For more on tokenization, see <a class="link" href="ch01.html" title="Chapter 1. Tokenizing Text and WordNet Basics">Chapter 1</a>, <em>Tokenizing Text and WordNet Basics</em>. And to learn more about tagged sentences, see the <em>Creating a part-of-speech tagged word corpus</em> recipe in <a class="link" href="ch03.html" title="Chapter 3. Creating Custom Corpora">Chapter 3</a>, <em>Creating Custom Corpora</em>. For a complete list of part-of-speech tags found in the treebank corpus, see <a class="link" href="apa.html" title="Appendix A. Penn Treebank Part-of-Speech Tags">Appendix</a>, <em>Penn Treebank Part-of-Speech Tags</em>.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch04lvl1sec38"/>Training a unigram part-of-speech tagger</h1></div></div></div><a id="id230" class="indexterm"/><p>A <strong>unigram</strong> generally refers to a single token. Therefore, a <em>unigram tagger</em> only uses a single word as its <em>context</em> for determining the part-of-speech tag.</p><a id="id231" class="indexterm"/><p>The <code class="literal">UnigramTagger</code> inherits from <a id="id232" class="indexterm"/>
<code class="literal">NgramTagger</code>, which is a subclass of <a id="id233" class="indexterm"/>
<code class="literal">ContextTagger</code>, which inherits from <code class="literal">SequentialBackoffTagger</code>. In other words, the <code class="literal">UnigramTagger</code> is a <em>context-based tagger</em> whose context is a single word, or unigram.</p><div><div><div><div><h2 class="title"><a id="ch04lvl2sec128"/>How to do it...</h2></div></div></div><a id="id234" class="indexterm"/><a id="id235" class="indexterm"/><p>
<code class="literal">UnigramTagger</code> can be trained by giving it a list of tagged sentences at initialization.</p><div><pre class="programlisting">&gt;&gt;&gt; from nltk.tag import UnigramTagger
&gt;&gt;&gt; from nltk.corpus import treebank
&gt;&gt;&gt; train_sents = treebank.tagged_sents()[:3000]
&gt;&gt;&gt; tagger = UnigramTagger(train_sents)
&gt;&gt;&gt; treebank.sents()[0]
['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov.', '29', '.']
&gt;&gt;&gt; tagger.tag(treebank.sents()[0])
[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]</pre></div><p>We use the first 3,000 tagged sentences of the <code class="literal">treebank</code> corpus as the training set to initialize the <code class="literal">UnigramTagger</code>. Then we see the first sentence as a list of words, and can see how it is transformed by the <code class="literal">tag()</code> function into a list of tagged tokens.</p></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec129"/>How it works...</h2></div></div></div><a id="id236" class="indexterm"/><p>The <code class="literal">UnigramTagger</code> builds a <em>context model</em> from the list of tagged sentences. Because <code class="literal">UnigramTagger</code> inherits from <code class="literal">ContextTagger</code>, instead of providing a <code class="literal">choose_tag()</code> method, it must implement a <code class="literal">context()</code> method, which takes the same three arguments as <code class="literal">choose_tag()</code>. The result of <code class="literal">context()</code> is, in this case, the word token. The context token is used to create the model, and also to look up the best tag once the model is created. Here's an inheritance diagram showing each class, starting at <code class="literal">SequentialBackoffTagger</code>:</p><div><img src="img/3609OS_04_02.jpg" alt="How it works..."/></div><a id="id237" class="indexterm"/><a id="id238" class="indexterm"/><p>Let's see how accurate the <code class="literal">UnigramTagger</code> is on the test sentences (see the previous recipe for how <code class="literal">test_sents</code> is created).</p><div><pre class="programlisting">&gt;&gt;&gt; tagger.evaluate(test_sents)
0.85763004532700193</pre></div><p>It has almost 86% accuracy for a tagger that only uses single word lookup to determine the part-of-speech tag. All accuracy gains from here on will be much smaller.</p></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec130"/>There's more...</h2></div></div></div><p>The model building is actually implemented in <code class="literal">ContextTagger</code>. Given the list of tagged sentences, it calculates the frequency that a tag has occurred for each context. The tag with the highest frequency for a context is stored in the model.</p><div><div><div><div><h3 class="title"><a id="ch04lvl3sec45"/>Overriding the context model</h3></div></div></div><a id="id239" class="indexterm"/><p>All taggers that inherit from <code class="literal">ContextTagger</code> can take a pre-built model instead of training their own. This model is simply a Python <code class="literal">dict</code> mapping a context key to a tag. The context keys will depend on what the <code class="literal">ContextTagger</code> subclass returns from its <code class="literal">context()</code> method. For <code class="literal">UnigramTagger</code>, context keys are individual words. But for other <code class="literal">NgramTagger</code> subclasses, the context keys will be tuples.</p><p>Here's an example where we pass a very simple model to the <code class="literal">UnigramTagger</code> instead of a training set:</p><div><pre class="programlisting">&gt;&gt;&gt; tagger = UnigramTagger(model={'Pierre': 'NN'})
&gt;&gt;&gt; tagger.tag(treebank.sents()[0])
[('Pierre', 'NN'), ('Vinken', None), (',', None), ('61', None), ('years', None), ('old', None), (',', None), ('will', None), ('join', None), ('the', None), ('board', None), ('as', None), ('a', None), ('nonexecutive', None),('director', None), ('Nov.', None), ('29', None), ('.', None)]</pre></div><p>Since the model only contained the context key, <code class="literal">'Pierre'</code>, only the first word got a tag. <a id="id240" class="indexterm"/>Every other word got <code class="literal">None</code> as the tag since the context word was not in the model. So unless you know exactly what you are doing, let the tagger train its own model instead of passing in your own.</p><p>One good case for passing a self-created model to the <code class="literal">UnigramTagger</code> is for when you have a dictionary of words and tags, and you know that every word should always map to its tag. Then, you can put this <code class="literal">UnigramTagger</code> as your first backoff tagger (covered in the next recipe), to look up tags for unambiguous words.</p></div><div><div><div><div><h3 class="title"><a id="ch04lvl3sec46"/>Minimum frequency cutoff</h3></div></div></div><p>The <code class="literal">ContextTagger</code> uses frequency of occurrence to decide which tag is most likely for a given context. By default, it will do this even if the context word and tag occurs only once. If you would like to set a minimum frequency threshold, then you can pass a <code class="literal">cutoff</code> value to the <code class="literal">UnigramTagger</code>.</p><div><pre class="programlisting">&gt;&gt;&gt; tagger = UnigramTagger(train_sents, cutoff=3)
&gt;&gt;&gt; tagger.evaluate(test_sents)
0.775350744657889</pre></div><p>In this case, using <code class="literal">cutoff=3</code> has decreased accuracy, but there may be times when a cutoff is a good idea.</p></div></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec131"/>See also</h2></div></div></div><p>In the next recipe, we will cover backoff tagging to combine taggers. And in the <em>Creating a model of likely word tags</em> recipe, we will learn how to statistically determine tags for very common words.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch04lvl1sec39"/>Combining taggers with backoff tagging</h1></div></div></div><a id="id241" class="indexterm"/><a id="id242" class="indexterm"/><p>
<strong>Backoff tagging</strong> is one of the core features of <code class="literal">SequentialBackoffTagger</code>. It allows you to chain taggers together so that if one tagger doesn't know how to tag a word, it can pass the word on to the next backoff tagger. If that one can't do it, it can pass the word on to the next backoff tagger, and so on until there are no backoff taggers left to check.</p><div><div><div><div><h2 class="title"><a id="ch04lvl2sec132"/>How to do it...</h2></div></div></div><a id="id243" class="indexterm"/><a id="id244" class="indexterm"/><p>Every subclass of <code class="literal">SequentialBackoffTagger</code> can take a <code class="literal">backoff</code> keyword argument whose value is another instance of a <code class="literal">SequentialBackoffTagger</code>. So we will use the <a id="id245" class="indexterm"/>
<code class="literal">DefaultTagger</code> from the <em>Default tagging</em> recipe as the <code class="literal">backoff</code> to the <code class="literal">UnigramTagger</code> from the <em>Training a unigram part-of-speech tagger</em> recipe. Refer to both recipes for details on <code class="literal">train_sents</code> and <code class="literal">test_sents</code>.</p><div><pre class="programlisting">&gt;&gt;&gt; tagger1 = DefaultTagger('NN')
&gt;&gt;&gt; tagger2 = UnigramTagger(train_sents, backoff=tagger1)
&gt;&gt;&gt; tagger2.evaluate(test_sents)
0.87459529462551266</pre></div><p>By using a default tag of <code class="literal">NN</code> whenever the <code class="literal">UnigramTagger</code> is unable to tag a word, we have increased the accuracy by almost 2%!</p></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec133"/>How it works...</h2></div></div></div><a id="id246" class="indexterm"/><p>When a <code class="literal">SequentialBackoffTagger</code> is initialized, it creates an internal list of backoff taggers with itself as the first element. If a <code class="literal">backoff</code> tagger is given, then the backoff tagger's internal list of taggers is appended. Here's some code to illustrate this:</p><div><pre class="programlisting">&gt;&gt;&gt; tagger1._taggers == [tagger1]
True
&gt;&gt;&gt; tagger2._taggers == [tagger2, tagger1]
True</pre></div><p>The <code class="literal">_taggers</code> is the internal list of backoff taggers that the <code class="literal">SequentialBackoffTagger</code> uses when the <code class="literal">tag()</code> method is called. It goes through its list of taggers, calling <code class="literal">choose_tag()</code> on each one. As soon as a tag is found, it stops and returns that tag. This means that if the primary tagger can tag the word, then that's the tag that will be returned. But if it returns <code class="literal">None</code>, then the next tagger is tried, and so on until a tag is found, or else <code class="literal">None</code> is returned. Of course, <code class="literal">None</code> will never be returned if your final backoff tagger is a <code class="literal">DefaultTagger</code>.</p></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec134"/>There's more...</h2></div></div></div><p>While most of the taggers included in NLTK are subclasses of <code class="literal">SequentialBackoffTagger</code>, not all of them are. There's a few taggers that we will cover in later recipes that cannot be used as part of a backoff tagging chain, such as the <code class="literal">BrillTagger</code>. However, these taggers generally take another tagger to use as a baseline, and a <code class="literal">SequentialBackoffTagger</code> is often a good choice for that baseline.</p><div><div><div><div><h3 class="title"><a id="ch04lvl3sec47"/>Pickling and unpickling a trained tagger</h3></div></div></div><a id="id247" class="indexterm"/><a id="id248" class="indexterm"/><p>Since training a tagger can take a while, and you generally only need to do the training once, pickling a trained tagger is a useful way to save it for later usage. If your trained tagger is called <code class="literal">tagger</code>, then here's how to dump and load it with <code class="literal">pickle</code>:</p><div><pre class="programlisting">&gt;&gt;&gt; import pickle
&gt;&gt;&gt; f = open('tagger.pickle', 'w')
&gt;&gt;&gt; pickle.dump(tagger, f)
&gt;&gt;&gt; f.close()
&gt;&gt;&gt; f = open('tagger.pickle', 'r')
&gt;&gt;&gt; tagger = pickle.load(f)</pre></div><p>If your tagger pickle file is located in a NLTK data directory, you could also use <code class="literal">nltk.data.load('tagger.pickle')</code> to load the tagger.</p></div></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec135"/>See also</h2></div></div></div><p>In the next recipe, we will combine more taggers with backoff tagging. Also see the previous two recipes for details on the <code class="literal">DefaultTagger</code> and <code class="literal">UnigramTagger</code>.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch04lvl1sec40"/>Training and combining Ngram taggers</h1></div></div></div><p>In addition to <code class="literal">UnigramTagger</code>, there are two more <a id="id249" class="indexterm"/>
<code class="literal">NgramTagger</code> subclasses: <code class="literal">BigramTagger</code>
<a id="id250" class="indexterm"/> and <a id="id251" class="indexterm"/>
<code class="literal">TrigramTagger</code>
<a id="id252" class="indexterm"/>. <code class="literal">BigramTagger</code> uses the previous tag as part of its context, while <code class="literal">TrigramTagger</code> uses the previous two tags. <a id="id253" class="indexterm"/>An <strong>ngram</strong> is a subsequence of <em>n</em> items, so the <code class="literal">BigramTagger</code> looks at two items (the previous tag and word), and the <code class="literal">TrigramTagger</code> looks at three items.</p><a id="id254" class="indexterm"/><a id="id255" class="indexterm"/><p>These two taggers are good at handling words whose part-of-speech tag is context dependent. Many words have a different part-of-speech depending on how they are used. For example, we have been talking about taggers that "tag" words. In this case, "tag" is used as a verb. But the result of tagging is a part-of-speech tag, so "tag" can also be a noun. The idea with the <code class="literal">NgramTagger</code> subclasses is that by looking at the previous words and part-of-speech tags, we can better guess the part-of-speech tag for the current word.</p><div><div><div><div><h2 class="title"><a id="ch04lvl2sec136"/>Getting ready</h2></div></div></div><p>Refer to the first two recipes of this chapter for details on constructing <code class="literal">train_sents</code> and <code class="literal">test_sents</code>.</p></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec137"/>How to do it...</h2></div></div></div><a id="id256" class="indexterm"/><a id="id257" class="indexterm"/><a id="id258" class="indexterm"/><a id="id259" class="indexterm"/><p>By themselves, <code class="literal">BigramTagger</code> and <code class="literal">TrigramTagger</code> perform quite poorly. This is partly because they cannot learn context from the first word(s) in a sentence.</p><div><pre class="programlisting">&gt;&gt;&gt; from nltk.tag import BigramTagger, TrigramTagger
&gt;&gt;&gt; bitagger = BigramTagger(train_sents)
&gt;&gt;&gt; bitagger.evaluate(test_sents)
0.11336067342974315
&gt;&gt;&gt; tritagger = TrigramTagger(train_sents)
&gt;&gt;&gt; tritagger.evaluate(test_sents)
0.0688107058061731</pre></div><p>Where they can make a contribution is when we combine them with backoff tagging. This time, instead of creating each tagger individually, we will create a function that will take <code class="literal">train_sents</code>, a list of <code class="literal">SequentialBackoffTagger</code> classes, and an optional final backoff tagger, and then train each tagger with the previous tagger as a backoff. Here's code from <code class="literal">tag_util.py</code>:</p><div><pre class="programlisting">def backoff_tagger(train_sents, tagger_classes, backoff=None):
  for cls in tagger_classes:
    backoff = cls(train_sents, backoff=backoff)
  return backoff</pre></div><p>And to use it, we can do the following:</p><div><pre class="programlisting">&gt;&gt;&gt; from tag_util import backoff_tagger
&gt;&gt;&gt; backoff = DefaultTagger('NN')
&gt;&gt;&gt; tagger = backoff_tagger(train_sents, [UnigramTagger, BigramTagger, TrigramTagger], backoff=backoff)
&gt;&gt;&gt; tagger.evaluate(test_sents)
0.88163177206993304</pre></div><p>So we have gained almost 1% accuracy by including the <code class="literal">BigramTagger</code> and <code class="literal">TrigramTagger</code> in the backoff chain. For corpora other than <code class="literal">treebank</code>, the accuracy gain may be more significant.</p></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec138"/>How it works...</h2></div></div></div><a id="id260" class="indexterm"/><p>The <code class="literal">backoff_tagger</code> function creates an instance of each tagger class in the list, giving it the <code class="literal">train_sents</code> and the previous tagger as a backoff. The order of the list of tagger classes is quite important—the first class in the list will be trained first, and be given the initial backoff tagger. This tagger will then become the backoff tagger for the next tagger class in the list. The final tagger returned will be an instance of the last tagger class in the list. Here's some code to clarify this chain:</p><div><pre class="programlisting">&gt;&gt;&gt; tagger._taggers[-1] == backoff
True
&gt;&gt;&gt; isinstance(tagger._taggers[0], TrigramTagger)
True
&gt;&gt;&gt; isinstance(tagger._taggers[1], BigramTagger)
True</pre></div><a id="id261" class="indexterm"/><a id="id262" class="indexterm"/><a id="id263" class="indexterm"/><a id="id264" class="indexterm"/><p>So we end up with a <code class="literal">TrigramTagger</code>, whose first backoff is a <code class="literal">BigramTagger</code>. Then the next backoff will be a <code class="literal">UnigramTagger</code>, whose backoff is the <code class="literal">DefaultTagger</code>.</p></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec139"/>There's more...</h2></div></div></div><p>The <code class="literal">backoff_tagger</code> function doesn't just work with <code class="literal">NgramTagger</code> classes. It can be used for constructing a chain containing any subclasses of <code class="literal">SequentialBackoffTagger</code>.</p><p>
<code class="literal">BigramTagger</code> and <code class="literal">TrigramTagger</code>, because they are subclasses of <code class="literal">NgramTagger</code> and <code class="literal">ContextTagger</code>, can also take a model and cutoff argument, just like the <code class="literal">UnigramTagger</code>. But unlike for <code class="literal">UnigramTagger</code>, the context keys of the model must be 2-tuples, where the first element is a section of the history, and the second element is the current token. For the <code class="literal">BigramTagger</code>, an appropriate context key looks like <code class="literal">((prevtag,), word)</code>, and for <code class="literal">TrigramTagger</code> it looks like <code class="literal">((prevtag1, prevtag2), word)</code>.</p><div><div><div><div><h3 class="title"><a id="ch04lvl3sec48"/>Quadgram Tagger</h3></div></div></div><a id="id265" class="indexterm"/><a id="id266" class="indexterm"/><p>The <code class="literal">NgramTagger</code> class can be used by itself to create a tagger that uses  Ngrams longer than three for its context key.</p><div><pre class="programlisting">&gt;&gt;&gt; from nltk.tag import NgramTagger
&gt;&gt;&gt; quadtagger = NgramTagger(4, train_sents)
&gt;&gt;&gt; quadtagger.evaluate(test_sents)
0.058191236779624435</pre></div><p>It's even worse than the <code class="literal">TrigramTagger</code>! Here's an alternative implementation of a <code class="literal">QuadgramTagger</code> that we can include in a list to <code class="literal">backoff_tagger</code>. This code can be found in <code class="literal">taggers.py</code>:</p><div><pre class="programlisting">from nltk.tag import NgramTagger

class QuadgramTagger(NgramTagger):
  def __init__(self, *args, **kwargs):
    NgramTagger.__init__(self, 4, *args, **kwargs)</pre></div><p>This is essentially how <code class="literal">BigramTagger</code> and <code class="literal">TrigramTagger</code> are implemented; simple subclasses of <code class="literal">NgramTagger</code> that pass in the number of <em>ngrams</em> to look at in the <code class="literal">history</code> argument of the <code class="literal">context()</code> method.</p><a id="id267" class="indexterm"/><a id="id268" class="indexterm"/><p>Now let's see how it does as part of a backoff chain:</p><div><pre class="programlisting">&gt;&gt;&gt; from taggers import QuadgramTagger
&gt;&gt;&gt; quadtagger = backoff_tagger(train_sents, [UnigramTagger, BigramTagger, TrigramTagger, QuadgramTagger], backoff=backoff)
&gt;&gt;&gt; quadtagger.evaluate(test_sents)
0.88111374919058927</pre></div><p>It's actually slightly worse than before when we stopped with the <code class="literal">TrigramTagger</code>. So the lesson is that too much context can have a negative effect on accuracy.</p></div></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec140"/>See also</h2></div></div></div><p>The previous two recipes cover the <code class="literal">UnigramTagger</code> and backoff tagging.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch04lvl1sec41"/>Creating a model of likely word tags</h1></div></div></div><p>As mentioned earlier in this chapter in the <em>Training a unigram part-of-speech tagger</em> recipe, using a custom model with a <code class="literal">UnigramTagger</code> should only be done if you know exactly what you are doing. In this recipe, we are going to create a model for the most common words, most of which always have the same tag no matter what.</p><div><div><div><div><h2 class="title"><a id="ch04lvl2sec141"/>How to do it...</h2></div></div></div><a id="id269" class="indexterm"/><a id="id270" class="indexterm"/><p>To find the most common words, we can use <code class="literal">nltk.probability.FreqDist</code> to count word frequencies in the <code class="literal">treebank</code> corpus. Then, we can create a <code class="literal">ConditionalFreqDist</code> for tagged words, where we count the frequency of every tag for every word. Using these counts, we can construct a model of the 200 most frequent words as keys, with the most frequent tag for each word as a value. Here's the model creation function defined in <code class="literal">tag_util.py</code>:</p><div><pre class="programlisting">from nltk.probability import FreqDist, ConditionalFreqDist

def word_tag_model(words, tagged_words, limit=200):
  fd = FreqDist(words)
  most_freq = fd.keys()[:limit]
  cfd = ConditionalFreqDist(tagged_words)
  return dict((word, cfd[word].max()) for word in most_freq)</pre></div><p>And to use it with a <code class="literal">UnigramTagger</code>, we can do the following:</p><div><pre class="programlisting">&gt;&gt;&gt; from tag_util import word_tag_model
&gt;&gt;&gt; from nltk.corpus import treebank
&gt;&gt;&gt; model = word_tag_model(treebank.words(), treebank.tagged_words())
&gt;&gt;&gt; tagger = UnigramTagger(model=model)
&gt;&gt;&gt; tagger.evaluate(test_sents)
0.55972372113101665</pre></div><p>An accuracy of almost 56% is ok, but nowhere near as good as the trained <code class="literal">UnigramTagger</code>. Let's try adding it to our backoff chain:</p><div><pre class="programlisting">&gt;&gt;&gt; default_tagger = DefaultTagger('NN')
&gt;&gt;&gt; likely_tagger = UnigramTagger(model=model, backoff=default_tagger)
&gt;&gt;&gt; tagger = backoff_tagger(train_sents, [UnigramTagger, BigramTagger, TrigramTagger], backoff=likely_tagger)
&gt;&gt;&gt; tagger.evaluate(test_sents)
0.88163177206993304</pre></div><p>The final accuracy is exactly the same as without the <code class="literal">likely_tagger</code>. This is because the frequency calculations we did to create the model are almost exactly what happens when we train a <code class="literal">UnigramTagger</code>.</p></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec142"/>How it works...</h2></div></div></div><a id="id271" class="indexterm"/><p>The <code class="literal">word_tag_model()</code> function takes a list of all words, a list of all tagged words, and the maximum number of words we want to use for our model. We give the list of words to a <code class="literal">FreqDist</code>, which counts the frequency of each word. Then we get the top 200 words from the <code class="literal">FreqDist</code> by calling <code class="literal">fd.keys()</code>, which returns all words ordered by highest frequency to lowest. We give the list of tagged words to a <code class="literal">ConditionalFreqDist</code>, which creates a <code class="literal">FreqDist</code> of tags for each word, with the word as the <em>condition</em>. Finally, we return a <code class="literal">dict</code> of the top 200 words mapped to their most likely tag.</p></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec143"/>There's more...</h2></div></div></div><p>It may seem useless to include this tagger as it does not change the accuracy. But the point of this recipe is to demonstrate how to construct a useful model for a <code class="literal">UnigramTagger</code>. Custom model construction is a way to create a manual override of trained taggers that are otherwise black boxes. And by putting the likely tagger in the front of the chain, we can actually improve accuracy a little bit:</p><div><pre class="programlisting">&gt;&gt;&gt; tagger = backoff_tagger(train_sents, [UnigramTagger, BigramTagger, TrigramTagger], backoff=default_tagger)
&gt;&gt;&gt; likely_tagger = UnigramTagger(model=model, backoff=tagger)
&gt;&gt;&gt; likely_tagger.evaluate(test_sents)
0.88245197496222749</pre></div><p>Putting custom model taggers at the front of the backoff chain gives you complete control over how specific words are tagged, while letting the trained taggers handle everything else.</p></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec144"/>See also</h2></div></div></div><p>The <em>Training a unigram part-of-speech tagger</em> recipe has details on the <code class="literal">UnigramTagger</code> and a simple custom model example. See the earlier recipes <em>Combining taggers with backoff tagging</em> and <em>Training and combining Ngram taggers</em> for details on backoff tagging.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch04lvl1sec42"/>Tagging with regular expressions</h1></div></div></div><a id="id272" class="indexterm"/><a id="id273" class="indexterm"/><p>You can use regular expression matching to tag words. For example, you can match numbers with <code class="literal">\d</code> to assign the tag <strong>CD</strong> (which refers to a <strong>Cardinal number</strong>). Or you could match on known word patterns, such as the suffix "ing". There's lot of flexibility here, but be careful of over-specifying since language is naturally inexact, and there are always exceptions to the rule.</p><div><div><div><div><h2 class="title"><a id="ch04lvl2sec145"/>Getting ready</h2></div></div></div><p>For this recipe to make sense, you should be familiar with regular expression syntax and Python's <code class="literal">re</code> module.</p></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec146"/>How to do it...</h2></div></div></div><p>The <code class="literal">RegexpTagger</code>
<a id="id274" class="indexterm"/>
<a id="id275" class="indexterm"/> expects a list of 2-tuples, where the first element in the tuple is a regular expression, and the second element is the tag. The following patterns can be found in <code class="literal">tag_util.py</code>:</p><div><pre class="programlisting">patterns = [
  (r'^\d+$', 'CD'),
  (r'.*ing$', 'VBG'), # gerunds, i.e. wondering
  (r'.*ment$', 'NN'), # i.e. wonderment
  (r'.*ful$', 'JJ') # i.e. wonderful
]</pre></div><p>Once you have constructed this list of patterns, you can pass it into <code class="literal">RegexpTagger</code>.</p><div><pre class="programlisting">&gt;&gt;&gt; from tag_util import patterns
&gt;&gt;&gt; from nltk.tag import RegexpTagger
&gt;&gt;&gt; tagger = RegexpTagger(patterns)
&gt;&gt;&gt; tagger.evaluate(test_sents)
0.037470321605870924</pre></div><p>So it's not too great with just a few patterns, but since <code class="literal">RegexpTagger</code> is a subclass of <code class="literal">SequentialBackoffTagger</code>, it can be useful as part of a backoff chain, especially if you are able to come up with more word patterns.</p></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec147"/>How it works...</h2></div></div></div><a id="id276" class="indexterm"/><a id="id277" class="indexterm"/><a id="id278" class="indexterm"/><p>The <code class="literal">RegexpTagger</code> saves the <code class="literal">patterns</code> given at initialization, then on each call to <code class="literal">choose_tag()</code>, it iterates over the patterns and returns the tag for the first expression that matches the current word using <code class="literal">re.match()</code>. This means that if you have two expressions that could match, the tag of the first one will always be returned, and the second expression won't even be tried.</p></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec148"/>There's more...</h2></div></div></div><p>The <code class="literal">RegexpTagger</code> can replace the <code class="literal">DefaultTagger</code> if you give it a pattern such as <code class="literal">(r'.*', 'NN')</code>. This pattern should, of course, be last in the list of patterns, otherwise no other patterns will match.</p></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec149"/>See also</h2></div></div></div><p>In the next recipe, we will cover the <code class="literal">AffixTagger</code>, which learns how to tag based on prefixes and suffixes of words. And see the <em>Default tagging</em> recipe for details on the <code class="literal">DefaultTagger</code>.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch04lvl1sec43"/>Affix tagging</h1></div></div></div><a id="id279" class="indexterm"/><a id="id280" class="indexterm"/><p>The <code class="literal">AffixTagger</code> is another <code class="literal">ContextTagger</code> subclass, but this time the <em>context</em> is either the <em>prefix</em> or the <em>suffix</em> of a word. This means the <code class="literal">AffixTagger</code> is able to learn tags based on fixed-length substrings of the beginning or ending of a word.</p><div><div><div><div><h2 class="title"><a id="ch04lvl2sec150"/>How to do it...</h2></div></div></div><p>The default arguments for an <code class="literal">AffixTagger</code> specify three-character suffixes, and that words must be at least five characters long. If a word is less than five characters long, then <code class="literal">None</code> is returned as the tag.</p><div><pre class="programlisting">&gt;&gt;&gt; from nltk.tag import AffixTagger
&gt;&gt;&gt; tagger = AffixTagger(train_sents)
&gt;&gt;&gt; tagger.evaluate(test_sents)
0.27528599179797109</pre></div><p>So it does ok by itself with the default arguments. Let's try it by specifying three-character prefixes:</p><div><pre class="programlisting">&gt;&gt;&gt; prefix_tagger = AffixTagger(train_sents, affix_length=3)
&gt;&gt;&gt; prefix_tagger.evaluate(test_sents)
0.23682279300669112</pre></div><p>To learn on two-character suffixes, the code looks like this:</p><div><pre class="programlisting">&gt;&gt;&gt; suffix_tagger = AffixTagger(train_sents, affix_length=-2)
&gt;&gt;&gt; suffix_tagger.evaluate(test_sents)
0.31953377940859057</pre></div></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec151"/>How it works...</h2></div></div></div><a id="id281" class="indexterm"/><p>A positive value for <code class="literal">affix_length</code> means that the <code class="literal">AffixTagger</code> will learn word prefixes, essentially <code class="literal">word[:affix_length]</code>. If the <code class="literal">affix_length</code> is negative, then suffixes are learned using <code class="literal">word[affix_length:]</code>.</p></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec152"/>There's more...</h2></div></div></div><p>You can combine multiple affix taggers in a backoff chain if you want to learn about multiple character length affixes. Here's an example of four <code class="literal">AffixTagger</code> classes learning about two and three-character prefixes and suffixes:</p><div><pre class="programlisting">&gt;&gt;&gt; pre3_tagger = AffixTagger(train_sents, affix_length=3)
&gt;&gt;&gt; pre3_tagger.evaluate(test_sents)
0.23682279300669112
&gt;&gt;&gt; pre2_tagger = AffixTagger(train_sents, affix_length=2, backoff=pre3_tagger)
&gt;&gt;&gt; pre2_tagger.evaluate(test_sents)
0.29816533563565722
&gt;&gt;&gt; suf2_tagger = AffixTagger(train_sents, affix_length=-2, backoff=pre2_tagger)
&gt;&gt;&gt; suf2_tagger.evaluate(test_sents)
0.32523203108137277
&gt;&gt;&gt; suf3_tagger = AffixTagger(train_sents, affix_length=-3, backoff=suf2_tagger)
&gt;&gt;&gt; suf3_tagger.evaluate(test_sents)
0.35924886682495144</pre></div><p>As you can see, the accuracy goes up each time.</p><div><div><h3 class="title"><a id="note11"/>Note</h3><p>The preceding ordering is not the best, nor is it the worst. I will leave it to you to explore the possibilities and discover the best backoff chain of <code class="literal">AffixTagger</code> and <code class="literal">affix_length</code> values.</p></div></div><div><div><div><div><h3 class="title"><a id="ch04lvl3sec49"/>Min stem length</h3></div></div></div><p>
<code class="literal">AffixTagger</code> also takes a <code class="literal">min_stem_length</code> keyword argument<a id="id282" class="indexterm"/> with a default value of <code class="literal">2</code>. If the word length is less than <code class="literal">min_stem_length</code> plus the absolute value of <code class="literal">affix_length</code>, then <code class="literal">None</code> is returned by the <code class="literal">context()</code> method. Increasing <code class="literal">min_stem_length</code> forces the <code class="literal">AffixTagger</code> to only learn on longer words, while decreasing <code class="literal">min_stem_length</code> will allow it to learn on shorter words. Of course, for shorter words, the <code class="literal">affix_length</code> could be equal to or greater than the word length, and <code class="literal">AffixTagger</code> would essentially be acting like a <code class="literal">UnigramTagger</code>.</p></div></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec153"/>See also</h2></div></div></div><p>You can manually specify prefixes and suffixes using regular expressions, as shown in the previous recipe. The <em>Training a unigram part-of-speech tagger</em> and <em>Training and combining Ngram taggers</em> recipes have details on <code class="literal">NgramTagger</code> subclasses, which are also subclasses of <code class="literal">ContextTagger</code>.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch04lvl1sec44"/>Training a Brill tagger</h1></div></div></div><a id="id283" class="indexterm"/><a id="id284" class="indexterm"/><a id="id285" class="indexterm"/><p>The <code class="literal">BrillTagger</code> is a transformation-based tagger. It is the first tagger that is not a subclass of <code class="literal">SequentialBackoffTagger</code>. Instead, the <code class="literal">BrillTagger</code> uses a series of rules to correct the results of an <em>initial tagger</em>. These rules are scored based on how many errors they correct minus the number of new errors they produce.</p><div><div><div><div><h2 class="title"><a id="ch04lvl2sec154"/>How to do it...</h2></div></div></div><p>Here's a function from <code class="literal">tag_util.py</code> that trains a <code class="literal">BrillTagger</code> using <code class="literal">FastBrillTaggerTrainer</code>. It requires an <code class="literal">initial_tagger</code> and <code class="literal">train_sents</code>.</p><div><pre class="programlisting">from nltk.tag import brill

def train_brill_tagger(initial_tagger, train_sents, **kwargs):
  sym_bounds = [(1,1), (2,2), (1,2), (1,3)]
  asym_bounds = [(-1,-1), (1,1)]
  templates = [
  brill.SymmetricProximateTokensTemplate(brill.ProximateTagsRule, *sym_bounds),
    brill.SymmetricProximateTokensTemplate(brill.ProximateWordsRule, *sym_bounds),
    brill.ProximateTokensTemplate(brill.ProximateTagsRule, *asym_bounds),
    brill.ProximateTokensTemplate(brill.ProximateWordsRule, *asym_bounds)
  ]
  trainer = brill.FastBrillTaggerTrainer(initial_tagger, templates, deterministic=True)
  return trainer.train(train_sents, **kwargs)</pre></div><p>To use it, we can create our <code class="literal">initial_tagger</code> from a backoff chain of <code class="literal">NgramTagger</code> classes, then pass that into the <code class="literal">train_brill_tagger()</code> function to get a <code class="literal">BrillTagger</code> back.</p><div><pre class="programlisting">&gt;&gt;&gt; default_tagger = DefaultTagger('NN')
&gt;&gt;&gt; initial_tagger = backoff_tagger(train_sents, [UnigramTagger, BigramTagger, TrigramTagger], backoff=default_tagger)
&gt;&gt;&gt; initial_tagger.evaluate(test_sents)
0.88163177206993304
&gt;&gt;&gt; from tag_util import train_brill_tagger
&gt;&gt;&gt; brill_tagger = train_brill_tagger(initial_tagger, train_sents)
&gt;&gt;&gt; brill_tagger.evaluate(test_sents)
0.88327217785452194</pre></div><p>So the <code class="literal">BrillTagger</code> has slightly increased accuracy over the <code class="literal">initial_tagger</code>.</p></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec155"/>How it works...</h2></div></div></div><a id="id286" class="indexterm"/><a id="id287" class="indexterm"/><a id="id288" class="indexterm"/><p>The <code class="literal">FastBrillTaggerTrainer</code> takes an <code class="literal">initial_tagger</code> and a list of <code class="literal">templates</code>. These templates must implement the <code class="literal">BrillTemplateI</code> interface. The two template implementations included with NLTK are <code class="literal">ProximateTokensTemplate</code> and <code class="literal">SymmetricProximateTokensTemplate</code>. Each template is used to generate a list of <code class="literal">BrillRule</code> subclasses. The actual class of the rules produced is passed in to the template at initialization. The basic workflow looks like this:</p><div><img src="img/3609OS_04_03.jpg" alt="How it works..."/></div><p>The two <code class="literal">BrillRule</code> subclasses used are <code class="literal">ProximateTagsRule</code> and <code class="literal">ProximateWordsRule</code>, which are both subclasses of <code class="literal">ProximateTokensRule</code>. <code class="literal">ProximateTagsRule</code> looks at surrounding tags to do error correction, and <code class="literal">ProximateWordsRule</code> looks at the surrounding words.</p><p>The <em>bounds</em> that we pass in to each template are lists of <code class="literal">(start, end)</code> tuples that get passed in to each rule as <em>conditions</em>. The conditions tell the rule which tokens it can look at. For example, if the condition is <code class="literal">(1, 1)</code>, then the rule will only look at the next token. But if the condition is <code class="literal">(1, 2)</code>, then the rule will look at both the next token and the token after it. For <code class="literal">(-1, -1)</code> the rule will look only at the previous token.</p><p>
<code class="literal">ProximateTokensTemplate</code> produces <code class="literal">ProximateTokensRule</code> that look at each token for its given conditions to do error correction. Positive and negative conditions must be explicitly specified. <code class="literal">SymmetricProximateTokensTemplate</code>, on the other hand, produces pairs of <code class="literal">ProximateTokensRule</code>, where one rule uses the given conditions, and the other rule uses the negative of the conditions. So when we pass a list of positive <code class="literal">(start, end)</code> tuples to a <code class="literal">SymmetricProximateTokensTemplate</code>, it will also produce a <code class="literal">ProximateTokensRule</code> that uses <code class="literal">(-start, -end</code>). This is why it's <em>symmetric</em>—it produces rules that look on both sides of the token.</p><div><div><h3 class="title"><a id="note12"/>Note</h3><p>Unlike with <code class="literal">ProximateTokensTemplate</code>, you should not give negative bounds to <code class="literal">SymmetricProximateTokensTemplate</code>, since it will produce those itself. Only use positive number bounds with <code class="literal">SymmetricProximateTokensTemplate</code>.</p></div></div></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec156"/>There's more...</h2></div></div></div><p>You can control the number of rules generated using the <code class="literal">max_rules</code> keyword argument to the <code class="literal">FastBrillTaggerTrainer.train()</code> method. The default value is <code class="literal">200</code>. You can also control the quality of rules used with the <code class="literal">min_score</code> keyword argument. The default value is <code class="literal">2</code>, though <code class="literal">3</code> can be a good choice as well.</p><div><div><h3 class="title"><a id="note13"/>Note</h3><p>Increasing <code class="literal">max_rules</code> or <code class="literal">min_score</code> will greatly increase training time, without necessarily increasing accuracy. Change these values with care.</p></div></div><div><div><div><div><h3 class="title"><a id="ch04lvl3sec50"/>Tracing</h3></div></div></div><p>You can watch the <code class="literal">FastBrillTaggerTrainer</code> do its work by passing <code class="literal">trace=1</code> into the constructor. This can give you output such as:</p><div><pre class="programlisting">Training Brill tagger on 3000 sentences...

    Finding initial useful rules...

        Found 10709 useful rules.

    Selecting rules...</pre></div><p>This means it found <code class="literal">10709</code> rules with a score of at least <code class="literal">min_score</code>, and then it selects the best rules, keeping no more than <code class="literal">max_rules</code>.</p><p>The default is <code class="literal">trace=0</code>, which means the trainer will work silently without printing its status.</p></div></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec157"/>See also</h2></div></div></div><p>The <em>Training and combining Ngram taggers</em> recipe details the construction of the <code class="literal">initial_tagger</code> used previously, and the <em>Default tagging</em> recipe explains the <code class="literal">default_tagger</code>.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch04lvl1sec45"/>Training the TnT tagger</h1></div></div></div><a id="id289" class="indexterm"/><p>
<strong>TnT</strong> stands for <strong>Trigrams'n'Tags</strong>. It is a statistical tagger based on second order Markov models. You can read the original paper that lead to the implementation at <a class="ulink" href="http://acl.ldc.upenn.edu/A/A00/A00-1031.pdf">http://acl.ldc.upenn.edu/A/A00/A00-1031.pdf</a>.</p><div><div><div><div><h2 class="title"><a id="ch04lvl2sec158"/>How to do it...</h2></div></div></div><a id="id290" class="indexterm"/><p>The <code class="literal">TnT</code> tagger has a slightly different API than previous taggers we have encountered. You must explicitly call the <code class="literal">train()</code> method after you have created it. Here's a basic example:</p><div><pre class="programlisting">&gt;&gt;&gt; from nltk.tag import tnt
&gt;&gt;&gt; tnt_tagger = tnt.TnT()
&gt;&gt;&gt; tnt_tagger.train(train_sents)
&gt;&gt;&gt; tnt_tagger.evaluate(test_sents)
0.87580401467731495</pre></div><p>It's quite a good tagger all by itself, only slightly less accurate than the <code class="literal">BrillTagger</code> from the previous recipe. But if you do not call <code class="literal">train()</code> before <code class="literal">evaluate()</code>, you will get an accuracy of 0%.</p></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec159"/>How it works...</h2></div></div></div><p>
<code class="literal">TnT</code> maintains a number of internal <code class="literal">FreqDist</code> and <code class="literal">ConditionalFreqDist</code> instances based on the training data. These frequency distributions count unigrams, bigrams, and trigrams. Then, during tagging, the frequencies are used to calculate the probabilities of possible tags for each word. So instead of constructing a backoff chain of <code class="literal">NgramTagger</code> subclasses, the <code class="literal">TnT</code> tagger uses all the ngram models together to choose the best tag. It also tries to guess the tags for the whole sentence at once, by choosing the most likely model for the entire sentence, based on the probabilities of each possible tag.</p><div><div><h3 class="title"><a id="note14"/>Note</h3><p>Training is fairly quick, but tagging is significantly slower than the other taggers we have covered. This is due to all the floating point math that must be done to calculate the tag probabilities of each word.</p></div></div></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec160"/>There's more...</h2></div></div></div><a id="id291" class="indexterm"/><a id="id292" class="indexterm"/><p>
<code class="literal">TnT</code> accepts a few optional keyword arguments. You can pass in a tagger for unknown words as <code class="literal">unk</code>. If this tagger is already trained, then you must also pass in <code class="literal">Trained=True</code>. Otherwise it will call <code class="literal">unk.train(data)</code> with the same data you pass in to the <code class="literal">train()</code> method. Since none of the previous taggers have a public <code class="literal">train()</code> method, we recommend always passing <code class="literal">Trained=True</code> if you also pass an <code class="literal">unk</code> tagger. Here's an example using a <code class="literal">DefaultTagger</code>, which does not require any training:</p><div><pre class="programlisting">&gt;&gt;&gt; from nltk.tag import DefaultTagger
&gt;&gt;&gt; unk = DefaultTagger('NN')
&gt;&gt;&gt; tnt_tagger = tnt.TnT(unk=unk, Trained=True)
&gt;&gt;&gt; tnt_tagger.train(train_sents)
&gt;&gt;&gt; tnt_tagger.evaluate(test_sents)
0.89272609540254699</pre></div><p>So we got an almost 2% increase in accuracy! You must use a tagger that can tag a single word without having seen that word before. This is because the unknown tagger's <code class="literal">tag()</code> method is only called with a single word sentence. Other good candidates for an unknown tagger are <code class="literal">RegexpTagger</code> or <code class="literal">AffixTagger</code>. Passing in a <code class="literal">UnigramTagger</code> that's been trained on the same data is pretty much useless, as it will have seen the exact same words, and therefore have the same unknown word blind spots.</p><div><div><div><div><h3 class="title"><a id="ch04lvl3sec51"/>Controlling the beam search</h3></div></div></div><p>Another parameter you can modify for <code class="literal">TnT</code> is <code class="literal">N</code>, which controls the number of possible solutions the tagger maintains while trying to guess the tags for a sentence. <code class="literal">N</code> defaults to 1,000. Increasing it will greatly increase the amount of memory used during tagging, without necessarily increasing accuracy. Decreasing <code class="literal">N</code> will decrease memory usage, but could also decrease accuracy. Here's what happens when you set <code class="literal">N=100</code>:</p><div><pre class="programlisting">&gt;&gt;&gt; tnt_tagger = tnt.TnT(N=100)
&gt;&gt;&gt; tnt_tagger.train(train_sents)
&gt;&gt;&gt; tnt_tagger.evaluate(test_sents)
0.87580401467731495</pre></div><p>So the accuracy is exactly the same, but we use significantly less memory to achieve it. However, don't assume that accuracy will not change if you decrease <code class="literal">N</code>; experiment with your own data to be sure.</p></div><div><div><div><div><h3 class="title"><a id="ch04lvl3sec52"/>Capitalization significance</h3></div></div></div><a id="id293" class="indexterm"/><p>You can pass <code class="literal">C=True</code> if you want capitalization of words to be significant. The default is <code class="literal">C=False</code>, which means all words are lowercased. The documentation on <code class="literal">C</code> says that treating capitalization as significant probably will not increase accuracy. In my own testing, there was a very slight (&lt; 0.01%) increase in accuracy with <code class="literal">C=True</code>, probably because case-sensitivity can help identify proper nouns.</p></div></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec161"/>See also</h2></div></div></div><p>We covered the <code class="literal">DefaultTagger</code> in the <em>Default tagging</em> recipe, backoff tagging in the <em>Combining taggers with backoff tagging</em> recipe, <code class="literal">NgramTagger</code> subclasses in the <em>Training a unigram part-of-speech tagger</em> and <em>Training combining Ngram taggers</em> recipes, <code class="literal">RegexpTagger</code> in the <em>Tagging with regular expressions</em> recipe, and the <code class="literal">AffixTagger</code> in the <em>Affix tagging</em> recipe.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch04lvl1sec46"/>Using WordNet for tagging</h1></div></div></div><a id="id294" class="indexterm"/><a id="id295" class="indexterm"/><p>If you remember from the <em>Looking up synsets for a word in Wordnet</em> recipe in <a class="link" href="ch01.html" title="Chapter 1. Tokenizing Text and WordNet Basics">Chapter 1</a>, <em>Tokenizing Text and WordNet Basics</em>, WordNet synsets specify a part-of-speech tag. It's a very restricted set of possible tags, and many words have multiple synsets with different part-of-speech tags, but this information can be useful for tagging unknown words. WordNet is essentially a giant dictionary, and it's likely to contain many words that are not in your training data.</p><div><div><div><div><h2 class="title"><a id="ch04lvl2sec162"/>Getting ready</h2></div></div></div><p>First, we need to decide how to map WordNet part-of-speech tags to the Penn Treebank part-of-speech tags we have been using. The following is a table mapping one to the other. See the <em>Looking up synsets for a word in Wordnet</em> recipe in <a class="link" href="ch01.html" title="Chapter 1. Tokenizing Text and WordNet Basics">Chapter 1</a>, <em>Tokenizing Text and WordNet Basics</em> for more details. The "s", which was not shown before, is just another kind of adjective, at least for tagging purposes.</p><div><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>WordNet Tag</p>
</th><th style="text-align: left" valign="bottom">
<p>Treebank Tag</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p>n</p>
</td><td style="text-align: left" valign="top">
<p>NN</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>a</p>
</td><td style="text-align: left" valign="top">
<p>JJ</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>s</p>
</td><td style="text-align: left" valign="top">
<p>JJ</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>r</p>
</td><td style="text-align: left" valign="top">
<p>RB</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>v</p>
</td><td style="text-align: left" valign="top">
<p>VB</p>
</td></tr></tbody></table></div></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec163"/>How to do it...</h2></div></div></div><a id="id296" class="indexterm"/><a id="id297" class="indexterm"/><p>Now we can create a class that will look up words in WordNet, then chose the most common tag from the synsets it finds. The <code class="literal">WordNetTagger</code> defined next can be found in <code class="literal">taggers.py</code>:</p><div><pre class="programlisting">from nltk.tag import SequentialBackoffTagger
from nltk.corpus import wordnet
from nltk.probability import FreqDist

class WordNetTagger(SequentialBackoffTagger):
  '''
  &gt;&gt;&gt; wt = WordNetTagger()
  &gt;&gt;&gt; wt.tag(['food', 'is', 'great'])
  [('food', 'NN'), ('is', 'VB'), ('great', 'JJ')]
  '''
  def __init__(self, *args, **kwargs):
    SequentialBackoffTagger.__init__(self, *args, **kwargs)
    self.wordnet_tag_map = {
      'n': 'NN',
      's': 'JJ',
      'a': 'JJ',
      'r': 'RB',
      'v': 'VB'
    }
  def choose_tag(self, tokens, index, history):
    word = tokens[index]
    fd = FreqDist()
    for synset in wordnet.synsets(word):
      fd.inc(synset.pos)
    return self.wordnet_tag_map.get(fd.max())</pre></div></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec164"/>How it works...</h2></div></div></div><a id="id298" class="indexterm"/><a id="id299" class="indexterm"/><p>The <code class="literal">WordNetTagger</code> simply counts the number of each part-of-speech tag found in the synsets for a word. The most common tag is then mapped to a <code class="literal">treebank</code> tag using an internal mapping. Here's some sample usage code:</p><div><pre class="programlisting">&gt;&gt;&gt; from taggers import WordNetTagger
&gt;&gt;&gt; wn_tagger = WordNetTagger()
&gt;&gt;&gt; wn_tagger.evaluate(train_sents)
0.18451574615215904</pre></div><a id="id300" class="indexterm"/><a id="id301" class="indexterm"/><p>So it's not too accurate, but that's to be expected. We only have enough information to produce four different kinds of tags, while there are 36 possible tags in <code class="literal">treebank</code>. And many words can have different part-of-speech tags depending on their context. But if we put the <code class="literal">WordNetTagger</code> at the end of an <code class="literal">NgramTagger</code> backoff chain, then we can <a id="id302" class="indexterm"/>
<a id="id303" class="indexterm"/>improve accuracy over the <code class="literal">DefaultTagger</code>.</p><div><pre class="programlisting">&gt;&gt;&gt; from tag_util import backoff_tagger
&gt;&gt;&gt; from nltk.tag import UnigramTagger, BigramTagger, TrigramTagger
&gt;&gt;&gt; tagger = backoff_tagger(train_sents, [UnigramTagger, BigramTagger, TrigramTagger], backoff=wn_tagger)
&gt;&gt;&gt; tagger.evaluate(test_sents)
0.88564644938484782</pre></div></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec165"/>See also</h2></div></div></div><p>The <em>Looking up synsets for a word in Wordnet</em> recipe in <a class="link" href="ch01.html" title="Chapter 1. Tokenizing Text and WordNet Basics">Chapter 1</a>, <em>Tokenizing Text and WordNet Basics</em> details how to use the <code class="literal">wordnet</code> corpus and what kinds of part-of-speech tags it knows about. And in the <em>Combining taggers with backoff tagging</em> and <em>Training and combining Ngram taggers</em> recipes, we went over backoff tagging with ngram taggers.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch04lvl1sec47"/>Tagging proper names</h1></div></div></div><a id="id304" class="indexterm"/><p>Using the included <code class="literal">names</code> corpus, we can create a simple tagger for tagging names as <em>proper nouns</em>.</p><div><div><div><div><h2 class="title"><a id="ch04lvl2sec166"/>How to do it...</h2></div></div></div><a id="id305" class="indexterm"/><a id="id306" class="indexterm"/><p>The <code class="literal">NamesTagger</code> is a subclass of <code class="literal">SequentialBackoffTagger</code> as it's probably only useful near the end of a backoff chain. At initialization, we create a set of all names in the <code class="literal">names</code> corpus, lowercasing each name to make lookup easier. Then we implement the <code class="literal">choose_tag()</code> method, which simply checks if the current word is in the <code class="literal">names_set</code>. If it is, we return the tag <em>NNP</em> (which is the tag for <em>proper nouns</em>). If it isn't, we return <code class="literal">None</code> so the next tagger in the chain can tag the word. The following code can be found in <code class="literal">taggers.py</code>:</p><div><pre class="programlisting">from nltk.tag import SequentialBackoffTagger
from nltk.corpus import names

class NamesTagger(SequentialBackoffTagger):
  def __init__(self, *args, **kwargs):
    SequentialBackoffTagger.__init__(self, *args, **kwargs)
    self.name_set = set([n.lower() for n in names.words()])
  def choose_tag(self, tokens, index, history):
    word = tokens[index]
    if word.lower() in self.name_set:
      return 'NNP'
    else:
      return None</pre></div></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec167"/>How it works...</h2></div></div></div><a id="id307" class="indexterm"/><a id="id308" class="indexterm"/><p>
<code class="literal">NamesTagger</code> should be pretty self-explanatory. Its usage is also simple:</p><div><pre class="programlisting">&gt;&gt;&gt; from taggers import NamesTagger
&gt;&gt;&gt; nt = NamesTagger()
&gt;&gt;&gt; nt.tag(['Jacob'])
[('Jacob', 'NNP')]</pre></div><p>It's probably best to use the <code class="literal">NamesTagger</code> right before a <code class="literal">DefaultTagger</code>, so it's at the end of a backoff chain. But it could probably go anywhere in the chain since it's unlikely to mistag a word.</p></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec168"/>See also</h2></div></div></div><p>The <em>Combining taggers with backoff tagging</em> recipe goes over the details of using <code class="literal">SequentialBackoffTagger</code> subclasses.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch04lvl1sec48"/>Classifier based tagging</h1></div></div></div><a id="id309" class="indexterm"/><a id="id310" class="indexterm"/><p>The <code class="literal">ClassifierBasedPOSTagger</code> uses <em>classification</em> to do part-of-speech tagging. <a id="id311" class="indexterm"/>
<strong>Features</strong> are extracted from words, then passed to an internal classifier. The classifier classifies the features and returns a label; in this case, a part-of-speech tag. Classification will be covered in detail in <a class="link" href="ch07.html" title="Chapter 7. Text Classification">Chapter 7</a>, <em>Text Classification</em>.</p><a id="id312" class="indexterm"/><p>
<code class="literal">ClassifierBasedPOSTagger</code> is a subclass of <code class="literal">ClassifierBasedTagger</code> that implements a <a id="id313" class="indexterm"/>
<strong>feature detector</strong> that combines many of the techniques of previous taggers into a single <strong>feature set</strong>
<a id="id314" class="indexterm"/>. The feature detector finds multiple length suffixes, does some regular expression matching, and looks at the unigram, bigram, and trigram history to produce a fairly complete set of features for each word. The feature sets it produces are used to train the internal classifier, and are used for classifying words into part-of-speech tags.</p><div><div><div><div><h2 class="title"><a id="ch04lvl2sec169"/>How to do it...</h2></div></div></div><p>Basic usage of the <code class="literal">ClassifierBasedPOSTagger</code> is much like any other <code class="literal">SequentialBackoffTaggger</code>. You pass in training sentences, it trains an internal classifier, and you get a very accurate tagger.</p><div><pre class="programlisting">&gt;&gt;&gt; from nltk.tag.sequential import ClassifierBasedPOSTagger
&gt;&gt;&gt; tagger = ClassifierBasedPOSTagger(train=train_sents)
&gt;&gt;&gt; tagger.evaluate(test_sents)
0.93097345132743359</pre></div><div><div><h3 class="title"><a id="note15"/>Note</h3><p>Notice a slight modification to initialization—<code class="literal">train_sents</code> must be passed in as the <code class="literal">train</code> keyword argument.</p></div></div></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec170"/>How it works...</h2></div></div></div><a id="id315" class="indexterm"/><p>
<code class="literal">ClassifierBasedPOSTagger</code> inherits from <code class="literal">ClassifierBasedTagger</code> and only implements a <code class="literal">feature_detector()</code> method. All the training and tagging is done in <code class="literal">ClassifierBasedTagger</code>. It defaults to training a <code class="literal">NaiveBayesClassifier</code> with the given training data. Once this classifier is trained, it is used to classify word features produced by the <code class="literal">feature_detector()</code> method.</p><div><div><h3 class="title"><a id="note16"/>Note</h3><p>The <code class="literal">ClassifierBasedTagger</code> is often the most accurate tagger, but it's also one of the slowest taggers. If speed is an issue, you should stick with a <code class="literal">BrillTagger</code> based on a backoff chain of <code class="literal">NgramTagger</code> subclasses and other simple taggers.</p></div></div><a id="id316" class="indexterm"/><p>The <code class="literal">ClassifierBasedTagger</code> also inherits from <code class="literal">FeatursetTaggerI</code> (which is just an empty class), creating an inheritance tree that looks like this:</p><div><img src="img/3609OS_04_04.jpg" alt="How it works..."/></div></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec171"/>There's more...</h2></div></div></div><a id="id317" class="indexterm"/><p>You can use a different classifier instead of <code class="literal">NaiveBayesClassifier</code> by passing in your own <code class="literal">classifier_builder</code> function. For example, to use a <code class="literal">MaxentClassifier</code>, you would do the following:</p><div><pre class="programlisting">&gt;&gt;&gt; from nltk.classify import MaxentClassifier
&gt;&gt;&gt; me_tagger = ClassifierBasedPOSTagger(train=train_sents, classifier_builder=MaxentClassifier.train)
&gt;&gt;&gt; me_tagger.evaluate(test_sents)
0.93093028275415501</pre></div><div><div><h3 class="title"><a id="note17"/>Note</h3><p>The <code class="literal">MaxentClassifier</code> takes even longer to train than <code class="literal">NaiveBayesClassifier</code>. If you have <code class="literal">scipy</code> and <code class="literal">numpy</code> installed, training will be faster than normal, but still slower than <code class="literal">NaiveBayesClassifier</code>.</p></div></div><div><div><div><div><h3 class="title"><a id="ch04lvl3sec53"/>Custom feature detector</h3></div></div></div><p>If you want to do your own feature detection, there are two ways to do it.</p><div><ol class="orderedlist arabic"><li class="listitem">Subclass <code class="literal">ClassifierBasedTagger</code> and implement a <code class="literal">feature_detector()</code> method.</li><li class="listitem">Pass a method as the <code class="literal">feature_detector</code> keyword argument into <code class="literal">ClassifierBasedTagger</code> at initialization.</li></ol></div><p>Either way, you need a feature detection method that can take the same arguments as <code class="literal">choose_tag()</code>: <code class="literal">tokens</code>, <code class="literal">index</code>, and <code class="literal">history</code>. But instead of returning a tag, you return a <code class="literal">dict</code> of key-value features, where the key is the feature name, and the value is the feature value. A very simple example would be a unigram feature detector (found in <code class="literal">tag_util.py</code>).</p><div><pre class="programlisting">def unigram_feature_detector(tokens, index, history):
  return {'word': tokens[index]}</pre></div><p>Then using the second method, you would pass the following into <code class="literal">ClassifierBasedTagger</code> as <code class="literal">feature_detector</code>:</p><div><pre class="programlisting">&gt;&gt;&gt; from nltk.tag.sequential import ClassifierBasedTagger
&gt;&gt;&gt; from tag_util import unigram_feature_detector
&gt;&gt;&gt; tagger = ClassifierBasedTagger(train=train_sents, feature_detector=unigram_feature_detector)
&gt;&gt;&gt; tagger.evaluate(test_sents)
0.87338657457371038</pre></div></div><div><div><div><div><h3 class="title"><a id="ch04lvl3sec54"/>Cutoff probability</h3></div></div></div><p>Because a classifier will always return the best result it can, passing in a backoff tagger is useless unless you also pass in a <code class="literal">cutoff_prob</code> to specify the probability threshold for classification. Then, if the probability of the chosen tag is less than <code class="literal">cutoff_prob</code>, the backoff tagger will be used. Here's an example using the <code class="literal">DefaultTagger</code> as the <code class="literal">backoff</code>, and setting <code class="literal">cutoff_prob</code> to <code class="literal">0.3</code>:</p><div><pre class="programlisting">&gt;&gt;&gt; default = DefaultTagger('NN')
&gt;&gt;&gt; tagger = ClassifierBasedPOSTagger(train=train_sents, backoff=default, cutoff_prob=0.3)
&gt;&gt;&gt; tagger.evaluate(test_sents)
0.93110295704726964</pre></div><p>So we get a slight increase in accuracy if the <code class="literal">ClassifierBasedPOSTagger</code> uses the <code class="literal">DefaultTagger</code> whenever its tag probability is less than 30%.</p></div><div><div><div><div><h3 class="title"><a id="ch04lvl3sec55"/>Pre-trained classifier</h3></div></div></div><p>If you want to use a classifier that's already been trained, then you can pass that in to <code class="literal">ClassifierBasedTagger</code> or <code class="literal">ClassifierBasedPOSTagger</code> as <code class="literal">classifier</code>. In this case, the <code class="literal">classifier_builder</code> argument is ignored and no training takes place. However, you must ensure that the classifier has been trained on and can classify feature sets produced by whatever <code class="literal">feature_detector()</code> method you use.</p></div></div><div><div><div><div><h2 class="title"><a id="ch04lvl2sec172"/>See also</h2></div></div></div><p>
<a class="link" href="ch07.html" title="Chapter 7. Text Classification">Chapter 7</a>, <em>Text Classification</em> will cover classification in depth.</p></div></div></body></html>