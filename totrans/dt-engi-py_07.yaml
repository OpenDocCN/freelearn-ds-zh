- en: '*Chapter 6*: Building a 311 Data Pipeline'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous three chapters, you learned how to use Python, Airflow, and
    NiFi to build data pipelines. In this chapter, you will use those skills to create
    a pipeline that connects to **SeeClickFix** and downloads all the issues for a
    city, and then loads it in Elasticsearch. I am currently running this pipeline
    every 8 hours. I use this pipeline as a source of open source intelligence – using
    it to monitor quality of life issues in neighborhoods, as well as reports of abandoned
    vehicles, graffiti, and needles. Also, it's really interesting to see what kinds
    of things people complain to their city about – during the COVID-19 pandemic,
    my city has seen several reports of people not social distancing at clubs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Building the data pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a Kibana dashboard
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building the data pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This data pipeline will be slightly different from the previous pipelines in
    that we will need to use a trick to start it off. We will have two paths to the
    same database – one of which we will turn off once it has run the first time,
    and we will have a processor that connects to itself for the success relationship.
    The following screenshot shows the completed pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – The complete pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_6.1_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.1 – The complete pipeline
  prefs: []
  type: TYPE_NORMAL
- en: The preceding screenshot may look complicated, but I assure you that it will
    make sense by the end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Mapping a data type
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before you can build the pipeline, you need to map a field in Elasticsearch
    so that you get the benefit of the coordinates by mapping them as the geopoint
    data type. To do that, open Kibana at `http://localhost:5601`. At the toolbar,
    select **Dev Tools** (the wrench icon) and enter the code shown in the left panel
    of the following screenshot, and then click the run arrow. If it was successful,
    you will see output in the right panel, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Adding geopoint mapping'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_6.2_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.2 – Adding geopoint mapping
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have created the `scf` index with geopoint mapping, when you run
    your pipeline, the `coords` field will be converted into spatial coordinates in
    Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start building.
  prefs: []
  type: TYPE_NORMAL
- en: Triggering a pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, I mentioned that you would need to trick the data pipeline
    into starting. Remember that this pipeline will connect to an API endpoint, and
    in order to do that, the NiFi processors for calling HTTP endpoints, as well as
    the `ExecuteScript` processer that you will use, require an incoming flowfile
    to start them. This processor cannot be the first processor in a data pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start the data pipeline, you will use the `GenerateFlowFile` processor.
    Drag and drop the processor on the canvas. Double-click on it to change the configuration.
    In the `Start Flow Fake Data`. This lets us know that this processor sends fake
    data just to start the flow. The configuration will use all the defaults and look
    like the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – Configuring the GenerateFlowfile processor'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_6.3_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.3 – Configuring the GenerateFlowfile processor
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, in the **SCHEDULING** tab, set the processor to run at your desired
    interval. I use 8 because I do not want to overwhelm the API.
  prefs: []
  type: TYPE_NORMAL
- en: The processor, when running, will generate a single flowfile with 0 bytes of
    data. It is empty, but it does contain metadata generated by NiFi. However, this
    empty flowfile will do the trick and start the next processor. That is where the
    work begins.
  prefs: []
  type: TYPE_NORMAL
- en: Querying SeeClickFix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous NiFi examples, you did not use any code, just configurations
    to make the processor do what you needed. We could do that in this pipeline. However,
    now is a good time to introduce coding using Python – Jython – into your pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 'Drag and drop the `ExecuteScript` processor to the canvas. Double-click on
    it to edit the configuration. Starting with the `Query SCF` so that I know it
    queries **SeeClickFix**. In the **Properties** tab, set **Script Engine** to **Python**.
    In the **Script Body** parameter, you will write the Python code that the processor
    will execute. The query steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You need to import the required libraries. The following code is the libraries
    that you will always need to include:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, you will create the class that will be called to handle the work. The
    `process` function will contain the code that will perform the task:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Lastly, assume that no errors have occurred, and check whether there is a flowfile.
    If there is one, write the flowfile calling the class. Next, check whether an
    error occurred. If there was an error, you will send the flowfile to the failure
    relationship, otherwise, send it to the success relationship:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding code is the boiler plate for any Python `ExecuteScript` processor.
    The only thing you will need to change will be in the process function, which
    we will do in the steps that follow.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Because NiFi uses Jython, you can add many Python libraries to the Jython environment,
    but that is beyond the scope of this book. For now, you will use the standard
    libraries.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To make a call to the SeeClickFix API, you will need to import the `urllib`
    libraries and `json`, as shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, you will put the code in the `process` function. The code will be a `try`
    `except` block that makes a request to the HTTP endpoint and writes out the response
    to `outputStream`. If there was an error, the `except` block will set `errorOccurred`
    to `True` and this will trigger the rest of the code to send the flowfile to the
    `Failure` relationship. The only line in the `try` block that is not standard
    Python for using `urllib` is `outputStream.write()`. This is where you write to
    the flowfile:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The preceding code, when successful, will output a JSON flowfile. The contents
    of the flowfile will contain some metadata and an array of issues. The two pieces
    of metadata we will be interested in are **page** and **pages**.
  prefs: []
  type: TYPE_NORMAL
- en: You have grabbed the first 100 issues for Bernalillo County, and will pass this
    flowfile to two different processors – `GetEveryPage` and `SplitJson`. We will
    follow the `SplitJson` path, as this path will send the data to Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: Transforming the data for Elasticsearch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following are the steps for transforming data for Elasticsearch:'
  prefs: []
  type: TYPE_NORMAL
- en: Drag and drop the `SplitJson` processor to the canvas. Double-click on it to
    modify the properties. In the **Properties** tab, set the **JsonPath Expression**
    property to **$.issues**. This processor will now split the 100 issues into their
    own flowfiles.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, you need to add coordinates in the format expected by NiFi. We will use
    an `x`, `y` string named `coords`. To do that, drag and drop an `ExecuteScript`
    processor to the canvas. Double-click on it and click the `import json` statement.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The `process` function will convert the input stream to a string. The input
    stream is the flowfile contents from the previous processor. In this case, it
    is a single issue. Then it will use the `json` library to load it as `json`. You
    then add a field named `coords` and assign it the value of a concatenated string
    of the `lat` and `lng` fields in the flowfile JSON. Lastly, you write the JSON
    back to the output stream as a new flowfile:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now you have a single issue, with a new field called `coords`, that is a string
    format that Elasticsearch recognizes as a geopoint. You are almost ready to load
    the data in Elasticsearch, but first you need a unique identifier.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To create the equivalent of a primary key in Elasticsearch, you can specify
    an ID. The JSON has an ID for each issue that you can use. To do so, drag and
    drop the `EvaluateJsonPath` processor on to the canvas. Double-click on it and
    select the `id` with the value of `$.id`. Remember that `$.` allows you to specify
    a JSON field to extract. The flowfile now contains a unique ID extracted from
    the JSON.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Drag and drop the `PutElasticsearchHttp` processor on to the canvas. Double-click
    on it to edit the properties. Set the `http://localhost:9200`. In the optional
    **Identifier Attribute** property, set the value to **id**. This is the attribute
    you just extracted in the previous processor. Set the **Index** to **SCF** (short
    for **SeeClickFix**), and the **Type** to **doc**. Lastly, you will set the **Index
    Operation** property to **upsert**. In Elasticsearch, **upsert** will index the
    document if the ID does not already exist, and it will update if the ID exists,
    and the data is different. Otherwise, nothing will happen, and the record will
    be ignored, which is what you want if the data is already the same.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The issues are now being loaded in Elasticsearch, and if you were to check,
    you will have 100 documents in your `scf` index. But there are a lot more than
    100 records in the SeeClickFix data for Bernalillo County; there are 44 pages
    of records (4,336 issues) according to the metadata from the `QuerySCF` processor.
  prefs: []
  type: TYPE_NORMAL
- en: The following section will show you how to grab all the data.
  prefs: []
  type: TYPE_NORMAL
- en: Getting every page
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you queried SeeClickFix, you sent the results to two paths. We took the
    `SplitJson` path. The reason for this is because on the initial query, you got
    back 100 issues and how many pages of issues exist (as part of the metadata).
    You sent the issues to the `SplitJson` path, because they were ready to process,
    but now you need to do something with the number of pages. We will do that by
    following the `GetEveryPage` path.
  prefs: []
  type: TYPE_NORMAL
- en: Drag and drop an `ExecuteScript` processor on to the canvas. Double-click on
    it to edit the `urllib` and `json` libraries.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `process` function will convert the input stream to JSON, and then it will
    load it using the `json` library. The main logic of the function states that if
    the current page is less than or equal to the total number of pages, call the
    API and request the next page (`next_page_url`), and then write out the JSON as
    a flowfile. Otherwise, it stops. The code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: You will connect the relationship success for this processor to the `SplitJson`
    processor in the last path we took. The flowfile will be split on issues, coordinates
    added, the ID extracted, and the issue sent to Elasticsearch. However, we need
    to do this 42 times.
  prefs: []
  type: TYPE_NORMAL
- en: To keep processing pages, you need to connect the success relationship to itself.
    That's right; you can connect a processor to itself. When you processed the first
    page through this processor, the next page was 2\. The issues were sent to `SplitJson`,
    and back to this processor, which said the current page is less than 44 and the
    next page is 3\.
  prefs: []
  type: TYPE_NORMAL
- en: You now have an Elasticsearch index with all of the current issues from SeeClickFix.
    However, the number of issues for Bernalillo County is much larger than the set
    of current issues – there is an archive. And now that you have a pipeline pulling
    new issues every 8 hours, you will always be up to date, but you can backfill
    Elasticsearch with all of the archived issues as well. Then you will have the
    full history of issues.
  prefs: []
  type: TYPE_NORMAL
- en: Backfilling data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To backfill the `SCF` index with historic data only requires the addition of
    a single parameter to the `params` object in the `QuerySCF` processor. To do that,
    right-click on the `QuerySCF` processor and select `QuerySCFArchive`. In the `params`
    object to the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The `status` parameter was added with the value `Archived`. Now, connect the
    `GenerateFlowfile` processor to this backfill processor to start it. Then, connect
    the processor to the `SplitJson` processor for the success relationship. This
    will send the issues to Elasticsearch. But you need to loop through all the pages,
    so connect the processor to the `GetEveryPage` processor too. This will loop through
    the archives and send all the issues to Elasticsearch. Once this pipeline finishes,
    you can stop the `QuerySCFArchive` processor.
  prefs: []
  type: TYPE_NORMAL
- en: When you have a system that is constantly adding new records – like a transactional
    system – you will follow this pattern often. You will build a data pipeline to
    extract the recent records and extract the new records at a set interval – daily
    or hourly depending on how often the system updates or how much in real time you
    need it to be. Once your pipeline is working, you will add a series of processors
    to grab all the historic data and backfill your warehouse. You may not need to
    go back to the beginning of time, but in this case, there were sufficiently few
    records to make it feasible.
  prefs: []
  type: TYPE_NORMAL
- en: You will also follow this pattern if something goes wrong or if you need to
    populate a new warehouse. If your warehouse becomes corrupted or you bring a new
    warehouse online, you can rerun this backfill pipeline to bring in all the data
    again, making the new database complete. But it will only contain current state.
    The next chapter deals with production pipelines and will help you solve this
    problem by improving your pipelines. For now, let's visualize your new Elasticsearch
    index in Kibana.
  prefs: []
  type: TYPE_NORMAL
- en: Building a Kibana dashboard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that your SeeClickFix data pipeline has loaded data in Elasticsearch, it
    would be nice to see the results of the data, as would an analyst. Using Kibana,
    you can do just that. In this section, you will build a Kibana dashboard for your
    data pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'To open Kibana, browse to `http://localhost:5601` and you will see the main
    window. At the bottom of the toolbar (on the left of the screen; you may need
    to expand it), click the management icon at the bottom. You need to select `scf*`,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – Creating the index pattern in Kibana](img/Figure_6.4_B15739.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – Creating the index pattern in Kibana
  prefs: []
  type: TYPE_NORMAL
- en: 'When you click the next step, you will be asked to select a `created_at`, as
    shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – Selecting the Time Filter field'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_6.5_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.5 – Selecting the Time Filter field
  prefs: []
  type: TYPE_NORMAL
- en: Once you have created the index in Kibana, you can move on to visualizations.
  prefs: []
  type: TYPE_NORMAL
- en: Creating visualizations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To create visualizations, select the visualization icon in the toolbar. Select
    **Create Visualization** and you will see a variety of types available, as shown
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – Available visualization types'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_6.6_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.6 – Available visualization types
  prefs: []
  type: TYPE_NORMAL
- en: 'You will see the `scf` — this will apply to all visualizations in this chapter.
    Leave the y axis as `created_at` and the interval will be **Monthly**. You will
    see a chart as shown in the following screenshot (yours may vary):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7 – Bar chart of created_at counts by month'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_6.7_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.7 – Bar chart of created_at counts by month
  prefs: []
  type: TYPE_NORMAL
- en: Save the bar chart and name it `scf-bar`, or anything that you will be able
    to associate with the SeeClickFix data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, select visualization again and choose metric. You will only add a custom
    label under the **Metrics** options. I chose **Issues**. By doing this, you remove
    the default count that gets placed under the numbers in the metric. This visualization
    is giving us a count of issues and will change when we apply filters in the dashboard.
    The configuration is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8 – Metrics visualization configuration'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_6.8_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.8 – Metrics visualization configuration
  prefs: []
  type: TYPE_NORMAL
- en: Again, save the visualization using any convention, or prefix it with `scf`,
    as I have done.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the next visualization, select a pie chart – which will default to a donut.
    Under **Buckets**, select **Split slices**. For **Aggregations**, select **Terms**.
    And for **Field**, select **request_type.title.keyword**. Leave the rest of the
    defaults set. This will give you the top five titles. The results are shown in
    the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.9 – Top five issue titles'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_6.9_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.9 – Top five issue titles
  prefs: []
  type: TYPE_NORMAL
- en: 'While not a visualization, **Markdown** can add value to your dashboard by
    providing some context or a description. Select **Markdown** from the visualization
    options. You can enter Markdown in the left pane and, by clicking the run symbol,
    see the preview in the right pane. I have just added an H1, some text, and a bullet
    list, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.10 – Using the Markdown editor'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_6.10_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.10 – Using the Markdown editor
  prefs: []
  type: TYPE_NORMAL
- en: 'The last visualization, `scf`. Once on the map screen, select **Add Layer**
    and the source will be **Documents**. This allows you to select an index. The
    following screenshot shows what you should see:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.11 – Adding a new layer with the source being documents](img/Figure_6.11_B15739.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.11 – Adding a new layer with the source being documents
  prefs: []
  type: TYPE_NORMAL
- en: 'When you select `scf` as the index pattern, Kibana will recognize the appropriate
    field and add the data to the map. Your map will be blank, and you may wonder
    went wrong. Kibana sets the time filter to the last 15 minutes, and you do not
    have data newer than the last 8 hours. Set the filter to a longer time frame,
    and the data will appear if the `create_at` field is in the window. The results
    are shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.12 – A map visualization from an Elasticsearch index'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_6.12_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.12 – A map visualization from an Elasticsearch index
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have created visualizations from your data, you can now move on
    to combining them into a dashboard. The next section will show you how.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a dashboard
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To build a dashboard, select the dashboard icon on the toolbar. You will then
    select `scf` – or any of the names you used to save your visualizations. Adding
    them to the dashboard, you can then position them and resize them. Make sure to
    save your dashboard once it is set up. I have built the dashboard shown in the
    following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.13 – A SeeClickFix dashboard'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_6.13_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.13 – A SeeClickFix dashboard
  prefs: []
  type: TYPE_NORMAL
- en: The dashboard has the Markdown, pie chart, map, metric, and bar chart added.
    I moved them around by grabbing the top of the panel and resized them by grabbing
    the lower-right corner and dragging. You can also click the gear icon and add
    a new name for your panels, so that they do not have the name that you used when
    you save the visualization.
  prefs: []
  type: TYPE_NORMAL
- en: 'With your dashboard, you can filter the data and all the visualizations will
    change. For example, I have clicked on the Graffiti label in the pie chart and
    the results are shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.14 – Filtering on Graffiti'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_6.14_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.14 – Filtering on Graffiti
  prefs: []
  type: TYPE_NORMAL
- en: 'Using filters is where the metric visualization comes in handy. It is nice
    to know what the number of records are. You can see that the map and the bar chart
    changed as well. You can also filter on the date range. I have selected the last
    7 days in the filter, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.15 – Filtering by time in a dashboard'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_6.15_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.15 – Filtering by time in a dashboard
  prefs: []
  type: TYPE_NORMAL
- en: 'The time filter allows you to select **Now**, **Relative**, or **Absolute**.
    **Relative** is a number of days, months, years, and so on from **Now**, while
    **Absolute** allows you to specify a start and end time on a calendar. The results
    of the seven-day filter are shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.16 – Dashboard with a seven-day filter'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_6.16_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.16 – Dashboard with a seven-day filter
  prefs: []
  type: TYPE_NORMAL
- en: 'The last filter I will show is the map filter. You can select an area or draw
    a polygon on the map to filter your dashboard. By clicking on the map tools icon,
    the options will appear as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.17 – Tools icon on the map'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_6.17_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.17 – Tools icon on the map
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the **Draw** bounds to filter data, I drew a rectangle on the map and
    the results are shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.18 – Filtering data using the map'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_6.18_B15739.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.18 – Filtering data using the map
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding dashboard, you can see the perfect rectangle of points. The
    map filter is one of my favorite filters.
  prefs: []
  type: TYPE_NORMAL
- en: Kibana dashboards make your data pipelines useful to non-data engineers. The
    work you put into moving and transforming data becomes live data that can be used
    by analysts and mangers to explore and learn from the data. Kibana dashboards
    are also an excellent way for you, the data engineer, to visualize the data you
    have extracted, transformed, and loaded to see whether there are any obvious issues
    in your data pipeline. They can be a type of debugging tool.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to build a data pipeline using data from a
    REST API. You also added a flow to the data pipeline to allow you to backfill
    the data, or to recreate a database with all of the data using a single pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: The second half of the chapter provided a basic overview of how to build a dashboard
    using Kibana. Dashboards will usually be outside the responsibilities of a data
    engineer. In smaller firms, however, this could very well be your job. Furthermore,
    being able to quickly build a dashboard can help validate your data pipeline and
    look for any possible errors in the data.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we begin a new section of this book, where you will take
    the skills you have learned and improve them by making your pipelines ready for
    production. You will learn about deployment, better validation techniques, and
    other skills needed when you are running pipelines in a production environment.
  prefs: []
  type: TYPE_NORMAL
