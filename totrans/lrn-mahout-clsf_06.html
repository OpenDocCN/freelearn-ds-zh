<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch06"/>Chapter 6. Learning Random Forest Using Mahout</h1></div></div></div><p>Random forest is one of the most popular techniques in classification. It starts with a machine learning technique called <strong>decision tree</strong>. In this chapter, we will explore the following topics:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Decision tree</li><li class="listitem" style="list-style-type: disc">Random forest</li><li class="listitem" style="list-style-type: disc">Using Mahout for Random forest</li></ul></div><div><div><div><div><h1 class="title"><a id="ch06lvl1sec29"/>Decision tree</h1></div></div></div><p>A decision tree is used <a id="id234" class="indexterm"/>for classification and regression problems. In simple terms, it is a predictive model that uses binary rules to calculate the target variable. In a decision tree, we use an iterative process of splitting the data into partitions, then we split it further on branches. As in other classification model creation processes, we start with the training dataset in which target variables or class labels are defined. The algorithm tries to break all the records in training datasets into two parts based on one of the explanatory variables. The partitioning is then applied to each new partition, and this process is continued until no more partitioning can be done. The core of the algorithm is to find out the rule that determines the initial split. There are algorithms to <a id="id235" class="indexterm"/>create decision trees, such as <strong>Iterative </strong>
<a id="id236" class="indexterm"/>
<strong>Dichotomiser 3</strong> (<strong>ID3</strong>), <strong>Classification and Regression Tree</strong> (<strong>CART</strong>), <strong>Chi-squared Automatic Interaction </strong>
<a id="id237" class="indexterm"/>
<strong>Detector</strong> (<strong>CHAID</strong>), and so on. A good explanation for ID3 can be found at <a class="ulink" href="http://www.cse.unsw.edu.au/~billw/cs9414/notes/ml/06prop/id3/id3.html">http://www.cse.unsw.edu.au/~billw/cs9414/notes/ml/06prop/id3/id3.html</a>.</p><p>Forming the explanatory variables to choose the best splitter in a node, the algorithm considers each variable in turn. Every possible split is considered and tried, and the best split is the one that produces the largest decrease in diversity of the classification label within each partition. This is repeated for all variables, and the winner is chosen as the best splitter for that node. The process is continued in the next node until we reach a node where we can make the decision.</p><p>We create a decision tree from a training dataset so it can suffer from the overfitting problem. This behavior creates a problem with real datasets. To improve this situation, a process called <a id="id238" class="indexterm"/>
<strong>pruning</strong> is used. In this process, we remove the branches and leaves of the tree to improve the performance. Algorithms used to build the tree work best at the starting or root node since all the information is available there. Later on, with each split, data is less and towards the end of the tree, a particular node can show patterns that are related to the set of data which is used to split. These patterns create problems when we use them to predict the real <a id="id239" class="indexterm"/>dataset. Pruning methods let the tree grow and remove the smaller branches that fail to generalize. Now take an example to understand the decision tree.</p><p>Consider we have a iris flower dataset. This dataset is hugely popular in the machine learning field. It was introduced by Sir Ronald Fisher. It contains 50 samples from each of three species of iris flower (Iris setosa, Iris virginica, and Iris versicolor). The four explanatory variables are the length and width of the sepals and petals in centimeters, and the target variable is the class to which the flower belongs.</p><div><img src="img/4959OS_06_01.jpg" alt="Decision tree"/></div><p>As you can see in the preceding diagram, all the groups were earlier considered as Sentosa species and then the explanatory variable and petal length were further used to divide the groups. At each step, the calculation for misclassified items was also done, which shows how many <a id="id240" class="indexterm"/>items were wrongly classified. Moreover, the petal width variable was taken into account. Usually, items at leaf nodes are correctly classified.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch06lvl1sec30"/>Random forest</h1></div></div></div><p>The Random forest <a id="id241" class="indexterm"/>algorithm was developed by Leo Breiman and Adele Cutler. Random forests grow many classification trees. They are an ensemble learning method for classification and regression that constructs a number of decision trees at training time and also outputs the class that is the mode of the classes outputted by individual trees.</p><p>Single decision trees show the bias–variance tradeoff. So they usually have high variance or high bias. The following are the parameters in the algorithm:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Bias</strong>: This is an <a id="id242" class="indexterm"/>error caused by an erroneous assumption in the learning algorithm</li><li class="listitem" style="list-style-type: disc"><strong>Variance</strong>: This is <a id="id243" class="indexterm"/>an error that ranges from sensitivity to small fluctuations in the training set</li></ul></div><p>Random forests attempt to mitigate this problem by averaging to find a natural balance between two extremes. A Random forest works on the idea of bagging, which is to average noisy and unbiased models to create a model with low variance. A Random forest algorithm works as a large collection of decorrelated decision trees. To understand the idea of a Random forest algorithm, let's work with an example.</p><p>Consider we have a training dataset that has lots of features (explanatory variables) and target variables or classes:</p><div><img src="img/4959OS_06_02.jpg" alt="Random forest"/></div><p>We create a sample set from the given dataset:</p><div><img src="img/4959OS_06_03.jpg" alt="Random forest"/></div><p>A different set of random features were taken into account to create the random sub-dataset. Now, from these sub-datasets, different decision trees will be created. So actually we have created a forest of the different decision trees. Using these different trees, we will create a ranking system for all the classifiers. To predict the class of a new unknown item, we will use <a id="id244" class="indexterm"/>all the decision trees and separately find out which class these trees are predicting. See the following diagram for a better understanding of this concept:</p><div><img src="img/4959OS_06_04.jpg" alt="Random forest"/><div><p>Different decision trees to predict the class of an unknown item</p></div></div><p>In this particular case, we have four different decision trees. We predict the class of an unknown dataset with each of the trees. As per the preceding figure, the first decision tree provides class 2 as the predicted class, the second decision tree predicts class 5, the third decision tree predicts class 5, and the fourth decision tree predicts class 3. Now, a Random forest will vote for each class. So we have one vote each for class 2 and class 3 and two votes for class 5. Therefore, it has decided that for the new unknown dataset, the predicted class is class 5. So the class that gets a higher vote is decided for the new dataset. A Random forest has a lot of <a id="id245" class="indexterm"/>benefits in classification and a few of them are mentioned in the following list:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Combination of learning models increases the accuracy of the classification</li><li class="listitem" style="list-style-type: disc">Runs effectively on large datasets as well</li><li class="listitem" style="list-style-type: disc">The generated forest can be saved and used for other datasets as well</li><li class="listitem" style="list-style-type: disc">Can handle a large amount of explanatory variables</li></ul></div><p>Now that we have understood the Random forest theoretically, let's move on to Mahout and use the Random forest algorithm, which is available in Apache Mahout.</p></div>
<div><div><div><div><h1 class="title"><a id="ch06lvl1sec31"/>Using Mahout for Random forest</h1></div></div></div><p>Mahout <a id="id246" class="indexterm"/>has implementation for the Random forest <a id="id247" class="indexterm"/>algorithm. It is very easy to understand and use. So let's get started.</p><p>
<strong>Dataset</strong>
</p><p>We will use the <a id="id248" class="indexterm"/>NSL-KDD dataset. Since 1999, KDD'99 has been the most widely used dataset for the evaluation of anomaly detection methods. This dataset is prepared by S. J. Stolfo and is built based on the data captured in the DARPA'98 IDS evaluation program (<em>R. P. Lippmann, D. J. Fried, I. Graf, J. W. Haines, K. R. Kendall, D. McClung, D. Weber, S. E. Webster, D. Wyschogrod, R. K. Cunningham, and M. A. Zissman, "Evaluating intrusion detection systems: The 1998 darpa off-line intrusion detection evaluation," discex, vol. 02, p. 1012, 2000</em>).</p><p>DARPA'98 is about 4 GB of compressed raw (binary) tcp dump data of 7 weeks of network traffic, which can be processed into about 5 million connection records, each with about 100 bytes. The <a id="id249" class="indexterm"/>two weeks of test data have around 2 million connection records. The KDD training dataset consists of approximately 4,900,000 single connection vectors, each of which contains 41 features and is labeled as either normal or an attack, with exactly one specific attack type.</p><p>NSL-KDD is a <a id="id250" class="indexterm"/>dataset suggested to solve some of the inherent problems of the KDD'99 dataset. You can download this dataset from <a class="ulink" href="http://nsl.cs.unb.ca/NSL-KDD/">http://nsl.cs.unb.ca/NSL-KDD/</a>.</p><p>We will download the <strong>KDDTrain+_20Percent.ARFF</strong> and <strong>KDDTest+.ARFF</strong> datasets.</p><div><img src="img/4959OS_06_05.jpg" alt="Using Mahout for Random forest"/></div><div><div><h3 class="title"><a id="note05"/>Note</h3><p>In <strong>KDDTrain+_20Percent.ARFF</strong> and <strong>KDDTest+.ARFF</strong>, remove the first 44 lines (that is, all lines starting with @attribute). If this is not done, we will not be able to <a id="id251" class="indexterm"/>generate a descriptor file.</p></div></div><div><img src="img/4959OS_06_06.jpg" alt="Using Mahout for Random forest"/></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec21"/>Steps to use the Random forest algorithm in Mahout</h2></div></div></div><p>The <a id="id252" class="indexterm"/>steps to implement the Random forest <a id="id253" class="indexterm"/>algorithm in Apache Mahout are as follows:</p><div><ol class="orderedlist arabic"><li class="listitem">Transfer the test and training datasets to <code class="literal">hdfs</code> using the following commands:<div><pre class="programlisting">
<strong>hadoop fs -mkdir /user/hue/KDDTrain</strong>
<strong>hadoop fs -mkdir /user/hue/KDDTest</strong>
<strong>hadoop fs –put /tmp/KDDTrain+_20Percent.arff  /user/hue/KDDTrain</strong>
<strong>hadoop fs –put /tmp/KDDTest+.arff  /user/hue/KDDTest</strong>
</pre></div></li><li class="listitem">Generate the descriptor file. Before you build a Random forest model based on the training data in <strong>KDDTrain+.arff</strong>, a descriptor file is required. This is because all information in the training dataset needs to be labeled. From the labeled dataset, the algorithm can understand which one is numerical and categorical. Use the following command to generate descriptor file:<div><pre class="programlisting">
<strong>hadoop jar  $MAHOUT_HOME/core/target/mahout-core-xyz.job.jar </strong>
<strong>org.apache.mahout.classifier.df.tools.Describe </strong>
<strong>-p /user/hue/KDDTrain/KDDTrain+_20Percent.arff </strong>
<strong>-f /user/hue/KDDTrain/KDDTrain+.info </strong>
<strong>-d N 3 C 2 N C 4 N C 8 N 2 C 19 N L</strong>
</pre></div><p>Jar: Mahout core jar (<code class="literal">xyz</code> stands for version). If you have directly installed Mahout, it can be found under the <code class="literal">/usr/lib/mahout</code> folder. The main class <code class="literal">Describe</code> is used here and it takes three parameters:</p><p>The <code class="literal">p</code> path for the data to be described.</p><p>The <code class="literal">f</code> location for the generated descriptor file.</p><p>
<code class="literal">d</code> is the information for the attribute on the data. N 3 C 2 N C 4 N C 8 N 2 C 19 N L defines that the dataset is starting with a numeric (N), followed by three categorical attributes, and so on. In the last, L defines the label.</p><p>The output of the previous command is shown in the following screenshot:</p><div><img src="img/4959OS_06_07.jpg" alt="Steps to use the Random forest algorithm in Mahout"/></div></li><li class="listitem">Build the Random forest using the following command:<div><pre class="programlisting">
<strong>hadoop jar $MAHOUT_HOME/examples/target/mahout-examples-xyz-job.jar org.apache.mahout.classifier.df.mapreduce.BuildForest  </strong>
<strong>-Dmapred.max.split.size=1874231 -d /user/hue/KDDTrain/KDDTrain+_20Percent.arff  </strong>
<strong>-ds /user/hue/KDDTrain/KDDTrain+.info </strong>
<strong>-sl 5 -p -t 100 –o /user/hue/ nsl-forest</strong>
</pre></div><p>Jar: Mahout example jar (<code class="literal">xyz</code> stands for version). If you have directly installed <a id="id254" class="indexterm"/>Mahout, it can be <a id="id255" class="indexterm"/>found under the <code class="literal">/usr/lib/mahout</code> folder. The main class <code class="literal">build forest</code> is used to build the forest with other arguments, which are shown in the following list:</p><p>
<code class="literal">Dmapred.max.split.size</code> indicates to Hadoop the maximum size of each partition.</p><p>
<code class="literal">d</code> stands for the data path.</p><p>
<code class="literal">ds</code> stands for the location of the descriptor file.</p><p>
<code class="literal">sl</code> is a variable to select randomly at each tree node. Here, each tree is built using five randomly selected attributes per node.</p><p>
<code class="literal">p</code> uses partial data implementation.</p><p>
<code class="literal">t</code> stands for the number of trees to grow. Here, the commands build 100 trees using partial implementation.</p><p>
<code class="literal">o</code> stands for the output path that will contain the decision forest.</p><div><img src="img/4959OS_06_08.jpg" alt="Steps to use the Random forest algorithm in Mahout"/></div><p>In the end, the process will show the following result:</p><div><img src="img/4959OS_06_09.jpg" alt="Steps to use the Random forest algorithm in Mahout"/></div></li><li class="listitem">Use this <a id="id256" class="indexterm"/>model to classify the new dataset:<div><pre class="programlisting">
<strong>hadoop jar $MAHOUT_HOME/examples/target/mahout-examples-xyz-job.jar org.apache.mahout.classifier.df.mapreduce.TestForest </strong>
<strong>-i /user/hue/KDDTest/KDDTest+.arff </strong>
<strong>-ds /user/hue/KDDTrain/KDDTrain+.info -m /user/hue/nsl-forest -a –mr</strong>
<strong> -o /user/hue/predictions</strong>
</pre></div><p>Jar: Mahout <a id="id257" class="indexterm"/>example jar (<code class="literal">xyz</code> stands for version). If you have directly installed Mahout, it can be found under the <code class="literal">/usr/lib/mahout</code> folder. The class to test the forest has the following parameters:</p><p>
<code class="literal">I</code> indicates the path for the test data</p><p>
<code class="literal">ds</code> stands for the location of the descriptor file</p><p>
<code class="literal">m</code> stands for the location of the generated forest from the previous command</p><p>
<code class="literal">a</code> informs to run the analyzer to compute the confusion matrix</p><p>
<code class="literal">mr</code> informs hadoop to distribute the classification</p><p>
<code class="literal">o</code> stands for the location to store the predictions in</p><div><img src="img/4959OS_06_10.jpg" alt="Steps to use the Random forest algorithm in Mahout"/></div><p>The <a id="id258" class="indexterm"/>job provides the <a id="id259" class="indexterm"/>following confusion matrix:</p></li></ol></div><div><img src="img/4959OS_06_11.jpg" alt="Steps to use the Random forest algorithm in Mahout"/></div><p>So, from the confusion matrix, it is clear that 9,396 instances were correctly classified and 315 normal instances were incorrectly classified as anomalies. And the accuracy percentage is 77.7635 (correctly classified instances by the model / classified instances). The output file <a id="id260" class="indexterm"/>in the prediction folder contains <a id="id261" class="indexterm"/>the list where 0 and 1. 0 defines the normal dataset and 1 defines the anomaly.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch06lvl1sec32"/>Summary</h1></div></div></div><p>In this chapter, we discussed the Random forest algorithm. We started our discussion by understanding the decision tree and continued with an understanding of the Random forest. We took up the NSL-KDD dataset, which is used to build predictive systems for cyber security. We used Mahout to build the Random forest tree, and used it with the test dataset and generated the confusion matrix and other statistics for the output.</p><p>In the next chapter, we will look at the final classification algorithm available in Apache Mahout. So stay tuned!</p></div></body></html>