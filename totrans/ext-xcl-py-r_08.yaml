- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Exploratory Data Analysis with R and Python
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 R 和 Python 进行探索性数据分析
- en: '**Exploratory data analysis** (**EDA**) is a crucial initial step in the data
    analysis process for data scientists. It involves the systematic examination and
    visualization of a dataset to uncover its underlying patterns, trends, and insights.
    The primary objectives of EDA are to gain a deeper understanding of the data,
    identify potential problems or anomalies, and inform subsequent analysis and modeling
    decisions.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**探索性数据分析**（**EDA**）是数据科学家在数据分析过程中的一个关键初始步骤。它涉及系统地检查和可视化数据集，以揭示其潜在的规律、趋势和见解。EDA
    的主要目标是更深入地了解数据，识别潜在的问题或异常，并指导后续的分析和建模决策。'
- en: EDA typically starts with a series of data summarization techniques, such as
    calculating basic statistics (mean, median, and standard deviation), generating
    frequency distributions, and examining data types and missing values. These preliminary
    steps provide an overview of the dataset’s structure and quality.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: EDA 通常从一系列数据汇总技术开始，例如计算基本统计量（均值、中位数和标准差）、生成频率分布以及检查数据类型和缺失值。这些初步步骤提供了数据集结构和质量的概述。
- en: Visualization plays a central role in EDA. Data scientists create various charts
    and graphs, including histograms, box plots, scatter plots, and heat maps, to
    visualize the distribution and associations within the data. These visualizations
    help reveal outliers, skewness, correlations, and clusters within the data, aiding
    in the identification of interesting patterns.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化在 EDA 中扮演着核心角色。数据科学家创建各种图表和图形，包括直方图、箱线图、散点图和热图，以可视化数据中的分布和关联。这些可视化有助于揭示数据中的异常值、偏度、相关性和聚类，有助于识别有趣的模式。
- en: Exploring categorical variables involves generating bar charts, pie charts,
    or stacked bar plots to understand the distribution of different categories and
    their relationships. This is valuable for tasks such as customer segmentation
    or market analysis.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 探索分类变量涉及生成条形图、饼图或堆叠条形图，以了解不同类别的分布及其关系。这对于客户细分或市场分析等任务非常有价值。
- en: EDA also involves assessing the relationships between variables. Data scientists
    use correlation matrices, scatter plots, and regression analysis to uncover connections
    and dependencies. Understanding these associations can guide feature selection
    for modeling and help identify potential multicollinearity issues.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: EDA 还涉及评估变量之间的关系。数据科学家使用相关矩阵、散点图和回归分析来揭示联系和依赖关系。理解这些关联可以指导模型中的特征选择，并帮助识别潜在的共线性问题。
- en: Data transformation and cleaning are often performed during EDA to address issues
    such as outliers, missing data, and skewness. Decisions about data imputation,
    scaling, or encoding categorical variables may be made based on the insights gained
    during exploration.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 数据转换和清洗通常在 EDA（探索性数据分析）过程中执行，以解决诸如异常值、缺失数据和偏度等问题。关于数据插补、缩放或编码分类变量的决策可能基于在探索过程中获得的见解。
- en: Overall, EDA is a critical phase in the data science workflow, as it sets the
    stage for subsequent data modeling, hypothesis testing, and decision-making. It
    empowers data scientists to make informed choices about data preprocessing, feature
    engineering, and modeling techniques by providing a comprehensive understanding
    of the dataset’s characteristics and nuances. EDA helps ensure that data-driven
    insights and decisions are based on a solid foundation of data understanding and
    exploration.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，EDA 是数据科学工作流程中的一个关键阶段，因为它为后续的数据建模、假设检验和决策奠定了基础。它通过提供对数据集特征和细微差别的全面理解，使数据科学家能够就数据预处理、特征工程和建模技术做出明智的选择。EDA
    有助于确保基于数据的见解和决策建立在坚实的数据理解和探索基础上。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Exploring data distributions
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索数据分布
- en: Data structure and completeness
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据结构和完整性
- en: EDA with various packages
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用各种包进行 EDA
- en: Technical requirements
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'For this chapter, all scripts and files can be found on GitHub at the following
    link: [https://github.com/PacktPublishing/Extending-Excel-with-Python-and-R/tree/main/Chapter%20](https://github.com/PacktPublishing/Extending-Excel-with-Python-and-R/tree/main/Chapter%208)8.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，所有脚本和文件都可以在以下链接的 GitHub 上找到：[https://github.com/PacktPublishing/Extending-Excel-with-Python-and-R/tree/main/Chapter%208](https://github.com/PacktPublishing/Extending-Excel-with-Python-and-R/tree/main/Chapter%208)。
- en: 'For the R section, we will cover the following libraries:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 R 部分，我们将涵盖以下库：
- en: '`skimr 2.1.5`'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`skimr 2.1.5`'
- en: '`GGally 2.2.0`'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`GGally 2.2.0`'
- en: '`DataExplorer 0.8.3`'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DataExplorer 0.8.3`'
- en: Understanding data with skimr
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 `skimr` 理解数据
- en: As an R programmer, the `skimr` package is a useful tool for providing summary
    statistics about variables that can come in a variety of forms such as data frames
    and vectors. The package provides a larger set of statistics in order to give
    the end user a more robust set of information as compared to the base R `summary()`
    function.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 作为 R 语言程序员，`skimr` 包是一个有用的工具，可以提供关于变量（如数据框和向量）的汇总统计信息。该包提供了一组更丰富的统计信息，以便与基础
    R 的 `summary()` 函数相比，为最终用户提供更稳健的信息集。
- en: To use the `skimr` package, it must first be installed from `CRAN` using the
    `install.packages("skimr")` command. Once installed, the package can be loaded
    using the `library(skimr)` command. The `skim()` function is then used to summarize
    a whole dataset. For example, `skim(iris)` would provide summary statistics for
    the `iris` dataset. The output of `skim()` is printed horizontally, with one section
    per variable type and one row per variable.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 `skimr` 包，首先必须使用 `install.packages("skimr")` 命令从 `CRAN` 安装它。一旦安装，就可以使用 `library(skimr)`
    命令加载包。然后使用 `skim()` 函数来汇总整个数据集。例如，`skim(iris)` 将提供 `iris` 数据集的汇总统计信息。`skim()`
    的输出是水平打印的，每个变量类型一个部分，每个变量一个行。
- en: The package also provides the `skim_to_wide()` function, which converts the
    output of `skim()` to a wide format. This can be useful for exporting the summary
    statistics to a spreadsheet or other external tool.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 该包还提供了 `skim_to_wide()` 函数，它将 `skim()` 的输出转换为宽格式。这可以用于将汇总统计信息导出到电子表格或其他外部工具。
- en: Overall, the `skimr` package is a useful tool for quickly and easily obtaining
    summary statistics about variables in R. It provides a larger set of statistics
    than the `summary()` R base function and is easy to use and customize. The package
    is particularly useful for data exploration and data cleaning tasks, as it allows
    the user to quickly identify potential issues with the data. Now that we have
    a basic understanding of the `skimr` package, let’s see it in use.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，`skimr` 包是一个有用的工具，可以快速轻松地获取 R 中变量的汇总统计信息。它提供的统计信息比 R 基础函数 `summary()` 更丰富，且易于使用和定制。该包特别适用于数据探索和数据清洗任务，因为它允许用户快速识别数据中的潜在问题。现在，我们已经对
    `skimr` 包有了基本的了解，让我们看看它的实际应用。
- en: This R code is used to generate a summary of the `iris` dataset using the `skimr`
    package. The `skimr` package provides a convenient way to quickly summarize and
    visualize key statistics for a dataset.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这段 R 代码用于使用 `skimr` 包生成 `iris` 数据集的汇总。`skimr` 包提供了一种方便的方式来快速汇总和可视化数据集的关键统计信息。
- en: 'Here’s an explanation of each line of code along with the expected output:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是对每一行代码的解释以及预期的输出：
- en: '`if(!require(skimr)){install.packages("skimr")}`: This line checks whether
    the `skimr` package is already installed. If it is not installed, it installs
    the package using `install.packages("skimr")`. This ensures that the `skimr` package
    is available for use in the subsequent code.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`if(!require(skimr)){install.packages("skimr")}`：这一行检查 `skimr` 包是否已经安装。如果没有安装，它将使用
    `install.packages("skimr")` 安装包。这确保了 `skimr` 包在后续代码中使用时可用。'
- en: '`library(skimr)`: This line loads the `skimr` package into the R session. Once
    the package is loaded, you can use its functions and features.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`library(skimr)`：这一行将 `skimr` 包加载到 R 会话中。一旦包被加载，就可以使用其函数和特性。'
- en: '`skim(iris)`: This line calls the `skim()` function from the `skimr` package
    and applies it to the `iris` dataset. The `skim()` function generates a summary
    of the dataset, including statistics and information about each variable (column)
    in the dataset.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`skim(iris)`：这一行调用 `skim()` 函数，来自 `skimr` 包，并应用于 `iris` 数据集。`skim()` 函数生成数据集的汇总，包括每个变量（列）的统计信息和相关信息。'
- en: 'Now, let’s discuss the expected output. When you run the `skim(iris)` command,
    you should see a summary of the `iris` dataset displayed in your R console. The
    output will include statistics and information such as the following:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们讨论预期的输出。当你运行 `skim(iris)` 命令时，你应该在 R 控制台中看到 `iris` 数据集的汇总。输出将包括如下统计信息和相关信息：
- en: '**Counts**: The number of non-missing values for each variable'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计数**：每个变量的非缺失值数量'
- en: '**Missing**: The number of missing values (if any) for each variable'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺失**：每个变量的缺失值数量（如果有）'
- en: '**Unique values**: The number of unique values for each variable'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**唯一值**：每个变量的唯一值数量'
- en: '**Mean**: The mean (average) value for each numeric variable'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均值**：每个数值变量的平均值'
- en: '**Min**: The minimum value for each numeric variable'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最小值**：每个数值变量的最小值'
- en: '**Max**: The maximum value for each numeric variable'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最大值**：每个数值变量的最大值'
- en: '**Standard deviation**: The standard deviation for each numeric variable'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标准差**：每个数值变量的标准差'
- en: Other summary statistics
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他汇总统计数据
- en: 'The output will look something like the following but with more detailed statistics:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于以下内容，但包含更详细的统计数据：
- en: '[PRE0]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This output provides a comprehensive summary of the `iris` dataset, helping
    you quickly understand its structure and key statistics. You can also pass off
    a grouped `tibble` to the `skim()` function and obtain results that way as well.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 此输出提供了对`iris`数据集的全面总结，帮助您快速了解其结构和关键统计数据。您还可以将分组后的`tibble`传递给`skim()`函数，并以此方式获得结果。
- en: Now that we have gone over a simple example of using the `skimr` package to
    explore our data, we can now move on to the `GGally` package.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经通过一个简单的示例了解了如何使用`skimr`包来探索我们的数据，我们现在可以继续了解`GGally`包。
- en: Using the GGally package in R
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在R中使用GGally包
- en: At its core, `GGally` is an extension of the immensely popular `ggplot2` package
    in R. It takes the elegance and flexibility of `ggplot2` and supercharges it with
    a dazzling array of functions, unleashing your creativity to visualize data in
    stunning ways.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在其核心，`GGally`是R中广受欢迎的`ggplot2`包的扩展。它将`ggplot2`的优雅和灵活性提升到一个令人眼花缭乱的函数集合，释放您的创造力，以惊人的方式可视化数据。
- en: With `GGally`, you can effortlessly create beautiful scatter plots, histograms,
    bar plots, and more. What makes it stand out? `GGally` simplifies the process
    of creating complex multivariate plots, saving you time and effort. Want to explore
    correlations, visualize regression models, or craft splendid survival curves?
    `GGally` has your back.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`GGally`，您可以轻松创建美丽的散点图、直方图、条形图等等。它有什么独特之处？`GGally`简化了创建复杂多元图表的过程，为您节省时间和精力。想要探索相关性、可视化回归模型或制作出色的生存曲线吗？`GGally`会支持你。
- en: But `GGally` is not just about aesthetics; it’s about insights. It empowers
    you to uncover hidden relationships within your data through visual exploration.
    The intuitive syntax and user-friendly interface make it accessible to both novices
    and seasoned data scientists.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 但`GGally`不仅仅关乎美观；它关乎洞察。它使您能够通过视觉探索揭示数据中的隐藏关系。直观的语法和用户友好的界面使其既适合新手也适合经验丰富的数据科学家。
- en: What’s even better is that `GGally` encourages collaboration. Its easy-to-share
    visualizations can be a powerful tool for communicating your findings to a wider
    audience, from colleagues to clients.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的是，`GGally`鼓励协作。其易于分享的可视化可以成为向更广泛的受众传达您发现的有力工具，从同事到客户。
- en: 'So, if you’re looking to elevate your data visualization game, give `GGally`
    a try. It’s your trusted ally in the world of data visualization, helping you
    turn numbers into captivating stories. Unlock the true potential of your data
    and let `GGally` be your creative partner in crime. Your data has never looked
    this good! Now, let’s get into using it with what is a simple use case:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果您想提升您的数据可视化技能，不妨试试`GGally`。它是您在数据可视化领域的可靠盟友，帮助您将数字转化为引人入胜的故事。释放数据的真正潜力，让`GGally`成为您的创意伙伴。您的数据从未如此出色！现在，让我们通过一个简单的用例来了解如何使用它：
- en: '[PRE1]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let’s break it down step by step:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步步来分析：
- en: '`if(!require(GGally)){install.packages("GGally")}`: This line checks whether
    the `GGally` package is already installed in your R environment. If it’s not installed,
    it proceeds to install it using `install.packages("GGally")`.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`if(!require(GGally)){install.packages("GGally")}`: 此行检查`GGally`包是否已安装在你的R环境中。如果没有安装，它将使用`install.packages("GGally")`继续安装。'
- en: '`library(GGally)`: After ensuring that `Ggally` is installed, the code loads
    the `GGally` package into the current R session. This package provides tools for
    creating various types of plots and visualizations, including scatter plot matrices.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`library(GGally)`: 在确保`Ggally`已安装后，此代码将`GGally`包加载到当前的R会话中。此包提供创建各种类型图表和可视化的工具，包括散点图矩阵。'
- en: '`library(TidyDensity)`: Similarly, this line loads the `TidyDensity` package,
    which is used for creating tidy density plots. Tidy density plots are a way to
    visualize the distribution of data in a neat and organized manner.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`library(TidyDensity)`: 类似地，此行加载了`TidyDensity`包，用于创建整洁密度图。整洁密度图是一种以整洁和有序的方式可视化数据分布的方法。'
- en: '`tidy_normal(.n = 200)`: Here, the code generates a dataset with 200 random
    data points. These data points are assumed to follow a normal distribution (a
    bell-shaped curve). The `tidy_normal` function is used to create this dataset.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tidy_normal(.n = 200)`：在此，代码生成一个包含 200 个随机数据点的数据集。这些数据点被假定为遵循正态分布（钟形曲线）。`tidy_normal`
    函数用于创建此数据集。'
- en: '`ggpairs(columns = c("y","p","q","dx","dy"))`: This is where the magic happens.
    The `ggpairs` function from the `GGally` package is called with the dataset generated
    earlier. It creates a scatter plot matrix where each combination of variables
    is plotted against each other. The `y`, `p`, `q`, `dx`, and `dy` variables are
    the columns of the dataset to be used for creating the scatter plots.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ggpairs(columns = c("y","p","q","dx","dy"))`：这里发生了魔法。`GGally` 软件包中的 `ggpairs`
    函数被调用，使用之前生成的数据集。它创建了一个散点图矩阵，其中每个变量的组合都相互绘制。`y`、`p`、`q`、`dx` 和 `dy` 变量是用于创建散点图的数据集列。'
- en: 'In summary, this code first installs and loads the necessary R packages (`GGally`
    and `TidyDensity`). Then, it generates a dataset of 200 random points following
    a normal distribution and creates a scatter plot matrix using the `ggpairs` function.
    The scatter plot matrix visualizes the relationships between the specified columns
    of the dataset, allowing you to explore the data’s patterns and correlations.
    Let’s take a look at the resulting plot:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，此代码首先安装并加载必要的 R 软件包（`GGally` 和 `TidyDensity`）。然后，它生成一个包含 200 个随机点的数据集，并使用
    `ggpairs` 函数创建一个散点图矩阵。散点图矩阵可视化数据集中指定列之间的关系，让您探索数据的模式和相关性。让我们看看生成的结果图：
- en: '![Figure 8.1 – Using GGally on 200 points generated from tidy_normal()](img/B19142_08_1.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.1 – 使用 GGally 在 tidy_normal() 生成的 200 个点上进行操作](img/B19142_08_1.jpg)'
- en: Figure 8.1 – Using GGally on 200 points generated from tidy_normal()
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1 – 使用 GGally 在 tidy_normal() 生成的 200 个点上进行操作
- en: The data here was randomly generated so you will most likely not get the exact
    same results. Now that we have covered the `GGally` package with a simple example,
    we can move on to the `DataExplorer` package and see how it compares.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这里生成的是随机数据，因此您可能不会得到完全相同的结果。现在我们已经用一个简单的例子介绍了 `GGally` 软件包，我们可以继续介绍 `DataExplorer`
    软件包，看看它如何与之比较。
- en: Using the DataExplorer package
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 DataExplorer 软件包
- en: The `DataExplorer` R package is created to streamline the majority of data management
    and visualization responsibilities during the EDA process. EDA is a critical and
    primary stage in data analysis, during which analysts take their initial glimpse
    at the data to formulate meaningful hypotheses and determine subsequent action.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`DataExplorer` R 软件包的创建是为了在 EDA 过程中简化大部分数据管理和可视化的责任。EDA 是数据分析中的一个关键和初级阶段，在此阶段，分析师对数据进行初步观察，以形成有意义的假设并确定后续行动。'
- en: '`DataExplorer` provides a variety of functions to do the following:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`DataExplorer` 提供了各种函数来完成以下任务：'
- en: '**Scan and analyze data variables**: The package can automatically scan and
    analyze each variable in a dataset, identifying its type, data distribution, outliers,
    and missing values.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**扫描和分析数据变量**：该软件包可以自动扫描和分析数据集中的每个变量，识别其类型、数据分布、异常值和缺失值。'
- en: '`DataExplorer` provides a variety of visualization functions to help analysts
    understand the relationships between variables and identify patterns in the data.
    These functions include histograms, scatter plots, box plots, heat maps, and correlation
    matrices.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DataExplorer` 提供了各种可视化函数，以帮助分析师理解变量之间的关系并识别数据中的模式。这些函数包括直方图、散点图、箱线图、热图和相关性矩阵。'
- en: '`DataExplorer` also provides functions to transform data, such as converting
    categorical variables to numerical variables, imputing missing values, and scaling
    numerical variables.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DataExplorer` 还提供了转换数据的函数，例如将分类变量转换为数值变量、填充缺失值以及缩放数值变量。'
- en: '`DataExplorer` can be used for a variety of EDA tasks, such as the following:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`DataExplorer` 可以用于各种 EDA 任务，例如以下内容：'
- en: '`DataExplorer` can be used to identify the different types of variables in
    a dataset, their distributions, and their relationships with each other'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DataExplorer` 可以用于识别数据集中的不同类型变量、它们的分布以及它们之间的关系'
- en: '`DataExplorer` can help analysts identify outliers and missing values in their
    data, which can be important to address before building predictive models'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DataExplorer` 可以帮助分析师识别数据中的异常值和缺失值，这在构建预测模型之前解决可能很重要。'
- en: '`DataExplorer` can help analysts generate hypotheses about the data by identifying
    patterns and relationships between variables'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DataExplorer`可以通过识别变量之间的模式和关系来帮助分析师生成关于数据的假设'
- en: 'Here are some examples of how to use the package:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些如何使用该包的示例：
- en: '[PRE2]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: First, we check to see whether the `DataExplorer` package is installed. If it’s
    not, we install it. Then, we load the `DataExplorer` package, as well as the `TidyDensity`
    and `dplyr` packages.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们检查`DataExplorer`包是否已安装。如果没有，我们就安装它。然后，我们加载`DataExplorer`包，以及`TidyDensity`和`dplyr`包。
- en: Next, we create a normally distributed dataset with 200 observations. We use
    the `tidy_normal()` function for this because it’s a convenient way to create
    normally distributed datasets in R. Typically, one will most likely use the *y*
    column only from the `tidy_normal()` output.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个包含200个观测值的正态分布数据集。我们使用`tidy_normal()`函数来做这件事，因为它是在R中创建正态分布数据集的一种方便方式。通常，人们最有可能只使用`tidy_normal()`输出的*y*列。
- en: Once we have our dataset, we use the `introduce()` function from the `DataExplorer`
    package to generate a summary of the data. This summary includes information about
    the number of observations and the types of variables.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了数据集，我们就使用`DataExplorer`包中的`introduce()`函数来生成数据的摘要。这个摘要包括关于观测数和变量类型的详细信息。
- en: Finally, we use the `glimpse()` function from the `dplyr` package to display
    the data transposed. This is a helpful way to get a quick overview of the data
    and to make sure that it looks like we expect it to.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用`dplyr`包中的`glimpse()`函数来显示转置后的数据。这是一种快速了解数据并确保其看起来符合预期的好方法。
- en: 'In other words, this code is a quick and easy way to explore a normally distributed
    dataset in R. It’s great for students and beginners, as well as for experienced
    data scientists who need to get up and running quickly. Now, let’s see the output:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，这段代码是探索R中正态分布数据集的一种快速简单的方法。它非常适合学生和初学者，也适合需要快速开始工作的经验丰富的数据科学家。现在，让我们看看输出：
- en: '[PRE3]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, let’s take a look at the `plot_intro()` function and see its output on
    the same data with a simple call of `df |>` `plot_intro()`:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看一下`plot_intro()`函数，并查看使用简单调用`df |>` `plot_intro()`在相同数据上的输出：
- en: "![Figure 8.2 – The plot_intro() function\uFEFF](img/B19142_08_2.jpg)"
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图8.2 – `plot_intro()`函数](img/B19142_08_2.jpg)'
- en: Figure 8.2 – The plot_intro() function
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2 – `plot_intro()`函数
- en: 'Lastly, we will view the output of the `plot_qq()` function:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将查看`plot_qq()`函数的输出：
- en: '![Figure 8.3 – The plot_qq() function with all data](img/B19142_08_3.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图8.3 – 包含所有数据的`plot_qq()`函数](img/B19142_08_3.jpg)'
- en: Figure 8.3 – The plot_qq() function with all data
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3 – 包含所有数据的`plot_qq()`函数
- en: 'We will now see a **quantile-quantile** (**Q-Q**) plot with only two variables,
    both the *q* and the *y* columns. Here is the code:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将看到一个只包含两个变量的**分位数-分位数**（**Q-Q**）图，这两个变量分别是*q*和*y*列。以下是代码：
- en: '[PRE4]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here is the plot for it:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这是它的图表：
- en: '![Figure 8.4 –  The plot_qq() function with only the y and q columns](img/B19142_08_4.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图8.4 – 只包含y和q列的`plot_qq()`函数](img/B19142_08_4.jpg)'
- en: Figure 8.4 – The plot_qq() function with only the y and q columns
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4 – 只包含y和q列的`plot_qq()`函数
- en: It is our intention for you to understand the Q-Q plot, but if that is not the
    case, a simple search on Google will yield many good results. Another item that
    was never discussed was how to handle missing data in R. There are a great many
    functions that can be used in capturing, cleaning, and otherwise understanding
    them such as `all()`, `any()`, `is.na()`, and `na.omit()`. Here, I advise you
    to explore these on the internet, all of which have been discussed extensively.
    Now that we have gone over several different examples of some functions from different
    packages in R, it’s time to explore the same for Python.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的意图是让您理解Q-Q图，但如果不是这样，简单的谷歌搜索也会得到许多好的结果。另一个从未讨论过的问题是如何在R中处理缺失数据。有许多函数可以用来捕获、清理和了解它们，例如`all()`、`any()`、`is.na()`和`na.omit()`。在这里，我建议您在网上探索这些，所有这些都已经广泛讨论过。现在我们已经讨论了R中不同包中一些函数的几个不同示例，是时候探索Python中的相同内容了。
- en: Getting started with EDA for Python
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始使用Python进行EDA
- en: As explained earlier, EDA is the process of visually and statistically exploring
    datasets to uncover patterns, relationships, and insights. It’s a critical step
    before diving into more complex data analysis tasks. In this section, we’ll introduce
    you to the fundamentals of EDA and show you how to prepare your Python environment
    for EDA.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，EDA（探索性数据分析）是视觉和统计上探索数据集以揭示模式、关系和洞察的过程。这是深入更复杂的数据分析任务之前的一个关键步骤。在本节中，我们将向您介绍EDA的基础知识，并展示如何为EDA准备您的Python环境。
- en: 'EDA is the initial phase of data analysis where you examine and summarize your
    dataset. The primary objectives of EDA are as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: EDA是数据分析的初始阶段，您在此阶段检查和总结您的数据集。EDA的主要目标如下：
- en: '**Understand the data**: Gain insights into the structure, content, and quality
    of your data'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**了解数据**：深入了解数据的结构、内容和质量'
- en: '**Identify patterns**: Discover patterns, trends, and relationships within
    the data'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**识别模式**：在数据中发现模式、趋势和关系'
- en: '**Detect anomalies**: Find outliers and anomalies that may require special
    attention'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**检测异常**：找到可能需要特别关注的异常值和异常'
- en: '**Generate hypotheses**: Formulate initial hypotheses about your data'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成假设**：对您的数据提出初始假设'
- en: '**Prepare for modeling**: Preprocess data for advanced modeling and analysis'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**为建模做准备**：预处理数据以进行高级建模和分析'
- en: 'Before you can perform EDA, you’ll need to set up your Python environment to
    work with Excel data. We have covered the first steps in previous chapters: namely,
    installing necessary libraries and loading data from Excel.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在您能够执行EDA之前，您需要设置您的Python环境以处理Excel数据。我们已在之前的章节中介绍了第一步：即安装必要的库并从Excel加载数据。
- en: 'Next, we will cover the most important basics of EDA: data cleaning and data
    exploration.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍EDA最重要的基础知识：数据清洗和数据探索。
- en: Data cleaning is an essential step in preparing your Excel data for EDA in Python.
    It involves identifying and rectifying various data quality issues that can affect
    the accuracy and reliability of your analysis.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清洗是准备您的Excel数据以在Python中进行EDA的一个关键步骤。它涉及识别和纠正可能影响您分析准确性和可靠性的各种数据质量问题。
- en: Let’s start with the intricacies of data cleaning, without which you cannot
    be confident in the results of your EDA. We will focus on both generic data cleaning
    challenges and those unique to data coming from Excel.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从数据清洗的复杂性开始，没有它，您无法对EDA的结果有信心。我们将关注通用的数据清洗挑战以及来自Excel的数据特有的挑战。
- en: Data cleaning in Python for Excel data
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python中处理Excel数据的数据清洗
- en: Data cleaning is a critical process when working with Excel data in Python.
    It ensures that your data is in the right format and free of errors, enabling
    you to perform accurate EDA.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python处理Excel数据时，数据清洗是一个关键过程。它确保您的数据处于正确的格式且无错误，从而使您能够进行准确的EDA。
- en: 'We will start with generating some dirty data as an example:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以生成一些脏数据作为示例：
- en: '[PRE5]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The data frame created looks like this:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 创建的数据框看起来如下：
- en: '![Figure 8.5 – Using GGally on 200 points generated from tidy_normal()](img/B19142_08_5.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图8.5 – 使用GGally对tidy_normal()生成的200个点进行操作](img/B19142_08_5.jpg)'
- en: Figure 8.5 – Using GGally on 200 points generated from tidy_normal()
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5 – 使用GGally对tidy_normal()生成的200个点进行操作
- en: With the dirty data ready, we can start cleaning it, which includes handling
    missing data, duplicates, and data type conversion when cleaning Excel data. Let’s
    see how to do this.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 脏数据准备就绪后，我们可以开始清洗它，这包括处理缺失数据、重复数据和在清洗Excel数据时进行数据类型转换。让我们看看如何做这件事。
- en: Handling missing data
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理缺失数据
- en: Begin by identifying cells or columns with missing data. In Python, missing
    values are typically represented as **NaN** (short for **Not a Number**) or **None**.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 首先识别有缺失数据的单元格或列。在Python中，缺失值通常表示为**NaN**（代表**Not a Number**）或**None**。
- en: 'Depending on the context, you can choose to replace missing values. Common
    techniques include the following:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上下文，您可以选择替换缺失值。常见的技术包括以下：
- en: Filling missing numeric values with the mean, median, or mode. Do note that
    this will artificially reduce standard error measurements if, for example, a regression
    is performed. Keep this in mind when cleaning data for modeling purposes!
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用均值、中位数或众数填充缺失的数值。请注意，如果进行回归分析，这将会人为地降低标准误差测量值。在为建模目的清洗数据时请记住这一点！
- en: Replacing missing categorical data with the mode (most frequent category).
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用众数（最频繁的类别）替换缺失的类别数据。
- en: Using forward-fill or backward-fill to propagate the previous or next valid
    value.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用前向填充或后向填充来传播前一个或下一个有效值。
- en: Interpolating missing values based on trends in the data.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据数据中的趋势插值缺失值。
- en: Python offers several solutions to do this efficiently and statistically robustly,
    ranging from basic `pandas` methods to dedicated packages with much more robust
    imputation methods such as `fancyimpute`. Imputation should never be applied blindly,
    though, as missing data can be information in its own right as well, and imputed
    values may lead to incorrect analysis results. Domain knowledge, as always, for
    the win.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Python提供了几种高效且统计上稳健的解决方案来完成这项任务，从基本的`pandas`方法到具有更稳健的插补方法的专用包，如`fancyimpute`。然而，插补不应盲目应用，因为缺失数据本身也可以是信息，而插补的值可能会导致分析结果不正确。正如往常一样，领域知识是获胜的关键。
- en: 'It is important to distinguish between the three types of missing data:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 区分三种类型的缺失数据非常重要：
- en: '**Missing completely at** **random** (**MCAR**):'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完全随机缺失**（**MCAR**）：'
- en: In this scenario, the missingness of data is completely random and unrelated
    to any observed or unobserved variables.
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这种情况下，数据的缺失是完全随机的，与任何观察到的或未观察到的变量无关。
- en: The probability of a data point being missing is the same for all observations.
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据点缺失的概率对所有观测值都是相同的。
- en: There are no systematic differences between missing and non-missing values.
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺失值和非缺失值之间没有系统性差异。
- en: 'Here is an example: a survey where respondents accidentally skip some questions.'
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以下是一个例子：受访者意外跳过一些问题的调查。
- en: '**Missing at** **random** (**MAR**):'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机缺失**（**MAR**）：'
- en: Missingness depends on observed data but not on unobserved data.
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺失性取决于观察到的数据，但不取决于未观察到的数据。
- en: The probability of a data point being missing is related to other observed variables
    in the dataset.
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据点缺失的概率与数据集中其他观察到的变量相关。
- en: Once other observed variables are accounted for, the missingness of data is
    random.
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦考虑到其他观察到的变量，数据的缺失性就是随机的。
- en: 'Here is an example: in a survey, men may be less likely to disclose their income
    compared to women. In this case, income data is missing at random conditional
    on gender.'
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以下是一个例子：在一份调查中，男性可能比女性更不愿意透露他们的收入。在这种情况下，收入数据在性别条件下是随机缺失的。
- en: '**Missing not at** **random** (**MNAR**):'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非随机缺失**（**MNAR**）：'
- en: Missingness depends on unobserved data or the missing values themselves.
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺失性取决于未观察到的数据或缺失值本身。
- en: The probability of a data point being missing is related to the missing values
    or other unobserved variables.
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据点缺失的概率与缺失值或其他未观察到的变量相关。
- en: Missingness is not random even after accounting for observed variables.
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使考虑到观察到的变量，缺失性也不是随机的。
- en: 'Here is an example: in a survey about income, high-income earners may be less
    likely to disclose their income.'
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以下是一个例子：在关于收入的调查中，高收入者可能不太愿意透露他们的收入。
- en: 'If a significant portion of a row or column contains missing data, you might
    consider removing that row or column entirely. Be cautious when doing this, as
    it should not result in a substantial loss of information:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一行或一列中有很大一部分包含缺失数据，你可能考虑完全删除该行或列。在这样做时要小心，因为这不应导致信息的大量丢失：
- en: '[PRE6]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Dealing with duplicates
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理重复数据
- en: 'Start with detecting duplicate rows. Python libraries such as `pandas` provide
    functions to detect and handle duplicate rows. To identify duplicates, you can
    use the `duplicated()` method. If applicable, continue with removing duplicate
    rows: after detecting duplicates, you can choose to remove them using the `drop_duplicates()`
    method. Be cautious when removing duplicates, especially in cases where duplicates
    are expected:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 从检测重复行开始。Python库，如`pandas`，提供了检测和处理重复行的函数。要识别重复项，可以使用`duplicated()`方法。如果适用，继续删除重复行：在检测到重复项后，你可以选择使用`drop_duplicates()`方法来删除它们。在删除重复项时要小心，特别是在预期存在重复项的情况下：
- en: '[PRE7]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Handling data type conversion
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理数据类型转换
- en: Reading data from Excel, while highly automated, can lead to incorrectly identified
    data types. In Python, data types are often automatically assigned when you read
    Excel data using libraries such as `pandas`. However, it’s essential to verify
    that each column is assigned the correct data type (e.g., numeric, text, date,
    etc.).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 从Excel读取数据，虽然高度自动化，但可能导致数据类型识别错误。在Python中，当使用`pandas`等库读取Excel数据时，数据类型通常会自动分配。然而，验证每列是否分配了正确的数据类型（例如，数值、文本、日期等）是至关重要的。
- en: The effect of that ranges from a too-large memory footprint to actual semantic
    errors. If, for example, Boolean values are stored as floats, they will be handled
    correctly but will take up a lot more memory (instead of a single bit with 64
    bytes), while trying to convert a string into a float may well break your code.
    To mitigate this risk, first identify data types in the loaded data. Then, convert
    data types to the appropriate types. To convert data types in Python, you can
    use the `astype()` method in `pandas`. For example, to convert a column to a numeric
    data type, you can use `df['Column Name'] =` `df['Column Name'].astype(float)`.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of how that can be done:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Excel-specific data issues
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s have a look at data issues that are specific to data loaded from Excel:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '**Merged cells**: Merged cells in Excel files can cause irregularities in your
    dataset when imported into Python. It’s advisable to unmerge cells in Excel before
    importing. If unmerging isn’t feasible, consider preprocessing these cells within
    Python.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pandas` `na_values` parameter when reading Excel files to specify values that
    should be treated as missing (NaN) during import'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manually adjusting the Excel file to remove unnecessary empty cells before import
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By addressing these common data cleaning issues within Python for Excel data,
    you’ll ensure that your data is in a clean and usable format for your EDA. Clean
    data leads to more accurate and meaningful insights during the exploration phase,
    which we will cover next.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Performing EDA in Python
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With your data loaded and cleaned, you can embark on your initial data exploration
    journey. This phase is crucial for gaining a deep understanding of your dataset,
    revealing its underlying patterns, and identifying potential areas of interest
    or concern.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: These preliminary steps not only provide a solid foundation for your EDA but
    also help you uncover hidden patterns and relationships within your data. Armed
    with this initial understanding, you can proceed to more advanced data exploration
    techniques and dive deeper into the Excel dataset.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: In the subsequent sections, we’ll delve into specific data exploration and visualization
    techniques to further enhance your insights into the dataset. With this knowledge,
    let’s move on to the next section, where we’ll explore techniques for understanding
    data distributions and relationships in greater detail.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Summary statistics
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Begin by generating summary statistics for your dataset. This includes basic
    metrics such as mean, median, standard deviation, and percentiles for numerical
    features. For categorical features, you can calculate frequency counts and percentages.
    These statistics provide an initial overview of the central tendency and spread
    of your data.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Summary statistics provide a concise and informative overview of your data’s
    central tendency, spread, and distribution. This step is essential for understanding
    the basic characteristics of your dataset, whether it contains numerical or categorical
    features. The following sections provide a closer look at generating summary statistics.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要统计信息提供了对数据集中趋势、分布和分布的简洁且信息丰富的概述。这一步骤对于理解数据集的基本特征至关重要，无论它包含数值特征还是分类特征。以下部分将更详细地探讨生成摘要统计信息。
- en: Numerical features
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数值特征
- en: 'For numerical features, the following statistics are a good starting point:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数值特征，以下统计信息是一个良好的起点：
- en: '**Mean**: The mean (average) is a measure of central tendency that represents
    the sum of all numerical values divided by the total number of data points. It
    provides insight into the data’s typical value.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均值**：平均值（平均数）是集中趋势的度量，表示所有数值的总和除以数据点的总数。它提供了对数据典型值的洞察。'
- en: '**Median**: The median is the middle value when all data points are sorted
    in ascending or descending order. It’s a robust measure of central tendency, less
    affected by extreme values (outliers).'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**中位数**：中位数是当所有数据点按升序或降序排序时的中间值。它是一个稳健的集中趋势度量，受极端值（异常值）的影响较小。'
- en: '**Standard deviation**: The standard deviation quantifies the degree of variation
    or dispersion in your data. A higher standard deviation indicates a greater data
    spread.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标准差**：标准差量化了数据中变化或分散的程度。标准差越高，数据分布越广。'
- en: '**Percentiles**: Percentiles represent specific data values below which a given
    percentage of observations fall. For example, the 25th percentile (Q1) marks the
    value below which 25% of the data points lie. Percentiles help identify data distribution
    characteristics.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**百分位数**：百分位数表示特定数据值，以下哪个百分比的数据点落在该值以下。例如，第25百分位数（Q1）表示25%的数据点位于其以下。百分位数有助于识别数据分布特征。'
- en: Categorical features
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分类特征
- en: 'Categorical features have their own set of summary statistics that will give
    you insights:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 分类特征有其自己的摘要统计信息，这将为您提供洞察：
- en: '**Frequency**: For categorical features, calculate the frequency count for
    each unique category. This shows how often each category appears in the dataset.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**频率**：对于分类特征，计算每个唯一类别的频率计数。这显示了每个类别在数据集中出现的频率。'
- en: '**Percentages**: Expressing frequency counts as percentages relative to the
    total number of observations provides insights into the relative prevalence of
    each category.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**百分比**：将频率计数表示为相对于观察总数百分比，可以提供对每个类别相对普遍性的洞察。'
- en: 'Generating summary statistics serves several purposes:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 生成摘要统计信息具有多个目的：
- en: '**Understanding your data**: Summary statistics help you understand the typical
    values and spread of your numerical data. For categorical data, it shows the distribution
    of categories.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**了解您的数据**：摘要统计信息有助于您了解数值数据的典型值和分布。对于分类数据，它显示了类别的分布。'
- en: '**Identifying outliers**: Outliers, which are data points significantly different
    from the norm, can often be detected through summary statistics. Unusually high
    or low mean values may indicate outliers.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**识别异常值**：异常值，即与正常值显著不同的数据点，通常可以通过摘要统计信息检测到。异常高的或低的平均值可能表明存在异常值。'
- en: '**Data quality assessment**: Summary statistics can reveal missing values,
    which may appear as NaN in your dataset. Identifying missing data is crucial for
    data cleaning.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据质量评估**：摘要统计信息可以揭示缺失值，这些值可能在您的数据集中以NaN的形式出现。识别缺失数据对于数据清理至关重要。'
- en: '**Initial insights**: These statistics offer preliminary insights into your
    data’s structure, which can guide subsequent analysis and visualization choices.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**初步洞察**：这些统计信息提供了对数据结构的初步洞察，这可以指导后续分析和可视化选择。'
- en: In Python, libraries such as `pandas` make it straightforward to calculate summary
    statistics. For numerical data, you can use functions such as `.mean()`, `.median()`,
    `.std()`, and `.describe()`. For categorical data, `.value_counts()` and custom
    functions can compute frequency counts and percentages.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，如`pandas`之类的库使计算摘要统计信息变得简单直接。对于数值数据，您可以使用如`.mean()`、`.median()`、`.std()`和`.describe()`之类的函数。对于分类数据，`.value_counts()`和自定义函数可以计算频率计数和百分比。
- en: 'Here’s an example of how you can generate summary statistics for both numerical
    and categorical data using Python and `pandas`. We’ll create sample data for demonstration
    purposes:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例，说明您如何使用Python和`pandas`生成数值和分类数据的摘要统计信息。我们将为演示目的创建样本数据：
- en: '[PRE9]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In this example, we created a sample `DataFrame`, `df`, with columns for `Age`,
    `Gender`, `Income`, and `Region`. We used `df.describe()` to calculate summary
    statistics (mean, standard deviation, min, max, quartiles, and so on) for numerical
    features (`Age` and `Income`). We then used `df['Gender'].value_counts(normalize=True)`
    to calculate frequency counts and percentages for the `Gender` categorical feature.
    The `normalize=True` parameter expresses the counts as percentages. If it is set
    to `False`, the `value_counts()` command returns a series containing the raw counts
    of each unique value in the `Gender` column.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们创建了一个包含`Age`（年龄）、`Gender`（性别）、`Income`（收入）和`Region`（地区）列的样本`DataFrame`，命名为`df`。我们使用`df.describe()`计算数值特征（`Age`和`Income`）的摘要统计（均值、标准差、最小值、最大值、四分位数等）。然后我们使用`df['Gender'].value_counts(normalize=True)`计算`Gender`分类特征的频率计数和百分比。`normalize=True`参数将计数表示为百分比。如果设置为`False`，`value_counts()`命令将返回一个包含`Gender`列中每个唯一值原始计数的序列。
- en: You can adapt this code to your Excel dataset by loading your data into a `pandas`
    `DataFrame` and then applying these summary statistics functions.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过将数据加载到`pandas` `DataFrame`中，然后应用这些摘要统计函数来调整此代码以适应你的Excel数据集。
- en: Once you’ve generated summary statistics, you’ll have a solid foundation for
    your data exploration journey. These statistics provide a starting point for further
    analysis and visualization, helping you uncover valuable insights within your
    Excel dataset.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你生成了摘要统计，你将为你的数据探索之旅打下坚实的基础。这些统计提供了进一步分析和可视化的起点，帮助你发现Excel数据集中的宝贵见解。
- en: Data distribution
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据分布
- en: Understanding the distribution of your data is fundamental. Is it normally distributed
    or skewed, or does it follow another distribution? Identifying the data’s distribution
    informs subsequent statistical analysis and model selection.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 理解你数据的分布是基础性的。它是正态分布、偏斜，还是遵循其他分布？识别数据的分布有助于后续的统计分析模型选择。
- en: 'Before delving deeper into your data, it’s essential to understand its distribution.
    The distribution of data refers to the way data values are spread or arranged.
    Knowing the data distribution helps you make better decisions regarding statistical
    analysis and modeling. Here are some key concepts related to data distribution:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入挖掘你的数据之前，了解其分布是至关重要的。数据的分布指的是数据值是如何分布或排列的。了解数据分布有助于你在统计分析建模方面做出更好的决策。以下是一些与数据分布相关的关键概念：
- en: '**Normal distribution**: In a normal distribution, data is symmetrically distributed
    around the mean, forming a bell-shaped curve. Many statistical techniques are
    simple to apply when the data follows a normal distribution. You can check for
    normality using visualizations such as histograms and Q-Q plots or statistical
    tests such as the Shapiro-Wilk test.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正态分布**：在正态分布中，数据对称地分布在均值周围，形成一个钟形曲线。当数据遵循正态分布时，许多统计技术都很容易应用。你可以使用直方图和Q-Q图等可视化方法或Shapiro-Wilk测试等统计测试来检查正态性。'
- en: '**Skewness**: Skewness measures the asymmetry of the data distribution. A positive
    skew indicates that the tail of the distribution extends to the right, while a
    negative skew means it extends to the left. Identifying skewness is important
    because it can affect the validity of some statistical tests.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**偏度**：偏度衡量数据分布的不对称性。正偏表示分布的尾部延伸到右侧，而负偏则表示延伸到左侧。识别偏度很重要，因为它可能影响某些统计测试的有效性。'
- en: '**Kurtosis**: Kurtosis measures the “heavy-tailedness” or “peakedness” of a
    distribution. A high kurtosis value indicates heavy tails, while a low value suggests
    light tails. Understanding kurtosis helps in selecting appropriate statistical
    models.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**峰度**：峰度衡量分布的“重尾性”或“尖峰性”。高峰度值表示重尾，而低值表示轻尾。理解峰度有助于选择合适的统计模型。'
- en: '**Other distributions**: Data can follow various distributions, such as exponential,
    log-normal, Poisson, or uniform. Counts often follow Poisson distribution, rainfall
    amounts are log-normal distributed, and these distributions are everywhere around
    us! Identifying the correct distribution is essential when choosing statistical
    models and making predictions.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**其他分布**：数据可以遵循各种分布，如指数分布、对数正态分布、泊松分布或均匀分布。计数通常遵循泊松分布，降雨量是对数正态分布的，这些分布无处不在！在选择统计模型和进行预测时，识别正确的分布是至关重要的。'
- en: To analyze data distributions using Python, you can create visualizations such
    as histograms, kernel density plots, and box plots. Additionally, statistical
    tests and libraries such as SciPy can help you identify and quantify departures
    from normality.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用Python分析数据分布，你可以创建诸如直方图、核密度图和箱线图等可视化图表。此外，统计测试和SciPy等库可以帮助你识别和量化与正态性的偏差。
- en: Let’s take a look at a sample code snippet that generates sample data from a
    `lognormal` distribution, performs the Shapiro-Wilk test, creates Q-Q plots with
    both a `Normal` and `lognormal` distribution, and calculates skewness and kurtosis
    statistics!
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个示例代码片段，该代码片段从`lognormal`分布生成样本数据，执行Shapiro-Wilk测试，创建具有`Normal`和`lognormal`分布的Q-Q图，并计算偏度和峰度统计量！
- en: 'First, let’s generate some sample data:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们生成一些样本数据：
- en: '[PRE10]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The resulting histogram shows a very clear skew, making a `Normal` distribution
    unlikely and a `lognormal` distribution the likely best choice (unsurprisingly,
    given how the sample data was generated):'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 结果直方图显示了一个非常明显的偏斜，这使得`Normal`分布不太可能，而对数正态分布很可能是最佳选择（考虑到样本数据的生成方式，这一点并不令人惊讶）：
- en: '![Figure 8.6 – Histogram of the data](img/B19142_08_6.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![图8.6 – 数据的直方图](img/B19142_08_6.jpg)'
- en: Figure 8.6 – Histogram of the data
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6 – 数据的直方图
- en: 'Next, we can perform some basic statistical analysis to confirm our suspicions
    from the histogram – starting with a Shapiro-Wilk test for normality, then plotting
    a Q-Q plot for the `Normal` distribution, and finally, a Q-Q plot against the
    suspected distribution of `lognormal`:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以进行一些基本的统计分析来确认从直方图产生的怀疑——首先进行正态性的Shapiro-Wilk检验，然后绘制`Normal`分布的Q-Q图，最后绘制与疑似`lognormal`分布的Q-Q图：
- en: '[PRE11]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The first Q-Q plot shows that the `Normal` distribution is a very poor fit
    for the data:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个Q-Q图显示，`Normal`分布对数据是一个非常差的拟合：
- en: '![Figure 8.7 – Q-Q plot for the normal distribution](img/B19142_08_7.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![图8.7 – 正态分布的Q-Q图](img/B19142_08_7.jpg)'
- en: Figure 8.7 – Q-Q plot for the normal distribution
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7 – 正态分布的Q-Q图
- en: 'The next Q-Q plot, this time for the `lognormal` distribution, shows a near-perfect
    fit, on the other hand:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个Q-Q图，这次是对`lognormal`分布的，显示了几乎完美的拟合，另一方面：
- en: '![Figure 8.8 –  Q-Q plot for the lognormal distribution](img/B19142_08_8.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![图8.8 – 对数正态分布的Q-Q图](img/B19142_08_8.jpg)'
- en: Figure 8.8 – Q-Q plot for the lognormal distribution
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8 – 对数正态分布的Q-Q图
- en: 'Finally, we calculate skewness and kurtosis to be sure we are on the right
    track and print all the results:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们计算偏度和峰度以确保我们走在正确的道路上，并打印出所有结果：
- en: '[PRE12]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: From the histogram, the Shapiro-Wilk test rejecting the hypothesis of a `normal`
    distribution, the two Q-Q plots, and the skewness and kurtosis estimators, we
    can conclude beyond reasonable doubt that the data was indeed generated from a
    `lognormal` distribution!
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 从直方图、Shapiro-Wilk测试拒绝`normal`分布的假设、两个Q-Q图和偏度与峰度估计量来看，我们可以合理地得出结论，数据确实是从`lognormal`分布生成的！
- en: It is now time to move on from single-variable analyses to the relationship
    between multiple variables.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候从单变量分析转向多个变量之间的关系了。
- en: Associations between variables
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变量之间的关联
- en: Explore relationships between variables through scatter plots and correlation
    matrices. Identify any strong correlations or dependencies between features. This
    step can guide feature selection or engineering in later stages of analysis. In
    this section, we’ll delve into methods to investigate these relationships using
    Python.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 通过散点图和相关性矩阵探索变量之间的关系。识别特征之间的任何强相关性或依赖性。这一步可以在分析的后期阶段引导特征选择或工程。在本节中，我们将深入探讨使用Python研究这些关系的方法。
- en: 'We will use the following libraries and sample dataset:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下库和样本数据集：
- en: '[PRE13]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: You can adapt this code to analyze relationships between variables in your Excel
    data as you have learned in previous chapters.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将此代码改编为分析你在前几章中学到的Excel数据中变量之间的关系。
- en: Correlation heat map
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相关性热图
- en: One of the most common ways to assess relationships is by plotting a correlation
    heat map. This heat map provides a visual representation of the correlations between
    numerical variables in your dataset. Using libraries such as `Seaborn`, you can
    create an informative heat map that color-codes correlations, making it easy to
    identify strong and weak relationships.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 评估关系最常见的方法之一是通过绘制相关性热图。这个热图提供了数据集中数值变量之间相关性的视觉表示。使用`Seaborn`等库，你可以创建一个信息丰富的热图，该热图通过颜色编码相关性，使识别强和弱关系变得容易。
- en: 'Let’s take a look at how to do that:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何做到这一点：
- en: '[PRE14]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In this code, we create a correlation heat map to visualize the relationships
    between these variables. The resulting heat map gives nice insights:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码中，我们创建了一个相关性热图来可视化这些变量之间的关系。生成的热图提供了很好的见解：
- en: '![Figure 8.9 – Correlation heat map](img/B19142_08_9.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.9 – 相关性热图](img/B19142_08_9.jpg)'
- en: Figure 8.9 – Correlation heat map
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.9 – 相关性热图
- en: Predictive Power Score (PPS)
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预测能力分数 (PPS)
- en: Beyond traditional correlation measures, the PPS offers insights into non-linear
    relationships and predictive capabilities between variables. PPS quantifies how
    well one variable can predict another.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 除了传统的相关性度量之外，PPS 还提供了关于变量之间非线性关系和预测能力的见解。PPS 量化了一个变量如何预测另一个变量。
- en: 'Now, we’ll demonstrate how to calculate and visualize PPS matrices using the
    `ppscore` library, giving you a deeper understanding of your data’s predictive
    potential:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将演示如何使用 `ppscore` 库计算和可视化 PPS 矩阵，这将使你对数据的预测潜力有更深入的了解：
- en: '[PRE15]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In this code snippet, we calculate the PPS matrix, which measures the predictive
    power of each feature with respect to the `Target` variable. Finally, we calculate
    the correlation and PPS specifically between `Feature1` and the `Target` variable.
    The results of the PPS heat map help gain further insights into the relationship
    between the variables:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个代码片段中，我们计算了 PPS 矩阵，该矩阵衡量了每个特征相对于 `目标` 变量的预测能力。最后，我们计算了 `特征1` 与 `目标` 变量之间的相关性和
    PPS。PPS 热图的结果有助于进一步了解变量之间的关系：
- en: '![Figure 8.10 – PPS heat map](img/B19142_08_10.jpg)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.10 – PPS 热图](img/B19142_08_10.jpg)'
- en: Figure 8.10 – PPS heat map
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.10 – PPS 热图
- en: Scatter plots
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 散点图
- en: Scatter plots are another valuable tool for visualizing relationships. They
    allow you to explore how two numerical variables interact by plotting data points
    on a 2D plane. We covered creating scatter plots in [*Chapter 6*](B19142_06.xhtml#_idTextAnchor119).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 散点图是另一种用于可视化关系的宝贵工具。它们允许你通过在二维平面上绘制数据点来探索两个数值变量之间的交互。我们在 [*第 6 章*](B19142_06.xhtml#_idTextAnchor119)
    中介绍了创建散点图的方法。
- en: Visualizing key attributes
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化关键属性
- en: Visualization is a powerful tool for understanding your data. You can create
    various plots and charts to visualize key attributes of the dataset. For numerical
    data, histograms, box plots, and scatter plots can reveal data distributions,
    outliers, and potential correlations. Categorical data can be explored through
    bar charts and pie charts, displaying frequency distributions and proportions.
    These visualizations provide insights that may not be apparent in summary statistics
    alone. We covered these methods in [*Chapter 6*](B19142_06.xhtml#_idTextAnchor119).
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化是理解数据的有力工具。你可以创建各种图表来可视化数据集的关键属性。对于数值数据，直方图、箱线图和散点图可以揭示数据分布、异常值和潜在的关联。分类数据可以通过条形图和饼图来探索，显示频率分布和比例。这些可视化提供了在仅汇总统计数据中可能不明显的信息。我们在
    [*第 6 章*](B19142_06.xhtml#_idTextAnchor119) 中介绍了这些方法。
- en: Summary
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we delved into two pivotal processes: data cleaning and EDA
    using R and Python, with a specific focus on Excel data.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们深入探讨了两个关键过程：使用 R 和 Python 进行数据清洗和 EDA，特别关注 Excel 数据。
- en: Data cleaning is a fundamental step. We learned how to address missing data,
    be it through imputation, removal, or interpolation. Dealing with duplicates was
    another key focus, as Excel data, often sourced from multiple places, can be plagued
    with redundancies. Ensuring the correct assignment of data types was emphasized
    to prevent analysis errors stemming from data type issues.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清洗是一个基本步骤。我们学习了如何处理缺失数据，无论是通过插补、删除还是插值。处理重复数据也是另一个关键焦点，因为 Excel 数据通常来自多个地方，可能会受到冗余的困扰。强调正确分配数据类型，以防止由数据类型问题引起的分析错误。
- en: In the realm of EDA, we started with summary statistics. These metrics, such
    as mean, median, standard deviation, and percentiles for numerical features, grant
    an initial grasp of data central tendencies and variability. We then explored
    data distribution, understanding which is critical for subsequent analysis and
    modeling decisions. Lastly, we delved into the relationships between variables,
    employing scatter plots and correlation matrices to unearth correlations and dependencies
    among features.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在 EDA 领域，我们首先从汇总统计开始。这些指标，如数值特征的均值、中位数、标准差和百分位数，为我们提供了对数据集中趋势和变异性的初步了解。然后，我们探讨了数据分布，这对于后续的分析和建模决策至关重要。最后，我们深入研究了变量之间的关系，通过散点图和相关性矩阵来揭示特征之间的相关性依赖。
- en: By mastering these techniques, you’re equipped to navigate Excel data intricacies.
    You can ensure data quality and harness its potential for informed decision making.
    Subsequent chapters will build on this foundation, allowing you to perform advanced
    analysis and visualization to extract actionable insights from your Excel datasets.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 通过掌握这些技术，你将能够应对 Excel 数据的复杂性。你可以确保数据质量并利用其潜力为明智的决策提供支持。随后的章节将在此基础上构建，使你能够执行高级分析和可视化，从而从你的
    Excel 数据集中提取可操作的见解。
- en: 'In the next chapter, we will put the cleaned and explored data to good use:
    we will start with **statistical analysis**, in particular, with linear and logistic
    regression.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将把清洗和探索后的数据充分利用起来：我们将从**统计分析**开始，特别是线性回归和逻辑回归。
