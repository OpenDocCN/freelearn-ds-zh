- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Exploratory Data Analysis with R and Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Exploratory data analysis** (**EDA**) is a crucial initial step in the data
    analysis process for data scientists. It involves the systematic examination and
    visualization of a dataset to uncover its underlying patterns, trends, and insights.
    The primary objectives of EDA are to gain a deeper understanding of the data,
    identify potential problems or anomalies, and inform subsequent analysis and modeling
    decisions.'
  prefs: []
  type: TYPE_NORMAL
- en: EDA typically starts with a series of data summarization techniques, such as
    calculating basic statistics (mean, median, and standard deviation), generating
    frequency distributions, and examining data types and missing values. These preliminary
    steps provide an overview of the dataset’s structure and quality.
  prefs: []
  type: TYPE_NORMAL
- en: Visualization plays a central role in EDA. Data scientists create various charts
    and graphs, including histograms, box plots, scatter plots, and heat maps, to
    visualize the distribution and associations within the data. These visualizations
    help reveal outliers, skewness, correlations, and clusters within the data, aiding
    in the identification of interesting patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring categorical variables involves generating bar charts, pie charts,
    or stacked bar plots to understand the distribution of different categories and
    their relationships. This is valuable for tasks such as customer segmentation
    or market analysis.
  prefs: []
  type: TYPE_NORMAL
- en: EDA also involves assessing the relationships between variables. Data scientists
    use correlation matrices, scatter plots, and regression analysis to uncover connections
    and dependencies. Understanding these associations can guide feature selection
    for modeling and help identify potential multicollinearity issues.
  prefs: []
  type: TYPE_NORMAL
- en: Data transformation and cleaning are often performed during EDA to address issues
    such as outliers, missing data, and skewness. Decisions about data imputation,
    scaling, or encoding categorical variables may be made based on the insights gained
    during exploration.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, EDA is a critical phase in the data science workflow, as it sets the
    stage for subsequent data modeling, hypothesis testing, and decision-making. It
    empowers data scientists to make informed choices about data preprocessing, feature
    engineering, and modeling techniques by providing a comprehensive understanding
    of the dataset’s characteristics and nuances. EDA helps ensure that data-driven
    insights and decisions are based on a solid foundation of data understanding and
    exploration.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring data distributions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data structure and completeness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EDA with various packages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this chapter, all scripts and files can be found on GitHub at the following
    link: [https://github.com/PacktPublishing/Extending-Excel-with-Python-and-R/tree/main/Chapter%20](https://github.com/PacktPublishing/Extending-Excel-with-Python-and-R/tree/main/Chapter%208)8.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the R section, we will cover the following libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '`skimr 2.1.5`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`GGally 2.2.0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DataExplorer 0.8.3`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding data with skimr
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As an R programmer, the `skimr` package is a useful tool for providing summary
    statistics about variables that can come in a variety of forms such as data frames
    and vectors. The package provides a larger set of statistics in order to give
    the end user a more robust set of information as compared to the base R `summary()`
    function.
  prefs: []
  type: TYPE_NORMAL
- en: To use the `skimr` package, it must first be installed from `CRAN` using the
    `install.packages("skimr")` command. Once installed, the package can be loaded
    using the `library(skimr)` command. The `skim()` function is then used to summarize
    a whole dataset. For example, `skim(iris)` would provide summary statistics for
    the `iris` dataset. The output of `skim()` is printed horizontally, with one section
    per variable type and one row per variable.
  prefs: []
  type: TYPE_NORMAL
- en: The package also provides the `skim_to_wide()` function, which converts the
    output of `skim()` to a wide format. This can be useful for exporting the summary
    statistics to a spreadsheet or other external tool.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the `skimr` package is a useful tool for quickly and easily obtaining
    summary statistics about variables in R. It provides a larger set of statistics
    than the `summary()` R base function and is easy to use and customize. The package
    is particularly useful for data exploration and data cleaning tasks, as it allows
    the user to quickly identify potential issues with the data. Now that we have
    a basic understanding of the `skimr` package, let’s see it in use.
  prefs: []
  type: TYPE_NORMAL
- en: This R code is used to generate a summary of the `iris` dataset using the `skimr`
    package. The `skimr` package provides a convenient way to quickly summarize and
    visualize key statistics for a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an explanation of each line of code along with the expected output:'
  prefs: []
  type: TYPE_NORMAL
- en: '`if(!require(skimr)){install.packages("skimr")}`: This line checks whether
    the `skimr` package is already installed. If it is not installed, it installs
    the package using `install.packages("skimr")`. This ensures that the `skimr` package
    is available for use in the subsequent code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`library(skimr)`: This line loads the `skimr` package into the R session. Once
    the package is loaded, you can use its functions and features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`skim(iris)`: This line calls the `skim()` function from the `skimr` package
    and applies it to the `iris` dataset. The `skim()` function generates a summary
    of the dataset, including statistics and information about each variable (column)
    in the dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let’s discuss the expected output. When you run the `skim(iris)` command,
    you should see a summary of the `iris` dataset displayed in your R console. The
    output will include statistics and information such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Counts**: The number of non-missing values for each variable'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Missing**: The number of missing values (if any) for each variable'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unique values**: The number of unique values for each variable'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mean**: The mean (average) value for each numeric variable'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Min**: The minimum value for each numeric variable'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Max**: The maximum value for each numeric variable'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Standard deviation**: The standard deviation for each numeric variable'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other summary statistics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The output will look something like the following but with more detailed statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This output provides a comprehensive summary of the `iris` dataset, helping
    you quickly understand its structure and key statistics. You can also pass off
    a grouped `tibble` to the `skim()` function and obtain results that way as well.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have gone over a simple example of using the `skimr` package to
    explore our data, we can now move on to the `GGally` package.
  prefs: []
  type: TYPE_NORMAL
- en: Using the GGally package in R
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At its core, `GGally` is an extension of the immensely popular `ggplot2` package
    in R. It takes the elegance and flexibility of `ggplot2` and supercharges it with
    a dazzling array of functions, unleashing your creativity to visualize data in
    stunning ways.
  prefs: []
  type: TYPE_NORMAL
- en: With `GGally`, you can effortlessly create beautiful scatter plots, histograms,
    bar plots, and more. What makes it stand out? `GGally` simplifies the process
    of creating complex multivariate plots, saving you time and effort. Want to explore
    correlations, visualize regression models, or craft splendid survival curves?
    `GGally` has your back.
  prefs: []
  type: TYPE_NORMAL
- en: But `GGally` is not just about aesthetics; it’s about insights. It empowers
    you to uncover hidden relationships within your data through visual exploration.
    The intuitive syntax and user-friendly interface make it accessible to both novices
    and seasoned data scientists.
  prefs: []
  type: TYPE_NORMAL
- en: What’s even better is that `GGally` encourages collaboration. Its easy-to-share
    visualizations can be a powerful tool for communicating your findings to a wider
    audience, from colleagues to clients.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, if you’re looking to elevate your data visualization game, give `GGally`
    a try. It’s your trusted ally in the world of data visualization, helping you
    turn numbers into captivating stories. Unlock the true potential of your data
    and let `GGally` be your creative partner in crime. Your data has never looked
    this good! Now, let’s get into using it with what is a simple use case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s break it down step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: '`if(!require(GGally)){install.packages("GGally")}`: This line checks whether
    the `GGally` package is already installed in your R environment. If it’s not installed,
    it proceeds to install it using `install.packages("GGally")`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`library(GGally)`: After ensuring that `Ggally` is installed, the code loads
    the `GGally` package into the current R session. This package provides tools for
    creating various types of plots and visualizations, including scatter plot matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`library(TidyDensity)`: Similarly, this line loads the `TidyDensity` package,
    which is used for creating tidy density plots. Tidy density plots are a way to
    visualize the distribution of data in a neat and organized manner.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tidy_normal(.n = 200)`: Here, the code generates a dataset with 200 random
    data points. These data points are assumed to follow a normal distribution (a
    bell-shaped curve). The `tidy_normal` function is used to create this dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ggpairs(columns = c("y","p","q","dx","dy"))`: This is where the magic happens.
    The `ggpairs` function from the `GGally` package is called with the dataset generated
    earlier. It creates a scatter plot matrix where each combination of variables
    is plotted against each other. The `y`, `p`, `q`, `dx`, and `dy` variables are
    the columns of the dataset to be used for creating the scatter plots.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In summary, this code first installs and loads the necessary R packages (`GGally`
    and `TidyDensity`). Then, it generates a dataset of 200 random points following
    a normal distribution and creates a scatter plot matrix using the `ggpairs` function.
    The scatter plot matrix visualizes the relationships between the specified columns
    of the dataset, allowing you to explore the data’s patterns and correlations.
    Let’s take a look at the resulting plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Using GGally on 200 points generated from tidy_normal()](img/B19142_08_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – Using GGally on 200 points generated from tidy_normal()
  prefs: []
  type: TYPE_NORMAL
- en: The data here was randomly generated so you will most likely not get the exact
    same results. Now that we have covered the `GGally` package with a simple example,
    we can move on to the `DataExplorer` package and see how it compares.
  prefs: []
  type: TYPE_NORMAL
- en: Using the DataExplorer package
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `DataExplorer` R package is created to streamline the majority of data management
    and visualization responsibilities during the EDA process. EDA is a critical and
    primary stage in data analysis, during which analysts take their initial glimpse
    at the data to formulate meaningful hypotheses and determine subsequent action.
  prefs: []
  type: TYPE_NORMAL
- en: '`DataExplorer` provides a variety of functions to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scan and analyze data variables**: The package can automatically scan and
    analyze each variable in a dataset, identifying its type, data distribution, outliers,
    and missing values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DataExplorer` provides a variety of visualization functions to help analysts
    understand the relationships between variables and identify patterns in the data.
    These functions include histograms, scatter plots, box plots, heat maps, and correlation
    matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DataExplorer` also provides functions to transform data, such as converting
    categorical variables to numerical variables, imputing missing values, and scaling
    numerical variables.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DataExplorer` can be used for a variety of EDA tasks, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`DataExplorer` can be used to identify the different types of variables in
    a dataset, their distributions, and their relationships with each other'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DataExplorer` can help analysts identify outliers and missing values in their
    data, which can be important to address before building predictive models'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DataExplorer` can help analysts generate hypotheses about the data by identifying
    patterns and relationships between variables'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are some examples of how to use the package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: First, we check to see whether the `DataExplorer` package is installed. If it’s
    not, we install it. Then, we load the `DataExplorer` package, as well as the `TidyDensity`
    and `dplyr` packages.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we create a normally distributed dataset with 200 observations. We use
    the `tidy_normal()` function for this because it’s a convenient way to create
    normally distributed datasets in R. Typically, one will most likely use the *y*
    column only from the `tidy_normal()` output.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have our dataset, we use the `introduce()` function from the `DataExplorer`
    package to generate a summary of the data. This summary includes information about
    the number of observations and the types of variables.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we use the `glimpse()` function from the `dplyr` package to display
    the data transposed. This is a helpful way to get a quick overview of the data
    and to make sure that it looks like we expect it to.
  prefs: []
  type: TYPE_NORMAL
- en: 'In other words, this code is a quick and easy way to explore a normally distributed
    dataset in R. It’s great for students and beginners, as well as for experienced
    data scientists who need to get up and running quickly. Now, let’s see the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let’s take a look at the `plot_intro()` function and see its output on
    the same data with a simple call of `df |>` `plot_intro()`:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 8.2 – The plot_intro() function\uFEFF](img/B19142_08_2.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 – The plot_intro() function
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, we will view the output of the `plot_qq()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – The plot_qq() function with all data](img/B19142_08_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 – The plot_qq() function with all data
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now see a **quantile-quantile** (**Q-Q**) plot with only two variables,
    both the *q* and the *y* columns. Here is the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the plot for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 –  The plot_qq() function with only the y and q columns](img/B19142_08_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 – The plot_qq() function with only the y and q columns
  prefs: []
  type: TYPE_NORMAL
- en: It is our intention for you to understand the Q-Q plot, but if that is not the
    case, a simple search on Google will yield many good results. Another item that
    was never discussed was how to handle missing data in R. There are a great many
    functions that can be used in capturing, cleaning, and otherwise understanding
    them such as `all()`, `any()`, `is.na()`, and `na.omit()`. Here, I advise you
    to explore these on the internet, all of which have been discussed extensively.
    Now that we have gone over several different examples of some functions from different
    packages in R, it’s time to explore the same for Python.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with EDA for Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As explained earlier, EDA is the process of visually and statistically exploring
    datasets to uncover patterns, relationships, and insights. It’s a critical step
    before diving into more complex data analysis tasks. In this section, we’ll introduce
    you to the fundamentals of EDA and show you how to prepare your Python environment
    for EDA.
  prefs: []
  type: TYPE_NORMAL
- en: 'EDA is the initial phase of data analysis where you examine and summarize your
    dataset. The primary objectives of EDA are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Understand the data**: Gain insights into the structure, content, and quality
    of your data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Identify patterns**: Discover patterns, trends, and relationships within
    the data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Detect anomalies**: Find outliers and anomalies that may require special
    attention'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generate hypotheses**: Formulate initial hypotheses about your data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prepare for modeling**: Preprocess data for advanced modeling and analysis'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Before you can perform EDA, you’ll need to set up your Python environment to
    work with Excel data. We have covered the first steps in previous chapters: namely,
    installing necessary libraries and loading data from Excel.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will cover the most important basics of EDA: data cleaning and data
    exploration.'
  prefs: []
  type: TYPE_NORMAL
- en: Data cleaning is an essential step in preparing your Excel data for EDA in Python.
    It involves identifying and rectifying various data quality issues that can affect
    the accuracy and reliability of your analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with the intricacies of data cleaning, without which you cannot
    be confident in the results of your EDA. We will focus on both generic data cleaning
    challenges and those unique to data coming from Excel.
  prefs: []
  type: TYPE_NORMAL
- en: Data cleaning in Python for Excel data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data cleaning is a critical process when working with Excel data in Python.
    It ensures that your data is in the right format and free of errors, enabling
    you to perform accurate EDA.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start with generating some dirty data as an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The data frame created looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5 – Using GGally on 200 points generated from tidy_normal()](img/B19142_08_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 – Using GGally on 200 points generated from tidy_normal()
  prefs: []
  type: TYPE_NORMAL
- en: With the dirty data ready, we can start cleaning it, which includes handling
    missing data, duplicates, and data type conversion when cleaning Excel data. Let’s
    see how to do this.
  prefs: []
  type: TYPE_NORMAL
- en: Handling missing data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Begin by identifying cells or columns with missing data. In Python, missing
    values are typically represented as **NaN** (short for **Not a Number**) or **None**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on the context, you can choose to replace missing values. Common
    techniques include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Filling missing numeric values with the mean, median, or mode. Do note that
    this will artificially reduce standard error measurements if, for example, a regression
    is performed. Keep this in mind when cleaning data for modeling purposes!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replacing missing categorical data with the mode (most frequent category).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using forward-fill or backward-fill to propagate the previous or next valid
    value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interpolating missing values based on trends in the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python offers several solutions to do this efficiently and statistically robustly,
    ranging from basic `pandas` methods to dedicated packages with much more robust
    imputation methods such as `fancyimpute`. Imputation should never be applied blindly,
    though, as missing data can be information in its own right as well, and imputed
    values may lead to incorrect analysis results. Domain knowledge, as always, for
    the win.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is important to distinguish between the three types of missing data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Missing completely at** **random** (**MCAR**):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this scenario, the missingness of data is completely random and unrelated
    to any observed or unobserved variables.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The probability of a data point being missing is the same for all observations.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: There are no systematic differences between missing and non-missing values.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is an example: a survey where respondents accidentally skip some questions.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Missing at** **random** (**MAR**):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Missingness depends on observed data but not on unobserved data.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The probability of a data point being missing is related to other observed variables
    in the dataset.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Once other observed variables are accounted for, the missingness of data is
    random.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is an example: in a survey, men may be less likely to disclose their income
    compared to women. In this case, income data is missing at random conditional
    on gender.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Missing not at** **random** (**MNAR**):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Missingness depends on unobserved data or the missing values themselves.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The probability of a data point being missing is related to the missing values
    or other unobserved variables.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Missingness is not random even after accounting for observed variables.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is an example: in a survey about income, high-income earners may be less
    likely to disclose their income.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If a significant portion of a row or column contains missing data, you might
    consider removing that row or column entirely. Be cautious when doing this, as
    it should not result in a substantial loss of information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Dealing with duplicates
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Start with detecting duplicate rows. Python libraries such as `pandas` provide
    functions to detect and handle duplicate rows. To identify duplicates, you can
    use the `duplicated()` method. If applicable, continue with removing duplicate
    rows: after detecting duplicates, you can choose to remove them using the `drop_duplicates()`
    method. Be cautious when removing duplicates, especially in cases where duplicates
    are expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Handling data type conversion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Reading data from Excel, while highly automated, can lead to incorrectly identified
    data types. In Python, data types are often automatically assigned when you read
    Excel data using libraries such as `pandas`. However, it’s essential to verify
    that each column is assigned the correct data type (e.g., numeric, text, date,
    etc.).
  prefs: []
  type: TYPE_NORMAL
- en: The effect of that ranges from a too-large memory footprint to actual semantic
    errors. If, for example, Boolean values are stored as floats, they will be handled
    correctly but will take up a lot more memory (instead of a single bit with 64
    bytes), while trying to convert a string into a float may well break your code.
    To mitigate this risk, first identify data types in the loaded data. Then, convert
    data types to the appropriate types. To convert data types in Python, you can
    use the `astype()` method in `pandas`. For example, to convert a column to a numeric
    data type, you can use `df['Column Name'] =` `df['Column Name'].astype(float)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of how that can be done:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Excel-specific data issues
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s have a look at data issues that are specific to data loaded from Excel:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Merged cells**: Merged cells in Excel files can cause irregularities in your
    dataset when imported into Python. It’s advisable to unmerge cells in Excel before
    importing. If unmerging isn’t feasible, consider preprocessing these cells within
    Python.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pandas` `na_values` parameter when reading Excel files to specify values that
    should be treated as missing (NaN) during import'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manually adjusting the Excel file to remove unnecessary empty cells before import
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By addressing these common data cleaning issues within Python for Excel data,
    you’ll ensure that your data is in a clean and usable format for your EDA. Clean
    data leads to more accurate and meaningful insights during the exploration phase,
    which we will cover next.
  prefs: []
  type: TYPE_NORMAL
- en: Performing EDA in Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With your data loaded and cleaned, you can embark on your initial data exploration
    journey. This phase is crucial for gaining a deep understanding of your dataset,
    revealing its underlying patterns, and identifying potential areas of interest
    or concern.
  prefs: []
  type: TYPE_NORMAL
- en: These preliminary steps not only provide a solid foundation for your EDA but
    also help you uncover hidden patterns and relationships within your data. Armed
    with this initial understanding, you can proceed to more advanced data exploration
    techniques and dive deeper into the Excel dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In the subsequent sections, we’ll delve into specific data exploration and visualization
    techniques to further enhance your insights into the dataset. With this knowledge,
    let’s move on to the next section, where we’ll explore techniques for understanding
    data distributions and relationships in greater detail.
  prefs: []
  type: TYPE_NORMAL
- en: Summary statistics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Begin by generating summary statistics for your dataset. This includes basic
    metrics such as mean, median, standard deviation, and percentiles for numerical
    features. For categorical features, you can calculate frequency counts and percentages.
    These statistics provide an initial overview of the central tendency and spread
    of your data.
  prefs: []
  type: TYPE_NORMAL
- en: Summary statistics provide a concise and informative overview of your data’s
    central tendency, spread, and distribution. This step is essential for understanding
    the basic characteristics of your dataset, whether it contains numerical or categorical
    features. The following sections provide a closer look at generating summary statistics.
  prefs: []
  type: TYPE_NORMAL
- en: Numerical features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For numerical features, the following statistics are a good starting point:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mean**: The mean (average) is a measure of central tendency that represents
    the sum of all numerical values divided by the total number of data points. It
    provides insight into the data’s typical value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Median**: The median is the middle value when all data points are sorted
    in ascending or descending order. It’s a robust measure of central tendency, less
    affected by extreme values (outliers).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Standard deviation**: The standard deviation quantifies the degree of variation
    or dispersion in your data. A higher standard deviation indicates a greater data
    spread.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Percentiles**: Percentiles represent specific data values below which a given
    percentage of observations fall. For example, the 25th percentile (Q1) marks the
    value below which 25% of the data points lie. Percentiles help identify data distribution
    characteristics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Categorical features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Categorical features have their own set of summary statistics that will give
    you insights:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Frequency**: For categorical features, calculate the frequency count for
    each unique category. This shows how often each category appears in the dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Percentages**: Expressing frequency counts as percentages relative to the
    total number of observations provides insights into the relative prevalence of
    each category.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Generating summary statistics serves several purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Understanding your data**: Summary statistics help you understand the typical
    values and spread of your numerical data. For categorical data, it shows the distribution
    of categories.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Identifying outliers**: Outliers, which are data points significantly different
    from the norm, can often be detected through summary statistics. Unusually high
    or low mean values may indicate outliers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data quality assessment**: Summary statistics can reveal missing values,
    which may appear as NaN in your dataset. Identifying missing data is crucial for
    data cleaning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Initial insights**: These statistics offer preliminary insights into your
    data’s structure, which can guide subsequent analysis and visualization choices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In Python, libraries such as `pandas` make it straightforward to calculate summary
    statistics. For numerical data, you can use functions such as `.mean()`, `.median()`,
    `.std()`, and `.describe()`. For categorical data, `.value_counts()` and custom
    functions can compute frequency counts and percentages.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example of how you can generate summary statistics for both numerical
    and categorical data using Python and `pandas`. We’ll create sample data for demonstration
    purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we created a sample `DataFrame`, `df`, with columns for `Age`,
    `Gender`, `Income`, and `Region`. We used `df.describe()` to calculate summary
    statistics (mean, standard deviation, min, max, quartiles, and so on) for numerical
    features (`Age` and `Income`). We then used `df['Gender'].value_counts(normalize=True)`
    to calculate frequency counts and percentages for the `Gender` categorical feature.
    The `normalize=True` parameter expresses the counts as percentages. If it is set
    to `False`, the `value_counts()` command returns a series containing the raw counts
    of each unique value in the `Gender` column.
  prefs: []
  type: TYPE_NORMAL
- en: You can adapt this code to your Excel dataset by loading your data into a `pandas`
    `DataFrame` and then applying these summary statistics functions.
  prefs: []
  type: TYPE_NORMAL
- en: Once you’ve generated summary statistics, you’ll have a solid foundation for
    your data exploration journey. These statistics provide a starting point for further
    analysis and visualization, helping you uncover valuable insights within your
    Excel dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Data distribution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Understanding the distribution of your data is fundamental. Is it normally distributed
    or skewed, or does it follow another distribution? Identifying the data’s distribution
    informs subsequent statistical analysis and model selection.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before delving deeper into your data, it’s essential to understand its distribution.
    The distribution of data refers to the way data values are spread or arranged.
    Knowing the data distribution helps you make better decisions regarding statistical
    analysis and modeling. Here are some key concepts related to data distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Normal distribution**: In a normal distribution, data is symmetrically distributed
    around the mean, forming a bell-shaped curve. Many statistical techniques are
    simple to apply when the data follows a normal distribution. You can check for
    normality using visualizations such as histograms and Q-Q plots or statistical
    tests such as the Shapiro-Wilk test.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Skewness**: Skewness measures the asymmetry of the data distribution. A positive
    skew indicates that the tail of the distribution extends to the right, while a
    negative skew means it extends to the left. Identifying skewness is important
    because it can affect the validity of some statistical tests.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kurtosis**: Kurtosis measures the “heavy-tailedness” or “peakedness” of a
    distribution. A high kurtosis value indicates heavy tails, while a low value suggests
    light tails. Understanding kurtosis helps in selecting appropriate statistical
    models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Other distributions**: Data can follow various distributions, such as exponential,
    log-normal, Poisson, or uniform. Counts often follow Poisson distribution, rainfall
    amounts are log-normal distributed, and these distributions are everywhere around
    us! Identifying the correct distribution is essential when choosing statistical
    models and making predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To analyze data distributions using Python, you can create visualizations such
    as histograms, kernel density plots, and box plots. Additionally, statistical
    tests and libraries such as SciPy can help you identify and quantify departures
    from normality.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a look at a sample code snippet that generates sample data from a
    `lognormal` distribution, performs the Shapiro-Wilk test, creates Q-Q plots with
    both a `Normal` and `lognormal` distribution, and calculates skewness and kurtosis
    statistics!
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s generate some sample data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting histogram shows a very clear skew, making a `Normal` distribution
    unlikely and a `lognormal` distribution the likely best choice (unsurprisingly,
    given how the sample data was generated):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.6 – Histogram of the data](img/B19142_08_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.6 – Histogram of the data
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we can perform some basic statistical analysis to confirm our suspicions
    from the histogram – starting with a Shapiro-Wilk test for normality, then plotting
    a Q-Q plot for the `Normal` distribution, and finally, a Q-Q plot against the
    suspected distribution of `lognormal`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The first Q-Q plot shows that the `Normal` distribution is a very poor fit
    for the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.7 – Q-Q plot for the normal distribution](img/B19142_08_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.7 – Q-Q plot for the normal distribution
  prefs: []
  type: TYPE_NORMAL
- en: 'The next Q-Q plot, this time for the `lognormal` distribution, shows a near-perfect
    fit, on the other hand:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.8 –  Q-Q plot for the lognormal distribution](img/B19142_08_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.8 – Q-Q plot for the lognormal distribution
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we calculate skewness and kurtosis to be sure we are on the right
    track and print all the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: From the histogram, the Shapiro-Wilk test rejecting the hypothesis of a `normal`
    distribution, the two Q-Q plots, and the skewness and kurtosis estimators, we
    can conclude beyond reasonable doubt that the data was indeed generated from a
    `lognormal` distribution!
  prefs: []
  type: TYPE_NORMAL
- en: It is now time to move on from single-variable analyses to the relationship
    between multiple variables.
  prefs: []
  type: TYPE_NORMAL
- en: Associations between variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Explore relationships between variables through scatter plots and correlation
    matrices. Identify any strong correlations or dependencies between features. This
    step can guide feature selection or engineering in later stages of analysis. In
    this section, we’ll delve into methods to investigate these relationships using
    Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the following libraries and sample dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: You can adapt this code to analyze relationships between variables in your Excel
    data as you have learned in previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Correlation heat map
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the most common ways to assess relationships is by plotting a correlation
    heat map. This heat map provides a visual representation of the correlations between
    numerical variables in your dataset. Using libraries such as `Seaborn`, you can
    create an informative heat map that color-codes correlations, making it easy to
    identify strong and weak relationships.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at how to do that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'In this code, we create a correlation heat map to visualize the relationships
    between these variables. The resulting heat map gives nice insights:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.9 – Correlation heat map](img/B19142_08_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.9 – Correlation heat map
  prefs: []
  type: TYPE_NORMAL
- en: Predictive Power Score (PPS)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Beyond traditional correlation measures, the PPS offers insights into non-linear
    relationships and predictive capabilities between variables. PPS quantifies how
    well one variable can predict another.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we’ll demonstrate how to calculate and visualize PPS matrices using the
    `ppscore` library, giving you a deeper understanding of your data’s predictive
    potential:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'In this code snippet, we calculate the PPS matrix, which measures the predictive
    power of each feature with respect to the `Target` variable. Finally, we calculate
    the correlation and PPS specifically between `Feature1` and the `Target` variable.
    The results of the PPS heat map help gain further insights into the relationship
    between the variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.10 – PPS heat map](img/B19142_08_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.10 – PPS heat map
  prefs: []
  type: TYPE_NORMAL
- en: Scatter plots
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Scatter plots are another valuable tool for visualizing relationships. They
    allow you to explore how two numerical variables interact by plotting data points
    on a 2D plane. We covered creating scatter plots in [*Chapter 6*](B19142_06.xhtml#_idTextAnchor119).
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing key attributes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Visualization is a powerful tool for understanding your data. You can create
    various plots and charts to visualize key attributes of the dataset. For numerical
    data, histograms, box plots, and scatter plots can reveal data distributions,
    outliers, and potential correlations. Categorical data can be explored through
    bar charts and pie charts, displaying frequency distributions and proportions.
    These visualizations provide insights that may not be apparent in summary statistics
    alone. We covered these methods in [*Chapter 6*](B19142_06.xhtml#_idTextAnchor119).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we delved into two pivotal processes: data cleaning and EDA
    using R and Python, with a specific focus on Excel data.'
  prefs: []
  type: TYPE_NORMAL
- en: Data cleaning is a fundamental step. We learned how to address missing data,
    be it through imputation, removal, or interpolation. Dealing with duplicates was
    another key focus, as Excel data, often sourced from multiple places, can be plagued
    with redundancies. Ensuring the correct assignment of data types was emphasized
    to prevent analysis errors stemming from data type issues.
  prefs: []
  type: TYPE_NORMAL
- en: In the realm of EDA, we started with summary statistics. These metrics, such
    as mean, median, standard deviation, and percentiles for numerical features, grant
    an initial grasp of data central tendencies and variability. We then explored
    data distribution, understanding which is critical for subsequent analysis and
    modeling decisions. Lastly, we delved into the relationships between variables,
    employing scatter plots and correlation matrices to unearth correlations and dependencies
    among features.
  prefs: []
  type: TYPE_NORMAL
- en: By mastering these techniques, you’re equipped to navigate Excel data intricacies.
    You can ensure data quality and harness its potential for informed decision making.
    Subsequent chapters will build on this foundation, allowing you to perform advanced
    analysis and visualization to extract actionable insights from your Excel datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will put the cleaned and explored data to good use:
    we will start with **statistical analysis**, in particular, with linear and logistic
    regression.'
  prefs: []
  type: TYPE_NORMAL
